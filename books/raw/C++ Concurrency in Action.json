{
  "metadata": {
    "title": "C++ Concurrency in Action, 2nd Edition",
    "author": "Anthony Williams",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 592,
    "conversion_date": "2025-11-28T12:02:07.038391",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "C++ Concurrency in Action.pdf",
    "extraction_method": "PyMuPDF (Direct: 590, OCR: 2)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "M A N N I N G\nSECOND EDITION\nAnthony Williams\nIN ACTION\n\n\nPraise for the first edition\n“It’s not just the best current treatment of C++11’s threading facilities ... it’s likely to\nremain the best for some time to come.”\n—Scott Meyers, author of Effective C++ and More Effective C++\n“Simplifies the dark art of C++ multithreading.”\n—Rick Wagner, Red Hat\n“Reading this made my brain hurt. But it’s a good hurt.”\n—Joshua Heyer, Ingersoll Rand\n“Anthony shows how to put concurrency into practice.”\n—Roger Orr, OR/2 Limited\n“A thoughtful, in-depth guide to the new concurrency standard for C++ straight from\nthe mouth of one the horses.”\n—Neil Horlock, Director, Credit Suisse\n“Any serious C++ developers should understand the contents of this important book.”\n—Dr. Jamie Allsop, Development Director\n\n\nC++ Concurrency\nin Action\nSecond Edition\nANTHONY WILLIAMS\nM A N N I N G\nSHELTER ISLAND\n\n\nFor online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2019 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nManning Publications Co.\nDevelopment editors: Cynthia Kane, Jennifer Stout\n20 Baldwin Road\nTechnical development editor: Alain Couniot\nPO Box 761\nReview editor: Aleksandar Dragosavljevic´\nShelter Island, NY 11964\nProduction editor: Janet Vail\nCopy editors: Safis Editing, Heidi Ward\nProofreader: Melody Dolab\nTechnical proofreader: Frédéric Flayol\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781617294693 \nPrinted in the United States of America\n1 2 3 4 5 6 7 8 9 10 – SP – 24 23 22 21 20 19\n\n\n To Kim, Hugh, and Erin\n\n\nvivi\nbrief contents\n1\n■\nHello, world of concurrency in C++!\n1\n2\n■\nManaging threads\n16\n3\n■\nSharing data between threads\n36\n4\n■\nSynchronizing concurrent operations\n72\n5\n■\nThe C++ memory model and operations on \natomic types\n124\n6\n■\nDesigning lock-based concurrent data structures\n173\n7\n■\nDesigning lock-free concurrent data structures\n205\n8\n■\nDesigning concurrent code\n251\n9\n■\nAdvanced thread management\n300\n10\n■\nParallel algorithms\n327\n11\n■\nTesting and debugging multithreaded applications\n339\n\n\nvii\ncontents\npreface\nxiii\nacknowledgments\nxv\nabout this book\nxvii\nabout the author\nxx\nabout the cover illustration\nxxi\n1 \nHello, world of concurrency in C++!\n1\n1.1\nWhat is concurrency?\n2\nConcurrency in computer systems\n2\n■Approaches to \nconcurrency\n4\n■Concurrency vs. parallelism\n6\n1.2\nWhy use concurrency?\n7\nUsing concurrency for separation of concerns\n7\n■Using \nconcurrency for performance: task and data parallelism\n8\nWhen not to use concurrency\n9\n1.3\nConcurrency and multithreading in C++\n10\nHistory of multithreading in C++\n10\n■Concurrency support in the \nC++11 standard\n11\n■More support for concurrency and \nparallelism in C++14 and C++17\n12\n■Efficiency in the C++ \nThread Library\n12\n■Platform-specific facilities\n13\n1.4\nGetting started\n13\nHello, Concurrent World\n14\n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary",
      "content": "CONTENTS\nviii\n2 \nManaging threads\n16\n2.1\nBasic thread management\n17\nLaunching a thread\n17\n■Waiting for a thread to complete\n20\nWaiting in exceptional circumstances\n20\n■Running threads in \nthe background\n22\n2.2\nPassing arguments to a thread function\n24\n2.3\nTransferring ownership of a thread\n27\n2.4\nChoosing the number of threads at runtime\n31\n2.5\nIdentifying threads\n34\n3 \nSharing data between threads\n36\n3.1\nProblems with sharing data between threads\n37\nRace conditions\n38\n■Avoiding problematic race conditions\n39\n3.2\nProtecting shared data with mutexes\n40\nUsing mutexes in C++\n41\n■Structuring code for protecting shared \ndata\n42\n■Spotting race conditions inherent in interfaces\n44\nDeadlock: the problem and a solution\n51\n■Further guidelines \nfor avoiding deadlock\n53\n■Flexible locking with \nstd::unique_lock\n59\n■Transferring mutex ownership between \nscopes\n61\n■Locking at an appropriate granularity\n62\n3.3\nAlternative facilities for protecting shared data\n64\nProtecting shared data during initialization\n65\n■Protecting rarely \nupdated data structures\n68\n■Recursive locking\n70\n4 \nSynchronizing concurrent operations\n72\n4.1\nWaiting for an event or other condition\n73\nWaiting for a condition with condition variables\n74\nBuilding a thread-safe queue with condition variables\n76\n4.2\nWaiting for one-off events with futures\n81\nReturning values from background tasks\n82\n■Associating a task \nwith a future\n84\n■Making (std::)promises\n87\n■Saving an \nexception for the future\n89\n■Waiting from multiple threads\n90\n4.3\nWaiting with a time limit\n93\nClocks\n93\n■Durations\n94\n■Time points\n96\n■Functions \nthat accept timeouts\n98\n\n\nCONTENTS\nix\n4.4\nUsing synchronization of operations to simplify code\n99\nFunctional programming with futures\n99\n■Synchronizing \noperations with message passing\n104\n■Continuation-style \nconcurrency with the Concurrency TS\n108\n■Chaining \ncontinuations\n110\n■Waiting for more than one future\n114\nWaiting for the first future in a set with when_any\n115\nLatches and barriers in the Concurrency TS\n118\n■A basic latch \ntype: std::experimental::latch\n118\n■std::experimental::barrier: \na basic barrier\n120\n■std::experimental::flex_barrier—\nstd::experimental::barrier’s flexible friend\n121\n5 \nThe C++ memory model and operations on atomic types\n124\n5.1\nMemory model basics\n125\nObjects and memory locations\n125\n■Objects, memory locations, \nand concurrency\n126\n■Modification orders\n127\n5.2\nAtomic operations and types in C++\n128\nThe standard atomic types\n128\n■Operations on \nstd::atomic_flag\n132\n■Operations on std::atomic<bool>\n134\nOperations on std::atomic<T*>: pointer arithmetic\n137\nOperations on standard atomic integral types\n138\nThe std::atomic<> primary class template\n138\nFree functions for atomic operations\n140\n5.3\nSynchronizing operations and enforcing ordering\n142\nThe synchronizes-with relationship\n143\n■The happens-before \nrelationship\n145\n■Memory ordering for atomic operations\n146\nRelease sequences and synchronizes-with\n164\n■Fences\n166\nOrdering non-atomic operations with atomics\n168\n■Ordering \nnon-atomic operations\n169\n6 \nDesigning lock-based concurrent data structures\n173\n6.1\nWhat does it mean to design for concurrency?\n174\nGuidelines for designing data structures for concurrency\n175\n6.2\nLock-based concurrent data structures\n176\nA thread-safe stack using locks\n176\n■A thread-safe queue using \nlocks and condition variables\n179\n■A thread-safe queue using \nfine-grained locks and condition variables\n183\n6.3\nDesigning more complex lock-based data structures\n194\nWriting a thread-safe lookup table using locks\n194\n■Writing a \nthread-safe list using locks\n199\n\n\nCONTENTS\nx\n7 \nDesigning lock-free concurrent data structures\n205\n7.1\nDefinitions and consequences\n206\nTypes of nonblocking data structures\n206\n■Lock-free data \nstructures\n207\n■Wait-free data structures\n208\n■The pros and \ncons of lock-free data structures\n208\n7.2\nExamples of lock-free data structures\n209\nWriting a thread-safe stack without locks\n210\n■Stopping those \npesky leaks: managing memory in lock-free data structures\n214\nDetecting nodes that can’t be reclaimed using hazard pointers\n218\nDetecting nodes in use with reference counting\n226\n■Applying the \nmemory model to the lock-free stack\n232\n■Writing a thread-safe \nqueue without locks\n236\n7.3\nGuidelines for writing lock-free data structures\n248\nGuideline: use std::memory_order_seq_cst for prototyping\n248\nGuideline: use a lock-free memory reclamation scheme\n248\nGuideline: watch out for the ABA problem\n249\n■Guideline: \nidentify busy-wait loops and help the other thread\n249\n8 \nDesigning concurrent code\n251\n8.1\nTechniques for dividing work between threads\n252\nDividing data between threads before processing begins\n253\nDividing data recursively\n254\n■Dividing work by task type\n258\n8.2\nFactors affecting the performance of concurrent \ncode\n260\nHow many processors?\n261\n■Data contention and cache \nping-pong\n262\n■False sharing\n264\n■How close is \nyour data?\n265\n■Oversubscription and excessive task \nswitching\n266\n8.3\nDesigning data structures for multithreaded \nperformance\n266\nDividing array elements for complex operations\n267\n■Data access \npatterns in other data structures\n269\n8.4\nAdditional considerations when designing for \nconcurrency\n270\nException safety in parallel algorithms\n271\n■Scalability and \nAmdahl’s law\n277\n■Hiding latency with multiple threads\n279\nImproving responsiveness with concurrency\n280\n\n\nCONTENTS\nxi\n8.5\nDesigning concurrent code in practice\n282\nA parallel implementation of std::for_each\n282\n■A parallel \nimplementation of std::find\n284\n■A parallel implementation of \nstd::partial_sum\n290\n9 \nAdvanced thread management\n300\n9.1\nThread pools\n301\nThe simplest possible thread pool\n301\n■Waiting for tasks \nsubmitted to a thread pool\n303\n■Tasks that wait for other \ntasks\n307\n■Avoiding contention on the work queue\n310\nWork stealing\n311\n9.2\nInterrupting threads\n315\nLaunching and interrupting another thread\n316\n■Detecting \nthat a thread has been interrupted\n318\n■Interrupting a \ncondition variable wait\n318\n■Interrupting a wait on \nstd::condition_variable_any\n321\n■Interrupting other \nblocking calls\n323\n■Handling interruptions\n324\nInterrupting background tasks on application exit\n325\n10 \nParallel algorithms\n327\n10.1\nParallelizing the standard library algorithms\n327\n10.2\nExecution policies\n328\nGeneral effects of specifying an execution policy\n328\nstd::execution::sequenced_policy\n330\nstd::execution::parallel_policy\n330\nstd::execution::parallel_unsequenced_policy\n331\n10.3\nThe parallel algorithms from the C++ Standard \nLibrary\n331\nExamples of using parallel algorithms\n334\nCounting visits\n336\n11 \nTesting and debugging multithreaded applications\n339\n11.1\nTypes of concurrency-related bugs\n340\nUnwanted blocking\n340\n■Race conditions\n341\n11.2\nTechniques for locating concurrency-related bugs\n342\nReviewing code to locate potential bugs\n342\n■Locating \nconcurrency-related bugs by testing\n344\n■Designing for \ntestability\n346\n■Multithreaded testing techniques\n347\nStructuring multithreaded test code\n350\n■Testing the performance \nof multithreaded code\n352\n\n\nCONTENTS\nxii\nappendix A\nBrief reference for some C++11 language features\n354\nappendix B\nBrief comparison of concurrency libraries\n382\nappendix C\nA message-passing framework and complete ATM example\n384\nappendix D\nC++ Thread Library reference\n401\nindex\n551\n\n\nxiii\npreface\nI encountered the concept of multithreaded code while working at my first job after I\nleft college. We were writing a data processing application that had to populate a data-\nbase with incoming data records. There was a lot of data, but each record was inde-\npendent and required a reasonable amount of processing before it could be inserted\ninto the database. To take full advantage of the power of our 10-CPU UltraSPARC, we\nran the code in multiple threads, each thread processing its own set of incoming\nrecords. We wrote the code in C++, using POSIX threads, and made a fair number of\nmistakes—multithreading was new to all of us—but we got there in the end. It was also\nwhile working on this project that I first became aware of the C++ Standards Commit-\ntee and the freshly published C++ Standard.\n I have had a keen interest in multithreading and concurrency ever since. Where\nothers saw it as difficult, complex, and a source of problems, I saw it as a powerful tool\nthat could enable your code to take advantage of the available hardware to run faster.\nLater on, I would learn how it could be used to improve the responsiveness and per-\nformance of applications even on single-core hardware, by using multiple threads to\nhide the latency of time-consuming operations such as I/O. I also learned how it\nworked at the OS level and how Intel CPUs handled task switching.\n Meanwhile, my interest in C++ brought me in contact with the ACCU and then the\nC++ Standards panel at BSI, as well as Boost. I followed the initial development of the\nBoost Thread Library with interest, and when it was abandoned by the original devel-\noper, I jumped at the chance to get involved. I was the primary developer and main-\ntainer of the Boost Thread Library for a number of years, though I have since handed\nthat responsibility on.\n\n\nPREFACE\nxiv\n As the work of the C++ Standards Committee shifted from fixing defects in the\nexisting standard to writing proposals for the C++11 standard (named C++0x in the\nhope that it would be finished by 2009, and then officially C++11, because it was finally\npublished in 2011), I got more involved with BSI and started drafting proposals of my\nown. Once it became clear that multithreading was on the agenda, I jumped in with\nboth feet and authored or co-authored many of the multithreading and concurrency-\nrelated proposals that shaped this part of the standard. I have continued to be\ninvolved with the concurrency group as we worked on the changes for C++17, the\nConcurrency TS, and proposals for the future. I feel privileged to have had the oppor-\ntunity to combine two of my major computer-related interests—C++ and multithread-\ning—in this way.\n This book draws on all my experience with both C++ and multithreading and aims\nto teach other C++ developers how to use the C++17 Thread Library and Concurrency\nTS safely and efficiently. I also hope to impart some of my enthusiasm for the subject\nalong the way.\n\n\nxv\nacknowledgments\nI will start by saying a big “Thank you” to my wife, Kim, for all the love and support she\nhas given me while writing this book. The first edition occupied a significant part of\nmy spare time for the four years before publication, and the second edition has again\nrequired a significant investment of time, and without her patience, support, and\nunderstanding, I couldn’t have managed it.\n Second, I would like to thank the team at Manning who have made this book possi-\nble: Marjan Bace, publisher; Michael Stephens, associate publisher; Cynthia Kane, my\ndevelopment editor; Aleksandar Dragosavljevic´, review editor; Safis Editing and Heidi\nWard, my copyeditors; and Melody Dolab, my proofreader. Without their efforts you\nwould not be reading this book right now.\n I would also like to thank the other members of the C++ Standards Committee\nwho wrote committee papers on the multithreading facilities: Andrei Alexandrescu,\nPete Becker, Bob Blainer, Hans Boehm, Beman Dawes, Lawrence Crowl, Peter Dimov,\nJeff Garland, Kevlin Henney, Howard Hinnant, Ben Hutchings, Jan Kristofferson,\nDoug Lea, Paul McKenney, Nick McLaren, Clark Nelson, Bill Pugh, Raul Silvera, Herb\nSutter, Detlef Vollmann, and Michael Wong, plus all those who commented on the\npapers, discussed them at the committee meetings, and otherwise helped shaped the\nmultithreading and concurrency support in C++11, C++14, C++17, and the Concur-\nrency TS.\n Finally, I would like to thank the following people, whose suggestions have greatly\nimproved this book: Dr. Jamie Allsop, Peter Dimov, Howard Hinnant, Rick Molloy,\nJonathan Wakely, and Dr. Russel Winder, with special thanks to Russel for his detailed\n",
      "page_number": 9
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-25)",
      "start_page": 17,
      "end_page": 25,
      "detection_method": "topic_boundary",
      "content": "ACKNOWLEDGMENTS\nxvi\nreviews and to Frédéric Flayol, who, as technical proofreader, painstakingly checked\nall the content for outright errors in the final manuscript during production. (Any\nremaining mistakes are, of course, all mine.) In addition, I’d like to thank my panel\nof reviewers for the second edition: Al Norman, Andrei de Araújo Formiga, Chad\nBrewbaker, Dwight Wilkins, Hugo Filipe Lopes, Vieira Durana, Jura Shikin, Kent R.\nSpillner, Maria Gemini, Mateusz Malenta, Maurizio Tomasi, Nat Luengnaruemitchai,\nRobert C. Green II, Robert Trausmuth, Sanchir Kartiev, and Steven Parr. Also, thanks\nto the readers of the MEAP edition who took the time to point out errors or highlight\nareas that needed clarifying.\n\n\nxvii\nabout this book\nThis book is an in-depth guide to the concurrency and multithreading facilities from\nthe new C++ Standard, from the basic usage of std::thread, std::mutex, and std::\nasync, to the complexities of atomic operations and the memory model.\nRoadmap\nThe first four chapters introduce the various library facilities provided by the library\nand show how they can be used.\n Chapter 5 covers the low-level nitty-gritty of the memory model and atomic opera-\ntions, including how atomic operations can be used to impose ordering constraints on\nother code, and marks the end of the introductory chapters.\n Chapters 6 and 7 start the coverage of higher-level topics, with some examples of\nhow to use the basic facilities to build more complex data structures—lock-based data\nstructures in chapter 6, and lock-free data structures in chapter 7.\n Chapter 8 continues the higher-level topics, with guidelines for designing multi-\nthreaded code, coverage of the issues that affect performance, and example imple-\nmentations of various parallel algorithms.\n Chapter 9 covers thread management—thread pools, work queues, and interrupt-\ning operations.\n Chapter 10 covers the new parallelism support from C++17, which comes in the\nform of additional overloads for many of the Standard Library algorithms.\n Chapter 11 covers testing and debugging—types of bugs, techniques for locating\nthem, how to test for them, and so forth.\n\n\nABOUT THIS BOOK\nxviii\n The appendixes include a brief description of some of the new language facilities\nintroduced with the new standard that are relevant to multithreading, the implemen-\ntation details of the message-passing library mentioned in chapter 4, and a complete\nreference to the C++17 Thread Library.\nWho should read this book\nIf you're writing multithreaded code in C++, you should read this book. If you're using\nthe new multithreading facilities from the C++ Standard Library, this book is an essen-\ntial guide. If you’re using alternative thread libraries, the guidelines and techniques\nfrom the later chapters should still prove useful.\n A good working knowledge of C++ is assumed, though familiarity with the new lan-\nguage features is not—these are covered in appendix A. Prior knowledge or experi-\nence of multithreaded programming is not assumed, though it may be useful.\nHow to use this book\nIf you’ve never written multithreaded code before, I suggest reading this book sequen-\ntially from beginning to end, though possibly skipping the more detailed parts of\nchapter 5. Chapter 7 relies heavily on the material in chapter 5, so if you skipped\nchapter 5, you should save chapter 7 until you’ve read it.\n If you haven’t used the new C++11 language facilities before, it might be worth\nskimming appendix A before you start to ensure that you’re up to speed with the\nexamples in the book. The uses of the new language facilities are highlighted in the\ntext, though, and you can always flip to the appendix if you encounter something you\nhaven’t seen before.\n If you have extensive experience with writing multithreaded code in other environ-\nments, the beginning chapters are probably still worth skimming so you can see how\nthe facilities you know map onto the new standard C++ ones. If you’re going to be\ndoing any low-level work with atomic variables, chapter 5 is a must. Chapter 8 is worth\nreviewing to ensure that you’re familiar with things like exception safety in multi-\nthreaded C++. If you have a particular task in mind, the index and table of contents\nshould help you find a relevant section quickly.\n Once you’re up to speed on the use of the C++ Thread Library, appendix D should\ncontinue to be useful, such as for looking up the exact details of each class and func-\ntion call. You may also like to dip back into the main chapters from time to time to\nrefresh your memory on a particular construct or to look at the sample code.\nCode conventions and downloads\nAll source code in listings or in text is in a fixed-width font like this to separate it\nfrom ordinary text. Code annotations accompany many of the listings, highlighting\nimportant concepts. In some cases, numbered bullets link to explanations that follow\nthe listing.\n\n\nABOUT THIS BOOK\nxix\n Source code for all working examples in this book is available for download from\nthe publisher’s website at www.manning.com/books/c-plus-plus-concurrency-in-action-\nsecond-edition. You may also download the source code from github at https://github\n.com/anthonywilliams/ccia_code_samples.\nSoftware requirements\nTo use the code from this book unchanged, you’ll need a recent C++ compiler that\nsupports the C++17 language features used in the examples (see appendix A), and\nyou’ll need a copy of the C++ Standard Thread Library.\n At the time of writing, the latest versions of g++, clang++, and Microsoft Visual Stu-\ndio all ship with implementations of the C++17 Standard Thread Library. They also\nsupport most of the language features from the appendix, and those features that\naren't supported are coming soon. \n My company, Just Software Solutions Ltd, sells a complete implementation of the\nC++11 Standard Thread Library for several older compilers, along with an implemen-\ntation of the Concurrency TS for newer versions of clang, gcc, and Microsoft Visual\nStudio.1 This implementation has been used for testing the examples in this book.\n The Boost Thread Library2 provides an API that’s based on the C++11 Standard\nThread Library proposals and is portable to many platforms. Most of the examples\nfrom the book can be modified to work with the Boost Thread Library by judicious\nreplacement of std:: with boost:: and use of the appropriate #include directives.\nThere are a few facilities that are either not supported (such as std::async) or have\ndifferent names (such as boost::unique_future) in the Boost Thread Library.\nBook forum\nPurchase of C++ Concurrency in Action, Second Edition includes free access to a pri-\nvate web forum run by Manning Publications where you can make comments about\nthe book, ask technical questions, and receive help from the author and from other\nusers. To access the forum, go to www.manning.com/books/c-plus-plus-concurrency-\nin-action-second-edition. You can also learn more about Manning’s forums and the\nrules of conduct at https://forums.manning.com/forums/about.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It’s not a commitment to any specific amount of participation on the part of the\nauthor, whose contribution to the book’s forum remains voluntary (and unpaid). We\nsuggest you try asking the author some challenging questions, lest his interest stray!\n The forum and the archives of previous discussions will be accessible from the pub-\nlisher’s website as long as the book is in print.\n1 The just::thread implementation of the C++ Standard Thread Library, http://www.stdthread.co.uk.\n2 The Boost C++ library collection, http://www.boost.org.\n\n\nxx\nabout the author\nAnthony Williams is a UK-based developer, consultant, and\ntrainer with over 20 years of experience in C++. He has been an\nactive member of the BSI C++ Standards Panel since 2001, and\nis the author or coauthor of many of the C++ Standards Com-\nmittee papers that led up to the inclusion of the thread library\nin the C++11 Standard. He continues to work on new facilities\nto enhance the C++ concurrency toolkit, both with standards\nproposals, and implementations of those facilities for the\njust::thread Pro extensions to the C++ thread library from Just\nSoftware Solutions Ltd. Anthony lives in the far west of Corn-\nwall, England.\n\n\nxxi\nabout the cover illustration\nThe illustration on the cover of C++ Concurrency in Action is captioned “Habit of a Lady\nof Japan.” The image is taken from the four-volume Collection of the Dress of Different\nNations by Thomas Jefferys, published in London between 1757 and 1772. The collec-\ntion includes beautiful hand-colored copperplate engravings of costumes from\naround the world and has influenced theatrical costume design since its publication.\nThe diversity of the drawings in the compendium speaks vividly of the richness of the\ncostumes presented on the London stage over 200 years ago. The costumes, both his-\ntorical and contemporaneous, offered a glimpse into the dress customs of people\nliving in different times and in different countries, making them come alive for\nLondon theater audiences.\n Dress codes have changed in the last century, and the diversity by region, so rich in\nthe past, has faded away. It’s now often hard to tell the inhabitant of one continent\nfrom another. Perhaps, trying to view it optimistically, we’ve traded a cultural and\nvisual diversity for a more varied personal life—or a more varied and interesting intel-\nlectual and technical life.\n We at Manning celebrate the inventiveness, the initiative, and the fun of the com-\nputer business with book covers based on the rich diversity of the regional and theatri-\ncal life of two centuries ago, brought back to life by the pictures from this collection.\n\n\n1\nHello, world of\nconcurrency in C++!\nThese are exciting times for C++ users. Thirteen years after the original C++ Stan-\ndard was published in 1998, the C++ Standards Committee gave the language and\nits supporting library a major overhaul. The new C++ Standard (referred to as\nC++11 or C++0x) was published in 2011 and brought with it a swath of changes that\nmade working with C++ easier and more productive. The Committee also commit-\nted to a new “train model” of releases, with a new C++ Standard to be published\nevery three years. So far, we've had two of these publications: the C++14 Standard in\n2014, and the C++17 Standard in 2017, as well as several Technical Specifications\ndescribing extensions to the C++ Standard.\nThis chapter covers\nWhat is meant by concurrency and multithreading\nWhy you might want to use concurrency and \nmultithreading in your applications\nSome of the history of the support for \nconcurrency in C++\nWhat a simple multithreaded C++ program \nlooks like\n\n\n2\nCHAPTER 1\nHello, world of concurrency in C++!\n One of the most significant new features in the C++11 Standard was the support of\nmultithreaded programs. For the first time, the C++ Standard acknowledged the exis-\ntence of multithreaded applications in the language and provided components in the\nlibrary for writing multithreaded applications. This made it possible to write multi-\nthreaded C++ programs without relying on platform-specific extensions and enabled\nyou to write portable multithreaded code with guaranteed behavior. It also came at a\ntime when programmers were increasingly looking to concurrency in general, and\nmultithreaded programming in particular, to improve application performance. The\nC++14 and C++17 Standards have built upon this baseline to provide further support\nfor writing multithreaded programs in C++, as have the Technical Specifications.\nThere’s a Technical Specification for concurrency extensions, and another for paral-\nlelism, though the latter has been incorporated into C++17.\n This book is about writing programs in C++ using multiple threads for concur-\nrency and the C++ language features and library facilities that make it possible. I’ll\nstart by explaining what I mean by concurrency and multithreading and why you\nwould want to use concurrency in your applications. After a quick detour into why you\nmight not want to use it in your applications, we’ll go through an overview of the con-\ncurrency support in C++, and we’ll round off this chapter with a simple example of\nC++ concurrency in action. Readers experienced with developing multithreaded\napplications may wish to skip the early sections. In subsequent chapters, we’ll cover\nmore extensive examples and look at the library facilities in more depth. The book\nwill finish with an in-depth reference to all the C++ Standard Library facilities for mul-\ntithreading and concurrency.\n So, what do I mean by concurrency and multithreading?\n1.1\nWhat is concurrency?\nAt the simplest and most basic level, concurrency is about two or more separate activi-\nties happening at the same time. We encounter concurrency as a natural part of life;\nwe can walk and talk at the same time or perform different actions with each hand,\nand we each go about our lives independently of each other—you can watch football\nwhile I go swimming, and so on.\n1.1.1\nConcurrency in computer systems\nWhen we talk about concurrency in terms of computers, we mean a single system per-\nforming multiple independent activities in parallel, rather than sequentially, or one\nafter the other. This isn’t a new phenomenon. Multitasking operating systems that\nallow a single desktop computer to run multiple applications at the same time\nthrough task switching have been commonplace for many years, as have high-end\nserver machines with multiple processors that enable genuine concurrency. What’s\nnew is the increased prevalence of computers that can genuinely run multiple tasks in\nparallel rather than giving the illusion of doing so.\n",
      "page_number": 17
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 26-36)",
      "start_page": 26,
      "end_page": 36,
      "detection_method": "topic_boundary",
      "content": "3\nWhat is concurrency?\n Historically, most desktop computers have had one processor, with a single process-\ning unit or core, and this remains true for many desktop machines today. Such a\nmachine can only perform one task at a time, but it can switch between tasks many times\nper second. By doing a bit of one task and then a bit of another and so on, it appears\nthat the tasks are happening concurrently. This is called task switching. We still talk about\nconcurrency with such systems; because the task switches are so fast, you can’t tell at\nwhich point a task may be suspended as the processor switches to another one. The task\nswitching provides the illusion of concurrency to both the user and the applications\nthemselves. Because there is only the illusion of concurrency, the behavior of applica-\ntions may be subtly different when executing in a single-processor task-switching envi-\nronment compared to when executing in an environment with true concurrency. In\nparticular, incorrect assumptions about the memory model (covered in chapter 5) may\nnot show up in such an environment. This is discussed in more depth in chapter 10.\n Computers containing multiple processors have been used for servers and high-\nperformance computing tasks for years, and computers based on processors with\nmore than one core on a single chip (multicore processors) are becoming increas-\ningly common as desktop machines. Whether they have multiple processors or multi-\nple cores within a processor (or both), these computers are capable of genuinely\nrunning more than one task in parallel. We call this hardware concurrency.\n Figure 1.1 shows an idealized scenario of a computer with precisely two tasks to do,\neach divided into 10 equally sized chunks. On a dual-core machine (which has two\nprocessing cores), each task can execute on its own core. On a single-core machine\ndoing task switching, the chunks from each task are interleaved. But they are also\nspaced out a bit (in figure 1.1, this is shown by the gray bars separating the chunks\nbeing thicker than the separator bars shown for the dual-core machine); in order to\ndo the interleaving, the system has to perform a context switch every time it changes\nfrom one task to another, and this takes time. In order to perform a context switch,\nthe OS has to save the CPU state and instruction pointer for the currently running\ntask, work out which task to switch to, and reload the CPU state for the task being\nswitched to. The CPU will then potentially have to load the memory for the instruc-\ntions and data for the new task into the cache, which can prevent the CPU from exe-\ncuting any instructions, causing further delay.\nSingle core\nCore 1\nCore 2\nDual core\nFigure 1.1\nTwo approaches to concurrency: parallel execution on a dual-core \nmachine versus task switching on a single-core machine\n\n\n4\nCHAPTER 1\nHello, world of concurrency in C++!\nThough the availability of concurrency in the hardware is most obvious with multipro-\ncessor or multicore systems, some processors can execute multiple threads on a single\ncore. The important factor to consider is the number of hardware threads, which is the\nmeasure of how many independent tasks the hardware can genuinely run concur-\nrently. Even with a system that has genuine hardware concurrency, it’s easy to have\nmore tasks than the hardware can run in parallel, so task switching is still used in these\ncases. For example, on a typical desktop computer there may be hundreds of tasks\nrunning, performing background operations, even when the computer is nominally\nidle. It’s the task switching that allows these background tasks to run and you to run\nyour word processor, compiler, editor, and web browser (or any combination of appli-\ncations) all at once. Figure 1.2 shows task switching among four tasks on a dual-core\nmachine, again for an idealized scenario with the tasks divided neatly into equally\nsized chunks. In practice, many issues will make the divisions uneven and the schedul-\ning irregular. Some of these issues are covered in chapter 8 when we look at factors\naffecting the performance of concurrent code.\nAll the techniques, functions, and classes covered in this book can be used whether\nyour application is running on a machine with one single-core processor or with many\nmulticore processors, and are not affected by whether the concurrency is achieved\nthrough task switching or by genuine hardware concurrency. But as you may imagine,\nhow you make use of concurrency in your application may well depend on the\namount of hardware concurrency available. This is covered in chapter 8, where I dis-\ncuss the issues involved in designing concurrent code in C++.\n1.1.2\nApproaches to concurrency\nImagine, for a moment, a pair of programmers working together on a software proj-\nect. If your developers are in separate offices, they can go about their work peacefully,\nwithout being disturbed by each other, and they each have their own set of reference\nmanuals. But communication isn’t straightforward; rather than turning around and\ntalking to each other, they have to use the phone or email, or get up and walk to the\nother’s office. Also, you have the overhead of two offices to manage and multiple cop-\nies of reference manuals to purchase.\n Now imagine that you move your developers into the same office. They can now\ntalk to each other freely to discuss the design of the application, and they can easily\nCore 1\nCore 2\nDual core\nFigure 1.2\nTask switching of four tasks on two cores\n\n\n5\nWhat is concurrency?\ndraw diagrams on paper or on a whiteboard to help with design ideas or explanations.\nYou have only one office to manage, and one set of resources will often suffice. On the\nnegative side, they might find it harder to concentrate, and there may be issues with\nsharing resources (“Where’s the reference manual gone now?”).\n These two ways of organizing your developers illustrate the two basic approaches\nto concurrency. Each developer represents a thread, and each office represents a pro-\ncess. The first approach is to have multiple single-threaded processes, which is similar\nto having each developer in their own office, and the second approach is to have mul-\ntiple threads in a single process, which is like having two developers in the same office.\nYou can combine these in an arbitrary fashion and have multiple processes, some of\nwhich are multithreaded and some of which are single-threaded, but the principles\nare the same. Let’s now have a brief look at these two approaches to concurrency in\nan application.\nCONCURRENCY WITH MULTIPLE PROCESSES\nThe first way to make use of concurrency within an appli-\ncation is to divide the application into multiple, separate,\nsingle-threaded processes that are run at the same time,\nmuch as you can run your web browser and word proces-\nsor at the same time. These separate processes can then\npass messages to each other through all the normal inter-\nprocess communication channels (signals, sockets, files,\npipes, and so on), as shown in figure 1.3. One downside is\nthat such communication between processes is often\neither complicated to set up or slow, or both, because\noperating systems typically provide a lot of protection\nbetween processes to avoid one process accidentally modi-\nfying data belonging to another process. Another downside\nis that there’s an inherent overhead in running multiple\nprocesses: it takes time to start a process, the operating\nsystem must devote internal resources to managing the\nprocess, and so forth.\n It’s not all negative: the added protection operating systems typically provide\nbetween processes and the higher-level communication mechanisms mean that it\ncan be easier to write safe concurrent code with processes rather than threads.\nIndeed, environments such as that provided for the Erlang (www.erlang.org/) pro-\ngramming language use processes as the fundamental building block of concur-\nrency to great effect.\n Using separate processes for concurrency also has an additional advantage—you\ncan run the separate processes on distinct machines connected over a network. Though\nthis increases the communication cost, on a carefully designed system it can be a cost-\neffective way of increasing the available parallelism and improving performance.\nProcess 1\nThread\nOperating\nsystem\nInterprocess\ncommunication\nProcess 2\nThread\nFigure 1.3\nCommunication \nbetween a pair of processes \nrunning concurrently\n\n\n6\nCHAPTER 1\nHello, world of concurrency in C++!\nCONCURRENCY WITH MULTIPLE THREADS\nThe alternative approach to concurrency is to run multiple\nthreads in a single process. Threads are much like light-\nweight processes: each thread runs independently of the\nothers, and each may run a different sequence of instruc-\ntions. But all threads in a process share the same address\nspace, and most of the data can be accessed directly from\nall threads—global variables remain global, and pointers or\nreferences to objects or data can be passed around among\nthreads. Although it’s often possible to share memory\namong processes, this is complicated to set up and often\nhard to manage, because memory addresses of the same\ndata aren’t necessarily the same in different processes. Fig-\nure 1.4 shows two threads within a process communicating\nthrough shared memory.\n The shared address space and lack of protection of data between threads makes\nthe overhead associated with using multiple threads much smaller than that from\nusing multiple processes, because the operating system has less bookkeeping to do.\nBut the flexibility of shared memory also comes with a price: if data is accessed by mul-\ntiple threads, the application programmer must ensure that the view of data seen by\neach thread is consistent whenever it’s accessed. The issues surrounding sharing data\nbetween threads, and the tools to use and guidelines to follow to avoid problems, are\ncovered throughout this book, notably in chapters 3, 4, 5, and 8. The problems aren’t\ninsurmountable, provided suitable care is taken when writing the code, but they do\nmean that a great deal of thought must go into the communication between threads.\n The low overhead associated with launching and communicating between multi-\nple threads within a process compared to launching and communicating between\nmultiple single-threaded processes means that this is the favored approach to concur-\nrency in mainstream languages, including C++, despite the potential problems arising\nfrom the shared memory. In addition, the C++ Standard doesn’t provide any intrinsic\nsupport for communication between processes, so applications that use multiple pro-\ncesses will have to rely on platform-specific APIs to do so. This book therefore focuses\nexclusively on using multithreading for concurrency, and future references to concur-\nrency assume that this is achieved by using multiple threads.\n There’s another word that gets used a lot around multithreaded code: parallelism.\nLet’s clarify the differences.\n1.1.3\nConcurrency vs. parallelism\nConcurrency and parallelism have largely overlapping meanings with respect to\nmultithreaded code. Indeed, to many they mean the same thing. The difference is\nprimarily a matter of nuance, focus, and intent. Both terms are about running mul-\ntiple tasks simultaneously, using the available hardware, but parallelism is much more\nProcess\nThread 1\nShared memory\nThread 2\nFigure 1.4\nCommunication \nbetween a pair of threads \nrunning concurrently in a \nsingle process\n\n\n7\nWhy use concurrency?\nperformance-oriented. People talk about parallelism when their primary concern is\ntaking advantage of the available hardware to increase the performance of bulk data\nprocessing, whereas people talk about concurrency when their primary concern is sepa-\nration of concerns, or responsiveness. This dichotomy is not cut and dried, and there\nis still considerable overlap in meaning, but it can help clarify discussions to know of\nthis distinction. Throughout this book, there will be examples of both.\n Having clarified what we mean by concurrency and parallelism, let’s look at why\nyou would use concurrency in your applications.\n1.2\nWhy use concurrency?\nThere are two main reasons to use concurrency in an application: separation of con-\ncerns and performance. In fact, I’d go so far as to say that they’re almost the only rea-\nsons to use concurrency; anything else boils down to one or the other (or maybe even\nboth) when you look hard enough (well, except for reasons like “because I want to”).\n1.2.1\nUsing concurrency for separation of concerns\nSeparation of concerns is almost always a good idea when writing software; by group-\ning related bits of code together and keeping unrelated bits of code apart, you can\nmake your programs easier to understand and test, and less likely to contain bugs. You\ncan use concurrency to separate distinct areas of functionality, even when the opera-\ntions in these distinct areas need to happen at the same time; without the explicit use\nof concurrency, you either have to write a task-switching framework or actively make\ncalls to unrelated areas of code during an operation.\n Consider a processing-intensive application with a user interface, such as a DVD\nplayer application for a desktop computer. This application fundamentally has two\nsets of responsibilities. Not only does it have to read the data from the disk, decode\nthe images and sound, and send them to the graphics and sound hardware in a timely\nfashion so the DVD plays without glitches, but it must also take input from the user,\nsuch as when the user clicks Pause or Return To Menu, or even Quit. In a single\nthread, the application has to check for user input at regular intervals during the play-\nback, conflating the DVD playback code with the user interface code. By using multi-\nthreading to separate these concerns, the user interface code and DVD playback code\nno longer have to be so closely intertwined; one thread can handle the user interface\nand another the DVD playback. There will have to be interaction between them, such\nas when the user clicks Pause, but now these interactions are directly related to the\ntask at hand.\n This gives the illusion of responsiveness, because the user interface thread can typ-\nically respond immediately to a user request, even if the response is to display a busy\ncursor or a Please Wait message while the request is conveyed to the thread doing the\nwork. Similarly, separate threads are often used to run tasks that must run continu-\nously in the background, such as monitoring the filesystem for changes in a desktop\nsearch application. Using threads in this way generally makes the logic in each thread\n\n\n8\nCHAPTER 1\nHello, world of concurrency in C++!\nmuch simpler, because the interactions between them can be limited to clearly identi-\nfiable points, rather than having to intersperse the logic of the different tasks.\n In this case, the number of threads is independent of the number of CPU cores\navailable, because the division into threads is based on the conceptual design rather\nthan an attempt to increase throughput.\n1.2.2\nUsing concurrency for performance: task and data parallelism\nMultiprocessor systems have existed for decades, but until recently they were mostly\nfound only in supercomputers, mainframes, and large server systems. But chip manu-\nfacturers have increasingly been favoring multicore designs with 2, 4, 16, or more pro-\ncessors on a single chip over better performance with a single core. Consequently,\nmulticore desktop computers, and even multicore embedded devices, are now increas-\ningly prevalent. The increased computing power of these machines comes not from\nrunning a single task faster but from running multiple tasks in parallel. In the past,\nprogrammers have been able to sit back and watch their programs get faster with each\nnew generation of processors, without any effort on their part. But now, as Herb Sut-\nter put it, “The free lunch is over.”1 If software is to take advantage of this increased\ncomputing power, it must be designed to run multiple tasks concurrently. Program-\nmers must therefore take heed, and those who have hitherto ignored concurrency\nmust now look to add it to their toolbox.\n There are two ways to use concurrency for performance. The first, and most obvi-\nous, is to divide a single task into parts and run each in parallel, reducing the total\nruntime. This is task parallelism. Although this sounds straightforward, it can be quite a\ncomplex process, because there may be many dependencies between the various\nparts. The divisions may be either in terms of processing—one thread performs one\npart of the algorithm while another thread performs a different part—or in terms of\ndata—each thread performs the same operation on different parts of the data. This\nlatter approach is called data parallelism.\n Algorithms that are readily susceptible to such parallelism are frequently called\nembarrassingly parallel. Despite the implication that you might be embarrassed to have\ncode so easy to parallelize, this is a good thing; other terms I’ve encountered for such\nalgorithms are naturally parallel and conveniently concurrent. Embarrassingly parallel algo-\nrithms have good scalability properties—as the number of available hardware threads\ngoes up, the parallelism in the algorithm can be increased to match. Such an algo-\nrithm is the perfect embodiment of the adage, “Many hands make light work.” For\nthose parts of the algorithm that aren’t embarrassingly parallel, you might be able to\ndivide the algorithm into a fixed (and therefore not scalable) number of parallel\ntasks. Techniques for dividing tasks between threads are covered in chapters 8 and 10.\n The second way to use concurrency for performance is to use the available paral-\nlelism to solve bigger problems; rather than processing one file at a time, process 2, or\n1 “The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software,” Herb Sutter, Dr. Dobb’s\nJournal, 30(3), March 2005. http://www.gotw.ca/publications/concurrency-ddj.htm.\n\n\n9\nWhy use concurrency?\n10, or 20, as appropriate. Although this is an application of data parallelism, by per-\nforming the same operation on multiple sets of data concurrently, there’s a different\nfocus. It still takes the same amount of time to process one chunk of data, but now\nmore data can be processed in the same amount of time. Obviously, there are limits\nto this approach, and this won’t be beneficial in all cases, but the increase in throughput\nthat comes from this approach can make new things possible—increased resolution\nin video processing, for example, if different areas of the picture can be processed\nin parallel.\n1.2.3\nWhen not to use concurrency\nIt’s just as important to know when not to use concurrency as it is to know when to use\nit. Fundamentally, the only reason not to use concurrency is when the benefit isn’t\nworth the cost. Code using concurrency is harder to understand in many cases, so\nthere’s a direct intellectual cost to writing and maintaining multithreaded code, and\nthe additional complexity can also lead to more bugs. Unless the potential perfor-\nmance gain is large enough or the separation of concerns is clear enough to justify the\nadditional development time required to get it right and the additional costs associ-\nated with maintaining multithreaded code, don’t use concurrency.\n Also, the performance gain might not be as large as expected; there’s an inherent\noverhead associated with launching a thread, because the OS has to allocate the asso-\nciated kernel resources and stack space and then add the new thread to the scheduler,\nall of which takes time. If the task being run on the thread is completed quickly, the\ntime taken by the task may be dwarfed by the overhead of launching the thread, possi-\nbly making the overall performance of the application worse than if the task had been\nexecuted directly by the spawning thread.\n Furthermore, threads are a limited resource. If you have too many threads run-\nning at once, this consumes OS resources and may make the system as a whole run\nslower. Not only that, but using too many threads can exhaust the available memory or\naddress space for a process, because each thread requires a separate stack space. This\nis particularly a problem for 32-bit processes with a flat architecture where there’s a 4 GB\nlimit to the available address space: if each thread has a 1 MB stack (as is typical on\nmany systems), then the address space would be used up with 4,096 threads, without\nallowing any space for code, static data, or heap data. Although 64-bit (or larger) sys-\ntems don’t have this direct address-space limit, they still have finite resources: if you\nrun too many threads, this will eventually cause problems. Though thread pools (see\nchapter 9) can be used to limit the number of threads, they aren’t a silver bullet, and\nthey do have their own issues.\n If the server side of a client/server application launches a separate thread for each\nconnection, this will work fine for a small number of connections, but can quickly\nexhaust system resources by launching too many threads if the same technique is used\nfor a high-demand server that has to handle many connections. In this scenario, care-\nful use of thread pools can provide optimal performance (see chapter 9).\n\n\n10\nCHAPTER 1\nHello, world of concurrency in C++!\n Finally, the more threads you have running, the more context switching the oper-\nating system has to do. Each context switch takes time that could be spent doing use-\nful work, so at some point, adding an extra thread will reduce the overall application\nperformance rather than increase it. For this reason, if you’re trying to achieve the\nbest possible performance of the system, it’s necessary to adjust the number of threads\nrunning to take into account the available hardware concurrency (or lack of it).\n The use of concurrency for performance is like any other optimization strategy: it\nhas the potential to greatly improve the performance of your application, but it can\nalso complicate the code, making it harder to understand and more prone to bugs.\nTherefore, it’s only worth doing for those performance-critical parts of the applica-\ntion where there’s the potential for measurable gain. Of course, if the potential for\nperformance gains is only secondary to clarity of design or separation of concerns, it\nmay still be worth using a multithreaded design.\n Assuming that you’ve decided you do want to use concurrency in your application,\nwhether for performance, separation of concerns, or because it’s “multithreading\nMonday,” what does that mean for C++ programmers?\n1.3\nConcurrency and multithreading in C++\nStandardized support for concurrency through multithreading is a relatively new\nthing for C++. It’s only since the C++11 Standard that you’ve been able to write multi-\nthreaded code without resorting to platform-specific extensions. In order to under-\nstand the rationale behind lots of the decisions in the Standard C++ Thread Library,\nit’s important to understand the history.\n1.3.1\nHistory of multithreading in C++\nThe 1998 C++ Standard doesn’t acknowledge the existence of threads, and the opera-\ntional effects of the various language elements are written in terms of a sequential\nabstract machine. Not only that, but the memory model isn’t formally defined, so you\ncan’t write multithreaded applications without compiler-specific extensions to the\n1998 C++ Standard.\n Compiler vendors are free to add extensions to the language, and the prevalence\nof C APIs for multithreading—such as those in the POSIX C standard and the Micro-\nsoft Windows API—has led many C++ compiler vendors to support multithreading\nwith various platform-specific extensions. This compiler support is generally limited to\nallowing the use of the corresponding C API for the platform and ensuring that the\nC++ Runtime Library (such as the code for the exception-handling mechanism) works\nin the presence of multiple threads. Although few compiler vendors have provided a\nformal multithreading-aware memory model, the behavior of the compilers and pro-\ncessors has been sufficiently good that a large number of multithreaded C++ pro-\ngrams have been written.\n Not content with using the platform-specific C APIs for handling multithreading,\nC++ programmers have looked to their class libraries to provide object-oriented\n\n\n11\nConcurrency and multithreading in C++\nmultithreading facilities. Application frameworks, such as MFC, and general-purpose\nC++ libraries, such as Boost and ACE, have accumulated sets of C++ classes that wrap\nthe underlying platform-specific APIs and provide higher-level facilities for multi-\nthreading that simplify tasks. Although the precise details of the class libraries vary\nconsiderably, particularly in the area of launching new threads, the overall shape of\nthe classes has a lot in common. One particularly important design that’s common to\nmany C++ class libraries, and that provides considerable benefit to the programmer, is\nthe use of the Resource Acquisition Is Initialization (RAII) idiom with locks to ensure\nthat mutexes are unlocked when the relevant scope is exited.\n For many cases, the multithreading support of existing C++ compilers combined\nwith the availability of platform-specific APIs and platform-independent class libraries,\nsuch as Boost and ACE, provide a solid foundation on which to write multithreaded\nC++ code, and as a result, there are probably millions of lines of C++ code written as\npart of multithreaded applications. But the lack of standard support means that there\nare occasions where the lack of a thread-aware memory model causes problems, par-\nticularly for those who try to gain higher performance by using knowledge of the pro-\ncessor hardware or for those writing cross-platform code where the behavior of the\ncompilers varies between platforms.\n1.3.2\nConcurrency support in the C++11 standard\nAll this changed with the release of the C++11 Standard. Not only is there a thread-\naware memory model, but the C++ Standard Library was extended to include classes\nfor managing threads (see chapter 2), protecting shared data (see chapter 3), syn-\nchronizing operations between threads (see chapter 4), and low-level atomic opera-\ntions (see chapter 5).\n The C++11 Thread Library is heavily based on the prior experience accumulated\nthrough the use of the C++ class libraries mentioned previously. In particular, the\nBoost Thread Library was used as the primary model on which the new library is\nbased, with many of the classes sharing their names and structure with the corre-\nsponding ones from Boost. As the standard has evolved, this has been a two-way flow,\nand the Boost Thread Library has itself changed to match the C++ Standard in many\nrespects, so users transitioning from Boost should find themselves at home.\n Concurrency support is one of the changes with the C++11 Standard—as men-\ntioned at the beginning of this chapter, there are many enhancements to the language\nto make programmers’ lives easier. Although these are generally outside the scope of\nthis book, some of those changes have had a direct impact on the Thread Library and\nthe ways in which it can be used. Appendix A provides a brief introduction to these\nlanguage features.\n\n\n12\nCHAPTER 1\nHello, world of concurrency in C++!\n1.3.3\nMore support for concurrency and parallelism in C++14 \nand C++17\nThe only specific support for concurrency and parallelism added in C++14 was a new\nmutex type for protecting shared data (see chapter 3). But C++17 adds considerably\nmore: a full suite of parallel algorithms (see chapter 10) for starters. Both of these\nStandards enhance the core language and the rest of the Standard Library, and these\nenhancements can simplify the writing of multithreaded code.\n As mentioned previously, there’s also a Technical Specification for concurrency,\nwhich describes extensions to the functions and classes provided by the C++ Standard,\nespecially around synchronizing operations between threads (see chapter 4).\n The support for atomic operations directly in C++ enables programmers to write\nefficient code with defined semantics without the need for platform-specific assembly\nlanguage. This is a real boon for those trying to write efficient, portable code; not only\ndoes the compiler take care of the platform specifics, but the optimizer can be written\nto take into account the semantics of the operations, enabling better optimization of\nthe program as a whole.\n1.3.4\nEfficiency in the C++ Thread Library\nOne of the concerns that developers involved in high-performance computing often\nraise regarding C++ in general, and C++ classes that wrap low-level facilities—such as\nthose in the new Standard C++ Thread Library specifically—is that of efficiency. If\nyou’re after the utmost in performance, it’s important to understand the implementa-\ntion costs associated with using any high-level facilities, compared to using the under-\nlying low-level facilities directly. This cost is the abstraction penalty.\n The C++ Standards Committee was aware of this when designing the C++ Standard\nLibrary in general and the Standard C++ Thread Library in particular; one of the\ndesign goals has been that there should be little or no benefit to be gained from using\nthe lower-level APIs directly, where the same facility is to be provided. The library has\ntherefore been designed to allow for efficient implementation (with a low abstraction\npenalty) on most major platforms.\n Another goal of the C++ Standards Committee has been to ensure that C++ pro-\nvides sufficient low-level facilities for those wishing to work close to the metal for the\nultimate performance. To this end, along with the new memory model comes a com-\nprehensive atomic operations library for direct control over individual bits and bytes\nand the inter-thread synchronization and visibility of any changes. These atomic types\nand the corresponding operations can now be used in many places where developers\nwould previously have chosen to drop down to platform-specific assembly language.\nCode using the new standard types and operations is more portable and easier to\nmaintain.\n The C++ Standard Library also provides higher-level abstractions and facilities that\nmake writing multithreaded code easier and less error-prone. Sometimes the use of\nthese facilities comes with a performance cost because of the additional code that\n\n\n13\nGetting started\nmust be executed. But this performance cost doesn’t necessarily imply a higher abstrac-\ntion penalty; in general, the cost is no higher than would be incurred by writing\nequivalent functionality by hand, and the compiler may inline much of the addi-\ntional code anyway.\n In some cases, the high-level facilities provide additional functionality beyond what\nmay be required for a specific use. Most of the time this isn’t an issue: you don’t pay\nfor what you don’t use. On rare occasions, this unused functionality will impact the\nperformance of other code. If you’re aiming for performance and the cost is too high,\nyou may be better off handcrafting the desired functionality from lower-level facilities.\nIn the vast majority of cases, the additional complexity and chance of errors far out-\nweigh the potential benefits from a small performance gain. Even if profiling does\ndemonstrate that the bottleneck is in the C++ Standard Library facilities, it may be due\nto poor application design rather than a poor library implementation. For example, if\ntoo many threads are competing for a mutex, it will impact the performance signifi-\ncantly. Rather than trying to shave a small fraction of time off the mutex operations, it\nwould probably be more beneficial to restructure the application so that there’s less\ncontention on the mutex. Designing applications to reduce contention is covered in\nchapter 8.\n In those rare cases where the C++ Standard Library doesn’t provide the perfor-\nmance or behavior required, it might be necessary to use platform-specific facilities.\n1.3.5\nPlatform-specific facilities\nAlthough the C++ Thread Library provides reasonably comprehensive facilities for\nmultithreading and concurrency, on any given platform there will be platform-specific\nfacilities that go beyond what’s offered. In order to gain easy access to those facilities\nwithout giving up the benefits of using the Standard C++ Thread Library, the types in\nthe C++ Thread Library may offer a native_handle() member function that allows\nthe underlying implementation to be directly manipulated using a platform-specific\nAPI. By its nature, any operations performed using native_handle() are entirely\nplatform dependent and beyond of the scope of this book (and the Standard C++\nLibrary itself).\n Before even considering using platform-specific facilities, it’s important to under-\nstand what the Standard Library provides, so let’s get started with an example.\n1.4\nGetting started\nOK, so you have a nice, shiny C++11/C++14/C++17 compiler. What’s next? What does\na multithreaded C++ program look like? It looks much like any other C++ program,\nwith the usual mix of variables, classes, and functions. The only real distinction is that\nsome functions might be running concurrently, so you need to ensure that shared\ndata is safe for concurrent access, as described in chapter 3. In order to run func-\ntions concurrently, specific functions and objects must be used to manage the differ-\nent threads.\n",
      "page_number": 26
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 37-49)",
      "start_page": 37,
      "end_page": 49,
      "detection_method": "topic_boundary",
      "content": "14\nCHAPTER 1\nHello, world of concurrency in C++!\n1.4.1\nHello, Concurrent World\nLet’s start with a classic example: a program to print “Hello World.” A simple Hello\nWorld program that runs in a single thread is shown here, to serve as a baseline when\nwe move to multiple threads:\n#include <iostream>\nint main()\n{\n    std::cout<<\"Hello World\\n\";\n}\nAll this program does is write “Hello World” to the standard output stream. Let’s com-\npare it to the simple Hello Concurrent World program shown in the following listing,\nwhich starts a separate thread to display the message.\n#include <iostream>\n#include <thread>           \nvoid hello()                       \n{\n    std::cout<<\"Hello Concurrent World\\n\";\n}\nint main()\n{\n    std::thread t(hello);   3\n    t.join();\n}\nThe first difference is the extra #include <thread>. The declarations for the multi-\nthreading support in the Standard C++ Library are in new headers: the functions and\nclasses for managing threads are declared in <thread>, whereas those for protecting\nshared data are declared in other headers.\n Second, the code for writing the message has been moved to a separate function.\nThis is because every thread has to have an initial function, where the new thread of\nexecution begins. For the initial thread in an application, this is main(), but for\nevery other thread it’s specified in the constructor of a std::thread object—in this\ncase, the std::thread object named t has the new hello() function as its initial\nfunction.\n This is the next difference: rather than writing directly to standard output or call-\ning hello() from main(), this program launches a new thread to do it, bringing the\nthread count to two—the initial thread that starts at main() and the new thread that\nstarts at hello().\n After the new thread has been launched, the initial thread continues execution. If\nit didn’t wait for the new thread to finish, it would merrily continue to the end of\nmain() and end the program—possibly before the new thread had a chance to run.\nThis is why the call to join() is there—as described in chapter 2, this causes the calling\nListing 1.1\nA simple Hello Concurrent World program\n\n\n15\nSummary\nthread (in main()) to wait for the thread associated with the std::thread object, in\nthis case, t.\n If this seems like a lot of effort to write a message to standard output, it is—as\ndescribed in section 1.2.3, it’s generally not worth the effort to use multiple threads\nfor such a simple task, especially if the initial thread has nothing to do in the mean-\ntime. Later in the book, you’ll work through examples of scenarios where there’s a\nclear gain to using multiple threads.\nSummary\nIn this chapter, I covered what’s meant by concurrency and multithreading and why\nyou’d choose to use it (or not) in your applications. I also covered the history of multi-\nthreading in C++, from the complete lack of support in the 1998 standard, through\nvarious platform-specific extensions, to proper multithreading support in the C++11\nStandard, and on to the C++14 and C++17 standards and the Technical Specification\nfor concurrency. This support has come in time to allow programmers to take advan-\ntage of the greater hardware concurrency becoming available with newer CPUs, as\nchip manufacturers choose to add more processing power in the form of multiple\ncores that allow more tasks to be executed concurrently, rather than increasing the\nexecution speed of a single core.\n I also showed how simple using the classes and functions from the C++ Standard\nLibrary can be in the examples in section 1.4. In C++, using multiple threads isn’t\ncomplicated in and of itself; the complexity lies in designing the code so that it\nbehaves as intended.\n After the examples of section 1.4, it’s time for something with a bit more substance.\nIn chapter 2, we’ll look at the classes and functions available for managing threads.\n\n\n16\nManaging threads\nOK, so you’ve decided to use concurrency for your application. In particular,\nyou’ve decided to use multiple threads. What now? How do you launch these\nthreads, check that they’ve finished, and keep tabs on them? The C++ Standard\nLibrary makes most thread-management tasks relatively easy, with almost every-\nthing managed through the std::thread object associated with a given thread, as\nyou’ll see. For those tasks that aren’t so straightforward, the library provides the\nflexibility to build what you need from the basic building blocks.\n In this chapter, I’ll start by covering the basics: launching a thread, waiting for it\nto finish, or running it in the background. We’ll then look at passing additional\nparameters to the thread function when it’s launched and how to transfer owner-\nship of a thread from one std::thread object to another. Finally, we’ll look at\nchoosing the number of threads to use and identifying particular threads.\nThis chapter covers\nStarting threads, and various ways of specifying \ncode to run on a new thread\nWaiting for a thread to finish versus leaving it \nto run\nUniquely identifying threads\n\n\n17\nBasic thread management\n2.1\nBasic thread management\nEvery C++ program has at least one thread, which is started by the C++ runtime: the\nthread running main(). Your program can then launch additional threads that have\nanother function as the entry point. These threads then run concurrently with each\nother and with the initial thread. In the same way that the program exits when it returns\nfrom main(), when the specified entry point function returns, the thread exits. As you’ll\nsee, if you have a std::thread object for a thread, you can wait for it to finish; but first\nyou have to start it, so let’s look at launching threads.\n2.1.1\nLaunching a thread\nAs you saw in chapter 1, threads are started by constructing a std::thread object that\nspecifies the task to run on that thread. In the simplest case, that task is a plain, ordi-\nnary void-returning function that takes no parameters. This function runs on its own\nthread until it returns, and then the thread stops. At the other extreme, the task could\nbe a function object that takes additional parameters and performs a series of inde-\npendent operations that are specified through some kind of messaging system while\nit’s running, and the thread stops only when it’s signaled to do so, again via some kind\nof messaging system. It doesn’t matter what the thread is going to do or where it’s\nlaunched from, but starting a thread using the C++ Standard Library always boils\ndown to constructing a std::thread object:\nvoid do_some_work();\nstd::thread my_thread(do_some_work);\nThis is about as simple as it gets. Of course, you have to make sure that the <thread>\nheader is included so the compiler can see the definition of the std::thread class. As\nwith much of the C++ Standard Library, std::thread works with any callable type, so\nyou can pass an instance of a class with a function call operator to the std::thread\nconstructor instead:\nclass background_task\n{\npublic:\n    void operator()() const\n    {\n        do_something();\n        do_something_else();\n    }\n};\nbackground_task f;\nstd::thread my_thread(f);\nIn this case, the supplied function object is copied into the storage belonging to the\nnewly created thread of execution and invoked from there. It’s therefore essential that\nthe copy behaves equivalently to the original, or the result may not be what’s expected.\n\n\n18\nCHAPTER 2\nManaging threads\n One thing to consider when passing a function object to the thread constructor is\nto avoid what’s dubbed “C++’s most vexing parse.” If you pass a temporary rather than\na named variable, the syntax can be the same as that of a function declaration, in\nwhich case the compiler interprets it as such, rather than an object definition. For\nexample,\nstd::thread my_thread(background_task());\ndeclares a my_thread function that takes a single parameter (of type pointer-to-a-\nfunction-taking-no-parameters-and-returning-a-background_task-object) and\nreturns a std::thread object, rather than launching a new thread. You can avoid this\nby naming your function object as shown previously, by using an extra set of parenthe-\nses, or by using the new uniform initialization syntax; for example:\nstd::thread my_thread((background_task()));       \nstd::thread my_thread{background_task()};         \nIn the first example, the extra parentheses prevent interpretation as a function decla-\nration, allowing my_thread to be declared as a variable of type std::thread. The sec-\nond example uses the new uniform initialization syntax with braces rather than\nparentheses, and thus would also declare a variable.\n One type of callable object that avoids this problem is a lambda expression. This is a\nnew feature from C++11 which allows you to write a local function, possibly capturing\nsome local variables and avoiding the need to pass additional arguments (see sec-\ntion 2.2). For full details on lambda expressions, see appendix A, section A.5. The\nprevious example can be written using a lambda expression as follows:\nstd::thread my_thread([]{\n    do_something();\n    do_something_else();\n});\nOnce you’ve started your thread, you need to explicitly decide whether to wait for it\nto finish (by joining with it—see section 2.1.2) or leave it to run on its own (by\ndetaching it—see section 2.1.3). If you don’t decide before the std::thread object\nis destroyed, then your program is terminated (the std::thread destructor calls\nstd::terminate()). It’s therefore imperative that you ensure that the thread is cor-\nrectly joined or detached, even in the presence of exceptions. See section 2.1.3 for a\ntechnique to handle this scenario. Note that you only have to make this decision\nbefore the std::thread object is destroyed—the thread itself may well have finished\nlong before you join with it or detach it, and if you detach it, then if the thread is\nstill running, it will continue to do so, and may continue running long after the\nstd::thread object is destroyed; it will only stop running when it finally returns from\nthe thread function.\n If you don’t wait for your thread to finish, you need to ensure that the data\naccessed by the thread is valid until the thread has finished with it. This isn’t a new\n\n\n19\nBasic thread management\nproblem—even in single-threaded code it’s undefined behavior to access an object\nafter it’s been destroyed—but the use of threads provides an additional opportunity to\nencounter such lifetime issues.\n One situation in which you can encounter such problems is when the thread func-\ntion holds pointers or references to local variables and the thread hasn’t finished\nwhen the function exits. The following listing shows an example of such a scenario.\nstruct func\n{\n    int& i;\n    func(int& i_):i(i_){}\n    void operator()()\n    {\n        for(unsigned j=0;j<1000000;++j)\n        {\n            do_something(i);      \n        }\n    }\n};\nvoid oops()\n{\n    int some_local_state=0;\n    func my_func(some_local_state);\n    std::thread my_thread(my_func);\n    my_thread.detach();             \n}                                \nIn this case, the new thread associated with my_thread will probably still be running\nwhen oops exits, because you’ve explicitly decided not to wait for it by calling detach().\nIf the thread is still running, you have the scenario shown in table 2.1: the next call to\ndo_something(i) will access an already destroyed variable. This is like normal single-\nthreaded code—allowing a pointer or reference to a local variable to persist beyond the\nfunction exit is never a good idea—but it’s easier to make the mistake with multithreaded\ncode, because it isn’t necessarily immediately apparent that this has happened.\nListing 2.1\nA function that returns while a thread still has access to local variables\nTable 2.1\nAccessing a local variable with a detached thread after it has been destroyed\nMain thread\nNew thread\nConstructs my_func with reference to \nsome_local_state\nStarts new thread my_thread\nStarted\nCalls func::operator()\nDetaches my_thread\nRunning func::operator(); may call do_something with \nreference to some_local_state\nPotential access to \ndangling reference\nDon’t wait for \nthread to finish\nNew thread might \nstill be running\n\n\n20\nCHAPTER 2\nManaging threads\nOne common way to handle this scenario is to make the thread function self-contained\nand copy the data into the thread rather than sharing the data. If you use a callable\nobject for your thread function, that object is copied into the thread, so the original\nobject can be destroyed immediately. But you still need to be wary of objects contain-\ning pointers or references, such as in listing 2.1. In particular, it’s a bad idea to create\na thread within a function that has access to the local variables in that function, unless\nthe thread is guaranteed to finish before the function exits.\n Alternatively, you can ensure that the thread has completed execution before the\nfunction exits by joining with the thread.\n2.1.2\nWaiting for a thread to complete\nIf you need to wait for a thread to complete, you can do this by calling join() on\nthe associated std::thread instance. In the case of listing 2.1, replacing the call to\nmy_thread.detach() before the closing brace of the function body with a call to\nmy_thread.join() would therefore be sufficient to ensure that the thread was fin-\nished before the function was exited and thus before the local variables were destroyed.\nIn this case, it would mean there was little point in running the function on a separate\nthread, because the first thread wouldn’t be doing anything useful in the meantime, but\nin real code the original thread would either have work to do or would have launched\nseveral threads to do useful work before waiting for all of them to complete.\n join() is a simple and brute-force technique—either you wait for a thread to fin-\nish or you don’t. If you need more fine-grained control over waiting for a thread, such\nas to check whether a thread is finished, or to wait only a certain period of time, then\nyou have to use alternative mechanisms such as condition variables and futures, which\nwe’ll look at in chapter 4. The act of calling join() also cleans up any storage associ-\nated with the thread, so the std::thread object is no longer associated with the now-\nfinished thread; it isn’t associated with any thread. This means that you can call join()\nonly once for a given thread; once you’ve called join(), the std::thread object is no\nlonger joinable, and joinable() will return false.\n2.1.3\nWaiting in exceptional circumstances\nAs mentioned earlier, you need to ensure that you’ve called either join() or\ndetach() before a std::thread object is destroyed. If you’re detaching a thread, you\ncan usually call detach() immediately after the thread has been started, so this isn’t a\nproblem. But if you’re intending to wait for the thread, you need to carefully pick the\nDestroys some_local_state\nStill running\nExits oops\nStill running func::operator(); may call do_something \nwith reference to some_local_state => undefined behavior\nTable 2.1\nAccessing a local variable with a detached thread after it has been destroyed (continued)\nMain thread\nNew thread\n\n\n21\nBasic thread management\nplace in the code where you call join(). This means that the call to join() is liable to\nbe skipped if an exception is thrown after the thread has been started but before the\ncall to join().\n To avoid your application being terminated when an exception is thrown, you\ntherefore need to make a decision about what to do in this case. In general, if you\nwere intending to call join() in a non-exceptional case, you also need to call join()\nin the presence of an exception to avoid accidental lifetime problems. The next listing\nshows some simple code that does just that.\nstruct func;         \nvoid f()\n{\n    int some_local_state=0;\n    func my_func(some_local_state);\n    std::thread t(my_func);\n    try\n    {\n        do_something_in_current_thread();\n    }\n    catch(...)\n    {\n        t.join();\n        throw;\n    }\n    t.join();\n}\nThe code in listing 2.2 uses a try/catch block to ensure that a thread with access to\nlocal state is finished before the function exits, whether the function exits normally, or\nby an exception. The use of try/catch blocks is verbose, and it’s easy to get the scope\nslightly wrong, so this isn’t an ideal scenario. If it’s important to ensure that the thread\ncompletes before the function exits—whether because it has a reference to other local\nvariables or for any other reason—then it’s important to ensure this is the case for all\npossible exit paths, whether normal or exceptional, and it’s desirable to provide a sim-\nple, concise mechanism for doing so.\n One way of doing this is to use the standard Resource Acquisition Is Initialization\n(RAII) idiom and provide a class that does the join() in its destructor, as in the fol-\nlowing listing. See how it simplifies the f() function.\nclass thread_guard\n{\n    std::thread& t;\npublic:\n    explicit thread_guard(std::thread& t_):\n        t(t_)\nListing 2.2\nWaiting for a thread to finish\nListing 2.3\nUsing RAII to wait for a thread to complete\nSee definition \nin listing 2.1\n\n\n22\nCHAPTER 2\nManaging threads\n    {}\n    ~thread_guard()\n    {\n        if(t.joinable())         \n        {\n            t.join();            \n        }\n    }\n    thread_guard(thread_guard const&)=delete;             \n    thread_guard& operator=(thread_guard const&)=delete;\n};\nstruct func;       \nvoid f()\n{\n    int some_local_state=0;\n    func my_func(some_local_state);\n    std::thread t(my_func);\n    thread_guard g(t);\n    do_something_in_current_thread();\n}                                            \nWhen the execution of the current thread reaches the end of f, the local objects are\ndestroyed in reverse order of construction. Consequently, the thread_guard object,\ng, is destroyed first, and the thread is joined with, in the destructor. This even hap-\npens if the function exits because do_something_in_current_thread throws an\nexception.\n The destructor of thread_guard in listing 2.3 first tests to see if the std::thread\nobject is joinable() before calling join(). This is important, because join() can be\ncalled only once for a given thread of execution, so it would be a mistake to do so if\nthe thread had already been joined.\n The copy constructor and copy-assignment operators are marked =delete to ensure\nthat they’re not automatically provided by the compiler. Copying or assigning such an\nobject would be dangerous, because it might then outlive the scope of the thread it was\njoining. By declaring them as deleted, any attempt to copy a thread_guard object will\ngenerate a compilation error. See appendix A, section A.2, for more about deleted\nfunctions.\n If you don’t need to wait for a thread to finish, you can avoid this exception-safety\nissue by detaching it. This breaks the association of the thread with the std::thread\nobject and ensures that std::terminate() won’t be called when the std::thread\nobject is destroyed, even though the thread is still running in the background.\n2.1.4\nRunning threads in the background\nCalling detach() on a std::thread object leaves the thread to run in the back-\nground, with no direct means of communicating with it. It’s no longer possible to wait\nfor that thread to complete; if a thread becomes detached, it isn’t possible to obtain\na std::thread object that references it, so it can no longer be joined. Detached\nthreads truly run in the background; ownership and control are passed over to the\nSee definition \nin listing 2.1\n\n\n23\nBasic thread management\nC++ Runtime Library, which ensures that the resources associated with the thread are\ncorrectly reclaimed when the thread exits.\n Detached threads are often called daemon threads after the UNIX concept of a dae-\nmon process that runs in the background without any explicit user interface. Such\nthreads are typically long-running; they run for almost the entire lifetime of the appli-\ncation, performing a background task such as monitoring the filesystem, clearing\nunused entries out of object caches, or optimizing data structures. At the other\nextreme, it may make sense to use a detached thread where there’s another mecha-\nnism for identifying when the thread has completed or where the thread is used for a\nfire-and-forget task.\n As you’ve saw in section 2.1.2, you detach a thread by calling the detach() mem-\nber function of the std::thread object. After the call completes, the std::thread\nobject is no longer associated with the actual thread of execution and is therefore no\nlonger joinable:\nstd::thread t(do_background_work);\nt.detach();\nassert(!t.joinable());\nIn order to detach the thread from a std::thread object, there must be a thread to\ndetach: you can’t call detach() on a std::thread object with no associated thread of\nexecution. This is exactly the same requirement as for join(), and you can check it in\nexactly the same way—you can only call t.detach() for a std::thread object t when\nt.joinable() returns true.\n Consider an application such as a word processor that can edit multiple docu-\nments at once. There are many ways to handle this, both at the UI level and internally.\nOne way that’s increasingly common at the moment is to have multiple, independent,\ntop-level windows, one for each document being edited. Although these windows\nappear to be completely independent, each with its own menus, they’re running\nwithin the same instance of the application. One way to handle this internally is to run\neach document-editing window in its own thread; each thread runs the same code but\nwith different data relating to the document being edited and the corresponding win-\ndow properties. Opening a new document therefore requires starting a new thread.\nThe thread handling the request isn’t going to care about waiting for that other\nthread to finish, because it’s working on an unrelated document, so this makes it a\nprime candidate for running a detached thread. \n The following listing shows a simple code outline for this approach.\nvoid edit_document(std::string const& filename)\n{\n    open_document_and_display_gui(filename);\n    while(!done_editing())\n    {\n        user_command cmd=get_user_input();\nListing 2.4\nDetaching a thread to handle other documents\n\n\n24\nCHAPTER 2\nManaging threads\n        if(cmd.type==open_new_document)\n        {\n            std::string const new_name=get_filename_from_user();\n            std::thread t(edit_document,new_name);                \n            t.detach();                              \n        }\n        else\n        {\n            process_user_input(cmd);\n        }\n    }\n}\nIf the user chooses to open a new document, you prompt them for the document to\nopen, start a new thread to open that document, and then detach it. Because the new\nthread is doing the same operation as the current thread but on a different file, you\ncan reuse the same function (edit_document) with the newly chosen filename as the\nsupplied argument.\n This example also shows a case where it’s helpful to pass arguments to the function\nused to start a thread: rather than just passing the name of the function to the\nstd::thread constructor, you also pass in the filename parameter. Although other\nmechanisms could be used to do this, such as using a function object with member\ndata instead of an ordinary function with parameters, the C++ Standard Library pro-\nvides you with an easy way of doing it.\n2.2\nPassing arguments to a thread function\nAs shown in listing 2.4, passing arguments to the callable object or function is funda-\nmentally as simple as passing additional arguments to the std::thread constructor.\nBut it’s important to bear in mind that by default, the arguments are copied into inter-\nnal storage, where they can be accessed by the newly created thread of execution, and\nthen passed to the callable object or function as rvalues as if they were temporaries.\nThis is done even if the corresponding parameter in the function is expecting a refer-\nence. Here’s an example:\nvoid f(int i,std::string const& s);\nstd::thread t(f,3,”hello”);\nThis creates a new thread of execution associated with t, which calls f(3,”hello”).\nNote that even though f takes a std::string as the second parameter, the string lit-\neral is passed as a char const* and converted to a std::string only in the context of\nthe new thread. This is particularly important when the argument supplied is a\npointer to an automatic variable, as follows:\nvoid f(int i,std::string const& s);\nvoid oops(int some_param)\n{\n    char buffer[1024];                 \n    sprintf(buffer, \"%i\",some_param);\n\n\n25\nPassing arguments to a thread function\n    std::thread t(f,3,buffer);         \n    t.detach();\n}\nIn this case, it’s the pointer to the local variable buffer that’s passed through to the\nnew thread and there’s a significant chance that the oops function will exit before\nthe buffer has been converted to a std::string on the new thread, thus leading to\nundefined behavior. The solution is to cast to std::string before passing the buffer\nto the std::thread constructor:\nvoid f(int i,std::string const& s);\nvoid not_oops(int some_param)\n{\n    char buffer[1024];\n    sprintf(buffer,\"%i\",some_param);\n    std::thread t(f,3,std::string(buffer));     \n    t.detach();\n}\nIn this case, the problem is that you were relying on the implicit conversion of the\npointer to the buffer into the std::string object expected as a function parameter,\nbut this conversion happens too late because the std::thread constructor copies the\nsupplied values as is, without converting to the expected argument type.\n It’s not possible to get the reverse scenario: the object is copied, and you wanted a\nnon-const reference, because this won't compile. You might try this if the thread is\nupdating a data structure that’s passed in by reference; for example:\nvoid update_data_for_widget(widget_id w,widget_data& data);    \nvoid oops_again(widget_id w)\n{\n    widget_data data;\n    std::thread t(update_data_for_widget,w,data);       \n    display_status();\n    t.join();\n    process_widget_data(data);         \n}\nAlthough update_data_for_widget expects the second parameter to be passed by ref-\nerence, the std::thread constructor doesn’t know that; it’s oblivious to the types of\nthe arguments expected by the function and blindly copies the supplied values. But\nthe internal code passes copied arguments as rvalues in order to work with move-only\ntypes, and will thus try to call update_data_for_widget with an rvalue. This will fail to\ncompile because you can't pass an rvalue to a function that expects a non-const refer-\nence. For those of you familiar with std::bind, the solution will be readily apparent:\nyou need to wrap the arguments that need to be references in std::ref. In this case,\nif you change the thread invocation to\nstd::thread t(update_data_for_widget,w,std::ref(data));\nUsing std::string \navoids dangling \npointer\n\n\n26\nCHAPTER 2\nManaging threads\nthen update_data_for_widget will be correctly passed a reference to data rather\nthan a temporary copy of data, and the code will now compile successfully.\n If you’re familiar with std::bind, the parameter-passing semantics will be unsur-\nprising, because both the operation of the std::thread constructor and the opera-\ntion of std::bind are defined in terms of the same mechanism. This means that, for\nexample, you can pass a member function pointer as the function, provided you sup-\nply a suitable object pointer as the first argument:\nclass X\n{\npublic:\n    void do_lengthy_work();\n};\nX my_x;\nstd::thread t(&X::do_lengthy_work,&my_x);\nThis code will invoke my_x.do_lengthy_work() on the new thread, because the address\nof my_x is supplied as the object pointer. You can also supply arguments to such a\nmember function call: the third argument to the std::thread constructor will be the\nfirst argument to the member function, and so forth.\n Another interesting scenario for supplying arguments is where the arguments\ncan’t be copied but can only be moved: the data held within one object is transferred\nover to another, leaving the original object empty. An example of such a type is\nstd::unique_ptr, which provides automatic memory management for dynamically\nallocated objects. Only one std::unique_ptr instance can point to a given object at a\ntime, and when that instance is destroyed, the pointed-to object is deleted. The move\nconstructor and move assignment operator allow the ownership of an object to be trans-\nferred around between std::unique_ptr instances (see appendix A, section A.1.1, for\nmore on move semantics). Such a transfer leaves the source object with a NULL\npointer. This moving of values allows objects of this type to be accepted as function\nparameters or returned from functions. Where the source object is temporary, the\nmove is automatic, but where the source is a named value, the transfer must be\nrequested directly by invoking std::move(). The following example shows the use of\nstd::move to transfer ownership of a dynamic object into a thread:\nvoid process_big_object(std::unique_ptr<big_object>);\nstd::unique_ptr<big_object> p(new big_object);\np->prepare_data(42);\nstd::thread t(process_big_object,std::move(p));\nBy specifying std::move(p) in the std::thread constructor, the ownership of big_\nobject is transferred first into internal storage for the newly created thread and then\ninto process_big_object.\n Several of the classes in the C++ Standard Library exhibit the same ownership\nsemantics as std::unique_ptr, and std::thread is one of them. Though std::thread\ninstances don’t own a dynamic object in the same way as std::unique_ptr does, they do\n",
      "page_number": 37
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 50-58)",
      "start_page": 50,
      "end_page": 58,
      "detection_method": "topic_boundary",
      "content": "27\nTransferring ownership of a thread\nown a resource: each instance is responsible for managing a thread of execution. This\nownership can be transferred between instances, because instances of std::thread are\nmovable, even though they aren’t copyable. This ensures that only one object is associ-\nated with a particular thread of execution at any one time while allowing program-\nmers the option of transferring that ownership between objects.\n2.3\nTransferring ownership of a thread\nSuppose you want to write a function that creates a thread to run in the background,\nbut passes ownership of the new thread back to the calling function rather than wait-\ning for it to complete; or maybe you want to do the reverse: create a thread and pass\nownership in to some function that should wait for it to complete. In either case, you\nneed to transfer ownership from one place to another.\n This is where the move support of std::thread comes in. As described in the pre-\nvious section, many resource-owning types in the C++ Standard Library, such as\nstd::ifstream and std::unique_ptr, are movable but not copyable, and std::thread\nis one of them. This means that the ownership of a particular thread of execution can\nbe moved between std::thread instances, as in the following example. The example\nshows the creation of two threads of execution and the transfer of ownership of those\nthreads among three std::thread instances, t1, t2, and t3:\nvoid some_function();\nvoid some_other_function();\nstd::thread t1(some_function);         \nstd::thread t2=std::move(t1);                \nt1=std::thread(some_other_function);   \nstd::thread t3;                              \nt3=std::move(t2);                      \nt1=std::move(t3);          \nFirst, a new thread is started and associated with t1. Ownership is then transferred\nover to t2 when t2 is constructed, by invoking std::move() to explicitly move owner-\nship. At this point, t1 no longer has an associated thread of execution; the thread run-\nning some_function is now associated with t2.\n Then, a new thread is started and associated with a temporary std::thread object.\nThe subsequent transfer of ownership into t1 doesn’t require a call to std::move() to\nexplicitly move ownership, because the owner is a temporary object—moving from\ntemporaries is automatic and implicit.\n t3 is default-constructed, which means that it’s created without any associated thread\nof execution. Ownership of the thread currently associated with t2 is transferred into\nt3, again with an explicit call to std::move(), because t2 is a named object. After all\nthese moves, t1 is associated with the thread running some_other_function, t2 has no\nassociated thread, and t3 is associated with the thread running some_function.\n The final move transfers ownership of the thread running some_function back to\nt1 where it started. But in this case t1 already had an associated thread (which was run-\nning some_other_function), so std::terminate() is called to terminate the program.\nThis assignment \nwill terminate the \nprogram!\n\n\n28\nCHAPTER 2\nManaging threads\nThis is done for consistency with the std::thread destructor. You saw in section 2.1.1\nthat you must explicitly wait for a thread to complete or detach it before destruction,\nand the same applies to assignment: you can’t just drop a thread by assigning a new\nvalue to the std::thread object that manages it.\n The move support in std::thread means that ownership can readily be trans-\nferred out of a function, as shown in the following listing.\nstd::thread f()\n{\n    void some_function();\n    return std::thread(some_function);\n}\nstd::thread g()\n{\n    void some_other_function(int);\n    std::thread t(some_other_function,42);\n    return t;\n}\nLikewise, if ownership should be transferred into a function, it can accept an instance\nof std::thread by value as one of the parameters, as shown here:\nvoid f(std::thread t);\nvoid g()\n{\n    void some_function();\n    f(std::thread(some_function));\n    std::thread t(some_function);\n    f(std::move(t));\n}\nOne benefit of the move support of std::thread is that you can build on the\nthread_guard class from listing 2.3 and have it take ownership of the thread. This\navoids any unpleasant consequences should the thread_guard object outlive the thread\nit was referencing, and it also means that no one else can join or detach the thread\nonce ownership has been transferred into the object. Because this would primarily be\naimed at ensuring that threads are completed before a scope is exited, I named this\nclass scoped_thread. The implementation is shown in the following listing, along with\na simple example.\nclass scoped_thread\n{\n    std::thread t;\npublic:\n    explicit scoped_thread(std::thread t_):        \n        t(std::move(t_))\nListing 2.5\nReturning a std::thread from a function\nListing 2.6\nscoped_thread and example usage\n\n\n29\nTransferring ownership of a thread\n    {\n        if(!t.joinable())                          \n            throw std::logic_error(“No thread”);\n    }\n    ~scoped_thread()\n    {\n        t.join();       \n    }\n    scoped_thread(scoped_thread const&)=delete;\n    scoped_thread& operator=(scoped_thread const&)=delete;\n};\nstruct func;    \nvoid f()\n{\n    int some_local_state;\n    scoped_thread t{std::thread(func(some_local_state))};   \n    do_something_in_current_thread();\n}                                          \nThe example is similar to listing 2.3, but the new thread is passed in directly to\nscoped_thread rather than having to create a separate named variable for it. When\nthe initial thread reaches the end of f, the scoped_thread object is destroyed and then\njoins with the thread supplied to the constructor. Whereas with the thread_guard class\nfrom listing 2.3 the destructor had to check that the thread was still joinable, you can\ndo that in the constructor and throw an exception if it’s not.\n One of the proposals for C++17 was for a joining_thread class that would be simi-\nlar to std::thread, except that it would automatically join in the destructor much like\nscoped_thread does. This didn't get consensus in the committee, so it wasn’t accepted\ninto the standard (though it’s still on track for C++20 as std::jthread), but it’s rela-\ntively easy to write. One possible implementation is shown in the next listing.\nclass joining_thread\n{\n    std::thread t;\npublic:\n    joining_thread() noexcept=default;\n    template<typename Callable,typename ... Args>\n    explicit joining_thread(Callable&& func,Args&& ... args):\n        t(std::forward<Callable>(func),std::forward<Args>(args)...)\n    {}\n    explicit joining_thread(std::thread t_) noexcept:\n        t(std::move(t_))\n    {}\n    joining_thread(joining_thread&& other) noexcept:\n        t(std::move(other.t))\n    {}\n    joining_thread& operator=(joining_thread&& other) noexcept\n    {\n        if(joinable())\n            join();\nListing 2.7\nA joining_thread class\nSee listing 2.1\n\n\n30\nCHAPTER 2\nManaging threads\n        t=std::move(other.t);\n        return *this;\n    }\n    joining_thread& operator=(std::thread other) noexcept\n    {\n        if(joinable())\n            join();\n        t=std::move(other);\n        return *this;\n    }\n    ~joining_thread() noexcept\n    {\n        if(joinable())\n            join();\n    }\n    void swap(joining_thread& other) noexcept\n    {\n        t.swap(other.t);\n    }\n    std::thread::id get_id() const noexcept{\n        return t.get_id();\n    }\n    bool joinable() const noexcept\n    {\n        return t.joinable();\n    }\n    void join()\n    {\n        t.join();\n    }\n    void detach()\n    {\n        t.detach();\n    }\n    std::thread& as_thread() noexcept\n    {\n        return t;\n    }\n    const std::thread& as_thread() const noexcept\n    {\n        return t;\n    }\n};\nThe move support in std::thread also allows for containers of std::thread objects,\nif those containers are move-aware (like the updated std::vector<>). This means\nthat you can write code like that in the following listing, which spawns a number of\nthreads and then waits for them to finish.\nvoid do_work(unsigned id);\nvoid f()\nListing 2.8\nSpawns some threads and waits for them to finish\n\n\n31\nChoosing the number of threads at runtime\n{\n    std::vector<std::thread> threads;\n    for(unsigned i=0;i<20;++i)\n    {\n        threads.emplace_back(do_work,i);     \n    }\n    for(auto& entry: threads)                \n        entry.join();\n}\nIf the threads are being used to subdivide the work of an algorithm, this is often what’s\nrequired; before returning to the caller, all threads must have finished. The simple\nstructure of listing 2.8 implies that the work done by the threads is self-contained, and\nthe result of their operations is purely the side effects on shared data. If f() were to\nreturn a value to the caller that depended on the results of the operations performed\nby these threads, then as written, this return value would have to be determined by\nexamining the shared data after the threads had terminated. Alternative schemes for\ntransferring the results of operations between threads are discussed in chapter 4.\n Putting std::thread objects in a std::vector is a step toward automating the\nmanagement of those threads: rather than creating separate variables for those\nthreads and joining with them directly, they can be treated as a group. You can take\nthis a step further by creating a dynamic number of threads determined at runtime,\nrather than creating a fixed number, as in listing 2.8.\n2.4\nChoosing the number of threads at runtime\nOne feature of the C++ Standard Library that helps here is std::thread::hardware_\nconcurrency(). This function returns an indication of the number of threads that can\ntruly run concurrently for a given execution of a program. On a multicore system it\nmight be the number of CPU cores, for example. This is only a hint, and the function\nmight return 0 if this information isn’t available, but it can be a useful guide for split-\nting a task among threads.\n Listing 2.9 shows a simple implementation of a parallel version of std::accumulate.\nIn real code you'll probably want to use the parallel version of std::reduce described\nin chapter 10, rather than implementing it yourself, but this illustrates the basic idea.\nIt divides the work among the threads, with a minimum number of elements per\nthread in order to avoid the overhead of too many threads. Note that this implementa-\ntion assumes that none of the operations will throw an exception, even though excep-\ntions are possible; the std::thread constructor will throw if it can’t start a new thread\nof execution, for example. Handling exceptions in such an algorithm is beyond the\nscope of this simple example and will be covered in chapter 8.\ntemplate<typename Iterator,typename T>\nstruct accumulate_block\nListing 2.9\nA naïve parallel version of std::accumulate\nSpawns threads\nCalls join() on each \nthread in turn\n\n\n32\nCHAPTER 2\nManaging threads\n{\n    void operator()(Iterator first,Iterator last,T& result)\n    {\n        result=std::accumulate(first,last,result);\n    }\n};\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)                                            \n        return init;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;    \n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=            \n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;      \n    std::vector<T> results(num_threads);\n    std::vector<std::thread>  threads(num_threads-1);       \n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_end=block_start;\n        std::advance(block_end,block_size);                 \n        threads[i]=std::thread(                             \n            accumulate_block<Iterator,T>(),\n            block_start,block_end,std::ref(results[i]));\n        block_start=block_end;                              \n    }\n    accumulate_block<Iterator,T>()(\n        block_start,last,results[num_threads-1]);    \n    \n    for(auto& entry: threads)\n           entry.join();                             \n    return std::accumulate(results.begin(),results.end(),init);   \n}\nAlthough this is a long function, it’s straightforward. If the input range is empty, you\nreturn the initial value supplied as the init parameter value. Otherwise, there’s at\nleast one element in the range, so you can divide the number of elements to process\nby the minimum block size in order to give the maximum number of threads . This is\nto avoid creating 32 threads on a 32-core machine when you have only five values in\nthe range.\n The number of threads to run is the minimum of your calculated maximum and\nthe number of hardware threads. You don’t want to run more threads than the hard-\nware can support (which is called oversubscription), because the context switching will\nmean that more threads will decrease the performance. If the call to std::thread::\nhardware_concurrency() returned 0, you’d substitute a number of your choice; in\n\n\n33\nChoosing the number of threads at runtime\nthis case I’ve chosen 2. You don’t want to run too many threads because that would\nslow things down on a single-core machine, but likewise you don’t want to run too few\nbecause you’d be passing up the available concurrency.\n The number of entries for each thread to process is the length of the range\ndivided by the number of threads. If you’re worrying about cases where the number\ndoesn’t divide evenly, don’t—you’ll handle that later.\n Now that you know how many threads you have, you can create a std::vector<T>\nfor the intermediate results and a std::vector<std::thread> for the threads. Note\nthat you need to launch one fewer thread than num_threads, because you already\nhave one.\n Launching the threads is a simple loop: advance the block_end iterator to the end\nof the current block and launch a new thread to accumulate the results for this block.\nThe start of the next block is the end of this one.\n After you’ve launched all the threads, this thread can then process the final block.\nThis is where you take account of any uneven division: you know the end of the final\nblock must be last, and it doesn’t matter how many elements are in that block.\n Once you’ve accumulated the results for the last block, you can wait for all the\nthreads you spawned with std::for_each, as in listing 2.8, and then add up the results\nwith a final call to std::accumulate.\n Before you leave this example, it’s worth pointing out that where the addition\noperator for the type T isn’t associative (such as for float or double), the results of\nthis parallel_accumulate may vary from those of std::accumulate because of the\ngrouping of the range into blocks. Also, the requirements on the iterators are slightly\nmore stringent: they must be at least forward iterators, whereas std::accumulate can\nwork with single-pass input iterators, and T must be default-constructible so that you can\ncreate the results vector. These sorts of requirement changes are common with par-\nallel algorithms; by their nature they’re different in order to make them parallel, and\nthis has consequences for the results and requirements. Implementing parallel algo-\nrithms is covered in more depth in chapter 8, and chapter 10 covers the standard sup-\nplied ones from C++17 (the equivalent to the parallel_accumulate described here\nbeing the parallel form of std::reduce). It’s also worth noting that because you can’t\nreturn a value directly from a thread, you must pass in a reference to the relevant\nentry in the results vector. Alternative ways of returning results from threads are\naddressed through the use of futures in chapter 4.\n In this case, all the information required by each thread was passed in when the\nthread was started, including the location in which to store the result of its calculation.\nThis isn’t always the case; sometimes it’s necessary to be able to identify the threads in\nsome way for part of the processing. You could pass in an identifying number, such as\nthe value of i in listing 2.8, but if the function that needs the identifier is several levels\ndeep in the call stack and could be called from any thread, it’s inconvenient to have to\ndo it that way. When we were designing the C++ Standard Library we foresaw this\nneed, so each thread has a unique identifier.\n\n\n34\nCHAPTER 2\nManaging threads\n2.5\nIdentifying threads\nThread identifiers are of type std::thread::id and can be retrieved in two ways.\nFirst, the identifier for a thread can be obtained from its associated std::thread object\nby calling the get_id() member function. If the std::thread object doesn’t have an\nassociated thread of execution, the call to get_id() returns a default-constructed\nstd::thread::id object, which indicates “not any thread.” Alternatively, the identifier\nfor the current thread can be obtained by calling std::this_thread:: get_id(),\nwhich is also defined in the <thread> header.\n Objects of type std::thread::id can be freely copied and compared; they wouldn’t\nbe of much use as identifiers otherwise. If two objects of type std::thread::id are\nequal, they represent the same thread, or both are holding the “not any thread” value.\nIf two objects aren’t equal, they represent different threads, or one represents a\nthread and the other is holding the “not any thread” value.\n The C++ Standard Library doesn’t limit you to checking whether thread identifiers\nare the same or not; objects of type std::thread::id offer the complete set of com-\nparison operators, which provide a total ordering for all distinct values. This allows\nthem to be used as keys in associative containers, or sorted, or compared in any other\nway that you as a programmer may see fit. The comparison operators provide a total\norder for all non-equal values of std::thread::id, so they behave as you’d intuitively\nexpect: if a<b and b<c, then a<c, and so forth. The Standard Library also provides\nstd::hash<std::thread::id> so that values of type std::thread::id can be used as\nkeys in the new unordered associative containers too.\n Instances of std::thread::id are often used to check whether a thread needs to\nperform some operation. For example, if threads are used to divide work, as in listing\n2.9, the initial thread that launched the others might need to perform its work slightly\ndifferently in the middle of the algorithm. In this case it could store the result of\nstd::this_thread::get_id() before launching the other threads, and then the core\npart of the algorithm (which is common to all threads) could check its own thread ID\nagainst the stored value:\nstd::thread::id master_thread;\nvoid some_core_part_of_algorithm()\n{\n    if(std::this_thread::get_id()==master_thread)\n    {\n        do_master_thread_work();\n    }\n    do_common_work();\n}\nAlternatively, the std::thread::id of the current thread could be stored in a data\nstructure as part of an operation. Later operations on that same data structure could\nthen check the stored ID against the ID of the thread performing the operation to\ndetermine what operations are permitted/required.\n\n\n35\nSummary\n Similarly, thread IDs could be used as keys into associative containers where spe-\ncific data needs to be associated with a thread and alternative mechanisms such as\nthread-local storage aren’t appropriate. Such a container could, for example, be used\nby a controlling thread to store information about each of the threads under its con-\ntrol or for passing information between threads.\n The idea is that std::thread::id will suffice as a generic identifier for a thread in\nmost circumstances; it’s only if the identifier has semantic meaning associated with it\n(such as being an index into an array) that alternatives should be necessary. You can\neven write out an instance of std::thread::id to an output stream such as std::cout:\nstd::cout<<std::this_thread::get_id();\nThe exact output you get is strictly implementation-dependent; the only guarantee\ngiven by the standard is that thread IDs that compare as equal should produce the\nsame output, and those that aren’t equal should give different output. This is there-\nfore primarily useful for debugging and logging, but the values have no semantic\nmeaning, so there’s not much more that could be said anyway.\nSummary\nIn this chapter, I covered the basics of thread management with the C++ Standard\nLibrary: starting threads, waiting for them to finish, and not waiting for them to finish\nbecause you want them to run in the background. We also saw how to pass arguments\ninto the thread function when a thread is started, how to transfer the responsibility for\nmanaging a thread from one part of the code to another, and how groups of threads\ncan be used to divide work. Finally, we discussed identifying threads in order to associ-\nate data or behavior with specific threads that’s inconvenient to associate through\nalternative means. Although you can do quite a lot with purely independent threads\nthat each operate on separate data, sometimes it’s desirable to share data among\nthreads while they’re running. Chapter 3 discusses the issues surrounding sharing\ndata directly among threads, and chapter 4 covers more general issues surrounding\nsynchronizing operations with and without shared data.\n",
      "page_number": 50
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 59-66)",
      "start_page": 59,
      "end_page": 66,
      "detection_method": "topic_boundary",
      "content": "36\nSharing data\nbetween threads\nOne of the key benefits of using threads for concurrency is the potential to easily\nand directly share data between them, so now that we’ve covered starting and man-\naging threads, let’s look at the issues surrounding shared data.\n Imagine for a moment that you’re sharing an apartment with a friend. There’s\nonly one kitchen and one bathroom. Unless you’re particularly friendly, you can’t\nboth use the bathroom at the same time, and if your roommate occupies the bath-\nroom for a long time, it can be frustrating if you need to use it. Likewise, though it\nmight be possible to both cook meals at the same time, if you have a combined oven\nand grill, it’s not going to end well if one of you tries to grill some sausages at the\nsame time as the other is baking a cake. Furthermore, we all know the frustration of\nsharing a space and getting halfway through a task only to find that someone has bor-\nrowed something we need or changed something from the way we left it.\n It’s the same with threads. If you’re sharing data between threads, you need to\nhave rules for which thread can access which bit of data when, and how any updates\nThis chapter covers\nProblems with sharing data between threads\nProtecting data with mutexes\nAlternative facilities for protecting shared data\n\n\n37\nProblems with sharing data between threads\nare communicated to the other threads that care about that data. The ease with which\ndata can be shared between multiple threads in a single process is not only a benefit—\nit can also be a big drawback. Incorrect use of shared data is one of the biggest causes\nof concurrency-related bugs, and the consequences can be far worse than sausage-\nflavored cakes.\n This chapter is about sharing data safely between threads in C++, avoiding the\npotential problems that can arise, and maximizing the benefits.\n3.1\nProblems with sharing data between threads\nWhen it comes down to it, the problems with sharing data between threads are all due\nto the consequences of modifying data. If all shared data is read-only, there’s no problem,\nbecause the data read by one thread is unaffected by whether or not another thread is reading the\nsame data. But if data is shared between threads, and one or more threads start modify-\ning the data, there’s a lot of potential for trouble. In this case, you must take care to\nensure that everything works out OK.\n One concept that’s widely used to help programmers reason about their code is\ninvariants—statements that are always true about a particular data structure, such as\n“this variable contains the number of items in the list.” These invariants are often bro-\nken during an update, especially if the data structure is of any complexity or the\nupdate requires modification of more than one value.\n Consider a doubly linked list, where each node holds a pointer to both the next\nnode in the list and the previous one. One of the invariants is that if you follow a\n“next” pointer from one node (A) to another (B), the “previous” pointer from that\nnode (B) points back to the first node (A). In order to remove a node from the list,\nthe nodes on either side have to be updated to point to each other. Once one has\nbeen updated, the invariant is broken until the node on the other side has been\nupdated too; after the update has completed, the invariant holds again.\n The steps in deleting an entry from such a list are shown in figure 3.1:\na\nIdentify the node to delete: N.\nb\nUpdate the link from the node prior to N to point to the node after N.\nc\nUpdate the link from the node after N to point to the node prior to N.\nd\nDelete node N.\nAs you can see in figure 3.1, between steps b and c, the links going in one direction\nare inconsistent with the links going in the opposite direction, and the invariant is\nbroken.\n The simplest potential problem with modifying data that’s shared between threads\nis that of broken invariants. If you don’t do anything special to ensure otherwise, if\none thread is reading the doubly linked list while another is removing a node, it’s\nquite possible for the reading thread to see the list with a node only partially removed\n(because only one of the links has been changed, as in step b of figure 3.1), so the\ninvariant is broken. The consequences of this broken invariant can vary; if the other\n\n\n38\nCHAPTER 3\nSharing data between threads\nthread is reading the list items from left to right in the diagram, it will skip the node\nbeing deleted. On the other hand, if the second thread is trying to delete the right-\nmost node in the diagram, it might end up permanently corrupting the data structure\nand eventually crashing the program. Whatever the outcome, this is an example of\none of the most common causes of bugs in concurrent code: a race condition.\n3.1.1\nRace conditions\nSuppose you’re buying tickets to see a movie at the movie theater. If it’s a big theater,\nmultiple cashiers will be taking money so more than one person can buy tickets at the\nsame time. If someone at another cashier’s desk is also buying tickets for the same\nFigure 3.1\nDeleting a node from a doubly linked list\n\n\n39\nProblems with sharing data between threads\nmovie as you are, which seats are available for you to choose from depends on\nwhether the other person books first or you do. If there are only a few seats left, this\ndifference can be quite crucial: it might literally be a race to see who gets the last tick-\nets. This is an example of a race condition: which seats you get (or even whether you get\ntickets) depends on the relative ordering of the two purchases.\n In concurrency, a race condition is anything where the outcome depends on the\nrelative ordering of execution of operations on two or more threads; the threads race\nto perform their respective operations. Most of the time, this is quite benign because\nall possible outcomes are acceptable, even though they may change with different rel-\native orderings. For example, if two threads are adding items to a queue for process-\ning, it generally doesn’t matter which item gets added first, provided that the\ninvariants of the system are maintained. It’s when the race condition leads to broken\ninvariants that there’s a problem, such as with the doubly linked list example men-\ntioned. When talking about concurrency, the term race condition is usually used to mean\na problematic race condition; benign race conditions aren’t so interesting and aren’t a\ncause of bugs. The C++ Standard also defines the term data race to mean the specific\ntype of race condition that arises because of concurrent modification to a single object\n(see section 5.1.2 for details); data races cause the dreaded undefined behavior.\n Problematic race conditions typically occur where completing an operation requires\nmodification of two or more distinct pieces of data, such as the two link pointers in\nthe example. Because the operation must access two separate pieces of data, these\nmust be modified in separate instructions, and another thread could potentially\naccess the data structure when only one of them has been completed. Race conditions\ncan often be hard to find and hard to duplicate because the window of opportunity is\nsmall. If the modifications are done as consecutive CPU instructions, the chance of\nthe problem exhibiting on any one run-through is small, even if the data structure is\nbeing accessed by another thread concurrently. As the load on the system increases,\nand the number of times the operation is performed increases, the chance of the\nproblematic execution sequence occurring also increases. It’s almost inevitable that\nsuch problems will show up at the most inconvenient time. Because race conditions\nare generally timing-sensitive, they can often disappear entirely when the application\nis run under the debugger, because the debugger affects the timing of the program,\neven if only slightly.\n If you’re writing multithreaded programs, race conditions can easily be the bane\nof your existence; a great deal of the complexity in writing software that uses concur-\nrency comes from avoiding problematic race conditions.\n3.1.2\nAvoiding problematic race conditions\nThere are several ways to deal with problematic race conditions. The simplest option\nis to wrap your data structure with a protection mechanism to ensure that only the\nthread performing a modification can see the intermediate states where the invariants\nare broken. From the point of view of other threads accessing that data structure, such\n\n\n40\nCHAPTER 3\nSharing data between threads\nmodifications either haven’t started or have completed. The C++ Standard Library\nprovides several of these mechanisms, which are described in this chapter.\n Another option is to modify the design of your data structure and its invariants so\nthat modifications are done as a series of indivisible changes, each of which preserves\nthe invariants. This is generally referred to as lock-free programming and is difficult to get\nright. If you’re working at this level, the nuances of the memory model and identify-\ning which threads can potentially see which set of values can get complicated. The\nmemory model is covered in chapter 5, and lock-free programming is discussed in\nchapter 7.\n Another way of dealing with race conditions is to handle the updates to the data\nstructure as a transaction, just as updates to a database are done within a transaction.\nThe required series of data modifications and reads is stored in a transaction log and\nthen committed in a single step. If the commit can’t proceed because the data struc-\nture has been modified by another thread, the transaction is restarted. This is termed\nsoftware transactional memory (STM), and it’s an active research area at the time of writ-\ning. It won’t be covered in this book, because there’s no direct support for STM in\nC++ (though there is a Technical Specification for Transactional Memory Extensions\nto C++1). But the basic idea of doing something privately and then committing in a\nsingle step is something that I’ll come back to later.\n The most basic mechanism for protecting shared data provided by the C++ Stan-\ndard is the mutex, so we’ll look at that first.\n3.2\nProtecting shared data with mutexes\nSo, you have a shared data structure such as the linked list from the previous section,\nand you want to protect it from race conditions and the potential broken invariants\nthat can ensue. Wouldn’t it be nice if you could mark all the pieces of code that access\nthe data structure as mutually exclusive, so that if any thread was running one of them,\nany other thread that tried to access that data structure had to wait until the first\nthread was finished? That would make it impossible for a thread to see a broken\ninvariant except when it was the thread doing the modification.\n Well, this isn’t a fairy tale wish—it’s precisely what you get if you use a synchroniza-\ntion primitive called a mutex (mutual exclusion). Before accessing a shared data struc-\nture, you lock the mutex associated with that data, and when you’ve finished accessing\nthe data structure, you unlock the mutex. The Thread Library then ensures that once\none thread has locked a specific mutex, all other threads that try to lock the same\nmutex have to wait until the thread that successfully locked the mutex unlocks it. This\nensures that all threads see a self-consistent view of the shared data, without any bro-\nken invariants.\n Mutexes are the most general of the data-protection mechanisms available in C++,\nbut they’re not a silver bullet; it’s important to structure your code to protect the right\n1 ISO/IEC TS 19841:2015—Technical Specification for C++ Extensions for Transactional Memory http://www\n.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=66343.\n\n\n41\nProtecting shared data with mutexes\ndata (see section 3.2.2) and avoid race conditions inherent in your interfaces (see sec-\ntion 3.2.3). Mutexes also come with their own problems in the form of a deadlock (see\nsection 3.2.4) and protecting either too much or too little data (see section 3.2.8).\nLet’s start with the basics.\n3.2.1\nUsing mutexes in C++\nIn C++, you create a mutex by constructing an instance of std::mutex, lock it with a\ncall to the lock() member function, and unlock it with a call to the unlock() member\nfunction. But it isn’t recommended practice to call the member functions directly,\nbecause this means that you have to remember to call unlock() on every code path\nout of a function, including those due to exceptions. Instead, the Standard C++\nLibrary provides the std::lock_guard class template, which implements that RAII\nidiom for a mutex; it locks the supplied mutex on construction and unlocks it on\ndestruction, ensuring a locked mutex is always correctly unlocked. The following list-\ning shows how to protect a list that can be accessed by multiple threads using\nstd::mutex, along with std::lock_guard. Both of these are declared in the <mutex>\nheader.\n#include <list>\n#include <mutex>\n#include <algorithm>\nstd::list<int> some_list;    \nstd::mutex some_mutex;        \nvoid add_to_list(int new_value)\n{\n    std::lock_guard<std::mutex> guard(some_mutex);   \n    some_list.push_back(new_value);\n}\nbool list_contains(int value_to_find) \n{\n    std::lock_guard<std::mutex> guard(some_mutex);   \n    return std::find(some_list.begin(),some_list.end(),value_to_find)\n        != some_list.end();\n}\nIn listing 3.1, there’s a single global variable B, and it’s protected with a correspond-\ning global instance of std::mutex c. The use of std::lock_guard<std::mutex> in\nadd_to_list() d, and again in list_contains() e, means that the accesses in\nthese functions are mutually exclusive: list_contains() will never see the list partway\nthrough a modification by add_to_list().\n C++17 has a new feature called class template argument deduction, which means\nthat for simple class templates like std::lock_guard, the template argument list can\noften be omitted. d and e can be reduced to\nstd::lock_guard guard(some_mutex);\nListing 3.1\nProtecting a list with a mutex\nB\nc\nd\ne\n\n\n42\nCHAPTER 3\nSharing data between threads\non a C++17 compiler. As we will see in section 3.2.4, C++17 also introduces an enhanced\nversion of lock guard called std::scoped_lock, so in a C++17 environment, this may\nwell be written as\nstd::scoped_lock guard(some_mutex);\nFor clarity of code and compatibility with older compilers, I’ll continue to use\nstd::lock_guard and specify the template arguments in other code snippets.\n Although there are occasions where this use of global variables is appropriate, in\nthe majority of cases it’s common to group the mutex and the protected data together\nin a class rather than use global variables. This is a standard application of object-\noriented design rules: by putting them in a class, you’re clearly marking them as\nrelated, and you can encapsulate the functionality and enforce the protection. In this\ncase, the add_to_list and list_contains functions would become member func-\ntions of the class, and the mutex and protected data would both become private\nmembers of the class, making it much easier to identify which code has access to the\ndata and thus which code needs to lock the mutex. If all the member functions of\nthe class lock the mutex before accessing any other data members and unlock it when\ndone, the data is nicely protected from all comers.\n Well, that’s not quite true, as the astute among you will have noticed: if one of the\nmember functions returns a pointer or reference to the protected data, then it doesn’t\nmatter that the member functions all lock the mutex in a nice, orderly fashion, because\nyou’ve blown a big hole in the protection. Any code that has access to that pointer or ref-\nerence can now access (and potentially modify) the protected data without locking the mutex.\nProtecting data with a mutex therefore requires careful interface design to ensure\nthat the mutex is locked before there’s any access to the protected data and that there\nare no backdoors.\n3.2.2\nStructuring code for protecting shared data\nAs you’ve seen, protecting data with a mutex is not quite as easy as slapping an\nstd::lock_guard object in every member function; one stray pointer or reference,\nand all that protection is for nothing. At one level, checking for stray pointers or refer-\nences is easy; as long as none of the member functions return a pointer or reference\nto the protected data to their caller either via their return value or via an out parame-\nter, the data is safe. If you dig a little deeper, it’s not that straightforward—nothing\never is. As well as checking that the member functions don’t pass out pointers or refer-\nences to their callers, it’s also important to check that they don’t pass these pointers or\nreferences in to functions they call that aren’t under your control. This is just as dan-\ngerous: those functions might store the pointer or reference in a place where it can\nlater be used without the protection of the mutex. Particularly dangerous in this\nregard are functions that are supplied at runtime via a function argument or other\nmeans, as in the next listing.\n\n\n43\nProtecting shared data with mutexes\nclass some_data\n{\n    int a;\n    std::string b;\npublic:\n    void do_something();\n};\nclass data_wrapper\n{\nprivate:\n    some_data data;\n    std::mutex m;\npublic:\n    template<typename Function>\n    void process_data(Function func)\n    {\n        std::lock_guard<std::mutex> l(m);\n        func(data);                      \n    }\n};\nsome_data* unprotected;\nvoid malicious_function(some_data& protected_data)\n{\n    unprotected=&protected_data;\n}\ndata_wrapper x;\nvoid foo()\n{\n    x.process_data(malicious_function);    \n    unprotected->do_something();         \n}\nIn this example, the code in process_data looks harmless enough, nicely protected\nwith std::lock_guard, but the call to the user-supplied func function B means that\nfoo can pass in malicious_function to bypass the protection c and then call\ndo_something() without the mutex being locked d.\n Fundamentally, the problem with this code is that it hasn’t done what you set out\nto do: mark all the pieces of code that access the data structure as mutually exclusive. In\nthis case, it missed the code in foo() that calls unprotected->do_something().\nUnfortunately, this part of the problem isn’t something the C++ Thread Library can\nhelp you with; it’s up to you as programmers to lock the right mutex to protect your\ndata. On the upside, you have a guideline to follow, which will help you in these cases:\nDon’t pass pointers and references to protected data outside the scope of the lock, whether by\nreturning them from a function, storing them in externally visible memory, or passing them as\narguments to user-supplied functions.\n Although this is a common mistake when trying to use mutexes to protect shared\ndata, it’s far from the only potential pitfall. As you’ll see in the next section, it’s still\npossible to have race conditions, even when data is protected with a mutex.\nListing 3.2\nAccidentally passing out a reference to protected data\nPass “protected” data to \nuser-supplied function\nB\nPass in a malicious \nfunction\nc\nUnprotected access \nto protected data\nd\n",
      "page_number": 59
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 67-78)",
      "start_page": 67,
      "end_page": 78,
      "detection_method": "topic_boundary",
      "content": "44\nCHAPTER 3\nSharing data between threads\n3.2.3\nSpotting race conditions inherent in interfaces\nJust because you’re using a mutex or other mechanism to protect shared data, it\ndoesn’t mean you’re protected from race conditions; you still have to ensure that the\nappropriate data is protected. Consider the doubly linked list example again. In order\nfor a thread to safely delete a node, you need to ensure that you’re preventing concur-\nrent accesses to three nodes: the node being deleted and the nodes on either side. If\nyou protected access to the pointers of each node individually, you’d be no better off\nthan with code that used no mutexes, because the race condition could still happen—\nit’s not the individual nodes that need protecting for the individual steps but the\nwhole data structure, for the whole delete operation. The easiest solution in this case\nis to have a single mutex that protects the entire list, as in listing 3.1.\n Just because individual operations on the list are safe, you’re not out of the woods\nyet; you can still get race conditions, even with a simple interface. Consider a stack\ndata structure like the std::stack container adapter shown in listing 3.3. Aside from\nthe constructors and swap(), there are only five things you can do to a std::stack:\nyou can push() a new element onto the stack, pop() an element off the stack, read the\ntop() element, check whether it’s empty(), and read the number of elements—the\nsize() of the stack. If you change top() so that it returns a copy rather than a refer-\nence (so you’re following the guideline from section 3.2.2) and protect the internal\ndata with a mutex, this interface is still inherently subject to race conditions. This\nproblem is not unique to a mutex-based implementation; it’s an interface problem, so\nthe race conditions would still occur with a lock-free implementation.\ntemplate<typename T,typename Container=std::deque<T> >\nclass stack\n{\npublic:\n    explicit stack(const Container&);\n    explicit stack(Container&& = Container());\n    template <class Alloc> explicit stack(const Alloc&);\n    template <class Alloc> stack(const Container&, const Alloc&);\n    template <class Alloc> stack(Container&&, const Alloc&);\n    template <class Alloc> stack(stack&&, const Alloc&);\n    bool empty() const;\n    size_t size() const;\n    T& top();\n    T const& top() const;\n    void push(T const&);\n    void push(T&&);\n    void pop();\n    void swap(stack&&);\n    template <class... Args> void emplace(Args&&... args);    \n};\nThe problem here is that the results of empty() and size() can’t be relied on. Although\nthey might be correct at the time of the call, once they’ve returned, other threads are\nListing 3.3\nThe interface to the std::stack container adapter\nNew in \nC++14\n\n\n45\nProtecting shared data with mutexes\nfree to access the stack and might push() new elements onto or pop() the existing\nones off of the stack before the thread that called empty() or size() could use that\ninformation.\n In particular, if the stack instance is not shared, it’s safe to check for empty() and\nthen call top() to access the top element if the stack is not empty, as follows:\nstack<int> s;\nif(!s.empty())   \n{\n    int const value=s.top();    \n    s.pop();               \n    do_something(value);\n}\nNot only is it safe in single-threaded code, it’s expected: calling top() on an empty\nstack is undefined behavior. With a shared stack object, this call sequence is no longer\nsafe, because there might be a call to pop() from another thread that removes the last\nelement in between the call to empty() B and the call to top() c. This is therefore a\nclassic race condition, and the use of a mutex internally to protect the stack contents\ndoesn’t prevent it; it’s a consequence of the interface.\n What’s the solution? Well, this problem happens as a consequence of the design of\nthe interface, so the solution is to change the interface. But that still begs the ques-\ntion: what changes need to be made? In the simplest case, you could declare that\ntop() will throw an exception if there aren’t any elements in the stack when it’s called.\nThough this directly addresses this issue, it makes for more cumbersome program-\nming, because now you need to be able to catch an exception, even if the call to\nempty() returned false. This makes the call to empty() an optimization to avoid the\noverhead of throwing an exception if the stack is already empty (though if the state\nchanges between the call to empty() and the call to top(), then the exception will still\nbe thrown), rather than a necessary part of the design.\n If you look closely at the previous snippet, there’s also potential for another race\ncondition, but this time between the call to top() c and the call to pop() d. Con-\nsider two threads running the previous snippet of code and both referencing the same\nstack object, s. This isn’t an unusual situation; when using threads for performance,\nit’s quite common to have several threads running the same code on different data,\nand a shared stack object is ideal for dividing work between them (though more com-\nmonly, a queue is used for this purpose—see the examples in chapters 6 and 7). Sup-\npose that initially the stack has two elements, so you don’t have to worry about the\nrace between empty() and top() on either thread, and consider the potential execu-\ntion patterns.\n If the stack is protected by a mutex internally, only one thread can be running a\nstack member function at any one time, so the calls get nicely interleaved, but the\ncalls to do_something() can run concurrently. One possible execution is shown in\ntable 3.1.\nb\nc\nd\n\n\n46\nCHAPTER 3\nSharing data between threads\nAs you can see, if these are the only threads running, there’s nothing in between the\ntwo calls to top() to modify the stack, so both threads will see the same value. Not only\nthat, but there are no calls to top() between the calls to pop(). Consequently, one of\nthe two values on the stack is discarded without ever having been read, whereas the\nother is processed twice. This is yet another race condition and far more insidious\nthan the undefined behavior of the empty()/top() race; there’s never anything obvi-\nously wrong going on, and the consequences of the bug are likely far removed from\nthe cause, although they obviously depend on exactly what do_something() does.\n This calls for a more radical change to the interface, one that combines the calls to\ntop() and pop() under the protection of the mutex. Tom Cargill2 pointed out that a\ncombined call can lead to issues if the copy constructor for the objects on the stack\ncan throw an exception. This problem was dealt with fairly comprehensively from an\nexception-safety point of view by Herb Sutter,3 but the potential for race conditions\nbrings something new to the mix.\n For those of you who aren’t aware of the issue, consider stack<vector<int>>.\nNow, a vector is a dynamically sized container, so when you copy a vector, the library\nhas to allocate some more memory from the heap in order to copy the contents. If the\nsystem is heavily loaded or there are significant resource constraints, this memory allo-\ncation can fail, so the copy constructor for vector might throw a std::bad_alloc\nexception. This is likely if the vector contains a lot of elements. If the pop() function\nwas defined to return the value popped, as well as remove it from the stack, you have a\npotential problem: the value being popped is returned to the caller only after the stack\nhas been modified, but the process of copying the data to return to the caller might\nthrow an exception. If this happens, the data popped is lost; it has been removed from\nTable 3.1\nA possible ordering of operations on a stack from two threads\nThread A\nThread B\nif(!s.empty())\nif(!s.empty())\n    int const value=s.top();\n    int const value=s.top();\n    s.pop();\n    do_something(value);\n    s.pop();\n    do_something(value);\n2 Tom Cargill, “Exception Handling: A False Sense of Security,” in C++ Report 6, no. 9 (November–December\n1994). Also available at http://www.informit.com/content/images/020163371x/supplements/Exception_\nHandling_Article.html.\n3 Herb Sutter, Exceptional C++: 47 Engineering Puzzles, Programming Problems, and Solutions (Addison Wes-\nley Professional, 1999).\n\n\n47\nProtecting shared data with mutexes\nthe stack, but the copy was unsuccessful! The designers of the std::stack interface\nhelpfully split the operation in two: get the top element (top()) and then remove it\nfrom the stack (pop()), so that if you can’t safely copy the data, it stays on the stack. If\nthe problem was lack of heap memory, maybe the application can free some memory\nand try again.\n Unfortunately, it’s precisely this split that you’re trying to avoid in eliminating the\nrace condition! Thankfully, there are alternatives, but they aren’t without cost.\nOPTION 1: PASS IN A REFERENCE\nThe first option is to pass a reference to a variable in which you want to receive the\npopped value as an argument in the call to pop():\nstd::vector<int> result;\nsome_stack.pop(result);\nThis works well for many cases, but it has the distinct disadvantage that it requires the\ncalling code to construct an instance of the stack’s value type prior to the call, in order\nto pass this in as the target. For some types this is impractical, because constructing an\ninstance is expensive in terms of time or resources. For other types this isn’t always\npossible, because the constructors require parameters that aren’t necessarily avail-\nable at this point in the code. Finally, it requires that the stored type be assignable.\nThis is an important restriction: many user-defined types do not support assign-\nment, though they may support move construction or even copy construction (and\nallow return by value).\nOPTION 2: REQUIRE A NO-THROW COPY CONSTRUCTOR OR MOVE CONSTRUCTOR\nThere’s only an exception safety problem with a value-returning pop() if the return by\nvalue can throw an exception. Many types have copy constructors that don’t throw\nexceptions, and with the new rvalue-reference support in the C++ Standard (see\nappendix A, section A.1), many more types will have a move constructor that doesn’t\nthrow exceptions, even if their copy constructor does. One valid option is to restrict\nthe use of your thread-safe stack to those types that can safely be returned by value\nwithout throwing an exception.\n Although this is safe, it’s not ideal. Even though you can detect at compile time the\nexistence of a copy or move constructor that doesn’t throw an exception using the\nstd::is_nothrow_copy_constructible and std::is_nothrow_move_constructible\ntype traits, it’s quite limiting. There are many more user-defined types with copy con-\nstructors that can throw and don’t have move constructors than there are types with\ncopy and/or move constructors that can’t throw (although this might change as peo-\nple get used to the rvalue-reference support in C++11). It would be unfortunate if\nsuch types couldn’t be stored in your thread-safe stack.\nOPTION 3: RETURN A POINTER TO THE POPPED ITEM\nThe third option is to return a pointer to the popped item rather than return the item\nby value. The advantage here is that pointers can be freely copied without throwing an\n\n\n48\nCHAPTER 3\nSharing data between threads\nexception, so you’ve avoided Cargill’s exception problem. The disadvantage is that\nreturning a pointer requires a means of managing the memory allocated to the\nobject, and for simple types such as integers, the overhead of such memory manage-\nment can exceed the cost of returning the type by value. For any interface that uses\nthis option, std::shared_ptr would be a good choice of pointer type; not only does it\navoid memory leaks, because the object is destroyed once the last pointer is destroyed,\nbut the library is in full control of the memory allocation scheme and doesn’t have to\nuse new and delete. This can be important for optimization purposes: requiring that\neach object in the stack be allocated separately with new would impose quite an over-\nhead compared to the original non-thread-safe version.\nOPTION 4: PROVIDE BOTH OPTION 1 AND EITHER OPTION 2 OR 3\nFlexibility should never be ruled out, especially in generic code. If you’ve chosen\noption 2 or 3, it’s relatively easy to provide option 1 as well, and this provides users of\nyour code the ability to choose whichever option is most appropriate for them at little\nadditional cost.\nEXAMPLE DEFINITION OF A THREAD-SAFE STACK\nListing 3.4 shows the class definition for a stack with no race conditions in the\ninterface and that implements options 1 and 3: there are two overloads of pop(),\none that takes a reference to a location in which to store the value and one that\nreturns std::shared_ptr<>. It has a simple interface, with only two functions: push()\nand pop().\n#include <exception>\n#include <memory>               \nstruct empty_stack: std::exception\n{\n    const char* what() const noexcept;\n};\ntemplate<typename T>\nclass threadsafe_stack\n{\npublic:\n    threadsafe_stack();\n    threadsafe_stack(const threadsafe_stack&);     \n    threadsafe_stack& operator=(const threadsafe_stack&) = delete;   \n    void push(T new_value);\n    std::shared_ptr<T> pop();\n    void pop(T& value);\n    bool empty() const;\n};\nBy paring down the interface you allow for maximum safety; even operations on the\nwhole stack are restricted. The stack itself can’t be assigned, because the assignment\noperator is deleted B (see appendix A, section A.2), and there’s no swap() function.\nListing 3.4\nAn outline class definition for a thread-safe stack\nFor std::shared_ptr<>\nb\nAssignment\noperator is\ndeleted\n\n\n49\nProtecting shared data with mutexes\nIt can, however, be copied, assuming the stack elements can be copied. The pop()\nfunctions throw an empty_stack exception if the stack is empty, so everything still\nworks even if the stack is modified after a call to empty(). As mentioned in the\ndescription of option 3, the use of std::shared_ptr allows the stack to take care of\nthe memory-allocation issues and avoid excessive calls to new and delete if desired.\nYour five stack operations have now become three: push(), pop(), and empty(). Even\nempty() is superfluous. This simplification of the interface allows for better control\nover the data; you can ensure that the mutex is locked for the entirety of an operation.\nThe following listing shows a simple implementation that’s a wrapper around\nstd::stack<>.\n#include <exception>\n#include <memory>\n#include <mutex>\n#include <stack>\nstruct empty_stack: std::exception\n{\n    const char* what() const throw();\n};\ntemplate<typename T>\nclass threadsafe_stack\n{\nprivate:\n    std::stack<T> data;\n    mutable std::mutex m;\npublic:\n    threadsafe_stack(){}\n    threadsafe_stack(const threadsafe_stack& other)\n    {\n        std::lock_guard<std::mutex> lock(other.m);\n        data=other.data;                            \n    }\n    threadsafe_stack& operator=(const threadsafe_stack&) = delete;\n    void push(T new_value)\n    {\n        std::lock_guard<std::mutex> lock(m);\n        data.push(std::move(new_value));\n    }\n    std::shared_ptr<T> pop()\n    {\n        std::lock_guard<std::mutex> lock(m);\n        if(data.empty()) throw empty_stack();    \n        std::shared_ptr<T> const res(std::make_shared<T>(data.top()));   \n        data.pop();                                          \n        return res;\n    }\n    void pop(T& value)\n    {\n        std::lock_guard<std::mutex> lock(m);\n        if(data.empty()) throw empty_stack();\nListing 3.5\nA fleshed-out class definition for a thread-safe stack\nCopy performed \nin constructor \nbody\nb\nCheck for empty \nbefore trying to \npop value\nAllocate return value\nbefore modifying stack\n\n\n50\nCHAPTER 3\nSharing data between threads\n        value=data.top();\n        data.pop();\n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lock(m);\n        return data.empty();\n    }\n};\nThis stack implementation is copyable—the copy constructor locks the mutex in the\nsource object and then copies the internal stack. You do the copy in the constructor\nbody B rather than the member initializer list in order to ensure that the mutex is\nheld across the copy.\n As the discussion of top() and pop() shows, problematic race conditions in inter-\nfaces arise because of locking at too small a granularity; the protection doesn’t cover\nthe entirety of the desired operation. Problems with mutexes can also arise from lock-\ning at too large a granularity; the extreme situation is a single global mutex that pro-\ntects all shared data. In a system where there’s a significant amount of shared data,\nthis can eliminate any performance benefits of concurrency, because the threads are\nforced to run one at a time, even when they’re accessing different bits of data. The\nfirst versions of the Linux kernel that were designed to handle multi-processor systems\nused a single global kernel lock. Although this worked, it meant that a two-processor\nsystem typically had much worse performance than two single-processor systems, and\nperformance on a four-processor system was nowhere near that of four single-processor\nsystems. There was too much contention for the kernel, so the threads running on the\nadditional processors were unable to perform useful work. Later revisions of the Linux\nkernel have moved to a more fine-grained locking scheme, so the performance of a\nfour-processor system is much nearer the ideal of four times that of a single-processor\nsystem, because there’s far less contention.\n One issue with fine-grained locking schemes is that sometimes you need more\nthan one mutex locked in order to protect all the data in an operation. As described\npreviously, sometimes the right thing to do is increase the granularity of the data cov-\nered by the mutexes, so that only one mutex needs to be locked. But sometimes that’s\nundesirable, such as when the mutexes are protecting separate instances of a class. In\nthis case, locking at the next level up would mean either leaving the locking to the\nuser or having a single mutex that protected all instances of that class, neither of\nwhich is particularly desirable.\n If you end up having to lock two or more mutexes for a given operation, there’s\nanother potential problem lurking in the wings: deadlock. This is almost the opposite\nof a race condition: rather than two threads racing to be first, each one is waiting for\nthe other, so neither makes any progress.\n\n\n51\nProtecting shared data with mutexes\n3.2.4\nDeadlock: the problem and a solution\nImagine that you have a toy that comes in two parts, and you need both parts to play\nwith it—a toy drum and drumstick, for example. Now imagine that you have two small\nchildren, both of whom like playing with it. If one of them gets both the drum and the\ndrumstick, that child can merrily play the drum until tiring of it. If the other child\nwants to play, they have to wait, however sad that makes them. Now imagine that the\ndrum and the drumstick are buried (separately) in the toy box, and your children\nboth decide to play with them at the same time, so they go rummaging in the toy box.\nOne finds the drum and the other finds the drumstick. Now they’re stuck; unless one\ndecides to be nice and let the other play, each will hold on to whatever they have and\ndemand that they be given the other piece, so neither gets to play.\n Now imagine that you have not children arguing over toys but threads arguing\nover locks on mutexes: each of a pair of threads needs to lock both of a pair of\nmutexes to perform some operation, and each thread has one mutex and is waiting\nfor the other. Neither thread can proceed, because each is waiting for the other to\nrelease its mutex. This scenario is called deadlock, and it’s the biggest problem with\nhaving to lock two or more mutexes in order to perform an operation.\n The common advice for avoiding deadlock is to always lock the two mutexes in the\nsame order: if you always lock mutex A before mutex B, then you’ll never deadlock.\nSometimes this is straightforward, because the mutexes are serving different pur-\nposes, but other times it’s not so simple, such as when the mutexes are each protect-\ning a separate instance of the same class. Consider, for example, an operation that\nexchanges data between two instances of the same class; in order to ensure that the\ndata is exchanged correctly, without being affected by concurrent modifications, the\nmutexes on both instances must be locked. But if a fixed order is chosen (for exam-\nple, the mutex for the instance supplied as the first parameter, then the mutex for the\ninstance supplied as the second parameter), this can backfire: all it takes is for two\nthreads to try to exchange data between the same two instances with the parameters\nswapped, and you have deadlock!\n Thankfully, the C++ Standard Library has a cure for this in the form of\nstd::lock—a function that can lock two or more mutexes at once without risk of\ndeadlock. The example in the next listing shows how to use this for a simple swap\noperation.\nclass some_big_object;\nvoid swap(some_big_object& lhs,some_big_object& rhs);\nclass X\n{\nprivate:\n    some_big_object some_detail;\n    std::mutex m;\npublic:\n    X(some_big_object const& sd):some_detail(sd){}\nListing 3.6\nUsing std::lock() and std::lock_guard in a swap operation\n\n\n52\nCHAPTER 3\nSharing data between threads\n    friend void swap(X& lhs, X& rhs)\n    {\n        if(&lhs==&rhs)\n            return;\n        std::lock(lhs.m,rhs.m);    \n        std::lock_guard<std::mutex> lock_a(lhs.m,std::adopt_lock);  \n        std::lock_guard<std::mutex> lock_b(rhs.m,std::adopt_lock);  \n        swap(lhs.some_detail,rhs.some_detail);\n    }\n};\nFirst, the arguments are checked to ensure they are different instances, because\nattempting to acquire a lock on std::mutex when you already hold it is undefined\nbehavior. (A mutex that does permit multiple locks by the same thread is provided in\nthe form of std::recursive_mutex. See section 3.3.3 for details.) Then, the call to\nstd::lock() B locks the two mutexes, and two std::lock_guard instances are con-\nstructed c and d, one for each mutex. The std::adopt_lock parameter is supplied\nin addition to the mutex to indicate to the std::lock_guard objects that the mutexes\nare already locked, and they should adopt the ownership of the existing lock on the\nmutex rather than attempt to lock the mutex in the constructor.\n This ensures that the mutexes are correctly unlocked on function exit in the gen-\neral case where the protected operation might throw an exception; it also allows for a\nsimple return. Also, it’s worth noting that locking either lhs.m or rhs.m inside the call\nto std::lock can throw an exception; in this case, the exception is propagated out of\nstd::lock. If std::lock has successfully acquired a lock on one mutex and an excep-\ntion is thrown when it tries to acquire a lock on the other mutex, this first lock is\nreleased automatically: std::lock provides all-or-nothing semantics with regard to\nlocking the supplied mutexes.\n C++17 provides additional support for this scenario, in the form of a new RAII tem-\nplate, std::scoped_lock<>. This is exactly equivalent to std::lock_guard<>, except\nthat it is a variadic template, accepting a list of mutex types as template parameters, and\na list of mutexes as constructor arguments. The mutexes supplied to the constructor\nare locked using the same algorithm as std::lock, so that when the constructor com-\npletes they are all locked, and they are then all unlocked in the destructor. The\nswap() operation from listing 3.6 can be rewritten as follows:\nvoid swap(X& lhs, X& rhs)\n    {\n        if(&lhs==&rhs)\n            return;\n        std::scoped_lock guard(lhs.m,rhs.m);    \n        swap(lhs.some_detail,rhs.some_detail);\n    }\nThis example uses another feature added with C++17: automatic deduction of class\ntemplate parameters. If you have a C++17 compiler (which is likely if you’re using\nstd::scoped_lock, because that is a C++17 library facility), the C++17 implicit class\nB\nc\nd\nb\n\n\n53\nProtecting shared data with mutexes\ntemplate parameter deduction mechanism will choose the correct mutex types from\nthe types of the objects passed to the constructor at object B. This line is equivalent\nto the fully specified version:\nstd::scoped_lock<std::mutex,std::mutex> guard(lhs.m,rhs.m);\nThe existence of std::scoped_lock means that most of the cases where you would\nhave used std::lock prior to C++17 can now be written using std::scoped_lock,\nwith less potential for mistakes, which can only be a good thing!\n Although std::lock (and std::scoped_lock<>) can help you avoid deadlock in\nthose cases where you need to acquire two or more locks together, it doesn’t help if\nthey’re acquired separately. In that case, you have to rely on your discipline as devel-\nopers to ensure you don’t get deadlock. This isn’t easy: deadlocks are one of the nasti-\nest problems to encounter in multithreaded code and are often unpredictable, with\neverything working fine the majority of the time. There are, however, some relatively\nsimple rules that can help you to write deadlock-free code.\n3.2.5\nFurther guidelines for avoiding deadlock\nDeadlock doesn’t only occur with locks, although that’s the most frequent cause; you\ncan create deadlock with two threads and no locks by having each thread call join()\non the std::thread object for the other. In this case, neither thread can make prog-\nress because it’s waiting for the other to finish, like the children fighting over their toy.\nThis simple cycle can occur anywhere that a thread can wait for another thread to per-\nform some action if the other thread can simultaneously be waiting for the first\nthread, and it isn’t limited to two threads: a cycle of three or more threads will still\ncause deadlock. The guidelines for avoiding deadlock all boil down to one idea: don’t\nwait for another thread if there’s a chance it’s waiting for you. The individual guide-\nlines provide ways of identifying and eliminating the possibility that the other thread\nis waiting for you.\nAVOID NESTED LOCKS\nThe first idea is the simplest: don’t acquire a lock if you already hold one. If you stick\nto this guideline, it’s impossible to get a deadlock from the lock usage alone because\neach thread only ever holds a single lock. You could still get deadlock from other\nthings (like the threads waiting for each other), but mutex locks are probably the\nmost common cause of deadlock. If you need to acquire multiple locks, do it as a sin-\ngle action with std::lock in order to acquire them without deadlock.\nAVOID CALLING USER-SUPPLIED CODE WHILE HOLDING A LOCK\nThis is a simple follow-on from the previous guideline. Because the code is user-\nsupplied, you have no idea what it could do; it could do anything, including acquiring\na lock. If you call user-supplied code while holding a lock, and that code acquires a\nlock, you’ve violated the guideline on avoiding nested locks and could get deadlock.\nSometimes this is unavoidable; if you’re writing generic code, such as the stack in\n\n\n54\nCHAPTER 3\nSharing data between threads\nsection 3.2.3, every operation on the parameter type or types is user-supplied code. In\nthis case, you need a new guideline.\nACQUIRE LOCKS IN A FIXED ORDER\nIf you absolutely must acquire two or more locks, and you can’t acquire them as a sin-\ngle operation with std::lock, the next best thing is to acquire them in the same order\nin every thread. I touched on this in section 3.2.4 as one way of avoiding deadlock\nwhen acquiring two mutexes: the key is to define the order in a way that’s consistent\nbetween threads. In some cases, this is relatively easy. For example, look at the stack\nfrom section 3.2.3—the mutex is internal to each stack instance, but the operations\non the data items stored in a stack require calling user-supplied code. You can, how-\never, add the constraint that none of the operations on the data items stored in the\nstack should perform any operation on the stack itself. This puts the burden on the\nuser of the stack, but it’s rather uncommon for the data stored in a container to access\nthat container, and it’s quite apparent when this is happening, so it’s not a particularly\ndifficult burden to carry.\n In other cases, this isn’t so straightforward, as you discovered with the swap opera-\ntion in section 3.2.4. At least in that case you could lock the mutexes simultaneously,\nbut that’s not always possible. If you look back at the linked list example from sec-\ntion 3.1, you’ll see that one possibility for protecting the list is to have a mutex per\nnode. Then, in order to access the list, threads must acquire a lock on every node\nthey’re interested in. For a thread to delete an item, it must then acquire the lock on\nthree nodes: the node being deleted and the nodes on either side, because they’re all\nbeing modified in some way. Likewise, to traverse the list, a thread must keep hold of\nthe lock on the current node while it acquires the lock on the next one in the\nsequence, in order to ensure that the next pointer isn’t modified in the meantime.\nOnce the lock on the next node has been acquired, the lock on the first can be\nreleased because it’s no longer necessary.\n This hand-over-hand locking style allows multiple threads to access the list, pro-\nvided each is accessing a different node. But in order to prevent deadlock, the\nnodes must always be locked in the same order: if two threads tried to traverse the\nlist in opposite orders using hand-over-hand locking, they could deadlock with\neach other in the middle of the list. If nodes A and B are adjacent in the list, the\nthread going one way will try to hold the lock on node A and try to acquire the lock\non node B. A thread going the other way would be holding the lock on node B and\ntrying to acquire the lock on node A—a classic scenario for deadlock, as shown in\nfigure 3.2.\n Likewise, when deleting node B that lies between nodes A and C, if that thread\nacquires the lock on B before the locks on A and C, it has the potential to deadlock\nwith a thread traversing the list. Such a thread would try to lock either A or C first\n(depending on the direction of traversal) but would then find that it couldn’t obtain a\nlock on B because the thread doing the deleting was holding the lock on B and trying\nto acquire the locks on A and C.\n\n\n55\nProtecting shared data with mutexes\nOne way to prevent deadlock here is to define an order of traversal, so a thread must\nalways lock A before B and B before C. This would eliminate the possibility of dead-\nlock at the expense of disallowing reverse traversal. Similar conventions can often be\nestablished for other data structures.\nUSE A LOCK HIERARCHY\nAlthough this is a particular case of defining lock ordering, a lock hierarchy can pro-\nvide a means of checking that the convention is adhered to at runtime. The idea is that\nyou divide your application into layers and identify all the mutexes that may be locked\nin any given layer. When code tries to lock a mutex, it isn’t permitted to lock that mutex\nif it already holds a lock from a lower layer. You can check this at runtime by assigning\nlayer numbers to each mutex and keeping a record of which mutexes are locked by\neach thread. This is a common pattern, but the C++ Standard Library does not provide\ndirect support for it, so you will need to write a custom hierarchical_mutex mutex\ntype, the code for which is shown in listing 3.8.\nThread 1\nThread 2\nLock master entry mutex\nRead head node pointer\nLock head node mutex\nUnlock master entry mutex\nLock master entry mutex\nRead head →next pointer\nLock tail node mutex\nLock next node mutex\nRead tail →prev pointer\nRead next →next pointer\nUnlock tail node mutex\n…\n…\nLock node A mutex\nLock node C mutex\nRead A →next pointer (which is B)\nRead C →next pointer (which is B)\nLock node B mutex\nBlock trying to lock node B mutex\nUnlock node C mutex\nRead B →prev pointer (which is A)\nBlock trying to lock node A mutex\nDeadlock!\nFigure 3.2\nDeadlock with threads traversing a list in opposite orders\n",
      "page_number": 67
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 79-87)",
      "start_page": 79,
      "end_page": 87,
      "detection_method": "topic_boundary",
      "content": "56\nCHAPTER 3\nSharing data between threads\n The following listing shows an example of two threads using a hierarchical mutex.\nhierarchical_mutex high_level_mutex(10000);    \nhierarchical_mutex low_level_mutex(5000);    \nhierarchical_mutex other_mutex(6000);   \nint do_low_level_stuff();\nint low_level_func()\n{\n    std::lock_guard<hierarchical_mutex> lk(low_level_mutex);   \n    return do_low_level_stuff();\n}\nvoid high_level_stuff(int some_param);\nvoid high_level_func()\n{\n    std::lock_guard<hierarchical_mutex> lk(high_level_mutex);  \n    high_level_stuff(low_level_func());         \n}\nvoid thread_a()      \n{\n    high_level_func();\n}\nvoid do_other_stuff();\nvoid other_stuff()\n{\n    high_level_func();     \n    do_other_stuff();\n}\nvoid thread_b()     \n{\n    std::lock_guard<hierarchical_mutex> lk(other_mutex);    \n    other_stuff();\n}\nThis code has three instances of hierarchical_mutex, (B, c, and d), which are\nconstructed with progressively lower hierarchy numbers. Because the mechanism is\ndefined so that if you hold a lock on a hierarchical_mutex, then you can only acquire\na lock on another hierarchical_mutex with a lower hierarchy number, this imposes\nrestrictions on what the code can do.\n Assuming do_low_level_stuff doesn’t lock any mutexes, low_level_func is the\nbottom of your hierarchy, and locks the low_level_mutex e. high_level_func calls\nlow_level_func f, while holding a lock on high_level_mutex g, but that’s OK,\nbecause the hierarchy level of high_level_mutex (B: 10000) is higher than that of\nlow_level_mutex (c: 5000).\n thread_a() h abides by the rules, so it runs fine.\n On the other hand, thread_b() i disregards the rules and therefore will fail at\nruntime. \nListing 3.7\nUsing a lock hierarchy to prevent deadlock\nb\nc\nd\ne\ng\nf\nh\n1)\ni\nj\n\n\n57\nProtecting shared data with mutexes\n First off, it locks other_mutex j, which has a hierarchy value of only 6000 d.\nThis means it should be somewhere midway in the hierarchy. When other_stuff()\ncalls high_level_func() 1), it’s violating the hierarchy: high_level_func() tries to\nacquire the high_level_mutex, which has a value of 10000, considerably more than\nthe current hierarchy value of 6000. The hierarchical_mutex will therefore report\nan error, possibly by throwing an exception or aborting the program. Deadlocks\nbetween hierarchical mutexes are impossible, because the mutexes themselves\nenforce the lock ordering. This does mean that you can’t hold two locks at the same\ntime if they’re the same level in the hierarchy, so hand-over-hand locking schemes\nrequire that each mutex in the chain has a lower hierarchy value than the prior one,\nwhich may be impractical in some cases.\n This example also demonstrates another point: the use of the std::lock_guard<>\ntemplate with a user-defined mutex type. hierarchical_mutex is not part of the stan-\ndard but is easy to write; a simple implementation is shown in listing 3.8. Even though\nit’s a user-defined type, it can be used with std::lock_guard<> because it implements\nthe three member functions required to satisfy the mutex concept: lock(), unlock(),\nand try_lock(). You haven’t yet seen try_lock() used directly, but it’s fairly simple: if\nthe lock on the mutex is held by another thread, it returns false rather than waiting\nuntil the calling thread can acquire the lock on the mutex. It may also be used by\nstd::lock() internally, as part of the deadlock-avoidance algorithm.\n The implementation of hierarchical_mutex uses a thread-local variable to store\nthe current hierarchy value. This value is accessible to all mutex instances, but has a\ndifferent value on each thread. This allows the code to check the behavior of each\nthread separately, and the code for each mutex can check whether or not the current\nthread is allowed to lock that mutex.\nclass hierarchical_mutex\n{\n    std::mutex internal_mutex;\n    unsigned long const hierarchy_value;\n    unsigned long previous_hierarchy_value;\n    static thread_local unsigned long this_thread_hierarchy_value;    \n    void check_for_hierarchy_violation()\n    {\n        if(this_thread_hierarchy_value <= hierarchy_value)    \n        {\n            throw std::logic_error(“mutex hierarchy violated”);\n        }\n    }\n    void update_hierarchy_value()\n    {\n        previous_hierarchy_value=this_thread_hierarchy_value;    \n        this_thread_hierarchy_value=hierarchy_value;\n    }\nListing 3.8\nA simple hierarchical mutex\nb\nc\nd\n\n\n58\nCHAPTER 3\nSharing data between threads\npublic:\n    explicit hierarchical_mutex(unsigned long value):\n        hierarchy_value(value),\n        previous_hierarchy_value(0)\n    {}\n    void lock()\n    {\n        check_for_hierarchy_violation();\n        internal_mutex.lock();          \n        update_hierarchy_value();      \n    }\n    void unlock()\n    {\n        if(this_thread_hierarchy_value!=hierarchy_value)\n            throw std::logic_error(“mutex hierarchy violated”);    \n        this_thread_hierarchy_value=previous_hierarchy_value;    \n        internal_mutex.unlock();\n    }\n    bool try_lock()\n    {\n        check_for_hierarchy_violation();\n        if(!internal_mutex.try_lock())     \n            return false;\n        update_hierarchy_value();\n        return true;\n    }\n};\nthread_local unsigned long\n    hierarchical_mutex::this_thread_hierarchy_value(ULONG_MAX);    \nThe key here is the use of the thread_local value representing the hierarchy value\nfor the current thread: this_thread_hierarchy_value B. It’s initialized to the\nmaximum value i, so initially any mutex can be locked. Because it’s declared\nthread_local, every thread has its own copy, so the state of the variable in one thread\nis entirely independent of the state of the variable when read from another thread.\nSee appendix A, section A.8, for more information about thread_local.\n So, the first time a thread locks an instance of hierarchical_mutex, the value of\nthis_thread_hierarchy_value is ULONG_MAX. By its nature, this is greater than any\nother value, so the check in check_for_hierarchy_violation() c passes. With that\ncheck out of the way, lock()delegates to the internal mutex for the locking e. Once\nthis lock has succeeded, you can update the hierarchy value f.\n If you now lock another hierarchical_mutex while holding the lock on this first\none, the value of this_thread_hierarchy_value reflects the hierarchy value of the\nfirst mutex. The hierarchy value of this second mutex must now be less than that of\nthe mutex already held in order for the check c to pass.\n Now, it’s important to save the previous value of the hierarchy value for the cur-\nrent thread so you can restore it in unlock() g; otherwise you’d never be able to\nlock a mutex with a higher hierarchy value again, even if the thread didn’t hold any\nlocks. Because you store this previous hierarchy value only when you hold the\ne\nf\nj\ng\nh\ni\n\n\n59\nProtecting shared data with mutexes\ninternal_mutex d, and you restore it before you unlock the internal mutex g, you\ncan safely store it in the hierarchical_mutex itself, because it’s safely protected by the\nlock on the internal mutex. In order to avoid the hierarchy getting confused due to\nout-of-order unlocking, you throw at j if the mutex being unlocked is not the most\nrecently locked one. Other mechanisms are possible, but this is the simplest.\n try_lock() works the same as lock(), except that if the call to try_lock() on the\ninternal_mutex fails h, then you don’t own the lock, so you don’t update the hierar-\nchy value, and return false rather than true.\n Although detection is a runtime check, it’s at least not timing-dependent—you\ndon’t have to wait around for the rare conditions that cause deadlock to show up.\nAlso, the design process required to divide the application and mutexes in this way\ncan help eliminate many possible causes of deadlock before they even get written. It\nmight be worth performing the design exercise even if you don’t go as far as writing\nthe runtime checks.\nEXTENDING THESE GUIDELINES BEYOND LOCKS\nAs I mentioned back at the beginning of this section, deadlock doesn’t only occur\nwith locks; it can occur with any synchronization construct that can lead to a wait\ncycle. It’s therefore worth extending these guidelines to cover those cases too. For\nexample, just as you should avoid acquiring nested locks if possible, it’s a bad idea to\nwait for a thread while holding a lock, because that thread might need to acquire the\nlock in order to proceed. Similarly, if you’re going to wait for a thread to finish, it\nmight be worth identifying a thread hierarchy, so that a thread waits only for threads\nlower down the hierarchy. One simple way to do this is to ensure that your threads are\njoined in the same function that started them, as described in sections 3.1.2 and 3.3.\n Once you’ve designed your code to avoid deadlock, std::lock() and std::\nlock_guard cover most of the cases of simple locking, but sometimes more flexibility\nis required. For those cases, the Standard Library provides the std::unique_lock\ntemplate. Like std::lock_guard, this is a class template parameterized on the mutex\ntype, and it also provides the same RAII-style lock management as std::lock_guard,\nbut with a bit more flexibility.\n3.2.6\nFlexible locking with std::unique_lock\nstd::unique_lock provides a bit more flexibility than std::lock_guard by relaxing\nthe invariants; an std::unique_lock instance doesn’t always own the mutex that it’s\nassociated with. First off, as you can pass std::adopt_lock as a second argument to the\nconstructor to have the lock object manage the lock on a mutex, you can also pass\nstd::defer_lock as the second argument to indicate that the mutex should remain\nunlocked on construction. The lock can then be acquired later by calling lock() on the\nstd::unique_lock object (not the mutex) or by passing the std:: unique_lock object\nto std::lock(). Listing 3.6 could easily have been written as shown in listing 3.9,\nusing std::unique_lock and std::defer_lock B, rather than std::lock_guard and\nstd::adopt_lock. The code has the same line count and is equivalent, apart from\n\n\n60\nCHAPTER 3\nSharing data between threads\none small thing: std::unique_lock takes more space and is slightly slower to use\nthan std::lock_guard. The flexibility of allowing an std::unique_lock instance not\nto own the mutex comes at a price: this information has to be stored, and it has to\nbe updated.\nclass some_big_object;\nvoid swap(some_big_object& lhs,some_big_object& rhs);\nclass X\n{\nprivate:\n    some_big_object some_detail;\n    std::mutex m;\npublic:\n    X(some_big_object const& sd):some_detail(sd){}\n    friend void swap(X& lhs, X& rhs)\n    {\n        if(&lhs==&rhs)                                \n            return;\n        std::unique_lock<std::mutex> lock_a(lhs.m,std::defer_lock);  \n        std::unique_lock<std::mutex> lock_b(rhs.m,std::defer_lock);  \n        std::lock(lock_a,lock_b);                        \n        swap(lhs.some_detail,rhs.some_detail);\n    }\n};\nIn listing 3.9, the std::unique_lock objects could be passed to std::lock() c,\nbecause std::unique_lock provides lock(), try_lock(), and unlock() member\nfunctions. These forward to the member functions of the same name on the underly-\ning mutex to do the work and update a flag inside the std::unique_lock instance to\nindicate whether the mutex is currently owned by that instance. This flag is necessary\nin order to ensure that unlock() is called correctly in the destructor. If the instance does\nown the mutex, the destructor must call unlock(), and if the instance does not own the\nmutex, it must not call unlock(). This flag can be queried by calling the owns_lock()\nmember function. Unless you’re going to be transferring lock ownership around or\ndoing something else that requires std::unique_lock, you’re still better off using the\nC++17 variadic std::scoped_lock if it’s available to you (see section 3.2.4).\n As you might expect, this flag has to be stored somewhere. Therefore, the size of a\nstd::unique_lock object is typically larger than that of a std::lock_guard object,\nand there’s also a slight performance penalty when using std::unique_lock over\nstd:: lock_guard because the flag has to be updated or checked, as appropriate. If\nstd::lock_ guard is sufficient for your needs, I’d therefore recommend using it in\npreference. That said, there are cases where std::unique_lock is a better fit for the\ntask at hand because you need to make use of the additional flexibility. One example\nis deferred locking, as you’ve already seen; another case is where the ownership of the\nlock needs to be transferred from one scope to another.\nListing 3.9\nUsing std::lock() and std::unique_lock in a swap operation\nstd::defer_lock\nleaves mutexes\nunlocked.\nB\nMutexes are \nlocked here.\nc\n\n\n61\nProtecting shared data with mutexes\n3.2.7\nTransferring mutex ownership between scopes\nBecause std::unique_lock instances don’t have to own their associated mutexes, the\nownership of a mutex can be transferred between instances by moving the instances\naround. In some cases this transfer is automatic, such as when returning an instance\nfrom a function, and in other cases you have to do it explicitly by calling std::move().\nFundamentally this depends on whether the source is an lvalue—a real variable or ref-\nerence to one—or an rvalue—a temporary of some kind. Ownership transfer is auto-\nmatic if the source is an rvalue and must be done explicitly for an lvalue in order to\navoid accidentally transferring ownership away from a variable. std::unique_lock is\nan example of a type that’s moveable but not copyable. See appendix A, section A.1.1, for\nmore about move semantics.\n One possible use is to allow a function to lock a mutex and transfer ownership of\nthat lock to the caller, so the caller can then perform additional actions under the\nprotection of the same lock. The following code snippet shows an example of this; the\nget_lock() function locks the mutex and then prepares the data before returning\nthe lock to the caller:\nstd::unique_lock<std::mutex> get_lock()\n{\n    extern std::mutex some_mutex;\n    std::unique_lock<std::mutex> lk(some_mutex);\n    prepare_data();\n    return lk;        \n}\nvoid process_data()\n{\n    std::unique_lock<std::mutex> lk(get_lock());    \n    do_something();\n}\nBecause lk is an automatic variable declared within the function, it can be returned\ndirectly B, without a call to std:move(); the compiler takes care of calling the move\nconstructor. The process_data() function can then transfer ownership directly into\nits own std::unique_lock instance c, and the call to do_something() can rely on\nthe data being correctly prepared without another thread altering the data in the\nmeantime.\n Typically this sort of pattern would be used where the mutex to be locked is depen-\ndent on the current state of the program or on an argument passed in to the function\nthat returns the std::unique_lock object. One such usage is where the lock isn’t\nreturned directly but is a data member of a gateway class used to ensure correctly\nlocked access to some protected data. In this case, all access to the data is through this\ngateway class: when you want to access the data, you obtain an instance of the gateway\nclass (by calling a function such as get_lock() in the preceding example), which\nacquires the lock. You can then access the data through member functions of the gate-\nway object. When you’re finished, you destroy the gateway object, which releases the\nB\nc\n\n\n62\nCHAPTER 3\nSharing data between threads\nlock and allows other threads to access the protected data. Such a gateway object may\nwell be moveable (so that it can be returned from a function), in which case the lock\nobject data member also needs to be moveable.\n The flexibility of std::unique_lock also allows instances to relinquish their locks\nbefore they’re destroyed. You can do this with the unlock() member function, like for\na mutex. std::unique_lock supports the same basic set of member functions for lock-\ning and unlocking as a mutex does, so that it can be used with generic functions such\nas std::lock. The ability to release a lock before the std::unique_lock instance is\ndestroyed means that you can optionally release it in a specific code branch if it’s\napparent that the lock is no longer required. This can be important for the perfor-\nmance of the application; holding a lock for longer than required can cause a drop in\nperformance, because other threads waiting for the lock are prevented from proceed-\ning for longer than necessary.\n3.2.8\nLocking at an appropriate granularity\nThe granularity of a lock is something I touched on earlier, in section 3.2.3: the lock\ngranularity is a hand-waving term to describe the amount of data protected by a single\nlock. A fine-grained lock protects a small amount of data, and a coarse-grained lock\nprotects a large amount of data. Not only is it important to choose a sufficiently coarse\nlock granularity to ensure the required data is protected, but it’s also important to\nensure that a lock is held only for the operations that require it. We all know the frus-\ntration of waiting in the checkout line in a supermarket with a cart full of groceries\nonly for the person currently being served to suddenly realize that they forgot some\ncranberry sauce and then leave everybody waiting while they go and find some, or\nfor the cashier to be ready for payment and the customer to only then start rummag-\ning in their bag for their wallet. Everything proceeds much more easily if everybody\ngets to the checkout with everything they want and with an appropriate method of\npayment ready.\n The same applies to threads: if multiple threads are waiting for the same resource\n(the cashier at the checkout), then if any thread holds the lock for longer than neces-\nsary, it will increase the total time spent waiting (don’t wait until you’ve reached the\ncheckout to start looking for the cranberry sauce). Where possible, lock a mutex only\nwhile accessing the shared data; try to do any processing of the data outside the lock.\nIn particular, don’t do any time-consuming activities like file I/O while holding a lock.\nFile I/O is typically hundreds (if not thousands) of times slower than reading or writ-\ning the same volume of data from memory. Unless the lock is intended to protect\naccess to the file, performing I/O while holding the lock will delay other threads\nunnecessarily (because they’ll block while waiting to acquire the lock), potentially\neliminating any performance gain from the use of multiple threads.\n std::unique_lock works well in this situation, because you can call unlock()\nwhen the code no longer needs access to the shared data and then call lock() again if\naccess is required later in the code:\n\n\n63\nProtecting shared data with mutexes\nvoid get_and_process_data()\n{\n    std::unique_lock<std::mutex> my_lock(the_mutex);\n    some_class data_to_process=get_next_data_chunk();\n    my_lock.unlock();                                 \n    result_type result=process(data_to_process);\n    my_lock.lock();                              \n    write_result(data_to_process,result);\n}\nYou don’t need the mutex locked across the call to process(), so you manually\nunlock it before the call B and then lock it again afterward c.\n Hopefully it’s obvious that if you have one mutex protecting an entire data struc-\nture, not only is there likely to be more contention for the lock, but also the potential\nfor reducing the time that the lock is held is diminished. More of the operation steps\nwill require a lock on the same mutex, so the lock must be held longer. This double\nwhammy of a cost is also a double incentive to move toward finer-grained locking\nwherever possible.\n As this example shows, locking at an appropriate granularity isn’t only about the\namount of data locked; it’s also about how long the lock is held and what operations\nare performed while the lock is held. In general, a lock should be held for only the mini-\nmum possible time needed to perform the required operations. This also means that time-\nconsuming operations such as acquiring another lock (even if you know it won’t\ndeadlock) or waiting for I/O to complete shouldn’t be done while holding a lock\nunless absolutely necessary.\n In listings 3.6 and 3.9, the operation that required locking the two mutexes was a\nswap operation, which obviously requires concurrent access to both objects. Suppose\ninstead you were trying to compare a simple data member that was a plain int. Would\nthis make a difference? ints are cheap to copy, so you could easily copy the data for\neach object being compared while only holding the lock for that object and then com-\npare the copied values. This would mean that you were holding the lock on each\nmutex for the minimum amount of time and also that you weren’t holding one lock\nwhile locking another. The following listing shows a class Y for which this is the case\nand a sample implementation of the equality comparison operator.\nclass Y\n{\nprivate:\n    int some_detail;\n    mutable std::mutex m;\n    int get_detail() const\n    {\n        std::lock_guard<std::mutex> lock_a(m);    \n        return some_detail;\n    }\nListing 3.10\nLocking one mutex at a time in a comparison operator\nDon’t need mutex \nlocked across call \nto process()\nB\nRelock mutex \nto write result\nc\nb\n\n\n64\nCHAPTER 3\nSharing data between threads\npublic:\n    Y(int sd):some_detail(sd){}\n    friend bool operator==(Y const& lhs, Y const& rhs)\n    {\n        if(&lhs==&rhs)\n            return true;\n        int const lhs_value=lhs.get_detail();    \n        int const rhs_value=rhs.get_detail();   \n        return lhs_value==rhs_value;         \n    }\n};\nIn this case, the comparison operator first retrieves the values to be compared by call-\ning the get_detail() member function, c and d. This function retrieves the value\nwhile protecting it with a lock B. The comparison operator then compares the\nretrieved values e. Note, however, that as well as reducing the locking periods so that\nonly one lock is held at a time (and eliminating the possibility of deadlock), this has\nsubtly changed the semantics of the operation compared to holding both locks together. In\nlisting 3.10, if the operator returns true, it means that the value of lhs.some_detail\nat one point in time is equal to the value of rhs.some_detail at another point in\ntime. The two values could have been changed in any way in between the two reads;\nthe values could have been swapped in between c and d, for example, rendering\nthe comparison meaningless. The equality comparison might return true to indicate\nthat the values were equal, even though there was never an instant in time when the\nvalues were equal. It’s therefore important to be careful when making these changes\nthat the semantics of the operation are not changed in a problematic fashion: if you\ndon’t hold the required locks for the entire duration of an operation, you’re exposing yourself to\nrace conditions.\n Sometimes, there isn’t an appropriate level of granularity because not all accesses\nto the data structure require the same level of protection. In this case, it might be\nappropriate to use an alternative mechanism, instead of a plain std::mutex.\n3.3\nAlternative facilities for protecting shared data\nAlthough they’re the most general mechanism, mutexes aren’t the only game in town\nwhen it comes to protecting shared data; there are alternatives that provide more\nappropriate protection in specific scenarios.\n One particularly extreme (but remarkably common) case is where the shared data\nneeds protection only from concurrent access while it’s being initialized, but after that\nno explicit synchronization is required. This might be because the data is read-only\nonce created, and so there are no possible synchronization issues, or it might be\nbecause the necessary protection is performed implicitly as part of the operations on\nthe data. In either case, locking a mutex after the data has been initialized, purely in\norder to protect the initialization, is unnecessary and a needless hit to performance.\nIt’s for this reason that the C++ Standard provides a mechanism purely for protecting\nshared data during initialization.\nc\nd\ne\n",
      "page_number": 79
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 88-95)",
      "start_page": 88,
      "end_page": 95,
      "detection_method": "topic_boundary",
      "content": "65\nAlternative facilities for protecting shared data\n3.3.1\nProtecting shared data during initialization\nSuppose you have a shared resource that’s so expensive to construct that you want to\ndo so only if it’s required; maybe it opens a database connection or allocates a lot of\nmemory. Lazy initialization such as this is common in single-threaded code—each\noperation that requires the resource first checks to see if it has been initialized and\nthen initializes it before use if not:\nstd::shared_ptr<some_resource> resource_ptr;\nvoid foo()\n{\n    if(!resource_ptr)\n    {\n        resource_ptr.reset(new some_resource);    \n    }\n    resource_ptr->do_something();\n}\nIf the shared resource itself is safe for concurrent access, the only part that needs pro-\ntecting when converting this to multithreaded code is the initialization B, but a naïve\ntranslation such as that in the following listing can cause unnecessary serialization of\nthreads using the resource. This is because each thread must wait on the mutex in\norder to check whether the resource has already been initialized.\nstd::shared_ptr<some_resource> resource_ptr;\nstd::mutex resource_mutex;\nvoid foo()\n{\n    std::unique_lock<std::mutex> lk(resource_mutex);   \n    if(!resource_ptr)\n    {\n        resource_ptr.reset(new some_resource);  \n    }\n    lk.unlock();\n    resource_ptr->do_something();\n}\nThis code is common enough, and the unnecessary serialization problematic enough,\nthat many people have tried to come up with a better way of doing this, including the\ninfamous double-checked locking pattern: the pointer is first read without acquiring the\nlock (B in the following code), and the lock is acquired only if the pointer is NULL.\nThe pointer is then checked again once the lock has been acquired (c, hence the\ndouble-checked part) in case another thread has done the initialization between the first\ncheck and this thread acquiring the lock:\nvoid undefined_behaviour_with_double_checked_locking()\n{\n    if(!resource_ptr)     \nListing 3.11\nThread-safe lazy initialization using a mutex\nb\nAll threads are \nserialized here\nOnly the \ninitialization \nneeds protection\nB\n\n\n66\nCHAPTER 3\nSharing data between threads\n    {\n        std::lock_guard<std::mutex> lk(resource_mutex);\n        if(!resource_ptr)                             \n        {\n            resource_ptr.reset(new some_resource);   \n        }\n    }\n    resource_ptr->do_something();   \n}\nUnfortunately, this pattern is infamous for a reason: it has the potential for nasty race\nconditions, because the read outside the lock B, isn’t synchronized with the write\ndone by another thread inside the lock d. This creates a race condition that covers\nnot only the pointer itself but also the object pointed to; even if a thread sees the\npointer written by another thread, it might not see the newly created instance of\nsome_resource, resulting in the call to do_something() e operating on incorrect val-\nues. This is an example of the type of race condition defined as a data race by the C++\nStandard and specified as undefined behavior. It’s therefore quite definitely something\nto avoid. See chapter 5 for a detailed discussion of the memory model, including what\nconstitutes a data race.\n The C++ Standards Committee also saw that this was an important scenario, and\nso the C++ Standard Library provides std::once_flag and std::call_once to han-\ndle this situation. Rather than locking a mutex and explicitly checking the pointer,\nevery thread can use std::call_once, safe in the knowledge that the pointer will\nhave been initialized by some thread (in a properly synchronized fashion) by the\ntime std::call_once returns. The necessary synchronization data is stored in the\nstd::once_flag instance; each instance of std::once_flag corresponds to a different\ninitialization. Use of std::call_once will typically have a lower overhead than using a\nmutex explicitly, especially when the initialization has already been done, so it should be\nused in preference where it matches the required functionality. The following example\nshows the same operation as listing 3.11, rewritten to use std::call_once. In this case,\nthe initialization is done by calling a function, but it could easily have been done with\nan instance of a class with a function call operator. Like most of the functions in the\nstandard library that take functions or predicates as arguments, std::call_once\nworks with any function or callable object:\nstd::shared_ptr<some_resource> resource_ptr;\nstd::once_flag resource_flag;        \nvoid init_resource()\n{\n    resource_ptr.reset(new some_resource);    \n}\nvoid foo()\n{\n    std::call_once(resource_flag,init_resource);   \n    resource_ptr->do_something();\n}\nc\nd\ne\nb\nInitialization is \ncalled exactly \nonce.\n\n\n67\nAlternative facilities for protecting shared data\nIn this example, both the std::once_flag B and data being initialized are namespace-\nscope objects, but std::call_once() can easily be used for lazy initialization of class\nmembers, as in the following listing.\nclass X\n{\nprivate:\n    connection_info connection_details;\n    connection_handle connection;\n    std::once_flag connection_init_flag;\n    void open_connection()\n    {\n        connection=connection_manager.open(connection_details);\n    }\npublic:\n    X(connection_info const& connection_details_):\n        connection_details(connection_details_)\n    {}\n    void send_data(data_packet const& data)    \n    {\n        std::call_once(connection_init_flag,&X::open_connection,this);  \n        connection.send_data(data);\n    }\n    data_packet receive_data()   \n    {\n        std::call_once(connection_init_flag,&X::open_connection,this);  \n        return connection.receive_data();\n    }\n};\nIn that example, the initialization is done either by the first call to send_data() B,\nor by the first call to receive_data() d. The use of the open_connection() member\nfunction to initialize the data also requires that the this pointer be passed in. Just as\nfor other functions in the Standard Library that accept callable objects, such as the\nconstructors for std::thread and std::bind(), this is done by passing an additional\nargument to std::call_once() c.\n It’s worth noting that like std::mutex, std::once_flag instances can’t be copied\nor moved, so if you use them as a class member like this, you’ll have to explicitly\ndefine these special member functions should you require them.\n One scenario where there’s a potential race condition over initialization is that of a\nlocal variable declared with static. The initialization of such a variable is defined to\noccur the first time control passes through its declaration; for multiple threads calling\nthe function, this means there’s the potential for a race condition to define first. On\nmany pre-C++11 compilers this race condition is problematic in practice, because\nmultiple threads may believe they’re first and try to initialize the variable, or threads\nmay try to use it after initialization has started on another thread but before it’s fin-\nished. In C++11 this problem is solved: the initialization is defined to happen on\nListing 3.12\nThread-safe lazy initialization of a class member using std::call_once\nB\nc\nd\n\n\n68\nCHAPTER 3\nSharing data between threads\nexactly one thread, and no other threads will proceed until that initialization is com-\nplete, so the race condition is over which thread gets to do the initialization rather\nthan anything more problematic. This can be used as an alternative to std::call_\nonce for those cases where a single global instance is required:\nclass my_class;\nmy_class& get_my_class_instance()\n{\n    static my_class instance;     \n    return instance;\n}\nMultiple threads can then call get_my_class_instance() safely B, without having to\nworry about race conditions on the initialization.\n Protecting data only for initialization is a special case of a more general scenario:\nthat of a rarely updated data structure. For most of the time, this data structure is\nread-only and can therefore be read by multiple threads concurrently, but on occa-\nsion the data structure may need updating. What’s needed here is a protection mech-\nanism that acknowledges this fact.\n3.3.2\nProtecting rarely updated data structures\nConsider a table used to store a cache of DNS entries for resolving domain names to\ntheir corresponding IP addresses. Typically, a given DNS entry will remain unchanged\nfor a long period of time—in many cases, DNS entries remain unchanged for years.\nAlthough new entries may be added to the table from time to time as users access dif-\nferent websites, this data will therefore remain largely unchanged throughout its life.\nIt’s important that the validity of the cached entries is checked periodically, but this\nstill requires an update only if the details have changed.\n Although updates are rare, they can still happen, and if this cache is to be accessed\nfrom multiple threads, it will need to be appropriately protected during updates to\nensure that none of the threads reading the cache see a broken data structure.\n In the absence of a special-purpose data structure that exactly fits the desired\nusage and that’s specially designed for concurrent updates and reads (such as those in\nchapters 6 and 7), this update requires that the thread doing the update has exclusive\naccess to the data structure until it has completed the operation. Once the change is\ncomplete, the data structure is again safe for multiple threads to access concurrently.\nUsing std::mutex to protect the data structure is therefore overly pessimistic, because\nit will eliminate the possible concurrency in reading the data structure when it isn’t\nundergoing modification; what’s needed is a different kind of mutex. This new kind\nof mutex is typically called a reader-writer mutex, because it allows for two different\nkinds of usage: exclusive access by a single “writer” thread or shared, and concurrent\naccess by multiple “reader” threads.\n The C++17 Standard Library provides two such mutexes out of the box, std::\nshared_mutex and std::shared_timed_mutex. C++14 only features std::shared_\ntimed_mutex, and C++11 didn’t provide either. If you’re struck with a pre-C++14\nInitialization guaranteed \nto be thread-safe\nB\n\n\n69\nAlternative facilities for protecting shared data\ncompiler, then you could use the implementation provided by the Boost library, which\nis based on the original proposal. The difference between std::shared_mutex and\nstd::shared_timed_mutex is that std::shared_timed_mutex supports additional\noperations (as described in section 4.3), so std::shared_mutex might offer a perfor-\nmance benefit on some platforms, if you don’t need the additional operations.\n As you’ll see in chapter 8, the use of such a mutex isn’t a panacea, and the perfor-\nmance is dependent on the number of processors involved and the relative workloads\nof the reader and updater threads. It’s therefore important to profile the perfor-\nmance of the code on the target system to ensure that there’s a benefit to the addi-\ntional complexity.\n Rather than using an instance of std::mutex for the synchronization, you use an\ninstance of std::shared_mutex. For the update operations, std::lock_guard\n<std::shared_mutex> and std::unique_lock<std::shared_mutex> can be used for\nthe locking, in place of the corresponding std::mutex specializations. These ensure\nexclusive access, as with std::mutex. Those threads that don’t need to update the\ndata structure can instead use std::shared_lock<std::shared_mutex> to obtain\nshared access. This RAII class template was added in C++14, and is used the same as\nstd::unique_lock, except that multiple threads may have a shared lock on the same\nstd::shared_mutex at the same time. The only constraint is that if any thread has a\nshared lock, a thread that tries to acquire an exclusive lock will block until all other\nthreads have relinquished their locks, and likewise if any thread has an exclusive lock,\nno other thread may acquire a shared or exclusive lock until the first thread has relin-\nquished its lock.\n The following listing shows a simple DNS cache like the one described, using\nstd::map to hold the cached data, protected using std::shared_mutex.\n#include <map>\n#include <string>\n#include <mutex>\n#include <shared_mutex>\nclass dns_entry;\nclass dns_cache\n{\n    std::map<std::string,dns_entry> entries;\n    mutable std::shared_mutex entry_mutex;\npublic:\n    dns_entry find_entry(std::string const& domain) const\n    {\n        std::shared_lock<std::shared_mutex> lk(entry_mutex);      \n        std::map<std::string,dns_entry>::const_iterator const it=\n            entries.find(domain);\n        return (it==entries.end())?dns_entry():it->second;\n    }\n    void update_or_add_entry(std::string const& domain,\n                             dns_entry const& dns_details)\nListing 3.13\nProtecting a data structure with std::shared_mutex\nb\n\n\n70\nCHAPTER 3\nSharing data between threads\n    {\n        std::lock_guard<std::shared_mutex> lk(entry_mutex);  \n        entries[domain]=dns_details;\n    }\n};\nIn listing 3.13, find_entry() uses an instance of std::shared_lock<> to protect it for\nshared, read-only access B; multiple threads can therefore call find_entry() simulta-\nneously without problems. On the other hand, update_or_add_entry() uses an\ninstance of std::lock_guard<> to provide exclusive access while the table is updated\nc; not only are other threads prevented from doing updates in a call to update_\nor_add_entry(), but threads that call find_entry() are blocked too.\n3.3.3\nRecursive locking\nWith std::mutex, it’s an error for a thread to try to lock a mutex it already owns, and\nattempting to do so will result in undefined behavior. But in some circumstances it\nwould be desirable for a thread to reacquire the same mutex several times without\nhaving first released it. For this purpose, the C++ Standard Library provides\nstd::recursive_mutex. It works like std::mutex, except that you can acquire multi-\nple locks on a single instance from the same thread. You must release all your locks\nbefore the mutex can be locked by another thread, so if you call lock() three times,\nyou must also call unlock() three times. The correct use of std::lock_guard\n<std::recursive_mutex> and std::unique_lock<std::recursive_mutex> will han-\ndle this for you.\n Most of the time, if you think you want a recursive mutex, you probably need to\nchange your design instead. A common use of recursive mutexes is where a class is\ndesigned to be accessible from multiple threads concurrently, so it has a mutex pro-\ntecting the member data. Each public member function locks the mutex, does the\nwork, and then unlocks the mutex. But sometimes it’s desirable for one public mem-\nber function to call another as part of its operation. In this case, the second member\nfunction will also try to lock the mutex, leading to undefined behavior. The quick-and-\ndirty solution is to change the mutex to a recursive mutex. This will allow the mutex\nlock in the second member function to succeed and the function to proceed.\n But such usage is not recommended because it can lead to sloppy thinking and\nbad design. In particular, the class invariants are typically broken while the lock is\nheld, which means that the second member function needs to work even when\ncalled with the invariants broken. It’s usually better to extract a new private member\nfunction that’s called from both member functions, which does not lock the mutex\n(it expects it to already be locked). You can then think carefully about the circum-\nstances under which that new function can be called and the state of the data under\nthose circumstances.\nc\n\n\n71\nSummary\nSummary\nIn this chapter I discussed how problematic race conditions can be disastrous when\nsharing data between threads and how to use std::mutex and careful interface design\nto avoid them. You saw that mutexes aren’t a panacea and do have their own problems\nin the form of deadlock, though the C++ Standard Library provides a tool to help\navoid that in the form of std::lock(). You then looked at some further techniques\nfor avoiding deadlock, followed by a brief look at transferring lock ownership and\nissues surrounding choosing the appropriate granularity for locking. Finally, I covered\nthe alternative data-protection facilities provided for specific scenarios, such as std::\ncall_once() and std::shared_mutex.\n One thing that I haven’t covered yet, however, is waiting for input from other\nthreads. Your thread-safe stack throws an exception if the stack is empty, so if one\nthread wanted to wait for another thread to push a value on the stack (which is, after\nall, one of the primary uses for a thread-safe stack), it would have to repeatedly try to\npop a value, retrying if an exception gets thrown. This consumes valuable processing\ntime in performing the check, without making any progress; indeed, the constant\nchecking might hamper progress by preventing the other threads in the system from\nrunning. What’s needed is some way for a thread to wait for another thread to com-\nplete a task without consuming CPU time in the process. Chapter 4 builds on the facil-\nities I’ve discussed for protecting shared data and introduces the various mechanisms\nfor synchronizing operations between threads in C++; chapter 6 shows how these can\nbe used to build larger reusable data structures.\n\n\n72\nSynchronizing\nconcurrent operations\nIn the last chapter, we looked at various ways of protecting data that’s shared between\nthreads. But sometimes you don’t just need to protect the data, you also need to syn-\nchronize actions on separate threads. One thread might need to wait for another\nthread to complete a task before the first thread can complete its own, for example.\nIn general, it’s common to want a thread to wait for a specific event to happen or a\ncondition to be true. Although it would be possible to do this by periodically check-\ning a “task complete” flag or something similar stored in shared data, this is far from\nideal. The need to synchronize operations between threads like this is such a com-\nmon scenario that the C++ Standard Library provides facilities to handle it, in the\nform of condition variables and futures. These facilities are extended in the Concur-\nrency Technical Specification (TS), which provides additional operations for futures,\nalongside new synchronization facilities in the form of latches and barriers.\nThis chapter covers\nWaiting for an event\nWaiting for one-off events with futures\nWaiting with a time limit\nUsing the synchronization of operations to \nsimplify code\n",
      "page_number": 88
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 96-103)",
      "start_page": 96,
      "end_page": 103,
      "detection_method": "topic_boundary",
      "content": "73\nWaiting for an event or other condition\n In this chapter, I’ll discuss how to wait for events with condition variables, futures,\nlatches, and barriers, and how to use them to simplify the synchronization of opera-\ntions.\n4.1\nWaiting for an event or other condition\nSuppose you’re traveling on an overnight train. One way to ensure you get off at the\nright station would be to stay awake all night and pay attention to where the train\nstops. You wouldn’t miss your station, but you’d be tired when you got there. Alterna-\ntively, you could look at the timetable to see when the train is supposed to arrive, set\nyour alarm a bit before, and go to sleep. That would be OK; you wouldn’t miss your\nstop, but if the train got delayed, you’d wake up too early. There’s also the possibility\nthat your alarm clock’s batteries would die, and you’d sleep too long and miss your sta-\ntion. What would be ideal is if you could go to sleep and have somebody or something\nwake you up when the train gets to your station, whenever that is.\n How does that relate to threads? Well, if one thread is waiting for a second thread\nto complete a task, it has several options. First, it could keep checking a flag in shared\ndata (protected by a mutex) and have the second thread set the flag when it com-\npletes the task. This is wasteful on two counts: the thread consumes valuable process-\ning time repeatedly checking the flag, and when the mutex is locked by the waiting\nthread, it can’t be locked by any other thread. Both of these work against the thread\ndoing the waiting: if the waiting thread is running, this limits the execution resources\navailable to run the thread being waited for, and while the waiting thread has locked\nthe mutex protecting the flag in order to check it, the thread being waited for is\nunable to lock the mutex to set the flag when it’s done. This is akin to staying awake all\nnight talking to the train driver: he has to drive the train more slowly because you\nkeep distracting him, so it takes longer to get there. Similarly, the waiting thread is\nconsuming resources that could be used by other threads in the system and may end\nup waiting longer than necessary.\n A second option is to have the waiting thread sleep for short periods between the\nchecks using the std::this_thread::sleep_for() function (see section 4.3):\nbool flag;\nstd::mutex m;\nvoid wait_for_flag()\n{\n    std::unique_lock<std::mutex> lk(m);\n    while(!flag)                                   \n    {\n        lk.unlock();                       \n        std::this_thread::sleep_for(std::chrono::milliseconds(100));   \n        lk.lock();       \n    }\n}\nIn the loop, the function unlocks the mutex B before the sleep c, and locks it again\nafterward d so another thread gets a chance to acquire it and set the flag.\nUnlock the \nmutex.\nB\nc\nSleep for\n100 ms.\nRelock the mutex.\nd\n\n\n74\nCHAPTER 4\nSynchronizing concurrent operations\n This is an improvement because the thread doesn’t waste processing time while it’s\nsleeping, but it’s hard to get the sleep period right. Too short a sleep in between\nchecks and the thread still wastes processing time checking; too long a sleep and the\nthread will keep on sleeping even when the task it’s waiting for is complete, introduc-\ning a delay. It’s rare that this oversleeping will have a direct impact on the operation of\nthe program, but it could mean dropped frames in a fast-paced game or overrunning\na time slice in a real-time application.\n The third and preferred option is to use the facilities from the C++ Standard\nLibrary to wait for the event itself. The most basic mechanism for waiting for an event\nto be triggered by another thread (such as the presence of additional work in the\npipeline mentioned previously) is the condition variable. Conceptually, a condition vari-\nable is associated with an event or other condition, and one or more threads can wait\nfor that condition to be satisfied. When a thread has determined that the condition is\nsatisfied, it can then notify one or more of the threads waiting on the condition vari-\nable in order to wake them up and allow them to continue processing.\n4.1.1\nWaiting for a condition with condition variables\nThe Standard C++ Library provides not one but two implementations of a condition\nvariable: std::condition_variable and std::condition_variable_any. Both of\nthese are declared in the <condition_variable> library header. In both cases, they\nneed to work with a mutex in order to provide appropriate synchronization; the for-\nmer is limited to working with std::mutex, whereas the latter can work with anything\nthat meets the minimal criteria for being mutex-like, hence the _any suffix. Because\nstd::condition_variable_any is more general, there’s the potential for additional\ncosts in terms of size, performance, or OS resources, so std::condition_variable\nshould be preferred unless the additional flexibility is required.\n So, how do you use std::condition_variable to handle the example in the\nintroduction? How do you let the thread that’s waiting for work sleep until there’s\ndata to process? The following listing shows one way you could do this with a condi-\ntion variable.\nstd::mutex mut;\nstd::queue<data_chunk> data_queue;      \nstd::condition_variable data_cond;\nvoid data_preparation_thread()\n{\n    while(more_data_to_prepare())\n    {\n        data_chunk const data=prepare_data();\n        {\n            std::lock_guard<std::mutex> lk(mut);\n            data_queue.push(data);              \n        }\n        data_cond.notify_one();     \nListing 4.1\nWaiting for data to process with std::condition_variable\nb\nc\nd\n\n\n75\nWaiting for an event or other condition\n    }\n}\nvoid data_processing_thread()\n{\n    while(true)\n    {\n        std::unique_lock<std::mutex> lk(mut);    \n        data_cond.wait(\n            lk,[]{return !data_queue.empty();});     \n        data_chunk data=data_queue.front();\n        data_queue.pop();\n        lk.unlock();          \n        process(data);\n        if(is_last_chunk(data))\n            break;\n    }\n}\nFirst off, you have a queue B that’s used to pass the data between the two threads.\nWhen the data is ready, the thread preparing the data locks the mutex protecting the\nqueue using a std::lock_guard and pushes the data onto the queue c. It then calls\nthe notify_one() member function on the std::condition_variable instance to\nnotify the waiting thread (if there is one) d. Note that you put the code to push the\ndata onto the queue in a smaller scope, so you notify the condition variable after\nunlocking the mutex — this is so that, if the waiting thread wakes immediately, it\ndoesn’t then have to block again, waiting for you to unlock the mutex.\n On the other side of the fence, you have the processing thread. This thread first\nlocks the mutex, but this time with a std::unique_lock rather than a std::lock_\nguard e—you’ll see why in a minute. The thread then calls wait() on the std::\ncondition_variable, passing in the lock object and a lambda function that expresses\nthe condition being waited for f. Lambda functions are a new feature in C++11 that\nallow you to write an anonymous function as part of another expression, and they’re\nideally suited for specifying predicates for standard library functions such as wait().\nIn this case, the simple []{return !data_queue.empty();} lambda function checks to\nsee if the data_queue is not empty()—that is, there’s some data in the queue ready for\nprocessing. Lambda functions are described in more detail in appendix A, section A.5.\n The implementation of wait() then checks the condition (by calling the supplied\nlambda function) and returns if it’s satisfied (the lambda function returned true). If\nthe condition isn’t satisfied (the lambda function returned false), wait() unlocks\nthe mutex and puts the thread in a blocked or waiting state. When the condition vari-\nable is notified by a call to notify_one() from the data-preparation thread, the thread\nwakes from its slumber (unblocks it), reacquires the lock on the mutex, and checks\nthe condition again, returning from wait() with the mutex still locked if the condi-\ntion has been satisfied. If the condition hasn’t been satisfied, the thread unlocks the\nmutex and resumes waiting. This is why you need the std::unique_lock rather than\nthe std::lock_guard—the waiting thread must unlock the mutex while it’s waiting\ne\nf\ng\n\n\n76\nCHAPTER 4\nSynchronizing concurrent operations\nand lock it again afterward, and std::lock_guard doesn’t provide that flexibility. If\nthe mutex remained locked while the thread was sleeping, the data-preparation\nthread wouldn’t be able to lock the mutex to add an item to the queue, and the wait-\ning thread would never be able to see its condition satisfied.\n Listing 4.1 uses a simple lambda function for the wait f, which checks to see if\nthe queue is not empty, but any function or callable object could be passed. If you\nalready have a function to check the condition (perhaps because it’s more compli-\ncated than a simple test like this), then this function can be passed in directly; there’s\nno need to wrap it in a lambda. During a call to wait(), a condition variable may\ncheck the supplied condition any number of times; but it always does so with the\nmutex locked and will return immediately if (and only if) the function provided to\ntest the condition returns true. When the waiting thread reacquires the mutex and\nchecks the condition, if it isn’t in direct response to a notification from another\nthread, it’s called a spurious wake. Because the number and frequency of any such spu-\nrious wakes are by definition indeterminate, it isn’t advisable to use a function with\nside effects for the condition check. If you do so, you must be prepared for the side\neffects to occur multiple times.\n Fundamentally, std::condition_variable::wait is an optimization over a busy-wait.\nIndeed, a conforming (though less than ideal) implementation technique is just a\nsimple loop:\ntemplate<typename Predicate>\nvoid minimal_wait(std::unique_lock<std::mutex>& lk,Predicate pred){\n    while(!pred()){\n        lk.unlock();\n        lk.lock();\n    }\n}\nYour code must be prepared to work with such a minimal implementation of wait(),\nas well as an implementation that only wakes up if notify_one() or notify_all() is\ncalled.\n The flexibility to unlock a std::unique_lock isn’t just used for the call to wait();\nit’s also used once you have the data to process but before processing it g. Processing\ndata can potentially be a time-consuming operation, and as you saw in chapter 3, it’s a\nbad idea to hold a lock on a mutex for longer than necessary. \n Using a queue to transfer data between threads, as in listing 4.1, is a common sce-\nnario. Done well, the synchronization can be limited to the queue itself, which greatly\nreduces the possible number of synchronization problems and race conditions. In\nview of this, let’s now work on extracting a generic thread-safe queue from listing 4.1.\n4.1.2\nBuilding a thread-safe queue with condition variables\nIf you’re going to be designing a generic queue, it’s worth spending a few minutes\nthinking about the operations that are likely to be required, as you did with the\n\n\n77\nWaiting for an event or other condition\nthread-safe stack back in section 3.2.3. Let’s look at the C++ Standard Library for\ninspiration, in the form of the std::queue<> container adaptor shown in the follow-\ning listing.\ntemplate <class T, class Container = std::deque<T> >\nclass queue {\npublic:\n    explicit queue(const Container&);\n    explicit queue(Container&& = Container());\n    template <class Alloc> explicit queue(const Alloc&);\n    template <class Alloc> queue(const Container&, const Alloc&);\n    template <class Alloc> queue(Container&&, const Alloc&);\n    template <class Alloc> queue(queue&&, const Alloc&);\n    void swap(queue& q);\n    bool empty() const;\n    size_type size() const;\n    T& front();\n    const T& front() const;\n    T& back();\n    const T& back() const;\n    void push(const T& x);\n    void push(T&& x);\n    void pop();\n    template <class... Args> void emplace(Args&&... args);\n};\nIf you ignore the construction, assignment, and swap operations, you’re left with three\ngroups of operations: those that query the state of the whole queue (empty() and\nsize()), those that query the elements of the queue (front() and back()), and those\nthat modify the queue (push(), pop() and emplace()). This is the same as you had\nback in section 3.2.3 for the stack, and therefore you have the same issues regarding\nrace conditions inherent in the interface. Consequently, you need to combine\nfront() and pop() into a single function call, much as you combined top() and\npop() for the stack. The code from listing 4.1 adds a new nuance, though: when using\na queue to pass data between threads, the receiving thread often needs to wait for the\ndata. Let’s provide two variants on pop(): try_pop(), which tries to pop the value\nfrom the queue but always returns immediately (with an indication of failure) even if\nthere wasn’t a value to retrieve; and wait_and_pop(), which waits until there’s a value\nto retrieve. If you take your lead for the signatures from the stack example, your inter-\nface looks like the following.\n#include <memory>        \ntemplate<typename T>\nclass threadsafe_queue\n{\nListing 4.2\nstd::queue interface\nListing 4.3\nThe interface of your threadsafe_queue\nFor std::shared_ptr\n\n\n78\nCHAPTER 4\nSynchronizing concurrent operations\npublic:\n    threadsafe_queue();\n    threadsafe_queue(const threadsafe_queue&);\n    threadsafe_queue& operator=(\n        const threadsafe_queue&) = delete;    \n    void push(T new_value);\n    bool try_pop(T& value);       \n    std::shared_ptr<T> try_pop();      \n    void wait_and_pop(T& value);\n    std::shared_ptr<T> wait_and_pop();\n    bool empty() const;\n};\nAs you did for the stack, you’ve cut down on the constructors and eliminated assign-\nment in order to simplify the code. You’ve also provided two versions of both\ntry_pop() and wait_for_pop(), as before. The first overload of try_pop() B stores\nthe retrieved value in the referenced variable, so it can use the return value for status;\nit returns true if it retrieved a value and false otherwise (see section A.2). The sec-\nond overload c can’t do this, because it returns the retrieved value directly. But the\nreturned pointer can be set to NULL if there’s no value to retrieve.\n So, how does all this relate to listing 4.1? Well, you can extract the code for push()\nand wait_and_pop() from there, as shown in the next listing.\n#include <queue>\n#include <mutex>\n#include <condition_variable>\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    std::mutex mut;\n    std::queue<T> data_queue;\n    std::condition_variable data_cond;\npublic:\n    void push(T new_value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        data_queue.push(new_value);\n        data_cond.notify_one();\n    }\n    void wait_and_pop(T& value)\n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        value=data_queue.front();\n        data_queue.pop();\n    }\n};\nthreadsafe_queue<data_chunk> data_queue;    \nvoid data_preparation_thread()\nListing 4.4\nExtracting push() and wait_and_pop() from listing 4.1\nDisallow assignment \nfor simplicity.\nb\nc\nb\n\n\n79\nWaiting for an event or other condition\n{\n    while(more_data_to_prepare())\n    {\n        data_chunk const data=prepare_data();\n        data_queue.push(data);        \n    }\n}\nvoid data_processing_thread()\n{\n    while(true)\n    {\n        data_chunk data;\n        data_queue.wait_and_pop(data);    \n        process(data);\n        if(is_last_chunk(data))\n            break;\n    }\n}\nThe mutex and condition variable are now contained within the threadsafe_queue\ninstance, so separate variables are no longer required B, and no external synchroni-\nzation is required for the call to push() c. Also, wait_and_pop() takes care of the\ncondition variable wait d.\n The other overload of wait_and_pop() is now trivial to write, and the remaining\nfunctions can be copied almost verbatim from the stack example in listing 3.5. The\nfinal queue implementation is shown here.\n#include <queue>\n#include <memory>\n#include <mutex>\n#include <condition_variable>\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    mutable std::mutex mut;    \n    std::queue<T> data_queue;\n    std::condition_variable data_cond;\npublic:\n    threadsafe_queue()\n    {}\n    threadsafe_queue(threadsafe_queue const& other)\n    {\n        std::lock_guard<std::mutex> lk(other.mut);\n        data_queue=other.data_queue;\n    }\n    void push(T new_value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        data_queue.push(new_value);\nListing 4.5\nFull class definition of a thread-safe queue using condition variables\nc\nd\nThe mutex must \nbe mutable.\nb\n\n\n80\nCHAPTER 4\nSynchronizing concurrent operations\n        data_cond.notify_one();\n    }\n    void wait_and_pop(T& value)\n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        value=data_queue.front();\n        data_queue.pop();\n    }\n    std::shared_ptr<T> wait_and_pop()\n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        std::shared_ptr<T> res(std::make_shared<T>(data_queue.front()));\n        data_queue.pop();\n        return res;\n    }\n    bool try_pop(T& value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return false;\n        value=data_queue.front();\n        data_queue.pop();\n        return true;\n    }\n    std::shared_ptr<T> try_pop()\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return std::shared_ptr<T>();\n        std::shared_ptr<T> res(std::make_shared<T>(data_queue.front()));\n        data_queue.pop();\n        return res;\n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        return data_queue.empty();\n    }\n};\nEven though empty() is a const member function, and the other parameter to the\ncopy constructor is a const reference, other threads may have non-const references\nto the object, and may be calling mutating member functions, so you still need to lock\nthe mutex. Since locking a mutex is a mutating operation, the mutex object must be\nmarked mutable B so it can be locked in empty() and in the copy constructor.\n Condition variables are also useful where there’s more than one thread waiting for\nthe same event. If the threads are being used to divide the workload, and thus only\none thread should respond to a notification, exactly the same structure as shown in\nlisting 4.1 can be used; just run multiple instances of the data-processing thread.\nWhen new data is ready, the call to notify_one() will trigger one of the threads\n",
      "page_number": 96
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 104-111)",
      "start_page": 104,
      "end_page": 111,
      "detection_method": "topic_boundary",
      "content": "81\nWaiting for one-off events with futures\ncurrently executing wait() to check its condition and return from wait() (because\nyou’ve just added an item to the data_queue). There’s no guarantee of which thread\nwill be notified or even if there’s a thread waiting to be notified; all the processing\nthreads might still be processing data.\n Another possibility is that several threads are waiting for the same event, and all of\nthem need to respond. This can happen where shared data is being initialized, and\nthe processing threads can all use the same data but need to wait for it to be initialized\n(although there are potentially better mechanisms for this, such as std::call_once;\nsee section 3.3.1 in chapter 3 for a discussion of the options), or where the threads\nneed to wait for an update to shared data, such as a periodic reinitialization. In these\ncases, the thread preparing the data can call the notify_all() member function on\nthe condition variable rather than notify_one(). As the name suggests, this causes all\nthe threads currently executing wait() to check the condition they’re waiting for.\n If the waiting thread is going to wait only once, so when the condition is true it will\nnever wait on this condition variable again, a condition variable might not be the best\nchoice of synchronization mechanisms. This is especially true if the condition being\nwaited for is the availability of a particular piece of data. In this scenario, a future might\nbe more appropriate.\n4.2\nWaiting for one-off events with futures\nSuppose you’re going on vacation abroad by plane. Once you get to the airport and\nclear the various check-in procedures, you still have to wait for notification that your\nflight is ready for boarding, possibly for several hours. Yes, you might be able to find\nsome means of passing the time, such as reading a book, surfing the internet, or eat-\ning in an overpriced airport café, but fundamentally you’re just waiting for one thing:\nthe signal that it’s time to get on the plane. Not only that, but a given flight goes only\nonce; the next time you’re going on vacation, you’ll be waiting for a different flight.\n The C++ Standard Library models this sort of one-off event with something called a\nfuture. If a thread needs to wait for a specific one-off event, it somehow obtains a future\nrepresenting that event. The thread can then periodically wait on the future for short\nperiods of time to see if the event has occurred (check the departures board) while per-\nforming some other task (eating in the overpriced café) between checks. Alternatively, it\ncan do another task until it needs the event to have happened before it can proceed and\nthen just wait for the future to become ready. A future may have data associated with it\n(such as which gate your flight is boarding at), or it may not. Once an event has hap-\npened (and the future has become ready), the future can’t be reset.\n There are two sorts of futures in the C++ Standard Library, implemented as two\nclass templates declared in the <future> library header: unique futures (std::future<>)\nand shared futures (std::shared_future<>). These are modeled after std::unique_ptr\nand std::shared_ptr. An instance of std::future is the one and only instance that\nrefers to its associated event, whereas multiple instances of std::shared_future may\nrefer to the same event. In the latter case, all the instances will become ready at the same\n\n\n82\nCHAPTER 4\nSynchronizing concurrent operations\ntime, and they may all access any data associated with the event. This associated data is\nthe reason these are templates; just like std::unique_ptr and std::shared_ptr, the\ntemplate parameter is the type of the associated data. The std:future<void> and\nstd::shared_future<void> template specializations should be used where there’s no\nassociated data. Although futures are used to communicate between threads, the\nfuture objects themselves don’t provide synchronized accesses. If multiple threads\nneed to access a single future object, they must protect access via a mutex or other syn-\nchronization mechanism, as described in chapter 3. But as you’ll see in section 4.2.5,\nmultiple threads may each access their own copy of std::shared_future<> without\nfurther synchronization, even if they all refer to the same asynchronous result.\n The Concurrency TS provides extended versions of these class templates in the\nstd::experimental namespace: std::experimental::future<> and std::experi-\nmental::shared_future<>. These behave identically to their counterparts in the std\nnamespace, but they have additional member functions to provide additional facili-\nties. It is important to note that the name std::experimental does not imply any-\nthing about the quality of the code (I would hope that the implementation will be the\nsame quality as everything else shipped from your library vendor), but highlights that\nthese are non-standard classes and functions, and therefore may not have exactly the\nsame syntax and semantics if and when they are finally adopted into a future C++ Stan-\ndard. To use these facilities, you must include the <experimental/future> header.\n The most basic of one-off events is the result of a calculation that has been run in\nthe background. Back in chapter 2 you saw that std::thread doesn’t provide an easy\nmeans of returning a value from such a task, and I promised that this would be\naddressed in chapter 4 with futures—now it’s time to see how.\n4.2.1\nReturning values from background tasks\nSuppose you have a long-running calculation that you expect will eventually yield a\nuseful result but for which you don’t currently need the value. Maybe you’ve found a\nway to determine the answer to Life, the Universe, and Everything, to pinch an exam-\nple from Douglas Adams.1 You could start a new thread to perform the calculation, but\nthat means you have to take care of transferring the result back, because std::thread\ndoesn’t provide a direct mechanism for doing so. This is where the std::async func-\ntion template (also declared in the <future> header) comes in.\n You use std::async to start an asynchronous task for which you don’t need the\nresult right away. Rather than giving you a std::thread object to wait on, std::async\nreturns a std::future object, which will eventually hold the return value of the func-\ntion. When you need the value, you just call get() on the future, and the thread\nblocks until the future is ready and then returns the value. The following listing shows\na simple example.\n \n1 In The Hitchhiker’s Guide to the Galaxy, the computer Deep Thought is built to determine “the answer to Life,\nthe Universe and Everything.” The answer is 42.\n\n\n83\nWaiting for one-off events with futures\n#include <future>\n#include <iostream>\nint find_the_answer_to_ltuae();\nvoid do_other_stuff();\nint main()\n{\n    std::future<int> the_answer=std::async(find_the_answer_to_ltuae);\n    do_other_stuff();\n    std::cout<<\"The answer is \"<<the_answer.get()<<std::endl;\n}\nstd::async allows you to pass additional arguments to the function by adding extra\narguments to the call, in the same way that std::thread does. If the first argument is\na pointer to a member function, the second argument provides the object on which to\napply the member function (either directly, or via a pointer, or wrapped in std::ref),\nand the remaining arguments are passed as arguments to the member function.\nOtherwise, the second and subsequent arguments are passed as arguments to the\nfunction or callable object specified as the first argument. Just as with std::thread,\nif the arguments are rvalues, the copies are created by moving the originals. This\nallows the use of move-only types as both the function object and the arguments.\nSee the following listing.\n#include <string>\n#include <future>\nstruct X\n{\n    void foo(int,std::string const&);\n    std::string bar(std::string const&);\n};\nX x;\nauto f1=std::async(&X::foo,&x,42,\"hello\");     \nauto f2=std::async(&X::bar,x,\"goodbye\");    \nstruct Y\n{\n    double operator()(double);\n};\nY y;\nauto f3=std::async(Y(),3.141);       \nauto f4=std::async(std::ref(y),2.718);     \nX baz(X&);\nstd::async(baz,std::ref(x));     \nclass move_only\n{\npublic:\n    move_only();\n    move_only(move_only&&)\n    move_only(move_only const&) = delete;\n    move_only& operator=(move_only&&);\nListing 4.6\nUsing std::future to get the return value of an asynchronous task\nListing 4.7\nPassing arguments to a function with std::async\nCalls p->foo(42,\"hello\") \nwhere p is &x\nCalls tmpx.bar(\"goodbye\") \nwhere tmpx is a copy of x\nCalls tmpy(3.141) where tmpy \nis move-constructed from Y()\nCalls y(2.718)\nCalls baz(x)\n\n\n84\nCHAPTER 4\nSynchronizing concurrent operations\n    move_only& operator=(move_only const&) = delete;\n    void operator()();\n};\nauto f5=std::async(move_only());    \nBy default, it’s up to the implementation whether std::async starts a new thread, or\nwhether the task runs synchronously when the future is waited for. In most cases this is\nwhat you want, but you can specify which to use with an additional parameter to\nstd::async before the function to call. This parameter is of the type std::launch,\nand can either be std::launch::deferred to indicate that the function call is to be\ndeferred until either wait() or get() is called on the future, std::launch::async to\nindicate that the function must be run on its own thread, or std::launch::deferred\n| std::launch::async to indicate that the implementation may choose. This last\noption is the default. If the function call is deferred, it may never run. For example:\nauto f6=std::async(std::launch::async,Y(),1.2);     \nauto f7=std::async(std::launch::deferred,baz,std::ref(x));    \nauto f8=std::async(                           \n   std::launch::deferred | std::launch::async,\n   baz,std::ref(x));                          \nauto f9=std::async(baz,std::ref(x));          \nf7.wait();                       \nAs you’ll see later in this chapter and again in chapter 8, using std::async makes it\neasy to divide algorithms into tasks that can be run concurrently. However, it’s not the\nonly way to associate a std::future with a task; you can also do it by wrapping the task\nin an instance of the std::packaged_task<> class template or by writing code to\nexplicitly set the values using the std::promise<> class template. std::packaged_\ntask is a higher-level abstraction than std::promise, so I’ll start with that.\n4.2.2\nAssociating a task with a future\nstd::packaged_task<> ties a future to a function or callable object. When the std::\npackaged_task<> object is invoked, it calls the associated function or callable object\nand makes the future ready, with the return value stored as the associated data. This\ncan be used as a building block for thread pools (see chapter 9) or other task manage-\nment schemes, such as running each task on its own thread, or running them all\nsequentially on a particular background thread. If a large operation can be divided\ninto self-contained sub-tasks, each of these can be wrapped in a std::packaged_\ntask<> instance, and then that instance passed to the task scheduler or thread pool.\nThis abstracts out the details of the tasks; the scheduler just deals with std::packaged\n_task<> instances rather than individual functions.\n The template parameter for the std::packaged_task<> class template is a func-\ntion signature, like void() for a function taking no parameters with no return value,\nor int(std::string&,double*) for a function that takes a non-const reference to a\nstd::string and a pointer to a double and returns an int. When you construct an\nCalls tmp() where tmp is constructed \nfrom std::move(move_only())\nRun in new thread\nRun in wait() \nor get()\nImplementation \nchooses\nInvoke deferred function\n\n\n85\nWaiting for one-off events with futures\ninstance of std::packaged_task, you must pass in a function or callable object that\ncan accept the specified parameters and that returns a type that’s convertible to the\nspecified return type. The types don’t have to match exactly; you can construct a\nstd::packaged_task<double(double)> from a function that takes an int and returns\na float because the types are implicitly convertible.\n The return type of the specified function signature identifies the type of the\nstd::future<> returned from the get_future() member function, whereas the argu-\nment list of the function signature is used to specify the signature of the packaged\ntask’s function call operator. For example, a partial class definition for std::packaged\n_task <std::string(std::vector<char>*,int)> would be as shown in the follow-\ning listing.\ntemplate<>\nclass packaged_task<std::string(std::vector<char>*,int)>\n{\npublic:\n    template<typename Callable>\n    explicit packaged_task(Callable&& f);\n    std::future<std::string> get_future();\n    void operator()(std::vector<char>*,int);\n};\nThe std::packaged_task object is a callable object, and it can be wrapped in a\nstd::function object, passed to a std::thread as the thread function, passed to\nanother function that requires a callable object, or even invoked directly. When the\nstd::packaged_task is invoked as a function object, the arguments supplied to the\nfunction call operator are passed on to the contained function, and the return value is\nstored as the asynchronous result in the std::future obtained from get_future().\nYou can thus wrap a task in a std::packaged_task and retrieve the future before pass-\ning the std::packaged_task object elsewhere to be invoked in due course. When you\nneed the result, you can wait for the future to become ready. The following example\nshows this in action.\nPASSING TASKS BETWEEN THREADS\nMany GUI frameworks require that updates to the GUI be done from specific threads,\nso if another thread needs to update the GUI, it must send a message to the right\nthread in order to do so. std:packaged_task provides one way of doing this without\nrequiring a custom message for each and every GUI-related activity, as shown here.\n#include <deque>\n#include <mutex>\n#include <future>\n#include <thread>\nListing 4.8\nPartial class definition for a specialization of std::packaged_task< >\nListing 4.9\nRunning code on a GUI thread using std::packaged_task\n\n\n86\nCHAPTER 4\nSynchronizing concurrent operations\n#include <utility>\nstd::mutex m;\nstd::deque<std::packaged_task<void()> > tasks;\nbool gui_shutdown_message_received();\nvoid get_and_process_gui_message();\nvoid gui_thread()                   \n{\n    while(!gui_shutdown_message_received())   \n    {\n        get_and_process_gui_message();    \n        std::packaged_task<void()> task;\n        {\n            std::lock_guard<std::mutex> lk(m);\n            if(tasks.empty())                 \n                continue;\n            task=std::move(tasks.front());   \n            tasks.pop_front();\n        }\n        task();    \n    }\n}\nstd::thread gui_bg_thread(gui_thread);\ntemplate<typename Func>\nstd::future<void> post_task_for_gui_thread(Func f)\n{\n    std::packaged_task<void()> task(f);       \n    std::future<void> res=task.get_future();     \n    std::lock_guard<std::mutex> lk(m);\n    tasks.push_back(std::move(task));     \n    return res;                      \n}\nThis code is simple: the GUI thread B loops until a message has been received telling\nthe GUI to shut down c, repeatedly polling for GUI messages to handle d, such as\nuser clicks, and for tasks on the task queue. If there are no tasks on the queue e, it\nloops again; otherwise, it extracts the task from the queue f, releases the lock on the\nqueue, and then runs the task g. The future associated with the task will then be\nmade ready when the task completes.\n Posting a task on the queue is equally simple: a new packaged task is created from\nthe supplied function h, the future is obtained from that task i by calling the get_-\nfuture() member function, and the task is put on the list j before the future is\nreturned to the caller 1). The code that posted the message to the GUI thread can\nthen wait for the future if it needs to know that the task has been completed, or it can\ndiscard the future if it doesn’t need to know.\n This example uses std::packaged_task<void()> for the tasks, which wraps a\nfunction or other callable object that takes no parameters and returns void (if it\nreturns anything else, the return value is discarded). This is the simplest possible task,\nbut as you saw earlier, std::packaged_task can also be used in more complex situa-\ntions—by specifying a different function signature as the template parameter, you can\nb\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n\n\n87\nWaiting for one-off events with futures\nchange the return type (and thus the type of data stored in the future’s associated\nstate) and also the argument types of the function call operator. This example could eas-\nily be extended to allow for tasks that are to be run on the GUI thread to accept argu-\nments and return a value in the std::future rather than just a completion indicator.\n What about those tasks that can’t be expressed as a simple function call or those\ntasks where the result may come from more than one place? These cases are dealt with\nby the third way of creating a future: using std::promise to set the value explicitly.\n4.2.3\nMaking (std::)promises\nWhen you have an application that needs to handle a lot of network connections, it’s\noften tempting to handle each connection on a separate thread, because this can\nmake the network communication easier to think about and easier to program. This\nworks well for low numbers of connections (and thus low numbers of threads). Unfor-\ntunately, as the number of connections rises, this becomes less suitable; the large num-\nbers of threads consequently consume large amounts of OS resources and potentially\ncause a lot of context switching (when the number of threads exceeds the available\nhardware concurrency), impacting performance. In extreme cases, the OS may run\nout of resources for running new threads before its capacity for network connections\nis exhausted. In applications with large numbers of network connections, it’s there-\nfore common to have a small number of threads (possibly only one) handling the con-\nnections, with each thread dealing with multiple connections at once.\n Consider one of these threads handling the connections. Data packets will come in\nfrom the various connections being handled in essentially random order, and like-\nwise, data packets will be queued to be sent in random order. In many cases, other\nparts of the application will be waiting either for data to be successfully sent or for a\nnew batch of data to be successfully received via a specific network connection.\n std::promise<T> provides a means of setting a value (of type T) that can later be\nread through an associated std::future<T> object. A std::promise/std::future\npair would provide one possible mechanism for this facility; the waiting thread could\nblock on the future, while the thread providing the data could use the promise half of\nthe pairing to set the associated value and make the future ready.\n You can obtain the std::future object associated with a given std::promise by\ncalling the get_future() member function, just like with std::packaged_task. When\nthe value of the promise is set (using the set_value() member function), the future\nbecomes ready and can be used to retrieve the stored value. If you destroy the\nstd::promise without setting a value, an exception is stored instead. Section 4.2.4\ndescribes how exceptions are transferred across threads.\n Listing 4.10 shows some example code for a thread that’s processing connections\nas just described. In this example, you use a std::promise<bool>/std::future<bool>\npair to identify the successful transmission of a block of outgoing data; the value asso-\nciated with the future is a simple success/failure flag. For incoming packets, the data\nassociated with the future is the payload of the data packet.\n\n\n88\nCHAPTER 4\nSynchronizing concurrent operations\n#include <future>\nvoid process_connections(connection_set& connections)\n{\n    while(!done(connections))   \n    {\n        for(connection_iterator        \n                connection=connections.begin(),end=connections.end();\n            connection!=end;\n            ++connection)\n        {\n            if(connection->has_incoming_data())    \n            {\n                data_packet data=connection->incoming();\n                std::promise<payload_type>& p=\n                    connection->get_promise(data.id);    \n                p.set_value(data.payload);\n            }\n            if(connection->has_outgoing_data())    \n            {\n                outgoing_packet data=\n                    connection->top_of_outgoing_queue();\n                connection->send(data.payload);\n                data.promise.set_value(true);     \n            }\n        }\n    }\n}\nThe process_connections() function loops until done() returns true B. Every\ntime it goes through the loop, it checks each connection in turn c, retrieving\nincoming data if there is any d or sending any queued outgoing data f. This\nassumes that an incoming packet has an ID and a payload with the data in it. The ID\nis mapped to a std::promise (perhaps by a lookup in an associative container) e,\nand the value is set to the packet’s payload. For outgoing packets, the packet is\nretrieved from the outgoing queue and sent through the connection. Once the send\nhas completed, the promise associated with the outgoing data is set to true to indi-\ncate successful transmission g. Whether this maps nicely to the network protocol\ndepends on the protocol; this promise/future style structure may not work for a par-\nticular scenario, although it does have a similar structure to the asynchronous I/O\nsupport of some OSes.\n All the code up to now has completely disregarded exceptions. Although it might\nbe nice to imagine a world in which everything worked all the time, this isn’t the case.\nSometimes disks fill up, sometimes what you’re looking for just isn’t there, sometimes\nthe network fails, and sometimes the database goes down. If you were performing the\noperation in the thread that needed the result, the code could just report an error\nwith an exception, so it would be unnecessarily restrictive to require that everything\ngoes well just because you wanted to use std::packaged_task or std::promise. The\nListing 4.10\nHandling multiple connections from a single thread using promises\nb\nc\nd\ne\nf\ng\n",
      "page_number": 104
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 112-122)",
      "start_page": 112,
      "end_page": 122,
      "detection_method": "topic_boundary",
      "content": "89\nWaiting for one-off events with futures\nC++ Standard Library therefore provides a clean way to deal with exceptions in such a\nscenario and allows them to be saved as part of the associated result.\n4.2.4\nSaving an exception for the future\nConsider the following short snippet of code. If you pass in -1 to the square_root()\nfunction, it throws an exception, and this gets seen by the caller:\ndouble square_root(double x)\n{\n    if(x<0)\n    {\n        throw std::out_of_range(“x<0”);\n    }\n    return sqrt(x);\n}\nNow suppose that instead of just invoking square_root() from the current thread\ndouble y=square_root(-1);\nyou run the call as an asynchronous call:\nstd::future<double> f=std::async(square_root,-1);\ndouble y=f.get();\nIt would be ideal if the behavior was exactly the same; just as y gets the result of the\nfunction call in either case, it would be great if the thread that called f.get() could\nsee the exception too, just as it would in the single-threaded case.\n Well, that’s exactly what happens: if the function call invoked as part of std::async\nthrows an exception, that exception is stored in the future in place of a stored value, the\nfuture becomes ready, and a call to get() rethrows that stored exception. (Note: the\nstandard leaves it unspecified whether it is the original exception object that’s rethrown\nor a copy; different compilers and libraries make different choices on this matter.) The\nsame happens if you wrap the function in a std::packaged_task—when the task is\ninvoked, if the wrapped function throws an exception, that exception is stored in the\nfuture in place of the result, ready to be thrown on a call to get().\n Naturally, std::promise provides the same facility, with an explicit function call. If\nyou wish to store an exception rather than a value, you call the set_exception()\nmember function rather than set_value(). This would typically be used in a catch\nblock for an exception thrown as part of the algorithm, to populate the promise with\nthat exception:\nextern std::promise<double> some_promise;\ntry\n{\n    some_promise.set_value(calculate_value());\n}\n\n\n90\nCHAPTER 4\nSynchronizing concurrent operations\ncatch(...)\n{\n    some_promise.set_exception(std::current_exception());\n}\nThis uses std::current_exception() to retrieve the thrown exception; the alterna-\ntive here would be to use std::make_exception_ptr() to store a new exception\ndirectly without throwing:\nsome_promise.set_exception(std::make_exception_ptr(std::logic_error(\"foo \")));\nThis is much cleaner than using a try/catch block if the type of the exception is\nknown, and it should be used in preference; not only does it simplify the code, but it\nalso provides the compiler with greater opportunity to optimize the code.\n Another way to store an exception in a future is to destroy the std::promise or\nstd::packaged_task associated with the future without calling either of the set func-\ntions on the promise or invoking the packaged task. In either case, the destructor of\nstd::promise or std::packaged_task will store a std::future_error exception with\nan error code of std::future_errc::broken_promise in the associated state if the\nfuture isn’t already ready; by creating a future you make a promise to provide a value\nor exception, and by destroying the source of that value or exception without provid-\ning one, you break that promise. If the compiler didn’t store anything in the future in\nthis case, waiting threads could potentially wait forever.\n Up until now, all the examples have used std::future. However, std::future has\nits limitations, not the least of which being that only one thread can wait for the result.\nIf you need to wait for the same event from more than one thread, you need to use\nstd::shared_future instead.\n4.2.5\nWaiting from multiple threads\nAlthough std::future handles all the synchronization necessary to transfer data from\none thread to another, calls to the member functions of a particular std::future\ninstance are not synchronized with each other. If you access a single std::future\nobject from multiple threads without additional synchronization, you have a data race\nand undefined behavior. This is by design: std::future models unique ownership of\nthe asynchronous result, and the one-shot nature of get() makes such concurrent\naccess pointless anyway—only one thread can retrieve the value, because after the first\ncall to get() there’s no value left to retrieve.\n If your fabulous design for your concurrent code requires that multiple threads\ncan wait for the same event, don’t despair just yet; std::shared_future allows exactly\nthat. Whereas std::future is only moveable (so ownership can be transferred between\ninstances, but only one instance refers to a particular asynchronous result at a time),\nstd::shared_future instances are copyable (so you can have multiple objects referring\nto the same associated state).\n\n\n91\nWaiting for one-off events with futures\n Now, with std::shared_future, member functions on an individual object are still\nunsynchronized, so to avoid data races when accessing a single object from multiple\nthreads, you must protect accesses with a lock. The preferred way to use it would be to\npass a copy of the shared_future object to each thread, so each thread can access its\nown local shared_future object safely, as the internals are now correctly synchronized\nby the library. Accessing the shared asynchronous state from multiple threads is safe if\neach thread accesses that state through its own std::shared_future object. See fig-\nure 4.1.\n One potential use of std::shared_future is for implementing parallel execution\nof something akin to a complex spreadsheet; each cell has a single final value, which\nmay be used by the formulas in multiple other cells. The formulas for calculating the\nresults of the dependent cells can then use std::shared_future to reference the first\nThread 1\nThread 2\nstd::shared_future<int>\nShared variable sf\nint\nsf.wait()\nsf.wait()\nData race on sf\nwithout synchronization\nShared variable sf\nint\nCopying is safe.\nThread 1\nlocal\nThread 2\nlocal\nRefers to\nRefers to\nSeparate objects,\nso no data race\nstd::shared_future<int>\nstd::shared_future<int>\nstd::shared_future<int>\nauto local=sf;\nauto local=sf;\nlocal.wait()\nlocal.wait()\nRefers to\nasynchronous\nresult\nRefers to\nasynchronous\nresult\nFigure 4.1\nUsing multiple std::shared_future objects to avoid data races\n\n\n92\nCHAPTER 4\nSynchronizing concurrent operations\ncell. If all the formulas for the individual cells are then executed in parallel, those\ntasks that can proceed to completion will do so, whereas those that depend on others\nwill block until their dependencies are ready. This will allow the system to make maxi-\nmum use of the available hardware concurrency.\n Instances of std::shared_future that reference some asynchronous state are con-\nstructed from instances of std::future that reference that state. Since std::future\nobjects don’t share ownership of the asynchronous state with any other object, the\nownership must be transferred into the std::shared_future using std::move, leav-\ning std::future in an empty state, as if it were a default constructor:\nstd::promise<int> p;\nstd::future<int> f(p.get_future());\nassert(f.valid());                        \nstd::shared_future<int> sf(std::move(f));\nassert(!f.valid());                       \nassert(sf.valid());   \nHere, the future f is initially valid B because it refers to the asynchronous state of\nthe promise p, but after transferring the state to sf, f is no longer valid c, whereas\nsf is d.\n Just as with other movable objects, the transfer of ownership is implicit for rval-\nues, so you can construct a std::shared_future directly from the return value of the\nget_future() member function of a std::promise object, for example:\nstd::promise<std::string> p;\nstd::shared_future<std::string> sf(p.get_future());    \nHere, the transfer of ownership is implicit; std::shared_future<> is constructed\nfrom an rvalue of type std::future<std::string> B.\n std::future also has an additional feature to facilitate the use of std::shared_\nfuture, with the new facility for automatically deducing the type of a variable from its\ninitializer (see appendix A, section A.6). std::future has a share() member func-\ntion that creates a new std::shared_future and transfers ownership to it directly.\nThis can save a lot of typing and makes code easier to change:\nstd::promise< std::map< SomeIndexType, SomeDataType, SomeComparator,\n    SomeAllocator>::iterator> p;\nauto sf=p.get_future().share();                                       \nIn this case, the type of sf is deduced to be std::shared_future< std::map< Some-\nIndexType, SomeDataType, SomeComparator, SomeAllocator>::iterator>, which is\nrather a mouthful. If the comparator or allocator is changed, you only need to change\nthe type of the promise; the type of the future is automatically updated to match.\n Sometimes you want to limit the amount of time you’re waiting for an event, either\nbecause you have a hard time limit on how long a particular section of code may take,\nor because there’s other useful work that the thread can be doing if the event isn’t\nThe future \nf is valid.\nb\nf is no \nlonger valid.\nc\nsf is now valid.\nd\nImplicit transfer \nof ownership\nb\n\n\n93\nWaiting with a time limit\ngoing to happen soon. To handle this facility, many of the waiting functions have vari-\nants that allow a timeout to be specified.\n4.3\nWaiting with a time limit\nAll the blocking calls introduced previously will block for an indefinite period of time,\nsuspending the thread until the event being waited for occurs. In many cases this is\nfine, but in some cases you may want to put a limit on how long you wait. This might\nbe to allow you to send some form of “I’m still alive” message either to an interactive\nuser, or another process, or indeed to allow you to abort the wait if the user has given\nup waiting and clicked Cancel.\n There are two sorts of timeouts you may wish to specify: a duration-based timeout,\nwhere you wait for a specific amount of time (for example, 30 milliseconds); or an\nabsolute timeout, where you wait until a specific point in time (for example,\n17:30:15.045987023 UTC on November 30, 2011). Most of the waiting functions pro-\nvide variants that handle both forms of timeouts. The variants that handle the duration-\nbased timeouts have a _for suffix, and those that handle the absolute timeouts have\nan _until suffix.\n So, for example, std::condition_variable has two overloads of the wait_for()\nmember function and two overloads of the wait_until() member function that cor-\nrespond to the two overloads of wait()—one overload that just waits until signaled, or\nthe timeout expires, or a spurious wakeup occurs; and another that will check the sup-\nplied predicate when woken and will return only when the supplied predicate is true\n(and the condition variable has been signaled) or the timeout expires.\n Before we look at the details of the functions that use the timeouts, let’s examine\nthe way that times are specified in C++, starting with clocks.\n4.3.1\nClocks\nAs far as the C++ Standard Library is concerned, a clock is a source of time informa-\ntion. Specifically, a clock is a class that provides four distinct pieces of information:\nThe time now\nThe type of the value used to represent the times obtained from the clock\nThe tick period of the clock\nWhether or not the clock ticks at a uniform rate and is therefore considered to\nbe a steady clock\nThe current time of a clock can be obtained by calling the now() static member function\nfor that clock class; for example, std::chrono::system_clock::now() will return the\ncurrent time of the system clock. The type of the time points for a particular clock is spec-\nified by the time_point member typedef, so the return type of some_clock::now() is\nsome_clock::time_point.\n The tick period of the clock is specified as a fractional number of seconds, which is\ngiven by the period member typedef of the clock—a clock that ticks 25 times per\n\n\n94\nCHAPTER 4\nSynchronizing concurrent operations\nsecond has a period of std::ratio<1,25>, whereas a clock that ticks every 2.5 sec-\nonds has a period of std::ratio<5,2>. If the tick period of a clock can’t be known\nuntil runtime, or it may vary during a given run of the application, the period may\nbe specified as the average tick period, smallest possible tick period, or some other\nvalue that the library writer deems appropriate. There’s no guarantee that the\nobserved tick period in a given run of the program matches the specified period for\nthat clock.\n If a clock ticks at a uniform rate (whether or not that rate matches the period) and\ncan’t be adjusted, the clock is said to be a steady clock. The is_steady static data mem-\nber of the clock class is true if the clock is steady, and false otherwise. Typically,\nstd::chrono::system_clock will not be steady, because the clock can be adjusted,\neven if such adjustment is done automatically to take account of local clock drift. Such\nan adjustment may cause a call to now() to return a value earlier than that returned by\na prior call to now(), which is in violation of the requirement for a uniform tick rate.\nSteady clocks are important for timeout calculations, as you’ll see shortly, so the C++\nStandard Library provides one in the form of std::chrono::steady_clock. The\nother clocks provided by the C++ Standard Library are std::chrono::system_clock\n(mentioned earlier), which represents the “real-time” clock of the system and pro-\nvides functions for converting its time points to and from time_t values, and\nstd::chrono::high_resolution_clock, which provides the smallest possible tick\nperiod (and thus the highest possible resolution) of all the library-supplied clocks. It\nmay be a typedef to one of the other clocks. These clocks are defined in the <chrono>\nlibrary header, along with the other time facilities.\n We’ll look at the representation of time points shortly, but first let’s look at how\ndurations are represented.\n4.3.2\nDurations\nDurations are the simplest part of the time support; they’re handled by the std::\nchrono::duration<> class template (all the C++ time-handling facilities used by the\nThread Library are in the std::chrono namespace). The first template parameter is\nthe type of the representation (such as int, long, or double), and the second is a frac-\ntion specifying how many seconds each unit of the duration represents. For example,\na number of minutes stored in a short is std::chrono::duration<short,std::\nratio<60,1>>, because there are 60 seconds in a minute. On the other hand, a count\nof milliseconds stored in a double is std::chrono::duration<double,std::ratio\n<1,1000>>, because each millisecond is 1/1000th of a second.\n The Standard Library provides a set of predefined typedefs in the std::chrono\nnamespace for various durations: nanoseconds, microseconds, milliseconds, sec-\nonds, minutes, and hours. They all use a sufficiently large integral type for the repre-\nsentation chosen such that you can represent a duration of over 500 years in the\nappropriate units if you so desire. There are also typedefs for all the SI ratios from\nstd::atto (10–18) to std::exa (1018) (and beyond, if your platform has 128-bit\n\n\n95\nWaiting with a time limit\ninteger types) for use when specifying custom durations such as std::duration<d-\nouble,std::centi> for a count of 1/100th of a second represented in a double.\n For convenience, there are a number of predefined literal suffix operators for\ndurations in the std::chrono_literals namespace, introduced with C++14. This can\nsimplify code that uses hard-coded duration values, such as\nusing namespace std::chrono_literals;\nauto one_day=24h;\nauto half_an_hour=30min;\nauto max_time_between_messages=30ms;\nWhen used with integer literals, these suffixes are equivalent to using the predefined\nduration typedefs, so 15ns and std::chrono::nanoseconds(15) are identical values.\nHowever, when used with floating-point literals, these suffixes create a suitably-scaled\nfloating-point duration with an unspecified representation type. Therefore, 2.5min\nwill be std::chrono::duration<some-floating-point-type,std::ratio<60,1>>. If\nyou are concerned about the range or precision of the implementation’s chosen float-\ning point type, then you will need to construct an object with a suitable representation\nyourself, rather than using the convenience of the literal suffixes.\n Conversion between durations is implicit where it does not require truncation of\nthe value (so converting hours to seconds is OK, but converting seconds to hours is\nnot). Explicit conversions can be done with std::chrono::duration_cast<>:\nstd::chrono::milliseconds ms(54802);\nstd::chrono::seconds s=\n    std::chrono::duration_cast<std::chrono::seconds>(ms);\nThe result is truncated rather than rounded, so s will have a value of 54 in this example.\n Durations support arithmetic, so you can add and subtract durations to get new\ndurations or multiply or divide by a constant of the underlying representation type\n(the first template parameter). Thus 5*seconds(1) is the same as seconds(5) or\nminutes(1) – seconds(55). The count of the number of units in the duration can be\nobtained with the count() member function. Thus std::chrono::milliseconds(1234)\n.count() is 1234.\n Duration-based waits are done with instances of std::chrono::duration<>. For\nexample, you can wait for up to 35 milliseconds for a future to be ready:\nstd::future<int> f=std::async(some_task);\nif(f.wait_for(std::chrono::milliseconds(35))==std::future_status::ready)\n    do_something_with(f.get());\nThe wait functions all return a status to indicate whether the wait timed out or the\nwaited-for event occurred. In this case, you’re waiting for a future, so the function\nreturns std::future_status::timeout if the wait times out, std::future_status::\nready if the future is ready, or std::future_status::deferred if the future’s task is\ndeferred. The time for a duration-based wait is measured using a steady clock internal\n\n\n96\nCHAPTER 4\nSynchronizing concurrent operations\nto the library, so 35 milliseconds means 35 milliseconds of elapsed time, even if the\nsystem clock was adjusted (forward or back) during the wait. Of course, the vagaries\nof system scheduling and the varying precisions of OS clocks means that the time\nbetween the thread issuing the call and returning from it may be much longer than\n35 ms.\n With durations under our belt, we can now move on to time points.\n4.3.3\nTime points\nThe time point for a clock is represented by an instance of the std::chrono::time_\npoint<> class template, which specifies which clock it refers to as the first template\nparameter and the units of measurement (a specialization of std::chrono::dura-\ntion<>) as the second template parameter. The value of a time point is the length of\ntime (in multiples of the specified duration) since a specific point in time called the\nepoch of the clock. The epoch of a clock is a basic property but not something that’s\ndirectly available to query or specified by the C++ Standard. Typical epochs include\n00:00 on January 1, 1970 and the instant when the computer running the application\nbooted up. Clocks may share an epoch or have independent epochs. If two clocks\nshare an epoch, the time_point typedef in one class may specify the other as the clock\ntype associated with the time_point. Although you can’t find out when the epoch is,\nyou can get the time_since_epoch() for a given time_point. This member function\nreturns a duration value specifying the length of time since the clock epoch to that\nparticular time point.\n For example, you might specify a time point as std::chrono::time_point<std::\nchrono::system_clock, std::chrono::minutes>. This would hold the time relative\nto the system clock but measured in minutes as opposed to the native precision of the\nsystem clock (which is typically seconds or less).\n You can add durations and subtract durations from instances of std::chrono::\ntime_point<> to produce new time points, so std::chrono::high_resolution_clock::\nnow() + std::chrono::nanoseconds(500) will give you a time 500 nanoseconds in the\nfuture. This is good for calculating an absolute timeout when you know the maximum\nduration of a block of code, but there are multiple calls to waiting functions within\nit or nonwaiting functions that precede a waiting function but take up some of the\ntime budget.\n You can also subtract one time point from another that shares the same clock. The\nresult is a duration specifying the length of time between the two time points. This is\nuseful for timing blocks of code, for example:\nauto start=std::chrono::high_resolution_clock::now();\ndo_something();\nauto stop=std::chrono::high_resolution_clock::now();\nstd::cout<<”do_something() took “\n  <<std::chrono::duration<double,std::chrono::seconds>(stop-start).count()\n  <<” seconds”<<std::endl;\n\n\n97\nWaiting with a time limit\nThe clock parameter of a std::chrono::time_point<> instance does more than just\nspecify the epoch, though. When you pass the time point to a wait function that takes\nan absolute timeout, the clock parameter of the time point is used to measure the\ntime. This has important consequences when the clock is changed, because the wait\ntracks the clock change and won’t return until the clock’s now() function returns a\nvalue later than the specified timeout. If the clock is adjusted forward, this may reduce\nthe total length of the wait (as measured by a steady clock), and if it’s adjusted back-\nward, this may increase the total length of the wait.\n As you may expect, time points are used with the _until variants of the wait func-\ntions. The typical use case is as an offset from some-clock::now() at a fixed point in the\nprogram, although time points associated with the system clock can be obtained by\nconverting from time_t using the std::chrono::system_clock::to_time_point()\nstatic member function to schedule operations at a user-visible time. For example, if\nyou have a maximum of 500 milliseconds to wait for an event associated with a condi-\ntion variable, you might do something like in the following listing.\n#include <condition_variable>\n#include <mutex>\n#include <chrono>\nstd::condition_variable cv;\nbool done;\nstd::mutex m;\nbool wait_loop()\n{\n    auto const timeout= std::chrono::steady_clock::now()+\n        std::chrono::milliseconds(500);\n    std::unique_lock<std::mutex> lk(m);\n    while(!done)\n    {\n        if(cv.wait_until(lk,timeout)==std::cv_status::timeout)\n            break;\n    }\n    return done;\n}\nThis is the recommended way to wait for condition variables with a time limit if you’re\nnot passing a predicate to wait. This way, the overall length of the loop is bounded. As\nyou saw in section 4.1.1, you need to loop when using condition variables if you don’t\npass in the predicate, in order to handle spurious wakeups. If you use wait_for() in a\nloop, you might end up waiting almost the full length of time before a spurious wake-\nup, and the next time through the wait time starts again. This may repeat any number\nof times, making the total wait time unbounded.\n With the basics of specifying timeouts under your belt, let’s look at the functions\nthat you can use timeout with.\nListing 4.11\nWaiting for a condition variable with a timeout\n\n\n98\nCHAPTER 4\nSynchronizing concurrent operations\n4.3.4\nFunctions that accept timeouts\nThe simplest use for a timeout is to add a delay to the processing of a particular\nthread so that it doesn’t take processing time away from other threads when it has\nnothing to do. You saw an example of this in section 4.1, where you polled a “done”\nflag in a loop. The two functions that handle this are std::this_thread::sleep_\nfor() and std::this_thread::sleep_until(). They work like a basic alarm clock:\nthe thread goes to sleep either for the specified duration (with sleep_for()) or until\nthe specified point in time (with sleep_until()). sleep_for() makes sense for exam-\nples like those in section 4.1, where something must be done periodically, and the\nelapsed time is what matters. On the other hand, sleep_until() allows you to sched-\nule the thread to wake at a particular point in time. This could be used to trigger the\nbackups at midnight, or the payroll print run at 6:00 a.m., or to suspend the thread\nuntil the next frame refresh when doing a video playback.\n Sleeping isn’t the only facility that takes a timeout; you already saw that you can\nuse timeouts with condition variables and futures. You can even use timeouts when\ntrying to acquire a lock on a mutex if the mutex supports it. Plain std::mutex and\nstd::recursive_mutex don’t support timeouts on locking, but std::timed_mutex\ndoes, as does std::recursive_timed_mutex. Both these types support try_lock_for()\nand try_lock_until() member functions that try to obtain the lock within a speci-\nfied time period or before a specified time point. Table 4.1 shows the functions from\nthe C++ Standard Library that can accept timeouts, their parameters, and their return\nvalues. Parameters listed as duration must be an instance of std::duration<>, and\nthose listed as time_point must be an instance of std::time_point<>.\nTable 4.1\nClass/Namespace\nFunctions\nReturn Values\nstd::this_thread namespace\nsleep_for(duration)\nsleep_until(time_point)\nN/A\nstd::condition_variable or \nstd::condition_variable_an\nywait_for(lock,duration)\nwait_until(lock,time_\npoint)\nstd::cv_status::timeout or \nstd::cv_status::no_timeout\nwait_for(lock,duration,\npredicate)\nwait_until(lock,time_point,\npredicate)\nbool—the return value \nof the predicate \nwhen woken\nstd::timed_mutex, \nstd::recursive_timed_mutex \nor std::shared_timed_\nmutextry_lock_for(duration)\ntry_lock_until(time_point)\nbool—true if the lock was acquired, \nfalse otherwise\nstd::shared_timed_mutex\ntry_lock_shared_for(duration)\ntry_lock_shared_until(time_\npoint)\nbool—true if the lock \nwas acquired, false \notherwise\n\n\n99\nUsing synchronization of operations to simplify code\nNow that I’ve covered the mechanics of condition variables, futures, promises, and\npackaged tasks, it’s time to look at the wider picture and how they can be used to sim-\nplify the synchronization of operations between threads.\n4.4\nUsing synchronization of operations to simplify code\nUsing the synchronization facilities described so far in this chapter as building blocks\nallows you to focus on the operations that need synchronizing rather than the\nmechanics. One way this can help simplify your code is that it accommodates a much\nmore functional (in the sense of functional programming) approach to programming\nconcurrency. Rather than sharing data directly between threads, each task can be pro-\nvided with the data it needs, and the result can be disseminated to any other threads\nthat need it through the use of futures.\n4.4.1\nFunctional programming with futures\nThe term functional programming (FP) refers to a style of programming where the\nresult of a function call depends solely on the parameters to that function and doesn’t\ndepend on any external state. This is related to the mathematical concept of a func-\ntion, and it means that if you invoke a function twice with the same parameters, the\nresult is exactly the same. This is a property of many of the mathematical functions in\nthe C++ Standard Library, such as sin, cos, and sqrt, and simple operations on basic\nstd::unique_lock<TimedLock\nable>unique_lock(lockable,\nduration)\nunique_lock(lockable,time_\npoint)\nN/A—owns_lock() on the newly-\nconstructed object returns true if the \nlock was acquired, false otherwise\ntry_lock_for(duration)\ntry_lock_until(time_point)\nbool—true if the lock \nwas acquired, false \notherwise\nstd::shared_lock<Shared-\nTimedLockable>shared_lock\n(lockable,duration)\nshared_lock(lockable,time_\npoint)\nN/A—owns_lock() on the newly-\nconstructed object returns true if the \nlock was acquired, false otherwise\ntry_lock_for(duration)\ntry_lock_until(time_point)\nbool—true if the lock \nwas acquired, false \notherwise\nstd::future<ValueType> or \nstd::shared_future<Value-\nType>wait_for(duration)\nwait_until(time_point)\nstd::future_status::timeout \nif the wait timed out, \nstd::future_status::ready if \nthe future is ready, or \nstd::future_status::deferred \nif the future holds a deferred function \nthat hasn’t yet started\nTable 4.1\n (continued)\nClass/Namespace\nFunctions\nReturn Values\n",
      "page_number": 112
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 123-131)",
      "start_page": 123,
      "end_page": 131,
      "detection_method": "topic_boundary",
      "content": "100\nCHAPTER 4\nSynchronizing concurrent operations\ntypes, such as 3+3, 6*9, or 1.3/4.7. A pure function doesn’t modify any external state\neither; the effects of the function are entirely limited to the return value.\n This makes things easy to think about, especially when concurrency is involved,\nbecause many of the problems associated with shared memory discussed in chapter 3\ndisappear. If there are no modifications to shared data, there can be no race conditions\nand thus no need to protect shared data with mutexes either. This is such a powerful\nsimplification that programming languages such as Haskell (http://www.haskell.org/),\nwhere all functions are pure by default, are becoming increasingly popular for pro-\ngramming concurrent systems. Because most things are pure, the impure functions\nthat actually do modify the shared state stand out all the more, and it’s therefore easier\nto reason about how they fit into the overall structure of the application.\n The benefits of FP aren’t limited to those languages where it’s the default para-\ndigm, however. C++ is a multiparadigm language, and it’s entirely possible to write\nprograms in the FP style. This is even easier in C++11 than it was in C++98, with the\nadvent of lambda functions (see appendix A, section A.6), the incorporation of\nstd::bind from Boost and TR1, and the introduction of automatic type deduction for\nvariables (see appendix A, section A.7). Futures are the final piece of the puzzle that\nmakes FP-style concurrency viable in C++; a future can be passed around between\nthreads to allow the result of one computation to depend on the result of another,\nwithout any explicit access to shared data.\nFP-STYLE QUICKSORT\nTo illustrate the use of futures for FP-style concurrency, let’s look at a simple imple-\nmentation of the Quicksort algorithm. The basic idea of the algorithm is simple: given\na list of values, take an element to be the pivot element, and then partition the list into\ntwo sets—those less than the pivot and those greater than or equal to the pivot. A\nsorted copy of the list is obtained by sorting the two sets and returning the sorted list\nof values less than the pivot, followed by the pivot, followed by the sorted list of val-\nues greater than or equal to the pivot. Figure 4.2 shows how a list of 10 integers is\n3\n4\n1\n2\n5\n7\n9\n8\n1\n2\n3\n4\n6\n7\n9\n1\n2\n1\n2\n8\n8\n1\n2\n3\n4\n6\n7\n8\n5\n7\n3\n4\n1\n9\n2\n8\n1\n2\n3\n4\n5\n6\n7\n8\n10\n8\n9\n9\n9\n10\n9\n6\n10\n10\n10\n10\n6\n10\nFigure 4.2\nFP-style recursive sorting\n\n\n101\nUsing synchronization of operations to simplify code\nsorted under this scheme. An FP-style sequential implementation is shown in the fol-\nlowing listing; it takes and returns a list by value rather than sorting in place like\nstd::sort() does.\ntemplate<typename T>\nstd::list<T> sequential_quick_sort(std::list<T> input)\n{\n    if(input.empty())\n    {\n        return input;\n    }\n    std::list<T> result;\n    result.splice(result.begin(),input,input.begin());    \n    T const& pivot=*result.begin();                   \n    \n    auto divide_point=std::partition(input.begin(),input.end(),\n            [&](T const& t){return t<pivot;});               \n    std::list<T> lower_part;\n    lower_part.splice(lower_part.end(),input,input.begin(),\n        divide_point);                                     \n    auto new_lower(\n        sequential_quick_sort(std::move(lower_part)));    \n    auto new_higher(\n        sequential_quick_sort(std::move(input)));     \n    result.splice(result.end(),new_higher);      \n    result.splice(result.begin(),new_lower);    \n    return result;\n}\nAlthough the interface is FP-style, if you used FP style throughout, you’d do a lot of\ncopying, so you use “normal” imperative style for the internals. You take the first ele-\nment as the pivot by slicing it off the front of the list using splice() B. Although this\ncan potentially result in a suboptimal sort (in terms of numbers of comparisons and\nexchanges), doing anything else with a std::list can add quite a bit of time because\nof the list traversal. You know you’re going to want it in the result, so you can splice it\ndirectly into the list you’ll be using for that. Now, you’re also going to want to use it for\ncomparisons, so let’s take a reference to it to avoid copying c. You can then use\nstd::partition to divide the sequence into those values less than the pivot and those\nnot less than the pivot d. The easiest way to specify the partition criteria is to use a\nlambda function; you use a reference capture to avoid copying the pivot value (see\nappendix A, section A.5 for more on lambda functions).\n std::partition() rearranges the list in place and returns an iterator marking the\nfirst element that’s not less than the pivot value. The full type for an iterator can be\nquite long-winded, so you just use the auto type specifier to force the compiler to\nwork it out for you (see appendix A, section A.7).\n Now, you’ve opted for an FP-style interface, so if you’re going to use recursion to\nsort the two “halves,” you’ll need to create two lists. You can do this by using splice()\nListing 4.12\nA sequential implementation of Quicksort\nb\nc\nd\ne\nf\ng\nh\ni\n\n\n102\nCHAPTER 4\nSynchronizing concurrent operations\nagain to move the values from input up to the divide_point into a new list: lower_\npart e. This leaves the remaining values alone in input. You can then sort the two\nlists with recursive calls, f and g. By using std::move() to pass the lists in, you can\navoid copying here too—the result is implicitly moved out anyway. Finally, you can use\nsplice() yet again to piece the result together in the right order. The new_higher\nvalues go on the end h, after the pivot, and the new_lower values go at the beginning,\nbefore the pivot i.\nFP-STYLE PARALLEL QUICKSORT\nBecause this uses a functional style already, it’s now easy to convert this to a parallel\nversion using futures, as shown in the next listing. The set of operations is the same as\nbefore, except that some of them now run in parallel. This version uses an implemen-\ntation of the Quicksort algorithm using futures and a functional style.\ntemplate<typename T>\nstd::list<T> parallel_quick_sort(std::list<T> input)\n{\n    if(input.empty())\n    {\n        return input;\n    }\n    std::list<T> result;\n    result.splice(result.begin(),input,input.begin());\n    T const& pivot=*result.begin();\n    auto divide_point=std::partition(input.begin(),input.end(),\n            [&](T const& t){return t<pivot;});\n    std::list<T> lower_part;\n    lower_part.splice(lower_part.end(),input,input.begin(),\n        divide_point);\n    std::future<std::list<T> > new_lower(                  \n        std::async(&parallel_quick_sort<T>,std::move(lower_part)));\n    auto new_higher(\n        parallel_quick_sort(std::move(input)));       \n    result.splice(result.end(),new_higher);           \n    result.splice(result.begin(),new_lower.get());   \n    return result;\n}\nThe big change here is that rather than sorting the lower portion on the current\nthread, you sort it on another thread using std::async() B. The upper portion of\nthe list is sorted with direct recursion as before c. By recursively calling parallel_\nquick_sort(), you can take advantage of the available hardware concurrency. If\nstd::async() starts a new thread every time, then if you recurse down three times,\nyou’ll have eight threads running; if you recurse down 10 times (for ~1000 ele-\nments), you’ll have 1,024 threads running if the hardware can handle it. If the library\ndecides there are too many spawned tasks (perhaps because the number of tasks has\nexceeded the available hardware concurrency), it may switch to spawning the new\nListing 4.13\nParallel Quicksort using futures\nb\nc\nd\ne\n\n\n103\nUsing synchronization of operations to simplify code\ntasks synchronously. They will run in the thread that calls get() rather than on a new\nthread, thus avoiding the overhead of passing the task to another thread when this\nwon’t help the performance. It’s worth noting that it’s perfectly conforming for an\nimplementation of std::async to start a new thread for each task (even in the face of\nmassive oversubscription) unless std::launch::deferred is explicitly specified, or to\nrun all tasks synchronously unless std::launch::async is explicitly specified. If you’re\nrelying on the library for automatic scaling, you’re advised to check the documenta-\ntion for your implementation to see what behavior it exhibits.\n Rather than using std::async(), you could write your own spawn_task() func-\ntion as a simple wrapper around std::packaged_task and std::thread, as shown in\nlisting 4.14; you’d create a std::packaged_task for the result of the function call, get\nthe future from it, run it on a thread, and return the future. This wouldn’t offer much\nof an advantage (and indeed would likely lead to massive oversubscription), but it\nwould pave the way to migrate to a more sophisticated implementation that adds the\ntask to a queue to be run by a pool of worker threads. We’ll look at thread pools in\nchapter 9. It’s probably worth going this way in preference to using std::async only if\nyou know what you’re doing and want complete control over the way the thread pool\nis built and executes tasks.\n Anyway, back to parallel_quick_sort. Because you just used direct recursion to\nget new_higher, you can splice it into place as before d. But new_lower is now\nstd::future<std::list<T>> rather than a list, so you need to call get() to retrieve\nthe value before you can call splice() e. This then waits for the background task to\ncomplete and moves the result into the splice() call; get() returns an rvalue refer-\nence to the contained result, so it can be moved out (see appendix A, section A.1.1 for\nmore on rvalue references and move semantics).\n Even assuming that std::async() makes optimal use of the available hardware\nconcurrency, this still isn’t an ideal parallel implementation of Quicksort. For one\nthing, std::partition does a lot of the work, and that’s still a sequential call, but it’s\ngood enough for now. If you’re interested in the fastest possible parallel implementa-\ntion, check the academic literature. Alternatively, you could use the parallel overload\nfrom the C++17 Standard Library (see chapter 10).\ntemplate<typename F,typename A>\nstd::future<std::result_of<F(A&&)>::type>\n    spawn_task(F&& f,A&& a)\n{\n    typedef std::result_of<F(A&&)>::type result_type;\n    std::packaged_task<result_type(A&&)>\n        task(std::move(f)));\n    std::future<result_type> res(task.get_future());\n    std::thread t(std::move(task),std::move(a));\n    t.detach();\n    return res;\n}\nListing 4.14\nA sample implementation of spawn_task\n\n\n104\nCHAPTER 4\nSynchronizing concurrent operations\nFP isn’t the only concurrent programming paradigm that eschews shared mutable\ndata; another paradigm is CSP (Communicating Sequential Processes),2 where threads\nare conceptually entirely separate, with no shared data but with communication chan-\nnels that allow messages to be passed between them. This is the paradigm adopted by\nthe programming language Erlang (http://www.erlang.org/) and by the MPI (Mes-\nsage Passing Interface; http://www.mpi-forum.org/) environment commonly used\nfor high-performance computing in C and C++. I’m sure that by now you’ll be unsur-\nprised to learn that this can also be supported in C++ with a bit of discipline; the fol-\nlowing section discusses one way to achieve this.\n4.4.2\nSynchronizing operations with message passing\nThe idea of CSP is simple: if there’s no shared data, each thread can be reasoned\nabout entirely independently, purely on the basis of how it behaves in response to the\nmessages that it received. Each thread is therefore effectively a state machine: when it\nreceives a message, it updates its state in some manner and maybe sends one or more\nmessages to other threads, with the processing performed depending on the initial\nstate. One way to write such threads would be to formalize this and implement a\nFinite State Machine model, but this isn’t the only way; the state machine can be\nimplicit in the structure of the application. Which method works better in any given\nscenario depends on the exact behavioral requirements of the situation and the\nexpertise of the programming team. However you choose to implement each thread,\nthe separation into independent processes has the potential to remove much of the\ncomplication from shared-data concurrency and therefore make programming easier,\nlowering the bug rate.\n True communicating sequential processes have no shared data, with all communi-\ncation passed through the message queues, but because C++ threads share an address\nspace, it’s not possible to enforce this requirement. This is where the discipline comes\nin: as application or library authors, it’s our responsibility to ensure that we don’t\nshare data between the threads. Of course, the message queues must be shared in\norder for the threads to communicate, but the details can be wrapped in the library.\n Imagine for a moment that you’re implementing the code for an ATM. This code\nneeds to handle interaction with the person trying to withdraw money and interaction\nwith the relevant bank, as well as control the physical machinery to accept the per-\nson’s card, display appropriate messages, handle key presses, issue money, and return\ntheir card.\n One way to handle everything would be to split the code into three independent\nthreads: one to handle the physical machinery, one to handle the ATM logic, and one\nto communicate with the bank. These threads could communicate purely by passing\nmessages rather than sharing any data. For example, the thread handling the machinery\n2 Communicating Sequential Processes, C.A.R. Hoare, Prentice Hall, 1985. Available free online at http://www\n.usingcsp.com/cspbook.pdf.\n\n\n105\nUsing synchronization of operations to simplify code\nwould send a message to the logic thread when the person at the machine entered\ntheir card or pressed a button, and the logic thread would send a message to the\nmachinery thread indicating how much money to dispense, and so forth.\n One way to model the ATM logic would be as a state machine. In each state, the\nthread waits for an acceptable message, which it then processes. This may result in\ntransitioning to a new state, and the cycle continues. The states involved in a simple\nimplementation are shown in figure 4.3. In this simplified implementation, the system\nwaits for a card to be inserted. Once the card is inserted, it then waits for the user to\nenter their PIN, one digit at a time. They can delete the last digit entered. Once\nenough digits have been entered, the PIN is verified. If the PIN is not OK, you’re fin-\nished, so you return the card to the customer and resume waiting for someone to\nenter their card. If the PIN is OK, you wait for them to either cancel the transaction or\nselect an amount to withdraw. If they cancel, you’re finished, and you return their\ncard. If they select an amount, you wait for confirmation from the bank before issuing\nthe cash and returning the card or displaying an “insufficient funds” message and\nreturning their card. Obviously, a real ATM is considerably more complex, but this is\nenough to illustrate the idea.\nHaving designed a state machine for your ATM logic, you can implement it with a\nclass that has a member function to represent each state. Each member function can\nthen wait for specific sets of incoming messages and handle them when they arrive,\npossibly triggering a switch to another state. Each distinct message type is represented\nInitial state\nGetting PIN\nCard inserted\nDigit pressed\nClear last digit pressed\nVerifying\nPIN\nDigit pressed (final digit)\nPIN OK\nDone\nCard taken\nPIN not OK\nCancel\npressed\nCancel pressed\nWaiting for\nconfirmation\nWithdraw (amount)\npressed\nWithdrawal OK\n(issue cash)\nInsufficient funds\nWaiting for\nwithdrawal\namount\nFigure 4.3\nA simple state machine model for an ATM\n\n\n106\nCHAPTER 4\nSynchronizing concurrent operations\nby a separate struct. Listing 4.15 shows part of a simple implementation of the ATM\nlogic in such a system, with the main loop and the implementation of the first state,\nwaiting for the card to be inserted. \n As you can see, all the necessary synchronization for the message passing is entirely\nhidden inside the message-passing library (a basic implementation of which is given in\nappendix C, along with the full code for this example).\nstruct card_inserted\n{\n    std::string account;\n};\nclass atm\n{\n    messaging::receiver incoming;\n    messaging::sender bank;\n    messaging::sender interface_hardware;\n    void (atm::*state)();\n    std::string account;\n    std::string pin;\n    void waiting_for_card()    \n    {\n        interface_hardware.send(display_enter_card());   \n        incoming.wait()                             \n            .handle<card_inserted>(\n                [&](card_inserted const& msg)    \n                {\n                    account=msg.account;\n                    pin=\"\";\n                    interface_hardware.send(display_enter_pin());\n                    state=&atm::getting_pin;\n                }\n                );\n    }\n    void getting_pin();\npublic:\n    void run()    \n    {\n        state=&atm::waiting_for_card;     \n        try\n        {\n            for(;;)\n            {\n                (this->*state)();    \n            }\n        }\n        catch(messaging::close_queue const&)\n        {\n        }\n    }\n};\nListing 4.15\nA simple implementation of an ATM logic class\nb\nc\nd\ne\nf\ng\nh\n\n\n107\nUsing synchronization of operations to simplify code\nAs already mentioned, the implementation described here is grossly simplified from\nthe real logic that would be required in an ATM, but it does give you a feel for the\nmessage-passing style of programming. There’s no need to think about synchroniza-\ntion and concurrency issues, just which messages may be received at any given point\nand which messages to send. The state machine for this ATM logic runs on a single\nthread, with other parts of the system such as the interface to the bank and the termi-\nnal interface running on separate threads. This style of program design is called the\nActor model—there are several discrete actors in the system (each running on a separate\nthread), which send messages to each other to perform the task at hand, and there’s\nno shared state except that which is directly passed via messages.\n Execution starts with the run() member function f, which sets the initial state to\nwaiting_for_card g and then repeatedly executes the member function represent-\ning the current state (whatever it is) h. The state functions are simple member func-\ntions of the atm class. The waiting_for_card state function B is also simple: it sends\na message to the interface to display a “waiting for card” message c, and then waits\nfor a message to handle d. The only type of message that can be handled here is a\ncard_inserted message, which you handle with a lambda function e. You could pass\nany function or function object to the handle function, but for a simple case like this,\nit’s easiest to use a lambda. Note that the handle() function call is chained onto the\nwait() function; if a message is received that doesn’t match the specified type, it’s dis-\ncarded, and the thread continues to wait until a matching message is received.\n The lambda function itself caches the account number from the card in a member\nvariable, clears the current PIN, sends a message to the interface hardware to display\nsomething asking the user to enter their PIN, and changes to the “getting PIN” state.\nOnce the message handler has completed, the state function returns, and the main\nloop then calls the new state function h.\n The getting_pin state function is a bit more complex in that it can handle three\ndistinct types of message, as in figure 4.3. This is shown in the following listing.\nvoid atm::getting_pin()\n{\n    incoming.wait()\n        .handle<digit_pressed>(     \n            [&](digit_pressed const& msg)\n            {\n                unsigned const pin_length=4;\n                pin+=msg.digit;\n                if(pin.length()==pin_length)\n                {\n                    bank.send(verify_pin(account,pin,incoming));\n                    state=&atm::verifying_pin;\n                }\n            }\n            )\nListing 4.16\nThe getting_pin state function for the simple ATM implementation\nb\n\n\n108\nCHAPTER 4\nSynchronizing concurrent operations\n        .handle<clear_last_pressed>(     \n            [&](clear_last_pressed const& msg)\n            {\n                if(!pin.empty())\n                {\n                    pin.resize(pin.length()-1);\n                }\n            }\n            )\n        .handle<cancel_pressed>(     \n            [&](cancel_pressed const& msg)\n            {\n                state=&atm::done_processing;\n            }\n            );\n}\nThis time, there are three message types you can process, so the wait() function has\nthree handle() calls chained on the end, B, c, and d. Each call to handle() speci-\nfies the message type as the template parameter and then passes in a lambda function\nthat takes that particular message type as a parameter. Because the calls are chained\ntogether in this way, the wait() implementation knows that it’s waiting for a digit_\npressed message, a clear_last_pressed message, or a cancel_pressed message.\nMessages of any other type are again discarded.\n This time, you don’t necessarily change state when you get a message. For exam-\nple, if you get a digit_pressed message, you add it to the pin unless it’s the final\ndigit. The main loop h in listing 4.15 will then call getting_pin() again to wait for\nthe next digit (or clear or cancel).\n This corresponds to the behavior shown in figure 4.3. Each state box is imple-\nmented by a distinct member function, which waits for the relevant messages and\nupdates the state as appropriate.\n As you can see, this style of programming can greatly simplify the task of designing\na concurrent system, because each thread can be treated entirely independently. It is\nan example of using multiple threads to separate concerns and as such requires you to\nexplicitly decide how to divide the tasks between threads.\n Back in section 4.2, I mentioned that the Concurrency TS provides extended ver-\nsions of futures. The core part of the extensions is the ability to specify continuations—\nadditional functions that are run automatically when the future becomes ready. Let’s\ntake the opportunity to explore how this can simplify our code.\n4.4.3\nContinuation-style concurrency with the Concurrency TS\nThe Concurrency TS provides new versions of std::promise and std::packaged_task\nin the std::experimental namespace that all differ from their std originals in the same\nway: they return instances of std::experimental::future rather than std::future.\nThis enables users to take advantage of the key new feature in std::experimental\n::future—continuations.\nc\nd\n",
      "page_number": 123
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 132-140)",
      "start_page": 132,
      "end_page": 140,
      "detection_method": "topic_boundary",
      "content": "109\nUsing synchronization of operations to simplify code\n Suppose you have a task running that will produce a result, and a future that will\nhold the result when it becomes available. You then have some code that needs to run\nin order to process that result. With std::future you would have to wait for the\nfuture to become ready, either with the fully-blocking wait() member function or\neither of the wait_for() or wait_until() member functions to allow a wait with a\ntimeout. This can be inconvenient, and can complicate the code. What you want is a\nmeans of saying “When the data is ready, then do this processing”. This is exactly what\ncontinuations give us; unsurprisingly, the member function to add a continuation to a\nfuture is called then(). Given a future fut, a continuation is added with the call\nfut.then(continuation).\n Just like std::future, std::experimental::future only allows the stored value to\nbe retrieved once. If that value is being consumed by a continuation, this means it can-\nnot be accessed by other code. Consequently, when a continuation is added with\nfut.then(), the original future, fut, becomes invalid. Instead, the call to fut.then()\nreturns a new future to hold the result of the continuation call. This is shown in the\nfollowing code:\nstd::experimental::future<int> find_the_answer;\nauto fut=find_the_answer();\nauto fut2=fut.then(find_the_question);\nassert(!fut.valid());\nassert(fut2.valid());\nThe find_the_question continuation function is scheduled to run “on an unspeci-\nfied thread” when the original future is ready. This gives the implementation freedom\nto run it on a thread pool or another library-managed thread. As it stands, this gives\nthe implementation a lot of freedom; this is deliberate, with the intention that when\ncontinuations are added to a future C++ Standard, the implementers will be able to\ndraw on their experience to better specify the choice of threads and provide users\nwith suitable mechanisms for controlling the choice of threads.\n Unlike direct calls to std::async or std::thread, you cannot pass arguments to a\ncontinuation function, because the argument is already defined by the library—the\ncontinuation is passed a ready future that holds the result that triggered the continua-\ntion. Assuming your find_the_answer function returns an int, the find_the_question\nfunction referenced in the previous example must take a std::experimental::\nfuture<int> as its sole parameter; for example:\nstd::string find_the_question(std::experimental::future<int> the_answer);\nThe reason for this is that the future on which the continuation was chained may end\nup holding a value or an exception. If the future was implicitly dereferenced to pass\nthe value directly to the continuation, then the library would have to decide how to\nhandle the exception, whereas by passing the future to the continuation, the continua-\ntion can handle the exception. In simple cases, this may be done by calling fut.get()\n\n\n110\nCHAPTER 4\nSynchronizing concurrent operations\nand allowing the re-thrown exception to propagate out of the continuation function.\nJust as for functions passed to std::async, exceptions that escape a continuation are\nstored in the future that holds the continuation result.\n Note that the Concurrency TS doesn’t specify that there is an equivalent to\nstd::async, though implementations may provide one as an extension. Writing such\na function is fairly straightforward: use std::experimental::promise to obtain a\nfuture, and then spawn a new thread running a lambda that sets the promise’s value to\nthe return value of the supplied function, as in the next listing.\ntemplate<typename Func>\nstd::experimental::future<decltype(std::declval<Func>()())>\nspawn_async(Func&& func){\n    std::experimental::promise<\n        decltype(std::declval<Func>()())> p;\n    auto res=p.get_future();\n    std::thread t(\n        [p=std::move(p),f=std::decay_t<Func>(func)]()\n            mutable{\n            try{\n                p.set_value_at_thread_exit(f());\n            } catch(...){\n                p.set_exception_at_thread_exit(std::current_exception());\n            }\n    });\n    t.detach();\n    return res;\n}\nThis stores the result of the function in the future, or catches the exception thrown\nfrom the function and stores that in the future, just as std::async does. Also, it uses\nset_value_at_thread_exit and set_exception_at_thread_exit to ensure that thread\n_local variables have been properly cleaned up before the future becomes ready.\n The value returned from a then() call is a fully-fledged future itself. This means\nthat you can chain continuations.\n4.4.4\nChaining continuations\nSuppose you have a series of time-consuming tasks to do, and you want to do them\nasynchronously in order to free up the main thread for other tasks. For example, when\nthe user logs in to your application, you might need to send the credentials to the\nbackend for authentication; then, when the details have been authenticated, make a\nfurther request to the backend for information about the user’s account; and finally,\nwhen that information has been retrieved, update the display with the relevant infor-\nmation. As sequential code, you might write something like the following listing.\n \nListing 4.17\nA simple equivalent to std::async for Concurrency TS futures\n\n\n111\nUsing synchronization of operations to simplify code\nvoid process_login(std::string const& username,std::string const& password)\n{\n    try {\n        user_id const id=backend.authenticate_user(username,password);\n        user_data const info_to_display=backend.request_current_info(id);\n        update_display(info_to_display);\n    } catch(std::exception& e){\n        display_error(e);\n    }\n}\nHowever, you don’t want sequential code; you want asynchronous code so you’re not\nblocking the UI thread. With plain std::async, you could punt it all to a background\nthread like the next listing, but that would still block that thread, consuming resources\nwhile waiting for the tasks to complete. If you have many such tasks, then you can end\nup with a large number of threads that are doing nothing except waiting.\nstd::future<void> process_login(\n    std::string const& username,std::string const& password)\n{\n    return std::async(std::launch::async,[=](){\n        try {\n            user_id const id=backend.authenticate_user(username,password);\n            user_data const info_to_display=\n                backend.request_current_info(id);\n            update_display(info_to_display);\n        } catch(std::exception& e){\n            display_error(e);\n        }\n    });\n}\nIn order to avoid all these blocked threads, you need some mechanism for chaining\ntasks as they each complete: continuations. The following listing shows the same over-\nall process, but this time split into a series of tasks, each of which is then chained on\nthe previous one as a continuation.\nstd::experimental::future<void> process_login(\n    std::string const& username,std::string const& password)\n{\n    return spawn_async([=](){\n        return backend.authenticate_user(username,password);\n    }).then([](std::experimental::future<user_id> id){\n        return backend.request_current_info(id.get());\n    }).then([](std::experimental::future<user_data> info_to_display){\nListing 4.18\nA simple sequential function to process user login\nListing 4.19\nProcessing user login with a single async task\nListing 4.20\nA function to process user login with continuations\n\n\n112\nCHAPTER 4\nSynchronizing concurrent operations\n        try{\n            update_display(info_to_display.get());\n        } catch(std::exception& e){\n            display_error(e);\n        }\n    });\n}\nNote how each continuation takes a std::experimental::future as the sole parame-\nter, and then uses .get() to retrieve the contained value. This means that exceptions\nget propagated all the way down the chain, so the call to info_to_display.get() in\nthe final continuation will throw if any of the functions in the chain threw an excep-\ntion, and the catch block here can handle all the exceptions, just like the catch block\nin listing 4.18 did.\n If the function calls to the backend block internally because they’re waiting for\nmessages to cross the network or for a database operation to complete, then you’re\nnot done yet. You may have split the task into its individual parts, but they’re still\nblocking calls, so you still get blocked threads. What you need is for the backend calls\nto return futures that become ready when the data is ready, without blocking any\nthreads. In this case, backend.async_authenticate_user(username,password) will\nnow return a std::experimental::future<user_id> rather than a plain user_id.\n You might think this would complicate the code, because returning a future from a\ncontinuation would give you future<future<some_value>>, or else you’d have to put\nthe .then calls inside the continuations. Thankfully, if you thought that, then you’d be\nmistaken: the continuation support has a nifty feature called future-unwrapping. If the\ncontinuation function you pass to a .then() call returns a future<some_type>, then the\n.then() call will return a future<some_type> in turn. This means your final code looks\nlike the next listing, and there is no blocking in your asynchronous function chain.\nstd::experimental::future<void> process_login(\n    std::string const& username,std::string const& password)\n{\n    return backend.async_authenticate_user(username,password).then(\n        [](std::experimental::future<user_id> id){\n            return backend.async_request_current_info(id.get());\n        }).then([](std::experimental::future<user_data> info_to_display){\n            try{\n                update_display(info_to_display.get());\n            } catch(std::exception& e){\n                display_error(e);\n            }\n        });\n}\nThis is almost as straightforward as the sequential code from listing 4.18, with a little bit\nmore boilerplate around the .then calls and the lambda declarations. If your compiler\nListing 4.21\nA function to process user login with fully asynchronous operations\n\n\n113\nUsing synchronization of operations to simplify code\nsupports C++14 generic lambdas, then the types of the futures in the lambda parame-\nters can be replaced with auto, which simplifies the code even further:\nreturn backend.async_authenticate_user(username,password).then(\n        [](auto id){\n            return backend.async_request_current_info(id.get());\n        });\nIf you need anything more complex than simple linear control flow, then you can\nimplement this by putting the logic in one of the lambdas; for truly complex control\nflow you probably need to write a separate function.\n So far, we’ve focused on the continuation support in std::experimental::future.\nAs you might expect, std::experimental::shared_future also supports continuations.\nThe difference here is that std::experimental::shared_future objects can have more\nthan one continuation, and the continuation parameter is a std::experimental::\nshared_future rather than a std::experimental::future. This naturally falls out of\nthe shared nature of std::experimental::shared_future—because multiple objects\ncan refer to the same shared state, if only one continuation was allowed, there would\nbe a race condition between two threads that were each trying to add continuations to\ntheir own  std::experimental::shared_future objects. This is obviously undesir-\nable, so multiple continuations are permitted. Once you have multiple continuations\npermitted, you may as well allow them to be added via the same std::experimental::\nshared_future instance, rather than only allowing one continuation per object. In\naddition, you can’t package the shared state in a one-shot std::experimental::\nfuture passed to the first continuation, when you’re going to want to also pass it to\nthe second continuation. Thus the parameter passed to the continuation function\nmust also be a std::experimental::shared_future:\nauto fut=spawn_async(some_function).share();\nauto fut2=fut.then([](std::experimental::shared_future<some_data> data){\n    do_stuff(data);\n    });\nauto fut3=fut.then([](std::experimental::shared_future<some_data> data){\n    return do_other_stuff(data);\n    });\nfut is a std::experimental::shared_future due to the share() call, so the continu-\nation function must take a std::experimental::shared_future as its parameter.\nHowever, the return value from the continuation is a plain std::experimental::\nfuture—that value isn’t currently shared until you do something to share it—so both\nfut2 and fut3 are std::experimental::futures.\n Continuations aren’t the only enhancement to futures in the Concurrency TS,\nthough they are probably the most important. Also provided are two overloaded func-\ntions that allow you to wait for either any one of a bunch of futures to become ready, or\nall of a bunch of futures to become ready.\n\n\n114\nCHAPTER 4\nSynchronizing concurrent operations\n4.4.5\nWaiting for more than one future\nSuppose you have a large volume of data to process, and each item can be processed\nindependently. This is a prime opportunity to make use of the available hardware by\nspawning a set of asynchronous tasks to process the data items, each of them return-\ning the processed data via a future. However, if you need to wait for all the tasks to\nfinish and then gather all the results for some final processing, this can be inconve-\nnient—you have to wait for each future in turn, and then gather the results. If you\nwant to do the result gathering with another asynchronous task, then you either have\nto spawn it up front so it is occupying a thread that’s waiting, or you have to keep poll-\ning the futures and spawn the new task when all the futures are ready. An example of\nsuch code is shown in the following listing.\nstd::future<FinalResult> process_data(std::vector<MyData>& vec)\n{\n    size_t const chunk_size=whatever;\n    std::vector<std::future<ChunkResult>> results;\n    for(auto begin=vec.begin(),end=vec.end();beg!=end;){\n        size_t const remaining_size=end-begin;\n        size_t const this_chunk_size=std::min(remaining_size,chunk_size);\n        results.push_back(\n            std::async(process_chunk,begin,begin+this_chunk_size));\n        begin+=this_chunk_size;\n    }\n    return std::async([all_results=std::move(results)](){\n        std::vector<ChunkResult> v;\n        v.reserve(all_results.size());\n        for(auto& f: all_results)\n        {\n            v.push_back(f.get());   \n        }\n        return gather_results(v);\n    });\n}\nThis code spawns a new asynchronous task to wait for the results, and then processes\nthem when they are all available. However, because it waits for each task individually, it\nwill repeatedly be woken by the scheduler at B as each result becomes available, and\nthen go back to sleep again when it finds another result that is not yet ready. Not only\ndoes this occupy the thread doing the waiting, but it adds additional context switches\nas each future becomes ready, which adds additional overhead.\n With std::experimental::when_all, this waiting and switching can be avoided.\nYou pass the set of futures to be waited on to when_all, and it returns a new future\nthat becomes ready when all the futures in the set are ready. This future can then be\nused with continuations to schedule additional work when the all the futures are\nready. See, for example, the next listing.\nListing 4.22\nGathering results from futures using std::async\nb\n\n\n115\nUsing synchronization of operations to simplify code\nstd::experimental::future<FinalResult> process_data(\n    std::vector<MyData>& vec)\n{\n    size_t const chunk_size=whatever;\n    std::vector<std::experimental::future<ChunkResult>> results;\n    for(auto begin=vec.begin(),end=vec.end();beg!=end;){\n        size_t const remaining_size=end-begin;\n        size_t const this_chunk_size=std::min(remaining_size,chunk_size);\n        results.push_back(\n            spawn_async(\n            process_chunk,begin,begin+this_chunk_size));\n        begin+=this_chunk_size;\n    }\n    return std::experimental::when_all(\n        results.begin(),results.end()).then(     \n        [](std::future<std::vector<\n             std::experimental::future<ChunkResult>>> ready_results)\n        {\n            std::vector<std::experimental::future<ChunkResult>>\n                all_results=ready_results .get();\n            std::vector<ChunkResult> v;\n            v.reserve(all_results.size());\n            for(auto& f: all_results)\n            {\n                v.push_back(f.get());    \n            }\n            return gather_results(v);\n        });\n}\nIn this case, you use when_all to wait for all the futures to become ready, and then\nschedule the function using .then rather than async B. Though the lambda is super-\nficially the same, it takes the results vector as a parameter (wrapped in a future)\nrather than as a capture, and the calls to get on the futures at c do not block, as all\nthe values are ready by the time execution gets there. This has the potential to reduce\nthe load on the system for little change to the code.\n To complement when_all, we also have when_any. This creates a future that\nbecomes ready when any of the supplied futures becomes ready. This works well for\nscenarios where you’ve spawned multiple tasks to take advantage of the available con-\ncurrency, but need to do something when the first one becomes ready.\n4.4.6\nWaiting for the first future in a set with when_any\nSuppose you are searching a large dataset for a value that meets particular criteria, but\nif there are multiple such values, then any will do. This is a prime target for parallel-\nism—you can spawn multiple threads, each of which checks a subset of the data; if a\ngiven thread finds a suitable value, then it sets a flag indicating that the other threads\nshould stop their search, and then sets the final return value. In this case, you want to\nListing 4.23\nGathering results from futures using std::experimental::when_all\nb\nc\n\n\n116\nCHAPTER 4\nSynchronizing concurrent operations\ndo the further processing when the first task completes its search, even if the other\ntasks haven’t finished cleaning up yet.\n Here, you can use std::experimental::when_any to gather the futures together,\nand provide a new future that is ready when at least one of the original set is ready.\nWhereas when_all gave you a future that wrapped the collection of futures you passed\nin, when_any adds a further layer, combining the collection with an index value that\nindicates which future triggered the combined future to be ready into an instance of\nthe std::experimental::when_any_result class template.\n An example of using when_any as described here is shown in the next listing.\nstd::experimental::future<FinalResult>\nfind_and_process_value(std::vector<MyData> &data)\n{\n    unsigned const concurrency = std::thread::hardware_concurrency();\n    unsigned const num_tasks = (concurrency > 0) ? concurrency : 2;\n    std::vector<std::experimental::future<MyData *>> results;\n    auto const chunk_size = (data.size() + num_tasks - 1) / num_tasks;\n    auto chunk_begin = data.begin();\n    std::shared_ptr<std::atomic<bool>> done_flag =\n        std::make_shared<std::atomic<bool>>(false);\n    for (unsigned i = 0; i < num_tasks; ++i) {        \n        auto chunk_end =\n            (i < (num_tasks - 1)) ? chunk_begin + chunk_size : data.end();\n        results.push_back(spawn_async([=] {               \n            for (auto entry = chunk_begin;\n                !*done_flag && (entry != chunk_end);\n                 ++entry) {\n                if (matches_find_criteria(*entry)) {\n                    *done_flag = true;\n                    return &*entry;\n                }\n            }\n            return (MyData *)nullptr;\n        }));\n        chunk_begin = chunk_end;\n    }\n    std::shared_ptr<std::experimental::promise<FinalResult>> final_result =\n        std::make_shared<std::experimental::promise<FinalResult>>();\n    struct DoneCheck {\n        std::shared_ptr<std::experimental::promise<FinalResult>>\n            final_result;\n        DoneCheck(\n            std::shared_ptr<std::experimental::promise<FinalResult>>\n                final_result_)\n            : final_result(std::move(final_result_)) {}\n        void operator()(                                     \n            std::experimental::future<std::experimental::when_any_result<\n                std::vector<std::experimental::future<MyData *>>>>\nListing 4.24\nUsing std::experimental::when_any to process the first value found\nb\nc\ne\n\n\n117\nUsing synchronization of operations to simplify code\n                results_param) {\n            auto results = results_param.get();\n            MyData *const ready_result =\n                results.futures[results.index].get();     \n            if (ready_result)\n                final_result->set_value(                 \n                    process_found_value(*ready_result));\n            else {\n                results.futures.erase(\n                    results.futures.begin() + results.index);    \n                if (!results.futures.empty()) {\n                    std::experimental::when_any(                       \n                        results.futures.begin(), results.futures.end())\n                        .then(std::move(*this));\n            } else {\n                final_result->set_exception(\n                    std::make_exception_ptr(               \n                        std::runtime_error(“Not found”)));\n            }\n        }\n    };\n    std::experimental::when_any(results.begin(), results.end())\n        .then(DoneCheck(final_result));                          \n    return final_result->get_future();     \n}\nThe initial loop B spawns off num_tasks asynchronous tasks, each running the\nlambda function from c. This lambda captures by copying, so each task will have its\nown values for chunk_begin and chunk_end, as well as a copy of the shared pointer,\ndone_flag. This avoids any concerns over lifetime issues.\n Once all the tasks have been spawned, you want to handle the case that a task\nreturned. This is done by chaining a continuation on the when_any call d. This time\nyou write the continuation as a class because you want to reuse it recursively. When\none of the initial tasks is ready, the DoneCheck function call operator is invoked e.\nFirst, it extracts the value from the future that is ready f, and then if the value was\nfound, you process it and set the final result g. Otherwise, you drop the ready future\nfrom the collection h, and if there are still more futures to check, issue a new call to\nwhen_any i, that will trigger its continuation when the next future is ready. If there\nare no futures left, then none of them found the value, so store an exception instead\nj. The return value of the function is the future for the final result 1). There are\nalternative ways to solve this problem, but I hope this shows how one might use\nwhen_any.\n Both these examples of using when_all and when_any have used the iterator-range\noverloads, which take a pair of iterators denoting the beginning and end of a set of\nfutures to wait for. Both functions also come in variadic forms, where they accept a\nnumber of futures directly as parameters to the function. In this case, the result is a\nfuture holding a tuple (or a when_any_result holding a tuple) rather than a vector:\nf\ng\nh\ni\nj\nd\n1)\n",
      "page_number": 132
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 141-154)",
      "start_page": 141,
      "end_page": 154,
      "detection_method": "topic_boundary",
      "content": "118\nCHAPTER 4\nSynchronizing concurrent operations\nstd::experimental::future<int> f1=spawn_async(func1);\nstd::experimental::future<std::string> f2=spawn_async(func2);\nstd::experimental::future<double> f3=spawn_async(func3);\nstd::experimental::future<\n    std::tuple<\n        std::experimental::future<int>,\n        std::experimental::future<std::string>,\n        std::experimental::future<double>>> result=\n    std::experimental::when_all(std::move(f1),std::move(f2),std::move(f3));\nThis example highlights something important about all the uses of when_any and\nwhen_all—they always move from any std::experimental::futures passed in via a\ncontainer, and they take their parameters by value, so you have to explicitly move the\nfutures in, or pass temporaries.\n Sometimes the event that you’re waiting for is for a set of threads to reach a partic-\nular point in the code, or to have processed a certain number of data items between\nthem. In these cases, you might be better served using a latch or a barrier rather than a\nfuture. Let’s look at the latches and barriers that are provided by the Concurrency TS.\n4.4.7\nLatches and barriers in the Concurrency TS\nFirst off, let’s consider what is meant when we talk of a latch or a barrier. A latch is a syn-\nchronization object that becomes ready when its counter is decremented to zero. Its\nname comes from the fact that it latches the output—once it is ready, it stays ready\nuntil it is destroyed. A latch is thus a lightweight facility for waiting for a series of\nevents to occur.\n On the other hand, a barrier is a reusable synchronization component used for\ninternal synchronization between a set of threads. Whereas a latch doesn’t care which\nthreads decrement the counter—the same thread can decrement the counter multi-\nple times, or multiple threads can each decrement the counter once, or some combi-\nnation of the two—with barriers, each thread can only arrive at the barrier once per\ncycle. When threads arrive at the barrier, they block until all of the threads involved\nhave arrived at the barrier, at which point they are all released. The barrier can then\nbe reused—the threads can then arrive at the barrier again to wait for all the threads\nfor the next cycle.\n Latches are inherently simpler than barriers, so let’s start with the latch type from\nthe Concurrency TS: std::experimental::latch.\n4.4.8\nA basic latch type: std::experimental::latch\nstd::experimental::latch comes from the <experimental/latch> header. When\nyou construct a std::experimental::latch, you specify the initial counter value as\nthe one and only argument to the constructor. Then, as the events that you are wait-\ning for occur, you call count_down on your latch object, and the latch becomes ready\nwhen that count reaches zero. If you need to wait for the latch to become ready, then\nyou can call wait on the latch; if you only need to check if it is ready, then you can call\n\n\n119\nUsing synchronization of operations to simplify code\nis_ready. Finally, if you need to both count down the counter and then wait for the\ncounter to reach zero, you can call count_down_and_wait. A basic example is shown\nin the following listing.\nvoid foo(){\n    unsigned const thread_count=...;\n    latch done(thread_count);         \n    my_data data[thread_count];\n    std::vector<std::future<void> > threads;\n    for(unsigned i=0;i<thread_count;++i)\n        threads.push_back(std::async(std::launch::async,[&,i]{    \n            data[i]=make_data(i);\n            done.count_down();   \n            do_more_stuff();     \n        }));\n    done.wait();                   \n    process_data(data,thread_count);   \n}     \nThis constructs done with the number of events that you need to wait for B, and then\nspawns the appropriate number of threads using std::async c. Each thread then\ncounts down the latch when it has generated the relevant chunk of data d before\ncontinuing on with further processing e. The main thread can wait for all the data to\nbe ready by waiting on the latch f before processing the generated data g. The data\nprocessing at g will potentially run concurrently with the final processing steps of\neach thread e—there is no guarantee that the threads have all completed until the\nstd::future destructors run at the end of the function h.\n One thing to note is that the lambda passed to std::async at c captures every-\nthing by reference except i, which is captured by value. This is because i is the loop\ncounter, and capturing that by reference would cause a data race and undefined\nbehavior, whereas data and done are things you need to share access to. Also, you only\nneed a latch at all in this scenario because the threads have additional processing to\ndo after the data is ready; otherwise you could wait for all the futures to ensure the\ntasks were complete before processing the data.\n It is safe to access data in the process_data call g, even though it is stored by\ntasks running in other threads, because the latch is a synchronization object, so\nchanges visible to a thread that call count_down are guaranteed to be visible to a\nthread that returns from a call to wait on the same latch object. Formally, the call to\ncount_down synchronizes with the call to wait—we’ll see what that means when we look\nat the low-level memory ordering and synchronization constraints in chapter 5.\n Alongside latches, the Concurrency TS gives us barriers—reusable synchronization\nobjects for synchronizing a group of threads. Let’s look at those next.\nListing 4.25\nWaiting for events with std::experimental::latch\nb\nc\nd\ne\nf\ng\nh\n\n\n120\nCHAPTER 4\nSynchronizing concurrent operations\n4.4.9\nstd::experimental::barrier: a basic barrier\nThe Concurrency TS provides two types of barriers in the <experimental/barrier>\nheader: std::experimental::barrier and std::experimental::flex_barrier. The\nformer is more basic, and potentially therefore has lower overhead, whereas the latter\nis more flexible, but potentially has more overhead.\n Suppose you have a group of threads that are operating on some data. Each thread\ncan do its processing on the data independently of the others, so no synchronization\nis needed during the processing, but all the threads must have completed their pro-\ncessing before the next data item can be processed, or before the subsequent process-\ning can be done. std::experimental::barrier is targeted at precisely this scenario.\nYou construct a barrier with a count specifying the number of threads involved in the\nsynchronization group. As each thread is done with its processing, it arrives at the bar-\nrier and waits for the rest of the group by calling arrive_and_wait on the barrier\nobject. When the last thread in the group arrives, all the threads are released, and the\nbarrier is reset. The threads in the group can then resume their processing and either\nprocess the next data item or proceed with the next stage of processing, as appropriate.\n Whereas latches latch, so once they are ready they stay ready, barriers do not—bar-\nriers release the waiting threads and then reset so they can be used again. They also\nonly synchronize within a group of threads—a thread cannot wait for a barrier to be\nready unless it is one of the threads in the synchronization group. Threads can explic-\nitly drop out of the group by calling arrive_and_drop on the barrier, in which case\nthat thread cannot wait for the barrier to be ready anymore, and the count of threads\nthat must arrive in the next cycle is one less than the number of threads that had to\narrive in the current cycle.\nresult_chunk process(data_chunk);\nstd::vector<data_chunk>\ndivide_into_chunks(data_block data, unsigned num_threads);\nvoid process_data(data_source &source, data_sink &sink) {\n    unsigned const concurrency = std::thread::hardware_concurrency();\n    unsigned const num_threads = (concurrency > 0) ? concurrency : 2;\n    std::experimental::barrier sync(num_threads);\n    std::vector<joining_thread> threads(num_threads);\n    std::vector<data_chunk> chunks;\n    result_block result;\n    for (unsigned i = 0; i < num_threads; ++i) {\n        threads[i] = joining_thread([&, i] {\n            while (!source.done()) {            \n                if (!i) {                 \n                    data_block current_block =\n                        source.get_next_data_block();\nListing 4.26\nUsing std::experimental::barrier\ng\nb\n\n\n121\nUsing synchronization of operations to simplify code\n                    chunks = divide_into_chunks(\n                        current_block, num_threads);\n                }\n                sync.arrive_and_wait();               \n                result.set_chunk(i, num_threads, process(chunks[i]));  \n                sync.arrive_and_wait();                 \n                if (!i) {                               \n                    sink.write_data(std::move(result));\n                }\n            }\n        });\n    }\n}       \nListing 4.26 shows an example of using a barrier to synchronize a group of threads.\nYou have data coming from source, and you’re writing the output to sink, but in\norder to make use of the available concurrency in the system, you’re splitting each\nblock of data into num_threads chunks. This has to be done serially, so you have an\ninitial block B that only runs on the thread for which i==0. All threads then wait on\nthe barrier for that serial code to complete c before you reach the parallel region,\nwhere each thread processes its individual chunk and updates the result with that d\nbefore synchronizing again e. You then have a second serial region where only\nthread 0 writes the result out to the sink f. All threads then keep looping until the\nsource reports that everything is done g. Note that as each thread loops round, the\nserial section at the bottom of the loop combines with the section at the top; because\nonly thread 0 has anything to do in either of these sections, this is OK, and all the\nthreads will synchronize together at the first use of the barrier c. When all the pro-\ncessing is done, then all the threads will exit the loop, and the destructors for the\njoining_thread objects will wait for them all to finish at the end of the outer func-\ntion h (joining_thread was introduced in chapter 2, listing 2.7).\n The key thing to note here is that the calls to arrive_and_wait are at the points in\nthe code where it is important that no threads proceed until all threads are ready. At\nthe first synchronization point, all the threads are waiting for thread 0 to arrive, but\nthe use of the barrier provides you with a clean line in the sand. At the second syn-\nchronization point, you have the reverse situation: it is thread 0 that is waiting for all\nthe other threads to arrive before it can write out the completed result to the sink.\n The Concurrency TS doesn’t just give you one barrier type; as well as std::experi-\nmental::barrier, you also get std::experimental::flex_barrier, which is more flex-\nible. One of the ways that it is more flexible is that it allows for a final serial region to be\nrun when all threads have arrived at the barrier, before they are all released again.\n4.4.10 std::experimental::flex_barrier—std::experimental::barrier’s \nflexible friend\nThe interface to std::experimental::flex_barrier differs from that of std::\nexperimental::barrier in only one way: there is an additional constructor that takes\nc\nd\ne\nf\nh\n\n\n122\nCHAPTER 4\nSynchronizing concurrent operations\na completion function, as well as a thread count. This function is run on exactly one\nof the threads that arrived at the barrier, once all the threads have arrived at the bar-\nrier. Not only does it provide a means of specifying a chunk of code that must be run\nserially, it also provides a means of changing the number of threads that must arrive at\nthe barrier for the next cycle. The thread count can be changed to any number,\nwhether higher or lower than the previous count; it is up to the programmer who uses\nthis facility to ensure that the correct number of threads will arrive at the barrier the\nnext time round.\n The following listing shows how listing 4.26 could be rewritten to use std::\nexperimental::flex_barrier to manage the serial region.\nvoid process_data(data_source &source, data_sink &sink) {\n    unsigned const concurrency = std::thread::hardware_concurrency();\n    unsigned const num_threads = (concurrency > 0) ? concurrency : 2;\n    std::vector<data_chunk> chunks;\n    auto split_source = [&] {     \n        if (!source.done()) {\n            data_block current_block = source.get_next_data_block();\n            chunks = divide_into_chunks(current_block, num_threads);\n        }\n    };\n    split_source();    \n    result_block result;\n    \n    std::experimental::flex_barrier sync(num_threads, [&] {    \n        sink.write_data(std::move(result));\n        split_source();               \n        return -1;          \n    });\n    std::vector<joining_thread> threads(num_threads);\n    for (unsigned i = 0; i < num_threads; ++i) {\n        threads[i] = joining_thread([&, i] {\n            while (!source.done()) {               \n                result.set_chunk(i, num_threads, process(chunks[i]));\n                sync.arrive_and_wait();       \n            }\n        });\n    }\n}\nThe first difference between this code and listing 4.26 is that you’ve extracted a\nlambda that splits the next data block into chunks B. This is called before you start\nc, and encapsulates the code that was run on thread 0 at the start of each iteration.\nListing 4.27\nUsing std::flex_barrier to provide a serial region\nb\nc\nd\ne\nf\ng\nh\n\n\n123\nSummary\n The second difference is that your sync object is now a std::experimental::flex\n_barrier, and you are passing a completion function as well as a thread count d.\nThis completion function is run on one thread after each thread has arrived, and so\ncan encapsulate the code that was to be run on thread 0 at the end of each iteration,\nand then there’s a call to your newly-extracted split_source lambda that would have\nbeen called at the start of the next iteration e. The return value of -1 f indicates that\nthe number of participating threads is to remain unchanged; a return value of zero or\nmore would specify the number of participating threads in the next cycle.\n The main loop g is now simplified: it only contains the parallel portion of the\ncode, and thus only needs a single synchronization point h. The use of std::\nexperimental::flex_barrier has thus simplified the code.\n The use of the completion function to provide a serial section is quite powerful, as\nis the ability to change the number of participating threads. For example, this could\nbe used by pipeline style code where the number of threads is less during the initial\npriming of the pipeline and the final draining of the pipeline than it is during the\nmain processing, when all the stages of the pipeline are operating.\nSummary\nSynchronizing operations between threads is an important part of writing an applica-\ntion that uses concurrency: if there’s no synchronization, the threads are essentially\nindependent and might as well be written as separate applications that are run as a\ngroup because of their related activities. In this chapter, I’ve covered various ways of\nsynchronizing operations from the basic condition variables, through futures, prom-\nises, packaged tasks, latches, and barriers. I’ve also discussed ways of approaching the\nsynchronization issues: functional-style programming, where each task produces a\nresult entirely dependent on its input rather than on the external environment; mes-\nsage passing, where communication between threads is via asynchronous messages\nsent through a messaging subsystem that acts as an intermediary; and continuation\nstyle, where the follow-on tasks for each operation are specified, and the system takes\ncare of the scheduling.\n Having discussed many of the high-level facilities available in C++, it’s now time to\nlook at the low-level facilities that make it all work: the C++ memory model and atomic\noperations.\n\n\n124\nThe C++ memory model\nand operations on\natomic types\nOne of the most important features of the C++ Standard is something most pro-\ngrammers won’t even notice. It’s not the new syntax features, nor is it the new\nlibrary facilities, but the new multithreading-aware memory model. Without the\nmemory model to define exactly how the fundamental building blocks work, none\nof the facilities I’ve covered could be relied on to work. There’s a reason that most\nprogrammers won’t notice: if you use mutexes to protect your data and condition\nvariables, futures, latches, or barriers to signal events, the details of why they work\naren’t important. It’s only when you start trying to get “close to the machine” that\nthe precise details of the memory model matter.\n Whatever else it is, C++ is a systems programming language. One of the goals of\nthe Standards Committee is that there will be no need for a lower-level language\nThis chapter covers\nThe details of the C++ memory model\nThe atomic types provided by the C++ \nStandard Library\nThe operations that are available on those types\nHow those operations can be used to provide \nsynchronization between threads\n\n\n125\nMemory model basics\nthan C++. Programmers should be provided with enough flexibility within C++ to do\nwhatever they need without the language getting in the way, allowing them to get\n“close to the machine” when the need arises. The atomic types and operations allow\njust that, providing facilities for low-level synchronization operations that will com-\nmonly reduce to one or two CPU instructions.\n In this chapter, I’ll start by covering the basics of the memory model, then move on\nto the atomic types and operations, and finally cover the various types of synchroniza-\ntion available with the operations on atomic types. This is quite complex: unless you’re\nplanning on writing code that uses the atomic operations for synchronization (such as\nthe lock-free data structures in chapter 7), you won’t need to know these details.\n Let’s ease into things with a look at the basics of the memory model.\n5.1\nMemory model basics\nThere are two aspects to the memory model: the basic structural aspects, which\nrelate to how things are laid out in memory, and the concurrency aspects. The struc-\ntural aspects are important for concurrency, particularly when you’re looking at low-\nlevel atomic operations, so I’ll start with those. In C++, it’s all about objects and\nmemory locations.\n5.1.1\nObjects and memory locations\nAll data in a C++ program is made up of objects. This is not to say that you can create a\nnew class derived from int, or that the fundamental types have member functions, or\nany of the other consequences often implied when people say “everything is an object”\nwhen discussing a language like Smalltalk or Ruby. It’s a statement about the building\nblocks of data in C++. The C++ Standard defines an object as “a region of storage,”\nalthough it goes on to assign properties to these objects, such as their type and lifetime.\n Some of these objects are simple values of a fundamental type such as int or\nfloat, whereas others are instances of user-defined classes. Some objects (such as\narrays, instances of derived classes, and instances of classes with non-static data\nmembers) have sub-objects, but others don’t.\n Whatever its type, an object is stored in one or more memory locations. Each mem-\nory location is either an object (or sub-object) of a scalar type such as unsigned short\nor my_class* or a sequence of adjacent bit fields. If you use bit fields, this is an\nimportant point to note: though adjacent bit fields are distinct objects, they’re still\ncounted as the same memory location. Figure 5.1 shows how a struct divides into\nobjects and memory locations.\n First, the entire struct is one object that consists of several sub-objects, one for\neach data member. The bf1 and bf2 bit fields share a memory location, and the\nstd::string object, s, consists of several memory locations internally, but otherwise\neach member has its own memory location. Note how the zero-length bit field bf3\n(the name is commented out because zero-length bit fields must be unnamed) sepa-\nrates bf4 into its own memory location, but doesn't have a memory location itself.\n\n\n126\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nThere are four important things to take away from this:\nEvery variable is an object, including those that are members of other objects.\nEvery object occupies at least one memory location. \nVariables of fundamental types such as int or char occupy exactly one memory\nlocation, whatever their size, even if they’re adjacent or part of an array.\nAdjacent bit fields are part of the same memory location.\nI’m sure you’re wondering what this has to do with concurrency, so let’s take a look.\n5.1.2\nObjects, memory locations, and concurrency\nNow, here’s the part that’s crucial for multithreaded applications in C++: everything\nhinges on those memory locations. If two threads access separate memory locations,\nthere’s no problem: everything works fine. On the other hand, if two threads access\nthe same memory location, then you have to be careful. If neither thread is updating\nthe memory location, you’re fine; read-only data doesn’t need protection or synchro-\nnization. If either thread is modifying the data, there’s a potential for a race condi-\ntion, as described in chapter 3.\n In order to avoid the race condition, there has to be an enforced ordering\nbetween the accesses in the two threads. This could be a fixed ordering such that one\naccess is always before the other, or it could be an ordering that varies between runs of\nthe application, but guarantees that there is some defined ordering. One way to ensure\nthere’s a defined ordering is to use mutexes as described in chapter 3; if the same\nmutex is locked prior to both accesses, only one thread can access the memory location\nstruct my_data\n{\nint i;\ndouble d;\nunsigned bf1:10;\nint bf2:25;\nint bf3:0;\nint bf4:9;\nint i2;\nchar c1,c2;\nstd::string s;\n};\ns\nc1\ni2\nbf4\nbf3\nbf1\nbf2\ni\nc2\nd\nObject\nMemory Location\nFigure 5.1\nThe division of a struct into objects and memory locations\n\n\n127\nMemory model basics\nat a time, so one must happen before the other (though, in general, you can't know in\nadvance which will be first). The other way is to use the synchronization properties of\natomic operations (see section 5.2 for the definition of atomic operations) either on\nthe same or other memory locations to enforce an ordering between the accesses in\nthe two threads. The use of atomic operations to enforce an ordering is described\nin section 5.3. If more than two threads access the same memory location, each pair of\naccesses must have a defined ordering.\n If there’s no enforced ordering between two accesses to a single memory location\nfrom separate threads, one or both of those accesses is not atomic, and if one or both\nis a write, then this is a data race and causes undefined behavior.\n This statement is crucially important: undefined behavior is one of the nastiest cor-\nners of C++. According to the language standard, once an application contains any\nundefined behavior, all bets are off; the behavior of the complete application is now\nundefined, and it may do anything at all. I know of one case where a particular\ninstance of undefined behavior caused someone’s monitor to catch fire. Although this\nis rather unlikely to happen to you, a data race is definitely a serious bug and should\nbe avoided at all costs.\n There’s another important point in that statement: you can also avoid the unde-\nfined behavior by using atomic operations to access the memory location involved\nin the race. This doesn’t prevent the race itself—which of the atomic operations\ntouches the memory location first is still not specified—but it does bring the program\nback into the realm of defined behavior.\n Before we look at atomic operations, there’s one more concept that’s important to\nunderstand about objects and memory locations: modification orders.\n5.1.3\nModification orders\nEvery object in a C++ program has a modification order composed of all the writes to\nthat object from all threads in the program, starting with the object’s initialization. In\nmost cases this order will vary between runs, but in any given execution of the pro-\ngram all threads in the system must agree on the order. If the object in question isn’t\none of the atomic types described in section 5.2, you’re responsible for making certain\nthat there’s sufficient synchronization to ensure that threads agree on the modifica-\ntion order of each variable. If different threads see distinct sequences of values for a\nsingle variable, you have a data race and undefined behavior (see section 5.1.2). If you\ndo use atomic operations, the compiler is responsible for ensuring that the necessary\nsynchronization is in place.\n This requirement means that certain kinds of speculative execution aren’t permit-\nted, because once a thread has seen a particular entry in the modification order, sub-\nsequent reads from that thread must return later values, and subsequent writes from\nthat thread to that object must occur later in the modification order. Also, a read of an\nobject that follows a write to that object in the same thread must either return the\nvalue written or another value that occurs later in the modification order of that\n\n\n128\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nobject. Although all threads must agree on the modification orders of each individual\nobject in a program, they don’t necessarily have to agree on the relative order of oper-\nations on separate objects. See section 5.3.3 for more on the ordering of operations\nbetween threads.\n So, what constitutes an atomic operation, and how can these be used to enforce\nordering?\n5.2\nAtomic operations and types in C++\nAn atomic operation is an indivisible operation. You can’t observe such an operation\nhalf-done from any thread in the system; it’s either done or not done. If the load oper-\nation that reads the value of an object is atomic, and all modifications to that object are\nalso atomic, that load will retrieve either the initial value of the object or the value\nstored by one of the modifications.\n The flip side of this is that a non-atomic operation might be seen as half-done by\nanother thread. If the non-atomic operation is composed of atomic operations (for\nexample, assignment to a struct with atomic members), then other threads may\nobserve some subset of the constituent atomic operations as complete, but others as\nnot yet started, so you might observe or end up with a value that is a mixed-up combi-\nnation of the various values stored. In any case, unsynchronized accesses to non-\natomic variables form a simple problematic race condition, as described in chapter 3,\nbut at this level it may constitute a data race (see section 5.1) and cause undefined\nbehavior.\n In C++, you need to use an atomic type to get an atomic operation in most cases, so\nlet’s look at those.\n5.2.1\nThe standard atomic types\nThe standard atomic types can be found in the <atomic> header. All operations on such\ntypes are atomic, and only operations on these types are atomic in the sense of the lan-\nguage definition, although you can use mutexes to make other operations appear\natomic. In fact, the standard atomic types themselves might use such emulation: they\n(almost) all have an is_lock_free() member function, which allows the user to deter-\nmine whether operations on a given type are done directly with atomic instructions\n(x.is_lock_free() returns true) or done by using a lock internal to the compiler\nand library (x.is_lock_free() returns false).\n This is important to know in many cases—the key use case for atomic operations is\nas a replacement for an operation that would otherwise use a mutex for synchroniza-\ntion; if the atomic operations themselves use an internal mutex then the hoped-for\nperformance gains will probably not materialize, and you might be better off using\nthe easier-to-get-right mutex-based implementation instead. This is the case with lock-\nfree data structures such as those discussed in chapter 7.\n In fact, this is so important that the library provides a set of macros to identify at\ncompile time whether the atomic types for the various integral types are lock-free.\n\n\n129\nAtomic operations and types in C++\nSince C++17, all atomic types have a static constexpr member variable, X::is_\nalways_lock_free, which is true if and only if the atomic type X is lock-free for all\nsupported hardware that the output of the current compilation might run on. For\nexample, for a given target platform, std::atomic<int> might always be lock-free, so\nstd::atomic<int>::is_always_lock_free will be true, but std::atomic<uintmax_t>\nmight only be lock-free if the hardware the program ends up running on supports the\nnecessary instructions, so this is a run-time property, and std::atomic<uintmax_t>\n::is_always_lock_free would be false when compiling for that platform.\n The macros are ATOMIC_BOOL_LOCK_FREE, ATOMIC_CHAR_LOCK_FREE, ATOMIC_\nCHAR16_T_LOCK_FREE, \nATOMIC_CHAR32_T_LOCK_FREE, \nATOMIC_WCHAR_T_LOCK_FREE,\nATOMIC_SHORT_LOCK_FREE, ATOMIC_INT_LOCK_FREE, ATOMIC_LONG_LOCK_FREE, ATOMIC\n_LLONG_LOCK_FREE, and ATOMIC_POINTER_LOCK_FREE. They specify the lock-free status\nof the corresponding atomic types for the specified built-in types and their unsigned\ncounterparts (LLONG refers to long long, and POINTER refers to all pointer types).\nThey evaluate to the value 0 if the atomic type is never lock-free, to the value 2 if the\natomic type is always lock-free, and to the value 1 if the lock-free status of the corre-\nsponding atomic type is a runtime property as described previously.\n The only type that doesn’t provide an is_lock_free() member function is\nstd::atomic_flag. This type is a simple Boolean flag, and operations on this type are\nrequired to be lock-free; once you have a simple lock-free Boolean flag, you can use that\nto implement a simple lock and implement all the other atomic types using that as a\nbasis. When I said simple, I meant it: objects of the std::atomic_flag type are initial-\nized to clear, and they can then either be queried and set (with the test_and_set()\nmember function) or cleared (with the clear() member function). That’s it: no\nassignment, no copy construction, no test and clear, no other operations at all.\n The remaining atomic types are all accessed through specializations of the\nstd::atomic<> class template and are a bit more full-featured but may not be lock-\nfree (as explained previously). On most popular platforms it’s expected that the\natomic variants of all the built-in types (such as std::atomic<int> and std::atomic\n<void*>) are indeed lock-free, but it isn’t required. As you’ll see shortly, the inter-\nface of each specialization reflects the properties of the type; bitwise operations\nsuch as &= aren’t defined for plain pointers, so they aren’t defined for atomic point-\ners either, for example.\n In addition to using the std::atomic<> class template directly, you can use the set\nof names shown in table 5.1 to refer to the implementation-supplied atomic types.\nBecause of the history of how atomic types were added to the C++ Standard, if you\nhave an older compiler, these alternative type names may refer either to the corre-\nsponding std::atomic<> specialization or to a base class of that specialization,\nwhereas in a compiler that fully supports C++17, these are always aliases for the corre-\nsponding std::atomic<> specializations. Mixing these alternative names with the\ndirect naming of std::atomic<> specializations in the same program can therefore\nlead to nonportable code.\n\n\n130\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nAs well as the basic atomic types, the C++ Standard Library also provides a set of\ntypedefs for the atomic types corresponding to the various non-atomic Standard\nLibrary typedefs such as std::size_t. These are shown in table 5.2.\nTable 5.1\nThe alternative names for the standard atomic types and their corresponding \nstd::atomic<> specializations\nAtomic type\nCorresponding specialization\natomic_bool\nstd::atomic<bool>\natomic_char\nstd::atomic<char>\natomic_schar\nstd::atomic<signed char>\natomic_uchar\nstd::atomic<unsigned char>\natomic_int\nstd::atomic<int>\natomic_uint\nstd::atomic<unsigned>\natomic_short\nstd::atomic<short>\natomic_ushort\nstd::atomic<unsigned short>\natomic_long\nstd::atomic<long>\natomic_ulong\nstd::atomic<unsigned long>\natomic_llong\nstd::atomic<long long>\natomic_ullong\nstd::atomic<unsigned long long>\natomic_char16_t\nstd::atomic<char16_t>\natomic_char32_t\nstd::atomic<char32_t>\natomic_wchar_t\nstd::atomic<wchar_t>\nTable 5.2\nThe standard atomic typedefs and their corresponding built-in typedefs\nAtomic typedef\nCorresponding Standard Library typedef\natomic_int_least8_t\nint_least8_t\natomic_uint_least8_t\nuint_least8_t\natomic_int_least16_t\nint_least16_t\natomic_uint_least16_t\nuint_least16_t\natomic_int_least32_t\nint_least32_t\natomic_uint_least32_t\nuint_least32_t\natomic_int_least64_t\nint_least64_t\natomic_uint_least64_t\nuint_least64_t\natomic_int_fast8_t\nint_fast8_t\n\n\n131\nAtomic operations and types in C++\nThat’s a lot of types! There’s a rather simple pattern to it; for a standard typedef T, the\ncorresponding atomic type is the same name with an atomic_ prefix: atomic_T. The\nsame applies to the built-in types, except that signed is abbreviated as s, unsigned as\nu, and long long as llong. It’s generally simpler to say std::atomic<T> for whichever\nT you want to work with, rather than use the alternative names.\n The standard atomic types are not copyable or assignable in the conventional\nsense, in that they have no copy constructors or copy assignment operators. They do,\nhowever, support assignment from and implicit conversion to the corresponding\nbuilt-in types as well as direct load() and store() member functions, exchange(),\ncompare_exchange_weak(), and compare_exchange_strong(). They also support the\ncompound assignment operators where appropriate: +=, -=, *=, |=, and so on, and\nthe integral types and std::atomic<> specializations for ++ and -- pointers support.\nThese operators also have corresponding named member functions with the same\nfunctionality: fetch_add(), fetch_or(), and so on. The return value from the assign-\nment operators and member functions is either the value stored (in the case of the\nassignment operators) or the value prior to the operation (in the case of the named\nfunctions). This avoids the potential problems that could stem from the usual habit of\nthese assignment operators returning a reference to the object being assigned to. In\norder to get the stored value from these references, the code would have to perform a\nseparate read, allowing another thread to modify the value between the assignment\nand the read and opening the door for a race condition.\natomic_uint_fast8_t\nuint_fast8_t\natomic_int_fast16_t\nint_fast16_t\natomic_uint_fast16_t\nuint_fast16_t\natomic_int_fast32_t\nint_fast32_t\natomic_uint_fast32_t\nuint_fast32_t\natomic_int_fast64_t\nint_fast64_t\natomic_uint_fast64_t\nuint_fast64_t\natomic_intptr_t\nintptr_t\natomic_uintptr_t\nuintptr_t\natomic_size_t\nsize_t\natomic_ptrdiff_t\nptrdiff_t\natomic_intmax_t\nintmax_t\natomic_uintmax_t\nuintmax_t\nTable 5.2\nThe standard atomic typedefs and their corresponding built-in typedefs (continued)\nAtomic typedef\nCorresponding Standard Library typedef\n",
      "page_number": 141
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 155-167)",
      "start_page": 155,
      "end_page": 167,
      "detection_method": "topic_boundary",
      "content": "132\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n The std::atomic<> class template isn’t only a set of specializations, though. It\ndoes have a primary template that can be used to create an atomic variant of a user-\ndefined type. Because it’s a generic class template, the operations are limited to\nload(), store() (and assignment from and conversion to the user-defined type),\nexchange(), compare_exchange_weak(), and compare_exchange_strong().\n Each of the operations on the atomic types has an optional memory-ordering argu-\nment which is one of the values of the std::memory_order enumeration. This argu-\nment is used to specify the required memory-ordering semantics. The std::memory\n_order enumeration has six possible values: std::memory_order_relaxed, std::\nmemory_order_acquire, std::memory_order_consume, std::memory_order_acq_rel,\nstd::memory_order_release, and std::memory_order_seq_cst. \n The permitted values for the memory ordering depend on the operation category.\nIf you don't specify an ordering value, then the default ordering is used, which is the\nstrongest ordering: std::memory_order_seq_cst. The precise semantics of the mem-\nory-ordering options are covered in section 5.3. For now, it suffices to know that the\noperations are divided into three categories:\nStore operations, which can have memory_order_relaxed, memory_order_release,\nor memory_order_seq_cst ordering \nLoad operations, which can have memory_order_relaxed, memory_order_consume,\nmemory_order_acquire, or memory_order_seq_cst ordering \nRead-modify-write operations, which can have memory_order_relaxed, memory_\norder_consume, memory_order_acquire, memory_order_release, memory_order\n_acq_rel, or memory_order_seq_cst ordering\nLet’s now look at the operations you can perform on each of the standard atomic\ntypes, starting with std::atomic_flag.\n5.2.2\nOperations on std::atomic_flag\nstd::atomic_flag is the simplest standard atomic type, which represents a Boolean\nflag. Objects of this type can be in one of two states: set or clear. It’s deliberately basic\nand is intended as a building block only. As such, I’d never expect to see it in use,\nexcept under special circumstances. Even so, it will serve as a starting point for discuss-\ning the other atomic types, because it shows some of the general policies that apply to\nthe atomic types.\n Objects of the std::atomic_flag type must be initialized with ATOMIC_FLAG_INIT.\nThis initializes the flag to a clear state. There’s no choice in the matter; the flag always\nstarts clear:\nstd::atomic_flag f=ATOMIC_FLAG_INIT;\nThis applies no matter where the object is declared and what scope it has. It’s the only\natomic type to require such special treatment for initialization, but it’s also the only\ntype guaranteed to be lock-free. If the std::atomic_flag object has static storage\n\n\n133\nAtomic operations and types in C++\nduration, it’s guaranteed to be statically initialized, which means that there are no\ninitialization-order issues; it will always be initialized by the time of the first operation\non the flag.\n Once you have your flag object initialized, there are only three things you can do\nwith it: destroy it, clear it, or set it and query the previous value. These correspond\nto the destructor, the clear() member function, and the test_and_set() member\nfunction, respectively. Both the clear() and test_and_set() member functions\ncan have a memory order specified. clear() is a store operation and so can’t have\nmemory_order_acquire or memory_order_acq_rel semantics, but test_and_set()\nis a read-modify-write operation and so can have any of the memory-ordering tags\napplied. As with every atomic operation, the default for both is memory_order_seq_cst.\nFor example:\nf.clear(std::memory_order_release);    \nbool x=f.test_and_set();           \nHere, the call to clear() B explicitly requests that the flag is cleared with release\nsemantics, whereas the call to test_and_set() c uses the default memory ordering\nfor setting the flag and retrieving the old value.\n You can’t copy-construct another std::atomic_flag object from the first, and\nyou can’t assign one std::atomic_flag to another. This isn’t something peculiar to\nstd::atomic_flag but something common with all the atomic types. All operations on\nan atomic type are defined as atomic, and assignment and copy-construction involve two\nobjects. A single operation on two distinct objects can’t be atomic. In the case of copy-\nconstruction or copy-assignment, the value must first be read from one object and then\nwritten to the other. These are two separate operations on two separate objects, and the\ncombination can’t be atomic. Therefore, these operations aren’t permitted.\n The limited feature set makes std::atomic_flag ideally suited to use as a spin-\nlock mutex. Initially, the flag is clear and the mutex is unlocked. To lock the mutex,\nloop on test_and_set() until the old value is false, indicating that this thread set the\nvalue to true. Unlocking the mutex is simply a matter of clearing the flag. This imple-\nmentation is shown in the following listing.\nclass spinlock_mutex\n{\n    std::atomic_flag flag;\npublic:\n    spinlock_mutex():\n        flag(ATOMIC_FLAG_INIT)\n    {}\n    void lock()\n    {\n        while(flag.test_and_set(std::memory_order_acquire));\n    }\nListing 5.1\nImplementation of a spinlock mutex using std::atomic_flag\nB\nc\n\n\n134\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n    void unlock()\n    {\n        flag.clear(std::memory_order_release);\n    }\n};\nThis mutex is basic, but it’s enough to use with std::lock_guard<> (see chapter 3). By\nits nature it does a busy-wait in lock(), so it’s a poor choice if you expect there to be any\ndegree of contention, but it’s enough to ensure mutual exclusion. When we look at the\nmemory-ordering semantics, you’ll see how this guarantees the necessary enforced\nordering that goes with a mutex lock. This example is covered in section 5.3.6.\n std::atomic_flag is so limited that it can’t even be used as a general Boolean flag,\nbecause it doesn’t have a simple nonmodifying query operation. For that you’re better\noff using std::atomic<bool>, so I’ll cover that next.\n5.2.3\nOperations on std::atomic<bool>\nThe most basic of the atomic integral types is std::atomic<bool>. This is a more full-\nfeatured Boolean flag than std::atomic_flag, as you might expect. Although it’s still\nnot copy-constructible or copy-assignable, you can construct it from a non-atomic\nbool, so it can be initially true or false, and you can also assign to instances of\nstd::atomic<bool> from a non-atomic bool:\nstd::atomic<bool> b(true);\nb=false;\nOne other thing to note about the assignment operator from a non-atomic bool is\nthat it differs from the general convention of returning a reference to the object it’s\nassigned to: it returns a bool with the value assigned instead. This is another common\npattern with the atomic types: the assignment operators they support return values (of\nthe corresponding non-atomic type) rather than references. If a reference to the\natomic variable was returned, any code that depended on the result of the assignment\nwould then have to explicitly load the value, potentially getting the result of a modifi-\ncation by another thread. By returning the result of the assignment as a non-atomic\nvalue, you can avoid this additional load, and you know that the value obtained is the\nvalue stored.\n Rather than using the restrictive clear() function of std::atomic_flag, writes (of\neither true or false) are done by calling store(), although the memory-order\nsemantics can still be specified. Similarly, test_and_set() has been replaced with the\nmore general exchange() member function that allows you to replace the stored\nvalue with a new one of your choosing and atomically retrieve the original value.\nstd::atomic<bool> also supports a plain nonmodifying query of the value with an\nimplicit conversion to plain bool or with an explicit call to load(). As you might\nexpect, store() is a store operation, whereas load() is a load operation. exchange()\nis a read-modify-write operation:\n\n\n135\nAtomic operations and types in C++\nstd::atomic<bool> b;\nbool x=b.load(std::memory_order_acquire);\nb.store(true);\nx=b.exchange(false,std::memory_order_acq_rel);\nexchange() isn’t the only read-modify-write operation supported by std::atomic<bool>;\nit also introduces an operation to store a new value if the current value is equal to an\nexpected value.\nSTORING A NEW VALUE (OR NOT) DEPENDING ON THE CURRENT VALUE\nThis new operation is called compare-exchange, and it comes in the form of the\ncompare_exchange_weak() and compare_exchange_strong() member functions. The\ncompare-exchange operation is the cornerstone of programming with atomic types;\nit compares the value of the atomic variable with a supplied expected value and\nstores the supplied desired value if they’re equal. If the values aren’t equal, the\nexpected value is updated with the value of the atomic variable. The return type of\nthe compare-exchange functions is a bool, which is true if the store was performed\nand false otherwise. The operation is said to succeed if the store was done (because\nthe values were equal), and fail otherwise; the return value is true for success, and\nfalse for failure.\n For compare_exchange_weak(), the store might not be successful even if the origi-\nnal value was equal to the expected value, in which case the value of the variable is\nunchanged and the return value of compare_exchange_weak() is false. This is most\nlikely to happen on machines that lack a single compare-and-exchange instruction, if\nthe processor can’t guarantee that the operation has been done atomically—possibly\nbecause the thread performing the operation was switched out in the middle of the\nnecessary sequence of instructions and another thread scheduled in its place by the\noperating system where there are more threads than processors. This is called a spuri-\nous failure, because the reason for the failure is a function of timing rather than the\nvalues of the variables.\n Because compare_exchange_weak() can fail spuriously, it must typically be used in\na loop:\nbool expected=false;\nextern atomic<bool> b; // set somewhere else\nwhile(!b.compare_exchange_weak(expected,true) && !expected);\nIn this case, you keep looping as long as expected is still false, indicating that the\ncompare_exchange_weak() call failed spuriously.\n On the other hand, compare_exchange_strong() is guaranteed to return false\nonly if the value wasn’t equal to the expected value. This can eliminate the need for\nloops like the one shown where you want to know whether you successfully changed a\nvariable or whether another thread got there first.\n If you want to change the variable whatever the initial value is (perhaps with an\nupdated value that depends on the current value), the update of expected becomes\n\n\n136\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nuseful; each time through the loop, expected is reloaded, so if no other thread modi-\nfies the value in the meantime, the compare_exchange_weak() or compare_exchange\n_strong() call should be successful the next time around the loop. If the calculation\nof the value to be stored is simple, it may be beneficial to use compare_exchange\n_weak() in order to avoid a double loop on platforms where compare_exchange_weak()\ncan fail spuriously (and so compare_exchange_strong() contains a loop). On the\nother hand, if the calculation of the value to be stored is time-consuming, it may make\nsense to use compare_exchange_strong() to avoid having to recalculate the value to\nstore when the expected value hasn’t changed. For std::atomic<bool> this isn’t so\nimportant—there are only two possible values after all—but for the larger atomic types\nthis can make a difference.\n The compare-exchange functions are also unusual in that they can take two memory-\nordering parameters. This allows for the memory-ordering semantics to differ in the\ncase of success and failure; it might be desirable for a successful call to have memory\n_order_acq_rel semantics, whereas a failed call has memory_order_relaxed seman-\ntics. A failed compare-exchange doesn’t do a store, so it can’t have memory_order\n_release or memory_order_acq_rel semantics. It’s therefore not permitted to supply\nthese values as the ordering for failure. You also can’t supply stricter memory ordering\nfor failure than for success; if you want memory_order_acquire or memory_order_\nseq_cst semantics for failure, you must specify those for success as well.\n If you don’t specify an ordering for failure, it’s assumed to be the same as that for\nsuccess, except that the release part of the ordering is stripped: memory_order_release\nbecomes memory_order_relaxed, and memory_order_acq_rel becomes memory_order\n_acquire. If you specify neither, they default to memory_order_seq_cst as usual,\nwhich provides the full sequential ordering for both success and failure. The following\ntwo calls to compare_exchange_weak() are equivalent:\nstd::atomic<bool> b;\nbool expected;\nb.compare_exchange_weak(expected,true,\n    memory_order_acq_rel,memory_order_acquire);\nb.compare_exchange_weak(expected,true,memory_order_acq_rel);\nI’ll leave the consequences of the choice of memory ordering to section 5.3.\n One further difference between std::atomic<bool> and std::atomic_flag is\nthat std::atomic<bool> may not be lock-free; the implementation may have to\nacquire a mutex internally in order to ensure the atomicity of the operations. For the\nrare case when this matters, you can use the is_lock_free() member function to\ncheck whether operations on std::atomic<bool> are lock-free. This is another fea-\nture common to all atomic types other than std::atomic_flag.\n The next simplest of the atomic types are the atomic pointer specializations\nstd::atomic<T*>, so we’ll look at those next.\n\n\n137\nAtomic operations and types in C++\n5.2.4\nOperations on std::atomic<T*>: pointer arithmetic\nThe atomic form of a pointer to some type T  is std::atomic<T*>, just as the atomic\nform of bool is std::atomic<bool>. The interface is the same, although it operates\non values of the corresponding pointer type rather than bool values. Like\nstd::atomic<bool>, it’s neither copy-constructible nor copy-assignable, although it\ncan be both constructed and assigned from the suitable pointer values. As well as the\nobligatory is_lock_free() member function, std::atomic<T*> also has load(),\nstore(), exchange(), compare_exchange_weak(), and compare_exchange_strong()\nmember functions, with similar semantics to those of std::atomic<bool>, again tak-\ning and returning T* rather than bool.\n The new operations provided by std::atomic<T*> are the pointer arithmetic\noperations. The basic operations are provided by the fetch_add() and fetch_sub()\nmember functions, which do atomic addition and subtraction on the stored address,\nand the += and -= operators, and both pre- and post-increment and decrement with\n++ and --, which provide convenient wrappers. The operators work as you’d expect\nfrom the built-in types: if x is std::atomic<Foo*> to the first entry of an array of Foo\nobjects, then x+=3 changes it to point to the fourth entry and returns a plain Foo* that\nalso points to that fourth entry. fetch_add() and fetch_sub() are slightly different in\nthat they return the original value (so x.fetch_add(3) will update x to point to the\nfourth value but return a pointer to the first value in the array). This operation is also\nknown as exchange-and-add, and it’s an atomic read-modify-write operation, like\nexchange() and compare_exchange_weak()/compare_exchange_strong(). As with\nthe other operations, the return value is a plain T* value rather than a reference to the\nstd::atomic<T*> object, so that the calling code can perform actions based on what\nthe previous value was:\nclass Foo{};\nFoo some_array[5];\nstd::atomic<Foo*> p(some_array);\nFoo* x=p.fetch_add(2);            \nassert(x==some_array);\nassert(p.load()==&some_array[2]);\nx=(p-=1);                         \nassert(x==&some_array[1]);\nassert(p.load()==&some_array[1]);\nThe function forms also allow the memory-ordering semantics to be specified as an\nadditional function call argument:\np.fetch_add(3,std::memory_order_release);\nBecause both fetch_add() and fetch_sub() are read-modify-write operations, they\ncan have any of the memory-ordering tags and can participate in a release sequence. Speci-\nfying the ordering semantics isn’t possible for the operator forms, because there’s no\nAdd 2 to p and \nreturn old value\nSubtract 1 from p and \nreturn new value\n\n\n138\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nway of providing the information: these forms therefore always have memory_order_\nseq_cst semantics.\n The remaining basic atomic types are all the same: they’re all atomic integral types\nand have the same interface as each other, except that the associated built-in type is\ndifferent. We’ll look at them as a group.\n5.2.5\nOperations on standard atomic integral types\nAs well as the usual set of operations (load(), store(), exchange(), compare_\nexchange_weak(), and compare_exchange_strong()), the atomic integral types such\nas std::atomic<int> or std::atomic<unsigned long long> have quite a comprehen-\nsive set of operations available: fetch_add(), fetch_sub(), fetch_and(), fetch_or(),\nfetch_xor(), compound-assignment forms of these operations (+=, -=, &=, |=, and\n^=), and pre- and post-increment and decrement (++x, x++, --x, and x--). It’s not quite\nthe full set of compound-assignment operations you could do on a normal integral\ntype, but it’s close enough: only division, multiplication, and shift operators are miss-\ning. Because atomic integral values are typically used either as counters or as bitmasks,\nthis isn’t a particularly noticeable loss; additional operations can easily be done using\ncompare_exchange_weak() in a loop, if required.\n The semantics closely match those of fetch_add() and fetch_sub() for\nstd::atomic<T*>; the named functions atomically perform their operation and\nreturn the old value, whereas the compound-assignment operators return the new\nvalue. Pre- and post- increment and decrement work as usual: ++x increments the vari-\nable and returns the new value, whereas x++ increments the variable and returns the\nold value. As you’ll be expecting, the result is a value of the associated integral type in\nboth cases.\n We’ve now looked at all the basic atomic types; all that remains is the generic\nstd::atomic<> primary class template rather than the specializations, so let’s look at\nthat next.\n5.2.6\nThe std::atomic<> primary class template\nThe presence of the primary template allows a user to create an atomic variant of a\nuser-defined type, in addition to the standard atomic types. Given a user-defined type\nUDT, std::atomic<UDT> provides the same interface as std::atomic<bool> (as\ndescribed in section 5.2.3), except that the bool parameters and return types that\nrelate to the stored value (rather than the success/failure result of the compare-\nexchange operations) are UDT instead. You can’t use just any user-defined type with\nstd::atomic<>, though; the type has to fulfill certain criteria. In order to use\nstd::atomic<UDT> for some user-defined type UDT,, this type must have a trivial copy-\nassignment operator. This means that the type must not have any virtual functions or\nvirtual base classes and must use the compiler-generated copy-assignment operator.\nNot only that, but every base class and non-static data member of a user-defined type\nmust also have a trivial copy-assignment operator. This permits the compiler to use\n\n\n139\nAtomic operations and types in C++\nmemcpy() or an equivalent operation for assignment operations, because there’s no\nuser-written code to run.\n Finally, it is worth noting that the compare-exchange operations do bitwise com-\nparison as if using memcmp, rather than using any comparison operator that may be\ndefined for UDT. If the type provides comparison operations that have different seman-\ntics, or the type has padding bits that do not participate in normal comparisons, then\nthis can lead to a compare-exchange operation failing, even though the values com-\npare equally.\n The reasoning behind these restrictions goes back to one of the guidelines from\nchapter 3: don’t pass pointers and references to protected data outside the scope of\nthe lock by passing them as arguments to user-supplied functions. In general, the\ncompiler isn’t going to be able to generate lock-free code for std::atomic<UDT>, so it\nwill have to use an internal lock for all the operations. If user-supplied copy-assignment\nor comparison operators were permitted, this would require passing a reference to\nthe protected data as an argument to a user-supplied function, violating the guideline.\nAlso, the library is entirely at liberty to use a single lock for all atomic operations that\nneed it, and allowing user-supplied functions to be called while holding that lock\nmight cause deadlock or cause other threads to block because a comparison opera-\ntion took a long time. Finally, these restrictions increase the chance that the compiler\nwill be able to make use of atomic instructions directly for std::atomic<UDT> (and\nmake a particular instantiation lock-free), because it can treat the user-defined type as\na set of raw bytes.\n Note that although you can use std::atomic<float> or std::atomic<double>,\nbecause the built-in floating point types do satisfy the criteria for use with memcpy and\nmemcmp, the behavior may be surprising in the case of compare_exchange_strong\n(compare_exchange_weak can always fail for arbitrary internal reasons, as described\npreviously). The operation may fail even though the old stored value was equal in\nvalue to the comparand, if the stored value had a different representation. Note that\nthere are no atomic arithmetic operations on floating-point values. You’ll get similar\nbehavior with compare_exchange_strong if you use std::atomic<> with a user-\ndefined type that has an equality-comparison operator defined, and that operator\ndiffers from the comparison using memcmp—the operation may fail because the\notherwise-equal values have a different representation.\n If your UDT is the same size as (or smaller than) an int or a void*, most common\nplatforms will be able to use atomic instructions for std::atomic<UDT>. Some plat-\nforms will also be able to use atomic instructions for user-defined types that are twice\nthe size of an int or void*. These platforms are typically those that support a so-called\ndouble-word-compare-and-swap (DWCAS) instruction corresponding to the compare_\nexchange_xxx functions. As you’ll see in chapter 7, such support can be helpful when\nwriting lock-free code.\n These restrictions mean that you can’t, for example, create std::atomic<std::\nvector<int>> (because it has a non-trivial copy constructor and copy assignment\n\n\n140\nCHAPTER 5\nThe C++ memory model and operations on atomic types\noperator), but you can instantiate std::atomic<> with classes containing counters or\nflags or pointers or even arrays of simple data elements. This isn’t particularly a prob-\nlem; the more complex the data structure, the more likely you’ll want to do opera-\ntions on it other than simple assignment and comparison. If that’s the case, you’re\nbetter off using an std::mutex to ensure that the data is appropriately protected for\nthe desired operations, as described in chapter 3.\n As already mentioned, when instantiated with a user-defined type T, the interface\nof std::atomic<T> is limited to the set of operations available for std::atomic<bool>:\nload(), store(), exchange(), compare_exchange_weak(), compare_exchange_strong(),\nand assignment from and conversion to an instance of type T.\n Table 5.3 shows the operations available on each atomic type.\n5.2.7\nFree functions for atomic operations\nUp until now I’ve limited myself to describing the member function forms of the\noperations on the atomic types. But there are also equivalent nonmember functions\nfor all the operations on the various atomic types. For the most part, the nonmember\nfunctions are named after the corresponding member functions but with an atomic_\nprefix (for example, std::atomic_load()). These functions are then overloaded for\nTable 5.3\nThe operations available on atomic types\nOperation\natomic_\nflag\natomic\n<bool>\natomic\n<T*>\natomic\n<integral\n-type>\natomic\n<other-\ntype>\ntest_and_set\nY\nclear\nY\nis_lock_free\nY\nY\nY\nY\nload\nY\nY\nY\nY\nstore\nY\nY\nY\nY\nexchange\nY\nY\nY\nY\ncompare_exchange\n_weak, compare_ \nexchange_strong\nY\nY\nY\nY\nfetch_add, +=\nY\nY\nfetch_sub, -=\nY\nY\nfetch_or, |=\nY\nfetch_and, &=\nY\nfetch_xor, ^=\nY\n++, --\nY\nY\n\n\n141\nAtomic operations and types in C++\neach of the atomic types. Where there’s opportunity for specifying a memory-ordering\ntag, they come in two varieties: one without the tag and one with an _explicit suffix\nand an additional parameter or parameters for the memory-ordering tag or tags (for\nexample, std::atomic_store(&atomic_var,new_value) versus std::atomic_store_\nexplicit(&atomic_var,new_value,std::memory_order_release). Whereas the atomic\nobject being referenced by the member functions is implicit, all the free functions\ntake a pointer to the atomic object as the first parameter.\n For example, std::atomic_is_lock_free() comes in one variety (though over-\nloaded for each type), and std::atomic_is_lock_free(&a) returns the same value as\na.is_lock_free() for an object of atomic type a. Likewise, std::atomic_load(&a) is\nthe same as a.load(), but the equivalent of a.load(std::memory_order_acquire) is\nstd::atomic_load_explicit(&a, std::memory_order_acquire).\n The free functions are designed to be C-compatible, so they use pointers rather\nthan references in all cases. For example, the first parameter of the compare_ex-\nchange_weak() and compare_exchange_strong() member functions (the expected\nvalue) is a reference, whereas the second parameter of std::atomic_compare_\nexchange_weak() (the first is the object pointer) is a pointer. std::atomic_compare\n_exchange_weak_explicit() also requires both the success and failure memory\norders to be specified, whereas the compare-exchange member functions have both a\nsingle memory order form (with a default of std::memory_order_seq_cst) and an\noverload that takes the success and failure memory orders separately.\n The operations on std::atomic_flag buck the trend in that they spell out the\nflag part in the names: std::atomic_flag_test_and_set(), std::atomic_flag_\nclear(). The additional variants that specify the memory ordering again have the\n_explicit suffix: std::atomic_flag_test_and_set_explicit() and std::atomic_\nflag_clear_explicit().\n The C++ Standard Library also provides free functions for accessing instances of\nstd::shared_ptr<> in an atomic fashion. This is a break from the principle that only\nthe atomic types support atomic operations, because std::shared_ptr<> is quite defi-\nnitely not an atomic type (accessing the same std::shared_ptr<T> object from multi-\nple threads without using the atomic access functions from all threads, or using\nsuitable other external synchronization, is a data race and undefined behavior). But\nthe C++ Standards Committee felt it was sufficiently important to provide these extra\nfunctions. The atomic operations available are load, store, exchange, and compare-\nexchange, which are provided as overloads of the same operations on the standard\natomic types, taking an std::shared_ptr<>* as the first argument:\nstd::shared_ptr<my_data> p;\nvoid process_global_data()\n{\n    std::shared_ptr<my_data> local=std::atomic_load(&p);\n    process_data(local);\n}\n\n\n142\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nvoid update_global_data()\n{\n    std::shared_ptr<my_data> local(new my_data);\n    std::atomic_store(&p,local);\n}\nAs with the atomic operations on other types, the _explicit variants are also pro-\nvided to allow you to specify the desired memory ordering, and the std::atomic_\nis_lock_free() function can be used to check whether the implementation uses\nlocks to ensure the atomicity.\n The Concurrency TS also provides std::experimental::atomic_shared_ptr<T>,\nwhich is an atomic type. To use it you must include the <experimental/atomic>\nheader. It provides the same set of operations as std::atomic<UDT>: load, store,\nexchange, compare-exchange. It is provided as a separate type because that allows for\na lock-free implementation that does not impose an additional cost on plain\nstd::shared_ptr instances. But as with the std::atomic template, you still need to\ncheck whether it is lock-free on your platform, which can be tested with the is_lock_\nfree member function. Even if it is not lock-free, std::experimental::atomic_\nshared_ptr is to be recommended over using the atomic free functions on a plain\nstd::shared_ptr, as it is clearer in your code, ensures that all accesses are atomic,\nand avoids the potential for data races due to forgetting to use the atomic free func-\ntions. As with all uses of atomic types and operations, if you are using them for a\npotential speed gain, it is important to profile, and compare with using alternative syn-\nchronization mechanisms.\n As described in the introduction, the standard atomic types do more than avoid\nthe undefined behavior associated with a data race; they allow the user to enforce an\nordering of operations between threads. This enforced ordering is the basis of the\nfacilities for protecting data and synchronizing operations such as std::mutex and\nstd::future<>. With that in mind, let’s move on to the real meat of this chapter: the\ndetails of the concurrency aspects of the memory model and how atomic operations\ncan be used to synchronize data and enforce ordering.\n5.3\nSynchronizing operations and enforcing ordering\nSuppose you have two threads, one of which is populating a data structure to be read\nby the second. In order to avoid a problematic race condition, the first thread sets a\nflag to indicate that the data is ready, and the second thread doesn’t read the data\nuntil the flag is set. The following listing shows such a scenario.\n#include <vector>\n#include <atomic>\n#include <iostream>\nstd::vector<int> data;\nstd::atomic<bool> data_ready(false);\nListing 5.2\nReading and writing variables from different threads\n\n\n143\nSynchronizing operations and enforcing ordering\nvoid reader_thread()\n{\n    while(!data_ready.load())   \n    {\n        std::this_thread::sleep(std::chrono::milliseconds(1));\n    }\n    std::cout<<”The answer=”<<data[0]<<”\\n”;    \n}\nvoid writer_thread()\n{\n    data.push_back(42);     \n    data_ready=true;     \n}\nSetting aside the inefficiency of the loop waiting for the data to be ready B, you\nneed this to work, because otherwise sharing data between threads becomes impracti-\ncal: every item of data is forced to be atomic. You’ve already learned that it’s unde-\nfined behavior to have non-atomic reads c and writes d accessing the same data\nwithout an enforced ordering, so for this to work there must be an enforced ordering\nsomewhere.\n The required enforced ordering comes from the operations on the std::\natomic<bool> variable, data_ready;, they provide the necessary ordering by virtue of\nthe memory model relations happens-before and synchronizes-with. The write of the data\nd happens before the write to the data_ready flag e, and the read of the flag B\nhappens before the read of the data c. When the value read from data_ready B is\ntrue, the write synchronizes with that read, creating a happens-before relationship.\nBecause happens-before is transitive, the write to the data d happens before the write\nto the flag e, which happens before the read of the true value from the flag B,\nwhich happens before the read of the data c, and you have an enforced ordering: the\nwrite of the data happens before the read of the data and everything is OK. Figure 5.2\nshows the important happens-before relationships in the two threads. I’ve added a\ncouple of iterations of the while loop from the reader thread.\n All this might seem fairly intuitive: the operation that writes a value happens before\nan operation that reads that value. With the default atomic operations, that’s indeed\ntrue (which is why this is the default), but it does need spelling out: the atomic opera-\ntions also have other options for the ordering requirements, which I’ll come to shortly.\n Now that you’ve seen happens-before and synchronizes-with in action, it’s time to\nlook at what they mean. I’ll start with synchronizes-with.\n5.3.1\nThe synchronizes-with relationship\nThe synchronizes-with relationship is something that you can get only between opera-\ntions on atomic types. Operations on a data structure (such as locking a mutex) might\nprovide this relationship if the data structure contains atomic types and the opera-\ntions on that data structure perform the appropriate atomic operations internally, but\nfundamentally it comes only from operations on atomic types.\nb\nc\nd\ne\n\n\n144\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nThe basic idea is this: a suitably-tagged atomic write operation, W, on a variable, x, syn-\nchronizes with a suitably-tagged atomic read operation on x that reads the value stored\nby either that write, W, or a subsequent atomic write operation on x by the same thread\nthat performed the initial write, W, or a sequence of atomic read-modify-write operations\non x (such as fetch_add() or compare_exchange_weak()) by any thread, where the\nvalue read by the first thread in the sequence is the value written by W (see section 5.3.4). \n Leave the “suitably-tagged” part aside for now, because all operations on atomic\ntypes are suitably tagged by default. This means what you might expect: if thread A\nstores a value and thread B reads that value, there’s a synchronizes-with relationship\nbetween the store in thread A and the load in thread B, as in listing 5.2. This is illus-\ntrated in figure 5.2.\n As I’m sure you’ve guessed, the nuances are all in the “suitably-tagged” part. The\nC++ memory model allows various ordering constraints to be applied to the opera-\ntions on atomic types, and this is the tagging to which I refer. The various options for\nmemory ordering and how they relate to the synchronizes-with relationship are cov-\nered in section 5.3.3. First, let’s step back and look at the happens-before relationship.\ndata.push_back(42)\ndata_ready=true\ndata_ready.load()\n(returns\n)\nfalse\ndata_ready.load()\n(returns\n)\nfalse\ndata_ready.load()\n(returns\n)\ntrue\ndata[0]\n(returns 42)\nWriter thread\nReader thread\nFigure 5.2\nEnforcing an ordering between non-atomic operations using atomic operations\n",
      "page_number": 155
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 168-175)",
      "start_page": 168,
      "end_page": 175,
      "detection_method": "topic_boundary",
      "content": "145\nSynchronizing operations and enforcing ordering\n5.3.2\nThe happens-before relationship\nThe happens-before and strongly-happens-before relationships are the basic building blocks\nof operation ordering in a program; it specifies which operations see the effects of\nwhich other operations. For a single thread, it’s largely straightforward: if one opera-\ntion is sequenced before another, then it also happens before it, and strongly-happens-\nbefore it. This means that if one operation (A) occurs in a statement prior to another\n(B) in the source code, then A happens before B, and A strongly-happens-before B.\nYou saw that in listing 5.2: the write to data d happens before the write to\ndata_ready e. If the operations occur in the same statement, in general there’s no\nhappens-before relationship between them, because they’re unordered. This is\nanother way of saying that the ordering is unspecified. You know that the program in\nthe following listing will output “1,2” or “2,1”, but it’s unspecified which, because\nthe order of the two calls to get_num()is unspecified.\n#include <iostream>\nvoid foo(int a,int b)\n{\n    std::cout<<a<<”,”<<b<<std::endl;\n}\nint get_num()\n{\n    static int i=0;\n    return ++i;\n}\nint main()\n{\n    foo(get_num(),get_num());   \n}\nThere are circumstances where operations within a single statement are sequenced,\nsuch as where the built-in comma operator is used or where the result of one expres-\nsion is used as an argument to another expression. But in general, operations within a\nsingle statement are nonsequenced, and there’s no sequenced-before (and thus no\nhappens-before) relationship between them. All operations in a statement happen\nbefore all of the operations in the next statement.\n This is a restatement of the single-threaded sequencing rules you’re used to, so\nwhat’s new? The new part is the interaction between threads: if operation A on one\nthread inter-thread happens before operation B on another thread, then A happens\nbefore B. This doesn’t help much: you’ve added a new relationship (inter-thread\nhappens-before), but this is an important relationship when you’re writing multi-\nthreaded code.\n At the basic level, inter-thread happens-before is relatively simple and relies on the\nsynchronizes-with relationship introduced in section 5.3.1: if operation A in one\nthread synchronizes with operation B in another thread, then A inter-thread happens\nListing 5.3\nOrder of evaluation of arguments to a function call is unspecified\nCalls to get_num() \nare unordered.\n\n\n146\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nbefore B. It’s also a transitive relation: if A inter-thread happens before B and B inter-\nthread happens before C, then A inter-thread happens before C. You saw this in list-\ning 5.2 as well.\n Inter-thread happens-before also combines with the sequenced-before relation: if\noperation A is sequenced before operation B, and operation B inter-thread happens\nbefore operation C, then A inter-thread happens before C. Similarly, if A synchronizes\nwith B and B is sequenced before C, then A inter-thread happens before C. These two\ntogether mean that if you make a series of changes to data in a single thread, you need\nonly one synchronizes-with relationship for the data to be visible to subsequent opera-\ntions on the thread that executed C.\n The strongly-happens-before relationship is slightly different, but in most cases\ncomes down the same. The same two rules described above apply: if operation A\nsynchronizes-with operation B, or operation A is sequenced-before operation B, then\nA strongly-happens-before B. Transitive ordering also applies: if A strongly-happens-\nbefore B, and B strongly-happens-before C, then A strongly-happens-before C. The\ndifference is that operations tagged with memory_order_consume (see section 5.3.3)\nparticipate in inter-thread-happens-before relationships (and thus happens-before\nrelationships), but not in strongly-happens-before relationships. Since the vast major-\nity of code should not be using memory_order_consume, this distinction is unlikely to\naffect you in practice. I will use “happens-before” in the rest of this book for brevity.\n These are the crucial rules that enforce the ordering of operations between\nthreads and make everything in listing 5.2 work. There are some additional nuances\nwith data dependency, as you’ll see shortly. In order for you to understand this, I need\nto cover the memory-ordering tags used for atomic operations and how they relate to\nthe synchronizes-with relation.\n5.3.3\nMemory ordering for atomic operations\nThere are six memory ordering options that can be applied to operations on atomic\ntypes: memory_order_relaxed, memory_order_consume, memory_order_acquire, memory\n_order_release, memory_order_acq_rel, and memory_order_seq_cst. Unless you\nspecify otherwise for a particular operation, the memory-ordering option for all oper-\nations on atomic types is memory_order_seq_cst, which is the most stringent of the\navailable options. Although there are six ordering options, they represent three mod-\nels: sequentially consistent ordering (memory_order_seq_cst), acquire-release ordering\n(memory_order_consume, memory_order_acquire, memory_order_release, and memo-\nry_order_acq_rel), and relaxed ordering (memory_order_relaxed).\n These distinct memory-ordering models can have varying costs on different CPU\narchitectures. For example, on systems based on architectures with fine control over\nthe visibility of operations by processors other than the one that made the change,\nadditional synchronization instructions can be required for sequentially consistent\nordering over acquire-release ordering or relaxed ordering and for acquire-release\nordering over relaxed ordering. If these systems have many processors, these additional\n\n\n147\nSynchronizing operations and enforcing ordering\nsynchronization instructions may take a significant amount of time, reducing the overall\nperformance of the system. On the other hand, CPUs that use the x86 or x8664 archi-\ntectures (such as the Intel and AMD processors common in desktop PCs) don’t require\nany additional instructions for acquire-release ordering beyond those necessary for\nensuring atomicity, and even sequentially-consistent ordering doesn’t require any spe-\ncial treatment for load operations, although there’s a small additional cost on stores.\n The availability of the distinct memory-ordering models allows experts to take\nadvantage of the increased performance of the more fine-grained ordering relation-\nships where they’re advantageous while allowing the use of the default sequentially-\nconsistent ordering (which is considerably easier to reason about than the others) for\nthose cases that are less critical.\n In order to choose which ordering model to use, or to understand the ordering\nrelationships in code that uses the different models, it’s important to know how the\nchoices affect the program behavior. Let’s therefore look at the consequences of each\nchoice for operation ordering and synchronizes-with.\nSEQUENTIALLY CONSISTENT ORDERING\nThe default ordering is named sequentially consistent because it implies that the behav-\nior of the program is consistent with a simple sequential view of the world. If all oper-\nations on instances of atomic types are sequentially consistent, the behavior of a\nmultithreaded program is as if all these operations were performed in some particular\nsequence by a single thread. This is by far the easiest memory ordering to understand,\nwhich is why it’s the default: all threads must see the same order of operations. This\nmakes it easy to reason about the behavior of code written with atomic variables. You\ncan write down all the possible sequences of operations by different threads, eliminate\nthose that are inconsistent, and verify that your code behaves as expected in the oth-\ners. It also means that operations can’t be reordered; if your code has one operation\nbefore another in one thread, that ordering must be seen by all other threads.\n From the point of view of synchronization, a sequentially consistent store synchro-\nnizes with a sequentially consistent load of the same variable that reads the value\nstored. This provides one ordering constraint on the operation of two (or more)\nthreads, but sequential consistency is more powerful than that. Any sequentially con-\nsistent atomic operations done after that load must also appear after the store to other\nthreads in the system using sequentially consistent atomic operations. The example in\nlisting 5.4 demonstrates this ordering constraint in action. This constraint doesn’t\ncarry forward to threads that use atomic operations with relaxed memory orderings;\nthey can still see the operations in a different order, so you must use sequentially con-\nsistent operations on all your threads in order to get the benefit.\n This ease of understanding can come at a price, though. On a weakly-ordered\nmachine with many processors, it can impose a noticeable performance penalty,\nbecause the overall sequence of operations must be kept consistent between the proces-\nsors, possibly requiring extensive (and expensive!) synchronization operations between\nthe processors. That said, some processor architectures (such as the common x86 and\n\n\n148\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nx86-64 architectures) offer sequential consistency relatively cheaply, so if you’re con-\ncerned about the performance implications of using sequentially consistent ordering,\ncheck the documentation for your target processor architectures.\n The following listing shows sequential consistency in action. The loads and stores\nto x and y are explicitly tagged with memory_order_seq_cst, although this tag could\nbe omitted in this case because it’s the default. \n#include <atomic>\n#include <thread>\n#include <assert.h>\nstd::atomic<bool> x,y;\nstd::atomic<int> z;\nvoid write_x()\n{\n    x.store(true,std::memory_order_seq_cst);  \n}\nvoid write_y()\n{\n    y.store(true,std::memory_order_seq_cst);  \n}\nvoid read_x_then_y()\n{\n    while(!x.load(std::memory_order_seq_cst));\n    if(y.load(std::memory_order_seq_cst))     \n        ++z;\n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_seq_cst));\n    if(x.load(std::memory_order_seq_cst))     \n        ++z;\n}\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x);\n    std::thread b(write_y);\n    std::thread c(read_x_then_y);\n    std::thread d(read_y_then_x);\n    a.join();\n    b.join();\n    c.join();\n    d.join();\n    assert(z.load()!=0);   \n}\nThe assert f can never fire, because either the store to x B or the store to y c must\nhappen first, even though it’s not specified which. If the load of y in read_x_then_y d\nListing 5.4\nSequential consistency implies a total ordering\nb\nc\nd\ne\nf\n\n\n149\nSynchronizing operations and enforcing ordering\nreturns false, the store to x must occur before the store to y, in which case the load of\nx in read_y_then_x e must return true, because the while loop ensures that the y is\ntrue at this point. Because the semantics of memory_order_seq_cst require a single\ntotal ordering over all operations tagged memory_order_seq_cst, there’s an implied\nordering relationship between a load of y that returns false d and the store to y B.\nFor there to be a single total order, if one thread sees x==true and then subse-\nquently sees y==false, this implies that the store to x occurs before the store to y in\nthis total order.\n Because everything is symmetrical, it could also happen the other way around, with\nthe load of x e returning false, forcing the load of y d to return true. In both\ncases, z is equal to 1. Both loads can return true, leading to z being 2, but under no\ncircumstances can z be 0.\n The operations and happens-before relationships for the case that read_x_then_y\nsees x as true and y as false are shown in figure 5.3. The dashed line from the load of\ny in read_x_then_y to the store to y in write_y shows the implied ordering relation-\nship required in order to maintain sequential consistency: the load must occur before\nthe store in the global order of memory_order_seq_cst operations in order to achieve\nthe outcomes given here.\nSequential consistency is the most straightforward and intuitive ordering, but it’s also\nthe most expensive memory ordering because it requires global synchronization\nbetween all threads. On a multiprocessor system this may require extensive and time-\nconsuming communication between processors.\n In order to avoid this synchronization cost, you need to step outside the world of\nsequential consistency and consider using other memory orderings.\nNON-SEQUENTIALLY CONSISTENT MEMORY ORDERINGS\nOnce you step outside the nice sequentially-consistent world, things start to get com-\nplicated. The single biggest issue to get to grips with is probably the fact that there’s no\ny.store(true)\nInitially\n,\nx=false y=false\nx.store(true)\nx.load()\nreturns true\ny.load()\nreturns false\ny.load()\nreturns true\nx.load()\nreturns true\nwrite_x\nread_x_then_y\nread_y_then_x\nwrite_y\nFigure 5.3\nSequential consistency and happens-before\n\n\n150\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nlonger a single global order of events. This means that different threads can see different\nviews of the same operations, and any mental model you have of operations from dif-\nferent threads neatly interleaved one after the other must be thrown away. Not only\ndo you have to account for things happening truly concurrently, but threads don’t have\nto agree on the order of events. In order to write (or even to understand) any code that\nuses a memory ordering other than the default memory_order_seq_cst, it’s absolutely\nvital to get your head around this. It’s not just that the compiler can reorder the\ninstructions. Even if the threads are running the same bit of code, they can disagree\non the order of events because of operations in other threads in the absence of\nexplicit ordering constraints, because the different CPU caches and internal buffers\ncan hold different values for the same memory. It’s so important I’ll say it again:\nthreads don’t have to agree on the order of events.\n Not only do you have to throw out mental models based on interleaving opera-\ntions, you also have to throw out mental models based on the idea of the compiler or\nprocessor reordering the instructions. In the absence of other ordering constraints, the only\nrequirement is that all threads agree on the modification order of each individual variable. Oper-\nations on distinct variables can appear in different orders on different threads, pro-\nvided the values seen are consistent with any additional ordering constraints imposed.\n This is best demonstrated by stepping completely outside the sequentially consis-\ntent world and using memory_order_relaxed for all operations. Once you’ve come to\ngrips with that, you can move back to acquire-release ordering, which allows you to\nselectively introduce ordering relationships between operations and claw back some\nof your sanity.\nRELAXED ORDERING\nOperations on atomic types performed with relaxed ordering don’t participate in\nsynchronizes-with relationships. Operations on the same variable within a single thread\nstill obey happens-before relationships, but there’s almost no requirement on order-\ning relative to other threads. The only requirement is that accesses to a single atomic\nvariable from the same thread can’t be reordered; once a given thread has seen a par-\nticular value of an atomic variable, a subsequent read by that thread can’t retrieve\nan earlier value of the variable. Without any additional synchronization, the modifi-\ncation order of each variable is the only thing shared between threads that are using\nmemory_order_relaxed. \n To demonstrate how relaxed your relaxed operations can be, you need only two\nthreads, as shown in the following listing.\n#include <atomic>\n#include <thread>\n#include <assert.h>\nstd::atomic<bool> x,y;\nstd::atomic<int> z;\nvoid write_x_then_y()\nListing 5.5\nRelaxed operations have few ordering requirements\n\n\n151\nSynchronizing operations and enforcing ordering\n{\n    x.store(true,std::memory_order_relaxed);   \n    y.store(true,std::memory_order_relaxed);  \n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_relaxed));   \n    if(x.load(std::memory_order_relaxed))     \n        ++z;\n}\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x_then_y);\n    std::thread b(read_y_then_x);\n    a.join();\n    b.join();\n    assert(z.load()!=0);    \n}\nThis time the assert f can fire, because the load of x e can read false, even though\nthe load of y d reads true and the store of x B happens before the store of y c. x\nand y are different variables, so there are no ordering guarantees relating to the visi-\nbility of values arising from operations on each.\n Relaxed operations on different variables can be freely reordered provided they\nobey any happens-before relationships they’re bound by (for example, within the\nsame thread). They don’t introduce synchronizes-with relationships. The happens-\nbefore relationships from listing 5.5 are shown in figure 5.4, along with a possible out-\ncome. Even though there’s a happens-before relationship between the stores and\nb\nc\nd\ne\nf\nInitially\n,\nx=false y=false\nx.store(true,\nrelaxed)\ny.load(relaxed)\nreturns true\nx.load(relaxed)\nreturns false\nwrite_x_then_y\nread_y_then_x\ny.store(true,\nrelaxed)\nFigure 5.4\nRelaxed atomics \nand happens-before\n\n\n152\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nbetween the loads, there isn’t one between either store and either load, and so the\nloads can see the stores out of order.\n Let’s look at the slightly more complex example with three variables and five\nthreads in the next listing.\n#include <thread>\n#include <atomic>\n#include <iostream>\nstd::atomic<int> x(0),y(0),z(0);   \nstd::atomic<bool> go(false);    \nunsigned const loop_count=10;\nstruct read_values\n{\n    int x,y,z;\n};\nread_values values1[loop_count];\nread_values values2[loop_count];\nread_values values3[loop_count];\nread_values values4[loop_count];\nread_values values5[loop_count];\nvoid increment(std::atomic<int>* var_to_inc,read_values* values)\n{\n    while(!go)                    \n        std::this_thread::yield();\n    for(unsigned i=0;i<loop_count;++i)\n    {\n        values[i].x=x.load(std::memory_order_relaxed);\n        values[i].y=y.load(std::memory_order_relaxed);\n        values[i].z=z.load(std::memory_order_relaxed);\n        var_to_inc->store(i+1,std::memory_order_relaxed);  \n        std::this_thread::yield();\n    }\n}\nvoid read_vals(read_values* values)\n{\n    while(!go)                      \n        std::this_thread::yield();\n    for(unsigned i=0;i<loop_count;++i)\n    {\n        values[i].x=x.load(std::memory_order_relaxed);\n        values[i].y=y.load(std::memory_order_relaxed);\n        values[i].z=z.load(std::memory_order_relaxed);\n        std::this_thread::yield();\n    }\n}\nvoid print(read_values* v)\n{\n    for(unsigned i=0;i<loop_count;++i)\n    {\n        if(i)\n            std::cout<<\",\";\nListing 5.6\nRelaxed operations on multiple threads\nB\nc\nSpin, waiting \nfor the signal\nd\ne\nSpin, waiting \nfor the signal\nf\n",
      "page_number": 168
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 176-183)",
      "start_page": 176,
      "end_page": 183,
      "detection_method": "topic_boundary",
      "content": "153\nSynchronizing operations and enforcing ordering\n        std::cout<<\"(\"<<v[i].x<<\",\"<<v[i].y<<\",\"<<v[i].z<<\")\";\n    }\n    std::cout<<std::endl;\n}\nint main()\n{\n    std::thread t1(increment,&x,values1);\n    std::thread t2(increment,&y,values2);\n    std::thread t3(increment,&z,values3);\n    std::thread t4(read_vals,values4);\n    std::thread t5(read_vals,values5);\n    go=true;          \n    t5.join();\n    t4.join();\n    t3.join();\n    t2.join();\n    t1.join();\n    print(values1);    \n    print(values2);\n    print(values3);\n    print(values4);\n    print(values5);\n} \nThis is a simple program. You have three shared global atomic variables B and five\nthreads. Each thread loops 10 times, reading the values of the three atomic variables\nusing memory_order_relaxed and storing them in an array. Three of the threads each\nupdate one of the atomic variables each time through the loop e, whereas the other\ntwo threads read. Once all the threads have been joined, you print the values from the\narrays stored by each thread h.\n The go atomic variable c is used to ensure that the threads all start the loop as\nnear to the same time as possible. Launching a thread is an expensive operation, and\nwithout the explicit delay, the first thread may be finished before the last one has\nstarted. Each thread waits for go to become true before entering the main loop d\nand f, and go is set to true only once all the threads have started g.\n One possible output from this program is as follows:\n(0,0,0),(1,0,0),(2,0,0),(3,0,0),(4,0,0),(5,7,0),(6,7,8),(7,9,8),(8,9,8),\n(9,9,10)\n(0,0,0),(0,1,0),(0,2,0),(1,3,5),(8,4,5),(8,5,5),(8,6,6),(8,7,9),(10,8,9),\n(10,9,10)\n(0,0,0),(0,0,1),(0,0,2),(0,0,3),(0,0,4),(0,0,5),(0,0,6),(0,0,7),(0,0,8),\n(0,0,9)\n(1,3,0),(2,3,0),(2,4,1),(3,6,4),(3,9,5),(5,10,6),(5,10,8),(5,10,10),\n(9,10,10),(10,10,10)\n(0,0,0),(0,0,0),(0,0,0),(6,3,7),(6,5,7),(7,7,7),(7,8,7),(8,8,7),(8,8,9),\n(8,8,9)\nThe first three lines are the threads doing the updating, and the last two are the\nthreads doing the reading. Each triplet is a set of the variables x, y, and z, in that\nSignal to start \nexecution of main loop\ng\nPrints the \nfinal values\nh\n\n\n154\nCHAPTER 5\nThe C++ memory model and operations on atomic types\norder, from one pass through the loop. There are a few things to notice from\nthis output:\nThe first set of values shows x increasing by one with each triplet, the second set\nhas y increasing by one, and the third has z increasing by one.\nThe x elements of each triplet only increase within a given set, as do the y and z\nelements, but the increments are uneven, and the relative orderings vary\nbetween all threads.\nThread 3 doesn’t see any of the updates to x or y; it sees only the updates it\nmakes to z. This doesn’t stop the other threads from seeing the updates to z\nmixed in with the updates to x and y, though.\nThis is a valid outcome for relaxed operations, but it’s not the only valid outcome. Any\nset of values that’s consistent with the three variables, each holding the values 0 to 10\nin turn, and that has the thread incrementing a given variable printing the values 0 to\n9 for that variable, is valid.\nUNDERSTANDING RELAXED ORDERING\nTo understand how this works, imagine that each variable is a man in a cubicle with a\nnotepad. On his notepad is a list of values. You can phone him and ask him to give you\na value, or you can tell him to write down a new value. If you tell him to write down a\nnew value, he writes it at the bottom of the list. If you ask him for a value, he reads you\na number from the list. \n The first time you talk to this man, if you ask him for a value, he may give you any\nvalue from the list he has on his pad at the time. If you then ask him for another value,\nhe may give you the same one again or a value from farther down the list. He’ll never\ngive you a value from farther up the list. If you tell him to write down a number and\nthen subsequently ask him for a value, he’ll give you either the number you told him\nto write down or a number below that on the list.\n Imagine for a moment that his list starts with the values 5, 10, 23, 3, 1, and 2. If you\nask for a value, you could get any of those. If he gives you 10, then the next time you ask\nhe could give you 10 again, or any of the later ones, but not 5. If you call him five\ntimes, he could say “10, 10, 1, 2, 2,” for example. If you tell him to write down 42, he’ll\nadd it to the end of the list. If you ask him for a number again, he’ll keep telling you\n“42” until he has another number on his list and he feels like telling it to you.\n Now, imagine your friend Carl also has this man’s number. Carl can also phone\nhim and either ask him to write down a number or ask for one, and he applies the\nsame rules to Carl as he does to you. He has only one phone, so he can only deal with\none of you at a time, so the list on his pad is a nice straightforward list. But just\nbecause you got him to write down a new number doesn’t mean he has to tell it to\nCarl, and vice versa. If Carl asked him for a number and was told “23,” then just\nbecause you asked the man to write down 42 doesn’t mean he’ll tell that to Carl next\ntime. He may tell Carl any of the numbers 23, 3, 1, 2, 42, or even the 67 that Fred told\nhim to write down after you called. He could very well tell Carl “23, 3, 3, 1, 67” without\n\n\n155\nSynchronizing operations and enforcing ordering\nbeing inconsistent with what he told you. It’s like he keeps track of which number he\ntold to whom with a little moveable sticky note for each person, like in figure 5.5.\nNow imagine that there’s not just one man in a cubicle but a whole cubicle farm, with\nloads of men with phones and notepads. These are all our atomic variables. Each vari-\nable has its own modification order (the list of values on the pad), but there’s no rela-\ntionship between them at all. If each caller (you, Carl, Anne, Dave, and Fred) is a\nthread, then this is what you get when every operation uses memory_order_relaxed.\nThere are a few additional things you can tell the man in the cubicle, such as “Write\ndown this number, and tell me what was at the bottom of the list” (exchange) and\n“Write down this number if the number on the bottom of the list is that; otherwise tell\nme what I should have guessed” (compare_exchange_strong), but that doesn’t affect\nthe general principle.\n If you think about the program logic from listing 5.5, then write_x_then_y is\nlike some guy calling up the man in cubicle x and telling him to write true, then\ncalling up the man in cubicle y and telling him to write true. The thread running\nread_y_then_x repeatedly calls up the man in cubicle y asking for a value until he says\ntrue and then calls the man in cubicle x to ask for a value. The man in cubicle x is\nunder no obligation to tell you any specific value off his list and is quite within his\nrights to say false.\n This makes relaxed atomic operations difficult to deal with. They must be used in\ncombination with atomic operations that feature stronger ordering semantics in order\nto be useful for inter-thread synchronization. I strongly recommend avoiding relaxed\natomic operations unless they’re absolutely necessary, and even then using them only\nwith extreme caution. Given the unintuitive results that can be achieved with only two\nthreads and two variables in listing 5.5, it’s not hard to imagine the possible complex-\nity when more threads and more variables are involved.\n One way to achieve additional synchronization without the overhead of full-blown\nsequential consistency is to use acquire-release ordering.\nACQUIRE-RELEASE ORDERING\nAcquire-release ordering is a step up from relaxed ordering; there’s still no total order\nof operations, but it does introduce some synchronization. Under this ordering model,\natomic loads are acquire operations (memory_order_acquire), atomic stores are release\noperations (memory_order_release), and atomic read-modify-write operations (such\nFigure 5.5\nThe notebook for \nthe man in the cubicle\n\n\n156\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nas fetch_add() or exchange()) are either acquire, release, or both (memory_order_\nacq_rel). Synchronization is pairwise between the thread that does the release and\nthe thread that does the acquire. A release operation synchronizes-with an acquire operation\nthat reads the value written. This means that different threads can still see different\norderings, but these orderings are restricted. The following listing is a reworking of\nlisting 5.4 using acquire-release semantics rather than sequentially-consistent ones.\n#include <atomic>\n#include <thread>\n#include <assert.h>\nstd::atomic<bool> x,y;\nstd::atomic<int> z;\nvoid write_x()\n{\n    x.store(true,std::memory_order_release);\n}\nvoid write_y()\n{\n    y.store(true,std::memory_order_release);\n}\nvoid read_x_then_y()\n{\n    while(!x.load(std::memory_order_acquire));\n    if(y.load(std::memory_order_acquire))     \n        ++z;\n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_acquire));\n    if(x.load(std::memory_order_acquire))     \n        ++z;\n}\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x);\n    std::thread b(write_y);\n    std::thread c(read_x_then_y);\n    std::thread d(read_y_then_x);\n    a.join();\n    b.join();\n    c.join();\n    d.join();\n    assert(z.load()!=0);   \n}\nIn this case the assert d can fire (like in the relaxed-ordering case), because it’s possi-\nble for both the load of x c and the load of y B to read false. x and y are written by\nListing 5.7\nAcquire-release doesn’t imply a total ordering\nb\nc\nd\n\n\n157\nSynchronizing operations and enforcing ordering\ndifferent threads, so the ordering from the release to the acquire in each case has no\neffect on the operations in the other threads.\n Figure 5.6 shows the happens-before relationships from listing 5.7, along with a\npossible outcome where the two reading threads each have a different view of the\nworld. This is possible because there’s no happens-before relationship to force an\nordering, as described previously.\nIn order to see the benefit of acquire-release ordering, you need to consider two\nstores from the same thread, like in listing 5.5. If you change the store to y to use mem-\nory_order_release and the load from y to use memory_order_acquire like in the fol-\nlowing listing, then you impose an ordering on the operations on x.\n#include <atomic>\n#include <thread>\n#include <assert.h>\nstd::atomic<bool> x,y;\nstd::atomic<int> z;\nvoid write_x_then_y()\n{\n    x.store(true,std::memory_order_relaxed);   \n    y.store(true,std::memory_order_release);   \n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_acquire));   \n    if(x.load(std::memory_order_relaxed))     \n        ++z;\n}\nListing 5.8\nAcquire-release operations can impose ordering on relaxed operations\ny.store(true,\nrelease)\nInitially\n,\nx=false y=false\nx.store(true,\nrelease)\nx.load(acquire)\nreturns true\ny.load(acquire)\nreturns false\ny.load(acquire)\nreturns true\nx.load(acquire)\nreturns false\nwrite_x\nread_x_then_y\nread_y_then_x\nwrite_y\nFigure 5.6\nAcquire-release and happens-before\nB\nc\nSpin, waiting for y \nto be set to true\nd\ne\n\n\n158\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x_then_y);\n    std::thread b(read_y_then_x);\n    a.join();\n    b.join();\n    assert(z.load()!=0);    \n}\nEventually, the load from y, d will see true as written by the store c. Because the\nstore uses memory_order_release and the load uses memory_order_acquire, the store\nsynchronizes with the load. The store to x B happens before the store to y c because\nthey’re in the same thread. Because the store to y synchronizes with the load from y,\nthe store to x also happens before the load from y and by extension happens before\nthe load from x e. Thus, the load from x must read true, and the assert f can’t fire.\nIf the load from y wasn’t in a while loop, this wouldn’t necessarily be the case; the\nload from y might read false, in which case there’d be no requirement on the value\nread from x. In order to provide any synchronization, acquire and release operations\nmust be paired up. The value stored by a release operation must be seen by an acquire\noperation for either to have any effect. If either the store at c or the load at d was a\nrelaxed operation, there’d be no ordering on the accesses to x, so there’d be no guar-\nantee that the load at e would read true, and the assert could fire.\n You can still think about acquire-release ordering in terms of our men with note-\npads in their cubicles, but you have to add more to the model. First, imagine that\nevery store that’s done is part of some batch of updates, so when you call a man to tell\nhim to write down a number, you also tell him which batch this update is part of:\n“Please write down 99, as part of batch 423.” For the last store in a batch, you tell this\nto the man too: “Please write down 147, which is the last store in batch 423.” The\nman in the cubicle will then duly write down this information, along with who gave\nhim the value. This models a store-release operation. The next time you tell some-\none to write down a value, you increase the batch number: “Please write down 41, as\npart of batch 424.”\n When you ask for a value, you now have a choice: you can either ask for a value\n(which is a relaxed load), in which case the man only gives you the number, or you\ncan ask for a value and information about whether it’s the last in a batch (which mod-\nels a load-acquire). If you ask for the batch information, and the value wasn’t the last\nin a batch, the man will tell you something like, “The number is 987, which is a ‘normal’\nvalue,” whereas if it was the last in a batch, he’ll tell you something like “The number is\n987, which is the last number in batch 956 from Anne.” Now, here’s where the acquire-\nrelease semantics kick in: if you tell the man all the batches you know about when you\nask for a value, he’ll look down his list for the last value from any of the batches you\nknow about and either give you that number or one further down the list.\nf\n\n\n159\nSynchronizing operations and enforcing ordering\n How does this model acquire-release semantics? Let’s look at our example and see.\nFirst off, thread a is running write_x_then_y and says to the man in cubicle x,\n“Please write true as part of batch 1 from thread a,” which he duly writes down.\nThread a then says to the man in cubicle y, “Please write true as the last write of\nbatch 1 from thread a,” which he duly writes down. In the meantime, thread b is\nrunning read_y_then_x. Thread b keeps asking the man in box y for a value with\nbatch information until he says “true.” It may have to ask many times, but eventually\nthe man will say “true.” The man in box y doesn’t only say “true” though; he also says,\n“This is the last write in batch 1 from thread a.” \n Now, thread b goes on to ask the man in box x for a value, but this time it says,\n“Please can I have a value, and by the way I know about batch 1 from thread a.” Now\nthe man from cubicle x has to look down his list for the last mention of batch 1 from\nthread a. The only mention he has is the value true, which is also the last value on his\nlist, so he must read out that value; otherwise, he’s breaking the rules of the game.\n If you look at the definition of inter-thread happens-before back in section 5.3.2,\none of the important properties is that it’s transitive: if A inter-thread happens before B\nand B inter-thread happens before C, then A inter-thread happens before C. This means that\nacquire-release ordering can be used to synchronize data across several threads, even\nwhen the “intermediate” threads haven’t touched the data.\nTRANSITIVE SYNCHRONIZATION WITH ACQUIRE-RELEASE ORDERING\nIn order to think about transitive ordering, you need at least three threads. The first\nthread modifies some shared variables and does a store-release to one of them. A sec-\nond thread then reads the variable subject to the store-release with a load-acquire and\nperforms a store-release on a second shared variable. Finally, a third thread does a\nload-acquire on that second shared variable. Provided that the load-acquire opera-\ntions see the values written by the store-release operations to ensure the synchronizes-\nwith relationships, this third thread can read the values of the other variables stored\nby the first thread, even if the intermediate thread didn’t touch any of them. This sce-\nnario is shown in the next listing.\nstd::atomic<int> data[5];\nstd::atomic<bool> sync1(false),sync2(false);\nvoid thread_1()\n{\n    data[0].store(42,std::memory_order_relaxed);\n    data[1].store(97,std::memory_order_relaxed);\n    data[2].store(17,std::memory_order_relaxed);\n    data[3].store(-141,std::memory_order_relaxed);\n    data[4].store(2003,std::memory_order_relaxed);\n    sync1.store(true,std::memory_order_release);    \n}\nvoid thread_2()\n{\n    while(!sync1.load(std::memory_order_acquire));  \nListing 5.9\nTransitive synchronization using acquire and release ordering\nSet sync1\nB\nLoop until \nsync1 is set\nc\n\n\n160\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n    sync2.store(true,std::memory_order_release);  \n}\nvoid thread_3()\n{\n    while(!sync2.load(std::memory_order_acquire));      \n    assert(data[0].load(std::memory_order_relaxed)==42);\n    assert(data[1].load(std::memory_order_relaxed)==97);\n    assert(data[2].load(std::memory_order_relaxed)==17);\n    assert(data[3].load(std::memory_order_relaxed)==-141);\n    assert(data[4].load(std::memory_order_relaxed)==2003);\n}\nEven though thread_2 only touches the variables sync1 c and sync2 d, this is\nenough for synchronization between thread_1 and thread_3 to ensure that the asserts\ndon’t fire. First off, the stores to data from thread_1 happens before the store to sync1\nB because they’re sequenced before it in the same thread. Because the load from\nsync1 B is in a while loop, it will eventually see the value stored from thread_1 and\nform the second half of the release-acquire pair. Therefore, the store to sync1 happens\nbefore the final load from sync1 in the while loop. This load is sequenced before (and\nthus happens before) the store to sync2 d, which forms a release-acquire pair with the\nfinal load from the while loop in thread_3 e. The store to sync2 d thus happens\nbefore the load e, which happens before the loads from data. Because of the transitive\nnature of happens-before, you can chain it all together: the stores to data happen\nbefore the store to sync1 B, which happens before the load from sync1 c, which hap-\npens before the store to sync2 d, which happens before the load from sync2 e, which\nhappens before the loads from data. Thus the stores to data in thread_1 happen\nbefore the loads from data in thread_3, and the asserts can’t fire.\n In this case, you could combine sync1 and sync2 into a single variable by using a\nread-modify-write operation with memory_order_acq_rel in thread_2. One option\nwould be to use compare_exchange_strong() to ensure that the value is updated only\nonce the store from thread_1 has been seen:\nstd::atomic<int> sync(0);\nvoid thread_1()\n{\n    // ...\n    sync.store(1,std::memory_order_release);\n}\nvoid thread_2()\n{\n    int expected=1;\n    while(!sync.compare_exchange_strong(expected,2,\n                                        std::memory_order_acq_rel))\n        expected=1;\n}\nvoid thread_3()\n{\n    while(sync.load(std::memory_order_acquire)<2);\n    // ...\n}\nSet sync2\nd\nLoop until \nsync2 is set\ne\n",
      "page_number": 176
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 184-192)",
      "start_page": 184,
      "end_page": 192,
      "detection_method": "topic_boundary",
      "content": "161\nSynchronizing operations and enforcing ordering\nIf you use read-modify-write operations, it’s important to pick which semantics you\ndesire. In this case, you want both acquire and release semantics, so memory_order\n_acq_rel is appropriate, but you can use other orderings too. A fetch_sub operation\nwith memory_order_acquire semantics doesn’t synchronize with anything, even though\nit stores a value, because it isn’t a release operation. Likewise, a store can’t synchronize\nwith a fetch_or with memory_order_release semantics, because the read part of the\nfetch_or isn’t an acquire operation. Read-modify-write operations with memory_order\n_acq_rel semantics behave as both an acquire and a release, so a prior store can syn-\nchronize with such an operation, and it can synchronize with a subsequent load, as is\nthe case in this example.\n If you mix acquire-release operations with sequentially consistent operations, the\nsequentially consistent loads behave like loads with acquire semantics, and sequen-\ntially consistent stores behave like stores with release semantics. Sequentially consis-\ntent read-modify-write operations behave as both acquire and release operations.\nRelaxed operations are still relaxed but are bound by the additional synchronizes-with\nand consequent happens-before relationships introduced through the use of acquire-\nrelease semantics.\n Despite the potentially non-intuitive outcomes, anyone who’s used locks has had to\ndeal with the same ordering issues: locking a mutex is an acquire operation, and\nunlocking the mutex is a release operation. With mutexes, you learn that you must\nensure that the same mutex is locked when you read a value as was locked when you\nwrote it, and the same applies here; your acquire and release operations have to be on\nthe same variable to ensure an ordering. If data is protected with a mutex, the exclu-\nsive nature of the lock means that the result is indistinguishable from what it would\nhave been had the lock and unlock been sequentially consistent operations. Similarly,\nif you use acquire and release orderings on atomic variables to build a simple lock,\nthen from the point of view of code that uses the lock, the behavior will appear\nsequentially consistent, even though the internal operations are not.\n If you don’t need the stringency of sequentially consistent ordering for your\natomic operations, the pairwise synchronization of acquire-release ordering has the\npotential for a much lower synchronization cost than the global ordering required for\nsequentially consistent operations. The trade-off here is the mental cost required to\nensure that the ordering works correctly and that the non-intuitive behavior across\nthreads isn’t problematic.\nDATA DEPENDENCY WITH ACQUIRE-RELEASE ORDERING AND MEMORY_ORDER_CONSUME\nIn the introduction to this section I said that memory_order_consume was part of the\nacquire-release ordering model, but it was conspicuously absent from the preceding\ndescription. This is because memory_order_consume is special: it’s all about data\ndependencies, and it introduces the data-dependency nuances to the inter-thread\nhappens-before relationship mentioned in section 5.3.2. It is also special in that the\nC++17 standard explicitly recommends that you do not use it. It is therefore only cov-\nered here for completeness: you should not use memory_order_consume in your code!\n\n\n162\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n The concept of a data dependency is relatively straightforward: there is a data\ndependency between two operations if the second one operates on the result of the\nfirst. There are two new relations that deal with data dependencies: dependency-ordered-\nbefore and carries-a-dependency-to. Like sequenced-before, carries-a-dependency-to applies\nstrictly within a single thread and models the data dependency between operations; if\nthe result of an operation (A) is used as an operand for an operation (B), then A car-\nries a dependency to B. If the result of operation A is a value of a scalar type such as an\nint, then the relationship still applies if the result of A is stored in a variable, and that\nvariable is then used as an operand for operation B. This operation is also transitive,\nso if A carries a dependency to B, and B carries a dependency to C, then A carries a\ndependency to C. \n On the other hand, the dependency-ordered-before relationship can apply\nbetween threads. It’s introduced by using atomic load operations tagged with memory\n_order_consume. This is a special case of memory_order_acquire that limits the syn-\nchronized data to direct dependencies; a store operation (A) tagged with memory_\norder_release, memory_order_acq_rel, or memory_order_seq_cst is dependency-\nordered-before a load operation (B) tagged with memory_order_consume if the consume\nreads the value stored. This is as opposed to the synchronizes-with relationship you get\nif the load uses memory_order_acquire. If this operation (B) then carries a depen-\ndency to some operation (C), then A is also dependency-ordered-before C.\n This wouldn’t do you any good for synchronization purposes if it didn’t affect the\ninter-thread happens-before relation, but it does: if A is dependency-ordered-before\nB, then A also inter-thread happens-before B.\n One important use for this kind of memory ordering is where the atomic opera-\ntion loads a pointer to some data. By using memory_order_consume on the load and\nmemory_order_release on the prior store, you ensure that the pointed-to data is cor-\nrectly synchronized, without imposing any synchronization requirements on any other\nnondependent data. The following listing shows an example of this scenario.\nstruct X\n{\n    int i;\n    std::string s;\n};\nstd::atomic<X*> p;\nstd::atomic<int> a;\nvoid create_x()\n{\n    X* x=new X;\n    x->i=42;\n    x->s=”hello”;\n    a.store(99,std::memory_order_relaxed);   \n    p.store(x,std::memory_order_release);    \n}\nListing 5.10\nUsing std::memory_order_consume to synchronize data\nB\nc\n\n\n163\nSynchronizing operations and enforcing ordering\nvoid use_x()\n{\n    X* x;\n    while(!(x=p.load(std::memory_order_consume)))       \n        std::this_thread::sleep(std::chrono::microseconds(1));\n    assert(x->i==42);                                  \n    assert(x->s==”hello”);                         \n    assert(a.load(std::memory_order_relaxed)==99);   \n}\nint main()\n{\n    std::thread t1(create_x);\n    std::thread t2(use_x);\n    t1.join();\n    t2.join();\n}\nEven though the store to a B is sequenced before the store to p c, and the store to p\nis tagged memory_order_release, the load of p d is tagged memory_order_consume.\nThis means that the store to p only happens before those expressions that are depen-\ndent on the value loaded from p. This means that the asserts on the data members\nof the X structure (e and f) are guaranteed not to fire, because the load of p car-\nries a dependency to those expressions through the variable x. On the other hand,\nthe assert on the value of a g may or may not fire; this operation isn’t dependent\non the value loaded from p, and so there’s no guarantee on the value that’s read.\nThis is particularly apparent because it’s tagged with memory_order_relaxed, as\nyou’ll see.\n Sometimes, you don’t want the overhead of carrying the dependency around. You\nwant the compiler to be able to cache values in registers and reorder operations to\noptimize the code rather than fussing about the dependencies. In these scenarios, you\ncan use std::kill_dependency() to explicitly break the dependency chain. std::\nkill_dependency() is a simple function template that copies the supplied argument to\nthe return value but breaks the dependency chain in doing so. For example, if you have\na global read-only array, and you use std::memory_order_consume when retrieving an\nindex into that array from another thread, you can use std::kill_dependency() to let\nthe compiler know that it doesn’t need to reread the contents of the array entry, as in\nthe following example:\nint global_data[]={ … };\nstd::atomic<int> index;\nvoid f()\n{\n    int i=index.load(std::memory_order_consume);\n    do_something_with(global_data[std::kill_dependency(i)]);\n}\nIn real code, you should always use memory_order_acquire where you might be\ntempted to use memory_order_consume, and std::kill_dependency is unnecessary.\nd\ne\nf\ng\n\n\n164\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n Now that I’ve covered the basics of the memory orderings, it’s time to look at the\nmore complex parts of the synchronizes-with relation, which manifest in the form of\nrelease sequences.\n5.3.4\nRelease sequences and synchronizes-with\nBack in section 5.3.1, I mentioned that you could get a synchronizes-with relationship\nbetween a store to an atomic variable and a load of that atomic variable from another\nthread, even when there’s a sequence of read-modify-write operations between the\nstore and the load, provided all the operations are suitably tagged. Now that I’ve cov-\nered the possible memory-ordering “tags,” I can elaborate on this. If the store is tagged\nwith memory_order_release, memory_order_acq_rel, or memory_order_seq_cst, and\nthe load is tagged with memory_order_consume, memory_order_acquire, or memory\n_order_seq_cst, and each operation in the chain loads the value written by the pre-\nvious operation, then the chain of operations constitutes a release sequence and the ini-\ntial store synchronizes with (for memory_order_acquire or memory_order_seq_cst)\nor is dependency-ordered-before (for memory_order_consume) the final load. Any\natomic read-modify-write operations in the chain can have any memory ordering\n(even memory_order_relaxed).\n To see what this means and why it’s important, consider atomic<int> being used\nas a count of the number of items in a shared queue, as in the following listing.\n#include <atomic>\n#include <thread>\nstd::vector<int> queue_data;\nstd::atomic<int> count;\nvoid populate_queue()\n{\n    unsigned const number_of_items=20;\n    queue_data.clear();\n    for(unsigned i=0;i<number_of_items;++i)\n    {\n        queue_data.push_back(i);\n    }\n    \n    count.store(number_of_items,std::memory_order_release);   \n}\nvoid consume_queue_items()\n{\n    while(true)\n    {\n        int item_index;                               \n        if((item_index=count.fetch_sub(1,std::memory_order_acquire))<=0)  \n        {\n            wait_for_more_items();  \n            continue;\n        }\nListing 5.11\nReading values from a queue with atomic operations\nThe initial \nstore\nb\nAn RMW\noperation c\nWait for \nmore items.\nd\n\n\n165\nSynchronizing operations and enforcing ordering\n        process(queue_data[item_index-1]);    \n    }\n}\nint main()\n{\n    std::thread a(populate_queue);\n    std::thread b(consume_queue_items);\n    std::thread c(consume_queue_items);\n    a.join();\n    b.join();\n    c.join();\n}\nOne way to handle things would be to have the thread that’s producing the data store\nthe items in a shared buffer and then do count.store(number_of_items, memory_\norder_release) B to let the other threads know that data is available. The threads\nconsuming the queue items might then do count.fetch_sub(1,memory_order_\nacquire) c to claim an item from the queue, prior to reading the shared buffer e.\nOnce the count becomes zero, there are no more items, and the thread must wait d.\n If there’s one consumer thread, this is fine; fetch_sub() is a read with memory\n_order_acquire semantics, and the store had memory_order_release semantics, so\nthe store synchronizes with the load and the thread can read the item from the buffer.\nIf there are two threads reading, the second fetch_sub() will see the value written by\nthe first and not the value written by the store. Without the rule about the release\nsequence, this second thread wouldn’t have a happens-before relationship with the\nfirst thread, and it wouldn’t be safe to read the shared buffer unless the first fetch_\nsub() also had memory_order_release semantics, which would introduce unnecessary\nsynchronization between the two consumer threads. Without the release sequence\nrule or memory_order_release on the fetch_sub operations, there would be nothing\nto require that the stores to the queue_data were visible to the second consumer, and\nyou would have a data race. Thankfully, the first fetch_sub() does participate in the\nrelease sequence, and so the store() synchronizes with the second fetch_sub().\nThere’s still no synchronizes-with relationship between the two consumer threads.\nThis is shown in figure 5.7. The dotted lines in figure 5.7 show the release sequence,\nand the solid lines show the happens-before relationships.\n There can be any number of links in the chain, but provided they’re all read-\nmodify-write operations such as fetch_sub(), the store() will still synchronize with\neach one that’s tagged memory_order_acquire. In this example, all the links are the\nsame, and all are acquire operations, but they could be a mix of different operations\nwith different memory-ordering semantics.\n Although most of the synchronization relationships come from the memory-\nordering semantics applied to operations on atomic variables, it’s also possible to\nintroduce additional ordering constraints by using fences.\nReading \nqueue_data \nis safe.\ne\n\n\n166\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n5.3.5\nFences\nAn atomic operations library wouldn’t be complete without a set of fences. These are\noperations that enforce memory-ordering constraints without modifying any data and\nare typically combined with atomic operations that use the memory_order_relaxed\nordering constraints. Fences are global operations and affect the ordering of other\natomic operations in the thread that executed the fence. Fences are also commonly\ncalled memory barriers, and they get their name because they put a line in the code that\ncertain operations can’t cross. As you may recall from section 5.3.3, relaxed operations\non separate variables can usually be freely reordered by the compiler or the hardware.\nFences restrict this freedom and introduce happens-before and synchronizes-with\nrelationships that weren’t present before.\n Let’s start by adding a fence between the two atomic operations on each thread in\nlisting 5.5, as shown in the following listing.\n#include <atomic>\n#include <thread>\n#include <assert.h>\nstd::atomic<bool> x,y;\nstd::atomic<int> z;\nvoid write_x_then_y()\nListing 5.12\nRelaxed operations can be ordered with fences\ncount.store()\nrelease\ncount.fetch_sub()\nacquire\nProcess\nqueue_data\ncount.fetch_sub()\nacquire\nProcess\nqueue_data\npopulate_queue\nconsume_queue_items\nconsume_queue_items\nPopulate\nqueue_data\nFigure 5.7\nThe release sequence for the queue operations from listing 5.11\n\n\n167\nSynchronizing operations and enforcing ordering\n{\n    x.store(true,std::memory_order_relaxed);          \n    std::atomic_thread_fence(std::memory_order_release);    \n    y.store(true,std::memory_order_relaxed);          \n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_relaxed));     \n    std::atomic_thread_fence(std::memory_order_acquire);    \n    if(x.load(std::memory_order_relaxed))     \n        ++z;\n}\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x_then_y);\n    std::thread b(read_y_then_x);\n    a.join();\n    b.join();\n    assert(z.load()!=0);   \n}\nThe release fence c synchronizes with the acquire fence f because the load from y\nat e reads the value stored at d. This means that the store to x at B happens before\nthe load from x at g, so the value read must be true and the assert at h won’t fire.\nThis is in contrast to the original case without the fences where the store to and load\nfrom x weren’t ordered, and so the assert could fire. Note that both fences are neces-\nsary: you need a release in one thread and an acquire in another to get a synchronizes-\nwith relationship.\n In this case, the release fence c has the same effect as if the store to y d was\ntagged with memory_order_release rather than memory_order_relaxed. Likewise, the\nacquire fence f makes it as if the load from y e was tagged with memory_order_\nacquire. This is the general idea with fences: if an acquire operation sees the result of\na store that takes place after a release fence, the fence synchronizes with that acquire\noperation; and if a load that takes place before an acquire fence sees the result of a\nrelease operation, the release operation synchronizes with the acquire fence. You can\nhave fences on both sides, as in the example here, in which case if a load that takes\nplace before the acquire fence sees a value written by a store that takes place after the\nrelease fence, the release fence synchronizes with the acquire fence.\n Although the fence synchronization depends on the values read or written by\noperations before or after the fence, it’s important to note that the synchronization\npoint is the fence itself. If you take write_x_then_y from listing 5.12 and move the\nwrite to x after the fence as follows, the condition in the assert is no longer guaranteed\nto be true, even though the write to x comes before the write to y:\n \nb\nc\nd\ne\nf\ng\nh\n\n\n168\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nvoid write_x_then_y()\n{\n    std::atomic_thread_fence(std::memory_order_release);\n    x.store(true,std::memory_order_relaxed);                    \n    y.store(true,std::memory_order_relaxed);                    \n}\nThese two operations are no longer separated by the fence and so are no longer\nordered. It’s only when the fence comes between the store to x and the store to y that\nit imposes an ordering. The presence or absence of a fence doesn’t affect any\nenforced orderings on happens-before relationships that exist because of other\natomic operations.\n This example, and almost every other example so far in this chapter, is built\nentirely from variables with an atomic type. But the real benefit of using atomic opera-\ntions to enforce an ordering is that they can enforce an ordering on non-atomic oper-\nations and avoid the undefined behavior of a data race, as you saw back in listing 5.2.\n5.3.6\nOrdering non-atomic operations with atomics\nIf you replace x from listing 5.12 with an ordinary non-atomic bool (as in the follow-\ning listing), the behavior is guaranteed to be the same.\n#include <atomic>\n#include <thread>\n#include <assert.h>\nbool x=false;          \nstd::atomic<bool> y;\nstd::atomic<int> z;\nvoid write_x_then_y()\n{\n    x=true;              \n    std::atomic_thread_fence(std::memory_order_release);\n    y.store(true,std::memory_order_relaxed);            \n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_relaxed));         \n    std::atomic_thread_fence(std::memory_order_acquire);\n    if(x)                    \n        ++z;\n}\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x_then_y);\n    std::thread b(read_y_then_x);\n    a.join();\nListing 5.13\nEnforcing ordering on non-atomic operations\nx is now a plain \nnon-atomic variable.\nb\nStore to x before \nthe fence\nc\nStore to y after \nthe fence\nd\nWait until you see \nthe write from c.\nThis will read the \nvalue written by B.\ne\n\n\n169\nSynchronizing operations and enforcing ordering\n    b.join();\n    assert(z.load()!=0);   \n}\nThe fences still provide an enforced ordering of the store to x B and the store to y\nc, and the load from y d and the load from x e, and there’s still a happens-before\nrelationship between the store to x and the load from x, so the assert f still won’t fire.\nThe store to c and load from y d still have to be atomic; otherwise, there would be a\ndata race on y, but the fences enforce an ordering on the operations on x, once the\nreading thread has seen the stored value of y. This enforced ordering means that\nthere’s no data race on x, even though it’s modified by one thread and read by\nanother.\n It’s not only fences that can order non-atomic operations. You saw the ordering\neffects back in listing 5.10 with a memory_order_release/memory_order_consume pair\nordering non-atomic accesses to a dynamically allocated object, and many of the\nexamples in this chapter could be rewritten with some of the memory_order_relaxed\noperations replaced with plain non-atomic operations instead.\n5.3.7\nOrdering non-atomic operations\nOrdering of non-atomic operations through the use of atomic operations is where the\nsequenced-before part of happens-before becomes so important. If a non-atomic\noperation is sequenced before an atomic operation, and that atomic operation hap-\npens before an operation in another thread, the non-atomic operation also happens\nbefore that operation in the other thread. This is where the ordering of the opera-\ntions on x in listing 5.13 comes from and why the example in listing 5.2 works. This is\nalso the basis for the higher-level synchronization facilities in the C++ Standard\nLibrary, such as mutexes and condition variables. To see how this works, consider the\nsimple spinlock mutex from listing 5.1.\n The lock() operation is a loop on flag.test_and_set() using std::memory_\norder_acquire ordering, and the unlock() is a call to flag.clear() with std::memory\n_order_release ordering. When the first thread calls lock(), the flag is initially clear,\nso the first call to test_and_set() will set the flag and return false, indicating that\nthis thread now has the lock, and terminating the loop. The thread is then free to\nmodify any data protected by the mutex. Any other thread that calls lock() at this\ntime will find the flag already set and will be blocked in the test_and_set() loop.\n When the thread with the lock has finished modifying the protected data, it calls\nunlock(), which calls flag.clear() with std::memory_order_release semantics.\nThis then synchronizes (see section 5.3.1) with a subsequent call to flag.test\n_and_set() from an invocation of lock() on another thread, because this call has\nstd::memory_order_acquire semantics. Because the modification of the protected\ndata is necessarily sequenced before the unlock() call, this modification happens\nbefore the unlock() and thus happens before the subsequent lock() call from the\nsecond thread (because of the synchronizes with relationship between the unlock()\nThis assert won’t fire.\nf\n",
      "page_number": 184
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 193-216)",
      "start_page": 193,
      "end_page": 216,
      "detection_method": "topic_boundary",
      "content": "170\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nand the lock()) and happens before any accesses to that data from this second thread\nonce it has acquired the lock.\n Although other mutex implementations will have different internal operations,\nthe basic principle is the same: lock() is an acquire operation on an internal memory\nlocation, and unlock() is a release operation on that same memory location.\n Each of the synchronization mechanisms described in chapters 2, 3, and 4 will pro-\nvide ordering guarantees in terms of the synchronizes-with relationship. This is what\nenables you to use them to synchronize your data, and provide ordering guarantees.\nThe following are the synchronization relationships provided by these facilities:\nstd::thread\nThe completion of the std::thread constructor synchronizes with the invoca-\ntion of the supplied function or callable object on the new thread.\nThe completion of a thread synchronizes with the return from a successful call\nto join on the std::thread object that owns that thread.\nstd::mutex, std::timed_mutex, std::recursive_mutex, std::recursive_timed_mutex\nAll calls to lock and unlock, and successful calls to try_lock, try_lock_for, or\ntry_lock_until, on a given mutex object form a single total order: the lock order\nof the mutex.\nA call to unlock on a given mutex object synchronizes with a subsequent call to\nlock, or a subsequent successful call to try_lock, try_lock_for, or try_\nlock_until, on that object in the lock order of the mutex.\nFailed calls to try_lock, try_lock_for, or try_lock_until do not participate\nin any synchronization relationships.\nstd::shared_mutex, std::shared_timed_mutex\nAll calls to lock, unlock, lock_shared, and unlock_shared, and successful calls\nto try_lock, try_lock_for, try_lock_until, try_lock_shared, try_lock_\nshared_for, or try_lock_shared_until, on a given mutex object form a single\ntotal order: the lock order of the mutex.\nA call to unlock on a given mutex object synchronizes with a subsequent call to\nlock or shared_lock, or a successful call to try_lock, try_lock_for, try_\nlock_until, try_lock_shared, try_lock_shared_for, or try_lock_shared\n_until, on that object in the lock order of the mutex.\nFailed calls to try_lock, try_lock_for, try_lock_until, try_lock_shared,\ntry_lock_shared_for, or try_lock_shared_until do not participate in any\nsynchronization relationships.\nstd::promise, std::future AND std::shared_future\nThe successful completion of a call to set_value or set_exception on a given\nstd::promise object synchronizes with a successful return from a call to wait\nor get, or a call to wait_for or wait_until that returns std::future_status::\nready on a future that shares the same asynchronous state as the promise.\n\n\n171\nSynchronizing operations and enforcing ordering\nThe destructor of a given std::promise object that stores an std::future_error\nexception in the shared asynchronous state associated with the promise synchro-\nnizes with a successful return from a call to wait or get, or a call to wait_for or\nwait_until that returns std::future_status::ready on a future that shares the\nsame asynchronous state as the promise.\nstd::packaged_task, std::future AND std::shared_future\nThe successful completion of a call to the function call operator of a given\nstd::packaged_task object synchronizes with a successful return from a call to\nwait or get, or a call to wait_for or wait_until that returns std::future\n_status::ready on a future that shares the same asynchronous state as the\npackaged task.\nThe destructor of a given std::packaged_task object that stores an std::\nfuture_error exception in the shared asynchronous state associated with the\npackaged task synchronizes with a successful return from a call to wait or get,\nor a call to wait_for or wait_until that returns std::future_status::ready\non a future that shares the same asynchronous state as the packaged task.\nstd::async, std::future AND std::shared_future\nThe completion of the thread running a task launched via a call to std::async\nwith a policy of std::launch::async synchronizes with a successful return from\na call to wait or get, or a call to wait_for or wait_until that returns\nstd::future_status::ready on a future that shares the same asynchronous\nstate as the spawned task.\nThe completion of a task launched via a call to std::async with a policy of\nstd::launch::deferred synchronizes with a successful return from a call to wait\nor get, or a call to wait_for or wait_until that returns std::future_status\n::ready on a future that shares the same asynchronous state as the promise.\nstd::experimental::future, std::experimental::shared_future AND CONTINUATIONS\nThe event that causes an asynchronous shared state to become ready syn-\nchronizes with the invocation of a continuation function scheduled on that\nshared state.\nThe completion of a continuation function synchronizes with a successful\nreturn from a call to wait or get, or a call to wait_for or wait_until that\nreturns std::future_status::ready on a future that shares the same asyn-\nchronous state as the future returned from the call to then that scheduled the\ncontinuation, or the invocation of any continuation scheduled on that future.\nstd::experimental::latch\nThe invocation of each call to count_down or count_down_and_wait on a given\ninstance of std::experimental::latch synchronizes with the completion of\neach successful call to wait or count_down_and_wait on that latch.\n\n\n172\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nstd::experimental::barrier\nThe invocation of each call to arrive_and_wait or arrive_and_drop on a\ngiven instance of std::experimental::barrier synchronizes with the comple-\ntion of each subsequent successful call to arrive_and_wait on that barrier.\nstd::experimental::flex_barrier\nThe invocation of each call to arrive_and_wait or arrive_and_drop on a given\ninstance of std::experimental::flex_barrier synchronizes with the comple-\ntion of each subsequent successful call to arrive_and_wait on that barrier.\nThe invocation of each call to arrive_and_wait or arrive_and_drop on a\ngiven instance of std::experimental::flex_barrier synchronizes with the\nsubsequent invocation of the completion function on that barrier.\nThe return from the completion function on a given instance of std::\nexperimental::flex_barrier synchronizes with the completion of each call to\narrive_and_wait on that barrier that was blocked waiting for that barrier when\nthe completion function was invoked.\nstd::condition_variable AND std::condition_variable_any\nCondition variables do not provide any synchronization relationships. They are\noptimizations over busy-wait loops, and all the synchronization is provided by\nthe operations on the associated mutex.\nSummary\nIn this chapter I’ve covered the low-level details of the C++ memory model and the\natomic operations that provide the basis for synchronization between threads. This\nincludes the basic atomic types provided by specializations of the std::atomic<> class\ntemplate as well as the generic atomic interface provided by the primary std::atomic<>\ntemplate and the std::experimental::atomic_shared_ptr<> template, the opera-\ntions on these types, and the complex details of the various memory-ordering options.\n We’ve also looked at fences and how they can be paired with operations on atomic\ntypes to enforce an ordering. Finally, we’ve come back to the beginning with a look at\nhow the atomic operations can be used to enforce an ordering between non-atomic\noperations on separate threads, and the synchronization relationships provided by the\nhigher-level facilities.\n In the next chapter we’ll look at using the high-level synchronization facilities\nalongside atomic operations to design efficient containers for concurrent access, and\nwe’ll write algorithms that process data in parallel.\n\n\n173\nDesigning lock-based\nconcurrent data structures\nIn the last chapter we looked at the low-level details of atomic operations and the\nmemory model. In this chapter we’ll take a break from the low-level details (although\nwe’ll need them for chapter 7) and think about data structures. \n The choice of data structure to use for a programming problem can be a key\npart of the overall solution, and parallel programming problems are no exception.\nIf a data structure is to be accessed from multiple threads, either it must be com-\npletely immutable so the data never changes and no synchronization is necessary,\nor the program must be designed to ensure that changes are correctly synchro-\nnized between threads. One option is to use a separate mutex and external locking\nto protect the data, using the techniques we looked at in chapters 3 and 4, and\nanother is to design the data structure itself for concurrent access.\nThis chapter covers\nWhat it means to design data structures for \nconcurrency\nGuidelines for doing so\nExample implementations of data structures \ndesigned for concurrency\n\n\n174\nCHAPTER 6\nDesigning lock-based concurrent data structures\n When designing a data structure for concurrency, you can use the basic building\nblocks of multithreaded applications from earlier chapters, such as mutexes and con-\ndition variables. Indeed, you’ve already seen a couple of examples showing how to\ncombine these building blocks to write data structures that are safe for concurrent\naccess from multiple threads.\n In this chapter we’ll start by looking at some general guidelines for designing data\nstructures for concurrency. We’ll then take the basic building blocks of locks and con-\ndition variables and revisit the design of those basic data structures before moving on\nto more complex data structures. In chapter 7 we’ll look at how to go right back to\nbasics and use the atomic operations described in chapter 5 to build data structures\nwithout locks.\n So, without further ado, let’s look at what’s involved in designing a data structure\nfor concurrency.\n6.1\nWhat does it mean to design for concurrency?\nAt the basic level, designing a data structure for concurrency means that multiple\nthreads can access the data structure concurrently, either performing the same or dis-\ntinct operations, and each thread will see a self-consistent view of the data structure.\nNo data will be lost or corrupted, all invariants will be upheld, and there’ll be no prob-\nlematic race conditions. This data structure is said to be thread-safe. In general, a data\nstructure will be safe only for particular types of concurrent access. It may be possible\nto have multiple threads performing one type of operation on the data structure con-\ncurrently, whereas another operation requires exclusive access by a single thread.\nAlternatively, it may be safe for multiple threads to access a data structure concur-\nrently if they’re performing different actions, whereas multiple threads performing the\nsame action would be problematic.\n Truly designing for concurrency means more than that, though: it means provid-\ning the opportunity for concurrency to threads accessing the data structure. By its nature,\na mutex provides mutual exclusion: only one thread can acquire a lock on the mutex at\na time. A mutex protects a data structure by explicitly preventing true concurrent\naccess to the data it protects.\n This is called serialization: threads take turns accessing the data protected by the\nmutex; they must access it serially rather than concurrently. Consequently, you must\nput careful thought into the design of the data structure to enable true concurrent\naccess. Some data structures have more scope for true concurrency than others, but in\nall cases the idea is the same: the smaller the protected region, the fewer operations\nare serialized, and the greater the potential for concurrency.\n Before we look at some data structure designs, let’s have a quick look at some sim-\nple guidelines for what to consider when designing for concurrency.\n\n\n175\nWhat does it mean to design for concurrency?\n6.1.1\nGuidelines for designing data structures for concurrency\nAs I mentioned, you have two aspects to consider when designing data structures for\nconcurrent access: ensuring that the accesses are safe and enabling genuine concurrent\naccess. I covered the basics of how to make the data structure thread-safe back in\nchapter 3:\nEnsure that no thread can see a state where the invariants of the data structure\nhave been broken by the actions of another thread.\nTake care to avoid race conditions inherent in the interface to the data structure\nby providing functions for complete operations rather than for operation steps.\nPay attention to how the data structure behaves in the presence of exceptions to\nensure that the invariants are not broken.\nMinimize the opportunities for deadlock when using the data structure by\nrestricting the scope of locks and avoiding nested locks where possible.\nBefore you think about any of these details, it’s also important to think about what\nconstraints you want to put on the users of the data structure; if one thread is access-\ning the data structure through a particular function, which functions are safe to call\nfrom other threads?\n This is a crucial question to consider. Generally, constructors and destructors\nrequire exclusive access to the data structure, but it’s up to the user to ensure that\nthey’re not accessed before construction is complete or after destruction has started.\nIf the data structure supports assignment, swap(), or copy construction, then as the\ndesigner of the data structure, you need to decide whether these operations are safe\nto call concurrently with other operations or whether they require the user to ensure\nexclusive access even though the majority of functions for manipulating the data\nstructure may be called from multiple threads concurrently without any problems.\n The second aspect to consider is that of enabling genuine concurrent access. I\ncan’t offer much in the way of guidelines for this; instead, here’s a list of questions to\nask yourself as the data structure designer:\nCan the scope of locks be restricted to allow some parts of an operation to be\nperformed outside the lock?\nCan different parts of the data structure be protected with different mutexes?\nDo all operations require the same level of protection?\nCan a simple change to the data structure improve the opportunities for con-\ncurrency without affecting the operational semantics?\nAll these questions are guided by a single idea: how can you minimize the amount of\nserialization that must occur and enable the greatest amount of true concurrency?\nIt’s not uncommon for data structures to allow concurrent access from multiple\nthreads that merely read the data structure, whereas a thread that can modify the\ndata structure must have exclusive access. This is supported by using constructs like\n\n\n176\nCHAPTER 6\nDesigning lock-based concurrent data structures\nstd::shared_mutex. Likewise, as you’ll see shortly, it’s quite common for a data\nstructure to support concurrent access from threads performing different operations\nwhile serializing threads that try to perform the same operation.\n The simplest thread-safe data structures typically use mutexes and locks to protect\nthe data. Although there are issues with this, as you saw in chapter 3, it’s relatively easy\nto ensure that only one thread is accessing the data structure at a time. To ease you\ninto the design of thread-safe data structures, we’ll stick to looking at such lock-based\ndata structures in this chapter and leave the design of concurrent data structures with-\nout locks for chapter 7.\n6.2\nLock-based concurrent data structures\nThe design of lock-based concurrent data structures is all about ensuring that the\nright mutex is locked when accessing the data and that the lock is held for the min-\nimum amount of time. This is hard enough when there’s just one mutex protecting\na data structure. You need to ensure that data can’t be accessed outside the protec-\ntion of the mutex lock and that there are no race conditions inherent in the inter-\nface, as you saw in chapter 3. If you use separate mutexes to protect separate parts\nof the data structure, these issues are compounded, and there’s now also the possi-\nbility of deadlock if the operations on the data structure require more than one\nmutex to be locked. You therefore need to consider the design of a data structure\nwith multiple mutexes even more carefully than the design of a data structure with a\nsingle mutex.\n In this section you’ll apply the guidelines from section 6.1.1 to the design of sev-\neral simple data structures, using mutexes and locks to protect the data. In each case\nyou’ll seek out opportunities for enabling greater concurrency while ensuring that the\ndata structure remains thread-safe.\n Let’s start by looking at the stack implementation from chapter 3; it’s one of the\nsimplest data structures around, and it uses only a single mutex. Is it thread-safe? How\ndoes it fare from the point of view of achieving true concurrency?\n6.2.1\nA thread-safe stack using locks\nThe thread-safe stack from chapter 3 is reproduced in the following listing. The intent\nis to write a thread-safe data structure akin to std::stack<>, which supports pushing\ndata items onto the stack and popping them off again.\n#include <exception>\nstruct empty_stack: std::exception\n{\n    const char* what() const throw();\n};\ntemplate<typename T>\nclass threadsafe_stack\nListing 6.1\nA class definition for a thread-safe stack\n\n\n177\nLock-based concurrent data structures\n{\nprivate:\n    std::stack<T> data;\n    mutable std::mutex m;\npublic:\n    threadsafe_stack(){}\n    threadsafe_stack(const threadsafe_stack& other)\n    {\n        std::lock_guard<std::mutex> lock(other.m);\n        data=other.data;\n    }\n    threadsafe_stack& operator=(const threadsafe_stack&) = delete;\n    void push(T new_value)\n    {\n        std::lock_guard<std::mutex> lock(m);\n        data.push(std::move(new_value));    \n    }\n    std::shared_ptr<T> pop()\n    {\n        std::lock_guard<std::mutex> lock(m);\n        if(data.empty()) throw empty_stack();   \n        std::shared_ptr<T> const res(\n           std::make_shared<T>(std::move(data.top())));    \n        data.pop();       \n        return res;\n    }\n    void pop(T& value)\n    {\n        std::lock_guard<std::mutex> lock(m);\n        if(data.empty()) throw empty_stack();\n        value=std::move(data.top());        \n        data.pop();                \n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lock(m);\n        return data.empty();\n    }\n};\nLet’s look at each of the guidelines in turn and see how they apply here.\n First, as you can see, the basic thread safety is provided by protecting each member\nfunction with a lock on the mutex, m. This ensures that only one thread is accessing\nthe data at any one time, so provided each member function maintains the invariants,\nno thread can see a broken invariant.\n Second, there’s a potential for a race condition between empty() and either of the\npop() functions, but because the code explicitly checks for the contained stack being\nempty while holding the lock in pop(), this race condition isn’t problematic. By\nreturning the popped data item directly as part of the call to pop(), you avoid a poten-\ntial race condition that would be present with separate top() and pop() member\nfunctions such as those in std::stack<>.\nb\nc\nd\ne\nf\ng\n\n\n178\nCHAPTER 6\nDesigning lock-based concurrent data structures\n Next, there are a few potential sources of exceptions. Locking a mutex may throw\nan exception, but not only is this likely to be exceedingly rare (because it indicates a\nproblem with the mutex or a lack of system resources), it’s also the first operation in\neach member function. Because no data has been modified, this is safe. Unlocking a\nmutex can’t fail, so that’s always safe, and the use of std::lock_guard<> ensures that\nthe mutex is never left locked.\n The call to data.push() B may throw an exception if either copying/moving the\ndata value throws an exception or not enough memory can be allocated to extend the\nunderlying data structure. Either way, std::stack<> guarantees it will be safe, so\nthat’s not a problem either. \n In the first overload of pop(), the code itself might throw an empty_stack excep-\ntion c, but nothing has been modified, so that’s safe. The creation of res d might\nthrow an exception, though, for a couple of reasons: the call to std::make_shared\nmight throw because it can’t allocate memory for the new object and the internal\ndata required for reference counting, or the copy constructor or move constructor\nof the data item to be returned might throw when copying/moving into the freshly-\nallocated memory. In both cases, the C++ runtime and Standard Library ensure that\nthere are no memory leaks and the new object (if any) is correctly destroyed. Because\nyou still haven’t modified the underlying stack, you’re OK. The call to data.pop() e\nis guaranteed not to throw, as is the return of the result, so this overload of pop() is\nexception-safe.\n The second overload of pop() is similar, except this time it’s the copy assignment\nor move assignment operator that can throw f, rather than the construction of a new\nobject and an std::shared_ptr instance. Again, you don’t modify the data structure\nuntil the call to data.pop() g, which is still guaranteed not to throw, so this overload\nis exception-safe too.\n Finally, empty() doesn’t modify any data, so that’s exception-safe.\n There are a couple of opportunities for deadlock here, because you call user code\nwhile holding a lock: the copy constructor or move constructor (B, d) and copy\nassignment or move assignment operator f on the contained data items, as well as\npotentially a user-defined operator new. If these functions either call member func-\ntions on the stack that the item is being inserted into or removed from or require a\nlock of any kind and another lock was held when the stack member function was\ninvoked, there’s the possibility of deadlock. But it’s sensible to require that users of\nthe stack be responsible for ensuring this; you can’t reasonably expect to add an item\nonto a stack or remove it from a stack without copying it or allocating memory for it.\n Because all the member functions use std::lock_guard<> to protect the data, it’s\nsafe for any number of threads to call the stack member functions. The only member\nfunctions that aren’t safe are the constructors and destructors, but this isn’t a prob-\nlem; the object can be constructed only once and destroyed only once. Calling mem-\nber functions on an incompletely constructed object or a partially destructed object is\nnever a good idea, whether done concurrently or not. As a consequence, the user must\n\n\n179\nLock-based concurrent data structures\nensure that other threads aren’t able to access the stack until it’s fully constructed and\nmust ensure that all threads have ceased accessing the stack before it’s destroyed.\n Although it’s safe for multiple threads to call the member functions concurrently,\nbecause of the use of locks, only one thread is ever doing any work in the stack data\nstructure at a time. This serialization of threads can potentially limit the performance\nof an application where there’s significant contention on the stack: while a thread is\nwaiting for the lock, it isn’t doing any useful work. Also, the stack doesn’t provide any\nmeans of waiting for an item to be added, so if a thread needs to wait, it must periodi-\ncally call empty(), or call pop() and catch the empty_stack exceptions. This makes\nthis stack implementation a poor choice if such a scenario is required, because a wait-\ning thread must either consume precious resources checking for data or the user\nmust write external wait and notification code (for example, using condition vari-\nables), which might render the internal locking unnecessary and therefore wasteful.\nThe queue from chapter 4 shows a way of incorporating this waiting into the data struc-\nture itself using a condition variable inside the data structure, so let’s look at that next.\n6.2.2\nA thread-safe queue using locks and condition variables\nThe thread-safe queue from chapter 4 is reproduced in listing 6.2. Much like the stack\nwas modeled after std::stack<>, this queue is modeled after std::queue<>. Again,\nthe interface differs from that of the standard container adaptor because of the con-\nstraints of writing a data structure that’s safe for concurrent access from multiple\nthreads.\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    mutable std::mutex mut;\n    std::queue<T> data_queue;\n    std::condition_variable data_cond;\npublic:\n    threadsafe_queue()\n    {}\n    void push(T new_value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        data_queue.push(std::move(new_value));\n        data_cond.notify_one();         \n    }\n    void wait_and_pop(T& value)    \n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        value=std::move(data_queue.front());\n        data_queue.pop();\n    }\nListing 6.2\nThe full class definition for a thread-safe queue using condition variables\nb\nc\n\n\n180\nCHAPTER 6\nDesigning lock-based concurrent data structures\n    std::shared_ptr<T> wait_and_pop()    \n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});    \n        std::shared_ptr<T> res(\n            std::make_shared<T>(std::move(data_queue.front())));\n        data_queue.pop();\n        return res;\n    }\n    bool try_pop(T& value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return false;\n        value=std::move(data_queue.front());\n        data_queue.pop();\n        return true;\n    }\n    std::shared_ptr<T> try_pop()\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return std::shared_ptr<T>();    \n        std::shared_ptr<T> res(\n            std::make_shared<T>(std::move(data_queue.front())));\n        data_queue.pop();\n        return res;\n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        return data_queue.empty();\n    }\n};\nThe structure of the queue implementation shown in listing 6.2 is similar to the stack\nfrom listing 6.1, except for the call to data_cond.notify_one() in push() B and the\nwait_and_pop() functions, c and d. The two overloads of try_pop() are almost\nidentical to the pop() functions from listing 6.1, except that they don’t throw an\nexception if the queue is empty. Instead, they return either a bool value indicating\nwhether a value was retrieved or a NULL pointer if no value could be retrieved by the\npointer-returning overload f. This would also have been a valid way of implementing\nthe stack. If you exclude the wait_and_pop() functions, the analysis you did for the\nstack applies just as well here.\n The new wait_and_pop() functions are a solution to the problem of waiting for a\nqueue entry that you saw with the stack; rather than continuously calling empty(), the\nwaiting thread can call wait_and_pop() and the data structure will handle the waiting\nwith a condition variable. The call to data_cond.wait() won’t return until the under-\nlying queue has at least one element, so you don’t have to worry about the possibility\nof an empty queue at this point in the code, and the data is still protected with the\nd\ne\nf\n\n\n181\nLock-based concurrent data structures\nlock on the mutex. These functions don’t therefore add any new race conditions or\npossibilities for deadlock, and the invariants will be upheld.\n There’s a slight twist with regard to exception safety in that if more than one\nthread is waiting when an entry is pushed onto the queue, only one thread will be\nwoken by the call to data_cond.notify_one(). But if that thread then throws an\nexception in wait_and_pop(), such as when the new std::shared_ptr<> is con-\nstructed e, none of the other threads will be woken. If this isn’t acceptable, the call is\nreadily replaced with data_cond.notify_all(), which will wake all the threads but at\nthe cost of most of them then going back to sleep when they find that the queue is\nempty after all. A second alternative is to have wait_and_pop() call notify_one() if\nan exception is thrown, so that another thread can attempt to retrieve the stored\nvalue. A third alternative is to move the std::shared_ptr<> initialization to the\npush() call and store std::shared_ptr<> instances rather than direct data values.\nCopying the std::shared_ptr<> out of the internal std::queue<> then can’t throw\nan exception, so wait_and_pop() is safe again. The following listing shows the queue\nimplementation revised with this in mind.\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    mutable std::mutex mut;\n    std::queue<std::shared_ptr<T> > data_queue;\n    std::condition_variable data_cond;\npublic:\n    threadsafe_queue()\n    {}\n    void wait_and_pop(T& value)\n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        value=std::move(*data_queue.front());     \n        data_queue.pop();\n    }\n    bool try_pop(T& value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return false;\n        value=std::move(*data_queue.front());     \n        data_queue.pop();\n        return true;\n    }\n    std::shared_ptr<T> wait_and_pop()\n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        std::shared_ptr<T> res=data_queue.front();            \nListing 6.3\nA thread-safe queue holding std::shared_ptr<> instances\nb\nc\nd\n\n\n182\nCHAPTER 6\nDesigning lock-based concurrent data structures\n        data_queue.pop();\n        return res;\n    }\n    std::shared_ptr<T> try_pop()\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return std::shared_ptr<T>();\n        std::shared_ptr<T> res=data_queue.front();    \n        data_queue.pop();\n        return res;\n    }\n    void push(T new_value)\n    {\n        std::shared_ptr<T> data(\n            std::make_shared<T>(std::move(new_value)));    \n        std::lock_guard<std::mutex> lk(mut);\n        data_queue.push(data);\n        data_cond.notify_one();\n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        return data_queue.empty();\n    }\n};\nThe basic consequences of holding the data by std::shared_ptr<> are straightfor-\nward: the pop functions that take a reference to a variable to receive the new value\nnow have to dereference the stored pointer, B and c, and the pop functions that\nreturn an std::shared_ptr<> instance can retrieve it from the queue, d and e,\nbefore returning it to the caller.\n If the data is held by std::shared_ptr<>, there’s an additional benefit: the alloca-\ntion of the new instance can now be done outside the lock in push() f, whereas in\nlisting 6.2 it had to be done while holding the lock in pop(). Because memory alloca-\ntion is typically quite an expensive operation, this can be beneficial for the perfor-\nmance of the queue, because it reduces the time the mutex is held, allowing other\nthreads to perform operations on the queue in the meantime.\n Just like in the stack example, the use of a mutex to protect the entire data struc-\nture limits the concurrency supported by this queue; although multiple threads might\nbe blocked on the queue in various member functions, only one thread can be doing\nany work at a time. But part of this restriction comes from the use of std::queue<> in\nthe implementation; by using the standard container you now have one data item\nthat’s either protected or not. By taking control of the detailed implementation of the\ndata structure, you can provide more fine-grained locking and allow a higher level of\nconcurrency.\ne\nf\n\n\n183\nLock-based concurrent data structures\n6.2.3\nA thread-safe queue using fine-grained locks and \ncondition variables\nIn listings 6.2 and 6.3 you have one protected data item (data_queue) and therefore\none mutex. In order to use finer-grained locking, you need to look inside the queue at\nits constituent parts and associate one mutex with each distinct data item.\n The simplest data structure for a queue is a singly linked list, as shown in figure 6.1.\nThe queue contains a head pointer, which points to the first item in the list, and each\nitem then points to the next item. Data items are removed from the queue by replac-\ning the head pointer with the pointer to the next item and then returning the data\nfrom the old head.\nItems are added to the queue at the other end. In order to do this, the queue also con-\ntains a tail pointer, which refers to the last item in the list. New nodes are added by\nchanging the next pointer of the last item to point to the new node and then updat-\ning the tail pointer to refer to the new item. When the list is empty, both the head\nand tail pointers are NULL.\n The following listing shows a simple implementation of this queue based on a cut-\ndown version of the interface to the queue in listing 6.2; you have only one try_pop()\nfunction and no wait_and_pop() because this queue only supports single-threaded use.\ntemplate<typename T>\nclass queue\n{\nprivate:\n    struct node\n    {\n        T data;\n        std::unique_ptr<node> next;\n        node(T data_):\n            data(std::move(data_))\n        {}\n    };\n    std::unique_ptr<node> head;    \n    node* tail;                \nListing 6.4\nA simple single-threaded queue implementation\nTail\nHead\nFigure 6.1\nA queue represented using a single-linked list\nb\nc\n\n\n184\nCHAPTER 6\nDesigning lock-based concurrent data structures\npublic:\n    queue(): tail(nullptr)\n    {}\n    queue(const queue& other)=delete;\n    queue& operator=(const queue& other)=delete;\n    std::shared_ptr<T> try_pop()\n    {\n        if(!head)\n        {\n            return std::shared_ptr<T>();\n        }\n        std::shared_ptr<T> const res(\n            std::make_shared<T>(std::move(head->data)));\n        std::unique_ptr<node> const old_head=std::move(head);\n        head=std::move(old_head->next);     \n        if(!head)\n            tail=nullptr;\n        return res;\n    }\n    void push(T new_value)\n    {\n        std::unique_ptr<node> p(new node(std::move(new_value)));\n        node* const new_tail=p.get();\n        if(tail)\n        {\n            tail->next=std::move(p);    \n        }\n        else\n        {\n            head=std::move(p);    \n        }\n        tail=new_tail;     \n    }\n};\nFirst off, note that listing 6.4 uses std::unique_ptr<node> to manage the nodes,\nbecause this ensures that they (and the data they refer to) get deleted when they’re no\nlonger needed, without having to write an explicit delete. This ownership chain is\nmanaged from head, with tail being a raw pointer to the last node, as it needs to\nrefer to a node already owned by std::unique_ptr<node>.\n Although this implementation works fine in a single-threaded context, a couple of\nthings will cause you problems if you try to use fine-grained locking in a multi-\nthreaded context. Given that you have two data items (head B and tail c), you\ncould in principle use two mutexes, one to protect head and one to protect tail, but\nthere are a couple of problems with that.\n The most obvious problem is that push() can modify both head f and tail g, so\nit would have to lock both mutexes. This isn’t too much of a problem, although it’s\nunfortunate, because locking both mutexes would be possible. The critical problem\nis that both push() and pop() access the next pointer of a node: push() updates\ntail->next e, and try_pop() reads head->next d. If there’s a single item in the\nd\ne\nf\ng\n\n\n185\nLock-based concurrent data structures\nqueue, then head==tail, so both head->next and tail->next are the same object,\nwhich therefore requires protection. Because you can’t tell if it’s the same object with-\nout reading both head and tail, you now have to lock the same mutex in both push()\nand try_pop(), so you’re no better off than before. Is there a way out of this dilemma?\nENABLING CONCURRENCY BY SEPARATING DATA\nYou can solve this problem by preallocating a dummy node with no data to ensure that\nthere’s always at least one node in the queue to separate the node being accessed at\nthe head from that being accessed at the tail. For an empty queue, head and tail now\nboth point to the dummy node rather than being NULL. This is fine, because\ntry_pop() doesn’t access head->next if the queue is empty. If you add a node to the\nqueue (so there’s one real node), then head and tail now point to separate nodes, so\nthere’s no race on head->next and tail->next. The downside is that you have to add\nan extra level of indirection to store the data by pointer in order to allow the dummy\nnodes. The following listing shows how the implementation looks now.\ntemplate<typename T>\nclass queue\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;     \n        std::unique_ptr<node> next;\n    };\n    std::unique_ptr<node> head;\n    node* tail;\npublic:\n    queue():\n        head(new node),tail(head.get())    \n    {}\n    queue(const queue& other)=delete;\n    queue& operator=(const queue& other)=delete;\n    std::shared_ptr<T> try_pop()\n    {\n        if(head.get()==tail)     \n        {\n            return std::shared_ptr<T>();\n        }\n        std::shared_ptr<T> const res(head->data);     \n        std::unique_ptr<node> old_head=std::move(head);\n        head=std::move(old_head->next);               \n        return res;                    \n    }\n    void push(T new_value)\n    {\n        std::shared_ptr<T> new_data(\n            std::make_shared<T>(std::move(new_value)));   \nListing 6.5\nA simple queue with a dummy node\nb\nc\nd\ne\nf\ng\nh\n\n\n186\nCHAPTER 6\nDesigning lock-based concurrent data structures\n        std::unique_ptr<node> p(new node);            \n        tail->data=new_data;        \n        node* const new_tail=p.get();\n        tail->next=std::move(p);\n        tail=new_tail;\n    }\n};\nThe changes to try_pop() are fairly minimal. First, you’re comparing head against\ntail d, rather than checking for NULL, because the dummy node means that head is\nnever NULL. Because head is a std::unique_ptr<node>, you need to call head.get()\nto do the comparison. Second, because the node now stores the data by pointer B,\nyou can retrieve the pointer directly e, rather than having to construct a new\ninstance of T. The big changes are in push(): you must first create a new instance of T\non the heap and take ownership of it in a std::shared_ptr<> h (note the use of\nstd::make_shared to avoid the overhead of a second memory allocation for the refer-\nence count). The new node you create is going to be the new dummy node, so you\ndon’t need to supply the new_value to the constructor i. Instead, you set the data on\nthe old dummy node to your newly allocated copy of the new_value j. Finally, in\norder to have a dummy node, you have to create it in the constructor c.\n By now, I’m sure you’re wondering what these changes buy you and how they help\nwith making the queue thread-safe. Well, push() now  accesses only tail, not head,\nwhich is an improvement. try_pop() accesses both head and tail, but tail is needed\nonly for the initial comparison, so the lock is short-lived. The big gain is that the\ndummy node means try_pop() and push() are never operating on the same node, so\nyou no longer need an overarching mutex. You can have one mutex for head and one\nfor tail. Where do you put the locks?\n You’re aiming for the maximum number of opportunities for concurrency, so you\nwant to hold the locks for the shortest possible length of time. push() is easy: the\nmutex needs to be locked across all accesses to tail, which means you lock the mutex\nafter the new node is allocated i, and before you assign the data to the current tail\nnode j. The lock then needs to be held until the end of the function.\n try_pop() isn’t so easy. First off, you need to lock the mutex on head and hold it\nuntil you’re finished with head. This is the mutex to determine which thread does the\npopping, so you want to do that first. Once head is changed f, you can unlock\nthe mutex; it doesn’t need to be locked when you return the result g. That leaves the\naccess to tail needing a lock on the tail mutex. Because you need to access tail only\nonce, you can just acquire the mutex for the time it takes to do the read. This is best\ndone by wrapping it in a function. In fact, because the code that needs the head\nmutex locked is only a subset of the member, it’s clearer to wrap that in a function too.\nThe final code is shown here.\n \n \n \ni\nj\n\n\n187\nLock-based concurrent data structures\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::unique_ptr<node> next;\n    };\n    std::mutex head_mutex;\n    std::unique_ptr<node> head;\n    std::mutex tail_mutex;\n    node* tail;\n    node* get_tail()\n    {\n        std::lock_guard<std::mutex> tail_lock(tail_mutex);\n        return tail;\n    }\n    std::unique_ptr<node> pop_head()\n    {\n        std::lock_guard<std::mutex> head_lock(head_mutex);\n            \n        if(head.get()==get_tail())\n        {\n            return nullptr;\n        }\n        std::unique_ptr<node> old_head=std::move(head);\n        head=std::move(old_head->next);\n        return old_head;\n    }\npublic:\n    threadsafe_queue():\n        head(new node),tail(head.get())\n    {}\n    threadsafe_queue(const threadsafe_queue& other)=delete;\n    threadsafe_queue& operator=(const threadsafe_queue& other)=delete;\n    std::shared_ptr<T> try_pop()\n    {\n        std::unique_ptr<node> old_head=pop_head();\n        return old_head?old_head->data:std::shared_ptr<T>();\n    }\n    void push(T new_value)\n    {\n        std::shared_ptr<T> new_data(\n            std::make_shared<T>(std::move(new_value)));\n        std::unique_ptr<node> p(new node);\n        node* const new_tail=p.get();\n        std::lock_guard<std::mutex> tail_lock(tail_mutex);\n        tail->data=new_data;\n        tail->next=std::move(p);\n        tail=new_tail;\n    }\n};\nListing 6.6\nA thread-safe queue with fine-grained locking\n\n\n188\nCHAPTER 6\nDesigning lock-based concurrent data structures\nLet’s look at this code with a critical eye, thinking about the guidelines listed in sec-\ntion 6.1.1. Before you look for broken invariants, you should be sure what they are:\n\ntail->next==nullptr.\n\ntail->data==nullptr.\n\nhead==tail implies an empty list.\nA single element list has head->next==tail.\nFor each node x in the list, where x!=tail, x->data points to an instance of T\nand x->next points to the next node in the list. x->next==tail implies x is the\nlast node in the list.\nFollowing the next nodes from head will eventually yield tail.\nOn its own, push() is straightforward: the only modifications to the data structure are\nprotected by tail_mutex, and they uphold the invariant because the new tail node is\nan empty node and data and next are correctly set for the old tail node, which is now\nthe last real node in the list.\n The interesting part is try_pop(). It turns out that not only is the lock on\ntail_mutex necessary to protect the read of tail itself, but it’s also necessary to\nensure that you don’t get a data race reading the data from the head. If you didn’t\nhave that mutex, it would be quite possible for a thread to call try_pop() and a thread\nto call push() concurrently, and there’d be no defined ordering on their operations.\nEven though each member function holds a lock on a mutex, they hold locks on differ-\nent mutexes, and they potentially access the same data; all data in the queue originates\nfrom a call to push(), after all. Because the threads would be potentially accessing the\nsame data without a defined ordering, this would be a data race, as you saw in chapter 5,\nand undefined behavior. Thankfully the lock on tail_mutex in get_tail() solves\neverything. Because the call to get_tail() locks the same mutex as the call to push(),\nthere’s a defined order between the two calls. Either the call to get_tail() occurs\nbefore the call to push(), in which case it sees the old value of tail, or it occurs after\nthe call to push(), in which case it sees the new value of tail and the new data\nattached to the previous value of tail.\n It’s also important that the call to get_tail() occurs inside the lock on head_\nmutex. If it didn’t, the call to pop_head() could be stuck in between the call to\nget_tail() and the lock on the head_mutex, because other threads called try_pop()\n(and thus pop_head()) and acquired the lock first, preventing your initial thread\nfrom making progress:\n    std::unique_ptr<node> pop_head()    \n    {\n        node* const old_tail=get_tail();                  \n        std::lock_guard<std::mutex> head_lock(head_mutex);\n            \n        if(head.get()==old_tail)    \n        {\n            return nullptr;\n        }\nThis is a broken \nimplementation.\nGet old tail value \noutside lock on \nhead_mutex\nb\nc\n\n\n189\nLock-based concurrent data structures\n        std::unique_ptr<node> old_head=std::move(head);\n        head=std::move(old_head->next);    \n        return old_head;\n    }\nIn this broken scenario, where the call to get_tail(0) B is made outside the scope\nof the lock, you might find that both head and tail have changed by the time your\ninitial thread can acquire the lock on head_mutex, and not only is the returned tail\nnode no longer the tail, but it’s no longer even part of the list. This could then\nmean that the comparison of head to old_tail c fails, even if head is the last\nnode. Consequently, when you update head d, you may end up moving head\nbeyond tail and off the end of the list, destroying the data structure. In the correct\nimplementation from listing 6.6, you keep the call to get_tail() inside the lock on\nhead_mutex. This ensures that no other threads can change head, and tail only\never moves further away (as new nodes are added in calls to push()), which is per-\nfectly safe. head can never pass the value returned from get_tail(), so the invari-\nants are upheld.\n Once pop_head() has removed the node from the queue by updating head, the\nmutex is unlocked, and try_pop() can extract the data and delete the node if there\nwas one (and return a NULL instance of std::shared_ptr<> if not), safe in the knowl-\nedge that it’s the only thread that can access this node.\n Next up, the external interface is a subset of that from listing 6.2, so the same anal-\nysis applies: there are no race conditions inherent in the interface.\n Exceptions are more interesting. Because you’ve changed the data allocation pat-\nterns, the exceptions can now come from different places. The only operations in\ntry_pop() that can throw exceptions are the mutex locks, and the data isn’t modified\nuntil the locks are acquired. Therefore try_pop() is exception-safe. On the other\nhand, push() allocates a new instance of T on the heap and a new instance of node,\neither of which might throw an exception. But both of the newly allocated objects are\nassigned to smart pointers, so they’ll be freed if an exception is thrown. Once the lock\nis acquired, none of the remaining operations in push() can throw an exception, so\nagain you’re home and dry and push() is exception-safe too.\n Because you haven’t changed the interface, there are no new external opportuni-\nties for deadlock. There are no internal opportunities, either; the only place that two\nlocks are acquired is in pop_head(), which always acquires the head_mutex, and then\nthe tail_mutex, so this will never deadlock.\n The remaining question concerns the possibilities for concurrency. This data struc-\nture has considerably more scope for concurrency than that from listing 6.2, because\nthe locks are more fine-grained and more is done outside the locks. For example, in\npush(), the new node and new data item are allocated with no locks held. This means\nthat multiple threads can be allocating new nodes and data items concurrently with-\nout a problem. Only one thread can add its new node to the list at a time, but the code\nto do so is only a few simple pointer assignments, so the lock isn’t held for much time\nd\n\n\n190\nCHAPTER 6\nDesigning lock-based concurrent data structures\nat all compared to the std::queue<>-based implementation where the lock is held\naround all the memory allocation operations internal to the std::queue<>.\n Also, try_pop()holds the tail_mutex for only a short time, to protect a read from\ntail. Consequently, almost the entirety of a call to try_pop() can occur concurrently\nwith a call to push(). Also, the operations performed while holding the head_mutex\nare quite minimal; the expensive delete (in the destructor of the node pointer) is out-\nside the lock. This will increase the number of calls to try_pop() that can happen\nconcurrently; only one thread can call pop_head() at  a time, but multiple threads can\nthen delete their old nodes and return the data safely.\nWAITING FOR AN ITEM TO POP\nOK, so listing 6.6 provides a thread-safe queue with fine-grained locking, but it sup-\nports only try_pop() (and only one overload at that). What about the handy wait_\nand_pop() functions back in listing 6.2? Can you implement an identical interface\nwith your fine-grained locking?\n The answer is yes, but the real question is how. Modifying push() is easy: add the\ndata_cond.notify_one() call at the end of the function, like in listing 6.2. It’s not\nquite that simple; you’re using fine-grained locking because you want the maximum\npossible amount of concurrency. If you leave the mutex locked across the call to\nnotify_one() (as in listing 6.2), then if the notified thread wakes up before the\nmutex has been unlocked, it will have to wait for the mutex. On the other hand, if you\nunlock the mutex before you call notify_one(), then the mutex is available for the\nwaiting thread to acquire when it wakes up (assuming no other thread locks it first).\nThis is a minor improvement, but it might be important in some cases.\n wait_and_pop() is more complicated, because you have to decide where to wait,\nwhat the predicate is, and which mutex needs to be locked. The condition you’re wait-\ning for is “queue not empty,” which is represented by head!=tail. Written like that, it\nwould require both head_mutex and tail_mutex to be locked, but you’ve already\ndecided in listing 6.6 that you only need to lock tail_mutex for the read of tail and\nnot for the comparison itself, so you can apply the same logic here. If you make the\npredicate head!=get_tail(), you only need to hold head_mutex, so you can use your\nlock on that for the call to data_cond.wait(). Once you’ve added the wait logic, the\nimplementation is the same as try_pop().\n The second overload of try_pop() and the corresponding wait_and_pop() over-\nload require careful thought. If you replace the return of std::shared_ptr<>\nretrieved from old_head with a copy assignment to the value parameter, there’s a\npotential exception-safety issue. At this point, the data item has been removed from\nthe queue and the mutex unlocked; all that remains is to return the data to the caller.\nBut if the copy assignment throws an exception (as it might), the data item is lost\nbecause it can’t be returned to the queue in the same place.\n If the actual type T used for the template argument has a no-throw move-assignment\noperator or a no-throw swap operation, you could use that, but you’d prefer a general\nsolution that could be used for any type T. In this case, you have to move the potential\n\n\n191\nLock-based concurrent data structures\nthrowing operation inside the locked region before the node is removed from the list.\nThis means you need an extra overload of pop_head() that retrieves the stored value\nprior to modifying the list.\n In comparison, empty() is trivial: lock head_mutex and check for head== get_tail()\n(see listing 6.10). The final code for the queue is shown in listings 6.7, 6.8, 6.9, and 6.10.\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::unique_ptr<node> next;\n    };\n    std::mutex head_mutex;\n    std::unique_ptr<node> head;\n    std::mutex tail_mutex;\n    node* tail;\n    std::condition_variable data_cond;\npublic:\n    threadsafe_queue():\n        head(new node),tail(head.get())\n    {}\n    threadsafe_queue(const threadsafe_queue& other)=delete;\n    threadsafe_queue& operator=(const threadsafe_queue& other)=delete;\n    std::shared_ptr<T> try_pop();\n    bool try_pop(T& value);\n    std::shared_ptr<T> wait_and_pop();\n    void wait_and_pop(T& value);\n    void push(T new_value);\n    bool empty();\n};\nPushing new nodes onto the queue is fairly straightforward—the implementation\n(shown in the following listing) is close to that shown previously.\ntemplate<typename T>\nvoid threadsafe_queue<T>::push(T new_value)\n{\n    std::shared_ptr<T> new_data(\n        std::make_shared<T>(std::move(new_value)));\n    std::unique_ptr<node> p(new node);\n    {\n        std::lock_guard<std::mutex> tail_lock(tail_mutex);\n        tail->data=new_data;\n        node* const new_tail=p.get();\n        tail->next=std::move(p);\nListing 6.7\nA thread-safe queue with locking and waiting: internals and interface\nListing 6.8\nA thread-safe queue with locking and waiting: pushing new values\n\n\n192\nCHAPTER 6\nDesigning lock-based concurrent data structures\n        tail=new_tail;\n    }\n    data_cond.notify_one();\n}\nAs already mentioned, the complexity is all in the pop side, which makes use of a series\nof helper functions to simplify matters. The next listing shows the implementation of\nwait_and_pop() and the associated helper functions.\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    node* get_tail()\n    {\n        std::lock_guard<std::mutex> tail_lock(tail_mutex);\n        return tail;\n    }\n    std::unique_ptr<node> pop_head()    \n    {\n        std::unique_ptr<node> old_head=std::move(head);\n        head=std::move(old_head->next);\n        return old_head;\n    }\n    std::unique_lock<std::mutex> wait_for_data()    \n    {\n        std::unique_lock<std::mutex> head_lock(head_mutex);\n        data_cond.wait(head_lock,[&]{return head.get()!=get_tail();});\n        return std::move(head_lock);         \n    }\n    std::unique_ptr<node> wait_pop_head()\n    {\n        std::unique_lock<std::mutex> head_lock(wait_for_data());    \n        return pop_head();\n    }\n    std::unique_ptr<node> wait_pop_head(T& value)\n    {\n        std::unique_lock<std::mutex> head_lock(wait_for_data());    \n        value=std::move(*head->data);\n        return pop_head();\n    }\npublic:\n    std::shared_ptr<T> wait_and_pop()\n    {\n        std::unique_ptr<node> const old_head=wait_pop_head();\n        return old_head->data;\n    }\n    void wait_and_pop(T& value)\n    {\n        std::unique_ptr<node> const old_head=wait_pop_head(value);\n    }\n};\nListing 6.9\nA thread-safe queue with locking and waiting: wait_and_pop()\nb\nc\nd\ne\nf\n\n\n193\nLock-based concurrent data structures\nThe implementation of the pop side shown in listing 6.9 has several little helper func-\ntions to simplify the code and reduce duplication, such as pop_head() B, which\nmodifies the list to remove the head item, and wait_for_data() c, which waits for\nthe queue to have some data to pop. wait_for_data() is particularly noteworthy,\nbecause not only does it wait on the condition variable using a lambda function for\nthe predicate, but it also returns the lock instance to the caller d. This is to ensure\nthat the same lock is held while the data is modified by the relevant wait_pop_head()\noverload, e and f. pop_head() is also reused by the try_pop() code shown in the\nnext listing.\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    std::unique_ptr<node> try_pop_head()\n    {\n        std::lock_guard<std::mutex> head_lock(head_mutex);\n        if(head.get()==get_tail())\n        {\n            return std::unique_ptr<node>();\n        }\n        return pop_head();\n    }\n    std::unique_ptr<node> try_pop_head(T& value)\n    {\n        std::lock_guard<std::mutex> head_lock(head_mutex);\n        if(head.get()==get_tail())\n        {\n            return std::unique_ptr<node>();\n        }\n        value=std::move(*head->data);\n        return pop_head();\n    }\npublic:\n    std::shared_ptr<T> try_pop()\n    {\n        std::unique_ptr<node> old_head=try_pop_head();\n        return old_head?old_head->data:std::shared_ptr<T>();\n    }\n    bool try_pop(T& value)\n    {\n        std::unique_ptr<node> const old_head=try_pop_head(value);\n        return old_head;\n    }\n    bool empty()\n    {\n        std::lock_guard<std::mutex> head_lock(head_mutex);\n        return (head.get()==get_tail());\n    }\n};\nListing 6.10\nA thread-safe queue with locking and waiting: try_pop() and empty()\n",
      "page_number": 193
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 217-226)",
      "start_page": 217,
      "end_page": 226,
      "detection_method": "topic_boundary",
      "content": "194\nCHAPTER 6\nDesigning lock-based concurrent data structures\nThis queue implementation will serve as the basis for the lock-free queue covered in\nchapter 7. It’s an unbounded queue; threads can continue to push new values onto\nthe queue as long as there’s available memory, even if no values are removed. The\nalternative to an unbounded queue is a bounded queue, in which the maximum\nlength of the queue is fixed when the queue is created. Once a bounded queue is\nfull, attempts to push further elements onto the queue will either fail or block until\nan element has been popped from the queue to make room. Bounded queues can\nbe useful for ensuring an even spread of work when dividing work between threads\nbased on tasks to be performed (see chapter 8). This prevents the thread(s) popu-\nlating the queue from running too far ahead of the thread(s) reading items from\nthe queue.\n The unbounded queue implementation shown here can easily be extended to\nlimit the length of the queue by waiting on the condition variable in push(). Rather\nthan waiting for the queue to have items (as is done in pop()), you need to wait for\nthe queue to have fewer than the maximum number of items. Further discussion of\nbounded queues is outside the scope of this book; for now, let’s move beyond queues\nand on to more complex data structures.\n6.3\nDesigning more complex lock-based data structures\nStacks and queues are simple: the interface is exceedingly limited, and they’re tightly\nfocused on a specific purpose. Not all data structures are that simple; most data struc-\ntures support a variety of operations. In principle, this can then lead to greater oppor-\ntunities for concurrency, but it also makes the task of protecting the data that much\nharder because the multiple access patterns need to be taken into account. The pre-\ncise nature of the various operations that can be performed is important when design-\ning these data structures for concurrent access.\n To see some of the issues involved, let’s look at the design of a lookup table.\n6.3.1\nWriting a thread-safe lookup table using locks\nA lookup table or dictionary associates values of one type (the key type) with values of\neither the same or a different type (the mapped type). In general, the intention behind\nsuch a structure is to allow code to query the data associated with a given key. In the C++\nStandard Library, this facility is provided by the associative containers: std::map<>,\nstd::multimap<>, std::unordered_map<>, and std::unordered_multimap<>. \n A lookup table has a different usage pattern than a stack or a queue. Whereas\nalmost every operation on a stack or a queue modifies it in some way, either to add an\nelement or remove one, a lookup table might be modified rarely. The simple DNS\ncache in listing 3.13 is one example of this scenario, which features a greatly reduced\ninterface compared to std::map<>. As you saw with the stack and queue, the inter-\nfaces of the standard containers aren’t suitable when the data structure is to be\naccessed from multiple threads concurrently, because there are inherent race condi-\ntions in the interface design, so they need to be cut down and revised.\n\n\n195\nDesigning more complex lock-based data structures\n The biggest problem with the std::map<> interface from a concurrency perspec-\ntive is the iterators. Although it’s possible to have an iterator that provides safe access\ninto a container even when other threads can access (and modify) the container, this\nis a tricky proposition. Correctly handling iterators requires you to deal with issues\nsuch as another thread deleting the element that the iterator is referring to, which\ncan get rather involved. For the first cut at a thread-safe lookup table interface, you’ll\nskip the iterators. Given that the interface to std::map<> (and the other associative\ncontainers in the standard library) is so heavily iterator-based, it’s probably worth set-\nting them aside and designing the interface from the ground up.\n There are only a few basic operations on a lookup table:\nAdd a new key/value pair.\nChange the value associated with a given key.\nRemove a key and its associated value.\nObtain the value associated with a given key, if any.\nThere are also a few container-wide operations that might be useful, such as a check\non whether the container is empty, a snapshot of the complete list of keys, or a snap-\nshot of the complete set of key/value pairs.\n If you stick to the simple thread-safety guidelines, such as not returning references,\nand put a simple mutex lock around the entirety of each member function, all of\nthese are safe; they either come before some modification from another thread or\nafter it. The biggest potential for a race condition is when a new key/value pair is\nbeing added; if two threads add a new value, only one will be first, and the second will\ntherefore fail. One possibility is to combine add and change into a single member\nfunction, as you did for the DNS cache in listing 3.13.\n The only other interesting point from an interface perspective is the if any part of\nobtaining an associated value. One option is to allow the user to provide a “default”\nresult that’s returned in the case when the key isn’t present:\nmapped_type get_value(key_type const& key, mapped_type default_value);\nIn this case, a default-constructed instance of mapped_type could be used if the\ndefault_value wasn’t explicitly provided. This could also be extended to return an\nstd::pair<mapped_type,bool> instead of just an instance of mapped_type, where\nthe bool indicates whether the value was present. Another option is to return a\nsmart pointer referring to the value; if the pointer value is NULL, there was no value\nto return.\n As already mentioned, once the interface has been decided, then (assuming no\ninterface race conditions) the thread safety could be guaranteed by using a single\nmutex and a simple lock around every member function to protect the underlying\ndata structure. But this would squander the possibilities for concurrency provided by\nthe separate functions for reading the data structure and modifying it. One option is\nto use a mutex that supports multiple reader threads or a single writer thread, such as\n\n\n196\nCHAPTER 6\nDesigning lock-based concurrent data structures\nstd::shared_mutex used in listing 3.13. Although this would indeed improve the pos-\nsibilities for concurrent access, only one thread could modify the data structure at a\ntime. Ideally, you’d like to do better than that.\nDESIGNING A MAP DATA STRUCTURE FOR FINE-GRAINED LOCKING\nAs with the queue discussed in section 6.2.3, in order to permit fine-grained locking\nyou need to look carefully at the details of the data structure rather than wrapping a\npre-existing container such as std::map<>. There are three common ways of imple-\nmenting an associative container like your lookup table:\nA binary tree, such as a red-black tree\nA sorted array\nA hash table\nA binary tree doesn’t provide much scope for extending the opportunities for concur-\nrency; every lookup or modification has to start by accessing the root node, which\ntherefore has to be locked. Although this lock can be released as the accessing thread\nmoves down the tree, this isn’t much better than a single lock across the whole data\nstructure.\n A sorted array is even worse, because you can’t tell in advance where in the array a\ngiven data value is going to be, so you need a single lock for the whole array.\n That leaves the hash table. Assuming a fixed number of buckets, which bucket a\nkey belongs to is purely a property of the key and its hash function. This means you\ncan safely have a separate lock per bucket. If you again use a mutex that supports mul-\ntiple readers or a single writer, you increase the opportunities for concurrency N-fold,\nwhere N is the number of buckets. The downside is that you need a good hash func-\ntion for the key. The C++ Standard Library provides the std::hash<> template, which\nyou can use for this purpose. It’s already specialized for fundamental types such as int\nand common library types such as std::string, and the user can easily specialize it\nfor other key types. If you follow the lead of the standard unordered containers and\ntake the type of the function object to use for doing the hashing as a template param-\neter, the user can choose whether to specialize std::hash<> for their key type or pro-\nvide a separate hash function.\n So, let’s look at some code. What might the implementation of a thread-safe\nlookup table look like? One possibility is shown here.\ntemplate<typename Key,typename Value,typename Hash=std::hash<Key> >\nclass threadsafe_lookup_table\n{\nprivate:\n    class bucket_type\n    {\n    private:\n        typedef std::pair<Key,Value> bucket_value;\n        typedef std::list<bucket_value> bucket_data;\nListing 6.11\nA thread-safe lookup table\n\n\n197\nDesigning more complex lock-based data structures\n        typedef typename bucket_data::iterator bucket_iterator;\n        bucket_data data;\n        mutable std::shared_mutex mutex;    \n        \n        bucket_iterator find_entry_for(Key const& key) const    \n        {\n            return std::find_if(data.begin(),data.end(),\n                                [&](bucket_value const& item)\n                                {return item.first==key;});\n        }\n    public:\n        Value value_for(Key const& key,Value const& default_value) const\n        {\n            std::shared_lock<std::shared_mutex> lock(mutex);     \n            bucket_iterator const found_entry=find_entry_for(key);\n            return (found_entry==data.end())?\n                default_value:found_entry->second;\n        }\n        void add_or_update_mapping(Key const& key,Value const& value)\n        {\n            std::unique_lock<std::shared_mutex> lock(mutex);      \n            bucket_iterator const found_entry=find_entry_for(key);\n            if(found_entry==data.end())\n            {\n                data.push_back(bucket_value(key,value));\n            }\n            else\n            {\n                found_entry->second=value;\n            }\n        }\n        void remove_mapping(Key const& key)\n        {\n            std::unique_lock<std::shared_mutex> lock(mutex);      \n            bucket_iterator const found_entry=find_entry_for(key);\n            if(found_entry!=data.end())\n            {\n                data.erase(found_entry);\n            }\n        }\n    };\n    std::vector<std::unique_ptr<bucket_type> > buckets;    \n    Hash hasher;\n    bucket_type& get_bucket(Key const& key) const    \n    {\n        std::size_t const bucket_index=hasher(key)%buckets.size();\n        return *buckets[bucket_index];\n    }\npublic:\n    typedef Key key_type;\n    typedef Value mapped_type;\n    typedef Hash hash_type;\n    threadsafe_lookup_table(\n        unsigned num_buckets=19,Hash const& hasher_=Hash()):\n        buckets(num_buckets),hasher(hasher_)\nb\nc\nd\ne\nf\ng\nh\n\n\n198\nCHAPTER 6\nDesigning lock-based concurrent data structures\n    {\n        for(unsigned i=0;i<num_buckets;++i)\n        {\n            buckets[i].reset(new bucket_type);\n        }\n    }\n    threadsafe_lookup_table(threadsafe_lookup_table const& other)=delete;\n    threadsafe_lookup_table& operator=(\n        threadsafe_lookup_table const& other)=delete;\n    Value value_for(Key const& key,\n                    Value const& default_value=Value()) const\n    {\n        return get_bucket(key).value_for(key,default_value);    \n    }\n    void add_or_update_mapping(Key const& key,Value const& value)\n    {\n        get_bucket(key).add_or_update_mapping(key,value);    \n    }\n    void remove_mapping(Key const& key)\n    {\n        get_bucket(key).remove_mapping(key);    \n    }\n};\nThis implementation uses a std::vector<std::unique_ptr<bucket_type>> g to\nhold the buckets, which allows the number of buckets to be specified in the con-\nstructor. The default is 19, which is an arbitrary prime number; hash tables work\nbest with a prime number of buckets. Each bucket is protected with an instance of\nstd::shared_mutex B to allow many concurrent reads or a single call to either of the\nmodification functions per bucket.\n Because the number of buckets is fixed, the get_bucket() function h can be\ncalled without any locking (i, j, and 1)), and then the bucket mutex can be locked\neither for shared (read-only) ownership d, or unique (read/write) ownership, e\nand f, as appropriate for each function.\n All three functions make use of the find_entry_for() member function c on the\nbucket to determine whether the entry is in the bucket. Each bucket contains just an\nstd::list<> of key/value pairs, so adding and removing entries is easy.\n I’ve already covered the concurrency angle, and everything is suitably protected\nwith mutex locks, so what about exception safety? value_for doesn’t modify anything,\nso that’s fine; if it throws an exception, it won’t affect the data structure. remove_mapping\nmodifies the list with the call to erase, but this is guaranteed not to throw, so that’s\nsafe. This leaves add_or_update_mapping, which might throw in either of the two\nbranches of if. push_back is exception-safe and will leave the list in the original state\nif it throws, so that branch is fine. The only problem is with the assignment in the case\nwhere you’re replacing an existing value; if the assignment throws, you’re relying on it\nleaving the original unchanged. But this doesn’t affect the data structure as a whole\nand is entirely a property of the user-supplied type, so you can safely leave it up to the\nuser to handle this.\ni\nj\n1)\n\n\n199\nDesigning more complex lock-based data structures\n At the beginning of this section, I mentioned that one nice-to-have feature of such\na lookup table would be the option of retrieving a snapshot of the current state into,\nfor example, a std::map<>. This would require locking the entire container in order\nto ensure that a consistent copy of the state is retrieved, which requires locking all the\nbuckets. Because the “normal” operations on the lookup table require a lock on only\none bucket at a time, this would be the only operation that requires a lock on all the\nbuckets. Therefore, provided you lock them in the same order every time (for exam-\nple, increasing bucket index), there’ll be no opportunity for deadlock. This imple-\nmentation is shown in the following listing.\nstd::map<Key,Value> threadsafe_lookup_table::get_map() const\n{\n    std::vector<std::unique_lock<std::shared_mutex> > locks;\n    for(unsigned i=0;i<buckets.size();++i)\n    {\n        locks.push_back(\n            std::unique_lock<std::shared_mutex>(buckets[i].mutex));\n    }\n    std::map<Key,Value> res;\n    for(unsigned i=0;i<buckets.size();++i)\n    {\n        for(bucket_iterator it=buckets[i].data.begin();\n            it!=buckets[i].data.end();\n            ++it)\n        {\n            res.insert(*it);\n        }\n    }\n    return res;\n}\nThe lookup table implementation from listing 6.11 increases the opportunity for con-\ncurrency of the lookup table as a whole by locking each bucket separately and by\nusing a std::shared_mutex to allow reader concurrency on each bucket. But what if\nyou could increase the potential for concurrency on a bucket by even finer-grained\nlocking? In the next section, you’ll do exactly that by using a thread-safe list container\nwith iterator support.\n6.3.2\nWriting a thread-safe list using locks\nA list is one of the most basic data structures, so it should be straightforward to write a\nthread-safe one, shouldn’t it? Well, that depends on what facilities you’re after, and\nyou need one that offers iterator support, something I shied away from adding to your\nmap on the basis that it was too complicated. The basic issue with STL-style iterator\nsupport is that the iterator must hold some kind of reference into the internal data\nstructure of the container. If the container can be modified from another thread, this\nreference must somehow remain valid, which requires that the iterator hold a lock on\nListing 6.12\nObtaining contents of a threadsafe_lookup_table as std::map<>\n\n\n200\nCHAPTER 6\nDesigning lock-based concurrent data structures\nsome part of the structure. Given that the lifetime of an STL-style iterator is com-\npletely outside the control of the container, this is a bad idea.\n The alternative is to provide iteration functions such as for_each as part of the\ncontainer itself. This puts the container squarely in charge of the iteration and lock-\ning, but it does fall foul of the deadlock avoidance guidelines from chapter 3. In order\nfor for_each to do anything useful, it must call user-supplied code while holding the\ninternal lock. Not only that, but it must also pass a reference to each item to this user-\nsupplied code in order for the user-supplied code to work on this item. You could\navoid this by passing a copy of each item to the user-supplied code, but that would be\nexpensive if the data items were large.\n So, for now you’ll leave it up to the user to ensure that they don’t cause deadlock\nby acquiring locks in the user-supplied operations and don’t cause data races by stor-\ning the references for access outside the locks. In the case of the list being used by\nthe lookup table, this is perfectly safe, because you know you’re not going to do any-\nthing naughty.\n That leaves you with the question of which operations to supply for your list. If\nyou cast your eyes back to listings 6.11 and 6.12, you can see the sorts of operations\nyou require:\nAdd an item to the list.\nRemove an item from the list if it meets a certain condition.\nFind an item in the list that meets a certain condition.\nUpdate an item that meets a certain condition.\nCopy each item in the list to another container.\nFor this to be a good general-purpose list container, it would be helpful to add further\noperations, such as a positional insert, but this is unnecessary for your lookup table, so\nI’ll leave it as an exercise for the reader.\n The basic idea with fine-grained locking for a linked list is to have one mutex per\nnode. If the list gets big, that’s a lot of mutexes! The benefit here is that operations on\nseparate parts of the list are truly concurrent: each operation holds only the locks on\nthe nodes it’s interested in and unlocks each node as it moves on to the next. The\nnext listing shows an implementation of this list.\ntemplate<typename T>\nclass threadsafe_list\n{\n    struct node      \n    {\n        std::mutex m;\n        std::shared_ptr<T> data;\n        std::unique_ptr<node> next;\n        node():        \n            next()\nListing 6.13\nA thread-safe list with iteration support\nb\nc\n\n\n201\nDesigning more complex lock-based data structures\n        {}\n        node(T const& value):              \n            data(std::make_shared<T>(value))\n        {}\n    };\n    node head;\npublic:\n    threadsafe_list()\n    {}\n    ~threadsafe_list()\n    {\n        remove_if([](node const&){return true;});\n    }\n    threadsafe_list(threadsafe_list const& other)=delete;\n    threadsafe_list& operator=(threadsafe_list const& other)=delete;\n    void push_front(T const& value)\n    {\n        std::unique_ptr<node> new_node(new node(value));   \n        std::lock_guard<std::mutex> lk(head.m);\n        new_node->next=std::move(head.next);    \n        head.next=std::move(new_node);     \n    }\n    template<typename Function>\n    void for_each(Function f)    \n    {\n        node* current=&head;\n        std::unique_lock<std::mutex> lk(head.m);   \n        while(node* const next=current->next.get())     \n        {\n            std::unique_lock<std::mutex> next_lk(next->m);    \n            lk.unlock();                                 \n            f(*next->data);     \n            current=next;\n            lk=std::move(next_lk);    \n        }\n    }\n    template<typename Predicate>\n    std::shared_ptr<T> find_first_if(Predicate p)     \n    {\n        node* current=&head;\n        std::unique_lock<std::mutex> lk(head.m);\n        while(node* const next=current->next.get())\n        {\n            std::unique_lock<std::mutex> next_lk(next->m);\n            lk.unlock();\n            if(p(*next->data))    \n            {\n                return next->data;    \n            }\n            current=next;\n            lk=std::move(next_lk);\n        }\n        return std::shared_ptr<T>();\n    }\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n1@\n1#\n1$\n1%\n1^\n\n\n202\nCHAPTER 6\nDesigning lock-based concurrent data structures\n    template<typename Predicate>\n    void remove_if(Predicate p)    \n    {\n        node* current=&head;\n        std::unique_lock<std::mutex> lk(head.m);\n        while(node* const next=current->next.get())\n        {\n            std::unique_lock<std::mutex> next_lk(next->m);\n            if(p(*next->data))                            \n            {\n                std::unique_ptr<node> old_next=std::move(current->next);\n                current->next=std::move(next->next);    \n                next_lk.unlock();\n            }                   \n            else\n            {\n                lk.unlock();     \n                current=next;\n                lk=std::move(next_lk);\n            }\n        }\n    }\n};\nThe threadsafe_list<> from listing 6.13 is a singly linked list, where each entry is a\nnode structure B. A default-constructed node is used for the head of the list, which\nstarts with a NULL next pointer c. New nodes are added with the push_front() func-\ntion; first a new node is constructed e, which allocates the stored data on the heap\nd, while leaving the next pointer as NULL. You then need to acquire the lock on the\nmutex for the head node in order to get the appropriate next value f and insert the\nnode at the front of the list by setting head.next to point to your new node g. So far,\nso good: you only need to lock one mutex in order to add a new item to the list, so\nthere’s no risk of deadlock. Also, the slow memory allocation happens outside the\nlock, so the lock is only protecting the update of a couple of pointer values that can’t\nfail. On to the iterative functions.\n First up, let’s look at for_each() h. This operation takes a Function of some type\nto apply to each element in the list; in common with most standard library algorithms,\nit takes this function by value and will work with either a genuine function or an\nobject of a type with a function call operator. In this case, the function must accept a\nvalue of type T as the sole parameter. Here’s where you do the hand-over-hand lock-\ning. To start with, you lock the mutex on the head node i. It’s then safe to obtain the\npointer to the next node (using get() because you’re not taking ownership of the\npointer). If that pointer isn’t NULL j, you lock the mutex on that node 1) in order to\nprocess the data. Once you have the lock on that node, you can release the lock on\nthe previous node 1! and call the specified function 1@. Once the function completes,\nyou can update the current pointer to the node you processed and move the owner-\nship of the lock from next_lk out to lk 1#. Because for_each passes each data item\n1&\n1*\n1(\n2)\n2!\n\n\n203\nSummary\ndirectly to the supplied Function, you can use this to update the items if necessary,\nor copy them into another container, or whatever. This is entirely safe if the func-\ntion is well behaved, because the mutex for the node holding the data item is held\nacross the call.\n find_first_if() 1$ is similar to for_each(); the crucial difference is that the\nsupplied Predicate must return true to indicate a match or false to indicate no\nmatch 1%. Once you have a match, you return the found data 1^, rather than continu-\ning to search. You could do this with for_each(), but it would needlessly continue\nprocessing the rest of the list even once a match had been found.\n remove_if() 1& is slightly different, because this function has to update the list;\nyou can’t use for_each() for this. If the Predicate returns true 1*, you remove the\nnode from the list by updating current->next 1(. Once you’ve done that, you can\nrelease the lock held on the mutex for the next node. The node is deleted when the\nstd::unique_ptr<node> you moved it into goes out of scope 2). In this case, you don’t\nupdate current because you need to check the new next node. If the Predicate\nreturns false, you want to move on as before 2!.\n So, are there any deadlocks or race conditions with all these mutexes? The answer\nhere is quite definitely no, provided that the supplied predicates and functions are\nwell behaved. The iteration is always one way, always starting from the head node, and\nalways locking the next mutex before releasing the current one, so there’s no possibil-\nity of different lock orders in different threads. The only potential candidate for a\nrace condition is the deletion of the removed node in remove_if() 2), because you\ndo this after you’ve unlocked the mutex (it’s undefined behavior to destroy a locked\nmutex). But a few moments’ thought reveals that this is indeed safe, because you still\nhold the mutex on the previous node (current), so no new thread can try to acquire\nthe lock on the node you’re deleting.\n What about opportunities for concurrency? The whole point of this fine-grained\nlocking was to improve the possibilities for concurrency over a single mutex, so have\nyou achieved that? Yes, you have: different threads can be working on different nodes\nin the list at the same time, whether they’re processing each item with for_each(),\nsearching with find_first_if(), or removing items with remove_if(). But because\nthe mutex for each node must be locked in turn, the threads can’t pass each other. If\none thread is spending a long time processing a particular node, other threads will\nhave to wait when they reach that particular node.\nSummary\nThis chapter started by looking at what it means to design a data structure for con-\ncurrency and providing some guidelines for doing so. We then worked through sev-\neral common data structures (stack, queue, hash map, and linked list), looking at\nhow to apply those guidelines to implement them in a way designed for concurrent\naccess, using locks to protect the data and prevent data races. You should now be\n",
      "page_number": 217
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 227-245)",
      "start_page": 227,
      "end_page": 245,
      "detection_method": "topic_boundary",
      "content": "204\nCHAPTER 6\nDesigning lock-based concurrent data structures\nable to look at the design of your own data structures to see where the opportunities\nfor concurrency lie and where there’s potential for race conditions.\n In chapter 7 we’ll look at ways of avoiding locks entirely, using the low-level atomic\noperations to provide the necessary ordering constraints, while sticking to the same\nset of guidelines.\n\n\n205\nDesigning lock-free\nconcurrent data structures\nIn the last chapter we looked at general aspects of designing data structures for\nconcurrency, with guidelines for thinking about the design to ensure they’re safe.\nWe then examined several common data structures and looked at example imple-\nmentations that used mutexes and locks to protect the shared data. The first cou-\nple of examples used one mutex to protect the entire data structure, but later ones\nused more than one to protect various smaller parts of the data structure and allow\ngreater levels of concurrency in accesses to the data structure.\n Mutexes are powerful mechanisms for ensuring that multiple threads can safely\naccess a data structure without encountering race conditions or broken invariants.\nIt’s also relatively straightforward to reason about the behavior of code that uses\nthem: either the code has the lock on the mutex protecting the data or it doesn’t.\nBut it’s not all a bed of roses; you saw in chapter 3 how the incorrect use of locks\nThis chapter covers\nImplementations of data structures designed for \nconcurrency without using locks\nTechniques for managing memory in lock-free \ndata structures\nSimple guidelines to aid in the writing of lock-free \ndata structures\n\n\n206\nCHAPTER 7\nDesigning lock-free concurrent data structures\ncan lead to deadlock, and you’ve seen with the lock-based queue and lookup table\nexamples how the granularity of locking can affect the potential for true concurrency.\nIf you can write data structures that are safe for concurrent access without locks,\nthere’s the potential to avoid these problems. This data structure is called a lock-free\ndata structure.\n In this chapter we’ll look at how the memory-ordering properties of the atomic\noperations introduced in chapter 5 can be used to build lock-free data structures. It is\nvital for the understanding of this chapter that you have read and understood all of\nchapter 5. You need to take extreme care when designing these data structures,\nbecause they’re hard to get right, and the conditions that cause the design to fail may\noccur very rarely. We’ll start by looking at what it means for data structures to be lock-\nfree; then we’ll move on to the reasons for using them before working through some\nexamples and drawing out some general guidelines.\n7.1\nDefinitions and consequences\nAlgorithms and data structures that use mutexes, condition variables, and futures to\nsynchronize the data are called blocking data structures and algorithms. The applica-\ntion calls library functions that will suspend the execution of a thread until another\nthread performs an action. These library calls are termed blocking calls because the\nthread can’t progress past this point until the block is removed. Typically, the OS will\nsuspend a blocked thread completely (and allocate its time slices to another thread)\nuntil it’s unblocked by the appropriate action of another thread, whether that’s unlock-\ning a mutex, notifying a condition variable, or making a future ready.\n Data structures and algorithms that don’t use blocking library functions are said to\nbe nonblocking. Not all these data structures are lock-free, though, so let’s look at the\nvarious types of nonblocking data structures.\n7.1.1\nTypes of nonblocking data structures\nBack in chapter 5, we implemented a basic mutex using std::atomic_flag as a spin\nlock. The code is reproduced in the following listing.\nclass spinlock_mutex\n{\n    std::atomic_flag flag;\npublic:\n    spinlock_mutex():\n        flag(ATOMIC_FLAG_INIT)\n    {}\n    void lock()\n    {\n        while(flag.test_and_set(std::memory_order_acquire));\n    }\n    void unlock()\n    {\nListing 7.1\nImplementation of a spin-lock mutex using std::atomic_flag\n\n\n207\nDefinitions and consequences\n        flag.clear(std::memory_order_release);\n    }\n};\nThis code doesn’t call any blocking functions; lock() keeps looping until the call to\ntest_and_set() returns false. This is why it gets the name spin lock—the code “spins”\naround the loop. There are no blocking calls, so any code that uses this mutex to pro-\ntect shared data is consequently nonblocking. It’s not lock-free, though. It’s still a mutex\nand can still be locked by only one thread at a time. For that reason, knowing something\nis nonblocking is not enough in most circumstances. Instead, you need to know which\n(if any) of the more specific terms defined here apply. These are\nObstruction-Free—If all other threads are paused, then any given thread will com-\nplete its operation in a bounded number of steps.\nLock-Free—If multiple threads are operating on a data structure, then after a\nbounded number of steps one of them will complete its operation.\nWait-Free—Every thread operating on a data structure will complete its opera-\ntion in a bounded number of steps, even if other threads are also operating on\nthe data structure.\nFor the most part, obstruction-free algorithms aren't particularly useful—it's not often\nthat all other threads are paused, so this is more useful as a characterization of a failed\nlock-free implementation. Let’s look more at what's involved in these characteriza-\ntions, starting with lock-free so you can see what kinds of data structures are covered.\n7.1.2\nLock-free data structures\nFor a data structure to qualify as lock-free, more than one thread must be able to\naccess the data structure concurrently. They don’t have to be able to do the same\noperations; a lock-free queue might allow one thread to push and one to pop but\nbreak if two threads try to push new items at the same time. Not only that, but if one of\nthe threads accessing the data structure is suspended by the scheduler midway\nthrough its operation, the other threads must still be able to complete their opera-\ntions without waiting for the suspended thread.\n Algorithms that use compare/exchange operations on the data structure often\nhave loops in them. The reason for using a compare/exchange operation is that\nanother thread might have modified the data in the meantime, in which case the code\nwill need to redo part of its operation before trying the compare/exchange again.\nThis code can still be lock-free if the compare/exchange would eventually succeed if\nthe other threads were suspended. If it didn’t, you’d have a spin lock, which is non-\nblocking but not lock-free.\n Lock-free algorithms with these loops can result in one thread being subject to star-\nvation. If another thread performs operations with the “wrong” timing, the other\nthread might make progress but the first thread continually has to retry its operation.\nData structures that avoid this problem are wait-free as well as lock-free.\n\n\n208\nCHAPTER 7\nDesigning lock-free concurrent data structures\n7.1.3\nWait-free data structures\nA wait-free data structure is a lock-free data structure with the additional property that\nevery thread accessing the data structure can complete its operation within a bounded\nnumber of steps, regardless of the behavior of other threads. Algorithms that can\ninvolve an unbounded number of retries because of clashes with other threads are not\nwait-free. Most of the examples in this chapter have that property—they have a while\nloop on a compare_exchange_weak or compare_exchange_strong operation, with no\nupper bound on the number of times the loop can run. The scheduling of threads by\nthe OS may mean that a given thread can loop an exceedingly large number of times,\nbut other threads loop very few times. These operations are thus not wait-free.\n Writing wait-free data structures correctly is extremely hard. In order to ensure\nthat every thread can complete its operations within a bounded number of steps,\nyou have to ensure that each operation can be performed in a single pass and that\nthe steps performed by one thread don’t cause an operation on another thread to\nfail. This can make the overall algorithms for the various operations considerably\nmore complex.\n Given how hard it is to get a lock-free or wait-free data structure right, you need\nsome pretty good reasons to write one; you need to be sure that the benefit outweighs\nthe cost. Let’s therefore examine the points that affect the balance.\n7.1.4\nThe pros and cons of lock-free data structures\nWhen it comes down to it, the primary reason for using lock-free data structures is to\nenable maximum concurrency. With lock-based containers, there’s always the poten-\ntial for one thread to have to block and wait for another to complete its operation\nbefore the first thread can proceed; preventing concurrency through mutual exclu-\nsion is the entire purpose of a mutex lock. With a lock-free data structure, some thread\nmakes progress with every step. With a wait-free data structure, every thread can make\nforward progress, regardless of what the other threads are doing; there’s no need for\nwaiting. This is a desirable property to have but hard to achieve. It’s all too easy to end\nup writing what’s essentially a spin lock.\n A second reason to use lock-free data structures is robustness. If a thread dies while\nholding a lock, that data structure is broken forever. But if a thread dies partway\nthrough an operation on a lock-free data structure, nothing is lost except that thread’s\ndata; other threads can proceed normally.\n The flip side here is that if you can’t exclude threads from accessing the data struc-\nture, then you must be careful to ensure that the invariants are upheld or choose\nalternative invariants that can be upheld. Also, you must pay attention to the ordering\nconstraints you impose on the operations. To avoid the undefined behavior associated\nwith a data race, you must use atomic operations for the modifications. But that alone\nisn’t enough; you must ensure that changes become visible to other threads in the cor-\nrect order. All this means that writing thread-safe data structures without using locks is\nconsiderably harder than writing them with locks.\n\n\n209\nExamples of lock-free data structures\n Because there aren’t any locks, deadlocks are impossible with lock-free data struc-\ntures, although there is the possibility of live locks instead. A live lock occurs when two\nthreads each try to change the data structure, but for each thread, the changes made\nby the other require the operation to be restarted, so both threads loop and try again.\nImagine two people trying to go through a narrow gap. If they both go at once, they\nget stuck, so they have to come out and try again. Unless someone gets there first\n(either by agreement, by being faster, or by sheer luck), the cycle will repeat. As in this\nsimple example, live locks are typically short-lived because they depend on the exact\nscheduling of threads. They therefore sap performance rather than cause long-term\nproblems, but they’re still something to watch out for. By definition, wait-free code\ncan’t suffer from live lock because there’s always an upper limit on the number of\nsteps needed to perform an operation. The flip side here is that the algorithm is likely\nmore complex than the alternative and may require more steps even when no other\nthread is accessing the data structure.\n This brings us to another downside of lock-free and wait-free code: although it can\nincrease the potential for concurrency of operations on a data structure and reduce\nthe time an individual thread spends waiting, it may well decrease overall performance.\nFirst, the atomic operations used for lock-free code can be much slower than non-\natomic operations, and there’ll likely be more of them in a lock-free data structure than\nin the mutex locking code for a lock-based data structure. Not only that, but the hard-\nware must synchronize data between threads that access the same atomic variables. As\nyou’ll see in chapter 8, the cache ping-pong associated with multiple threads accessing\nthe same atomic variables can be a significant performance drain. As with everything,\nit’s important to check the relevant performance aspects (whether that’s worst-case wait\ntime, average wait time, overall execution time, or something else) both with a lock-\nbased data structure and a lock-free one before committing either way.\n Now let’s look at some examples.\n7.2\nExamples of lock-free data structures\nIn order to demonstrate some of the techniques used in designing lock-free data\nstructures, we’ll look at the lock-free implementation of a series of simple data struc-\ntures. Not only will each example describe the implementation of a useful data\nstructure, but I’ll use the examples to highlight particular aspects of lock-free data\nstructure design.\n As already mentioned, lock-free data structures rely on the use of atomic opera-\ntions and the associated memory-ordering guarantees in order to ensure that data\nbecomes visible to other threads in the correct order. Initially, we’ll use the default\nmemory_order_seq_cst memory ordering for all atomic operations, because that’s the\neasiest to reason about (remember that all memory_order_seq_cst operations form a\ntotal order). But for later examples we’ll look at reducing some of the ordering con-\nstraints to memory_order_acquire, memory_order_release, or even memory_order_\nrelaxed. Although none of these examples use mutex locks directly, it’s worth bearing\n\n\n210\nCHAPTER 7\nDesigning lock-free concurrent data structures\nin mind that only std::atomic_flag is guaranteed not to use locks in the implemen-\ntation. On some platforms, what appears to be lock-free code might be using locks\ninternal to the C++ Standard Library implementation (see chapter 5 for more details).\nOn these platforms, a simple lock-based data structure might be more appropriate,\nbut there’s more to it than that; before choosing an implementation, you must iden-\ntify your requirements and profile the various options that meet those requirements.\n So, back to the beginning with the simplest of data structures: a stack.\n7.2.1\nWriting a thread-safe stack without locks\nThe basic premise of a stack is relatively simple: nodes are retrieved in the reverse\norder to which they were added—last in, first out (LIFO). It’s therefore important\nto ensure that once a value is added to the stack, it can safely be retrieved immedi-\nately by another thread, and it’s also important to ensure that only one thread\nreturns a given value. The simplest stack is a linked list; the head pointer identifies\nthe first node (which will be the next to retrieve), and each node then points to the\nnext node in turn.\n Under this scheme, adding a node is relatively simple:\n1\nCreate a new node.\n2\nSet its next pointer to the current head node.\n3\nSet the head node to point to it.\nThis works fine in a single-threaded context, but if other threads are also modifying\nthe stack, it’s not enough. Crucially, if two threads are adding nodes, there’s a race\ncondition between steps 2 and 3: a second thread could modify the value of head\nbetween when your thread reads it in step 2 and you update it in step 3. This would\nthen result in the changes made by that other thread being discarded or something\neven worse. Before we look at addressing this race condition, it’s also important to\nnote that once head has been updated to point to your new node, another thread\ncould read that node. It’s therefore vital that your new node is thoroughly prepared\nbefore head is set to point to it; you can’t modify the node afterward.\n OK, so what can you do about this nasty race condition? The answer is to use an\natomic compare/exchange operation at step 3 to ensure that head hasn’t been modi-\nfied since you read it in step 2. If it has, you can loop and try again. The following list-\ning shows how you can implement a thread-safe push() without locks.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node\n    {\n        T data;\n        node* next;\nListing 7.2\nImplementing push() without locks\n\n\n211\nExamples of lock-free data structures\n        node(T const& data_):    \n            data(data_)\n        {}\n    };\n    std::atomic<node*> head;\npublic:\n    void push(T const& data)\n    {\n        node* const new_node=new node(data);   \n        new_node->next=head.load();                     \n        while(!head.compare_exchange_weak(new_node->next,new_node));   \n    }\n};\nThis code neatly matches the preceding three-point plan: create a new node c, set\nthe node’s next pointer to the current head d, and set the head pointer to the new\nnode e. By populating the data in the node structure itself from the node construc-\ntor B, you’ve ensured that the node is ready to roll as soon as it’s constructed, so\nthat’s the easy problem solved. Then you use compare_exchange_weak() to ensure\nthat the head pointer still has the same value as you stored in new_node->next d, and\nyou set it to new_node if so. This bit of code also uses a nifty part of the com-\npare/exchange functionality: if it returns false to indicate that the comparison\nfailed (for example, because head was modified by another thread), the value sup-\nplied as the first parameter (new_node->next) is updated to the current value of\nhead. You therefore don’t have to reload head each time through the loop, because\nthe compiler does that for you. Also, because you’re looping directly on failure, you\ncan use compare_exchange_weak, which can result in more optimal code than\ncompare_exchange_strong on some architectures (see chapter 5).\n So, you might not have a pop() operation yet, but you can quickly check push()\nfor compliance with the guidelines. The only place that can throw an exception is\nthe construction of the new node B, but this will clean up after itself, and the list\nhasn’t been modified yet, so that’s perfectly safe. Because you build the data to be\nstored as part of the node, and you use compare_exchange_weak() to update the\nhead pointer, there are no problematic race conditions here. Once the compare/\nexchange succeeds, the node is on the list and ready for the taking. There are no\nlocks, so there’s no possibility of deadlock, and your push() function passes with fly-\ning colors.\n Now that you have a means of adding data to the stack, you need a way to remove\nit. On the face of it, this is quite simple:\n1\nRead the current value of head.\n2\nRead head->next.\n3\nSet head to head->next.\n4\nReturn the data from the retrieved node.\n5\nDelete the retrieved node.\nb\nc\nd\ne\n\n\n212\nCHAPTER 7\nDesigning lock-free concurrent data structures\nBut in the presence of multiple threads, this isn’t so simple. If there are two threads\nremoving items from the stack, they both might read the same value of head at step 1.\nIf one thread then proceeds all the way through to step 5 before the other gets to\nstep 2, the second thread will be dereferencing a dangling pointer. This is one of the\nbiggest issues in writing lock-free code, so for now you’ll leave out step 5 and leak\nthe nodes.\n This doesn’t resolve all the problems, though. There’s another problem: if two\nthreads read the same value of head, they’ll return the same node. This violates the\nintent of the stack data structure, so you need to avoid this. You can resolve this the\nsame way you resolved the race in push(): use compare/exchange to update head. If\nthe compare/exchange fails, either a new node has been pushed on or another\nthread popped the node you were trying to pop. Either way, you need to return to step 1\n(although the compare/exchange call rereads head for you).\n Once the compare/exchange call succeeds, you know you’re the only thread that’s\npopping the given node off the stack, so you can safely execute step 4. Here’s a first try\nat pop():\ntemplate<typename T>\nclass lock_free_stack\n{\npublic:\n    void pop(T& result)\n    {\n        node* old_head=head.load();\n        while(!head.compare_exchange_weak(old_head,old_head->next));\n        result=old_head->data;\n    }\n};\nAlthough this is nice and succinct, there are still a couple of problems aside from the\nleaking node. First, it doesn’t work on an empty list: if head is a null pointer, it will\ncause undefined behavior as it tries to read the next pointer. This is easily fixed by\nchecking for nullptr in the while loop and either throwing an exception on an\nempty stack or returning a bool to indicate success or failure.\n The second problem is an exception-safety issue. When we first introduced the\nthread-safe stack back in chapter 3, you saw how returning the object by value left you\nwith an exception safety issue: if an exception is thrown when copying the return\nvalue, the value is lost. In that case, passing in a reference to the result was an accept-\nable solution because you could ensure that the stack was left unchanged if an excep-\ntion was thrown. Unfortunately, here you don’t have that luxury; you can only safely\ncopy the data once you know you’re the only thread returning the node, which means\nthe node has already been removed from the queue. Consequently, passing in the tar-\nget for the return value by reference is no longer an advantage: you might as well\nreturn by value. If you want to return the value safely, you have to use the other option\nfrom chapter 3: return a (smart) pointer to the data value.\n\n\n213\nExamples of lock-free data structures\n If you return a smart pointer, you can return nullptr to indicate that there’s no\nvalue to return, but this requires that the data be allocated on the heap. If you do the\nheap allocation as part of pop(), you’re still no better off, because the heap allocation\nmight throw an exception. Instead, you can allocate the memory when you push() the\ndata onto the stack—you have to allocate memory for the node anyway. Returning\nstd::shared_ptr<> won’t throw an exception, so pop() is now safe. Putting all this\ntogether gives the following listing.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;   \n        node* next;\n        node(T const& data_):\n            data(std::make_shared<T>(data_))    \n        {}\n    };\n    std::atomic<node*> head;\npublic:\n    void push(T const& data)\n    {\n        node* const new_node=new node(data);\n        new_node->next=head.load();\n        while(!head.compare_exchange_weak(new_node->next,new_node));\n    }\n    std::shared_ptr<T> pop()\n    {\n        node* old_head=head.load();\n        while(old_head &&                      \n            !head.compare_exchange_weak(old_head,old_head->next));\n        return old_head ? old_head->data : std::shared_ptr<T>();     \n    }\n};\nThe data is held by the pointer now B, so you have to allocate the data on the heap in\nthe node constructor c. You also have to check for a null pointer before you derefer-\nence old_head in the compare_exchange_weak() loop d. Finally, you either return\nthe data associated with your node, if there is one, or a null pointer if not e. Note that\nalthough this is lock-free, it’s not wait-free, because the while loops in both push() and\npop() could in theory loop forever if the compare_exchange_weak() keeps failing.\n If you have a garbage collector picking up after you (like in managed languages\nsuch as C# or Java), you’re finished; the old node will be collected and recycled once\nit’s no longer being accessed by any threads. But not many C++ compilers ship with a\ngarbage collector, so you generally have to tidy up after yourself. \nListing 7.3\nA lock-free stack that leaks nodes\nData is now held \nby pointer\nb\nCreate std::shared_ptr \nfor newly allocated T\nc\nCheck old_head is not \na null pointer before \nyou dereference it\nd\ne\n\n\n214\nCHAPTER 7\nDesigning lock-free concurrent data structures\n7.2.2\nStopping those pesky leaks: managing memory in lock-free \ndata structures\nWhen you first looked at pop(), you opted to leak nodes in order to avoid the race\ncondition where one thread deletes a node while another thread still holds a pointer\nto it that it’s about to dereference. But leaking memory isn’t acceptable in any sensible\nC++ program, so you have to do something about that. Now it’s time to look at the\nproblem and work out a solution.\n The basic problem is that you want to free a node, but you can’t do so until you’re\nsure there are no other threads that still hold pointers to it. If only one thread ever\ncalls pop() on a particular stack instance, you’re home free. Nodes are created in calls\nto push(), and push() doesn't access the contents of existing nodes, so the only\nthreads that can access a given node are the thread that added that node to the stack,\nand any threads that call pop(). push() doesn’t touch the node once it’s been added\nto the stack, so that leaves the threads that call pop()—if there's only one of them,\nthen the thread that called pop() must be the only thread that can touch the node,\nand it can safely delete it.\n On the other hand, if you need to handle multiple threads calling pop() on the\nsame stack instance, you need some way to track when it’s safe to delete a node. This\nmeans you need to write a special-purpose garbage collector for nodes. Now, this\nmight sound scary, and although it’s certainly tricky, it’s not that bad: you’re only\nchecking for nodes, and you’re only checking for nodes accessed from pop(). You’re\nnot worried about nodes in push(), because they’re only accessible from one thread\nuntil they’re on the stack, whereas multiple threads might be accessing the same node\nin pop().\n If there are no threads calling pop(), it’s perfectly safe to delete all the nodes cur-\nrently awaiting deletion. Therefore, if you add the nodes to a “to be deleted” list when\nyou’ve extracted the data, then you can delete them all when there are no threads call-\ning pop(). How do you know there aren’t any threads calling pop()? Simple—count\nthem. If you increment a counter on entry and decrement that counter on exit, it’s\nsafe to delete the nodes from the “to be deleted” list when the counter is zero. It will\nhave to be an atomic counter so it can safely be accessed from multiple threads. The\nfollowing listing shows the amended pop() function, and listing 7.5 shows the sup-\nporting functions for this implementation.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    std::atomic<unsigned> threads_in_pop;    \n    void try_reclaim(node* old_head);\npublic:\n    std::shared_ptr<T> pop()\n    {\nListing 7.4\nReclaiming nodes when no threads are in pop()\nAtomic \nvariable\nb\n\n\n215\nExamples of lock-free data structures\n        ++threads_in_pop;            \n        node* old_head=head.load();\n        while(old_head &&\n              !head.compare_exchange_weak(old_head,old_head->next));\n        std::shared_ptr<T> res;\n        if(old_head)\n        {\n            res.swap(old_head->data);   \n        }\n        try_reclaim(old_head);   \n        return res;\n    }\n};\nThe atomic variable threads_in_pop B is used to count the threads currently trying\nto pop an item off the stack. It’s incremented at the start of pop() c, and decre-\nmented inside try_reclaim(), which is called once the node has been removed e.\nBecause you’re going to potentially delay the deletion of the node itself, you can use\nswap() to remove the data from the node d rather than copying the pointer, so that\nthe data will be deleted automatically when you no longer need it rather than it being\nkept alive because there’s still a reference in a not-yet-deleted node. The next listing\nshows what goes into try_reclaim().\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    std::atomic<node*> to_be_deleted;\n    static void delete_nodes(node* nodes)\n    {\n        while(nodes)\n        {\n            node* next=nodes->next;\n            delete nodes;\n            nodes=next;\n         }\n    }\n    void try_reclaim(node* old_head)\n    {\n        if(threads_in_pop==1)    \n        {                                         \n            node* nodes_to_delete=to_be_deleted.exchange(nullptr); \n            if(!--threads_in_pop)                \n            {\n                delete_nodes(nodes_to_delete);    \n            }\n            else if(nodes_to_delete)    \n            {\n                chain_pending_nodes(nodes_to_delete);   \n            }\nListing 7.5\nThe reference-counted reclamation machinery\nIncrease counter before \ndoing anything else\nc\nReclaim deleted \nnodes if you can\nd\nExtract data from node \nrather than copying pointer\ne\nb\nClaim list of\nto-be-deleted\nnodes\nc\nAre you the only \nthread in pop()?\nd\ne\nf\ng\n\n\n216\nCHAPTER 7\nDesigning lock-free concurrent data structures\n            delete old_head;    \n        }\n        else\n        {\n            chain_pending_node(old_head);   \n            --threads_in_pop;\n        }\n    }\n    void chain_pending_nodes(node* nodes)\n    {\n        node* last=nodes;\n        while(node* const next=last->next)    \n        {\n            last=next;\n        }\n        chain_pending_nodes(nodes,last);\n    }\n    void chain_pending_nodes(node* first,node* last)\n    {\n        last->next=to_be_deleted;                   \n        while(!to_be_deleted.compare_exchange_weak(   \n                  last->next,first));\n    }\n    void chain_pending_node(node* n)\n    {\n        chain_pending_nodes(n,n);   \n    }\n};\nIf the count of threads_in_pop is 1 when you’re trying to reclaim the node B,\nyou’re the only thread currently in pop(), which means it’s safe to delete the node\nyou just removed h, and it may also be safe to delete the pending nodes. If the count\nis not 1, it’s not safe to delete any nodes, so you have to add the node to the pending\nlist i.\n Assume for a moment that threads_in_pop is 1. You now need to try to reclaim\nthe pending nodes; if you don’t, they’ll stay pending until you destroy the stack. To do\nthis, you first claim the list for yourself with an atomic exchange operation c, and\nthen decrement the count of threads_in_pop d. If the count is zero after the decre-\nment, you know that no other thread can be accessing this list of pending nodes.\nThere may be new pending nodes, but you’re not bothered about them for now, as\nlong as it’s safe to reclaim your list. You can then call delete_nodes to iterate down\nthe list and delete them e.\n If the count is not zero after the decrement, it’s not safe to reclaim the nodes, so if\nthere are any f, you must chain them back onto the list of nodes pending deletion\ng. This can happen if there are multiple threads accessing the data structure concur-\nrently. Other threads might have called pop() in between the first test of threads_\nin_pop B and the “claiming” of the list c, potentially adding new nodes to the list\nthat are still being accessed by one or more of those other threads. In figure 7.1, thread\nC adds node Y to the to_be_deleted list, even though thread B is still referencing it as\nh\ni\nFollow the next pointer \nchain to the end.\nj\n1)\nLoop to guarantee that \nlast->next is correct.\n1!\n1@\n\n\n217\nExamples of lock-free data structures\nhead\nZ\nthreads_in_pop == 2\nThread C calls pop() and runs until pop() returns\nA\nto_be_deleted\nY\n(Threads A and B. C is done)\nX\nY\nhead\nZ\nA\nto_be_deleted\nInitially\nthreads_in_pop == 0\nY\nhead\nZ\nthreads_in_pop == 2\nold_head\nThread B calls pop() and is preempted after the first read of head\nA\nto_be_deleted\n(Threads A and B)\nY\nhead\nZ\nthreads_in_pop == 1\nA\nto_be_deleted\n(Thread A)\nThread A calls\nand is preempted in\npop()\ntry_reclaim()\nthreads_in_pop\nafter first read of\nthreads_in_pop == 2\nA\nnodes_to_delete\nY\nto_be_deleted\nnullptr\nhead\nZ\nThread A resumes and is then preempted after only executing\nto_be_deleted.exchange(nullptr)\nIf we don't test\nagainnodes Y and A will bedeleted\nthreads_in_pop\nthreads_in_pop == 2\nY\nold_head\nhead\nZ\nThread B resumes and reads\nfor the\nold_head->next\ncall\ncompare_exchange_strong()\nNode Y is on A's\nlist\nto_be_deleted\nFigure 7.1\nThree threads call pop() concurrently, showing why you \nmust check threads_in_pop after claiming the nodes to be deleted \nin try_reclaim().\n\n\n218\nCHAPTER 7\nDesigning lock-free concurrent data structures\nold_head, and will try and read its next pointer. Thread A can’t therefore delete the\nnodes without potentially causing undefined behavior for thread B.\n To chain the nodes that are pending deletion onto the pending list, you reuse the\nnext pointer from the nodes to link them together. In the case of relinking an existing\nchain back onto the list, you traverse the chain to find the end j, replace the next\npointer from the last node with the current to_be_deleted pointer 1), and store the\nfirst node in the chain as the new to_be_deleted pointer 1!. You have to use\ncompare_exchange_weak in a loop here in order to ensure that you don’t leak any\nnodes that have been added by another thread. This has the benefit of updating the\nnext pointer from the end of the chain if it has been changed. Adding a single node\nonto the list is a special case where the first node in the chain to be added is the same\nas the last one 1@.\n This works reasonably well in low-load situations, where there are suitable quies-\ncent points at which no threads are in pop(). But this is potentially a transient situa-\ntion, which is why you need to test that the threads_in_pop count decrements to zero\nd before doing the reclaim and why this test occurs before you delete the just-\nremoved node h. Deleting a node is potentially a time-consuming operation, and you\nwant the window in which other threads can modify the list to be as small as possible.\nThe longer the time between when the thread first finds threads_in_pop to be equal\nto 1 and the attempt to delete the nodes, the more chance there is that another\nthread has called pop(), and that threads_in_pop is no longer equal to 1, preventing\nthe nodes from being deleted.\n In high-load situations, there may never be this quiescent state, because other\nthreads have entered pop() before all the threads initially in pop() have left. Under\nthis scenario, the to_be_deleted list would grow without bounds, and you’d be leak-\ning memory again. If there aren’t going to be any quiescent periods, you need to find\nan alternative mechanism for reclaiming the nodes. The key is to identify when no\nmore threads are accessing a particular node so that it can be reclaimed. By far the\neasiest such mechanism to reason about is the use of hazard pointers.\n7.2.3\nDetecting nodes that can’t be reclaimed using hazard pointers\nThe term hazard pointers is a reference to a technique discovered by Maged Michael.1\nThey are so called because deleting a node that might still be referenced by other\nthreads is hazardous. If other threads do indeed hold references to that node and\nproceed to access the node through that reference, you have undefined behavior.\nThe basic idea is that if a thread is going to access an object that another thread\nmight want to delete, it first sets a hazard pointer to reference the object, informing\nthe other thread that deleting the object would indeed be hazardous. Once the\nobject is no longer needed, the hazard pointer is cleared. If you’ve ever watched the\n1 “Safe Memory Reclamation for Dynamic Lock-Free Objects Using Atomic Reads and Writes,” Maged M.\nMichael, in PODC ’02: Proceedings of the Twenty-first Annual Symposium on Principles of Distributed Com-\nputing (2002), ISBN 1-58113-485-1.\n\n\n219\nExamples of lock-free data structures\nOxford/Cambridge boat race, you’ve seen a similar mechanism used when starting\nthe race: the cox of either boat can raise their hand to indicate that they aren’t ready.\nWhile either cox has their hand raised, the umpire may not start the race. If both\ncoxes have their hands down, the race may start, but a cox may raise their hand again\nif the race hasn’t started and they feel the situation has changed.\n When a thread wants to delete an object, it must first check the hazard pointers\nbelonging to the other threads in the system. If none of the hazard pointers reference\nthe object, it can safely be deleted. Otherwise, it must be left until later. Periodically,\nthe list of objects that have been left until later is checked to see if any of them can\nnow be deleted.\n Described at such a high level, it sounds relatively straightforward, so how do you\ndo this in C++?\n Well, first off you need a location in which to store the pointer to the object you’re\naccessing, the hazard pointer itself. This location must be visible to all threads, and\nyou need one of these for each thread that might access the data structure. Allocating\nthem correctly and efficiently can be a challenge, so you’ll leave that for later and\nassume you have a function get_hazard_pointer_for_current_thread() that returns\na reference to your hazard pointer. You then need to set it when you read a pointer\nthat you intend to dereference—in this case the head value from the list:\nstd::shared_ptr<T> pop()\n{\n    std::atomic<void*>& hp=get_hazard_pointer_for_current_thread();\n    node* old_head=head.load();     \n    node* temp;\n    do\n    {\n        temp=old_head;\n        hp.store(old_head);    \n        old_head=head.load();\n    } while(old_head!=temp);    \n    // ...\n}\nYou have to do this in a while loop to ensure that the node hasn’t been deleted\nbetween the reading of the old head pointer B and the setting of the hazard pointer\nc. During this window no other thread knows you’re accessing this particular node.\nFortunately, if the old head node is going to be deleted, head itself must have changed,\nso you can check this and keep looping until you know that the head pointer still has the\nsame value you set your hazard pointer to d. Using hazard pointers like this relies on\nthe fact that it's safe to use the value of a pointer after the object it references has been\ndeleted. This is technically undefined behavior if you are using the default implementa-\ntion of new and delete, so either you need to ensure that your implementation permits\nit, or you need to use a custom allocator that permits this usage.\n Now that you’ve set your hazard pointer, you can proceed with the rest of pop(),\nsafe in the knowledge that no other thread will delete the nodes from under you.\nb\nc\nd\n\n\n220\nCHAPTER 7\nDesigning lock-free concurrent data structures\nWell, almost: every time you reload old_head, you need to update the hazard pointer\nbefore you dereference the freshly read pointer value. Once you’ve extracted a node\nfrom the list, you can clear your hazard pointer. If there are no other hazard pointers\nreferencing your node, you can safely delete it; otherwise, you have to add it to a list of\nnodes to be deleted later. The following listing shows a full implementation of pop()\nusing this scheme.\nstd::shared_ptr<T> pop()\n{\n    std::atomic<void*>& hp=get_hazard_pointer_for_current_thread();\n    node* old_head=head.load();\n    do\n    {\n        node* temp;\n        do              \n        {\n            temp=old_head;\n            hp.store(old_head);\n            old_head=head.load();\n        } while(old_head!=temp);\n    }\n    while(old_head &&\n          !head.compare_exchange_strong(old_head,old_head->next));\n    hp.store(nullptr);              \n    std::shared_ptr<T> res;\n    if(old_head)\n    {\n        res.swap(old_head->data);\n        if(outstanding_hazard_pointers_for(old_head))  \n        {\n            reclaim_later(old_head);    \n        }\n        else\n        {\n            delete old_head;     \n        }\n        delete_nodes_with_no_hazards();    \n    }\n    return res;\n}\nFirst off, you’ve moved the loop that sets the hazard pointer inside the outer loop for\nreloading old_head if the compare/exchange fails B. You’re using compare_exchange\n_strong() here because you’re doing work inside the while loop: a spurious failure\non compare_exchange_weak() would result in resetting the hazard pointer unneces-\nsarily. This ensures that the hazard pointer is correctly set before you dereference\nold_head. Once you’ve claimed the node as yours, you can clear your hazard pointer\nc. If you did get a node, you need to check the hazard pointers belonging to other\nthreads to see if they reference it d. If so, you can’t delete it yet, so you must put it on\nListing 7.6\nAn implementation of pop() using hazard pointers\nLoop until you’ve set the \nhazard pointer to head.\nb\nClear hazard pointer \nonce you’re finished\nc\nCheck for hazard \npointers referencing \na node before you \ndelete it.\nd\ne\nf\ng\n\n\n221\nExamples of lock-free data structures\na list to be reclaimed later e; otherwise, you can delete it right away f. Finally, you\nput in a call to check for any nodes for which you had to call reclaim_later(). If\nthere are no longer any hazard pointers referencing those nodes, you can safely\ndelete them g. Any nodes for which there are still outstanding hazard pointers will be\nleft for the next thread that calls pop().\n There’s still a lot of detail hidden in these new functions—get_hazard_pointer_\nfor_current_thread(), reclaim_later(), outstanding_hazard_pointers_for(), and\ndelete_nodes_with_no_hazards()—so let’s draw back the curtain and look at how\nthey work.\n The exact scheme for allocating hazard pointer instances to threads used by\nget_hazard_pointer_for_current_thread() doesn’t matter for the program logic\n(although it can affect the efficiency, as you’ll see later). For now you’ll go with a sim-\nple structure: a fixed-size array of pairs of thread IDs and pointers. get_hazard_\npointer_for_current_thread() then searches through the array to find the first\nfree slot and sets the ID entry of that slot to the ID of the current thread. When the\nthread exits, the slot is freed by resetting the ID entry to a default-constructed\nstd::thread::id(). This is shown in the following listing.\nunsigned const max_hazard_pointers=100;\nstruct hazard_pointer\n{\n    std::atomic<std::thread::id> id;\n    std::atomic<void*> pointer;\n};\nhazard_pointer hazard_pointers[max_hazard_pointers];\nclass hp_owner\n{\n    hazard_pointer* hp;\n     \npublic:\n    hp_owner(hp_owner const&)=delete;\n    hp_owner operator=(hp_owner const&)=delete;\n    hp_owner():                                                        \n        hp(nullptr)\n    {\n        for(unsigned i=0;i<max_hazard_pointers;++i)\n        {\n            std::thread::id old_id;                  \n            if(hazard_pointers[i].id.compare_exchange_strong(  \n                old_id,std::this_thread::get_id()))\n            {\n                hp=&hazard_pointers[i];                                \n                break;\n            }\n        }\n        if(!hp)    \n        {\nListing 7.7\nA simple implementation of get_hazard_pointer_for_current\n_thread()\nTry to claim \nownership \nof a hazard \npointer.\nb\n\n\n222\nCHAPTER 7\nDesigning lock-free concurrent data structures\n            throw std::runtime_error(\"No hazard pointers available\");\n        }\n    }\n    std::atomic<void*>& get_pointer()\n    {\n        return hp->pointer;\n    }\n    ~hp_owner()   \n    {\n        hp->pointer.store(nullptr);\n        hp->id.store(std::thread::id());\n    }\n};\nstd::atomic<void*>& get_hazard_pointer_for_current_thread()    \n{\n    thread_local static hp_owner hazard;   \n    return hazard.get_pointer();    \n}\nThe implementation of get_hazard_pointer_for_current_thread() itself is decep-\ntively simple d: it has a thread_local variable of type hp_owner e, which stores the\nhazard pointer for the current thread. It then returns the pointer from that object f.\nThis works as follows: the first time each thread calls this function, a new instance of\nhp_owner is created. The constructor for this new instance B then searches through\nthe table of owner/pointer pairs looking for an entry without an owner. It uses com-\npare_exchange_strong() to check for an entry without an owner and claim it in one\ngo c. If the compare_exchange_strong() fails, another thread owns that entry, so\nyou move on to the next. If the exchange succeeds, you’ve successfully claimed the\nentry for the current thread, so you store it and stop the search d. If you get to the\nend of the list without finding a free entry e, there are too many threads using haz-\nard pointers, so you throw an exception.\n Once the hp_owner instance has been created for a given thread, further accesses\nare much faster because the pointer is cached, so the table doesn’t have to be\nscanned again.\n When each thread exits, if an instance of hp_owner was created for that thread, then\nit’s destroyed. The destructor then resets the pointer to nullptr before setting the\nowner ID to std::thread::id(), allowing another thread to reuse the entry later f.\n With this implementation of get_hazard_pointer_for_current_thread(), the\nimplementation of outstanding_hazard_pointers_for() is simple—scan through\nthe hazard pointer table looking for entries:\nbool outstanding_hazard_pointers_for(void* p)\n{\n    for(unsigned i=0;i<max_hazard_pointers;++i)\n    {\n        if(hazard_pointers[i].pointer.load()==p)\n        {\n            return true;\n        }\nc\nd\nEach thread has its \nown hazard pointer.\ne\nf\n",
      "page_number": 227
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 246-258)",
      "start_page": 246,
      "end_page": 258,
      "detection_method": "topic_boundary",
      "content": "223\nExamples of lock-free data structures\n    }\n    return false;\n}\nIt’s not even worth checking whether each entry has an owner: unowned entries\nwill have a null pointer, so the comparison will return false anyway, and it simpli-\nfies the code.\n reclaim_later() and delete_nodes_with_no_hazards() can then work on a sim-\nple linked list; reclaim_later() adds nodes to the list, and delete_nodes_with_no_\nhazards() scans through the list, deleting entries with no outstanding hazards. The\nnext listing shows this implementation.\ntemplate<typename T>\nvoid do_delete(void* p)\n{\n    delete static_cast<T*>(p);\n}\nstruct data_to_reclaim\n{\n    void* data;\n    std::function<void(void*)> deleter;\n    data_to_reclaim* next;\n    template<typename T>\n    data_to_reclaim(T* p):    \n        data(p),\n        deleter(&do_delete<T>),\n        next(0)\n    {}\n    ~data_to_reclaim()\n    {\n        deleter(data);   \n    }\n};\nstd::atomic<data_to_reclaim*> nodes_to_reclaim;\nvoid add_to_reclaim_list(data_to_reclaim* node)    \n{\n    node->next=nodes_to_reclaim.load();\n    while(!nodes_to_reclaim.compare_exchange_weak(node->next,node));\n}\ntemplate<typename T>\nvoid reclaim_later(T* data)   \n{\n    add_to_reclaim_list(new data_to_reclaim(data));   \n}\nvoid delete_nodes_with_no_hazards()\n{\n    data_to_reclaim* current=nodes_to_reclaim.exchange(nullptr);   \n    while(current)\n    {\n        data_to_reclaim* const next=current->next;\n        if(!outstanding_hazard_pointers_for(current->data))   \nListing 7.8\nA simple implementation of the reclaim functions\nb\nc\nd\ne\nf\ng\nh\n\n\n224\nCHAPTER 7\nDesigning lock-free concurrent data structures\n        {\n            delete current;   \n        }\n        else\n        {\n            add_to_reclaim_list(current);   \n        }\n        current=next;\n    }\n}\nFirst off, I expect you’ve spotted that reclaim_later() is a function template rather\nthan a plain function, e. This is because hazard pointers are a general-purpose util-\nity, so you don’t want to tie yourselves to stack nodes. You’ve been using std::\natomic<void*> to store the pointers already. You therefore need to handle any\npointer type, but you can’t use void* because you want to delete the data items when\nyou can, and delete requires the real type of the pointer. The constructor of\ndata_to_reclaim handles that nicely, as you’ll see in a minute; reclaim_later() cre-\nates a new instance of data_to_reclaim for your pointer and adds it to the reclaim list\nf. add_to_reclaim_list() itself d is a simple compare_exchange_weak() loop on\nthe list head like you’ve seen before.\n Back to the constructor of data_to_reclaim B: the constructor is also a tem-\nplate. It stores the data to be deleted as a void* in the data member and then stores\na pointer to the appropriate instantiation of do_delete()—a simple function that\ncasts the supplied void* to the chosen pointer type and then deletes the pointed-to\nobject. std::function<> wraps this function pointer safely, so that the destructor of\ndata_to_reclaim can then delete the data by invoking the stored function c.\n The destructor of data_to_reclaim isn’t called when you’re adding nodes to the\nlist; it’s called when there are no more hazard pointers to that node. This is the respon-\nsibility of delete_nodes_with_no_hazards().\n delete_nodes_with_no_hazards() first claims the entire list of nodes to be\nreclaimed for itself with a simple exchange() g. This simple but crucial step ensures\nthat this is the only thread trying to reclaim this particular set of nodes. Other threads\nare now free to add further nodes to the list or even try to reclaim them without\nimpacting the operation of this thread.\n Then, as long as there are still nodes left in the list, you check each node in turn to\nsee if there are any outstanding hazard pointers h. If there aren’t, you can safely\ndelete the entry (and clean up the stored data) i. Otherwise, you add the item back\non the list for reclaiming later j.\n Although this simple implementation does indeed safely reclaim the deleted\nnodes, it adds quite a bit of overhead to the process. Scanning the hazard pointer\narray requires checking max_hazard_pointers atomic variables, and this is done for\nevery pop() call. Atomic operations are inherently slow—often 100 times slower than\nan equivalent non-atomic operation on desktop CPUs—so this makes pop() an expen-\nsive operation. Not only do you scan the hazard pointer list for the node you’re about\ni\nj\n\n\n225\nExamples of lock-free data structures\nto remove, but you also scan it for each node in the waiting list. Clearly this is a bad\nidea. There may well be max_hazard_pointers nodes in the list, and you’re checking\nall of them against max_hazard_pointers stored hazard pointers. Ouch! There has to\nbe a better way.\nBETTER RECLAMATION STRATEGIES USING HAZARD POINTERS\nThere is a better way. What I’ve shown here is a simple and naïve implementation of\nhazard pointers to help explain the technique. The first thing you can do is trade\nmemory for performance. Rather than checking every node on the reclamation list\nevery time you call pop(), you don’t try to reclaim any nodes at all unless there are\nmore than max_hazard_pointers nodes on the list. That way you’re guaranteed to be\nable to reclaim at least one node. If you wait until there are max_hazard_pointers+1\nnodes on the list, you’re not much better off. Once you get to max_hazard_pointers\nnodes, you’ll be trying to reclaim nodes for most calls to pop(), so you’re not doing\nmuch better. But if you wait until there are 2*max_hazard_pointers nodes on the list,\nthen at most max_hazard_pointers of those will still be active, so you’re guaranteed to\nbe able to reclaim at least max_hazard_pointers nodes, and it will then be at least\nmax_hazard_pointers calls to pop() before you try to reclaim any nodes again. This is\nmuch better. Rather than checking around max_hazard_pointers nodes every call to\npush() (and not necessarily reclaiming any), you’re checking 2*max_hazard_pointers\nnodes every max_hazard_pointers calls to pop() and reclaiming at least max_hazard_\npointers nodes. That’s effectively two nodes checked for every pop(), one of which\nis reclaimed.\n Even this has a downside (beyond the increased memory usage from the larger\nreclamation list, and the larger number of potentially reclaimable nodes): you now\nhave to count the nodes on the reclamation list, which means using an atomic count,\nand you still have multiple threads competing to access the reclamation list itself. If\nyou have memory to spare, you can trade increased memory usage for an even better\nreclamation scheme: each thread keeps its own reclamation list in a thread-local vari-\nable. There’s no need for atomic variables for the count or the list access. Instead, you\nhave max_hazard_pointers*max_hazard_pointers nodes allocated. If a thread exits\nbefore all its nodes have been reclaimed, they can be stored in the global list as before\nand added to the local list of the next thread doing a reclamation process.\n Another downside of hazard pointers is that they’re covered by a patent applica-\ntion submitted by IBM.2 Though I believe this patent has now expired, if you write\nsoftware for use in a country where the patents are valid, it is a good idea to get a pat-\nent lawyer to verify that for you, or you need to make sure you have a suitable licensing\narrangement in place. This is something common to many of the lock-free memory\nreclamation techniques; this is an active research area, so large companies are taking\nout patents where they can. You may be asking why I’ve devoted so many pages to a\n2 Maged M. Michael, U.S. Patent and Trademark Office application number 20040107227, “Method for effi-\ncient implementation of dynamic lock-free data structures with safe memory reclamation.” \n\n\n226\nCHAPTER 7\nDesigning lock-free concurrent data structures\ntechnique that people may be unable to use, and that’s a fair question. First, it may be\npossible to use the technique without paying for a license. For example, if you’re\ndeveloping free software licensed under the GPL,3 your software may be covered by\nIBM’s statement of non-assertion.4 Second, and most important, the explanation of\nthe techniques shows some of the things that are important to think about when writ-\ning lock-free code, such as the costs of atomic operations. Finally, there is a proposal\nto incorporate hazard pointers into a future revision of the C++ Standard,5 so it is\ngood to know how they work, even if you will hopefully be able to use your compiler\nvendor’s implementation in the future.\n So, are there any unpatented memory reclamation techniques that can be used\nwith lock-free code? Luckily, there are. One such mechanism is reference counting.\n7.2.4\nDetecting nodes in use with reference counting\nBack in section 7.2.2, you saw that the problem with deleting nodes is detecting which\nnodes are still being accessed by reader threads. If you could safely identify precisely\nwhich nodes were being referenced and when no threads were accessing these nodes,\nyou could delete them. Hazard pointers tackle the problem by storing a list of the\nnodes in use. Reference counting tackles the problem by storing a count of the num-\nber of threads accessing each node.\n This may seem nice and straightforward, but it’s quite hard to manage in practice.\nAt first, you might think that something like std::shared_ptr<> would be up to the\ntask; after all, it’s a reference-counted pointer. Unfortunately, although some opera-\ntions on std::shared_ptr<> are atomic, they aren’t guaranteed to be lock-free.\nAlthough by itself this is no different than any of the operations on the atomic types,\nstd::shared_ptr<> is intended for use in many contexts, and making the atomic\noperations lock-free would likely impose an overhead on all uses of the class. If your\nplatform supplies an implementation for which std::atomic_is_lock_free(&some_\nshared_ptr) returns true, the whole memory reclamation issue goes away. Use\nstd::shared_ptr<node> for the list, as in listing 7.9. Note the need to clear the next\npointer from the popped node in order to avoid the potential for deeply nested\ndestruction of nodes when the last std::shared_ptr referencing a given node is\ndestroyed.\n \n \n \n3 GNU General Public License http://www.gnu.org/licenses/gpl.html.\n4 IBM Statement of Non-Assertion of Named Patents Against OSS, http://www.ibm.com/ibm/licensing/pat-\nents/pledgedpatents.pdf.\n5 P0566: Proposed Wording for Concurrent Data Structures: Hazard Pointer and ReadCopyUpdate (RCU),\nMichael Wong, Maged M. Michael, Paul McKenney, Geoffrey Romer, Andrew Hunter, Arthur O'Dwyer, David\nS. Hollman, JF Bastien, Hans Boehm, David Goldblatt, Frank Birbacher http://www.open-std.org/jtc1/sc22/\nwg21/docs/papers/2018/p0566r5.pdf\n\n\n227\nExamples of lock-free data structures\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::shared_ptr<node> next;\n        node(T const& data_):\n            data(std::make_shared<T>(data_))\n        {}\n    };\n    std::shared_ptr<node> head;\npublic:\n    void push(T const& data)\n    {\n        std::shared_ptr<node> const new_node=std::make_shared<node>(data);\n        new_node->next=std::atomic_load(&head);\n        while(!std::atomic_compare_exchange_weak(&head,\n                  &new_node->next,new_node));\n    }\n    std::shared_ptr<T> pop()\n    {\n        std::shared_ptr<node> old_head=std::atomic_load(&head);\n        while(old_head && !std::atomic_compare_exchange_weak(&head,\n                  &old_head,std::atomic_load(&old_head->next)));\n        if(old_head) {\n            std::atomic_store(&old_head->next,std::shared_ptr<node>());\n            return old_head->data;\n        }\n        return std::shared_ptr<T>();\n    }\n    ~lock_free_stack(){\n        while(pop());\n    }\n};\nNot only is it rare for an implementation to provide lock-free atomic operations on\nstd::shared_ptr<>, but remembering to use the atomic operations consistently is hard.\nThe Concurrency TS helps you out, if you have an implementation available, because it\nprovides std::experimental::atomic_shared_ptr<T> in the <experimental/atomic>\nheader. This is in many ways equivalent to a theoretical std::atomic<std::shared\n_ptr<T>>, except that std::shared_ptr<T> can't be used with std::atomic<>,\nbecause it has nontrivial copy semantics to ensure that the reference count is handled\ncorrectly.  std::experimental::atomic_shared_ptr<T> handles the reference count-\ning correctly, while still ensuring atomic operations. Like the other atomic types\ndescribed in chapter 5, it may or may not be lock-free on any given implementation.\nListing 7.9 can thus be rewritten as in listing 7.10. See how much simpler it is without\nhaving to remember to include the atomic_load and atomic_store calls.\nListing 7.9\nA lock-free stack using a lock-free std::shared_ptr<> implementation\n\n\n228\nCHAPTER 7\nDesigning lock-free concurrent data structures\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::experimental::atomic_shared_ptr<node> next;\n        node(T const& data_):\n            data(std::make_shared<T>(data_))\n        {}\n    };\n    std::experimental::atomic_shared_ptr<node> head;\npublic:\n    void push(T const& data)\n    {\n        std::shared_ptr<node> const new_node=std::make_shared<node>(data);\n        new_node->next=head.load();\n        while(!head.compare_exchange_weak(new_node->next,new_node));\n    }\n    std::shared_ptr<T> pop()\n    {\n        std::shared_ptr<node> old_head=head.load();\n        while(old_head && !head.compare_exchange_weak(\n                  old_head,old_head->next.load()));\n        if(old_head) {\n            old_head->next=std::shared_ptr<node>();\n            return old_head->data;\n        }\n        return std::shared_ptr<T>();\n    }\n    ~lock_free_stack(){\n        while(pop());\n    }\n};\nIn the probable case that your std::shared_ptr<> implementation isn’t lock-free,\nand your implementation doesn’t provide a lock-free std::experimental::atomic_\nshared_ptr<> either, you need to manage the reference counting manually.\n One possible technique involves the use of not one but two reference counts for\neach node: an internal count and an external count. The sum of these values is the\ntotal number of references to the node. The external count is kept alongside the pointer\nto the node and is increased every time the pointer is read. When the reader is fin-\nished with the node, it decreases the internal count. A simple operation that reads the\npointer will leave the external count increased by one and the internal count decreased\nby one when it’s finished.\n When the external count/pointer pairing is no longer required (the node is no\nlonger accessible from a location accessible to multiple threads), the internal count is\nListing 7.10\nStack implementation using std::experimental::atomic\n_shared_ptr<>\n\n\n229\nExamples of lock-free data structures\nincreased by the value of the external count minus one and the external counter is\ndiscarded. Once the internal count is equal to zero, there are no outstanding refer-\nences to the node and it can be safely deleted. It’s still important to use atomic opera-\ntions for updates of shared data. Let’s now look at an implementation of a lock-free\nstack that uses this technique to ensure that the nodes are reclaimed only when it’s\nsafe to do so.\n The following listing shows the internal data structure and the implementation of\npush(), which is nice and straightforward.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node;\n    struct counted_node_ptr   \n    {\n        int external_count;\n        node* ptr;\n    };\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::atomic<int> internal_count;    \n        counted_node_ptr next;           \n        node(T const& data_):\n            data(std::make_shared<T>(data_)),\n            internal_count(0)\n        {}\n    };\n    std::atomic<counted_node_ptr> head;   \npublic:\n    ~lock_free_stack()\n    {\n        while(pop());\n    }\n    void push(T const& data)   \n    {\n        counted_node_ptr new_node;\n        new_node.ptr=new node(data);\n        new_node.external_count=1;\n        new_node.ptr->next=head.load();\n        while(!head.compare_exchange_weak(new_node.ptr->next,new_node));\n    }\n};\nFirst, the external count is wrapped together with the node pointer in the counted_\nnode_ptr structure B. This can then be used for the next pointer in the node struc-\nture, d alongside the internal count c. Because counted_node_ptr is a simple\nstruct, you can use it with the std::atomic<> template for the head of the list e.\nListing 7.11\nPushing a node on a lock-free stack using split reference counts\nb\nc\nd\ne\nf\n\n\n230\nCHAPTER 7\nDesigning lock-free concurrent data structures\n On those platforms that support a double-word-compare-and-swap operation, this\nstructure will be small enough for std::atomic<counted_node_ptr> to be lock-free. If\nit isn’t on your platform, you might be better off using the std::shared_ptr<> ver-\nsion from listing 7.9, because std::atomic<> will use a mutex to guarantee atomicity\nwhen the type is too large for the platform’s atomic instructions (rendering your\n“lock-free” algorithm lock-based after all). Alternatively, if you’re willing to limit the\nsize of the counter, and you know that your platform has spare bits in a pointer (for\nexample, because the address space is only 48 bits but a pointer is 64 bits), you can\nstore the count inside the spare bits of the pointer to fit it all back in a single machine\nword. These tricks require platform-specific knowledge and are thus outside the scope\nof this book.\n push() is relatively simple f. You construct a counted_node_ptr that refers to a\nfreshly allocated node with associated data and set the next value of the node to the\ncurrent value of head. You can then use compare_exchange_weak() to set the value of\nhead, as in the previous listings. The counts are set up so the internal_count is zero,\nand the external_count is one. Because this is a new node, there’s currently only one\nexternal reference to the node (the head pointer itself).\n As usual, the complexities come to light in the implementation of pop(), which is\nshown in the following listing.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    // other parts as in listing 7.11\n    void increase_head_count(counted_node_ptr& old_counter)\n    {\n        counted_node_ptr new_counter;\n        do\n        {\n            new_counter=old_counter;\n            ++new_counter.external_count;\n        }\n        while(!head.compare_exchange_strong(old_counter,new_counter));  \n        old_counter.external_count=new_counter.external_count;\n    }\npublic:\n    std::shared_ptr<T> pop()#\n    {\n        counted_node_ptr old_head=head.load();\n        for(;;)\n        {\n            increase_head_count(old_head);\n            node* const ptr=old_head.ptr;    \n            if(!ptr)\n            {\n                return std::shared_ptr<T>();\n            }\nListing 7.12\nPopping a node from a lock-free stack using split reference counts\nb\nc\n\n\n231\nExamples of lock-free data structures\n            if(head.compare_exchange_strong(old_head,ptr->next))   \n            {\n                std::shared_ptr<T> res;\n                res.swap(ptr->data);        \n                int const count_increase=old_head.external_count-2;   \n                if(ptr->internal_count.fetch_add(count_increase)==   \n                   -count_increase)\n                {\n                    delete ptr;\n                }\n                return res;    \n            }\n            else if(ptr->internal_count.fetch_sub(1)==1)\n            {\n                delete ptr;    \n            }\n        }\n    }\n};\nThis time, once you’ve loaded the value of head, you must first increase the count of\nexternal references to the head node to indicate that you’re referencing it and to\nensure that it’s safe to dereference it. If you dereference the pointer before increasing\nthe reference count, another thread could free the node before you access it, leaving\nyou with a dangling pointer. This is the primary reason for using the split reference\ncount: by incrementing the external reference count, you ensure that the pointer\nremains valid for the duration of your access. The increment is done with a compare\n_exchange_strong() loop B, which compares and sets the whole structure to ensure\nthat the pointer hasn’t been changed by another thread in the meantime.\n Once the count has been increased, you can safely dereference the ptr field of\nthe value loaded from head in order to access the pointed-to node c. If the pointer\nis a null pointer, you’re at the end of the list: no more entries. If the pointer isn’t a\nnull pointer, you can try to remove the node by a compare_exchange_strong() call\non head d.\n If the compare_exchange_strong() succeeds, you’ve taken ownership of the node\nand can swap out the data in preparation for returning it e. This ensures that the\ndata isn’t kept alive just because other threads accessing the stack happen to still have\npointers to its node. Then you can add the external count to the internal count on the\nnode with an atomic fetch_add g. If the reference count is now zero, the previous\nvalue (which is what fetch_add returns) was the negative of what you added, in which\ncase you can delete the node. It’s important to note that the value you add is two less\nthan the external count f; you’ve removed the node from the list, so you drop one\noff the count for that, and you’re no longer accessing the node from this thread, so\nyou drop another off the count for that. Whether or not you deleted the node, you’ve\nfinished, so you can return the data h.\n If the compare/exchange d fails, another thread removed your node before you\ndid, or another thread added a new node to the stack. Either way, you need to start\nd\ne\nf\ng\nh\ni\n\n\n232\nCHAPTER 7\nDesigning lock-free concurrent data structures\nagain with the fresh value of head returned by the compare/exchange call. But first\nyou must decrease the reference count on the node you were trying to remove. This\nthread won’t access it anymore. If you’re the last thread to hold a reference (because\nanother thread removed it from the stack), the internal reference count will be 1, so\nsubtracting 1 will set the count to zero. In this case, you can delete the node here\nbefore you loop i.\n So far, you’ve been using the default std::memory_order_seq_cst memory order-\ning for all your atomic operations. On most systems these are more expensive in terms\nof execution time and synchronization overhead than the other memory orderings,\nand on some systems considerably so. Now that you have the logic of your data struc-\nture right, you can think about relaxing some of these memory-ordering require-\nments; you don’t want to impose any unnecessary overhead on the users of the stack.\nBefore leaving your stack behind and moving on to the design of a lock-free queue,\nlet’s examine the stack operations and ask ourselves, can we use more relaxed mem-\nory orderings for some operations and still get the same level of safety?\n7.2.5\nApplying the memory model to the lock-free stack\nBefore you go about changing the memory orderings, you need to examine the oper-\nations and identify the required relationships between them. You can then go back\nand find the minimum memory orderings that provide these required relationships.\nIn order to do this, you’ll have to look at the situation from the point of view of\nthreads in several different scenarios. The simplest possible scenario has to be where\none thread pushes a data item onto the stack and another thread then pops that data\nitem off the stack some time later, so we’ll start from there.\n In this simple case, three important pieces of data are involved. First is the count-\ned_node_ptr used for transferring the data: head. Second is the node structure that\nhead refers to, and third is the data item pointed to by that node.\n The thread doing the push() first constructs the data item and the node and then\nsets head. The thread doing the pop() first loads the value of head, then does a com-\npare/exchange loop on head to increase the reference count, and then reads the\nnode structure to obtain the next value. Right here you can see a required relation-\nship; the next value is a plain non-atomic object, so in order to read this safely, there\nmust be a happens-before relationship between the store (by the pushing thread) and\nthe load (by the popping thread). Because the only atomic operation in the push() is\nthe compare_exchange_weak(), and you need a release operation to get a happens-\nbefore relationship between threads, the compare_exchange_weak() must be std::\nmemory_order_release or stronger. If the compare_exchange_weak() call fails, noth-\ning has changed and you keep looping, so you need only std::memory_order_\nrelaxed in that case:\nvoid push(T const& data)\n{\n    counted_node_ptr new_node;\n    new_node.ptr=new node(data);\n\n\n233\nExamples of lock-free data structures\n    new_node.external_count=1;\n    new_node.ptr->next=head.load(std::memory_order_relaxed)\n    while(!head.compare_exchange_weak(new_node.ptr->next,new_node,\n        std::memory_order_release,std::memory_order_relaxed));\n}\nWhat about the pop() code? In order to get the happens-before relationship you\nneed, you must have an operation that’s std::memory_order_acquire or stronger\nbefore the access to next. The pointer you dereference to access the next field is the\nold value read by the compare_exchange_strong() in increase_head_count(), so\nyou need the ordering on that if it succeeds. As with the call in push(), if the\nexchange fails, you just loop again, so you can use relaxed ordering on failure:\nvoid increase_head_count(counted_node_ptr& old_counter)\n{\n    counted_node_ptr new_counter;\n    do\n    {\n        new_counter=old_counter;\n        ++new_counter.external_count;\n    }\n    while(!head.compare_exchange_strong(old_counter,new_counter,\n        std::memory_order_acquire,std::memory_order_relaxed));\n    old_counter.external_count=new_counter.external_count;\n}\nIf the compare_exchange_strong() call succeeds, you know that the value read had\nthe ptr field set to what’s now stored in old_counter. Because the store in push() was\na release operation, and this compare_exchange_strong() is an acquire operation,\nthe store synchronizes with the load and you have a happens-before relationship. Con-\nsequently, the store to the ptr field in the push() happens before the ptr->next\naccess in pop(), and you’re safe.\n Note that the memory ordering on the initial head.load() didn’t matter to this\nanalysis, so you can safely use std::memory_order_relaxed for that.\n Next up, let’s consider the compare_exchange_strong() to set head to old_head\n.ptr->next. Do you need anything from this operation to guarantee the data integrity\nof this thread? If the exchange succeeds, you access ptr->data, so you need to ensure\nthat the store to ptr->data in the push() thread happens before the load. But you\nalready have that guarantee: the acquire operation in increase_head_count() ensures\nthat there’s a synchronizes-with relationship between the store in the push() thread\nand that compare/exchange. Because the store to data in the push() thread is\nsequenced before the store to head and the call to increase_head_count() is sequenced\nbefore the load of ptr->data, there’s a happens-before relationship, and all is well\neven if this compare/exchange in pop() uses std::memory_order_relaxed. The only\nother place where ptr->data is changed is the call to swap() that you’re looking at,\nand no other thread can be operating on the same node; that’s the whole point of the\ncompare/exchange.\n\n\n234\nCHAPTER 7\nDesigning lock-free concurrent data structures\n If the compare_exchange_strong() fails, the new value of old_head isn’t touched\nuntil next time around the loop, and you already decided that the std::memory_order\n_acquire in increase_head_count() was enough, so std::memory_order_relaxed is\nenough there also.\n What about other threads? Do you need anything stronger here to ensure other\nthreads are still safe? The answer is no, because head is only ever modified by com-\npare/exchange operations. Because these are read-modify-write operations, they form\npart of the release sequence headed by the compare/exchange in push(). Therefore,\nthe compare_exchange_weak() in push() synchronizes with a call to compare_exchange\n_strong() in increase_head_count(), which reads the value stored, even if many\nother threads modify head in the meantime.\n You’ve nearly finished: the only remaining operations to deal with are the\nfetch_add() operations for modifying the reference count. The thread that got to\nreturn the data from this node can proceed, safe in the knowledge that no other\nthread can have modified the node data. But any thread that did not successfully\nretrieve the data knows that another thread did modify the node data; the successful\nthread used swap() to extract the referenced data item. Therefore you need to ensure\nthat swap() happens before the delete in order to avoid a data race. The easy way to\ndo this is to make the fetch_add() in the successful-return branch use std::memory_\norder_release and the fetch_add() in the loop-again branch use std::memory_order\n_acquire. But this is still overkill: only one thread does the delete (the one that sets\nthe count to zero), so only that thread needs to do an acquire operation. Thankfully,\nbecause fetch_add() is a read-modify-write operation, it forms part of the release\nsequence, so you can do that with an additional load(). If the loop-again branch\ndecreases the reference count to zero, it can reload the reference count with\nstd::memory_order_acquire in order to ensure the required synchronizes-with rela-\ntionship, and the fetch_add() itself can use std::memory_order_relaxed. The final\nstack implementation with the new version of pop() is shown here.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node;\n    struct counted_node_ptr\n    {\n        int external_count;\n        node* ptr;\n    };\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::atomic<int> internal_count;\n        counted_node_ptr next;\n        node(T const& data_):\nListing 7.13\nA lock-free stack with reference counting and relaxed atomic operations\n\n\n235\nExamples of lock-free data structures\n            data(std::make_shared<T>(data_)),\n            internal_count(0)\n        {}\n    };\n    std::atomic<counted_node_ptr> head;\n    void increase_head_count(counted_node_ptr& old_counter)\n    {\n        counted_node_ptr new_counter;\n        do\n        {\n            new_counter=old_counter;\n            ++new_counter.external_count;\n        }\n        while(!head.compare_exchange_strong(old_counter,new_counter,\n                                            std::memory_order_acquire,\n                                            std::memory_order_relaxed));\n        old_counter.external_count=new_counter.external_count;\n    }\npublic:\n    ~lock_free_stack()\n    {\n        while(pop());\n    }\n    void push(T const& data)\n    {\n        counted_node_ptr new_node;\n        new_node.ptr=new node(data);\n        new_node.external_count=1;\n        new_node.ptr->next=head.load(std::memory_order_relaxed)\n        while(!head.compare_exchange_weak(new_node.ptr->next,new_node,\n                                          std::memory_order_release,\n                                          std::memory_order_relaxed));\n    }\n    std::shared_ptr<T> pop()\n    {\n        counted_node_ptr old_head=\n            head.load(std::memory_order_relaxed);\n        for(;;)\n        {\n            increase_head_count(old_head);\n            node* const ptr=old_head.ptr;\n            if(!ptr)\n            {\n                return std::shared_ptr<T>();\n            }\n            if(head.compare_exchange_strong(old_head,ptr->next,\n                                            std::memory_order_relaxed))\n            {\n                std::shared_ptr<T> res;\n                res.swap(ptr->data);\n                int const count_increase=old_head.external_count-2;\n                if(ptr->internal_count.fetch_add(count_increase,\n                       std::memory_order_release)==-count_increase)\n                {\n                    delete ptr;\n",
      "page_number": 246
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 259-269)",
      "start_page": 259,
      "end_page": 269,
      "detection_method": "topic_boundary",
      "content": "236\nCHAPTER 7\nDesigning lock-free concurrent data structures\n                }\n                return res;\n            }\n            else if(ptr->internal_count.fetch_add(-1,\n                        std::memory_order_relaxed)==1)\n            {\n                ptr->internal_count.load(std::memory_order_acquire);\n                delete ptr;\n            }\n        }\n    }\n};\nThat was quite a workout, but you got there in the end, and the stack is better for it. By\nusing more relaxed operations in a carefully thought-out manner, the performance is\nimproved without impacting the correctness. As you can see, the implementation of\npop() is now 37 lines rather than the 8 lines of the equivalent pop() in the lock-based\nstack of listing 6.1 and the 7 lines of the basic lock-free stack without memory manage-\nment in listing 7.2. As we move on to look at writing a lock-free queue, you’ll see a sim-\nilar pattern: lots of the complexity in lock-free code comes from managing memory.\n7.2.6\nWriting a thread-safe queue without locks\nA queue offers a slightly different challenge to a stack, because the push() and pop()\noperations access different parts of the data structure in a queue, whereas they both\naccess the same head node for a stack. Consequently, the synchronization needs are\ndifferent. You need to ensure that changes made to one end are correctly visible to\naccesses at the other. But the structure of try_pop() for the queue in listing 6.6 isn’t\nthat far off that of pop() for the simple lock-free stack in listing 7.2, so you can reason-\nably assume that the lock-free code won’t be that dissimilar. Let’s see how.\n If you take listing 6.6 as a basis, you need two node pointers: one for the head of the\nlist and one for the tail. You’re going to be accessing these from multiple threads, so\nthey’d better be atomic in order to allow you to do away with the corresponding\nmutexes. Let’s start by making that small change and see where it gets you. The follow-\ning listing shows the result.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;\n        node* next;\n        node():\n            next(nullptr)\n        {}\n    };\nListing 7.14\nA single-producer, single-consumer lock-free queue\n\n\n237\nExamples of lock-free data structures\n    std::atomic<node*> head;\n    std::atomic<node*> tail;\n    node* pop_head()\n    {\n        node* const old_head=head.load();\n        if(old_head==tail.load())       \n        {\n            return nullptr;\n        }\n        head.store(old_head->next);\n        return old_head;\n    }\npublic:\n    lock_free_queue():\n        head(new node),tail(head.load())\n    {}\n    lock_free_queue(const lock_free_queue& other)=delete;\n    lock_free_queue& operator=(const lock_free_queue& other)=delete;\n    ~lock_free_queue()\n    {\n        while(node* const old_head=head.load())\n        {\n            head.store(old_head->next);\n            delete old_head;\n        }\n    }\n    std::shared_ptr<T> pop()\n    {\n        node* old_head=pop_head();\n        if(!old_head)\n        {\n            return std::shared_ptr<T>();\n        }\n        std::shared_ptr<T> const res(old_head->data);   \n        delete old_head;\n        return res;\n    }\n    void push(T new_value)\n    {\n        std::shared_ptr<T> new_data(std::make_shared<T>(new_value));\n        node* p=new node;                          \n        node* const old_tail=tail.load();     \n        old_tail->data.swap(new_data);   \n        old_tail->next=p;       \n        tail.store(p);    \n    }\n};\nAt first glance, this doesn’t seem too bad, and if there’s only one thread calling push()\nat a time, and only one thread calling pop(), then this is perfectly fine. The important\nthing in that case is the happens-before relationship between the push() and the\npop() to ensure that it’s safe to retrieve the data. The store to tail h synchronizes\nwith the load from tail B; the store to the preceding node’s data pointer f is\nb\nc\nd\ne\nf\ng\nh\n\n\n238\nCHAPTER 7\nDesigning lock-free concurrent data structures\nsequenced before the store to tail; and the load from tail is sequenced before the\nload from the data pointer c, so the store to data happens before the load, and\neverything is OK. This is therefore a perfectly serviceable single-producer, single-consumer\n(SPSC) queue. \n The problems come when multiple threads call push() concurrently or multiple\nthreads call pop() concurrently. Let’s look at push() first. If you have two threads call-\ning push() concurrently, they both allocate new nodes to be the new dummy node d,\nboth read the same value for tail e, and consequently both update the data mem-\nbers of the same node when setting the data and next pointers, f and g. This is a\ndata race!\n There are similar problems in pop_head(). If two threads call concurrently, they\nwill both read the same value of head, and both then overwrite the old value with the\nsame next pointer. Both threads will now think they’ve retrieved the same node—a\nrecipe for disaster. Not only do you have to ensure that only one thread uses pop()on\na given item, but you also need to ensure that other threads can safely access the next\nmember of the node they read from head. This is exactly the problem you saw with\npop() for your lock-free stack, so any of the solutions for that could be used here.\n So if pop() is a “solved problem,” what about push()? The problem here is that in\norder to get the required happens-before relationship between push() and pop(), you\nneed to set the data items on the dummy node before you update tail. But this\nmeans that concurrent calls to push() are racing over those same data items, because\nthey’ve read the same tail pointer.\nHANDLING MULTIPLE THREADS IN PUSH()\nOne option is to add a dummy node between the real nodes. This way, the only part of\nthe current tail node that needs updating is the next pointer, which could therefore\nbe made atomic. If a thread manages to successfully change the next pointer from\nnullptr to its new node, then it has successfully added the pointer; otherwise, it\nwould have to start again and reread the tail. This would then require a minor\nchange to pop() in order to discard nodes with a null data pointer and loop again.\nThe downside here is that every pop() call will typically have to remove two nodes, and\nthere are twice as many memory allocations.\n A second option is to make the data pointer atomic and set that with a call to com-\npare/exchange. If the call succeeds, this is your tail node, and you can safely set the\nnext pointer to your new node and then update tail. If the compare/exchange fails\nbecause another thread has stored the data, you loop around, reread tail, and start\nagain. If the atomic operations on std::shared_ptr<> are lock-free, you’re home\nfree. If not, you need an alternative. One possibility is to have pop() return\nstd::unique_ptr<> (after all, it’s the only reference to the object) and store the data\nas a plain pointer in the queue. This would allow you to store it as std::atomic<T*>,\nwhich would then support the necessary compare_exchange_strong() call. If you’re\nusing the reference-counting scheme from listing 7.12 to handle multiple threads in\npop(), push() now looks like this.\n\n\n239\nExamples of lock-free data structures\nvoid push(T new_value)\n{\n    std::unique_ptr<T> new_data(new T(new_value));\n    counted_node_ptr new_next;\n    new_next.ptr=new node;\n    new_next.external_count=1;\n    for(;;)\n    {\n        node* const old_tail=tail.load();    \n        T* old_data=nullptr;\n        if(old_tail->data.compare_exchange_strong(\n            old_data,new_data.get()))    \n        {\n            old_tail->next=new_next;\n            tail.store(new_next.ptr);    \n            new_data.release();\n            break;\n        }\n    }\n}\nUsing the reference-counting scheme avoids this particular race, but it’s not the only\nrace in push(). If you look at the revised version of push() in listing 7.15, you’ll see a\npattern you saw in the stack: load an atomic pointer B and dereference that pointer\nc. In the meantime, another thread could update the pointer d, eventually leading\nto the node being deallocated (in pop()). If the node is deallocated before you deref-\nerence the pointer, you have undefined behavior. Ouch! It’s tempting to add an exter-\nnal count in tail the same as you did for head, but each node already has an external\ncount in the next pointer of the previous node in the queue. Having two external\ncounts for the same node requires a modification to the reference-counting scheme\nto avoid deleting the node too early. You can address this by also counting the number\nof external counters inside the node structure and decreasing this number when each\nexternal counter is destroyed (as well as adding the corresponding external count to\nthe internal count). If the internal count is zero and there are no external counters,\nyou know the node can safely be deleted. This is a technique I first encountered\nthrough Joe Seigh’s Atomic Ptr Plus Project (http://atomic-ptr-plus.sourceforge.net/).\nThe following listing shows how push() looks under this scheme.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    struct node;\n    struct counted_node_ptr\n    {\n        int external_count;\nListing 7.15\nA (broken) first attempt at revising push()\nListing 7.16\nImplementing push() for a lock-free queue with a reference-counted tail\nb\nc\nd\n\n\n240\nCHAPTER 7\nDesigning lock-free concurrent data structures\n        node* ptr;\n    };\n    std::atomic<counted_node_ptr> head;\n    std::atomic<counted_node_ptr> tail;    \n    struct node_counter\n    {\n        unsigned internal_count:30;\n        unsigned external_counters:2;   \n    };\n    struct node\n    {\n        std::atomic<T*> data;\n        std::atomic<node_counter> count;   \n        counted_node_ptr next;\n        node()\n        {\n            node_counter new_count;\n            new_count.internal_count=0;\n            new_count.external_counters=2;   \n            count.store(new_count);\n            \n            next.ptr=nullptr;\n            next.external_count=0;\n        }\n    };\npublic:\n    void push(T new_value)\n    {\n        std::unique_ptr<T> new_data(new T(new_value));\n        counted_node_ptr new_next;\n        new_next.ptr=new node;\n        new_next.external_count=1;\n        counted_node_ptr old_tail=tail.load();\n        for(;;)\n        {\n            increase_external_count(tail,old_tail);    \n            T* old_data=nullptr;\n            if(old_tail.ptr->data.compare_exchange_strong(   \n               old_data,new_data.get()))\n            {\n                old_tail.ptr->next=new_next;\n                old_tail=tail.exchange(new_next);\n                free_external_counter(old_tail);    \n                new_data.release();\n                break;\n            }\n            old_tail.ptr->release_ref();\n        }\n    }\n};\nIn listing 7.16, tail is now atomic<counted_node_ptr>, the same as head B, and the\nnode structure has a count member to replace the internal_count from before d. This\ncount is a structure containing the internal_count and an additional external_counters\nb\nc\nd\ne\nf\ng\nh\n\n\n241\nExamples of lock-free data structures\nmember c. Note that you need only 2 bits for the external_counters because there\nare at most two such counters. By using a bit field for this and specifying internal\n_count as a 30-bit value, you keep the total counter size to 32 bits. This gives you plenty\nof scope for large internal count values while ensuring that the whole structure fits\ninside a machine word on 32-bit and 64-bit machines. It’s important to update these\ncounts together as a single entity in order to avoid race conditions, as you’ll see shortly.\nKeeping the structure within a machine word makes it more likely that the atomic oper-\nations can be lock-free on many platforms.\n The node is initialized with the internal_count set to zero and the external_\ncounters set to 2 e, because every new node starts out referenced from tail and\nfrom the next pointer of the previous node once you’ve added it to the queue.\npush()itself is similar to listing 7.15, except that before you dereference the value\nloaded from tail in order to call to compare_exchange_strong() on the data mem-\nber of the node g, you call a new function increase_external_count() to increase\nthe count f, and then afterward you call free_external_counter() on the old tail\nvalue h.\n With the push() side dealt with, let’s take a look at pop(). This is shown in the fol-\nlowing listing and blends the reference-counting logic from the pop() implementa-\ntion in listing 7.12 with the queue-pop logic from listing 7.14.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    struct node\n    {\n        void release_ref();\n    };\npublic:\n    std::unique_ptr<T> pop()\n    {\n        counted_node_ptr old_head=head.load(std::memory_order_relaxed); \n        for(;;)\n        {\n            increase_external_count(head,old_head);   \n            node* const ptr=old_head.ptr;\n            if(ptr==tail.load().ptr)\n            {\n                ptr->release_ref();        \n                return std::unique_ptr<T>();\n            }\n            if(head.compare_exchange_strong(old_head,ptr->next))   \n            {\n                T* const res=ptr->data.exchange(nullptr);\n                free_external_counter(old_head);        \n                return std::unique_ptr<T>(res);\n            }\nListing 7.17\nPopping a node from a lock-free queue with a reference-counted tail\nb\nc\nd\ne\nf\n\n\n242\nCHAPTER 7\nDesigning lock-free concurrent data structures\n            ptr->release_ref();   \n        }\n    }\n};\nYou prime the pump by loading the old_head value before you enter the loop B, and\nbefore you increase the external count on the loaded value,c. If the head node is the\nsame as the tail node, you can release the reference d and return a null pointer\nbecause there’s no data in the queue. If there is data, you want to try to claim it for\nyourself, and you do this with the call to compare_exchange_strong() e. As with the\nstack in listing 7.12, this compares the external count and pointer as a single entity; if\neither changes, you need to loop again, after releasing the reference g. If the\nexchange succeeded, you’ve claimed the data in the node as yours, so you can return\nthat to the caller after you’ve released the external counter to the popped node f.\nOnce both the external reference counts have been freed and the internal count has\ndropped to zero, the node itself can be deleted. The reference-counting functions\nthat take care of all this are shown in listings 7.18, 7.19, and 7.20.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    struct node\n    {\n        void release_ref()\n        {\n            node_counter old_counter=\n                count.load(std::memory_order_relaxed);\n            node_counter new_counter;\n            do\n            {\n                new_counter=old_counter;\n                --new_counter.internal_count;    \n            }\n            while(!count.compare_exchange_strong(    \n                  old_counter,new_counter,\n                  std::memory_order_acquire,std::memory_order_relaxed));\n            if(!new_counter.internal_count && \n               !new_counter.external_counters)\n            {\n                delete this;    \n            }\n        }\n    };\n};\nThe implementation of node::release_ref() is only slightly changed from the equiv-\nalent code in the implementation of lock_free_stack::pop() from listing 7.12.\nListing 7.18\nReleasing a node reference in a lock-free queue\ng\nb\nc\nd\n\n\n243\nExamples of lock-free data structures\nWhereas the code in listing 7.12 only has to handle a single external count so you\ncould use a simple fetch_sub, the whole count structure now has to be updated atomi-\ncally, even though you only want to modify the internal_count field B. This therefore\nrequires a compare/exchange loop c. Once you’ve decremented internal_count, if\nboth the internal and external counts are now zero, this is the last reference, so you\ncan delete the node d.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    static void increase_external_count(\n        std::atomic<counted_node_ptr>& counter,\n        counted_node_ptr& old_counter)\n    {\n        counted_node_ptr new_counter;\n        do\n        {\n            new_counter=old_counter;\n            ++new_counter.external_count;\n        }\n        while(!counter.compare_exchange_strong(\n              old_counter,new_counter,\n              std::memory_order_acquire,std::memory_order_relaxed));\n        old_counter.external_count=new_counter.external_count;\n    }\n};\nListing 7.19 is the other side. This time, rather than releasing a reference, you’re obtain-\ning a fresh one and increasing the external count. increase_external_count() is simi-\nlar to the increase_head_count() function from listing 7.13, except that it has been\nmade into a static member function that takes the external counter to update as the\nfirst parameter rather than operating on a fixed counter.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    static void free_external_counter(counted_node_ptr &old_node_ptr)\n    {\n        node* const ptr=old_node_ptr.ptr;\n        int const count_increase=old_node_ptr.external_count-2;\n        node_counter old_counter=\n            ptr->count.load(std::memory_order_relaxed);\n        node_counter new_counter;\n        do\n        {\n            new_counter=old_counter;\nListing 7.19\nObtaining a new reference to a node in a lock-free queue\nListing 7.20\nFreeing an external counter to a node in a lock-free queue\n\n\n244\nCHAPTER 7\nDesigning lock-free concurrent data structures\n            --new_counter.external_counters;            \n            new_counter.internal_count+=count_increase;    \n        }\n        while(!ptr->count.compare_exchange_strong(    \n              old_counter,new_counter,\n              std::memory_order_acquire,std::memory_order_relaxed));\n        if(!new_counter.internal_count && \n           !new_counter.external_counters)\n        {\n            delete ptr;    \n        }\n    }\n};\nThe counterpart to increase_external_count() is free_external_counter(). This\nis similar to the equivalent code from lock_free_stack::pop() in listing 7.12, but\nmodified to handle the external_counters count. It updates the two counts using a\nsingle compare_exchange_strong() on the whole count structure d, as you did\nwhen decreasing the internal_count in release_ref(). The internal_count value\nis updated as in listing 7.12 c, and the external_counters value is decreased by\none B. If both the values are now zero, there are no more references to the node, so it\ncan be safely deleted e. This has to be done as a single action (which therefore\nrequires the compare/exchange loop) to avoid a race condition. If they’re updated\nseparately, two threads may both think they are the last one and both delete the node,\nresulting in undefined behavior.\n Although this now works and is race-free, there’s still a performance issue. Once\none thread has started a push() operation by successfully completing the compare_\nexchange_strong() on old_tail.ptr->data (f from listing 7.16), no other thread\ncan perform a push() operation. Any thread that tries will see the new value rather\nthan nullptr, which will cause the compare_exchange_strong() call to fail and\nmake that thread loop again. This is a busy wait, which consumes CPU cycles with-\nout achieving anything. Consequently, this is effectively a lock. The first push() call\nblocks other threads until it has completed, so this code is no longer lock-free. Not\nonly that, but whereas the operating system can give priority to the thread that holds\nthe lock on a mutex if there are blocked threads, it can’t do so in this case, so the\nblocked threads will waste CPU cycles until the first thread is done. This calls for the\nnext trick from the lock-free bag of tricks: the waiting thread can help the thread\nthat’s doing the push().\nMAKING THE QUEUE LOCK-FREE BY HELPING OUT ANOTHER THREAD\nIn order to restore the lock-free property of the code, you need to find a way for a\nwaiting thread to make progress even if the thread doing the push() is stalled. One\nway to do this is to help the stalled thread by doing its work for it.\n In this case, you know exactly what needs to be done: the next pointer on the tail\nnode needs to be set to a new dummy node, and then the tail pointer itself must be\nupdated. The thing about dummy nodes is that they’re all equivalent, so it doesn’t\nb\nc\nd\ne\n\n\n245\nExamples of lock-free data structures\nmatter if you use the dummy node created by the thread that successfully pushed the\ndata or the dummy node from one of the threads that’s waiting to push. If you make\nthe next pointer in a node atomic, you can then use compare_exchange_strong() to\nset the pointer. Once the next pointer is set, you can then use a compare_exchange_\nweak() loop to set the tail while ensuring that it’s still referencing the same original\nnode. If it isn’t, someone else has updated it, and you can stop trying and loop again.\nThis requires a minor change to pop() as well in order to load the next pointer; this is\nshown in the following listing.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    struct node\n    {\n        std::atomic<T*> data;\n        std::atomic<node_counter> count;\n        std::atomic<counted_node_ptr> next;    \n    };\npublic:\n    std::unique_ptr<T> pop()\n    {\n        counted_node_ptr old_head=head.load(std::memory_order_relaxed);\n        for(;;)\n        {\n            increase_external_count(head,old_head);\n            node* const ptr=old_head.ptr;\n            if(ptr==tail.load().ptr)\n            {\n                return std::unique_ptr<T>();\n            }\n            counted_node_ptr next=ptr->next.load();        \n            if(head.compare_exchange_strong(old_head,next))\n            {\n                T* const res=ptr->data.exchange(nullptr);\n                free_external_counter(old_head);\n                return std::unique_ptr<T>(res);\n            }\n            ptr->release_ref();\n        }\n    }\n};\nAs I mentioned, the changes here are simple: the next pointer is now atomic B, so\nthe load at c is atomic. In this example, you’re using the default memory_order_\nseq_cst ordering, so you could omit the explicit call to load() and rely on the load in\nthe implicit conversion to counted_node_ptr, but putting in the explicit call reminds\nyou where to add the explicit memory ordering later.\nListing 7.21\npop() modified to allow helping on the push() side\nb\nc\n\n\n246\nCHAPTER 7\nDesigning lock-free concurrent data structures\n The code for push() is more involved and is shown here.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    void set_new_tail(counted_node_ptr &old_tail,      \n                      counted_node_ptr const &new_tail)\n    {\n        node* const current_tail_ptr=old_tail.ptr;\n        while(!tail.compare_exchange_weak(old_tail,new_tail) &&    \n              old_tail.ptr==current_tail_ptr);\n        if(old_tail.ptr==current_tail_ptr)      \n            free_external_counter(old_tail);    \n        else\n            current_tail_ptr->release_ref();   \n    }\npublic:\n    void push(T new_value)\n    {\n        std::unique_ptr<T> new_data(new T(new_value));\n        counted_node_ptr new_next;\n        new_next.ptr=new node;\n        new_next.external_count=1;\n        counted_node_ptr old_tail=tail.load();\n        for(;;)\n        {\n            increase_external_count(tail,old_tail);\n            T* old_data=nullptr;\n            if(old_tail.ptr->data.compare_exchange_strong(    \n                   old_data,new_data.get()))\n            {\n                counted_node_ptr old_next={0};\n                if(!old_tail.ptr->next.compare_exchange_strong(     \n                       old_next,new_next))\n                {\n                    delete new_next.ptr;    \n                    new_next=old_next;    \n                }\n                set_new_tail(old_tail, new_next);\n                new_data.release();\n                break;\n            }\n            else     \n            {\n                counted_node_ptr old_next={0};\n                if(old_tail.ptr->next.compare_exchange_strong(    \n                       old_next,new_next))\n                {\n                    old_next=new_next;     \n                    new_next.ptr=new node;     \n                }\nListing 7.22\nA sample push() with helping for a lock-free queue\nb\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n1@\n1#\n",
      "page_number": 259
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 270-277)",
      "start_page": 270,
      "end_page": 277,
      "detection_method": "topic_boundary",
      "content": "247\nExamples of lock-free data structures\n                set_new_tail(old_tail, old_next);    \n            }\n        }\n    }\n};\nThis is similar to the original push() from listing 7.16, but there are a few crucial dif-\nferences. If you do set the data pointer g, you need to handle the case where another\nthread has helped you, and there’s now an else clause to do the helping 1).\n Having set the data pointer in the node g, this new version of push() updates the\nnext pointer using compare_exchange_strong() h. You use compare_exchange_\nstrong() to avoid looping. If the exchange fails, you know that another thread has\nalready set the next pointer, so you don’t need the new node you allocated at the\nbeginning, and you can delete it i. You also want to use the next value that the other\nthread set for updating tail j.\n The update of the tail pointer has been extracted into set_new_tail() B. This\nuses a compare_exchange_weak() loop c to update the tail, because if other threads\nare trying to push() a new node, the external_count part may have changed, and\nyou don’t want to lose it. But you also need to take care that you don’t replace the\nvalue if another thread has successfully changed it already; otherwise, you may end up\nwith loops in the queue, which would be a rather bad idea. Consequently, you need to\nensure that the ptr part of the loaded value is the same if the compare/exchange\nfails. If the ptr is the same once the loop has exited d, then you must have success-\nfully set the tail, so you need to free the old external counter e. If the ptr value is\ndifferent, then another thread will have freed the counter, so you need to release the\nsingle reference held by this thread f.\n If the thread calling push() failed to set the data pointer this time through the\nloop, it can help the successful thread to complete the update. First off, you try to\nupdate the next pointer to the new node allocated on this thread 1!. If this succeeds,\nyou want to use the node you allocated as the new tail node 1@, and you need to allo-\ncate another new node in anticipation of managing to push an item on the queue 1#.\nYou can then try to set the tail node by calling set_new_tail before looping around\nagain 1$.\n You may have noticed that there are a lot of new and delete calls for such a small\npiece of code, because new nodes are allocated on push() and destroyed in pop().\nThe efficiency of the memory allocator therefore has a considerable impact on the\nperformance of this code; a poor allocator can completely destroy the scalability prop-\nerties of a lock-free container like this. The selection and implementation of these\nallocators are beyond the scope of this book, but it’s important to bear in mind that\nthe only way to know that an allocator is better is to try it and measure the perfor-\nmance of the code before and after. Common techniques for optimizing memory allo-\ncation include having a separate memory allocator on each thread and using free lists\nto recycle nodes rather than returning them to the allocator. \n1$\n\n\n248\nCHAPTER 7\nDesigning lock-free concurrent data structures\n That’s enough examples for now; instead, let’s look at extracting some guidelines\nfor writing lock-free data structures from the examples.\n7.3\nGuidelines for writing lock-free data structures\nIf you’ve followed through all the examples in this chapter, you’ll appreciate the com-\nplexities involved in getting lock-free code right. If you’re going to design your own\ndata structures, it helps to have some guidelines to focus on. The general guidelines\nregarding concurrent data structures from the beginning of chapter 6 still apply, but\nyou need more than that. I’ve pulled a few useful guidelines out from the examples,\nwhich you can then refer to when designing your own lock-free data structures.\n7.3.1\nGuideline: use std::memory_order_seq_cst for prototyping\nstd::memory_order_seq_cst is much easier to reason about than any other memory\nordering because all these operations form a total order. In all the examples in this\nchapter, you’ve started with std::memory_order_seq_cst and only relaxed the\nmemory-ordering constraints once the basic operations were working. In this sense,\nusing other memory orderings is an optimization, and as such you need to avoid doing\nit prematurely. In general, you can only determine which operations can be relaxed\nwhen you can see the full set of code that can operate on the guts of the data struc-\nture. Attempting to do otherwise makes your life harder. This is complicated by the\nfact that the code may work when tested but isn’t guaranteed. Unless you have an\nalgorithm checker that can systematically test all possible combinations of thread visi-\nbilities that are consistent with the specified ordering guarantees (and these things do\nexist), running the code isn’t enough.\n7.3.2\nGuideline: use a lock-free memory reclamation scheme\nOne of the biggest difficulties with lock-free code is managing memory. It’s essential\nto avoid deleting objects when other threads might still have references to them, but\nyou still want to delete the object as soon as possible in order to avoid excessive mem-\nory consumption. In this chapter you’ve seen three techniques for ensuring that mem-\nory can safely be reclaimed:\nWaiting until no threads are accessing the data structure and deleting all objects\nthat are pending deletion\nUsing hazard pointers to identify that a thread is accessing a particular object\nReference counting the objects so that they aren’t deleted until there are no\noutstanding references\nIn all cases, the key idea is to use some method to keep track of how many threads are\naccessing a particular object and only delete each object when it’s no longer refer-\nenced from anywhere. There are many other ways of reclaiming memory in lock-free\ndata structures. For example, this is the ideal scenario for using a garbage collector.\nIt’s much easier to write the algorithms if you know that the garbage collector will free\nthe nodes when they’re no longer used, but not before.\n\n\n249\nGuidelines for writing lock-free data structures\n Another alternative is to recycle nodes and only free them completely when the\ndata structure is destroyed. Because the nodes are reused, the memory never\nbecomes invalid, so some of the difficulties in avoiding undefined behavior go away.\nThe downside here is that another problem becomes more prevalent. This is the so-\ncalled ABA problem.\n7.3.3\nGuideline: watch out for the ABA problem\nThe ABA problem is something to be wary of in any compare/exchange–based algo-\nrithm. It goes like this:\n1\nThread 1 reads an atomic variable, x, and finds it has value A.\n2\nThread 1 performs some operation based on this value, such as dereferencing it\n(if it’s a pointer) or doing a lookup, or something.\n3\nThread 1 is stalled by the operating system.\n4\nAnother thread performs some operations on x that change its value to B.\n5\nA thread then changes the data associated with the value A such that the value\nheld by thread 1 is no longer valid. This may be as drastic as freeing the\npointed-to memory or changing an associated value. \n6\nA thread then changes x back to A based on this new data. If this is a pointer, it\nmay be a new object that happens to share the same address as the old one.\n7\nThread 1 resumes and performs a compare/exchange on x, comparing against\nA. The compare/exchange succeeds (because the value is indeed A), but this is\nthe wrong A value. The data originally read at step 2 is no longer valid, but\nthread 1 has no way of telling and will corrupt the data structure.\nNone of the algorithms presented here suffer from this problem, but it’s easy to write\nlock-free algorithms that do. The most common way to avoid this problem is to\ninclude an ABA counter alongside the variable x. The compare/exchange operation\nis then done on the combined structure of x plus the counter as a single unit. Every\ntime the value is replaced, the counter is incremented, so even if x has the same value,\nthe compare/exchange will fail if another thread has modified x.\n The ABA problem is particularly prevalent in algorithms that use free lists or other-\nwise recycle nodes rather than returning them to the allocator.\n7.3.4\nGuideline: identify busy-wait loops and help the other thread\nIn the final queue example, you saw how a thread performing a push operation had\nto wait for another thread also performing a push to complete its operation before it\ncould proceed. Left alone, this would have been a busy-wait loop, with the waiting\nthread wasting CPU time while failing to proceed. If you end up with a busy-wait loop,\nyou effectively have a blocking operation and might as well use mutexes and locks. By\nmodifying the algorithm so that the waiting thread performs the incomplete steps if\nit’s scheduled to run before the original thread completes the operation, you can\nremove the busy-wait and the operation is no longer blocking. In the queue example\n\n\n250\nCHAPTER 7\nDesigning lock-free concurrent data structures\nthis required changing a data member to be an atomic variable rather than a non-\natomic variable and using compare/exchange operations to set it, but in more com-\nplex data structures it might require more extensive changes.\nSummary\nFollowing from the lock-based data structures of chapter 6, this chapter has described\nsimple implementations of various lock-free data structures, starting with a stack and a\nqueue, as before. You saw how you must take care with the memory ordering on your\natomic operations to ensure that there are no data races and that each thread sees a\ncoherent view of the data structure. You also saw how memory management becomes\nmuch harder for lock-free data structures than lock-based ones and examined a cou-\nple of mechanisms for handling it. You also saw how to avoid creating wait loops by\nhelping the thread you’re waiting for to complete its operation.\n Designing lock-free data structures is a difficult task, and it’s easy to make mistakes,\nbut these data structures have scalability properties that are important in some situa-\ntions. Hopefully, by following through the examples in this chapter and reading the\nguidelines, you’ll be better equipped to design your own lock-free data structure,\nimplement one from a research paper, or find the bug in the one your former col-\nleague wrote before they left the company.\n Wherever data is shared between threads, you need to think about the data struc-\ntures used and how the data is synchronized between threads. By designing data struc-\ntures for concurrency, you can encapsulate that responsibility in the data structure\nitself, so the rest of the code can focus on the task it’s trying to perform with the data\nrather than the data synchronization. You’ll see this in action in chapter 8 as we move\non from concurrent data structures to concurrent code in general. Parallel algorithms\nuse multiple threads to improve their performance, and the choice of concurrent\ndata structure is crucial where the algorithms need their worker threads to share data.\n\n\n251\nDesigning concurrent code\nMost of the preceding chapters have focused on the tools you have in your C++\ntoolbox for writing concurrent code. In chapters 6 and 7 we looked at how to\nuse those tools to design basic data structures that are safe for concurrent access\nby multiple threads. Much as a carpenter needs to know more than how to build\na hinge or a joint in order to make a cupboard or a table, there’s more to design-\ning concurrent code than the design and use of basic data structures. You now\nneed to look at the wider context so you can build bigger structures that per-\nform useful work. I’ll be using multithreaded implementations of some of the\nThis chapter covers\nTechniques for dividing data between threads\nFactors that affect the performance of concurrent \ncode \nHow performance factors affect the design of \ndata structures\nException safety in multithreaded code\nScalability\nExample implementations of several parallel \nalgorithms\n\n\n252\nCHAPTER 8\nDesigning concurrent code\nC++ Standard Library algorithms as examples, but the same principles apply at all\nscales of an application.\n Just as with any programming project, it’s vital to think carefully about the design\nof concurrent code. But with multithreaded code, there are even more factors to con-\nsider than with sequential code. Not only must you think about the usual factors, such\nas encapsulation, coupling, and cohesion (which are amply described in the many\nbooks on software design), but you also need to consider which data to share, how to\nsynchronize accesses to that data, which threads need to wait for which other threads\nto complete certain operations, and so on.\n In this chapter we’ll be focusing on these issues, from the high-level (but funda-\nmental) considerations of how many threads to use, which code to execute on which\nthread, and how this can affect the clarity of the code, to the low-level details of how to\nstructure the shared data for optimal performance.\n Let’s start by looking at techniques for dividing work between threads.\n8.1\nTechniques for dividing work between threads\nImagine for a moment that you’ve been tasked with building a house. In order to\ncomplete the job, you’ll need to dig the foundation, build walls, put in plumbing, add\nthe wiring, and so on. Theoretically, you could do it all yourself with sufficient train-\ning, but it would probably take a long time, and you’d be continually switching tasks as\nnecessary. Alternatively, you could hire a few other people to help out. You now have\nto choose how many people to hire and decide what skills they need. You could, for\nexample, hire a couple of people with general skills and have everybody chip in with\neverything. You’d still all switch tasks as necessary, but now things can be done more\nquickly because there are more of you.\n Alternatively, you could hire a team of specialists: a bricklayer, a carpenter, an\nelectrician, and a plumber, for example. Your specialists do whatever their specialty\nis, so if there’s no plumbing needed, your plumber sits around drinking tea or cof-\nfee. Things still get done more quickly than before, because there are more of you,\nand the plumber can put the toilet in while the electrician wires up the kitchen, but\nthere’s more waiting around when there’s no work for a particular specialist. Even\nwith the idle time, you might find that the work is done faster with specialists than\nwith a team of general handymen. Your specialists don’t need to keep changing\ntools, and they can probably each do their tasks quicker than the generalists can.\nWhether or not this is the case depends on the particular circumstances—you’d\nhave to try it and see.\n Even if you hire specialists, you can still choose to hire different numbers of each.\nIt might make sense to have more bricklayers than electricians, for example. Also, the\nmakeup of your team and the overall efficiency might change if you had to build more\nthan one house. Even though your plumber might not have lots of work to do on any\ngiven house, you might have enough work to keep him busy all the time if you’re\nbuilding many houses at once. Also, if you don’t have to pay your specialists when\n\n\n253\nTechniques for dividing work between threads\nthere’s no work for them to do, you might be able to afford a larger team overall even\nif you have only the same number of people working at any one time.\n OK, enough about building; what does all this have to do with threads? Well, with\nthreads the same issues apply. You need to decide how many threads to use and what\ntasks they should be doing. You need to decide whether to have “generalist” threads\nthat do whatever work is necessary at any point in time or “specialist” threads that do\none thing well, or some combination. You need to make these choices whatever the\ndriving reason for using concurrency, and how you do this will have a critical effect on\nthe performance and clarity of the code. It’s therefore vital to understand the options\nso you can make an appropriately informed decision when designing the structure of\nyour application. In this section, we’ll look at several techniques for dividing the tasks,\nstarting with dividing data between threads before we do any other work. \n8.1.1\nDividing data between threads before processing begins\nThe easiest algorithms to parallelize are simple algorithms, such as std::for_each,\nthat perform an operation on each element in a data set. In order to parallelize this\nalgorithm, you can assign each element to one of the processing threads. How the ele-\nments are best divided for optimal performance depends on the details of the data\nstructure, as you’ll see later in this chapter when we look at performance issues.\n The simplest means of dividing the data is to allocate the first N elements to one\nthread, the next N elements to another thread, and so on, as shown in figure 8.1, but\nother patterns could be used too. No matter how the data is divided, each thread then\nprocesses the elements it has been assigned without any communication with the\nother threads until it has completed its processing.\n This structure will be familiar to anyone who has programmed using the Message\nPassing Interface (MPI, http://www.mpi-forum.org/) or OpenMP (http://www.openmp\n.org/) frameworks: a task is split into a set of parallel tasks, the worker threads run these\ntasks independently, and the results are combined in a final reduction step. It’s the\nThread 1\nThread 2\nThread m\nFigure 8.1\nDistributing consecutive chunks of data between threads\n\n\n254\nCHAPTER 8\nDesigning concurrent code\napproach used by the accumulate example from section 2.4; in this case, both the par-\nallel tasks and the final reduction step are accumulations. For a simple for_each, the\nfinal step is a no-op because there are no results to reduce.\n Identifying this final step as a reduction is important; a naive implementation such\nas listing 2.9 will perform this reduction as a final serial step. But this step can often be\nparallelized as well; accumulate is a reduction operation, so listing 2.9 could be modi-\nfied to call itself recursively where the number of threads is larger than the minimum\nnumber of items to process on a thread, for example. Alternatively, the worker threads\ncould be made to perform some of the reduction steps as each one completes its task,\nrather than spawning new threads each time.\n Although this technique is powerful, it can’t be applied to everything. Sometimes\nthe data can’t be divided neatly up front because the necessary divisions become appar-\nent only as the data is processed. This is particularly apparent with recursive algo-\nrithms such as Quicksort; they therefore need a different approach.\n8.1.2\nDividing data recursively\nThe Quicksort algorithm has two basic steps: partition the data into items that come\nbefore or after one of the elements (the pivot) in the final sort order and recursively\nsort those two “halves.” You can’t parallelize this by dividing the data up front, because\nit’s only by processing the items that you know which “half” they go in. If you’re going\nto parallelize this algorithm, you need to make use of the recursive nature. With each\nlevel of recursion there are more calls to the quick_sort function, because you have to\nsort both the elements that belong before the pivot and those that belong after it.\nThese recursive calls are entirely independent, because they access separate sets of\nelements, and so are prime candidates for concurrent execution. Figure 8.2 shows this\nrecursive division.\n In chapter 4, you saw this implementation. Rather than performing two recursive\ncalls for the higher and lower chunks, you used std::async() to spawn asynchronous\nFigure 8.2\nRecursively dividing data\n",
      "page_number": 270
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 278-289)",
      "start_page": 278,
      "end_page": 289,
      "detection_method": "topic_boundary",
      "content": "255\nTechniques for dividing work between threads\ntasks for the lower chunk at each stage. By using std::async(), you ask the C++\nThread Library to decide when to run the task on a new thread and when to run it\nsynchronously.\n This is important: if you’re sorting a large set of data, spawning a new thread for\neach recursion would quickly result in a lot of threads. As you’ll see when we look at\nperformance, if you have too many threads, you might slow down the application.\nThere’s also a possibility of running out of threads if the data set is large. The idea of\ndividing the overall task in a recursive fashion like this is a good one; you just need to\nkeep a tighter rein on the number of threads. std::async() can handle this in simple\ncases, but it’s not the only choice.\n One alternative is to use the std::thread::hardware_concurrency() function to\nchoose the number of threads, as you did with the parallel version of accumulate()\nfrom listing 2.9. Then, rather than starting a new thread for the recursive calls, you\ncan push the chunk to be sorted onto a thread-safe stack, such as one of those\ndescribed in chapters 6 and 7. If a thread has nothing else to do, either because it has\nfinished processing all its chunks or because it’s waiting for a chunk to be sorted, it\ncan take a chunk from the stack and sort that.\n The following listing shows a sample implementation that uses this technique. As\nwith most of the examples, this is intended to demonstrate an idea rather than being\nproduction-ready code. If you're using a C++17 compiler and your library supports it,\nyou're better off using the parallel algorithms provided by Standard Library, as cov-\nered in chapter 10.\ntemplate<typename T>\nstruct sorter        \n{\n    struct chunk_to_sort\n    {\n        std::list<T> data;\n        std::promise<std::list<T> > promise;\n    };\n    thread_safe_stack<chunk_to_sort> chunks;   \n    std::vector<std::thread> threads;       \n    unsigned const max_thread_count;\n    std::atomic<bool> end_of_data;\n    sorter():\n        max_thread_count(std::thread::hardware_concurrency()-1),\n        end_of_data(false)\n    {}\n    ~sorter()   \n    {\n        end_of_data=true;   \n        for(unsigned i=0;i<threads.size();++i)\n        {\n            threads[i].join();   \n        }\nListing 8.1\nParallel Quicksort using a stack of pending chunks to sort\nb\nc\nd\ne\nf\ng\n\n\n256\nCHAPTER 8\nDesigning concurrent code\n    }\n    void try_sort_chunk()\n    {\n        boost::shared_ptr<chunk_to_sort > chunk=chunks.pop();   \n        if(chunk)\n        {\n            sort_chunk(chunk);   \n        }\n    }\n    std::list<T> do_sort(std::list<T>& chunk_data)   \n    {\n        if(chunk_data.empty())\n        {\n            return chunk_data;\n        }\n        std::list<T> result;\n        result.splice(result.begin(),chunk_data,chunk_data.begin());\n        T const& partition_val=*result.begin();\n        typename std::list<T>::iterator divide_point=    \n            std::partition(chunk_data.begin(),chunk_data.end(),\n                           [&](T const& val){return val<partition_val;});\n        chunk_to_sort new_lower_chunk;\n        new_lower_chunk.data.splice(new_lower_chunk.data.end(),\n                                    chunk_data,chunk_data.begin(),\n                                    divide_point);\n        std::future<std::list<T> > new_lower=\n            new_lower_chunk.promise.get_future();\n        chunks.push(std::move(new_lower_chunk));     \n        if(threads.size()<max_thread_count)     \n        {\n            threads.push_back(std::thread(&sorter<T>::sort_thread,this));\n        }\n        std::list<T> new_higher(do_sort(chunk_data));\n        result.splice(result.end(),new_higher);\n        while(new_lower.wait_for(std::chrono::seconds(0)) !=\n              std::future_status::ready)  \n        {\n            try_sort_chunk();    \n        }\n        result.splice(result.begin(),new_lower.get());\n        return result;\n    }\n    void sort_chunk(boost::shared_ptr<chunk_to_sort > const& chunk)\n    {\n        chunk->promise.set_value(do_sort(chunk->data));   \n    }\n    void sort_thread()\n    {\n        while(!end_of_data)   \n        {\n            try_sort_chunk();   \n            std::this_thread::yield();    \n        }\n    }\n};\nh\ni\nj\n1)\n1!\n1@\n1#\n1$\n1%\n1^\n1&\n1*\n\n\n257\nTechniques for dividing work between threads\ntemplate<typename T>\nstd::list<T> parallel_quick_sort(std::list<T> input)    \n{\n    if(input.empty())\n    {\n        return input;\n    }\n    sorter<T> s;\n    return s.do_sort(input);    \n}\nHere, the parallel_quick_sort function 1( delegates most of the functionality to\nthe sorter class B, which provides an easy way of grouping the stack of unsorted\nchunks c and the set of threads d. The main work is done in the do_sort member\nfunction j, which does the usual partitioning of the data 1). This time, rather than\nspawning a new thread for one chunk, it pushes it onto the stack 1! and spawns a new\nthread while you still have processors to spare 1@. Because the lower chunk might be\nhandled by another thread, you then have to wait for it to be ready 1#. In order to\nhelp things along (in case you’re the only thread or all the others are already busy),\nyou try to process chunks from the stack on this thread while you’re waiting 1$.\ntry_sort_chunk pops a chunk off the stack h, and sorts it i, storing the result in the\npromise, ready to be picked up by the thread that posted the chunk on the stack 1%.\n Your freshly spawned threads sit in a loop trying to sort chunks off the stack 1&,\nwhile the end_of_data flag isn’t set 1^. In between checking, they yield to other\nthreads 1* to give them a chance to put some more work on the stack. This code relies\non the destructor of your sorter class e to tidy up these threads. When all the data\nhas been sorted, do_sort will return (even though the worker threads are still run-\nning), so your main thread will return from parallel_quick_sort 2) and destroy\nyour sorter object. This sets the end_of_data flag f and waits for the threads to fin-\nish g. Setting the flag terminates the loop in the thread function 1^.\n With this approach you no longer have the problem of unbounded threads that\nyou have with a spawn_task that launches a new thread, and you’re no longer rely-\ning on the C++ Thread Library to choose the number of threads for you, as it does\nwith std::async(). Instead, you limit the number of threads to the value of std::\nthread::hardware_concurrency() in order to avoid excessive task switching. You do,\nhowever, have another potential problem: the management of these threads and the\ncommunication between them add quite a lot of complexity to the code. Also,\nalthough the threads are processing separate data elements, they all access the stack\nto add new chunks and to remove chunks for processing. This heavy contention can\nreduce performance, even if you use a lock-free (and hence nonblocking) stack, for\nreasons you’ll see shortly.\n This approach is a specialized version of a thread pool—that’s a set of threads that\neach take work to do from a list of pending work, do the work, and then go back to\nthe list for more. Some of the potential problems with thread pools (including the\ncontention on the work list) and ways of addressing them are covered in chapter 9.\n1(\n2)\n\n\n258\nCHAPTER 8\nDesigning concurrent code\nThe problems of scaling your application to multiple processors are discussed in more\ndetail later in this chapter (see section 8.2.1).\n Both dividing the data before processing begins and dividing it recursively pre-\nsume that the data itself is fixed beforehand, and you’re looking at ways of dividing it.\nThis isn’t always the case; if the data is dynamically generated or is coming from exter-\nnal input, this approach doesn’t work. In this case, it might make more sense to divide\nthe work by task type rather than dividing based on the data.\n8.1.3\nDividing work by task type\nDividing work between threads by allocating different chunks of data to each thread\n(whether up front or recursively during processing) still rests on the assumption\nthat the threads are going to be doing the same work on each chunk of data. An\nalternative to dividing the work is to make the threads specialists, where each per-\nforms a distinct task, just as plumbers and electricians perform distinct tasks when\nbuilding a house. Threads may or may not work on the same data, but if they do, it’s\nfor different purposes.\n This is the sort of division of work that results from separating concerns with con-\ncurrency; each thread has a different task, which it carries out independently of other\nthreads. Occasionally other threads may give it data or trigger events that it needs to\nhandle, but in general each thread focuses on doing one thing well. In itself, this is\nbasic good design; each piece of code should have a single responsibility.\nDIVIDING WORK BY TASK TYPE TO SEPARATE CONCERNS\nA single-threaded application has to handle conflicts with the single responsibility\nprinciple where there are multiple tasks that need to be run continuously over a\nperiod of time, or where the application needs to be able to handle incoming events\n(such as user key presses or incoming network data) in a timely fashion, even while\nother tasks are ongoing. In the single-threaded world you end up manually writing\ncode that performs a bit of task A, a bit of task B, checks for key presses, checks for\nincoming network packets, and then loops back to perform another bit of task A. This\nmeans that the code for task A ends up being complicated by the need to save its state\nand return control to the main loop periodically. If you add too many tasks to the\nloop, things might slow down too much, and the user may find it takes too long to\nrespond to the key press. I’m sure you’ve all seen the extreme form of this in action\nwith some application or other: you set it to doing some task, and the interface freezes\nuntil it has completed the task.\n This is where threads come in. If you run each of the tasks in a separate thread, the\noperating system handles this for you. In the code for task A, you can focus on per-\nforming the task and not worry about saving state and returning to the main loop or\nhow long you spend before doing so. The operating system will automatically save the\nstate and switch to task B or C when appropriate, and if the target system has multiple\ncores or processors, tasks A and B may be able to run concurrently. The code for han-\ndling the key press or network packet will now be run in a timely fashion, and everybody\n\n\n259\nTechniques for dividing work between threads\nwins: the user gets timely responses, and you, as the developer, have simpler code\nbecause each thread can focus on doing operations related directly to its responsibili-\nties, rather than getting mixed up with control flow and user interaction.\n That sounds like a nice, rosy vision. Can it be like that? As with everything, it\ndepends on the details. If everything is independent, and the threads have no need to\ncommunicate with each other, then it can be this easy. Unfortunately, the world is\nrarely like that. These nice background tasks are often doing something that the user\nrequested, and they need to let the user know when they’re done by updating the user\ninterface in some manner. Alternatively, the user might want to cancel the task, which\ntherefore requires the user interface to somehow send a message to the background\ntask telling it to stop. Both these cases require careful thought and design and suit-\nable synchronization, but the concerns are still separate. The user interface thread\nstill handles the user interface, but it might have to update it when asked to do so by\nother threads. Likewise, the thread running the background task still focuses on the\noperations required for that task; it just happens that one of them is “allow task to\nbe stopped by another thread.” In neither case do the threads care where the\nrequest came from, only that it was intended for them and relates directly to their\nresponsibilities.\n There are two big dangers with separating concerns with multiple threads. The\nfirst is that you’ll end up separating the wrong concerns. The symptoms to check for\nare that there is a lot of data shared between the threads or the different threads end\nup waiting for each other; both cases boil down to too much communication between\nthreads. If this happens, it’s worth looking at the reasons for the communication. If all\nthe communication relates to the same issue, maybe that should be the key responsi-\nbility of a single thread and extracted from all the threads that refer to it. Alterna-\ntively, if two threads are communicating a lot with each other but much less with other\nthreads, maybe they should be combined into a single thread.\n When dividing work across threads by task type, you don’t have to limit yourself to\ncompletely isolated cases. If multiple sets of input data require the same sequence of\noperations to be applied, you can divide the work so each thread performs one stage\nfrom the overall sequence.\nDIVIDING A SEQUENCE OF TASKS BETWEEN THREADS\nIf your task consists of applying the same sequence of operations to many indepen-\ndent data items, you can use a pipeline to exploit the available concurrency of your sys-\ntem. This is by analogy to a physical pipeline: data flows in at one end through a series\nof operations (pipes) and out at the other end.\n To divide the work this way, you create a separate thread for each stage in the\npipeline—one thread for each of the operations in the sequence. When the opera-\ntion is completed, the data element is put in a queue to be picked up by the next\nthread. This allows the thread performing the first operation in the sequence to\nstart on the next data element while the second thread in the pipeline is working on\nthe first element.\n\n\n260\nCHAPTER 8\nDesigning concurrent code\n This is an alternative to dividing the data between threads, as described in sec-\ntion 8.1.1, and is appropriate in circumstances where the input data itself isn’t all\nknown when the operation is started. For example, the data might be coming in over\na network, or the first operation in the sequence might be to scan a filesystem in order\nto identify files to process.\n Pipelines are also good when each operation in the sequence is time-consuming;\nby dividing the tasks between threads rather than the data, you change the perfor-\nmance profile. Suppose you have 20 data items to process on 4 cores, and each data\nitem requires 4 steps, which take 3 seconds each. If you divide the data between four\nthreads, then each thread has five items to process. Assuming there’s no other pro-\ncessing that might affect the timings, after 12 seconds you’ll have 4 items processed,\nafter 24 seconds 8 items processed, and so on. All 20 items will be done after 1 minute.\nWith a pipeline, things work differently. Each of your four steps can be assigned to a\nprocessing core. Now the first item has to be processed by each core, so it still takes\nthe full 12 seconds. Indeed, after 12 seconds you only have 1 item processed, which\nisn’t as good as with the division by data. But once the pipeline is primed, things pro-\nceed a bit differently; after the first core has processed the first item, it moves on to\nthe second, so once the final core has processed the first item, it can perform its step\non the second. You now get 1 item processed every 3 seconds rather than having the\nitems processed in batches of 4 every 12 seconds.\n The overall time to process the entire batch takes longer because you have to wait\nnine seconds before the final core starts processing the first item. But smoother, more\nregular processing can be beneficial in some circumstances. Consider, for example, a\nsystem for watching high-definition digital videos. In order for the video to be watch-\nable, you typically need at least 25 frames per second and ideally more. Also, the\nviewer needs these to be evenly spaced to give the impression of continuous move-\nment; an application that can decode 100 frames per second is still of no use if it\npauses for a second, then displays 100 frames, then pauses for another second, and\ndisplays another 100 frames. On the other hand, viewers are probably happy to accept\na delay of a couple of seconds when they start watching a video. In this case, paralleliz-\ning using a pipeline that outputs frames at a nice steady rate is probably preferable.\n Having looked at various techniques for dividing the work between threads, let’s\ntake a look at the factors affecting the performance of a multithreaded system and\nhow that can impact your choice of techniques.\n8.2\nFactors affecting the performance of concurrent code\nIf you’re using concurrency to improve the performance of your code on systems with\nmultiple processors, you need to know what factors are going to affect the perfor-\nmance. Even if you’re using multiple threads to separate concerns, you need to\nensure that this doesn’t adversely affect the performance. Customers won’t thank you\nif your application runs more slowly on their shiny new 16-core machine than it did on\ntheir old single-core one.\n\n\n261\nFactors affecting the performance of concurrent code\n As you’ll see shortly, many factors affect the performance of multithreaded code—\neven something as simple as changing which data elements are processed by each\nthread (while keeping everything else identical) can have a dramatic effect on perfor-\nmance. Without further ado, let’s look at some of these factors, starting with the obvi-\nous one: how many processors does your target system have?\n8.2.1\nHow many processors?\nThe number (and structure) of processors is the first big factor that affects the perfor-\nmance of a multithreaded application, and it’s a crucial one. In some cases you know\nexactly what the target hardware is and can design with this in mind, taking real mea-\nsurements on the target system or an exact duplicate. If so, you’re one of the lucky\nones; in general, you don’t have that luxury. You might be developing on a similar sys-\ntem, but the differences can be crucial. For example, you might be developing on a\ndual- or quad-core system, but your customers’ systems may have one multicore proces-\nsor (with any number of cores), or multiple single-core processors, or even multiple\nmulticore processors. The behavior and performance characteristics of a concurrent\nprogram can vary considerably under these different circumstances, so you need to\nthink carefully about what the impact may be and test things where possible.\n To a first approximation, a single 16-core processor is the same as 4 quad-core pro-\ncessors or 16 single-core processors: in each case the system can run 16 threads con-\ncurrently. If you want to take advantage of this, your application must have at least 16\nthreads. If it has fewer than 16, you’re leaving processor power on the table (unless\nthe system is running other applications too, but we’ll ignore that possibility for now).\nOn the other hand, if you have more than 16 threads ready to run (and not blocked,\nwaiting for something), your application will waste processor time switching between\nthe threads, as discussed in chapter 1. When this happens, the situation is called over-\nsubscription.\n To allow applications to scale the number of threads in line with the number of\nthreads the hardware can run concurrently, the C++11 Standard Thread Library pro-\nvides std::thread::hardware_concurrency(). You’ve already seen how that can be\nused to scale the number of threads to the hardware.\n Using std::thread::hardware_concurrency() directly requires care; your code\ndoesn’t take into account any of the other threads that are running on the system\nunless you explicitly share that information. In the worst-case scenario, if multiple\nthreads call a function that uses std::thread::hardware_concurrency() for scaling\nat the same time, there will be huge oversubscription. std::async() avoids this prob-\nlem because the library is aware of all calls and can schedule appropriately. Careful\nuse of thread pools can also avoid this problem.\n But even if you take into account all threads running in your application, you’re\nstill subject to the impact of other applications running at the same time. Although\nthe use of multiple CPU-intensive applications simultaneously is rare on single-user\nsystems, there are some domains where it’s more common. Systems designed to handle\n\n\n262\nCHAPTER 8\nDesigning concurrent code\nthis scenario typically offer mechanisms to allow each application to choose an appro-\npriate number of threads, although these mechanisms are outside the scope of the\nC++ Standard. One option is for a facility like std::async() to take into account the\ntotal number of asynchronous tasks run by all applications when choosing the num-\nber of threads. Another is to limit the number of processing cores that can be used by\na given application. I’d expect this limit to be reflected in the value returned by\nstd::thread::hardware_concurrency() on these platforms, although this isn’t guar-\nanteed. If you need to handle this scenario, consult your system documentation to see\nwhat options are available to you.\n One additional twist to this situation is that the ideal algorithm for a problem can\ndepend on the size of the problem compared to the number of processing units. If\nyou have a massively parallel system with many processing units, an algorithm that per-\nforms more operations overall may finish more quickly than one that performs fewer\noperations, because each processor performs only a few operations.\n As the number of processors increases, so does the likelihood and performance\nimpact of another problem: that of multiple processors trying to access the same data.\n8.2.2\nData contention and cache ping-pong\nIf two threads are executing concurrently on different processors and they’re both\nreading the same data, this usually won’t cause a problem; the data will be copied into\ntheir respective caches, and both processors can proceed. But if one of the threads\nmodifies the data, this change then has to propagate to the cache on the other core,\nwhich takes time. Depending on the nature of the operations on the two threads, and\nthe memory orderings used for the operations, this modification may cause the sec-\nond processor to stop in its tracks and wait for the change to propagate through the\nmemory hardware. In terms of CPU instructions, this can be a phenomenally slow oper-\nation, equivalent to many hundreds of individual instructions, although the exact tim-\ning depends primarily on the physical structure of the hardware.\n Consider the following simple piece of code:\nstd::atomic<unsigned long> counter(0);\nvoid processing_loop()\n{\n    while(counter.fetch_add(1,std::memory_order_relaxed)<100000000)\n    {\n        do_something();\n    }\n}\nThe counter is global, so any threads that call processing_loop() are modifying the\nsame variable. Therefore, for each increment the processor must ensure it has an\nup-to-date copy of counter in its cache, modify the value, and publish it to other pro-\ncessors. Even though you’re using std::memory_order_relaxed, so the compiler\ndoesn’t have to synchronize any other data, fetch_add is a read-modify-write opera-\ntion and therefore needs to retrieve the most recent value of the variable. If another\n\n\n263\nFactors affecting the performance of concurrent code\nthread on another processor is running the same code, the data for counter must\ntherefore be passed back and forth between the two processors and their correspond-\ning caches so that each processor has the latest value for counter when it does the\nincrement. If do_something() is short enough, or if there are too many processors\nrunning this code, the processors might find themselves waiting for each other; one\nprocessor is ready to update the value, but another processor is currently doing that,\nso it has to wait until the second processor has completed its update and the change\nhas propagated. This situation is called high contention. If the processors rarely have to\nwait for each other, you have low contention.\n In a loop like this one, the data for counter will be passed back and forth between\nthe caches many times. This is called cache ping-pong, and it can seriously impact the\nperformance of the application. If a processor stalls because it has to wait for a cache\ntransfer, it can’t do any work in the meantime, even if there are other threads waiting\nthat could do useful work, so this is bad news for the whole application.\n You might think that this won’t happen to you; after all, you don’t have any loops\nlike that. Are you sure? What about mutex locks? If you acquire a mutex in a loop,\nyour code is similar to the previous code from the point of view of data accesses. In\norder to lock the mutex, another thread must transfer the data that makes up the\nmutex to its processor and modify it. When it’s done, it modifies the mutex again to\nunlock it, and the mutex data has to be transferred to the next thread to acquire the\nmutex. This transfer time is in addition to any time that the second thread has to wait\nfor the first to release the mutex:\nstd::mutex m;\nmy_data data;\nvoid processing_loop_with_mutex()\n{\n    while(true)\n    {\n        std::lock_guard<std::mutex> lk(m);\n        if(done_processing(data)) break;\n    }\n}\nNow, here’s the worst part: if the data and mutex are accessed by more than one\nthread, then as you add more cores and processors to the system, it becomes more\nlikely that you will get high contention and one processor having to wait for another.\nIf you’re using multiple threads to process the same data more quickly, the threads are\ncompeting for the data and thus competing for the same mutex. The more of them\nthere are, the more likely they’ll try to acquire the mutex at the same time, or access\nthe atomic variable at the same time, and so forth.\n The effects of contention with mutexes are usually different from the effects of\ncontention with atomic operations for the simple reason that the use of a mutex natu-\nrally serializes threads at the operating system level rather than at the processor level.\nIf you have enough threads ready to run, the operating system can schedule another\n\n\n264\nCHAPTER 8\nDesigning concurrent code\nthread to run while one thread is waiting for the mutex, whereas a processor stall pre-\nvents any threads from running on that processor. But it will still impact the perfor-\nmance of those threads that are competing for the mutex; they can only run one at a\ntime, after all.\n Back in chapter 3, you saw how a rarely updated data structure can be protected\nwith a single-writer, multiple-reader mutex (see section 3.3.2). Cache ping-pong\neffects can nullify the benefits of this mutex if the workload is unfavorable, because all\nthreads accessing the data (even reader threads) still have to modify the mutex itself.\nAs the number of processors accessing the data goes up, the contention on the mutex\nitself increases, and the cache line holding the mutex must be transferred between\ncores, potentially increasing the time taken to acquire and release locks to undesir-\nable levels. There are techniques to ameliorate this problem by spreading out the\nmutex across multiple cache lines, but unless you implement your own mutex, you are\nsubject to whatever your system provides.\n If this cache ping-pong is bad, how can you avoid it? As you’ll see later in the chap-\nter, the answer ties in nicely with general guidelines for improving the potential for\nconcurrency: do what you can to reduce the potential for two threads competing for\nthe same memory location.\n It’s not quite that simple, though; things never are. Even if a particular memory\nlocation is only ever accessed by one thread, you can still get cache ping-pong due to\nan effect known as false sharing.\n8.2.3\nFalse sharing\nProcessor caches don’t generally deal in individual memory locations; instead, they\ndeal in blocks of memory called cache lines. These blocks of memory are typically 32 or\n64 bytes in size, but the exact details depend on the particular processor model being\nused. Because the cache hardware only deals in cache-line-sized blocks of memory,\nsmall data items in adjacent memory locations will be in the same cache line. Sometimes\nthis is good: if a set of data accessed by a thread is in the same cache line, this is better\nfor the performance of the application than if the same set of data was spread over mul-\ntiple cache lines. But if the data items in a cache line are unrelated and need to be\naccessed by different threads, this can be a major cause of performance problems.\n Suppose you have an array of int values and a set of threads that each access their\nown entry in the array but do so repeatedly, including updates. Because an int is typi-\ncally much smaller than a cache line, quite a few of those array entries will be in the\nsame cache line. Consequently, even though each thread only accesses its own array\nentry, the cache hardware still has to play cache ping-pong. Every time the thread\naccessing entry 0 needs to update the value, ownership of the cache line needs to be\ntransferred to the processor running that thread, only to be transferred to the cache\nfor the processor running the thread for entry 1 when that thread needs to update its\ndata item. The cache line is shared, even though none of the data is, hence the term\nfalse sharing. The solution here is to structure the data so that data items to be accessed\n\n\n265\nFactors affecting the performance of concurrent code\nby the same thread are close together in memory (and thus more likely to be in the\nsame cache line), whereas those that are to be accessed by separate threads are far\napart in memory and thus more likely to be in separate cache lines. You’ll see how this\naffects the design of the code and data later in this chapter. The C++17 standard\ndefines std::hardware_destructive_interference_size in the header <new>, which\nspecifies the maximum number of consecutive bytes that may be subject to false shar-\ning for the current compilation target. If you ensure that your data is at least this num-\nber of bytes apart, then there will be no false sharing.\n If having multiple threads access data from the same cache line is bad, how does\nthe memory layout of data accessed by a single thread affect things?\n8.2.4\nHow close is your data?\nAlthough false sharing is caused by having data accessed by one thread too close to\ndata accessed by another thread, another pitfall associated with data layout directly\nimpacts the performance of a single thread on its own. The issue is data proximity: if\nthe data accessed by a single thread is spread out in memory, it’s likely that it lies on\nseparate cache lines. On the flip side, if the data accessed by a single thread is close\ntogether in memory, it’s more likely to lie on the same cache line. Consequently, if\ndata is spread out, more cache lines must be loaded from memory onto the processor\ncache, which can increase memory access latency and reduce performance compared\nto data that’s located close together.\n Also, if the data is spread out, there’s an increased chance that a given cache line\ncontaining data for the current thread also contains data that’s not for the current\nthread. At the extreme, there’ll be more data in the cache that you don’t care about\nthan data that you do. This wastes precious cache space and increases the chance that\nthe processor will experience a cache miss and have to fetch a data item from main\nmemory even if it once held it in the cache, because it had to remove the item from\nthe cache to make room for another.\n Now, this is important with single-threaded code, so why am I bringing it up here?\nThe reason is task switching. If there are more threads than cores in the system, each\ncore is going to be running multiple threads. This increases the pressure on the\ncache, as you try to ensure that different threads are accessing different cache lines in\norder to avoid false sharing. Consequently, when the processor switches threads, it’s\nmore likely to have to reload the cache lines if each thread uses data spread across\nmultiple cache lines than if each thread’s data is close together in the same cache line.\nThe C++17 standard specifies the constant std::hardware_constructive_interfer-\nence_size, also in the header <new>, which is the maximum number of consecutive\nbytes guaranteed to be on the same cache line (if suitably aligned). If you can fit data\nthat is needed together within this number of bytes, it will potentially reduce the num-\nber of cache misses.\n If there are more threads than cores or processors, the operating system might also\nchoose to schedule a thread on one core for one time slice and then on another core\n\n\n266\nCHAPTER 8\nDesigning concurrent code\nfor the next time slice. This will therefore require transferring the cache lines for that\nthread’s data from the cache for the first core to the cache for the second; the more\ncache lines that need transferring, the more time-consuming this will be. Although\noperating systems typically avoid this when they can, it does happen and does impact\nperformance.\n Task-switching problems are particularly prevalent when lots of threads are ready to\nrun as opposed to waiting. This is an issue we’ve already touched on: oversubscription.\n8.2.5\nOversubscription and excessive task switching\nIn multithreaded systems, it’s typical to have more threads than processors, unless\nyou’re running on massively parallel hardware. But threads often spend time waiting\nfor external I/O to complete, blocked on mutexes, waiting for condition variables, and\nso forth, so this isn’t a problem. Having the extra threads enables the application to per-\nform useful work rather than having processors sitting idle while the threads wait.\n This isn’t always a good thing. If you have too many additional threads, there will\nbe more threads ready to run than there are available processors, and the operating\nsystem will have to start task switching quite heavily in order to ensure they all get a\nfair time slice. As you saw in chapter 1, this can increase the overhead of the task\nswitching as well as compound any cache problems resulting from lack of proximity.\nOversubscription can arise when you have a task that repeatedly spawns new threads\nwithout limits, as the recursive quick sort from chapter 4 did, or where the natural\nnumber of threads when you separate by task type is more than the number of proces-\nsors and the work is naturally CPU-bound rather than I/O-bound.\n If you’re spawning too many threads because of data division, you can limit the\nnumber of worker threads, as you saw in section 8.1.2. If the oversubscription is due to\nthe natural division of work, there’s not a lot you can do to ameliorate the problem\nsave choosing a different division. In that case, choosing the appropriate division may\nrequire more knowledge of the target platform than you have available and is only\nworth doing if performance is unacceptable and it can be demonstrated that chang-\ning the division of work does improve performance.\n Other factors can affect the performance of multithreaded code. The cost of cache\nping-pong can vary quite considerably between two single-core processors and a single\ndual-core processor, even if they’re the same CPU type and clock speed, for example,\nbut these are the major ones that will have a visible impact. Let’s now look at how that\naffects the design of the code and data structures.\n8.3\nDesigning data structures for multithreaded \nperformance\nIn section 8.1 we looked at various ways of dividing work between threads, and in sec-\ntion 8.2 we looked at various factors that can affect the performance of your code.\nHow can you use this information when designing data structures for multithreaded\nperformance? This is a different question than that addressed in chapters 6 and 7,\n",
      "page_number": 278
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 290-297)",
      "start_page": 290,
      "end_page": 297,
      "detection_method": "topic_boundary",
      "content": "267\nDesigning data structures for multithreaded performance\nwhich were about designing data structures that are safe for concurrent access. As\nyou’ve seen in section 8.2, the layout of the data used by a single thread can have an\nimpact, even if that data isn’t shared with any other threads.\n The key things to bear in mind when designing your data structures for multi-\nthreaded performance are contention, false sharing, and data proximity. All three of these\ncan have a big impact on performance, and you can often improve things by altering\nthe data layout or changing which data elements are assigned to which thread. First\noff, let’s look at an easy win: dividing array elements between threads.\n8.3.1\nDividing array elements for complex operations\nSuppose you’re doing some heavy-duty math, and you need to multiply two large\nsquare matrices together. To multiply matrices, you multiply each element in the first\nrow of the first matrix with the corresponding element of the first column of the second\nmatrix and add up the products to give the top-left element of the result. You then\nrepeat this with the second row and the first column to give the second element in the\nfirst column of the result, and with the first row and second column to give the first\nelement in the second column of the result, and so forth. This is shown in figure 8.3;\nthe highlighting shows that the second row of the first matrix is paired with the third\ncolumn of the second matrix to give the entry in the second row of the third column\nof the result.\nNow let’s assume that these are large matrices with several thousand rows and col-\numns, in order to make it worthwhile to use multiple threads to optimize the multipli-\ncation. Typically, a nonsparse matrix is represented by a big array in memory, with all\nthe elements of the first row followed by all the elements of the second row, and so\nforth. To multiply your matrices you have three of these huge arrays. In order to get\noptimal performance, you need to pay careful attention to the data access patterns,\nparticularly the writes to the third array.\n There are many ways you can divide the work between threads. Assuming you have\nmore rows/columns than available processors, you could have each thread calculate\nthe values for a number of columns in the result matrix, or have each thread calculate\na1,1 a2,1 a3,1 a4,1\nan,1\na1,2 a2,2 a3,2 a4,2\nan,2\na1,3 a2,3 a3,3 a4,3\nan,3\na1,ma2,ma3,ma4,m\nan,m\nb1,1 b2,1 b3,1 b4,1\nbk,1\nb1,2 b2,2 b3,2 b4,2\nbk,2\nb1,3 b2,3 b3,3 b4,3\nbk,3\nb1,n b2,n b3,n b4,n\nbk,n\nc1,1 c2,1 c3,1 c4,1\nck,1\nc1,2 c2,2 c3,2 c4,2\nck,2\nc1,3 c2,3 c3,3 c4,3\nck,3\nc1,m c2,m c3,m c4,m\nck,m\n=\nFigure 8.3\nMatrix multiplication\n\n\n268\nCHAPTER 8\nDesigning concurrent code\nthe results for a number of rows, or even have each thread calculate the results for a\nrectangular subset of the matrix.\n Back in sections 8.2.3 and 8.2.4, you saw that it’s better to access contiguous ele-\nments from an array rather than values all over the place, because this reduces cache\nusage and the chance of false sharing. If you have each thread compute a set of col-\numns, it needs to read every value from the first matrix and the values from the corre-\nsponding columns in the second matrix, but you only have to write the column values.\nGiven that the matrices are stored with the rows contiguous, this means that you’re\naccessing N elements from the first row, N elements from the second, and so forth\n(where N is the number of columns you’re processing). Because other threads will be\naccessing the other elements of each row, it’s clear that you ought to be accessing adja-\ncent columns, so the N elements from each row are adjacent, and you minimize false\nsharing. If the space occupied by your N elements is an exact number of cache lines,\nthere’ll be no false sharing because threads will be working on separate cache lines.\n On the other hand, if you have each thread compute a set of rows, then it needs to\nread every value from the second matrix and the values from the corresponding rows of\nthe first matrix, but it only has to write the row values. Because the matrices are stored\nwith the rows contiguous, you’re now accessing all elements from N rows. If you again\nchoose adjacent rows, this means that the thread is now the only thread writing to\nthose N rows; it has a contiguous block of memory that’s not touched by any other\nthread. This is likely an improvement over having each thread compute a set of col-\numns, because the only possibility of false sharing is for the last few elements of one\nblock with the first few of the next, but it’s worth timing it on the target architecture\nto confirm.\n What about your third option—dividing into rectangular blocks? This can be\nviewed as dividing into columns and then dividing into rows. As such, it has the same\nfalse-sharing potential as division by columns. If you can choose the number of col-\numns in the block to avoid this possibility, there’s an advantage to rectangular division\nfrom the read side: you don’t need to read the entirety of either source matrix. You\nonly need to read the values corresponding to the rows and columns of the target rect-\nangle. To look at this in concrete terms, consider multiplying two matrices that have\n1,000 rows and 1,000 columns. That’s 1 million elements. If you have 100 processors,\nthey can compute 10 rows each for a nice round 10,000 elements. But to calculate the\nresults of those 10,000 elements, they need to access the entirety of the second matrix\n(1 million elements) plus the 10,000 elements from the corresponding rows in the\nfirst matrix, for a grand total of 1,010,000 elements. On the other hand, if they each\ncompute a block of 100 elements by 100 elements (which is still 10,000 elements\ntotal), they need to access the values from 100 rows of the first matrix (100 x 1,000 =\n100,000 elements) and 100 columns of the second matrix (another 100,000). This is\nonly 200,000 elements, which is a five-fold reduction in the number of elements read.\nIf you’re reading fewer elements, there’s less chance of a cache miss and the potential\nfor greater performance.\n\n\n269\nDesigning data structures for multithreaded performance\n It may therefore be better to divide the result matrix into small, square or almost-\nsquare blocks rather than have each thread compute the entirety of a small number of\nrows. You can adjust the size of each block at runtime, depending on the size of the\nmatrices and the available number of processors. As ever, if performance is important,\nit’s vital to profile various options on the target architecture, and check the literature\nrelevant to the field—I make no claim that these are the only or best options if you are\ndoing matrix multiplication\n Chances are you’re not doing matrix multiplication, so how does this apply to you?\nThe same principles apply to any situation where you have large blocks of data to\ndivide between threads; look at all the aspects of the data access patterns carefully, and\nidentify the potential causes of performance hits. There may be similar circumstances\nin your problem domain where changing the division of work can improve perfor-\nmance without requiring any change to the basic algorithm.\n OK, so we’ve looked at how access patterns in arrays can affect performance. What\nabout other types of data structures?\n8.3.2\nData access patterns in other data structures\nFundamentally, the same considerations apply when trying to optimize the data access\npatterns of other data structures as when optimizing access to arrays:\nTry to adjust the data distribution between threads so that data that’s close\ntogether is worked on by the same thread.\nTry to minimize the data required by any given thread.\nTry to ensure that data accessed by separate threads is sufficiently far apart to\navoid false sharing using std::hardware_destructive_interference_size as\na guide.\nThat’s not easy to apply to other data structures. For example, binary trees are inher-\nently difficult to subdivide in any unit other than a subtree, which may or may not be\nuseful, depending on how balanced the tree is and how many sections you need to\ndivide it into. Also, the nature of the trees means that the nodes are likely dynamically\nallocated and thus end up in different places on the heap.\n Now, having data end up in different places on the heap isn’t a particular problem\nin itself, but it does mean that the processor has to keep more things in cache. This\ncan be beneficial. If multiple threads need to traverse the tree, then they all need to\naccess the tree nodes, but if the tree nodes only contain pointers to the real data held\nat the node, then the processor only has to load the data from memory if it’s\nneeded. If the data is being modified by the threads that need it, this can avoid the\nperformance hit of false sharing between the node data itself and the data that pro-\nvides the tree structure.\n There’s a similar issue with data protected by a mutex. Suppose you have a simple\nclass that contains a few data items and a mutex used to protect accesses from multiple\nthreads. If the mutex and the data items are close together in memory, this is ideal for\n\n\n270\nCHAPTER 8\nDesigning concurrent code\na thread that acquires the mutex; the data it needs may already be in the processor\ncache, because it was loaded in order to modify the mutex. But there’s also a downside:\nif other threads try to lock the mutex while it’s held by the first thread, they’ll need\naccess to that memory. Mutex locks are typically implemented as a read-modify-write\natomic operation on a memory location within the mutex to try to acquire the mutex,\nfollowed by a call to the operating system kernel if the mutex is already locked. This\nread-modify-write operation may cause the data held in the cache by the thread that\nowns the mutex to be invalidated. As far as the mutex goes, this isn’t a problem; that\nthread isn’t going to touch the mutex until it unlocks it. But if the mutex shares a\ncache line with the data being used by the thread, the thread that owns the mutex can\ntake a performance hit because another thread tried to lock the mutex!\n One way to test whether this kind of false sharing is a problem is to add huge\nblocks of padding between the data elements that can be concurrently accessed by dif-\nferent threads. For example, you can use\nstruct protected_data\n{\n    std::mutex m;\n    char padding[std::hardware_destructive_interference_size];     \n    my_data data_to_protect;\n};\nto test the mutex contention issue or\nstruct my_data\n{\n    data_item1 d1;\n    data_item2 d2;\n    char padding[std::hardware_destructive_interference_size];\n};\nmy_data some_array[256];\nto test for false sharing of array data. If this improves the performance, you know that\nfalse sharing was a problem, and you can either leave the padding in or work to elimi-\nnate the false sharing in another way by rearranging the data accesses.\n There’s more than the data access patterns to consider when designing for concur-\nrency, so let’s look at some of these additional considerations.\n8.4\nAdditional considerations when designing for \nconcurrency\nSo far in this chapter we’ve looked at ways of dividing work between threads, factors\naffecting performance, and how these factors affect your choice of data access pat-\nterns and data structures. There’s more to designing code for concurrency than that,\nthough. You also need to consider things such as exception safety and scalability. Code\nis said to be scalable if the performance (whether in terms of reduced speed of execution\nIf std::hardware_destructive_interference_size is\nnot available with your compiler, you could use\nsomething like 65536 bytes which is likely to be\norders of magnitude larger than a cache line\n\n\n271\nAdditional considerations when designing for concurrency\nor increased throughput) increases as more processing cores are added to the system.\nIdeally, the performance increase is linear, so a system with 100 processors performs\n100 times better than a system with one processor.\n Although code can work even if it isn’t scalable—a single-threaded application is\ncertainly not scalable, for example—exception safety is a matter of correctness. If your\ncode isn’t exception-safe, you can end up with broken invariants or race conditions, or\nyour application might terminate unexpectedly because an operation threw an excep-\ntion. With this in mind, we’ll look at exception safety first.\n8.4.1\nException safety in parallel algorithms\nException safety is an essential aspect of good C++ code, and code that uses concur-\nrency is no exception. In fact, parallel algorithms often require that you take more\ncare with exceptions than normal sequential algorithms. If an operation in a sequen-\ntial algorithm throws an exception, the algorithm only has to worry about ensuring\nthat it tidies up after itself to avoid resource leaks and broken invariants; it can mer-\nrily allow the exception to propagate to the caller for them to handle. By contrast, in\na parallel algorithm many of the operations will be running on separate threads. In\nthis case, the exception can’t be allowed to propagate because it’s on the wrong call\nstack. If a function spawned on a new thread exits with an exception, the application\nis terminated.\n As a concrete example, let’s revisit the parallel_accumulate function from list-\ning 2.9, which is reproduced here.\ntemplate<typename Iterator,typename T>\nstruct accumulate_block\n{\n    void operator()(Iterator first,Iterator last,T& result)\n    {\n        result=std::accumulate(first,last,result);    \n    }\n};\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);    \n    if(!length)\n        return init;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\n    std::vector<T> results(num_threads);           \nListing 8.2\nA naive parallel version of std::accumulate (from listing 2.9)\nb\nc\nd\n\n\n272\nCHAPTER 8\nDesigning concurrent code\n    std::vector<std::thread>  threads(num_threads-1);    \n    Iterator block_start=first;      \n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_end=block_start;     \n        std::advance(block_end,block_size);\n        threads[i]=std::thread(             \n            accumulate_block<Iterator,T>(),\n            block_start,block_end,std::ref(results[i]));\n        block_start=block_end;  \n    }\n    accumulate_block<Iterator,T>()(\n        block_start,last,results[num_threads-1]);    \n    std::for_each(threads.begin(),threads.end(),\n        std::mem_fn(&std::thread::join));\n    return std::accumulate(results.begin(),results.end(),init);   \n}\nNow let’s go through and identify the places where an exception can be thrown: any-\nwhere where you call a function you know can throw or you perform an operation on\na user-defined type that may throw.\n First up, you have the call to distance c, which performs operations on the user-\nsupplied iterator type. Because you haven’t yet done any work, and this is on the call-\ning thread, it’s fine. Next up, you have the allocation of the results vector d and the\nthreads vector e. Again, these are on the calling thread, and you haven’t done any\nwork or spawned any threads, so this is fine. If the construction of threads throws, the\nmemory allocated for results will have to be cleaned up, but the destructor will take\ncare of that for you.\n Skipping over the initialization of block_start f, because that’s similarly safe,\nyou come to the operations in the thread-spawning loop, g, h, and i. Once you’ve\nbeen through the creation of the first thread at h, you’re in trouble if you throw any\nexceptions; the destructors of your new std::thread objects will call std::terminate\nand abort your program. This isn’t a good place to be.\n The call to accumulate_block j, can potentially throw, with similar conse-\nquences; your thread objects will be destroyed and call std::terminate. On the other\nhand, the final call to std::accumulate 1) can throw without causing any hardship,\nbecause all the threads have been joined by this point.\n That’s it for the main thread, but there’s more: the calls to accumulate_block on\nthe new threads might throw at B. There aren’t any catch blocks, so this exception\nwill be left unhandled and cause the library to call std::terminate() to abort the\napplication.\n In case it’s not glaringly obvious, this code isn’t exception-safe.\nADDING EXCEPTION SAFETY\nOK, so we’ve identified all the possible throw points and the nasty consequences of\nexceptions. What can you do about it? Let’s start by addressing the issue of the excep-\ntions thrown on your new threads.\ne\nf\ng\nh\ni\nj\n1)\n\n\n273\nAdditional considerations when designing for concurrency\n You encountered the tool for this job in chapter 4. If you look carefully at what\nyou’re trying to achieve with new threads, it’s apparent that you’re trying to calculate a\nresult to return while allowing for the possibility that the code might throw an excep-\ntion. This is precisely what the combination of std::packaged_task and std::future\nis designed for. If you rearrange your code to use std::packaged_task, you end up\nwith the following code.\ntemplate<typename Iterator,typename T>\nstruct accumulate_block\n{\n    T operator()(Iterator first,Iterator last)   \n    {\n        return std::accumulate(first,last,T());    \n    }\n};\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return init;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\n    std::vector<std::future<T> > futures(num_threads-1);    \n    std::vector<std::thread> threads(num_threads-1);\n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_end=block_start;\n        std::advance(block_end,block_size);\n        std::packaged_task<T(Iterator,Iterator)> task(    \n            accumulate_block<Iterator,T>());\n        futures[i]=task.get_future();     \n        threads[i]=std::thread(std::move(task),block_start,block_end); \n        block_start=block_end;\n    }\n    T last_result=accumulate_block<Iterator,T>()(block_start,last);   \n    std::for_each(threads.begin(),threads.end(),\n        std::mem_fn(&std::thread::join));\n    T result=init;    \n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        result+=futures[i].get();   \n    }\nListing 8.3\nA parallel version of std::accumulate using std::packaged_task\nb\nc\nd\ne\nf\ng\nh\ni\nj\n\n\n274\nCHAPTER 8\nDesigning concurrent code\n    result += last_result;    \n    return result;\n}\nThe first change is that the function call operator of accumulate_block now returns\nthe result directly, rather than taking a reference to somewhere to store it B. You’re\nusing std::packaged_task and std::future for the exception safety, so you can use it\nto transfer the result too. This does require that you explicitly pass a default-constructed\nT in the call to std::accumulate c, rather than reusing the supplied result value,\nbut that’s a minor change.\n The next change is that rather than having a vector of results, you have a vector of\nfutures d to store an std::future<T> for each spawned thread. In the thread-\nspawning loop, you first create a task for accumulate_block e. std::packaged\n_task<T(Iterator, Iterator)> declares a task that takes two Iterators and returns\na T, which is what your function does. You then get the future for that task f and run\nthat task on a new thread, passing in the start and end of the block to process g.\nWhen the task runs, the result will be captured in the future, as will any exception\nthrown.\n Because you’ve been using futures, you don’t have a result array, so you must store\nthe result from the final block in a variable h, rather than in a slot in the array. Also,\nbecause you have to get the values out of the futures, it’s now simpler to use a basic\nfor loop rather than std::accumulate, starting with the supplied initial value i and\nadding in the result from each future j. If the corresponding task threw an excep-\ntion, this will have been captured in the future and will now be thrown again by the\ncall to get(). Finally, you add the result from the last block 1) before returning the\noverall result to the caller.\n So, that’s removed one of the potential problems: exceptions thrown in the worker\nthreads are rethrown in the main thread. If more than one of the worker threads\nthrows an exception, only one will be propagated, but that’s not too big a deal. If it\nmatters, you can use something like std::nested_exception to capture all the excep-\ntions and throw that instead.\n The remaining problem is the leaking threads if an exception is thrown between\nwhen you spawn the first thread and when you’ve joined with them all. The simplest\nsolution is to catch any exceptions, join with the threads that are still joinable(), and\nrethrow the exception:\ntry\n{\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        // ... as before\n    }\n    T last_result=accumulate_block<Iterator,T>()(block_start,last);\n    std::for_each(threads.begin(),threads.end(),\n        std::mem_fn(&std::thread::join));\n}\n1)\n",
      "page_number": 290
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 298-305)",
      "start_page": 298,
      "end_page": 305,
      "detection_method": "topic_boundary",
      "content": "275\nAdditional considerations when designing for concurrency\ncatch(...)\n{\n    for(unsigned long i=0;i<(num_thread-1);++i)\n    {\n        if(threads[i].joinable())\n            thread[i].join();\n    }\n    throw;\n}\nNow this works. All the threads will be joined, no matter how the code leaves the\nblock. But try-catch blocks are ugly, and you have duplicate code. You’re joining\nthe threads both in the “normal” control flow and in the catch block. Duplicate code\nis rarely a good thing, because it means more places to change. Instead, let’s extract\nthis out into the destructor of an object; it is, after all, the idiomatic way of cleaning up\nresources in C++. Here’s your class:\nclass join_threads\n{\n    std::vector<std::thread>& threads;\npublic:\n    explicit join_threads(std::vector<std::thread>& threads_):\n        threads(threads_)\n    {}\n    ~join_threads()\n    {\n        for(unsigned long i=0;i<threads.size();++i)\n        {\n            if(threads[i].joinable())\n                threads[i].join();\n        }\n    }\n};\nThis is similar to your thread_guard class from listing 2.3, except it’s extended for the\nwhole vector of threads. You can then simplify your code as follows.\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return init;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\nListing 8.4\nAn exception-safe parallel version of std::accumulate \n\n\n276\nCHAPTER 8\nDesigning concurrent code\n    std::vector<std::future<T> > futures(num_threads-1);\n    std::vector<std::thread> threads(num_threads-1);\n    join_threads joiner(threads);    \n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_end=block_start;\n        std::advance(block_end,block_size);\n        std::packaged_task<T(Iterator,Iterator)> task(\n            accumulate_block<Iterator,T>());\n        futures[i]=task.get_future();\n        threads[i]=std::thread(std::move(task),block_start,block_end);\n        block_start=block_end;\n    }\n    T last_result=accumulate_block<Iterator,T>()(block_start,last);\n    T result=init;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        result+=futures[i].get();   \n    }\n    result += last_result;\n    return result;\n}\nOnce you’ve created your container of threads, you create an instance of your new\nclass B to join with all the threads on exit. You can then remove your explicit join\nloop, safe in the knowledge that the threads will be joined however the function exits.\nNote that the calls to futures[i].get() c will block until the results are ready, so\nyou don’t need to have explicitly joined with the threads at this point. This is unlike\nthe original from listing 8.2, where you needed to have joined with the threads to\nensure that the results vector was correctly populated. Not only do you get exception-\nsafe code, but your function is shorter because you’ve extracted the join code into\nyour new (reusable) class.\nEXCEPTION SAFETY WITH STD::ASYNC()\nNow that you’ve seen what’s required for exception safety when explicitly managing\nthe threads, let’s take a look at the same thing done with std::async(). As you’ve\nalready seen, in this case the library takes care of managing the threads for you, and\nany threads spawned are completed when the future is ready. The key thing to note\nfor exception safety is that if you destroy the future without waiting for it, the destruc-\ntor will wait for the thread to complete. This neatly avoids the problem of leaked\nthreads that are still executing and holding references to the data. The next listing\nshows an exception-safe implementation using std::async().\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);    \nListing 8.5\nAn exception-safe parallel version of std::accumulate using std::async\nb\nc\nb\n\n\n277\nAdditional considerations when designing for concurrency\n    unsigned long const max_chunk_size=25;\n    if(length<=max_chunk_size)\n    {\n        return std::accumulate(first,last,init);   \n    }\n    else\n    {\n    Iterator mid_point=first;\n        std::advance(mid_point,length/2);    \n        std::future<T> first_half_result=\n            std::async(parallel_accumulate<Iterator,T>,     \n                       first,mid_point,init);\n        T second_half_result=parallel_accumulate(mid_point,last,T());  \n        return first_half_result.get()+second_half_result;    \n    }\n}\nThis version uses a recursive division of the data rather than pre-calculating the divi-\nsion of the data into chunks, but it’s a whole lot simpler than the previous version, and\nit’s still exception-safe. As before, you start by finding the length of the sequence B, and\nif it’s smaller than the maximum chunk size, you resort to calling std::accumulate\ndirectly c. If there are more elements than your chunk size, you find the midpoint d\nand then spawn an asynchronous task to handle that half e. The second half of the\nrange is handled with a direct recursive call f, and then the results from the two\nchunks are added together g. The library ensures that the std::async calls make use\nof the hardware threads that are available without creating an overwhelming number\nof threads. Some of the “asynchronous” calls will be executed synchronously in the\ncall to get() g.\n The beauty of this is that not only can it take advantage of the hardware concur-\nrency, but it’s also trivially exception-safe. If an exception is thrown by the recursive\ncall f, the future created from the call to std::async e will be destroyed as the\nexception propagates. This will in turn wait for the asynchronous task to finish, avoid-\ning a dangling thread. On the other hand, if the asynchronous call throws, this is cap-\ntured by the future, and the call to get() g will rethrow the exception.\n What other considerations do you need to take into account when designing con-\ncurrent code? Let’s look at scalability. How much does the performance improve if you\nmove your code to a system with more processors?\n8.4.2\nScalability and Amdahl’s law\nScalability is all about ensuring that your application can take advantage of additional\nprocessors in the system it’s running on. At one extreme you have a single-threaded\napplication that’s completely unscalable; even if you add 100 processors to your sys-\ntem, the performance will remain unchanged. At the other extreme you have some-\nthing like the SETI@Home (http://setiathome.ssl.berkeley.edu/) project, which is\ndesigned to take advantage of thousands of additional processors (in the form of indi-\nvidual computers added to the network by users) as they become available.\nc\nd\ne\nf\ng\n\n\n278\nCHAPTER 8\nDesigning concurrent code\n For any given multithreaded program, the number of threads that are performing\nuseful work will vary as the program runs. Even if every thread is doing useful work for\nthe entirety of its existence, the application may initially have only one thread, which\nwill then have the task of spawning all the others. But even that’s a highly unlikely sce-\nnario. Threads often spend time waiting for each other or waiting for I/O operations\nto complete.\n Every time one thread has to wait for something (whatever that something is),\nunless there’s another thread ready to take its place on the processor, you have a pro-\ncessor sitting idle that could be doing useful work.\n A simplified way of looking at this is to divide the program into “serial” sections\nwhere only one thread is doing any useful work and “parallel” sections where all the\navailable processors are doing useful work. If you run your application on a system\nwith more processors, the “parallel” sections will theoretically be able to complete\nmore quickly, because the work can be divided between more processors, whereas the\n“serial” sections will remain serial. Under such a simplified set of assumptions, you can\ntherefore estimate the potential performance gain to be achieved by increasing the\nnumber of processors: if the “serial” sections constitute a fraction, fs, of the program,\nthen the performance gain, P, from using N processors can be estimated as\nThis is Amdahl’s law, which is often cited when talking about the performance of con-\ncurrent code. If everything can be parallelized, so the serial fraction is 0, the speedup\nis N. Alternatively, if the serial fraction is one-third, even with an infinite number of\nprocessors you’re not going to get a speedup of more than 3.\n But this paints a naive picture, because tasks are rarely infinitely divisible in the way\nthat would be required for the equation to hold, and it’s also rare for everything to be\nCPU-bound in the way that’s assumed. As you’ve seen, threads may wait for many\nthings while executing.\n One thing that’s clear from Amdahl’s law is that when you’re using concurrency\nfor performance, it’s worth looking at the overall design of the application to maxi-\nmize the potential for concurrency and ensure that there’s always useful work for the\nprocessors to be doing. If you can reduce the size of the “serial” sections or reduce the\npotential for threads to wait, you can improve the potential for performance gains on\nsystems with more processors. Alternatively, if you can provide more data for the sys-\ntem to process, and thus keep the parallel sections primed with work, you can reduce\nthe serial fraction and increase the performance gain, P.\n Scalability is about reducing the time it takes to perform an action or increasing the amount\nof data that can be processed in a given time as more processors are added. Sometimes\nthese are equivalent (you can process more data if each element is processed faster),\nbut not always. Before choosing the techniques to use for dividing work between\nP\n1\nfs\n1 fs\n–\nN\n---------\n+\n-------------------\n=\n\n\n279\nAdditional considerations when designing for concurrency\nthreads, it’s important to identify which of these aspects of scalability are important\nto you.\n I mentioned at the beginning of this section that threads don’t always have useful\nwork to do. Sometimes they have to wait for other threads, or for I/O to complete, or\nfor something else. If you give the system something useful to do during this wait, you\ncan effectively “hide” the waiting.\n8.4.3\nHiding latency with multiple threads\nFor most of the discussions of the performance of multithreaded code, we’ve been\nassuming that the threads are running “flat out” and always have useful work to do\nwhen they’re running on a processor. This is not true; in application code, threads fre-\nquently block while waiting for something. For example, they may be waiting for some\nI/O to complete, waiting to acquire a mutex, waiting for another thread to complete\nsome operation and notify a condition variable or populate a future, or even sleeping\nfor a period of time.\n Whatever the reason for the waits, if you have only as many threads as there are\nphysical processing units in the system, having blocked threads means you’re wasting\nCPU time. The processor that would otherwise be running a blocked thread is instead\ndoing nothing. Consequently, if you know that one of your threads is likely to spend a\nconsiderable portion of its time waiting around, you can make use of that spare CPU\ntime by running one or more additional threads.\n Consider a virus-scanner application, which divides the work across threads using\na pipeline. The first thread searches the filesystem for files to check and puts them\nin a queue. Meanwhile, another thread takes filenames from the queue, loads the files,\nand scans them for viruses. You know that the thread searching the filesystem for files\nto scan is definitely going to be I/O-bound, so you make use of the “spare” CPU time\nby running an additional scanning thread. You’d then have one file-searching thread\nand as many scanning threads as there are physical cores or processors in the system.\nBecause the scanning thread may also have to read significant portions of the files off\nthe disk in order to scan them, it might make sense to have even more scanning\nthreads. But at some point there’ll be too many threads, and the system will slow down\nagain as it spends more and more time task switching, as described in section 8.2.5.\n As ever, this is an optimization, so it’s important to measure performance before\nand after any change in the number of threads; the optimal number of threads will be\nhighly dependent on the nature of the work being done and the percentage of time\nthe thread spends waiting.\n Depending on the application, it might be possible to use up this spare CPU time\nwithout running additional threads. For example, if a thread is blocked because it’s\nwaiting for an I/O operation to complete, it might make sense to use asynchronous\nI/O if that’s available, and then the thread can perform other useful work while the\nI/O is performed in the background. In other cases, if a thread is waiting for another\nthread to perform an operation, then rather than blocking, the waiting thread might\n\n\n280\nCHAPTER 8\nDesigning concurrent code\nbe able to perform that operation itself, as you saw with the lock-free queue in chap-\nter 7. In an extreme case, if a thread is waiting for a task to be completed and that task\nhasn’t yet been started by any thread, the waiting thread might perform the task in\nentirety itself or another task that’s incomplete. You saw an example of this in listing 8.1,\nwhere the sort function repeatedly tries to sort outstanding chunks as long as the\nchunks it needs are not yet sorted.\n Rather than adding threads to ensure that all available processors are being used,\nsometimes it pays to add threads to ensure that external events are handled in a timely\nmanner to increase the responsiveness of the system.\n8.4.4\nImproving responsiveness with concurrency\nMost modern graphical user interface frameworks are event-driven; the user performs\nactions on the user interface by pressing keys or moving the mouse, which generate\na series of events or messages that the application then handles. The system may also\ngenerate messages or events on its own. In order to ensure that all events and mes-\nsages are correctly handled, the application typically has an event loop that looks\nlike this:\nwhile(true)\n{\n    event_data event=get_event();\n    if(event.type==quit)\n        break;\n    process(event);\n}\nObviously, the details of the API will vary, but the structure is generally the same: wait\nfor an event, do whatever processing is necessary to handle it, and then wait for the\nnext one. If you have a single-threaded application, this can make long-running tasks\nhard to write, as described in section 8.1.3. In order to ensure that user input is han-\ndled in a timely manner, get_event() and process() must be called with reasonable\nfrequency, whatever the application is doing. This means that either the task must\nperiodically suspend itself and return control to the event loop, or the get_event()/\nprocess() code must be called from within the code at convenient points. Either\noption complicates the implementation of the task.\n By separating the concerns with concurrency, you can put the lengthy task on a\nwhole new thread and leave a dedicated GUI thread to process the events. The threads\ncan then communicate through simple mechanisms rather than having to somehow\nmix the event-handling code in with the task code. The following listing shows a sim-\nple outline for this separation.\nstd::thread task_thread;\nstd::atomic<bool> task_cancelled(false);\nvoid gui_thread()\nListing 8.6\nSeparating GUI thread from task thread\n\n\n281\nAdditional considerations when designing for concurrency\n{\n    while(true)\n    {\n        event_data event=get_event();\n        if(event.type==quit)\n            break;\n        process(event);\n    }\n}\nvoid task()\n{\n    while(!task_complete() && !task_cancelled)\n    {\n        do_next_operation();\n    }\n    if(task_cancelled)\n    {\n        perform_cleanup();\n    }\n    else\n    {\n        post_gui_event(task_complete);\n    }\n}\nvoid process(event_data const& event)\n{\n    switch(event.type)\n    {\n    case start_task:\n        task_cancelled=false;\n        task_thread=std::thread(task);\n        break;\n    case stop_task:\n        task_cancelled=true;\n        task_thread.join();\n        break;\n    case task_complete:\n        task_thread.join();\n        display_results();\n        break;\n    default:\n        //...\n    }\n}\nBy separating the concerns in this way, the user thread is always able to respond to the\nevents in a timely fashion, even if the task takes a long time. This responsiveness is\noften key to the user experience when using an application; applications that com-\npletely lock up whenever a particular operation is being performed (whatever that\nmay be) are inconvenient to use. By providing a dedicated event-handling thread, the\nGUI can handle GUI-specific messages (such as resizing or repainting the window)\nwithout interrupting the execution of the time-consuming processing, while still pass-\ning on the relevant messages where they do affect the long-running task.\n\n\n282\nCHAPTER 8\nDesigning concurrent code\n So far in this chapter you’ve had a thorough look at the issues that need to be con-\nsidered when designing concurrent code. Taken as a whole, these can be quite over-\nwhelming, but as you get used to working with your “multithreaded programming\nhat” on, most of them will become second nature. If these considerations are new to\nyou, hopefully they’ll become clearer as you look at how they impact some concrete\nexamples of multithreaded code.\n8.5\nDesigning concurrent code in practice\nWhen designing concurrent code for a particular task, the extent to which you’ll need\nto consider each of the issues described previously will depend on the task. To demon-\nstrate how they apply, we’ll look at the implementation of parallel versions of three\nfunctions from the C++ Standard Library. This will give you a familiar basis on which\nto build, while providing a platform for looking at the issues. As a bonus, we’ll also\nhave usable implementations of the functions, which could be used to help with paral-\nlelizing a larger task.\n I’ve primarily selected these implementations to demonstrate particular tech-\nniques rather than to be state-of-the-art implementations; more advanced implemen-\ntations that make better use of the available hardware concurrency may be found in\nthe academic literature on parallel algorithms or in specialist multithreading libraries\nsuch as Intel’s Threading Building Blocks (http://threadingbuildingblocks.org/).\n Conceptually, the simplest parallel algorithm is a parallel version of std::for_\neach, so we’ll start with that.\n8.5.1\nA parallel implementation of std::for_each\nstd::for_each is simple in concept; it calls a user-supplied function on every ele-\nment in a range in turn. The big difference between a parallel implementation and\nthe sequential std::for_each is the order of the function calls. std::for_each calls\nthe function with the first element in the range, then the second, and so on, whereas\nwith a parallel implementation there’s no guarantee as to the order in which the\nelements will be processed, and they may (indeed, we hope they will) be processed\nconcurrently.\n To implement a parallel version of this, you need to divide the range into sets of ele-\nments to process on each thread. You know the number of elements in advance, so you\ncan divide the data before processing begins (section 8.1.1). We’ll assume that this is the\nonly parallel task running, so you can use std::thread::hardware_concurrency() to\ndetermine the number of threads. You also know that the elements can be processed\nentirely independently, so you can use contiguous blocks to avoid false sharing (sec-\ntion 8.2.3).\n This algorithm is similar in concept to the parallel version of std::accumulate\ndescribed in section 8.4.1, but rather than computing the sum of each element, you\nmerely have to apply the specified function. Although you might imagine this would\ngreatly simplify the code, because there’s no result to return, if you want to pass on\n",
      "page_number": 298
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 306-313)",
      "start_page": 306,
      "end_page": 313,
      "detection_method": "topic_boundary",
      "content": "283\nDesigning concurrent code in practice\nexceptions to the caller, you still need to use the std::packaged_task and std::\nfuture mechanisms to transfer the exception between threads. A sample implementa-\ntion is shown here.\ntemplate<typename Iterator,typename Func>\nvoid parallel_for_each(Iterator first,Iterator last,Func f)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\n    std::vector<std::future<void> > futures(num_threads-1);    \n    std::vector<std::thread> threads(num_threads-1);\n    join_threads joiner(threads);\n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_end=block_start;\n        std::advance(block_end,block_size);\n        std::packaged_task<void(void)> task(   \n            [=]()\n            {\n                std::for_each(block_start,block_end,f);\n            });\n        futures[i]=task.get_future();\n        threads[i]=std::thread(std::move(task));    \n        block_start=block_end;\n    }\n    std::for_each(block_start,last,f);\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        futures[i].get();    \n    }\n}\nThe basic structure of the code is identical to that of listing 8.4, which is unsurprising.\nThe key difference is that the futures vector stores std::future<void> B, because\nthe worker threads don’t return a value, and a simple lambda function that invokes\nthe function f on the range from block_start to block_end is used for the task c.\nThis avoids having to pass the range into the thread constructor d. Because the\nworker threads don’t return a value, the calls to futures[i].get() e provide a\nmeans of retrieving any exceptions thrown on the worker threads; if you don’t want to\npass on the exceptions, you could omit this.\nListing 8.7\nA parallel version of std::for_each\nb\nc\nd\ne\n\n\n284\nCHAPTER 8\nDesigning concurrent code\n Just as your parallel implementation of std::accumulate could be simplified using\nstd::async, so can your parallel_for_each. This implementation follows.\ntemplate<typename Iterator,typename Func>\nvoid parallel_for_each(Iterator first,Iterator last,Func f)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return;\n    unsigned long const min_per_thread=25;\n    if(length<(2*min_per_thread))\n    {\n        std::for_each(first,last,f);   \n    }\n    else\n    {\n        Iterator const mid_point=first+length/2;\n        std::future<void> first_half=                    \n            std::async(&parallel_for_each<Iterator,Func>,\n                       first,mid_point,f);\n        parallel_for_each(mid_point,last,f);  \n        first_half.get();                   \n    }\n}\nAs with your std::async-based parallel_accumulate from listing 8.5, you split the\ndata recursively rather than before execution, because you don’t know how many\nthreads the library will use. As before, you divide the data in half at each stage, run-\nning one half asynchronously c and the other directly d, until the remaining data is\ntoo small to be worth dividing, in which case you defer to std::for_each B. Again,\nthe use of std::async and the get() member function of std::future e provides\nthe exception propagation semantics.\n Let’s move on from algorithms that must perform the same operation on each ele-\nment (of which there are several; std::count and std::replace spring to mind, for\nstarters) to a slightly more complicated example in the shape of std::find.\n8.5.2\nA parallel implementation of std::find\nstd::find is a useful algorithm to consider next because it’s one of several algorithms\nthat can complete without every element having been processed. For example, if the\nfirst element in the range matches the search criterion, there’s no need to examine\nany other elements. As you’ll see shortly, this is an important property for performance,\nand it has direct consequences for the design of the parallel implementation. It’s a par-\nticular example of how data access patterns can affect the design of your code (section\n8.3.2). Other algorithms in this category include std::equal and std::any_of.\n If you and your partner were searching for an old photograph through the boxes\nof keepsakes in your attic, you wouldn’t let them continue searching if you found the\nListing 8.8\nA parallel version of std::for_each using std::async\nb\nc\nd\ne\n\n\n285\nDesigning concurrent code in practice\nphotograph. Instead, you’d let them know you’d found the photograph (perhaps by\nshouting, “Found it!”), so that they could stop searching and move on to something\nelse. The nature of many algorithms requires that they process every element, so they\nhave no equivalent to shouting, “Found it!” For algorithms such as std::find, the\nability to complete “early” is an important property and not something to squander.\nYou therefore need to design your code to make use of it—to interrupt the other tasks\nin some way when the answer is known, so that the code doesn’t have to wait for the\nother worker threads to process the remaining elements.\n If you don’t interrupt the other threads, the serial version may outperform your\nparallel implementation, because the serial algorithm can stop searching and return\nonce a match is found. If, for example, the system can support four concurrent\nthreads, each thread will have to examine one quarter of the elements in the range,\nand your naive parallel implementation would take approximately one quarter of the\ntime a single thread would take to check every element. If the matching element lies\nin the first quarter of the range, the sequential algorithm will return first, because it\ndoesn’t need to check the remainder of the elements.\n One way in which you can interrupt the other threads is by making use of an\natomic variable as a flag and checking the flag after processing every element. If the\nflag is set, one of the other threads has found a match, so you can cease processing\nand return. By interrupting the threads in this way, you preserve the property that you\ndon’t have to process every value and improve the performance compared to the\nserial version in more circumstances. The downside to this is that atomic loads can be\nslow operations, so this can impede the progress of each thread.\n Now you have two choices as to how to return the values and how to propagate any\nexceptions. You can use an array of futures, std::packaged_task, for transferring the\nvalues and exceptions, and then process the results back in the main thread; or you\ncan use std::promise to set the final result directly from the worker threads. It all\ndepends on how you want to handle exceptions from the worker threads. If you want\nto stop on the first exception (even if you haven’t processed all elements), you can use\nstd::promise to set both the value and the exception. On the other hand, if you want\nto allow the other workers to keep searching, you can use std::packaged_task, store\nall the exceptions, and then rethrow one of them if a match isn’t found.\n In this case I’ve opted to use std::promise because the behavior matches that of\nstd::find more closely. One thing to watch out for here is the case where the ele-\nment being searched for isn’t in the supplied range. You therefore need to wait for all\nthe threads to finish before getting the result from the future. If you block on the\nfuture, you’ll be waiting forever if the value isn’t there. The result is shown here.\ntemplate<typename Iterator,typename MatchType>\nIterator parallel_find(Iterator first,Iterator last,MatchType match)\n{\n    struct find_element    \nListing 8.9\nAn implementation of a parallel find algorithm\nb\n\n\n286\nCHAPTER 8\nDesigning concurrent code\n    {\n        void operator()(Iterator begin,Iterator end,\n                        MatchType match,\n                        std::promise<Iterator>* result,\n                        std::atomic<bool>* done_flag)\n        {\n            try\n            {\n                for(;(begin!=end) && !done_flag->load();++begin)    \n                {\n                    if(*begin==match)\n                    {\n                        result->set_value(begin);    \n                        done_flag->store(true);   \n                        return;\n                    }\n                }\n            }\n            catch(...)   \n            {\n                try\n                {\n                    result->set_exception(std::current_exception());   \n                    done_flag->store(true);\n                }\n                catch(...)   \n                {}\n            }\n        }\n    };\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return last;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\n    std::promise<Iterator> result;     \n    std::atomic<bool> done_flag(false);             \n    std::vector<std::thread> threads(num_threads-1);\n    {        \n        join_threads joiner(threads);\n        Iterator block_start=first;\n        for(unsigned long i=0;i<(num_threads-1);++i)\n        {\n            Iterator block_end=block_start;\n            std::advance(block_end,block_size);\n            threads[i]=std::thread(find_element(),    \n                                   block_start,block_end,match,\n                                   &result,&done_flag);\n            block_start=block_end;\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n\n\n287\nDesigning concurrent code in practice\n        }\n        find_element()(block_start,last,match,&result,&done_flag);   \n    }\n    if(!done_flag.load())    \n    {\n        return last;\n    }\n    return result.get_future().get();    \n}\nThe main body of listing 8.9 is similar to the previous examples. This time, the work is\ndone in the function call operator of the local find_element class B. This loops\nthrough the elements in the block it’s been given, checking the flag at each step c. If\na match is found, it sets the final result value in the promise d, and then sets the\ndone_flag e before returning.\n If an exception is thrown, this is caught by the catchall handler f, and you try to\nstore the exception in the promise g before setting the done_flag. Setting the value\non the promise might throw an exception if the promise is already set, so you catch\nand discard any exceptions that happen here h.\n This means that if a thread calling find_element either finds a match or throws an\nexception, all other threads will see done_flag set and will stop. If multiple threads\nfind a match or throw at the same time, they’ll race to set the result in the promise.\nBut this is a benign race condition; whichever succeeds is nominally “first” and there-\nfore an acceptable result.\n Back in the main parallel_find function itself, you have the promise i and flag\nj used to stop the search, both of which are passed in to the new threads along with\nthe range to search 1!. The main thread also uses find_element to search the remain-\ning elements 1@. As already mentioned, you need to wait for all threads to finish\nbefore you check the result, because there might not be any matching elements. You\ndo this by enclosing the thread launching-and-joining code in a block 1) so all threads\nare joined when you check the flag to see whether a match was found 1#. If a match\nwas found, you can get the result or throw the stored exception by calling get() on\nthe std::future<Iterator> you can get from the promise 1$.\n Again, this implementation assumes that you’re going to be using all available\nhardware threads or that you have some other mechanism to determine the number\nof threads to use for the upfront division of work between threads. As before, you can\nuse std::async and recursive data division to simplify your implementation, while\nusing the automatic scaling facility of the C++ Standard Library. An implementation\nof parallel_find using std::async is shown in the following listing.\ntemplate<typename Iterator,typename MatchType>   \nIterator parallel_find_impl(Iterator first,Iterator last,MatchType match,\n                            std::atomic<bool>& done)\n{\nListing 8.10\nAn implementation of a parallel find algorithm using std::async\n1@\n1#\n1$\nb\n\n\n288\nCHAPTER 8\nDesigning concurrent code\n    try\n    {\n        unsigned long const length=std::distance(first,last);\n        unsigned long const min_per_thread=25;   \n        if(length<(2*min_per_thread))        \n        {\n            for(;(first!=last) && !done.load();++first)    \n            {\n                if(*first==match)\n                {\n                    done=true;    \n                    return first;\n                }\n            }\n            return last;   \n        }\n        else\n        {\n            Iterator const mid_point=first+(length/2);   \n            std::future<Iterator> async_result=\n                std::async(&parallel_find_impl<Iterator,MatchType>,   \n                           mid_point,last,match,std::ref(done));\n            Iterator const direct_result=\n                    parallel_find_impl(first,mid_point,match,done);   \n            return (direct_result==mid_point)?\n                async_result.get():direct_result;   \n        }\n    }\n    catch(...)\n    {\n        done=true;   \n        throw;\n    }\n}\ntemplate<typename Iterator,typename MatchType>\nIterator parallel_find(Iterator first,Iterator last,MatchType match)\n{\n    std::atomic<bool> done(false);\n    return parallel_find_impl(first,last,match,done);   \n}\nThe desire to finish early if you find a match means that you need to introduce a flag\nthat is shared between all threads to indicate that a match has been found. This there-\nfore needs to be passed in to all recursive calls. The simplest way to achieve this is by\ndelegating to an implementation function B, which takes an additional parameter—\na reference to the done flag, which is passed in from the main entry point 1@.\n The core implementation then proceeds along familiar lines. In common with\nmany of the implementations here, you set a minimum number of items to process on\na single thread c; if you can’t cleanly divide into two halves of at least that size, you\nrun everything on the current thread d. The algorithm is a simple loop through\nthe specified range, looping until you reach the end of the range or the done flag is\nset e. If you do find a match, the done flag is set before returning f. If you stop\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n1@\n\n\n289\nDesigning concurrent code in practice\nsearching either because you got to the end of the list, or because another thread set\nthe done flag, you return last to indicate that no match was found here g.\n If the range can be divided, you first find the midpoint h before using\nstd::async to run the search in the second half of the range i, being careful to use\nstd::ref to pass a reference to the done flag. In the meantime, you can search in the\nfirst half of the range by doing a direct recursive call j. Both the asynchronous call\nand the direct recursion may result in further subdivisions if the original range is\nbig enough.\n If the direct search returned mid_point, then it failed to find a match, so you need\nto get the result of the asynchronous search. If no result was found in that half, the\nresult will be last, which is the correct return value to indicate that the value was not\nfound 1). If the “asynchronous” call was deferred rather than truly asynchronous, it\nwill run here in the call to get(); in these circumstances, the search of the top half of\nthe range is skipped if the search in the bottom half was successful. If the asynchro-\nnous search is running on another thread, the destructor of the async_result vari-\nable will wait for the thread to complete, so you don’t have any leaking threads.\n As before, the use of std::async provides you with exception safety and exception-\npropagation features. If the direct recursion throws an exception, the future’s\ndestructor will ensure that the thread running the asynchronous call has terminated\nbefore the function returns, and if the asynchronous call throws, the exception is\npropagated through the get() call 1). The use of a try/catch block around the\nwhole thing is only there to set the done flag on an exception and ensure that all\nthreads terminate quickly if an exception is thrown 1!. The implementation would\nstill be correct without it but would keep checking elements until every thread\nwas finished.\n A key feature that both implementations of this algorithm share with the other\nparallel algorithms you’ve seen is that there’s no longer the guarantee that items are\nprocessed in the sequence that you get from std::find. This is essential if you’re\ngoing to parallelize the algorithm. You can’t process elements concurrently if the order\nmatters. If the elements are independent, it doesn’t matter for things like parallel\n_for_each, but it means that your parallel_find might return an element toward\nthe end of the range even when there’s a match toward the beginning, which might\nbe surprising if you’re not expecting it.\n OK, so you’ve managed to parallelize std::find. As I stated at the beginning of\nthis section, there are other similar algorithms that can complete without processing\nevery data element, and the same techniques can be used for those. We’ll also look\nfurther at the issue of interrupting threads in chapter 9.\n To complete our trio of examples, we’ll go in a different direction and look at\nstd::partial_sum. This algorithm doesn’t get a lot of press, but it’s an interesting\nalgorithm to parallelize and highlights some additional design choices.\n\n\n290\nCHAPTER 8\nDesigning concurrent code\n8.5.3\nA parallel implementation of std::partial_sum\nstd::partial_sum calculates the running totals in a range, so each element is replaced\nby the sum of that element and all the elements prior to it in the original sequence.\nThus the sequence 1, 2, 3, 4, 5 becomes 1, (1+2)=3, (1+2+3)=6, (1+2+3+4)=10,\n(1+2+3+4+5)=15. This is interesting to parallelize because you can’t just divide the\nrange into chunks and calculate each chunk independently. For example, the initial\nvalue of the first element needs to be added to every other element.\n One approach to determining the partial sum of a range is to calculate the partial\nsum of individual chunks and then add the resulting value of the last element in the\nfirst chunk onto the elements in the next chunk, and so forth. If you have the ele-\nments 1, 2, 3, 4, 5, 6, 7, 8, 9 and you’re splitting into three chunks, you get {1, 3, 6},\n{4, 9, 15}, {7, 15, 24} in the first instance. If you then add 6 (the sum for the last element\nin the first chunk) onto the elements in the second chunk, you get {1, 3, 6}, {10, 15, 21},\n{7, 15, 24}. Then you add the last element of the second chunk (21) onto the elements\nin the third and final chunk to get the final result: {1, 3, 6}, {10, 15, 21}, {28, 36, 55}.\n As well as the original division into chunks, the addition of the partial sum from\nthe previous block can also be parallelized. If the last element of each block is\nupdated first, the remaining elements in a block can be updated by one thread while a\nsecond thread updates the next block, and so forth. This works well when there are\nmany more elements in the list than processing cores, because each core has a reason-\nable number of elements to process at each stage.\n If you have a lot of processing cores (as many or more than the number of ele-\nments), this doesn’t work so well. If you divide the work among the processors, you\nend up working in pairs of elements at the first step. Under these conditions, this for-\nward propagation of results means that many processors are left waiting, so you need\nto find some work for them to do. You can then take a different approach to the prob-\nlem. Rather than doing the full forward propagation of the sums from one chunk to\nthe next, you do a partial propagation: first sum adjacent elements as before, but then\nadd those sums to those two elements away, then add the next set of results to the\nresults from four elements away, and so forth. If you start with the same initial nine\nelements, you get 1, 3, 5, 7, 9, 11, 13, 15, 17 after the first round, which gives you the\nfinal results for the first two elements. After the second you then have 1, 3, 6, 10, 14,\n18, 22, 26, 30, which is correct for the first four elements. After round three you have\n1, 3, 6, 10, 15, 21, 28, 36, 44, which is correct for the first eight elements, and finally\nafter round four you have 1, 3, 6, 10, 15, 21, 28, 36, 45, which is the final answer.\nAlthough there are more total steps than in the first approach, there’s greater scope\nfor parallelism if you have many processors; each processor can update one entry with\neach step.\n Overall, the second approach takes log2(N) steps of approximately N operations\n(one per processor), where N is the number of elements in the list. This compares to\nthe first algorithm where each thread has to perform N/k operations for the initial\npartial sum of the chunk allocated to it and then further N/k operations to do the\n",
      "page_number": 306
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 314-321)",
      "start_page": 314,
      "end_page": 321,
      "detection_method": "topic_boundary",
      "content": "291\nDesigning concurrent code in practice\nforward propagation, where k is the number of threads. Thus the first approach is\nO(N), whereas the second is O(N log(N)) in terms of the total number of operations.\nBut if you have as many processors as list elements, the second approach requires only\nlog(N) operations per processor, whereas the first serializes the operations when k gets\nlarge, because of the forward propagation. For small numbers of processing units, the\nfirst approach will therefore finish faster, whereas for massively parallel systems, the sec-\nond will finish faster. This is an extreme example of the issues discussed in section 8.2.1.\n Anyway, efficiency issues aside, let’s look at some code. The following listing shows\nthe first approach.\ntemplate<typename Iterator>\nvoid parallel_partial_sum(Iterator first,Iterator last)\n{\n    typedef typename Iterator::value_type value_type;\n    \n    struct process_chunk     \n    {\n        void operator()(Iterator begin,Iterator last,\n                        std::future<value_type>* previous_end_value,\n                        std::promise<value_type>* end_value)\n        {\n            try\n            {\n                Iterator end=last;\n                ++end;\n                std::partial_sum(begin,end,begin);    \n                if(previous_end_value)    \n                {\n                    value_type& addend=previous_end_value->get();    \n                    *last+=addend;     \n                    if(end_value)\n                    {\n                        end_value->set_value(*last);   \n                    }\n                    std::for_each(begin,last,[addend](value_type& item) \n                                  {\n                                      item+=addend;\n                                  });\n                }\n                else if(end_value)\n                {\n                    end_value->set_value(*last);   \n                }\n            }\n            catch(...)    \n            {\n                if(end_value)\n                {\n                    end_value->set_exception(std::current_exception()); \n                }\nListing 8.11\nCalculating partial sums in parallel by dividing the problem\nb\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n\n\n292\nCHAPTER 8\nDesigning concurrent code\n                else\n                {\n                    throw;    \n                }\n            }\n        }\n    };\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return;\n    unsigned long const min_per_thread=25;    \n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\n    typedef typename Iterator::value_type value_type;\n    std::vector<std::thread> threads(num_threads-1);    \n    std::vector<std::promise<value_type> > \n         end_values(num_threads-1);          \n    std::vector<std::future<value_type> > \n         previous_end_values;      \n    previous_end_values.reserve(num_threads-1);    \n    join_threads joiner(threads);\n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_last=block_start;\n        std::advance(block_last,block_size-1);    \n        threads[i]=std::thread(process_chunk(),         \n                               block_start,block_last,\n                               (i!=0)?&previous_end_values[i-1]:0,\n                               &end_values[i]);\n        block_start=block_last;\n        ++block_start;      \n        previous_end_values.push_back(end_values[i].get_future());   \n    }\n    Iterator final_element=block_start;\n    std::advance(final_element,std::distance(block_start,last)-1);   \n    process_chunk()(block_start,final_element,                  \n                    (num_threads>1)?&previous_end_values.back():0,\n                    0);\n}\nIn this instance, the general structure is the same as with the previous algorithms,\ndividing the problem into chunks, with a minimum chunk size per thread 1@. In this\ncase, as well as the vector of threads 1#, you have a vector of promises 1$, which is used\nto store the value of the last element in the chunk, and a vector of futures 1%, which is\nused to retrieve the last value from the previous chunk. You can reserve the space for\nthe futures 1^ to avoid a reallocation while spawning threads, because you know how\nmany you’re going to have.\n1!\n1@\n1#\n1$\n1%\n1^\n1&\n1*\n1(\n2)\n2!\n2@\n\n\n293\nDesigning concurrent code in practice\n The main loop is the same as before, except this time you want the iterator that\npoints to the last element in each block, rather than being the usual one past the end\n1&, so that you can do the forward propagation of the last element in each range. The\nprocessing is done in the process_chunk function object, which we’ll look at shortly;\nthe start and end iterators for this chunk are passed in as arguments alongside the\nfuture for the end value of the previous range (if any) and the promise to hold the end\nvalue of this range 1*.\n After you’ve spawned the thread, you can update the block start, remembering to\nadvance it past that last element 1(, and store the future for the last value in the cur-\nrent chunk into the vector of futures so it will be picked up next time around the\nloop 2).\n Before you process the final chunk, you need to get an iterator for the last element\n2!, which you can pass in to process_chunk 2@. std::partial_sum doesn’t return a\nvalue, so you don’t need to do anything once the final chunk has been processed. The\noperation is complete once all the threads have finished.\n OK, now it’s time to look at the process_chunk function object that does all the\nwork B. You start by calling std::partial_sum for the entire chunk, including the\nfinal element c, but then you need to know if you’re the first chunk or not d. If\nyou are not the first chunk, then there was a previous_end_value from the previ-\nous chunk, so you need to wait for that e. In order to maximize the parallelism of\nthe algorithm, you then update the last element first f, so you can pass the value\non to the next chunk (if there is one) g. Once you’ve done that, you can use\nstd::for_each and a simple lambda function h, to update all the remaining ele-\nments in the range.\n If there was not a previous_end_value, you’re the first chunk, so you can update\nthe end_value for the next chunk (again, if there is one—you might be the only\nchunk) i.\n Finally, if any of the operations threw an exception, you catch it j and store it in\nthe promise 1) so it will propagate to the next chunk when it tries to get the previous\nend value e. This will propagate all exceptions into the final chunk, which then\nrethrows 1!, because you know you’re running on the main thread.\n Because of the synchronization between the threads, this code isn’t readily amena-\nble to rewriting with std::async. The tasks wait on results made available partway\nthrough the execution of other tasks, so all tasks must be running concurrently.\n With the block-based, forward-propagation approach out of the way, let’s look at\nthe second approach to computing the partial sums of a range.\nIMPLEMENTING THE INCREMENTAL PAIRWISE ALGORITHM FOR PARTIAL SUMS\nThis second approach to calculating the partial sums by adding elements increasingly\nfurther away works best where your processors can execute the additions in lockstep.\nIn this case, no further synchronization is necessary because all the intermediate\nresults can be propagated directly to the next processor that needs them. But in prac-\ntice, you rarely have these systems to work with, except for those cases where a single\n\n\n294\nCHAPTER 8\nDesigning concurrent code\nprocessor can execute the same instruction across a small number of data elements\nsimultaneously with so-called Single-Instruction/Multiple-Data (SIMD) instructions.\nTherefore, you must design your code for the general case and explicitly synchronize\nthe threads at each step.\n One way to do this is to use a barrier—a synchronization mechanism that causes\nthreads to wait until the required number of threads has reached the barrier. Once all\nthe threads have reached the barrier, they’re all unblocked and may proceed. The\nC++11 Thread Library doesn’t offer this facility directly, so you have to design one\nyourself.\n Imagine a roller coaster at the fairground. If there’s a reasonable number of peo-\nple waiting, the fairground staff will ensure that every seat is filled before the roller\ncoaster leaves the platform. A barrier works the same way: you specify up front the\nnumber of “seats,” and threads have to wait until all the “seats” are filled. Once there\nare enough waiting threads, they can all proceed; the barrier is reset and starts waiting\nfor the next batch of threads. Often, this construct is used in a loop, where the same\nthreads come around and wait until next time. The idea is to keep the threads in lock-\nstep, so one thread doesn’t run away in front of the others and get out of step. For an\nalgorithm such as this one, that would be disastrous, because the runaway thread\nwould potentially modify data that was still being used by other threads or use data\nthat hadn’t been correctly updated yet.\n The following listing shows a simple implementation of a barrier.\nclass barrier\n{\n    unsigned const count;\n    std::atomic<unsigned> spaces;\n    std::atomic<unsigned> generation;\npublic:\n    explicit barrier(unsigned count_):         \n        count(count_),spaces(count),generation(0)\n    {}\n    void wait()\n    {\n        unsigned const my_generation=generation;   \n        if(!--spaces)                \n        {\n            spaces=count;      \n            ++generation;   \n        }\n        else\n        {\n            while(generation==my_generation)    \n                std::this_thread::yield();    \n        }\n    }\n};\nListing 8.12\nA simple barrier class\nb\nc\nd\ne\nf\ng\nh\n\n\n295\nDesigning concurrent code in practice\nWith this implementation, you construct a barrier with the number of “seats” B,\nwhich is stored in the count variable. Initially, the number of spaces at the barrier is\nequal to this count. As each thread waits, the number of spaces is decremented d.\nWhen it reaches zero, the number of spaces is reset back to count e, and the\ngeneration is increased to signal to the other threads that they can continue f. If\nthe number of free spaces does not reach zero, you have to wait. This implementa-\ntion uses a simple spin lock g, checking the generation against the value you retrieved\nat the beginning of wait() c. Because the generation is only updated when all the\nthreads have reached the barrier f, you yield() while waiting h, so the waiting\nthread doesn’t hog the CPU in a busy wait.\n When I said this implementation was simple, I meant it: it uses a spin wait, so it’s\nnot ideal for cases where threads are likely to be waiting a long time, and it doesn’t\nwork if there’s more than count threads that can potentially call wait() at any one\ntime. If you need to handle either of those scenarios, you must use a more robust (but\nmore complex) implementation instead. I’ve also stuck to sequentially consistent\noperations on the atomic variables, because that makes everything easier to reason\nabout, but you could potentially relax some of the ordering constraints. This global\nsynchronization is expensive on massively parallel architectures, because the cache\nline holding the barrier state must be shuttled between all the processors involved\n(see the discussion of cache ping-pong in section 8.2.2), so you must take great care to\nensure that this is the best choice here. If your C++ Standard Library provides the\nfacilities from the Concurrency TS, you could use std::experimental::barrier\nhere. See chapter 4 for details.\n This is what you need here; you have a fixed number of threads that need to run in\na lockstep loop. Well, it’s almost a fixed number of threads. As you may remember, the\nitems at the beginning of the list acquire their final values after a couple of steps. This\nmeans that either you have to keep those threads looping until the entire range has\nbeen processed, or you need to allow your barrier to handle threads dropping out and\ndecreasing count. I opted for the latter option because it avoids having threads doing\nunnecessary work, looping until the final step is done.\n This means you have to change count to be an atomic variable, so you can update\nit from multiple threads without external synchronization:\nstd::atomic<unsigned> count;\nThe initialization remains the same, but now you have to explicitly load() from count\nwhen you reset the number of spaces:\nspaces=count.load();\nThese are all the changes that you need on the wait() front; now you need a new\nmember function to decrement count. Let’s call it done_waiting(), because a thread\nis declaring that it is done with waiting:\n\n\n296\nCHAPTER 8\nDesigning concurrent code\nvoid done_waiting()\n{\n    --count;       \n    if(!--spaces)    \n    {\n        spaces=count.load();    \n        ++generation;\n    }\n}\nThe first thing you do is decrement the count B so that the next time spaces is reset\nit reflects the new lower number of waiting threads. Then you need to decrease the\nnumber of free spaces c. If you don’t do this, the other threads will be waiting for-\never, because spaces was initialized to the old, larger value. If you’re the last thread\nthrough on this batch, you need to reset the counter and increase the generation d,\nas you do in wait(). The key difference here is that if you’re the last thread in the\nbatch, you don’t have to wait. \n You’re now ready to write your second implementation of partial sum. At each\nstep, every thread calls wait() on the barrier to ensure the threads step through\ntogether, and once each thread is done, it calls done_waiting() on the barrier to dec-\nrement the count. If you use a second buffer alongside the original range, the barrier\nprovides all the synchronization you need. At each step, the threads read from either\nthe original range or the buffer and write the new value to the corresponding element\nof the other. If the threads read from the original range on one step, they read from\nthe buffer on the next, and vice versa. This ensures there are no race conditions\nbetween the reads and writes by separate threads. Once a thread has finished looping,\nit must ensure that the correct final value has been written to the original range. The\nfollowing listing pulls this all together.\nstruct barrier\n{\n    std::atomic<unsigned> count;\n    std::atomic<unsigned> spaces;\n    std::atomic<unsigned> generation;\n    barrier(unsigned count_):\n        count(count_),spaces(count_),generation(0)\n    {}\n    void wait()\n    {\n        unsigned const gen=generation.load();\n        if(!--spaces)\n        {\n            spaces=count.load();\n            ++generation;\n        }\n        else\n        {\nListing 8.13\nA parallel implementation of partial_sum by pairwise updates\nb\nc\nd\n\n\n297\nDesigning concurrent code in practice\n            while(generation.load()==gen)\n            {\n                std::this_thread::yield();\n            }\n        }\n    }\n    void done_waiting()\n    {\n        --count;\n        if(!--spaces)\n        {\n            spaces=count.load();\n            ++generation;\n        }\n    }\n};\ntemplate<typename Iterator>\nvoid parallel_partial_sum(Iterator first,Iterator last)\n{\n    typedef typename Iterator::value_type value_type;\n    struct process_element       \n    {\n        void operator()(Iterator first,Iterator last,\n                        std::vector<value_type>& buffer,\n                        unsigned i,barrier& b)\n        {\n            value_type& ith_element=*(first+i);\n            bool update_source=false;\n            \n            for(unsigned step=0,stride=1;stride<=i;++step,stride*=2)\n            {\n                value_type const& source=(step%2)?    \n                    buffer[i]:ith_element;\n                value_type& dest=(step%2)?\n                    ith_element:buffer[i];\n                value_type const& addend=(step%2)?     \n                    buffer[i-stride]:*(first+i-stride);\n                dest=source+addend;     \n                update_source=!(step%2);\n                b.wait();              \n            }\n            if(update_source)    \n            {\n                ith_element=buffer[i];\n            }\n            b.done_waiting();   \n        }\n    };\n    unsigned long const length=std::distance(first,last);\n    if(length<=1)\n        return;\n    std::vector<value_type> buffer(length);\n    barrier b(length);\n    std::vector<std::thread> threads(length-1);    \n    join_threads joiner(threads);\nb\nc\nd\ne\nf\ng\nh\ni\n\n\n298\nCHAPTER 8\nDesigning concurrent code\n    Iterator block_start=first;\n    for(unsigned long i=0;i<(length-1);++i)\n    {\n        threads[i]=std::thread(process_element(),first,last,   \n                               std::ref(buffer),i,std::ref(b));\n    }\n    process_element()(first,last,buffer,length-1,b);   \n}\nThe overall structure of this code is probably becoming familiar by now. You have a\nclass with a function call operator (process_element) for doing the work B, which\nyou run on a bunch of threads j stored in a vector i, and which you also call from\nthe main thread 1). The key difference this time is that the number of threads is\ndependent on the number of items in the list rather than on std::thread::hardware\n_concurrency. As I said already, unless you’re on a massively parallel machine where\nthreads are cheap, this is probably a bad idea, but it shows the overall structure. It\nwould be possible to have fewer threads, with each thread handling several values\nfrom the source range, but there will come a point where there are sufficiently few\nthreads that this is less efficient than the forward-propagation algorithm.\n The key work is done in the function call operator of process_element. At each\nstep, you either take the ith element from the original range or the ith element from\nthe buffer c and add it to the value stride elements prior d, storing it in the buffer\nif you started in the original range or back in the original range if you started in the\nbuffer e. You then wait on the barrier f before starting the next step. You’ve fin-\nished when the stride takes you off the start of the range, in which case you need to\nupdate the element in the original range if your final result was stored in the buffer g.\nFinally, you tell the barrier that you’re done_waiting() h.\n Note that this solution isn’t exception-safe. If an exception is thrown in process-\n_element on one of the worker threads, it will terminate the application. You could\ndeal with this by using std::promise to store the exception, as you did for the\nparallel_find implementation from listing 8.9, or even using std::exception_ptr\nprotected by a mutex.\n That concludes our three examples. Hopefully, they’ve helped to crystallize some\nof the design considerations highlighted in sections 8.1, 8.2, 8.3, and 8.4, and have\ndemonstrated how these techniques can be brought to bear in real code.\nSummary\nWe’ve covered quite a lot of ground in this chapter. We started with various tech-\nniques for dividing work between threads, such as dividing the data beforehand or\nusing a number of threads to form a pipeline. We then looked at the issues sur-\nrounding the performance of multithreaded code from a low-level perspective, with\na look at false sharing and data contention before moving on to how the patterns of\ndata access can affect the performance of a bit of code. We then looked at addi-\ntional considerations in the design of concurrent code, such as exception safety and\nj\n1)\n",
      "page_number": 314
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 322-329)",
      "start_page": 322,
      "end_page": 329,
      "detection_method": "topic_boundary",
      "content": "299\nSummary\nscalability. Finally, we ended with a number of examples of parallel algorithm imple-\nmentations, each of which highlighted particular issues that can occur when design-\ning multithreaded code.\n One item that has cropped up a couple of times in this chapter is the idea of a\nthread pool—a preconfigured group of threads that run tasks assigned to the pool.\nQuite a lot of thought goes into the design of a good thread pool, so we’ll look at\nsome of the issues in the next chapter, along with other aspects of advanced thread\nmanagement.\n\n\n300\nAdvanced thread\nmanagement\nIn earlier chapters, you’ve been explicitly managing threads by creating std::thread\nobjects for every thread. In a couple of places you’ve seen how this can be undesir-\nable, because you then have to manage the lifetime of the thread objects, deter-\nmine the number of threads appropriate to the problem and to the current\nhardware, and so forth. The ideal scenario would be that you could divide the code\ninto the smallest pieces that could be executed concurrently, pass them over to the\ncompiler and library, and say, “Parallelize this for optimal performance.” As we'll\nsee in chapter 10, there are cases where you can do this: if your code that requires\nparallelization can be expressed as a call to a standard library algorithm, then you\ncan ask the library to do the parallelization for you in most cases.\n Another recurring theme in several of the examples is that you might use sev-\neral threads to solve a problem but require that they finish early if some condition\nis met. This might be because the result has already been determined, or because\nThis chapter covers\nThread pools\nHandling dependencies between pool tasks\nWork stealing for pool threads\nInterrupting threads\n\n\n301\nThread pools\nan error has occurred, or even because the user has explicitly requested that the oper-\nation be aborted. Whatever the reason, the threads need to be sent a “Please stop”\nrequest so that they can give up on the task they were given, tidy up, and finish as soon\nas possible.\n In this chapter, we’ll look at mechanisms for managing threads and tasks, starting\nwith the automatic management of the number of threads and the division of tasks\nbetween them.\n9.1\nThread pools\nIn many companies, employees who would normally spend their time in the office are\noccasionally required to visit clients or suppliers or to attend a trade show or confer-\nence. Although these trips might be necessary, and on any given day there might be\nseveral people making this trip, it may well be months or even years between these\ntrips for any particular employee. Because it would therefore be rather expensive and\nimpractical for each employee to have a company car, companies often offer a car pool\ninstead; they have a limited number of cars that are available to all employees. When\nan employee needs to make an off-site trip, they book one of the pool cars for the\nappropriate time and return it for others to use when they return to the office. If\nthere are no pool cars free on a given day, the employee will have to reschedule their\ntrip for a subsequent date.\n A thread pool is a similar idea, except that threads are being shared rather than\ncars. On most systems, it’s impractical to have a separate thread for every task that\ncan potentially be done in parallel with other tasks, but you’d still like to take advan-\ntage of the available concurrency where possible. A thread pool allows you to accom-\nplish this; tasks that can be executed concurrently are submitted to the pool, which\nputs them on a queue of pending work. Each task is then taken from the queue by\none of the worker threads, which executes the task before looping back to take another\nfrom the queue.\n There are several key design issues when building a thread pool, such as how many\nthreads to use, the most efficient way to allocate tasks to threads, and whether or not\nyou can wait for a task to complete. In this section we’ll look at some thread pool\nimplementations that address these design issues, starting with the simplest possible\nthread pool.\n9.1.1\nThe simplest possible thread pool\nAt its simplest, a thread pool is a fixed number of worker threads (typically the same\nnumber as the value returned by std::thread::hardware_concurrency()) that pro-\ncess work. When you have work to do, you call a function to put it on the queue of\npending work. Each worker thread takes work off the queue, runs the specified task,\nand then goes back to the queue for more work. In the simplest case there’s no way to\nwait for the task to complete. If you need to do this, you have to manage the synchro-\nnization yourself.\n\n\n302\nCHAPTER 9\nAdvanced thread management\n The following listing shows a sample implementation of this thread pool.\nclass thread_pool\n{\n    std::atomic_bool done;\n    threadsafe_queue<std::function<void()> > work_queue;     \n    std::vector<std::thread> threads;             \n    join_threads joiner;            \n    void worker_thread()\n    {\n        while(!done)     \n        {\n            std::function<void()> task;\n            if(work_queue.try_pop(task))    \n            {\n                task();    \n            }\n            else\n            {\n                std::this_thread::yield();    \n            }\n        }\n    }\npublic:\n    thread_pool():\n        done(false),joiner(threads)\n    {\n        unsigned const thread_count=std::thread::hardware_concurrency(); \n        try\n        {\n            for(unsigned i=0;i<thread_count;++i)\n            {\n                threads.push_back(\n                    std::thread(&thread_pool::worker_thread,this));   \n            }\n        }\n        catch(...)\n        {\n            done=true;    \n            throw;\n        }\n    }\n    ~thread_pool()\n    {\n        done=true;   \n    }\n    template<typename FunctionType>\n    void submit(FunctionType f)\n    {\n        work_queue.push(std::function<void()>(f));    \n    }\n};\nListing 9.1\nSimple thread pool\nb\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n1@\n\n\n303\nThread pools\nThis implementation has a vector of worker threads c and uses one of the thread-safe\nqueues from chapter 6 B to manage the queue of work. In this case, users can’t wait for\nthe tasks, and they can’t return any values, so you can use std::function<void()> to\nencapsulate your tasks. The submit() function then wraps whatever function or call-\nable object is supplied inside an std::function<void()> instance and pushes it on\nthe queue 1@.\n The threads are started in the constructor: you use std::thread::hardware_\nconcurrency() to tell you how many concurrent threads the hardware can support\ni, and you create that many threads running your worker_thread() member func-\ntion j.\n Starting a thread can fail by throwing an exception, so you need to ensure that any\nthreads you’ve already started are stopped and cleaned up nicely in this case. This is\nachieved with a try-catch block that sets the done flag when an exception is thrown\n1), alongside an instance of the join_threads class from chapter 8 d to join all the\nthreads. This also works with the destructor: you can set the done flag 1!, and the\njoin_threads instance will ensure that all the threads have completed before the\npool is destroyed. Note that the order of declaration of the members is important:\nboth the done flag and the worker_queue must be declared before the threads vector,\nwhich must in turn be declared before the joiner. This ensures that the members are\ndestroyed in the right order; you can’t destroy the queue safely until all the threads\nhave stopped, for example.\n The worker_thread function itself is quite simple: it sits in a loop waiting until the\ndone flag is set e, pulling tasks off the queue f and executing them g in the mean-\ntime. If there are no tasks on the queue, the function calls std::this_thread::\nyield() to take a small break h and give another thread a chance to put some work\non the queue before it tries to take some off again the next time around.\n For many purposes this simple thread pool will suffice, especially if the tasks are\nentirely independent and don’t return any values or perform any blocking opera-\ntions. But there are also many circumstances where this simple thread pool may not\nadequately address your needs, and yet others where it can cause problems such as\ndeadlock. Also, in simple cases you may be better served using std::async as in\nmany of the examples in chapter 8. Throughout this chapter, we’ll look at more com-\nplex thread pool implementations that have additional features either to address\nuser needs or reduce the potential for problems. First up: waiting for the tasks we’ve\nsubmitted.\n9.1.2\nWaiting for tasks submitted to a thread pool\nIn the examples in chapter 8 that explicitly spawned threads, after dividing the work\nbetween threads, the master thread always waited for the newly spawned threads to\nfinish, to ensure that the overall task was complete before returning to the caller. With\nthread pools, you’d need to wait for the tasks submitted to the thread pool to com-\nplete, rather than the worker threads themselves. This is similar to the way that the\n\n\n304\nCHAPTER 9\nAdvanced thread management\nstd::async-based examples in chapter 8 waited for the futures. With the simple\nthread pool from listing 9.1, you’d have to do this manually using the techniques from\nchapter 4: condition variables and futures. This adds complexity to the code; it would\nbe better if you could wait for the tasks directly.\n By moving that complexity into the thread pool itself, you can wait for the tasks\ndirectly. You can have the submit() function return a task handle of some description\nthat you can then use to wait for the task to complete. This task handle would wrap the\nuse of condition variables or futures, simplifying the code that uses the thread pool.\n A special case of having to wait for the spawned task to finish occurs when the main\nthread needs a result computed by the task. You’ve seen this in examples throughout\nthe book, such as the parallel_accumulate() function from chapter 2. In this case,\nyou can combine the waiting with the result transfer through the use of futures. List-\ning 9.2 shows the changes required to the simple thread pool that allow you to wait\nfor tasks to complete and then pass return values from the task to the waiting\nthread. Because std::packaged_task<> instances are not copyable, just movable, you\ncan no longer use std::function<> for the queue entries, because std::function<>\nrequires that the stored function objects are copy-constructible. Instead, you must\nuse a custom function wrapper that can handle move-only types. This is a simple\ntype-erasure class with a function call operator. You only need to handle functions\nthat take no parameters and return void, so this is a straightforward virtual call in\nthe implementation.\nclass function_wrapper\n{\n    struct impl_base {\n        virtual void call()=0;\n        virtual ~impl_base() {}\n    };\n    std::unique_ptr<impl_base> impl;\n    template<typename F>\n    struct impl_type: impl_base\n    {\n        F f;\n        impl_type(F&& f_): f(std::move(f_)) {}\n        void call() { f(); }\n    };\npublic:\n    template<typename F>\n    function_wrapper(F&& f):\n        impl(new impl_type<F>(std::move(f)))\n    {}\n    void operator()() { impl->call(); }\n    function_wrapper() = default;\n    function_wrapper(function_wrapper&& other):\n        impl(std::move(other.impl))\n    {}\nListing 9.2\nA thread pool with waitable tasks\n\n\n305\nThread pools\n    function_wrapper& operator=(function_wrapper&& other)\n    {\n        impl=std::move(other.impl);\n        return *this;\n    }\n    function_wrapper(const function_wrapper&)=delete;\n    function_wrapper(function_wrapper&)=delete;\n    function_wrapper& operator=(const function_wrapper&)=delete;\n};\nclass thread_pool\n{\n    thread_safe_queue<function_wrapper> work_queue;   \n    void worker_thread()\n    {\n        while(!done)\n        {\n            function_wrapper task;                    \n            if(work_queue.try_pop(task))\n            {\n                task();\n            }\n            else\n            {\n                std::this_thread::yield();\n            }\n        }\n    }\npublic:\n    template<typename FunctionType>\n    std::future<typename std::result_of<FunctionType()>::type>   \n        submit(FunctionType f)\n    {\n        typedef typename std::result_of<FunctionType()>::type\n            result_type;                                      \n        std::packaged_task<result_type()> task(std::move(f));    \n        std::future<result_type> res(task.get_future());     \n        work_queue.push(std::move(task));    \n        return res;   \n    }\n    // rest as before\n};\nFirst, the modified submit() function B returns a std::future<> to hold the return\nvalue of the task and allow the caller to wait for the task to complete. This requires\nthat you know the return type of the supplied function f, which is where std::\nresult_of<> comes in: std::result_of<FunctionType()>::type is the type of the\nresult of invoking an instance of type FunctionType (such as f) with no arguments.\nYou use the same std::result_of<> expression for the result_type typedef c\ninside the function.\n You then wrap the function f in a std::packaged_task<result_type()> d,\nbecause f is a function or callable object that takes no parameters and returns an\ninstance of type result_type, as we deduced. You can now get your future from the\nUse function_\nwrapper rather \nthan std::function\n b\nc\nd\n e\nf\ng\n\n\n306\nCHAPTER 9\nAdvanced thread management\nstd::packaged_task<> e before pushing the task onto the queue f and returning\nthe future g. Note that you have to use std::move() when pushing the task onto\nthe queue, because std::packaged_task<> isn’t copyable. The queue now stores\nfunction_wrapper objects rather than std::function<void()> objects in order to\nhandle this.\n This pool allows you to wait for your tasks and have them return results. The next\nlisting shows what the parallel_accumulate function looks like with this thread pool.\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return init;\n    unsigned long const block_size=25;\n    unsigned long const num_blocks=(length+block_size-1)/block_size;   \n    std::vector<std::future<T> > futures(num_blocks-1);\n    thread_pool pool;\n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_blocks-1);++i)\n    {\n        Iterator block_end=block_start;\n        std::advance(block_end,block_size);\n        futures[i]=pool.submit([=]{\n            accumulate_block<Iterator,T>()(block_start,block_end);\n        });   \n        block_start=block_end;\n    }\n    T last_result=accumulate_block<Iterator,T>()(block_start,last);\n    T result=init;\n    for(unsigned long i=0;i<(num_blocks-1);++i)\n    {\n        result+=futures[i].get();\n    }\n    result += last_result;\n    return result;\n}\nWhen you compare this against listing 8.4, there are a couple of things to notice. First,\nyou’re working in terms of the number of blocks to use (num_blocks) B rather than\nthe number of threads. In order to make the most use of the scalability of your thread\npool, you need to divide the work into the smallest blocks that it’s worth working with\nconcurrently. When there are only a few threads in the pool, each thread will process\nmany blocks, but as the number of threads grows with the hardware, the number of\nblocks processed in parallel will also grow.\n You need to be careful when choosing the “smallest blocks worth working with con-\ncurrently.” There’s an inherent overhead to submitting a task to a thread pool, having\nthe worker thread run it, and passing the return value through a std::future<>, and\nListing 9.3\nparallel_accumulate using a thread pool with waitable tasks\nb\nc\n",
      "page_number": 322
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 330-337)",
      "start_page": 330,
      "end_page": 337,
      "detection_method": "topic_boundary",
      "content": "307\nThread pools\nfor small tasks it’s not worth the payoff. If you choose too small a task size, the code\nmay run more slowly with a thread pool than with one thread.\n Assuming the block size is sensible, you don’t have to worry about packaging the\ntasks, obtaining the futures, or storing the std::thread objects so you can join with\nthe threads later; the thread pool takes care of that. All you need to do is call submit()\nwith your task c.\n The thread pool takes care of the exception safety too. Any exception thrown by\nthe task gets propagated through the future returned from submit(), and if the func-\ntion exits with an exception, the thread pool destructor abandons any not-yet-completed\ntasks and waits for the pool threads to finish.\n This works well for simple cases like this, where the tasks are independent. But it’s\nnot so good for situations where the tasks depend on other tasks also submitted to the\nthread pool.\n9.1.3\nTasks that wait for other tasks\nThe Quicksort algorithm is an example that I’ve used throughout this book. It’s sim-\nple in concept: the data to be sorted is partitioned into those items that go before a\npivot item and those that go after it in the sorted sequence. These two sets of items are\nrecursively sorted and then stitched back together to form a fully sorted set. When\nparallelizing this algorithm, you need to ensure that these recursive calls make use of\nthe available concurrency.\n Back in chapter 4, when I first introduced this example, you used std::async to\nrun one of the recursive calls at each stage, letting the library choose between running\nit on a new thread and running it synchronously when the relevant get() was called.\nThis works well, because each task is either running on its own thread or will be\ninvoked when required.\n When we revisited the implementation in chapter 8, you saw an alternative struc-\nture that used a fixed number of threads related to the available hardware concur-\nrency. In this case, you used a stack of pending chunks that needed sorting. As each\nthread partitioned the data it was sorting, it added a new chunk to the stack for one of\nthe sets of data and then sorted the other one directly. At this point, a straightforward\nwait for the sorting of the other chunk to complete would potentially deadlock,\nbecause you’d be consuming one of your limited number of threads waiting. It would\nbe easy to end up in a situation where all of the threads were waiting for chunks to be\nsorted and no threads were doing any sorting. We addressed this issue by having the\nthreads pull chunks off the stack and sort them while the particular chunk they were\nwaiting for was unsorted.\n You’d get the same problem if you substituted a simple thread pool like the ones\nyou’ve seen so far in this chapter, instead of std::async in the example from chapter 4.\nThere are now only a limited number of threads, and they might end up all waiting\nfor tasks that haven’t been scheduled because there are no free threads. You therefore\nneed to use a solution similar to the one you used in chapter 8: process outstanding\n\n\n308\nCHAPTER 9\nAdvanced thread management\nchunks while you’re waiting for your chunk to complete. If you’re using the thread\npool to manage the list of tasks and their association with threads—which is, after all,\nthe whole point of using a thread pool—you don’t have access to the task list to do\nthis. What you need to do is modify the thread pool to do this automatically.\n The simplest way to do this is to add a new function on thread_pool to run a task\nfrom the queue and manage the loop yourself, so we’ll go with that. Advanced thread\npool implementations might add logic into the wait function or additional wait func-\ntions to handle this case, possibly prioritizing the task being waited for. The following\nlisting shows the new run_pending_task() function, and a modified Quicksort to\nmake use of it is shown in listing 9.5.\nvoid thread_pool::run_pending_task()\n{\n    function_wrapper task;\n    if(work_queue.try_pop(task))\n    {\n        task();\n    }\n    else\n    {\n        std::this_thread::yield();\n    }\n}\nThis implementation of run_pending_task() is lifted straight out of the main loop of\nthe worker_thread() function, which can now be modified to call the extracted\nrun_pending_task(). This tries to take a task off the queue and run it if there is one;\notherwise, it yields to allow the OS to reschedule the thread. The Quicksort imple-\nmentation in listing 9.5 is a lot simpler than the corresponding version from listing\n8.1, because all the thread-management logic has been moved to the thread pool.\ntemplate<typename T>\nstruct sorter        \n{\n    thread_pool pool;    \n    \n    std::list<T> do_sort(std::list<T>& chunk_data)\n    {\n        if(chunk_data.empty())\n        {\n            return chunk_data;\n        }\n        std::list<T> result;\n        result.splice(result.begin(),chunk_data,chunk_data.begin());\n        T const& partition_val=*result.begin();\nListing 9.4\nAn implementation of run_pending_task()\nListing 9.5\nA thread-pool–based implementation of Quicksort\nb\nc\n\n\n309\nThread pools\n        typename std::list<T>::iterator divide_point=\n            std::partition(chunk_data.begin(),chunk_data.end(),\n                           [&](T const& val){return val<partition_val;});\n        std::list<T> new_lower_chunk;\n        new_lower_chunk.splice(new_lower_chunk.end(),\n                               chunk_data,chunk_data.begin(),\n                               divide_point);\n        std::future<std::list<T> > new_lower=    \n            pool.submit(std::bind(&sorter::do_sort,this,\n                                  std::move(new_lower_chunk)));\n        std::list<T> new_higher(do_sort(chunk_data));\n        result.splice(result.end(),new_higher);\n        while(new_lower.wait_for(std::chrono::seconds(0)) ==\n            std::future_status::timeout)\n        {\n            pool.run_pending_task();    \n        }\n        result.splice(result.begin(),new_lower.get());\n        return result;\n    }\n};\ntemplate<typename T>\nstd::list<T> parallel_quick_sort(std::list<T> input)\n{\n    if(input.empty())\n    {\n        return input;\n    }\n    sorter<T> s;\n    return s.do_sort(input);\n}\nAs in listing 8.1, you’ve delegated the real work to the do_sort() member function of\nthe sorter class template B, although in this case the class is only there to wrap the\nthread_pool instance c. \n Your thread and task management are now reduced to submitting a task to the\npool d and running pending tasks while waiting e. This is much simpler than in list-\ning 8.1, where you had to explicitly manage the threads and the stack of chunks to\nsort. When submitting the task to the pool, you use std::bind() to bind the this\npointer to do_sort() and to supply the chunk to sort. In this case, you call std::move()\non new_lower_chunk as you pass it in, to ensure that the data is moved rather than\ncopied.\n Although this has now addressed the crucial deadlock-causing problem with tasks\nthat wait for other tasks, this thread pool is still far from ideal. For starters, every call to\nsubmit() and every call to run_pending_task()accesses the same queue. You saw in\nchapter 8 how having a single set of data modified by multiple threads can have a det-\nrimental effect on performance, so you need to address this problem.\nd\ne\n\n\n310\nCHAPTER 9\nAdvanced thread management\n9.1.4\nAvoiding contention on the work queue\nEvery time a thread calls submit() on a particular instance of the thread pool, it has\nto push a new item onto the single shared work queue. Likewise, the worker threads\nare continually popping items off the queue in order to run the tasks. This means that\nas the number of processors increases, there’s increasing contention on the queue.\nThis can be a real performance drain; even if you use a lock-free queue so there’s no\nexplicit waiting, cache ping-pong can be a substantial time sink.\n One way to avoid cache ping-pong is to use a separate work queue per thread.\nEach thread then posts new items to its own queue and takes work from the global\nwork queue only if there’s no work on its own individual queue. The following listing\nshows an implementation that makes use of a thread_local variable to ensure that\neach thread has its own work queue, as well as the global one.\nclass thread_pool\n{\n    threadsafe_queue<function_wrapper> pool_work_queue;\n    typedef std::queue<function_wrapper> local_queue_type;   \n    static thread_local std::unique_ptr<local_queue_type>\n        local_work_queue;    \n    void worker_thread()\n    {\n        local_work_queue.reset(new local_queue_type);   \n        \n        while(!done)\n        {\n            run_pending_task();\n        }\n    }\npublic:\n    template<typename FunctionType>\n    std::future<typename std::result_of<FunctionType()>::type>\n        submit(FunctionType f)\n    {\n        typedef typename std::result_of<FunctionType()>::type result_type;\n        std::packaged_task<result_type()> task(f);\n        std::future<result_type> res(task.get_future());\n        if(local_work_queue)         \n        {\n            local_work_queue->push(std::move(task));\n        }\n        else\n        {\n            pool_work_queue.push(std::move(task));   \n        }\n        return res;\n    }\n    void run_pending_task()\n    {\n        function_wrapper task;\nListing 9.6\nA thread pool with thread-local work queues\nb\nc\nd\ne\nf\n\n\n311\nThread pools\n        if(local_work_queue && !local_work_queue->empty())    \n        {\n            task=std::move(local_work_queue->front());\n            local_work_queue->pop();\n            task();\n        }\n        else if(pool_work_queue.try_pop(task))    \n        {\n            task();\n        }\n        else\n        {\n            std::this_thread::yield();\n        }\n    }\n    // rest as before\n};\nYou’ve used a std::unique_ptr<> to hold the thread-local work queue c because\nyou don’t want other threads that aren't part of your thread pool to have one; this is\ninitialized in the worker_thread() function before the processing loop d. The destruc-\ntor of std::unique_ptr<> will ensure that the work queue is destroyed when the\nthread exits.\n submit() then checks to see if the current thread has a work queue e. If it does,\nit’s a pool thread, and you can put the task on the local queue; otherwise, you need to\nput the task on the pool queue as before f.\n There’s a similar check in run_pending_task() g, except this time you also need\nto check to see if there are any items on the local queue. If there are, you can take the\nfront one and process it; notice that the local queue can be a plain std::queue< B\nbecause it’s only ever accessed by the one thread. If there are no tasks on the local\nqueue, you try the pool queue as before h.\n This works fine for reducing contention, but when the distribution of work is\nuneven, it can easily result in one thread having a lot of work in its queue while the\nothers have no work do to. For example, with the Quicksort example, only the top-\nmost chunk would make it to the pool queue, because the remaining chunks would\nend up on the local queue of the worker thread that processed that one. This defeats\nthe purpose of using a thread pool.\n Thankfully, there is a solution to this: allow the threads to steal work from each\nother’s queues if there’s no work in their queue and no work in the global queue.\n9.1.5\nWork stealing\nIn order to allow a thread with no work to do to take work from another thread with a\nfull queue, the queue must be accessible to the thread doing the stealing from run_\npending_tasks(). This requires that each thread register its queue with the thread\npool or be given one by the thread pool. Also, you must ensure that the data in the work\nqueue is suitably synchronized and protected so that your invariants are protected.\ng\nh\n\n\n312\nCHAPTER 9\nAdvanced thread management\n It’s possible to write a lock-free queue that allows the owner thread to push and\npop at one end while other threads can steal entries from the other, but the imple-\nmentation of this queue is beyond the scope of this book. In order to demonstrate the\nidea, we’ll stick to using a mutex to protect the queue’s data. We hope work stealing is\na rare event, so there should be little contention on the mutex, and this simple queue\nshould therefore have minimal overhead. A simple lock-based implementation is\nshown here.\nclass work_stealing_queue\n{\nprivate:\n    typedef function_wrapper data_type;\n    std::deque<data_type> the_queue;    \n    mutable std::mutex the_mutex;\npublic:\n    work_stealing_queue()\n    {}\n    work_stealing_queue(const work_stealing_queue& other)=delete;\n    work_stealing_queue& operator=(\n        const work_stealing_queue& other)=delete;\n    void push(data_type data)   \n    {\n        std::lock_guard<std::mutex> lock(the_mutex);\n        the_queue.push_front(std::move(data));\n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lock(the_mutex);\n        return the_queue.empty();\n    }\n    bool try_pop(data_type& res)   \n    {\n        std::lock_guard<std::mutex> lock(the_mutex);\n        if(the_queue.empty())\n        {\n            return false;\n        }\n        res=std::move(the_queue.front());\n        the_queue.pop_front();\n        return true;\n    }\n    bool try_steal(data_type& res)   \n    {\n        std::lock_guard<std::mutex> lock(the_mutex);\n        if(the_queue.empty())\n        {\n            return false;\n        }\n        res=std::move(the_queue.back());\n        the_queue.pop_back();\n        return true;\nListing 9.7\nLock-based queue for work stealing\nb\nc\nd\ne\n\n\n313\nThread pools\n    }\n};\nThis queue is a simple wrapper around a std::deque<function_wrapper> B that\nprotects all accesses with a mutex lock. Both push() c and try_pop() d work on the\nfront of the queue, while try_steal() e works on the back.\n This means that this “queue” is a last-in-first-out stack for its own thread; the task\nmost recently pushed on is the first one off again. This can help improve performance\nfrom a cache perspective, because the data related to that task is more likely to still be\nin the cache than the data related to a task pushed on the queue previously. Also, it\nmaps nicely to algorithms such as Quicksort. In the previous implementation, each\ncall to do_sort() pushes one item on the stack and then waits for it. By processing the\nmost recent item first, you ensure that the chunk needed for the current call to com-\nplete is processed before the chunks needed for the other branches, reducing the\nnumber of active tasks and the total stack usage. try_steal() takes items from the\nopposite end of the queue to try_pop() in order to minimize contention; you could\npotentially use the techniques discussed in chapters 6 and 7 to enable concurrent calls\nto try_pop() and try_steal().\n OK, so you have your nice sparkly work queue that permits stealing; how do you\nuse it in your thread pool? Here’s one potential implementation.\nclass thread_pool\n{\n    typedef function_wrapper task_type;\n    std::atomic_bool done;\n    threadsafe_queue<task_type> pool_work_queue;\n    std::vector<std::unique_ptr<work_stealing_queue> > queues;   \n    std::vector<std::thread> threads;\n    join_threads joiner;\n    static thread_local work_stealing_queue* local_work_queue;   \n    static thread_local unsigned my_index;\n    void worker_thread(unsigned my_index_)\n    {\n        my_index=my_index_;\n        local_work_queue=queues[my_index].get();   \n        while(!done)\n        {\n            run_pending_task();\n        }\n    }\n    bool pop_task_from_local_queue(task_type& task)\n    {\n        return local_work_queue && local_work_queue->try_pop(task);\n    }\n    bool pop_task_from_pool_queue(task_type& task)\n    {\n        return pool_work_queue.try_pop(task);\n    }\nListing 9.8\nA thread pool that uses work stealing\nb\nc\nd\n\n\n314\nCHAPTER 9\nAdvanced thread management\n    bool pop_task_from_other_thread_queue(task_type& task)   \n    {\n        for(unsigned i=0;i<queues.size();++i)\n        {\n            unsigned const index=(my_index+i+1)%queues.size();   \n            if(queues[index]->try_steal(task))\n            {\n                return true;\n            }\n        }\n        return false;\n    }\npublic:\n    thread_pool():\n        done(false),joiner(threads)\n    {\n        unsigned const thread_count=std::thread::hardware_concurrency();\n        try\n        {\n            for(unsigned i=0;i<thread_count;++i)\n            {\n                queues.push_back(std::unique_ptr<work_stealing_queue>(  \n                                     new work_stealing_queue));\n            }\n            for(unsigned i=0;i<thread_count;++i)\n            {\n                threads.push_back(\n                    std::thread(&thread_pool::worker_thread,this,i));\n            }\n        }\n        catch(...)\n        {\n            done=true;\n            throw;\n        }\n    }\n    ~thread_pool()\n    {\n        done=true;\n    }\n    template<typename FunctionType>\n    std::future<typename std::result_of<FunctionType()>::type> submit(\n        FunctionType f)\n    {\n        typedef typename std::result_of<FunctionType()>::type result_type;\n        std::packaged_task<result_type()> task(f);\n        std::future<result_type> res(task.get_future());\n        if(local_work_queue)\n        {\n            local_work_queue->push(std::move(task));\n        }\n        else\n        {\n            pool_work_queue.push(std::move(task));\n        }\ne\nf\ng\n",
      "page_number": 330
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 338-346)",
      "start_page": 338,
      "end_page": 346,
      "detection_method": "topic_boundary",
      "content": "315\nInterrupting threads\n        return res;\n    }\n    void run_pending_task()\n    {\n        task_type task;\n        if(pop_task_from_local_queue(task) ||    \n           pop_task_from_pool_queue(task) ||      \n           pop_task_from_other_thread_queue(task))   \n        {\n            task();\n        }\n        else\n        {\n            std::this_thread::yield();\n        }\n    }\n};\nThis code is similar to listing 9.6. The first difference is that each thread has a\nwork_stealing_queue rather than a plain std::queue<> c. When each thread is cre-\nated, rather than allocating its own work queue, the pool constructor allocates one g,\nwhich is then stored in the list of work queues for this pool B. The index of the queue\nin the list is then passed in to the thread function and used to retrieve the pointer to\nthe queue d. This means that the thread pool can access the queue when trying to\nsteal a task for a thread that has no work to do. run_pending_task() will now try to\ntake a task from its thread’s own queue h, take a task from the pool queue i, or take\na task from the queue of another thread j.\n pop_task_from_other_thread_queue() e iterates through the queues belonging\nto all the threads in the pool, trying to steal a task from each in turn. In order to\navoid every thread trying to steal from the first thread in the list, each thread starts\nat the next thread in the list by offsetting the index of the queue to check by its own\nindex f.\n Now you have a working thread pool that’s good for many potential uses. There\nare still a myriad of ways to improve it for any particular usage, but that’s left as an\nexercise for the reader. One aspect that hasn’t been explored is the idea of dynami-\ncally resizing the thread pool to ensure that there’s optimal CPU usage even when\nthreads are blocked waiting for something such as I/O or a mutex lock.\n Next on the list of “advanced” thread-management techniques is interrupting\nthreads.\n9.2\nInterrupting threads\nIn many situations it’s desirable to signal to a long-running thread that it’s time to\nstop. This might be because it’s a worker thread for a thread pool and the pool is\nnow being destroyed, or because the work being done by the thread has been explic-\nitly canceled by the user, or a myriad of other reasons. Whatever the reason, the idea\nis the same: you need to signal from one thread that another should stop before it\nh\ni j\n\n\n316\nCHAPTER 9\nAdvanced thread management\nreaches the natural end of its processing, and you need to do this in a way that\nallows that thread to terminate nicely rather than abruptly pulling the rug out from\nunder it.\n You could potentially design a separate mechanism for every case where you need\nto do this, but that would be overkill. Not only does a common mechanism make it\neasier to write the code on subsequent occasions, but it can allow you to write code\nthat can be interrupted, without having to worry about where that code is being used.\nThe C++11 Standard doesn’t provide this mechanism (though there is an active pro-\nposal for adding interrupt support to a future C++ standard1), but it’s relatively\nstraightforward to build one. Let’s look at how you can do that, starting from the\npoint of view of the interface for launching and interrupting a thread rather than that\nof the thread being interrupted.\n9.2.1\nLaunching and interrupting another thread\nTo start with, let’s look at the external interface. What do you need from an interrupt-\nible thread? At the basic level, all you need is the same interface as you have for\nstd::thread, with an additional interrupt() function:\nclass interruptible_thread\n{\npublic:\n    template<typename FunctionType>\n    interruptible_thread(FunctionType f);\n    void join();\n    void detach();\n    bool joinable() const;\n    void interrupt();\n};\nInternally, you can use std::thread to manage the thread itself and use some custom\ndata structure to handle the interruption. Now, what about from the point of view of\nthe thread itself? At the most basic level you want to be able to say “I can be inter-\nrupted here”—you want an interruption point. For this to be usable without having to\npass down additional data, it needs to be a simple function that can be called without\nany parameters: interruption_point(). This implies that the interruption-specific\ndata structure needs to be accessible through a thread_local variable that’s set when\nthe thread is started, so that when a thread calls your interruption_point() func-\ntion, it checks the data structure for the currently-executing thread. We’ll look at the\nimplementation of interruption_point() later.\n This thread_local flag is the primary reason you can’t use plain std::thread to\nmanage the thread; it needs to be allocated in a way that the interruptible_thread\ninstance can access, as well as the newly started thread. You can do this by wrapping\n1 P0660: A Cooperatively Interruptible Joining Thread, Rev 3, Nicolai Josuttis, Herb Sutter, Anthony Williams\nhttp://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0660r3.pdf.\n\n\n317\nInterrupting threads\nthe supplied function before you pass it to std::thread to launch the thread in the\nconstructor, as shown in the next listing.\nclass interrupt_flag\n{\npublic:\n    void set();\n    bool is_set() const;\n};\nthread_local interrupt_flag this_thread_interrupt_flag;    \nclass interruptible_thread\n{\n    std::thread internal_thread;\n    interrupt_flag* flag;\npublic:\n    template<typename FunctionType>\n    interruptible_thread(FunctionType f)\n    {\n        std::promise<interrupt_flag*> p;    \n        internal_thread=std::thread([f,&p]{           \n                p.set_value(&this_thread_interrupt_flag);\n                f();                         \n            });\n        flag=p.get_future().get();   \n    }\n    void interrupt()\n    {\n        if(flag)\n        {\n            flag->set();   \n        }\n    }\n};\nThe supplied function f is wrapped in a lambda function d, which holds a copy of f\nand a reference to the local promise, p c. The lambda sets the value of the promise\nto the address of the this_thread_interrupt_flag (which is declared thread_local\nB) for the new thread before invoking the copy of the supplied function e. The\ncalling thread then waits for the future associated with the promise to become ready\nand stores the result in the flag member variable f. Note that even though the\nlambda is running on the new thread and has a dangling reference to the local vari-\nable, p, this is OK because the interruptible_thread constructor waits until p is no\nlonger referenced by the new thread before returning. Note that this implementa-\ntion doesn’t take account of handling joining with the thread, or detaching it. You\nneed to ensure that the flag variable is cleared when the thread exits, or is detached,\nto avoid a dangling pointer.\n The interrupt() function is then relatively straightforward: if you have a valid\npointer to an interrupt flag, you have a thread to interrupt, so you can set the flag g.\nListing 9.9\nBasic implementation of interruptible_thread\nb\nc\nd\ne\nf\ng\n\n\n318\nCHAPTER 9\nAdvanced thread management\nIt’s then up to the interrupted thread what it does with the interruption. Let’s explore\nthat next.\n9.2.2\nDetecting that a thread has been interrupted\nYou can now set the interruption flag, but that doesn’t do you any good if the thread\ndoesn’t check whether it’s being interrupted. In the simplest case you can do this with\nan interruption_point() function; you can call this function at a point where it’s\nsafe to be interrupted, and it throws a thread_interrupted exception if the flag is set:\nvoid interruption_point()\n{\n    if(this_thread_interrupt_flag.is_set())\n    {\n        throw thread_interrupted();\n    }\n}\nYou can use this function by calling it at convenient points within your code:\nvoid foo()\n{\n    while(!done)\n    {\n        interruption_point();\n        process_next_item();\n    }\n}\nAlthough this works, it’s not ideal. Some of the best places for interrupting a thread\nare where it’s blocked waiting for something, which means that the thread isn’t run-\nning in order to call interruption_point()! What you need here is a means for wait-\ning for something in an interruptible fashion.\n9.2.3\nInterrupting a condition variable wait\nOK, so you can detect interruptions at carefully chosen places in your code, with\nexplicit calls to interruption_point(), but that doesn’t help when you want to do a\nblocking wait, such as waiting for a condition variable to be notified. You need a new\nfunction—interruptible_wait()—which you can then overload for the various\nthings you might want to wait for, and you can work out how to interrupt the waiting.\nI’ve already mentioned that one thing you might be waiting for is a condition variable,\nso let’s start there: what do you need to do in order to be able to interrupt a wait on a\ncondition variable? The simplest thing that would work is to notify the condition vari-\nable once you’ve set the interrupt flag, and put an interruption point immediately\nafter the wait. But for this to work, you’d have to notify all threads waiting on the con-\ndition variable in order to ensure that your thread of interest wakes up. Waiters have\nto handle spurious wake-ups anyway, so other threads would handle this the same as a\nspurious wake-up—they wouldn’t be able to tell the difference. The interrupt_flag\n\n\n319\nInterrupting threads\nstructure would need to be able to store a pointer to a condition variable so that it can\nbe notified in a call to set(). One possible implementation of interruptible_wait()\nfor condition variables might look like the following listing.\nvoid interruptible_wait(std::condition_variable& cv,\n                        std::unique_lock<std::mutex>& lk)\n{\n    interruption_point();\n    this_thread_interrupt_flag.set_condition_variable(cv);   \n    cv.wait(lk);                                      \n    this_thread_interrupt_flag.clear_condition_variable();   \n    interruption_point();\n}\nAssuming the presence of some functions for setting and clearing an association of a\ncondition variable with an interrupt flag, this code is nice and simple. It checks for\ninterruption, associates the condition variable with interrupt_flag for the current\nthread B, waits on the condition variable c, clears the association with the condition\nvariable d, and checks for interruption again. If the thread is interrupted during the\nwait on the condition variable, the interrupting thread will broadcast the condition\nvariable and wake you from the wait, so you can check for interruption. Unfortunately,\nthis code is broken: there are two problems with it. The first problem is relatively obvi-\nous if you have your exception safety hat on: std::condition_variable::wait() can\nthrow an exception, so you might exit the function without removing the association\nof the interrupt flag with the condition variable. This is easily fixed with a structure\nthat removes the association in its destructor.\n The second, less obvious problem is that there’s a race condition. If the thread is\ninterrupted after the initial call to interruption_point(), but before the call to\nwait(), then it doesn’t matter whether the condition variable has been associated with\nthe interrupt flag, because the thread isn’t waiting and so can’t be woken by a notify on the\ncondition variable. You need to ensure that the thread can’t be notified between the last\ncheck for interruption and the call to wait(). Without delving into the internals of\nstd::condition_variable, you have only one way of doing that: use the mutex held\nby lk to protect this too, which requires passing it in on the call to set_condition\n_variable(). Unfortunately, this creates its own problems: you’d be passing a refer-\nence to a mutex whose lifetime you don’t know to another thread (the thread doing\nthe interrupting) for that thread to lock (in the call to interrupt()), without know-\ning whether that thread has locked the mutex already when it makes the call. This has\nthe potential for deadlock and the potential to access a mutex after it has already been\ndestroyed, so it’s a nonstarter. It would be rather too restrictive if you couldn’t reliably\ninterrupt a condition variable wait—you can do almost as well without a special\ninterruptible_wait()—so what other options do you have? One option is to put a\nListing 9.10\nA broken version of interruptible_wait for std::condition\n_variable\nb\nc\nd\n\n\n320\nCHAPTER 9\nAdvanced thread management\ntimeout on the wait; use wait_for() rather than wait() with a small timeout value\n(such as 1 ms). This puts an upper limit on how long the thread will have to wait\nbefore it sees the interruption (subject to the tick granularity of the clock). If you do\nthis, the waiting thread will see more “spurious” wakes resulting from the timeout, but\nit can’t easily be helped. This implementation is shown in the next listing, along with\nthe corresponding implementation of interrupt_flag.\nclass interrupt_flag\n{\n    std::atomic<bool> flag;\n    std::condition_variable* thread_cond;\n    std::mutex set_clear_mutex;\npublic:\n    interrupt_flag():\n        thread_cond(0)\n    {}\n    void set()\n    {\n        flag.store(true,std::memory_order_relaxed);\n        std::lock_guard<std::mutex> lk(set_clear_mutex);\n        if(thread_cond)\n        {\n            thread_cond->notify_all();\n        }\n    }\n    bool is_set() const\n    {\n        return flag.load(std::memory_order_relaxed);\n    }\n    void set_condition_variable(std::condition_variable& cv)\n    {\n        std::lock_guard<std::mutex> lk(set_clear_mutex);\n        thread_cond=&cv;\n    }\n    void clear_condition_variable()\n    {\n        std::lock_guard<std::mutex> lk(set_clear_mutex);\n        thread_cond=0;\n    }\n    struct clear_cv_on_destruct\n    {\n        ~clear_cv_on_destruct()\n        {\n            this_thread_interrupt_flag.clear_condition_variable();\n        }\n    };\n};\nvoid interruptible_wait(std::condition_variable& cv,\n                        std::unique_lock<std::mutex>& lk)\n{\nListing 9.11\nUsing a timeout in interruptible_wait for std::condition\n_variable\n\n\n321\nInterrupting threads\n    interruption_point();\n    this_thread_interrupt_flag.set_condition_variable(cv);          \n    interrupt_flag::clear_cv_on_destruct guard;\n    interruption_point();\n    cv.wait_for(lk,std::chrono::milliseconds(1));\n    interruption_point();\n}\nIf you have the predicate that’s being waited for, then the 1 ms timeout can be com-\npletely hidden inside the predicate loop:\ntemplate<typename Predicate>\nvoid interruptible_wait(std::condition_variable& cv,\n                        std::unique_lock<std::mutex>& lk,\n                        Predicate pred)\n{\n    interruption_point();\n    this_thread_interrupt_flag.set_condition_variable(cv);          \n    interrupt_flag::clear_cv_on_destruct guard;\n    while(!this_thread_interrupt_flag.is_set() && !pred())\n    {\n        cv.wait_for(lk,std::chrono::milliseconds(1));\n    }\n    interruption_point();\n}\nThis will result in the predicate being checked more often than it might otherwise be,\nbut it’s easily used in place of a plain call to wait(). The variants with timeouts are eas-\nily implemented: wait either for the time specified, or 1 ms, whichever is shortest. OK,\nso std::condition_variable waits are now taken care of; what about std::condition\n_variable_any? Is this the same, or can you do better?\n9.2.4\nInterrupting a wait on std::condition_variable_any\nstd::condition_variable_any differs from std::condition_variable in that it works\nwith any lock type rather than just std::unique_lock<std::mutex>. It turns out that\nthis makes things much easier, and you can do better with std::condition_variable\n_any than you could with std::condition_variable. Because it works with any lock\ntype, you can build your own lock type that locks/unlocks both the internal set_clear\n_mutex in your interrupt_flag and the lock supplied to the wait call, as shown here.\nclass interrupt_flag\n{\n    std::atomic<bool> flag;\n    std::condition_variable* thread_cond;\n    std::condition_variable_any* thread_cond_any;\n    std::mutex set_clear_mutex;\npublic:\n    interrupt_flag():\nListing 9.12\ninterruptible_wait for std::condition_variable_any\n\n\n322\nCHAPTER 9\nAdvanced thread management\n        thread_cond(0),thread_cond_any(0)\n    {}\n    void set()\n    {\n        flag.store(true,std::memory_order_relaxed);\n        std::lock_guard<std::mutex> lk(set_clear_mutex);\n        if(thread_cond)\n        {\n            thread_cond->notify_all();\n        }\n        else if(thread_cond_any)\n        {\n            thread_cond_any->notify_all();\n        }\n    }\n    template<typename Lockable>\n    void wait(std::condition_variable_any& cv,Lockable& lk)\n    {\n        struct custom_lock\n        {\n            interrupt_flag* self;\n            Lockable& lk;\n            custom_lock(interrupt_flag* self_,\n                        std::condition_variable_any& cond,\n                        Lockable& lk_):\n                self(self_),lk(lk_)\n            {\n                self->set_clear_mutex.lock();   \n                self->thread_cond_any=&cond;   \n            }\n            void unlock()    \n            {\n                lk.unlock();\n                self->set_clear_mutex.unlock();\n            }\n            void lock()\n            {\n                std::lock(self->set_clear_mutex,lk);    \n            }\n            ~custom_lock()\n            {\n                self->thread_cond_any=0;   \n                self->set_clear_mutex.unlock();\n            }\n        };\n        custom_lock cl(this,cv,lk);\n        interruption_point();\n        cv.wait(cl);\n        interruption_point();\n    }\n    // rest as before    \n};\ntemplate<typename Lockable>\nvoid interruptible_wait(std::condition_variable_any& cv,\n                        Lockable& lk)\nB\nc\nd\ne\nf\n\n\n323\nInterrupting threads\n{\n    this_thread_interrupt_flag.wait(cv,lk);\n}\nYour custom lock type acquires the lock on the internal set_clear_mutex when it’s\nconstructed B, and then sets the thread_cond_any pointer to refer to the std:: con-\ndition_variable_any passed in to the constructor c. The Lockable reference is\nstored for later; this must already be locked. You can now check for an interruption\nwithout worrying about races. If the interrupt flag is set at this point, it was set before\nyou acquired the lock on set_clear_mutex. When the condition variable calls your\nunlock() function inside wait(), you unlock the Lockable object and the internal\nset_clear_mutex d. This allows threads that are trying to interrupt you to acquire\nthe lock on set_clear_mutex and check the thread_cond_any pointer once you’re\ninside the wait() call but not before. This is exactly what you were after (but couldn’t\nmanage) with std::condition_variable. Once wait() has finished waiting (either\nbecause it was notified or because of a spurious wake), it will call your lock() func-\ntion, which again acquires the lock on the internal set_clear_mutex and the lock on\nthe Lockable object e. You can now check again for interruptions that happened\nduring the wait() call before clearing the thread_cond_any pointer in your cus-\ntom_lock destructor f, where you also unlock the set_clear_mutex.\n9.2.5\nInterrupting other blocking calls\nThat rounds up interrupting condition variable waits, but what about other blocking\nwaits: mutex locks, waiting for futures, and the like? In general you have to go for the\ntimeout option you used for std::condition_variable because there’s no way to\ninterrupt the wait short of fulfilling the condition being waited for, without access to\nthe internals of the mutex or future. But with those other things, you do know what\nyou’re waiting for, so you can loop within the interruptible_wait() function. As an\nexample, here’s an overload of interruptible_wait() for std::future<>:\ntemplate<typename T>\nvoid interruptible_wait(std::future<T>& uf)\n{\n   while(!this_thread_interrupt_flag.is_set())\n   {\n       if(uf.wait_for(lk,std::chrono::milliseconds(1))==\n           std::future_status::ready)\n           break;\n   }\n   interruption_point();\n}\nThis waits until either the interrupt flag is set or the future is ready but does a block-\ning wait on the future for 1 ms at a time. This means that on average it will be around\n0.5 ms before an interrupt request is acknowledged, assuming a high-resolution clock.\nThe wait_for will typically wait at least a whole clock tick, so if your clock ticks every\n",
      "page_number": 338
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 347-354)",
      "start_page": 347,
      "end_page": 354,
      "detection_method": "topic_boundary",
      "content": "324\nCHAPTER 9\nAdvanced thread management\n15 ms, you’ll end up waiting around 15 ms rather than 1 ms. This may or may not be\nacceptable, depending on the circumstances. You can always reduce the timeout if\nnecessary (and if the clock supports it). The downside of reducing the timeout is that\nthe thread will wake more often to check the flag, and this will increase the task-\nswitching overhead.\n OK, we’ve looked at how you might detect interruption with the interruption\n_point() and interruptible_wait() functions, but how do you handle that?\n9.2.6\nHandling interruptions\nFrom the point of view of the thread being interrupted, an interruption is a thread\n_interrupted exception, which can therefore be handled like any other exception. In\nparticular, you can catch it in a standard catch block:\ntry\n{\n    do_something();\n}\ncatch(thread_interrupted&)\n{\n    handle_interruption();\n}\nThis means that you could catch the interruption, handle it in some way, and then\ncarry on regardless. If you do this, and another thread calls interrupt() again, your\nthread will be interrupted again the next time it calls an interruption point. You might\nwant to do this if your thread is performing a series of independent tasks; interrupting\none task will cause that task to be abandoned, and the thread can then move on to\nperforming the next task in the list.\n Because thread_interrupted is an exception, all the usual exception-safety precau-\ntions must also be taken when calling code that can be interrupted, in order to ensure\nthat resources aren’t leaked, and your data structures are left in a coherent state. Often,\nit will be desirable to let the interruption terminate the thread, so you can let the excep-\ntion propagate up. But if you let exceptions propagate out of the thread function\npassed to the std::thread constructor, std::terminate() will be called, and the\nwhole program will be terminated. In order to avoid having to remember to put a catch\n(thread_interrupted) handler in every function you pass to interruptible_thread,\nyou can instead put that catch block inside the wrapper you use for initializing the\ninterrupt_flag. This makes it safe to allow the interruption exception to propagate\nunhandled, because it will then terminate that individual thread. The initialization of\nthe thread in the interruptible_thread constructor now looks like this:\ninternal_thread=std::thread([f,&p]{\n        p.set_value(&this_thread_interrupt_flag);\n        try\n        {\n            f();\n\n\n325\nInterrupting threads\n        }\n        catch(thread_interrupted const&)\n        {}\n    });\nLet’s now look at a concrete example where interruption is useful.\n9.2.7\nInterrupting background tasks on application exit\nConsider for a moment a desktop search application. As well as interacting with the\nuser, the application needs to monitor the state of the filesystem, identifying any\nchanges and updating its index. This processing is typically left to a background\nthread in order to avoid affecting the responsiveness of the GUI. This background\nthread needs to run for the entire lifetime of the application; it will be started as part\nof the application initialization and left to run until the application is shut down. For\nsuch an application this is typically only when the machine itself is being shut down,\nbecause the application needs to run the whole time in order to maintain an up-to-\ndate index. In any case, when the application is being shut down, you need to close\ndown the background threads in an orderly manner; one way to do this is by inter-\nrupting them.\n The following listing shows a sample implementation of the thread-management\nparts of this system.\nstd::mutex config_mutex;\nstd::vector<interruptible_thread> background_threads;\nvoid background_thread(int disk_id)\n{\n    while(true)\n    {\n        interruption_point();     \n        fs_change fsc=get_fs_changes(disk_id);   \n        if(fsc.has_changes())\n        {\n            update_index(fsc);   \n        }\n    }\n}\nvoid start_background_processing()\n{\n    background_threads.push_back(\n        interruptible_thread(background_thread,disk_1));\n    background_threads.push_back(\n        interruptible_thread(background_thread,disk_2));\n}\nint main()\n{\n    start_background_processing();   \n    process_gui_until_exit();                  \n    std::unique_lock<std::mutex> lk(config_mutex);\nListing 9.13\nMonitoring the filesystem in the background\nB\nc\nd\ne\nf\n\n\n326\nCHAPTER 9\nAdvanced thread management\n    for(unsigned i=0;i<background_threads.size();++i)\n    {\n        background_threads[i].interrupt();    \n    }\n    for(unsigned i=0;i<background_threads.size();++i)\n    {\n        background_threads[i].join();   \n    }\n}\nAt startup, the background threads are launched d. The main thread then proceeds\nwith handling the GUI e. When the user has requested that the application exit, the\nbackground threads are interrupted f, and then the main thread waits for each back-\nground thread to complete before exiting g. The background threads sit in a loop,\nchecking for disk changes h and updating the index c. Every time around the loop\nthey check for interruption by calling interruption_point() B.\n Why do you interrupt all the threads before waiting for any? Why not interrupt\neach and then wait for it before moving on to the next? The answer is concurrency.\nThreads will likely not finish immediately when they’re interrupted, because they have\nto proceed to the next interruption point and then run any destructor calls and\nexception-handling code necessary before they exit. By joining with each thread\nimmediately, you therefore cause the interrupting thread to wait, even though it still\nhas useful work it could do—interrupt the other threads. Only when you have no\nmore work to do (all the threads have been interrupted) do you wait. This also allows\nall the threads being interrupted to process their interruptions in parallel and poten-\ntially finish sooner.\n This interruption mechanism could easily be extended to add further interrupt-\nible calls or to disable interruptions across a specific block of code, but this is left as an\nexercise for the reader.\nSummary\nIn this chapter, we’ve looked at various advanced thread-management techniques:\nthread pools and interrupting threads. You’ve seen how the use of local work queues\nand work stealing can reduce the synchronization overhead and potentially improve\nthe throughput of the thread pool and how running other tasks from the queue while\nwaiting for a subtask to complete can eliminate the potential for deadlock.\n We’ve also looked at various ways of allowing one thread to interrupt the process-\ning of another, such as the use of specific interruption points and functions that per-\nform what would otherwise be a blocking wait in a way that can be interrupted.\ng\nh\n\n\n327\nParallel algorithms\nIn the last chapter we looked at advanced thread management and thread pools,\nand in chapter 8 we looked at designing concurrent code, using parallel versions of\nsome algorithms as examples. In this chapter, we’ll look at the parallel algorithms\nprovided by the C++17 standard, so let’s start, without further ado.\n10.1\nParallelizing the standard library algorithms\nThe C++17 standard added the concept of parallel algorithms to the C++ Standard\nLibrary. These are additional overloads of many of the functions that operate on\nranges, such as std::find, std::transform and std::reduce. The parallel ver-\nsions have the same signature as the “normal” single-threaded versions, except for\nthe addition of a new first parameter, which specifies the execution policy to use. For\nexample:\nstd::vector<int> my_data;\nstd::sort(std::execution::par,my_data.begin(),my_data.end());\nThe execution policy of std::execution::par indicates to the standard library that\nit is allowed to perform this call as a parallel algorithm, using multiple threads. Note\nThis chapter covers\nUsing the C++17 parallel algorithms\n\n\n328\nCHAPTER 10\nParallel algorithms\nthat this is permission, not a requirement—the library may still execute the code on a sin-\ngle thread if it wishes. It is also important to note that by specifying an execution pol-\nicy, the requirements on the algorithm complexity have changed, and are usually\nslacker than the requirements for the normal serial algorithm. This is because parallel\nalgorithms often do more total work in order to take advantage of the parallelism of\nthe system — if you can divide the work across 100 processors, then you can still get an\noverall speed up to 50, even if the implementation does twice as much total work.\n Before we get onto the algorithms themselves, let’s take a look at the execution\npolicies.\n10.2\nExecution policies\nThe standard specifies three execution policies:\n\nstd::execution::sequenced_policy \n\nstd::execution::parallel_policy \n\nstd::execution::parallel_unsequenced_policy \nThese are classes defined in the <execution> header. The header also defines three\ncorresponding policy objects to pass to the algorithms:\n\nstd::execution::seq \n\nstd::execution::par \n\nstd::execution::par_unseq \nYou cannot rely on being able to construct objects from these policy classes yourself,\nexcept by copying these three objects, because they might have special initialization\nrequirements. Implementations may also define additional execution policies that have\nimplementation-specific behavior. You cannot define your own execution policies.\n The consequences of these policies on the behavior of the algorithms are described\nin section 10.2.1. Any given implementation is also allowed to provide additional exe-\ncution policies, with whatever semantics they want. Let’s now take a look at the conse-\nquences of using one of the standard execution policies, starting with the general\nchanges for all algorithm overloads that take an exception policy.\n10.2.1 General effects of specifying an execution policy\nIf you pass an execution policy to one of the standard library algorithms, then the\nbehavior of that algorithm is now governed by the execution policy. This affects sev-\neral aspects of the behavior:\nThe algorithm’s complexity\nThe behavior when an exception is thrown\nWhere, how, and when the steps of the algorithm are executed\nEFFECTS ON ALGORITHM COMPLEXITY\nIf an execution policy is supplied to an algorithm, then that algorithm’s complexity\nmay be changed: in addition to the scheduling overhead of managing the parallel\n\n\n329\nExecution policies\nexecution, many parallel algorithms will perform more of the core operations of the\nalgorithm (whether swaps, comparisons, or applications of a supplied function object), with\nthe intention that this provides an overall improvement in the performance in terms\nof total elapsed time.\n The precise details of the complexity change will vary with each algorithm, but the\ngeneral policy is that if an algorithm specifies something will happen exactly some-\nexpression times, or at most some-expression times, then the overload with an execution\npolicy will slacken that requirement to O(some-expression). This means that the overload\nwith an execution policy may perform some multiple of the number of operations\nperformed by its counterpart without an execution policy, where that multiple will\ndepend on the internals of the library and the platform, rather than the data supplied\nto the algorithm.\nEXCEPTIONAL BEHAVIOR\nIf an exception is thrown during execution of an algorithm with an execution policy,\nthen the consequences are determined by the execution policy. All the standard-\nsupplied execution policies will call std::terminate if there are any uncaught excep-\ntions. The only exception that may be thrown by a call to a standard library algorithm\nwith one of the standard execution policies is std::bad_alloc, which is thrown if the\nlibrary cannot obtain sufficient memory resources for its internal operations. For\nexample, the following call to std::for_each, without an execution policy, will propa-\ngate the exception\nstd::for_each(v.begin(),v.end(),[](auto x){ throw my_exception(); });\nwhereas the corresponding call with an execution policy will terminate the program:\nstd::for_each(\n    std::execution::seq,v.begin(),v.end(),\n    [](auto x){ throw my_exception(); });\nThis is one of the key differences between using std::execution::seq and not pro-\nviding an execution policy.\nWHERE AND WHEN ALGORITHM STEPS ARE EXECUTED\nThis is the fundamental aspect of an execution policy, and is the only aspect that dif-\nfers between the standard execution policies. The policy specifies which execution\nagents are used to perform the steps of the algorithm, be they “normal” threads, vec-\ntor streams, GPU threads, or anything else. The execution policy will also specify\nwhether there are any ordering constraints on how the algorithm steps are run:\nwhether or not they are run in any particular order, whether or not parts of separate\nalgorithm steps may be interleaved with each other, or run in parallel with each other,\nand so forth.\n The details for each of the standard execution policies are given in sections 10.2.2,\n10.2.3, and 10.2.4, starting with the most basic policy: std::execution::sequenced\n_policy.\n\n\n330\nCHAPTER 10\nParallel algorithms\n10.2.2 std::execution::sequenced_policy\nThe sequenced policy is not a policy for parallelism: using it forces the implementa-\ntion to perform all operations on the thread that called the function, so there is no\nparallelism. But it is still an execution policy, and therefore has the same conse-\nquences on algorithmic complexity and the effect of exceptions as the other stan-\ndard policies.\n Not only must all operations be performed on the same thread, but they must be\nperformed in some definite order, so they are not interleaved. The precise order is\nunspecified, and may be different between different invocations of the function. In\nparticular, the order of execution of the operations is not guaranteed to be the same\nas that of the corresponding overload without an execution policy. For example, the\nfollowing call to std::for_each will populate the vector with the numbers 1-1,000, in\nan unspecified order. This is in contrast to the overload without an execution policy,\nwhich will store the numbers in order:\nstd::vector<int> v(1000);\nint count=0;\nstd::for_each(std::execution::seq,v.begin(),v.end(),\n    [&](int& x){ x=++count; });\nThe numbers may be stored in order, but you cannot rely on it.\n This means that the sequenced policy imposes few requirements on the iterators,\nvalues, and callable objects used with the algorithm: they may freely use synchroniza-\ntion mechanisms, and may rely on all operations being invoked on the same thread,\nthough they cannot rely on the order of these operations.\n10.2.3 std::execution::parallel_policy\nThe parallel policy provides basic parallel execution across a number of threads.\nOperations may be performed either on the thread that invoked the algorithm, or on\nthreads created by the library. Operations performed on a given thread must be per-\nformed in a definite order, and not interleaved, but the precise order is unspecified,\nand may vary between invocations. A given operation will run on a fixed thread for its\nentire duration.\n This imposes additional requirements on the iterators, values, and callable objects\nused with the algorithm over the sequenced policy: they must not cause data races if\ninvoked in parallel, and must not rely on being run on the same thread as any other\noperation, or indeed rely on not being run on the same thread as any other operation.\n You can use the parallel execution policy for the vast majority of cases where you\nwould have used a standard library algorithm without an execution policy. It’s only\nwhere there is specific ordering between elements that is required, or unsynchronized\naccess to shared data, that is problematic. Incrementing all the values in a vector can\nbe done in parallel:\nstd::for_each(std::execution::par,v.begin(),v.end(),[](auto& x){++x;});\n\n\n331\nThe parallel algorithms from the C++ Standard Library\nThe previous example of populating a vector is not OK if done with the parallel exe-\ncution policy; specifically, it is undefined behavior:\nstd::for_each(std::execution::par,v.begin(),v.end(),\n    [&](int& x){ x=++count; });\nHere, the variable count is modified from every invocation of the lambda, so if the\nlibrary were to execute the lambdas across multiple threads, this would be a data race,\nand thus undefined behavior. The requirements for std::execution::parallel_\npolicy pre-empt this: it is undefined behavior to make the preceding call, even if the\nlibrary doesn’t use multiple threads for this call. Whether or not something exhibits\nundefined behavior is a static property of the call, rather than dependent on imple-\nmentation details of the library. Synchronization between the function invocations is\npermitted, however, so you could make this defined behavior again either by making\ncount an std::atomic<int> rather than a plain int, or by using a mutex. In this case,\nthat would likely defeat the point of using the parallel execution policy, because that\nwould serialize all the calls, but in the general case it would allow for synchronized\naccess to a shared state.\n10.2.4 std::execution::parallel_unsequenced_policy\nThe parallel unsequenced policy provides the library with the greatest scope for paral-\nlelizing the algorithm in exchange for imposing the strictest requirements on the iter-\nators, values, and callable objects used with the algorithm.\n An algorithm invoked with the parallel unsequenced policy may perform the\nalgorithm steps on unspecified threads of execution, unordered and unsequenced\nwith respect to one another. This means that operations may now be interleaved with\neach other on a single thread, such that a second operation is started on the same\nthread before the first has finished, and may be migrated between threads, so a given\noperation may start on one thread, run further on a second thread, and complete on\na third.\n If you use the parallel unsequenced policy, then the operations invoked on the\niterators, values, and callable objects supplied to the algorithm must not use any form\nof synchronization or call any function that synchronizes with another, or any func-\ntion such that some other code synchronizes with it.\n This means that the operations must only operate on the relevant element, or any\ndata that can be accessed based on that element, and must not modify any state shared\nbetween threads, or between elements.\n We’ll flesh these out with some examples later. For now, let’s take a look at the par-\nallel algorithms themselves.\n10.3\nThe parallel algorithms from the C++ Standard Library\nMost of the algorithms from the <algorithm> and <numeric> headers have overloads\nthat take an execution policy. This comprises: all_of, any_of, none_of, for_each,\n",
      "page_number": 347
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 355-362)",
      "start_page": 355,
      "end_page": 362,
      "detection_method": "topic_boundary",
      "content": "332\nCHAPTER 10\nParallel algorithms\nfor_each_n, find, find_if, find_end, find_first_of, adjacent_find, count, count_if,\nmismatch, equal, search, search_n, copy, copy_n, copy_if, move, swap_ranges,\ntransform, replace, replace_if, replace_copy, replace_copy_if, fill, fill_n,\ngenerate, generate_n, remove, remove_if, remove_copy, remove_copy_if, unique,\nunique_copy, reverse, reverse_copy, rotate, rotate_copy, is_partitioned,\npartition, stable_partition, partition_copy, sort, stable_sort, partial_sort,\npartial_sort_copy, is_sorted, is_sorted_until, nth_element, merge, inplace\n_merge, includes, set_union, set_intersection, set_difference, set_symmetric\n_difference, is_heap, is_heap_until, min_element, max_element, minmax_element,\nlexicographical_compare, reduce, transform_reduce, exclusive_scan, inclusive\n_scan, transform_exclusive_scan, transform_inclusive_scan, and adjacent_\ndifference.\n That’s quite a list; pretty much every algorithm in the C++ Standard Library that\ncould be parallelized is in this list. Notable exceptions are things like std::accumulate,\nwhich is strictly a serial accumulation, but its generalized counterpart in std::reduce\ndoes appear in the list — with a suitable warning in the standard that if the reduction\noperation is not both associative and commutative, then the result may be nondeter-\nministic due to the unspecified order of operations.\n For each of the algorithms in the list, every “normal” overload has a new variant\nwhich takes an execution policy as the first argument—the corresponding arguments\nfor the “normal” overload then come after this execution policy. For example,\nstd::sort has two “normal” overloads without an execution policy:\ntemplate<class RandomAccessIterator>\nvoid sort(RandomAccessIterator first, RandomAccessIterator last);\ntemplate<class RandomAccessIterator, class Compare>\nvoid sort(\n    RandomAccessIterator first, RandomAccessIterator last, Compare comp);\nIt therefore also has two overloads with an execution policy:\ntemplate<class ExecutionPolicy, class RandomAccessIterator>\nvoid sort(\n    ExecutionPolicy&& exec,\n    RandomAccessIterator first, RandomAccessIterator last);\ntemplate<class ExecutionPolicy, class RandomAccessIterator, class Compare>\nvoid sort(\n    ExecutionPolicy&& exec,\n    RandomAccessIterator first, RandomAccessIterator last, Compare comp);\nThere is one important difference between the signatures with and without the execu-\ntion policy argument, which only impacts some algorithms: if the “normal” algorithm\nallows Input Iterators or Output Iterators, then the overloads with an execution policy\nrequire Forward Iterators instead. This is because Input Iterators are fundamentally\nsingle-pass: you can only access the current element, and you cannot store iterators to\n\n\n333\nThe parallel algorithms from the C++ Standard Library\nprevious elements. Similarly, Output Iterators only allow writing to the current ele-\nment: you cannot advance them to write a later element, and then backtrack to write a\nprevious one.\nThus, given the “normal” signature for std::copy\ntemplate<class InputIterator, class OutputIterator>\nOutputIterator copy(\n    InputIterator first, InputIterator last, OutputIterator result);\nthe overload with an execution policy is\ntemplate<class ExecutionPolicy,\n    class ForwardIterator1, class ForwardIterator2>\nForwardIterator2 copy(\n    ExecutionPolicy&& policy,\n    ForwardIterator1 first, ForwardIterator1 last, \n    ForwardIterator2 result);\nThough the naming of the template parameters doesn’t carry any direct consequence\nfrom the compiler’s perspective, it does from the C++ Standard’s perspective: the\nnames of the template parameters for Standard Library algorithms denote semantic\nconstraints on the types, and the algorithms will rely on the operations implied by\nIterator categories in the C++ Standard Library\nThe C++ Standard Library defines five categories of iterators: Input Iterators, Output\nIterators, Forward Iterators, Bidirectional Iterators, and Random Access Iterators.\nInput Iterators are single-pass iterators for retrieving values. They are typically used\nfor things like input from a console or network, or generated sequences. Advancing\nan Input Iterator invalidates any copies of that iterator.\nOutput Iterators are single-pass iterators for writing values. They are typically used for\noutput to files, or adding values to a container. Advancing an Output Iterator invali-\ndates any copies of that iterator.\nForward Iterators are multipass iterators for one-way iteration through persistent\ndata. Though you can't make an iterator go back to a previous element, you can store\ncopies and use them to reference earlier elements. Forward Iterators return real ref-\nerences to the elements, and so can be used for both reading and writing (if the tar-\nget is non-const).\nBidirectional Iterators are multipass iterators like Forward Iterators, but they can also\nbe made to go backward to access previous elements.\nRandom Access Iterators are multipass iterators that can go forward and backward\nlike Bidirectional Iterators, but they can go forward and backward in steps larger than\na single element, and you can directly access elements at an offset, using the array\nindex operator.\n\n\n334\nCHAPTER 10\nParallel algorithms\nthose constraints existing, with the specified semantics. In the case of Input Iterators\nvs. Forward Iterators, the former allows dereferencing the iterator to return a proxy\ntype, which is convertible to the value type of the iterator, whereas the latter requires\nthat dereferencing the iterator returns a real reference to the value and that all equal\niterators return a reference to the same value.\n This is important for parallelism: it means that the iterators can be freely copied\naround, and used equivalently. Also, the requirement that incrementing a Forward\nIterator does not invalidate other copies is important, as it means that separate\nthreads can operate on their own copies of the iterators, incrementing them when\nrequired, without concern about invalidating the iterators held by the other threads.\nIf the overload with an execution policy allowed use of Input Iterators, this would\nforce any threads to serialize access to the one and only iterator that was used for read-\ning from the source sequence, which obviously limits the potential for parallelism.\n Let’s have a look at some concrete examples.\n10.3.1 Examples of using parallel algorithms\nThe simplest possible example surely has to be the parallel loop: do something for\neach element of a container. This is the classic example of an embarrassingly parallel\nscenario: each item is independent, so you have the maximum possibility of parallel-\nism. With a compiler that supports OpenMP, you might write\n#pragma omp parallel for\nfor(unsigned i=0;i<v.size();++i){\n    do_stuff(v[i]);\n}\nWith the C++ Standard Library algorithms, you can instead write\nstd::for_each(std::execution::par,v.begin(),v.end(),do_stuff);\nThis will divide the elements of the range between the internal threads created by the\nlibrary, and invoke do_stuff(x) on each element x in the range. How those elements\nare divided between the threads is an implementation detail.\nCHOICE OF EXECUTION POLICY\nstd::execution::par is the policy that you’ll want to use most often, unless your\nimplementation provides a nonstandard policy better suited to your needs. If your\ncode is suitable for parallelization, then it should work with std::execution::par. In\nsome circumstances, you may be able to use std::execution::par_unseq instead.\nThis may do nothing at all (none of the standard execution policies make a guarantee\nabout the level of parallelism that will be attained), but it may give the library addi-\ntional scope to improve the performance of the code by reordering and interleaving\nthe tasks, in exchange for the tighter requirements on your code. Most notable of\nthese tighter requirements is that there is no synchronization used in accessing the\nelements, or performing the operations on the elements. This means that you cannot\n\n\n335\nThe parallel algorithms from the C++ Standard Library\nuse mutexes or atomic variables, or any of the other mechanisms described in previ-\nous chapters, to ensure that accesses from multiple threads are safe; instead, you must\nrely on the algorithm itself not accessing the same element from multiple threads,\nand use external synchronization outside the call to the parallel algorithm to prevent\nother threads accessing the data.\n The example from listing 10.1 shows some code that can be used with std::\nexecution::par, but not std::execution::par_unseq. The use of the internal mutex\nfor synchronization means that attempting to use std::execution::par_unseq would\nbe undefined behavior.\nclass X{\n    mutable std::mutex m;\n    int data;\npublic:\n    X():data(0){}\n    int get_value() const{\n        std::lock_guard guard(m);\n        return data;\n    }\n    void increment(){\n        std::lock_guard guard(m);\n        ++data;\n    }\n};\nvoid increment_all(std::vector<X>& v){\n    std::for_each(std::execution::par,v.begin(),v.end(),\n        [](X& x){\n            x.increment();\n        });\n}\nThe next listing shows an alternative that can be used with std::execution::par_un-\nseq. In this case, the internal per-element mutex has been replaced with a whole-con-\ntainer mutex.\nclass Y{\n    int data;\npublic:\n    Y():data(0){}\n    int get_value() const{\n        return data;\n    }\n    void increment(){\n        ++data;\n    }\n};\nListing 10.1\nParallel algorithms on a class with internal synchronization\nListing 10.2\nParallel algorithms on a class without internal synchronization\n\n\n336\nCHAPTER 10\nParallel algorithms\nclass ProtectedY{\n    std::mutex m;\n    std::vector<Y> v;\npublic:\n   void lock(){\n         m.lock();\n     }\n   void unlock(){\n         m.unlock();\n     }\n     std::vector<Y>& get_vec(){\n         return v;\n     }\n};\nvoid increment_all(ProtectedY& data){\n    std::lock_guard guard(data);\n    auto& v=data.get_vec();\n    std::for_each(std::execution::par_unseq,v.begin(),v.end(),\n        [](Y& y){\n            y.increment();\n        });\n}\nThe element accesses in listing 10.2 now have no synchronization, and it is safe to use\nstd::execution::par_unseq. The downside is that concurrent accesses from other\nthreads outside the parallel algorithm invocation must now wait for the entire opera-\ntion to complete, rather than the per-element granularity of listing 10.1.\n Let’s now take a look at a more realistic example of how the parallel algorithms\nmight be used: counting visits to a website.\n10.3.2 Counting visits\nSuppose you run a busy website, such that the logs contain millions of entries, and you\nwant to process those logs to see aggregate data: how many visits per page, where do\nthose visits come from, which browsers were used to access the website, and so forth.\nAnalyzing these logs has two parts: processing each line to extract the relevant infor-\nmation, and aggregating the results together. This is an ideal scenario for using paral-\nlel algorithms, because processing each individual line is entirely independent of\neverything else, and aggregating the results can be done piecemeal, provided the final\ntotals are correct.\n In particular, this is the sort of task that transform_reduce is designed for. The\nfollowing listing shows how this could be used for this task.\n#include <vector>\n#include <string>\n#include <unordered_map>\n#include <numeric>\nListing 10.3\nUsing transform_reduce to count visits to pages of a website\n\n\n337\nThe parallel algorithms from the C++ Standard Library\nstruct log_info {\n    std::string page;\n    time_t visit_time;\n    std::string browser;\n    // any other fields\n};\nextern log_info parse_log_line(std::string const &line);   \nusing visit_map_type= std::unordered_map<std::string, unsigned long long>;\nvisit_map_type\ncount_visits_per_page(std::vector<std::string> const &log_lines) {\n    struct combine_visits {\n        visit_map_type\n        operator()(visit_map_type lhs, visit_map_type rhs) const {   \n            if(lhs.size() < rhs.size())\n                std::swap(lhs, rhs);\n            for(auto const &entry : rhs) {\n                lhs[entry.first]+= entry.second;\n            }\n            return lhs;\n        }\n        visit_map_type operator()(log_info log,visit_map_type map) const{ \n            ++map[log.page];\n            return map;\n        }\n        visit_map_type operator()(visit_map_type map,log_info log) const{ \n            ++map[log.page];\n            return map;\n        }\n        visit_map_type operator()(log_info log1,log_info log2) const{ \n            visit_map_type map;\n            ++map[log1.page];\n            ++map[log2.page];\n            return map;\n        }\n    };\n    return std::transform_reduce(      \n        std::execution::par, log_lines.begin(), log_lines.end(),\n        visit_map_type(), combine_visits(), parse_log_line);\n}\nAssuming you’ve got some function parse_log_line to extract the relevant informa-\ntion from a log entry B, your count_visits_per_page function is a simple wrapper\naround a call to std::transform_reduce c. The complexity comes from the reduction\noperation: you need to be able to combine two log_info structures to produce a map,\na log_info structure and a map (either way around), and two maps. This therefore\nmeans that your combine_visits function object needs four overloads of the function\nb\nd\ne\nf\ng\nc\n\n\n338\nCHAPTER 10\nParallel algorithms\ncall operator, d, e, f, and g, which precludes doing it with a simple lambda, even\nthough the implementation of these four overloads is simple.\n The implementation of std::transform_reduce will therefore use the available\nhardware to perform this calculation in parallel (because you passed std::execution\n::par). Writing this algorithm manually is nontrivial, as we saw in the previous chap-\nter, so this allows you to delegate the hard work of implementing the parallelism to\nthe Standard Library implementers, so you can focus on the required outcome.\nSummary\nIn this chapter we looked at the parallel algorithms available in the C++ Standard\nLibrary and how to use them. We looked at the various execution policies, the impact\nyour choice of execution policy has on the behavior of the algorithm, and the restric-\ntions it imposes on your code. We then looked at an example of how this algorithm\nmight be used in real code.\n\n\n339\nTesting and debugging\nmultithreaded applications\nUp to now, I’ve focused on what’s involved in writing concurrent code—the tools that\nare available, how to use them, and the overall design and structure of the code. But\nthere’s a crucial part of software development that I haven’t addressed yet: testing\nand debugging. If you’re reading this chapter hoping for an easy way to test concur-\nrent code, you’re going to be sorely disappointed. Testing and debugging concurrent\ncode is hard. What I am going to give you are some techniques that will make things\neasier, alongside some issues that are important to think about.\n Testing and debugging are like two sides of a coin—you subject your code to\ntests in order to find any bugs that might be there, and you debug it to remove\nthose bugs. With any luck, you only have to remove the bugs found by your own\ntests rather than bugs found by the end users of your application. Before we look at\neither testing or debugging, it’s important to understand the problems that might\narise, so let’s look at those.\nThis chapter covers\nConcurrency-related bugs\nLocating bugs through testing and code review\nDesigning multithreaded tests\nTesting the performance of multithreaded code\n",
      "page_number": 355
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 363-372)",
      "start_page": 363,
      "end_page": 372,
      "detection_method": "topic_boundary",
      "content": "340\nCHAPTER 11\nTesting and debugging multithreaded applications\n11.1\nTypes of concurrency-related bugs\nYou can get any sort of bug in concurrent code; it’s not special in that regard. But\nsome types of bugs are directly related to the use of concurrency and therefore of par-\nticular relevance to this book. Typically, these concurrency-related bugs fall into two\ncategories:\nUnwanted blocking\nRace conditions\nThese are broad categories, so let’s divide them up a bit. First, let’s look at unwanted\nblocking.\n11.1.1 Unwanted blocking\nWhat do I mean by unwanted blocking? A thread is blocked when it’s unable to proceed\nbecause it’s waiting for something. This is typically something like a mutex, a condi-\ntion variable, or a future, but it could be waiting for I/O. This is a natural part of\nmultithreaded code, but it’s not always desirable—hence the problem of unwanted\nblocking. This leads us to the next question: why is this blocking unwanted? Typi-\ncally, this is because some other thread is also waiting for the blocked thread to per-\nform some action, and so that thread in turn is blocked. There are several variations\non this theme:\nDeadlock—As you saw in chapter 3, in the case of deadlock, one thread is waiting\nfor another, which is in turn waiting for the first. If your threads deadlock, the\ntasks they’re supposed to be doing won’t get done. In the most visible cases, one\nof the threads involved is the thread responsible for the user interface, in which\ncase the interface will cease to respond. In other cases, the interface will remain\nresponsive, but some required tasks won’t complete, such as a search not\nreturning or a document not printing.\nLivelock—Livelock is similar to deadlock in that one thread is waiting for\nanother, which is in turn waiting for the first. The key difference here is that the\nwait is not a blocking wait but an active checking loop, such as a spin lock. In\nserious cases, the symptoms are the same as deadlock (the app doesn’t make\nany progress), except that the CPU usage is high because threads are still run-\nning but blocking each other. In not-so-serious cases, the livelock will eventually\nresolve because of the random scheduling, but there will be a long delay in the\ntask that got livelocked, with a high CPU usage during that delay.\nBlocking on I/O or other external input—If your thread is blocked waiting for exter-\nnal input, it can’t proceed, even if the waited-for input is never going to come.\nIt’s therefore undesirable to block on external input from a thread that also\nperforms tasks that other threads may be waiting for.\nThat briefly covers unwanted blocking. What about race conditions?\n\n\n341\nTypes of concurrency-related bugs\n11.1.2 Race conditions\nRace conditions are the most common cause of problems in multithreaded code—\nmany deadlocks and livelocks only manifest because of a race condition. Not all race\nconditions are problematic—a race condition occurs any time the behavior depends\non the relative scheduling of operations in separate threads. A large number of race\nconditions are entirely benign; for example, which worker thread processes the next\ntask in the task queue is largely irrelevant. But many concurrency bugs are due to race\nconditions. In particular, race conditions often cause the following types of problems:\nData races—A data race is the specific type of race condition that results in\nundefined behavior because of unsynchronized concurrent access to a shared\nmemory location. I introduced data races in chapter 5 when we looked at the\nC++ memory model. Data races usually occur through incorrect usage of atomic\noperations to synchronize threads or through access to shared data without\nlocking the appropriate mutex.\nBroken invariants—These can manifest as dangling pointers (because another\nthread deleted the data being accessed), random memory corruption (due to a\nthread reading inconsistent values resulting from partial updates), and double-\nfree (such as when two threads pop the same value from a queue, and so both\ndelete some associated data), among others. The invariants being broken can\nbe temporal- as well as value-based. If operations on separate threads are required\nto execute in a particular order, incorrect synchronization can lead to a race\ncondition in which the required order is sometimes violated.\nLifetime issues—Although you could bundle these problems in with broken\ninvariants, this is a separate category. The basic problem with bugs in this cate-\ngory is that the thread outlives the data that it accesses, so it is accessing data\nthat has been deleted or otherwise destroyed, and potentially the storage is\neven reused for another object. You typically get lifetime issues where a thread\nreferences local variables that go out of scope before the thread function has\ncompleted, but they aren’t limited to that scenario. Whenever the lifetime of\nthe thread and the data it operates on aren’t tied together in some way, there’s\nthe potential for the data to be destroyed before the thread has finished and for\nthe thread function to have the rug pulled out from under its feet. If you manu-\nally call join() in order to wait for the thread to complete, you need to ensure\nthat the call to join() can’t be skipped if an exception is thrown. This is basic\nexception safety applied to threads.\nIt’s the problematic race conditions that are the killers. With deadlock and livelock,\nthe application appears to hang and become completely unresponsive or takes too\nlong to complete a task. Often, you can attach a debugger to the running process to\nidentify which threads are involved in the deadlock or livelock and which synchroniza-\ntion objects they’re fighting over. With data races, broken invariants, and lifetime\nissues, the visible symptoms of the problem (such as random crashes or incorrect\n\n\n342\nCHAPTER 11\nTesting and debugging multithreaded applications\noutput) can manifest anywhere in the code—the code may overwrite memory used by\nanother part of the system that isn’t touched until much later. The fault will then man-\nifest in code completely unrelated to the location of the buggy code, possibly much\nlater in the execution of the program. This is the true curse of shared memory sys-\ntems—however much you try to limit which data is accessible by which thread, and try\nto ensure that correct synchronization is used, any thread can overwrite the data\nbeing used by any other thread in the application.\n Now that we’ve briefly identified the sorts of problems we’re looking for, let’s look\nat what you can do to locate any instances in your code so you can fix them.\n11.2\nTechniques for locating concurrency-related bugs\nIn the previous section we looked at the types of concurrency-related bugs you might\nsee and how they might manifest in your code. With that information in mind, you\ncan then look at your code to see where bugs might lie and how you can attempt to\ndetermine whether there are any bugs in a particular section.\n Perhaps the most obvious and straightforward thing to do is look at the code.\nAlthough this might seem obvious, it’s difficult to do in a thorough way. When you\nread code you’ve written, it’s all too easy to read what you intended to write rather\nthan what’s there. Likewise, when reviewing code that others have written, it’s tempt-\ning to give it a quick read-through, check it off against your local coding standards,\nand highlight any glaringly obvious problems. What’s needed is to spend the time\ngoing through the code with a fine-tooth comb, thinking about the concurrency\nissues—and the nonconcurrency issues as well. (You might as well, while you’re doing\nit. After all, a bug is a bug.) We’ll cover specific things to think about when reviewing\ncode shortly.\n Even after thoroughly reviewing your code, you still might have missed some bugs,\nand in any case, you need to confirm that it does work, for peace of mind if nothing\nelse. Consequently, we’ll continue on from reviewing the code to a few techniques to\nemploy when testing multithreaded code.\n11.2.1 Reviewing code to locate potential bugs\nAs I’ve already mentioned, when reviewing multithreaded code to check for concurrency-\nrelated bugs, it’s important to review it thoroughly. If possible, get someone else to\nreview it. Because they haven’t written the code, they’ll have to think through how it\nworks, and this will help to uncover any bugs that may be there. It’s important that the\nreviewer have the time to do the review properly—not a casual two-minute quick\nglance, but a proper, considered review. Most concurrency bugs require more than a\nquick glance to spot—they usually rely on subtle timing issues to manifest.\n If you get one of your colleagues to review the code, they’ll be coming at it fresh.\nThey’ll therefore see things from a different point of view and may spot things that\nyou can’t. If you don’t have colleagues you can ask, ask a friend, or even post the\ncode on the internet (taking care not to upset your company lawyers). If you can’t\n\n\n343\nTechniques for locating concurrency-related bugs\nget anybody to review your code for you, or they don’t find anything, don’t worry—\nthere’s still more you can do. For starters, it might be worth leaving the code alone\nfor a while—work on another part of the application, read a book, or go for a walk.\nIf you take a break, your subconscious can work on the problem in the background\nwhile you’re consciously focused on something else. Also, the code will be less famil-\niar when you come back to it—you might manage to look at it from a different per-\nspective yourself.\n An alternative to getting someone else to review your code is to do it yourself. One\nuseful technique is to try to explain how it works in detail to someone else. They don’t\neven have to be physically there—many teams have a bear or rubber chicken for this\npurpose, and I personally find that writing detailed notes can be hugely beneficial. As\nyou explain, think about each line, what could happen, which data it accesses, and so\nforth. Ask yourself questions about the code, and explain the answers. I find this to be\nan incredibly powerful technique—by asking myself these questions and thinking\ncarefully about the answers, the problem often reveals itself. These questions can be\nhelpful for any code review, not just when reviewing your own code.\nQUESTIONS TO THINK ABOUT WHEN REVIEWING MULTITHREADED CODE\nAs I’ve already mentioned, it can be useful for a reviewer (whether the code’s author\nor someone else) to think about specific questions relating to the code being\nreviewed. These questions can focus the reviewer’s mind on the relevant details of the\ncode and can help identify potential problems. The questions I like to ask include the\nfollowing, though this is most definitely not an exhaustive list. You might find other\nquestions that help you to focus better. Here are my questions:\nWhich data needs to be protected from concurrent access?\nHow do you ensure that the data is protected?\nWhere in the code could other threads be at this time?\nWhich mutexes does this thread hold?\nWhich mutexes might other threads hold?\nAre there any ordering requirements between the operations done in this\nthread and those done in another? How are those requirements enforced?\nIs the data loaded by this thread still valid? Could it have been modified by\nother threads?\nIf you assume that another thread could be modifying the data, what would that\nmean and how could you ensure that this never happens?\nThis last question is my favorite, because it makes me think about the relationships\nbetween the threads. By assuming the existence of a bug related to a particular line of\ncode, you can then act as a detective and track down the cause. In order to convince\nyourself that there’s no bug, you have to consider every corner case and possible\nordering. This is particularly useful where the data is protected by more than one\nmutex over its lifetime, such as with the thread-safe queue from chapter 6 where you\nhad separate mutexes for the head and tail of the queue: in order to be sure that an\n\n\n344\nCHAPTER 11\nTesting and debugging multithreaded applications\naccess is safe while holding one mutex, you have to be certain that a thread holding\nthe other mutex can’t also access the same element. It also makes it obvious that public\ndata, or data for which other code can readily obtain a pointer or reference, has to\ncome under particular scrutiny.\n The penultimate question in the list is also important, because it addresses a mis-\ntake that is easy to make: if you release and then reacquire a mutex, you must assume\nthat other threads may have modified the shared data. Although this is obvious, if the\nmutex locks aren’t immediately visible—perhaps because they’re internal to an\nobject—you may unwittingly be doing exactly that. In chapter 6 you saw how this can\nlead to race conditions and bugs where the functions provided on a thread-safe data\nstructure are too fine-grained. Whereas for a non-thread-safe stack it makes sense to\nhave separate top() and pop() operations, for a stack that may be accessed by multi-\nple threads concurrently, this is no longer the case because the lock on the internal\nmutex is released between the two calls, and so another thread can modify the stack.\nAs you saw in chapter 6, the solution is to combine the two operations so they are both\nperformed under the protection of the same mutex lock, eliminating the potential\nrace condition.\n OK, so you’ve reviewed your code (or got someone else to review it). You’re sure\nthere are no bugs. The proof of the pudding is, as they say, in the eating—how can\nyou test your code to confirm or disprove your belief in its lack of bugs?\n11.2.2 Locating concurrency-related bugs by testing\nWhen developing single-threaded applications, testing your applications is relatively\nstraightforward, if time-consuming. You could, in principle, identify all the possible\nsets of input data (or at least all the interesting cases) and run them through the\napplication. If the application produced the correct behavior and output, you’d know\nit works for that given set of input. Testing for error states such as the handling of disk-\nfull errors is more complicated than that, but the idea is the same: set up the initial\nconditions and allow the application to run.\n Testing multithreaded code is an order of magnitude harder, because the precise\nscheduling of the threads is indeterminate and may vary from run to run. Conse-\nquently, even if you run the application with the same input data, it might work cor-\nrectly some of the time, and fail at other times if there’s a race condition lurking in\nthe code. Having a potential race condition doesn’t mean the code will fail always, just\nthat it might fail sometimes. \n Given the inherent difficulty of reproducing concurrency-related bugs, it pays to\ndesign your tests carefully. You want each test to run the smallest amount of code that\ncould potentially demonstrate a problem, so that you can best isolate the code that’s\nfaulty if the test fails—it’s better to test a concurrent queue directly to verify that con-\ncurrent pushes and pops work rather than testing it through a whole chunk of code\nthat uses the queue. It can help if you think about how code should be tested when\ndesigning it—see the section on designing for testability later in this chapter.\n\n\n345\nTechniques for locating concurrency-related bugs\n It’s also worth eliminating the concurrency from the test in order to verify that the\nproblem is concurrency-related. If you have a problem when everything is running in\na single thread, it’s a common, or garden-variety, bug rather than a concurrency-\nrelated bug. This is particularly important when trying to track down a bug that occurs\n“in the wild” as opposed to being detected in your test harness. Just because a bug\noccurs in the multithreaded portion of your application doesn’t mean it’s automati-\ncally concurrency-related. If you’re using thread pools to manage the level of concur-\nrency, there’s usually a configuration parameter you can set to specify the number of\nworker threads. If you’re managing threads manually, you’ll have to modify the code\nto use a single thread for the test. Either way, if you can reduce your application to a\nsingle thread, you can eliminate concurrency as a cause. On the flip side, if the prob-\nlem goes away on a single-core system (even with multiple threads running) but is pres-\nent on multicore systems or multiprocessor systems, you have a race condition and\npossibly a synchronization or memory-ordering issue.\n There’s more to testing concurrent code than the structure of the code being\ntested; the structure of the test is just as important, as is the test environment. If you\ncontinue on with the example of testing a concurrent queue, you have to think about\nvarious scenarios:\nOne thread calling push() or pop() on its own to verify that the queue works at\na basic level\nOne thread calling push() on an empty queue while another thread calls pop()\nMultiple threads calling push() on an empty queue\nMultiple threads calling push() on a full queue\nMultiple threads calling pop() on an empty queue\nMultiple threads calling pop() on a full queue\nMultiple threads calling pop() on a partially full queue with insufficient items\nfor all threads\nMultiple threads calling push() while one thread calls pop() on an empty queue\nMultiple threads calling push() while one thread calls pop() on a full queue\nMultiple threads calling push() while multiple threads call pop() on an empty\nqueue\nMultiple threads calling push() while multiple threads call pop() on a full queue\nHaving thought about all these scenarios and more, you then need to consider addi-\ntional factors about the test environment:\nWhat you mean by “multiple threads” in each case (3, 4, 1,024?)\nWhether there are enough processing cores in the system for each thread to\nrun on its own core\nWhich processor architectures the tests should be run on\nHow you ensure suitable scheduling for the “while” parts of your tests\n\n\n346\nCHAPTER 11\nTesting and debugging multithreaded applications\nThere are additional factors to think about specific to your particular situation. Of\nthese four environmental considerations, the first and last affect the structure of the\ntest itself (and are covered in section 11.2.5), whereas the other two are related to the\nphysical test system being used. The number of threads to use relates to the particular\ncode being tested, but there are various ways of structuring tests to obtain suitable\nscheduling. Before we look at these techniques, let’s look at how you can design your\napplication code to be easier to test.\n11.2.3 Designing for testability\nTesting multithreaded code is difficult, so you want to do what you can to make it eas-\nier. One of the most important things you can do is design the code for testability. A lot\nhas been written about designing single-threaded code for testability, and much of the\nadvice still applies. In general, code is easier to test if the following factors apply:\nThe responsibilities of each function and class are clear\nThe functions are short and to the point\nYour tests can take complete control of the environment surrounding the code\nbeing tested\nThe code that performs the particular operation being tested is close together\nrather than spread throughout the system\nYou thought about how to test the code before you wrote it\nAll of these are still true for multithreaded code. In fact, I’d argue that it’s even more\nimportant to pay attention to the testability of multithreaded code than for single-\nthreaded code, because it’s inherently that much harder to test. That last point is\nimportant: even if you don’t go as far as writing your tests before the code, it’s well\nworth thinking about how you can test the code before you write it—what inputs to\nuse, which conditions are likely to be problematic, how to stimulate the code in poten-\ntially problematic ways, and so on.\n One of the best ways to design concurrent code for testing is to eliminate the con-\ncurrency. If you can break down the code into those parts that are responsible for the\ncommunication paths between threads and those parts that operate on the communi-\ncated data within a single thread, then you’ve greatly reduced the problem. Those\nparts of the application that operate on data that’s being accessed by only that one\nthread can then be tested using the normal single-threaded techniques. The hard-to-\ntest concurrent code that deals with communicating between threads and ensuring\nthat only one thread at a time is accessing a particular block of data is now much\nsmaller and the testing more tractable.\n For example, if your application is designed as a multithreaded state machine, you\ncould split it into several parts. The state logic for each thread, which ensures that the\ntransitions and operations are correct for each possible set of input events, can be\ntested independently with single-threaded techniques, with the test harness providing\nthe input events that would be coming from other threads. Then, the core state\n\n\n347\nTechniques for locating concurrency-related bugs\nmachine and message routing code that ensures that events are correctly delivered to\nthe right thread in the right order can be tested independently, but with multiple con-\ncurrent threads and simple state logic designed specifically for the tests.\n Alternatively, if you can divide your code into multiple blocks of read shared\ndata/transform data/update shared data, you can test the transform data portions using\nall the usual single-threaded techniques, because this is now single-threaded code.\nThe hard problem of testing a multithreaded transformation will be reduced to test-\ning the reading and updating of the shared data, which is much simpler.\n One thing to watch out for is that library calls can use internal variables to store state,\nwhich then becomes shared if multiple threads use the same set of library calls. This can\nbe a problem because it’s not immediately apparent that the code accesses shared data.\nBut with time you learn which library calls these are, and they stick out like sore thumbs.\nYou can then either add appropriate protection and synchronization or use an alternate\nfunction that’s safe for concurrent access from multiple threads.\n There’s more to designing multithreaded code for testability than structuring your\ncode to minimize the amount of code that needs to deal with concurrency-related\nissues and paying attention to the use of non-thread-safe library calls. It’s also helpful\nto bear in mind the same set of questions you ask yourself when reviewing the code,\nfrom section 11.2.1. Although these questions aren’t directly about testing and test-\nability, if you think about the issues with your “testing hat” on and consider how to test\nthe code, it will affect which design choices you make and will make testing easier.\n Now that we’ve looked at designing code to make testing easier, and potentially\nmodified the code to separate the “concurrent” parts (such as the thread-safe contain-\ners or state machine event logic) from the “single-threaded” parts (which may still\ninteract with other threads through the concurrent chunks), let’s look at the tech-\nniques for testing concurrency-aware code.\n11.2.4 Multithreaded testing techniques\nSo, you’ve thought through the scenario you want to test and written a small amount\nof code that exercises the functions being tested. How do you ensure that any poten-\ntially problematic scheduling sequences are exercised in order to flush out the bugs?\n Well, there are a few ways of approaching this, starting with brute-force testing, or\nstress testing.\nBRUTE-FORCE TESTING\nThe idea behind brute-force testing is to stress the code to see if it breaks. This typi-\ncally means running the code many times, possibly with many threads running at\nonce. If there’s a bug that manifests only when the threads are scheduled in a particu-\nlar fashion, then the more times the code is run, the more likely the bug is to appear.\nIf you run the test once and it passes, you might feel a bit of confidence that the code\nworks. If you run it ten times in a row and it passes every time, you’ll likely feel more\nconfident. If you run the test a billion times and it passes every time, you’ll feel more\nconfident still.\n\n\n348\nCHAPTER 11\nTesting and debugging multithreaded applications\n The confidence you have in the results does depend on the amount of code being\ntested by each test. If your tests are quite fine-grained, like the tests outlined previ-\nously for a thread-safe queue, this brute-force testing can give you a high degree of\nconfidence in your code. On the other hand, if the code being tested is considerably\nlarger, the number of possible scheduling permutations is so vast that even a billion\ntest runs might yield a low level of confidence.\n The downside to brute-force testing is that it might give you false confidence. If the\nway you’ve written the test means that the problematic circumstances can’t occur, you\ncan run the test as many times as you like and it won’t fail, even if it would fail every\ntime in slightly different circumstances. The worst example is where the problematic\ncircumstances can’t occur on your test system because of the way the particular system\nyou’re testing on happens to run. Unless your code is to run only on systems identical\nto the one being tested, the particular hardware and operating system combination\nmay not allow the circumstances that would cause a problem to arise.\n The classic example here is testing a multithreaded application on a single-processor\nsystem. Because every thread has to run on the same processor, everything is auto-\nmatically serialized, and many race conditions and cache ping-pong problems that\nyou may get with a true multiprocessor system evaporate. This isn’t the only variable,\nthough; different processor architectures provide different synchronization and\nordering facilities. For example, on x86 and x86-64 architectures, atomic load opera-\ntions are always the same, whether tagged memory_order_relaxed or memory_order\n_seq_cst (see section 5.3.3). This means that code written using relaxed memory\nordering may work on systems with an x86 architecture, where it would fail on a sys-\ntem with a finer-grained set of memory-ordering instructions, such as SPARC.\n If you need your application to be portable across a range of target systems, it’s\nimportant to test it on representative instances of those systems. This is why I listed the\nprocessor architectures being used for testing as a consideration in section 11.2.2.\n Avoiding the potential for false confidence is crucial to successful brute-force test-\ning. This requires careful thought over test design, not just with respect to the choice\nof unit for the code being tested but also with respect to the design of the test harness\nand the choice of testing environment. You need to ensure that you test as many of\nthe code paths and the possible thread interactions as feasible. Not only that, but you\nneed to know which options are covered and which are left untested.\n Although brute-force testing does give you some degree of confidence in your\ncode, it’s not guaranteed to find all the problems. There’s one technique that is guar-\nanteed to find the problems, if you have the time to apply it to your code and the\nappropriate software. I call it combination simulation testing.\nCOMBINATION SIMULATION TESTING\nThat’s a bit of a mouthful, so I’ll explain what I mean. The idea is that you run your\ncode with a special piece of software that simulates the real runtime environment of\nthe code. You may be aware of software that allows you to run multiple virtual machines\non a single physical computer, where the characteristics of the virtual machine and its\n\n\n349\nTechniques for locating concurrency-related bugs\nhardware are emulated by the supervisor software. The idea here is similar, except\nrather than emulating the system, the simulation software records the sequences of\ndata accesses, locks, and atomic operations from each thread. It then uses the rules of\nthe C++ memory model to repeat the run with every permitted combination of opera-\ntions and identify race conditions and deadlocks.\n Although this exhaustive combination testing is guaranteed to find all the prob-\nlems the system is designed to detect, for anything but the most trivial of programs it\nwill take a huge amount of time, because the number of combinations increases\nexponentially with the number of threads and the number of operations performed\nby each thread. This technique is best reserved for fine-grained tests of individual\npieces of code rather than an entire application. The other obvious downside is that\nit relies on the availability of simulation software that can handle the operations used\nin your code.\n So, you have a technique that involves running your test many times under normal\nconditions but that might miss problems, and you have a technique that involves run-\nning your test many times under special conditions but that’s more likely to find any\nproblems that exist. Are there any other options?\n A third option is to use a library that detects problems as they occur in the running\nof the tests.\nDETECTING PROBLEMS EXPOSED BY TESTS WITH A SPECIAL LIBRARY\nAlthough this option doesn’t provide the exhaustive checking of a combination simu-\nlation test, you can identify many problems by using a special implementation of the\nlibrary synchronization primitives such as mutexes, locks, and condition variables. For\nexample, it’s common to require that all accesses to a piece of shared data be done\nwith a particular mutex locked. If you could check which mutexes were locked when\nthe data was accessed, you could verify that the appropriate mutex was indeed locked\nby the calling thread when the data was accessed and report a failure if this was not\nthe case. By marking your shared data in some way, you can allow the library to check\nthis for you.\n This library implementation can also record the sequence of locks if more than\none mutex is held by a particular thread at once. If another thread locks the same\nmutexes in a different order, this could be recorded as a potential deadlock even if the\ntest didn’t deadlock while running.\n Another type of special library that could be used when testing multithreaded\ncode is one where the implementations of the threading primitives such as mutexes\nand condition variables give the test writer control over which thread gets the lock\nwhen multiple threads are waiting or which thread is notified by a notify_one() call\non a condition variable. This would allow you to set up particular scenarios and verify\nthat your code works as expected in those scenarios.\n Some of these testing facilities would have to be supplied as part of the C++ Stan-\ndard Library implementation, whereas others can be built on top of the Standard\nLibrary as part of your test harness.\n",
      "page_number": 363
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 373-381)",
      "start_page": 373,
      "end_page": 381,
      "detection_method": "topic_boundary",
      "content": "350\nCHAPTER 11\nTesting and debugging multithreaded applications\n Having looked at various ways of executing test code, let’s now look at ways of\nstructuring the code to achieve the scheduling you want.\n11.2.5 Structuring multithreaded test code\nBack in section 11.2.2, I said that you need to find ways of providing suitable scheduling\nfor the “while” part of your tests. Now it’s time to look at the issues involved in that.\n The basic issue is that you need to arrange for a set of threads to each be executing\na chosen piece of code at a time that you specify. In the most basic case you have two\nthreads, but this could easily be extended to more. In the first step, you need to iden-\ntify the distinct parts of each test:\nThe general setup code that must be executed before anything else\nThe thread-specific setup code that must run on each thread\nThe code for each thread that you want to run concurrently\nThe code to be run after the concurrent execution has finished, possibly\nincluding assertions on the state of the code\nTo explain further, let’s consider a specific example from the test list in section 11.2.2:\none thread calling push() on an empty queue while another thread calls pop().\n The general setup code is simple: you must create the queue. The thread executing\npop() has no thread-specific setup code. The thread-specific setup code for the thread\nexecuting push() depends on the interface to the queue and the type of object being\nstored. If the object being stored is expensive to construct or must be heap-allocated,\nyou want to do this as part of the thread-specific setup, so that it doesn’t affect the test.\nOn the other hand, if the queue is just storing plain ints, there’s nothing to be gained\nby constructing an int in the setup code. The code being tested is relatively straight-\nforward—a call to push() from one thread and a call to pop() from another—but\nwhat about the “after completion” code?\n In this case, it depends on what you want pop() to do. If it’s supposed to block\nuntil there is data, then clearly you want to see that the returned data is what was sup-\nplied to the push() call and that the queue is empty afterward. If pop() is not blocking\nand may complete even when the queue is empty, you need to test for two possibilities:\neither the pop() returned the data item supplied to the push() and the queue is\nempty or the pop() signaled that there was no data and the queue has one element.\nOne or the other must be true; what you want to avoid is the scenario that pop() sig-\nnaled “no data” but the queue is empty, or that pop() returned the value and the\nqueue is still not empty. In order to simplify the test, assume you have a blocking\npop(). The final code is therefore an assertion that the popped value is the pushed\nvalue and that the queue is empty.\n Now, having identified the various chunks of code, you need to do the best you can\nto ensure that everything runs as planned. One way to do this is to use a set of\nstd::promises to indicate when everything is ready. Each thread sets a promise to\nindicate that it’s ready and then waits on a (copy of a) std::shared_future obtained\n\n\n351\nTechniques for locating concurrency-related bugs\nfrom a third std::promise; the main thread waits for all the promises from all the\nthreads to be set and then triggers the threads to go. This ensures that each thread has\nstarted and comes before the chunk of code that should be run concurrently; any\nthread-specific setup should be done before setting that thread’s promise. Finally, the\nmain thread waits for the threads to complete and checks the final state. You also need\nto be aware of exceptions and make sure you don’t have any threads left waiting for\nthe go signal when that’s not going to happen. The following listing shows one way of\nstructuring this test.\nvoid test_concurrent_push_and_pop_on_empty_queue()\n{\n    threadsafe_queue<int> q;                     \n    std::promise<void> go,push_ready,pop_ready;    \n    std::shared_future<void> ready(go.get_future());   \n    std::future<void> push_done;     \n    std::future<int> pop_done;\n    try\n    {\n        push_done=std::async(std::launch::async,     \n                             [&q,ready,&push_ready]()\n                             {\n                                 push_ready.set_value();\n                                 ready.wait();\n                                 q.push(42);\n                             }\n            );\n        pop_done=std::async(std::launch::async,    \n                            [&q,ready,&pop_ready]()\n                            {\n                                pop_ready.set_value();\n                                ready.wait();\n                                return q.pop();    \n                            }\n            );\n        push_ready.get_future().wait();   \n        pop_ready.get_future().wait();\n        go.set_value();      \n        push_done.get();           \n        assert(pop_done.get()==42);    \n        assert(q.empty());\n    }\n    catch(...)\n    {\n        go.set_value();   \n        throw;\n    }\n}\nThe structure is pretty much as described previously. First, you create your empty\nqueue as part of the general setup B. Then, you create all your promises for the\nListing 11.1\nAn example test for concurrent push() and pop() calls on a queue\nB\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n1@\n\n\n352\nCHAPTER 11\nTesting and debugging multithreaded applications\n“ready” signals c and get std::shared_future for the go signal d. Then, you create\nthe futures you’ll use to indicate that the threads have finished e. These have to go\noutside the try block so that you can set the go signal on an exception without waiting\nfor the test threads to complete (which would deadlock—a deadlock in the test code\nwould be less than ideal).\n Inside the try block you can then start the threads, f and g—you use std::\nlaunch::async to guarantee that the tasks are each running on their own thread.\nNote that the use of std::async makes your exception-safety task easier than it would\nbe with plain std::thread because the destructor for the future will join with the\nthread. The lambda captures specify that each task will reference the queue and the\nrelevant promise for signaling readiness, while taking a copy of the ready future you\ngot from the go promise.\n As described previously, each task sets its own ready signal and then waits for the\ngeneral ready signal before running the test code. The main thread does the\nreverse—it waits for the signals from both threads i before signaling them to start\nthe real test j.\n Finally, the main thread calls get() on the futures from the async calls to wait for\nthe tasks to finish, 1) and 1!, and checks the results. Note that the pop task returns the\nretrieved value through the future h, so you can use that to get the result for the\nassert 1!.\n If an exception is thrown, you set the go signal to avoid any chance of a dangling\nthread and rethrow the exception 1@. The futures corresponding to the tasks e were\ndeclared last, so they’ll be destroyed first, and their destructors will wait for the tasks\nto complete, if they haven’t already.\n Although this seems like quite a lot of boilerplate to test two simple calls, it’s neces-\nsary to use something similar in order to have the best chance of testing what you want\nto test. For example, starting a thread can be quite a time-consuming process, so if you\ndidn’t make the threads wait for the go signal, then the push thread may have com-\npleted before the pop thread even started, which would completely defeat the point\nof the test. Using the futures in this way ensures that both threads are running and\nblocked on the same future. Unblocking the future then allows both threads to run.\nOnce you’re familiar with the structure, it should be relatively straightforward to cre-\nate new tests in the same pattern. For tests that require more than two threads, this\npattern is readily extended to additional threads.\n So far, we’ve been looking at the correctness of multithreaded code. Although this is\nthe most important issue, it’s not the only reason you test: it’s also important to test\nthe performance of multithreaded code, so let’s look at that next.\n11.2.6 Testing the performance of multithreaded code\nOne of the main reasons you might choose to use concurrency in an application is\nto make use of the increasing prevalence of multicore processors to improve the\nperformance of your applications. It’s therefore important to test your code to\n\n\n353\nSummary\nconfirm that the performance does indeed improve, as you’d do with any other\nattempt at optimization.\n The particular issue with using concurrency for performance is the scalability—you\nwant code that runs approximately 24 times faster or processes 24 times as much data\non a 24-core machine as on a single-core machine, all else being equal. You don’t want\ncode that runs twice as fast on a dual-core machine but is slower on a 24-core\nmachine. As you saw in section 8.4.2, if a significant section of your code runs on only\none thread, this can limit the potential performance gain. It’s therefore worth looking\nat the overall design of the code before you start testing, so you know whether you’re\nhoping for a factor-of-24 improvement, or whether the serial portion of your code\nmeans you’re limited to a maximum of a factor of 3.\n As you’ve already seen in previous chapters, contention between processors for access\nto a data structure can have a big performance impact. Something that scales nicely with\nthe number of processors when that number is small may perform badly when the num-\nber of processors is much larger because of the huge increase in contention.\n Consequently, when testing for the performance of multithreaded code, it’s best to\ncheck the performance on systems with as many different configurations as possible, so\nyou get a picture of the scalability graph. At the very least, you ought to test on a single-\nprocessor system and a system with as many processing cores as are available to you.\nSummary\nIn this chapter, we looked at the various types of concurrency-related bugs that you\nmight encounter, from deadlocks and livelocks to data races and other problematic race\nconditions. We followed that with techniques for locating bugs. These included issues to\nthink about during code reviews, guidelines for writing testable code, and how to struc-\nture tests for concurrent code. Finally, we looked at some utility components that can\nhelp with testing.\n\n\n354\nappendix A\nBrief reference for some\nC++11 language features\nThe new C++ Standard brings more than just concurrency support; there are a host\nof other language features and new libraries as well. In this appendix I give a brief\noverview of the new language features that are used in the Thread Library and the\nrest of the book. Aside from thread_local (which is covered in section A.8), none\nof them are directly related to concurrency, though they are important and/or use-\nful for multithreaded code. I’ve limited this list to those that are either necessary\n(such as rvalue references) or serve to make the code simpler or easier to under-\nstand. Code that uses these features may be difficult to understand at first because of\nlack of familiarity, but as you become familiar with them, they should generally make\ncode easier to understand. As the use of C++11 becomes more widespread, code mak-\ning use of these features will become more common.\n Without further ado, let’s start by looking at rvalue references, which are used\nextensively by the Thread Library to facilitate the transfer of ownership (of threads,\nlocks, or whatever) between objects.\nA.1\nRvalue references\nIf you’ve been doing C++ programming for any time, you’ll be familiar with refer-\nences; C++ references allow you to create a new name for an existing object. All\naccesses and modifications done through the new reference affect the original; for\nexample:\nint var=42;\nint& ref=var;   \nref=99;\nassert(var==99);       \nCreate a reference \nto var.\nOriginal updated because \nof assignment to reference\n\n\n355\nRvalue references\nThe only references that existed prior to C++11 are lvalue references—references to lval-\nues. The term lvalue comes from C and refers to things that can be on the left side of\nan assignment expression, named objects, objects allocated on the stack or heap, or\nmembers of other objects, all things with a defined storage location. The term rvalue\nalso comes from C and refers to things that can occur only on the right side of an\nassignment expression—literals and temporaries, for example. Lvalue references can\nonly be bound to lvalues, not rvalues. You can’t write\nint& i=42;    \nfor example, because 42 is an rvalue. OK, that’s not quite true; you’ve always been able\nto bind an rvalue to a const lvalue reference:\nint const& i=42;\nBut this is a deliberate exception on the part of the standard, introduced before we\nhad rvalue references in order to allow you to pass temporaries to functions taking ref-\nerences. This allows implicit conversions, so you can write things like this:\nvoid print(std::string const& s);\nprint(\"hello\");                  \nThe C++11 Standard introduced rvalue references, which bind only to rvalues, not to lval-\nues, and are declared with two ampersands rather than one:\nint&& i=42;\nint j=42;\nint&& k=j;      \nYou can use function overloading to determine whether function parameters are lval-\nues or rvalues by having one overload take an lvalue reference and another take an\nrvalue reference. This is the cornerstone of move semantics.\nA.1.1\nMove semantics\nRvalues are typically temporary and so can be freely modified; if you know that your\nfunction parameter is an rvalue, you can use it as temporary storage, or “steal” its con-\ntents without affecting program correctness. This means that rather than copying the\ncontents of an rvalue parameter, you can move the contents. For large dynamic struc-\ntures, this saves a lot of memory allocation and provides a lot of scope for optimiza-\ntion. Consider a function that takes an std::vector<int> as a parameter and needs\nto have an internal copy for modification, without touching the original. The old way\nof doing this would be to take the parameter as a const lvalue reference and make the\ncopy internally:\nvoid process_copy(std::vector<int> const& vec_)\n{\nWon’t compile\nCreate temporary \nstd::string object\nWon’t \ncompile\n\n\n356\nAPPENDIX A\nBrief reference for some C++11 language features\n     std::vector<int> vec(vec_);\n     vec.push_back(42);\n}\nThis allows the function to take both lvalues and rvalues but forces the copy in every\ncase. If you overload the function with a version that takes an rvalue reference, you\ncan avoid the copy in the rvalue case, because you know you can freely modify the\noriginal:\nvoid process_copy(std::vector<int> && vec)\n{\n     vec.push_back(42);\n}\nNow, if the function in question is the constructor of your class, you can pilfer the\ninnards of the rvalue and use them for your new instance. Consider the class in the\nfollowing listing. In the default constructor it allocates a large chunk of memory,\nwhich is freed in the destructor.\nclass X\n{\nprivate:\n    int* data;\npublic:\n    X():\n        data(new int[1000000])\n    {}\n    ~X()\n    {\n        delete [] data;\n    }\n    X(const X& other):     \n        data(new int[1000000])\n    {\n        std::copy(other.data,other.data+1000000,data);\n    }\n    X(X&& other):         \n        data(other.data)\n    {\n        other.data=nullptr;\n    }\n};\nThe copy constructor B is defined as you might expect: allocate a new block of memory\nand copy the data across. But you also have a new constructor that takes the old value\nby rvalue reference c. This is the move constructor. In this case you copy the pointer to\nthe data and leave the other instance with a null pointer, saving yourself a huge chunk\nof memory and time when creating variables from rvalues.\nListing A.1\nA class with a move constructor\nB\nc\n\n\n357\nRvalue references\n For class X the move constructor is an optimization, but in some cases it makes\nsense to provide a move constructor even when it doesn’t make sense to provide a\ncopy constructor. For example, the whole point of std::unique_ptr<> is that each\nnon-null instance is the one and only pointer to its object, so a copy constructor\nmakes no sense. But a move constructor allows ownership of the pointer to be trans-\nferred between instances and permits std::unique_ptr<> to be used as a function\nreturn value—the pointer is moved rather than copied.\n If you want to explicitly move from a named object that you know you’ll no longer\nuse, you can cast it to an rvalue either by using static_cast<X&&> or by calling\nstd::move():\nX x1;\nX x2=std::move(x1);\nX x3=static_cast<X&&>(x2);\nThis can be beneficial when you want to move the parameter value into a local or\nmember variable without copying, because although an rvalue reference parameter\ncan bind to rvalues, within the function it is treated as an lvalue:\nvoid do_stuff(X&& x_)\n{\n    X a(x_);            \n    X b(std::move(x_));    \n}\ndo_stuff(X());      \nX x;\ndo_stuff(x);     \nMove semantics are used extensively in the Thread Library, both where copies make\nno semantic sense but resources can be transferred, and as an optimization to avoid\nexpensive copies where the source is going to be destroyed anyway. You saw an example\nof this in section 2.2 where you used std::move() to transfer an std::unique_ptr<>\ninstance into a newly constructed thread, and then again in section 2.3 where we\nlooked at transferring the ownership of threads between std::thread instances. \n std::thread, std::unique_lock<>, std::future<>, std::promise<>, and std::\npackaged_task<> can’t be copied, but they all have move constructors to allow the\nassociated resource to be transferred between instances and support their use as func-\ntion return values. std::string and std::vector<> both can be copied as always, but\nthey also have move constructors and move-assignment operators to avoid copying\nlarge quantities of data from an rvalue.\n The C++ Standard Library never does anything with an object that has been explic-\nitly moved into another object, except destroy it or assign to it (either with a copy or,\nmore likely, a move). But it’s good practice to ensure that the invariant of the class\nencompasses the moved-from state. An std::thread instance that has been used as\nthe source of a move is equivalent to a default-constructed std::thread instance, for\nexample, and an instance of std::string that has been used as the source of a move\nCopies\nMoves\nOK; rvalue binds to \nrvalue reference\nError; lvalue can’t bind \nto rvalue reference\n\n\n358\nAPPENDIX A\nBrief reference for some C++11 language features\nwill still have a valid state, although no guarantees are made as to what that state is (in\nterms of how long the string is or what characters it contains).\nA.1.2\nRvalue references and function templates\nThere’s a final nuance when you use rvalue references for parameters to a function\ntemplate: if the function parameter is an rvalue reference to a template parameter,\nautomatic template argument type deduction deduces the type to be an lvalue refer-\nence if an lvalue is supplied or a plain unadorned type if an rvalue is supplied. That’s\na bit of a mouthful, so let’s look at an example. Consider the following function:\ntemplate<typename T>\nvoid foo(T&& t)\n{}\nIf you call it with an rvalue as follows, then T is deduced to be the type of the value:\nfoo(42);       \nfoo(3.14159);       \nfoo(std::string());    \nBut if you call foo with an lvalue, T is deduced to be an lvalue reference:\nint i=42;\nfoo(i);    \nBecause the function parameter is declared as T&&, this is therefore a reference to a ref-\nerence, which is treated as the original reference type. The signature of foo<int&>() is\nvoid foo<int&>(int& t);\nThis allows a single function template to accept both lvalue and rvalue parameters\nand is used by the std::thread constructor (sections 2.1 and 2.2) so that the supplied\ncallable object can be moved into internal storage rather than copied if the parameter\nis an rvalue.\nA.2\nDeleted functions\nSometimes it doesn’t make sense to allow a class to be copied. std::mutex is a prime\nexample of this—what would it mean if you did copy a mutex? std::unique_lock<> is\nanother—an instance is the one and only owner of the lock it holds. To truly copy it\nwould mean that the copy also held the lock, which doesn’t make sense. Moving own-\nership between instances, as described in section A.1.2, makes sense, but that’s not\ncopying. I’m sure you’ve seen other examples.\n The standard idiom for preventing copies of a class used to be declaring the copy\nconstructor and copy assignment operator private and then not providing an imple-\nmentation. This would cause a compile error if any code outside the class in question\nCalls foo<int>(42)\nCalls foo<double>(3.14159)\nCalls foo<std::string>(std::string())\nCalls foo<int&>(i)\n",
      "page_number": 373
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 382-396)",
      "start_page": 382,
      "end_page": 396,
      "detection_method": "topic_boundary",
      "content": "359\nDeleted functions\ntried to copy an instance and a link-time error (due to lack of an implementation) if\nany of the class’s member functions or friends tried to copy an instance: \nclass no_copies\n{\npublic:\n    no_copies(){}\nprivate:\n    no_copies(no_copies const&);               \n    no_copies& operator=(no_copies const&);    \n};\nno_copies a;\nno_copies b(a);    \nWith C++11, the committee realized that this was a common idiom but also realized\nthat it’s a bit of a hack. The committee therefore provided a more general mechanism\nthat can be applied in other cases too: you can declare a function as deleted by adding =\ndelete to the function declaration. no_copies can be written as\nclass no_copies\n{\npublic:\n    no_copies(){}\n    no_copies(no_copies const&) = delete;\n    no_copies& operator=(no_copies const&) = delete;\n};\nThis is much more descriptive than the original code and clearly expresses the intent.\nIt also allows the compiler to give more descriptive error messages and moves the\nerror from link time to compile time if you try to perform the copy within a member\nfunction of your class.\n If, in addition to deleting the copy constructor and copy-assignment operator, you\nalso explicitly write a move constructor and move-assignment operator, your class\nbecomes move-only, the same as std::thread and std::unique_lock<>. The follow-\ning listing shows an example of this move-only type.\nclass move_only\n{\n    std::unique_ptr<my_class> data;\npublic:\n    move_only(const move_only&) = delete;\n    move_only(move_only&& other):\n        data(std::move(other.data))\n    {}\n    move_only& operator=(const move_only&) = delete;\n    move_only& operator=(move_only&& other)\n    {\n        data=std::move(other.data);\n        return *this;\nListing A.2\nA simple move-only type\nNo implementation\nWon’t compile\n\n\n360\nAPPENDIX A\nBrief reference for some C++11 language features\n    }\n};\nmove_only m1;\nmove_only m2(m1);            \nmove_only m3(std::move(m1));   \nMove-only objects can be passed as function parameters and returned from functions,\nbut if you want to move from an lvalue, you always have to be explicit and use\nstd::move() or a static_cast<T&&>.\n You can apply the = delete specifier to any function, not just copy constructors\nand assignment operators. This makes it clear that the function isn’t available. It does\na bit more than that too, though; a deleted function participates in overload resolu-\ntion in the normal way and only causes a compilation error if it’s selected. This can be\nused to remove specific overloads. For example, if your function takes a short param-\neter, you can prevent the narrowing of int values by writing an overload that takes an\nint and declaring it deleted:\nvoid foo(short);\nvoid foo(int) = delete;\nAny attempts to call foo with an int will now be met with a compilation error, and the\ncaller will have to explicitly cast supplied values to short:\nfoo(42);        \nfoo((short)42);    \nA.3\nDefaulted functions\nWhereas deleted functions allow you to explicitly declare that a function isn’t imple-\nmented, defaulted functions are the opposite extreme: they allow you to specify that\nthe compiler should write the function for you, with its “default” implementation. You\ncan only do this for functions that the compiler can autogenerate anyway: default con-\nstructors, destructors, copy constructors, move constructors, copy-assignment opera-\ntors, and move-assignment operators.\n Why would you want to do that? There are several reasons why you might:\n■\nIn order to change the accessibility of the function—By default, the compiler-generated\nfunctions are public. If you want to make them protected or even private,\nyou must write them yourself. By declaring them as defaulted, you can get the\ncompiler to write the function and change the access level.\n■\nAs documentation—If the compiler-generated version is sufficient, it might be\nworth explicitly declaring it as such so that when you or someone else looks at\nthe code later, it’s clear that this was intended.\n■\nIn order to force the compiler to generate the function when it would not otherwise have\ndone so—This is typically done with default constructors, which are only normally\nError; copy constructor \nis declared deleted\nOK; move constructor found\nError; int overload \ndeclared deleted\nOK\n\n\n361\nDefaulted functions\ncompiler-generated if there are no user-defined constructors. If you need to\ndefine a custom copy constructor (for example), you can still get a compiler-\ngenerated default constructor by declaring it as defaulted.\n■\nIn order to make a destructor virtual while leaving it as compiler-generated.\n■\nTo force a particular declaration of the copy constructor, such as having it take the source\nparameter by a non-const reference rather than by a const reference.\n■\nTo take advantage of the special properties of the compiler-generated function, which are\nlost if you provide an implementation—More on this in a moment.\nJust as deleted functions are declared by following the declaration with = delete,\ndefaulted functions are declared by following the declaration by = default; for example:\nclass Y\n{\nprivate:\n    Y() = default;   \npublic:\n    Y(Y&) = default;        \n    T& operator=(const Y&) = default;    \nprotected:\n    virtual ~Y() = default;   \n};\nI mentioned previously that compiler-generated functions can have special properties\nthat you can’t get from a user-defined version. The biggest difference is that a compiler-\ngenerated function can be trivial. This has a few consequences, including the following:\n■\nObjects with trivial copy constructors, trivial copy assignment operators, and\ntrivial destructors can be copied with memcpy or memmove.\n■\nLiteral types used for constexpr functions (see section A.4) must have a trivial\nconstructor, copy constructor, and destructor. \n■\nClasses with a trivial default constructor, copy constructor, copy assignment\noperator, and destructor can be used in a union with a user-defined constructor\nand destructor. \n■\nClasses with trivial copy assignment operators can be used with the std::atomic<>\nclass template (see section 5.2.6) in order to provide a value of that type with\natomic operations.\nJust declaring the function as = default doesn’t make it trivial—it will only be trivial if\nthe class also supports all the other criteria for the corresponding function to be triv-\nial—but explicitly writing the function in user code does prevent it from being trivial.\n The second difference between classes with compiler-generated functions and\nuser-supplied equivalents is that a class with no user-supplied constructors can be an\naggregate and thus can be initialized with an aggregate initializer:\nstruct aggregate\n{\n    aggregate() = default;\nChange access\nTake a non-const \nreference\nDeclare as defaulted \nfor documentation\nChange access \nand add virtual\n\n\n362\nAPPENDIX A\nBrief reference for some C++11 language features\n    aggregate(aggregate const&) = default;\n    int a;\n    double b;\n};\naggregate x={42,3.141};\nIn this case, x.a is initialized to 42 and x.b is initialized to 3.141.\n The third difference between a compiler-generated function and a user-supplied\nequivalent is quite esoteric and applies only to the default constructor and only to the\ndefault constructor of classes that meet certain criteria. Consider the following class:\nstruct X\n{\n    int a;\n};\nIf you create an instance of class X without an initializer, the contained int (a) is\ndefault initialized. If the object has static storage duration, it’s initialized to zero; other-\nwise, it has an indeterminate value that can potentially cause undefined behavior if it’s\naccessed before being assigned a new value:\nX x1;     \nIf, on the other hand, you initialize your instance of X by explicitly invoking the\ndefault constructor, then a is initialized to zero:\nX x2=X();     \nThis bizarre property also extends to base classes and members. If your class has a\ncompiler-generated default constructor and any of your data members and base\nclasses also have a compiler-generated default constructor, data members of those\nbases and members that are built-in types are also either left with an indeterminate\nvalue or initialized to zero, depending on whether or not the outer class has its default\nconstructor explicitly invoked.\n Although this rule is confusing and potentially error-prone, it does have its uses,\nand if you write the default constructor yourself, you lose this property; either data\nmembers like a are always initialized (because you specify a value or explicitly default\nconstruct) or always uninitialized (because you don’t):\nX::X():a(){}    \nX::X():a(42){}   \nX::X(){}      \nIf you omit the initialization of a from the constructor of X as in the third example B,\nthen a is left uninitialized for nonstatic instances of X and initialized to zero for\ninstances of X with static storage duration.\nx1.a has an \nindeterminate value.\nx2.a==0\na==0 always.\na==42 always.\nb\n\n\n363\nconstexpr functions\n Under normal circumstances, if you write any other constructor manually, the\ncompiler will no longer generate the default constructor for you, so if you want one\nyou have to write it, which means you lose this bizarre initialization property. But by\nexplicitly declaring the constructor as defaulted, you can force the compiler to gener-\nate the default constructor for you, and this property is retained:\nX::X() = default;     \nThis property is used for the atomic types (see section 5.2), which have their default\nconstructor explicitly defaulted. Their initial value is always undefined unless either\n(a) they have static storage duration (and thus are statically initialized to zero), (b)\nyou explicitly invoke the default constructor to request zero initialization, or (c) you\nexplicitly specify a value. Note that in the case of the atomic types, the constructor for\ninitialization with a value is declared constexpr (see section A.4) in order to allow\nstatic initialization.\nA.4\nconstexpr functions\nInteger literals such as 42 are constant expressions, as are simple arithmetic expressions\nsuch as 23*2-4. You can even use const variables of integral type that are themselves\ninitialized with constant expressions as part of a new constant expression:\nconst int i=23;\nconst int two_i=i*2;\nconst int four=4;\nconst int forty_two=two_i-four;\nAside from using constant expressions to create variables that can be used in other\nconstant expressions, there are a few things you can only do with constant expressions:\n■\nSpecify the bounds of an array:\nint bounds=99;\nint array[bounds];      \nconst int bounds2=99;\nint array2[bounds2];     \n■\nSpecify the value of a nontype template parameter:\ntemplate<unsigned size>\nstruct test\n{};\ntest<bounds> ia;       \ntest<bounds2> ia2;         \n■\nProvide an initializer for a static const class data member of integral type in\nthe class definition:\nDefault initialization \nrules for a apply\nError bounds is not a \nconstant expression\nOK, bounds2 is a \nconstant expression.\nError bounds is not a \nconstant expression\nOK, bounds2 is a \nconstant expression.\n\n\n364\nAPPENDIX A\nBrief reference for some C++11 language features\nclass X\n{\n    static const int the_answer=forty_two;\n};\n■\nProvide an initializer for a built-in type or aggregate that can be used for static\ninitialization:\nstruct my_aggregate\n{\n    int a;\n    int b;\n};\nstatic my_aggregate ma1={forty_two,123};    \nint dummy=257;\nstatic my_aggregate ma2={dummy,dummy};     \n■\nStatic initialization like this can be used to avoid order-of-initialization prob-\nlems and race conditions.\nNone of this is new—you could do all that with the 1998 edition of the C++ Stan-\ndard. But with the C++11 Standard what constitutes a constant expression has been\nextended with the introduction of the constexpr keyword. The C++14 and C++17\nstandards extend the constexpr facility further; a full primer is beyond the scope of\nthis appendix.\n The constexpr keyword is primarily a function modifier. If the parameter and\nreturn type of a function meet certain requirements and the body is sufficiently sim-\nple, a function can be declared constexpr, in which case it can be used in constant\nexpressions; for example:\nconstexpr int square(int x)\n{\n    return x*x;\n}\nint array[square(5)];\nIn this case, array will have 25 entries, because square is declared constexpr. Just\nbecause the function can be used in a constant expression doesn’t mean that all uses\nare automatically constant expressions:\nint dummy=4;\nint array[square(dummy)];     \nIn this example, dummy is not a constant expression B, so square(dummy) isn’t either—\nit’s a normal function call—and thus can’t be used to specify the bounds of array.\nStatic initialization\nDynamic \ninitialization\nError, dummy is not a \nconstant expression\nb\n\n\n365\nconstexpr functions\nA.4.1\nconstexpr and user-defined types\nUp to now, all the examples have been with built-in types such as int. But the new C++\nStandard allows constant expressions to be of any type that satisfies the requirements\nfor a literal type. For a class type to be classified as a literal type, the following must all\nbe true:\n■\nIt must have a trivial copy constructor.\n■\nIt must have a trivial destructor.\n■\nAll non-static data members and base classes must be trivial types.\n■\nIt must have either a trivial default constructor or a constexpr constructor\nother than the copy constructor.\nWe’ll look at constexpr constructors shortly. For now, we’ll focus on classes with a triv-\nial default constructor, such as class CX in the following listing.\nclass CX\n{\nprivate:\n    int a;\n    int b;\npublic:\n    CX() = default;    \n    CX(int a_, int b_):    \n        a(a_),b(b_)\n    {}\n    int get_a() const\n    {\n        return a;\n    }\n    int get_b() const\n    {\n        return b;\n    }\n    int foo() const\n    {\n        return a+b;\n    }\n};\nNote that we’ve explicitly declared the default constructor B as defaulted (see section\nA.3) in order to preserve it as trivial in the face of the user-defined constructor c.\nThis type therefore fits all the qualifications for being a literal type, and you can use it\nin constant expressions. You can, for example, provide a constexpr function that cre-\nates new instances:\nconstexpr CX create_cx()\n{\n    return CX();\n}\nListing A.3\nA class with a trivial default constructor\nB\nc\n\n\n366\nAPPENDIX A\nBrief reference for some C++11 language features\nYou can also create a simple constexpr function that copies its parameter:\nconstexpr CX clone(CX val)\n{\n    return val;\n}\nBut that’s about all you can do in C++11—a constexpr function can only call other\nconstexpr functions. In C++14, this restriction is lifted, and you can do almost any-\nthing in a constexpr function, provided it doesn’t modify any objects with non-local\nscope. What you can do, even in C++11, is apply constexpr to the member functions\nand constructor of CX:\nclass CX\n{\nprivate:\n    int a;\n    int b;\npublic:\n    CX() = default;\n    constexpr CX(int a_, int b_):\n        a(a_),b(b_)\n    {}\n    constexpr int get_a() const    \n    {\n        return a;\n    }\n    constexpr int get_b()    \n    {\n        return b;\n    }\n    constexpr int foo()\n    {\n        return a+b;\n    }\n};\nIn C++11, the const qualification on get_a() B is now superfluous, because it’s\nimplied by the use of constexpr, and get_b() is thus const even though the const\nqualification is omitted c. In C++14, this is changed (due to the extended capabilities\nof constexpr functions), so get_b() is no longer implicitly const. This now allows\nmore complex constexpr functions such as the following:\nconstexpr CX make_cx(int a)\n{\n    return CX(a,1);\n}\nconstexpr CX half_double(CX old)\n{\n    return CX(old.get_a()/2,old.get_b()*2);\n}\nconstexpr int foo_squared(CX val)\nb\nc\n\n\n367\nconstexpr functions\n{\n    return square(val.foo());\n}\nint array[foo_squared(half_double(make_cx(10)))];   \nInteresting though this is, it’s a lot of effort to go to if all you get is a fancy way of com-\nputing some array bounds or an integral constant. The key benefit of constant expres-\nsions and constexpr functions involving user-defined types is that objects of a literal\ntype initialized with a constant expression are statically initialized, and so their initial-\nization is free from race conditions and initialization order issues:\nCX si=half_double(CX(42,19));    \nThis covers constructors too. If the constructor is declared constexpr and the con-\nstructor parameters are constant expressions, the initialization is constant initialization\nand happens as part of the static initialization phase. This is one of the most important\nchanges in C++11 as far as concurrency goes: by allowing user-defined constructors that\ncan still undergo static initialization, you can avoid any race conditions over their ini-\ntialization, because they’re guaranteed to be initialized before any code is run.\n This is particularly relevant for things like std::mutex (see section 3.2.1) or\nstd::atomic<> (see section 5.2.6) where you might want to use a global instance to\nsynchronize access to other variables and avoid race conditions in that access. This\nwouldn’t be possible if the constructor of the mutex was subject to race conditions, so\nthe default constructor of std::mutex is declared constexpr to ensure that mutex ini-\ntialization is always done as part of the static initialization phase.\nA.4.2\nconstexpr objects\nSo far, we’ve looked at constexpr as applied to functions. constexpr can also be\napplied to objects. This is primarily for diagnostic purposes; it verifies that the object\nis initialized with a constant expression, constexpr constructor, or aggregate initial-\nizer made of constant expressions. It also declares the object as const:\nconstexpr int i=45;              \n    \nconstexpr std::string s(“hello”);      \nint foo();\nconstexpr int j=foo();   \nA.4.3\nconstexpr function requirements\nIn order to declare a function as constexpr it must meet a few requirements; if it\ndoesn’t meet these requirements, declaring it constexpr is a compilation error. In\nC++11, the requirements for a constexpr function were as follows:\n■\nAll parameters must be of a literal type.\n■\nThe return type must be a literal type.\n■\nThe function body must consist of a single return statement.\n49 elements\nStatically initialized\nOK\nError; std::string \nisn’t a literal type\nError; foo() isn’t \ndeclared constexpr\n\n\n368\nAPPENDIX A\nBrief reference for some C++11 language features\n■\nThe expression in the return statement must qualify as a constant expression.\n■\nAny constructor or conversion operator used to construct the return value from\nthe expression must be constexpr.\nThis is straightforward; you must be able to inline the function into a constant expres-\nsion and it will still be a constant expression, and you must not modify anything.\nconstexpr functions are pure functions with no side effects.\n In C++14, the requirements were slackened quite considerably. Though the overall\nidea of a pure function with no side effects is preserved, the body is allowed to contain\nconsiderably more:\n■\nMultiple return statements are allowed.\n■\nObjects created within the function can be modified.\n■\nLoops, conditionals, and switch statements are allowed.\nFor constexpr class member functions there are additional requirements:\n■\nconstexpr member functions can’t be virtual.\n■\nThe class for which the function is a member must be a literal type.\nThe rules are different for constexpr constructors:\n■\nThe constructor body must be empty for a C++11 compiler; for a C++14 or later\ncompiler it must satisfy the requirements for a constexpr function.\n■\nEvery base class must be initialized.\n■\nEvery non-static data member must be initialized.\n■\nAny expressions used in the member initialization list must qualify as constant\nexpressions.\n■\nThe constructors chosen for the initialization of the data members and base\nclasses must be constexpr constructors.\n■\nAny constructor or conversion operator used to construct the data members\nand base classes from their corresponding initialization expression must be\nconstexpr.\nThis is the same set of rules as for functions, except that there’s no return value, so no\nreturn statement. Instead, the constructor initializes all the bases and data members\nin the member initialization list. Trivial copy constructors are implicitly constexpr.\nA.4.4\nconstexpr and templates\nWhen constexpr is applied to a function template, or to a member function of a class\ntemplate, it’s ignored if the parameters and return types of a particular instantiation\nof the template aren’t literal types. This allows you to write function templates that are\nconstexpr if the type of the template parameters is appropriate and just plain inline\nfunctions otherwise, for example:\ntemplate<typename T>\nconstexpr T sum(T a,T b)\n\n\n369\nLambda functions\n{\n    return a+b;\n}\nconstexpr int i=sum(3,42);    \nstd::string s=\n    sum(std::string(\"hello\"),\n        std::string(\" world\"));     \nThe function must satisfy all the other requirements for a constexpr function. You\ncan’t declare a function with multiple statements constexpr just because it’s a func-\ntion template; that’s still a compilation error.\nA.5\nLambda functions\nLambda functions are one of the most exciting features of the C++11 Standard,\nbecause they have the potential to greatly simplify code and eliminate much of the\nboilerplate associated with writing callable objects. The C++11 lambda function syntax\nallows a function to be defined at the point where it’s needed in another expression.\nThis works well for things like predicates provided to the wait functions of std::\ncondition_variable (as in the example in section 4.1.1), because it allows the seman-\ntics to be quickly expressed in terms of the accessible variables rather than capturing\nthe necessary state in the member variables of a class with a function call operator.\n At its simplest, a lambda expression defines a self-contained function that takes no\nparameters and relies only on global variables and functions. It doesn’t even have to\nreturn a value. This lambda expression is a series of statements enclosed in brackets,\nprefixed with square brackets (the lambda introducer):\n[]{               \n    do_stuff();\n    do_more_stuff();\n}();               \nIn this example, the lambda expression is called by following it with parentheses, but\nthis is unusual. For one thing, if you’re going to call it directly, you could usually do\naway with the lambda and write the statements directly in the source. It’s more com-\nmon to pass it as a parameter to a function template that takes a callable object as one\nof its parameters, in which case it likely needs to take parameters or return a value or\nboth. If you need to take parameters, you can do this by following the lambda intro-\nducer with a parameter list like for a normal function. For example, the following\ncode writes all the elements of the vector to std::cout separated by newlines:\nstd::vector<int> data=make_data();\nstd::for_each(data.begin(),data.end(),[](int i){std::cout<<i<<\"\\n\";});\nReturn values are almost as easy. If your lambda function body consists of a single\nreturn statement, the return type of the lambda is the type of the expression being\nOK; sum<int> \nis constexpr.\nOK, but sum<std::string> \nisn’t constexpr.\nStart the lambda \nexpression with [].\nFinish the lambda, \nand call it.\n\n\n370\nAPPENDIX A\nBrief reference for some C++11 language features\nreturned. For example, you might use a simple lambda like this to wait for a flag to be\nset with std::condition_variable (see section 4.1.1), as in the following listing.\nstd::condition_variable cond;\nbool data_ready;\nstd::mutex m;\nvoid wait_for_data()\n{\n    std::unique_lock<std::mutex> lk(m);\n    cond.wait(lk,[]{return data_ready;});    \n}\nThe return type of the lambda passed to cond.wait() B is deduced from the type of\ndata_ready and is thus bool. Whenever the condition variable wakes from waiting, it\nthen calls the lambda with the mutex locked and only returns from the call to wait()\nonce data_ready is true.\n What if you can’t write your lambda body as a single return statement? In that case\nyou have to specify the return type explicitly. You can do this even if your body is a sin-\ngle return statement, but you have to do it if your lambda body is more complex. The\nreturn type is specified by following the lambda parameter list with an arrow (->) and\nthe return type. If your lambda doesn’t take any parameters, you must still include the\n(empty) parameter list in order to specify the return value explicitly. Your condition\nvariable predicate can be written\ncond.wait(lk,[]()->bool{return data_ready;});\nBy specifying the return type, you can expand the lambda to log messages or do some\nmore complex processing:\ncond.wait(lk,[]()->bool{\n    if(data_ready)\n    {\n        std::cout<<”Data ready”<<std::endl;\n        return true;\n    }\n    else\n    {\n        std::cout<<”Data not ready, resuming wait”<<std::endl;\n        return false;\n    }\n});\nAlthough simple lambdas like this are powerful and can simplify code quite a lot, the\nreal power of lambdas comes when they capture local variables.\nListing A.4\nA simple lambda with a deduced return type\nb\n\n\n371\nLambda functions\nA.5.1\nLambda functions that reference local variables\nLambda functions with a lambda introducer of [] can’t reference any local variables\nfrom the containing scope; they can only use global variables and anything passed in\nas a parameter. If you want to access a local variable, you need to capture it. The sim-\nplest way to do this is to capture the entire set of variables within the local scope by\nusing a lambda introducer of [=]. That’s all there is to it—your lambda can now\naccess copies of the local variables at the time the lambda was created.\n To see this in action, consider the following simple function:\nstd::function<int(int)> make_offseter(int offset)\n{\n   return [=](int j){return offset+j;};\n}\nEvery call to make_offseter returns a new lambda function object through the\nstd::function<> function wrapper. This returned function adds the supplied offset\nto any parameter supplied. For example,\nint main()\n{\n    std::function<int(int)> offset_42=make_offseter(42);\n    std::function<int(int)> offset_123=make_offseter(123);\n    std::cout<<offset_42(12)<<”,“<<offset_123(12)<<std::endl;\n    std::cout<<offset_42(12)<<”,“<<offset_123(12)<<std::endl;\n}\nwill write out 54,135 twice because the function returned from the first call to make_\noffseter always adds 42 to the supplied argument, whereas the function returned\nfrom the second call to make_offseter always adds 123 to the supplied argument.\n This is the safest form of local variable capture; everything is copied, so you can\nreturn the lambda and call it outside the scope of the original function. It’s not the\nonly choice though; you can choose to capture everything by reference instead. In this\ncase it’s undefined behavior to call the lambda once the variables it references have\nbeen destroyed by exiting the function or block scope to which they belong, just as it’s\nundefined behavior to reference a variable that has already been destroyed in any\nother circumstance.\n A lambda function that captures all the local variables by reference is introduced\nusing [&], as in the following example:\nint main()\n{\n    int offset=42;     \n    std::function<int(int)> offset_a=[&](int j){return offset+j;};    \n    offset=123;                                                   \n    std::function<int(int)> offset_b=[&](int j){return offset+j;};    \n    std::cout<<offset_a(12)<<”,”<<offset_b(12)<<std::endl;         \n    offset=99;                                             \n    std::cout<<offset_a(12)<<”,”<<offset_b(12)<<std::endl;    \n}\nB\nc\nd\ne\nf\ng\nh\n\n\n372\nAPPENDIX A\nBrief reference for some C++11 language features\nWhereas in the make_offseter function from the previous example you used the [=]\nlambda introducer to capture a copy of the offset, the offset_a function in this exam-\nple uses the [&] lambda introducer to capture offset by reference c. It doesn’t mat-\nter that the initial value of offset is 42 B; the result of calling offset_a(12) will\nalways depend on the current value of offset. Even though the value of offset is\nthen changed to 123 d, before you produce the second (identical) lambda function,\noffset_b e, this second lambda again captures by reference, so the result depends\non the current value of offset. \n Now, when you print the first line of output f, offset is still 123, so the output is\n135,135. But at the second line of output h, offset has been changed to 99 g, so\nthis time the output is 111,111. Both offset_a and offset_b add the current value of\noffset (99) to the supplied argument (12).\n Now, C++ being C++, you’re not stuck with these all-or-nothing options; you\ncan choose to capture some variables by copy and some by reference, and you can\nchoose to capture only those variables you have explicitly chosen by tweaking the\nlambda introducer. If you want to copy all the used variables except for one or two,\nyou can use the [=] form of the lambda introducer but follow the equals sign with a\nlist of variables to capture by reference preceded with ampersands. The following\nexample will print 1239, because i is copied into the lambda, but j and k are cap-\ntured by reference:\nint main()\n{\n    int i=1234,j=5678,k=9;\n    std::function<int()> f=[=,&j,&k]{return i+j+k;};\n    i=1;\n    j=2;\n    k=3;\n    std::cout<<f()<<std::endl;\n}\nAlternatively, you can capture by reference by default but capture a specific subset of\nvariables by copying. In this case, you use the [&] form of the lambda introducer but\nfollow the ampersand with a list of variables to capture by copy. The following exam-\nple prints 5688 because i is captured by reference, but j and k are copied:\nint main()\n{\n    int i=1234,j=5678,k=9;\n    std::function<int()> f=[&,j,k]{return i+j+k;};\n    i=1;\n    j=2;\n    k=3;\n    std::cout<<f()<<std::endl;\n}\nIf you only want to capture the named variables, then you can omit the leading = or &\nand just list the variables to be captured, prefixing them with an ampersand to capture\n\n\n373\nLambda functions\nby reference rather than copy. The following code will print 5682 because i and k are\ncaptured by reference, but j is copied:\nint main()\n{\n    int i=1234,j=5678,k=9;\n    std::function<int()> f=[&i,j,&k]{return i+j+k;};\n    i=1;\n    j=2;\n    k=3;\n    std::cout<<f()<<std::endl;\n}\nThis final variant allows you to ensure that only the intended variables are being cap-\ntured, because any reference to a local variable not in the capture list will cause a com-\npilation error. If you choose this option, you have to be careful when accessing class\nmembers if the function containing the lambda is a member function. Class members\ncan’t be captured directly; if you want to access class members from your lambda, you\nhave to capture the this pointer by adding it to the capture list. In the following\nexample, the lambda captures this to allow access to the some_data class member:\nstruct X\n{\n    int some_data;\n    void foo(std::vector<int>& vec)\n    {\n        std::for_each(vec.begin(),vec.end(),\n            [this](int& i){i+=some_data;});\n    }\n};\nIn the context of concurrency, lambdas are most useful as predicates for std::condition\n_variable::wait() (section 4.1.1) and with std::packaged_task<> (section 4.2.1)\nor thread pools for packaging small tasks. They can also be passed to the std::thread\nconstructor as a thread function (section 2.1.1) and as the function when using paral-\nlel algorithms such as parallel_for_each() (from section 8.5.1).\n Since C++14, lambdas can also be generic lamdas, where the parameter types are\ndeclared as auto rather than a specified type. In this case, the function call operator is\nimplicitly a template, and the type of the parameter is deduced from the supplied\nargument when the lambda is invoked; for example:\nauto f=[](auto x){ std::cout<<”x=”<<x<<std::endl;};\nf(42); // x is of type int; outputs “x=42”\nf(“hello”); // x is of type const char*; outputs “x=hello”\nC++14 also adds the concept of generalized captures, so you can capture the results of\nexpressions, rather than a direct copy of or reference to a local variable. Most com-\nmonly this can be used to capture move-only types by moving them, rather than hav-\ning to capture by reference; for example:\n",
      "page_number": 382
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 397-404)",
      "start_page": 397,
      "end_page": 404,
      "detection_method": "topic_boundary",
      "content": "374\nAPPENDIX A\nBrief reference for some C++11 language features\nstd::future<int> spawn_async_task(){\n    std::promise<int> p;\n    auto f=p.get_future();\n    std::thread t([p=std::move(p)](){ p.set_value(find_the_answer());});\n    t.detach();\n    return f;\n}\nHere, the promise is moved into the lambda by the p=std::move(p) generalized cap-\nture, so it is safe to detach the thread, without the worry of a dangling reference to a\nlocal variable that has been destroyed. After the construction of the lambda, the origi-\nnal p is now in a moved-from state, which is why you had to get the future beforehand.\nA.6\nVariadic templates\nVariadic templates are templates with a variable number of parameters. Just as you’ve\nalways been able to have variadic functions, such as printf, that take a variable num-\nber of parameters, you can now have variadic templates that have a variable number of\ntemplate parameters. Variadic templates are used throughout the C++ Thread Library.\nFor example, the std::thread constructor for starting a thread (section 2.1.1) is a\nvariadic function template, and std::packaged_task<> (section 4.2.2) is a variadic\nclass template. From a user’s point of view, it’s enough to know that the template takes\nan unbounded number of parameters, but if you want to write this template, or if\nyou’re interested in how it all works, you need to know the details.\n Just as variadic functions are declared with an ellipsis (...) in the function param-\neter list, variadic templates are declared with an ellipsis in the template parameter list:\ntemplate<typename ... ParameterPack>\nclass my_template\n{};\nYou can use variadic templates for a partial specialization of a template too, even if the\nprimary template isn’t variadic. For example, the primary template for std::packaged\n_task<> (section 4.2.1) is a simple template with a single template parameter:\ntemplate<typename FunctionType>\nclass packaged_task;\nBut this primary template is never defined anywhere; it’s a placeholder for the partial\nspecialization:\ntemplate<typename ReturnType,typename ... Args>\nclass packaged_task<ReturnType(Args...)>;\nIt’s this partial specialization that contains the real definition of the class; you saw in\nchapter 4 that you can write std::packaged_task<int(std::string,double)> to\ndeclare a task that takes an std::string and a double as parameters when you call it\nand that provides the result through an std::future<int>.\n\n\n375\nVariadic templates\n This declaration shows two additional features of variadic templates. The first\nfeature is relatively simple: you can have normal template parameters (such as\nReturnType) as well as variadic ones (Args) in the same declaration. The second fea-\nture demonstrated is the use of Args... in the template argument list of the special-\nization to show that the types that make up Args when the template is instantiated\nare to be listed here. Because this is a partial specialization, it works as a pattern\nmatch; the types that occur in this context in the instantiation are captured as Args.\nThe variadic parameter Args is called a parameter pack, and the use of Args... is\ncalled a pack expansion.\n Like with variadic functions, the variadic part may be an empty list or may have\nmany entries. For example, with std::packaged_task<my_class()> the ReturnType\nparameter is my_class, and the Args parameter pack is empty, whereas with\nstd::packaged_task<void(int,double,my_class&,std::string*)> the ReturnType\nis void, and Args is the list int, double, my_class&, std::string*.\nA.6.1\nExpanding the parameter pack\nThe power of variadic templates comes from what you can do with that pack expan-\nsion: you aren’t limited to expanding the list of types as is. First off, you can use a pack\nexpansion directly anywhere a list of types is required, such as in the argument list for\nanother template:\ntemplate<typename ... Params>\nstruct dummy\n{\n    std::tuple<Params...> data;\n};\nIn this case the single member variable data is an instantiation of std::tuple<>\ncontaining all the types specified, so dummy<int,double,char> has a member of\ntype std::tuple<int,double,char>. You can combine pack expansions with nor-\nmal types:\ntemplate<typename ... Params>\nstruct dummy2\n{\n    std::tuple<std::string,Params...> data;\n};\nThis time, the tuple has an additional (first) member of type std::string. The nifty\npart is that you can create a pattern with the pack expansion, which is then copied for\neach element in the expansion. You do this by putting the ... that marks the pack\nexpansion at the end of the pattern. For example, rather than just creating a tuple of\nthe elements supplied in your parameter pack, you can create a tuple of pointers to\nthe elements or even a tuple of std::unique_ptr<> to your elements:\ntemplate<typename ... Params>\nstruct dummy3\n\n\n376\nAPPENDIX A\nBrief reference for some C++11 language features\n{\n    std::tuple<Params* ...> pointers;\n    std::tuple<std::unique_ptr<Params> ...> unique_pointers;\n};\nThe type expression can be as complex as you like, provided the parameter pack\noccurs in the type expression, and provided the expression is followed by the ... that\nmarks the expansion. When the parameter pack is expanded, for each entry in the pack\nthat type is substituted into the type expression to generate the corresponding entry in\nthe resulting list. If your parameter pack Params contains the types int,int,char, then\nthe expansion of std::tuple<std::pair<std::unique_ptr<Params>,double> ... > is\nstd::tuple<std::pair<std::unique_ptr<int>,double>, std::pair<std::unique_\nptr<int>,double>, std::pair<std::unique_ptr<char>,double> >. If the pack expan-\nsion is used as a template argument list, that template doesn’t have to have variadic\nparameters, but if it doesn’t, the size of the pack must exactly match the number of\ntemplate parameters required:\ntemplate<typename ... Types>\nstruct dummy4\n{\n    std::pair<Types...> data;\n};\ndummy4<int,char> a;           \ndummy4<int> b;                \ndummy4<int,int,int> c;         \nThe second thing you can do with a pack expansion is use it to declare a list of func-\ntion parameters:\ntemplate<typename ... Args>\nvoid foo(Args ... args);\nThis creates a new parameter pack, args, which is a list of the function parameters\nrather than a list of types, which you can expand with ... as before. Now, you can use\na pattern with the pack expansion for declaring the function parameters, just as you\ncan use a pattern when you expand the pack elsewhere. For example, this is used by\nthe std::thread constructor to take all the function arguments by rvalue reference\n(see section A.1):\ntemplate<typename CallableType,typename ... Args>\nthread::thread(CallableType&& func,Args&& ... args);\nThe function parameter pack can then be used to call another function, by specifying\nthe pack expansion in the argument list of the called function. As with the type expan-\nsions, you can use a pattern for each expression in the resulting argument list. For\nexample, one common idiom with rvalue references is to use std::forward<> to pre-\nserve the rvalue-ness of the supplied function arguments:\nOK, data is \nstd::pair<int,char>.\nError; no second type.\nError; too many types.\n\n\n377\nAutomatically deducing the type of a variable\ntemplate<typename ... ArgTypes>\nvoid bar(ArgTypes&& ... args)\n{\n    foo(std::forward<ArgTypes>(args)...);\n}\nNote that in this case, the pack expansion contains both the type pack ArgTypes and\nthe function parameter pack args, and the ellipsis follows the whole expression. If you\ncall bar like this,\nint i;\nbar(i,3.141,std::string(\"hello \"));\nthen the expansion becomes\ntemplate<>\nvoid bar<int&,double,std::string>(\n    int& args_1,\n    double&& args_2,\n    std::string&& args_3)\n{\n    foo(std::forward<int&>(args_1),\n        std::forward<double>(args_2),\n        std::forward<std::string>(args_3));\n}\nwhich correctly passes the first argument on to foo as an lvalue reference, while pass-\ning the others as rvalue references.\n The final thing you can do with a parameter pack is find its size with the sizeof...\noperator. This is quite simple: sizeof...(p) is the number of elements in the param-\neter pack p. It doesn’t matter whether this is a type parameter pack or a function argu-\nment parameter pack; the result is the same. This is probably the only case where you\ncan use a parameter pack and not follow it with an ellipsis; the ellipsis is already part\nof the sizeof... operator. The following function returns the number of arguments\nsupplied to it:\ntemplate<typename ... Args>\nunsigned count_args(Args ... args)\n{\n    return sizeof... (Args);\n}\nAs with the normal sizeof operator, the result of sizeof... is a constant expression,\nso it can be used for specifying array bounds and so forth.\nA.7\nAutomatically deducing the type of a variable\nC++ is a statically typed language: the type of every variable is known at compile time.\nNot only that, but as a programmer you have to specify the type of each variable. In\nsome cases this can lead to quite unwieldy names; for example:\n\n\n378\nAPPENDIX A\nBrief reference for some C++11 language features\nstd::map<std::string,std::unique_ptr<some_data>> m;\nstd::map<std::string,std::unique_ptr<some_data>>::iterator\n    iter=m.find(\"my key\");\nTraditionally, the solution has been to use typedefs to reduce the length of a type\nidentifier and potentially eliminate problems due to inconsistent types. This still\nworks in C++11, but there’s now a new way: if a variable is initialized in its declaration\nfrom a value of the same type, then you can specify the type as auto. In this case, the\ncompiler will automatically deduce the type of the variable to be the same as the ini-\ntializer. The iterator example can be written as\nauto iter=m.find(\"my key\");\nNow, you’re not restricted to plain auto; you can embellish it to declare const vari-\nables or pointer or reference variables too. Here are a few variable declarations using\nauto and the corresponding type of the variable:\nauto i=42;        // int\nauto& j=i;        // int&\nauto const k=i;   // int const\nauto* const p=&i; // int * const\nThe rules for deducing the type of the variable are based on the rules for the only\nother place in the language where types are deduced: parameters of function tem-\nplates. In a declaration of the form\nsome-type-expression-involving-auto var=some-expression;\nthe type of var is the same as the type deduced for the parameter of a function tem-\nplate declared with the same type expression, except replacing auto with the name of\na template type parameter:\ntemplate<typename T>\nvoid f(type-expression var);\nf(some-expression);\nThis means that array types decay to pointers, and references are dropped unless the\ntype expression explicitly declares the variable as a reference; for example:\nint some_array[45];\nauto p=some_array;   // int*\nint& r=*p;\nauto x=r;            // int\nauto& y=r;           // int&\nThis can greatly simplify the declaration of variables, particularly where the full type\nidentifier is long or possibly not even known (for example, the type of the result of a\nfunction call in a template).\n\n\n379\nThread-local variables\nA.8\nThread-local variables\nThread-local variables allow you to have a separate instance of a variable for each thread\nin your program. You mark a variable as being thread-local by declaring it with the\nthread_local keyword. Variables at namespace scope, static data members of classes,\nand local variables can be declared thread-local, and are said to have thread storage\nduration:\nthread_local int x;    \nclass X\n{\n    static thread_local std::string s;       \n};\nstatic thread_local std::string X::s;     \nvoid foo()\n{\n    thread_local std::vector<int> v;    \n}\nThread-local variables at namespace scope and thread-local static class data members\nare constructed before the first use of a thread-local variable from the same transla-\ntion unit, but it isn’t specified how much before. Some implementations may construct\nthread-local variables when the thread is started; others may construct them immedi-\nately before their first use on each thread, and others may construct them at other\ntimes, or in some combination depending on their usage context. Indeed, if none of\nthe thread-local variables from a given translation unit is used, there’s no guarantee\nthat they will be constructed at all. This allows for the dynamic loading of modules\ncontaining thread-local variables—these variables can be constructed on a given\nthread the first time that thread references a thread-local variable from the dynami-\ncally-loaded module.\n Thread-local variables declared inside a function are initialized the first time the\nflow of control passes through their declaration on a given thread. If the function is\nnot called by a given thread, any thread-local variables declared in that function are\nnot constructed. This is the same as the behavior for local static variables, except it\napplies separately to each thread.\n Thread-local variables share other properties with static variables—they’re zero-\ninitialized prior to any further initialization (such as dynamic initialization), and if the\nconstruction of a thread-local variable throws an exception, std::terminate() is called\nto abort the application.\n The destructors for all thread-local variables that have been constructed on a given\nthread are run when the thread function returns, in the reverse order of construction.\nBecause the order of initialization is unspecified, it’s important to ensure that there\nare no interdependencies between the destructors of these variables. If the destructor\nof a thread-local variable exits with an exception, std::terminate() is called, as for\nconstruction.\nA thread-local variable \nat namespace scope\nA thread-local static \nclass data member\nThe definition of X::s is \nrequired.\nA thread-local \nlocal variable\n\n\n380\nAPPENDIX A\nBrief reference for some C++11 language features\n Thread-local variables are also destroyed for a thread if that thread calls\nstd::exit() or returns from main() (which is equivalent to calling std::exit() with\nthe return value of main()). If any other threads are still running when the applica-\ntion exits, the destructors of thread-local variables on those threads are not called.\n Though thread-local variables have a different address on each thread, you can\nstill obtain a normal pointer to this variable. The pointer then references the object in\nthe thread that took the address, and can be used to allow other threads to access that\nobject. It’s undefined behavior to access an object after it’s been destroyed (as always),\nso if you pass a pointer to a thread-local variable to another thread, you need to\nensure it’s not dereferenced once the owning thread has finished.\nA.9\nClass Template Argument Deduction\nC++17 extends the idea of automatically deducing types to template parameters: if you\nare declaring an object of a templated type, then in many cases the type of the tem-\nplate parameters can be deduced from the object initializer.\n Specifically, if an object is declared with the name of a class template, without spec-\nifying a template argument list, then constructors specified in the class template are\nused to deduce the template arguments from the object's initializer, as per the normal\ntype deduction rules for function templates.\n For example, std::lock_guard takes a single template parameter, which is the\ntype of the mutex. The constructor also takes a single parameter, which is a reference\nto that type. If you declare an object to be of type std::lock_guard, then the type\nparameter can be deduced from the type of the supplied mutex:\nstd::mutex m;\nstd::lock_guard guard(m); // deduces std::lock_guard<std::mutex>\nThe same applies to std::scoped_lock, except that it has multiple template parame-\nters, which can be deduced from multiple mutex arguments:\nstd::mutex m1;\nstd::shared_mutex m2;\nstd::scoped_lock guard(m1,m2);\n// deduces std::scoped_lock<std::mutex,std::shared_mutex>\nFor those templates where the constructors would lead to the wrong types being\ndeduced, the template author can write explicit deduction guides to ensure the cor-\nrect types are deduced. But these are beyond the scope of this book.\nSummary\nThis appendix has only scratched the surface of the new language features introduced\nwith the C++11 Standard, because we’ve only looked at those features that actively\naffect the usage of the Thread Library. Other new language features include static\nassertions, strongly typed enumerations, delegating constructors, Unicode support,\ntemplate aliases, and a new uniform initialization sequence, along with a host of\n\n\n381\nSummary\nsmaller changes. Describing all the new features in detail is outside the scope of this\nbook; it would probably require a book in itself. There are also a considerable number\nof changes added with C++14 and C++17, but again these are outside the scope of this\nbook. The best overview of the entire set of changes to the standard at the time of writ-\ning is probably the documentation at cppreference.com,1 as well as Bjarne Strous-\ntrup’s C++11 FAQ,2 though popular C++ reference books will be revised to cover it in\ndue course.\n Hopefully the brief introduction to the new features covered in this appendix has\nprovided enough depth to show how they relate to the Thread Library and to enable\nyou to write and understand multithreaded code that uses these new features.\nAlthough this appendix should provide enough depth for simple uses of the features\ncovered, this is still only a brief introduction and not a complete reference or tutorial\nfor the use of these features. If you intend to make extensive use of them, I recom-\nmend acquiring a reference or tutorial in order to gain the most benefit from them. \n1 http://www.cppreference.com\n2 http://www.research.att.com/~bs/C++0xFAQ.html\n",
      "page_number": 397
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 405-412)",
      "start_page": 405,
      "end_page": 412,
      "detection_method": "topic_boundary",
      "content": "382\nappendix B\nBrief comparison of\nconcurrency libraries\nConcurrency and multithreading support in programming languages and libraries\naren’t something new, even though standardized support in C++ is new. For exam-\nple, Java has had multithreading support since it was first released, platforms that\nconform to the POSIX standard provide a C interface for multithreading, and\nErlang provides support for message-passing concurrency. There are even C++ class\nlibraries, such as Boost, that wrap the underlying programming interface for multi-\nthreading used on any given platform (whether it’s the POSIX C interface or some-\nthing else) to provide a portable interface across the supported platforms.\n For those who are already experienced in writing multithreaded applications\nand would like to use that experience to write code using the new C++ multithread-\ning facilities, this appendix provides a comparison between the facilities available in\nJava, POSIX C, C++ with the Boost Thread Library, and C++11, along with cross-\nreferences to the relevant chapters of this book.\n \n \n \n \n \n \n \n \n \n \n \n\n\n383\nFeature\nJava\nPOSIX C\nBoost threads\nC++11\nChapter\nreference\nStarting\nthreads\njava.lang.thread\nclass\npthread_t type and\nassociated API functions:\npthread_create(),\npthread_detach(), and\npthread_join()\nboost::thread class and\nmember functions\nstd::thread class and\nmember functions\nChapter 2\nMutual\nexclusion\nsynchronized\nblocks\npthread_mutex_t type\nand associated API functions:\npthread_mutex_lock(),\npthread_mutex_unlock(),\netc.\nboost::mutex class and\nmember functions,\nboost::lock_guard<>\nand boost::unique_lock<>\ntemplates\nstd::mutex class and\nmember functions,\nstd::lock_guard<> and\nstd::unique_lock<>\ntemplates\nChapter 3\nMonitors/\nwaits for a\npredicate\nwait()\nnotify()\nand\nmethods of the\njava.lang.Object class,\nused inside synchronized\nblocks\npthread_cond_t type\nand associated API functions:\npthread_cond_wait(),\npthread_cond_timed_\nwait(), etc.\nboost::condition_\nvariable and\nboost::condition_\nvariable_any classes and\nmember functions\nstd::condition_\nvariable and\nstd::condition_\nvariable_any classes and\nmember functions\nChapter 4\nAtomic opera-\ntions and\nconcurrency-\naware mem-\nory model\nvolatile variables,\nthe types in the\njava.util.concurrent\n.atomic package\nN/A\nN/A\nstd::atomic_xxx types,\nstd::atomic<> class\ntemplate,\nstd::atomic_thread_\nfence() function\nChapter 5\nThread-safe\ncontainers\nThe containers in the\njava.util.concurrent\npackage\nN/A\nN/A\nN/A\nChapters 6\nand 7\nFutures\njava.util.concurrent\n.future interface and\nassociated classes\nN/A\nboost::unique_future<>\nand\nboost::shared_future<>\nclass templates\nstd::future<>,\nstd::shared_future<>\nand\nstd::atomic_future<>\nclass templates\nChapter 4\nThread\npools\njava.util.concurrent\n.ThreadPoolExecutor\nclass\nN/A\nN/A\nN/A\nChapter 9\nThread\ninterruption\ninterrupt() method of\njava.lang.Thread\npthread_cancel()\ninterrupt() member function\nof\nclass\nboost::thread\nN/A\nChapter 9\n\n\n384\nappendix C\nA message-passing\nframework and complete\nATM example\nBack in chapter 4, I presented an example of sending messages between threads\nusing a message-passing framework, using a simple implementation of the code in\nan ATM as an example. What follows is the complete code for this example, includ-\ning the message-passing framework.\n Listing C.1 shows the message queue. It stores a list of messages as pointers to a\nbase class; the specific message type is handled with a template class derived from\nthat base class. Pushing an entry constructs an appropriate instance of the wrapper\nclass and stores a pointer to it; popping an entry returns that pointer. Because the\nmessage_base class doesn’t have any member functions, the popping thread will\nneed to cast the pointer to a suitable wrapped_message<T> pointer before it can\naccess the stored message.\n#include <mutex>\n#include <condition_variable>\n#include <queue>\n#include <memory>\nnamespace messaging\n{\n    struct message_base      \n    {\n        virtual ~message_base()\n        {}\n    };\nListing C.1\nA simple message queue\nBase class of your \nqueue entries\n\n\n385\n    template<typename Msg>\n    struct wrapped_message:      \n        message_base\n    {\n        Msg contents;\n        explicit wrapped_message(Msg const& contents_):\n            contents(contents_)\n        {}\n    };\n    class queue         \n    {\n        std::mutex m;\n        std::condition_variable c;\n        std::queue<std::shared_ptr<message_base> > q;   \n    public:\n        template<typename T>\n        void push(T const& msg)\n        {\n            std::lock_guard<std::mutex> lk(m);\n            q.push(std::make_shared<wrapped_message<T> >(msg));   \n            c.notify_all();\n        }\n        std::shared_ptr<message_base> wait_and_pop()\n        {\n            std::unique_lock<std::mutex> lk(m);\n            c.wait(lk,[&]{return !q.empty();});    \n            auto res=q.front();\n            q.pop();\n            return res;\n        }\n    };\n}\nSending messages is handled through an instance of the sender class shown in list-\ning C.2. This is a thin wrapper around a message queue that only allows messages to\nbe pushed. Copying instances of sender copies the pointer to the queue rather than\nthe queue itself.\nnamespace messaging\n{\n    class sender\n    {\n        queue*q;    \n    public:\n        sender():          \n            q(nullptr)\n        {}\n        explicit sender(queue*q_):     \n            q(q_)\n        {}\n        template<typename Message>\n        void send(Message const& msg)\nListing C.2\nThe sender class\nEach message type \nhas a specialization.\nYour message \nqueue\nInternal queue stores \npointers to message_base\nWrap posted \nmessage and \nstore pointer\nBlock until queue \nisn’t empty\nsender is a wrapper \naround the queue pointer.\nDefault-constructed \nsender has no queue\nAllow construction \nfrom pointer to queue\n\n\n386\nAPPENDIX C\nA message-passing framework and complete ATM example\n        {\n            if(q)\n            {\n                q->push(msg);  \n            }\n        }\n    };\n}\nReceiving messages is a bit more complicated. Not only do you have to wait for a mes-\nsage from the queue, but you also have to check to see if the type matches any of the\nmessage types being waited on and call the appropriate handler function. This all\nstarts with the receiver class, shown in the following listing.\nnamespace messaging\n{\n    class receiver\n    {\n        queue q;     \n    public:\n        operator sender()      \n        {\n            return sender(&q);\n        }\n        dispatcher wait()        \n        {\n            return dispatcher(&q);\n        }\n    };\n}\nWhereas a sender references a message queue, a receiver owns it. You can obtain a\nsender that references the queue by using the implicit conversion. The complexity of\ndoing the message dispatch starts with a call to wait(). This creates a dispatcher\nobject that references the queue from the receiver. The dispatcher class is shown in\nthe next listing; as you can see, the work is done in the destructor. In this case, that\nwork consists of waiting for a message and dispatching it.\nnamespace messaging\n{\n    class close_queue      \n    {};\n    class dispatcher\n    {\n        queue* q;\n        bool chained;\n        dispatcher(dispatcher const&)=delete;       \n        dispatcher& operator=(dispatcher const&)=delete;\nListing C.3\nThe receiver class\nListing C.4\nThe dispatcher class\nSending pushes \nmessage on the queue\nA receiver owns \nthe queue.\nAllow implicit conversion \nto a sender that \nreferences the queue.\nWaiting for a queue \ncreates a dispatcher\nThe message for \nclosing the queue\ndispatcher instances \ncannot be copied.\n\n\n387\n        template<\n            typename Dispatcher,\n            typename Msg,\n            typename Func>               \n        friend class TemplateDispatcher;\n        void wait_and_dispatch()\n        {\n            for(;;)          \n            {\n                auto msg=q->wait_and_pop();\n                dispatch(msg);\n            }\n        }\n        bool dispatch(                \n            std::shared_ptr<message_base> const& msg)\n        {\n            if(dynamic_cast<wrapped_message<close_queue>*>(msg.get()))\n            {\n                throw close_queue();\n            }\n            return false;\n        }\n    public:\n        dispatcher(dispatcher&& other):      \n            q(other.q),chained(other.chained)\n        {\n            other.chained=true;         \n        }\n        explicit dispatcher(queue* q_):          \n            q(q_),chained(false)\n        {}\n        template<typename Message,typename Func>\n        TemplateDispatcher<dispatcher,Message,Func> \n        handle(Func&& f)                             \n        {\n            return TemplateDispatcher<dispatcher,Message,Func>(\n                q,this,std::forward<Func>(f));\n        }\n        ~dispatcher() noexcept(false)   \n        {\n            if(!chained)\n            {\n                wait_and_dispatch();\n            }\n        }\n    };\n}\nThe dispatcher instance that’s returned from wait() will be destroyed immediately,\nbecause it’s temporary, and as mentioned, the destructor does the work. The destruc-\ntor calls wait_and_dispatch(), which is a loop B that waits for a message and passes it\nto dispatch(). dispatch() itself c is rather simple, it checks whether the message\nis a close_queue message and throws an exception if it is; otherwise, it returns false\nto indicate that the message was unhandled. This close_queue exception is why the\nAllow TemplateDispatcher \ninstances to access the \ninternals.\nLoop, waiting for, and \ndispatching messages\nb\ndispatch() checks \nfor a close_queue \nmessage, and throws.\nc\nDispatcher instances \ncan be moved.\nThe source shouldn’t \nwait for messages.\nHandle a specific type \nof message with a \nTemplateDispatcher.\nd\nThe destructor might \nthrow exceptions.\ne\n\n\n388\nAPPENDIX C\nA message-passing framework and complete ATM example\ndestructor is marked noexcept(false); without this annotation, the default exception\nspecification for the destructor would be noexcept(true) e, indicating that no excep-\ntions can be thrown, and the close_queue exception would terminate the program.\n It’s not often that you’re going to call wait() on its own, though; most of the time\nyou’ll want to handle a message. This is where the handle() member function d\ncomes in. It’s a template, and the message type isn’t deducible, so you must specify\nwhich message type to handle and pass in a function (or callable object) to handle it.\nhandle() itself passes the queue, the current dispatcher object, and the handler\nfunction to a new instance of the TemplateDispatcher class template, to handle mes-\nsages of the specified type, shown in listing C.5. This is why you test the chained value\nin the destructor before waiting for messages; not only does it prevent moved-from\nobjects waiting for messages, but it also allows you to transfer the responsibility of wait-\ning to your new TemplateDispatcher instance.\nnamespace messaging\n{\n    template<typename PreviousDispatcher,typename Msg,typename Func>\n    class TemplateDispatcher\n    {\n        queue* q;\n        PreviousDispatcher* prev;\n        Func f;\n        bool chained;\n        TemplateDispatcher(TemplateDispatcher const&)=delete;\n        TemplateDispatcher& operator=(TemplateDispatcher const&)=delete;\n        template<typename Dispatcher,typename OtherMsg,typename OtherFunc>\n        friend class TemplateDispatcher;    \n        void wait_and_dispatch()\n        {\n            for(;;)\n            {\n                auto msg=q->wait_and_pop();\n                if(dispatch(msg))            \n                    break;\n            }\n        }\n        bool dispatch(std::shared_ptr<message_base> const& msg)\n        {\n            if(wrapped_message<Msg>* wrapper=\n                dynamic_cast<wrapped_message<Msg>*>(msg.get()))   \n            {\n                f(wrapper->contents);    \n                return true;\n            }\n            else\n            {\n                return prev->dispatch(msg);    \n            }\n        }\nListing C.5\nThe TemplateDispatcher class template\nTemplateDispatcher \ninstantiations are \nfriends of each other.\nIf you handle the message, \nbreak out of the loop.\nB\nCheck the message type\nand call the function. c\nChain to the \nprevious dispatcher.\nd\n\n\n389\n    public:\n        TemplateDispatcher(TemplateDispatcher&& other):\n            q(other.q),prev(other.prev),f(std::move(other.f)),\n            chained(other.chained)\n        {\n            other.chained=true;\n        }\n        TemplateDispatcher(queue* q_,PreviousDispatcher* prev_,Func&& f_):\n            q(q_),prev(prev_),f(std::forward<Func>(f_)),chained(false)\n        {\n            prev_->chained=true;\n        }\n        template<typename OtherMsg,typename OtherFunc>\n        TemplateDispatcher<TemplateDispatcher,OtherMsg,OtherFunc>\n        handle(OtherFunc&& of)                        \n        {\n            return TemplateDispatcher<\n                TemplateDispatcher,OtherMsg,OtherFunc>(\n                    q,this,std::forward<OtherFunc>(of));\n        }\n        ~TemplateDispatcher() noexcept(false)   \n        {\n            if(!chained)\n            {\n                wait_and_dispatch();\n            }\n        }\n    };\n}\nThe TemplateDispatcher<> class template is modeled on the dispatcher class and is\nalmost identical. In particular, the destructor still calls wait_and_dispatch() to wait\nfor a message.\n Because you don’t throw exceptions if you handle the message, you now need to\ncheck whether you did handle the message in your message loop B. Your message\nprocessing stops when you’ve successfully handled a message, so that you can wait for\na different set of messages next time. If you do get a match for the specified message\ntype, the supplied function is called c rather than throwing an exception (although\nthe handler function may throw an exception itself). If you don’t get a match, you\nchain to the previous dispatcher d. In the first instance, this will be a dispatcher, but\nif you chain calls to handle() e to allow multiple types of messages to be handled, this\nmay be a prior instantiation of TemplateDispatcher<>, which will in turn chain to the\nprevious handler if the message doesn’t match. Because any of the handlers might\nthrow an exception (including the dispatcher’s default handler for close_queue mes-\nsages), the destructor must once again be declared noexcept(false) f.\n This simple framework allows you to push any type of message on the queue and\nthen selectively match against messages you can handle on the receiving end. It also\nallows you to pass around a reference to the queue for pushing messages on, while\nkeeping the receiving end private.\nAdditional handlers \ncan be chained.\ne\nThe destructor is \nnoexcept(false) \nagain.\nf\n",
      "page_number": 405
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 413-421)",
      "start_page": 413,
      "end_page": 421,
      "detection_method": "topic_boundary",
      "content": "390\nAPPENDIX C\nA message-passing framework and complete ATM example\n To complete the example from chapter 4, the messages are given in listing C.6, the\nvarious state machines in listings C.7, C.8, and C.9, and the driving code in listing C.10.\nstruct withdraw\n{\n    std::string account;\n    unsigned amount;\n    mutable messaging::sender atm_queue;\n    withdraw(std::string const& account_,\n             unsigned amount_,\n             messaging::sender atm_queue_):\n        account(account_),amount(amount_),\n        atm_queue(atm_queue_)\n    {}\n};\nstruct withdraw_ok\n{};\nstruct withdraw_denied\n{};\nstruct cancel_withdrawal\n{\n    std::string account;\n    unsigned amount;\n    cancel_withdrawal(std::string const& account_,\n                      unsigned amount_):\n        account(account_),amount(amount_)\n    {}\n};\nstruct withdrawal_processed\n{\n    std::string account;\n    unsigned amount;\n    withdrawal_processed(std::string const& account_,\n                         unsigned amount_):\n        account(account_),amount(amount_)\n    {}\n};\nstruct card_inserted\n{\n    std::string account;\n    explicit card_inserted(std::string const& account_):\n        account(account_)\n    {}\n    \n};\nstruct digit_pressed\n{\n    char digit;\n    explicit digit_pressed(char digit_):\n        digit(digit_)\n    {}\n    \n};\nListing C.6\nATM messages\n\n\n391\nstruct clear_last_pressed\n{};\nstruct eject_card\n{};\nstruct withdraw_pressed\n{\n    unsigned amount;\n    explicit withdraw_pressed(unsigned amount_):\n        amount(amount_)\n    {}\n    \n};\nstruct cancel_pressed\n{};\nstruct issue_money\n{\n    unsigned amount;\n    issue_money(unsigned amount_):\n        amount(amount_)\n    {}\n};\nstruct verify_pin\n{\n    std::string account;\n    std::string pin;\n    mutable messaging::sender atm_queue;\n    verify_pin(std::string const& account_,std::string const& pin_,\n               messaging::sender atm_queue_):\n        account(account_),pin(pin_),atm_queue(atm_queue_)\n    {}\n};\nstruct pin_verified\n{};\nstruct pin_incorrect\n{};\nstruct display_enter_pin\n{};\nstruct display_enter_card\n{};\nstruct display_insufficient_funds\n{};\nstruct display_withdrawal_cancelled\n{};\nstruct display_pin_incorrect_message\n{};\nstruct display_withdrawal_options\n{};\nstruct get_balance\n{\n    std::string account;\n    mutable messaging::sender atm_queue;\n    get_balance(std::string const& account_,messaging::sender atm_queue_):\n        account(account_),atm_queue(atm_queue_)\n    {}\n};\n\n\n392\nAPPENDIX C\nA message-passing framework and complete ATM example\nstruct balance\n{\n    unsigned amount;\n    \n    explicit balance(unsigned amount_):\n        amount(amount_)\n    {}\n};\nstruct display_balance\n{\n    unsigned amount;\n    explicit display_balance(unsigned amount_):\n        amount(amount_)\n    {}\n};\nstruct balance_pressed\n{};\nclass atm\n{\n    messaging::receiver incoming;\n    messaging::sender bank;\n    messaging::sender interface_hardware;\n    void (atm::*state)();\n    std::string account;\n    unsigned withdrawal_amount;\n    std::string pin;\n    void process_withdrawal()\n    {\n        incoming.wait()\n            .handle<withdraw_ok>(\n                [&](withdraw_ok const& msg)\n                {\n                    interface_hardware.send(\n                        issue_money(withdrawal_amount));\n                    bank.send(\n                        withdrawal_processed(account,withdrawal_amount));\n                    state=&atm::done_processing;\n                }\n                )\n            .handle<withdraw_denied>(\n                [&](withdraw_denied const& msg)\n                {\n                    interface_hardware.send(display_insufficient_funds());\n                    state=&atm::done_processing;\n                }\n                )\n            .handle<cancel_pressed>(\n                [&](cancel_pressed const& msg)\n                {\n                    bank.send(\n                        cancel_withdrawal(account,withdrawal_amount));\n                    interface_hardware.send(\nListing C.7\nThe ATM state machine\n\n\n393\n                        display_withdrawal_cancelled());\n                    state=&atm::done_processing;\n                }\n                );\n    }\n    void process_balance()\n    {\n        incoming.wait()\n            .handle<balance>(\n                [&](balance const& msg)\n                {\n                    interface_hardware.send(display_balance(msg.amount));\n                    state=&atm::wait_for_action;\n                }\n                )\n            .handle<cancel_pressed>(\n                [&](cancel_pressed const& msg)\n                {\n                    state=&atm::done_processing;\n                }\n                );\n    }\n    void wait_for_action()\n    {\n        interface_hardware.send(display_withdrawal_options());\n        incoming.wait()\n            .handle<withdraw_pressed>(\n                [&](withdraw_pressed const& msg)\n                {\n                    withdrawal_amount=msg.amount;\n                    bank.send(withdraw(account,msg.amount,incoming));\n                    state=&atm::process_withdrawal;\n                }\n                )\n            .handle<balance_pressed>(\n                [&](balance_pressed const& msg)\n                {\n                    bank.send(get_balance(account,incoming));\n                    state=&atm::process_balance;\n                }\n                )\n            .handle<cancel_pressed>(\n                [&](cancel_pressed const& msg)\n                {\n                    state=&atm::done_processing;\n                }\n                );\n    }\n    void verifying_pin()\n    {\n        incoming.wait()\n            .handle<pin_verified>(\n                [&](pin_verified const& msg)\n                {\n                    state=&atm::wait_for_action;\n\n\n394\nAPPENDIX C\nA message-passing framework and complete ATM example\n                }\n                )\n            .handle<pin_incorrect>(\n                [&](pin_incorrect const& msg)\n                {\n                    interface_hardware.send(\n                        display_pin_incorrect_message());\n                    state=&atm::done_processing;\n                }\n                )\n            .handle<cancel_pressed>(\n                [&](cancel_pressed const& msg)\n                {\n                    state=&atm::done_processing;\n                }\n                );\n    }\n    void getting_pin()\n    {\n        incoming.wait()\n            .handle<digit_pressed>(\n                [&](digit_pressed const& msg)\n                {\n                    unsigned const pin_length=4;\n                    pin+=msg.digit;\n                    if(pin.length()==pin_length)\n                    {\n                        bank.send(verify_pin(account,pin,incoming));\n                        state=&atm::verifying_pin;\n                    }\n                }\n                )\n            .handle<clear_last_pressed>(\n                [&](clear_last_pressed const& msg)\n                {\n                    if(!pin.empty())\n                    {\n                        pin.pop_back();\n                    }\n                }\n                )\n            .handle<cancel_pressed>(\n                [&](cancel_pressed const& msg)\n                {\n                    state=&atm::done_processing;\n                }\n                );\n    }\n    void waiting_for_card()\n    {\n        interface_hardware.send(display_enter_card());\n        incoming.wait()\n            .handle<card_inserted>(\n                [&](card_inserted const& msg)\n                {\n\n\n395\n                    account=msg.account;\n                    pin=\"\";\n                    interface_hardware.send(display_enter_pin());\n                    state=&atm::getting_pin;\n                }\n                );\n    }\n    void done_processing()\n    {\n        interface_hardware.send(eject_card());\n        state=&atm::waiting_for_card;\n    }\n    atm(atm const&)=delete;\n    atm& operator=(atm const&)=delete;\npublic:\n    atm(messaging::sender bank_,\n        messaging::sender interface_hardware_):\n        bank(bank_),interface_hardware(interface_hardware_)\n    {}\n    void done()\n    {\n        get_sender().send(messaging::close_queue());\n    }\n    void run()\n    {\n        state=&atm::waiting_for_card;\n        try\n        {\n            for(;;)\n            {\n                (this->*state)();\n            }\n        }\n        catch(messaging::close_queue const&)\n        {\n        }\n    }\n    messaging::sender get_sender()\n    {\n        return incoming;\n    }\n};\nclass bank_machine\n{\n    messaging::receiver incoming;\n    unsigned balance;\npublic:\n    bank_machine():\n        balance(199)\n    {}\n    void done()\n    {\nListing C.8\nThe bank state machine\n\n\n396\nAPPENDIX C\nA message-passing framework and complete ATM example\n        get_sender().send(messaging::close_queue());\n    }\n    void run()\n    {\n        try\n        {\n            for(;;)\n            {\n                incoming.wait()\n                    .handle<verify_pin>(\n                        [&](verify_pin const& msg)\n                        {\n                            if(msg.pin==\"1937\")\n                            {\n                                msg.atm_queue.send(pin_verified());\n                            }\n                            else\n                            {\n                                msg.atm_queue.send(pin_incorrect());\n                            }\n                        }\n                        )\n                    .handle<withdraw>(\n                        [&](withdraw const& msg)\n                        {\n                            if(balance>=msg.amount)\n                            {\n                                msg.atm_queue.send(withdraw_ok());\n                                balance-=msg.amount;\n                            }\n                            else\n                            {\n                                msg.atm_queue.send(withdraw_denied());\n                            }\n                        }\n                        )\n                    .handle<get_balance>(\n                        [&](get_balance const& msg)\n                        {\n                            msg.atm_queue.send(::balance(balance));\n                        }\n                        )\n                    .handle<withdrawal_processed>(\n                        [&](withdrawal_processed const& msg)\n                        {\n                        }\n                        )\n                    .handle<cancel_withdrawal>(\n                        [&](cancel_withdrawal const& msg)\n                        {\n                        }\n                        );\n            }\n        }\n        catch(messaging::close_queue const&)\n\n\n397\n        {\n        }\n    }\n    \n    messaging::sender get_sender()\n    {\n        return incoming;\n    }\n};\nclass interface_machine\n{\n    messaging::receiver incoming;\npublic:\n    void done()\n    {\n        get_sender().send(messaging::close_queue());\n    }\n    void run()\n    {\n        try\n        {\n            for(;;)\n            {\n                incoming.wait()\n                    .handle<issue_money>(\n                        [&](issue_money const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Issuing \"\n                                         <<msg.amount<<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_insufficient_funds>(\n                        [&](display_insufficient_funds const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Insufficient funds\"<<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_enter_pin>(\n                        [&](display_enter_pin const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout\n                                    <<\"Please enter your PIN (0-9)\"\n                                    <<std::endl;\n                            }\nListing C.9\nThe user-interface state machine\n\n\n398\nAPPENDIX C\nA message-passing framework and complete ATM example\n                        }\n                        )\n                    .handle<display_enter_card>(\n                        [&](display_enter_card const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Please enter your card (I)\"\n                                         <<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_balance>(\n                        [&](display_balance const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout\n                                    <<\"The balance of your account is \"\n                                    <<msg.amount<<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_withdrawal_options>(\n                        [&](display_withdrawal_options const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Withdraw 50? (w)\"<<std::endl;\n                                std::cout<<\"Display Balance? (b)\"\n                                         <<std::endl;\n                                std::cout<<\"Cancel? (c)\"<<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_withdrawal_cancelled>(\n                        [&](display_withdrawal_cancelled const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Withdrawal cancelled\"\n                                         <<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_pin_incorrect_message>(\n                        [&](display_pin_incorrect_message const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"PIN incorrect\"<<std::endl;\n                            }\n                        }\n                        )\n                    .handle<eject_card>(\n",
      "page_number": 413
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 422-446)",
      "start_page": 422,
      "end_page": 446,
      "detection_method": "topic_boundary",
      "content": "399\n                        [&](eject_card const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Ejecting card\"<<std::endl;\n                            }\n                        }\n                        );\n            }\n        }\n        catch(messaging::close_queue&)\n        {\n        }\n    }\n    messaging::sender get_sender()\n    {\n        return incoming;\n    }    \n};\nint main()\n{\n    bank_machine bank;\n    interface_machine interface_hardware;\n    atm machine(bank.get_sender(),interface_hardware.get_sender());\n    std::thread bank_thread(&bank_machine::run,&bank);\n    std::thread if_thread(&interface_machine::run,&interface_hardware);\n    std::thread atm_thread(&atm::run,&machine);\n    messaging::sender atmqueue(machine.get_sender());\n    bool quit_pressed=false;\n    while(!quit_pressed)\n    {\n        char c=getchar();\n        switch(c)\n        {\n        case '0':\n        case '1':\n        case '2':\n        case '3':\n        case '4':\n        case '5':\n        case '6':\n        case '7':\n        case '8':\n        case '9':\n            atmqueue.send(digit_pressed(c));\n            break;\n        case 'b':\n            atmqueue.send(balance_pressed());\n            break;\n        case 'w':\n            atmqueue.send(withdraw_pressed(50));\n            break;\nListing C.10\nThe driving code\n\n\n400\nAPPENDIX C\nA message-passing framework and complete ATM example\n        case 'c':\n            atmqueue.send(cancel_pressed());\n            break;\n        case 'q':\n            quit_pressed=true;\n            break;\n        case 'i':\n            atmqueue.send(card_inserted(\"acc1234\"));\n            break;\n        }\n    }\n    bank.done();\n    machine.done();\n    interface_hardware.done();\n    atm_thread.join();\n    bank_thread.join();\n    if_thread.join();\n}\n\n\n401\nappendix D\nC++ Thread\nLibrary reference\nD.1\nThe <chrono> header \nThe <chrono> header provides classes for representing points in time, durations,\nand clock classes, which act as a source of time_points. Each clock has an is_steady\nstatic data member, which indicates whether it’s a steady clock that advances at a\nuniform rate (and can’t be adjusted). The std::chrono::steady_clock class is the\nonly clock guaranteed to be steady. \nHeader contents\nnamespace std\n{\n   namespace chrono\n   {\n       template<typename Rep,typename Period = ratio<1>>\n       class duration;\n       template<\n           typename Clock,\n           typename Duration = typename Clock::duration>\n       class time_point;\n       class system_clock;\n       class steady_clock;\n       typedef unspecified-clock-type high_resolution_clock;\n   }\n}\nD.1.1\nstd::chrono::duration class template \nThe std::chrono::duration class template provides a facility for representing\ndurations. The template parameters Rep and Period are the data type to store the\nduration value and an instantiation of the std::ratio class template indicating the\n\n\n402\nAPPENDIX D\nC++ Thread Library reference\nlength of time (as a fraction of a second) between successive “ticks,” respectively. Thus\nstd::chrono::duration<int, std::milli> is a count of milliseconds stored in a value\nof type int, whereas std::chrono::duration<short, std::ratio<1,50>> is a count of\nfiftieths of a second stored in a value of type short, and std::chrono:: d-uration\n<long long, std::ratio<60,1>> is a count of minutes stored in a value of type long\nlong. \nClass definition\ntemplate <class Rep, class Period=ratio<1> >\nclass duration\n{\npublic:\n    typedef Rep rep;\n    typedef Period period;\n    constexpr duration() = default;\n    ~duration() = default;\n    duration(const duration&) = default;\n    duration& operator=(const duration&) = default;\n    template <class Rep2>\n    constexpr explicit duration(const Rep2& r);\n    template <class Rep2, class Period2>\n    constexpr duration(const duration<Rep2, Period2>& d);\n    constexpr rep count() const;\n    constexpr duration operator+() const;\n    constexpr duration operator-() const;\n    duration& operator++();\n    duration operator++(int);\n    duration& operator--();\n    duration operator--(int);\n    duration& operator+=(const duration& d);\n    duration& operator-=(const duration& d);\n    duration& operator*=(const rep& rhs);\n    duration& operator/=(const rep& rhs);\n    duration& operator%=(const rep& rhs);\n    duration& operator%=(const duration& rhs);\n    static constexpr duration zero();\n    static constexpr duration min();\n    static constexpr duration max();\n};\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator==(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator!=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\n\n\n403\nThe <chrono> header\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator<(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator<=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator>(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator>=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\ntemplate <class ToDuration, class Rep, class Period>\nconstexpr ToDuration duration_cast(const duration<Rep, Period>& d);\nRequirements\nRep must be a built-in numeric type, or a number-like user-defined type. Period\nmust be an instantiation of std::ratio<>.\nSTD::CHRONO::DURATION::REP TYPEDEF \nThis is a typedef for the type used to hold the number of ticks in a duration value. \nDeclaration\ntypedef Rep rep;\nrep is the type of value used to hold the internal representation of the duration\nobject.\nSTD::CHRONO::DURATION::PERIOD TYPEDEF \nThis typedef is for an instantiation of the std::ratio class template that specifies the\nfraction of a second represented by the duration count. For example, if period is\nstd::ratio<1,50>, a duration value with a count() of N represents N fiftieths of\na second.\nDeclaration\ntypedef Period period;\nSTD::CHRONO::DURATION DEFAULT CONSTRUCTOR\nConstructs an std::chrono::duration instance with a default value.\nDeclaration\nconstexpr duration() = default;\nEffects\nThe internal value of the duration (of type rep) is default initialized.\n\n\n404\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::DURATION CONVERTING CONSTRUCTOR FROM A COUNT VALUE\nConstructs an std::chrono::duration instance with a specified count.\nDeclaration\ntemplate <class Rep2>\nconstexpr explicit duration(const Rep2& r);\nEffects\nThe internal value of the duration object is initialized with static_cast<rep>(r).\nRequirements\nThis constructor only participates in overload resolution if Rep2 is implicitly convert-\nible to Rep and either Rep is a floating point type or Rep2 is not a floating point type.\nPostcondition\nthis->count()==static_cast<rep>(r)\nSTD::CHRONO::DURATION CONVERTING CONSTRUCTOR FROM ANOTHER STD::CHRONO::DURATION VALUE\nConstructs an std::chrono::duration instance by scaling the count value of another\nstd::chrono::duration object.\nDeclaration\ntemplate <class Rep2, class Period2>\nconstexpr duration(const duration<Rep2,Period2>& d);\nEffects\nThe internal value of the duration object is initialized with duration_cast<duration\n<Rep,Period>>(d).count().\nRequirements\nThis constructor only participates in overload resolution if Rep is a floating point\ntype or Rep2 is not a floating point type and Period2 is a whole number multiple of\nPeriod (that is, ratio_divide<Period2,Period>::den==1). This avoids accidental\ntruncation (and corresponding loss of precision) from storing a duration with\nsmall periods in a variable representing a duration with a longer period.\nPostcondition\nthis->count()==duration_cast<duration<Rep,Period>>(d).count()\nExamples\nduration<int,ratio<1,1000>> ms(5);  \nduration<int,ratio<1,1>> s(ms);                   \nduration<double,ratio<1,1>> s2(ms);                \nduration<int,ratio<1,1000000>> us(ms);    \nSTD::CHRONO::DURATION::COUNT MEMBER FUNCTION\nRetrieves the value of the duration.\nDeclaration\nconstexpr rep count() const;\nReturns\nThe internal value of the duration object, as a value of type rep.\nFive milliseconds\nError: can’t store ms \nas integral seconds\nOK: s2.count()==0.005\nOK: us.count()==5000\n\n\n405\nThe <chrono> header\nSTD::CHRONO::DURATION::OPERATOR+ UNARY PLUS OPERATOR\nThis is a no-op: it just returns a copy of *this.\nDeclaration\nconstexpr duration operator+() const;\nReturns\n*this\nSTD::CHRONO::DURATION::OPERATOR- UNARY MINUS OPERATOR\nReturns a duration such that the count() value is the negative value of this->\ncount().\nDeclaration\nconstexpr duration operator-() const;\nReturns\nduration(-this->count());\nSTD::CHRONO::DURATION::OPERATOR++ PRE-INCREMENT OPERATOR\nIncrements the internal count.\nDeclaration\nduration& operator++();\nEffects\n++this->internal_count;\nReturns\n*this\nSTD::CHRONO::DURATION::OPERATOR++ POST-INCREMENT OPERATOR\nIncrements the internal count and returns the value of *this prior to the increment.\nDeclaration\nduration operator++(int);\nEffects\nduration temp(*this);\n++(*this);\nreturn temp;\nSTD::CHRONO::DURATION::OPERATOR-- PRE-DECREMENT OPERATOR\nDecrements the internal count.\nDeclaration\nduration& operator--();\nEffects\n--this->internal_count;\nReturns\n*this\n\n\n406\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::DURATION::OPERATOR-- POST-DECREMENT OPERATOR\nDecrements the internal count and returns the value of *this prior to the decrement.\nDeclaration\nduration operator--(int);\nEffects\nduration temp(*this);\n--(*this);\nreturn temp;\nSTD::CHRONO::DURATION::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR\nAdds the count for another duration object to the internal count for *this.\nDeclaration\nduration& operator+=(duration const& other);\nEffects\ninternal_count+=other.count();\nReturns \n*this\nSTD::CHRONO::DURATION::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR\nSubtracts the count for another duration object from the internal count for *this.\nDeclaration\nduration& operator-=(duration const& other);\nEffects\ninternal_count-=other.count();\nReturns \n*this\nSTD::CHRONO::DURATION::OPERATOR*= COMPOUND ASSIGNMENT OPERATOR\nMultiplies the internal count for *this by the specified value.\nDeclaration\nduration& operator*=(rep const& rhs);\nEffects\ninternal_count*=rhs;\nReturns \n*this\nSTD::CHRONO::DURATION::OPERATOR/= COMPOUND ASSIGNMENT OPERATOR\nDivides the internal count for *this by the specified value.\nDeclaration\nduration& operator/=(rep const& rhs);\n\n\n407\nThe <chrono> header\nEffects\ninternal_count/=rhs;\nReturns \n*this\nSTD::CHRONO::DURATION::OPERATOR%= COMPOUND ASSIGNMENT OPERATOR\nAdjusts the internal count for *this to be the remainder when divided by the speci-\nfied value.\nDeclaration\nduration& operator%=(rep const& rhs);\nEffects\ninternal_count%=rhs;\nReturns \n*this\nSTD::CHRONO::DURATION::OPERATOR%= COMPOUND ASSIGNMENT OPERATOR\nAdjusts the internal count for *this to be the remainder when divided by the count of\nthe other duration object.\nDeclaration\nduration& operator%=(duration const& rhs);\nEffects\ninternal_count%=rhs.count();\nReturns \n*this\nSTD::CHRONO::DURATION::ZERO STATIC MEMBER FUNCTION\nReturns a duration object representing a value of zero.\nDeclaration\nconstexpr duration zero();\nReturns \nduration(duration_values<rep>::zero());\nSTD::CHRONO::DURATION::MIN STATIC MEMBER FUNCTION\nReturns a duration object holding the minimum possible value for the specified\ninstantiation.\nDeclaration\nconstexpr duration min();\nReturns \nduration(duration_values<rep>::min());\n\n\n408\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::DURATION::MAX STATIC MEMBER FUNCTION\nReturns a duration object holding the maximum possible value for the specified\ninstantiation.\nDeclaration\nconstexpr duration max();\nReturns \nduration(duration_values<rep>::max());\nSTD::CHRONO::DURATION EQUALITY COMPARISON OPERATOR\nCompares two duration objects for equality, even if they have distinct representations\nand/or periods.\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator==(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly convert to the other, the expression is ill-formed. \nEffects\nIf CommonDuration is a synonym for std::common_type< duration< Rep1, Period1>,\nduration< Rep2, Period2>>::type, then lhs==rhs returns CommonDuration(lhs)\n.count()==CommonDuration(rhs).count().\nSTD::CHRONO::DURATION INEQUALITY COMPARISON OPERATOR\nCompares two duration objects for inequality, even if they have distinct representa-\ntions and/or periods.\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator!=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly convert to the other, the expression is ill-formed. \nReturns\n!(lhs==rhs)\nSTD::CHRONO::DURATION LESS-THAN COMPARISON OPERATOR\nCompares two duration objects to see if one is less than the other, even if they have\ndistinct representations and/or periods.\n\n\n409\nThe <chrono> header\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator<(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly converted to the other, the expression is ill-formed. \nEffects\nIf CommonDuration is a synonym for std::common_type< duration< Rep1, Period1>,\nduration< Rep2, Period2>>::type, then lhs<rhs returns CommonDuration(lhs)\n.count()<CommonDuration(rhs).count().\nSTD::CHRONO::DURATION GREATER-THAN COMPARISON OPERATOR\nCompares two duration objects to see if one is greater than the other, even if they\nhave distinct representations and/or periods.\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator>(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly convert to the other, the expression is ill-formed. \nReturns\nrhs<lhs\nSTD::CHRONO::DURATION LESS-THAN-OR-EQUALS COMPARISON OPERATOR\nCompares two duration objects to see if one is less than or equal to the other, even if\nthey have distinct representations and/or periods.\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator<=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly convert to the other, the expression is ill-formed. \nReturns\n!(rhs<lhs)\n\n\n410\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::DURATION GREATER-THAN-OR-EQUALS COMPARISON OPERATOR\nCompares two duration objects to see if one is greater than or equal to the other,\neven if they have distinct representations and/or periods.\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator>=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly convert to the other, the expression is ill-formed. \nReturns\n!(lhs<rhs)\nSTD::CHRONO::DURATION_CAST NONMEMBER FUNCTION\nExplicitly converts an std::chrono::duration object to a specific std::chrono::\nduration instantiation.\nDeclaration\ntemplate <class ToDuration, class Rep, class Period>\nconstexpr ToDuration duration_cast(const duration<Rep, Period>& d);\nRequirements\nToDuration must be an instantiation of std::chrono::duration. \nReturns\nThe duration, d converted to the duration type specified by ToDuration. This is\ndone in such a way as to minimize any loss of precision resulting from conversions\nbetween different scales and representation types.\nD.1.2\nstd::chrono::time_point class template \nThe std::chrono::time_point class template represents a point in time, as measured by\na particular clock. It’s specified as a duration since the epoch of that particular clock. The\ntemplate parameter Clock identifies the clock (each distinct clock must have a unique\ntype), whereas the Duration template parameter is the type to use for measuring the\nduration since the epoch and must be an instantiation of the std::chrono::duration\nclass template. The Duration defaults to the default duration type of the Clock. \nClass definition\ntemplate <class Clock,class Duration = typename Clock::duration>\nclass time_point\n{\npublic:\n    typedef Clock clock;\n    typedef Duration duration;\n    typedef typename duration::rep rep;\n    typedef typename duration::period period;\n\n\n411\nThe <chrono> header\n    time_point();\n    explicit time_point(const duration& d);\n    template <class Duration2>\n    time_point(const time_point<clock, Duration2>& t);\n    duration time_since_epoch() const;\n    time_point& operator+=(const duration& d);\n    time_point& operator-=(const duration& d);\n    static constexpr time_point min();\n    static constexpr time_point max();\n};\nSTD::CHRONO::TIME_POINT DEFAULT CONSTRUCTOR\nConstructs a time_point representing the epoch of the associated Clock; the internal\nduration is initialized with Duration::zero().\nDeclaration\ntime_point();\nPostcondition\nFor a newly default-constructed time_point object, tp, tp.time_since_epoch() ==\ntp::duration::zero(). \nSTD::CHRONO::TIME_POINT DURATION CONSTRUCTOR\nConstructs a time_point representing the specified duration since the epoch of the\nassociated Clock.\nDeclaration\nexplicit time_point(const duration& d);\nPostcondition\nFor a time_point object, tp, constucted with tp(d) for some duration, d, tp.time_\nsince_epoch()==d.\nSTD::CHRONO::TIME_POINT CONVERSION CONSTRUCTOR\nConstructs a time_point object from another time_point object with the same Clock\nbut a distinct Duration.\nDeclaration\ntemplate <class Duration2>\ntime_point(const time_point<clock, Duration2>& t);\nRequirements\nDuration2 shall be implicitly convertible to Duration.\nEffects\nAs-if time_point(t.time_since_epoch())\nThe value returned from t.time_since_epoch() is implicitly converted to an\nobject of the Duration type, and that value is stored in the newly constructed time_\npoint object.\n\n\n412\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::TIME_POINT::TIME_SINCE_EPOCH MEMBER FUNCTION\nRetrieves the duration since the clock epoch for a particular time_point object.\nDeclaration\nduration time_since_epoch() const;\nReturns\nThe duration value stored in *this.\nSTD::CHRONO::TIME_POINT::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR\nAdds the specified duration to the value stored in the specified time_point object.\nDeclaration\ntime_point& operator+=(const duration& d);\nEffects\nAdds d to the internal duration object of *this, as-if\nthis->internal_duration += d;\nReturns\n*this\nSTD::CHRONO::TIME_POINT::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR\nSubtracts the specified duration from the value stored in the specified time_point\nobject.\nDeclaration\ntime_point& operator-=(const duration& d);\nEffects\nSubtracts d from the internal duration object of *this, as-if\nthis->internal_duration -= d;\nReturns\n*this\nSTD::CHRONO::TIME_POINT::MIN STATIC MEMBER FUNCTION\nObtains a time_point object representing the minimum possible value for its type.\nDeclaration\nstatic constexpr time_point min();\nReturns\ntime_point(time_point::duration::min()) (see 11.1.1.15)\nSTD::CHRONO::TIME_POINT::MAX STATIC MEMBER FUNCTION\nObtains a time_point object representing the maximum possible value for its type.\nDeclaration\nstatic constexpr time_point max();\nReturns\ntime_point(time_point::duration::max()) (see 11.1.1.16)\n\n\n413\nThe <chrono> header\nD.1.3\nstd::chrono::system_clock class \nThe std::chrono::system_clock class provides a means of obtaining the current\nwall-clock time from the system-wide real-time clock. The current time can be obtained\nby calling std::chrono::system_clock::now(). Instances of std::chrono::system_\nclock::time_point can be converted to and from time_t with the std::chrono::\nsystem_clock::to_time_t() and std::chrono::system_clock::to_time_point()\nfunctions. The system clock isn’t steady, so a subsequent call to std::chrono::system_\nclock::now() may return an earlier time than a previous call (for example, if the\noperating system clock is manually adjusted or synchronized with an external clock). \nClass definition\nclass system_clock\n{\npublic:\n    typedef unspecified-integral-type rep;\n    typedef std::ratio<unspecified,unspecified> period;\n    typedef std::chrono::duration<rep,period> duration;\n    typedef std::chrono::time_point<system_clock> time_point;\n    static const bool is_steady=unspecified;\n    static time_point now() noexcept;\n    static time_t to_time_t(const time_point& t) noexcept;\n    static time_point from_time_t(time_t t) noexcept;\n};\nSTD::CHRONO::SYSTEM_CLOCK::REP TYPEDEF \nA typedef for an integral type used to hold the number of ticks in a duration value. \nDeclaration\ntypedef unspecified-integral-type rep;\nSTD::CHRONO::SYSTEM_CLOCK::PERIOD TYPEDEF \nA typedef for an instantiation of the std::ratio class template that specifies the\nsmallest number of seconds (or fractions of a second) between distinct values of\nduration or time_point. The period specifies the precision of the clock, not the tick\nfrequency. \nDeclaration\ntypedef std::ratio<unspecified,unspecified> period;\nSTD::CHRONO::SYSTEM_CLOCK::DURATION TYPEDEF \nAn instantiation of the std::chrono::duration class template that can hold the dif-\nference between any two time points returned by the system-wide real-time clock. \nDeclaration\ntypedef std::chrono::duration<\n    std::chrono::system_clock::rep,\n    std::chrono::system_clock::period> duration;\n\n\n414\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::SYSTEM_CLOCK::TIME_POINT TYPEDEF \nAn instantiation of the std::chrono::time_point class template that can hold time\npoints returned by the system-wide real-time clock. \nDeclaration\ntypedef std::chrono::time_point<std::chrono::system_clock> time_point;\nSTD::CHRONO::SYSTEM_CLOCK::NOW STATIC MEMBER FUNCTION \nObtains the current wall-clock time from the system-wide real-time clock. \nDeclaration\ntime_point now() noexcept;\nReturns\nA time_point representing the current time of the system-wide real-time clock. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::CHRONO::SYSTEM_CLOCK::TO_TIME_T STATIC MEMBER FUNCTION \nConverts an instance of time_point to time_t. \nDeclaration\ntime_t to_time_t(time_point const& t) noexcept;\nReturns\nA time_t value that represents the same point in time as t, rounded or truncated\nto seconds precision. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::CHRONO::SYSTEM_CLOCK::FROM_TIME_T STATIC MEMBER FUNCTION \nConverts an instance of time_t to time_point. \nDeclaration\ntime_point from_time_t(time_t const& t) noexcept;\nReturns\nA time_point value that represents the same point in time as t. \nThrows\nAn exception of type std::system_error if an error occurs. \nD.1.4\nstd::chrono::steady_clock class \nThe std::chrono::steady_clock class provides access to the system-wide steady clock.\nThe current time can be obtained by calling std::chrono::steady_clock::now().\nThere is no fixed relationship between values returned by std::chrono::steady_\nclock::now() and wall-clock time. A steady clock can’t go backwards, so if one call to\nstd::chrono::steady_clock::now() happens-before another call to std::chrono\n::steady_clock::now(), the second call must return a time point equal to or later\nthan the first. The clock advances at a uniform rate as far as possible. \n\n\n415\nThe <chrono> header\nClass definition\nclass steady_clock\n{\npublic:\n    typedef unspecified-integral-type rep;\n    typedef std::ratio<\n        unspecified,unspecified> period;\n    typedef std::chrono::duration<rep,period> duration;\n    typedef std::chrono::time_point<steady_clock>\n        time_point;\n    static const bool is_steady=true;\n    static time_point now() noexcept;\n};\nSTD::CHRONO::STEADY_CLOCK::REP TYPEDEF \nThis typedef is for an integral type used to hold the number of ticks in a duration\nvalue. \nDeclaration\ntypedef unspecified-integral-type rep;\nSTD::CHRONO::STEADY_CLOCK::PERIOD TYPEDEF \nThis is a typedef for an instantiation of the std::ratio class template that specifies\nthe smallest number of seconds (or fractions of a second) between distinct values of\nduration or time_point. The period specifies the precision of the clock, not the tick\nfrequency. \nDeclaration\ntypedef std::ratio<unspecified,unspecified> period;\nSTD::CHRONO::STEADY_CLOCK::DURATION TYPEDEF \nThis is an instantiation of the std::chrono::duration class template that can hold\nthe difference between any two time points returned by the system-wide steady clock. \nDeclaration\ntypedef std::chrono::duration<\n    std::chrono::steady_clock::rep,\n    std::chrono::steady_clock::period> duration;\nSTD::CHRONO::STEADY_CLOCK::TIME_POINT TYPEDEF \nThis instantiation of the std::chrono::time_point class template can hold time\npoints returned by the system-wide steady clock. \nDeclaration\ntypedef std::chrono::time_point<std::chrono::steady_clock> time_point;\nSTD::CHRONO::STEADY_CLOCK::NOW STATIC MEMBER FUNCTION \nObtains the current time from the system-wide steady clock. \nDeclaration\ntime_point now() noexcept;\n\n\n416\nAPPENDIX D\nC++ Thread Library reference\nReturns\nA time_point representing the current time of the system-wide steady clock. \nThrows\nAn exception of type std::system_error if an error occurs. \nSynchronization\nIf one call to std::chrono::steady_clock::now() happens-before another, the\ntime_point returned by the first call shall compare less-than or equal-to the time_\npoint returned by the second call. \nD.1.5\nstd::chrono::high_resolution_clock typedef \nThe std::chrono::high_resolution_clock class provides access to the system-wide\nclock with the highest resolution. As for all clocks, the current time can be obtained\nby calling std::chrono::high_resolution_clock::now(). std::chrono::high_\nresolution_clock may be a typedef for the std::chrono::system_clock class or the\nstd::chrono::steady_clock class, or it may be a separate type. \n Although std::chrono::high_resolution_clock has the highest resolution of all\nthe library-supplied clocks, std::chrono::high_resolution_clock::now() still takes\na finite amount of time. You must take care to account for the overhead of calling\nstd::chrono::high_resolution_clock::now() when timing short operations. \nClass definition\nclass high_resolution_clock\n{\npublic:\n    typedef unspecified-integral-type rep;\n    typedef std::ratio<\n        unspecified,unspecified> period;\n    typedef std::chrono::duration<rep,period> duration;\n    typedef std::chrono::time_point<\n        unspecified> time_point;\n    static const bool is_steady=unspecified;\n    static time_point now() noexcept;\n};\nD.2\n<condition_variable> header \nThe <condition_variable> header provides condition variables. These are basic-\nlevel synchronization mechanisms that allow a thread to block until notified that some\ncondition is true or a timeout period has elapsed. \nHeader contents\nnamespace std\n{\n    enum class cv_status { timeout, no_timeout };\n    class condition_variable;\n    class condition_variable_any;\n}\n\n\n417\n<condition_variable> header\nD.2.1\nstd::condition_variable class \nThe std::condition_variable class allows a thread to wait for a condition to become\ntrue. Instances of std::condition_variable aren’t CopyAssignable, CopyConstruct-\nible, MoveAssignable, or MoveConstructible. \nClass definition\nclass condition_variable\n{\npublic:\n    condition_variable();\n    ~condition_variable();\n    condition_variable(condition_variable const& ) = delete;\n    condition_variable& operator=(condition_variable const& ) = delete;\n    void notify_one() noexcept;\n    void notify_all() noexcept;\n    void wait(std::unique_lock<std::mutex>& lock);\n    template <typename Predicate>\n    void wait(std::unique_lock<std::mutex>& lock,Predicate pred);\n    template <typename Clock, typename Duration>\n    cv_status wait_until(\n        std::unique_lock<std::mutex>& lock,\n        const std::chrono::time_point<Clock, Duration>& absolute_time);\n    template <typename Clock, typename Duration, typename Predicate>\n    bool wait_until(\n        std::unique_lock<std::mutex>& lock,\n        const std::chrono::time_point<Clock, Duration>& absolute_time,\n        Predicate pred);\n    template <typename Rep, typename Period>\n    cv_status wait_for(\n        std::unique_lock<std::mutex>& lock,\n        const std::chrono::duration<Rep, Period>& relative_time);\n    template <typename Rep, typename Period, typename Predicate>\n    bool wait_for(\n        std::unique_lock<std::mutex>& lock,\n        const std::chrono::duration<Rep, Period>& relative_time,\n        Predicate pred);\n};\nvoid notify_all_at_thread_exit(condition_variable&,unique_lock<mutex>);\nSTD::CONDITION_VARIABLE DEFAULT CONSTRUCTOR \nConstructs an std::condition_variable object. \nDeclaration\ncondition_variable();\nEffects\nConstructs a new std::condition_variable instance. \n\n\n418\nAPPENDIX D\nC++ Thread Library reference\nThrows\nAn exception of type std::system_error if the condition variable could not be\nconstructed. \nSTD::CONDITION_VARIABLE DESTRUCTOR \nDestroys an std::condition_variable object. \nDeclaration\n~condition_variable();\nPreconditions\nThere are no threads blocked on *this in a call to wait(), wait_for(), or\nwait_until(). \nEffects\nDestroys *this. \nThrows\nNothing. \nSTD::CONDITION_VARIABLE::NOTIFY_ONE MEMBER FUNCTION \nWakes one of the threads currently waiting on a std::condition_variable. \nDeclaration\nvoid notify_one() noexcept;\nEffects\nWakes one of the threads waiting on *this at the point of the call. If there are no\nthreads waiting, the call has no effect. \nThrows\nstd::system_error if the effects can’t be achieved. \nSynchronization \nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE::NOTIFY_ALL MEMBER FUNCTION \nWake all of the threads currently waiting on a std::condition_variable. \nDeclaration\nvoid notify_all() noexcept;\nEffects\nWakes all of the threads waiting on *this at the point of the call. If there are no\nthreads waiting, the call has no effect. \nThrows\nstd::system_error if the effects can’t be achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \n\n\n419\n<condition_variable> header\nSTD::CONDITION_VARIABLE::WAIT MEMBER FUNCTION \nWaits until std::condition_variable is woken by a call to notify_one(), a call to\nnotify_all(), or a spurious wakeup. \nDeclaration\nvoid wait(std::unique_lock<std::mutex>& lock);\nPreconditions\nlock.owns_lock()is true, and the lock is owned by the calling thread. \nEffects\nAtomically unlocks the supplied lock object and block until the thread is woken by\na call to notify_one()or notify_all()by another thread, or the thread is woken\nspuriously. The lock object is locked again before the call to wait() returns. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait(), it’s locked again on exit, even if the function exits via an\nexception. \nNOTE\nThe spurious wakeups mean that a thread calling wait() may wake\neven though no thread has called notify_one() or notify_all(). It’s there-\nfore recommended that the overload of wait() that takes a predicate is used\nin preference where possible. Otherwise, it’s recommended that wait() be\ncalled in a loop that tests the predicate associated with the condition variable. \nSynchronization \nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE::WAIT MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWaits until std::condition_variable is woken by a call to notify_one() or notify_\nall(), and the predicate is true. \nDeclaration\ntemplate<typename Predicate>\nvoid wait(std::unique_lock<std::mutex>& lock,Predicate pred);\nPreconditions\nThe expression pred() shall be valid and shall return a value convertible to bool.\nlock.owns_lock() shall be true, and the lock shall be owned by the thread calling\nwait(). \nEffects\nAs-if \nwhile(!pred())\n{\n    wait(lock);\n}\n\n\n420\nAPPENDIX D\nC++ Thread Library reference\nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects couldn’t\nbe achieved. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex ref-\nerenced by lock locked, and the function shall return if (and only if) an eval-\nuation of (bool)pred() returns true. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for() and wait_until() on a\nsingle std::condition_variable instance are serialized. A call to notify_one() or\nnotify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE::WAIT_FOR MEMBER FUNCTION \nWaits until std::condition_variable is notified by a call to notify_one() or noti-\nfy_all(), or until a specified time period has elapsed or the thread is woken spuri-\nously. \nDeclaration\ntemplate<typename Rep,typename Period>\ncv_status wait_for(\n    std::unique_lock<std::mutex>& lock,\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nlock.owns_lock() is true, and the lock is owned by the calling thread. \nEffects\nAtomically unlocks the supplied lock object and block until the thread is woken by\na call to notify_one() or notify_all() by another thread, or the time period\nspecified by relative_time has elapsed, or the thread is woken spuriously. The\nlock object is locked again before the call to wait_for() returns. \nReturns\nstd::cv_status::no_timeout if the thread was woken by a call to notify_one(), a\ncall to notify_all(), or a spurious wakeup, std::cv_status::timeout otherwise. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait_for(), it’s locked again on exit, even if the function exits\nvia an exception. \nNOTE\nThe spurious wakeups mean that a thread calling wait_for() may\nwake even though no thread has called notify_one() or notify_all(). It’s\ntherefore recommended that the overload of wait_for() that takes a predi-\ncate is used in preference where possible. Otherwise, it’s recommended that\nwait_for() be called in a loop that tests the predicate associated with the con-\ndition variable. Care must be taken when doing this to ensure that the timeout\nis still valid; wait_until() may be more appropriate in many circumstances.\n\n\n421\n<condition_variable> header\nThe thread may be blocked for longer than the specified duration. Where\npossible, the elapsed time is determined by a steady clock. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE::WAIT_FOR MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWait until std::condition_variable is woken by a call to notify_one() or notify_\nall() and the predicate is true, or until the specified time period has elapsed. \nDeclaration\ntemplate<typename Rep,typename Period,typename Predicate>\nbool wait_for(\n    std::unique_lock<std::mutex>& lock,\n    std::chrono::duration<Rep,Period> const& relative_time,\n    Predicate pred);\nPreconditions\nThe expression pred() shall be valid and shall return a value that’s convertible to\nbool. lock.owns_lock() shall be true, and the lock shall be owned by the thread\ncalling wait(). \nEffects\nAs-if \ninternal_clock::time_point end=internal_clock::now()+relative_time;\nwhile(!pred())\n{\n    std::chrono::duration<Rep,Period> remaining_time=\n        end-internal_clock::now();\n    if(wait_for(lock,remaining_time)==std::cv_status::timeout)\n        return pred();\n}\nreturn true;\nReturns\ntrue if the most recent call to pred() returned true, false if the time period spec-\nified by relative_time has elapsed and pred() returned false. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex\nreferenced by lock locked, and the function shall return if (and only if) an\nevaluation of (bool)pred() returns true or the time period specified by\nrelative_time has elapsed. The thread may be blocked for longer than the\nspecified duration. Where possible, the elapsed time is determined by a\nsteady clock. \nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects couldn’t\nbe achieved. \n\n\n422\nAPPENDIX D\nC++ Thread Library reference\nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE::WAIT_UNTIL MEMBER FUNCTION \nWaits until std::condition_variable is notified by a call to notify_one() or notify\n_all(), until a specified time has been reached, or the thread is woken spuriously. \nDeclaration\ntemplate<typename Clock,typename Duration>\ncv_status wait_until(\n    std::unique_lock<std::mutex>& lock,\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nlock.owns_lock() is true, and the lock is owned by the calling thread. \nEffects \nAtomically unlocks the supplied lock object and block until the thread is woken by a\ncall to notify_one() or notify_all() by another thread, or Clock::now() returns a\ntime equal to or later than absolute_time or the thread is woken spuriously. The\nlock object is locked again before the call to wait_until() returns. \nReturns\nstd::cv_status::no_timeout if the thread was woken by a call to notify_one(), a\ncall to notify_all(), or a spurious wakeup, std::cv_status::timeout otherwise. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait_until(), it’s locked again on exit, even if the function exits\nvia an exception. \nNOTE\nThe spurious wakeups mean that a thread calling wait_until() may\nwake even though no thread has called notify_one() or notify_all(). It’s\ntherefore recommended that the overload of wait_until() that takes a pred-\nicate is used in preference where possible. Otherwise, it’s recommended that\nwait_until() be called in a loop that tests the predicate associated with the\ncondition variable. There’s no guarantee as to how long the calling thread\nwill be blocked, only that if the function returns false, then Clock::now()\nreturns a time equal to or later than absolute_time at the point at which the\nthread became unblocked. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \n\n\n423\n<condition_variable> header\nSTD::CONDITION_VARIABLE::WAIT_UNTIL MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWait until std::condition_variable is woken by a call to notify_one() or notify_\nall() and the predicate is true, or until the specified time has been reached. \nDeclaration\ntemplate<typename Clock,typename Duration,typename Predicate>\nbool wait_until(\n    std::unique_lock<std::mutex>& lock,\n    std::chrono::time_point<Clock,Duration> const& absolute_time,\n    Predicate pred);\nPreconditions\nThe expression pred() shall be valid and shall return a value convertible to bool.\nlock.owns_lock() shall be true, and the lock shall be owned by the thread calling\nwait(). \nEffects\nAs-if \nwhile(!pred())\n{\n    if(wait_until(lock,absolute_time)==std::cv_status::timeout)\n        return pred();\n}\nreturn true;\nReturns \ntrue if the most recent call to pred() returned true, false if a call to Clock::now()\nreturned a time equal to or later than the time specified by absolute_time and\npred() returned false. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex ref-\nerenced by lock locked, and the function shall return if (and only if) an eval-\nuation of (bool)pred() returns true or Clock::now() returns a time equal to\nor later than absolute_time. There’s no guarantee as to how long the calling\nthread will be blocked, only that if the function returns false, then Clock::\nnow() returns a time equal to or later than absolute_time at the point at\nwhich the thread became unblocked. \nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects couldn’t\nbe achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_until(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one() or\nnotify_all() will wake only threads that started waiting prior to that call. \n",
      "page_number": 422
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 447-454)",
      "start_page": 447,
      "end_page": 454,
      "detection_method": "topic_boundary",
      "content": "424\nAPPENDIX D\nC++ Thread Library reference\nSTD::NOTIFY_ALL_AT_THREAD_EXIT NONMEMBER FUNCTION \nWake all of the threads waiting on a specific a std::condition_variable when the\ncurrent thread exits. \nDeclaration\nvoid notify_all_at_thread_exit(\n    condition_variable& cv,unique_lock<mutex> lk);\nPreconditions\nlk.owns_lock() is true, and the lock is owned by the calling thread. lk.mutex()\nshall return the same value as for any of the lock objects supplied to wait(),\nwait_for(), or wait_until() on cv from concurrently waiting threads. \nEffects\nTransfers ownership of the lock held by lk into internal storage and schedules cv\nto be notified when the calling thread exits. This notification shall be as-if \nlk.unlock();\ncv.notify_all();\nThrows\nstd::system_error if the effects can’t be achieved. \nNOTE\nThe lock is held until the thread exits, so care must be taken to avoid\ndeadlock. It’s recommended that the calling thread should exit as soon as\npossible and that no blocking operations be performed on this thread. \nThe user should ensure that waiting threads don’t erroneously assume that the\nthread has exited when they are woken, particularly with the potential for spurious\nwakeups. This can be achieved by testing a predicate on the waiting thread that’s\nonly made true by the notifying thread under the protection of the mutex and\nwithout releasing the lock on the mutex prior to the call of notify_all_at_thread\n_exit.std::condition_variable_any class.\nD.2.2\nstd::condition_variable_any class\nThe std::condition_variable_any class allows a thread to wait for a condition to\nbecome true. Whereas std::condition_variable can be used only with std::unique_\nlock<std::mutex>, std::condition_variable_any can be used with any type that\nmeets the Lockable requirements. \n Instances of std::condition_variable_any aren’t CopyAssignable, Copy-\nConstructible, MoveAssignable, or MoveConstructible. \nClass definition\nclass condition_variable_any\n{\npublic:\n    condition_variable_any();\n    ~condition_variable_any();\n    condition_variable_any(\n        condition_variable_any const& ) = delete;\n\n\n425\n<condition_variable> header\n    condition_variable_any& operator=(\n        condition_variable_any const& ) = delete;\n    void notify_one() noexcept;\n    void notify_all() noexcept;\n    template<typename Lockable>\n    void wait(Lockable& lock);\n    template <typename Lockable, typename Predicate>\n    void wait(Lockable& lock, Predicate pred);\n    template <typename Lockable, typename Clock,typename Duration>\n    std::cv_status wait_until(\n        Lockable& lock,\n        const std::chrono::time_point<Clock, Duration>& absolute_time);\n    template <\n        typename Lockable, typename Clock,\n        typename Duration, typename Predicate>\n    bool wait_until(\n        Lockable& lock,\n        const std::chrono::time_point<Clock, Duration>& absolute_time,\n        Predicate pred);\n    template <typename Lockable, typename Rep, typename Period>\n    std::cv_status wait_for(\n        Lockable& lock,\n        const std::chrono::duration<Rep, Period>& relative_time);\n    template <\n        typename Lockable, typename Rep,\n        typename Period, typename Predicate>\n    bool wait_for(\n        Lockable& lock,\n        const std::chrono::duration<Rep, Period>& relative_time,\n        Predicate pred);\n};\nSTD::CONDITION_VARIABLE_ANY DEFAULT CONSTRUCTOR \nConstructs an std::condition_variable_any object. \nDeclaration\ncondition_variable_any();\nEffects\nConstructs a new std::condition_variable_any instance. \nThrows\nAn exception of type std::system_error if the condition variable couldn’t be con-\nstructed. \nSTD::CONDITION_VARIABLE_ANY DESTRUCTOR \nDestroys an std::condition_variable_any object. \nDeclaration\n~condition_variable_any();\n\n\n426\nAPPENDIX D\nC++ Thread Library reference\nPreconditions\nThere are no threads blocked on *this in a call to wait(), wait_for(), or wait_\nuntil(). \nEffects \nDestroys *this. \nThrows\nNothing. \nSTD::CONDITION_VARIABLE_ANY::NOTIFY_ONE MEMBER FUNCTION \nWakes one of the threads currently waiting on a specific a std::condition_variable\n_any. \nDeclaration\nvoid notify_one() noexcept;\nEffects\nWakes one of the threads waiting on *this at the point of the call. If there are no\nthreads waiting, the call has no effect. \nThrows\nstd::system_error if the effects can’t be achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE_ANY::NOTIFY_ALL MEMBER FUNCTION \nWakes all of the threads currently waiting on a specific a std::condition_variable\n_any. \nDeclaration\nvoid notify_all() noexcept;\nEffects\nWakes all of the threads waiting on *this at the point of the call. If there are no\nthreads waiting, the call has no effect. \nThrows\nstd::system_error if the effects can’t be achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE_ANY::WAIT MEMBER FUNCTION \nWaits until std::condition_variable_any is woken by a call to notify_one(), a call\nto notify_all(), or a spurious wakeup. \nDeclaration\ntemplate<typename Lockable>\nvoid wait(Lockable& lock);\n\n\n427\n<condition_variable> header\nPreconditions\nLockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAtomically unlocks the supplied lock object and block until the thread is woken by\na call to notify_one() or notify_all() by another thread, or the thread is woken\nspuriously. The lock object is locked again before the call to wait() returns. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait(), it’s locked again on exit, even if the function exits via an\nexception. \nNOTE\nThe spurious wakeups mean that a thread calling wait() may wake\neven though no thread has called notify_one() or notify_all(). It’s there-\nfore recommended that the overload of wait() that takes a predicate is used\nin preference where possible. Otherwise, it’s recommended that wait() be\ncalled in a loop that tests the predicate associated with the condition variable. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE_ANY::WAIT MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWaits until std::condition_variable_any is woken by a call to notify_one() or\nnotify_all() and the predicate is true. \nDeclaration\ntemplate<typename Lockable,typename Predicate>\nvoid wait(Lockable& lock,Predicate pred);\nPreconditions\nThe expression pred() shall be valid and shall return a value that’s convertible to\nbool. Lockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAs-if \nwhile(!pred())\n{\n    wait(lock);\n}\nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects could\nnot be achieved. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex ref-\nerenced by lock locked, and the function shall return if (and only if) an eval-\nuation of (bool)pred() returns true. \n\n\n428\nAPPENDIX D\nC++ Thread Library reference\nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE_ANY::WAIT_FOR MEMBER FUNCTION \nWaits until std::condition_variable_any is notified by a call to notify_one() or\nnotify_all(), until a specified time period has elapsed, or the thread is woken spu-\nriously. \nDeclaration\ntemplate<typename Lockable,typename Rep,typename Period>\nstd::cv_status wait_for(\n    Lockable& lock,\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions \nLockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAtomically unlocks the supplied lock object and block until the thread is woken by\na call to notify_one() or notify_all() by another thread or the time period spec-\nified by relative_time has elapsed or the thread is woken spuriously. The lock\nobject is locked again before the call to wait_for() returns. \nReturns\nstd::cv_status::no_timeout if the thread was woken by a call to notify_one(), a\ncall to notify_all(), or a spurious wakeup, std::cv_status::timeout otherwise. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait_for(), it’s locked again on exit, even if the function exits\nvia an exception. \nNOTE\nThe spurious wakeups mean that a thread calling wait_for() may\nwake even though no thread has called notify_one() or notify_all(). It’s\ntherefore recommended that the overload of wait_for() that takes a predi-\ncate is used in preference where possible. Otherwise, it’s recommended that\nwait_for() be called in a loop that tests the predicate associated with the\ncondition variable. Care must be taken when doing this to ensure that the\ntimeout is still valid; wait_until() may be more appropriate in many circum-\nstances. The thread may be blocked for longer than the specified duration.\nWhere possible, the elapsed time is determined by a steady clock. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \n\n\n429\n<condition_variable> header\nSTD::CONDITION_VARIABLE_ANY::WAIT_FOR MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWaits until std::condition_variable_any is woken by a call to notify_one() or notify\n_all() and the predicate is true, or until the specified time period has elapsed. \nDeclaration\ntemplate<typename Lockable,typename Rep,\n    typename Period, typename Predicate>\nbool wait_for(\n    Lockable& lock,\n    std::chrono::duration<Rep,Period> const& relative_time,\n    Predicate pred);\nPreconditions\nThe expression pred() shall be valid and shall return a value that’s convertible to\nbool. Lockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAs-if \ninternal_clock::time_point end=internal_clock::now()+relative_time;\nwhile(!pred())\n{\n    std::chrono::duration<Rep,Period> remaining_time=\n        end-internal_clock::now();\n    if(wait_for(lock,remaining_time)==std::cv_status::timeout)\n        return pred();\n}\nreturn true;\nReturns\ntrue if the most recent call to pred() returned true, false if the time period spec-\nified by relative_time has elapsed and pred() returned false. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex\nreferenced by lock locked, and the function shall return if (and only if) an\nevaluation of (bool)pred() returns true or the time period specified by\nrelative_time has elapsed. The thread may be blocked for longer than the\nspecified duration. Where possible, the elapsed time is determined by a\nsteady clock. \nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects couldn’t\nbe achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \n\n\n430\nAPPENDIX D\nC++ Thread Library reference\nSTD::CONDITION_VARIABLE_ANY::WAIT_UNTIL MEMBER FUNCTION \nWaits until std::condition_variable_any is notified by a call to notify_one() or\nnotify_all(), until a specified time has been reached, or the thread is woken\nspuriously. \nDeclaration\ntemplate<typename Lockable,typename Clock,typename Duration>\nstd::cv_status wait_until(\n    Lockable& lock,\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nLockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAtomically unlocks the supplied lock object and block until the thread is woken by\na call to notify_one() or notify_all() by another thread, Clock::now() returns\na time equal to or later than absolute_time, or the thread is woken spuriously. The\nlock object is locked again before the call to wait_until() returns. \nReturns\nstd::cv_status::no_timeout if the thread was woken by a call to notify_one(), a\ncall to notify_all(), or a spurious wakeup, std::cv_status::timeout otherwise. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait_until(), it’s locked again on exit, even if the function exits\nvia an exception. \nNOTE\nThe spurious wakeups mean that a thread calling wait_until() may\nwake even though no thread has called notify_one() or notify_all(). It’s\ntherefore recommended that the overload of wait_until() that takes a pred-\nicate is used in preference where possible. Otherwise, it’s recommended that\nwait_until() be called in a loop that tests the predicate associated with the\ncondition variable. There’s no guarantee as to how long the calling thread\nwill be blocked, only that if the function returns false, then Clock::now()\nreturns a time equal to or later than absolute_time at the point at which the\nthread became unblocked. \nSynchronization \nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE_ANY::WAIT_UNTIL MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWaits until std::condition_variable_any is woken by a call to notify_one() or\nnotify_all() and the predicate is true, or until the specified time has been reached. \nDeclaration\ntemplate<typename Lockable,typename Clock,\n    typename Duration, typename Predicate>\n\n\n431\n<atomic> header\nbool wait_until(\n    Lockable& lock,\n    std::chrono::time_point<Clock,Duration> const& absolute_time,\n    Predicate pred);\nPreconditions\nThe expression pred() shall be valid, and shall return a value that’s convertible to\nbool. Lockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAs-if \nwhile(!pred())\n{\n    if(wait_until(lock,absolute_time)==std::cv_status::timeout)\n        return pred();\n}\nreturn true;\nReturns\ntrue if the most recent call to pred() returned true, false if a call to Clock::\nnow() returned a time equal to or later than the time specified by absolute_time,\nand pred() returned false. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex ref-\nerenced by lock locked, and the function shall return if (and only if) an eval-\nuation of (bool)pred() returns true or Clock::now() returns a time equal to\nor later than absolute_time. There’s no guarantee as to how long the calling\nthread will be blocked, only that if the function returns false, then Clock::\nnow() returns a time equal to or later than absolute_time at the point at\nwhich the thread became unblocked. \nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects couldn’t\nbe achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_until(), and wait_until()\non a single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nD.3\n<atomic> header \nThe <atomic> header provides the set of basic atomic types and operations on those\ntypes and a class template for constructing an atomic version of a user-defined type\nthat meets certain criteria. \nHeader contents\n#define ATOMIC_BOOL_LOCK_FREE see description\n#define ATOMIC_CHAR_LOCK_FREE see description\n#define ATOMIC_SHORT_LOCK_FREE see description\n",
      "page_number": 447
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 455-489)",
      "start_page": 455,
      "end_page": 489,
      "detection_method": "topic_boundary",
      "content": "432\nAPPENDIX D\nC++ Thread Library reference\n#define ATOMIC_INT_LOCK_FREE see description\n#define ATOMIC_LONG_LOCK_FREE see description\n#define ATOMIC_LLONG_LOCK_FREE see description\n#define ATOMIC_CHAR16_T_LOCK_FREE see description\n#define ATOMIC_CHAR32_T_LOCK_FREE see description\n#define ATOMIC_WCHAR_T_LOCK_FREE see description\n#define ATOMIC_POINTER_LOCK_FREE see description\n#define ATOMIC_VAR_INIT(value) see description\nnamespace std\n{\n    enum memory_order;\n    struct atomic_flag;\n    typedef see description atomic_bool;\n    typedef see description atomic_char;\n    typedef see description atomic_char16_t;\n    typedef see description atomic_char32_t;\n    typedef see description atomic_schar;\n    typedef see description atomic_uchar;\n    typedef see description atomic_short;\n    typedef see description atomic_ushort;\n    typedef see description atomic_int;\n    typedef see description atomic_uint;\n    typedef see description atomic_long;\n    typedef see description atomic_ulong;\n    typedef see description atomic_llong;\n    typedef see description atomic_ullong;\n    typedef see description atomic_wchar_t;\n    typedef see description atomic_int_least8_t;\n    typedef see description atomic_uint_least8_t;\n    typedef see description atomic_int_least16_t;\n    typedef see description atomic_uint_least16_t;\n    typedef see description atomic_int_least32_t;\n    typedef see description atomic_uint_least32_t;\n    typedef see description atomic_int_least64_t;\n    typedef see description atomic_uint_least64_t;\n    typedef see description atomic_int_fast8_t;\n    typedef see description atomic_uint_fast8_t;\n    typedef see description atomic_int_fast16_t;\n    typedef see description atomic_uint_fast16_t;\n    typedef see description atomic_int_fast32_t;\n    typedef see description atomic_uint_fast32_t;\n    typedef see description atomic_int_fast64_t;\n    typedef see description atomic_uint_fast64_t;\n    typedef see description atomic_int8_t;\n    typedef see description atomic_uint8_t;\n    typedef see description atomic_int16_t;\n    typedef see description atomic_uint16_t;\n    typedef see description atomic_int32_t;\n    typedef see description atomic_uint32_t;\n    typedef see description atomic_int64_t;\n    typedef see description atomic_uint64_t;\n    typedef see description atomic_intptr_t;\n    typedef see description atomic_uintptr_t;\n\n\n433\n<atomic> header\n    typedef see description atomic_size_t;\n    typedef see description atomic_ssize_t;\n    typedef see description atomic_ptrdiff_t;\n    typedef see description atomic_intmax_t;\n    typedef see description atomic_uintmax_t;\n    template<typename T>\n    struct atomic;\n    extern \"C\" void atomic_thread_fence(memory_order order);\n    extern \"C\" void atomic_signal_fence(memory_order order);\n    template<typename T>\n    T kill_dependency(T);\n}\nD.3.1\nstd::atomic_xxx typedefs \nFor compatibility with the forthcoming C Standard, typedefs for the atomic integral\ntypes are provided. For C++17, these must be typedefs to the corresponding std::\natomic<T> specialization; for prior C++ standards, they may instead be a base class of\nthat specialization with the same interface. \nTable D.1\nAtomic typedefs and their corresponding std::atomic<> specializations\nstd::atomic_itype \nstd::atomic<> specialization\nstd::atomic_char \nstd::atomic<char> \nstd::atomic_schar \nstd::atomic<signed char> \nstd::atomic_uchar \nstd::atomic<unsigned char> \nstd::atomic_short \nstd::atomic<short> \nstd::atomic_ushort \nstd::atomic<unsigned short> \nstd::atomic_int \nstd::atomic<int> \nstd::atomic_uint \nstd::atomic<unsigned int> \nstd::atomic_long \nstd::atomic<long> \nstd::atomic_ulong \nstd::atomic<unsigned long> \nstd::atomic_llong \nstd::atomic<long long> \nstd::atomic_ullong \nstd::atomic<unsigned long long> \nstd::atomic_wchar_t \nstd::atomic<wchar_t> \nstd::atomic_char16_t \nstd::atomic<char16_t> \nstd::atomic_char32_t \nstd::atomic<char32_t> \n\n\n434\nAPPENDIX D\nC++ Thread Library reference\nD.3.2\nATOMIC_xxx_LOCK_FREE macros\nThese macros specify whether the atomic types corresponding to particular built-in\ntypes are lock-free. \nMacro declarations\n#define ATOMIC_BOOL_LOCK_FREE see description\n#define ATOMIC_CHAR_LOCK_FREE see description\n#define ATOMIC_SHORT_LOCK_FREE see description\n#define ATOMIC_INT_LOCK_FREE see description\n#define ATOMIC_LONG_LOCK_FREE see description\n#define ATOMIC_LLONG_LOCK_FREE see description\n#define ATOMIC_CHAR16_T_LOCK_FREE see description\n#define ATOMIC_CHAR32_T_LOCK_FREE see description\n#define ATOMIC_WCHAR_T_LOCK_FREE see description\n#define ATOMIC_POINTER_LOCK_FREE see description\nThe value of ATOMIC_xxx_LOCK_FREE is either 0, 1, or 2. A value of 0 means that\noperations on both the signed and unsigned atomic types corresponding to the\nnamed type are never lock-free, a value of 1 means that the operations may be lock-\nfree for particular instances of those types and not for others, and a value of 2\nmeans that the operations are always lock-free. For example, if ATOMIC_INT_\nLOCK_FREE is 2, operations on instances of std::atomic<int> and std::atomic\n<unsigned> are always lock-free. \nThe ATOMIC_POINTER_LOCK_FREE macro describes the lock-free property of oper-\nations on the atomic pointer specializations std::atomic<T*>. \nD.3.3\nATOMIC_VAR_INIT macro \nThe ATOMIC_VAR_INIT macro provides a means of initializing an atomic variable to a\nparticular value. \nDeclaration\n#define ATOMIC_VAR_INIT(value) see description\nThe macro expands to a token sequence that can be used to initialize one of the stan-\ndard atomic types with the specified value in an expression of the following form: \nstd::atomic<type> x = ATOMIC_VAR_INIT(val);\nThe specified value must be compatible with the nonatomic type corresponding to\nthe atomic variable; for example:\nstd::atomic<int> i = ATOMIC_VAR_INIT(42);\nstd::string s;\nstd::atomic<std::string*> p = ATOMIC_VAR_INIT(&s);\nThis initialization is not atomic, and any access by another thread to the variable\nbeing initialized where the initialization doesn’t happen-before that access is a data\nrace and thus undefined behavior. \n\n\n435\n<atomic> header\nD.3.4\nstd::memory_order enumeration \nThe std::memory_order enumeration is used to specify the ordering constraints of\natomic operations. \nDeclaration\ntypedef enum memory_order\n{\n    memory_order_relaxed,memory_order_consume,\n    memory_order_acquire,memory_order_release,\n    memory_order_acq_rel,memory_order_seq_cst\n} memory_order;\nOperations tagged with the various memory order values behave as follows (see\nchapter 5 for detailed descriptions of the ordering constraints). \nSTD::MEMORY_ORDER_RELAXED \nThe operation doesn’t provide any additional ordering constraints. \nSTD::MEMORY_ORDER_RELEASE \nThe operation is a release operation on the specified memory location. This therefore\nsynchronizes-with an acquire operation on the same memory location that reads the\nstored value. \nSTD::MEMORY_ORDER_ACQUIRE \nThe operation is an acquire operation on the specified memory location. If the stored\nvalue was written by a release operation, that store synchronizes-with this operation. \nSTD::MEMORY_ORDER_ACQ_REL \nThe operation must be a read-modify-write operation, and it behaves as both std::\nmemory_order_acquire and std::memory_order_release on the specified location. \nSTD::MEMORY_ORDER_SEQ_CST \nThe operation forms part of the single global total order of sequentially consistent\noperations. In addition, if it’s a store, it behaves like an std::memory_order_release\noperation; if it’s a load, it behaves like an std::memory_order_acquire operation;\nand if it’s a read-modify-write operation, it behaves as both std::memory_order_\nacquire and std::memory_order_release. This is the default for all operations. \nSTD::MEMORY_ORDER_CONSUME \nThe operation is a consume operation on the specified memory location. The C++17\nStandard states that this memory ordering should not be used.\nD.3.5\nstd::atomic_thread_fence function \nThe std::atomic_thread_fence() function inserts a “memory barrier” or “fence” in\nthe code to force memory-ordering constraints between operations. \nDeclaration\nextern \"C\" void atomic_thread_fence(std::memory_order order);\n\n\n436\nAPPENDIX D\nC++ Thread Library reference\nEffects\nInserts a fence with the required memory-ordering constraints. \nA fence with an order of std::memory_order_release, std::memory_order_\nacq_rel, or std::memory_order_seq_cst synchronizes-with an acquire operation\non the same memory location if that acquire operation reads a value stored by an\natomic operation following the fence on the same thread as the fence. \nA release operation synchronizes-with a fence with an order of std::memory\n_order_acquire, std::memory_order_acq_rel, or std::memory_order_seq_cst if\nthat release operation stores a value that’s read by an atomic operation prior to the\nfence on the same thread as the fence. \nThrows\nNothing. \nD.3.6\nstd::atomic_signal_fence function \nThe std::atomic_signal_fence() function inserts a memory barrier or fence in the\ncode to force memory ordering constraints between operations on a thread and oper-\nations in a signal handler on that thread. \nDeclaration\nextern \"C\" void atomic_signal_fence(std::memory_order order);\nEffects\nInserts a fence with the required memory-ordering constraints. This is equivalent to\nstd::atomic_thread_fence(order) except that the constraints apply only between\na thread and a signal handler on the same thread. \nThrows\nNothing. \nD.3.7\nstd::atomic_flag class \nThe std::atomic_flag class provides a simple bare-bones atomic flag. It’s the only\ndata type that’s guaranteed to be lock-free by the C++11 Standard (although many\natomic types will be lock-free in most implementations).\n An instance of std::atomic_flag is either set or clear. \nClass definition\nstruct atomic_flag\n{\n    atomic_flag() noexcept = default;\n    atomic_flag(const atomic_flag&) = delete;\n    atomic_flag& operator=(const atomic_flag&) = delete;\n    atomic_flag& operator=(const atomic_flag&) volatile = delete;\n    bool test_and_set(memory_order = memory_order_seq_cst) volatile \nnoexcept;\n    bool test_and_set(memory_order = memory_order_seq_cst) noexcept;\n    void clear(memory_order = memory_order_seq_cst) volatile noexcept;\n    void clear(memory_order = memory_order_seq_cst) noexcept;\n};\n\n\n437\n<atomic> header\nbool atomic_flag_test_and_set(volatile atomic_flag*) noexcept;\nbool atomic_flag_test_and_set(atomic_flag*) noexcept;\nbool atomic_flag_test_and_set_explicit(\n    volatile atomic_flag*, memory_order) noexcept;\nbool atomic_flag_test_and_set_explicit(\n    atomic_flag*, memory_order) noexcept;\nvoid atomic_flag_clear(volatile atomic_flag*) noexcept;\nvoid atomic_flag_clear(atomic_flag*) noexcept;\nvoid atomic_flag_clear_explicit(\n    volatile atomic_flag*, memory_order) noexcept;\nvoid atomic_flag_clear_explicit(\n    atomic_flag*, memory_order) noexcept;\n#define ATOMIC_FLAG_INIT unspecified\nSTD::ATOMIC_FLAG DEFAULT CONSTRUCTOR \nIt’s unspecified whether a default-constructed instance of std::atomic_flag is clear\nor set. For objects of static storage duration, initialization shall be static initialization.\nDeclaration\nstd::atomic_flag() noexcept = default;\nEffects\nConstructs a new std::atomic_flag object in an unspecified state. \nThrows\nNothing.\nSTD::ATOMIC_FLAG INITIALIZATION WITH ATOMIC_FLAG_INIT \nAn instance of std::atomic_flag may be initialized using the ATOMIC_FLAG_INIT\nmacro, in which case it’s initialized into the clear state. For objects of static storage\nduration, initialization shall be static initialization.\nDeclaration\n#define ATOMIC_FLAG_INIT unspecified\nUsage\nstd::atomic_flag flag=ATOMIC_FLAG_INIT;\nEffects\nConstructs a new std::atomic_flag object in the clear state. \nThrows\nNothing. \nSTD::ATOMIC_FLAG::TEST_AND_SET MEMBER FUNCTION \nAtomically sets the flag and checks whether or not it was set. \nDeclaration\nbool test_and_set(memory_order order = memory_order_seq_cst) volatile \nnoexcept;\nbool test_and_set(memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically sets the flag. \n\n\n438\nAPPENDIX D\nC++ Thread Library reference\nReturns\ntrue if the flag was set at the point of the call, false if the flag was clear. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FLAG_TEST_AND_SET NONMEMBER FUNCTION \nAtomically sets the flag and checks whether or not it was set. \nDeclaration\nbool atomic_flag_test_and_set(volatile atomic_flag* flag) noexcept;\nbool atomic_flag_test_and_set(atomic_flag* flag) noexcept;\nEffects\nreturn flag->test_and_set();\nSTD::ATOMIC_FLAG_TEST_AND_SET_EXPLICIT NONMEMBER FUNCTION \nAtomically sets the flag and checks whether or not it was set. \nDeclaration\nbool atomic_flag_test_and_set_explicit(\n    volatile atomic_flag* flag, memory_order order) noexcept;\nbool atomic_flag_test_and_set_explicit(\n    atomic_flag* flag, memory_order order) noexcept;\nEffects\nreturn flag->test_and_set(order);\nSTD::ATOMIC_FLAG::CLEAR MEMBER FUNCTION \nAtomically clears the flag. \nDeclaration\nvoid clear(memory_order order = memory_order_seq_cst) volatile noexcept;\nvoid clear(memory_order order = memory_order_seq_cst) noexcept;\nPreconditions\nThe supplied order must be one of std::memory_order_relaxed, std::memory_\norder_release, or std::memory_order_seq_cst. \nEffects\nAtomically clears the flag. \nThrows\nNothing. \nNOTE\nThis is an atomic store operation for the memory location comprising\n*this. \n\n\n439\n<atomic> header\nSTD::ATOMIC_FLAG_CLEAR NONMEMBER FUNCTION \nAtomically clears the flag. \nDeclaration\nvoid atomic_flag_clear(volatile atomic_flag* flag) noexcept;\nvoid atomic_flag_clear(atomic_flag* flag) noexcept;\nEffects\nflag->clear();\nSTD::ATOMIC_FLAG_CLEAR_EXPLICIT NONMEMBER FUNCTION \nAtomically clears the flag. \nDeclaration\nvoid atomic_flag_clear_explicit(\n    volatile atomic_flag* flag, memory_order order) noexcept;\nvoid atomic_flag_clear_explicit(\n    atomic_flag* flag, memory_order order) noexcept;\nEffects\nreturn flag->clear(order);\nD.3.8\nstd::atomic class template \nThe std::atomic class provides a wrapper with atomic operations for any type that\nsatisfies the following requirements. \n The template parameter BaseType must \n■\nHave a trivial default constructor \n■\nHave a trivial copy-assignment operator \n■\nHave a trivial destructor \n■\nBe bitwise-equality comparable \nThis means that std::atomic<some-built-in-type> is fine, as is std::atomic<some-\nsimple-struct>, but things like std::atomic<std::string> are not. \n In addition to the primary template, there are specializations for the built-in inte-\ngral types and pointers to provide additional operations, such as x++. \n Instances of std::atomic are not CopyConstructible or CopyAssignable, because\nthese operations can’t be performed as a single atomic operation. \nClass definition\ntemplate<typename BaseType>\nstruct atomic\n{\n    using value_type = T;\n    static constexpr bool is_always_lock_free = implementation-defined ;\n    atomic() noexcept = default;\n    constexpr atomic(BaseType) noexcept;\n    BaseType operator=(BaseType) volatile noexcept;\n    BaseType operator=(BaseType) noexcept;\n    atomic(const atomic&) = delete;\n    atomic& operator=(const atomic&) = delete;\n    atomic& operator=(const atomic&) volatile = delete;\n\n\n440\nAPPENDIX D\nC++ Thread Library reference\n    bool is_lock_free() const volatile noexcept;\n    bool is_lock_free() const noexcept;\n    void store(BaseType,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    void store(BaseType,memory_order = memory_order_seq_cst) noexcept;\n    BaseType load(memory_order = memory_order_seq_cst)\n        const volatile noexcept;\n    BaseType load(memory_order = memory_order_seq_cst) const noexcept;\n    BaseType exchange(BaseType,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    BaseType exchange(BaseType,memory_order = memory_order_seq_cst)\n        noexcept;\n    bool compare_exchange_strong(\n        BaseType & old_value, BaseType new_value,\n        memory_order order = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_strong(\n        BaseType & old_value, BaseType new_value,\n        memory_order order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(\n        BaseType & old_value, BaseType new_value,\n        memory_order success_order,\n        memory_order failure_order) volatile noexcept;\n    bool compare_exchange_strong(\n        BaseType & old_value, BaseType new_value,\n        memory_order success_order,\n        memory_order failure_order) noexcept;\n    bool compare_exchange_weak(\n        BaseType & old_value, BaseType new_value,\n        memory_order order = memory_order_seq_cst)\n        volatile noexcept;\n    bool compare_exchange_weak(\n        BaseType & old_value, BaseType new_value,\n        memory_order order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_weak(\n        BaseType & old_value, BaseType new_value,\n        memory_order success_order,\n        memory_order failure_order) volatile noexcept;\n    bool compare_exchange_weak(\n        BaseType & old_value, BaseType new_value,\n        memory_order success_order,\n        memory_order failure_order) noexcept;\n    operator BaseType () const volatile noexcept;\n    operator BaseType () const noexcept;\n};\ntemplate<typename BaseType>\nbool atomic_is_lock_free(volatile const atomic<BaseType>*) noexcept;\ntemplate<typename BaseType>\nbool atomic_is_lock_free(const atomic<BaseType>*) noexcept;\ntemplate<typename BaseType>\nvoid atomic_init(volatile atomic<BaseType>*, void*) noexcept;\ntemplate<typename BaseType>\nvoid atomic_init(atomic<BaseType>*, void*) noexcept;\ntemplate<typename BaseType>\n\n\n441\n<atomic> header\nBaseType atomic_exchange(volatile atomic<BaseType>*, memory_order)\n    noexcept;\ntemplate<typename BaseType>\nBaseType atomic_exchange(atomic<BaseType>*, memory_order) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_exchange_explicit(\n    volatile atomic<BaseType>*, memory_order) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_exchange_explicit(\n    atomic<BaseType>*, memory_order) noexcept;\ntemplate<typename BaseType>\nvoid atomic_store(volatile atomic<BaseType>*, BaseType) noexcept;\ntemplate<typename BaseType>\nvoid atomic_store(atomic<BaseType>*, BaseType) noexcept;\ntemplate<typename BaseType>\nvoid atomic_store_explicit(\n    volatile atomic<BaseType>*, BaseType, memory_order) noexcept;\ntemplate<typename BaseType>\nvoid atomic_store_explicit(\n    atomic<BaseType>*, BaseType, memory_order) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load(volatile const atomic<BaseType>*) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load(const atomic<BaseType>*) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load_explicit(\n    volatile const atomic<BaseType>*, memory_order) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load_explicit(\n    const atomic<BaseType>*, memory_order) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong(\n    volatile atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong(\n    atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong_explicit(\n    volatile atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong_explicit(\n    atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak(\n    volatile atomic<BaseType>*,BaseType * old_value,BaseType new_value)\n    noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak(\n    atomic<BaseType>*,BaseType * old_value,BaseType new_value) noexcept;\n\n\n442\nAPPENDIX D\nC++ Thread Library reference\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak_explicit(\n    volatile atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak_explicit(\n    atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\nNOTE\nAlthough the nonmember functions are specified as templates, they\nmay be provided as an overloaded set of functions, and explicit specification\nof the template arguments shouldn’t be used.\nSTD::ATOMIC DEFAULT CONSTRUCTOR \nConstructs an instance of std::atomic with a default-initialized value. \nDeclaration\natomic() noexcept;\nEffects\nConstructs a new std::atomic object with a default-initialized value. For objects\nwith static storage duration, this is static initialization.\nNOTE\nInstances of std::atomic with nonstatic storage duration initialized\nwith the default constructor can’t be relied on to have a predictable value. \nThrows\nNothing. \nSTD::ATOMIC_INIT NONMEMBER FUNCTION \nNonatomically stores the supplied value in an instance of std::atomic<BaseType>. \nDeclaration\ntemplate<typename BaseType>\nvoid atomic_init(atomic<BaseType> volatile* p, BaseType v) noexcept;\ntemplate<typename BaseType>\nvoid atomic_init(atomic<BaseType>* p, BaseType v) noexcept;\nEffects\nNonatomically stores the value of v in *p. Invoking atomic_init() on an instance\nof atomic<BaseType> that hasn’t been default constructed, or that has had any\noperations performed on it since construction, is undefined behavior.\nNOTE\nBecause this store is nonatomic, any concurrent access to the object\npointed to by p from another thread (even with atomic operations) consti-\ntutes a data race. \nThrows\nNothing. \n\n\n443\n<atomic> header\nSTD::ATOMIC CONVERSION CONSTRUCTOR \nConstructs an instance of std::atomic with the supplied BaseType value. \nDeclaration\nconstexpr atomic(BaseType b) noexcept;\nEffects\nConstructs a new std::atomic object with a value of b. For objects with static stor-\nage duration, this is static initialization.\nThrows\nNothing. \nSTD::ATOMIC CONVERSION ASSIGNMENT OPERATOR \nStores a new value in *this. \nDeclaration\nBaseType operator=(BaseType b) volatile noexcept;\nBaseType operator=(BaseType b) noexcept;\nEffects\nreturn this->store(b);\nSTD::ATOMIC::IS_LOCK_FREE MEMBER FUNCTION \nDetermines if operations on *this are lock-free. \nDeclaration\nbool is_lock_free() const volatile noexcept;\nbool is_lock_free() const noexcept;\nReturns\ntrue if operations on *this are lock-free, false otherwise. \nThrows\nNothing. \nSTD::ATOMIC_IS_LOCK_FREE NONMEMBER FUNCTION \nDetermines if operations on *this are lock-free. \nDeclaration\ntemplate<typename BaseType>\nbool atomic_is_lock_free(volatile const atomic<BaseType>* p) noexcept;\ntemplate<typename BaseType>\nbool atomic_is_lock_free(const atomic<BaseType>* p) noexcept;\nEffects\nreturn p->is_lock_free();\nSTD::ATOMIC::IS_ALWAYS_LOCK_FREE STATIC DATA MEMBER \nDetermines if operations on all objects of this type are always lock-free. \nDeclaration\nstatic constexpr bool is_always_lock_free() = implementation-defined;\nValue\ntrue if operations on all objects of this type are always lock-free, false otherwise. \n\n\n444\nAPPENDIX D\nC++ Thread Library reference\nSTD::ATOMIC::LOAD MEMBER FUNCTION \nAtomically loads the current value of the std::atomic instance. \nDeclaration\nBaseType load(memory_order order = memory_order_seq_cst)\n    const volatile noexcept;\nBaseType load(memory_order order = memory_order_seq_cst) const noexcept;\nPreconditions\nThe supplied order must be one of std::memory_order_relaxed, std::memory_\norder_acquire, std::memory_order_consume, or std::memory_order_seq_cst.\nEffects\nAtomically loads the value stored in *this. \nReturns\nThe value stored in *this at the point of the call. \nThrows\nNothing. \nNOTE\nThis is an atomic load operation for the memory location comprising\n*this. \nSTD::ATOMIC_LOAD NONMEMBER FUNCTION \nAtomically loads the current value of the std::atomic instance. \nDeclaration\ntemplate<typename BaseType>\nBaseType atomic_load(volatile const atomic<BaseType>* p) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load(const atomic<BaseType>* p) noexcept;\nEffects\nreturn p->load();\nSTD::ATOMIC_LOAD_EXPLICIT NONMEMBER FUNCTION \nAtomically loads the current value of the std::atomic instance. \nDeclaration\ntemplate<typename BaseType>\nBaseType atomic_load_explicit(\n    volatile const atomic<BaseType>* p, memory_order order) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load_explicit(\n    const atomic<BaseType>* p, memory_order order) noexcept;\nEffects\nreturn p->load(order);\nSTD::ATOMIC::OPERATOR BASETYPE CONVERSION OPERATOR \nLoads the value stored in *this. \nDeclaration\noperator BaseType() const volatile noexcept;\noperator BaseType() const noexcept;\n\n\n445\n<atomic> header\nEffects\nreturn this->load();\nSTD::ATOMIC::STORE MEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance.\nDeclaration\nvoid store(BaseType new_value,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nvoid store(BaseType new_value,memory_order order = memory_order_seq_cst)\n    noexcept;\nPreconditions\nThe supplied order must be one of std::memory_order_relaxed, std::memory_\norder_release, or std::memory_order_seq_cst. \nEffects\nAtomically stores new_value in *this. \nThrows\nNothing. \nNOTE\nThis is an atomic store operation for the memory location comprising\n*this. \nSTD::ATOMIC_STORE NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance.\nDeclaration\ntemplate<typename BaseType>\nvoid atomic_store(volatile atomic<BaseType>* p, BaseType new_value)\n    noexcept;\ntemplate<typename BaseType>\nvoid atomic_store(atomic<BaseType>* p, BaseType new_value) noexcept;\nEffects\np->store(new_value);\nSTD::ATOMIC_STORE_EXPLICIT NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance.\nDeclaration\ntemplate<typename BaseType>\nvoid atomic_store_explicit(\n    volatile atomic<BaseType>* p, BaseType new_value, memory_order order)\n    noexcept;\ntemplate<typename BaseType>\nvoid atomic_store_explicit(\n    atomic<BaseType>* p, BaseType new_value, memory_order order) noexcept;\nEffects\np->store(new_value,order);\n\n\n446\nAPPENDIX D\nC++ Thread Library reference\nSTD::ATOMIC::EXCHANGE MEMBER FUNCTION \nAtomically stores a new value and reads the old one. \nDeclaration\nBaseType exchange(\n    BaseType new_value,\n    memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nEffects\nAtomically stores new_value in *this and retrieves the existing value of *this. \nReturns\nThe value of *this immediately prior to the store. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_EXCHANGE NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance and reads the prior\nvalue.\nDeclaration\ntemplate<typename BaseType>\nBaseType atomic_exchange(volatile atomic<BaseType>* p, BaseType new_value)\n    noexcept;\ntemplate<typename BaseType>\nBaseType atomic_exchange(atomic<BaseType>* p, BaseType new_value) noexcept;\nEffects\nreturn p->exchange(new_value);\nSTD::ATOMIC_EXCHANGE_EXPLICIT NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance and reads the prior\nvalue.\nDeclaration\ntemplate<typename BaseType>\nBaseType atomic_exchange_explicit(\n    volatile atomic<BaseType>* p, BaseType new_value, memory_order order)\n    noexcept;\ntemplate<typename BaseType>\nBaseType atomic_exchange_explicit(\n    atomic<BaseType>* p, BaseType new_value, memory_order order) noexcept;\nEffects\nreturn p->exchange(new_value,order);\n\n\n447\n<atomic> header\nSTD::ATOMIC::COMPARE_EXCHANGE_STRONG MEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the values\nare equal. If the values aren’t equal, updates the expected value with the value read. \nDeclaration\nbool compare_exchange_strong(\n    BaseType& expected,BaseType new_value,\n    memory_order order = std::memory_order_seq_cst) volatile noexcept;\nbool compare_exchange_strong(\n    BaseType& expected,BaseType new_value,\n    memory_order order = std::memory_order_seq_cst) noexcept;\nbool compare_exchange_strong(\n    BaseType& expected,BaseType new_value,\n    memory_order success_order,memory_order failure_order)\n    volatile noexcept;\nbool compare_exchange_strong(\n    BaseType& expected,BaseType new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nPreconditions \nfailure_order shall not be std::memory_order_release or std::memory_order\n_acq_rel. \nEffects\nAtomically compares expected to the value stored in *this using bitwise compari-\nson and stores new_value in *this if equal; otherwise updates expected to the\nvalue read. \nReturns\ntrue if the existing value of *this was equal to expected, false otherwise. \nThrows\nNothing. \nNOTE\nThe three-parameter overload is equivalent to the four-parameter\noverload with success_order==order and failure_order==order, except\nthat if order is std::memory_order_acq_rel, then failure_order is std::\nmemory_order_acquire, and if order is std::memory_order_release, then\nfailure_order is std::memory_order_relaxed. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this if the result is true, with memory ordering success_order;\notherwise, it’s an atomic load operation for the memory location comprising\n*this with memory ordering failure_order. \nSTD::ATOMIC_COMPARE_EXCHANGE_STRONG NONMEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the values\nare equal. If the values aren’t equal, updates the expected value with the value read.\nDeclaration\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong(\n\n\n448\nAPPENDIX D\nC++ Thread Library reference\n    volatile atomic<BaseType>* p,BaseType * old_value,BaseType new_value)\n    noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong(\n    atomic<BaseType>* p,BaseType * old_value,BaseType new_value) noexcept;\nEffects\nreturn p->compare_exchange_strong(*old_value,new_value);\nSTD::ATOMIC_COMPARE_EXCHANGE_STRONG_EXPLICIT NONMEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the values\nare equal. If the values aren’t equal, updates the expected value with the value read.\nDeclaration\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong_explicit(\n    volatile atomic<BaseType>* p,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong_explicit(\n    atomic<BaseType>* p,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\nEffects\nreturn p->compare_exchange_strong(\n    *old_value,new_value,success_order,failure_order) noexcept;\nSTD::ATOMIC::COMPARE_EXCHANGE_WEAK MEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the val-\nues are equal and the update can be done atomically. If the values aren’t equal or the\nupdate can’t be done atomically, updates the expected value with the value read.\nDeclaration\nbool compare_exchange_weak(\n    BaseType& expected,BaseType new_value,\n    memory_order order = std::memory_order_seq_cst) volatile noexcept;\nbool compare_exchange_weak(\n    BaseType& expected,BaseType new_value,\n    memory_order order = std::memory_order_seq_cst) noexcept;\nbool compare_exchange_weak(\n    BaseType& expected,BaseType new_value,\n    memory_order success_order,memory_order failure_order)\n    volatile noexcept;\nbool compare_exchange_weak(\n    BaseType& expected,BaseType new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nPreconditions\nfailure_order shall not be std::memory_order_release or std::memory_order\n_acq_rel. \n\n\n449\n<atomic> header\nEffects\nAtomically compares expected to the value stored in *this using bitwise compari-\nson and stores new_value in *this if equal. If the values aren’t equal or the update\ncan’t be done atomically, updates expected to the value read. \nReturns\ntrue if the existing value of *this was equal to expected and new_value was suc-\ncessfully stored in *this, false otherwise. \nThrows \nNothing. \nNOTE\nThe three-parameter overload is equivalent to the four-parameter\noverload with success_order==order and failure_order==order, except\nthat if order is std::memory_order_acq_rel, then failure_order is std::\nmemory_order_acquire, and if order is std::memory_order_release, then\nfailure_order is std::memory_order_relaxed. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this if the result is true, with memory ordering success_order;\notherwise, it’s an atomic load operation for the memory location comprising\n*this with memory ordering failure_order. \nSTD::ATOMIC_COMPARE_EXCHANGE_WEAK NONMEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the val-\nues are equal and the update can be done atomically. If the values aren’t equal or the\nupdate can’t be done atomically, updates the expected value with the value read.\nDeclaration\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak(\n    volatile atomic<BaseType>* p,BaseType * old_value,BaseType new_value)\n    noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak(\n    atomic<BaseType>* p,BaseType * old_value,BaseType new_value) noexcept;\nEffects\nreturn p->compare_exchange_weak(*old_value,new_value);\nSTD::ATOMIC_COMPARE_EXCHANGE_WEAK_EXPLICIT NONMEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the val-\nues are equal and the update can be done atomically. If the values aren’t equal or the\nupdate can’t be done atomically, updates the expected value with the value read.\nDeclaration\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak_explicit(\n    volatile atomic<BaseType>* p,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\ntemplate<typename BaseType>\n\n\n450\nAPPENDIX D\nC++ Thread Library reference\nbool atomic_compare_exchange_weak_explicit(\n    atomic<BaseType>* p,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\nEffects\nreturn p->compare_exchange_weak(\n    *old_value,new_value,success_order,failure_order);\nD.3.9\nSpecializations of the std::atomic template \nSpecializations of the std::atomic class template are provided for the integral types\nand pointer types. For the integral types, these specializations provide atomic addi-\ntion, subtraction, and bitwise operations in addition to the operations provided by the\nprimary template. For pointer types, the specializations provide atomic pointer arith-\nmetic in addition to the operations provided by the primary template. \n Specializations are provided for the following integral types: \nstd::atomic<bool>\nstd::atomic<char>\nstd::atomic<signed char>\nstd::atomic<unsigned char>\nstd::atomic<short>\nstd::atomic<unsigned short>\nstd::atomic<int>\nstd::atomic<unsigned>\nstd::atomic<long>\nstd::atomic<unsigned long>\nstd::atomic<long long>\nstd::atomic<unsigned long long>\nstd::atomic<wchar_t>\nstd::atomic<char16_t>\nstd::atomic<char32_t>\nand std::atomic<T*> for all types T. \nD.3.10 std::atomic<integral-type> specializations \nThe std::atomic<integral-type> specializations of the std::atomic class template\nprovide an atomic integral data type for each fundamental integer type, with a com-\nprehensive set of operations. \n The following description applies to these specializations of the std::atomic<>\nclass template:\nstd::atomic<char>\nstd::atomic<signed char>\nstd::atomic<unsigned char>\nstd::atomic<short>\nstd::atomic<unsigned short>\nstd::atomic<int>\nstd::atomic<unsigned>\nstd::atomic<long>\n\n\n451\n<atomic> header\nstd::atomic<unsigned long>\nstd::atomic<long long>\nstd::atomic<unsigned long long>\nstd::atomic<wchar_t>\nstd::atomic<char16_t>\nstd::atomic<char32_t>\nInstances of these specializations are not CopyConstructible or CopyAssignable,\nbecause these operations can’t be performed as a single atomic operation. \nClass definition\ntemplate<>\nstruct atomic<integral-type>\n{\n    atomic() noexcept = default;\n    constexpr atomic(integral-type) noexcept;\n    bool operator=(integral-type) volatile noexcept;\n    atomic(const atomic&) = delete;\n    atomic& operator=(const atomic&) = delete;\n    atomic& operator=(const atomic&) volatile = delete;\n    bool is_lock_free() const volatile noexcept;\n    bool is_lock_free() const noexcept;\n    void store(integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    void store(integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type load(memory_order = memory_order_seq_cst)\n        const volatile noexcept;\n    integral-type load(memory_order = memory_order_seq_cst) const noexcept;\n    integral-type exchange(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type exchange(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(\n        integral-type & old_value,integral-type new_value,\n         memory_order order = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_strong(\n        integral-type & old_value,integral-type new_value,\n         memory_order order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(\n        integral-type & old_value,integral-type new_value,\n        memory_order success_order,memory_order failure_order)\n        volatile noexcept;\n    bool compare_exchange_strong(\n        integral-type & old_value,integral-type new_value,\n        memory_order success_order,memory_order failure_order) noexcept;\n    bool compare_exchange_weak(\n        integral-type & old_value,integral-type new_value,\n         memory_order order = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_weak(\n        integral-type & old_value,integral-type new_value,\n         memory_order order = memory_order_seq_cst) noexcept;\n\n\n452\nAPPENDIX D\nC++ Thread Library reference\n    bool compare_exchange_weak(\n        integral-type & old_value,integral-type new_value,\n        memory_order success_order,memory_order failure_order)\n        volatile noexcept;\n    bool compare_exchange_weak(\n        integral-type & old_value,integral-type new_value,\n        memory_order success_order,memory_order failure_order) noexcept;\n    operator integral-type() const volatile noexcept;\n    operator integral-type() const noexcept;\n    integral-type fetch_add(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type fetch_add(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type fetch_sub(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type fetch_sub(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type fetch_and(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type fetch_and(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type fetch_or(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type fetch_or(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type fetch_xor(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type fetch_xor(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type operator++() volatile noexcept;\n    integral-type operator++() noexcept;\n    integral-type operator++(int) volatile noexcept;\n    integral-type operator++(int) noexcept;\n    integral-type operator--() volatile noexcept;\n    integral-type operator--() noexcept;\n    integral-type operator--(int) volatile noexcept;\n    integral-type operator--(int) noexcept;\n    integral-type operator+=(integral-type) volatile noexcept;\n    integral-type operator+=(integral-type) noexcept;\n    integral-type operator-=(integral-type) volatile noexcept;\n    integral-type operator-=(integral-type) noexcept;\n    integral-type operator&=(integral-type) volatile noexcept;\n    integral-type operator&=(integral-type) noexcept;\n    integral-type operator|=(integral-type) volatile noexcept;\n    integral-type operator|=(integral-type) noexcept;\n    integral-type operator^=(integral-type) volatile noexcept;\n    integral-type operator^=(integral-type) noexcept;\n};\n\n\n453\n<atomic> header\nbool atomic_is_lock_free(volatile const atomic<integral-type>*) noexcept;\nbool atomic_is_lock_free(const atomic<integral-type>*) noexcept;\nvoid atomic_init(volatile atomic<integral-type>*,integral-type) noexcept;\nvoid atomic_init(atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_exchange(\n    volatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_exchange(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_exchange_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_exchange_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nvoid atomic_store(volatile atomic<integral-type>*,integral-type) noexcept;\nvoid atomic_store(atomic<integral-type>*,integral-type) noexcept;\nvoid atomic_store_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nvoid atomic_store_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_load(volatile const atomic<integral-type>*) noexcept;\nintegral-type atomic_load(const atomic<integral-type>*) noexcept;\nintegral-type atomic_load_explicit(\n    volatile const atomic<integral-type>*,memory_order) noexcept;\nintegral-type atomic_load_explicit(\n    const atomic<integral-type>*,memory_order) noexcept;\nbool atomic_compare_exchange_strong(\n    volatile atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value) noexcept;\nbool atomic_compare_exchange_strong(\n    atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value) noexcept;\nbool atomic_compare_exchange_strong_explicit(\n    volatile atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_strong_explicit(\n    atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_weak(\n    volatile atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value) noexcept;\nbool atomic_compare_exchange_weak(\n    atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value) noexcept;\nbool atomic_compare_exchange_weak_explicit(\n    volatile atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_weak_explicit(\n    atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nintegral-type atomic_fetch_add(\n    volatile atomic<integral-type>*,integral-type) noexcept;\n\n\n454\nAPPENDIX D\nC++ Thread Library reference\nintegral-type atomic_fetch_add(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_add_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_add_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_sub(\n    volatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_sub(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_sub_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_sub_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_and(\n    volatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_and(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_and_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_and_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_or(\n    volatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_or(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_or_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_or_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_xor(\n    volatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_xor(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_xor_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_xor_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nThose operations that are also provided by the primary template (see D.3.8) have\nthe same semantics.\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_ADD MEMBER FUNCTION \nAtomically loads a value and replaces it with the sum of that value and the supplied\nvalue i. \nDeclaration\nintegral-type fetch_add(\n    integral-type i,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nintegral-type fetch_add(\n    integral-type i,memory_order order = memory_order_seq_cst) noexcept;\n\n\n455\n<atomic> header\nEffects\nAtomically retrieves the existing value of *this and stores old-value + i in *this. \nReturns \nThe value of *this immediately prior to the store. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_ADD NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith that value plus the supplied value i.\nDeclaration\nintegral-type atomic_fetch_add(\n    volatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_add(\n    atomic<integral-type>* p, integral-type i) noexcept;\nEffects\nreturn p->fetch_add(i);\nSTD::ATOMIC_FETCH_ADD_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith that value plus the supplied value i.\nDeclaration\nintegral-type atomic_fetch_add_explicit(\n    volatile atomic<integral-type>* p, integral-type i,\n    memory_order order) noexcept;\nintegral-type atomic_fetch_add_explicit(\n    atomic<integral-type>* p, integral-type i, memory_order order)\n    noexcept;\nEffects\nreturn p->fetch_add(i,order);\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_SUB MEMBER FUNCTION \nAtomically loads a value and replaces it with the sum of that value and the supplied\nvalue i. \nDeclaration\nintegral-type fetch_sub(\n    integral-type i,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nintegral-type fetch_sub(\n    integral-type i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value - i in *this. \n\n\n456\nAPPENDIX D\nC++ Thread Library reference\nReturns\nThe value of *this immediately prior to the store. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_SUB NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith that value minus the supplied value i.\nDeclaration\nintegral-type atomic_fetch_sub(\n    volatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_sub(\n    atomic<integral-type>* p, integral-type i) noexcept;\nEffects\nreturn p->fetch_sub(i);\nSTD::ATOMIC_FETCH_SUB_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith that value minus the supplied value i.\nDeclaration\nintegral-type atomic_fetch_sub_explicit(\n    volatile atomic<integral-type>* p, integral-type i,\n    memory_order order) noexcept;\nintegral-type atomic_fetch_sub_explicit(\n    atomic<integral-type>* p, integral-type i, memory_order order)\n    noexcept;\nEffects\nreturn p->fetch_sub(i,order);\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_AND MEMBER FUNCTION \nAtomically loads a value and replaces it with the bitwise-and of that value and the sup-\nplied value i. \nDeclaration\nintegral-type fetch_and(\n    integral-type i,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nintegral-type fetch_and(\n    integral-type i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value & i in *this. \nReturns\nThe value of *this immediately prior to the store. \n\n\n457\n<atomic> header\nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_AND NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-and of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_and(\n    volatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_and(\n    atomic<integral-type>* p, integral-type i) noexcept;\nEffects\nreturn p->fetch_and(i);\nSTD::ATOMIC_FETCH_AND_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-and of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_and_explicit(\n    volatile atomic<integral-type>* p, integral-type i,\n    memory_order order) noexcept;\nintegral-type atomic_fetch_and_explicit(\n    atomic<integral-type>* p, integral-type i, memory_order order)\n    noexcept;\nEffects\nreturn p->fetch_and(i,order);\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_OR MEMBER FUNCTION \nAtomically loads a value and replaces it with the bitwise-or of that value and the sup-\nplied value i. \nDeclaration\nintegral-type fetch_or(\n    integral-type i,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nintegral-type fetch_or(\n    integral-type i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value | i in *this. \nReturns\nThe value of *this immediately prior to the store. \n\n\n458\nAPPENDIX D\nC++ Thread Library reference\nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_OR NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-or of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_or(\n    volatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_or(\n    atomic<integral-type>* p, integral-type i) noexcept;\nEffects\nreturn p->fetch_or(i);\nSTD::ATOMIC_FETCH_OR_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-or of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_or_explicit(\n    volatile atomic<integral-type>* p, integral-type i,\n    memory_order order) noexcept;\nintegral-type atomic_fetch_or_explicit(\n    atomic<integral-type>* p, integral-type i, memory_order order)\n    noexcept;\nEffects\nreturn p->fetch_or(i,order);\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_XOR MEMBER FUNCTION \nAtomically loads a value and replaces it with the bitwise-xor of that value and the sup-\nplied value i. \nDeclaration\nintegral-type fetch_xor(\n    integral-type i,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nintegral-type fetch_xor(\n    integral-type i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value ^ i in *this. \nReturns\nThe value of *this immediately prior to the store. \nThrows\nNothing. \n\n\n459\n<atomic> header\nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_XOR NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-xor of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_xor(\n    volatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_xor(\n    atomic<integral-type>* p, integral-type i) noexcept;\nEffects\nreturn p->fetch_xor(i);\nSTD::ATOMIC_FETCH_XOR_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-xor of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_xor_explicit(\n    volatile atomic<integral-type>* p, integral-type i,\n    memory_order order) noexcept;\nintegral-type atomic_fetch_xor_explicit(\n    atomic<integral-type>* p, integral-type i, memory_order order)\n    noexcept;\nEffects\nreturn p->fetch_xor(i,order);\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR++ PREINCREMENT OPERATOR \nAtomically increments the value stored in *this and returns the new value. \nDeclaration\nintegral-type operator++() volatile noexcept;\nintegral-type operator++() noexcept;\nEffects\nreturn this->fetch_add(1) + 1; \nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR++ POSTINCREMENT OPERATOR \nAtomically increments the value stored in *this and returns the old value. \nDeclaration\nintegral-type operator++(int) volatile noexcept;\nintegral-type operator++(int) noexcept;\nEffects\nreturn this->fetch_add(1); \n\n\n460\nAPPENDIX D\nC++ Thread Library reference\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR-- PREDECREMENT OPERATOR \nAtomically decrements the value stored in *this and returns the new value. \nDeclaration\nintegral-type operator--() volatile noexcept;\nintegral-type operator--() noexcept;\nEffects\nreturn this->fetch_sub(1) – 1; \nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR-- POSTDECREMENT OPERATOR \nAtomically decrements the value stored in *this and returns the old value. \nDeclaration\nintegral-type operator--(int) volatile noexcept;\nintegral-type operator--(int) noexcept;\nEffects\nreturn this->fetch_sub(1);\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR \nAtomically adds the supplied value to the value stored in *this and returns the new\nvalue. \nDeclaration\nintegral-type operator+=(integral-type i) volatile noexcept;\nintegral-type operator+=(integral-type i) noexcept;\nEffects\nreturn this->fetch_add(i) + i;\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR \nAtomically subtracts the supplied value from the value stored in *this and returns the\nnew value. \nDeclaration\nintegral-type operator-=(integral-type i) volatile noexcept;\nintegral-type operator-=(integral-type i) noexcept;\nEffects\nreturn this->fetch_sub(i,std::memory_order_seq_cst) – i;\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR&= COMPOUND ASSIGNMENT OPERATOR \nAtomically replaces the value stored in *this with the bitwise-and of the supplied\nvalue and the value stored in *this and returns the new value. \nDeclaration\nintegral-type operator&=(integral-type i) volatile noexcept;\nintegral-type operator&=(integral-type i) noexcept;\nEffects\nreturn this->fetch_and(i) & i;\n\n\n461\n<atomic> header\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR|= COMPOUND ASSIGNMENT OPERATOR \nAtomically replaces the value stored in *this with the bitwise-or of the supplied value\nand the value stored in *this and returns the new value. \nDeclaration\nintegral-type operator|=(integral-type i) volatile noexcept;\nintegral-type operator|=(integral-type i) noexcept;\nEffects\nreturn this->fetch_or(i,std::memory_order_seq_cst) | i;\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR^= COMPOUND ASSIGNMENT OPERATOR \nAtomically replaces the value stored in *this with the bitwise-xor of the supplied\nvalue and the value stored in *this and returns the new value. \nDeclaration\nintegral-type operator^=(integral-type i) volatile noexcept;\nintegral-type operator^=(integral-type i) noexcept;\nEffects\nreturn this->fetch_xor(i,std::memory_order_seq_cst) ^ i; \nSTD::ATOMIC<T*> PARTIAL SPECIALIZATION \nThe std::atomic<T*> partial specialization of the std::atomic class template provides\nan atomic data type for each pointer type, with a comprehensive set of operations. \n Instances of std::atomic<T*> are not CopyConstructible or CopyAssignable,\nbecause these operations can’t be performed as a single atomic operation. \nClass definition\ntemplate<typename T>\nstruct atomic<T*>\n{\n    atomic() noexcept = default;\n    constexpr atomic(T*) noexcept;\n    bool operator=(T*) volatile;\n    bool operator=(T*);\n    atomic(const atomic&) = delete;\n    atomic& operator=(const atomic&) = delete;\n    atomic& operator=(const atomic&) volatile = delete;\n    bool is_lock_free() const volatile noexcept;\n    bool is_lock_free() const noexcept;\n    void store(T*,memory_order = memory_order_seq_cst) volatile noexcept;\n    void store(T*,memory_order = memory_order_seq_cst) noexcept;\n    T* load(memory_order = memory_order_seq_cst) const volatile noexcept;\n    T* load(memory_order = memory_order_seq_cst) const noexcept;\n    T* exchange(T*,memory_order = memory_order_seq_cst) volatile noexcept;\n    T* exchange(T*,memory_order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(\n        T* & old_value, T* new_value,\n        memory_order order = memory_order_seq_cst) volatile noexcept;\n\n\n462\nAPPENDIX D\nC++ Thread Library reference\n    bool compare_exchange_strong(\n        T* & old_value, T* new_value,\n        memory_order order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(\n        T* & old_value, T* new_value,\n        memory_order success_order,memory_order failure_order)\n        volatile noexcept;\n    bool compare_exchange_strong(\n        T* & old_value, T* new_value,\n        memory_order success_order,memory_order failure_order) noexcept;\n    bool compare_exchange_weak(\n        T* & old_value, T* new_value,\n        memory_order order = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_weak(\n        T* & old_value, T* new_value,\n        memory_order order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_weak(\n        T* & old_value, T* new_value,\n        memory_order success_order,memory_order failure_order)\n        volatile noexcept;\n    bool compare_exchange_weak(\n        T* & old_value, T* new_value,\n        memory_order success_order,memory_order failure_order) noexcept;\n    operator T*() const volatile noexcept;\n    operator T*() const noexcept;\n    T* fetch_add(\n        ptrdiff_t,memory_order = memory_order_seq_cst) volatile noexcept;\n    T* fetch_add(\n        ptrdiff_t,memory_order = memory_order_seq_cst) noexcept;\n    T* fetch_sub(\n        ptrdiff_t,memory_order = memory_order_seq_cst) volatile noexcept;\n    T* fetch_sub(\n        ptrdiff_t,memory_order = memory_order_seq_cst) noexcept;\n    T* operator++() volatile noexcept;\n    T* operator++() noexcept;\n    T* operator++(int) volatile noexcept;\n    T* operator++(int) noexcept;\n    T* operator--() volatile noexcept;\n    T* operator--() noexcept;\n    T* operator--(int) volatile noexcept;\n    T* operator--(int) noexcept;\n    T* operator+=(ptrdiff_t) volatile noexcept;\n    T* operator+=(ptrdiff_t) noexcept;\n    T* operator-=(ptrdiff_t) volatile noexcept;\n    T* operator-=(ptrdiff_t) noexcept;\n};\nbool atomic_is_lock_free(volatile const atomic<T*>*) noexcept;\nbool atomic_is_lock_free(const atomic<T*>*) noexcept;\nvoid atomic_init(volatile atomic<T*>*, T*) noexcept;\nvoid atomic_init(atomic<T*>*, T*) noexcept;\nT* atomic_exchange(volatile atomic<T*>*, T*) noexcept;\nT* atomic_exchange(atomic<T*>*, T*) noexcept;\n\n\n463\n<atomic> header\nT* atomic_exchange_explicit(volatile atomic<T*>*, T*, memory_order)\n    noexcept;\nT* atomic_exchange_explicit(atomic<T*>*, T*, memory_order) noexcept;\nvoid atomic_store(volatile atomic<T*>*, T*) noexcept;\nvoid atomic_store(atomic<T*>*, T*) noexcept;\nvoid atomic_store_explicit(volatile atomic<T*>*, T*, memory_order)\n    noexcept;\nvoid atomic_store_explicit(atomic<T*>*, T*, memory_order) noexcept;\nT* atomic_load(volatile const atomic<T*>*) noexcept;\nT* atomic_load(const atomic<T*>*) noexcept;\nT* atomic_load_explicit(volatile const atomic<T*>*, memory_order) noexcept;\nT* atomic_load_explicit(const atomic<T*>*, memory_order) noexcept;\nbool atomic_compare_exchange_strong(\n    volatile atomic<T*>*,T* * old_value,T* new_value) noexcept;\nbool atomic_compare_exchange_strong(\n    volatile atomic<T*>*,T* * old_value,T* new_value) noexcept;\nbool atomic_compare_exchange_strong_explicit(\n    atomic<T*>*,T* * old_value,T* new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_strong_explicit(\n    atomic<T*>*,T* * old_value,T* new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_weak(\n    volatile atomic<T*>*,T* * old_value,T* new_value) noexcept;\nbool atomic_compare_exchange_weak(\n    atomic<T*>*,T* * old_value,T* new_value) noexcept;\nbool atomic_compare_exchange_weak_explicit(\n    volatile atomic<T*>*,T* * old_value, T* new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_weak_explicit(\n    atomic<T*>*,T* * old_value, T* new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nT* atomic_fetch_add(volatile atomic<T*>*, ptrdiff_t) noexcept;\nT* atomic_fetch_add(atomic<T*>*, ptrdiff_t) noexcept;\nT* atomic_fetch_add_explicit(\n    volatile atomic<T*>*, ptrdiff_t, memory_order) noexcept;\nT* atomic_fetch_add_explicit(\n    atomic<T*>*, ptrdiff_t, memory_order) noexcept;\nT* atomic_fetch_sub(volatile atomic<T*>*, ptrdiff_t) noexcept;\nT* atomic_fetch_sub(atomic<T*>*, ptrdiff_t) noexcept;\nT* atomic_fetch_sub_explicit(\n    volatile atomic<T*>*, ptrdiff_t, memory_order) noexcept;\nT* atomic_fetch_sub_explicit(\n    atomic<T*>*, ptrdiff_t, memory_order) noexcept;\nThose operations that are also provided by the primary template (see 11.3.8) have\nthe same semantics.\nSTD::ATOMIC<T*>::FETCH_ADD MEMBER FUNCTION \nAtomically loads a value and replaces it with the sum of that value and the supplied\nvalue i using standard pointer arithmetic rules, and returns the old value. \nDeclaration\nT* fetch_add(\n\n\n464\nAPPENDIX D\nC++ Thread Library reference\n    ptrdiff_t i,memory_order order = memory_order_seq_cst)\n    volatile noexcept;\nT* fetch_add(\n    ptrdiff_t i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value + i in *this. \nReturns\nThe value of *this immediately prior to the store. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_ADD NONMEMBER FUNCTION \nAtomically reads the value from an atomic<T*> instance and replaces it with that\nvalue plus the supplied value i using standard pointer arithmetic rules.\nDeclaration\nT* atomic_fetch_add(volatile atomic<T*>* p, ptrdiff_t i) noexcept;\nT* atomic_fetch_add(atomic<T*>* p, ptrdiff_t i) noexcept;\nEffects\nreturn p->fetch_add(i);\nSTD::ATOMIC_FETCH_ADD_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<T*> instance and replaces it with that\nvalue plus the supplied value i using standard pointer arithmetic rules.\nDeclaration\nT* atomic_fetch_add_explicit(\n    volatile atomic<T*>* p, ptrdiff_t i,memory_order order) noexcept;\nT* atomic_fetch_add_explicit(\n    atomic<T*>* p, ptrdiff_t i, memory_order order) noexcept;\nEffects\nreturn p->fetch_add(i,order);\nSTD::ATOMIC<T*>::FETCH_SUB MEMBER FUNCTION \nAtomically loads a value and replaces it with that value minus the supplied value i\nusing standard pointer arithmetic rules, and returns the old value. \nDeclaration\nT* fetch_sub(\n    ptrdiff_t i,memory_order order = memory_order_seq_cst)\n    volatile noexcept;\nT* fetch_sub(\n    ptrdiff_t i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value - i in *this. \n\n\n465\n<atomic> header\nReturns\nThe value of *this immediately prior to the store. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_SUB NONMEMBER FUNCTION \nAtomically reads the value from an atomic<T*> instance and replaces it with that\nvalue minus the supplied value i using standard pointer arithmetic rules.\nDeclaration\nT* atomic_fetch_sub(volatile atomic<T*>* p, ptrdiff_t i) noexcept;\nT* atomic_fetch_sub(atomic<T*>* p, ptrdiff_t i) noexcept;\nEffects\nreturn p->fetch_sub(i);\nSTD::ATOMIC_FETCH_SUB_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<T*> instance and replaces it with that\nvalue minus the supplied value i using standard pointer arithmetic rules.\nDeclaration\nT* atomic_fetch_sub_explicit(\n    volatile atomic<T*>* p, ptrdiff_t i,memory_order order) noexcept;\nT* atomic_fetch_sub_explicit(\n    atomic<T*>* p, ptrdiff_t i, memory_order order) noexcept;\nEffects\nreturn p->fetch_sub(i,order);\nSTD::ATOMIC<T*>::OPERATOR++ PREINCREMENT OPERATOR \nAtomically increments the value stored in *this using standard pointer arithmetic\nrules and returns the new value. \nDeclaration\nT* operator++() volatile noexcept;\nT* operator++() noexcept;\nEffects\nreturn this->fetch_add(1) + 1;\nSTD::ATOMIC<T*>::OPERATOR++ POSTINCREMENT OPERATOR \nAtomically increments the value stored in *this and returns the old value. \nDeclaration\nT* operator++(int) volatile noexcept;\nT* operator++(int) noexcept;\nEffects\nreturn this->fetch_add(1);\n\n\n466\nAPPENDIX D\nC++ Thread Library reference\nSTD::ATOMIC<T*>::OPERATOR-- PREDECREMENT OPERATOR \nAtomically decrements the value stored in *this using standard pointer arithmetic\nrules and returns the new value. \nDeclaration\nT* operator--() volatile noexcept;\nT* operator--() noexcept;\nEffects\nreturn this->fetch_sub(1) - 1;\nSTD::ATOMIC<T*>::OPERATOR-- POSTDECREMENT OPERATOR \nAtomically decrements the value stored in *this using standard pointer arithmetic\nrules and returns the old value. \nDeclaration\nT* operator--(int) volatile noexcept;\nT* operator--(int) noexcept;\nEffects\nreturn this->fetch_sub(1);\nSTD::ATOMIC<T*>::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR \nAtomically adds the supplied value to the value stored in *this using standard pointer\narithmetic rules and returns the new value. \nDeclaration\nT* operator+=(ptrdiff_t i) volatile noexcept;\nT* operator+=(ptrdiff_t i) noexcept;\nEffects\nreturn this->fetch_add(i) + i;\nSTD::ATOMIC<T*>::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR \nAtomically subtracts the supplied value from the value stored in *this using standard\npointer arithmetic rules and returns the new value. \nDeclaration\nT* operator-=(ptrdiff_t i) volatile noexcept;\nT* operator-=(ptrdiff_t i) noexcept;\nEffects\nreturn this->fetch_sub(i) - i;\nD.4\n<future> header \nThe <future> header provides facilities for handling asynchronous results from oper-\nations that may be performed on another thread. \nHeader contents\nnamespace std\n{\n    enum class future_status {\n        ready, timeout, deferred };\n",
      "page_number": 455
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 490-512)",
      "start_page": 490,
      "end_page": 512,
      "detection_method": "topic_boundary",
      "content": "467\n<future> header\n    enum class future_errc\n    {\n        broken_promise,\n        future_already_retrieved,\n        promise_already_satisfied,\n        no_state\n    };\n    class future_error;\n    const error_category& future_category();\n    error_code make_error_code(future_errc e);\n    error_condition make_error_condition(future_errc e);\n    template<typename ResultType>\n    class future;\n    template<typename ResultType>\n    class shared_future;\n    template<typename ResultType>\n    class promise;\n    template<typename FunctionSignature>\n    class packaged_task; // no definition provided\n    template<typename ResultType,typename ... Args>\n    class packaged_task<ResultType (Args...)>;\n    enum class launch {\n        async, deferred\n    };\n    template<typename FunctionType,typename ... Args>\n    future<result_of<FunctionType(Args...)>::type>\n    async(FunctionType&& func,Args&& ... args);\n    template<typename FunctionType,typename ... Args>\n    future<result_of<FunctionType(Args...)>::type>\n    async(std::launch policy,FunctionType&& func,Args&& ... args);\n}\nD.4.1\nstd::future class template \nThe std::future class template provides a means of waiting for an asynchronous\nresult from another thread, in conjunction with the std::promise and std:: pack-\naged_task class templates and the std::async function template, which can be used\nto provide that asynchronous result. Only one std::future instance references any\ngiven asynchronous result at any time. \n Instances of std::future are MoveConstructible and MoveAssignable but not\nCopyConstructible or CopyAssignable. \nClass definition\ntemplate<typename ResultType>\nclass future\n{\n\n\n468\nAPPENDIX D\nC++ Thread Library reference\npublic:\n    future() noexcept;\n    future(future&&) noexcept;\n    future& operator=(future&&) noexcept;\n    ~future();\n    future(future const&) = delete;\n    future& operator=(future const&) = delete;\n    shared_future<ResultType> share();\n    bool valid() const noexcept;\n    see description get();\n    void wait();\n    template<typename Rep,typename Period>\n    future_status wait_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock,typename Duration>\n    future_status wait_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n};\nSTD::FUTURE DEFAULT CONSTRUCTOR \nConstructs an std::future object without an associated asynchronous result. \nDeclaration\nfuture() noexcept;\nEffects\nConstructs a new std::future instance. \nPostconditions\nvalid() returns false. \nThrows\nNothing. \nSTD::FUTURE MOVE CONSTRUCTOR \nConstructs one std::future object from another, transferring ownership of the asyn-\nchronous result associated with the other std::future object to the newly con-\nstructed instance.\nDeclaration \nfuture(future&& other) noexcept;\nEffects\nMove-constructs a new std::future instance from other. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with the newly constructed std::future object. other has\nno associated asynchronous result. this->valid() returns the same value that\nother.valid() returned before the invocation of this constructor. other\n.valid() returns false. \n\n\n469\n<future> header\nThrows\nNothing. \nSTD::FUTURE MOVE ASSIGNMENT OPERATOR \nTransfers ownership of the asynchronous result associated with the one std::future\nobject to another. \nDeclaration\nfuture(future&& other) noexcept;\nEffects\nTransfers ownership of an asynchronous state between std::future instances. \nPostconditions \nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with *this. other has no associated asynchronous result. The\nownership of the asynchronous state (if any) associated with *this prior to the call\nis released, and the state destroyed if this is the last reference. this->valid()\nreturns the same value that other.valid() returned before the invocation of this\nconstructor. other.valid() returns false. \nThrows\nNothing. \nSTD::FUTURE DESTRUCTOR \nDestroys an std::future object. \nDeclaration\n~future();\nEffects\nDestroys *this. If this is the last reference to the asynchronous result associated\nwith *this (if any), then destroy that asynchronous result. \nThrows\nNothing.\nSTD::FUTURE::SHARE MEMBER FUNCTION \nConstructs a new std::shared_future instance and transfers ownership of the asyn-\nchronous result associated with *this to this newly constructed std::shared_future\ninstance. \nDeclaration\nshared_future<ResultType> share();\nEffects\nAs-if shared_future<ResultType>(std::move(*this)). \nPostconditions\nThe asynchronous result associated with *this prior to the invocation of share()\n(if any) is associated with the newly constructed std::shared_future instance.\nthis->valid() returns false. \n\n\n470\nAPPENDIX D\nC++ Thread Library reference\nThrows\nNothing. \nSTD::FUTURE::VALID MEMBER FUNCTION \nChecks if an std::future instance is associated with an asynchronous result. \nDeclaration\nbool valid() const noexcept;\nReturns\ntrue if *this has an associated asynchronous result, false otherwise. \nThrows \nNothing. \nSTD::FUTURE::WAIT MEMBER FUNCTION \nIf the state associated with *this contains a deferred function, invokes the deferred\nfunction. Otherwise, waits until the asynchronous result associated with an instance of\nstd::future is ready. \nDeclaration\nvoid wait();\nPreconditions \nthis->valid() would return true. \nEffects\nIf the associated state contains a deferred function, invokes the deferred function\nand stores the returned value or thrown exception as the asynchronous result. Oth-\nerwise, blocks until the asynchronous result associated with *this is ready. \nThrows\nNothing. \nSTD::FUTURE::WAIT_FOR MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::future is\nready or until a specified time period has elapsed. \nDeclaration\ntemplate<typename Rep,typename Period>\nfuture_status wait_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nthis->valid() would return true. \nEffects\nIf the asynchronous result associated with *this contains a deferred function arising\nfrom a call to std::async that hasn’t yet started execution, returns immediately with-\nout blocking. Otherwise blocks until the asynchronous result associated with *this is\nready or the time period specified by relative_time has elapsed. \nReturns\nstd::future_status::deferred if the asynchronous result associated with *this\ncontains a deferred function arising from a call to std::async that hasn’t yet\n\n\n471\n<future> header\nstarted execution, std::future_status::ready if the asynchronous result associ-\nated with *this is ready, std::future_status::timeout if the time period speci-\nfied by relative_time has elapsed. \nNOTE\nThe thread may be blocked for longer than the specified duration.\nWhere possible, the elapsed time is determined by a steady clock. \nThrows\nNothing. \nSTD::FUTURE::WAIT_UNTIL MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::future is\nready or until a specified time period has elapsed.\nDeclaration\ntemplate<typename Clock,typename Duration>\nfuture_status wait_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions \nthis->valid() would return true. \nEffects\nIf the asynchronous result associated with *this contains a deferred function aris-\ning from a call to std::async that hasn’t yet started execution, returns immediately\nwithout blocking. Otherwise blocks until the asynchronous result associated with\n*this is ready or Clock::now() returns a time equal to or later than absolute_\ntime. \nReturns\nstd::future_status::deferred if the asynchronous result associated with *this\ncontains a deferred function arising from a call to std::async that hasn’t yet\nstarted execution, std::future_status::ready if the asynchronous result associ-\nated with *this is ready, std::future_status::timeout if Clock::now() returns a\ntime equal to or later than absolute_time. \nNOTE\nThere’s no guarantee as to how long the calling thread will be\nblocked, only that if the function returns std::future_status::timeout,\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked. \nThrows\nNothing. \nSTD::FUTURE::GET MEMBER FUNCTION \nIf the associated state contains a deferred function from a call to std::async, invokes\nthat function and returns the result; otherwise, waits until the asynchronous result\nassociated with an instance of std::future is ready, and then returns the stored value\nor throws the stored exception. \n\n\n472\nAPPENDIX D\nC++ Thread Library reference\nDeclaration\nvoid future<void>::get();\nR& future<R&>::get();\nR future<R>::get();\nPreconditions\nthis->valid() would return true. \nEffects\nIf the state associated with *this contains a deferred function, invokes the deferred\nfunction and returns the result or propagates any thrown exception. \nOtherwise, blocks until the asynchronous result associated with *this is ready.\nIf the result is a stored exception, throws that exception. Otherwise, returns the\nstored value. \nReturns\nIf the associated state contains a deferred function, the result of the function invo-\ncation is returned. Otherwise, if ResultType is void, the call returns normally. If\nResultType is R& for some type R, the stored reference is returned. Otherwise, the\nstored value is returned. \nThrows\nThe exception thrown by the deferred exception or stored in the asynchronous\nresult, if any. \nPostcondition\nthis->valid()==false \nD.4.2\nstd::shared_future class template \nThe std::shared_future class template provides a means of waiting for an asynchro-\nnous result from another thread, in conjunction with the std::promise and std::\npackaged_task class templates and the std::async function template, which can be\nused to provide that asynchronous result. Multiple std::shared_future instances can\nreference the same asynchronous result. \n Instances of std::shared_future are CopyConstructible and CopyAssignable.\nYou can also move-construct a std::shared_future from a std::future with the\nsame ResultType. \n Accesses to a given instance of std::shared_future aren’t synchronized. It’s\ntherefore not safe for multiple threads to access the same std::shared_future\ninstance without external synchronization. But accesses to the associated state are syn-\nchronized, so it is safe for multiple threads to each access separate instances of std::\nshared_future that share the same associated state without external synchronization. \nClass definition\ntemplate<typename ResultType>\nclass shared_future\n{\npublic:\n    shared_future() noexcept;\n    shared_future(future<ResultType>&&) noexcept;\n\n\n473\n<future> header\n    shared_future(shared_future&&) noexcept;\n    shared_future(shared_future const&);\n    shared_future& operator=(shared_future const&);\n    shared_future& operator=(shared_future&&) noexcept;\n    ~shared_future();\n    bool valid() const noexcept;\n    see description get() const;\n    void wait() const;\n    template<typename Rep,typename Period>\n    future_status wait_for(\n        std::chrono::duration<Rep,Period> const& relative_time) const;\n    template<typename Clock,typename Duration>\n    future_status wait_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time) const;\n};\nSTD::SHARED_FUTURE DEFAULT CONSTRUCTOR \nConstructs an std::shared_future object without an associated asynchronous result. \nDeclaration\nshared_future() noexcept;\nEffects\nConstructs a new std::shared_future instance. \nPostconditions\nvalid() returns false for the newly constructed instance. \nThrows\nNothing. \nSTD::SHARED_FUTURE MOVE CONSTRUCTOR \nConstructs one std::shared_future object from another, transferring ownership of\nthe asynchronous result associated with the other std::shared_future object to the\nnewly constructed instance. \nDeclaration\nshared_future(shared_future&& other) noexcept;\nEffects\nConstructs a new std::shared_future instance. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with the newly constructed std::shared_future object. other\nhas no associated asynchronous result. \nThrows\nNothing. \n\n\n474\nAPPENDIX D\nC++ Thread Library reference\nSTD::SHARED_FUTURE MOVE-FROM-STD::FUTURE CONSTRUCTOR \nConstructs an std::shared_future object from astd::future, transferring owner-\nship of the asynchronous result associated with the std::future object to the newly\nconstructed std::shared_future. \nDeclaration\nshared_future(std::future<ResultType>&& other) noexcept;\nEffects\nConstructs a new std::shared_future instance. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with the newly constructed std::shared_future object. other\nhas no associated asynchronous result. \nThrows\nNothing. \nSTD::SHARED_FUTURE COPY CONSTRUCTOR \nConstructs one std::shared_future object from another, so that both the source and\nthe copy refer to the asynchronous result associated with the source std::shared_\nfuture object, if any. \nDeclaration\nshared_future(shared_future const& other);\nEffects\nConstructs a new std::shared_future instance. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with the newly constructed std::shared_future object and\nother. \nThrows\nNothing. \nSTD::SHARED_FUTURE DESTRUCTOR \nDestroys an std::shared_future object. \nDeclaration\n~shared_future();\nEffects\nDestroys *this. If there’s no longer an std::promise or std::packaged_task\ninstance associated with the asynchronous result associated with *this, and this is\nthe last std::shared_future instance associated with that asynchronous result,\ndestroys that asynchronous result. \nThrows\nNothing. \n\n\n475\n<future> header\nSTD::SHARED_FUTURE::VALID MEMBER FUNCTION \nChecks if an std::shared_future instance is associated with an asynchronous result. \nDeclaration\nbool valid() const noexcept;\nReturns\ntrue if *this has an associated asynchronous result, false otherwise. \nThrows\nNothing. \nSTD::SHARED_FUTURE::WAIT MEMBER FUNCTION \nIf the state associated with *this contains a deferred function, invokes the deferred\nfunction. Otherwise, waits until the asynchronous result associated with an instance of\nstd::shared_future is ready. \nDeclaration\nvoid wait() const;\nPreconditions\nthis->valid() would return true. \nEffects\nCalls to get() and wait() from multiple threads on std::shared_future instances\nthat share the same associated state are serialized. If the associated state contains a\ndeferred function, the first call to get() or wait() invokes the deferred function\nand stores the returned value or thrown exception as the asynchronous result. \nBlocks until the asynchronous result associated with *this is ready. \nThrows\nNothing. \nSTD::SHARED_FUTURE::WAIT_FOR MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::shared_\nfuture is ready or until a specified time period has elapsed. \nDeclaration\ntemplate<typename Rep,typename Period>\nfuture_status wait_for(\n    std::chrono::duration<Rep,Period> const& relative_time) const;\nPreconditions \nthis->valid() would return true. \nEffects \nIf the asynchronous result associated with *this contains a deferred function aris-\ning from a call to std::async that has not yet started execution, returns immedi-\nately without blocking. Otherwise, blocks until the asynchronous result associated\nwith *this is ready or the time period specified by relative_time has elapsed. \nReturns\nstd::future_status::deferred if the asynchronous result associated with *this\ncontains a deferred function arising from a call to std::async that hasn’t yet\n\n\n476\nAPPENDIX D\nC++ Thread Library reference\nstarted execution, std::future_status::ready if the asynchronous result associ-\nated with *this is ready, std::future_status::timeout if the time period speci-\nfied by relative_time has elapsed. \nNOTE\nThe thread may be blocked for longer than the specified duration.\nWhere possible, the elapsed time is determined by a steady clock. \nThrows\nNothing. \nSTD::SHARED_FUTURE::WAIT_UNTIL MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::shared_\nfuture is ready or until a specified time period has elapsed. \nDeclaration\ntemplate<typename Clock,typename Duration>\nbool wait_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time) const;\nPreconditions \nthis->valid() would return true. \nEffects\nIf the asynchronous result associated with *this contains a deferred function aris-\ning from a call to std::async that hasn’t yet started execution, returns immediately\nwithout blocking. Otherwise, blocks until the asynchronous result associated with\n*this is ready or Clock::now() returns a time equal to or later than absolute_\ntime. \nReturns\nstd::future_status::deferred if the asynchronous result associated with *this\ncontains a deferred function arising from a call to std::async that hasn’t yet\nstarted execution, std::future_status::ready if the asynchronous result associ-\nated with *this is ready, std::future_status::timeout if Clock::now() returns a\ntime equal to or later than absolute_time. \nNOTE\nThere’s no guarantee as to how long the calling thread will be\nblocked, only that if the function returns std::future_status::timeout,\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked. \nThrows\nNothing. \nSTD::SHARED_FUTURE::GET MEMBER FUNCTION \nIf the associated state contains a deferred function from a call to std::async, invokes\nthat function and return the result. Otherwise, waits until the asynchronous result\nassociated with an instance of std::shared_future is ready, and then returns the\nstored value or throws the stored exception. \n\n\n477\n<future> header\nDeclaration\nvoid shared_future<void>::get() const;\nR& shared_future<R&>::get() const;\nR const& shared_future<R>::get() const;\nPreconditions\nthis->valid() would return true. \nEffects\nCalls to get() and wait() from multiple threads on std::shared_future instances\nthat share the same associated state are serialized. If the associated state contains a\ndeferred function, the first call to get() or wait() invokes the deferred function\nand stores the returned value or thrown exception as the asynchronous result. \nBlocks until the asynchronous result associated with *this is ready. If the asyn-\nchronous result is a stored exception, throws that exception. Otherwise, returns the\nstored value. \nReturns\nIf ResultType is void, returns normally. If ResultType is R& for some type R, returns\nthe stored reference. Otherwise, returns a const reference to the stored value. \nThrows\nThe stored exception, if any. \nD.4.3\nstd::packaged_task class template \nThe std::packaged_task class template packages a function or other callable object\nso that when the function is invoked through the std::packaged_task instance, the\nresult is stored as an asynchronous result for retrieval through an instance of\nstd::future. \n Instances of std::packaged_task are MoveConstructible and MoveAssignable\nbut not CopyConstructible or CopyAssignable. \nClass definition\ntemplate<typename FunctionType>\nclass packaged_task; // undefined\ntemplate<typename ResultType,typename... ArgTypes>\nclass packaged_task<ResultType(ArgTypes...)>\n{\npublic:\n    packaged_task() noexcept;\n    packaged_task(packaged_task&&) noexcept;\n    ~packaged_task();\n    packaged_task& operator=(packaged_task&&) noexcept;\n    packaged_task(packaged_task const&) = delete;\n    packaged_task& operator=(packaged_task const&) = delete;\n    void swap(packaged_task&) noexcept;\n\n\n478\nAPPENDIX D\nC++ Thread Library reference\n    template<typename Callable>\n    explicit packaged_task(Callable&& func);\n    template<typename Callable,typename Allocator>\n    packaged_task(std::allocator_arg_t, const Allocator&,Callable&&);\n    bool valid() const noexcept;\n    std::future<ResultType> get_future();\n    void operator()(ArgTypes...);\n    void make_ready_at_thread_exit(ArgTypes...);\n    void reset();\n};\nSTD::PACKAGED_TASK DEFAULT CONSTRUCTOR \nConstructs an std::packaged_task object. \nDeclaration\npackaged_task() noexcept;\nEffects \nConstructs an std::packaged_task instance with no associated task or asynchro-\nnous result. \nThrows\nNothing. \nSTD::PACKAGED_TASK CONSTRUCTION FROM A CALLABLE OBJECT \nConstructs an std::packaged_task object with an associated task and asynchronous\nresult. \nDeclaration\ntemplate<typename Callable>\npackaged_task(Callable&& func);\nPreconditions\nThe expression func(args...) shall be valid, where each element args-i in args...\nshall be a value of the corresponding type ArgTypes-i in ArgTypes.... The return\nvalue shall be convertible to ResultType. \nEffects\nConstructs an std::packaged_task instance with an associated asynchronous\nresult of type ResultType that isn’t ready and an associated task of type Callable\nthat’s a copy of func. \nThrows\nAn exception of type std::bad_alloc if the constructor is unable to allocate mem-\nory for the asynchronous result. Any exception thrown by the copy or move con-\nstructor of Callable. \nSTD::PACKAGED_TASK CONSTRUCTION FROM A CALLABLE OBJECT WITH AN ALLOCATOR\nConstructs an std::packaged_task object with an associated task and asynchronous\nresult, using the supplied allocator to allocate memory for the associated asynchro-\nnous result and task. \n\n\n479\n<future> header\nDeclaration\ntemplate<typename Allocator,typename Callable>\npackaged_task(\n    std::allocator_arg_t, Allocator const& alloc,Callable&& func);\nPreconditions\nThe expression func(args...) shall be valid, where each element args-i in args...\nshall be a value of the corresponding type ArgTypes-i in ArgTypes.... The return\nvalue shall be convertible to ResultType. \nEffects\nConstructs an std::packaged_task instance with an associated asynchronous\nresult of type ResultType that isn’t ready and an associated task of type Callable\nthat’s a copy of func. The memory for the asynchronous result and task is allocated\nthrough the allocator alloc or a copy thereof.\nThrows\nAny exception thrown by the allocator when trying to allocate memory for the asyn-\nchronous result or task. Any exception thrown by the copy or move constructor of\nCallable. \nSTD::PACKAGED_TASK MOVE CONSTRUCTOR \nConstructs one std::packaged_task object from another, transferring ownership of\nthe asynchronous result and task associated with the other std::packaged_task\nobject to the newly constructed instance. \nDeclaration\npackaged_task(packaged_task&& other) noexcept;\nEffects\nConstructs a new std::packaged_task instance. \nPostconditions\nThe asynchronous result and task associated with other prior to the invocation of\nthe constructor is associated with the newly constructed std::packaged_task\nobject. other has no associated asynchronous result. \nThrows\nNothing. \nSTD::PACKAGED_TASK MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of the asynchronous result associated with one std::packaged_\ntask object to another. \nDeclaration\npackaged_task& operator=(packaged_task&& other) noexcept;\nEffects\nTransfers ownership of the asynchronous result and task associated with other to\n*this, and discards any prior asynchronous result, as-if std::packaged_task(other)\n.swap(*this).\n\n\n480\nAPPENDIX D\nC++ Thread Library reference\nPostconditions\nThe asynchronous result and task associated with other prior to the invocation of\nthe move-assignment operator is associated with *this. other has no associated\nasynchronous result. \nReturns\n*this \nThrows\nNothing. \nSTD::PACKAGED_TASK::SWAP MEMBER FUNCTION \nExchanges ownership of the asynchronous results associated with two std::packaged\n_task objects. \nDeclaration\nvoid swap(packaged_task& other) noexcept;\nEffects \nExchanges ownership of the asynchronous results and tasks associated with other\nand *this. \nPostconditions\nThe asynchronous result and task associated with other prior to the invocation of\nswap (if any) is associated with *this. The asynchronous result and task associated\nwith *this prior to the invocation of swap (if any) is associated with other. \nThrows\nNothing. \nSTD::PACKAGED_TASK DESTRUCTOR \nDestroys an std::packaged_task object. \nDeclaration\n~packaged_task();\nEffects\nDestroys *this. If *this has an associated asynchronous result, and that result\ndoesn’t have a stored task or exception, then that result becomes ready with an\nstd::future_error exception with an error code of std::future_errc::broken\n_promise. \nThrows\nNothing. \nSTD::PACKAGED_TASK::GET_FUTURE MEMBER FUNCTION \nRetrieves an std::future instance for the asynchronous result associated with *this. \nDeclaration\nstd::future<ResultType> get_future();\nPreconditions \n*this has an associated asynchronous result. \n\n\n481\n<future> header\nReturns\nAn std::future instance for the asynchronous result associated with *this. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::future_already_retrieved if a std::future has already been obtained for\nthis asynchronous result through a prior call to get_future(). \nSTD::PACKAGED_TASK::RESET MEMBER FUNCTION \nAssociates an std::packaged_task instance with a new asynchronous result for the\nsame task. \nDeclaration\nvoid reset();\nPreconditions\n*this has an associated asynchronous task. \nEffects\nAs-if *this=packaged_task(std::move(f)), where f is the stored task associated\nwith *this. \nThrows\nAn exception of type std::bad_alloc if memory couldn’t be allocated for the new\nasynchronous result. \nSTD::PACKAGED_TASK::VALID MEMBER FUNCTION \nChecks whether *this has an associated task and asynchronous result. \nDeclaration\nbool valid() const noexcept;\nReturns\ntrue if *this has an associated task and asynchronous result, false otherwise. \nThrows\nNothing. \nSTD::PACKAGED_TASK::OPERATOR() FUNCTION CALL OPERATOR \nInvokes the task associated with an std::packaged_task instance, and stores the return\nvalue or exception in the associated asynchronous result. \nDeclaration\nvoid operator()(ArgTypes... args);\nPreconditions\n*this has an associated task. \nEffects\nInvokes the associated task func as-if INVOKE(func,args...). If the invocation\nreturns normally, stores the return value in the asynchronous result associated with\n*this. If the invocation returns with an exception, stores the exception in the asyn-\nchronous result associated with *this. \n\n\n482\nAPPENDIX D\nC++ Thread Library reference\nPostconditions\nThe asynchronous result associated with *this is ready with a stored value or excep-\ntion. Any threads blocked waiting for the asynchronous result are unblocked. \nThrows\nAn exception of type std::future_error with an error code of std::future_errc::\npromise_already_satisfied if the asynchronous result already has a stored value\nor exception.\nSynchronization\nA successful call to the function call operator synchronizes-with a call to std::\nfuture<ResultType>::get() or std::shared_future<ResultType>::get(), which\nretrieves the value or exception stored. \nSTD::PACKAGED_TASK::MAKE_READY_AT_THREAD_EXIT MEMBER FUNCTION \nInvokes the task associated with an std::packaged_task instance, and stores the\nreturn value or exception in the associated asynchronous result without making the\nassociated asynchronous result ready until thread exit. \nDeclaration\nvoid make_ready_at_thread_exit(ArgTypes... args);\nPreconditions\n*this has an associated task. \nEffects\nInvokes the associated task func as-if INVOKE(func,args...). If the invocation\nreturns normally, stores the return value in the asynchronous result associated with\n*this. If the invocation returns with an exception, stores the exception in the asyn-\nchronous result associated with *this. Schedules the associated asynchronous state\nto be made ready when the current thread exits. \nPostconditions\nThe asynchronous result associated with *this has a stored value or exception but\nisn’t ready until the current thread exits. Threads blocked waiting for the asynchro-\nnous result will be unblocked when the current thread exits. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::promise_already_satisfied if the asynchronous result already has a stored\nvalue or exception. An exception of type std::future_error with an error code of\nstd::future_errc::no_state if *this has no associated asynchronous state. \nSynchronization\nThe completion of the thread that made a successful call to make_ready_at_thread_\nexit() synchronizes-with a call to std::future<ResultType>::get() or std::shared\n_future<ResultType>::get(), which retrieves the value or exception stored. \n\n\n483\n<future> header\nD.4.4\nstd::promise class template \nThe std::promise class template provides a means of setting an asynchronous result,\nwhich may be retrieved from another thread through an instance of std::future. \n The ResultType template parameter is the type of the value that can be stored in\nthe asynchronous result. \n A std::future associated with the asynchronous result of a particular std::promise\ninstance can be obtained by calling the get_future() member function. The asyn-\nchronous result is set either to a value of type ResultType with the set_value() mem-\nber function or to an exception with the set_exception() member function. \n Instances of std::promise are MoveConstructible and MoveAssignable but not\nCopyConstructible or CopyAssignable. \nClass definition\ntemplate<typename ResultType>\nclass promise\n{\npublic:\n    promise();\n    promise(promise&&) noexcept;\n    ~promise();\n    promise& operator=(promise&&) noexcept;\n    template<typename Allocator>\n    promise(std::allocator_arg_t, Allocator const&);\n    promise(promise const&) = delete;\n    promise& operator=(promise const&) = delete;\n    void swap(promise& ) noexcept;\n    std::future<ResultType> get_future();\n    void set_value(see description);\n    void set_exception(std::exception_ptr p);\n};\nSTD::PROMISE DEFAULT CONSTRUCTOR \nConstructs an std::promise object. \nDeclaration\npromise();\nEffects \nConstructs an std::promise instance with an associated asynchronous result of\ntype ResultType that’s not ready. \nThrows\nAn exception of type std::bad_alloc if the constructor is unable to allocate mem-\nory for the asynchronous result. \nSTD::PROMISE ALLOCATOR CONSTRUCTOR \nConstructs an std::promise object, using the supplied allocator to allocate memory\nfor the associated asynchronous result. \n\n\n484\nAPPENDIX D\nC++ Thread Library reference\nDeclaration\ntemplate<typename Allocator>\npromise(std::allocator_arg_t, Allocator const& alloc);\nEffects\nConstructs an std::promise instance with an associated asynchronous result of\ntype ResultType that isn’t ready. The memory for the asynchronous result is allo-\ncated through the allocator alloc. \nThrows\nAny exception thrown by the allocator when attempting to allocate memory for the\nasynchronous result. \nSTD::PROMISE MOVE CONSTRUCTOR \nConstructs one std::promise object from another, transferring ownership of the\nasynchronous result associated with the other std::promise object to the newly con-\nstructed instance. \nDeclaration\npromise(promise&& other) noexcept;\nEffects\nConstructs a new std::promise instance. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with the newly constructed std::promise object. other has\nno associated asynchronous result. \nThrows\nNothing. \nSTD::PROMISE MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of the asynchronous result associated with one std::promise\nobject to another. \nDeclaration\npromise& operator=(promise&& other) noexcept;\nEffects\nTransfers ownership of the asynchronous result associated with other to *this. If\n*this already had an associated asynchronous result, that asynchronous result is\nmade ready with an exception of type std::future_error and an error code of\nstd::future_errc::broken_promise. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the move-\nassignment operator is associated with *this. other has no associated asynchro-\nnous result. \nReturns\n*this \n\n\n485\n<future> header\nThrows\nNothing. \nSTD::PROMISE::SWAP MEMBER FUNCTION \nExchanges ownership of the asynchronous results associated with two std::promise\nobjects. \nDeclaration\nvoid swap(promise& other);\nEffects\nExchanges ownership of the asynchronous results associated with other and *this. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of swap (if\nany) is associated with *this. The asynchronous result associated with *this prior\nto the invocation of swap (if any) is associated with other.\nThrows\nNothing. \nSTD::PROMISE DESTRUCTOR \nDestroys an std::promise object. \nDeclaration\n~promise();\nEffects\nDestroys *this. If *this has an associated asynchronous result, and that result doesn’t\nhave a stored value or exception, that result becomes ready with an std::future_\nerror exception with an error code of std::future_errc::broken_promise. \nThrows\nNothing. \nSTD::PROMISE::GET_FUTURE MEMBER FUNCTION \nRetrieves an std::future instance for the asynchronous result associated with *this. \nDeclaration\nstd::future<ResultType> get_future();\nPreconditions\n*this has an associated asynchronous result. \nReturns\nAn std::future instance for the asynchronous result associated with *this. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::future_already_retrieved if a std::future has already been obtained for\nthis asynchronous result through a prior call to get_future(). \n \n\n\n486\nAPPENDIX D\nC++ Thread Library reference\nSTD::PROMISE::SET_VALUE MEMBER FUNCTION \nStores a value in the asynchronous result associated with *this. \nDeclaration\nvoid promise<void>::set_value();\nvoid promise<R&>::set_value(R& r);\nvoid promise<R>::set_value(R const& r);\nvoid promise<R>::set_value(R&& r);\nPreconditions\n*this has an associated asynchronous result. \nEffects\nStores r in the asynchronous result associated with *this if ResultType isn’t void. \nPostconditions\nThe asynchronous result associated with *this is ready with a stored value. Any\nthreads blocked waiting for the asynchronous result are unblocked. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::promise_already_satisfied if the asynchronous result already has a stored\nvalue or exception. Any exceptions thrown by the copy-constructor or move-con-\nstructor of r. \nSynchronization\nMultiple concurrent calls to set_value(), set_value_at_thread_exit(), set_\nexception(), and set_exception_at_thread_exit() are serialized. A successful\ncall to set_value() happens-before a call to std::future<ResultType>::get() or\nstd::shared_future<ResultType>::get(), which retrieves the value stored. \nSTD::PROMISE::SET_VALUE_AT_THREAD_EXIT MEMBER FUNCTION \nStores a value in the asynchronous result associated with *this without making that\nresult ready until the current thread exits. \nDeclaration\nvoid promise<void>::set_value_at_thread_exit();\nvoid promise<R&>::set_value_at_thread_exit(R& r);\nvoid promise<R>::set_value_at_thread_exit(R const& r);\nvoid promise<R>::set_value_at_thread_exit(R&& r);\nPreconditions\n*this has an associated asynchronous result. \nEffects\nStores r in the asynchronous result associated with *this if ResultType isn’t void.\nMarks the asynchronous result as having a stored value. Schedules the associated\nasynchronous result to be made ready when the current thread exits. \nPostconditions\nThe asynchronous result associated with *this has a stored value but isn’t ready\nuntil the current thread exits. Threads blocked waiting for the asynchronous result\nwill be unblocked when the current thread exits. \n\n\n487\n<future> header\nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::promise_already_satisfied if the asynchronous result already has a stored\nvalue or exception. Any exceptions thrown by the copy-constructor or move-\nconstructor of r. \nSynchronization\nMultiple concurrent calls to set_value(), set_value_at_thread_exit(), set_\nexception(), and set_exception_at_thread_exit() are serialized. The comple-\ntion of the thread that made a successful call to set_value_at_thread_exit() hap-\npens-before a call to std::future<ResultType>::get() or std::shared_future\n<ResultType>::get(), which retrieves the stored exception. \nSTD::PROMISE::SET_EXCEPTION MEMBER FUNCTION \nStores an exception in the asynchronous result associated with *this. \nDeclaration\nvoid set_exception(std::exception_ptr e);\nPreconditions\n*this has an associated asynchronous result. (bool)e is true. \nEffects\nStores e in the asynchronous result associated with *this. \nPostconditions\nThe asynchronous result associated with *this is ready with a stored exception.\nAny threads blocked waiting for the asynchronous result are unblocked. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::promise_already_satisfied if the asynchronous result already has a stored\nvalue or exception. \nSynchronization\nMultiple concurrent calls to set_value() and set_exception() are serialized. A\nsuccessful call to set_exception() happens-before a call to std::future<Result-\nType>::get() or std::shared_future<ResultType>::get(), which retrieves the\nstored exception. \nSTD::PROMISE::SET_EXCEPTION_AT_THREAD_EXIT MEMBER FUNCTION \nStores an exception in the asynchronous result associated with *this without making\nthat result ready until the current thread exits. \nDeclaration\nvoid set_exception_at_thread_exit(std::exception_ptr e);\nPreconditions\n*this has an associated asynchronous result. (bool)e is true. \nEffects\nStores e in the asynchronous result associated with *this. Schedules the associated\nasynchronous result to be made ready when the current thread exits. \n\n\n488\nAPPENDIX D\nC++ Thread Library reference\nPostconditions\nThe asynchronous result associated with *this has a stored exception but isn’t\nready until the current thread exits. Threads blocked waiting for the asynchronous\nresult will be unblocked when the current thread exits. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::promise_already_satisfied if the asynchronous result already has a stored\nvalue or exception. \nSynchronization\nMultiple concurrent calls to set_value(), set_value_at_thread_exit(), set_\nexception(), and set_exception_at_thread_exit() are serialized. The comple-\ntion of the thread that made a successful call to set_exception_at_thread_exit()\nhappens-before a call to std::future<ResultType>::get() or std::shared_\nfuture<ResultType>::get(), which retrieves the exception stored. \nD.4.5\nstd::async function template \nstd::async is a simple way of running self-contained asynchronous tasks to make use\nof the available hardware concurrency. A call to std::async returns a std::future\nthat will contain the result of the task. Depending on the launch policy, the task is\neither run asynchronously on its own thread or synchronously on whichever thread\ncalls the wait() or get() member functions on that future. \nDeclaration\nenum class launch\n{\n    async,deferred\n};\ntemplate<typename Callable,typename ... Args>\nfuture<result_of<Callable(Args...)>::type>\nasync(Callable&& func,Args&& ... args);\ntemplate<typename Callable,typename ... Args>\nfuture<result_of<Callable(Args...)>::type>\nasync(launch policy,Callable&& func,Args&& ... args);\nPreconditions\nThe expression INVOKE(func,args) is valid for the supplied values of func and\nargs. Callable and every member of Args are MoveConstructible. \nEffects\nConstructs copies of func and args... in internal storage (denoted by fff and\nxyz..., respectively). \nIf policy is std::launch::async, runs INVOKE(fff,xyz...) on its own thread.\nThe returned std::future will become ready when this thread is complete and will\nhold either the return value or the exception thrown by the function invocation.\nThe destructor of the last future object associated with the asynchronous state of\nthe returned std::future blocks until the future is ready. \n\n\n489\n<mutex> header\nIf policy is std::launch::deferred, fff and xyz... are stored in the returned\nstd::future as a deferred function call. The first call to the wait() or get() mem-\nber functions on a future that shares the same associated state will execute INVOKE\n(fff,xyz...) synchronously on the thread that called wait() or get(). \nThe value returned or exception thrown by the execution of INVOKE(fff,\nxyz...) will be returned from a call to get() on that std::future. \nIf policy is std::launch::async | std::launch::deferred or the policy\nargument is omitted, the behavior is as-if either std::launch::async or std::\nlaunch::deferred had been specified. The implementation will choose the behav-\nior on a call-by-call basis in order to take advantage of the available hardware con-\ncurrency without excessive oversubscription. \nIn all cases, the std::async call returns immediately. \nSynchronization\nThe completion of the function invocation happens-before a successful return\nfrom a call to wait(), get(), wait_for(), or wait_until() on any std::future or\nstd::shared_future instance that references the same associated state as the\nstd:: future object returned from the std::async call. In the case of a policy of\nstd::launch::async, the completion of the thread on which the function invoca-\ntion occurs also happens-before the successful return from these calls. \nThrows\nstd::bad_alloc if the required internal storage can’t be allocated, otherwise\nstd::future_error when the effects can’t be achieved, or any exception thrown\nduring the construction of fff or xyz.... \nD.5\n<mutex> header \nThe <mutex> header provides facilities for ensuring mutual exclusion: mutex types,\nlock types and functions, and a mechanism for ensuring an operation is performed\nexactly once. \nHeader contents\nnamespace std\n{\n    class mutex;\n    class recursive_mutex;\n    class timed_mutex;\n    class recursive_timed_mutex;\n    class shared_mutex;\n    class shared_timed_mutex;\n    struct adopt_lock_t;\n    struct defer_lock_t;\n    struct try_to_lock_t;\n    constexpr adopt_lock_t adopt_lock{};\n    constexpr defer_lock_t defer_lock{};\n    constexpr try_to_lock_t try_to_lock{};\n    template<typename LockableType>\n    class lock_guard;\n",
      "page_number": 490
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 513-528)",
      "start_page": 513,
      "end_page": 528,
      "detection_method": "topic_boundary",
      "content": "490\nAPPENDIX D\nC++ Thread Library reference\n    template<typename LockableType>\n    class unique_lock;\n    template<typename LockableType>\n    class shared_lock;\n    template<typename ... LockableTypes>\n    class scoped_lock;\n    template<typename LockableType1,typename... LockableType2>\n    void lock(LockableType1& m1,LockableType2& m2...);\n    template<typename LockableType1,typename... LockableType2>\n    int try_lock(LockableType1& m1,LockableType2& m2...);\n    struct once_flag;\n    template<typename Callable,typename... Args>\n    void call_once(once_flag& flag,Callable func,Args args...);\n}\nD.5.1\nstd::mutex class \nThe std::mutex class provides a basic mutual exclusion and synchronization facility\nfor threads that can be used to protect shared data. Prior to accessing the data pro-\ntected by the mutex, the mutex must be locked by calling lock() or try_lock(). Only\none thread may hold the lock at a time, so if another thread also tries to lock the\nmutex, it will fail (try_lock()) or block (lock()) as appropriate. Once a thread is\ndone accessing the shared data, it then must call unlock() to release the lock and\nallow other threads to acquire it. \n std::mutex meets the Lockable requirements. \nClass definition \nclass mutex\n{\npublic:\n    mutex(mutex const&)=delete;\n    mutex& operator=(mutex const&)=delete;\n    constexpr mutex() noexcept;\n    ~mutex();\n    void lock();\n    void unlock();\n    bool try_lock();\n};\nSTD::MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::mutex object. \nDeclaration\nconstexpr mutex() noexcept;\nEffects\nConstructs an std::mutex instance. \n\n\n491\n<mutex> header\nPostconditions\nThe newly constructed std::mutex object is initially unlocked. \nThrows\nNothing.\nSTD::MUTEX DESTRUCTOR \nDestroys an std::mutex object. \nDeclaration\n~mutex();\nPreconditions\n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing.\nSTD::MUTEX::LOCK MEMBER FUNCTION \nAcquires a lock on an std::mutex object for the current thread. \nDeclaration\nvoid lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until a lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a lock on an std::mutex object for the current thread. \nDeclaration\nbool try_lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a lock on *this for the calling thread without blocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \n\n\n492\nAPPENDIX D\nC++ Thread Library reference\nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::MUTEX::UNLOCK MEMBER FUNCTION \nReleases a lock on an std::mutex object held by the current thread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold a lock on *this. \nEffects\nReleases the lock on *this held by the current thread. If any threads are blocked\nwaiting to acquire a lock on *this, unblocks one of them. \nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nD.5.2\nstd::recursive_mutex class \nThe std::recursive_mutex class provides a basic mutual exclusion and synchroniza-\ntion facility for threads that can be used to protect shared data. Prior to accessing the\ndata protected by the mutex, the mutex must be locked by calling lock() or\ntry_lock(). Only one thread may hold the lock at a time, so if another thread also\ntries to lock the recursive_mutex, it will fail (try_lock) or block (lock) as appropri-\nate. Once a thread is done accessing the shared data, it then must call unlock() to\nrelease the lock and allow other threads to acquire it. \n This mutex is recursive so a thread that holds a lock on a particular std::recursive\n_mutex instance may make further calls to lock() or try_lock() to increase the lock\ncount. The mutex can’t be locked by another thread until the thread that acquired\nthe locks has called unlock once for each successful call to lock() or try_lock(). \n std::recursive_mutex meets the Lockable requirements. \nClass definition \nclass recursive_mutex\n{\npublic:\n    recursive_mutex(recursive_mutex const&)=delete;\n    recursive_mutex& operator=(recursive_mutex const&)=delete;\n    recursive_mutex() noexcept;\n    ~recursive_mutex();\n    void lock();\n    void unlock();\n    bool try_lock() noexcept;\n};\n\n\n493\n<mutex> header\nSTD::RECURSIVE_MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::recursive_mutex object. \nDeclaration\nrecursive_mutex() noexcept;\nEffects\nConstructs an std::recursive_mutex instance. \nPostconditions\nThe newly constructed std::recursive_mutex object is initially unlocked. \nThrows\nAn exception of type std::system_error if unable to create a new std::recursive\n_mutex instance. \nSTD::RECURSIVE_MUTEX DESTRUCTOR \nDestroys an std::recursive_mutex object. \nDeclaration\n~recursive_mutex();\nPreconditions \n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing. \nSTD::RECURSIVE_MUTEX::LOCK MEMBER FUNCTION \nAcquires a lock on an std::recursive_mutex object for the current thread. \nDeclaration\nvoid lock();\nEffects\nBlocks the current thread until a lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread. If the calling thread already held a lock on\n*this, the lock count is increased by one. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::RECURSIVE_MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a lock on an std::recursive_mutex object for the current thread. \nDeclaration\nbool try_lock() noexcept;\nEffects\nAttempts to acquire a lock on *this for the calling thread without blocking. \n\n\n494\nAPPENDIX D\nC++ Thread Library reference\nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\nA new lock on *this has been obtained for the calling thread if the function\nreturns true. \nThrows\nNothing. \nNOTE\nIf the calling thread already holds the lock on *this, the function\nreturns true and the count of locks on *this held by the calling thread is\nincreased by one. If the current thread doesn’t already hold a lock on *this,\nthe function may fail to acquire the lock (and return false) even if no other\nthread holds a lock on *this. \nSTD::RECURSIVE_MUTEX::UNLOCK MEMBER FUNCTION \nReleases a lock on an std::recursive_mutex object held by the current thread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold a lock on *this. \nEffects\nReleases a lock on *this held by the current thread. If this is the last lock on *this\nheld by the calling thread, any threads are blocked waiting to acquire a lock on\n*this. Unblocks one of them. \nPostconditions\nThe number of locks on *this held by the calling thread is reduced by one. \nThrows\nNothing. \nD.5.3\nstd::timed_mutex class \nThe std::timed_mutex class provides support for locks with timeouts on top of the\nbasic mutual exclusion and synchronization facility provided by std::mutex. Prior to\naccessing the data protected by the mutex, the mutex must be locked by calling lock(),\ntry_lock(), try_lock_for(), or try_lock_until(). If a lock is already held by\nanother thread, an attempt to acquire the lock will fail (try_lock()), block until the\nlock can be acquired (lock()), or block until the lock can be acquired or the lock\nattempt times out (try_lock_for() or try_lock_until()). Once a lock has been\nacquired (whichever function was used to acquire it), it must be released, by calling\nunlock(), before another thread can acquire the lock on the mutex. \n std::timed_mutex meets the TimedLockable requirements. \nClass definition \nclass timed_mutex\n{\n\n\n495\n<mutex> header\npublic:\n    timed_mutex(timed_mutex const&)=delete;\n    timed_mutex& operator=(timed_mutex const&)=delete;\n    timed_mutex();\n    ~timed_mutex();\n    void lock();\n    void unlock();\n    bool try_lock();\n    template<typename Rep,typename Period>\n    bool try_lock_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock,typename Duration>\n    bool try_lock_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n};\nSTD::TIMED_MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::timed_mutex object. \nDeclaration\ntimed_mutex();\nEffects\nConstructs an std::timed_mutex instance. \nPostconditions\nThe newly constructed std::timed_mutex object is initially unlocked. \nThrows\nAn exception of type std::system_error if unable to create a new std::timed\n_mutex instance. \nSTD::TIMED_MUTEX DESTRUCTOR \nDestroys an std::timed_mutex object. \nDeclaration\n~timed_mutex();\nPreconditions\n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing.\nSTD::TIMED_MUTEX::LOCK MEMBER FUNCTION \nAcquires a lock on an std::timed_mutex object for the current thread. \nDeclaration\nvoid lock();\n\n\n496\nAPPENDIX D\nC++ Thread Library reference\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until a lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::TIMED_MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a lock on an std::timed_mutex object for the current thread. \nDeclaration\nbool try_lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a lock on *this for the calling thread without blocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::TIMED_MUTEX::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire a lock on an std::timed_mutex object for the current thread. \nDeclaration\ntemplate<typename Rep,typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a lock on *this for the calling thread within the time specified\nby relative_time. If relative_time.count() is zero or negative, the call will\nreturn immediately, as if it was a call to try_lock(). Otherwise, the call blocks until\neither the lock has been acquired or the time period specified by relative_time\nhas elapsed. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \n\n\n497\n<mutex> header\nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. The thread may be blocked for longer\nthan the specified duration. Where possible, the elapsed time is determined\nby a steady clock. \nSTD::TIMED_MUTEX::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a lock on an std::timed_mutex object for the current thread. \nDeclaration\ntemplate<typename Clock,typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a lock on *this for the calling thread before the time specified\nby absolute_time. If absolute_time<=Clock::now() on entry, the call will return\nimmediately, as if it was a call to try_lock(). Otherwise, the call blocks until either\nthe lock has been acquired or Clock::now() returns a time equal to or later than\nabsolute_time. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. There’s no guarantee as to how long\nthe calling thread will be blocked, only that if the function returns false,\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked. \nSTD::TIMED_MUTEX::UNLOCK MEMBER FUNCTION \nReleases a lock on an std::timed_mutex object held by the current thread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold a lock on *this. \n\n\n498\nAPPENDIX D\nC++ Thread Library reference\nEffects \nReleases the lock on *this held by the current thread. If any threads are blocked\nwaiting to acquire a lock on *this, unblocks one of them. \nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nD.5.4\nstd::recursive_timed_mutex class \nThe std::recursive_timed_mutex class provides support for locks with timeouts on\ntop of the mutual exclusion and synchronization facility provided by std::recursive_\nmutex. Prior to accessing the data protected by the mutex, the mutex must be locked by\ncalling lock(), try_lock(), try_lock_for(), or try_lock_until(). If a lock is already\nheld by another thread, an attempt to acquire the lock will fail (try_lock()), block\nuntil the lock can be acquired (lock()), or block until the lock can be acquired or the\nlock attempt times out (try_lock_for() or try_lock_until()). Once a lock has\nbeen acquired (whichever function was used to acquire it), it must be released by call-\ning unlock() before another thread can acquire the lock on the mutex. \n This mutex is recursive, so a thread that holds a lock on a particular instance of\nstd::recursive_timed_mutex may acquire additional locks on that instance through\nany of the lock functions. All of these locks must be released by a corresponding call\nto unlock() before another thread can acquire a lock on that instance. \n std::recursive_timed_mutex meets the TimedLockable requirements. \nClass definition \nclass recursive_timed_mutex\n{\npublic:\n    recursive_timed_mutex(recursive_timed_mutex const&)=delete;\n    recursive_timed_mutex& operator=(recursive_timed_mutex const&)=delete;\n    recursive_timed_mutex();\n    ~recursive_timed_mutex();\n    void lock();\n    void unlock();\n    bool try_lock() noexcept;\n    template<typename Rep,typename Period>\n    bool try_lock_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock,typename Duration>\n    bool try_lock_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n};\n\n\n499\n<mutex> header\nSTD::RECURSIVE_TIMED_MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::recursive_timed_mutex object. \nDeclaration\nrecursive_timed_mutex();\nEffects\nConstructs an std::recursive_timed_mutex instance. \nPostconditions\nThe newly constructed std::recursive_timed_mutex object is initially unlocked. \nThrows\nAn exception of type std::system_error if unable to create a new std::recursive\n_timed_mutex instance. \nSTD::RECURSIVE_TIMED_MUTEX DESTRUCTOR \nDestroys an std::recursive_timed_mutex object. \nDeclaration\n~recursive_timed_mutex();\nPreconditions\n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing. \nSTD::RECURSIVE_TIMED_MUTEX::LOCK MEMBER FUNCTION \nAcquires a lock on an std::recursive_timed_mutex object for the current thread. \nDeclaration\nvoid lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until a lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread. If the calling thread already held a lock on\n*this, the lock count is increased by one. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::RECURSIVE_TIMED_MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a lock on an std::recursive_timed_mutex object for the current\nthread. \nDeclaration\nbool try_lock() noexcept;\n\n\n500\nAPPENDIX D\nC++ Thread Library reference\nEffects\nAttempts to acquire a lock on *this for the calling thread without blocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nIf the calling thread already holds the lock on *this, the function\nreturns true and the count of locks on *this held by the calling thread is\nincreased by one. If the current thread doesn’t already hold a lock on *this,\nthe function may fail to acquire the lock (and return false) even if no other\nthread holds a lock on *this. \nSTD::RECURSIVE_TIMED_MUTEX::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire a lock on an std::recursive_timed_mutex object for the current\nthread. \nDeclaration\ntemplate<typename Rep,typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nEffects\nAttempts to acquire a lock on *this for the calling thread within the time specified\nby relative_time. If relative_time.count() is zero or negative, the call will\nreturn immediately, as if it was a call to try_lock(). Otherwise, the call blocks until\neither the lock has been acquired or the time period specified by relative_time\nhas elapsed. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nIf the calling thread already holds the lock on *this, the function\nreturns true and the count of locks on *this held by the calling thread is\nincreased by one. If the current thread doesn’t already hold a lock on *this,\nthe function may fail to acquire the lock (and return false) even if no other\nthread holds a lock on *this. The thread may be blocked for longer than the\nspecified duration. Where possible, the elapsed time is determined by a\nsteady clock. \n\n\n501\n<mutex> header\nSTD::RECURSIVE_TIMED_MUTEX::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a lock on an std::recursive_timed_mutex object for the current\nthread. \nDeclaration\ntemplate<typename Clock,typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nEffects\nAttempts to acquire a lock on *this for the calling thread before the time specified\nby absolute_time. If absolute_time<=Clock::now() on entry, the call will return\nimmediately, as if it was a call to try_lock(). Otherwise, the call blocks until either\nthe lock has been acquired or Clock::now() returns a time equal to or later than\nabsolute_time. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nIf the calling thread already holds the lock on *this, the function\nreturns true and the count of locks on *this held by the calling thread is\nincreased by one. If the current thread doesn’t already hold a lock on *this,\nthe function may fail to acquire the lock (and return false) even if no other\nthread holds a lock on *this. There’s no guarantee as to how long the calling\nthread will be blocked, only that if the function returns false, then\nClock::now() returns a time equal to or later than absolute_time at the\npoint at which the thread became unblocked. \nSTD::RECURSIVE_TIMED_MUTEX::UNLOCK MEMBER FUNCTION \nReleases a lock on an std::recursive_timed_mutex object held by the current\nthread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold a lock on *this. \nEffects\nReleases a lock on *this held by the current thread. If this is the last lock on *this\nheld by the calling thread, any threads are blocked waiting to acquire a lock on\n*this. Unblocks one of them. \nPostconditions\nThe number of locks on *this held by the calling thread is reduced by one. \nThrows\nNothing. \n\n\n502\nAPPENDIX D\nC++ Thread Library reference\nD.5.5\nstd::shared_mutex class \nThe std::shared_mutex class provides a mutual exclusion and synchronization facil-\nity for threads that can be used to protect shared data that is frequently read and\nrarely modified. It allows one thread to hold an exclusive lock, or one or more threads\nto hold a shared lock. Prior to modifying the data protected by the mutex, the mutex\nmust be locked with an exclusive lock by calling lock() or try_lock(). Only one\nthread may hold an exclusive lock at a time, so if another thread also tries to lock the\nmutex, it will fail (try_lock()) or block (lock()) as appropriate. Once a thread is\ndone modifying the shared data, it then must call unlock() to release the lock and\nallow other threads to acquire it. Threads that only want to read the protected data\nmay obtain a shared lock by calling lock_shared() or try_lock_shared(). Multiple\nthreads may hold a shared lock at a time, so if one thread holds a shared lock, then\nanother thread may also acquire a shared lock. If a thread tries to acquire an exclusive\nlock, that thread will wait. Once a thread that has acquired a shared lock is done\naccessing the protected data, it must call unlock_shared() to release the shared lock.\n std::shared_mutex meets the Lockable requirements. \nClass definition \nclass shared_mutex\n{\npublic:\n    shared_mutex(shared_mutex const&)=delete;\n    shared_mutex& operator=(shared_mutex const&)=delete;\n    shared_mutex() noexcept;\n    ~shared_mutex();\n    void lock();\n    void unlock();\n    bool try_lock();\n    void lock_shared();\n    void unlock_shared();\n    bool try_lock_shared();\n};\nSTD::SHARED_MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::shared_mutex object. \nDeclaration\nshared_mutex() noexcept;\nEffects\nConstructs an std::shared_mutex instance. \nPostconditions\nThe newly constructed std::shared_mutex object is initially unlocked. \nThrows\nNothing.\n\n\n503\n<mutex> header\nSTD::SHARED_MUTEX DESTRUCTOR \nDestroys an std::shared_mutex object. \nDeclaration\n~shared_mutex();\nPreconditions\n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing.\nSTD::SHARED_MUTEX::LOCK MEMBER FUNCTION \nAcquires an exclusive lock on an std::shared_mutex object for the current thread. \nDeclaration\nvoid lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until an exclusive lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread with an exclusive lock. \nThrows\nAn exception ofif an error occurs. \nSTD::SHARED_MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire an exclusive lock on an std::shared_mutex object for the cur-\nrent thread. \nDeclaration\nbool try_lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire an exclusive lock on *this for the calling thread without\nblocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread with an exclusive lock if the function returns\ntrue. \n\n\n504\nAPPENDIX D\nC++ Thread Library reference\nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::SHARED_MUTEX::UNLOCK MEMBER FUNCTION \nReleases an exclusive lock on an std::shared_mutex object held by the current\nthread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold an exclusive lock on *this. \nEffects\nReleases the exclusive lock on *this held by the current thread. If any threads are\nblocked waiting to acquire a lock on *this, unblocks one thread waiting for an\nexclusive lock or some number of threads waiting for a shared lock. \nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nSTD::SHARED_MUTEX::LOCK_SHARED MEMBER FUNCTION \nAcquires a shared lock on an std::shared_mutex object for the current thread. \nDeclaration\nvoid lock_shared();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until a shared lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread with a shared lock. \nThrows\nAn exception ofif an error occurs. \nSTD::SHARED_MUTEX::TRY_LOCK_SHARED MEMBER FUNCTION \nAttempts to acquire a shared lock on an std::shared_mutex object for the current\nthread. \nDeclaration\nbool try_lock_shared();\nPreconditions\nThe calling thread must not hold a lock on *this. \n\n\n505\n<mutex> header\nEffects\nAttempts to acquire a shared lock on *this for the calling thread without blocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread with a shared lock if the function returns\ntrue. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::SHARED_MUTEX::UNLOCK_SHARED MEMBER FUNCTION \nReleases a shared lock on an std::shared_mutex object held by the current thread. \nDeclaration\nvoid unlock_shared();\nPreconditions\nThe calling thread must hold a shared lock on *this. \nEffects\nReleases the shared lock on *this held by the current thread. If this is the last\nshared lock on *this, and any threads are blocked waiting to acquire a lock on\n*this, unblocks one thread waiting for an exclusive lock or some number of\nthreads waiting for a shared lock. \nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nD.5.6\nstd::shared_timed_mutex class \nThe std::shared_timed_mutex class provides a mutual exclusion and synchroniza-\ntion facility for threads that can be used to protect shared data that is frequently read\nand rarely modified. It allows one thread to hold an exclusive lock, or one or more\nthreads to hold a shared lock. Prior to modifying the data protected by the mutex, the\nmutex must be locked with an exclusive lock by calling lock() or try_lock(). Only\none thread may hold an exclusive lock at a time, so if another thread also tries to lock\nthe mutex, it will fail (try_lock()) or block (lock()) as appropriate. Once a thread is\ndone modifying the shared data, it then must call unlock() to release the lock and\nallow other threads to acquire it. Threads that only want to read the protected data\nmay obtain a shared lock by calling lock_shared() or try_lock_shared(). Multiple\nthreads may hold a shared lock at a time, so if one thread holds a shared lock, then\nanother thread may also acquire a shared lock. If a thread tries to acquire an exclusive\n",
      "page_number": 513
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 529-537)",
      "start_page": 529,
      "end_page": 537,
      "detection_method": "topic_boundary",
      "content": "506\nAPPENDIX D\nC++ Thread Library reference\nlock, that thread will wait. Once a thread that has acquired a shared lock is done\naccessing the protected data, it must call unlock_shared() to release the shared lock.\n std::shared_timed_mutex meets the Lockable requirements. \nClass definition \nclass shared_timed_mutex\n{\npublic:\n    shared_timed_mutex(shared_timed_mutex const&)=delete;\n    shared_timed_mutex& operator=(shared_timed_mutex const&)=delete;\n    shared_timed_mutex() noexcept;\n    ~shared_timed_mutex();\n    void lock();\n    void unlock();\n    bool try_lock();\n    template<typename Rep,typename Period>\n    bool try_lock_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock,typename Duration>\n    bool try_lock_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n    void lock_shared();\n    void unlock_shared();\n    bool try_lock_shared();\n    template<typename Rep,typename Period>\n    bool try_lock_shared_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock,typename Duration>\n    bool try_lock_shared_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n};\nSTD::SHARED_TIMED_MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::shared_timed_mutex object. \nDeclaration\nshared_timed_mutex() noexcept;\nEffects\nConstructs an std::shared_timed_mutex instance. \nPostconditions\nThe newly constructed std::shared_timed_mutex object is initially unlocked. \nThrows\nNothing.\n\n\n507\n<mutex> header\nSTD::SHARED_TIMED_MUTEX DESTRUCTOR \nDestroys an std::shared_timed_mutex object. \nDeclaration\n~shared_timed_mutex();\nPreconditions\n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing.\nSTD::SHARED_TIMED_MUTEX::LOCK MEMBER FUNCTION \nAcquires an exclusive lock on an std::shared_timed_mutex object for the current\nthread. \nDeclaration\nvoid lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until an exclusive lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread with an exclusive lock. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::SHARED_TIMED_MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire an exclusive lock on an std::shared_timed_mutex object for the\ncurrent thread. \nDeclaration\nbool try_lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire an exclusive lock on *this for the calling thread without\nblocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread with an exclusive lock if the function returns\ntrue. \nThrows\nNothing. \n\n\n508\nAPPENDIX D\nC++ Thread Library reference\nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::SHARED_TIMED_MUTEX::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire an exclusive lock on an std::shared_timed_mutex object for the\ncurrent thread. \nDeclaration\ntemplate<typename Rep,typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire an exclusive lock on *this for the calling thread within the\ntime specified by relative_time. If relative_time.count() is zero or negative,\nthe call will return immediately, as if it was a call to try_lock(). Otherwise, the call\nblocks until either the lock has been acquired or the time period specified by\nrelative_time has elapsed. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. The thread may be blocked for longer\nthan the specified duration. Where possible, the elapsed time is determined\nby a steady clock. \nSTD::SHARED_TIMED_MUTEX::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire an exclusive lock on an std::shared_timed_mutex object for the\ncurrent thread. \nDeclaration\ntemplate<typename Clock,typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire an exclusive lock on *this for the calling thread before the\ntime specified by absolute_time. If absolute_time<=Clock::now() on entry, the\ncall will return immediately, as if it was a call to try_lock(). Otherwise, the call\n\n\n509\n<mutex> header\nblocks until either the lock has been acquired or Clock::now() returns a time equal\nto or later than absolute_time. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. There’s no guarantee as to how long\nthe calling thread will be blocked, only that if the function returns false,\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked. \nSTD::SHARED_TIMED_MUTEX::UNLOCK MEMBER FUNCTION \nReleases an exclusive lock on an std::shared_timed_mutex object held by the cur-\nrent thread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold an exclusive lock on *this. \nEffects\nReleases the exclusive lock on *this held by the current thread. If any threads are\nblocked waiting to acquire a lock on *this, unblocks one thread waiting for an\nexclusive lock or some number of threads waiting for a shared lock. \nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nSTD::SHARED_TIMED_MUTEX::LOCK_SHARED MEMBER FUNCTION \nAcquires a shared lock on an std::shared_timed_mutex object for the current thread. \nDeclaration\nvoid lock_shared();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until a shared lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread with a shared lock. \nThrows\nAn exception of type std::system_error if an error occurs. \n\n\n510\nAPPENDIX D\nC++ Thread Library reference\nSTD::SHARED_TIMED_MUTEX::TRY_LOCK_SHARED MEMBER FUNCTION \nAttempts to acquire a shared lock on an std::shared_timed_mutex object for the cur-\nrent thread. \nDeclaration\nbool try_lock_shared();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a shared lock on *this for the calling thread without blocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread with a shared lock if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::SHARED_TIMED_MUTEX::TRY_LOCK_SHARED_FOR MEMBER FUNCTION \nAttempts to acquire a shared lock on an std::shared_timed_mutex object for the cur-\nrent thread. \nDeclaration\ntemplate<typename Rep,typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a shared lock on *this for the calling thread within the time\nspecified by relative_time. If relative_time.count() is zero or negative, the call\nwill return immediately, as if it was a call to try_lock(). Otherwise, the call blocks\nuntil either the lock has been acquired or the time period specified by relative\n_time has elapsed. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. The thread may be blocked for longer\n\n\n511\n<mutex> header\nthan the specified duration. Where possible, the elapsed time is determined\nby a steady clock. \nSTD::SHARED_TIMED_MUTEX::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a shared lock on an std::shared_timed_mutex object for the cur-\nrent thread. \nDeclaration\ntemplate<typename Clock,typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a shared lock on *this for the calling thread before the time\nspecified by absolute_time. If absolute_time<=Clock::now() on entry, the call\nwill return immediately, as if it was a call to try_lock(). Otherwise, the call blocks\nuntil either the lock has been acquired or Clock::now() returns a time equal to or\nlater than absolute_time. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. There’s no guarantee as to how long\nthe calling thread will be blocked, only that if the function returns false,\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked. \nSTD::SHARED_TIMED_MUTEX::UNLOCK_SHARED MEMBER FUNCTION \nReleases a shared lock on an std::shared_timed_mutex object held by the current\nthread. \nDeclaration\nvoid unlock_shared();\nPreconditions\nThe calling thread must hold a shared lock on *this. \nEffects\nReleases the shared lock on *this held by the current thread. If this is the last shared\nlock on *this, and any threads are blocked waiting to acquire a lock on *this,\nunblocks one thread waiting for an exclusive lock or some number of threads wait-\ning for a shared lock. \n\n\n512\nAPPENDIX D\nC++ Thread Library reference\nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nD.5.7\nstd::lock_guard class template \nThe std::lock_guard class template provides a basic lock ownership wrapper. The\ntype of mutex being locked is specified by template parameter Mutex and must meet\nthe Lockable requirements. The specified mutex is locked in the constructor and\nunlocked in the destructor. This provides a simple means of locking a mutex for a\nblock of code and ensuring that the mutex is unlocked when the block is left, whether\nthat’s by running off the end, by the use of a control flow statement such as break or\nreturn, or by throwing an exception. \n Instances of std::lock_guard are not MoveConstructible, CopyConstructible,\nor CopyAssignable. \nClass definition\ntemplate <class Mutex>\nclass lock_guard\n{\npublic:\n    typedef Mutex mutex_type;\n    explicit lock_guard(mutex_type& m);\n    lock_guard(mutex_type& m, adopt_lock_t);\n    ~lock_guard();\n    lock_guard(lock_guard const& ) = delete;\n    lock_guard& operator=(lock_guard const& ) = delete;\n};\nSTD::LOCK_GUARD LOCKING CONSTRUCTOR \nConstructs an std::lock_guard instance that locks the supplied mutex. \nDeclaration\nexplicit lock_guard(mutex_type& m);\nEffects\nConstructs an std::lock_guard instance that references the supplied mutex. Calls\nm.lock(). \nThrows \nAny exceptions thrown by m.lock(). \nPostconditions \n*this owns a lock on m. \nSTD::LOCK_GUARD LOCK-ADOPTING CONSTRUCTOR \nConstructs an std::lock_guard instance that owns the lock on the supplied mutex. \nDeclaration\nlock_guard(mutex_type& m,std::adopt_lock_t);\n\n\n513\n<mutex> header\nPreconditions\nThe calling thread must own a lock on m. \nEffects\nConstructs an std::lock_guard instance that references the supplied mutex and\ntakes ownership of the lock on m held by the calling thread. \nThrows\nNothing. \nPostconditions \n*this owns the lock on m held by the calling thread. \nSTD::LOCK_GUARD DESTRUCTOR \nDestroys an std::lock_guard instance and unlocks the corresponding mutex. \nDeclaration\n~lock_guard();\nEffects\nCalls m.unlock() for the mutex instance, m, supplied when *this was constructed. \nThrows\nNothing. \nD.5.8\nstd::scoped_lock class template \nThe std::scoped_lock class template provides a basic lock ownership wrapper for\nmultiple mutexes at once. The type of mutex being locked is specified by the template\nparameter pack Mutexes and each must meet the Lockable requirements. The speci-\nfied mutexes are locked in the constructor and unlocked in the destructor. This pro-\nvides a simple means of locking a set of mutexes for a block of code and ensuring that\nthe mutexes are unlocked when the block is left, whether that’s by running off the\nend, by the use of a control flow statement such as break or return, or by throwing an\nexception. \n Instances of std::scoped_lock are not MoveConstructible, CopyConstructible,\nor CopyAssignable. \nClass definition\ntemplate <class ... Mutexes>\nclass scoped_lock\n{\npublic:\n    explicit scoped_lock(Mutexes& ... m);\n    scoped_lock(Mutexes& ... m, adopt_lock_t);\n    ~scoped_lock();\n    scoped_lock(scoped_lock const& ) = delete;\n    scoped_lock& operator=(scoped_lock const& ) = delete;\n};\nSTD::SCOPED_LOCK LOCKING CONSTRUCTOR \nConstructs an std::scoped_lock instance that locks the supplied mutexes. \n\n\n514\nAPPENDIX D\nC++ Thread Library reference\nDeclaration\nexplicit scoped_lock(Mutexes& ... m);\nEffects\nConstructs an std::scoped_lock instance that references the supplied mutexes.\nUses a combination of calls to m.lock(), m.try_lock(), and m.unlock() on each\nof the mutexes, in order to avoid deadlock, using the same algorithm as the\nstd::lock() free function.\nThrows \nAny exceptions thrown by the m.lock() and m.try_lock() calls.\nPostconditions \n*this owns a lock on the supplied mutexes. \nSTD::SCOPED_LOCK LOCK-ADOPTING CONSTRUCTOR \nConstructs an std::scoped_lock instance that owns the lock on the supplied mutexes;\nthey must already be locked by the calling thread. \nDeclaration\nscoped_lock(Mutexes& ... m,std::adopt_lock_t);\nPreconditions\nThe calling thread must own a lock on the mutexes in m. \nEffects\nConstructs an std::scoped_lock instance that references the supplied mutexes\nand takes ownership of the lock on the mutexes in m held by the calling thread. \nThrows\nNothing. \nPostconditions \n*this owns the lock on the supplied mutexes held by the calling thread. \nSTD::SCOPED_LOCK DESTRUCTOR \nDestroys an std::scoped_lock instance and unlocks the corresponding mutexes. \nDeclaration\n~scoped_lock();\nEffects\nCalls m.unlock() for each of the mutex instances m supplied when *this was con-\nstructed. \nThrows\nNothing. \nD.5.9\nstd::unique_lock class template \nThe std::unique_lock class template provides a more general lock ownership wrap-\nper than std::lock_guard. The type of mutex being locked is specified by the template\nparameter Mutex, which must meet the BasicLockable requirements. In general, the\nspecified mutex is locked in the constructor and unlocked in the destructor, although\n",
      "page_number": 529
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 538-555)",
      "start_page": 538,
      "end_page": 555,
      "detection_method": "topic_boundary",
      "content": "515\n<mutex> header\nadditional constructors and member functions are provided to allow other possibili-\nties. This provides a means of locking a mutex for a block of code and ensuring that\nthe mutex is unlocked when the block is left, whether that’s by running off the end, by\nthe use of a control flow statement such as break or return, or by throwing an excep-\ntion. The wait functions of std::condition_variable require an instance of std::\nunique_lock<std::mutex>, and all instantiations of std::unique_lock are suitable\nfor use with the Lockable parameter for the std::condition_variable_any wait\nfunctions. \n If the supplied Mutex type meets the Lockable requirements, then std::unique_\nlock<Mutex> also meets the Lockable requirements. If, in addition, the supplied\nMutex type meets the TimedLockable requirements, then std::unique_lock<Mutex>\nalso meets the TimedLockable requirements. \n Instances of std::unique_lock are MoveConstructible and MoveAssignable but\nnot CopyConstructible or CopyAssignable. \nClass definition\ntemplate <class Mutex>\nclass unique_lock\n{\npublic:\n    typedef Mutex mutex_type;\n    unique_lock() noexcept;\n    explicit unique_lock(mutex_type& m);\n    unique_lock(mutex_type& m, adopt_lock_t);\n    unique_lock(mutex_type& m, defer_lock_t) noexcept;\n    unique_lock(mutex_type& m, try_to_lock_t);\n    template<typename Clock,typename Duration>\n    unique_lock(\n        mutex_type& m,\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n    template<typename Rep,typename Period>\n    unique_lock(\n        mutex_type& m,\n        std::chrono::duration<Rep,Period> const& relative_time);\n    ~unique_lock();\n    unique_lock(unique_lock const& ) = delete;\n    unique_lock& operator=(unique_lock const& ) = delete;\n    unique_lock(unique_lock&& );\n    unique_lock& operator=(unique_lock&& );\n    void swap(unique_lock& other) noexcept;\n    void lock();\n    bool try_lock();\n    template<typename Rep, typename Period>\n    bool try_lock_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock, typename Duration>\n    bool try_lock_until(\n\n\n516\nAPPENDIX D\nC++ Thread Library reference\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n    void unlock();\n    explicit operator bool() const noexcept;\n    bool owns_lock() const noexcept;\n    Mutex* mutex() const noexcept;\n    Mutex* release() noexcept;\n};\nSTD::UNIQUE_LOCK DEFAULT CONSTRUCTOR \nConstructs an std::unique_lock instance with no associated mutex. \nDeclaration\nunique_lock() noexcept;\nEffects\nConstructs an std::unique_lock instance that has no associated mutex. \nPostconditions\nthis->mutex()==NULL, this->owns_lock()==false. \nSTD::UNIQUE_LOCK LOCKING CONSTRUCTOR \nConstructs an std::unique_lock instance that locks the supplied mutex. \nDeclaration\nexplicit unique_lock(mutex_type& m);\nEffects \nConstructs an std::unique_lock instance that references the supplied mutex.\nCalls m.lock(). \nThrows\nAny exceptions thrown by m.lock(). \nPostconditions\nthis->owns_lock()==true, this->mutex()==&m. \nSTD::UNIQUE_LOCK LOCK-ADOPTING CONSTRUCTOR \nConstructs an std::unique_lock instance that owns the lock on the supplied mutex. \nDeclaration\nunique_lock(mutex_type& m,std::adopt_lock_t);\nPreconditions\nThe calling thread must own a lock on m. \nEffects\nConstructs an std::unique_lock instance that references the supplied mutex and\ntakes ownership of the lock on m held by the calling thread. \nThrows\nNothing. \nPostconditions\nthis->owns_lock()==true, this->mutex()==&m. \n\n\n517\n<mutex> header\nSTD::UNIQUE_LOCK DEFERRED-LOCK CONSTRUCTOR \nConstructs an std::unique_lock instance that doesn’t own the lock on the supplied\nmutex. \nDeclaration\nunique_lock(mutex_type& m,std::defer_lock_t) noexcept;\nEffects\nConstructs an std::unique_lock instance that references the supplied mutex. \nThrows \nNothing. \nPostconditions\nthis->owns_lock()==false, this->mutex()==&m. \nSTD::UNIQUE_LOCK TRY-TO-LOCK CONSTRUCTOR \nConstructs an std::unique_lock instance associated with the supplied mutex and\ntries to acquire a lock on that mutex. \nDeclaration\nunique_lock(mutex_type& m,std::try_to_lock_t);\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the Lockable\nrequirements. \nEffects\nConstructs an std::unique_lock instance that references the supplied mutex.\nCalls m.try_lock(). \nThrows\nNothing. \nPostconditions\nthis->owns_lock() returns the result of the m.try_lock() call, this->mutex()==&m. \nSTD::UNIQUE_LOCK TRY-TO-LOCK CONSTRUCTOR WITH A DURATION TIMEOUT \nConstructs an std::unique_lock instance associated with the supplied mutex and\ntries to acquire a lock on that mutex. \nDeclaration\ntemplate<typename Rep,typename Period>\nunique_lock(\n    mutex_type& m,\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the Timed-Lockable\nrequirements. \nEffects\nConstructs an std::unique_lock instance that references the supplied mutex. Calls\nm.try_lock_for(relative_time). \n\n\n518\nAPPENDIX D\nC++ Thread Library reference\nThrows\nNothing. \nPostconditions\nthis->owns_lock() returns the result of the m.try_lock_for() call, this->mutex()\n==&m. \nSTD::UNIQUE_LOCK TRY-TO-LOCK CONSTRUCTOR WITH A TIME_POINT TIMEOUT \nConstructs an std::unique_lock instance associated with the supplied mutex and\ntries to acquire a lock on that mutex. \nDeclaration\ntemplate<typename Clock,typename Duration>\nunique_lock(\n    mutex_type& m,\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the Timed-Lockable\nrequirements. \nEffects\nConstructs an std::unique_lock instance that references the supplied mutex. Calls\nm.try_lock_until(absolute_time). \nThrows\nNothing. \nPostconditions\nthis->owns_lock() returns the result of the m.try_lock_until() call, this->\nmutex()==&m. \nSTD::UNIQUE_LOCK MOVE-CONSTRUCTOR \nTransfers ownership of a lock from one std::unique_lock object to a newly-created\nstd::unique_lock object. \nDeclaration\nunique_lock(unique_lock&& other) noexcept;\nEffects\nConstructs an std::unique_lock instance. If other owned a lock on a mutex prior\nto the constructor invocation, that lock is now owned by the newly created\nstd::unique_lock object. \nPostconditions\nFor a newly constructed std::unique_lock object, x, x.mutex() is equal to the value\nof other.mutex() prior to the constructor invocation, and x.owns_lock() is equal to\nthe value of other.owns_lock() prior to the constructor invocation. other.mutex()\n==NULL, other.owns_lock()==false. \nThrows\nNothing. \n\n\n519\n<mutex> header\nNOTE\nstd::unique_lock objects are not CopyConstructible, so there’s no\ncopy constructor, only this move constructor.\nSTD::UNIQUE_LOCK MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of a lock from one std::unique_lock object to another std::\nunique_lock object. \nDeclaration\nunique_lock& operator=(unique_lock&& other) noexcept;\nEffects\nIf this->owns_lock()returns true prior to the call, calls this->unlock(). If other\nowned a lock on a mutex prior to the assignment, that lock is now owned by *this. \nPostconditions\nthis->mutex() is equal to the value of other.mutex() prior to the assignment, and\nthis->owns_lock() is equal to the value of other.owns_lock() prior to the assign-\nment. other.mutex()==NULL, other.owns_lock()==false. \nThrows\nNothing. \nNOTE\nstd::unique_lock objects are not CopyAssignable, so there’s no copy-\nassignment operator, only this move-assignment operator. \nSTD::UNIQUE_LOCK DESTRUCTOR \nDestroys an std::unique_lock instance and unlocks the corresponding mutex if it’s\nowned by the destroyed instance. \nDeclaration\n~unique_lock();\nEffects\nIf this->owns_lock()returns true, calls this->mutex()->unlock(). \nThrows\nNothing. \nSTD::UNIQUE_LOCK::SWAP MEMBER FUNCTION \nExchanges ownership of their associated unique_locks of execution between two\nstd::unique_lock objects. \nDeclaration\nvoid swap(unique_lock& other) noexcept;\nEffects\nIf other owns a lock on a mutex prior to the call, that lock is now owned by *this.\nIf *this owns a lock on a mutex prior to the call, that lock is now owned by other. \nPostconditions\nthis->mutex() is equal to the value of other.mutex() prior to the call. other\n.mutex() is equal to the value of this->mutex() prior to the call. this->owns_lock()\n\n\n520\nAPPENDIX D\nC++ Thread Library reference\nis equal to the value of other.owns_lock() prior to the call. other.owns_lock() is\nequal to the value of this->owns_lock() prior to the call. \nThrows\nNothing. \nSWAP NONMEMBER FUNCTION FOR STD::UNIQUE_LOCK\nExchanges ownership of their associated mutex locks between two std::unique_lock\nobjects. \nDeclaration\nvoid swap(unique_lock& lhs,unique_lock& rhs) noexcept;\nEffects\nlhs.swap(rhs) \nThrows\nNothing. \nSTD::UNIQUE_LOCK::LOCK MEMBER FUNCTION \nAcquires a lock on the mutex associated with *this. \nDeclaration\nvoid lock();\nPreconditions\nthis->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->lock(). \nThrows\nAny exceptions thrown by this->mutex()->lock(). std::system_error with an\nerror code of std::errc::operation_not_permitted if this->mutex()==NULL.\nstd::system_error with an error code of std::errc::resource_deadlock_would\n_occur if this->owns_lock()==true on entry. \nPostconditions\nthis->owns_lock()==true. \nSTD::UNIQUE_LOCK::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a lock on the mutex associated with *this. \nDeclaration\nbool try_lock();\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the Lockable\nrequirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock(). \n\n\n521\n<mutex> header\nReturns\ntrue if the call to this->mutex()->try_lock() returned true, false otherwise. \nThrows\nAny exceptions thrown by this->mutex()->try_lock(). std::system_error with\nan error code of std::errc::operation_not_permitted if this->mutex()==NULL.\nstd::system_error with an error code of std::errc::resource_deadlock_would\n_occur if this->owns_lock()==true on entry. \nPostconditions\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::UNIQUE_LOCK::UNLOCK MEMBER FUNCTION \nReleases a lock on the mutex associated with *this. \nDeclaration\nvoid unlock();\nPreconditions\nthis->mutex()!=NULL, this->owns_lock()==true. \nEffects\nCalls this->mutex()->unlock(). \nThrows\nAny exceptions thrown by this->mutex()->unlock(). std::system_error with an\nerror code of std::errc::operation_not_permitted if this->owns_lock()==false\non entry. \nPostconditions\nthis->owns_lock()==false. \nSTD::UNIQUE_LOCK::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire a lock on the mutex associated with *this within the time specified. \nDeclaration\ntemplate<typename Rep, typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the TimedLock-\nable requirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock_for(relative_time). \nReturns\ntrue if the call to this->mutex()->try_lock_for() returned true, false otherwise. \nThrows\nAny exceptions thrown by this->mutex()->try_lock_for(). std::system_error\nwith an error code of std::errc::operation_not_permitted if this->mutex()==\n\n\n522\nAPPENDIX D\nC++ Thread Library reference\nNULL. std::system_error with an error code of std::errc::resource_deadlock_\nwould_occur if this->owns_lock()==true on entry. \nPostconditions\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::UNIQUE_LOCK::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a lock on the mutex associated with *this within the time speci-\nfied. \nDeclaration\ntemplate<typename Clock, typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the Timed-Lockable\nrequirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock_until(absolute_time). \nReturns\ntrue if the call to this->mutex()->try_lock_until() returned true, false other-\nwise. \nThrows\nAny exceptions thrown by this->mutex()->try_lock_until(). std::system_error\nwith an error code of std::errc::operation_not_permitted if this-> mutex()==\nNULL. std::system_error with an error code of std::errc::resource_deadlock\n_would_occur if this->owns_lock()==true on entry. \nPostcondition\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::UNIQUE_LOCK::OPERATOR BOOL MEMBER FUNCTION \nChecks whether or not *this owns a lock on a mutex. \nDeclaration\nexplicit operator bool() const noexcept;\nReturns\nthis->owns_lock(). \nThrows\nNothing. \nNOTE\nThis is an explicit conversion operator, so it’s only implicitly called\nin contexts where the result is used as a Boolean and not where the result\nwould be treated as an integer value of 0 or 1.\n\n\n523\n<mutex> header\nSTD::UNIQUE_LOCK::OWNS_LOCK MEMBER FUNCTION \nChecks whether or not *this owns a lock on a mutex. \nDeclaration\nbool owns_lock() const noexcept;\nReturns\ntrue if *this owns a lock on a mutex, false otherwise. \nThrows\nNothing. \nSTD::UNIQUE_LOCK::MUTEX MEMBER FUNCTION \nReturns the mutex associated with *this if any. \nDeclaration\nmutex_type* mutex() const noexcept;\nReturns\nA pointer to the mutex associated with *this if any, NULL otherwise. \nThrows\nNothing. \nSTD::UNIQUE_LOCK::RELEASE MEMBER FUNCTION \nReturns the mutex associated with *this if any, and releases that association. \nDeclaration\nmutex_type* release() noexcept;\nEffects\nBreaks the association of the mutex with *this without unlocking any locks held. \nReturns\nA pointer to the mutex associated with *this prior to the call if any, NULL otherwise.\nPostconditions\nthis->mutex()==NULL, this->owns_lock()==false. \nThrows\nNothing. \nNOTE\nIf this->owns_lock() would have returned true prior to the call, the\ncaller would now be responsible for unlocking the mutex. \nD.5.10 std::shared_lock class template \nThe std::shared_lock class template provides an equivalent to std::unique_lock,\nexcept that it acquires a shared lock rather than an exclusive lock. The type of mutex\nbeing locked is specified by the template parameter Mutex, which must meet the\nSharedLockable requirements. In general, the specified mutex is locked in the con-\nstructor and unlocked in the destructor, although additional constructors and mem-\nber functions are provided to allow other possibilities. This provides a means of\nlocking a mutex for a block of code and ensuring that the mutex is unlocked when\n\n\n524\nAPPENDIX D\nC++ Thread Library reference\nthe block is left, whether that’s by running off the end, by the use of a control flow\nstatement such as break or return, or by throwing an exception. All instantiations of\nstd::shared_lock are suitable for use with the Lockable parameter for the std::\ncondition_variable_any wait functions. \n Every std::shared_lock<Mutex> meets the Lockable requirements. If, in addi-\ntion, the supplied Mutex type meets the SharedTimedLockable requirements, then\nstd::shared_lock<Mutex> also meets the TimedLockable requirements. \n Instances of std::shared_lock are MoveConstructible and MoveAssignable but\nnot CopyConstructible or CopyAssignable. \nClass definition\ntemplate <class Mutex>\nclass shared_lock\n{\npublic:\n    typedef Mutex mutex_type;\n    shared_lock() noexcept;\n    explicit shared_lock(mutex_type& m);\n    shared_lock(mutex_type& m, adopt_lock_t);\n    shared_lock(mutex_type& m, defer_lock_t) noexcept;\n    shared_lock(mutex_type& m, try_to_lock_t);\n    template<typename Clock,typename Duration>\n    shared_lock(\n        mutex_type& m,\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n    template<typename Rep,typename Period>\n    shared_lock(\n        mutex_type& m,\n        std::chrono::duration<Rep,Period> const& relative_time);\n    ~shared_lock();\n    shared_lock(shared_lock const& ) = delete;\n    shared_lock& operator=(shared_lock const& ) = delete;\n    shared_lock(shared_lock&& );\n    shared_lock& operator=(shared_lock&& );\n    void swap(shared_lock& other) noexcept;\n    void lock();\n    bool try_lock();\n    template<typename Rep, typename Period>\n    bool try_lock_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock, typename Duration>\n    bool try_lock_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n    void unlock();\n    explicit operator bool() const noexcept;\n    bool owns_lock() const noexcept;\n    Mutex* mutex() const noexcept;\n\n\n525\n<mutex> header\n    Mutex* release() noexcept;\n};\nSTD::SHARED_LOCK DEFAULT CONSTRUCTOR \nConstructs an std::shared_lock instance with no associated mutex. \nDeclaration\nshared_lock() noexcept;\nEffects\nConstructs an std::shared_lock instance that has no associated mutex. \nPostconditions\nthis->mutex()==NULL, this->owns_lock()==false. \nSTD::SHARED_LOCK LOCKING CONSTRUCTOR \nConstructs an std::shared_lock instance that acquires a shared lock on the supplied\nmutex. \nDeclaration\nexplicit shared_lock(mutex_type& m);\nEffects \nConstructs an std::shared_lock instance that references the supplied mutex. Calls\nm.lock_shared(). \nThrows\nAny exceptions thrown by m.lock_shared(). \nPostconditions\nthis->owns_lock()==true, this->mutex()==&m. \nSTD::SHARED_LOCK LOCK-ADOPTING CONSTRUCTOR \nConstructs an std::shared_lock instance that owns the lock on the supplied mutex. \nDeclaration\nshared_lock(mutex_type& m,std::adopt_lock_t);\nPreconditions\nThe calling thread must own a shared lock on m. \nEffects\nConstructs an std::shared_lock instance that references the supplied mutex and\ntakes ownership of the shared lock on m held by the calling thread. \nThrows\nNothing. \nPostconditions\nthis->owns_lock()==true, this->mutex()==&m. \nSTD::SHARED_LOCK DEFERRED-LOCK CONSTRUCTOR \nConstructs an std::shared_lock instance that doesn’t own the lock on the supplied\nmutex. \n\n\n526\nAPPENDIX D\nC++ Thread Library reference\nDeclaration\nshared_lock(mutex_type& m,std::defer_lock_t) noexcept;\nEffects\nConstructs an std::shared_lock instance that references the supplied mutex. \nThrows \nNothing. \nPostconditions\nthis->owns_lock()==false, this->mutex()==&m. \nSTD::SHARED_LOCK TRY-TO-LOCK CONSTRUCTOR \nConstructs an std::shared_lock instance associated with the supplied mutex and\ntries to acquire a shared lock on that mutex. \nDeclaration\nshared_lock(mutex_type& m,std::try_to_lock_t);\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the Lockable\nrequirements. \nEffects\nConstructs an std::shared_lock instance that references the supplied mutex.\nCalls m.try_lock_shared(). \nThrows\nNothing. \nPostconditions\nthis->owns_lock() returns the result of the m.try_lock_shared() call, this->\nmutex()==&m. \nSTD::SHARED_LOCK TRY-TO-LOCK CONSTRUCTOR WITH A DURATION TIMEOUT \nConstructs an std::shared_lock instance associated with the supplied mutex and\ntries to acquire a shared lock on that mutex. \nDeclaration\ntemplate<typename Rep,typename Period>\nshared_lock(\n    mutex_type& m,\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nLockable requirements. \nEffects\nConstructs an std::shared_lock instance that references the supplied mutex.\nCalls m.try_lock_shared_for(relative_time). \nThrows\nNothing. \n\n\n527\n<mutex> header\nPostconditions\nthis->owns_lock() returns the result of the m.try_lock_shared_for() call, this->\nmutex()==&m. \nSTD::SHARED_LOCK TRY-TO-LOCK CONSTRUCTOR WITH A TIME_POINT TIMEOUT \nConstructs an std::shared_lock instance associated with the supplied mutex and\ntries to acquire a shared lock on that mutex. \nDeclaration\ntemplate<typename Clock,typename Duration>\nshared_lock(\n    mutex_type& m,\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nLockable requirements. \nEffects\nConstructs an std::shared_lock instance that references the supplied mutex.\nCalls m.try_lock_shared_until(absolute_time). \nThrows\nNothing. \nPostconditions\nthis->owns_lock() returns the result of the m.try_lock_shared_until() call, this\n->mutex()==&m. \nSTD::SHARED_LOCK MOVE-CONSTRUCTOR \nTransfers ownership of a shared lock from one std::shared_lock object to a newly\ncreated std::shared_lock object. \nDeclaration\nshared_lock(shared_lock&& other) noexcept;\nEffects\nConstructs an std::shared_lock instance. If other owned a shared lock on a\nmutex prior to the constructor invocation, that lock is now owned by the newly cre-\nated std::shared_lock object. \nPostconditions\nFor a newly-constructed std::shared_lock object, x, x.mutex() is equal to the value\nof other.mutex() prior to the constructor invocation, and x.owns_lock() is equal to\nthe value of other.owns_lock() prior to the constructor invocation. other.mutex()\n==NULL, other.owns_lock()==false. \nThrows\nNothing. \nNOTE\nstd::shared_lock objects are not CopyConstructible, so there’s no\ncopy constructor, only this move constructor.\n\n\n528\nAPPENDIX D\nC++ Thread Library reference\nSTD::SHARED_LOCK MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of a shared lock from one std::shared_lock object to another\nstd::shared_lock object. \nDeclaration\nshared_lock& operator=(shared_lock&& other) noexcept;\nEffects\nIf this->owns_lock()returns true prior to the call, calls this->unlock(). If other\nowned a shared lock on a mutex prior to the assignment, that lock is now owned by\n*this. \nPostconditions\nthis->mutex() is equal to the value of other.mutex() prior to the assignment, and\nthis->owns_lock() is equal to the value of other.owns_lock() prior to the assign-\nment. other.mutex()==NULL, other.owns_lock()==false. \nThrows\nNothing. \nNOTE\nstd::shared_lock objects are not CopyAssignable, so there’s no copy-\nassignment operator, only this move-assignment operator. \nSTD::SHARED_LOCK DESTRUCTOR \nDestroys an std::shared_lock instance and unlocks the corresponding mutex if it’s\nowned by the destroyed instance. \nDeclaration\n~shared_lock();\nEffects\nIf this->owns_lock()returns true, calls this->mutex()->unlock_shared(). \nThrows\nNothing. \nSTD::SHARED_LOCK::SWAP MEMBER FUNCTION \nExchanges ownership of their associated shared_locks of execution between two\nstd::shared_lock objects. \nDeclaration\nvoid swap(shared_lock& other) noexcept;\nEffects\nIf other owns a lock on a mutex prior to the call, that lock is now owned by *this.\nIf *this owns a lock on a mutex prior to the call, that lock is now owned by other. \nPostconditions\nthis->mutex() is equal to the value of other.mutex() prior to the call. other\n.mutex() is equal to the value of this->mutex() prior to the call. this->owns\n_lock() is equal to the value of other.owns_lock() prior to the call. other.owns\n_lock() is equal to the value of this->owns_lock() prior to the call. \n\n\n529\n<mutex> header\nThrows\nNothing. \nSWAP NONMEMBER FUNCTION FOR STD::SHARED_LOCK\nExchanges ownership of their associated mutex locks between two std::shared_lock\nobjects. \nDeclaration\nvoid swap(shared_lock& lhs,shared_lock& rhs) noexcept;\nEffects\nlhs.swap(rhs) \nThrows\nNothing. \nSTD::SHARED_LOCK::LOCK MEMBER FUNCTION \nAcquires a shared lock on the mutex associated with *this. \nDeclaration\nvoid lock();\nPreconditions\nthis->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->lock_shared(). \nThrows\nAny exceptions thrown by this->mutex()->lock_shared(). std::system_error\nwith an error code of std::errc::operation_not_permitted if this->mutex()\n==NULL. std::system_error with an error code of std::errc::resource_deadlock\n_would_occur if this->owns_lock()==true on entry. \nPostconditions\nthis->owns_lock()==true. \nSTD::SHARED_LOCK::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a shared lock on the mutex associated with *this. \nDeclaration\nbool try_lock();\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the Lockable\nrequirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock_shared(). \nReturns\ntrue if the call to this->mutex()->try_lock_shared() returned true, false\notherwise. \n\n\n530\nAPPENDIX D\nC++ Thread Library reference\nThrows\nAny exceptions thrown by this->mutex()->try_lock_shared(). std::system_error\nwith an error code of std::errc::operation_not_permitted if this->mutex()==\nNULL. std::system_error with an error code of std::errc::resource_deadlock\n_would_occur if this->owns_lock()==true on entry. \nPostconditions\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::SHARED_LOCK::UNLOCK MEMBER FUNCTION \nReleases a shared lock on the mutex associated with *this. \nDeclaration\nvoid unlock();\nPreconditions\nthis->mutex()!=NULL, this->owns_lock()==true. \nEffects\nCalls this->mutex()->unlock_shared(). \nThrows\nAny exceptions thrown by this->mutex()->unlock_shared(). std::system_error\nwith an error code of std::errc::operation_not_permitted if this->owns_lock()\n== false on entry. \nPostconditions\nthis->owns_lock()==false. \nSTD::SHARED_LOCK::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire a shared lock on the mutex associated with *this within the time\nspecified. \nDeclaration\ntemplate<typename Rep, typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nLockable requirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock_shared_for(relative_time). \nReturns\ntrue if the call to this->mutex()->try_lock_shared_for() returned true, false\notherwise. \nThrows\nAny exceptions thrown by this->mutex()->try_lock_shared_for(). std::system\n_error with an error code of std::errc::operation_not_permitted if this->\n\n\n531\n<mutex> header\nmutex()==NULL. std::system_error with an error code of std::errc::resource\n_deadlock_would_occur if this->owns_lock()==true on entry. \nPostconditions\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::SHARED_LOCK::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a shared lock on the mutex associated with *this within the time\nspecified. \nDeclaration\ntemplate<typename Clock, typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nLockable requirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock_shared_until(absolute_time). \nReturns\ntrue if the call to this->mutex()->try_lock_shared_until() returned true, false\notherwise. \nThrows\nAny exceptions thrown by this->mutex()->try_lock_shared_until(). std::system\n_error with an error code of std::errc::operation_not_permitted if this->\nmutex()==NULL. std::system_error with an error code of std::errc::resource_\ndeadlock_would_occur if this->owns_lock()==true on entry. \nPostcondition\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::SHARED_LOCK::OPERATOR BOOL MEMBER FUNCTION \nChecks whether or not *this owns a shared lock on a mutex. \nDeclaration\nexplicit operator bool() const noexcept;\nReturns\nthis->owns_lock(). \nThrows\nNothing. \nNOTE\nThis is an explicit conversion operator, so it’s only implicitly called\nin contexts where the result is used as a Boolean and not where the result\nwould be treated as an integer value of 0 or 1.\n\n\n532\nAPPENDIX D\nC++ Thread Library reference\nSTD::SHARED_LOCK::OWNS_LOCK MEMBER FUNCTION \nChecks whether or not *this owns a shared lock on a mutex. \nDeclaration\nbool owns_lock() const noexcept;\nReturns\ntrue if *this owns a shared lock on a mutex, false otherwise. \nThrows\nNothing. \nSTD::SHARED_LOCK::MUTEX MEMBER FUNCTION \nReturns the mutex associated with *this if any. \nDeclaration\nmutex_type* mutex() const noexcept;\nReturns\nA pointer to the mutex associated with *this if any, NULL otherwise. \nThrows\nNothing. \nSTD::SHARED_LOCK::RELEASE MEMBER FUNCTION \nReturns the mutex associated with *this if any, and releases that association. \nDeclaration\nmutex_type* release() noexcept;\nEffects\nBreaks the association of the mutex with *this without unlocking any locks held. \nReturns\nA pointer to the mutex associated with *this prior to the call if any, NULL otherwise. \nPostconditions\nthis->mutex()==NULL, this->owns_lock()==false. \nThrows\nNothing. \nNOTE\nIf this->owns_lock() would have returned true prior to the call, the\ncaller would now be responsible for unlocking the mutex. \n",
      "page_number": 538
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 556-563)",
      "start_page": 556,
      "end_page": 563,
      "detection_method": "topic_boundary",
      "content": "533\n<mutex> header\nD.5.11 std::lock function template \nThe std::lock function template provides a means of locking more than one mutex\nat the same time, without risk of deadlock resulting from inconsistent lock orders. \nDeclaration\ntemplate<typename LockableType1,typename... LockableType2>\nvoid lock(LockableType1& m1,LockableType2& m2...);\nPreconditions\nThe types of the supplied lockable objects, LockableType1, LockableType2, ...,\nshall conform to the Lockable requirements. \nEffects\nAcquires a lock on each of the supplied lockable objects, m1, m2, ..., by an unspeci-\nfied sequence of calls to the lock(), try_lock(), and unlock() members of those\ntypes that avoid deadlock. \nPostconditions\nThe current thread owns a lock on each of the supplied lockable objects. \nThrows\nAny exceptions thrown by the calls to lock(), try_lock(), and unlock(). \nNOTE\nIf an exception propagates out of the call to std::lock, then\nunlock() shall have been called for any of the objects m1, m2, ... for which a\nlock has been acquired in the function by a call to lock() or try_lock(). \nD.5.12 std::try_lock function template \nThe std::try_lock function template allows you to try to lock a set of lockable\nobjects in one go, so either they are all locked or none are locked. \nDeclaration\ntemplate<typename LockableType1,typename... LockableType2>\nint try_lock(LockableType1& m1,LockableType2& m2...);\nPreconditions\nThe types of the supplied lockable objects, LockableType1, LockableType2, ...,\nshall conform to the Lockable requirements. \nEffects\nTries to acquires a lock on each of the supplied lockable objects, m1, m2, ..., by call-\ning try_lock() on each in turn. If a call to try_lock() returns false or throws an\nexception, locks already acquired are released by calling unlock() on the corre-\nsponding lockable object. \nReturns\n-1 if all locks were acquired (each call to try_lock() returned true), otherwise the\nzero-based index of the object for which the call to try_lock() returned false. \nPostconditions\nIf the function returns -1, the current thread owns a lock on each of the supplied\nlockable objects. Otherwise, any locks acquired by this call have been released. \n\n\n534\nAPPENDIX D\nC++ Thread Library reference\nThrows\nAny exceptions thrown by the calls to try_lock(). \nNOTE\nIf an exception propagates out of the call to std::try_lock, then\nunlock() shall have been called for any of the objects, m1, m2, ..., for which a\nlock has been acquired in the function by a call to try_lock(). \nD.5.13 std::once_flag class \nInstances of std::once_flag are used with std::call_once to ensure that a particular\nfunction is called exactly once, even if multiple threads invoke the call concurrently. \n Instances of std::once_flag are not CopyConstructible, CopyAssignable, Move-\nConstructible, or MoveAssignable. \nClass definition\nstruct once_flag\n{\n    constexpr once_flag() noexcept;\n    once_flag(once_flag const& ) = delete;\n    once_flag& operator=(once_flag const& ) = delete;\n};\nSTD::ONCE_FLAG DEFAULT CONSTRUCTOR \nThe std::once_flag default constructor creates a new std::once_flag instance in a\nstate, which indicates that the associated function hasn’t been called. \nDeclaration\nconstexpr once_flag() noexcept;\nEffects\nConstructs a new std::once_flag instance in a state, which indicates that the asso-\nciated function hasn’t been called. Because this is a constexpr constructor, an\ninstance with static storage duration is constructed as part of the static initialization\nphase, which avoids race conditions and order-of-initialization problems. \nD.5.14 std::call_once function template \nstd::call_once is used with an instance of std::once_flag to ensure that a particular\nfunction is called exactly once, even if multiple threads invoke the call concurrently. \nDeclaration\ntemplate<typename Callable,typename... Args>\nvoid call_once(std::once_flag& flag,Callable func,Args args...);\nPreconditions\nThe expression INVOKE(func,args) is valid for the supplied values of func and\nargs. Callable and every member of Args are MoveConstructible. \nEffects\nInvocations of std::call_once on the same std::once_flag object are serialized.\nIf there has been no prior effective std::call_once invocation on the same\n\n\n535\n<ratio> header\nstd::once_flag object, the argument func (or a copy thereof) is called as-if by\nINVOKE(func,args), and the invocation of std::call_once is effective if and only\nif the invocation of func returns without throwing an exception. If an exception is\nthrown, the exception is propagated to the caller. If there has been a prior effective\nstd::call_once on the same std::once_flag object, the invocation of std::\ncall_once returns without invoking func. \nSynchronization\nThe completion of an effective std::call_once invocation on an std::once_flag\nobject happens-before all subsequent std::call_once invocations on the same\nstd::once_flag object. \nThrows\nstd::system_error when the effects can’t be achieved or for any exception propa-\ngated from the invocation of func. \nD.6\n<ratio> header \nThe <ratio> header provides support for compile-time rational arithmetic. \nHeader contents\nnamespace std\n{\n    template<intmax_t N,intmax_t D=1>\n    class ratio;\n    // ratio arithmetic\n    template <class R1, class R2>\n    using ratio_add = see description;\n    template <class R1, class R2>\n    using ratio_subtract = see description;\n    template <class R1, class R2>\n    using ratio_multiply = see description;\n    template <class R1, class R2>\n    using ratio_divide = see description;\n    // ratio comparison\n    template <class R1, class R2>\n    struct ratio_equal;\n    template <class R1, class R2>\n    struct ratio_not_equal;\n    template <class R1, class R2>\n    struct ratio_less;\n    template <class R1, class R2>\n    struct ratio_less_equal;\n    template <class R1, class R2>\n    struct ratio_greater;\n    template <class R1, class R2>\n    struct ratio_greater_equal;\n\n\n536\nAPPENDIX D\nC++ Thread Library reference\n    typedef ratio<1, 1000000000000000000> atto;\n    typedef ratio<1, 1000000000000000> femto;\n    typedef ratio<1, 1000000000000> pico;\n    typedef ratio<1, 1000000000> nano;\n    typedef ratio<1, 1000000> micro;\n    typedef ratio<1, 1000> milli;\n    typedef ratio<1, 100> centi;\n    typedef ratio<1, 10> deci;\n    typedef ratio<10, 1> deca;\n    typedef ratio<100, 1> hecto;\n    typedef ratio<1000, 1> kilo;\n    typedef ratio<1000000, 1> mega;\n    typedef ratio<1000000000, 1> giga;\n    typedef ratio<1000000000000, 1> tera;\n    typedef ratio<1000000000000000, 1> peta;\n    typedef ratio<1000000000000000000, 1> exa;\n}\nD.6.1\nstd::ratio class template \nThe std::ratio class template provides a mechanism for compile-time arithmetic\ninvolving rational values such as one half (std::ratio<1,2>), two thirds (std::\nratio<2,3>), or fifteen forty-thirds (std::ratio<15,43>). It’s used within the C++\nStandard Library for specifying the period for instantiating the std::chrono::duration\nclass template. \nClass definition\ntemplate <intmax_t N, intmax_t D = 1>\nclass ratio\n{\npublic:\n    typedef ratio<num, den> type;\n    static constexpr intmax_t num= see below;\n    static constexpr intmax_t den= see below;\n};\nRequirements\nD may not be zero. \nDescription\nnum and den are the numerator and denominator of the fraction N/D reduced to\nlowest terms. den is always positive. If N and D are the same sign, num is positive;\notherwise num is negative.\nExamples \nratio<4,6>::num == 2\nratio<4,6>::den == 3\nratio<4,-6>::num == -2\nratio<4,-6>::den == 3\n\n\n537\n<ratio> header\nD.6.2\nstd::ratio_add template alias\nThe std::ratio_add template alias provides a mechanism for adding two std::ratio\nvalues at compile time, using rational arithmetic. \nDefinition\ntemplate <class R1, class R2>\nusing ratio_add = std::ratio<see below>;\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nEffects\nratio_add<R1,R2> is defined as an alias for an instantiation of std::ratio that\nrepresents the sum of the fractions represented by R1 and R2 if that sum can be cal-\nculated without overflow. If the calculation of the result overflows, the program is\nill-formed. In the absence of arithmetic overflow, std::ratio_add<R1,R2> shall\nhave the same num and den values as std::ratio<R1::num * R2::den + R2::num *\nR1::den, R1::den * R2::den>. \nExamples\nstd::ratio_add<std::ratio<1,3>, std::ratio<2,5> >::num == 11\nstd::ratio_add<std::ratio<1,3>, std::ratio<2,5> >::den == 15\nstd::ratio_add<std::ratio<1,3>, std::ratio<7,6> >::num == 3\nstd::ratio_add<std::ratio<1,3>, std::ratio<7,6> >::den == 2\nD.6.3\nstd::ratio_subtract template alias\nThe std::ratio_subtract template alias provides a mechanism for subtracting two\nstd::ratio values at compile time, using rational arithmetic. \nDefinition\ntemplate <class R1, class R2>\nusing ratio_subtract = std::ratio<see below>;\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nEffects\nratio_subtract<R1,R2> is defined as an alias for an instantiation of std::ratio\nthat represents the difference of the fractions represented by R1 and R2 if that dif-\nference can be calculated without overflow. If the calculation of the result over-\nflows, the program is ill-formed. In the absence of arithmetic overflow, std::ratio\n_subtract<R1,R2> shall have the same num and den values as std::ratio<R1::num\n* R2::den - R2::num * R1::den, R1::den * R2::den>. \nExamples\nstd::ratio_subtract<std::ratio<1,3>, std::ratio<1,5> >::num == 2\nstd::ratio_subtract<std::ratio<1,3>, std::ratio<1,5> >::den == 15\nstd::ratio_subtract<std::ratio<1,3>, std::ratio<7,6> >::num == -5\nstd::ratio_subtract<std::ratio<1,3>, std::ratio<7,6> >::den == 6\n\n\n538\nAPPENDIX D\nC++ Thread Library reference\nD.6.4\nstd::ratio_multiply template alias\nThe std::ratio_multiply template alias provides a mechanism for multiplying two\nstd::ratio values at compile time, using rational arithmetic. \nDefinition\ntemplate <class R1, class R2>\nusing ratio_multiply = std::ratio<see below>;\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nEffects\nratio_multiply<R1,R2> is defined as an alias for an instantiation of std::ratio\nthat represents the product of the fractions represented by R1 and R2 if that prod-\nuct can be calculated without overflow. If the calculation of the result overflows, the\nprogram is ill-formed. In the absence of arithmetic overflow, std::ratio_multiply\n<R1,R2> shall have the same num and den values as std::ratio<R1::num * R2::num,\nR1::den * R2::den>. \nExamples\nstd::ratio_multiply<std::ratio<1,3>, std::ratio<2,5> >::num == 2\nstd::ratio_multiply<std::ratio<1,3>, std::ratio<2,5> >::den == 15\nstd::ratio_multiply<std::ratio<1,3>, std::ratio<15,7> >::num == 5\nstd::ratio_multiply<std::ratio<1,3>, std::ratio<15,7> >::den == 7\nD.6.5\nstd::ratio_divide template alias\nThe std::ratio_divide template alias provides a mechanism for dividing two std::\nratio values at compile time, using rational arithmetic. \nDefinition\ntemplate <class R1, class R2>\nusing ratio_divide = std::ratio<see below>;\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nEffects\nratio_divide<R1,R2> is defined as an alias for an instantiation of std::ratio that\nrepresents the result of dividing the fractions represented by R1 and R2 if that result\ncan be calculated without overflow. If the calculation overflows, the program is ill-\nformed. In the absence of arithmetic overflow, std::ratio_divide<R1,R2> shall\nhave the same num and den values as std::ratio<R1::num * R2::den, R1::den *\nR2::num>. \nExamples\nstd::ratio_divide<std::ratio<1,3>, std::ratio<2,5> >::num == 5\nstd::ratio_divide<std::ratio<1,3>, std::ratio<2,5> >::den == 6\nstd::ratio_divide<std::ratio<1,3>, std::ratio<15,7> >::num == 7\nstd::ratio_divide<std::ratio<1,3>, std::ratio<15,7> >::den == 45\n\n\n539\n<ratio> header\nD.6.6\nstd::ratio_equal class template \nThe std::ratio_equal class template provides a mechanism for comparing two std::\nratio values for equality at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_equal:\n    public std::integral_constant<\n        bool,(R1::num == R2::num) && (R1::den == R2::den)>\n{};\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nExamples\nstd::ratio_equal<std::ratio<1,3>, std::ratio<2,6> >::value == true\nstd::ratio_equal<std::ratio<1,3>, std::ratio<1,6> >::value == false\nstd::ratio_equal<std::ratio<1,3>, std::ratio<2,3> >::value == false\nstd::ratio_equal<std::ratio<1,3>, std::ratio<1,3> >::value == true\nD.6.7\nstd::ratio_not_equal class template \nThe std::ratio_not_equal class template provides a mechanism for comparing two\nstd::ratio values for inequality at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_not_equal:\n    public std::integral_constant<bool,!ratio_equal<R1,R2>::value>\n{};\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nExamples\nstd::ratio_not_equal<std::ratio<1,3>, std::ratio<2,6> >::value == false\nstd::ratio_not_equal<std::ratio<1,3>, std::ratio<1,6> >::value == true\nstd::ratio_not_equal<std::ratio<1,3>, std::ratio<2,3> >::value == true\nstd::ratio_not_equal<std::ratio<1,3>, std::ratio<1,3> >::value == false\nD.6.8\nstd::ratio_less class template \nThe std::ratio_less class template provides a mechanism for comparing two std::\nratio values at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_less:\n    public std::integral_constant<bool,see below>\n{};\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template.\n\n\n540\nAPPENDIX D\nC++ Thread Library reference\nEffects\nstd::ratio_less<R1,R2> derives from std::integral_constant<bool, value >,\nwhere value is (R1::num * R2::den) < (R2::num * R1::den). Where possible, imple-\nmentations shall use a method of calculating the result that avoids overflow. If over-\nflow occurs, the program is ill-formed.\nExamples\nstd::ratio_less<std::ratio<1,3>, std::ratio<2,6> >::value == false\nstd::ratio_less<std::ratio<1,6>, std::ratio<1,3> >::value == true\nstd::ratio_less<\n    std::ratio<999999999,1000000000>, \n    std::ratio<1000000001,1000000000> >::value == true\nstd::ratio_less<\n    std::ratio<1000000001,1000000000>,\n    std::ratio<999999999,1000000000> >::value == false\nD.6.9\nstd::ratio_greater class template \nThe std::ratio_greater class template provides a mechanism for comparing two\nstd::ratio values at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_greater:\n    public std::integral_constant<bool,ratio_less<R2,R1>::value>\n{};\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nD.6.10 std::ratio_less_equal class template \nThe std::ratio_less_equal class template provides a mechanism for comparing two\nstd::ratio values at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_less_equal:\n    public std::integral_constant<bool,!ratio_less<R2,R1>::value>\n{};\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nD.6.11 std::ratio_greater_equal class template \nThe std::ratio_greater_equal class template provides a mechanism for comparing\ntwo std::ratio values at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_greater_equal:\n    public std::integral_constant<bool,!ratio_less<R1,R2>::value>\n{};\n",
      "page_number": 556
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 564-572)",
      "start_page": 564,
      "end_page": 572,
      "detection_method": "topic_boundary",
      "content": "541\n<thread> header\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nD.7\n<thread> header \nThe <thread> header provides facilities for managing and identifying threads and\nprovides functions for making the current thread sleep. \nHeader contents\nnamespace std\n{\n    class thread;\n    namespace this_thread\n    {\n        thread::id get_id() noexcept;\n        void yield() noexcept;\n        template<typename Rep,typename Period>\n        void sleep_for(\n            std::chrono::duration<Rep,Period> sleep_duration);\n        template<typename Clock,typename Duration>\n        void sleep_until(\n            std::chrono::time_point<Clock,Duration> wake_time);\n    }\n}\nD.7.1\nstd::thread class \nThe std::thread class is used to manage a thread of execution. It provides a means of\nstarting a new thread of execution and waiting for the completion of a thread of exe-\ncution. It also provides a means for identifying and provides other functions for man-\naging threads of execution. \nClass definition \nclass thread\n{\npublic:\n    // Types\n    class id;\n    typedef implementation-defined native_handle_type; // optional\n    // Construction and Destruction\n    thread() noexcept;\n    ~thread();\n    template<typename Callable,typename Args...>\n    explicit thread(Callable&& func,Args&&... args);\n    // Copying and Moving\n    thread(thread const& other) = delete;\n    thread(thread&& other) noexcept;\n    thread& operator=(thread const& other) = delete;\n    thread& operator=(thread&& other) noexcept;\n\n\n542\nAPPENDIX D\nC++ Thread Library reference\n    void swap(thread& other) noexcept;\n    void join();\n    void detach();\n    bool joinable() const noexcept;\n    id get_id() const noexcept;\n    native_handle_type native_handle();\n    static unsigned hardware_concurrency() noexcept;\n};\nvoid swap(thread& lhs,thread& rhs);\nSTD::THREAD::ID CLASS \nAn instance of std::thread::id identifies a particular thread of execution. \nClass definition\nclass thread::id\n{\npublic:\n    id() noexcept;\n};\nbool operator==(thread::id x, thread::id y) noexcept;\nbool operator!=(thread::id x, thread::id y) noexcept;\nbool operator<(thread::id x, thread::id y) noexcept;\nbool operator<=(thread::id x, thread::id y) noexcept;\nbool operator>(thread::id x, thread::id y) noexcept;\nbool operator>=(thread::id x, thread::id y) noexcept;\ntemplate<typename charT, typename traits>\nbasic_ostream<charT, traits>&\noperator<< (basic_ostream<charT, traits>&& out, thread::id id);\nNotes\nThe std::thread::id value that identifies a particular thread of execution shall be\ndistinct from the value of a default-constructed std::thread::id instance and\nfrom any value that represents another thread of execution. \nThe std::thread::id values for particular threads aren’t predictable and may\nvary between executions of the same program. \nstd::thread::id is CopyConstructible and CopyAssignable, so instances of\nstd::thread::id may be freely copied and assigned. \nSTD::THREAD::ID DEFAULT CONSTRUCTOR \nConstructs an std::thread::id object that doesn’t represent any thread of execution. \nDeclaration\nid() noexcept;\nEffects\nConstructs an std::thread::id instance that has the singular not any thread value. \nThrows\nNothing. \n\n\n543\n<thread> header\nNOTE\nAll default-constructed std::thread::id instances store the same value. \nSTD::THREAD::ID EQUALITY COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if they represent the same thread\nof execution. \nDeclaration\nbool operator==(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\ntrue if both lhs and rhs represent the same thread of execution or both have the\nsingular not any thread value. false if lhs and rhs represent different threads of\nexecution or one represents a thread of execution and the other has the singular\nnot any thread value. \nThrows \nNothing. \nSTD::THREAD::ID INEQUALITY COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if they represent different threads\nof execution. \nDeclaration\nbool operator!=(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\n!(lhs==rhs) \nThrows\nNothing. \nSTD::THREAD::ID LESS-THAN COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies before the other in the\ntotal ordering of thread ID values. \nDeclaration\nbool operator<(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\ntrue if the value of lhs occurs before the value of rhs in the total ordering of\nthread ID values. If lhs!=rhs, exactly one of lhs<rhs or rhs<lhs returns true and\nthe other returns false. If lhs==rhs, lhs<rhs and rhs<lhs both return false. \nThrows\nNothing. \nNOTE\nThe singular not any thread value held by a default-constructed std::\nthread::id instance compares less than any std::thread::id instance that\nrepresents a thread of execution. If two instances of std::thread::id are\nequal, neither is less than the other. Any set of distinct std::thread::id val-\nues forms a total order, which is consistent throughout an execution of a pro-\ngram. This order may vary between executions of the same program. \n\n\n544\nAPPENDIX D\nC++ Thread Library reference\nSTD::THREAD::ID LESS-THAN OR EQUAL COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies before the other in the\ntotal ordering of thread ID values or is equal to it. \nDeclaration\nbool operator<=(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\n!(rhs<lhs)\nThrows\nNothing.\nSTD::THREAD::ID GREATER-THAN COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies after the other in the\ntotal ordering of thread ID values. \nDeclaration\nbool operator>(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\nrhs<lhs \nThrows\nNothing.\nSTD::THREAD::ID GREATER-THAN OR EQUAL COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies after the other in the\ntotal ordering of thread ID values or is equal to it. \nDeclaration\nbool operator>=(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\n!(lhs<rhs)\nThrows\nNothing. \nSTD::THREAD::ID STREAM INSERTION OPERATOR \nWrites a string representation of the std::thread::id value into the specified stream. \nDeclaration\ntemplate<typename charT, typename traits>\nbasic_ostream<charT, traits>&\noperator<< (basic_ostream<charT, traits>&& out, thread::id id);\nEffects\nInserts a string representation of the std::thread::id value into the specified stream. \nReturns\nout \nThrows\nNothing. \n\n\n545\n<thread> header\nNOTE\nThe format of the string representation isn’t specified. Instances of\nstd::thread::id that compare equal have the same representation, and\ninstances that aren’t equal have distinct representations. \nSTD::THREAD::NATIVE_HANDLE_TYPE TYPEDEF \nnative_handle_type is a typedef to a type that can be used with platform-specific APIs. \nDeclaration\ntypedef implementation-defined native_handle_type;\nNOTE\nThis typedef is optional. If present, the implementation should provide\na type that’s suitable for use with native platform-specific APIs.\nSTD::THREAD::NATIVE_HANDLE MEMBER FUNCTION \nReturns a value of type native_handle_type that represents the thread of execution\nassociated with *this. \nDeclaration\nnative_handle_type native_handle();\nNOTE\nThis function is optional. If present, the value returned should be suit-\nable for use with the native platform-specific APIs.\nSTD::THREAD DEFAULT CONSTRUCTOR \nConstructs an std::thread object without an associated thread of execution. \nDeclaration\nthread() noexcept;\nEffects\nConstructs an std::thread instance that has no associated thread of execution. \nPostconditions\nFor a newly constructed std::thread object, x, x.get_id()==id(). \nThrows\nNothing. \nSTD::THREAD CONSTRUCTOR \nConstructs an std::thread object associated with a new thread of execution. \nDeclaration\ntemplate<typename Callable,typename Args...>\nexplicit thread(Callable&& func,Args&&... args);\nPreconditions \nfunc and each element of args must be MoveConstructible. \nEffects\nConstructs an std::thread instance and associates it with a newly created thread of\nexecution. Copies or moves func and each element of args into internal storage\nthat persists for the lifetime of the new thread of execution. Performs INVOKE\n(copy-of-func,copy-of-args) on the new thread of execution.\n\n\n546\nAPPENDIX D\nC++ Thread Library reference\nPostconditions\nFor a newly constructed std::thread object, x, x.get_id()!=id(). \nThrows\nAn exception of type std::system_error if unable to start the new thread. Any\nexception thrown by copying func or args into internal storage. \nSynchronization\nThe invocation of the constructor happens-before the execution of the supplied\nfunction on the newly created thread of execution. \nSTD::THREAD MOVE-CONSTRUCTOR \nTransfers ownership of a thread of execution from one std::thread object to a newly\ncreated std::thread object. \nDeclaration\nthread(thread&& other) noexcept;\nEffects\nConstructs an std::thread instance. If other has an associated thread of execution\nprior to the constructor invocation, that thread of execution is now associated with\nthe newly created std::thread object. Otherwise, the newly created std::thread\nobject has no associated thread of execution. \nPostconditions \nFor a newly constructed std::thread object, x, x.get_id() is equal to the value of\nother.get_id() prior to the constructor invocation. other.get_id()==id(). \nThrows \nNothing. \nNOTE\nstd::thread objects are not CopyConstructible, so there’s no copy\nconstructor, only this move constructor.\nSTD::THREAD DESTRUCTOR \nDestroys an std::thread object. \nDeclaration\n~thread();\nEffects\nDestroys *this. If *this has an associated thread of execution (this->joinable()\nwould return true), calls std::terminate() to abort the program. \nThrows\nNothing. \nSTD::THREAD MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of a thread of execution from one std::thread object to another\nstd::thread object. \nDeclaration\nthread& operator=(thread&& other) noexcept;\n\n\n547\n<thread> header\nEffects\nIf this->joinable()returns true prior to the call, calls std::terminate() to abort\nthe program. If other has an associated thread of execution prior to the assign-\nment, that thread of execution is now associated with *this. Otherwise *this has\nno associated thread of execution. \nPostconditions\nthis->get_id() is equal to the value of other.get_id() prior to the call. other\n.get_id()==id(). \nThrows\nNothing. \nNOTE\nstd::thread objects are not CopyAssignable, so there’s no copy-\nassignment operator, only this move-assignment operator. \nSTD::THREAD::SWAP MEMBER FUNCTION \nExchanges ownership of their associated threads of execution between two std::\nthread objects. \nDeclaration\nvoid swap(thread& other) noexcept;\nEffects \nIf other has an associated thread of execution prior to the call, that thread of exe-\ncution is now associated with *this. Otherwise *this has no associated thread of\nexecution. If *this has an associated thread of execution prior to the call, that\nthread of execution is now associated with other. Otherwise other has no associ-\nated thread of execution. \nPostconditions\nthis->get_id() is equal to the value of other.get_id() prior to the call. other\n.get_id() is equal to the value of this->get_id() prior to the call. \nThrows\nNothing. \nSWAP NONMEMBER FUNCTION FOR STD::THREADS \nExchanges ownership of their associated threads of execution between two std::\nthread objects. \nDeclaration\nvoid swap(thread& lhs,thread& rhs) noexcept;\nEffects\nlhs.swap(rhs) \nThrows\nNothing.\nSTD::THREAD::JOINABLE MEMBER FUNCTION \nQueries whether or not *this has an associated thread of execution. \n\n\n548\nAPPENDIX D\nC++ Thread Library reference\nDeclaration\nbool joinable() const noexcept;\nReturns\ntrue if *this has an associated thread of execution, false otherwise. \nThrows\nNothing. \nSTD::THREAD::JOIN MEMBER FUNCTION \nWaits for the thread of execution associated with *this to finish. \nDeclaration\nvoid join();\nPreconditions\nthis->joinable() would return true. \nEffects\nBlocks the current thread until the thread of execution associated with *this has\nfinished. \nPostconditions\nthis->get_id()==id(). The thread of execution associated with *this prior to the\ncall has finished. \nSynchronization\nThe completion of the thread of execution associated with *this prior to the call\nhappens-before the call to join() returns. \nThrows\nstd::system_error if the effects can’t be achieved or this->joinable() returns\nfalse. \nSTD::THREAD::DETACH MEMBER FUNCTION \nDetaches the thread of execution associated with *this to finish. \nDeclaration\nvoid detach();\nPreconditions\nthis->joinable()returns true.\nEffects\nDetaches the thread of execution associated with *this. \nPostconditions\nthis->get_id()==id(), this->joinable()==false\nThe thread of execution associated with *this prior to the call is detached and no\nlonger has an associated std::thread object. \nThrows\nstd::system_error if the effects can’t be achieved or this->joinable()returns\nfalse on invocation.\n\n\n549\n<thread> header\nSTD::THREAD::GET_ID MEMBER FUNCTION \nReturns a value of type std::thread::id that identifies the thread of execution asso-\nciated with *this. \nDeclaration\nthread::id get_id() const noexcept;\nReturns\nIf *this has an associated thread of execution, returns an instance of std::\nthread::id that identifies that thread. Otherwise returns a default-constructed\nstd::thread::id. \nThrows\nNothing.\nSTD::THREAD::HARDWARE_CONCURRENCY STATIC MEMBER FUNCTION \nReturns a hint as to the number of threads that can run concurrently on the current\nhardware. \nDeclaration\nunsigned hardware_concurrency() noexcept;\nReturns\nThe number of threads that can run concurrently on the current hardware. This\nmay be the number of processors in the system, for example. Where this informa-\ntion is not available or well-defined, this function returns 0. \nThrows\nNothing.\nD.7.2\nNamespace this_thread \nThe functions in the std::this_thread namespace operate on the calling thread. \nSTD::THIS_THREAD::GET_ID NONMEMBER FUNCTION \nReturns a value of type std::thread::id that identifies the current thread of execution. \nDeclaration\nthread::id get_id() noexcept;\nReturns \nAn instance of std::thread::id that identifies the current thread. \nThrows\nNothing.\nSTD::THIS_THREAD::YIELD NONMEMBER FUNCTION \nUsed to inform the library that the thread that invoked the function doesn’t need to\nrun at the point of the call. Commonly used in tight loops to avoid consuming exces-\nsive CPU time. \nDeclaration\nvoid yield() noexcept;\n",
      "page_number": 564
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 573-581)",
      "start_page": 573,
      "end_page": 581,
      "detection_method": "topic_boundary",
      "content": "550\nAPPENDIX D\nC++ Thread Library reference\nEffects\nProvides the library an opportunity to schedule something else in place of the cur-\nrent thread. \nThrows\nNothing.\nSTD::THIS_THREAD::SLEEP_FOR NONMEMBER FUNCTION \nSuspends execution of the current thread for the specified duration. \nDeclaration\ntemplate<typename Rep,typename Period>\nvoid sleep_for(std::chrono::duration<Rep,Period> const& relative_time);\nEffects\nBlocks the current thread until the specified relative_time has elapsed. \nNOTE\nThe thread may be blocked for longer than the specified duration.\nWhere possible, the elapsed time is determined by a steady clock. \nThrows\nNothing.\nSTD::THIS_THREAD::SLEEP_UNTIL NONMEMBER FUNCTION \nSuspends execution of the current thread until the specified time point has been\nreached. \nDeclaration\ntemplate<typename Clock,typename Duration>\nvoid sleep_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nEffects\nBlocks the current thread until the specified absolute_time has been reached for\nthe specified Clock. \nNOTE\nThere’s no guarantee as to how long the calling thread will be blocked\nfor, only that Clock::now() returned a time equal to or later than abso-\nlute_time at the point at which the thread became unblocked. \nThrows\nNothing.\n\n\n551\nindex\nA\nABA problem 249\nabsolute_time function 550\nabstraction penalty 12\naccumulate operation 254\nacquire-release ordering 155–159\ndata dependency with 161–164\noverview of 146\ntransitive synchronization with 159–161\nActor model 107\nadd_or_update_mapping 198\nadd_to_list( ) function 41\nalgorithms\neffects of execution policies on complexity \nof 328–329\nincremental pairwise algorithms 293–299\noverview of 8\nparallelizing standard library algorithms\n327–328\nwhere and when steps are executed 329\nallocators 478\nAmdahl’s law 277–279\nArgs 375\nArgTypes 377\narguments, passing to thread functions\n24–27\narray elements 267–269\narrive_and_drop function 120\nasynchronous tasks 82\natomic operations 128–142\nfree functions for 140–142\nmemory ordering for 146–164\nacquire-release ordering 155–164\ndata dependency with \nmemory_order_consume 161–164\nnon-sequentially consistent memory \norderings 149–150\nrelaxed ordering 150–155\nsequentially consistent ordering 147–149\non standard atomic integral types 138\non std::atomic 134–137\non std::atomic_flag 132–134\non std::atomic<T*> 137–138\noverview of 127, 144\nstd::atomic<> primary class templates\n138–140\nAtomic Ptr Plus Project 239\natomic types 128, 130, 132–142\natomic variables 155\n<atomic> headers 431–466\nATOMIC_VAR_INIT macro 434\nspecializations of std::atomic templates 450\nstd::atomic class templates 439–449\nstd::atomic specializations 450–466\nstd::atomic_flag classes 436–439\nstd::atomic_signal_fence functions 436\nstd::atomic_thread_fence functions 435\nstd::atomic_xxx typedefs 433\nstd::memory_order enumeration 435\natomic_flag_init 437\natomic_load function 227\natomics 168–169\natomic_store function 227\nATOMIC_VAR_INIT macro 434\nB\nback( ) function 77\nbackground tasks\ninterrupting on application exit 325–326\nreturning values from 82–84\n\n\nINDEX\n552\nbarrier class 294\nbarriers in Concurrency TS 118\nBidirectional Iterators 333\nblocked threads 340\nblocking calls 323–324\nblocking, unwanted 340\nBoost Thread Library 11, 382\nbounded queue 194\nbroken invariants 37, 341\nbrute-force testing 347–348\nbusy-wait loops 249–250\nC\nC++ programming language\nC++11 11\nC++14 12\nC++17 12\nefficiency in Thread Library 12–13\nhistory of multithreading in 10–11\nC++ Standard Library, parallel algorithms \nfrom 331–338\ncounting visits 336–338\nexamples of using 334–336\nC++ Standards Committee 12\nC++ Thread Library 384, 401–550\n<atomic> header 431–466\n<chrono> header 401–416\n<condition_variable> header 416–431\nefficiency in 12–13\n<future> header 466–489\n<mutex> header 489–535\n<ratio> header 535–540\n<thread> header 541–549\ncache ping-pong 262, 264\ncallable objects 478\ncarries-a-dependency-to 162\nchaining continuations 110–113\nchar const 24\ncheck_for_hierarchy_violation( ) function 58\n<chrono> headers 401–416\nstd::chrono::duration class templates 401–410\nstd::chrono::high_resolution_clock \ntypedefs 416\nstd::chrono::steady_clock classes 414–416\nstd::chrono::system_clock classes 413–414\nstd::chrono::time_point class templates 410–412\nclass template argument deduction 380–381\nclear( ) function 133–134\nclocks 93–94\nclose_queue exception 387\ncode\nreviewing to locate potential bugs 342–344\nsimplifying with synchronization of \noperations 99–123\nbarriers in Concurrency TS 118\nchaining continuations 110–113\ncontinuation-style concurrency with Concur-\nrency TS 108–110\nFP with futures 99–104\nlatches in Concurrency TS 118\nstd::experimental::barrier 120–121\nstd::experimental::flex_barrier 121–123\nstd::experimental::latch 118–119\nsynchronizing operations with message \npassing 104–108\nwaiting for first future in set with \nwhen_any 115–118\nwaiting for more than one future 114–115\nstructuring for protecting shared data 42–43\ncombine_visits function 337\nCommonDuration 409\ncompare_exchange_strong( ) function 135, 160, \n208, 220\ncompare_exchange_weak( ) function 135, 138, \n208, 211\ncompiler vendors 10\nconcerns, separating 7–8\nconcurrency 126–127\napproaches to 4–6\ncomparison of libraries 382\nconcurrent operations, synchronizing 72–123\nto simplify code 99–123\nwaiting for conditions 73–81\nwaiting for events 73–81\nwaiting for one-off events with futures\n81–93\nwaiting with time limits 93–99\ncontinuation-style 108–110\ndesigning concurrent code 251–270, \n282–299\ndesigning data structures for multithreaded \nperformance 266–270\nexception safety in parallel algorithms\n271–277\nhiding latency with multiple threads\n279–280\nimproving responsiveness with \nconcurrency 280–282\nparallel implementation of std::find\n284–289\nparallel implementation of std::for_each\n282\nparallel implementation of \nstd::partial_sum 290–299\nscalability and Amdahl’s law 277–279\ntechniques for dividing work between \nthreads 252–260\ndesigning data structures for 175–176\ndesigning for 174–176\n\n\nINDEX\n553\nconcurrency (continued)\ndisadvantages of 9–10\nenabling by separating data 185–190\nimproving responsiveness with 280–282\nin computer systems 2–4\noverview of 2–7\nparallelism vs. 6–7\nperformance of concurrent code, factors \naffecting 260–266\ncache ping-pong 262–264\ncontending data 262–264\ndata proximity 265–266\nexcessive task switching 266\nfalse sharing 264–265\nnumber of processors 261–262\noversubscription 266\nsupport in C++11 11\nsupport in C++14 12\nsupport in C++17 12\nTechnical Specification for 12\nuses for 7–10\nseparating concerns 7–8\ntask and data parallelism 8–9\nwith multiple processes 5\nwith multiple threads 6\nConcurrency TS\nbarriers in 118\ncontinuation-style concurrency with\n108–110\nlatches in 118\ncondition variables\nbuilding thread-safe queues with 76–81\nfine-grained 183–194\ninterrupting condition variable wait\n318–321\noverview of 72, 172\nthread-safe queues using 179–182\nwaiting for conditions with 74–76\nconditions, waiting for 73–81\nbuilding thread-safe queues with condition \nvariables 76–81\nwith condition variables 74–76\n<condition_variable> headers 416–431\nstd::condition_variable classes 417–424\nstd::condition_variable_any classes 424–431\nconstant initialization 367\nconstexpr functions 363–369\noverview of 361\nrequirements 367–368\ntemplates and 368–369\nuser-defined types and 365–367\nconstexpr keyword 364\nconstexpr objects 367\ncontention 262–264, 310–311\ncontext switch 3\ncontinuations, chaining 110–113\ncontinuation-style concurrency 108–110\nconveniently concurrent algorithms 8\nconverting std::chrono::duration constructors\nfrom count value 404\nfrom std::chrono::duration value 404\ncopy constructor 356\ncopyable types 61\nCopyAssignable 542\ncopy-assignment operator 138\nCopyConstructible 542, 546\ncount values 404\ncount_down function 118\ncount_down_and_wait function 119\ncounted_node_ptr 232, 245\ncounting\nreferences 226–232\nvisits 336–338\ncount_visits_per_page function 337\nCPU cycles 244, 279\ncustom_lock destructor 323\nD\ndaemon threads 23\ndata\naccess patterns in data structures 269–270\ncontending 262–264\ndividing between threads before processing \nbegins 253–254\ndividing recursively 254–260\nparallelism of 8–9\nproximity of 265–266\nseparating to enable concurrency 185–190\nsharing between threads 36–71\nproblems with 37–40\nprotecting shared data with mutexes\n40–64\ndata dependency\nwith acquire-release ordering 161–164\nwith memory_order_consume 161–164\ndata parallelism 8–9\ndata proximity 267\ndata race 39, 66, 90, 341\ndata structures\ndata access patterns in 269–270\ndesigning for concurrency 175–176\ndesigning for multithreaded performance\n266–270\nmap data structures 196–199\nrarely updated 68–70\nthread-safe 174\ndata_cond.notify_all( ) function 181\ndata_cond.notify_one( ) function 180, 190\ndata.pop( ) function 178\n\n\nINDEX\n554\ndata_ready flag 143\ndeadlocks 51–53\nguidelines for avoiding 53–59\nacquiring locks in fixed order 54–55\navoiding calling user-supplied code while \nholding locks 53–54\navoiding nested locks 53\nusing lock hierarchy 55–59\noverview of 51, 340\ndebugging 339–353\ntechniques for locating bugs 342–353\nby reviewing code 342–344\nby testing 344–346\ndesigning for testability 346–347\nmultithreaded testing techniques\n347–350\nstructuring multithreaded test code\n350–352\ntesting performance of multithreaded \ncode 352–353\ntypes of bugs 340–342\nrace conditions 341–342\nunwanted blocking 340\ndefault-constructible 33\ndelete_nodes_with_no_hazards( ) function\n221, 224\ndependency-ordered-before 162\ndetach( ) function 20, 23\ndetached threads 23\ndispatcher class 386\ndividing\ndata between threads before processing \nbegins 253–254\ndata recursively 254–260\nsequence of tasks between threads\n259–260\nwork between threads, techniques for\n252–260\nwork by task type 258–260\ndocumentation 360\ndo_delete( ) function 224\ndone flag 303\ndone( ) function 88\nDoneCheck function 117\ndone_waiting( ) function 295–296\ndo_something( ) function 45\ndo_sort( ) function 257, 309\ndouble-checked locking pattern 65\ndouble-word-compare-and-swap (DWCAS) 139\nDuration template parameter 410\nduration timeouts\nstd::shared_lock try-to-lock constructors \nwith 526\nstd::unique_lock try-to-lock constructors \nwith 517\nDuration::zero( ) function 411\nduration-based timeout 93\ndurations 94–96\nDWCAS (double-word-compare-and-swap) 139\nE\nedit_document function 24\nemplace( ) function 77\nempty( ) function 44, 49, 177\nempty_stack exception 178\nend_of_data flag 257\nenforcing ordering 142–172\nfences 166–168\nmemory ordering for atomic operations\n146–164\nordering non-atomic operations 169–172\nordering non-atomic operations with \natomics 168–169\nrelease sequences and synchronizes-with\n164–165\nErlang 5, 104\nevent driven frameworks 280\nevents\nwaiting for 73–81\nwaiting for one-off events with futures\n81–93\nassociating tasks with futures 84–87\npromises 87–89\nreturning values from background \ntasks 82–84\nsaving exceptions for future 89–90\nwaiting for multiple threads 90–93\nexception safety\nadding 272–276\nin parallel algorithms 271–277\noverview of 277\nwith std::async( ) 276–277\nexceptions 89–90\nexchange( ) function 131\nexchange-and-add operation 137\nexecution policies 328–331\nchoosing 334–336\ngeneral effects of specifying 328–329\neffects on algorithm complexity\n328–329\nexceptional behavior 329\nwhere and when algorithm steps are \nexecuted 329\noverview of 327\nstd::execution::parallel_policy 330–331\nstd::execution::parallel_unsequenced_policy\n331\nstd::execution::sequenced_policy 330\nexiting applications 325–326\n\n\nINDEX\n555\nexternal input 340\nexternal_count 230\nF\nf( ) function 21\nfacilities\nfor protecting shared data 64–71\nplatform-specific 13\nfalse sharing 264–265\nfences 166–168\nfetch_add( ) function 131, 137–138, 234\nfetch_or( ) function 131\nfetch_sub( ) function 137–138, 165\nf.get( ) function 89\nfind_element class 287\nfind_entry( ) function 70\nfind_entry_for( ) function 198\nfind_first_if( ) function 203\nfind_the_answer function 109\nfind_the_question function 109\nfine-grained condition variables, thread-safe \nqueues using 183–194\nenabling concurrency by separating \ndata 185–190\nwaiting for items to pop 190–194\nfine-grained locks\ndesigning map data structures for 196–199\nthread-safe queues using 183–194\nenabling concurrency by separating \ndata 185–190\nwaiting for items to pop 190–194\nFinite State Machine model 104\nflag.clear( ) function 169\nflag.test_and_set( ) function 169\nflexible locking 59–60\nfoo( ) function 43\nfor_each( ) function 200, 203\nforward iterators 33, 333\nFP (functional programming), with futures\n99–104\nparallel Quicksort 102–104\nQuicksort 100–102\nframeworks, event driven 280\nfree_external_counter( ) function 241\nfront( ) function 77\nfunc function 43\nfunction templates 358\nfunctions\naccepting timeouts 98–99\ndefaulted 360–363\ndeleted 358–360\nfree functions for atomic operations\n140–142\nfut.get( ) function 109\nfut.then( ) function 109\n<future> headers 466–489\nstd::async function templates 488\nstd::future class templates 467–472\nstd::packaged_task class templates 477–482\nstd::promise class templates 483–488\nstd::shared_future class templates 472–477\nfutures\nassociating tasks with 84–87\nFP with 99–104\nparallel Quicksort 102–104\nQuicksort 100–102\nwaiting for first in set with when_any 115–118\nwaiting for more than one 114–115\nwaiting for one-off events with 81–93\npromises 87–89\nreturning values from background \ntasks 82–84\nsaving exceptions for future 89–90\nwaiting for multiple threads 90–93\nG\ngeneralized captures 373\ngeneric lamdas 373\nget( ) function 274, 307\nget_bucket( ) function 198\nget_detail( ) function 64\nget_event( ) function 280\nget_future( ) function 85–86, 92\nget_hazard_pointer_for_current_thread( ) \nfunction 219, 222\nget_id( ) function 34\nget_lock( ) function 61\nget_my_class_instance( ) function 68\nget_num( ) function 145\nget_tail( ) function 188\ngo atomic variable 153\nH\nhandle( ) function 107–108, 388, 405\nhappens-before relationships 145–146\nhardware concurrency 3\nhardware threads 4\nHaskell 100\nhazard pointers\nfor detecting nodes 218–226\nreclamation strategies with 225–226\nhazard_pointer_for_current_thread( ) \nfunction 221\nhead pointer 183, 210\nhead.load( ) function 233\nHello World program 14–15\nhello( ) function 14\n\n\nINDEX\n556\nhierarchical_mutex type 55–56, 58\nhigh contention 263\nI\nI/O operations 278\nimpure functions 100\nincrease_external_count( ) function 241, 243\nincrease_head_count( ) function 243\nincremental pairwise algorithms 293–299\ninfo_to_display.get( ) function 112\ninit parameter value 32\ninitial function 14\ninitialization\nof std::atomic_flag 437\noverview of 367\nprotecting shared data during 65–68\ninput iterators 33, 332\ninterfaces, race conditions inherent in 44–50\npassing references 47\nreturning pointers to popped items 47–48\nthread-safe stacks 48–50\ninternal_count 230\ninterrupt( ) function 316–317, 319\ninterruptible_wait( ) function 318, 323\ninterrupting\nbackground tasks on application exit\n325–326\nblocking calls 323–324\ncondition variable wait 318–321\nthreads 315–316, 318–326\ndetecting interrupted threads 318\nhandling interruptions 324–325\nlaunching threads 316–318\nwait on std::condition_variable_any 321–323\ninterruption_point( ) function 316, 318, 324\ninter-thread happens-before 159\nints 63, 350\ninvariants 37\nis_lock_free( ) function 128–129\nJ\njoin( ) function 20–22, 341\njoinable( ) function 20, 22, 274\njoining_thread class 29, 121\njoin_threads class 303\nL\nlambda expression 18\nlambda functions 369–374\nlast in, first out (LIFO) 210\nlatches in Concurrency TS 118\nlatency 279–280\nlazy initialization 65\nlhs.some_detail 64\nlibraries 382\nlifetime issues 341\nLIFO (last in, first out) 210\nlist_contains( ) function 41\nlivelock 209, 340\nload( ) function 131–132\nlock( ) function 41, 70, 169\nlock-based concurrent data structures 176–194\ndesigning 173–194, 204\nfor concurrency 174–176\nwriting thread-safe lists using locks\n199–204\nwriting thread-safe lookup tables using \nlocks 194–196\nthread-safe queues using fine-grained locks \nand condition variables 183–194\nenabling concurrency by separating \ndata 185–190\nwaiting for items to pop 190–194\nthread-safe queues using locks and condition \nvariables 179–182\nthread-safe stacks using locks 176–179\nlock-free concurrent data structures 205–250\nadvantages and disadvantages of 208–209\ndefinitions and consequences 206–209\ntypes of nonblocking data structures\n206–207\nwait-free data structures 208\nexamples of 209–248\napplying memory models to lock-free \nstacks 232–236\ndetecting nodes in use with reference \ncounting 226–232\ndetecting nodes using hazard pointers\n218–226\nmanaging memory in lock-free data \nstructures 214–218\nwriting thread-safe queues without \nlocks 236–248\nwriting thread-safe stacks without \nlocks 210–213\nguidelines for writing 248–250\nABA problem 249\nhelp other threads 249–250\nidentifying busy-wait loops 249–250\nlock-free memory reclamation schemes\n248–249\nstd::memory_order_seq_cst for \nprototyping 248\nmanaging memory in 214–218\nlock-free memory reclamation schemes\n248–249\nlock-free queues 244–248\n\n\nINDEX\n557\nlock-free stacks 232–236\nlocking\nflexible 59–60\nrecursive 70–71\nlocks\nacquiring in fixed order 54–55\ncalling user-supplied code while holding\n53–54\nfine-grained\ndesigning map data structures for 196–199\nthread-safe queues using 183–194\nhierarchy of 55–59\nnested 53\nthread-safe queues using 179–182\nthread-safe stacks using 176–179\nwriting thread-safe lists using 199–204\nwriting thread-safe lookup tables using\n194–199\nwriting thread-safe queues without 236–248\nhandling multiple threads in push( )\n238–244\nmaking queues lock-free by helping other \nthreads 244–248\nwriting thread-safe stacks without 210–213\nlog(N) operations 291\nlow contention 263\nlvalue 61, 355\nM\nmain( ) function 14, 17\nmake_offseter function 371\nmalicious_function 43\nmap data structures 196–199\nmathematical functions 99\nmatrix multiplication 267\nmax_hazard_pointers atomic variables 224\nmax_hazard_pointers nodes 225\nmemcpy( ) function 139\nmemory\nin lock-free data structures 214–218\nlocations of 125–127\nmemory barriers 166\nmemory locations 125\nmemory models 125–128\napplying to lock-free stacks 232–236\nconcurrency 126–127\nmemory locations 125–127\nmodification orders 127–128\nobjects 125–127\nmemory ordering for atomic operations\n146–164\nacquire-release ordering 155–164\ndata dependency with \nmemory_order_consume 161–164\nnon-sequentially consistent memory \norderings 149–150\nrelaxed ordering 150–155\nsequentially consistent ordering 147–149\nmemory_order_acq_rel 136, 146\nmemory_order_acquire 136, 146, 157\nmemory_order_consume 146, 161–164\nmemory-ordering parameters 136\nmemory_order_relaxed 136, 146, 150, 153, 348\nmemory_order_release 146, 157\nmemory_order_seq_cst 133, 146, 149, 245, 348\nmessage passing\nframeworks for 384–401\nsynchronizing operations with 104–108\nMessage Passing Interface (MPI) 104, 253\nmessage_base class 384\nmodification orders 127–128\nmove assignment operator 26\nmove constructors 26\nmove semantics 355–358\nmoveable types 61\nMoveConstructible 545\nmove-from-std::future constructor 474\nMPI (Message Passing Interface) 104, 253\nmultithreading\nconcurrency and 10–13\nconcurrency support in C++11 11\nefficiency in C++ Thread Library 12–13\nplatform-specific facilities 13\nsupport for concurrency and parallelism in \nC++14 and C++17 12\nhistory of 10–11\nreviewing multithreaded code 343–344\nstructuring test code 350–352\ntesting performance of multithreaded \ncode 352–353\ntesting techniques 347–350\nbrute-force testing 347–348\ncombination simulation testing 348–349\ndetecting problems exposed by tests with \nspecial library 349–350\n<mutex> headers 489–535\nstd::call_once function templates 534–535\nstd::lock function templates 533\nstd::lock_guard class templates 512–513\nstd::mutex classes 490–492\nstd::once_flag classes 534\nstd::recursive_mutex classes 492–494\nstd::recursive_timed_mutex classes\n498–501\nstd::scoped_lock class templates 513–514\nstd::shared_lock class templates 523–532\nstd::shared_mutex classes 502–505\nstd::shared_timed_mutex classes\n505–512\n\n\nINDEX\n558\n<mutex> headers (continued)\nstd::timed_mutex classes 494–498\nstd::try_lock function templates 533\nstd::unique_lock class templates 514–523\nmutexes 41–42\nlocking 40\nprotecting shared data with 40–64\ndeadlock 51–59\nflexible locking with std::unique_lock\n59–60\nlocking at appropriate granularity\n62–64\nrace conditions inherent in interfaces\n44–50\nstructuring code for protecting shared \ndata 42–43\ntransferring ownership between scopes\n61–62\nunlocking 40\nmutual exclusion 174\nmy_thread function 18–19\nN\nN elements 253\nN/k operations 290\nnamespace this_thread 549–550\nnative_handle( ) function 13\nnaturally parallel algorithms 8\nnested locks 53\nnew_higher values 102\nnew_lower values 102\nnext pointer 183\nno_copies class 359\nnode pointer 190\nnodes\ndetecting with hazard pointers\n218–226\ndetecting with reference counting\n226–232\nnon-atomic operations, ordering 168–172\nnonblocking data structures 206–207\nnonmember functions\nswap for std::shared_lock 529\nswap for std::threads 547\nswap for std::unique_lock 520\nnonmodifying query operations 134\nnon-sequentially consistent memory \norderings 149–150\nnon-static data members 125\nnonwaiting functions 96\nnotify_all( ) function 76, 81\nnotify_one( ) function 75–76, 81, 349\nnow( ) function 93\nnullptr 212, 238\nO\nobjects\ncallable 478\nconstexpr 367\noverview of 125–127\nobstruction-free 207\nOpenMP 253\noperations\ndividing array elements for 267–269\nnon-atomic\nordering 169–172\nordering with atomics 168–169\non standard atomic integral types 138\non std::atomic 134–137\non std::atomic_flag 132–134\non std::atomic<T*> 137–138\nsynchronizing 142–172\nhappens-before relationships 145–146\nrelease sequences and synchronizes-with\n164–165\nsynchronizes-with relationships 143–144\nwith message passing 104–108\nordering\nacquire-release 155–159\ndata dependency with 161–164\ntransitive synchronization with 159–161\nenforcing 142–172\nfences 166–168\nmemory ordering for atomic operations\n146–164\nrelease sequences and synchronizes-with\n164–165\nnon-atomic operations 168–172\nrelaxed 150–155\nsequentially consistent 147–149\nOutput Iterators 332\noutstanding_ hazard_pointers_for( ) \nfunction 221–222\noversubscription 32, 261, 266\nownership\nof mutexes, transferring 61–62\nof threads, transferring 27–31\nP\npack expansion 375\nparallel algorithms 327–338\nexception safety in 271–277\nadding exception safety 272–276\nexception safety with std::async( )\n276–277\nexecution policies 328–331\ngeneral effects of specifying 328–329\nstd::execution::parallel_policy 330–331\n",
      "page_number": 573
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 582-589)",
      "start_page": 582,
      "end_page": 589,
      "detection_method": "topic_boundary",
      "content": "INDEX\n559\nparallel algorithms (continued)\nstd::execution::parallel_unsequenced_policy\n331\nstd::execution::sequenced_policy 330\nfrom C++ Standard Library 331–338\ncounting visits 336–338\nexamples of using 334–336\nparallelizing standard library algorithms\n327–328\nparallel implementation\nof std::find 284–289\nof std::for_each 282\nof std::partial_sum 290, 293–299\nparallel Quicksort 102–104\nparallel_ quick_sort( ) function 102\nparallel_accumulate( ) function 33, 271, 304, 306\nparallel_find function 287\nparallel_for_each function 289\nparallelism\nconcurrency vs. 6–7\nof data 8–9\nof standard library algorithms 327–328\nof tasks 8–9\nsupport in C++14 12\nsupport in C++17 12\nparallel_quick_sort function 257\nparameter packs 375–377\nparse_log_line function 337\npointers\nhazard pointers\nfor detecting nodes 218–226\nreclamation strategies with 225–226\nreturning to popped items 47–48\npop( ) function 44, 48, 50, 77, 177, 182, 214, 236, \n344–345\npop_head( ) function 188\npopped items\nreturning pointers to 47–48\nwaiting for 190–194\npop_task_from_other_thread_queue( ) \nfunction 315\nPOSIX C interface 382\npredicates\nstd::condition_variable::wait member function \noverloads taking 419\nstd::condition_variable::wait_for member \nfunction overloads taking 421\nstd::condition_variable::wait_until member \nfunction overloads taking 423\nstd::condition_variable_any::wait member \nfunction overloads taking 427\nstd::condition_variable_any::wait_for member \nfunction overloads taking 429\nstd::condition_variable_any::wait_until member \nfunction overloads taking 430\nprevious_end_value 293\nprimed pipelines 260\nprintf function 374\nprivate functions 360\nproblematic race condition 39\nprocess( ) function 63, 280\nprocess_chunk function 293\nprocess_connections( ) function 88\nprocess_data( ) function 61\nprocessing_loop( ) function 262\nprocessors, number of 261–262\nprotected data 43\nprotected functions 360\nprototyping 248\nproximity of data 265–266\nptr value 247\npublic functions 360\npure functions 100, 368\npush( ) function 44, 48, 77, 182, 210, 214, 236, \n238–244, 345\npush_front( ) function 202\nQ\nqueues\navoiding contention on 310–311\nlock-free by helping other threads\n244–248\nQuicksort\nFP-style 100–102\nparallel Quicksort 102–104\nquick_sort function 254\nR\nrace conditions 38–39, 341–342\ninherent in interfaces 44–50\npassing references 47\nrequiring no-throw copy constructors 47\nreturning pointers to popped items 47–48\nthread-safe stacks 48–50\nproblematic 39–40\nRAII (Resource Acquisition Is Initialization)\n11, 21\nRandom Access Iterators 333\n<ratio> headers 535–540\nreader-writer mutex 68\nread-modify-write operations 132\nreceive_data( ) function 67\nreceiver class 386\nreclaim_later( ) function 221, 224\nreclamation strategies with hazard pointers\n225–226\nrecursive locking 70–71\nreference counting 226–232\n\n\nINDEX\n560\nreferences\npassing 47\nRvalue references 354–358\nfunction templates and 358\nmove semantics 355–358\nrelaxed ordering 146, 150–155\nrelease operation 232\nrelease sequences 164–165\nremove_if( ) function 203\nResource Acquisition Is Initialization (RAII)\n11, 21\nresponsiveness, improving 280–282\nresults vector 33, 272, 276\nreturn statement 368\nrhs.some_detail 64\nrun( ) function 107\nrun_pending_task( ) function 308–309, 311\nruntime 31–33\nRvalue references 354–358\nfunction templates and 358\nmove semantics 355–358\nrvalues 25, 47, 61, 83\nS\nsame memory locations 126\nscalability 277–279, 353\nscoped_thread class 28\nscopes 61–62\nsend_data( ) function 67\nsender class 385\nseparate memory locations 126\nseparating\nconcerns 7–8, 259\ndata 185–190\nsequentially consistent ordering 147–149\nserialization 174, 179\nset_ condition_variable( ) function 319\nset_clear_mutex 323\nset_exception( ) function 89\nset_exception_at_thread_exit 110\nset_new_tail( ) function 247\nset_value( ) function 89\nset_value_at_thread_exit 110\nshared access 69\nshared data\nalternative facilities for protecting 64–71\nprotecting rarely updated data structures\n68–70\nrecursive locking 70–71\nprotecting during initialization 65–68\nprotecting with mutexes 40–64\ndeadlock 51–59\nflexible locking with std::unique_lock\n59–60\nlocking at appropriate granularity 62–64\nrace conditions inherent in interfaces 44–50\ntransferring mutex ownership between \nscopes 61–62\nusing mutexes in C++ 41–42\nstructuring code for protecting 42–43\nshared futures 81\nshared_timed_mutex 170\nSIMD (Single-Instruction/Multiple-Data) 294\nsimple state machine model 105\nsimulation testing 348–349\nsingle-core system 345\nSingle-Instruction/Multiple-Data (SIMD) 294\nsingle-producer, single-consumer (SPSC) 238\nsize( ) function 44\nsizeof operator 377\nsleep_for( ) function 98\nsleep_until( ) function 98\nslow operations 262\nsoftware transactional memory (STM) 40\nsome-expression times 328\nsort function 280\nspawn_task( ) function 103\nsplice( ) function 101\nsplit reference count 231\nSPSC (single-producer, single-consumer) 238\nspurious failure 135\nspurious wake 76\nsquare_root( ) function 89\nsstd::thread::hardware_concurrency( ) \nfunction 255\nstatic constexpr member variable 129\nstd::accumulate 31\nstd::adopt_lock parameter 52, 59\nstd::async function templates 488\nstd::async( ) function 82, 171, 254, 276–277\nstd::atomic 134–137\nclass templates 439–449\nconversion assignment operators 443\nconversion constructors 443\ndefault constructors 442\ntemplate specializations 450\nstd::atomic_ is_lock_free( ) function 142\nstd::atomic specializations 450\nstd::atomic::compare_exchange_strong member \nfunction 447\nstd::atomic::compare_exchange_weak member \nfunction 448\nstd::atomic::exchange member function 446\nstd::atomic::is_always_lock_free static data \nmember 443\nstd::atomic::is_lock_free member function 443\nstd::atomic::load member function 444\nstd::atomic::operator basetype conversion \noperator 444\n\n\nINDEX\n561\nstd::atomic::store member function 445\nstd::atomic<> primary class template 138–140\nstd::atomic_compare_exchange_strong \nnonmember function 447\nstd::atomic_compare_exchange_strong_explicit \nnonmember function 448\nstd::atomic_compare_exchange_weak \nnonmember function 449\nstd::atomic_compare_exchange_weak_explicit \nnonmember function 449\nstd::atomic_exchange nonmember function 446\nstd::atomic_exchange_explicit nonmember \nfunction 446\nstd::atomic_fetch_add nonmember function 455, \n464\nstd::atomic_fetch_add_explicit nonmember \nfunction 455, 464\nstd::atomic_fetch_and nonmember function 457\nstd::atomic_fetch_and_explicit nonmember \nfunction 457\nstd::atomic_fetch_or nonmember function 458\nstd::atomic_fetch_or_explicit nonmember \nfunction 458\nstd::atomic_fetch_sub nonmember function 456, \n465\nstd::atomic_fetch_sub_explicit nonmember \nfunction 456, 465\nstd::atomic_fetch_xor nonmember function 459\nstd::atomic_fetch_xor_explicit nonmember \nfunction 459\nstd::atomic_flag\nclasses 436–439\ndefault constructors 437\ninitialization with atomic_flag_init 437\noperations on 132–134\noverview of 129, 206\nstd::atomic_flag::clear member function 438\nstd::atomic_flag::test_and_set member \nfunction 437\nstd::atomic_flag_clear nonmember \nfunction 439\nstd::atomic_flag_clear_explicit nonmember \nfunction 439\nstd::atomic_flag_test_and_set nonmember \nfunction 438\nstd::atomic_flag_test_and_set_explicit \nnonmember function 438\nstd::atomic_init nonmember function 442\nstd::atomic<integral-type> specializations 466\nstd::atomic<integral-type>::fetch_add member \nfunction 454\nstd::atomic<integral-type>::fetch_and member \nfunction 456\nstd::atomic<integral-type>::fetch_or member \nfunction 457\nstd::atomic<integral-type>::fetch_sub member \nfunction 455\nstd::atomic<integral-type>::fetch_xor member \nfunction 458\nstd::atomic<integral-type>::operator--\npostdecrement operators 460\npredecrement operators 460\nstd::atomic<integral-type>::operator&= \ncompound assignment operator 460\nstd::atomic<integral-type>::operator++\npostincrement operators 459\npreincrement operators 459\nstd::atomic<integral-type>::operator+= compound \nassignment operator 460\nstd::atomic<integral-type>::operator-= compound \nassignment operator 460\nstd::atomic<integral-type>::operator= compound \nassignment operator 461\nstd::atomic<integral-type>::operator|= compound \nassignment operator 461\nstd::atomic_is_lock_free nonmember \nfunction 443\nstd::atomic_load nonmember function 444\nstd::atomic_load_explicit nonmember \nfunction 444\nstd::atomic_signal_fence function 436\nstd::atomic_store nonmember function 445\nstd::atomic_store_explicit nonmember \nfunction 445\nstd::atomic<T*> 137–138\nstd::atomic<t*> partial specialization 461–463\nstd::atomic<t*>::fetch_add member function 463\nstd::atomic<t*>::fetch_sub member function 464\nstd::atomic<t*>::operator--\npostdecrement operators 466\npredecrement operators 466\nstd::atomic<t*>::operator++\npostincrement operators 465\npreincrement operators 465\nstd::atomic<t*>::operator+= compound assign-\nment operator 466\nstd::atomic<t*>::operator-= compound assignment \noperator 466\nstd::atomic_thread_fence function 435\nstd::atomic_xxx typedefs 433\nstd::bad_alloc exception 46, 329\nstd::bind( ) function 26, 309\nstd::call_once function template 534–535\nstd::chrono::duration\nclass templates 401–410\nconverting constructors\nfrom count value 404\nfrom std::chrono::duration value 404\ndefault constructors 403\nequality comparison operators 408\n\n\nINDEX\n562\nstd::chrono::duration (continued)\ngreater-than comparison operators 409\ngreater-than-or-equals comparison \noperators 410\ninequality comparison operators 408\nless-than comparison operator 408\nless-than-or-equals comparison operator 409\nvalues 404\nstd::chrono::duration::count member \nfunction 404\nstd::chrono::duration::max static member \nfunction 408\nstd::chrono::duration::min static member \nfunction 407\nstd::chrono::duration::operator--\npost-decrement operators 406\npre-decrement operators 405\nstd::chrono::duration::operator- unary minus \noperator 405\nstd::chrono::duration::operator*= compound \nassignment operator 406\nstd::chrono::duration::operator/= compound \nassignment operators 406\nstd::chrono::duration::operator%= compound \nassignment operator 407\nstd::chrono::duration::operator+ unary plus \noperator 405\nstd::chrono::duration::operator++\npost-increment operators 405\npre-increment operators 388, 405\nstd::chrono::duration::operator+= compound \nassignment operator 406\nstd::chrono::duration::operator-= compound \nassignment operator 406\nstd::chrono::duration::period typedef 403\nstd::chrono::duration::rep typedef 403\nstd::chrono::duration::zero static member \nfunction 407\nstd::chrono::duration_cast nonmember \nfunction 410\nstd::chrono::high_resolution_clock typedef 416\nstd::chrono::steady_clock class 94, 401, 414–416\nstd::chrono::steady_clock::duration typedef 415\nstd::chrono::steady_clock::now static member \nfunction 415\nstd::chrono::steady_clock::period typedef 415\nstd::chrono::steady_clock::rep typedef 415\nstd::chrono::steady_clock::time_point \ntypedef 415\nstd::chrono::system_clock class 94, 413–414\nstd::chrono::system_clock::duration typedef 413\nstd::chrono::system_clock::from_time_t static \nmember function 414\nstd::chrono::system_clock::now static member \nfunction 414\nstd::chrono::system_clock::period typedef 413\nstd::chrono::system_clock::rep typedef 413\nstd::chrono::system_clock::time_point \ntypedef 414\nstd::chrono::system_clock::to_time_t static \nmember function 414\nstd::chrono::time_point\nclass templates 410–412\nconversion constructors 411\ndefault constructors 411\nduration constructors 411\nstd::chrono::time_point::max static member \nfunction 412\nstd::chrono::time_point::min static member \nfunction 412\nstd::chrono::time_point::operator+= compound \nassignment operator 412\nstd::chrono::time_point::operator-= compound \nassignment operator 412\nstd::chrono::time_point::time_since_epoch \nmember function 412\nstd::chrono_literals namespace 95\nstd::condition_variable\nclasses 417–424\ndefault constructors 417\ndestructors 418\noverview of 74, 93, 172, 369\nstd::condition_variable::notify_all member \nfunction 418\nstd::condition_variable::notify_one member \nfunction 418\nstd::condition_variable::wait member \nfunction 419\nstd::condition_variable::wait_for member \nfunction 420–421\nstd::condition_variable::wait_until member \nfunction 422–423\nstd::condition_variable_any\nclasses 424–431\ndefault constructors 425\ndestructors 425\ninterrupting wait on 321–323\noverview of 74, 172\nstd::condition_variable_any::notify_all member \nfunction 426\nstd::condition_variable_any::notify_one member \nfunction 426\nstd::condition_variable_any::wait member \nfunction 426–427\nstd::condition_variable_any::wait_for member \nfunction 428–429\nstd::condition_variable_any::wait_until member \nfunction 430\nstd::defer_lock argument 59\nstd::execution::par 327–328\n\n\nINDEX\n563\nstd::execution::parallel_policy 328, 330–331\nstd::execution::parallel_unsequenced_policy 328, \n331\nstd::execution::par_unseq 328, 334\nstd::execution::seq 328\nstd::execution::sequenced_policy 328–330\nstd::experimental::atomic_shared_ptr 142, 172, \n227–228\nstd::experimental::barrier 120–121, 172, 295\nstd::experimental::flex_barrier 120–123, 172\nstd::experimental::future 108, 113, 171\nstd::experimental::latch 118–119, 171\nstd::experimental::shared_future 113, 171\nstd::experimental::when_all 114\nstd::experimental::when_any 116\nstd::find 284–289\nstd::future\nclass templates 467–472\ndefault constructors 468\ndestructors 469\nmove assignment operators 469\nmove constructors 468\noverview of 170\nstd::future::get member function 471\nstd::future::share member function 469\nstd::future::valid member function 470\nstd::future::wait member function 470\nstd::future::wait_for member function 470\nstd::future::wait_until member function 471\nstd::hardware_constructive_interference_size 265\nstd::hardware_destructive_interference_size 265, \n269\nstd::hash 196\nstd::ifstream 27\nstd::is_nothrow_copy_constructible 47\nstd::is_nothrow_move_ constructible 47\nstd::kill_dependency( ) function 163\nstd::launch::async function 84, 103\nstd::launch::deferred function 103\nstd::lock function template 51, 54, 533\nstd::lock_guard class 41, 59, 380\nclass templates 512–513\ndestructors 513\nlock-adapting constructors 512\nlocking constructors 512\nstd::map<> interface 195\nstd::memory_order enumeration 132, 435\nstd::memory_order_acq_rel 435\nstd::memory_order_acquire 233, 435\nstd::memory_order_consume 163, 435\nstd::memory_order_relaxed 435\nstd::memory_order_release 435\nstd::memory_order_seq_cst 232, 248\nstd::move( ) function 26, 309, 360\nstd::mutex 170\nclasses 490–492\ndefault constructors 490\ndestructors 491\nstd::mutex::lock member function 491\nstd::mutex::try_lock member function 491\nstd::mutex::unlock member function 492\nstd::notify_all_at_thread_exit nonmember \nfunction 424\nstd::once_flag\nclasses 534\ndefault constructors 534\noverview of 66\nstd::packaged_task\nclass templates 477–482\nconstruction from callable objects 478\nconstruction from callable objects with \nallocators 478\ndefault constructors 478\ndestructors 480\nmove constructors 479\nmove-assignment operators 479\noverview of 171, 273, 285\nstd::packaged_task::get_future member \nfunction 480\nstd::packaged_task::make_ready_at_thread_exit \nmember function 482\nstd::packaged_task::operator( ) function call \noperator 481\nstd::packaged_task::reset member function 481\nstd::packaged_task::swap member function 480\nstd::packaged_task::valid member function 481\nstd::partial_sum 290–299\nstd::partition( ) function 101\nstd::promise 87–89\nallocator constructors 483\nclass templates 483–488\ndefault constructors 483\ndestructors 485\nmove constructors 484\nmove-assignment operators 484\nstd::promise::get_future member function 485\nstd::promise::set_exception member \nfunction 487\nstd::promise::set_exception_at_thread_exit \nmember function 487\nstd::promise::set_value member function 486\nstd::promise::set_value_at_thread_exit member \nfunction 486\nstd::promise::swap member function 485\nstd::ratio class template 401, 536\nstd::ratio_add template alias 537\nstd::ratio_divide template alias 538\nstd::ratio_equal class template 539\nstd::ratio_greater class template 540\nstd::ratio_greater_equal class template 540\n\n\nINDEX\n564\nstd::ratio_less class template 539\nstd::ratio_less_equal class template 540\nstd::ratio_multiply template alias 538\nstd::ratio_not_equal class template 539\nstd::ratio_subtract template alias 537\nstd::recursive_mutex\nclasses 492–494\ndefault constructors 493\ndestructors 493\noverview of 52, 170\nstd::recursive_mutex::lock member function 493\nstd::recursive_mutex::try_lock member \nfunction 493\nstd::recursive_mutex::unlock member \nfunction 494\nstd::recursive_timed_mutex\nclasses 498–501\ndefault constructors 499\ndestructors 499\noverview of 98, 170\nstd::recursive_timed_mutex::lock member \nfunction 499\nstd::recursive_timed_mutex::try_lock member \nfunction 499\nstd::recursive_timed_mutex::try_lock_for member \nfunction 500\nstd::recursive_timed_mutex::try_lock_until mem-\nber function 501\nstd::recursive_timed_mutex::unlock member \nfunction 501\nstd::reduce 31\nstd::scoped_lock\nclass templates 513–514\ndestructors 514\nlock-adopting constructors 514\nlocking constructors 513\noverview of 52, 60, 380\nstd::shared_future\nclass templates 472–477\ncopy constructors 474\ndefault constructors 473\ndestructors 474\nmove constructors 473\nmove-from-std::future constructor 474\noverview of 90, 170\nstd::shared_future::get member function 476\nstd::shared_future::valid member function 475\nstd::shared_future::wait member function 475\nstd::shared_future::wait_for member \nfunction 475\nstd::shared_future::wait_until member \nfunction 476\nstd::shared_lock\nclass templates 523–532\ndefault constructors 525\ndeferred-lock constructors 525\ndestructors 528\nlock-adopting constructors 525\nlocking constructors 525\nmove constructors 527\nmove-assignment operators 528\nswap nonmember functions for 529\ntry-to-lock constructors\nwith duration timeout 526\nwith time_point timeout 527\nstd::shared_lock::lock member function 529\nstd::shared_lock::mutex member function 532\nstd::shared_lock::operator bool member \nfunction 531\nstd::shared_lock::owns_lock member \nfunction 532\nstd::shared_lock::release member function 532\nstd::shared_lock::swap member function 528\nstd::shared_lock::try_lock member function 529\nstd::shared_lock::try_lock_for member \nfunction 530\nstd::shared_lock::try_lock_until member \nfunction 531\nstd::shared_lock::unlock member function 530\nstd::shared_mutex\nclasses 502–505\ndefault constructors 502\ndestructors 503\noverview of 68, 170, 176, 196\nstd::shared_mutex::lock member function 503\nstd::shared_mutex::lock_shared member \nfunction 504\nstd::shared_mutex::try_lock member \nfunction 503\nstd::shared_mutex::try_lock_shared member \nfunction 504\nstd::shared_mutex::unlock member function 504\nstd::shared_mutex::unlock_shared member \nfunction 505\nstd::shared_ptr 48\nstd::shared_timed_mutex\nclasses 505–512\ndefault constructors 506\ndestructors 507\noverview of 68\nstd::shared_timed_mutex::lock member \nfunction 507\nstd::shared_timed_mutex::lock_shared member \nfunction 509\nstd::shared_timed_mutex::try_lock member \nfunction 507\nstd::shared_timed_mutex::try_lock_for member \nfunction 508\nstd::shared_timed_mutex::try_lock_shared \nmember function 510\n\n\nINDEX\n565\nstd::shared_timed_mutex::try_lock_shared_for \nmember function 510\nstd::shared_timed_mutex::try_lock_until member \nfunction 508, 511\nstd::shared_timed_mutex::unlock member \nfunction 509\nstd::shared_timed_mutex::unlock_shared mem-\nber function 511\nstd::sort( ) function 101\nstd::stack container adapter 44\nstd::string object 25\nstd::terminate( ) function 22, 27, 272, 379\nstd::this_thread::get_id nonmember function 549\nstd::this_thread::sleep_for nonmember \nfunction 550\nstd::this_thread::sleep_until nonmember \nfunction 550\nstd::this_thread::yield nonmember function 549\nstd::thread\nclasses 541–549\nconstructors 545\ndefault constructors 545\ndestructors 546\nmove constructors 546\nmove-assignment operators 546\nswap nonmember functions for 547\nstd::thread::detach member function 548\nstd::thread::get_id member function 549\nstd::thread::hardware_ concurrency( ) \nfunction 31–32, 257, 261, 282, 301, 549\nstd::thread::id\nclasses 542\ndefault constructors 542\nequality comparison operators 543\ngreater-than comparison operators 544\ngreater-than or equal comparison \noperators 544\ninequality comparison operators 543\nless-than comparison operators 543\nless-than-or-equals comparison operator 544\nstream insertion operators 544\nstd::thread::join member function 548\nstd::thread::joinable member function 547\nstd::thread::native_handle member \nfunctions 545\nstd::thread::native_handle_type typedef 545\nstd::thread::swap member function 547\nstd::timed_mutex\nclasses 494–498\ndefault constructors 495\ndestructors 495\nstd::timed_mutex::lock member function 495\nstd::timed_mutex::try_lock member function 496\nstd::timed_mutex::try_lock_for member \nfunction 496\nstd::timed_mutex::try_lock_until member \nfunction 497\nstd::timed_mutex::unlock member function 497\nstd::try_lock function template 533\nstd::unique_lock\nclass templates 514–523\ndefault constructors 516\ndeferred-lock constructors 517\ndestructors 519\nflexible locking with 59–60\nlock-adopting constructors 516\nlocking constructors 516\nmove constructors 518\nmove-assignment operators 519\nswap nonmember functions for 520\ntry-to-lock constructors 517\nwith duration timeout 517\nwith time_point timeout 518\nstd::unique_lock::mutex member function 523\nstd::unique_lock::operator bool member \nfunction 522\nstd::unique_lock::owns_lock member \nfunction 523\nstd::unique_lock::release member function 523\nstd::unique_lock::swap member function 519\nstd::unique_lock::try_lock member \nfunction 520\nstd::unique_lock::try_lock_until member \nfunction 522\nstd::unique_lock::unlock member function 521\nstd::unique_ptr 26–27\nstd::vector 31\nstd::vector<int> parameter 355\nSTM (software transactional memory) 40\nstore operations 132\nstore( ) function 131, 134, 165\nstrongly-happens-before relationship 145\nstruct, division of 126\nsubmit( ) function 303–305, 307, 310\nsums, partial 293–299\nswap( ) function 48, 52, 175, 215, 234\nswapping nonmember functions\nfor std::shared_lock 529\nfor std::threads 547\nfor std::unique_lock 520\nswitching 265\nsynchronizes-with relationships 143–144, \n164–165\nsynchronizing\nconcurrent operations\nto simplify code 99–123\nwaiting for conditions 73–81\nwaiting for events 73–81\nwaiting for one-off events with futures 81–93\nwaiting with time limits 93–99\n\n\nINDEX\n566\nsynchronizing (continued)\noperations 142–172\nhappens-before relationships 145–146\nrelease sequences and synchronizes-with\n164–165\nsynchronizes-with relationships 143–144\nwith message passing 104–108\ntransitive synchronization with acquire-release \nordering 159–161\nT\ntail pointer 183\ntask parallelism 8\ntask switching 3, 265\ntasks\nassociating with futures 84–87\nbackground tasks\ninterrupting on application exit 325–326\nreturning values from 82–84\ndividing sequence of 259–260\ndividing work by types of 258–260\ndividing sequence of tasks between \nthreads 259–260\nto separate concerns 258–259\nexcessive switching 266\nparallelism of 8–9\npassing between threads 85–87\nsubmitted to thread pools 303–307\nwaiting for other tasks 307–309\nTemplateDispatcher class 388–389\ntemplates\nconstexpr functions and 368–369\nfunction templates and Rvalue references 358\nvariadic templates 374–377\ntest_and_set( ) function 129, 133–134, 207\ntesting\ndesigning for 346–347\nlocating bugs by 344–346\nmultithreaded testing techniques 347–350\nbrute-force testing 347–348\ncombination simulation testing 348–349\ndetecting problems exposed by tests with \nspecial library 349–350\nperformance of multithreaded code\n352–353\nstructuring multithreaded test code\n350–352\nthen( ) function 110\nthis_thread_hierarchy_value 58\nthis_thread_interrupt_flag 317\nthread pools 301–315\navoiding contention on work queues\n310–311\nsimple 301–303\ntasks waiting for other tasks 307–309\nwaiting for tasks submitted to 303–307\nwork stealing 311–315\nthread storage duration 379\n<thread> headers 541–549\nnamespace this_thread 549–550\nstd::thread class 541–549\nthread_cond_any pointer 323\nthread_guard class 22, 29, 275\nThreading Building Blocks, Intel 282\nthread_local flag 316\nthread_local variable 110, 222, 310, 316\nthread-local variables 379–380\nthreads 16\nblocked 340\nchoosing number of at runtime 31–33\ndesigning data structures for multithreaded \nperformance 266–270\ndata access patterns in data structures\n269–270\ndividing array elements for complex \noperations 267–269\ndividing data between before processing \nbegins 253–254\ndividing sequence of tasks between\n259–260\nhandling multiple in push( ) 238–244\nhelping 249–250\nidentifying 34–35\ninterrupting 315–316, 318–326\nbackground tasks on application exit\n325–326\nblocking calls 323–324\ncondition variable wait 318–321\ndetecting interrupted threads 318\nhandling interruptions 324–325\nwait on std::condition_variable_any\n321–323\nlaunching 17–20, 316–318\nmaking queues lock-free by helping\n244–248\nmanaging 17–24, 300–326\nmultiple\nconcurrency with 6\nhiding latency with 279–280\nwaiting from 90–93\npassing arguments to thread function 24–27\npassing tasks between 85–87\nrunning in background 22–24\nserialization of 179\nsharing data between 36–71\nalternative facilities for protecting shared \ndata 64–71\nproblems with 37–40\nprotecting shared data with mutexes 40–64\n",
      "page_number": 582
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 590-592)",
      "start_page": 590,
      "end_page": 592,
      "detection_method": "topic_boundary",
      "content": "INDEX\n567\nthreads (continued)\ntechniques for dividing work between\n252–260\ndividing data recursively 254–260\ndividing work by task type 258–260\ntransferring ownership of 27–31\nwaiting to complete 20–22\nthreads vector 272\nthread-safe data structures 174\nthread-safe lists 199–204\nthread-safe lookup tables 194–199\nthread-safe queues\nbuilding with condition variables 76–81\nusing condition variables 179–182\nusing fine-grained condition variables\n183–194\nenabling concurrency by separating \ndata 185–190\nwaiting for items to pop 190–194\nusing fine-grained locks 183–194\nenabling concurrency by separating \ndata 185–190\nwaiting for items to pop 190–194\nusing locks 179–182\nwriting without locks 236–248\nhandling multiple threads in push( )\n238–244\nmaking queues lock-free by helping other \nthreads 244–248\nthread-safe stacks\nexample definitions of 48–50\nusing locks 176–179\nwriting without locks 210–213\nthreads_in_pop variable 215, 218\nthread-specific setup code 350\ntime limits, waiting with 93–99\nclocks 93–94\ndurations 94–96\nfunctions accepting timeouts 98–99\ntime points 96–97\ntime points 96–97\ntimeouts 98–99\ntime_point timeouts\nstd::shared_lock try-to-lock constructors \nwith 527\nstd::unique_lock try-to-lock constructors \nwith 518\ntime_since_ epoch( ) function 96\nto_be_deleted pointer 218\ntop( ) function 44, 50, 344\ntransform_reduce function 336\ntransitive synchronization 159–161\ntrivial copy-assignment operator 138\ntry/catch blocks 21\ntry_lock( ) function 57\ntry_lock_for( ) function 98\ntry_lock_until( ) function 98\ntry_pop( ) function 78, 183, 188, 236, \n313\ntry_reclaim( ) function 215, 217\ntry_steal( ) function 313\ntry-to-lock constructors, std::shared_lock\nwith duration timeouts 517, 526\nwith time_point timeouts 518, 527\nt.time_since_epoch( ) function 411\ntypedefs 94, 130\nU\nunbounded queue 194\nundefined behavior 39, 70\nuniform rate 94\nunique futures 81\nunlock( ) function 41, 70, 169\nunlocking mutexes 40\nunsequenced policy 331\nupdate_data_for_widget 25\nupdate_or_add_entry( ) function 70\nuser-defined types 365–367\nuser-interface state machine 397\nuser-supplied code 53–54\nV\nvalues\nreturning from background tasks\n82–84\nstoring new depending on current\n135–136\nvariables\nautomatically deducing type of\n377–378\nlocal 371–374\nthread-local 379–380\nvariadic templates 52, 374–377\nvectors 46\nvisits, counting 336–338\nvoid( ) function 84\nW\nwait( ) function 75, 81, 108, 295–296, \n387\nwait_and_dispatch( ) function 387, 389\nwait_and_pop( ) function 180, 183\nwait_for( ) function 93, 97, 109\nwait_for_data( ) function 193\nwait_for_pop( ) function 78\nwait-free 207, 213\nwait-free data structures 208\n\n\nINDEX\n568\nwaiting\nfor conditions 73–81\nbuilding thread-safe queues with condition \nvariables 76–81\nwith condition variables 74–76\nfor events 73–81\nfor first future in set with when_any\n115–118\nfor items to pop 190–194\nfor more than one future 114–115\nfor multiple threads 90–93\nfor one-off events with futures 81–93\nassociating tasks with futures 84–87\npromises 87–89\nreturning values from background tasks\n82–84\nsaving exceptions for future 89–90\nfor tasks 307–309\nfor tasks submitted to thread pools 303–307\ninterrupting condition variable wait 318–321\ninterrupting wait on \nstd::condition_variable_any 321–323\nwith time limits 93–99\nclocks 93–94\ndurations 94–96\nfunctions accepting timeouts 98–99\ntime points 96–97\nwait_until( ) function 93, 109\nwhen_all function 115\nwhen_any function 115–118\nwhile loop 158, 160, 212–213\nwork stealing 311–315\nworker_thread( ) function 301, 303, 308, 311\nwrapped_message pointer 384\nX\nx.is_lock_free( ) function 128\n\n\nAnthony Williams\nY\nou choose C++ when your applications need to run fast. \nWell-designed concurrency makes them go even faster. \nC++17 delivers strong support for the multithreaded, \nmultiprocessor programming required for fast graphic process-\ning, machine learning, and other performance-sensitive tasks. \nThis exceptional book unpacks the features, patterns, and best \npractices of production-grade C++ concurrency.\nC++ Concurrency in Action, Second Edition is the deﬁ nitive guide \nto writing elegant multithreaded applications in C++. Updated \nfor C++17, it carefully addresses every aspect of concurrent \ndevelopment, from starting new threads to designing fully \nfunctional multithreaded algorithms and data structures. \nConcurrency master Anthony Williams presents examples and \npractical tasks in every chapter, including insights that will \ndelight even the most experienced developer.  \nWhat’s Inside\n● Full coverage of new C++17 features\n● Starting and managing threads\n● Synchronizing concurrent operations\n● Designing concurrent code\n● Debugging multithreaded applications\nWritten for intermediate C and C++ developers. No prior \nexperience with concurrency required.\nAnthony Williams has been an active member of the BSI C++  \nPanel since 2001 and is the developer of the just::thread Pro \nextensions to the C++11 thread library.\nTo download their free eBook in PDF, ePub, and Kindle formats, \nowners of this book should visit \nmanning.com/books/c-plus-plus-concurrency-in-action-second-edition\n$69.99 / Can $92.99  [INCLUDING eBOOK]\nC++ Concurrency IN ACTION\nPROGRAMMING LANGUAGES\nM A N N I N G\n“\nThis book should be on \nevery C++ programmer’s \ndesk. It’s clear, concise, \nand valuable.”\n \n—Rob Green\nBowling Green State University\n“\nA thorough presentation \nof C++ concurrency \ncapabilities.”\n—Maurizio Tomasi\nUniversity of Milan \n“\nHighly recommended for \nprogrammers who want to \nfurther their knowledge of \nthe latest C++ standard.”\n \n—Frédéric Flayol, 4Pro Web C++\n“\nThe guide contains \nsnippets for everyday use in \nyour own projects and to \nhelp take your concurrency \nC++ skills from the Padawan \nto the Jedi level.”\n \n—Jura Shikin, IVI Technologies\nSee first page\n",
      "page_number": 590
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "M A N N I N G\nSECOND EDITION\nAnthony Williams\nIN ACTION\n",
      "content_length": 56,
      "extraction_method": "Direct"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "Praise for the first edition\n“It’s not just the best current treatment of C++11’s threading facilities ... it’s likely to\nremain the best for some time to come.”\n—Scott Meyers, author of Effective C++ and More Effective C++\n“Simplifies the dark art of C++ multithreading.”\n—Rick Wagner, Red Hat\n“Reading this made my brain hurt. But it’s a good hurt.”\n—Joshua Heyer, Ingersoll Rand\n“Anthony shows how to put concurrency into practice.”\n—Roger Orr, OR/2 Limited\n“A thoughtful, in-depth guide to the new concurrency standard for C++ straight from\nthe mouth of one the horses.”\n—Neil Horlock, Director, Credit Suisse\n“Any serious C++ developers should understand the contents of this important book.”\n—Dr. Jamie Allsop, Development Director\n",
      "content_length": 738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "C++ Concurrency\nin Action\nSecond Edition\nANTHONY WILLIAMS\nM A N N I N G\nSHELTER ISLAND\n",
      "content_length": 87,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "For online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2019 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nManning Publications Co.\nDevelopment editors: Cynthia Kane, Jennifer Stout\n20 Baldwin Road\nTechnical development editor: Alain Couniot\nPO Box 761\nReview editor: Aleksandar Dragosavljevic´\nShelter Island, NY 11964\nProduction editor: Janet Vail\nCopy editors: Safis Editing, Heidi Ward\nProofreader: Melody Dolab\nTechnical proofreader: Frédéric Flayol\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781617294693 \nPrinted in the United States of America\n1 2 3 4 5 6 7 8 9 10 – SP – 24 23 22 21 20 19\n",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": " To Kim, Hugh, and Erin\n",
      "content_length": 24,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "vivi\nbrief contents\n1\n■\nHello, world of concurrency in C++!\n1\n2\n■\nManaging threads\n16\n3\n■\nSharing data between threads\n36\n4\n■\nSynchronizing concurrent operations\n72\n5\n■\nThe C++ memory model and operations on \natomic types\n124\n6\n■\nDesigning lock-based concurrent data structures\n173\n7\n■\nDesigning lock-free concurrent data structures\n205\n8\n■\nDesigning concurrent code\n251\n9\n■\nAdvanced thread management\n300\n10\n■\nParallel algorithms\n327\n11\n■\nTesting and debugging multithreaded applications\n339\n",
      "content_length": 493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "vii\ncontents\npreface\nxiii\nacknowledgments\nxv\nabout this book\nxvii\nabout the author\nxx\nabout the cover illustration\nxxi\n1 \nHello, world of concurrency in C++!\n1\n1.1\nWhat is concurrency?\n2\nConcurrency in computer systems\n2\n■Approaches to \nconcurrency\n4\n■Concurrency vs. parallelism\n6\n1.2\nWhy use concurrency?\n7\nUsing concurrency for separation of concerns\n7\n■Using \nconcurrency for performance: task and data parallelism\n8\nWhen not to use concurrency\n9\n1.3\nConcurrency and multithreading in C++\n10\nHistory of multithreading in C++\n10\n■Concurrency support in the \nC++11 standard\n11\n■More support for concurrency and \nparallelism in C++14 and C++17\n12\n■Efficiency in the C++ \nThread Library\n12\n■Platform-specific facilities\n13\n1.4\nGetting started\n13\nHello, Concurrent World\n14\n",
      "content_length": 773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "CONTENTS\nviii\n2 \nManaging threads\n16\n2.1\nBasic thread management\n17\nLaunching a thread\n17\n■Waiting for a thread to complete\n20\nWaiting in exceptional circumstances\n20\n■Running threads in \nthe background\n22\n2.2\nPassing arguments to a thread function\n24\n2.3\nTransferring ownership of a thread\n27\n2.4\nChoosing the number of threads at runtime\n31\n2.5\nIdentifying threads\n34\n3 \nSharing data between threads\n36\n3.1\nProblems with sharing data between threads\n37\nRace conditions\n38\n■Avoiding problematic race conditions\n39\n3.2\nProtecting shared data with mutexes\n40\nUsing mutexes in C++\n41\n■Structuring code for protecting shared \ndata\n42\n■Spotting race conditions inherent in interfaces\n44\nDeadlock: the problem and a solution\n51\n■Further guidelines \nfor avoiding deadlock\n53\n■Flexible locking with \nstd::unique_lock\n59\n■Transferring mutex ownership between \nscopes\n61\n■Locking at an appropriate granularity\n62\n3.3\nAlternative facilities for protecting shared data\n64\nProtecting shared data during initialization\n65\n■Protecting rarely \nupdated data structures\n68\n■Recursive locking\n70\n4 \nSynchronizing concurrent operations\n72\n4.1\nWaiting for an event or other condition\n73\nWaiting for a condition with condition variables\n74\nBuilding a thread-safe queue with condition variables\n76\n4.2\nWaiting for one-off events with futures\n81\nReturning values from background tasks\n82\n■Associating a task \nwith a future\n84\n■Making (std::)promises\n87\n■Saving an \nexception for the future\n89\n■Waiting from multiple threads\n90\n4.3\nWaiting with a time limit\n93\nClocks\n93\n■Durations\n94\n■Time points\n96\n■Functions \nthat accept timeouts\n98\n",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "CONTENTS\nix\n4.4\nUsing synchronization of operations to simplify code\n99\nFunctional programming with futures\n99\n■Synchronizing \noperations with message passing\n104\n■Continuation-style \nconcurrency with the Concurrency TS\n108\n■Chaining \ncontinuations\n110\n■Waiting for more than one future\n114\nWaiting for the first future in a set with when_any\n115\nLatches and barriers in the Concurrency TS\n118\n■A basic latch \ntype: std::experimental::latch\n118\n■std::experimental::barrier: \na basic barrier\n120\n■std::experimental::flex_barrier—\nstd::experimental::barrier’s flexible friend\n121\n5 \nThe C++ memory model and operations on atomic types\n124\n5.1\nMemory model basics\n125\nObjects and memory locations\n125\n■Objects, memory locations, \nand concurrency\n126\n■Modification orders\n127\n5.2\nAtomic operations and types in C++\n128\nThe standard atomic types\n128\n■Operations on \nstd::atomic_flag\n132\n■Operations on std::atomic<bool>\n134\nOperations on std::atomic<T*>: pointer arithmetic\n137\nOperations on standard atomic integral types\n138\nThe std::atomic<> primary class template\n138\nFree functions for atomic operations\n140\n5.3\nSynchronizing operations and enforcing ordering\n142\nThe synchronizes-with relationship\n143\n■The happens-before \nrelationship\n145\n■Memory ordering for atomic operations\n146\nRelease sequences and synchronizes-with\n164\n■Fences\n166\nOrdering non-atomic operations with atomics\n168\n■Ordering \nnon-atomic operations\n169\n6 \nDesigning lock-based concurrent data structures\n173\n6.1\nWhat does it mean to design for concurrency?\n174\nGuidelines for designing data structures for concurrency\n175\n6.2\nLock-based concurrent data structures\n176\nA thread-safe stack using locks\n176\n■A thread-safe queue using \nlocks and condition variables\n179\n■A thread-safe queue using \nfine-grained locks and condition variables\n183\n6.3\nDesigning more complex lock-based data structures\n194\nWriting a thread-safe lookup table using locks\n194\n■Writing a \nthread-safe list using locks\n199\n",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "CONTENTS\nx\n7 \nDesigning lock-free concurrent data structures\n205\n7.1\nDefinitions and consequences\n206\nTypes of nonblocking data structures\n206\n■Lock-free data \nstructures\n207\n■Wait-free data structures\n208\n■The pros and \ncons of lock-free data structures\n208\n7.2\nExamples of lock-free data structures\n209\nWriting a thread-safe stack without locks\n210\n■Stopping those \npesky leaks: managing memory in lock-free data structures\n214\nDetecting nodes that can’t be reclaimed using hazard pointers\n218\nDetecting nodes in use with reference counting\n226\n■Applying the \nmemory model to the lock-free stack\n232\n■Writing a thread-safe \nqueue without locks\n236\n7.3\nGuidelines for writing lock-free data structures\n248\nGuideline: use std::memory_order_seq_cst for prototyping\n248\nGuideline: use a lock-free memory reclamation scheme\n248\nGuideline: watch out for the ABA problem\n249\n■Guideline: \nidentify busy-wait loops and help the other thread\n249\n8 \nDesigning concurrent code\n251\n8.1\nTechniques for dividing work between threads\n252\nDividing data between threads before processing begins\n253\nDividing data recursively\n254\n■Dividing work by task type\n258\n8.2\nFactors affecting the performance of concurrent \ncode\n260\nHow many processors?\n261\n■Data contention and cache \nping-pong\n262\n■False sharing\n264\n■How close is \nyour data?\n265\n■Oversubscription and excessive task \nswitching\n266\n8.3\nDesigning data structures for multithreaded \nperformance\n266\nDividing array elements for complex operations\n267\n■Data access \npatterns in other data structures\n269\n8.4\nAdditional considerations when designing for \nconcurrency\n270\nException safety in parallel algorithms\n271\n■Scalability and \nAmdahl’s law\n277\n■Hiding latency with multiple threads\n279\nImproving responsiveness with concurrency\n280\n",
      "content_length": 1776,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "CONTENTS\nxi\n8.5\nDesigning concurrent code in practice\n282\nA parallel implementation of std::for_each\n282\n■A parallel \nimplementation of std::find\n284\n■A parallel implementation of \nstd::partial_sum\n290\n9 \nAdvanced thread management\n300\n9.1\nThread pools\n301\nThe simplest possible thread pool\n301\n■Waiting for tasks \nsubmitted to a thread pool\n303\n■Tasks that wait for other \ntasks\n307\n■Avoiding contention on the work queue\n310\nWork stealing\n311\n9.2\nInterrupting threads\n315\nLaunching and interrupting another thread\n316\n■Detecting \nthat a thread has been interrupted\n318\n■Interrupting a \ncondition variable wait\n318\n■Interrupting a wait on \nstd::condition_variable_any\n321\n■Interrupting other \nblocking calls\n323\n■Handling interruptions\n324\nInterrupting background tasks on application exit\n325\n10 \nParallel algorithms\n327\n10.1\nParallelizing the standard library algorithms\n327\n10.2\nExecution policies\n328\nGeneral effects of specifying an execution policy\n328\nstd::execution::sequenced_policy\n330\nstd::execution::parallel_policy\n330\nstd::execution::parallel_unsequenced_policy\n331\n10.3\nThe parallel algorithms from the C++ Standard \nLibrary\n331\nExamples of using parallel algorithms\n334\nCounting visits\n336\n11 \nTesting and debugging multithreaded applications\n339\n11.1\nTypes of concurrency-related bugs\n340\nUnwanted blocking\n340\n■Race conditions\n341\n11.2\nTechniques for locating concurrency-related bugs\n342\nReviewing code to locate potential bugs\n342\n■Locating \nconcurrency-related bugs by testing\n344\n■Designing for \ntestability\n346\n■Multithreaded testing techniques\n347\nStructuring multithreaded test code\n350\n■Testing the performance \nof multithreaded code\n352\n",
      "content_length": 1665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "CONTENTS\nxii\nappendix A\nBrief reference for some C++11 language features\n354\nappendix B\nBrief comparison of concurrency libraries\n382\nappendix C\nA message-passing framework and complete ATM example\n384\nappendix D\nC++ Thread Library reference\n401\nindex\n551\n",
      "content_length": 256,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "xiii\npreface\nI encountered the concept of multithreaded code while working at my first job after I\nleft college. We were writing a data processing application that had to populate a data-\nbase with incoming data records. There was a lot of data, but each record was inde-\npendent and required a reasonable amount of processing before it could be inserted\ninto the database. To take full advantage of the power of our 10-CPU UltraSPARC, we\nran the code in multiple threads, each thread processing its own set of incoming\nrecords. We wrote the code in C++, using POSIX threads, and made a fair number of\nmistakes—multithreading was new to all of us—but we got there in the end. It was also\nwhile working on this project that I first became aware of the C++ Standards Commit-\ntee and the freshly published C++ Standard.\n I have had a keen interest in multithreading and concurrency ever since. Where\nothers saw it as difficult, complex, and a source of problems, I saw it as a powerful tool\nthat could enable your code to take advantage of the available hardware to run faster.\nLater on, I would learn how it could be used to improve the responsiveness and per-\nformance of applications even on single-core hardware, by using multiple threads to\nhide the latency of time-consuming operations such as I/O. I also learned how it\nworked at the OS level and how Intel CPUs handled task switching.\n Meanwhile, my interest in C++ brought me in contact with the ACCU and then the\nC++ Standards panel at BSI, as well as Boost. I followed the initial development of the\nBoost Thread Library with interest, and when it was abandoned by the original devel-\noper, I jumped at the chance to get involved. I was the primary developer and main-\ntainer of the Boost Thread Library for a number of years, though I have since handed\nthat responsibility on.\n",
      "content_length": 1836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "PREFACE\nxiv\n As the work of the C++ Standards Committee shifted from fixing defects in the\nexisting standard to writing proposals for the C++11 standard (named C++0x in the\nhope that it would be finished by 2009, and then officially C++11, because it was finally\npublished in 2011), I got more involved with BSI and started drafting proposals of my\nown. Once it became clear that multithreading was on the agenda, I jumped in with\nboth feet and authored or co-authored many of the multithreading and concurrency-\nrelated proposals that shaped this part of the standard. I have continued to be\ninvolved with the concurrency group as we worked on the changes for C++17, the\nConcurrency TS, and proposals for the future. I feel privileged to have had the oppor-\ntunity to combine two of my major computer-related interests—C++ and multithread-\ning—in this way.\n This book draws on all my experience with both C++ and multithreading and aims\nto teach other C++ developers how to use the C++17 Thread Library and Concurrency\nTS safely and efficiently. I also hope to impart some of my enthusiasm for the subject\nalong the way.\n",
      "content_length": 1122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "xv\nacknowledgments\nI will start by saying a big “Thank you” to my wife, Kim, for all the love and support she\nhas given me while writing this book. The first edition occupied a significant part of\nmy spare time for the four years before publication, and the second edition has again\nrequired a significant investment of time, and without her patience, support, and\nunderstanding, I couldn’t have managed it.\n Second, I would like to thank the team at Manning who have made this book possi-\nble: Marjan Bace, publisher; Michael Stephens, associate publisher; Cynthia Kane, my\ndevelopment editor; Aleksandar Dragosavljevic´, review editor; Safis Editing and Heidi\nWard, my copyeditors; and Melody Dolab, my proofreader. Without their efforts you\nwould not be reading this book right now.\n I would also like to thank the other members of the C++ Standards Committee\nwho wrote committee papers on the multithreading facilities: Andrei Alexandrescu,\nPete Becker, Bob Blainer, Hans Boehm, Beman Dawes, Lawrence Crowl, Peter Dimov,\nJeff Garland, Kevlin Henney, Howard Hinnant, Ben Hutchings, Jan Kristofferson,\nDoug Lea, Paul McKenney, Nick McLaren, Clark Nelson, Bill Pugh, Raul Silvera, Herb\nSutter, Detlef Vollmann, and Michael Wong, plus all those who commented on the\npapers, discussed them at the committee meetings, and otherwise helped shaped the\nmultithreading and concurrency support in C++11, C++14, C++17, and the Concur-\nrency TS.\n Finally, I would like to thank the following people, whose suggestions have greatly\nimproved this book: Dr. Jamie Allsop, Peter Dimov, Howard Hinnant, Rick Molloy,\nJonathan Wakely, and Dr. Russel Winder, with special thanks to Russel for his detailed\n",
      "content_length": 1689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "ACKNOWLEDGMENTS\nxvi\nreviews and to Frédéric Flayol, who, as technical proofreader, painstakingly checked\nall the content for outright errors in the final manuscript during production. (Any\nremaining mistakes are, of course, all mine.) In addition, I’d like to thank my panel\nof reviewers for the second edition: Al Norman, Andrei de Araújo Formiga, Chad\nBrewbaker, Dwight Wilkins, Hugo Filipe Lopes, Vieira Durana, Jura Shikin, Kent R.\nSpillner, Maria Gemini, Mateusz Malenta, Maurizio Tomasi, Nat Luengnaruemitchai,\nRobert C. Green II, Robert Trausmuth, Sanchir Kartiev, and Steven Parr. Also, thanks\nto the readers of the MEAP edition who took the time to point out errors or highlight\nareas that needed clarifying.\n",
      "content_length": 718,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "xvii\nabout this book\nThis book is an in-depth guide to the concurrency and multithreading facilities from\nthe new C++ Standard, from the basic usage of std::thread, std::mutex, and std::\nasync, to the complexities of atomic operations and the memory model.\nRoadmap\nThe first four chapters introduce the various library facilities provided by the library\nand show how they can be used.\n Chapter 5 covers the low-level nitty-gritty of the memory model and atomic opera-\ntions, including how atomic operations can be used to impose ordering constraints on\nother code, and marks the end of the introductory chapters.\n Chapters 6 and 7 start the coverage of higher-level topics, with some examples of\nhow to use the basic facilities to build more complex data structures—lock-based data\nstructures in chapter 6, and lock-free data structures in chapter 7.\n Chapter 8 continues the higher-level topics, with guidelines for designing multi-\nthreaded code, coverage of the issues that affect performance, and example imple-\nmentations of various parallel algorithms.\n Chapter 9 covers thread management—thread pools, work queues, and interrupt-\ning operations.\n Chapter 10 covers the new parallelism support from C++17, which comes in the\nform of additional overloads for many of the Standard Library algorithms.\n Chapter 11 covers testing and debugging—types of bugs, techniques for locating\nthem, how to test for them, and so forth.\n",
      "content_length": 1427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "ABOUT THIS BOOK\nxviii\n The appendixes include a brief description of some of the new language facilities\nintroduced with the new standard that are relevant to multithreading, the implemen-\ntation details of the message-passing library mentioned in chapter 4, and a complete\nreference to the C++17 Thread Library.\nWho should read this book\nIf you're writing multithreaded code in C++, you should read this book. If you're using\nthe new multithreading facilities from the C++ Standard Library, this book is an essen-\ntial guide. If you’re using alternative thread libraries, the guidelines and techniques\nfrom the later chapters should still prove useful.\n A good working knowledge of C++ is assumed, though familiarity with the new lan-\nguage features is not—these are covered in appendix A. Prior knowledge or experi-\nence of multithreaded programming is not assumed, though it may be useful.\nHow to use this book\nIf you’ve never written multithreaded code before, I suggest reading this book sequen-\ntially from beginning to end, though possibly skipping the more detailed parts of\nchapter 5. Chapter 7 relies heavily on the material in chapter 5, so if you skipped\nchapter 5, you should save chapter 7 until you’ve read it.\n If you haven’t used the new C++11 language facilities before, it might be worth\nskimming appendix A before you start to ensure that you’re up to speed with the\nexamples in the book. The uses of the new language facilities are highlighted in the\ntext, though, and you can always flip to the appendix if you encounter something you\nhaven’t seen before.\n If you have extensive experience with writing multithreaded code in other environ-\nments, the beginning chapters are probably still worth skimming so you can see how\nthe facilities you know map onto the new standard C++ ones. If you’re going to be\ndoing any low-level work with atomic variables, chapter 5 is a must. Chapter 8 is worth\nreviewing to ensure that you’re familiar with things like exception safety in multi-\nthreaded C++. If you have a particular task in mind, the index and table of contents\nshould help you find a relevant section quickly.\n Once you’re up to speed on the use of the C++ Thread Library, appendix D should\ncontinue to be useful, such as for looking up the exact details of each class and func-\ntion call. You may also like to dip back into the main chapters from time to time to\nrefresh your memory on a particular construct or to look at the sample code.\nCode conventions and downloads\nAll source code in listings or in text is in a fixed-width font like this to separate it\nfrom ordinary text. Code annotations accompany many of the listings, highlighting\nimportant concepts. In some cases, numbered bullets link to explanations that follow\nthe listing.\n",
      "content_length": 2765,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "ABOUT THIS BOOK\nxix\n Source code for all working examples in this book is available for download from\nthe publisher’s website at www.manning.com/books/c-plus-plus-concurrency-in-action-\nsecond-edition. You may also download the source code from github at https://github\n.com/anthonywilliams/ccia_code_samples.\nSoftware requirements\nTo use the code from this book unchanged, you’ll need a recent C++ compiler that\nsupports the C++17 language features used in the examples (see appendix A), and\nyou’ll need a copy of the C++ Standard Thread Library.\n At the time of writing, the latest versions of g++, clang++, and Microsoft Visual Stu-\ndio all ship with implementations of the C++17 Standard Thread Library. They also\nsupport most of the language features from the appendix, and those features that\naren't supported are coming soon. \n My company, Just Software Solutions Ltd, sells a complete implementation of the\nC++11 Standard Thread Library for several older compilers, along with an implemen-\ntation of the Concurrency TS for newer versions of clang, gcc, and Microsoft Visual\nStudio.1 This implementation has been used for testing the examples in this book.\n The Boost Thread Library2 provides an API that’s based on the C++11 Standard\nThread Library proposals and is portable to many platforms. Most of the examples\nfrom the book can be modified to work with the Boost Thread Library by judicious\nreplacement of std:: with boost:: and use of the appropriate #include directives.\nThere are a few facilities that are either not supported (such as std::async) or have\ndifferent names (such as boost::unique_future) in the Boost Thread Library.\nBook forum\nPurchase of C++ Concurrency in Action, Second Edition includes free access to a pri-\nvate web forum run by Manning Publications where you can make comments about\nthe book, ask technical questions, and receive help from the author and from other\nusers. To access the forum, go to www.manning.com/books/c-plus-plus-concurrency-\nin-action-second-edition. You can also learn more about Manning’s forums and the\nrules of conduct at https://forums.manning.com/forums/about.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It’s not a commitment to any specific amount of participation on the part of the\nauthor, whose contribution to the book’s forum remains voluntary (and unpaid). We\nsuggest you try asking the author some challenging questions, lest his interest stray!\n The forum and the archives of previous discussions will be accessible from the pub-\nlisher’s website as long as the book is in print.\n1 The just::thread implementation of the C++ Standard Thread Library, http://www.stdthread.co.uk.\n2 The Boost C++ library collection, http://www.boost.org.\n",
      "content_length": 2833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "xx\nabout the author\nAnthony Williams is a UK-based developer, consultant, and\ntrainer with over 20 years of experience in C++. He has been an\nactive member of the BSI C++ Standards Panel since 2001, and\nis the author or coauthor of many of the C++ Standards Com-\nmittee papers that led up to the inclusion of the thread library\nin the C++11 Standard. He continues to work on new facilities\nto enhance the C++ concurrency toolkit, both with standards\nproposals, and implementations of those facilities for the\njust::thread Pro extensions to the C++ thread library from Just\nSoftware Solutions Ltd. Anthony lives in the far west of Corn-\nwall, England.\n",
      "content_length": 651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "xxi\nabout the cover illustration\nThe illustration on the cover of C++ Concurrency in Action is captioned “Habit of a Lady\nof Japan.” The image is taken from the four-volume Collection of the Dress of Different\nNations by Thomas Jefferys, published in London between 1757 and 1772. The collec-\ntion includes beautiful hand-colored copperplate engravings of costumes from\naround the world and has influenced theatrical costume design since its publication.\nThe diversity of the drawings in the compendium speaks vividly of the richness of the\ncostumes presented on the London stage over 200 years ago. The costumes, both his-\ntorical and contemporaneous, offered a glimpse into the dress customs of people\nliving in different times and in different countries, making them come alive for\nLondon theater audiences.\n Dress codes have changed in the last century, and the diversity by region, so rich in\nthe past, has faded away. It’s now often hard to tell the inhabitant of one continent\nfrom another. Perhaps, trying to view it optimistically, we’ve traded a cultural and\nvisual diversity for a more varied personal life—or a more varied and interesting intel-\nlectual and technical life.\n We at Manning celebrate the inventiveness, the initiative, and the fun of the com-\nputer business with book covers based on the rich diversity of the regional and theatri-\ncal life of two centuries ago, brought back to life by the pictures from this collection.\n",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "1\nHello, world of\nconcurrency in C++!\nThese are exciting times for C++ users. Thirteen years after the original C++ Stan-\ndard was published in 1998, the C++ Standards Committee gave the language and\nits supporting library a major overhaul. The new C++ Standard (referred to as\nC++11 or C++0x) was published in 2011 and brought with it a swath of changes that\nmade working with C++ easier and more productive. The Committee also commit-\nted to a new “train model” of releases, with a new C++ Standard to be published\nevery three years. So far, we've had two of these publications: the C++14 Standard in\n2014, and the C++17 Standard in 2017, as well as several Technical Specifications\ndescribing extensions to the C++ Standard.\nThis chapter covers\nWhat is meant by concurrency and multithreading\nWhy you might want to use concurrency and \nmultithreading in your applications\nSome of the history of the support for \nconcurrency in C++\nWhat a simple multithreaded C++ program \nlooks like\n",
      "content_length": 990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "2\nCHAPTER 1\nHello, world of concurrency in C++!\n One of the most significant new features in the C++11 Standard was the support of\nmultithreaded programs. For the first time, the C++ Standard acknowledged the exis-\ntence of multithreaded applications in the language and provided components in the\nlibrary for writing multithreaded applications. This made it possible to write multi-\nthreaded C++ programs without relying on platform-specific extensions and enabled\nyou to write portable multithreaded code with guaranteed behavior. It also came at a\ntime when programmers were increasingly looking to concurrency in general, and\nmultithreaded programming in particular, to improve application performance. The\nC++14 and C++17 Standards have built upon this baseline to provide further support\nfor writing multithreaded programs in C++, as have the Technical Specifications.\nThere’s a Technical Specification for concurrency extensions, and another for paral-\nlelism, though the latter has been incorporated into C++17.\n This book is about writing programs in C++ using multiple threads for concur-\nrency and the C++ language features and library facilities that make it possible. I’ll\nstart by explaining what I mean by concurrency and multithreading and why you\nwould want to use concurrency in your applications. After a quick detour into why you\nmight not want to use it in your applications, we’ll go through an overview of the con-\ncurrency support in C++, and we’ll round off this chapter with a simple example of\nC++ concurrency in action. Readers experienced with developing multithreaded\napplications may wish to skip the early sections. In subsequent chapters, we’ll cover\nmore extensive examples and look at the library facilities in more depth. The book\nwill finish with an in-depth reference to all the C++ Standard Library facilities for mul-\ntithreading and concurrency.\n So, what do I mean by concurrency and multithreading?\n1.1\nWhat is concurrency?\nAt the simplest and most basic level, concurrency is about two or more separate activi-\nties happening at the same time. We encounter concurrency as a natural part of life;\nwe can walk and talk at the same time or perform different actions with each hand,\nand we each go about our lives independently of each other—you can watch football\nwhile I go swimming, and so on.\n1.1.1\nConcurrency in computer systems\nWhen we talk about concurrency in terms of computers, we mean a single system per-\nforming multiple independent activities in parallel, rather than sequentially, or one\nafter the other. This isn’t a new phenomenon. Multitasking operating systems that\nallow a single desktop computer to run multiple applications at the same time\nthrough task switching have been commonplace for many years, as have high-end\nserver machines with multiple processors that enable genuine concurrency. What’s\nnew is the increased prevalence of computers that can genuinely run multiple tasks in\nparallel rather than giving the illusion of doing so.\n",
      "content_length": 3003,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "3\nWhat is concurrency?\n Historically, most desktop computers have had one processor, with a single process-\ning unit or core, and this remains true for many desktop machines today. Such a\nmachine can only perform one task at a time, but it can switch between tasks many times\nper second. By doing a bit of one task and then a bit of another and so on, it appears\nthat the tasks are happening concurrently. This is called task switching. We still talk about\nconcurrency with such systems; because the task switches are so fast, you can’t tell at\nwhich point a task may be suspended as the processor switches to another one. The task\nswitching provides the illusion of concurrency to both the user and the applications\nthemselves. Because there is only the illusion of concurrency, the behavior of applica-\ntions may be subtly different when executing in a single-processor task-switching envi-\nronment compared to when executing in an environment with true concurrency. In\nparticular, incorrect assumptions about the memory model (covered in chapter 5) may\nnot show up in such an environment. This is discussed in more depth in chapter 10.\n Computers containing multiple processors have been used for servers and high-\nperformance computing tasks for years, and computers based on processors with\nmore than one core on a single chip (multicore processors) are becoming increas-\ningly common as desktop machines. Whether they have multiple processors or multi-\nple cores within a processor (or both), these computers are capable of genuinely\nrunning more than one task in parallel. We call this hardware concurrency.\n Figure 1.1 shows an idealized scenario of a computer with precisely two tasks to do,\neach divided into 10 equally sized chunks. On a dual-core machine (which has two\nprocessing cores), each task can execute on its own core. On a single-core machine\ndoing task switching, the chunks from each task are interleaved. But they are also\nspaced out a bit (in figure 1.1, this is shown by the gray bars separating the chunks\nbeing thicker than the separator bars shown for the dual-core machine); in order to\ndo the interleaving, the system has to perform a context switch every time it changes\nfrom one task to another, and this takes time. In order to perform a context switch,\nthe OS has to save the CPU state and instruction pointer for the currently running\ntask, work out which task to switch to, and reload the CPU state for the task being\nswitched to. The CPU will then potentially have to load the memory for the instruc-\ntions and data for the new task into the cache, which can prevent the CPU from exe-\ncuting any instructions, causing further delay.\nSingle core\nCore 1\nCore 2\nDual core\nFigure 1.1\nTwo approaches to concurrency: parallel execution on a dual-core \nmachine versus task switching on a single-core machine\n",
      "content_length": 2840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "4\nCHAPTER 1\nHello, world of concurrency in C++!\nThough the availability of concurrency in the hardware is most obvious with multipro-\ncessor or multicore systems, some processors can execute multiple threads on a single\ncore. The important factor to consider is the number of hardware threads, which is the\nmeasure of how many independent tasks the hardware can genuinely run concur-\nrently. Even with a system that has genuine hardware concurrency, it’s easy to have\nmore tasks than the hardware can run in parallel, so task switching is still used in these\ncases. For example, on a typical desktop computer there may be hundreds of tasks\nrunning, performing background operations, even when the computer is nominally\nidle. It’s the task switching that allows these background tasks to run and you to run\nyour word processor, compiler, editor, and web browser (or any combination of appli-\ncations) all at once. Figure 1.2 shows task switching among four tasks on a dual-core\nmachine, again for an idealized scenario with the tasks divided neatly into equally\nsized chunks. In practice, many issues will make the divisions uneven and the schedul-\ning irregular. Some of these issues are covered in chapter 8 when we look at factors\naffecting the performance of concurrent code.\nAll the techniques, functions, and classes covered in this book can be used whether\nyour application is running on a machine with one single-core processor or with many\nmulticore processors, and are not affected by whether the concurrency is achieved\nthrough task switching or by genuine hardware concurrency. But as you may imagine,\nhow you make use of concurrency in your application may well depend on the\namount of hardware concurrency available. This is covered in chapter 8, where I dis-\ncuss the issues involved in designing concurrent code in C++.\n1.1.2\nApproaches to concurrency\nImagine, for a moment, a pair of programmers working together on a software proj-\nect. If your developers are in separate offices, they can go about their work peacefully,\nwithout being disturbed by each other, and they each have their own set of reference\nmanuals. But communication isn’t straightforward; rather than turning around and\ntalking to each other, they have to use the phone or email, or get up and walk to the\nother’s office. Also, you have the overhead of two offices to manage and multiple cop-\nies of reference manuals to purchase.\n Now imagine that you move your developers into the same office. They can now\ntalk to each other freely to discuss the design of the application, and they can easily\nCore 1\nCore 2\nDual core\nFigure 1.2\nTask switching of four tasks on two cores\n",
      "content_length": 2659,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "5\nWhat is concurrency?\ndraw diagrams on paper or on a whiteboard to help with design ideas or explanations.\nYou have only one office to manage, and one set of resources will often suffice. On the\nnegative side, they might find it harder to concentrate, and there may be issues with\nsharing resources (“Where’s the reference manual gone now?”).\n These two ways of organizing your developers illustrate the two basic approaches\nto concurrency. Each developer represents a thread, and each office represents a pro-\ncess. The first approach is to have multiple single-threaded processes, which is similar\nto having each developer in their own office, and the second approach is to have mul-\ntiple threads in a single process, which is like having two developers in the same office.\nYou can combine these in an arbitrary fashion and have multiple processes, some of\nwhich are multithreaded and some of which are single-threaded, but the principles\nare the same. Let’s now have a brief look at these two approaches to concurrency in\nan application.\nCONCURRENCY WITH MULTIPLE PROCESSES\nThe first way to make use of concurrency within an appli-\ncation is to divide the application into multiple, separate,\nsingle-threaded processes that are run at the same time,\nmuch as you can run your web browser and word proces-\nsor at the same time. These separate processes can then\npass messages to each other through all the normal inter-\nprocess communication channels (signals, sockets, files,\npipes, and so on), as shown in figure 1.3. One downside is\nthat such communication between processes is often\neither complicated to set up or slow, or both, because\noperating systems typically provide a lot of protection\nbetween processes to avoid one process accidentally modi-\nfying data belonging to another process. Another downside\nis that there’s an inherent overhead in running multiple\nprocesses: it takes time to start a process, the operating\nsystem must devote internal resources to managing the\nprocess, and so forth.\n It’s not all negative: the added protection operating systems typically provide\nbetween processes and the higher-level communication mechanisms mean that it\ncan be easier to write safe concurrent code with processes rather than threads.\nIndeed, environments such as that provided for the Erlang (www.erlang.org/) pro-\ngramming language use processes as the fundamental building block of concur-\nrency to great effect.\n Using separate processes for concurrency also has an additional advantage—you\ncan run the separate processes on distinct machines connected over a network. Though\nthis increases the communication cost, on a carefully designed system it can be a cost-\neffective way of increasing the available parallelism and improving performance.\nProcess 1\nThread\nOperating\nsystem\nInterprocess\ncommunication\nProcess 2\nThread\nFigure 1.3\nCommunication \nbetween a pair of processes \nrunning concurrently\n",
      "content_length": 2916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "6\nCHAPTER 1\nHello, world of concurrency in C++!\nCONCURRENCY WITH MULTIPLE THREADS\nThe alternative approach to concurrency is to run multiple\nthreads in a single process. Threads are much like light-\nweight processes: each thread runs independently of the\nothers, and each may run a different sequence of instruc-\ntions. But all threads in a process share the same address\nspace, and most of the data can be accessed directly from\nall threads—global variables remain global, and pointers or\nreferences to objects or data can be passed around among\nthreads. Although it’s often possible to share memory\namong processes, this is complicated to set up and often\nhard to manage, because memory addresses of the same\ndata aren’t necessarily the same in different processes. Fig-\nure 1.4 shows two threads within a process communicating\nthrough shared memory.\n The shared address space and lack of protection of data between threads makes\nthe overhead associated with using multiple threads much smaller than that from\nusing multiple processes, because the operating system has less bookkeeping to do.\nBut the flexibility of shared memory also comes with a price: if data is accessed by mul-\ntiple threads, the application programmer must ensure that the view of data seen by\neach thread is consistent whenever it’s accessed. The issues surrounding sharing data\nbetween threads, and the tools to use and guidelines to follow to avoid problems, are\ncovered throughout this book, notably in chapters 3, 4, 5, and 8. The problems aren’t\ninsurmountable, provided suitable care is taken when writing the code, but they do\nmean that a great deal of thought must go into the communication between threads.\n The low overhead associated with launching and communicating between multi-\nple threads within a process compared to launching and communicating between\nmultiple single-threaded processes means that this is the favored approach to concur-\nrency in mainstream languages, including C++, despite the potential problems arising\nfrom the shared memory. In addition, the C++ Standard doesn’t provide any intrinsic\nsupport for communication between processes, so applications that use multiple pro-\ncesses will have to rely on platform-specific APIs to do so. This book therefore focuses\nexclusively on using multithreading for concurrency, and future references to concur-\nrency assume that this is achieved by using multiple threads.\n There’s another word that gets used a lot around multithreaded code: parallelism.\nLet’s clarify the differences.\n1.1.3\nConcurrency vs. parallelism\nConcurrency and parallelism have largely overlapping meanings with respect to\nmultithreaded code. Indeed, to many they mean the same thing. The difference is\nprimarily a matter of nuance, focus, and intent. Both terms are about running mul-\ntiple tasks simultaneously, using the available hardware, but parallelism is much more\nProcess\nThread 1\nShared memory\nThread 2\nFigure 1.4\nCommunication \nbetween a pair of threads \nrunning concurrently in a \nsingle process\n",
      "content_length": 3033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "7\nWhy use concurrency?\nperformance-oriented. People talk about parallelism when their primary concern is\ntaking advantage of the available hardware to increase the performance of bulk data\nprocessing, whereas people talk about concurrency when their primary concern is sepa-\nration of concerns, or responsiveness. This dichotomy is not cut and dried, and there\nis still considerable overlap in meaning, but it can help clarify discussions to know of\nthis distinction. Throughout this book, there will be examples of both.\n Having clarified what we mean by concurrency and parallelism, let’s look at why\nyou would use concurrency in your applications.\n1.2\nWhy use concurrency?\nThere are two main reasons to use concurrency in an application: separation of con-\ncerns and performance. In fact, I’d go so far as to say that they’re almost the only rea-\nsons to use concurrency; anything else boils down to one or the other (or maybe even\nboth) when you look hard enough (well, except for reasons like “because I want to”).\n1.2.1\nUsing concurrency for separation of concerns\nSeparation of concerns is almost always a good idea when writing software; by group-\ning related bits of code together and keeping unrelated bits of code apart, you can\nmake your programs easier to understand and test, and less likely to contain bugs. You\ncan use concurrency to separate distinct areas of functionality, even when the opera-\ntions in these distinct areas need to happen at the same time; without the explicit use\nof concurrency, you either have to write a task-switching framework or actively make\ncalls to unrelated areas of code during an operation.\n Consider a processing-intensive application with a user interface, such as a DVD\nplayer application for a desktop computer. This application fundamentally has two\nsets of responsibilities. Not only does it have to read the data from the disk, decode\nthe images and sound, and send them to the graphics and sound hardware in a timely\nfashion so the DVD plays without glitches, but it must also take input from the user,\nsuch as when the user clicks Pause or Return To Menu, or even Quit. In a single\nthread, the application has to check for user input at regular intervals during the play-\nback, conflating the DVD playback code with the user interface code. By using multi-\nthreading to separate these concerns, the user interface code and DVD playback code\nno longer have to be so closely intertwined; one thread can handle the user interface\nand another the DVD playback. There will have to be interaction between them, such\nas when the user clicks Pause, but now these interactions are directly related to the\ntask at hand.\n This gives the illusion of responsiveness, because the user interface thread can typ-\nically respond immediately to a user request, even if the response is to display a busy\ncursor or a Please Wait message while the request is conveyed to the thread doing the\nwork. Similarly, separate threads are often used to run tasks that must run continu-\nously in the background, such as monitoring the filesystem for changes in a desktop\nsearch application. Using threads in this way generally makes the logic in each thread\n",
      "content_length": 3185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "8\nCHAPTER 1\nHello, world of concurrency in C++!\nmuch simpler, because the interactions between them can be limited to clearly identi-\nfiable points, rather than having to intersperse the logic of the different tasks.\n In this case, the number of threads is independent of the number of CPU cores\navailable, because the division into threads is based on the conceptual design rather\nthan an attempt to increase throughput.\n1.2.2\nUsing concurrency for performance: task and data parallelism\nMultiprocessor systems have existed for decades, but until recently they were mostly\nfound only in supercomputers, mainframes, and large server systems. But chip manu-\nfacturers have increasingly been favoring multicore designs with 2, 4, 16, or more pro-\ncessors on a single chip over better performance with a single core. Consequently,\nmulticore desktop computers, and even multicore embedded devices, are now increas-\ningly prevalent. The increased computing power of these machines comes not from\nrunning a single task faster but from running multiple tasks in parallel. In the past,\nprogrammers have been able to sit back and watch their programs get faster with each\nnew generation of processors, without any effort on their part. But now, as Herb Sut-\nter put it, “The free lunch is over.”1 If software is to take advantage of this increased\ncomputing power, it must be designed to run multiple tasks concurrently. Program-\nmers must therefore take heed, and those who have hitherto ignored concurrency\nmust now look to add it to their toolbox.\n There are two ways to use concurrency for performance. The first, and most obvi-\nous, is to divide a single task into parts and run each in parallel, reducing the total\nruntime. This is task parallelism. Although this sounds straightforward, it can be quite a\ncomplex process, because there may be many dependencies between the various\nparts. The divisions may be either in terms of processing—one thread performs one\npart of the algorithm while another thread performs a different part—or in terms of\ndata—each thread performs the same operation on different parts of the data. This\nlatter approach is called data parallelism.\n Algorithms that are readily susceptible to such parallelism are frequently called\nembarrassingly parallel. Despite the implication that you might be embarrassed to have\ncode so easy to parallelize, this is a good thing; other terms I’ve encountered for such\nalgorithms are naturally parallel and conveniently concurrent. Embarrassingly parallel algo-\nrithms have good scalability properties—as the number of available hardware threads\ngoes up, the parallelism in the algorithm can be increased to match. Such an algo-\nrithm is the perfect embodiment of the adage, “Many hands make light work.” For\nthose parts of the algorithm that aren’t embarrassingly parallel, you might be able to\ndivide the algorithm into a fixed (and therefore not scalable) number of parallel\ntasks. Techniques for dividing tasks between threads are covered in chapters 8 and 10.\n The second way to use concurrency for performance is to use the available paral-\nlelism to solve bigger problems; rather than processing one file at a time, process 2, or\n1 “The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software,” Herb Sutter, Dr. Dobb’s\nJournal, 30(3), March 2005. http://www.gotw.ca/publications/concurrency-ddj.htm.\n",
      "content_length": 3382,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "9\nWhy use concurrency?\n10, or 20, as appropriate. Although this is an application of data parallelism, by per-\nforming the same operation on multiple sets of data concurrently, there’s a different\nfocus. It still takes the same amount of time to process one chunk of data, but now\nmore data can be processed in the same amount of time. Obviously, there are limits\nto this approach, and this won’t be beneficial in all cases, but the increase in throughput\nthat comes from this approach can make new things possible—increased resolution\nin video processing, for example, if different areas of the picture can be processed\nin parallel.\n1.2.3\nWhen not to use concurrency\nIt’s just as important to know when not to use concurrency as it is to know when to use\nit. Fundamentally, the only reason not to use concurrency is when the benefit isn’t\nworth the cost. Code using concurrency is harder to understand in many cases, so\nthere’s a direct intellectual cost to writing and maintaining multithreaded code, and\nthe additional complexity can also lead to more bugs. Unless the potential perfor-\nmance gain is large enough or the separation of concerns is clear enough to justify the\nadditional development time required to get it right and the additional costs associ-\nated with maintaining multithreaded code, don’t use concurrency.\n Also, the performance gain might not be as large as expected; there’s an inherent\noverhead associated with launching a thread, because the OS has to allocate the asso-\nciated kernel resources and stack space and then add the new thread to the scheduler,\nall of which takes time. If the task being run on the thread is completed quickly, the\ntime taken by the task may be dwarfed by the overhead of launching the thread, possi-\nbly making the overall performance of the application worse than if the task had been\nexecuted directly by the spawning thread.\n Furthermore, threads are a limited resource. If you have too many threads run-\nning at once, this consumes OS resources and may make the system as a whole run\nslower. Not only that, but using too many threads can exhaust the available memory or\naddress space for a process, because each thread requires a separate stack space. This\nis particularly a problem for 32-bit processes with a flat architecture where there’s a 4 GB\nlimit to the available address space: if each thread has a 1 MB stack (as is typical on\nmany systems), then the address space would be used up with 4,096 threads, without\nallowing any space for code, static data, or heap data. Although 64-bit (or larger) sys-\ntems don’t have this direct address-space limit, they still have finite resources: if you\nrun too many threads, this will eventually cause problems. Though thread pools (see\nchapter 9) can be used to limit the number of threads, they aren’t a silver bullet, and\nthey do have their own issues.\n If the server side of a client/server application launches a separate thread for each\nconnection, this will work fine for a small number of connections, but can quickly\nexhaust system resources by launching too many threads if the same technique is used\nfor a high-demand server that has to handle many connections. In this scenario, care-\nful use of thread pools can provide optimal performance (see chapter 9).\n",
      "content_length": 3278,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "10\nCHAPTER 1\nHello, world of concurrency in C++!\n Finally, the more threads you have running, the more context switching the oper-\nating system has to do. Each context switch takes time that could be spent doing use-\nful work, so at some point, adding an extra thread will reduce the overall application\nperformance rather than increase it. For this reason, if you’re trying to achieve the\nbest possible performance of the system, it’s necessary to adjust the number of threads\nrunning to take into account the available hardware concurrency (or lack of it).\n The use of concurrency for performance is like any other optimization strategy: it\nhas the potential to greatly improve the performance of your application, but it can\nalso complicate the code, making it harder to understand and more prone to bugs.\nTherefore, it’s only worth doing for those performance-critical parts of the applica-\ntion where there’s the potential for measurable gain. Of course, if the potential for\nperformance gains is only secondary to clarity of design or separation of concerns, it\nmay still be worth using a multithreaded design.\n Assuming that you’ve decided you do want to use concurrency in your application,\nwhether for performance, separation of concerns, or because it’s “multithreading\nMonday,” what does that mean for C++ programmers?\n1.3\nConcurrency and multithreading in C++\nStandardized support for concurrency through multithreading is a relatively new\nthing for C++. It’s only since the C++11 Standard that you’ve been able to write multi-\nthreaded code without resorting to platform-specific extensions. In order to under-\nstand the rationale behind lots of the decisions in the Standard C++ Thread Library,\nit’s important to understand the history.\n1.3.1\nHistory of multithreading in C++\nThe 1998 C++ Standard doesn’t acknowledge the existence of threads, and the opera-\ntional effects of the various language elements are written in terms of a sequential\nabstract machine. Not only that, but the memory model isn’t formally defined, so you\ncan’t write multithreaded applications without compiler-specific extensions to the\n1998 C++ Standard.\n Compiler vendors are free to add extensions to the language, and the prevalence\nof C APIs for multithreading—such as those in the POSIX C standard and the Micro-\nsoft Windows API—has led many C++ compiler vendors to support multithreading\nwith various platform-specific extensions. This compiler support is generally limited to\nallowing the use of the corresponding C API for the platform and ensuring that the\nC++ Runtime Library (such as the code for the exception-handling mechanism) works\nin the presence of multiple threads. Although few compiler vendors have provided a\nformal multithreading-aware memory model, the behavior of the compilers and pro-\ncessors has been sufficiently good that a large number of multithreaded C++ pro-\ngrams have been written.\n Not content with using the platform-specific C APIs for handling multithreading,\nC++ programmers have looked to their class libraries to provide object-oriented\n",
      "content_length": 3071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "11\nConcurrency and multithreading in C++\nmultithreading facilities. Application frameworks, such as MFC, and general-purpose\nC++ libraries, such as Boost and ACE, have accumulated sets of C++ classes that wrap\nthe underlying platform-specific APIs and provide higher-level facilities for multi-\nthreading that simplify tasks. Although the precise details of the class libraries vary\nconsiderably, particularly in the area of launching new threads, the overall shape of\nthe classes has a lot in common. One particularly important design that’s common to\nmany C++ class libraries, and that provides considerable benefit to the programmer, is\nthe use of the Resource Acquisition Is Initialization (RAII) idiom with locks to ensure\nthat mutexes are unlocked when the relevant scope is exited.\n For many cases, the multithreading support of existing C++ compilers combined\nwith the availability of platform-specific APIs and platform-independent class libraries,\nsuch as Boost and ACE, provide a solid foundation on which to write multithreaded\nC++ code, and as a result, there are probably millions of lines of C++ code written as\npart of multithreaded applications. But the lack of standard support means that there\nare occasions where the lack of a thread-aware memory model causes problems, par-\nticularly for those who try to gain higher performance by using knowledge of the pro-\ncessor hardware or for those writing cross-platform code where the behavior of the\ncompilers varies between platforms.\n1.3.2\nConcurrency support in the C++11 standard\nAll this changed with the release of the C++11 Standard. Not only is there a thread-\naware memory model, but the C++ Standard Library was extended to include classes\nfor managing threads (see chapter 2), protecting shared data (see chapter 3), syn-\nchronizing operations between threads (see chapter 4), and low-level atomic opera-\ntions (see chapter 5).\n The C++11 Thread Library is heavily based on the prior experience accumulated\nthrough the use of the C++ class libraries mentioned previously. In particular, the\nBoost Thread Library was used as the primary model on which the new library is\nbased, with many of the classes sharing their names and structure with the corre-\nsponding ones from Boost. As the standard has evolved, this has been a two-way flow,\nand the Boost Thread Library has itself changed to match the C++ Standard in many\nrespects, so users transitioning from Boost should find themselves at home.\n Concurrency support is one of the changes with the C++11 Standard—as men-\ntioned at the beginning of this chapter, there are many enhancements to the language\nto make programmers’ lives easier. Although these are generally outside the scope of\nthis book, some of those changes have had a direct impact on the Thread Library and\nthe ways in which it can be used. Appendix A provides a brief introduction to these\nlanguage features.\n",
      "content_length": 2902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "12\nCHAPTER 1\nHello, world of concurrency in C++!\n1.3.3\nMore support for concurrency and parallelism in C++14 \nand C++17\nThe only specific support for concurrency and parallelism added in C++14 was a new\nmutex type for protecting shared data (see chapter 3). But C++17 adds considerably\nmore: a full suite of parallel algorithms (see chapter 10) for starters. Both of these\nStandards enhance the core language and the rest of the Standard Library, and these\nenhancements can simplify the writing of multithreaded code.\n As mentioned previously, there’s also a Technical Specification for concurrency,\nwhich describes extensions to the functions and classes provided by the C++ Standard,\nespecially around synchronizing operations between threads (see chapter 4).\n The support for atomic operations directly in C++ enables programmers to write\nefficient code with defined semantics without the need for platform-specific assembly\nlanguage. This is a real boon for those trying to write efficient, portable code; not only\ndoes the compiler take care of the platform specifics, but the optimizer can be written\nto take into account the semantics of the operations, enabling better optimization of\nthe program as a whole.\n1.3.4\nEfficiency in the C++ Thread Library\nOne of the concerns that developers involved in high-performance computing often\nraise regarding C++ in general, and C++ classes that wrap low-level facilities—such as\nthose in the new Standard C++ Thread Library specifically—is that of efficiency. If\nyou’re after the utmost in performance, it’s important to understand the implementa-\ntion costs associated with using any high-level facilities, compared to using the under-\nlying low-level facilities directly. This cost is the abstraction penalty.\n The C++ Standards Committee was aware of this when designing the C++ Standard\nLibrary in general and the Standard C++ Thread Library in particular; one of the\ndesign goals has been that there should be little or no benefit to be gained from using\nthe lower-level APIs directly, where the same facility is to be provided. The library has\ntherefore been designed to allow for efficient implementation (with a low abstraction\npenalty) on most major platforms.\n Another goal of the C++ Standards Committee has been to ensure that C++ pro-\nvides sufficient low-level facilities for those wishing to work close to the metal for the\nultimate performance. To this end, along with the new memory model comes a com-\nprehensive atomic operations library for direct control over individual bits and bytes\nand the inter-thread synchronization and visibility of any changes. These atomic types\nand the corresponding operations can now be used in many places where developers\nwould previously have chosen to drop down to platform-specific assembly language.\nCode using the new standard types and operations is more portable and easier to\nmaintain.\n The C++ Standard Library also provides higher-level abstractions and facilities that\nmake writing multithreaded code easier and less error-prone. Sometimes the use of\nthese facilities comes with a performance cost because of the additional code that\n",
      "content_length": 3146,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "13\nGetting started\nmust be executed. But this performance cost doesn’t necessarily imply a higher abstrac-\ntion penalty; in general, the cost is no higher than would be incurred by writing\nequivalent functionality by hand, and the compiler may inline much of the addi-\ntional code anyway.\n In some cases, the high-level facilities provide additional functionality beyond what\nmay be required for a specific use. Most of the time this isn’t an issue: you don’t pay\nfor what you don’t use. On rare occasions, this unused functionality will impact the\nperformance of other code. If you’re aiming for performance and the cost is too high,\nyou may be better off handcrafting the desired functionality from lower-level facilities.\nIn the vast majority of cases, the additional complexity and chance of errors far out-\nweigh the potential benefits from a small performance gain. Even if profiling does\ndemonstrate that the bottleneck is in the C++ Standard Library facilities, it may be due\nto poor application design rather than a poor library implementation. For example, if\ntoo many threads are competing for a mutex, it will impact the performance signifi-\ncantly. Rather than trying to shave a small fraction of time off the mutex operations, it\nwould probably be more beneficial to restructure the application so that there’s less\ncontention on the mutex. Designing applications to reduce contention is covered in\nchapter 8.\n In those rare cases where the C++ Standard Library doesn’t provide the perfor-\nmance or behavior required, it might be necessary to use platform-specific facilities.\n1.3.5\nPlatform-specific facilities\nAlthough the C++ Thread Library provides reasonably comprehensive facilities for\nmultithreading and concurrency, on any given platform there will be platform-specific\nfacilities that go beyond what’s offered. In order to gain easy access to those facilities\nwithout giving up the benefits of using the Standard C++ Thread Library, the types in\nthe C++ Thread Library may offer a native_handle() member function that allows\nthe underlying implementation to be directly manipulated using a platform-specific\nAPI. By its nature, any operations performed using native_handle() are entirely\nplatform dependent and beyond of the scope of this book (and the Standard C++\nLibrary itself).\n Before even considering using platform-specific facilities, it’s important to under-\nstand what the Standard Library provides, so let’s get started with an example.\n1.4\nGetting started\nOK, so you have a nice, shiny C++11/C++14/C++17 compiler. What’s next? What does\na multithreaded C++ program look like? It looks much like any other C++ program,\nwith the usual mix of variables, classes, and functions. The only real distinction is that\nsome functions might be running concurrently, so you need to ensure that shared\ndata is safe for concurrent access, as described in chapter 3. In order to run func-\ntions concurrently, specific functions and objects must be used to manage the differ-\nent threads.\n",
      "content_length": 3010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "14\nCHAPTER 1\nHello, world of concurrency in C++!\n1.4.1\nHello, Concurrent World\nLet’s start with a classic example: a program to print “Hello World.” A simple Hello\nWorld program that runs in a single thread is shown here, to serve as a baseline when\nwe move to multiple threads:\n#include <iostream>\nint main()\n{\n    std::cout<<\"Hello World\\n\";\n}\nAll this program does is write “Hello World” to the standard output stream. Let’s com-\npare it to the simple Hello Concurrent World program shown in the following listing,\nwhich starts a separate thread to display the message.\n#include <iostream>\n#include <thread>           \nvoid hello()                       \n{\n    std::cout<<\"Hello Concurrent World\\n\";\n}\nint main()\n{\n    std::thread t(hello);   3\n    t.join();\n}\nThe first difference is the extra #include <thread>. The declarations for the multi-\nthreading support in the Standard C++ Library are in new headers: the functions and\nclasses for managing threads are declared in <thread>, whereas those for protecting\nshared data are declared in other headers.\n Second, the code for writing the message has been moved to a separate function.\nThis is because every thread has to have an initial function, where the new thread of\nexecution begins. For the initial thread in an application, this is main(), but for\nevery other thread it’s specified in the constructor of a std::thread object—in this\ncase, the std::thread object named t has the new hello() function as its initial\nfunction.\n This is the next difference: rather than writing directly to standard output or call-\ning hello() from main(), this program launches a new thread to do it, bringing the\nthread count to two—the initial thread that starts at main() and the new thread that\nstarts at hello().\n After the new thread has been launched, the initial thread continues execution. If\nit didn’t wait for the new thread to finish, it would merrily continue to the end of\nmain() and end the program—possibly before the new thread had a chance to run.\nThis is why the call to join() is there—as described in chapter 2, this causes the calling\nListing 1.1\nA simple Hello Concurrent World program\n",
      "content_length": 2152,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "15\nSummary\nthread (in main()) to wait for the thread associated with the std::thread object, in\nthis case, t.\n If this seems like a lot of effort to write a message to standard output, it is—as\ndescribed in section 1.2.3, it’s generally not worth the effort to use multiple threads\nfor such a simple task, especially if the initial thread has nothing to do in the mean-\ntime. Later in the book, you’ll work through examples of scenarios where there’s a\nclear gain to using multiple threads.\nSummary\nIn this chapter, I covered what’s meant by concurrency and multithreading and why\nyou’d choose to use it (or not) in your applications. I also covered the history of multi-\nthreading in C++, from the complete lack of support in the 1998 standard, through\nvarious platform-specific extensions, to proper multithreading support in the C++11\nStandard, and on to the C++14 and C++17 standards and the Technical Specification\nfor concurrency. This support has come in time to allow programmers to take advan-\ntage of the greater hardware concurrency becoming available with newer CPUs, as\nchip manufacturers choose to add more processing power in the form of multiple\ncores that allow more tasks to be executed concurrently, rather than increasing the\nexecution speed of a single core.\n I also showed how simple using the classes and functions from the C++ Standard\nLibrary can be in the examples in section 1.4. In C++, using multiple threads isn’t\ncomplicated in and of itself; the complexity lies in designing the code so that it\nbehaves as intended.\n After the examples of section 1.4, it’s time for something with a bit more substance.\nIn chapter 2, we’ll look at the classes and functions available for managing threads.\n",
      "content_length": 1721,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "16\nManaging threads\nOK, so you’ve decided to use concurrency for your application. In particular,\nyou’ve decided to use multiple threads. What now? How do you launch these\nthreads, check that they’ve finished, and keep tabs on them? The C++ Standard\nLibrary makes most thread-management tasks relatively easy, with almost every-\nthing managed through the std::thread object associated with a given thread, as\nyou’ll see. For those tasks that aren’t so straightforward, the library provides the\nflexibility to build what you need from the basic building blocks.\n In this chapter, I’ll start by covering the basics: launching a thread, waiting for it\nto finish, or running it in the background. We’ll then look at passing additional\nparameters to the thread function when it’s launched and how to transfer owner-\nship of a thread from one std::thread object to another. Finally, we’ll look at\nchoosing the number of threads to use and identifying particular threads.\nThis chapter covers\nStarting threads, and various ways of specifying \ncode to run on a new thread\nWaiting for a thread to finish versus leaving it \nto run\nUniquely identifying threads\n",
      "content_length": 1152,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "17\nBasic thread management\n2.1\nBasic thread management\nEvery C++ program has at least one thread, which is started by the C++ runtime: the\nthread running main(). Your program can then launch additional threads that have\nanother function as the entry point. These threads then run concurrently with each\nother and with the initial thread. In the same way that the program exits when it returns\nfrom main(), when the specified entry point function returns, the thread exits. As you’ll\nsee, if you have a std::thread object for a thread, you can wait for it to finish; but first\nyou have to start it, so let’s look at launching threads.\n2.1.1\nLaunching a thread\nAs you saw in chapter 1, threads are started by constructing a std::thread object that\nspecifies the task to run on that thread. In the simplest case, that task is a plain, ordi-\nnary void-returning function that takes no parameters. This function runs on its own\nthread until it returns, and then the thread stops. At the other extreme, the task could\nbe a function object that takes additional parameters and performs a series of inde-\npendent operations that are specified through some kind of messaging system while\nit’s running, and the thread stops only when it’s signaled to do so, again via some kind\nof messaging system. It doesn’t matter what the thread is going to do or where it’s\nlaunched from, but starting a thread using the C++ Standard Library always boils\ndown to constructing a std::thread object:\nvoid do_some_work();\nstd::thread my_thread(do_some_work);\nThis is about as simple as it gets. Of course, you have to make sure that the <thread>\nheader is included so the compiler can see the definition of the std::thread class. As\nwith much of the C++ Standard Library, std::thread works with any callable type, so\nyou can pass an instance of a class with a function call operator to the std::thread\nconstructor instead:\nclass background_task\n{\npublic:\n    void operator()() const\n    {\n        do_something();\n        do_something_else();\n    }\n};\nbackground_task f;\nstd::thread my_thread(f);\nIn this case, the supplied function object is copied into the storage belonging to the\nnewly created thread of execution and invoked from there. It’s therefore essential that\nthe copy behaves equivalently to the original, or the result may not be what’s expected.\n",
      "content_length": 2335,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "18\nCHAPTER 2\nManaging threads\n One thing to consider when passing a function object to the thread constructor is\nto avoid what’s dubbed “C++’s most vexing parse.” If you pass a temporary rather than\na named variable, the syntax can be the same as that of a function declaration, in\nwhich case the compiler interprets it as such, rather than an object definition. For\nexample,\nstd::thread my_thread(background_task());\ndeclares a my_thread function that takes a single parameter (of type pointer-to-a-\nfunction-taking-no-parameters-and-returning-a-background_task-object) and\nreturns a std::thread object, rather than launching a new thread. You can avoid this\nby naming your function object as shown previously, by using an extra set of parenthe-\nses, or by using the new uniform initialization syntax; for example:\nstd::thread my_thread((background_task()));       \nstd::thread my_thread{background_task()};         \nIn the first example, the extra parentheses prevent interpretation as a function decla-\nration, allowing my_thread to be declared as a variable of type std::thread. The sec-\nond example uses the new uniform initialization syntax with braces rather than\nparentheses, and thus would also declare a variable.\n One type of callable object that avoids this problem is a lambda expression. This is a\nnew feature from C++11 which allows you to write a local function, possibly capturing\nsome local variables and avoiding the need to pass additional arguments (see sec-\ntion 2.2). For full details on lambda expressions, see appendix A, section A.5. The\nprevious example can be written using a lambda expression as follows:\nstd::thread my_thread([]{\n    do_something();\n    do_something_else();\n});\nOnce you’ve started your thread, you need to explicitly decide whether to wait for it\nto finish (by joining with it—see section 2.1.2) or leave it to run on its own (by\ndetaching it—see section 2.1.3). If you don’t decide before the std::thread object\nis destroyed, then your program is terminated (the std::thread destructor calls\nstd::terminate()). It’s therefore imperative that you ensure that the thread is cor-\nrectly joined or detached, even in the presence of exceptions. See section 2.1.3 for a\ntechnique to handle this scenario. Note that you only have to make this decision\nbefore the std::thread object is destroyed—the thread itself may well have finished\nlong before you join with it or detach it, and if you detach it, then if the thread is\nstill running, it will continue to do so, and may continue running long after the\nstd::thread object is destroyed; it will only stop running when it finally returns from\nthe thread function.\n If you don’t wait for your thread to finish, you need to ensure that the data\naccessed by the thread is valid until the thread has finished with it. This isn’t a new\n",
      "content_length": 2823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "19\nBasic thread management\nproblem—even in single-threaded code it’s undefined behavior to access an object\nafter it’s been destroyed—but the use of threads provides an additional opportunity to\nencounter such lifetime issues.\n One situation in which you can encounter such problems is when the thread func-\ntion holds pointers or references to local variables and the thread hasn’t finished\nwhen the function exits. The following listing shows an example of such a scenario.\nstruct func\n{\n    int& i;\n    func(int& i_):i(i_){}\n    void operator()()\n    {\n        for(unsigned j=0;j<1000000;++j)\n        {\n            do_something(i);      \n        }\n    }\n};\nvoid oops()\n{\n    int some_local_state=0;\n    func my_func(some_local_state);\n    std::thread my_thread(my_func);\n    my_thread.detach();             \n}                                \nIn this case, the new thread associated with my_thread will probably still be running\nwhen oops exits, because you’ve explicitly decided not to wait for it by calling detach().\nIf the thread is still running, you have the scenario shown in table 2.1: the next call to\ndo_something(i) will access an already destroyed variable. This is like normal single-\nthreaded code—allowing a pointer or reference to a local variable to persist beyond the\nfunction exit is never a good idea—but it’s easier to make the mistake with multithreaded\ncode, because it isn’t necessarily immediately apparent that this has happened.\nListing 2.1\nA function that returns while a thread still has access to local variables\nTable 2.1\nAccessing a local variable with a detached thread after it has been destroyed\nMain thread\nNew thread\nConstructs my_func with reference to \nsome_local_state\nStarts new thread my_thread\nStarted\nCalls func::operator()\nDetaches my_thread\nRunning func::operator(); may call do_something with \nreference to some_local_state\nPotential access to \ndangling reference\nDon’t wait for \nthread to finish\nNew thread might \nstill be running\n",
      "content_length": 1981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "20\nCHAPTER 2\nManaging threads\nOne common way to handle this scenario is to make the thread function self-contained\nand copy the data into the thread rather than sharing the data. If you use a callable\nobject for your thread function, that object is copied into the thread, so the original\nobject can be destroyed immediately. But you still need to be wary of objects contain-\ning pointers or references, such as in listing 2.1. In particular, it’s a bad idea to create\na thread within a function that has access to the local variables in that function, unless\nthe thread is guaranteed to finish before the function exits.\n Alternatively, you can ensure that the thread has completed execution before the\nfunction exits by joining with the thread.\n2.1.2\nWaiting for a thread to complete\nIf you need to wait for a thread to complete, you can do this by calling join() on\nthe associated std::thread instance. In the case of listing 2.1, replacing the call to\nmy_thread.detach() before the closing brace of the function body with a call to\nmy_thread.join() would therefore be sufficient to ensure that the thread was fin-\nished before the function was exited and thus before the local variables were destroyed.\nIn this case, it would mean there was little point in running the function on a separate\nthread, because the first thread wouldn’t be doing anything useful in the meantime, but\nin real code the original thread would either have work to do or would have launched\nseveral threads to do useful work before waiting for all of them to complete.\n join() is a simple and brute-force technique—either you wait for a thread to fin-\nish or you don’t. If you need more fine-grained control over waiting for a thread, such\nas to check whether a thread is finished, or to wait only a certain period of time, then\nyou have to use alternative mechanisms such as condition variables and futures, which\nwe’ll look at in chapter 4. The act of calling join() also cleans up any storage associ-\nated with the thread, so the std::thread object is no longer associated with the now-\nfinished thread; it isn’t associated with any thread. This means that you can call join()\nonly once for a given thread; once you’ve called join(), the std::thread object is no\nlonger joinable, and joinable() will return false.\n2.1.3\nWaiting in exceptional circumstances\nAs mentioned earlier, you need to ensure that you’ve called either join() or\ndetach() before a std::thread object is destroyed. If you’re detaching a thread, you\ncan usually call detach() immediately after the thread has been started, so this isn’t a\nproblem. But if you’re intending to wait for the thread, you need to carefully pick the\nDestroys some_local_state\nStill running\nExits oops\nStill running func::operator(); may call do_something \nwith reference to some_local_state => undefined behavior\nTable 2.1\nAccessing a local variable with a detached thread after it has been destroyed (continued)\nMain thread\nNew thread\n",
      "content_length": 2963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "21\nBasic thread management\nplace in the code where you call join(). This means that the call to join() is liable to\nbe skipped if an exception is thrown after the thread has been started but before the\ncall to join().\n To avoid your application being terminated when an exception is thrown, you\ntherefore need to make a decision about what to do in this case. In general, if you\nwere intending to call join() in a non-exceptional case, you also need to call join()\nin the presence of an exception to avoid accidental lifetime problems. The next listing\nshows some simple code that does just that.\nstruct func;         \nvoid f()\n{\n    int some_local_state=0;\n    func my_func(some_local_state);\n    std::thread t(my_func);\n    try\n    {\n        do_something_in_current_thread();\n    }\n    catch(...)\n    {\n        t.join();\n        throw;\n    }\n    t.join();\n}\nThe code in listing 2.2 uses a try/catch block to ensure that a thread with access to\nlocal state is finished before the function exits, whether the function exits normally, or\nby an exception. The use of try/catch blocks is verbose, and it’s easy to get the scope\nslightly wrong, so this isn’t an ideal scenario. If it’s important to ensure that the thread\ncompletes before the function exits—whether because it has a reference to other local\nvariables or for any other reason—then it’s important to ensure this is the case for all\npossible exit paths, whether normal or exceptional, and it’s desirable to provide a sim-\nple, concise mechanism for doing so.\n One way of doing this is to use the standard Resource Acquisition Is Initialization\n(RAII) idiom and provide a class that does the join() in its destructor, as in the fol-\nlowing listing. See how it simplifies the f() function.\nclass thread_guard\n{\n    std::thread& t;\npublic:\n    explicit thread_guard(std::thread& t_):\n        t(t_)\nListing 2.2\nWaiting for a thread to finish\nListing 2.3\nUsing RAII to wait for a thread to complete\nSee definition \nin listing 2.1\n",
      "content_length": 1985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "22\nCHAPTER 2\nManaging threads\n    {}\n    ~thread_guard()\n    {\n        if(t.joinable())         \n        {\n            t.join();            \n        }\n    }\n    thread_guard(thread_guard const&)=delete;             \n    thread_guard& operator=(thread_guard const&)=delete;\n};\nstruct func;       \nvoid f()\n{\n    int some_local_state=0;\n    func my_func(some_local_state);\n    std::thread t(my_func);\n    thread_guard g(t);\n    do_something_in_current_thread();\n}                                            \nWhen the execution of the current thread reaches the end of f, the local objects are\ndestroyed in reverse order of construction. Consequently, the thread_guard object,\ng, is destroyed first, and the thread is joined with, in the destructor. This even hap-\npens if the function exits because do_something_in_current_thread throws an\nexception.\n The destructor of thread_guard in listing 2.3 first tests to see if the std::thread\nobject is joinable() before calling join(). This is important, because join() can be\ncalled only once for a given thread of execution, so it would be a mistake to do so if\nthe thread had already been joined.\n The copy constructor and copy-assignment operators are marked =delete to ensure\nthat they’re not automatically provided by the compiler. Copying or assigning such an\nobject would be dangerous, because it might then outlive the scope of the thread it was\njoining. By declaring them as deleted, any attempt to copy a thread_guard object will\ngenerate a compilation error. See appendix A, section A.2, for more about deleted\nfunctions.\n If you don’t need to wait for a thread to finish, you can avoid this exception-safety\nissue by detaching it. This breaks the association of the thread with the std::thread\nobject and ensures that std::terminate() won’t be called when the std::thread\nobject is destroyed, even though the thread is still running in the background.\n2.1.4\nRunning threads in the background\nCalling detach() on a std::thread object leaves the thread to run in the back-\nground, with no direct means of communicating with it. It’s no longer possible to wait\nfor that thread to complete; if a thread becomes detached, it isn’t possible to obtain\na std::thread object that references it, so it can no longer be joined. Detached\nthreads truly run in the background; ownership and control are passed over to the\nSee definition \nin listing 2.1\n",
      "content_length": 2394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "23\nBasic thread management\nC++ Runtime Library, which ensures that the resources associated with the thread are\ncorrectly reclaimed when the thread exits.\n Detached threads are often called daemon threads after the UNIX concept of a dae-\nmon process that runs in the background without any explicit user interface. Such\nthreads are typically long-running; they run for almost the entire lifetime of the appli-\ncation, performing a background task such as monitoring the filesystem, clearing\nunused entries out of object caches, or optimizing data structures. At the other\nextreme, it may make sense to use a detached thread where there’s another mecha-\nnism for identifying when the thread has completed or where the thread is used for a\nfire-and-forget task.\n As you’ve saw in section 2.1.2, you detach a thread by calling the detach() mem-\nber function of the std::thread object. After the call completes, the std::thread\nobject is no longer associated with the actual thread of execution and is therefore no\nlonger joinable:\nstd::thread t(do_background_work);\nt.detach();\nassert(!t.joinable());\nIn order to detach the thread from a std::thread object, there must be a thread to\ndetach: you can’t call detach() on a std::thread object with no associated thread of\nexecution. This is exactly the same requirement as for join(), and you can check it in\nexactly the same way—you can only call t.detach() for a std::thread object t when\nt.joinable() returns true.\n Consider an application such as a word processor that can edit multiple docu-\nments at once. There are many ways to handle this, both at the UI level and internally.\nOne way that’s increasingly common at the moment is to have multiple, independent,\ntop-level windows, one for each document being edited. Although these windows\nappear to be completely independent, each with its own menus, they’re running\nwithin the same instance of the application. One way to handle this internally is to run\neach document-editing window in its own thread; each thread runs the same code but\nwith different data relating to the document being edited and the corresponding win-\ndow properties. Opening a new document therefore requires starting a new thread.\nThe thread handling the request isn’t going to care about waiting for that other\nthread to finish, because it’s working on an unrelated document, so this makes it a\nprime candidate for running a detached thread. \n The following listing shows a simple code outline for this approach.\nvoid edit_document(std::string const& filename)\n{\n    open_document_and_display_gui(filename);\n    while(!done_editing())\n    {\n        user_command cmd=get_user_input();\nListing 2.4\nDetaching a thread to handle other documents\n",
      "content_length": 2717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "24\nCHAPTER 2\nManaging threads\n        if(cmd.type==open_new_document)\n        {\n            std::string const new_name=get_filename_from_user();\n            std::thread t(edit_document,new_name);                \n            t.detach();                              \n        }\n        else\n        {\n            process_user_input(cmd);\n        }\n    }\n}\nIf the user chooses to open a new document, you prompt them for the document to\nopen, start a new thread to open that document, and then detach it. Because the new\nthread is doing the same operation as the current thread but on a different file, you\ncan reuse the same function (edit_document) with the newly chosen filename as the\nsupplied argument.\n This example also shows a case where it’s helpful to pass arguments to the function\nused to start a thread: rather than just passing the name of the function to the\nstd::thread constructor, you also pass in the filename parameter. Although other\nmechanisms could be used to do this, such as using a function object with member\ndata instead of an ordinary function with parameters, the C++ Standard Library pro-\nvides you with an easy way of doing it.\n2.2\nPassing arguments to a thread function\nAs shown in listing 2.4, passing arguments to the callable object or function is funda-\nmentally as simple as passing additional arguments to the std::thread constructor.\nBut it’s important to bear in mind that by default, the arguments are copied into inter-\nnal storage, where they can be accessed by the newly created thread of execution, and\nthen passed to the callable object or function as rvalues as if they were temporaries.\nThis is done even if the corresponding parameter in the function is expecting a refer-\nence. Here’s an example:\nvoid f(int i,std::string const& s);\nstd::thread t(f,3,”hello”);\nThis creates a new thread of execution associated with t, which calls f(3,”hello”).\nNote that even though f takes a std::string as the second parameter, the string lit-\neral is passed as a char const* and converted to a std::string only in the context of\nthe new thread. This is particularly important when the argument supplied is a\npointer to an automatic variable, as follows:\nvoid f(int i,std::string const& s);\nvoid oops(int some_param)\n{\n    char buffer[1024];                 \n    sprintf(buffer, \"%i\",some_param);\n",
      "content_length": 2331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "25\nPassing arguments to a thread function\n    std::thread t(f,3,buffer);         \n    t.detach();\n}\nIn this case, it’s the pointer to the local variable buffer that’s passed through to the\nnew thread and there’s a significant chance that the oops function will exit before\nthe buffer has been converted to a std::string on the new thread, thus leading to\nundefined behavior. The solution is to cast to std::string before passing the buffer\nto the std::thread constructor:\nvoid f(int i,std::string const& s);\nvoid not_oops(int some_param)\n{\n    char buffer[1024];\n    sprintf(buffer,\"%i\",some_param);\n    std::thread t(f,3,std::string(buffer));     \n    t.detach();\n}\nIn this case, the problem is that you were relying on the implicit conversion of the\npointer to the buffer into the std::string object expected as a function parameter,\nbut this conversion happens too late because the std::thread constructor copies the\nsupplied values as is, without converting to the expected argument type.\n It’s not possible to get the reverse scenario: the object is copied, and you wanted a\nnon-const reference, because this won't compile. You might try this if the thread is\nupdating a data structure that’s passed in by reference; for example:\nvoid update_data_for_widget(widget_id w,widget_data& data);    \nvoid oops_again(widget_id w)\n{\n    widget_data data;\n    std::thread t(update_data_for_widget,w,data);       \n    display_status();\n    t.join();\n    process_widget_data(data);         \n}\nAlthough update_data_for_widget expects the second parameter to be passed by ref-\nerence, the std::thread constructor doesn’t know that; it’s oblivious to the types of\nthe arguments expected by the function and blindly copies the supplied values. But\nthe internal code passes copied arguments as rvalues in order to work with move-only\ntypes, and will thus try to call update_data_for_widget with an rvalue. This will fail to\ncompile because you can't pass an rvalue to a function that expects a non-const refer-\nence. For those of you familiar with std::bind, the solution will be readily apparent:\nyou need to wrap the arguments that need to be references in std::ref. In this case,\nif you change the thread invocation to\nstd::thread t(update_data_for_widget,w,std::ref(data));\nUsing std::string \navoids dangling \npointer\n",
      "content_length": 2311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "26\nCHAPTER 2\nManaging threads\nthen update_data_for_widget will be correctly passed a reference to data rather\nthan a temporary copy of data, and the code will now compile successfully.\n If you’re familiar with std::bind, the parameter-passing semantics will be unsur-\nprising, because both the operation of the std::thread constructor and the opera-\ntion of std::bind are defined in terms of the same mechanism. This means that, for\nexample, you can pass a member function pointer as the function, provided you sup-\nply a suitable object pointer as the first argument:\nclass X\n{\npublic:\n    void do_lengthy_work();\n};\nX my_x;\nstd::thread t(&X::do_lengthy_work,&my_x);\nThis code will invoke my_x.do_lengthy_work() on the new thread, because the address\nof my_x is supplied as the object pointer. You can also supply arguments to such a\nmember function call: the third argument to the std::thread constructor will be the\nfirst argument to the member function, and so forth.\n Another interesting scenario for supplying arguments is where the arguments\ncan’t be copied but can only be moved: the data held within one object is transferred\nover to another, leaving the original object empty. An example of such a type is\nstd::unique_ptr, which provides automatic memory management for dynamically\nallocated objects. Only one std::unique_ptr instance can point to a given object at a\ntime, and when that instance is destroyed, the pointed-to object is deleted. The move\nconstructor and move assignment operator allow the ownership of an object to be trans-\nferred around between std::unique_ptr instances (see appendix A, section A.1.1, for\nmore on move semantics). Such a transfer leaves the source object with a NULL\npointer. This moving of values allows objects of this type to be accepted as function\nparameters or returned from functions. Where the source object is temporary, the\nmove is automatic, but where the source is a named value, the transfer must be\nrequested directly by invoking std::move(). The following example shows the use of\nstd::move to transfer ownership of a dynamic object into a thread:\nvoid process_big_object(std::unique_ptr<big_object>);\nstd::unique_ptr<big_object> p(new big_object);\np->prepare_data(42);\nstd::thread t(process_big_object,std::move(p));\nBy specifying std::move(p) in the std::thread constructor, the ownership of big_\nobject is transferred first into internal storage for the newly created thread and then\ninto process_big_object.\n Several of the classes in the C++ Standard Library exhibit the same ownership\nsemantics as std::unique_ptr, and std::thread is one of them. Though std::thread\ninstances don’t own a dynamic object in the same way as std::unique_ptr does, they do\n",
      "content_length": 2719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "27\nTransferring ownership of a thread\nown a resource: each instance is responsible for managing a thread of execution. This\nownership can be transferred between instances, because instances of std::thread are\nmovable, even though they aren’t copyable. This ensures that only one object is associ-\nated with a particular thread of execution at any one time while allowing program-\nmers the option of transferring that ownership between objects.\n2.3\nTransferring ownership of a thread\nSuppose you want to write a function that creates a thread to run in the background,\nbut passes ownership of the new thread back to the calling function rather than wait-\ning for it to complete; or maybe you want to do the reverse: create a thread and pass\nownership in to some function that should wait for it to complete. In either case, you\nneed to transfer ownership from one place to another.\n This is where the move support of std::thread comes in. As described in the pre-\nvious section, many resource-owning types in the C++ Standard Library, such as\nstd::ifstream and std::unique_ptr, are movable but not copyable, and std::thread\nis one of them. This means that the ownership of a particular thread of execution can\nbe moved between std::thread instances, as in the following example. The example\nshows the creation of two threads of execution and the transfer of ownership of those\nthreads among three std::thread instances, t1, t2, and t3:\nvoid some_function();\nvoid some_other_function();\nstd::thread t1(some_function);         \nstd::thread t2=std::move(t1);                \nt1=std::thread(some_other_function);   \nstd::thread t3;                              \nt3=std::move(t2);                      \nt1=std::move(t3);          \nFirst, a new thread is started and associated with t1. Ownership is then transferred\nover to t2 when t2 is constructed, by invoking std::move() to explicitly move owner-\nship. At this point, t1 no longer has an associated thread of execution; the thread run-\nning some_function is now associated with t2.\n Then, a new thread is started and associated with a temporary std::thread object.\nThe subsequent transfer of ownership into t1 doesn’t require a call to std::move() to\nexplicitly move ownership, because the owner is a temporary object—moving from\ntemporaries is automatic and implicit.\n t3 is default-constructed, which means that it’s created without any associated thread\nof execution. Ownership of the thread currently associated with t2 is transferred into\nt3, again with an explicit call to std::move(), because t2 is a named object. After all\nthese moves, t1 is associated with the thread running some_other_function, t2 has no\nassociated thread, and t3 is associated with the thread running some_function.\n The final move transfers ownership of the thread running some_function back to\nt1 where it started. But in this case t1 already had an associated thread (which was run-\nning some_other_function), so std::terminate() is called to terminate the program.\nThis assignment \nwill terminate the \nprogram!\n",
      "content_length": 3043,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "28\nCHAPTER 2\nManaging threads\nThis is done for consistency with the std::thread destructor. You saw in section 2.1.1\nthat you must explicitly wait for a thread to complete or detach it before destruction,\nand the same applies to assignment: you can’t just drop a thread by assigning a new\nvalue to the std::thread object that manages it.\n The move support in std::thread means that ownership can readily be trans-\nferred out of a function, as shown in the following listing.\nstd::thread f()\n{\n    void some_function();\n    return std::thread(some_function);\n}\nstd::thread g()\n{\n    void some_other_function(int);\n    std::thread t(some_other_function,42);\n    return t;\n}\nLikewise, if ownership should be transferred into a function, it can accept an instance\nof std::thread by value as one of the parameters, as shown here:\nvoid f(std::thread t);\nvoid g()\n{\n    void some_function();\n    f(std::thread(some_function));\n    std::thread t(some_function);\n    f(std::move(t));\n}\nOne benefit of the move support of std::thread is that you can build on the\nthread_guard class from listing 2.3 and have it take ownership of the thread. This\navoids any unpleasant consequences should the thread_guard object outlive the thread\nit was referencing, and it also means that no one else can join or detach the thread\nonce ownership has been transferred into the object. Because this would primarily be\naimed at ensuring that threads are completed before a scope is exited, I named this\nclass scoped_thread. The implementation is shown in the following listing, along with\na simple example.\nclass scoped_thread\n{\n    std::thread t;\npublic:\n    explicit scoped_thread(std::thread t_):        \n        t(std::move(t_))\nListing 2.5\nReturning a std::thread from a function\nListing 2.6\nscoped_thread and example usage\n",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "29\nTransferring ownership of a thread\n    {\n        if(!t.joinable())                          \n            throw std::logic_error(“No thread”);\n    }\n    ~scoped_thread()\n    {\n        t.join();       \n    }\n    scoped_thread(scoped_thread const&)=delete;\n    scoped_thread& operator=(scoped_thread const&)=delete;\n};\nstruct func;    \nvoid f()\n{\n    int some_local_state;\n    scoped_thread t{std::thread(func(some_local_state))};   \n    do_something_in_current_thread();\n}                                          \nThe example is similar to listing 2.3, but the new thread is passed in directly to\nscoped_thread rather than having to create a separate named variable for it. When\nthe initial thread reaches the end of f, the scoped_thread object is destroyed and then\njoins with the thread supplied to the constructor. Whereas with the thread_guard class\nfrom listing 2.3 the destructor had to check that the thread was still joinable, you can\ndo that in the constructor and throw an exception if it’s not.\n One of the proposals for C++17 was for a joining_thread class that would be simi-\nlar to std::thread, except that it would automatically join in the destructor much like\nscoped_thread does. This didn't get consensus in the committee, so it wasn’t accepted\ninto the standard (though it’s still on track for C++20 as std::jthread), but it’s rela-\ntively easy to write. One possible implementation is shown in the next listing.\nclass joining_thread\n{\n    std::thread t;\npublic:\n    joining_thread() noexcept=default;\n    template<typename Callable,typename ... Args>\n    explicit joining_thread(Callable&& func,Args&& ... args):\n        t(std::forward<Callable>(func),std::forward<Args>(args)...)\n    {}\n    explicit joining_thread(std::thread t_) noexcept:\n        t(std::move(t_))\n    {}\n    joining_thread(joining_thread&& other) noexcept:\n        t(std::move(other.t))\n    {}\n    joining_thread& operator=(joining_thread&& other) noexcept\n    {\n        if(joinable())\n            join();\nListing 2.7\nA joining_thread class\nSee listing 2.1\n",
      "content_length": 2049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "30\nCHAPTER 2\nManaging threads\n        t=std::move(other.t);\n        return *this;\n    }\n    joining_thread& operator=(std::thread other) noexcept\n    {\n        if(joinable())\n            join();\n        t=std::move(other);\n        return *this;\n    }\n    ~joining_thread() noexcept\n    {\n        if(joinable())\n            join();\n    }\n    void swap(joining_thread& other) noexcept\n    {\n        t.swap(other.t);\n    }\n    std::thread::id get_id() const noexcept{\n        return t.get_id();\n    }\n    bool joinable() const noexcept\n    {\n        return t.joinable();\n    }\n    void join()\n    {\n        t.join();\n    }\n    void detach()\n    {\n        t.detach();\n    }\n    std::thread& as_thread() noexcept\n    {\n        return t;\n    }\n    const std::thread& as_thread() const noexcept\n    {\n        return t;\n    }\n};\nThe move support in std::thread also allows for containers of std::thread objects,\nif those containers are move-aware (like the updated std::vector<>). This means\nthat you can write code like that in the following listing, which spawns a number of\nthreads and then waits for them to finish.\nvoid do_work(unsigned id);\nvoid f()\nListing 2.8\nSpawns some threads and waits for them to finish\n",
      "content_length": 1209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "31\nChoosing the number of threads at runtime\n{\n    std::vector<std::thread> threads;\n    for(unsigned i=0;i<20;++i)\n    {\n        threads.emplace_back(do_work,i);     \n    }\n    for(auto& entry: threads)                \n        entry.join();\n}\nIf the threads are being used to subdivide the work of an algorithm, this is often what’s\nrequired; before returning to the caller, all threads must have finished. The simple\nstructure of listing 2.8 implies that the work done by the threads is self-contained, and\nthe result of their operations is purely the side effects on shared data. If f() were to\nreturn a value to the caller that depended on the results of the operations performed\nby these threads, then as written, this return value would have to be determined by\nexamining the shared data after the threads had terminated. Alternative schemes for\ntransferring the results of operations between threads are discussed in chapter 4.\n Putting std::thread objects in a std::vector is a step toward automating the\nmanagement of those threads: rather than creating separate variables for those\nthreads and joining with them directly, they can be treated as a group. You can take\nthis a step further by creating a dynamic number of threads determined at runtime,\nrather than creating a fixed number, as in listing 2.8.\n2.4\nChoosing the number of threads at runtime\nOne feature of the C++ Standard Library that helps here is std::thread::hardware_\nconcurrency(). This function returns an indication of the number of threads that can\ntruly run concurrently for a given execution of a program. On a multicore system it\nmight be the number of CPU cores, for example. This is only a hint, and the function\nmight return 0 if this information isn’t available, but it can be a useful guide for split-\nting a task among threads.\n Listing 2.9 shows a simple implementation of a parallel version of std::accumulate.\nIn real code you'll probably want to use the parallel version of std::reduce described\nin chapter 10, rather than implementing it yourself, but this illustrates the basic idea.\nIt divides the work among the threads, with a minimum number of elements per\nthread in order to avoid the overhead of too many threads. Note that this implementa-\ntion assumes that none of the operations will throw an exception, even though excep-\ntions are possible; the std::thread constructor will throw if it can’t start a new thread\nof execution, for example. Handling exceptions in such an algorithm is beyond the\nscope of this simple example and will be covered in chapter 8.\ntemplate<typename Iterator,typename T>\nstruct accumulate_block\nListing 2.9\nA naïve parallel version of std::accumulate\nSpawns threads\nCalls join() on each \nthread in turn\n",
      "content_length": 2733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "32\nCHAPTER 2\nManaging threads\n{\n    void operator()(Iterator first,Iterator last,T& result)\n    {\n        result=std::accumulate(first,last,result);\n    }\n};\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)                                            \n        return init;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;    \n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=            \n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;      \n    std::vector<T> results(num_threads);\n    std::vector<std::thread>  threads(num_threads-1);       \n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_end=block_start;\n        std::advance(block_end,block_size);                 \n        threads[i]=std::thread(                             \n            accumulate_block<Iterator,T>(),\n            block_start,block_end,std::ref(results[i]));\n        block_start=block_end;                              \n    }\n    accumulate_block<Iterator,T>()(\n        block_start,last,results[num_threads-1]);    \n    \n    for(auto& entry: threads)\n           entry.join();                             \n    return std::accumulate(results.begin(),results.end(),init);   \n}\nAlthough this is a long function, it’s straightforward. If the input range is empty, you\nreturn the initial value supplied as the init parameter value. Otherwise, there’s at\nleast one element in the range, so you can divide the number of elements to process\nby the minimum block size in order to give the maximum number of threads . This is\nto avoid creating 32 threads on a 32-core machine when you have only five values in\nthe range.\n The number of threads to run is the minimum of your calculated maximum and\nthe number of hardware threads. You don’t want to run more threads than the hard-\nware can support (which is called oversubscription), because the context switching will\nmean that more threads will decrease the performance. If the call to std::thread::\nhardware_concurrency() returned 0, you’d substitute a number of your choice; in\n",
      "content_length": 2410,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "33\nChoosing the number of threads at runtime\nthis case I’ve chosen 2. You don’t want to run too many threads because that would\nslow things down on a single-core machine, but likewise you don’t want to run too few\nbecause you’d be passing up the available concurrency.\n The number of entries for each thread to process is the length of the range\ndivided by the number of threads. If you’re worrying about cases where the number\ndoesn’t divide evenly, don’t—you’ll handle that later.\n Now that you know how many threads you have, you can create a std::vector<T>\nfor the intermediate results and a std::vector<std::thread> for the threads. Note\nthat you need to launch one fewer thread than num_threads, because you already\nhave one.\n Launching the threads is a simple loop: advance the block_end iterator to the end\nof the current block and launch a new thread to accumulate the results for this block.\nThe start of the next block is the end of this one.\n After you’ve launched all the threads, this thread can then process the final block.\nThis is where you take account of any uneven division: you know the end of the final\nblock must be last, and it doesn’t matter how many elements are in that block.\n Once you’ve accumulated the results for the last block, you can wait for all the\nthreads you spawned with std::for_each, as in listing 2.8, and then add up the results\nwith a final call to std::accumulate.\n Before you leave this example, it’s worth pointing out that where the addition\noperator for the type T isn’t associative (such as for float or double), the results of\nthis parallel_accumulate may vary from those of std::accumulate because of the\ngrouping of the range into blocks. Also, the requirements on the iterators are slightly\nmore stringent: they must be at least forward iterators, whereas std::accumulate can\nwork with single-pass input iterators, and T must be default-constructible so that you can\ncreate the results vector. These sorts of requirement changes are common with par-\nallel algorithms; by their nature they’re different in order to make them parallel, and\nthis has consequences for the results and requirements. Implementing parallel algo-\nrithms is covered in more depth in chapter 8, and chapter 10 covers the standard sup-\nplied ones from C++17 (the equivalent to the parallel_accumulate described here\nbeing the parallel form of std::reduce). It’s also worth noting that because you can’t\nreturn a value directly from a thread, you must pass in a reference to the relevant\nentry in the results vector. Alternative ways of returning results from threads are\naddressed through the use of futures in chapter 4.\n In this case, all the information required by each thread was passed in when the\nthread was started, including the location in which to store the result of its calculation.\nThis isn’t always the case; sometimes it’s necessary to be able to identify the threads in\nsome way for part of the processing. You could pass in an identifying number, such as\nthe value of i in listing 2.8, but if the function that needs the identifier is several levels\ndeep in the call stack and could be called from any thread, it’s inconvenient to have to\ndo it that way. When we were designing the C++ Standard Library we foresaw this\nneed, so each thread has a unique identifier.\n",
      "content_length": 3310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "34\nCHAPTER 2\nManaging threads\n2.5\nIdentifying threads\nThread identifiers are of type std::thread::id and can be retrieved in two ways.\nFirst, the identifier for a thread can be obtained from its associated std::thread object\nby calling the get_id() member function. If the std::thread object doesn’t have an\nassociated thread of execution, the call to get_id() returns a default-constructed\nstd::thread::id object, which indicates “not any thread.” Alternatively, the identifier\nfor the current thread can be obtained by calling std::this_thread:: get_id(),\nwhich is also defined in the <thread> header.\n Objects of type std::thread::id can be freely copied and compared; they wouldn’t\nbe of much use as identifiers otherwise. If two objects of type std::thread::id are\nequal, they represent the same thread, or both are holding the “not any thread” value.\nIf two objects aren’t equal, they represent different threads, or one represents a\nthread and the other is holding the “not any thread” value.\n The C++ Standard Library doesn’t limit you to checking whether thread identifiers\nare the same or not; objects of type std::thread::id offer the complete set of com-\nparison operators, which provide a total ordering for all distinct values. This allows\nthem to be used as keys in associative containers, or sorted, or compared in any other\nway that you as a programmer may see fit. The comparison operators provide a total\norder for all non-equal values of std::thread::id, so they behave as you’d intuitively\nexpect: if a<b and b<c, then a<c, and so forth. The Standard Library also provides\nstd::hash<std::thread::id> so that values of type std::thread::id can be used as\nkeys in the new unordered associative containers too.\n Instances of std::thread::id are often used to check whether a thread needs to\nperform some operation. For example, if threads are used to divide work, as in listing\n2.9, the initial thread that launched the others might need to perform its work slightly\ndifferently in the middle of the algorithm. In this case it could store the result of\nstd::this_thread::get_id() before launching the other threads, and then the core\npart of the algorithm (which is common to all threads) could check its own thread ID\nagainst the stored value:\nstd::thread::id master_thread;\nvoid some_core_part_of_algorithm()\n{\n    if(std::this_thread::get_id()==master_thread)\n    {\n        do_master_thread_work();\n    }\n    do_common_work();\n}\nAlternatively, the std::thread::id of the current thread could be stored in a data\nstructure as part of an operation. Later operations on that same data structure could\nthen check the stored ID against the ID of the thread performing the operation to\ndetermine what operations are permitted/required.\n",
      "content_length": 2751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "35\nSummary\n Similarly, thread IDs could be used as keys into associative containers where spe-\ncific data needs to be associated with a thread and alternative mechanisms such as\nthread-local storage aren’t appropriate. Such a container could, for example, be used\nby a controlling thread to store information about each of the threads under its con-\ntrol or for passing information between threads.\n The idea is that std::thread::id will suffice as a generic identifier for a thread in\nmost circumstances; it’s only if the identifier has semantic meaning associated with it\n(such as being an index into an array) that alternatives should be necessary. You can\neven write out an instance of std::thread::id to an output stream such as std::cout:\nstd::cout<<std::this_thread::get_id();\nThe exact output you get is strictly implementation-dependent; the only guarantee\ngiven by the standard is that thread IDs that compare as equal should produce the\nsame output, and those that aren’t equal should give different output. This is there-\nfore primarily useful for debugging and logging, but the values have no semantic\nmeaning, so there’s not much more that could be said anyway.\nSummary\nIn this chapter, I covered the basics of thread management with the C++ Standard\nLibrary: starting threads, waiting for them to finish, and not waiting for them to finish\nbecause you want them to run in the background. We also saw how to pass arguments\ninto the thread function when a thread is started, how to transfer the responsibility for\nmanaging a thread from one part of the code to another, and how groups of threads\ncan be used to divide work. Finally, we discussed identifying threads in order to associ-\nate data or behavior with specific threads that’s inconvenient to associate through\nalternative means. Although you can do quite a lot with purely independent threads\nthat each operate on separate data, sometimes it’s desirable to share data among\nthreads while they’re running. Chapter 3 discusses the issues surrounding sharing\ndata directly among threads, and chapter 4 covers more general issues surrounding\nsynchronizing operations with and without shared data.\n",
      "content_length": 2166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "36\nSharing data\nbetween threads\nOne of the key benefits of using threads for concurrency is the potential to easily\nand directly share data between them, so now that we’ve covered starting and man-\naging threads, let’s look at the issues surrounding shared data.\n Imagine for a moment that you’re sharing an apartment with a friend. There’s\nonly one kitchen and one bathroom. Unless you’re particularly friendly, you can’t\nboth use the bathroom at the same time, and if your roommate occupies the bath-\nroom for a long time, it can be frustrating if you need to use it. Likewise, though it\nmight be possible to both cook meals at the same time, if you have a combined oven\nand grill, it’s not going to end well if one of you tries to grill some sausages at the\nsame time as the other is baking a cake. Furthermore, we all know the frustration of\nsharing a space and getting halfway through a task only to find that someone has bor-\nrowed something we need or changed something from the way we left it.\n It’s the same with threads. If you’re sharing data between threads, you need to\nhave rules for which thread can access which bit of data when, and how any updates\nThis chapter covers\nProblems with sharing data between threads\nProtecting data with mutexes\nAlternative facilities for protecting shared data\n",
      "content_length": 1311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "37\nProblems with sharing data between threads\nare communicated to the other threads that care about that data. The ease with which\ndata can be shared between multiple threads in a single process is not only a benefit—\nit can also be a big drawback. Incorrect use of shared data is one of the biggest causes\nof concurrency-related bugs, and the consequences can be far worse than sausage-\nflavored cakes.\n This chapter is about sharing data safely between threads in C++, avoiding the\npotential problems that can arise, and maximizing the benefits.\n3.1\nProblems with sharing data between threads\nWhen it comes down to it, the problems with sharing data between threads are all due\nto the consequences of modifying data. If all shared data is read-only, there’s no problem,\nbecause the data read by one thread is unaffected by whether or not another thread is reading the\nsame data. But if data is shared between threads, and one or more threads start modify-\ning the data, there’s a lot of potential for trouble. In this case, you must take care to\nensure that everything works out OK.\n One concept that’s widely used to help programmers reason about their code is\ninvariants—statements that are always true about a particular data structure, such as\n“this variable contains the number of items in the list.” These invariants are often bro-\nken during an update, especially if the data structure is of any complexity or the\nupdate requires modification of more than one value.\n Consider a doubly linked list, where each node holds a pointer to both the next\nnode in the list and the previous one. One of the invariants is that if you follow a\n“next” pointer from one node (A) to another (B), the “previous” pointer from that\nnode (B) points back to the first node (A). In order to remove a node from the list,\nthe nodes on either side have to be updated to point to each other. Once one has\nbeen updated, the invariant is broken until the node on the other side has been\nupdated too; after the update has completed, the invariant holds again.\n The steps in deleting an entry from such a list are shown in figure 3.1:\na\nIdentify the node to delete: N.\nb\nUpdate the link from the node prior to N to point to the node after N.\nc\nUpdate the link from the node after N to point to the node prior to N.\nd\nDelete node N.\nAs you can see in figure 3.1, between steps b and c, the links going in one direction\nare inconsistent with the links going in the opposite direction, and the invariant is\nbroken.\n The simplest potential problem with modifying data that’s shared between threads\nis that of broken invariants. If you don’t do anything special to ensure otherwise, if\none thread is reading the doubly linked list while another is removing a node, it’s\nquite possible for the reading thread to see the list with a node only partially removed\n(because only one of the links has been changed, as in step b of figure 3.1), so the\ninvariant is broken. The consequences of this broken invariant can vary; if the other\n",
      "content_length": 3006,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "38\nCHAPTER 3\nSharing data between threads\nthread is reading the list items from left to right in the diagram, it will skip the node\nbeing deleted. On the other hand, if the second thread is trying to delete the right-\nmost node in the diagram, it might end up permanently corrupting the data structure\nand eventually crashing the program. Whatever the outcome, this is an example of\none of the most common causes of bugs in concurrent code: a race condition.\n3.1.1\nRace conditions\nSuppose you’re buying tickets to see a movie at the movie theater. If it’s a big theater,\nmultiple cashiers will be taking money so more than one person can buy tickets at the\nsame time. If someone at another cashier’s desk is also buying tickets for the same\nFigure 3.1\nDeleting a node from a doubly linked list\n",
      "content_length": 794,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "39\nProblems with sharing data between threads\nmovie as you are, which seats are available for you to choose from depends on\nwhether the other person books first or you do. If there are only a few seats left, this\ndifference can be quite crucial: it might literally be a race to see who gets the last tick-\nets. This is an example of a race condition: which seats you get (or even whether you get\ntickets) depends on the relative ordering of the two purchases.\n In concurrency, a race condition is anything where the outcome depends on the\nrelative ordering of execution of operations on two or more threads; the threads race\nto perform their respective operations. Most of the time, this is quite benign because\nall possible outcomes are acceptable, even though they may change with different rel-\native orderings. For example, if two threads are adding items to a queue for process-\ning, it generally doesn’t matter which item gets added first, provided that the\ninvariants of the system are maintained. It’s when the race condition leads to broken\ninvariants that there’s a problem, such as with the doubly linked list example men-\ntioned. When talking about concurrency, the term race condition is usually used to mean\na problematic race condition; benign race conditions aren’t so interesting and aren’t a\ncause of bugs. The C++ Standard also defines the term data race to mean the specific\ntype of race condition that arises because of concurrent modification to a single object\n(see section 5.1.2 for details); data races cause the dreaded undefined behavior.\n Problematic race conditions typically occur where completing an operation requires\nmodification of two or more distinct pieces of data, such as the two link pointers in\nthe example. Because the operation must access two separate pieces of data, these\nmust be modified in separate instructions, and another thread could potentially\naccess the data structure when only one of them has been completed. Race conditions\ncan often be hard to find and hard to duplicate because the window of opportunity is\nsmall. If the modifications are done as consecutive CPU instructions, the chance of\nthe problem exhibiting on any one run-through is small, even if the data structure is\nbeing accessed by another thread concurrently. As the load on the system increases,\nand the number of times the operation is performed increases, the chance of the\nproblematic execution sequence occurring also increases. It’s almost inevitable that\nsuch problems will show up at the most inconvenient time. Because race conditions\nare generally timing-sensitive, they can often disappear entirely when the application\nis run under the debugger, because the debugger affects the timing of the program,\neven if only slightly.\n If you’re writing multithreaded programs, race conditions can easily be the bane\nof your existence; a great deal of the complexity in writing software that uses concur-\nrency comes from avoiding problematic race conditions.\n3.1.2\nAvoiding problematic race conditions\nThere are several ways to deal with problematic race conditions. The simplest option\nis to wrap your data structure with a protection mechanism to ensure that only the\nthread performing a modification can see the intermediate states where the invariants\nare broken. From the point of view of other threads accessing that data structure, such\n",
      "content_length": 3371,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "40\nCHAPTER 3\nSharing data between threads\nmodifications either haven’t started or have completed. The C++ Standard Library\nprovides several of these mechanisms, which are described in this chapter.\n Another option is to modify the design of your data structure and its invariants so\nthat modifications are done as a series of indivisible changes, each of which preserves\nthe invariants. This is generally referred to as lock-free programming and is difficult to get\nright. If you’re working at this level, the nuances of the memory model and identify-\ning which threads can potentially see which set of values can get complicated. The\nmemory model is covered in chapter 5, and lock-free programming is discussed in\nchapter 7.\n Another way of dealing with race conditions is to handle the updates to the data\nstructure as a transaction, just as updates to a database are done within a transaction.\nThe required series of data modifications and reads is stored in a transaction log and\nthen committed in a single step. If the commit can’t proceed because the data struc-\nture has been modified by another thread, the transaction is restarted. This is termed\nsoftware transactional memory (STM), and it’s an active research area at the time of writ-\ning. It won’t be covered in this book, because there’s no direct support for STM in\nC++ (though there is a Technical Specification for Transactional Memory Extensions\nto C++1). But the basic idea of doing something privately and then committing in a\nsingle step is something that I’ll come back to later.\n The most basic mechanism for protecting shared data provided by the C++ Stan-\ndard is the mutex, so we’ll look at that first.\n3.2\nProtecting shared data with mutexes\nSo, you have a shared data structure such as the linked list from the previous section,\nand you want to protect it from race conditions and the potential broken invariants\nthat can ensue. Wouldn’t it be nice if you could mark all the pieces of code that access\nthe data structure as mutually exclusive, so that if any thread was running one of them,\nany other thread that tried to access that data structure had to wait until the first\nthread was finished? That would make it impossible for a thread to see a broken\ninvariant except when it was the thread doing the modification.\n Well, this isn’t a fairy tale wish—it’s precisely what you get if you use a synchroniza-\ntion primitive called a mutex (mutual exclusion). Before accessing a shared data struc-\nture, you lock the mutex associated with that data, and when you’ve finished accessing\nthe data structure, you unlock the mutex. The Thread Library then ensures that once\none thread has locked a specific mutex, all other threads that try to lock the same\nmutex have to wait until the thread that successfully locked the mutex unlocks it. This\nensures that all threads see a self-consistent view of the shared data, without any bro-\nken invariants.\n Mutexes are the most general of the data-protection mechanisms available in C++,\nbut they’re not a silver bullet; it’s important to structure your code to protect the right\n1 ISO/IEC TS 19841:2015—Technical Specification for C++ Extensions for Transactional Memory http://www\n.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=66343.\n",
      "content_length": 3276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "41\nProtecting shared data with mutexes\ndata (see section 3.2.2) and avoid race conditions inherent in your interfaces (see sec-\ntion 3.2.3). Mutexes also come with their own problems in the form of a deadlock (see\nsection 3.2.4) and protecting either too much or too little data (see section 3.2.8).\nLet’s start with the basics.\n3.2.1\nUsing mutexes in C++\nIn C++, you create a mutex by constructing an instance of std::mutex, lock it with a\ncall to the lock() member function, and unlock it with a call to the unlock() member\nfunction. But it isn’t recommended practice to call the member functions directly,\nbecause this means that you have to remember to call unlock() on every code path\nout of a function, including those due to exceptions. Instead, the Standard C++\nLibrary provides the std::lock_guard class template, which implements that RAII\nidiom for a mutex; it locks the supplied mutex on construction and unlocks it on\ndestruction, ensuring a locked mutex is always correctly unlocked. The following list-\ning shows how to protect a list that can be accessed by multiple threads using\nstd::mutex, along with std::lock_guard. Both of these are declared in the <mutex>\nheader.\n#include <list>\n#include <mutex>\n#include <algorithm>\nstd::list<int> some_list;    \nstd::mutex some_mutex;        \nvoid add_to_list(int new_value)\n{\n    std::lock_guard<std::mutex> guard(some_mutex);   \n    some_list.push_back(new_value);\n}\nbool list_contains(int value_to_find) \n{\n    std::lock_guard<std::mutex> guard(some_mutex);   \n    return std::find(some_list.begin(),some_list.end(),value_to_find)\n        != some_list.end();\n}\nIn listing 3.1, there’s a single global variable B, and it’s protected with a correspond-\ning global instance of std::mutex c. The use of std::lock_guard<std::mutex> in\nadd_to_list() d, and again in list_contains() e, means that the accesses in\nthese functions are mutually exclusive: list_contains() will never see the list partway\nthrough a modification by add_to_list().\n C++17 has a new feature called class template argument deduction, which means\nthat for simple class templates like std::lock_guard, the template argument list can\noften be omitted. d and e can be reduced to\nstd::lock_guard guard(some_mutex);\nListing 3.1\nProtecting a list with a mutex\nB\nc\nd\ne\n",
      "content_length": 2291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "42\nCHAPTER 3\nSharing data between threads\non a C++17 compiler. As we will see in section 3.2.4, C++17 also introduces an enhanced\nversion of lock guard called std::scoped_lock, so in a C++17 environment, this may\nwell be written as\nstd::scoped_lock guard(some_mutex);\nFor clarity of code and compatibility with older compilers, I’ll continue to use\nstd::lock_guard and specify the template arguments in other code snippets.\n Although there are occasions where this use of global variables is appropriate, in\nthe majority of cases it’s common to group the mutex and the protected data together\nin a class rather than use global variables. This is a standard application of object-\noriented design rules: by putting them in a class, you’re clearly marking them as\nrelated, and you can encapsulate the functionality and enforce the protection. In this\ncase, the add_to_list and list_contains functions would become member func-\ntions of the class, and the mutex and protected data would both become private\nmembers of the class, making it much easier to identify which code has access to the\ndata and thus which code needs to lock the mutex. If all the member functions of\nthe class lock the mutex before accessing any other data members and unlock it when\ndone, the data is nicely protected from all comers.\n Well, that’s not quite true, as the astute among you will have noticed: if one of the\nmember functions returns a pointer or reference to the protected data, then it doesn’t\nmatter that the member functions all lock the mutex in a nice, orderly fashion, because\nyou’ve blown a big hole in the protection. Any code that has access to that pointer or ref-\nerence can now access (and potentially modify) the protected data without locking the mutex.\nProtecting data with a mutex therefore requires careful interface design to ensure\nthat the mutex is locked before there’s any access to the protected data and that there\nare no backdoors.\n3.2.2\nStructuring code for protecting shared data\nAs you’ve seen, protecting data with a mutex is not quite as easy as slapping an\nstd::lock_guard object in every member function; one stray pointer or reference,\nand all that protection is for nothing. At one level, checking for stray pointers or refer-\nences is easy; as long as none of the member functions return a pointer or reference\nto the protected data to their caller either via their return value or via an out parame-\nter, the data is safe. If you dig a little deeper, it’s not that straightforward—nothing\never is. As well as checking that the member functions don’t pass out pointers or refer-\nences to their callers, it’s also important to check that they don’t pass these pointers or\nreferences in to functions they call that aren’t under your control. This is just as dan-\ngerous: those functions might store the pointer or reference in a place where it can\nlater be used without the protection of the mutex. Particularly dangerous in this\nregard are functions that are supplied at runtime via a function argument or other\nmeans, as in the next listing.\n",
      "content_length": 3062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "43\nProtecting shared data with mutexes\nclass some_data\n{\n    int a;\n    std::string b;\npublic:\n    void do_something();\n};\nclass data_wrapper\n{\nprivate:\n    some_data data;\n    std::mutex m;\npublic:\n    template<typename Function>\n    void process_data(Function func)\n    {\n        std::lock_guard<std::mutex> l(m);\n        func(data);                      \n    }\n};\nsome_data* unprotected;\nvoid malicious_function(some_data& protected_data)\n{\n    unprotected=&protected_data;\n}\ndata_wrapper x;\nvoid foo()\n{\n    x.process_data(malicious_function);    \n    unprotected->do_something();         \n}\nIn this example, the code in process_data looks harmless enough, nicely protected\nwith std::lock_guard, but the call to the user-supplied func function B means that\nfoo can pass in malicious_function to bypass the protection c and then call\ndo_something() without the mutex being locked d.\n Fundamentally, the problem with this code is that it hasn’t done what you set out\nto do: mark all the pieces of code that access the data structure as mutually exclusive. In\nthis case, it missed the code in foo() that calls unprotected->do_something().\nUnfortunately, this part of the problem isn’t something the C++ Thread Library can\nhelp you with; it’s up to you as programmers to lock the right mutex to protect your\ndata. On the upside, you have a guideline to follow, which will help you in these cases:\nDon’t pass pointers and references to protected data outside the scope of the lock, whether by\nreturning them from a function, storing them in externally visible memory, or passing them as\narguments to user-supplied functions.\n Although this is a common mistake when trying to use mutexes to protect shared\ndata, it’s far from the only potential pitfall. As you’ll see in the next section, it’s still\npossible to have race conditions, even when data is protected with a mutex.\nListing 3.2\nAccidentally passing out a reference to protected data\nPass “protected” data to \nuser-supplied function\nB\nPass in a malicious \nfunction\nc\nUnprotected access \nto protected data\nd\n",
      "content_length": 2064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "44\nCHAPTER 3\nSharing data between threads\n3.2.3\nSpotting race conditions inherent in interfaces\nJust because you’re using a mutex or other mechanism to protect shared data, it\ndoesn’t mean you’re protected from race conditions; you still have to ensure that the\nappropriate data is protected. Consider the doubly linked list example again. In order\nfor a thread to safely delete a node, you need to ensure that you’re preventing concur-\nrent accesses to three nodes: the node being deleted and the nodes on either side. If\nyou protected access to the pointers of each node individually, you’d be no better off\nthan with code that used no mutexes, because the race condition could still happen—\nit’s not the individual nodes that need protecting for the individual steps but the\nwhole data structure, for the whole delete operation. The easiest solution in this case\nis to have a single mutex that protects the entire list, as in listing 3.1.\n Just because individual operations on the list are safe, you’re not out of the woods\nyet; you can still get race conditions, even with a simple interface. Consider a stack\ndata structure like the std::stack container adapter shown in listing 3.3. Aside from\nthe constructors and swap(), there are only five things you can do to a std::stack:\nyou can push() a new element onto the stack, pop() an element off the stack, read the\ntop() element, check whether it’s empty(), and read the number of elements—the\nsize() of the stack. If you change top() so that it returns a copy rather than a refer-\nence (so you’re following the guideline from section 3.2.2) and protect the internal\ndata with a mutex, this interface is still inherently subject to race conditions. This\nproblem is not unique to a mutex-based implementation; it’s an interface problem, so\nthe race conditions would still occur with a lock-free implementation.\ntemplate<typename T,typename Container=std::deque<T> >\nclass stack\n{\npublic:\n    explicit stack(const Container&);\n    explicit stack(Container&& = Container());\n    template <class Alloc> explicit stack(const Alloc&);\n    template <class Alloc> stack(const Container&, const Alloc&);\n    template <class Alloc> stack(Container&&, const Alloc&);\n    template <class Alloc> stack(stack&&, const Alloc&);\n    bool empty() const;\n    size_t size() const;\n    T& top();\n    T const& top() const;\n    void push(T const&);\n    void push(T&&);\n    void pop();\n    void swap(stack&&);\n    template <class... Args> void emplace(Args&&... args);    \n};\nThe problem here is that the results of empty() and size() can’t be relied on. Although\nthey might be correct at the time of the call, once they’ve returned, other threads are\nListing 3.3\nThe interface to the std::stack container adapter\nNew in \nC++14\n",
      "content_length": 2761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "45\nProtecting shared data with mutexes\nfree to access the stack and might push() new elements onto or pop() the existing\nones off of the stack before the thread that called empty() or size() could use that\ninformation.\n In particular, if the stack instance is not shared, it’s safe to check for empty() and\nthen call top() to access the top element if the stack is not empty, as follows:\nstack<int> s;\nif(!s.empty())   \n{\n    int const value=s.top();    \n    s.pop();               \n    do_something(value);\n}\nNot only is it safe in single-threaded code, it’s expected: calling top() on an empty\nstack is undefined behavior. With a shared stack object, this call sequence is no longer\nsafe, because there might be a call to pop() from another thread that removes the last\nelement in between the call to empty() B and the call to top() c. This is therefore a\nclassic race condition, and the use of a mutex internally to protect the stack contents\ndoesn’t prevent it; it’s a consequence of the interface.\n What’s the solution? Well, this problem happens as a consequence of the design of\nthe interface, so the solution is to change the interface. But that still begs the ques-\ntion: what changes need to be made? In the simplest case, you could declare that\ntop() will throw an exception if there aren’t any elements in the stack when it’s called.\nThough this directly addresses this issue, it makes for more cumbersome program-\nming, because now you need to be able to catch an exception, even if the call to\nempty() returned false. This makes the call to empty() an optimization to avoid the\noverhead of throwing an exception if the stack is already empty (though if the state\nchanges between the call to empty() and the call to top(), then the exception will still\nbe thrown), rather than a necessary part of the design.\n If you look closely at the previous snippet, there’s also potential for another race\ncondition, but this time between the call to top() c and the call to pop() d. Con-\nsider two threads running the previous snippet of code and both referencing the same\nstack object, s. This isn’t an unusual situation; when using threads for performance,\nit’s quite common to have several threads running the same code on different data,\nand a shared stack object is ideal for dividing work between them (though more com-\nmonly, a queue is used for this purpose—see the examples in chapters 6 and 7). Sup-\npose that initially the stack has two elements, so you don’t have to worry about the\nrace between empty() and top() on either thread, and consider the potential execu-\ntion patterns.\n If the stack is protected by a mutex internally, only one thread can be running a\nstack member function at any one time, so the calls get nicely interleaved, but the\ncalls to do_something() can run concurrently. One possible execution is shown in\ntable 3.1.\nb\nc\nd\n",
      "content_length": 2861,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "46\nCHAPTER 3\nSharing data between threads\nAs you can see, if these are the only threads running, there’s nothing in between the\ntwo calls to top() to modify the stack, so both threads will see the same value. Not only\nthat, but there are no calls to top() between the calls to pop(). Consequently, one of\nthe two values on the stack is discarded without ever having been read, whereas the\nother is processed twice. This is yet another race condition and far more insidious\nthan the undefined behavior of the empty()/top() race; there’s never anything obvi-\nously wrong going on, and the consequences of the bug are likely far removed from\nthe cause, although they obviously depend on exactly what do_something() does.\n This calls for a more radical change to the interface, one that combines the calls to\ntop() and pop() under the protection of the mutex. Tom Cargill2 pointed out that a\ncombined call can lead to issues if the copy constructor for the objects on the stack\ncan throw an exception. This problem was dealt with fairly comprehensively from an\nexception-safety point of view by Herb Sutter,3 but the potential for race conditions\nbrings something new to the mix.\n For those of you who aren’t aware of the issue, consider stack<vector<int>>.\nNow, a vector is a dynamically sized container, so when you copy a vector, the library\nhas to allocate some more memory from the heap in order to copy the contents. If the\nsystem is heavily loaded or there are significant resource constraints, this memory allo-\ncation can fail, so the copy constructor for vector might throw a std::bad_alloc\nexception. This is likely if the vector contains a lot of elements. If the pop() function\nwas defined to return the value popped, as well as remove it from the stack, you have a\npotential problem: the value being popped is returned to the caller only after the stack\nhas been modified, but the process of copying the data to return to the caller might\nthrow an exception. If this happens, the data popped is lost; it has been removed from\nTable 3.1\nA possible ordering of operations on a stack from two threads\nThread A\nThread B\nif(!s.empty())\nif(!s.empty())\n    int const value=s.top();\n    int const value=s.top();\n    s.pop();\n    do_something(value);\n    s.pop();\n    do_something(value);\n2 Tom Cargill, “Exception Handling: A False Sense of Security,” in C++ Report 6, no. 9 (November–December\n1994). Also available at http://www.informit.com/content/images/020163371x/supplements/Exception_\nHandling_Article.html.\n3 Herb Sutter, Exceptional C++: 47 Engineering Puzzles, Programming Problems, and Solutions (Addison Wes-\nley Professional, 1999).\n",
      "content_length": 2648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "47\nProtecting shared data with mutexes\nthe stack, but the copy was unsuccessful! The designers of the std::stack interface\nhelpfully split the operation in two: get the top element (top()) and then remove it\nfrom the stack (pop()), so that if you can’t safely copy the data, it stays on the stack. If\nthe problem was lack of heap memory, maybe the application can free some memory\nand try again.\n Unfortunately, it’s precisely this split that you’re trying to avoid in eliminating the\nrace condition! Thankfully, there are alternatives, but they aren’t without cost.\nOPTION 1: PASS IN A REFERENCE\nThe first option is to pass a reference to a variable in which you want to receive the\npopped value as an argument in the call to pop():\nstd::vector<int> result;\nsome_stack.pop(result);\nThis works well for many cases, but it has the distinct disadvantage that it requires the\ncalling code to construct an instance of the stack’s value type prior to the call, in order\nto pass this in as the target. For some types this is impractical, because constructing an\ninstance is expensive in terms of time or resources. For other types this isn’t always\npossible, because the constructors require parameters that aren’t necessarily avail-\nable at this point in the code. Finally, it requires that the stored type be assignable.\nThis is an important restriction: many user-defined types do not support assign-\nment, though they may support move construction or even copy construction (and\nallow return by value).\nOPTION 2: REQUIRE A NO-THROW COPY CONSTRUCTOR OR MOVE CONSTRUCTOR\nThere’s only an exception safety problem with a value-returning pop() if the return by\nvalue can throw an exception. Many types have copy constructors that don’t throw\nexceptions, and with the new rvalue-reference support in the C++ Standard (see\nappendix A, section A.1), many more types will have a move constructor that doesn’t\nthrow exceptions, even if their copy constructor does. One valid option is to restrict\nthe use of your thread-safe stack to those types that can safely be returned by value\nwithout throwing an exception.\n Although this is safe, it’s not ideal. Even though you can detect at compile time the\nexistence of a copy or move constructor that doesn’t throw an exception using the\nstd::is_nothrow_copy_constructible and std::is_nothrow_move_constructible\ntype traits, it’s quite limiting. There are many more user-defined types with copy con-\nstructors that can throw and don’t have move constructors than there are types with\ncopy and/or move constructors that can’t throw (although this might change as peo-\nple get used to the rvalue-reference support in C++11). It would be unfortunate if\nsuch types couldn’t be stored in your thread-safe stack.\nOPTION 3: RETURN A POINTER TO THE POPPED ITEM\nThe third option is to return a pointer to the popped item rather than return the item\nby value. The advantage here is that pointers can be freely copied without throwing an\n",
      "content_length": 2960,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "48\nCHAPTER 3\nSharing data between threads\nexception, so you’ve avoided Cargill’s exception problem. The disadvantage is that\nreturning a pointer requires a means of managing the memory allocated to the\nobject, and for simple types such as integers, the overhead of such memory manage-\nment can exceed the cost of returning the type by value. For any interface that uses\nthis option, std::shared_ptr would be a good choice of pointer type; not only does it\navoid memory leaks, because the object is destroyed once the last pointer is destroyed,\nbut the library is in full control of the memory allocation scheme and doesn’t have to\nuse new and delete. This can be important for optimization purposes: requiring that\neach object in the stack be allocated separately with new would impose quite an over-\nhead compared to the original non-thread-safe version.\nOPTION 4: PROVIDE BOTH OPTION 1 AND EITHER OPTION 2 OR 3\nFlexibility should never be ruled out, especially in generic code. If you’ve chosen\noption 2 or 3, it’s relatively easy to provide option 1 as well, and this provides users of\nyour code the ability to choose whichever option is most appropriate for them at little\nadditional cost.\nEXAMPLE DEFINITION OF A THREAD-SAFE STACK\nListing 3.4 shows the class definition for a stack with no race conditions in the\ninterface and that implements options 1 and 3: there are two overloads of pop(),\none that takes a reference to a location in which to store the value and one that\nreturns std::shared_ptr<>. It has a simple interface, with only two functions: push()\nand pop().\n#include <exception>\n#include <memory>               \nstruct empty_stack: std::exception\n{\n    const char* what() const noexcept;\n};\ntemplate<typename T>\nclass threadsafe_stack\n{\npublic:\n    threadsafe_stack();\n    threadsafe_stack(const threadsafe_stack&);     \n    threadsafe_stack& operator=(const threadsafe_stack&) = delete;   \n    void push(T new_value);\n    std::shared_ptr<T> pop();\n    void pop(T& value);\n    bool empty() const;\n};\nBy paring down the interface you allow for maximum safety; even operations on the\nwhole stack are restricted. The stack itself can’t be assigned, because the assignment\noperator is deleted B (see appendix A, section A.2), and there’s no swap() function.\nListing 3.4\nAn outline class definition for a thread-safe stack\nFor std::shared_ptr<>\nb\nAssignment\noperator is\ndeleted\n",
      "content_length": 2393,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "49\nProtecting shared data with mutexes\nIt can, however, be copied, assuming the stack elements can be copied. The pop()\nfunctions throw an empty_stack exception if the stack is empty, so everything still\nworks even if the stack is modified after a call to empty(). As mentioned in the\ndescription of option 3, the use of std::shared_ptr allows the stack to take care of\nthe memory-allocation issues and avoid excessive calls to new and delete if desired.\nYour five stack operations have now become three: push(), pop(), and empty(). Even\nempty() is superfluous. This simplification of the interface allows for better control\nover the data; you can ensure that the mutex is locked for the entirety of an operation.\nThe following listing shows a simple implementation that’s a wrapper around\nstd::stack<>.\n#include <exception>\n#include <memory>\n#include <mutex>\n#include <stack>\nstruct empty_stack: std::exception\n{\n    const char* what() const throw();\n};\ntemplate<typename T>\nclass threadsafe_stack\n{\nprivate:\n    std::stack<T> data;\n    mutable std::mutex m;\npublic:\n    threadsafe_stack(){}\n    threadsafe_stack(const threadsafe_stack& other)\n    {\n        std::lock_guard<std::mutex> lock(other.m);\n        data=other.data;                            \n    }\n    threadsafe_stack& operator=(const threadsafe_stack&) = delete;\n    void push(T new_value)\n    {\n        std::lock_guard<std::mutex> lock(m);\n        data.push(std::move(new_value));\n    }\n    std::shared_ptr<T> pop()\n    {\n        std::lock_guard<std::mutex> lock(m);\n        if(data.empty()) throw empty_stack();    \n        std::shared_ptr<T> const res(std::make_shared<T>(data.top()));   \n        data.pop();                                          \n        return res;\n    }\n    void pop(T& value)\n    {\n        std::lock_guard<std::mutex> lock(m);\n        if(data.empty()) throw empty_stack();\nListing 3.5\nA fleshed-out class definition for a thread-safe stack\nCopy performed \nin constructor \nbody\nb\nCheck for empty \nbefore trying to \npop value\nAllocate return value\nbefore modifying stack\n",
      "content_length": 2061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "50\nCHAPTER 3\nSharing data between threads\n        value=data.top();\n        data.pop();\n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lock(m);\n        return data.empty();\n    }\n};\nThis stack implementation is copyable—the copy constructor locks the mutex in the\nsource object and then copies the internal stack. You do the copy in the constructor\nbody B rather than the member initializer list in order to ensure that the mutex is\nheld across the copy.\n As the discussion of top() and pop() shows, problematic race conditions in inter-\nfaces arise because of locking at too small a granularity; the protection doesn’t cover\nthe entirety of the desired operation. Problems with mutexes can also arise from lock-\ning at too large a granularity; the extreme situation is a single global mutex that pro-\ntects all shared data. In a system where there’s a significant amount of shared data,\nthis can eliminate any performance benefits of concurrency, because the threads are\nforced to run one at a time, even when they’re accessing different bits of data. The\nfirst versions of the Linux kernel that were designed to handle multi-processor systems\nused a single global kernel lock. Although this worked, it meant that a two-processor\nsystem typically had much worse performance than two single-processor systems, and\nperformance on a four-processor system was nowhere near that of four single-processor\nsystems. There was too much contention for the kernel, so the threads running on the\nadditional processors were unable to perform useful work. Later revisions of the Linux\nkernel have moved to a more fine-grained locking scheme, so the performance of a\nfour-processor system is much nearer the ideal of four times that of a single-processor\nsystem, because there’s far less contention.\n One issue with fine-grained locking schemes is that sometimes you need more\nthan one mutex locked in order to protect all the data in an operation. As described\npreviously, sometimes the right thing to do is increase the granularity of the data cov-\nered by the mutexes, so that only one mutex needs to be locked. But sometimes that’s\nundesirable, such as when the mutexes are protecting separate instances of a class. In\nthis case, locking at the next level up would mean either leaving the locking to the\nuser or having a single mutex that protected all instances of that class, neither of\nwhich is particularly desirable.\n If you end up having to lock two or more mutexes for a given operation, there’s\nanother potential problem lurking in the wings: deadlock. This is almost the opposite\nof a race condition: rather than two threads racing to be first, each one is waiting for\nthe other, so neither makes any progress.\n",
      "content_length": 2734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "51\nProtecting shared data with mutexes\n3.2.4\nDeadlock: the problem and a solution\nImagine that you have a toy that comes in two parts, and you need both parts to play\nwith it—a toy drum and drumstick, for example. Now imagine that you have two small\nchildren, both of whom like playing with it. If one of them gets both the drum and the\ndrumstick, that child can merrily play the drum until tiring of it. If the other child\nwants to play, they have to wait, however sad that makes them. Now imagine that the\ndrum and the drumstick are buried (separately) in the toy box, and your children\nboth decide to play with them at the same time, so they go rummaging in the toy box.\nOne finds the drum and the other finds the drumstick. Now they’re stuck; unless one\ndecides to be nice and let the other play, each will hold on to whatever they have and\ndemand that they be given the other piece, so neither gets to play.\n Now imagine that you have not children arguing over toys but threads arguing\nover locks on mutexes: each of a pair of threads needs to lock both of a pair of\nmutexes to perform some operation, and each thread has one mutex and is waiting\nfor the other. Neither thread can proceed, because each is waiting for the other to\nrelease its mutex. This scenario is called deadlock, and it’s the biggest problem with\nhaving to lock two or more mutexes in order to perform an operation.\n The common advice for avoiding deadlock is to always lock the two mutexes in the\nsame order: if you always lock mutex A before mutex B, then you’ll never deadlock.\nSometimes this is straightforward, because the mutexes are serving different pur-\nposes, but other times it’s not so simple, such as when the mutexes are each protect-\ning a separate instance of the same class. Consider, for example, an operation that\nexchanges data between two instances of the same class; in order to ensure that the\ndata is exchanged correctly, without being affected by concurrent modifications, the\nmutexes on both instances must be locked. But if a fixed order is chosen (for exam-\nple, the mutex for the instance supplied as the first parameter, then the mutex for the\ninstance supplied as the second parameter), this can backfire: all it takes is for two\nthreads to try to exchange data between the same two instances with the parameters\nswapped, and you have deadlock!\n Thankfully, the C++ Standard Library has a cure for this in the form of\nstd::lock—a function that can lock two or more mutexes at once without risk of\ndeadlock. The example in the next listing shows how to use this for a simple swap\noperation.\nclass some_big_object;\nvoid swap(some_big_object& lhs,some_big_object& rhs);\nclass X\n{\nprivate:\n    some_big_object some_detail;\n    std::mutex m;\npublic:\n    X(some_big_object const& sd):some_detail(sd){}\nListing 3.6\nUsing std::lock() and std::lock_guard in a swap operation\n",
      "content_length": 2873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "52\nCHAPTER 3\nSharing data between threads\n    friend void swap(X& lhs, X& rhs)\n    {\n        if(&lhs==&rhs)\n            return;\n        std::lock(lhs.m,rhs.m);    \n        std::lock_guard<std::mutex> lock_a(lhs.m,std::adopt_lock);  \n        std::lock_guard<std::mutex> lock_b(rhs.m,std::adopt_lock);  \n        swap(lhs.some_detail,rhs.some_detail);\n    }\n};\nFirst, the arguments are checked to ensure they are different instances, because\nattempting to acquire a lock on std::mutex when you already hold it is undefined\nbehavior. (A mutex that does permit multiple locks by the same thread is provided in\nthe form of std::recursive_mutex. See section 3.3.3 for details.) Then, the call to\nstd::lock() B locks the two mutexes, and two std::lock_guard instances are con-\nstructed c and d, one for each mutex. The std::adopt_lock parameter is supplied\nin addition to the mutex to indicate to the std::lock_guard objects that the mutexes\nare already locked, and they should adopt the ownership of the existing lock on the\nmutex rather than attempt to lock the mutex in the constructor.\n This ensures that the mutexes are correctly unlocked on function exit in the gen-\neral case where the protected operation might throw an exception; it also allows for a\nsimple return. Also, it’s worth noting that locking either lhs.m or rhs.m inside the call\nto std::lock can throw an exception; in this case, the exception is propagated out of\nstd::lock. If std::lock has successfully acquired a lock on one mutex and an excep-\ntion is thrown when it tries to acquire a lock on the other mutex, this first lock is\nreleased automatically: std::lock provides all-or-nothing semantics with regard to\nlocking the supplied mutexes.\n C++17 provides additional support for this scenario, in the form of a new RAII tem-\nplate, std::scoped_lock<>. This is exactly equivalent to std::lock_guard<>, except\nthat it is a variadic template, accepting a list of mutex types as template parameters, and\na list of mutexes as constructor arguments. The mutexes supplied to the constructor\nare locked using the same algorithm as std::lock, so that when the constructor com-\npletes they are all locked, and they are then all unlocked in the destructor. The\nswap() operation from listing 3.6 can be rewritten as follows:\nvoid swap(X& lhs, X& rhs)\n    {\n        if(&lhs==&rhs)\n            return;\n        std::scoped_lock guard(lhs.m,rhs.m);    \n        swap(lhs.some_detail,rhs.some_detail);\n    }\nThis example uses another feature added with C++17: automatic deduction of class\ntemplate parameters. If you have a C++17 compiler (which is likely if you’re using\nstd::scoped_lock, because that is a C++17 library facility), the C++17 implicit class\nB\nc\nd\nb\n",
      "content_length": 2719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "53\nProtecting shared data with mutexes\ntemplate parameter deduction mechanism will choose the correct mutex types from\nthe types of the objects passed to the constructor at object B. This line is equivalent\nto the fully specified version:\nstd::scoped_lock<std::mutex,std::mutex> guard(lhs.m,rhs.m);\nThe existence of std::scoped_lock means that most of the cases where you would\nhave used std::lock prior to C++17 can now be written using std::scoped_lock,\nwith less potential for mistakes, which can only be a good thing!\n Although std::lock (and std::scoped_lock<>) can help you avoid deadlock in\nthose cases where you need to acquire two or more locks together, it doesn’t help if\nthey’re acquired separately. In that case, you have to rely on your discipline as devel-\nopers to ensure you don’t get deadlock. This isn’t easy: deadlocks are one of the nasti-\nest problems to encounter in multithreaded code and are often unpredictable, with\neverything working fine the majority of the time. There are, however, some relatively\nsimple rules that can help you to write deadlock-free code.\n3.2.5\nFurther guidelines for avoiding deadlock\nDeadlock doesn’t only occur with locks, although that’s the most frequent cause; you\ncan create deadlock with two threads and no locks by having each thread call join()\non the std::thread object for the other. In this case, neither thread can make prog-\nress because it’s waiting for the other to finish, like the children fighting over their toy.\nThis simple cycle can occur anywhere that a thread can wait for another thread to per-\nform some action if the other thread can simultaneously be waiting for the first\nthread, and it isn’t limited to two threads: a cycle of three or more threads will still\ncause deadlock. The guidelines for avoiding deadlock all boil down to one idea: don’t\nwait for another thread if there’s a chance it’s waiting for you. The individual guide-\nlines provide ways of identifying and eliminating the possibility that the other thread\nis waiting for you.\nAVOID NESTED LOCKS\nThe first idea is the simplest: don’t acquire a lock if you already hold one. If you stick\nto this guideline, it’s impossible to get a deadlock from the lock usage alone because\neach thread only ever holds a single lock. You could still get deadlock from other\nthings (like the threads waiting for each other), but mutex locks are probably the\nmost common cause of deadlock. If you need to acquire multiple locks, do it as a sin-\ngle action with std::lock in order to acquire them without deadlock.\nAVOID CALLING USER-SUPPLIED CODE WHILE HOLDING A LOCK\nThis is a simple follow-on from the previous guideline. Because the code is user-\nsupplied, you have no idea what it could do; it could do anything, including acquiring\na lock. If you call user-supplied code while holding a lock, and that code acquires a\nlock, you’ve violated the guideline on avoiding nested locks and could get deadlock.\nSometimes this is unavoidable; if you’re writing generic code, such as the stack in\n",
      "content_length": 3019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "54\nCHAPTER 3\nSharing data between threads\nsection 3.2.3, every operation on the parameter type or types is user-supplied code. In\nthis case, you need a new guideline.\nACQUIRE LOCKS IN A FIXED ORDER\nIf you absolutely must acquire two or more locks, and you can’t acquire them as a sin-\ngle operation with std::lock, the next best thing is to acquire them in the same order\nin every thread. I touched on this in section 3.2.4 as one way of avoiding deadlock\nwhen acquiring two mutexes: the key is to define the order in a way that’s consistent\nbetween threads. In some cases, this is relatively easy. For example, look at the stack\nfrom section 3.2.3—the mutex is internal to each stack instance, but the operations\non the data items stored in a stack require calling user-supplied code. You can, how-\never, add the constraint that none of the operations on the data items stored in the\nstack should perform any operation on the stack itself. This puts the burden on the\nuser of the stack, but it’s rather uncommon for the data stored in a container to access\nthat container, and it’s quite apparent when this is happening, so it’s not a particularly\ndifficult burden to carry.\n In other cases, this isn’t so straightforward, as you discovered with the swap opera-\ntion in section 3.2.4. At least in that case you could lock the mutexes simultaneously,\nbut that’s not always possible. If you look back at the linked list example from sec-\ntion 3.1, you’ll see that one possibility for protecting the list is to have a mutex per\nnode. Then, in order to access the list, threads must acquire a lock on every node\nthey’re interested in. For a thread to delete an item, it must then acquire the lock on\nthree nodes: the node being deleted and the nodes on either side, because they’re all\nbeing modified in some way. Likewise, to traverse the list, a thread must keep hold of\nthe lock on the current node while it acquires the lock on the next one in the\nsequence, in order to ensure that the next pointer isn’t modified in the meantime.\nOnce the lock on the next node has been acquired, the lock on the first can be\nreleased because it’s no longer necessary.\n This hand-over-hand locking style allows multiple threads to access the list, pro-\nvided each is accessing a different node. But in order to prevent deadlock, the\nnodes must always be locked in the same order: if two threads tried to traverse the\nlist in opposite orders using hand-over-hand locking, they could deadlock with\neach other in the middle of the list. If nodes A and B are adjacent in the list, the\nthread going one way will try to hold the lock on node A and try to acquire the lock\non node B. A thread going the other way would be holding the lock on node B and\ntrying to acquire the lock on node A—a classic scenario for deadlock, as shown in\nfigure 3.2.\n Likewise, when deleting node B that lies between nodes A and C, if that thread\nacquires the lock on B before the locks on A and C, it has the potential to deadlock\nwith a thread traversing the list. Such a thread would try to lock either A or C first\n(depending on the direction of traversal) but would then find that it couldn’t obtain a\nlock on B because the thread doing the deleting was holding the lock on B and trying\nto acquire the locks on A and C.\n",
      "content_length": 3283,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "55\nProtecting shared data with mutexes\nOne way to prevent deadlock here is to define an order of traversal, so a thread must\nalways lock A before B and B before C. This would eliminate the possibility of dead-\nlock at the expense of disallowing reverse traversal. Similar conventions can often be\nestablished for other data structures.\nUSE A LOCK HIERARCHY\nAlthough this is a particular case of defining lock ordering, a lock hierarchy can pro-\nvide a means of checking that the convention is adhered to at runtime. The idea is that\nyou divide your application into layers and identify all the mutexes that may be locked\nin any given layer. When code tries to lock a mutex, it isn’t permitted to lock that mutex\nif it already holds a lock from a lower layer. You can check this at runtime by assigning\nlayer numbers to each mutex and keeping a record of which mutexes are locked by\neach thread. This is a common pattern, but the C++ Standard Library does not provide\ndirect support for it, so you will need to write a custom hierarchical_mutex mutex\ntype, the code for which is shown in listing 3.8.\nThread 1\nThread 2\nLock master entry mutex\nRead head node pointer\nLock head node mutex\nUnlock master entry mutex\nLock master entry mutex\nRead head →next pointer\nLock tail node mutex\nLock next node mutex\nRead tail →prev pointer\nRead next →next pointer\nUnlock tail node mutex\n…\n…\nLock node A mutex\nLock node C mutex\nRead A →next pointer (which is B)\nRead C →next pointer (which is B)\nLock node B mutex\nBlock trying to lock node B mutex\nUnlock node C mutex\nRead B →prev pointer (which is A)\nBlock trying to lock node A mutex\nDeadlock!\nFigure 3.2\nDeadlock with threads traversing a list in opposite orders\n",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "56\nCHAPTER 3\nSharing data between threads\n The following listing shows an example of two threads using a hierarchical mutex.\nhierarchical_mutex high_level_mutex(10000);    \nhierarchical_mutex low_level_mutex(5000);    \nhierarchical_mutex other_mutex(6000);   \nint do_low_level_stuff();\nint low_level_func()\n{\n    std::lock_guard<hierarchical_mutex> lk(low_level_mutex);   \n    return do_low_level_stuff();\n}\nvoid high_level_stuff(int some_param);\nvoid high_level_func()\n{\n    std::lock_guard<hierarchical_mutex> lk(high_level_mutex);  \n    high_level_stuff(low_level_func());         \n}\nvoid thread_a()      \n{\n    high_level_func();\n}\nvoid do_other_stuff();\nvoid other_stuff()\n{\n    high_level_func();     \n    do_other_stuff();\n}\nvoid thread_b()     \n{\n    std::lock_guard<hierarchical_mutex> lk(other_mutex);    \n    other_stuff();\n}\nThis code has three instances of hierarchical_mutex, (B, c, and d), which are\nconstructed with progressively lower hierarchy numbers. Because the mechanism is\ndefined so that if you hold a lock on a hierarchical_mutex, then you can only acquire\na lock on another hierarchical_mutex with a lower hierarchy number, this imposes\nrestrictions on what the code can do.\n Assuming do_low_level_stuff doesn’t lock any mutexes, low_level_func is the\nbottom of your hierarchy, and locks the low_level_mutex e. high_level_func calls\nlow_level_func f, while holding a lock on high_level_mutex g, but that’s OK,\nbecause the hierarchy level of high_level_mutex (B: 10000) is higher than that of\nlow_level_mutex (c: 5000).\n thread_a() h abides by the rules, so it runs fine.\n On the other hand, thread_b() i disregards the rules and therefore will fail at\nruntime. \nListing 3.7\nUsing a lock hierarchy to prevent deadlock\nb\nc\nd\ne\ng\nf\nh\n1)\ni\nj\n",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "57\nProtecting shared data with mutexes\n First off, it locks other_mutex j, which has a hierarchy value of only 6000 d.\nThis means it should be somewhere midway in the hierarchy. When other_stuff()\ncalls high_level_func() 1), it’s violating the hierarchy: high_level_func() tries to\nacquire the high_level_mutex, which has a value of 10000, considerably more than\nthe current hierarchy value of 6000. The hierarchical_mutex will therefore report\nan error, possibly by throwing an exception or aborting the program. Deadlocks\nbetween hierarchical mutexes are impossible, because the mutexes themselves\nenforce the lock ordering. This does mean that you can’t hold two locks at the same\ntime if they’re the same level in the hierarchy, so hand-over-hand locking schemes\nrequire that each mutex in the chain has a lower hierarchy value than the prior one,\nwhich may be impractical in some cases.\n This example also demonstrates another point: the use of the std::lock_guard<>\ntemplate with a user-defined mutex type. hierarchical_mutex is not part of the stan-\ndard but is easy to write; a simple implementation is shown in listing 3.8. Even though\nit’s a user-defined type, it can be used with std::lock_guard<> because it implements\nthe three member functions required to satisfy the mutex concept: lock(), unlock(),\nand try_lock(). You haven’t yet seen try_lock() used directly, but it’s fairly simple: if\nthe lock on the mutex is held by another thread, it returns false rather than waiting\nuntil the calling thread can acquire the lock on the mutex. It may also be used by\nstd::lock() internally, as part of the deadlock-avoidance algorithm.\n The implementation of hierarchical_mutex uses a thread-local variable to store\nthe current hierarchy value. This value is accessible to all mutex instances, but has a\ndifferent value on each thread. This allows the code to check the behavior of each\nthread separately, and the code for each mutex can check whether or not the current\nthread is allowed to lock that mutex.\nclass hierarchical_mutex\n{\n    std::mutex internal_mutex;\n    unsigned long const hierarchy_value;\n    unsigned long previous_hierarchy_value;\n    static thread_local unsigned long this_thread_hierarchy_value;    \n    void check_for_hierarchy_violation()\n    {\n        if(this_thread_hierarchy_value <= hierarchy_value)    \n        {\n            throw std::logic_error(“mutex hierarchy violated”);\n        }\n    }\n    void update_hierarchy_value()\n    {\n        previous_hierarchy_value=this_thread_hierarchy_value;    \n        this_thread_hierarchy_value=hierarchy_value;\n    }\nListing 3.8\nA simple hierarchical mutex\nb\nc\nd\n",
      "content_length": 2641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "58\nCHAPTER 3\nSharing data between threads\npublic:\n    explicit hierarchical_mutex(unsigned long value):\n        hierarchy_value(value),\n        previous_hierarchy_value(0)\n    {}\n    void lock()\n    {\n        check_for_hierarchy_violation();\n        internal_mutex.lock();          \n        update_hierarchy_value();      \n    }\n    void unlock()\n    {\n        if(this_thread_hierarchy_value!=hierarchy_value)\n            throw std::logic_error(“mutex hierarchy violated”);    \n        this_thread_hierarchy_value=previous_hierarchy_value;    \n        internal_mutex.unlock();\n    }\n    bool try_lock()\n    {\n        check_for_hierarchy_violation();\n        if(!internal_mutex.try_lock())     \n            return false;\n        update_hierarchy_value();\n        return true;\n    }\n};\nthread_local unsigned long\n    hierarchical_mutex::this_thread_hierarchy_value(ULONG_MAX);    \nThe key here is the use of the thread_local value representing the hierarchy value\nfor the current thread: this_thread_hierarchy_value B. It’s initialized to the\nmaximum value i, so initially any mutex can be locked. Because it’s declared\nthread_local, every thread has its own copy, so the state of the variable in one thread\nis entirely independent of the state of the variable when read from another thread.\nSee appendix A, section A.8, for more information about thread_local.\n So, the first time a thread locks an instance of hierarchical_mutex, the value of\nthis_thread_hierarchy_value is ULONG_MAX. By its nature, this is greater than any\nother value, so the check in check_for_hierarchy_violation() c passes. With that\ncheck out of the way, lock()delegates to the internal mutex for the locking e. Once\nthis lock has succeeded, you can update the hierarchy value f.\n If you now lock another hierarchical_mutex while holding the lock on this first\none, the value of this_thread_hierarchy_value reflects the hierarchy value of the\nfirst mutex. The hierarchy value of this second mutex must now be less than that of\nthe mutex already held in order for the check c to pass.\n Now, it’s important to save the previous value of the hierarchy value for the cur-\nrent thread so you can restore it in unlock() g; otherwise you’d never be able to\nlock a mutex with a higher hierarchy value again, even if the thread didn’t hold any\nlocks. Because you store this previous hierarchy value only when you hold the\ne\nf\nj\ng\nh\ni\n",
      "content_length": 2398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "59\nProtecting shared data with mutexes\ninternal_mutex d, and you restore it before you unlock the internal mutex g, you\ncan safely store it in the hierarchical_mutex itself, because it’s safely protected by the\nlock on the internal mutex. In order to avoid the hierarchy getting confused due to\nout-of-order unlocking, you throw at j if the mutex being unlocked is not the most\nrecently locked one. Other mechanisms are possible, but this is the simplest.\n try_lock() works the same as lock(), except that if the call to try_lock() on the\ninternal_mutex fails h, then you don’t own the lock, so you don’t update the hierar-\nchy value, and return false rather than true.\n Although detection is a runtime check, it’s at least not timing-dependent—you\ndon’t have to wait around for the rare conditions that cause deadlock to show up.\nAlso, the design process required to divide the application and mutexes in this way\ncan help eliminate many possible causes of deadlock before they even get written. It\nmight be worth performing the design exercise even if you don’t go as far as writing\nthe runtime checks.\nEXTENDING THESE GUIDELINES BEYOND LOCKS\nAs I mentioned back at the beginning of this section, deadlock doesn’t only occur\nwith locks; it can occur with any synchronization construct that can lead to a wait\ncycle. It’s therefore worth extending these guidelines to cover those cases too. For\nexample, just as you should avoid acquiring nested locks if possible, it’s a bad idea to\nwait for a thread while holding a lock, because that thread might need to acquire the\nlock in order to proceed. Similarly, if you’re going to wait for a thread to finish, it\nmight be worth identifying a thread hierarchy, so that a thread waits only for threads\nlower down the hierarchy. One simple way to do this is to ensure that your threads are\njoined in the same function that started them, as described in sections 3.1.2 and 3.3.\n Once you’ve designed your code to avoid deadlock, std::lock() and std::\nlock_guard cover most of the cases of simple locking, but sometimes more flexibility\nis required. For those cases, the Standard Library provides the std::unique_lock\ntemplate. Like std::lock_guard, this is a class template parameterized on the mutex\ntype, and it also provides the same RAII-style lock management as std::lock_guard,\nbut with a bit more flexibility.\n3.2.6\nFlexible locking with std::unique_lock\nstd::unique_lock provides a bit more flexibility than std::lock_guard by relaxing\nthe invariants; an std::unique_lock instance doesn’t always own the mutex that it’s\nassociated with. First off, as you can pass std::adopt_lock as a second argument to the\nconstructor to have the lock object manage the lock on a mutex, you can also pass\nstd::defer_lock as the second argument to indicate that the mutex should remain\nunlocked on construction. The lock can then be acquired later by calling lock() on the\nstd::unique_lock object (not the mutex) or by passing the std:: unique_lock object\nto std::lock(). Listing 3.6 could easily have been written as shown in listing 3.9,\nusing std::unique_lock and std::defer_lock B, rather than std::lock_guard and\nstd::adopt_lock. The code has the same line count and is equivalent, apart from\n",
      "content_length": 3232,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "60\nCHAPTER 3\nSharing data between threads\none small thing: std::unique_lock takes more space and is slightly slower to use\nthan std::lock_guard. The flexibility of allowing an std::unique_lock instance not\nto own the mutex comes at a price: this information has to be stored, and it has to\nbe updated.\nclass some_big_object;\nvoid swap(some_big_object& lhs,some_big_object& rhs);\nclass X\n{\nprivate:\n    some_big_object some_detail;\n    std::mutex m;\npublic:\n    X(some_big_object const& sd):some_detail(sd){}\n    friend void swap(X& lhs, X& rhs)\n    {\n        if(&lhs==&rhs)                                \n            return;\n        std::unique_lock<std::mutex> lock_a(lhs.m,std::defer_lock);  \n        std::unique_lock<std::mutex> lock_b(rhs.m,std::defer_lock);  \n        std::lock(lock_a,lock_b);                        \n        swap(lhs.some_detail,rhs.some_detail);\n    }\n};\nIn listing 3.9, the std::unique_lock objects could be passed to std::lock() c,\nbecause std::unique_lock provides lock(), try_lock(), and unlock() member\nfunctions. These forward to the member functions of the same name on the underly-\ning mutex to do the work and update a flag inside the std::unique_lock instance to\nindicate whether the mutex is currently owned by that instance. This flag is necessary\nin order to ensure that unlock() is called correctly in the destructor. If the instance does\nown the mutex, the destructor must call unlock(), and if the instance does not own the\nmutex, it must not call unlock(). This flag can be queried by calling the owns_lock()\nmember function. Unless you’re going to be transferring lock ownership around or\ndoing something else that requires std::unique_lock, you’re still better off using the\nC++17 variadic std::scoped_lock if it’s available to you (see section 3.2.4).\n As you might expect, this flag has to be stored somewhere. Therefore, the size of a\nstd::unique_lock object is typically larger than that of a std::lock_guard object,\nand there’s also a slight performance penalty when using std::unique_lock over\nstd:: lock_guard because the flag has to be updated or checked, as appropriate. If\nstd::lock_ guard is sufficient for your needs, I’d therefore recommend using it in\npreference. That said, there are cases where std::unique_lock is a better fit for the\ntask at hand because you need to make use of the additional flexibility. One example\nis deferred locking, as you’ve already seen; another case is where the ownership of the\nlock needs to be transferred from one scope to another.\nListing 3.9\nUsing std::lock() and std::unique_lock in a swap operation\nstd::defer_lock\nleaves mutexes\nunlocked.\nB\nMutexes are \nlocked here.\nc\n",
      "content_length": 2667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "61\nProtecting shared data with mutexes\n3.2.7\nTransferring mutex ownership between scopes\nBecause std::unique_lock instances don’t have to own their associated mutexes, the\nownership of a mutex can be transferred between instances by moving the instances\naround. In some cases this transfer is automatic, such as when returning an instance\nfrom a function, and in other cases you have to do it explicitly by calling std::move().\nFundamentally this depends on whether the source is an lvalue—a real variable or ref-\nerence to one—or an rvalue—a temporary of some kind. Ownership transfer is auto-\nmatic if the source is an rvalue and must be done explicitly for an lvalue in order to\navoid accidentally transferring ownership away from a variable. std::unique_lock is\nan example of a type that’s moveable but not copyable. See appendix A, section A.1.1, for\nmore about move semantics.\n One possible use is to allow a function to lock a mutex and transfer ownership of\nthat lock to the caller, so the caller can then perform additional actions under the\nprotection of the same lock. The following code snippet shows an example of this; the\nget_lock() function locks the mutex and then prepares the data before returning\nthe lock to the caller:\nstd::unique_lock<std::mutex> get_lock()\n{\n    extern std::mutex some_mutex;\n    std::unique_lock<std::mutex> lk(some_mutex);\n    prepare_data();\n    return lk;        \n}\nvoid process_data()\n{\n    std::unique_lock<std::mutex> lk(get_lock());    \n    do_something();\n}\nBecause lk is an automatic variable declared within the function, it can be returned\ndirectly B, without a call to std:move(); the compiler takes care of calling the move\nconstructor. The process_data() function can then transfer ownership directly into\nits own std::unique_lock instance c, and the call to do_something() can rely on\nthe data being correctly prepared without another thread altering the data in the\nmeantime.\n Typically this sort of pattern would be used where the mutex to be locked is depen-\ndent on the current state of the program or on an argument passed in to the function\nthat returns the std::unique_lock object. One such usage is where the lock isn’t\nreturned directly but is a data member of a gateway class used to ensure correctly\nlocked access to some protected data. In this case, all access to the data is through this\ngateway class: when you want to access the data, you obtain an instance of the gateway\nclass (by calling a function such as get_lock() in the preceding example), which\nacquires the lock. You can then access the data through member functions of the gate-\nway object. When you’re finished, you destroy the gateway object, which releases the\nB\nc\n",
      "content_length": 2702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "62\nCHAPTER 3\nSharing data between threads\nlock and allows other threads to access the protected data. Such a gateway object may\nwell be moveable (so that it can be returned from a function), in which case the lock\nobject data member also needs to be moveable.\n The flexibility of std::unique_lock also allows instances to relinquish their locks\nbefore they’re destroyed. You can do this with the unlock() member function, like for\na mutex. std::unique_lock supports the same basic set of member functions for lock-\ning and unlocking as a mutex does, so that it can be used with generic functions such\nas std::lock. The ability to release a lock before the std::unique_lock instance is\ndestroyed means that you can optionally release it in a specific code branch if it’s\napparent that the lock is no longer required. This can be important for the perfor-\nmance of the application; holding a lock for longer than required can cause a drop in\nperformance, because other threads waiting for the lock are prevented from proceed-\ning for longer than necessary.\n3.2.8\nLocking at an appropriate granularity\nThe granularity of a lock is something I touched on earlier, in section 3.2.3: the lock\ngranularity is a hand-waving term to describe the amount of data protected by a single\nlock. A fine-grained lock protects a small amount of data, and a coarse-grained lock\nprotects a large amount of data. Not only is it important to choose a sufficiently coarse\nlock granularity to ensure the required data is protected, but it’s also important to\nensure that a lock is held only for the operations that require it. We all know the frus-\ntration of waiting in the checkout line in a supermarket with a cart full of groceries\nonly for the person currently being served to suddenly realize that they forgot some\ncranberry sauce and then leave everybody waiting while they go and find some, or\nfor the cashier to be ready for payment and the customer to only then start rummag-\ning in their bag for their wallet. Everything proceeds much more easily if everybody\ngets to the checkout with everything they want and with an appropriate method of\npayment ready.\n The same applies to threads: if multiple threads are waiting for the same resource\n(the cashier at the checkout), then if any thread holds the lock for longer than neces-\nsary, it will increase the total time spent waiting (don’t wait until you’ve reached the\ncheckout to start looking for the cranberry sauce). Where possible, lock a mutex only\nwhile accessing the shared data; try to do any processing of the data outside the lock.\nIn particular, don’t do any time-consuming activities like file I/O while holding a lock.\nFile I/O is typically hundreds (if not thousands) of times slower than reading or writ-\ning the same volume of data from memory. Unless the lock is intended to protect\naccess to the file, performing I/O while holding the lock will delay other threads\nunnecessarily (because they’ll block while waiting to acquire the lock), potentially\neliminating any performance gain from the use of multiple threads.\n std::unique_lock works well in this situation, because you can call unlock()\nwhen the code no longer needs access to the shared data and then call lock() again if\naccess is required later in the code:\n",
      "content_length": 3273,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "63\nProtecting shared data with mutexes\nvoid get_and_process_data()\n{\n    std::unique_lock<std::mutex> my_lock(the_mutex);\n    some_class data_to_process=get_next_data_chunk();\n    my_lock.unlock();                                 \n    result_type result=process(data_to_process);\n    my_lock.lock();                              \n    write_result(data_to_process,result);\n}\nYou don’t need the mutex locked across the call to process(), so you manually\nunlock it before the call B and then lock it again afterward c.\n Hopefully it’s obvious that if you have one mutex protecting an entire data struc-\nture, not only is there likely to be more contention for the lock, but also the potential\nfor reducing the time that the lock is held is diminished. More of the operation steps\nwill require a lock on the same mutex, so the lock must be held longer. This double\nwhammy of a cost is also a double incentive to move toward finer-grained locking\nwherever possible.\n As this example shows, locking at an appropriate granularity isn’t only about the\namount of data locked; it’s also about how long the lock is held and what operations\nare performed while the lock is held. In general, a lock should be held for only the mini-\nmum possible time needed to perform the required operations. This also means that time-\nconsuming operations such as acquiring another lock (even if you know it won’t\ndeadlock) or waiting for I/O to complete shouldn’t be done while holding a lock\nunless absolutely necessary.\n In listings 3.6 and 3.9, the operation that required locking the two mutexes was a\nswap operation, which obviously requires concurrent access to both objects. Suppose\ninstead you were trying to compare a simple data member that was a plain int. Would\nthis make a difference? ints are cheap to copy, so you could easily copy the data for\neach object being compared while only holding the lock for that object and then com-\npare the copied values. This would mean that you were holding the lock on each\nmutex for the minimum amount of time and also that you weren’t holding one lock\nwhile locking another. The following listing shows a class Y for which this is the case\nand a sample implementation of the equality comparison operator.\nclass Y\n{\nprivate:\n    int some_detail;\n    mutable std::mutex m;\n    int get_detail() const\n    {\n        std::lock_guard<std::mutex> lock_a(m);    \n        return some_detail;\n    }\nListing 3.10\nLocking one mutex at a time in a comparison operator\nDon’t need mutex \nlocked across call \nto process()\nB\nRelock mutex \nto write result\nc\nb\n",
      "content_length": 2568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "64\nCHAPTER 3\nSharing data between threads\npublic:\n    Y(int sd):some_detail(sd){}\n    friend bool operator==(Y const& lhs, Y const& rhs)\n    {\n        if(&lhs==&rhs)\n            return true;\n        int const lhs_value=lhs.get_detail();    \n        int const rhs_value=rhs.get_detail();   \n        return lhs_value==rhs_value;         \n    }\n};\nIn this case, the comparison operator first retrieves the values to be compared by call-\ning the get_detail() member function, c and d. This function retrieves the value\nwhile protecting it with a lock B. The comparison operator then compares the\nretrieved values e. Note, however, that as well as reducing the locking periods so that\nonly one lock is held at a time (and eliminating the possibility of deadlock), this has\nsubtly changed the semantics of the operation compared to holding both locks together. In\nlisting 3.10, if the operator returns true, it means that the value of lhs.some_detail\nat one point in time is equal to the value of rhs.some_detail at another point in\ntime. The two values could have been changed in any way in between the two reads;\nthe values could have been swapped in between c and d, for example, rendering\nthe comparison meaningless. The equality comparison might return true to indicate\nthat the values were equal, even though there was never an instant in time when the\nvalues were equal. It’s therefore important to be careful when making these changes\nthat the semantics of the operation are not changed in a problematic fashion: if you\ndon’t hold the required locks for the entire duration of an operation, you’re exposing yourself to\nrace conditions.\n Sometimes, there isn’t an appropriate level of granularity because not all accesses\nto the data structure require the same level of protection. In this case, it might be\nappropriate to use an alternative mechanism, instead of a plain std::mutex.\n3.3\nAlternative facilities for protecting shared data\nAlthough they’re the most general mechanism, mutexes aren’t the only game in town\nwhen it comes to protecting shared data; there are alternatives that provide more\nappropriate protection in specific scenarios.\n One particularly extreme (but remarkably common) case is where the shared data\nneeds protection only from concurrent access while it’s being initialized, but after that\nno explicit synchronization is required. This might be because the data is read-only\nonce created, and so there are no possible synchronization issues, or it might be\nbecause the necessary protection is performed implicitly as part of the operations on\nthe data. In either case, locking a mutex after the data has been initialized, purely in\norder to protect the initialization, is unnecessary and a needless hit to performance.\nIt’s for this reason that the C++ Standard provides a mechanism purely for protecting\nshared data during initialization.\nc\nd\ne\n",
      "content_length": 2875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "65\nAlternative facilities for protecting shared data\n3.3.1\nProtecting shared data during initialization\nSuppose you have a shared resource that’s so expensive to construct that you want to\ndo so only if it’s required; maybe it opens a database connection or allocates a lot of\nmemory. Lazy initialization such as this is common in single-threaded code—each\noperation that requires the resource first checks to see if it has been initialized and\nthen initializes it before use if not:\nstd::shared_ptr<some_resource> resource_ptr;\nvoid foo()\n{\n    if(!resource_ptr)\n    {\n        resource_ptr.reset(new some_resource);    \n    }\n    resource_ptr->do_something();\n}\nIf the shared resource itself is safe for concurrent access, the only part that needs pro-\ntecting when converting this to multithreaded code is the initialization B, but a naïve\ntranslation such as that in the following listing can cause unnecessary serialization of\nthreads using the resource. This is because each thread must wait on the mutex in\norder to check whether the resource has already been initialized.\nstd::shared_ptr<some_resource> resource_ptr;\nstd::mutex resource_mutex;\nvoid foo()\n{\n    std::unique_lock<std::mutex> lk(resource_mutex);   \n    if(!resource_ptr)\n    {\n        resource_ptr.reset(new some_resource);  \n    }\n    lk.unlock();\n    resource_ptr->do_something();\n}\nThis code is common enough, and the unnecessary serialization problematic enough,\nthat many people have tried to come up with a better way of doing this, including the\ninfamous double-checked locking pattern: the pointer is first read without acquiring the\nlock (B in the following code), and the lock is acquired only if the pointer is NULL.\nThe pointer is then checked again once the lock has been acquired (c, hence the\ndouble-checked part) in case another thread has done the initialization between the first\ncheck and this thread acquiring the lock:\nvoid undefined_behaviour_with_double_checked_locking()\n{\n    if(!resource_ptr)     \nListing 3.11\nThread-safe lazy initialization using a mutex\nb\nAll threads are \nserialized here\nOnly the \ninitialization \nneeds protection\nB\n",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "66\nCHAPTER 3\nSharing data between threads\n    {\n        std::lock_guard<std::mutex> lk(resource_mutex);\n        if(!resource_ptr)                             \n        {\n            resource_ptr.reset(new some_resource);   \n        }\n    }\n    resource_ptr->do_something();   \n}\nUnfortunately, this pattern is infamous for a reason: it has the potential for nasty race\nconditions, because the read outside the lock B, isn’t synchronized with the write\ndone by another thread inside the lock d. This creates a race condition that covers\nnot only the pointer itself but also the object pointed to; even if a thread sees the\npointer written by another thread, it might not see the newly created instance of\nsome_resource, resulting in the call to do_something() e operating on incorrect val-\nues. This is an example of the type of race condition defined as a data race by the C++\nStandard and specified as undefined behavior. It’s therefore quite definitely something\nto avoid. See chapter 5 for a detailed discussion of the memory model, including what\nconstitutes a data race.\n The C++ Standards Committee also saw that this was an important scenario, and\nso the C++ Standard Library provides std::once_flag and std::call_once to han-\ndle this situation. Rather than locking a mutex and explicitly checking the pointer,\nevery thread can use std::call_once, safe in the knowledge that the pointer will\nhave been initialized by some thread (in a properly synchronized fashion) by the\ntime std::call_once returns. The necessary synchronization data is stored in the\nstd::once_flag instance; each instance of std::once_flag corresponds to a different\ninitialization. Use of std::call_once will typically have a lower overhead than using a\nmutex explicitly, especially when the initialization has already been done, so it should be\nused in preference where it matches the required functionality. The following example\nshows the same operation as listing 3.11, rewritten to use std::call_once. In this case,\nthe initialization is done by calling a function, but it could easily have been done with\nan instance of a class with a function call operator. Like most of the functions in the\nstandard library that take functions or predicates as arguments, std::call_once\nworks with any function or callable object:\nstd::shared_ptr<some_resource> resource_ptr;\nstd::once_flag resource_flag;        \nvoid init_resource()\n{\n    resource_ptr.reset(new some_resource);    \n}\nvoid foo()\n{\n    std::call_once(resource_flag,init_resource);   \n    resource_ptr->do_something();\n}\nc\nd\ne\nb\nInitialization is \ncalled exactly \nonce.\n",
      "content_length": 2607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "67\nAlternative facilities for protecting shared data\nIn this example, both the std::once_flag B and data being initialized are namespace-\nscope objects, but std::call_once() can easily be used for lazy initialization of class\nmembers, as in the following listing.\nclass X\n{\nprivate:\n    connection_info connection_details;\n    connection_handle connection;\n    std::once_flag connection_init_flag;\n    void open_connection()\n    {\n        connection=connection_manager.open(connection_details);\n    }\npublic:\n    X(connection_info const& connection_details_):\n        connection_details(connection_details_)\n    {}\n    void send_data(data_packet const& data)    \n    {\n        std::call_once(connection_init_flag,&X::open_connection,this);  \n        connection.send_data(data);\n    }\n    data_packet receive_data()   \n    {\n        std::call_once(connection_init_flag,&X::open_connection,this);  \n        return connection.receive_data();\n    }\n};\nIn that example, the initialization is done either by the first call to send_data() B,\nor by the first call to receive_data() d. The use of the open_connection() member\nfunction to initialize the data also requires that the this pointer be passed in. Just as\nfor other functions in the Standard Library that accept callable objects, such as the\nconstructors for std::thread and std::bind(), this is done by passing an additional\nargument to std::call_once() c.\n It’s worth noting that like std::mutex, std::once_flag instances can’t be copied\nor moved, so if you use them as a class member like this, you’ll have to explicitly\ndefine these special member functions should you require them.\n One scenario where there’s a potential race condition over initialization is that of a\nlocal variable declared with static. The initialization of such a variable is defined to\noccur the first time control passes through its declaration; for multiple threads calling\nthe function, this means there’s the potential for a race condition to define first. On\nmany pre-C++11 compilers this race condition is problematic in practice, because\nmultiple threads may believe they’re first and try to initialize the variable, or threads\nmay try to use it after initialization has started on another thread but before it’s fin-\nished. In C++11 this problem is solved: the initialization is defined to happen on\nListing 3.12\nThread-safe lazy initialization of a class member using std::call_once\nB\nc\nd\n",
      "content_length": 2427,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "68\nCHAPTER 3\nSharing data between threads\nexactly one thread, and no other threads will proceed until that initialization is com-\nplete, so the race condition is over which thread gets to do the initialization rather\nthan anything more problematic. This can be used as an alternative to std::call_\nonce for those cases where a single global instance is required:\nclass my_class;\nmy_class& get_my_class_instance()\n{\n    static my_class instance;     \n    return instance;\n}\nMultiple threads can then call get_my_class_instance() safely B, without having to\nworry about race conditions on the initialization.\n Protecting data only for initialization is a special case of a more general scenario:\nthat of a rarely updated data structure. For most of the time, this data structure is\nread-only and can therefore be read by multiple threads concurrently, but on occa-\nsion the data structure may need updating. What’s needed here is a protection mech-\nanism that acknowledges this fact.\n3.3.2\nProtecting rarely updated data structures\nConsider a table used to store a cache of DNS entries for resolving domain names to\ntheir corresponding IP addresses. Typically, a given DNS entry will remain unchanged\nfor a long period of time—in many cases, DNS entries remain unchanged for years.\nAlthough new entries may be added to the table from time to time as users access dif-\nferent websites, this data will therefore remain largely unchanged throughout its life.\nIt’s important that the validity of the cached entries is checked periodically, but this\nstill requires an update only if the details have changed.\n Although updates are rare, they can still happen, and if this cache is to be accessed\nfrom multiple threads, it will need to be appropriately protected during updates to\nensure that none of the threads reading the cache see a broken data structure.\n In the absence of a special-purpose data structure that exactly fits the desired\nusage and that’s specially designed for concurrent updates and reads (such as those in\nchapters 6 and 7), this update requires that the thread doing the update has exclusive\naccess to the data structure until it has completed the operation. Once the change is\ncomplete, the data structure is again safe for multiple threads to access concurrently.\nUsing std::mutex to protect the data structure is therefore overly pessimistic, because\nit will eliminate the possible concurrency in reading the data structure when it isn’t\nundergoing modification; what’s needed is a different kind of mutex. This new kind\nof mutex is typically called a reader-writer mutex, because it allows for two different\nkinds of usage: exclusive access by a single “writer” thread or shared, and concurrent\naccess by multiple “reader” threads.\n The C++17 Standard Library provides two such mutexes out of the box, std::\nshared_mutex and std::shared_timed_mutex. C++14 only features std::shared_\ntimed_mutex, and C++11 didn’t provide either. If you’re struck with a pre-C++14\nInitialization guaranteed \nto be thread-safe\nB\n",
      "content_length": 3030,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "69\nAlternative facilities for protecting shared data\ncompiler, then you could use the implementation provided by the Boost library, which\nis based on the original proposal. The difference between std::shared_mutex and\nstd::shared_timed_mutex is that std::shared_timed_mutex supports additional\noperations (as described in section 4.3), so std::shared_mutex might offer a perfor-\nmance benefit on some platforms, if you don’t need the additional operations.\n As you’ll see in chapter 8, the use of such a mutex isn’t a panacea, and the perfor-\nmance is dependent on the number of processors involved and the relative workloads\nof the reader and updater threads. It’s therefore important to profile the perfor-\nmance of the code on the target system to ensure that there’s a benefit to the addi-\ntional complexity.\n Rather than using an instance of std::mutex for the synchronization, you use an\ninstance of std::shared_mutex. For the update operations, std::lock_guard\n<std::shared_mutex> and std::unique_lock<std::shared_mutex> can be used for\nthe locking, in place of the corresponding std::mutex specializations. These ensure\nexclusive access, as with std::mutex. Those threads that don’t need to update the\ndata structure can instead use std::shared_lock<std::shared_mutex> to obtain\nshared access. This RAII class template was added in C++14, and is used the same as\nstd::unique_lock, except that multiple threads may have a shared lock on the same\nstd::shared_mutex at the same time. The only constraint is that if any thread has a\nshared lock, a thread that tries to acquire an exclusive lock will block until all other\nthreads have relinquished their locks, and likewise if any thread has an exclusive lock,\nno other thread may acquire a shared or exclusive lock until the first thread has relin-\nquished its lock.\n The following listing shows a simple DNS cache like the one described, using\nstd::map to hold the cached data, protected using std::shared_mutex.\n#include <map>\n#include <string>\n#include <mutex>\n#include <shared_mutex>\nclass dns_entry;\nclass dns_cache\n{\n    std::map<std::string,dns_entry> entries;\n    mutable std::shared_mutex entry_mutex;\npublic:\n    dns_entry find_entry(std::string const& domain) const\n    {\n        std::shared_lock<std::shared_mutex> lk(entry_mutex);      \n        std::map<std::string,dns_entry>::const_iterator const it=\n            entries.find(domain);\n        return (it==entries.end())?dns_entry():it->second;\n    }\n    void update_or_add_entry(std::string const& domain,\n                             dns_entry const& dns_details)\nListing 3.13\nProtecting a data structure with std::shared_mutex\nb\n",
      "content_length": 2651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "70\nCHAPTER 3\nSharing data between threads\n    {\n        std::lock_guard<std::shared_mutex> lk(entry_mutex);  \n        entries[domain]=dns_details;\n    }\n};\nIn listing 3.13, find_entry() uses an instance of std::shared_lock<> to protect it for\nshared, read-only access B; multiple threads can therefore call find_entry() simulta-\nneously without problems. On the other hand, update_or_add_entry() uses an\ninstance of std::lock_guard<> to provide exclusive access while the table is updated\nc; not only are other threads prevented from doing updates in a call to update_\nor_add_entry(), but threads that call find_entry() are blocked too.\n3.3.3\nRecursive locking\nWith std::mutex, it’s an error for a thread to try to lock a mutex it already owns, and\nattempting to do so will result in undefined behavior. But in some circumstances it\nwould be desirable for a thread to reacquire the same mutex several times without\nhaving first released it. For this purpose, the C++ Standard Library provides\nstd::recursive_mutex. It works like std::mutex, except that you can acquire multi-\nple locks on a single instance from the same thread. You must release all your locks\nbefore the mutex can be locked by another thread, so if you call lock() three times,\nyou must also call unlock() three times. The correct use of std::lock_guard\n<std::recursive_mutex> and std::unique_lock<std::recursive_mutex> will han-\ndle this for you.\n Most of the time, if you think you want a recursive mutex, you probably need to\nchange your design instead. A common use of recursive mutexes is where a class is\ndesigned to be accessible from multiple threads concurrently, so it has a mutex pro-\ntecting the member data. Each public member function locks the mutex, does the\nwork, and then unlocks the mutex. But sometimes it’s desirable for one public mem-\nber function to call another as part of its operation. In this case, the second member\nfunction will also try to lock the mutex, leading to undefined behavior. The quick-and-\ndirty solution is to change the mutex to a recursive mutex. This will allow the mutex\nlock in the second member function to succeed and the function to proceed.\n But such usage is not recommended because it can lead to sloppy thinking and\nbad design. In particular, the class invariants are typically broken while the lock is\nheld, which means that the second member function needs to work even when\ncalled with the invariants broken. It’s usually better to extract a new private member\nfunction that’s called from both member functions, which does not lock the mutex\n(it expects it to already be locked). You can then think carefully about the circum-\nstances under which that new function can be called and the state of the data under\nthose circumstances.\nc\n",
      "content_length": 2761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "71\nSummary\nSummary\nIn this chapter I discussed how problematic race conditions can be disastrous when\nsharing data between threads and how to use std::mutex and careful interface design\nto avoid them. You saw that mutexes aren’t a panacea and do have their own problems\nin the form of deadlock, though the C++ Standard Library provides a tool to help\navoid that in the form of std::lock(). You then looked at some further techniques\nfor avoiding deadlock, followed by a brief look at transferring lock ownership and\nissues surrounding choosing the appropriate granularity for locking. Finally, I covered\nthe alternative data-protection facilities provided for specific scenarios, such as std::\ncall_once() and std::shared_mutex.\n One thing that I haven’t covered yet, however, is waiting for input from other\nthreads. Your thread-safe stack throws an exception if the stack is empty, so if one\nthread wanted to wait for another thread to push a value on the stack (which is, after\nall, one of the primary uses for a thread-safe stack), it would have to repeatedly try to\npop a value, retrying if an exception gets thrown. This consumes valuable processing\ntime in performing the check, without making any progress; indeed, the constant\nchecking might hamper progress by preventing the other threads in the system from\nrunning. What’s needed is some way for a thread to wait for another thread to com-\nplete a task without consuming CPU time in the process. Chapter 4 builds on the facil-\nities I’ve discussed for protecting shared data and introduces the various mechanisms\nfor synchronizing operations between threads in C++; chapter 6 shows how these can\nbe used to build larger reusable data structures.\n",
      "content_length": 1707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "72\nSynchronizing\nconcurrent operations\nIn the last chapter, we looked at various ways of protecting data that’s shared between\nthreads. But sometimes you don’t just need to protect the data, you also need to syn-\nchronize actions on separate threads. One thread might need to wait for another\nthread to complete a task before the first thread can complete its own, for example.\nIn general, it’s common to want a thread to wait for a specific event to happen or a\ncondition to be true. Although it would be possible to do this by periodically check-\ning a “task complete” flag or something similar stored in shared data, this is far from\nideal. The need to synchronize operations between threads like this is such a com-\nmon scenario that the C++ Standard Library provides facilities to handle it, in the\nform of condition variables and futures. These facilities are extended in the Concur-\nrency Technical Specification (TS), which provides additional operations for futures,\nalongside new synchronization facilities in the form of latches and barriers.\nThis chapter covers\nWaiting for an event\nWaiting for one-off events with futures\nWaiting with a time limit\nUsing the synchronization of operations to \nsimplify code\n",
      "content_length": 1223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "73\nWaiting for an event or other condition\n In this chapter, I’ll discuss how to wait for events with condition variables, futures,\nlatches, and barriers, and how to use them to simplify the synchronization of opera-\ntions.\n4.1\nWaiting for an event or other condition\nSuppose you’re traveling on an overnight train. One way to ensure you get off at the\nright station would be to stay awake all night and pay attention to where the train\nstops. You wouldn’t miss your station, but you’d be tired when you got there. Alterna-\ntively, you could look at the timetable to see when the train is supposed to arrive, set\nyour alarm a bit before, and go to sleep. That would be OK; you wouldn’t miss your\nstop, but if the train got delayed, you’d wake up too early. There’s also the possibility\nthat your alarm clock’s batteries would die, and you’d sleep too long and miss your sta-\ntion. What would be ideal is if you could go to sleep and have somebody or something\nwake you up when the train gets to your station, whenever that is.\n How does that relate to threads? Well, if one thread is waiting for a second thread\nto complete a task, it has several options. First, it could keep checking a flag in shared\ndata (protected by a mutex) and have the second thread set the flag when it com-\npletes the task. This is wasteful on two counts: the thread consumes valuable process-\ning time repeatedly checking the flag, and when the mutex is locked by the waiting\nthread, it can’t be locked by any other thread. Both of these work against the thread\ndoing the waiting: if the waiting thread is running, this limits the execution resources\navailable to run the thread being waited for, and while the waiting thread has locked\nthe mutex protecting the flag in order to check it, the thread being waited for is\nunable to lock the mutex to set the flag when it’s done. This is akin to staying awake all\nnight talking to the train driver: he has to drive the train more slowly because you\nkeep distracting him, so it takes longer to get there. Similarly, the waiting thread is\nconsuming resources that could be used by other threads in the system and may end\nup waiting longer than necessary.\n A second option is to have the waiting thread sleep for short periods between the\nchecks using the std::this_thread::sleep_for() function (see section 4.3):\nbool flag;\nstd::mutex m;\nvoid wait_for_flag()\n{\n    std::unique_lock<std::mutex> lk(m);\n    while(!flag)                                   \n    {\n        lk.unlock();                       \n        std::this_thread::sleep_for(std::chrono::milliseconds(100));   \n        lk.lock();       \n    }\n}\nIn the loop, the function unlocks the mutex B before the sleep c, and locks it again\nafterward d so another thread gets a chance to acquire it and set the flag.\nUnlock the \nmutex.\nB\nc\nSleep for\n100 ms.\nRelock the mutex.\nd\n",
      "content_length": 2854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "74\nCHAPTER 4\nSynchronizing concurrent operations\n This is an improvement because the thread doesn’t waste processing time while it’s\nsleeping, but it’s hard to get the sleep period right. Too short a sleep in between\nchecks and the thread still wastes processing time checking; too long a sleep and the\nthread will keep on sleeping even when the task it’s waiting for is complete, introduc-\ning a delay. It’s rare that this oversleeping will have a direct impact on the operation of\nthe program, but it could mean dropped frames in a fast-paced game or overrunning\na time slice in a real-time application.\n The third and preferred option is to use the facilities from the C++ Standard\nLibrary to wait for the event itself. The most basic mechanism for waiting for an event\nto be triggered by another thread (such as the presence of additional work in the\npipeline mentioned previously) is the condition variable. Conceptually, a condition vari-\nable is associated with an event or other condition, and one or more threads can wait\nfor that condition to be satisfied. When a thread has determined that the condition is\nsatisfied, it can then notify one or more of the threads waiting on the condition vari-\nable in order to wake them up and allow them to continue processing.\n4.1.1\nWaiting for a condition with condition variables\nThe Standard C++ Library provides not one but two implementations of a condition\nvariable: std::condition_variable and std::condition_variable_any. Both of\nthese are declared in the <condition_variable> library header. In both cases, they\nneed to work with a mutex in order to provide appropriate synchronization; the for-\nmer is limited to working with std::mutex, whereas the latter can work with anything\nthat meets the minimal criteria for being mutex-like, hence the _any suffix. Because\nstd::condition_variable_any is more general, there’s the potential for additional\ncosts in terms of size, performance, or OS resources, so std::condition_variable\nshould be preferred unless the additional flexibility is required.\n So, how do you use std::condition_variable to handle the example in the\nintroduction? How do you let the thread that’s waiting for work sleep until there’s\ndata to process? The following listing shows one way you could do this with a condi-\ntion variable.\nstd::mutex mut;\nstd::queue<data_chunk> data_queue;      \nstd::condition_variable data_cond;\nvoid data_preparation_thread()\n{\n    while(more_data_to_prepare())\n    {\n        data_chunk const data=prepare_data();\n        {\n            std::lock_guard<std::mutex> lk(mut);\n            data_queue.push(data);              \n        }\n        data_cond.notify_one();     \nListing 4.1\nWaiting for data to process with std::condition_variable\nb\nc\nd\n",
      "content_length": 2751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "75\nWaiting for an event or other condition\n    }\n}\nvoid data_processing_thread()\n{\n    while(true)\n    {\n        std::unique_lock<std::mutex> lk(mut);    \n        data_cond.wait(\n            lk,[]{return !data_queue.empty();});     \n        data_chunk data=data_queue.front();\n        data_queue.pop();\n        lk.unlock();          \n        process(data);\n        if(is_last_chunk(data))\n            break;\n    }\n}\nFirst off, you have a queue B that’s used to pass the data between the two threads.\nWhen the data is ready, the thread preparing the data locks the mutex protecting the\nqueue using a std::lock_guard and pushes the data onto the queue c. It then calls\nthe notify_one() member function on the std::condition_variable instance to\nnotify the waiting thread (if there is one) d. Note that you put the code to push the\ndata onto the queue in a smaller scope, so you notify the condition variable after\nunlocking the mutex — this is so that, if the waiting thread wakes immediately, it\ndoesn’t then have to block again, waiting for you to unlock the mutex.\n On the other side of the fence, you have the processing thread. This thread first\nlocks the mutex, but this time with a std::unique_lock rather than a std::lock_\nguard e—you’ll see why in a minute. The thread then calls wait() on the std::\ncondition_variable, passing in the lock object and a lambda function that expresses\nthe condition being waited for f. Lambda functions are a new feature in C++11 that\nallow you to write an anonymous function as part of another expression, and they’re\nideally suited for specifying predicates for standard library functions such as wait().\nIn this case, the simple []{return !data_queue.empty();} lambda function checks to\nsee if the data_queue is not empty()—that is, there’s some data in the queue ready for\nprocessing. Lambda functions are described in more detail in appendix A, section A.5.\n The implementation of wait() then checks the condition (by calling the supplied\nlambda function) and returns if it’s satisfied (the lambda function returned true). If\nthe condition isn’t satisfied (the lambda function returned false), wait() unlocks\nthe mutex and puts the thread in a blocked or waiting state. When the condition vari-\nable is notified by a call to notify_one() from the data-preparation thread, the thread\nwakes from its slumber (unblocks it), reacquires the lock on the mutex, and checks\nthe condition again, returning from wait() with the mutex still locked if the condi-\ntion has been satisfied. If the condition hasn’t been satisfied, the thread unlocks the\nmutex and resumes waiting. This is why you need the std::unique_lock rather than\nthe std::lock_guard—the waiting thread must unlock the mutex while it’s waiting\ne\nf\ng\n",
      "content_length": 2750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "76\nCHAPTER 4\nSynchronizing concurrent operations\nand lock it again afterward, and std::lock_guard doesn’t provide that flexibility. If\nthe mutex remained locked while the thread was sleeping, the data-preparation\nthread wouldn’t be able to lock the mutex to add an item to the queue, and the wait-\ning thread would never be able to see its condition satisfied.\n Listing 4.1 uses a simple lambda function for the wait f, which checks to see if\nthe queue is not empty, but any function or callable object could be passed. If you\nalready have a function to check the condition (perhaps because it’s more compli-\ncated than a simple test like this), then this function can be passed in directly; there’s\nno need to wrap it in a lambda. During a call to wait(), a condition variable may\ncheck the supplied condition any number of times; but it always does so with the\nmutex locked and will return immediately if (and only if) the function provided to\ntest the condition returns true. When the waiting thread reacquires the mutex and\nchecks the condition, if it isn’t in direct response to a notification from another\nthread, it’s called a spurious wake. Because the number and frequency of any such spu-\nrious wakes are by definition indeterminate, it isn’t advisable to use a function with\nside effects for the condition check. If you do so, you must be prepared for the side\neffects to occur multiple times.\n Fundamentally, std::condition_variable::wait is an optimization over a busy-wait.\nIndeed, a conforming (though less than ideal) implementation technique is just a\nsimple loop:\ntemplate<typename Predicate>\nvoid minimal_wait(std::unique_lock<std::mutex>& lk,Predicate pred){\n    while(!pred()){\n        lk.unlock();\n        lk.lock();\n    }\n}\nYour code must be prepared to work with such a minimal implementation of wait(),\nas well as an implementation that only wakes up if notify_one() or notify_all() is\ncalled.\n The flexibility to unlock a std::unique_lock isn’t just used for the call to wait();\nit’s also used once you have the data to process but before processing it g. Processing\ndata can potentially be a time-consuming operation, and as you saw in chapter 3, it’s a\nbad idea to hold a lock on a mutex for longer than necessary. \n Using a queue to transfer data between threads, as in listing 4.1, is a common sce-\nnario. Done well, the synchronization can be limited to the queue itself, which greatly\nreduces the possible number of synchronization problems and race conditions. In\nview of this, let’s now work on extracting a generic thread-safe queue from listing 4.1.\n4.1.2\nBuilding a thread-safe queue with condition variables\nIf you’re going to be designing a generic queue, it’s worth spending a few minutes\nthinking about the operations that are likely to be required, as you did with the\n",
      "content_length": 2811,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "77\nWaiting for an event or other condition\nthread-safe stack back in section 3.2.3. Let’s look at the C++ Standard Library for\ninspiration, in the form of the std::queue<> container adaptor shown in the follow-\ning listing.\ntemplate <class T, class Container = std::deque<T> >\nclass queue {\npublic:\n    explicit queue(const Container&);\n    explicit queue(Container&& = Container());\n    template <class Alloc> explicit queue(const Alloc&);\n    template <class Alloc> queue(const Container&, const Alloc&);\n    template <class Alloc> queue(Container&&, const Alloc&);\n    template <class Alloc> queue(queue&&, const Alloc&);\n    void swap(queue& q);\n    bool empty() const;\n    size_type size() const;\n    T& front();\n    const T& front() const;\n    T& back();\n    const T& back() const;\n    void push(const T& x);\n    void push(T&& x);\n    void pop();\n    template <class... Args> void emplace(Args&&... args);\n};\nIf you ignore the construction, assignment, and swap operations, you’re left with three\ngroups of operations: those that query the state of the whole queue (empty() and\nsize()), those that query the elements of the queue (front() and back()), and those\nthat modify the queue (push(), pop() and emplace()). This is the same as you had\nback in section 3.2.3 for the stack, and therefore you have the same issues regarding\nrace conditions inherent in the interface. Consequently, you need to combine\nfront() and pop() into a single function call, much as you combined top() and\npop() for the stack. The code from listing 4.1 adds a new nuance, though: when using\na queue to pass data between threads, the receiving thread often needs to wait for the\ndata. Let’s provide two variants on pop(): try_pop(), which tries to pop the value\nfrom the queue but always returns immediately (with an indication of failure) even if\nthere wasn’t a value to retrieve; and wait_and_pop(), which waits until there’s a value\nto retrieve. If you take your lead for the signatures from the stack example, your inter-\nface looks like the following.\n#include <memory>        \ntemplate<typename T>\nclass threadsafe_queue\n{\nListing 4.2\nstd::queue interface\nListing 4.3\nThe interface of your threadsafe_queue\nFor std::shared_ptr\n",
      "content_length": 2216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "78\nCHAPTER 4\nSynchronizing concurrent operations\npublic:\n    threadsafe_queue();\n    threadsafe_queue(const threadsafe_queue&);\n    threadsafe_queue& operator=(\n        const threadsafe_queue&) = delete;    \n    void push(T new_value);\n    bool try_pop(T& value);       \n    std::shared_ptr<T> try_pop();      \n    void wait_and_pop(T& value);\n    std::shared_ptr<T> wait_and_pop();\n    bool empty() const;\n};\nAs you did for the stack, you’ve cut down on the constructors and eliminated assign-\nment in order to simplify the code. You’ve also provided two versions of both\ntry_pop() and wait_for_pop(), as before. The first overload of try_pop() B stores\nthe retrieved value in the referenced variable, so it can use the return value for status;\nit returns true if it retrieved a value and false otherwise (see section A.2). The sec-\nond overload c can’t do this, because it returns the retrieved value directly. But the\nreturned pointer can be set to NULL if there’s no value to retrieve.\n So, how does all this relate to listing 4.1? Well, you can extract the code for push()\nand wait_and_pop() from there, as shown in the next listing.\n#include <queue>\n#include <mutex>\n#include <condition_variable>\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    std::mutex mut;\n    std::queue<T> data_queue;\n    std::condition_variable data_cond;\npublic:\n    void push(T new_value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        data_queue.push(new_value);\n        data_cond.notify_one();\n    }\n    void wait_and_pop(T& value)\n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        value=data_queue.front();\n        data_queue.pop();\n    }\n};\nthreadsafe_queue<data_chunk> data_queue;    \nvoid data_preparation_thread()\nListing 4.4\nExtracting push() and wait_and_pop() from listing 4.1\nDisallow assignment \nfor simplicity.\nb\nc\nb\n",
      "content_length": 1909,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "79\nWaiting for an event or other condition\n{\n    while(more_data_to_prepare())\n    {\n        data_chunk const data=prepare_data();\n        data_queue.push(data);        \n    }\n}\nvoid data_processing_thread()\n{\n    while(true)\n    {\n        data_chunk data;\n        data_queue.wait_and_pop(data);    \n        process(data);\n        if(is_last_chunk(data))\n            break;\n    }\n}\nThe mutex and condition variable are now contained within the threadsafe_queue\ninstance, so separate variables are no longer required B, and no external synchroni-\nzation is required for the call to push() c. Also, wait_and_pop() takes care of the\ncondition variable wait d.\n The other overload of wait_and_pop() is now trivial to write, and the remaining\nfunctions can be copied almost verbatim from the stack example in listing 3.5. The\nfinal queue implementation is shown here.\n#include <queue>\n#include <memory>\n#include <mutex>\n#include <condition_variable>\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    mutable std::mutex mut;    \n    std::queue<T> data_queue;\n    std::condition_variable data_cond;\npublic:\n    threadsafe_queue()\n    {}\n    threadsafe_queue(threadsafe_queue const& other)\n    {\n        std::lock_guard<std::mutex> lk(other.mut);\n        data_queue=other.data_queue;\n    }\n    void push(T new_value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        data_queue.push(new_value);\nListing 4.5\nFull class definition of a thread-safe queue using condition variables\nc\nd\nThe mutex must \nbe mutable.\nb\n",
      "content_length": 1522,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "80\nCHAPTER 4\nSynchronizing concurrent operations\n        data_cond.notify_one();\n    }\n    void wait_and_pop(T& value)\n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        value=data_queue.front();\n        data_queue.pop();\n    }\n    std::shared_ptr<T> wait_and_pop()\n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        std::shared_ptr<T> res(std::make_shared<T>(data_queue.front()));\n        data_queue.pop();\n        return res;\n    }\n    bool try_pop(T& value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return false;\n        value=data_queue.front();\n        data_queue.pop();\n        return true;\n    }\n    std::shared_ptr<T> try_pop()\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return std::shared_ptr<T>();\n        std::shared_ptr<T> res(std::make_shared<T>(data_queue.front()));\n        data_queue.pop();\n        return res;\n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        return data_queue.empty();\n    }\n};\nEven though empty() is a const member function, and the other parameter to the\ncopy constructor is a const reference, other threads may have non-const references\nto the object, and may be calling mutating member functions, so you still need to lock\nthe mutex. Since locking a mutex is a mutating operation, the mutex object must be\nmarked mutable B so it can be locked in empty() and in the copy constructor.\n Condition variables are also useful where there’s more than one thread waiting for\nthe same event. If the threads are being used to divide the workload, and thus only\none thread should respond to a notification, exactly the same structure as shown in\nlisting 4.1 can be used; just run multiple instances of the data-processing thread.\nWhen new data is ready, the call to notify_one() will trigger one of the threads\n",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "81\nWaiting for one-off events with futures\ncurrently executing wait() to check its condition and return from wait() (because\nyou’ve just added an item to the data_queue). There’s no guarantee of which thread\nwill be notified or even if there’s a thread waiting to be notified; all the processing\nthreads might still be processing data.\n Another possibility is that several threads are waiting for the same event, and all of\nthem need to respond. This can happen where shared data is being initialized, and\nthe processing threads can all use the same data but need to wait for it to be initialized\n(although there are potentially better mechanisms for this, such as std::call_once;\nsee section 3.3.1 in chapter 3 for a discussion of the options), or where the threads\nneed to wait for an update to shared data, such as a periodic reinitialization. In these\ncases, the thread preparing the data can call the notify_all() member function on\nthe condition variable rather than notify_one(). As the name suggests, this causes all\nthe threads currently executing wait() to check the condition they’re waiting for.\n If the waiting thread is going to wait only once, so when the condition is true it will\nnever wait on this condition variable again, a condition variable might not be the best\nchoice of synchronization mechanisms. This is especially true if the condition being\nwaited for is the availability of a particular piece of data. In this scenario, a future might\nbe more appropriate.\n4.2\nWaiting for one-off events with futures\nSuppose you’re going on vacation abroad by plane. Once you get to the airport and\nclear the various check-in procedures, you still have to wait for notification that your\nflight is ready for boarding, possibly for several hours. Yes, you might be able to find\nsome means of passing the time, such as reading a book, surfing the internet, or eat-\ning in an overpriced airport café, but fundamentally you’re just waiting for one thing:\nthe signal that it’s time to get on the plane. Not only that, but a given flight goes only\nonce; the next time you’re going on vacation, you’ll be waiting for a different flight.\n The C++ Standard Library models this sort of one-off event with something called a\nfuture. If a thread needs to wait for a specific one-off event, it somehow obtains a future\nrepresenting that event. The thread can then periodically wait on the future for short\nperiods of time to see if the event has occurred (check the departures board) while per-\nforming some other task (eating in the overpriced café) between checks. Alternatively, it\ncan do another task until it needs the event to have happened before it can proceed and\nthen just wait for the future to become ready. A future may have data associated with it\n(such as which gate your flight is boarding at), or it may not. Once an event has hap-\npened (and the future has become ready), the future can’t be reset.\n There are two sorts of futures in the C++ Standard Library, implemented as two\nclass templates declared in the <future> library header: unique futures (std::future<>)\nand shared futures (std::shared_future<>). These are modeled after std::unique_ptr\nand std::shared_ptr. An instance of std::future is the one and only instance that\nrefers to its associated event, whereas multiple instances of std::shared_future may\nrefer to the same event. In the latter case, all the instances will become ready at the same\n",
      "content_length": 3428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "82\nCHAPTER 4\nSynchronizing concurrent operations\ntime, and they may all access any data associated with the event. This associated data is\nthe reason these are templates; just like std::unique_ptr and std::shared_ptr, the\ntemplate parameter is the type of the associated data. The std:future<void> and\nstd::shared_future<void> template specializations should be used where there’s no\nassociated data. Although futures are used to communicate between threads, the\nfuture objects themselves don’t provide synchronized accesses. If multiple threads\nneed to access a single future object, they must protect access via a mutex or other syn-\nchronization mechanism, as described in chapter 3. But as you’ll see in section 4.2.5,\nmultiple threads may each access their own copy of std::shared_future<> without\nfurther synchronization, even if they all refer to the same asynchronous result.\n The Concurrency TS provides extended versions of these class templates in the\nstd::experimental namespace: std::experimental::future<> and std::experi-\nmental::shared_future<>. These behave identically to their counterparts in the std\nnamespace, but they have additional member functions to provide additional facili-\nties. It is important to note that the name std::experimental does not imply any-\nthing about the quality of the code (I would hope that the implementation will be the\nsame quality as everything else shipped from your library vendor), but highlights that\nthese are non-standard classes and functions, and therefore may not have exactly the\nsame syntax and semantics if and when they are finally adopted into a future C++ Stan-\ndard. To use these facilities, you must include the <experimental/future> header.\n The most basic of one-off events is the result of a calculation that has been run in\nthe background. Back in chapter 2 you saw that std::thread doesn’t provide an easy\nmeans of returning a value from such a task, and I promised that this would be\naddressed in chapter 4 with futures—now it’s time to see how.\n4.2.1\nReturning values from background tasks\nSuppose you have a long-running calculation that you expect will eventually yield a\nuseful result but for which you don’t currently need the value. Maybe you’ve found a\nway to determine the answer to Life, the Universe, and Everything, to pinch an exam-\nple from Douglas Adams.1 You could start a new thread to perform the calculation, but\nthat means you have to take care of transferring the result back, because std::thread\ndoesn’t provide a direct mechanism for doing so. This is where the std::async func-\ntion template (also declared in the <future> header) comes in.\n You use std::async to start an asynchronous task for which you don’t need the\nresult right away. Rather than giving you a std::thread object to wait on, std::async\nreturns a std::future object, which will eventually hold the return value of the func-\ntion. When you need the value, you just call get() on the future, and the thread\nblocks until the future is ready and then returns the value. The following listing shows\na simple example.\n \n1 In The Hitchhiker’s Guide to the Galaxy, the computer Deep Thought is built to determine “the answer to Life,\nthe Universe and Everything.” The answer is 42.\n",
      "content_length": 3242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "83\nWaiting for one-off events with futures\n#include <future>\n#include <iostream>\nint find_the_answer_to_ltuae();\nvoid do_other_stuff();\nint main()\n{\n    std::future<int> the_answer=std::async(find_the_answer_to_ltuae);\n    do_other_stuff();\n    std::cout<<\"The answer is \"<<the_answer.get()<<std::endl;\n}\nstd::async allows you to pass additional arguments to the function by adding extra\narguments to the call, in the same way that std::thread does. If the first argument is\na pointer to a member function, the second argument provides the object on which to\napply the member function (either directly, or via a pointer, or wrapped in std::ref),\nand the remaining arguments are passed as arguments to the member function.\nOtherwise, the second and subsequent arguments are passed as arguments to the\nfunction or callable object specified as the first argument. Just as with std::thread,\nif the arguments are rvalues, the copies are created by moving the originals. This\nallows the use of move-only types as both the function object and the arguments.\nSee the following listing.\n#include <string>\n#include <future>\nstruct X\n{\n    void foo(int,std::string const&);\n    std::string bar(std::string const&);\n};\nX x;\nauto f1=std::async(&X::foo,&x,42,\"hello\");     \nauto f2=std::async(&X::bar,x,\"goodbye\");    \nstruct Y\n{\n    double operator()(double);\n};\nY y;\nauto f3=std::async(Y(),3.141);       \nauto f4=std::async(std::ref(y),2.718);     \nX baz(X&);\nstd::async(baz,std::ref(x));     \nclass move_only\n{\npublic:\n    move_only();\n    move_only(move_only&&)\n    move_only(move_only const&) = delete;\n    move_only& operator=(move_only&&);\nListing 4.6\nUsing std::future to get the return value of an asynchronous task\nListing 4.7\nPassing arguments to a function with std::async\nCalls p->foo(42,\"hello\") \nwhere p is &x\nCalls tmpx.bar(\"goodbye\") \nwhere tmpx is a copy of x\nCalls tmpy(3.141) where tmpy \nis move-constructed from Y()\nCalls y(2.718)\nCalls baz(x)\n",
      "content_length": 1951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "84\nCHAPTER 4\nSynchronizing concurrent operations\n    move_only& operator=(move_only const&) = delete;\n    void operator()();\n};\nauto f5=std::async(move_only());    \nBy default, it’s up to the implementation whether std::async starts a new thread, or\nwhether the task runs synchronously when the future is waited for. In most cases this is\nwhat you want, but you can specify which to use with an additional parameter to\nstd::async before the function to call. This parameter is of the type std::launch,\nand can either be std::launch::deferred to indicate that the function call is to be\ndeferred until either wait() or get() is called on the future, std::launch::async to\nindicate that the function must be run on its own thread, or std::launch::deferred\n| std::launch::async to indicate that the implementation may choose. This last\noption is the default. If the function call is deferred, it may never run. For example:\nauto f6=std::async(std::launch::async,Y(),1.2);     \nauto f7=std::async(std::launch::deferred,baz,std::ref(x));    \nauto f8=std::async(                           \n   std::launch::deferred | std::launch::async,\n   baz,std::ref(x));                          \nauto f9=std::async(baz,std::ref(x));          \nf7.wait();                       \nAs you’ll see later in this chapter and again in chapter 8, using std::async makes it\neasy to divide algorithms into tasks that can be run concurrently. However, it’s not the\nonly way to associate a std::future with a task; you can also do it by wrapping the task\nin an instance of the std::packaged_task<> class template or by writing code to\nexplicitly set the values using the std::promise<> class template. std::packaged_\ntask is a higher-level abstraction than std::promise, so I’ll start with that.\n4.2.2\nAssociating a task with a future\nstd::packaged_task<> ties a future to a function or callable object. When the std::\npackaged_task<> object is invoked, it calls the associated function or callable object\nand makes the future ready, with the return value stored as the associated data. This\ncan be used as a building block for thread pools (see chapter 9) or other task manage-\nment schemes, such as running each task on its own thread, or running them all\nsequentially on a particular background thread. If a large operation can be divided\ninto self-contained sub-tasks, each of these can be wrapped in a std::packaged_\ntask<> instance, and then that instance passed to the task scheduler or thread pool.\nThis abstracts out the details of the tasks; the scheduler just deals with std::packaged\n_task<> instances rather than individual functions.\n The template parameter for the std::packaged_task<> class template is a func-\ntion signature, like void() for a function taking no parameters with no return value,\nor int(std::string&,double*) for a function that takes a non-const reference to a\nstd::string and a pointer to a double and returns an int. When you construct an\nCalls tmp() where tmp is constructed \nfrom std::move(move_only())\nRun in new thread\nRun in wait() \nor get()\nImplementation \nchooses\nInvoke deferred function\n",
      "content_length": 3100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "85\nWaiting for one-off events with futures\ninstance of std::packaged_task, you must pass in a function or callable object that\ncan accept the specified parameters and that returns a type that’s convertible to the\nspecified return type. The types don’t have to match exactly; you can construct a\nstd::packaged_task<double(double)> from a function that takes an int and returns\na float because the types are implicitly convertible.\n The return type of the specified function signature identifies the type of the\nstd::future<> returned from the get_future() member function, whereas the argu-\nment list of the function signature is used to specify the signature of the packaged\ntask’s function call operator. For example, a partial class definition for std::packaged\n_task <std::string(std::vector<char>*,int)> would be as shown in the follow-\ning listing.\ntemplate<>\nclass packaged_task<std::string(std::vector<char>*,int)>\n{\npublic:\n    template<typename Callable>\n    explicit packaged_task(Callable&& f);\n    std::future<std::string> get_future();\n    void operator()(std::vector<char>*,int);\n};\nThe std::packaged_task object is a callable object, and it can be wrapped in a\nstd::function object, passed to a std::thread as the thread function, passed to\nanother function that requires a callable object, or even invoked directly. When the\nstd::packaged_task is invoked as a function object, the arguments supplied to the\nfunction call operator are passed on to the contained function, and the return value is\nstored as the asynchronous result in the std::future obtained from get_future().\nYou can thus wrap a task in a std::packaged_task and retrieve the future before pass-\ning the std::packaged_task object elsewhere to be invoked in due course. When you\nneed the result, you can wait for the future to become ready. The following example\nshows this in action.\nPASSING TASKS BETWEEN THREADS\nMany GUI frameworks require that updates to the GUI be done from specific threads,\nso if another thread needs to update the GUI, it must send a message to the right\nthread in order to do so. std:packaged_task provides one way of doing this without\nrequiring a custom message for each and every GUI-related activity, as shown here.\n#include <deque>\n#include <mutex>\n#include <future>\n#include <thread>\nListing 4.8\nPartial class definition for a specialization of std::packaged_task< >\nListing 4.9\nRunning code on a GUI thread using std::packaged_task\n",
      "content_length": 2446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "86\nCHAPTER 4\nSynchronizing concurrent operations\n#include <utility>\nstd::mutex m;\nstd::deque<std::packaged_task<void()> > tasks;\nbool gui_shutdown_message_received();\nvoid get_and_process_gui_message();\nvoid gui_thread()                   \n{\n    while(!gui_shutdown_message_received())   \n    {\n        get_and_process_gui_message();    \n        std::packaged_task<void()> task;\n        {\n            std::lock_guard<std::mutex> lk(m);\n            if(tasks.empty())                 \n                continue;\n            task=std::move(tasks.front());   \n            tasks.pop_front();\n        }\n        task();    \n    }\n}\nstd::thread gui_bg_thread(gui_thread);\ntemplate<typename Func>\nstd::future<void> post_task_for_gui_thread(Func f)\n{\n    std::packaged_task<void()> task(f);       \n    std::future<void> res=task.get_future();     \n    std::lock_guard<std::mutex> lk(m);\n    tasks.push_back(std::move(task));     \n    return res;                      \n}\nThis code is simple: the GUI thread B loops until a message has been received telling\nthe GUI to shut down c, repeatedly polling for GUI messages to handle d, such as\nuser clicks, and for tasks on the task queue. If there are no tasks on the queue e, it\nloops again; otherwise, it extracts the task from the queue f, releases the lock on the\nqueue, and then runs the task g. The future associated with the task will then be\nmade ready when the task completes.\n Posting a task on the queue is equally simple: a new packaged task is created from\nthe supplied function h, the future is obtained from that task i by calling the get_-\nfuture() member function, and the task is put on the list j before the future is\nreturned to the caller 1). The code that posted the message to the GUI thread can\nthen wait for the future if it needs to know that the task has been completed, or it can\ndiscard the future if it doesn’t need to know.\n This example uses std::packaged_task<void()> for the tasks, which wraps a\nfunction or other callable object that takes no parameters and returns void (if it\nreturns anything else, the return value is discarded). This is the simplest possible task,\nbut as you saw earlier, std::packaged_task can also be used in more complex situa-\ntions—by specifying a different function signature as the template parameter, you can\nb\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n",
      "content_length": 2327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "87\nWaiting for one-off events with futures\nchange the return type (and thus the type of data stored in the future’s associated\nstate) and also the argument types of the function call operator. This example could eas-\nily be extended to allow for tasks that are to be run on the GUI thread to accept argu-\nments and return a value in the std::future rather than just a completion indicator.\n What about those tasks that can’t be expressed as a simple function call or those\ntasks where the result may come from more than one place? These cases are dealt with\nby the third way of creating a future: using std::promise to set the value explicitly.\n4.2.3\nMaking (std::)promises\nWhen you have an application that needs to handle a lot of network connections, it’s\noften tempting to handle each connection on a separate thread, because this can\nmake the network communication easier to think about and easier to program. This\nworks well for low numbers of connections (and thus low numbers of threads). Unfor-\ntunately, as the number of connections rises, this becomes less suitable; the large num-\nbers of threads consequently consume large amounts of OS resources and potentially\ncause a lot of context switching (when the number of threads exceeds the available\nhardware concurrency), impacting performance. In extreme cases, the OS may run\nout of resources for running new threads before its capacity for network connections\nis exhausted. In applications with large numbers of network connections, it’s there-\nfore common to have a small number of threads (possibly only one) handling the con-\nnections, with each thread dealing with multiple connections at once.\n Consider one of these threads handling the connections. Data packets will come in\nfrom the various connections being handled in essentially random order, and like-\nwise, data packets will be queued to be sent in random order. In many cases, other\nparts of the application will be waiting either for data to be successfully sent or for a\nnew batch of data to be successfully received via a specific network connection.\n std::promise<T> provides a means of setting a value (of type T) that can later be\nread through an associated std::future<T> object. A std::promise/std::future\npair would provide one possible mechanism for this facility; the waiting thread could\nblock on the future, while the thread providing the data could use the promise half of\nthe pairing to set the associated value and make the future ready.\n You can obtain the std::future object associated with a given std::promise by\ncalling the get_future() member function, just like with std::packaged_task. When\nthe value of the promise is set (using the set_value() member function), the future\nbecomes ready and can be used to retrieve the stored value. If you destroy the\nstd::promise without setting a value, an exception is stored instead. Section 4.2.4\ndescribes how exceptions are transferred across threads.\n Listing 4.10 shows some example code for a thread that’s processing connections\nas just described. In this example, you use a std::promise<bool>/std::future<bool>\npair to identify the successful transmission of a block of outgoing data; the value asso-\nciated with the future is a simple success/failure flag. For incoming packets, the data\nassociated with the future is the payload of the data packet.\n",
      "content_length": 3350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "88\nCHAPTER 4\nSynchronizing concurrent operations\n#include <future>\nvoid process_connections(connection_set& connections)\n{\n    while(!done(connections))   \n    {\n        for(connection_iterator        \n                connection=connections.begin(),end=connections.end();\n            connection!=end;\n            ++connection)\n        {\n            if(connection->has_incoming_data())    \n            {\n                data_packet data=connection->incoming();\n                std::promise<payload_type>& p=\n                    connection->get_promise(data.id);    \n                p.set_value(data.payload);\n            }\n            if(connection->has_outgoing_data())    \n            {\n                outgoing_packet data=\n                    connection->top_of_outgoing_queue();\n                connection->send(data.payload);\n                data.promise.set_value(true);     \n            }\n        }\n    }\n}\nThe process_connections() function loops until done() returns true B. Every\ntime it goes through the loop, it checks each connection in turn c, retrieving\nincoming data if there is any d or sending any queued outgoing data f. This\nassumes that an incoming packet has an ID and a payload with the data in it. The ID\nis mapped to a std::promise (perhaps by a lookup in an associative container) e,\nand the value is set to the packet’s payload. For outgoing packets, the packet is\nretrieved from the outgoing queue and sent through the connection. Once the send\nhas completed, the promise associated with the outgoing data is set to true to indi-\ncate successful transmission g. Whether this maps nicely to the network protocol\ndepends on the protocol; this promise/future style structure may not work for a par-\nticular scenario, although it does have a similar structure to the asynchronous I/O\nsupport of some OSes.\n All the code up to now has completely disregarded exceptions. Although it might\nbe nice to imagine a world in which everything worked all the time, this isn’t the case.\nSometimes disks fill up, sometimes what you’re looking for just isn’t there, sometimes\nthe network fails, and sometimes the database goes down. If you were performing the\noperation in the thread that needed the result, the code could just report an error\nwith an exception, so it would be unnecessarily restrictive to require that everything\ngoes well just because you wanted to use std::packaged_task or std::promise. The\nListing 4.10\nHandling multiple connections from a single thread using promises\nb\nc\nd\ne\nf\ng\n",
      "content_length": 2514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "89\nWaiting for one-off events with futures\nC++ Standard Library therefore provides a clean way to deal with exceptions in such a\nscenario and allows them to be saved as part of the associated result.\n4.2.4\nSaving an exception for the future\nConsider the following short snippet of code. If you pass in -1 to the square_root()\nfunction, it throws an exception, and this gets seen by the caller:\ndouble square_root(double x)\n{\n    if(x<0)\n    {\n        throw std::out_of_range(“x<0”);\n    }\n    return sqrt(x);\n}\nNow suppose that instead of just invoking square_root() from the current thread\ndouble y=square_root(-1);\nyou run the call as an asynchronous call:\nstd::future<double> f=std::async(square_root,-1);\ndouble y=f.get();\nIt would be ideal if the behavior was exactly the same; just as y gets the result of the\nfunction call in either case, it would be great if the thread that called f.get() could\nsee the exception too, just as it would in the single-threaded case.\n Well, that’s exactly what happens: if the function call invoked as part of std::async\nthrows an exception, that exception is stored in the future in place of a stored value, the\nfuture becomes ready, and a call to get() rethrows that stored exception. (Note: the\nstandard leaves it unspecified whether it is the original exception object that’s rethrown\nor a copy; different compilers and libraries make different choices on this matter.) The\nsame happens if you wrap the function in a std::packaged_task—when the task is\ninvoked, if the wrapped function throws an exception, that exception is stored in the\nfuture in place of the result, ready to be thrown on a call to get().\n Naturally, std::promise provides the same facility, with an explicit function call. If\nyou wish to store an exception rather than a value, you call the set_exception()\nmember function rather than set_value(). This would typically be used in a catch\nblock for an exception thrown as part of the algorithm, to populate the promise with\nthat exception:\nextern std::promise<double> some_promise;\ntry\n{\n    some_promise.set_value(calculate_value());\n}\n",
      "content_length": 2100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "90\nCHAPTER 4\nSynchronizing concurrent operations\ncatch(...)\n{\n    some_promise.set_exception(std::current_exception());\n}\nThis uses std::current_exception() to retrieve the thrown exception; the alterna-\ntive here would be to use std::make_exception_ptr() to store a new exception\ndirectly without throwing:\nsome_promise.set_exception(std::make_exception_ptr(std::logic_error(\"foo \")));\nThis is much cleaner than using a try/catch block if the type of the exception is\nknown, and it should be used in preference; not only does it simplify the code, but it\nalso provides the compiler with greater opportunity to optimize the code.\n Another way to store an exception in a future is to destroy the std::promise or\nstd::packaged_task associated with the future without calling either of the set func-\ntions on the promise or invoking the packaged task. In either case, the destructor of\nstd::promise or std::packaged_task will store a std::future_error exception with\nan error code of std::future_errc::broken_promise in the associated state if the\nfuture isn’t already ready; by creating a future you make a promise to provide a value\nor exception, and by destroying the source of that value or exception without provid-\ning one, you break that promise. If the compiler didn’t store anything in the future in\nthis case, waiting threads could potentially wait forever.\n Up until now, all the examples have used std::future. However, std::future has\nits limitations, not the least of which being that only one thread can wait for the result.\nIf you need to wait for the same event from more than one thread, you need to use\nstd::shared_future instead.\n4.2.5\nWaiting from multiple threads\nAlthough std::future handles all the synchronization necessary to transfer data from\none thread to another, calls to the member functions of a particular std::future\ninstance are not synchronized with each other. If you access a single std::future\nobject from multiple threads without additional synchronization, you have a data race\nand undefined behavior. This is by design: std::future models unique ownership of\nthe asynchronous result, and the one-shot nature of get() makes such concurrent\naccess pointless anyway—only one thread can retrieve the value, because after the first\ncall to get() there’s no value left to retrieve.\n If your fabulous design for your concurrent code requires that multiple threads\ncan wait for the same event, don’t despair just yet; std::shared_future allows exactly\nthat. Whereas std::future is only moveable (so ownership can be transferred between\ninstances, but only one instance refers to a particular asynchronous result at a time),\nstd::shared_future instances are copyable (so you can have multiple objects referring\nto the same associated state).\n",
      "content_length": 2773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "91\nWaiting for one-off events with futures\n Now, with std::shared_future, member functions on an individual object are still\nunsynchronized, so to avoid data races when accessing a single object from multiple\nthreads, you must protect accesses with a lock. The preferred way to use it would be to\npass a copy of the shared_future object to each thread, so each thread can access its\nown local shared_future object safely, as the internals are now correctly synchronized\nby the library. Accessing the shared asynchronous state from multiple threads is safe if\neach thread accesses that state through its own std::shared_future object. See fig-\nure 4.1.\n One potential use of std::shared_future is for implementing parallel execution\nof something akin to a complex spreadsheet; each cell has a single final value, which\nmay be used by the formulas in multiple other cells. The formulas for calculating the\nresults of the dependent cells can then use std::shared_future to reference the first\nThread 1\nThread 2\nstd::shared_future<int>\nShared variable sf\nint\nsf.wait()\nsf.wait()\nData race on sf\nwithout synchronization\nShared variable sf\nint\nCopying is safe.\nThread 1\nlocal\nThread 2\nlocal\nRefers to\nRefers to\nSeparate objects,\nso no data race\nstd::shared_future<int>\nstd::shared_future<int>\nstd::shared_future<int>\nauto local=sf;\nauto local=sf;\nlocal.wait()\nlocal.wait()\nRefers to\nasynchronous\nresult\nRefers to\nasynchronous\nresult\nFigure 4.1\nUsing multiple std::shared_future objects to avoid data races\n",
      "content_length": 1500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "92\nCHAPTER 4\nSynchronizing concurrent operations\ncell. If all the formulas for the individual cells are then executed in parallel, those\ntasks that can proceed to completion will do so, whereas those that depend on others\nwill block until their dependencies are ready. This will allow the system to make maxi-\nmum use of the available hardware concurrency.\n Instances of std::shared_future that reference some asynchronous state are con-\nstructed from instances of std::future that reference that state. Since std::future\nobjects don’t share ownership of the asynchronous state with any other object, the\nownership must be transferred into the std::shared_future using std::move, leav-\ning std::future in an empty state, as if it were a default constructor:\nstd::promise<int> p;\nstd::future<int> f(p.get_future());\nassert(f.valid());                        \nstd::shared_future<int> sf(std::move(f));\nassert(!f.valid());                       \nassert(sf.valid());   \nHere, the future f is initially valid B because it refers to the asynchronous state of\nthe promise p, but after transferring the state to sf, f is no longer valid c, whereas\nsf is d.\n Just as with other movable objects, the transfer of ownership is implicit for rval-\nues, so you can construct a std::shared_future directly from the return value of the\nget_future() member function of a std::promise object, for example:\nstd::promise<std::string> p;\nstd::shared_future<std::string> sf(p.get_future());    \nHere, the transfer of ownership is implicit; std::shared_future<> is constructed\nfrom an rvalue of type std::future<std::string> B.\n std::future also has an additional feature to facilitate the use of std::shared_\nfuture, with the new facility for automatically deducing the type of a variable from its\ninitializer (see appendix A, section A.6). std::future has a share() member func-\ntion that creates a new std::shared_future and transfers ownership to it directly.\nThis can save a lot of typing and makes code easier to change:\nstd::promise< std::map< SomeIndexType, SomeDataType, SomeComparator,\n    SomeAllocator>::iterator> p;\nauto sf=p.get_future().share();                                       \nIn this case, the type of sf is deduced to be std::shared_future< std::map< Some-\nIndexType, SomeDataType, SomeComparator, SomeAllocator>::iterator>, which is\nrather a mouthful. If the comparator or allocator is changed, you only need to change\nthe type of the promise; the type of the future is automatically updated to match.\n Sometimes you want to limit the amount of time you’re waiting for an event, either\nbecause you have a hard time limit on how long a particular section of code may take,\nor because there’s other useful work that the thread can be doing if the event isn’t\nThe future \nf is valid.\nb\nf is no \nlonger valid.\nc\nsf is now valid.\nd\nImplicit transfer \nof ownership\nb\n",
      "content_length": 2863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "93\nWaiting with a time limit\ngoing to happen soon. To handle this facility, many of the waiting functions have vari-\nants that allow a timeout to be specified.\n4.3\nWaiting with a time limit\nAll the blocking calls introduced previously will block for an indefinite period of time,\nsuspending the thread until the event being waited for occurs. In many cases this is\nfine, but in some cases you may want to put a limit on how long you wait. This might\nbe to allow you to send some form of “I’m still alive” message either to an interactive\nuser, or another process, or indeed to allow you to abort the wait if the user has given\nup waiting and clicked Cancel.\n There are two sorts of timeouts you may wish to specify: a duration-based timeout,\nwhere you wait for a specific amount of time (for example, 30 milliseconds); or an\nabsolute timeout, where you wait until a specific point in time (for example,\n17:30:15.045987023 UTC on November 30, 2011). Most of the waiting functions pro-\nvide variants that handle both forms of timeouts. The variants that handle the duration-\nbased timeouts have a _for suffix, and those that handle the absolute timeouts have\nan _until suffix.\n So, for example, std::condition_variable has two overloads of the wait_for()\nmember function and two overloads of the wait_until() member function that cor-\nrespond to the two overloads of wait()—one overload that just waits until signaled, or\nthe timeout expires, or a spurious wakeup occurs; and another that will check the sup-\nplied predicate when woken and will return only when the supplied predicate is true\n(and the condition variable has been signaled) or the timeout expires.\n Before we look at the details of the functions that use the timeouts, let’s examine\nthe way that times are specified in C++, starting with clocks.\n4.3.1\nClocks\nAs far as the C++ Standard Library is concerned, a clock is a source of time informa-\ntion. Specifically, a clock is a class that provides four distinct pieces of information:\nThe time now\nThe type of the value used to represent the times obtained from the clock\nThe tick period of the clock\nWhether or not the clock ticks at a uniform rate and is therefore considered to\nbe a steady clock\nThe current time of a clock can be obtained by calling the now() static member function\nfor that clock class; for example, std::chrono::system_clock::now() will return the\ncurrent time of the system clock. The type of the time points for a particular clock is spec-\nified by the time_point member typedef, so the return type of some_clock::now() is\nsome_clock::time_point.\n The tick period of the clock is specified as a fractional number of seconds, which is\ngiven by the period member typedef of the clock—a clock that ticks 25 times per\n",
      "content_length": 2757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "94\nCHAPTER 4\nSynchronizing concurrent operations\nsecond has a period of std::ratio<1,25>, whereas a clock that ticks every 2.5 sec-\nonds has a period of std::ratio<5,2>. If the tick period of a clock can’t be known\nuntil runtime, or it may vary during a given run of the application, the period may\nbe specified as the average tick period, smallest possible tick period, or some other\nvalue that the library writer deems appropriate. There’s no guarantee that the\nobserved tick period in a given run of the program matches the specified period for\nthat clock.\n If a clock ticks at a uniform rate (whether or not that rate matches the period) and\ncan’t be adjusted, the clock is said to be a steady clock. The is_steady static data mem-\nber of the clock class is true if the clock is steady, and false otherwise. Typically,\nstd::chrono::system_clock will not be steady, because the clock can be adjusted,\neven if such adjustment is done automatically to take account of local clock drift. Such\nan adjustment may cause a call to now() to return a value earlier than that returned by\na prior call to now(), which is in violation of the requirement for a uniform tick rate.\nSteady clocks are important for timeout calculations, as you’ll see shortly, so the C++\nStandard Library provides one in the form of std::chrono::steady_clock. The\nother clocks provided by the C++ Standard Library are std::chrono::system_clock\n(mentioned earlier), which represents the “real-time” clock of the system and pro-\nvides functions for converting its time points to and from time_t values, and\nstd::chrono::high_resolution_clock, which provides the smallest possible tick\nperiod (and thus the highest possible resolution) of all the library-supplied clocks. It\nmay be a typedef to one of the other clocks. These clocks are defined in the <chrono>\nlibrary header, along with the other time facilities.\n We’ll look at the representation of time points shortly, but first let’s look at how\ndurations are represented.\n4.3.2\nDurations\nDurations are the simplest part of the time support; they’re handled by the std::\nchrono::duration<> class template (all the C++ time-handling facilities used by the\nThread Library are in the std::chrono namespace). The first template parameter is\nthe type of the representation (such as int, long, or double), and the second is a frac-\ntion specifying how many seconds each unit of the duration represents. For example,\na number of minutes stored in a short is std::chrono::duration<short,std::\nratio<60,1>>, because there are 60 seconds in a minute. On the other hand, a count\nof milliseconds stored in a double is std::chrono::duration<double,std::ratio\n<1,1000>>, because each millisecond is 1/1000th of a second.\n The Standard Library provides a set of predefined typedefs in the std::chrono\nnamespace for various durations: nanoseconds, microseconds, milliseconds, sec-\nonds, minutes, and hours. They all use a sufficiently large integral type for the repre-\nsentation chosen such that you can represent a duration of over 500 years in the\nappropriate units if you so desire. There are also typedefs for all the SI ratios from\nstd::atto (10–18) to std::exa (1018) (and beyond, if your platform has 128-bit\n",
      "content_length": 3223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "95\nWaiting with a time limit\ninteger types) for use when specifying custom durations such as std::duration<d-\nouble,std::centi> for a count of 1/100th of a second represented in a double.\n For convenience, there are a number of predefined literal suffix operators for\ndurations in the std::chrono_literals namespace, introduced with C++14. This can\nsimplify code that uses hard-coded duration values, such as\nusing namespace std::chrono_literals;\nauto one_day=24h;\nauto half_an_hour=30min;\nauto max_time_between_messages=30ms;\nWhen used with integer literals, these suffixes are equivalent to using the predefined\nduration typedefs, so 15ns and std::chrono::nanoseconds(15) are identical values.\nHowever, when used with floating-point literals, these suffixes create a suitably-scaled\nfloating-point duration with an unspecified representation type. Therefore, 2.5min\nwill be std::chrono::duration<some-floating-point-type,std::ratio<60,1>>. If\nyou are concerned about the range or precision of the implementation’s chosen float-\ning point type, then you will need to construct an object with a suitable representation\nyourself, rather than using the convenience of the literal suffixes.\n Conversion between durations is implicit where it does not require truncation of\nthe value (so converting hours to seconds is OK, but converting seconds to hours is\nnot). Explicit conversions can be done with std::chrono::duration_cast<>:\nstd::chrono::milliseconds ms(54802);\nstd::chrono::seconds s=\n    std::chrono::duration_cast<std::chrono::seconds>(ms);\nThe result is truncated rather than rounded, so s will have a value of 54 in this example.\n Durations support arithmetic, so you can add and subtract durations to get new\ndurations or multiply or divide by a constant of the underlying representation type\n(the first template parameter). Thus 5*seconds(1) is the same as seconds(5) or\nminutes(1) – seconds(55). The count of the number of units in the duration can be\nobtained with the count() member function. Thus std::chrono::milliseconds(1234)\n.count() is 1234.\n Duration-based waits are done with instances of std::chrono::duration<>. For\nexample, you can wait for up to 35 milliseconds for a future to be ready:\nstd::future<int> f=std::async(some_task);\nif(f.wait_for(std::chrono::milliseconds(35))==std::future_status::ready)\n    do_something_with(f.get());\nThe wait functions all return a status to indicate whether the wait timed out or the\nwaited-for event occurred. In this case, you’re waiting for a future, so the function\nreturns std::future_status::timeout if the wait times out, std::future_status::\nready if the future is ready, or std::future_status::deferred if the future’s task is\ndeferred. The time for a duration-based wait is measured using a steady clock internal\n",
      "content_length": 2784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "96\nCHAPTER 4\nSynchronizing concurrent operations\nto the library, so 35 milliseconds means 35 milliseconds of elapsed time, even if the\nsystem clock was adjusted (forward or back) during the wait. Of course, the vagaries\nof system scheduling and the varying precisions of OS clocks means that the time\nbetween the thread issuing the call and returning from it may be much longer than\n35 ms.\n With durations under our belt, we can now move on to time points.\n4.3.3\nTime points\nThe time point for a clock is represented by an instance of the std::chrono::time_\npoint<> class template, which specifies which clock it refers to as the first template\nparameter and the units of measurement (a specialization of std::chrono::dura-\ntion<>) as the second template parameter. The value of a time point is the length of\ntime (in multiples of the specified duration) since a specific point in time called the\nepoch of the clock. The epoch of a clock is a basic property but not something that’s\ndirectly available to query or specified by the C++ Standard. Typical epochs include\n00:00 on January 1, 1970 and the instant when the computer running the application\nbooted up. Clocks may share an epoch or have independent epochs. If two clocks\nshare an epoch, the time_point typedef in one class may specify the other as the clock\ntype associated with the time_point. Although you can’t find out when the epoch is,\nyou can get the time_since_epoch() for a given time_point. This member function\nreturns a duration value specifying the length of time since the clock epoch to that\nparticular time point.\n For example, you might specify a time point as std::chrono::time_point<std::\nchrono::system_clock, std::chrono::minutes>. This would hold the time relative\nto the system clock but measured in minutes as opposed to the native precision of the\nsystem clock (which is typically seconds or less).\n You can add durations and subtract durations from instances of std::chrono::\ntime_point<> to produce new time points, so std::chrono::high_resolution_clock::\nnow() + std::chrono::nanoseconds(500) will give you a time 500 nanoseconds in the\nfuture. This is good for calculating an absolute timeout when you know the maximum\nduration of a block of code, but there are multiple calls to waiting functions within\nit or nonwaiting functions that precede a waiting function but take up some of the\ntime budget.\n You can also subtract one time point from another that shares the same clock. The\nresult is a duration specifying the length of time between the two time points. This is\nuseful for timing blocks of code, for example:\nauto start=std::chrono::high_resolution_clock::now();\ndo_something();\nauto stop=std::chrono::high_resolution_clock::now();\nstd::cout<<”do_something() took “\n  <<std::chrono::duration<double,std::chrono::seconds>(stop-start).count()\n  <<” seconds”<<std::endl;\n",
      "content_length": 2866,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "97\nWaiting with a time limit\nThe clock parameter of a std::chrono::time_point<> instance does more than just\nspecify the epoch, though. When you pass the time point to a wait function that takes\nan absolute timeout, the clock parameter of the time point is used to measure the\ntime. This has important consequences when the clock is changed, because the wait\ntracks the clock change and won’t return until the clock’s now() function returns a\nvalue later than the specified timeout. If the clock is adjusted forward, this may reduce\nthe total length of the wait (as measured by a steady clock), and if it’s adjusted back-\nward, this may increase the total length of the wait.\n As you may expect, time points are used with the _until variants of the wait func-\ntions. The typical use case is as an offset from some-clock::now() at a fixed point in the\nprogram, although time points associated with the system clock can be obtained by\nconverting from time_t using the std::chrono::system_clock::to_time_point()\nstatic member function to schedule operations at a user-visible time. For example, if\nyou have a maximum of 500 milliseconds to wait for an event associated with a condi-\ntion variable, you might do something like in the following listing.\n#include <condition_variable>\n#include <mutex>\n#include <chrono>\nstd::condition_variable cv;\nbool done;\nstd::mutex m;\nbool wait_loop()\n{\n    auto const timeout= std::chrono::steady_clock::now()+\n        std::chrono::milliseconds(500);\n    std::unique_lock<std::mutex> lk(m);\n    while(!done)\n    {\n        if(cv.wait_until(lk,timeout)==std::cv_status::timeout)\n            break;\n    }\n    return done;\n}\nThis is the recommended way to wait for condition variables with a time limit if you’re\nnot passing a predicate to wait. This way, the overall length of the loop is bounded. As\nyou saw in section 4.1.1, you need to loop when using condition variables if you don’t\npass in the predicate, in order to handle spurious wakeups. If you use wait_for() in a\nloop, you might end up waiting almost the full length of time before a spurious wake-\nup, and the next time through the wait time starts again. This may repeat any number\nof times, making the total wait time unbounded.\n With the basics of specifying timeouts under your belt, let’s look at the functions\nthat you can use timeout with.\nListing 4.11\nWaiting for a condition variable with a timeout\n",
      "content_length": 2401,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "98\nCHAPTER 4\nSynchronizing concurrent operations\n4.3.4\nFunctions that accept timeouts\nThe simplest use for a timeout is to add a delay to the processing of a particular\nthread so that it doesn’t take processing time away from other threads when it has\nnothing to do. You saw an example of this in section 4.1, where you polled a “done”\nflag in a loop. The two functions that handle this are std::this_thread::sleep_\nfor() and std::this_thread::sleep_until(). They work like a basic alarm clock:\nthe thread goes to sleep either for the specified duration (with sleep_for()) or until\nthe specified point in time (with sleep_until()). sleep_for() makes sense for exam-\nples like those in section 4.1, where something must be done periodically, and the\nelapsed time is what matters. On the other hand, sleep_until() allows you to sched-\nule the thread to wake at a particular point in time. This could be used to trigger the\nbackups at midnight, or the payroll print run at 6:00 a.m., or to suspend the thread\nuntil the next frame refresh when doing a video playback.\n Sleeping isn’t the only facility that takes a timeout; you already saw that you can\nuse timeouts with condition variables and futures. You can even use timeouts when\ntrying to acquire a lock on a mutex if the mutex supports it. Plain std::mutex and\nstd::recursive_mutex don’t support timeouts on locking, but std::timed_mutex\ndoes, as does std::recursive_timed_mutex. Both these types support try_lock_for()\nand try_lock_until() member functions that try to obtain the lock within a speci-\nfied time period or before a specified time point. Table 4.1 shows the functions from\nthe C++ Standard Library that can accept timeouts, their parameters, and their return\nvalues. Parameters listed as duration must be an instance of std::duration<>, and\nthose listed as time_point must be an instance of std::time_point<>.\nTable 4.1\nClass/Namespace\nFunctions\nReturn Values\nstd::this_thread namespace\nsleep_for(duration)\nsleep_until(time_point)\nN/A\nstd::condition_variable or \nstd::condition_variable_an\nywait_for(lock,duration)\nwait_until(lock,time_\npoint)\nstd::cv_status::timeout or \nstd::cv_status::no_timeout\nwait_for(lock,duration,\npredicate)\nwait_until(lock,time_point,\npredicate)\nbool—the return value \nof the predicate \nwhen woken\nstd::timed_mutex, \nstd::recursive_timed_mutex \nor std::shared_timed_\nmutextry_lock_for(duration)\ntry_lock_until(time_point)\nbool—true if the lock was acquired, \nfalse otherwise\nstd::shared_timed_mutex\ntry_lock_shared_for(duration)\ntry_lock_shared_until(time_\npoint)\nbool—true if the lock \nwas acquired, false \notherwise\n",
      "content_length": 2613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "99\nUsing synchronization of operations to simplify code\nNow that I’ve covered the mechanics of condition variables, futures, promises, and\npackaged tasks, it’s time to look at the wider picture and how they can be used to sim-\nplify the synchronization of operations between threads.\n4.4\nUsing synchronization of operations to simplify code\nUsing the synchronization facilities described so far in this chapter as building blocks\nallows you to focus on the operations that need synchronizing rather than the\nmechanics. One way this can help simplify your code is that it accommodates a much\nmore functional (in the sense of functional programming) approach to programming\nconcurrency. Rather than sharing data directly between threads, each task can be pro-\nvided with the data it needs, and the result can be disseminated to any other threads\nthat need it through the use of futures.\n4.4.1\nFunctional programming with futures\nThe term functional programming (FP) refers to a style of programming where the\nresult of a function call depends solely on the parameters to that function and doesn’t\ndepend on any external state. This is related to the mathematical concept of a func-\ntion, and it means that if you invoke a function twice with the same parameters, the\nresult is exactly the same. This is a property of many of the mathematical functions in\nthe C++ Standard Library, such as sin, cos, and sqrt, and simple operations on basic\nstd::unique_lock<TimedLock\nable>unique_lock(lockable,\nduration)\nunique_lock(lockable,time_\npoint)\nN/A—owns_lock() on the newly-\nconstructed object returns true if the \nlock was acquired, false otherwise\ntry_lock_for(duration)\ntry_lock_until(time_point)\nbool—true if the lock \nwas acquired, false \notherwise\nstd::shared_lock<Shared-\nTimedLockable>shared_lock\n(lockable,duration)\nshared_lock(lockable,time_\npoint)\nN/A—owns_lock() on the newly-\nconstructed object returns true if the \nlock was acquired, false otherwise\ntry_lock_for(duration)\ntry_lock_until(time_point)\nbool—true if the lock \nwas acquired, false \notherwise\nstd::future<ValueType> or \nstd::shared_future<Value-\nType>wait_for(duration)\nwait_until(time_point)\nstd::future_status::timeout \nif the wait timed out, \nstd::future_status::ready if \nthe future is ready, or \nstd::future_status::deferred \nif the future holds a deferred function \nthat hasn’t yet started\nTable 4.1\n (continued)\nClass/Namespace\nFunctions\nReturn Values\n",
      "content_length": 2425,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "100\nCHAPTER 4\nSynchronizing concurrent operations\ntypes, such as 3+3, 6*9, or 1.3/4.7. A pure function doesn’t modify any external state\neither; the effects of the function are entirely limited to the return value.\n This makes things easy to think about, especially when concurrency is involved,\nbecause many of the problems associated with shared memory discussed in chapter 3\ndisappear. If there are no modifications to shared data, there can be no race conditions\nand thus no need to protect shared data with mutexes either. This is such a powerful\nsimplification that programming languages such as Haskell (http://www.haskell.org/),\nwhere all functions are pure by default, are becoming increasingly popular for pro-\ngramming concurrent systems. Because most things are pure, the impure functions\nthat actually do modify the shared state stand out all the more, and it’s therefore easier\nto reason about how they fit into the overall structure of the application.\n The benefits of FP aren’t limited to those languages where it’s the default para-\ndigm, however. C++ is a multiparadigm language, and it’s entirely possible to write\nprograms in the FP style. This is even easier in C++11 than it was in C++98, with the\nadvent of lambda functions (see appendix A, section A.6), the incorporation of\nstd::bind from Boost and TR1, and the introduction of automatic type deduction for\nvariables (see appendix A, section A.7). Futures are the final piece of the puzzle that\nmakes FP-style concurrency viable in C++; a future can be passed around between\nthreads to allow the result of one computation to depend on the result of another,\nwithout any explicit access to shared data.\nFP-STYLE QUICKSORT\nTo illustrate the use of futures for FP-style concurrency, let’s look at a simple imple-\nmentation of the Quicksort algorithm. The basic idea of the algorithm is simple: given\na list of values, take an element to be the pivot element, and then partition the list into\ntwo sets—those less than the pivot and those greater than or equal to the pivot. A\nsorted copy of the list is obtained by sorting the two sets and returning the sorted list\nof values less than the pivot, followed by the pivot, followed by the sorted list of val-\nues greater than or equal to the pivot. Figure 4.2 shows how a list of 10 integers is\n3\n4\n1\n2\n5\n7\n9\n8\n1\n2\n3\n4\n6\n7\n9\n1\n2\n1\n2\n8\n8\n1\n2\n3\n4\n6\n7\n8\n5\n7\n3\n4\n1\n9\n2\n8\n1\n2\n3\n4\n5\n6\n7\n8\n10\n8\n9\n9\n9\n10\n9\n6\n10\n10\n10\n10\n6\n10\nFigure 4.2\nFP-style recursive sorting\n",
      "content_length": 2475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "101\nUsing synchronization of operations to simplify code\nsorted under this scheme. An FP-style sequential implementation is shown in the fol-\nlowing listing; it takes and returns a list by value rather than sorting in place like\nstd::sort() does.\ntemplate<typename T>\nstd::list<T> sequential_quick_sort(std::list<T> input)\n{\n    if(input.empty())\n    {\n        return input;\n    }\n    std::list<T> result;\n    result.splice(result.begin(),input,input.begin());    \n    T const& pivot=*result.begin();                   \n    \n    auto divide_point=std::partition(input.begin(),input.end(),\n            [&](T const& t){return t<pivot;});               \n    std::list<T> lower_part;\n    lower_part.splice(lower_part.end(),input,input.begin(),\n        divide_point);                                     \n    auto new_lower(\n        sequential_quick_sort(std::move(lower_part)));    \n    auto new_higher(\n        sequential_quick_sort(std::move(input)));     \n    result.splice(result.end(),new_higher);      \n    result.splice(result.begin(),new_lower);    \n    return result;\n}\nAlthough the interface is FP-style, if you used FP style throughout, you’d do a lot of\ncopying, so you use “normal” imperative style for the internals. You take the first ele-\nment as the pivot by slicing it off the front of the list using splice() B. Although this\ncan potentially result in a suboptimal sort (in terms of numbers of comparisons and\nexchanges), doing anything else with a std::list can add quite a bit of time because\nof the list traversal. You know you’re going to want it in the result, so you can splice it\ndirectly into the list you’ll be using for that. Now, you’re also going to want to use it for\ncomparisons, so let’s take a reference to it to avoid copying c. You can then use\nstd::partition to divide the sequence into those values less than the pivot and those\nnot less than the pivot d. The easiest way to specify the partition criteria is to use a\nlambda function; you use a reference capture to avoid copying the pivot value (see\nappendix A, section A.5 for more on lambda functions).\n std::partition() rearranges the list in place and returns an iterator marking the\nfirst element that’s not less than the pivot value. The full type for an iterator can be\nquite long-winded, so you just use the auto type specifier to force the compiler to\nwork it out for you (see appendix A, section A.7).\n Now, you’ve opted for an FP-style interface, so if you’re going to use recursion to\nsort the two “halves,” you’ll need to create two lists. You can do this by using splice()\nListing 4.12\nA sequential implementation of Quicksort\nb\nc\nd\ne\nf\ng\nh\ni\n",
      "content_length": 2643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "102\nCHAPTER 4\nSynchronizing concurrent operations\nagain to move the values from input up to the divide_point into a new list: lower_\npart e. This leaves the remaining values alone in input. You can then sort the two\nlists with recursive calls, f and g. By using std::move() to pass the lists in, you can\navoid copying here too—the result is implicitly moved out anyway. Finally, you can use\nsplice() yet again to piece the result together in the right order. The new_higher\nvalues go on the end h, after the pivot, and the new_lower values go at the beginning,\nbefore the pivot i.\nFP-STYLE PARALLEL QUICKSORT\nBecause this uses a functional style already, it’s now easy to convert this to a parallel\nversion using futures, as shown in the next listing. The set of operations is the same as\nbefore, except that some of them now run in parallel. This version uses an implemen-\ntation of the Quicksort algorithm using futures and a functional style.\ntemplate<typename T>\nstd::list<T> parallel_quick_sort(std::list<T> input)\n{\n    if(input.empty())\n    {\n        return input;\n    }\n    std::list<T> result;\n    result.splice(result.begin(),input,input.begin());\n    T const& pivot=*result.begin();\n    auto divide_point=std::partition(input.begin(),input.end(),\n            [&](T const& t){return t<pivot;});\n    std::list<T> lower_part;\n    lower_part.splice(lower_part.end(),input,input.begin(),\n        divide_point);\n    std::future<std::list<T> > new_lower(                  \n        std::async(&parallel_quick_sort<T>,std::move(lower_part)));\n    auto new_higher(\n        parallel_quick_sort(std::move(input)));       \n    result.splice(result.end(),new_higher);           \n    result.splice(result.begin(),new_lower.get());   \n    return result;\n}\nThe big change here is that rather than sorting the lower portion on the current\nthread, you sort it on another thread using std::async() B. The upper portion of\nthe list is sorted with direct recursion as before c. By recursively calling parallel_\nquick_sort(), you can take advantage of the available hardware concurrency. If\nstd::async() starts a new thread every time, then if you recurse down three times,\nyou’ll have eight threads running; if you recurse down 10 times (for ~1000 ele-\nments), you’ll have 1,024 threads running if the hardware can handle it. If the library\ndecides there are too many spawned tasks (perhaps because the number of tasks has\nexceeded the available hardware concurrency), it may switch to spawning the new\nListing 4.13\nParallel Quicksort using futures\nb\nc\nd\ne\n",
      "content_length": 2546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "103\nUsing synchronization of operations to simplify code\ntasks synchronously. They will run in the thread that calls get() rather than on a new\nthread, thus avoiding the overhead of passing the task to another thread when this\nwon’t help the performance. It’s worth noting that it’s perfectly conforming for an\nimplementation of std::async to start a new thread for each task (even in the face of\nmassive oversubscription) unless std::launch::deferred is explicitly specified, or to\nrun all tasks synchronously unless std::launch::async is explicitly specified. If you’re\nrelying on the library for automatic scaling, you’re advised to check the documenta-\ntion for your implementation to see what behavior it exhibits.\n Rather than using std::async(), you could write your own spawn_task() func-\ntion as a simple wrapper around std::packaged_task and std::thread, as shown in\nlisting 4.14; you’d create a std::packaged_task for the result of the function call, get\nthe future from it, run it on a thread, and return the future. This wouldn’t offer much\nof an advantage (and indeed would likely lead to massive oversubscription), but it\nwould pave the way to migrate to a more sophisticated implementation that adds the\ntask to a queue to be run by a pool of worker threads. We’ll look at thread pools in\nchapter 9. It’s probably worth going this way in preference to using std::async only if\nyou know what you’re doing and want complete control over the way the thread pool\nis built and executes tasks.\n Anyway, back to parallel_quick_sort. Because you just used direct recursion to\nget new_higher, you can splice it into place as before d. But new_lower is now\nstd::future<std::list<T>> rather than a list, so you need to call get() to retrieve\nthe value before you can call splice() e. This then waits for the background task to\ncomplete and moves the result into the splice() call; get() returns an rvalue refer-\nence to the contained result, so it can be moved out (see appendix A, section A.1.1 for\nmore on rvalue references and move semantics).\n Even assuming that std::async() makes optimal use of the available hardware\nconcurrency, this still isn’t an ideal parallel implementation of Quicksort. For one\nthing, std::partition does a lot of the work, and that’s still a sequential call, but it’s\ngood enough for now. If you’re interested in the fastest possible parallel implementa-\ntion, check the academic literature. Alternatively, you could use the parallel overload\nfrom the C++17 Standard Library (see chapter 10).\ntemplate<typename F,typename A>\nstd::future<std::result_of<F(A&&)>::type>\n    spawn_task(F&& f,A&& a)\n{\n    typedef std::result_of<F(A&&)>::type result_type;\n    std::packaged_task<result_type(A&&)>\n        task(std::move(f)));\n    std::future<result_type> res(task.get_future());\n    std::thread t(std::move(task),std::move(a));\n    t.detach();\n    return res;\n}\nListing 4.14\nA sample implementation of spawn_task\n",
      "content_length": 2945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "104\nCHAPTER 4\nSynchronizing concurrent operations\nFP isn’t the only concurrent programming paradigm that eschews shared mutable\ndata; another paradigm is CSP (Communicating Sequential Processes),2 where threads\nare conceptually entirely separate, with no shared data but with communication chan-\nnels that allow messages to be passed between them. This is the paradigm adopted by\nthe programming language Erlang (http://www.erlang.org/) and by the MPI (Mes-\nsage Passing Interface; http://www.mpi-forum.org/) environment commonly used\nfor high-performance computing in C and C++. I’m sure that by now you’ll be unsur-\nprised to learn that this can also be supported in C++ with a bit of discipline; the fol-\nlowing section discusses one way to achieve this.\n4.4.2\nSynchronizing operations with message passing\nThe idea of CSP is simple: if there’s no shared data, each thread can be reasoned\nabout entirely independently, purely on the basis of how it behaves in response to the\nmessages that it received. Each thread is therefore effectively a state machine: when it\nreceives a message, it updates its state in some manner and maybe sends one or more\nmessages to other threads, with the processing performed depending on the initial\nstate. One way to write such threads would be to formalize this and implement a\nFinite State Machine model, but this isn’t the only way; the state machine can be\nimplicit in the structure of the application. Which method works better in any given\nscenario depends on the exact behavioral requirements of the situation and the\nexpertise of the programming team. However you choose to implement each thread,\nthe separation into independent processes has the potential to remove much of the\ncomplication from shared-data concurrency and therefore make programming easier,\nlowering the bug rate.\n True communicating sequential processes have no shared data, with all communi-\ncation passed through the message queues, but because C++ threads share an address\nspace, it’s not possible to enforce this requirement. This is where the discipline comes\nin: as application or library authors, it’s our responsibility to ensure that we don’t\nshare data between the threads. Of course, the message queues must be shared in\norder for the threads to communicate, but the details can be wrapped in the library.\n Imagine for a moment that you’re implementing the code for an ATM. This code\nneeds to handle interaction with the person trying to withdraw money and interaction\nwith the relevant bank, as well as control the physical machinery to accept the per-\nson’s card, display appropriate messages, handle key presses, issue money, and return\ntheir card.\n One way to handle everything would be to split the code into three independent\nthreads: one to handle the physical machinery, one to handle the ATM logic, and one\nto communicate with the bank. These threads could communicate purely by passing\nmessages rather than sharing any data. For example, the thread handling the machinery\n2 Communicating Sequential Processes, C.A.R. Hoare, Prentice Hall, 1985. Available free online at http://www\n.usingcsp.com/cspbook.pdf.\n",
      "content_length": 3142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "105\nUsing synchronization of operations to simplify code\nwould send a message to the logic thread when the person at the machine entered\ntheir card or pressed a button, and the logic thread would send a message to the\nmachinery thread indicating how much money to dispense, and so forth.\n One way to model the ATM logic would be as a state machine. In each state, the\nthread waits for an acceptable message, which it then processes. This may result in\ntransitioning to a new state, and the cycle continues. The states involved in a simple\nimplementation are shown in figure 4.3. In this simplified implementation, the system\nwaits for a card to be inserted. Once the card is inserted, it then waits for the user to\nenter their PIN, one digit at a time. They can delete the last digit entered. Once\nenough digits have been entered, the PIN is verified. If the PIN is not OK, you’re fin-\nished, so you return the card to the customer and resume waiting for someone to\nenter their card. If the PIN is OK, you wait for them to either cancel the transaction or\nselect an amount to withdraw. If they cancel, you’re finished, and you return their\ncard. If they select an amount, you wait for confirmation from the bank before issuing\nthe cash and returning the card or displaying an “insufficient funds” message and\nreturning their card. Obviously, a real ATM is considerably more complex, but this is\nenough to illustrate the idea.\nHaving designed a state machine for your ATM logic, you can implement it with a\nclass that has a member function to represent each state. Each member function can\nthen wait for specific sets of incoming messages and handle them when they arrive,\npossibly triggering a switch to another state. Each distinct message type is represented\nInitial state\nGetting PIN\nCard inserted\nDigit pressed\nClear last digit pressed\nVerifying\nPIN\nDigit pressed (final digit)\nPIN OK\nDone\nCard taken\nPIN not OK\nCancel\npressed\nCancel pressed\nWaiting for\nconfirmation\nWithdraw (amount)\npressed\nWithdrawal OK\n(issue cash)\nInsufficient funds\nWaiting for\nwithdrawal\namount\nFigure 4.3\nA simple state machine model for an ATM\n",
      "content_length": 2124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "106\nCHAPTER 4\nSynchronizing concurrent operations\nby a separate struct. Listing 4.15 shows part of a simple implementation of the ATM\nlogic in such a system, with the main loop and the implementation of the first state,\nwaiting for the card to be inserted. \n As you can see, all the necessary synchronization for the message passing is entirely\nhidden inside the message-passing library (a basic implementation of which is given in\nappendix C, along with the full code for this example).\nstruct card_inserted\n{\n    std::string account;\n};\nclass atm\n{\n    messaging::receiver incoming;\n    messaging::sender bank;\n    messaging::sender interface_hardware;\n    void (atm::*state)();\n    std::string account;\n    std::string pin;\n    void waiting_for_card()    \n    {\n        interface_hardware.send(display_enter_card());   \n        incoming.wait()                             \n            .handle<card_inserted>(\n                [&](card_inserted const& msg)    \n                {\n                    account=msg.account;\n                    pin=\"\";\n                    interface_hardware.send(display_enter_pin());\n                    state=&atm::getting_pin;\n                }\n                );\n    }\n    void getting_pin();\npublic:\n    void run()    \n    {\n        state=&atm::waiting_for_card;     \n        try\n        {\n            for(;;)\n            {\n                (this->*state)();    \n            }\n        }\n        catch(messaging::close_queue const&)\n        {\n        }\n    }\n};\nListing 4.15\nA simple implementation of an ATM logic class\nb\nc\nd\ne\nf\ng\nh\n",
      "content_length": 1568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "107\nUsing synchronization of operations to simplify code\nAs already mentioned, the implementation described here is grossly simplified from\nthe real logic that would be required in an ATM, but it does give you a feel for the\nmessage-passing style of programming. There’s no need to think about synchroniza-\ntion and concurrency issues, just which messages may be received at any given point\nand which messages to send. The state machine for this ATM logic runs on a single\nthread, with other parts of the system such as the interface to the bank and the termi-\nnal interface running on separate threads. This style of program design is called the\nActor model—there are several discrete actors in the system (each running on a separate\nthread), which send messages to each other to perform the task at hand, and there’s\nno shared state except that which is directly passed via messages.\n Execution starts with the run() member function f, which sets the initial state to\nwaiting_for_card g and then repeatedly executes the member function represent-\ning the current state (whatever it is) h. The state functions are simple member func-\ntions of the atm class. The waiting_for_card state function B is also simple: it sends\na message to the interface to display a “waiting for card” message c, and then waits\nfor a message to handle d. The only type of message that can be handled here is a\ncard_inserted message, which you handle with a lambda function e. You could pass\nany function or function object to the handle function, but for a simple case like this,\nit’s easiest to use a lambda. Note that the handle() function call is chained onto the\nwait() function; if a message is received that doesn’t match the specified type, it’s dis-\ncarded, and the thread continues to wait until a matching message is received.\n The lambda function itself caches the account number from the card in a member\nvariable, clears the current PIN, sends a message to the interface hardware to display\nsomething asking the user to enter their PIN, and changes to the “getting PIN” state.\nOnce the message handler has completed, the state function returns, and the main\nloop then calls the new state function h.\n The getting_pin state function is a bit more complex in that it can handle three\ndistinct types of message, as in figure 4.3. This is shown in the following listing.\nvoid atm::getting_pin()\n{\n    incoming.wait()\n        .handle<digit_pressed>(     \n            [&](digit_pressed const& msg)\n            {\n                unsigned const pin_length=4;\n                pin+=msg.digit;\n                if(pin.length()==pin_length)\n                {\n                    bank.send(verify_pin(account,pin,incoming));\n                    state=&atm::verifying_pin;\n                }\n            }\n            )\nListing 4.16\nThe getting_pin state function for the simple ATM implementation\nb\n",
      "content_length": 2876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "108\nCHAPTER 4\nSynchronizing concurrent operations\n        .handle<clear_last_pressed>(     \n            [&](clear_last_pressed const& msg)\n            {\n                if(!pin.empty())\n                {\n                    pin.resize(pin.length()-1);\n                }\n            }\n            )\n        .handle<cancel_pressed>(     \n            [&](cancel_pressed const& msg)\n            {\n                state=&atm::done_processing;\n            }\n            );\n}\nThis time, there are three message types you can process, so the wait() function has\nthree handle() calls chained on the end, B, c, and d. Each call to handle() speci-\nfies the message type as the template parameter and then passes in a lambda function\nthat takes that particular message type as a parameter. Because the calls are chained\ntogether in this way, the wait() implementation knows that it’s waiting for a digit_\npressed message, a clear_last_pressed message, or a cancel_pressed message.\nMessages of any other type are again discarded.\n This time, you don’t necessarily change state when you get a message. For exam-\nple, if you get a digit_pressed message, you add it to the pin unless it’s the final\ndigit. The main loop h in listing 4.15 will then call getting_pin() again to wait for\nthe next digit (or clear or cancel).\n This corresponds to the behavior shown in figure 4.3. Each state box is imple-\nmented by a distinct member function, which waits for the relevant messages and\nupdates the state as appropriate.\n As you can see, this style of programming can greatly simplify the task of designing\na concurrent system, because each thread can be treated entirely independently. It is\nan example of using multiple threads to separate concerns and as such requires you to\nexplicitly decide how to divide the tasks between threads.\n Back in section 4.2, I mentioned that the Concurrency TS provides extended ver-\nsions of futures. The core part of the extensions is the ability to specify continuations—\nadditional functions that are run automatically when the future becomes ready. Let’s\ntake the opportunity to explore how this can simplify our code.\n4.4.3\nContinuation-style concurrency with the Concurrency TS\nThe Concurrency TS provides new versions of std::promise and std::packaged_task\nin the std::experimental namespace that all differ from their std originals in the same\nway: they return instances of std::experimental::future rather than std::future.\nThis enables users to take advantage of the key new feature in std::experimental\n::future—continuations.\nc\nd\n",
      "content_length": 2557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "109\nUsing synchronization of operations to simplify code\n Suppose you have a task running that will produce a result, and a future that will\nhold the result when it becomes available. You then have some code that needs to run\nin order to process that result. With std::future you would have to wait for the\nfuture to become ready, either with the fully-blocking wait() member function or\neither of the wait_for() or wait_until() member functions to allow a wait with a\ntimeout. This can be inconvenient, and can complicate the code. What you want is a\nmeans of saying “When the data is ready, then do this processing”. This is exactly what\ncontinuations give us; unsurprisingly, the member function to add a continuation to a\nfuture is called then(). Given a future fut, a continuation is added with the call\nfut.then(continuation).\n Just like std::future, std::experimental::future only allows the stored value to\nbe retrieved once. If that value is being consumed by a continuation, this means it can-\nnot be accessed by other code. Consequently, when a continuation is added with\nfut.then(), the original future, fut, becomes invalid. Instead, the call to fut.then()\nreturns a new future to hold the result of the continuation call. This is shown in the\nfollowing code:\nstd::experimental::future<int> find_the_answer;\nauto fut=find_the_answer();\nauto fut2=fut.then(find_the_question);\nassert(!fut.valid());\nassert(fut2.valid());\nThe find_the_question continuation function is scheduled to run “on an unspeci-\nfied thread” when the original future is ready. This gives the implementation freedom\nto run it on a thread pool or another library-managed thread. As it stands, this gives\nthe implementation a lot of freedom; this is deliberate, with the intention that when\ncontinuations are added to a future C++ Standard, the implementers will be able to\ndraw on their experience to better specify the choice of threads and provide users\nwith suitable mechanisms for controlling the choice of threads.\n Unlike direct calls to std::async or std::thread, you cannot pass arguments to a\ncontinuation function, because the argument is already defined by the library—the\ncontinuation is passed a ready future that holds the result that triggered the continua-\ntion. Assuming your find_the_answer function returns an int, the find_the_question\nfunction referenced in the previous example must take a std::experimental::\nfuture<int> as its sole parameter; for example:\nstd::string find_the_question(std::experimental::future<int> the_answer);\nThe reason for this is that the future on which the continuation was chained may end\nup holding a value or an exception. If the future was implicitly dereferenced to pass\nthe value directly to the continuation, then the library would have to decide how to\nhandle the exception, whereas by passing the future to the continuation, the continua-\ntion can handle the exception. In simple cases, this may be done by calling fut.get()\n",
      "content_length": 2964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "110\nCHAPTER 4\nSynchronizing concurrent operations\nand allowing the re-thrown exception to propagate out of the continuation function.\nJust as for functions passed to std::async, exceptions that escape a continuation are\nstored in the future that holds the continuation result.\n Note that the Concurrency TS doesn’t specify that there is an equivalent to\nstd::async, though implementations may provide one as an extension. Writing such\na function is fairly straightforward: use std::experimental::promise to obtain a\nfuture, and then spawn a new thread running a lambda that sets the promise’s value to\nthe return value of the supplied function, as in the next listing.\ntemplate<typename Func>\nstd::experimental::future<decltype(std::declval<Func>()())>\nspawn_async(Func&& func){\n    std::experimental::promise<\n        decltype(std::declval<Func>()())> p;\n    auto res=p.get_future();\n    std::thread t(\n        [p=std::move(p),f=std::decay_t<Func>(func)]()\n            mutable{\n            try{\n                p.set_value_at_thread_exit(f());\n            } catch(...){\n                p.set_exception_at_thread_exit(std::current_exception());\n            }\n    });\n    t.detach();\n    return res;\n}\nThis stores the result of the function in the future, or catches the exception thrown\nfrom the function and stores that in the future, just as std::async does. Also, it uses\nset_value_at_thread_exit and set_exception_at_thread_exit to ensure that thread\n_local variables have been properly cleaned up before the future becomes ready.\n The value returned from a then() call is a fully-fledged future itself. This means\nthat you can chain continuations.\n4.4.4\nChaining continuations\nSuppose you have a series of time-consuming tasks to do, and you want to do them\nasynchronously in order to free up the main thread for other tasks. For example, when\nthe user logs in to your application, you might need to send the credentials to the\nbackend for authentication; then, when the details have been authenticated, make a\nfurther request to the backend for information about the user’s account; and finally,\nwhen that information has been retrieved, update the display with the relevant infor-\nmation. As sequential code, you might write something like the following listing.\n \nListing 4.17\nA simple equivalent to std::async for Concurrency TS futures\n",
      "content_length": 2346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "111\nUsing synchronization of operations to simplify code\nvoid process_login(std::string const& username,std::string const& password)\n{\n    try {\n        user_id const id=backend.authenticate_user(username,password);\n        user_data const info_to_display=backend.request_current_info(id);\n        update_display(info_to_display);\n    } catch(std::exception& e){\n        display_error(e);\n    }\n}\nHowever, you don’t want sequential code; you want asynchronous code so you’re not\nblocking the UI thread. With plain std::async, you could punt it all to a background\nthread like the next listing, but that would still block that thread, consuming resources\nwhile waiting for the tasks to complete. If you have many such tasks, then you can end\nup with a large number of threads that are doing nothing except waiting.\nstd::future<void> process_login(\n    std::string const& username,std::string const& password)\n{\n    return std::async(std::launch::async,[=](){\n        try {\n            user_id const id=backend.authenticate_user(username,password);\n            user_data const info_to_display=\n                backend.request_current_info(id);\n            update_display(info_to_display);\n        } catch(std::exception& e){\n            display_error(e);\n        }\n    });\n}\nIn order to avoid all these blocked threads, you need some mechanism for chaining\ntasks as they each complete: continuations. The following listing shows the same over-\nall process, but this time split into a series of tasks, each of which is then chained on\nthe previous one as a continuation.\nstd::experimental::future<void> process_login(\n    std::string const& username,std::string const& password)\n{\n    return spawn_async([=](){\n        return backend.authenticate_user(username,password);\n    }).then([](std::experimental::future<user_id> id){\n        return backend.request_current_info(id.get());\n    }).then([](std::experimental::future<user_data> info_to_display){\nListing 4.18\nA simple sequential function to process user login\nListing 4.19\nProcessing user login with a single async task\nListing 4.20\nA function to process user login with continuations\n",
      "content_length": 2138,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "112\nCHAPTER 4\nSynchronizing concurrent operations\n        try{\n            update_display(info_to_display.get());\n        } catch(std::exception& e){\n            display_error(e);\n        }\n    });\n}\nNote how each continuation takes a std::experimental::future as the sole parame-\nter, and then uses .get() to retrieve the contained value. This means that exceptions\nget propagated all the way down the chain, so the call to info_to_display.get() in\nthe final continuation will throw if any of the functions in the chain threw an excep-\ntion, and the catch block here can handle all the exceptions, just like the catch block\nin listing 4.18 did.\n If the function calls to the backend block internally because they’re waiting for\nmessages to cross the network or for a database operation to complete, then you’re\nnot done yet. You may have split the task into its individual parts, but they’re still\nblocking calls, so you still get blocked threads. What you need is for the backend calls\nto return futures that become ready when the data is ready, without blocking any\nthreads. In this case, backend.async_authenticate_user(username,password) will\nnow return a std::experimental::future<user_id> rather than a plain user_id.\n You might think this would complicate the code, because returning a future from a\ncontinuation would give you future<future<some_value>>, or else you’d have to put\nthe .then calls inside the continuations. Thankfully, if you thought that, then you’d be\nmistaken: the continuation support has a nifty feature called future-unwrapping. If the\ncontinuation function you pass to a .then() call returns a future<some_type>, then the\n.then() call will return a future<some_type> in turn. This means your final code looks\nlike the next listing, and there is no blocking in your asynchronous function chain.\nstd::experimental::future<void> process_login(\n    std::string const& username,std::string const& password)\n{\n    return backend.async_authenticate_user(username,password).then(\n        [](std::experimental::future<user_id> id){\n            return backend.async_request_current_info(id.get());\n        }).then([](std::experimental::future<user_data> info_to_display){\n            try{\n                update_display(info_to_display.get());\n            } catch(std::exception& e){\n                display_error(e);\n            }\n        });\n}\nThis is almost as straightforward as the sequential code from listing 4.18, with a little bit\nmore boilerplate around the .then calls and the lambda declarations. If your compiler\nListing 4.21\nA function to process user login with fully asynchronous operations\n",
      "content_length": 2629,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "113\nUsing synchronization of operations to simplify code\nsupports C++14 generic lambdas, then the types of the futures in the lambda parame-\nters can be replaced with auto, which simplifies the code even further:\nreturn backend.async_authenticate_user(username,password).then(\n        [](auto id){\n            return backend.async_request_current_info(id.get());\n        });\nIf you need anything more complex than simple linear control flow, then you can\nimplement this by putting the logic in one of the lambdas; for truly complex control\nflow you probably need to write a separate function.\n So far, we’ve focused on the continuation support in std::experimental::future.\nAs you might expect, std::experimental::shared_future also supports continuations.\nThe difference here is that std::experimental::shared_future objects can have more\nthan one continuation, and the continuation parameter is a std::experimental::\nshared_future rather than a std::experimental::future. This naturally falls out of\nthe shared nature of std::experimental::shared_future—because multiple objects\ncan refer to the same shared state, if only one continuation was allowed, there would\nbe a race condition between two threads that were each trying to add continuations to\ntheir own  std::experimental::shared_future objects. This is obviously undesir-\nable, so multiple continuations are permitted. Once you have multiple continuations\npermitted, you may as well allow them to be added via the same std::experimental::\nshared_future instance, rather than only allowing one continuation per object. In\naddition, you can’t package the shared state in a one-shot std::experimental::\nfuture passed to the first continuation, when you’re going to want to also pass it to\nthe second continuation. Thus the parameter passed to the continuation function\nmust also be a std::experimental::shared_future:\nauto fut=spawn_async(some_function).share();\nauto fut2=fut.then([](std::experimental::shared_future<some_data> data){\n    do_stuff(data);\n    });\nauto fut3=fut.then([](std::experimental::shared_future<some_data> data){\n    return do_other_stuff(data);\n    });\nfut is a std::experimental::shared_future due to the share() call, so the continu-\nation function must take a std::experimental::shared_future as its parameter.\nHowever, the return value from the continuation is a plain std::experimental::\nfuture—that value isn’t currently shared until you do something to share it—so both\nfut2 and fut3 are std::experimental::futures.\n Continuations aren’t the only enhancement to futures in the Concurrency TS,\nthough they are probably the most important. Also provided are two overloaded func-\ntions that allow you to wait for either any one of a bunch of futures to become ready, or\nall of a bunch of futures to become ready.\n",
      "content_length": 2800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "114\nCHAPTER 4\nSynchronizing concurrent operations\n4.4.5\nWaiting for more than one future\nSuppose you have a large volume of data to process, and each item can be processed\nindependently. This is a prime opportunity to make use of the available hardware by\nspawning a set of asynchronous tasks to process the data items, each of them return-\ning the processed data via a future. However, if you need to wait for all the tasks to\nfinish and then gather all the results for some final processing, this can be inconve-\nnient—you have to wait for each future in turn, and then gather the results. If you\nwant to do the result gathering with another asynchronous task, then you either have\nto spawn it up front so it is occupying a thread that’s waiting, or you have to keep poll-\ning the futures and spawn the new task when all the futures are ready. An example of\nsuch code is shown in the following listing.\nstd::future<FinalResult> process_data(std::vector<MyData>& vec)\n{\n    size_t const chunk_size=whatever;\n    std::vector<std::future<ChunkResult>> results;\n    for(auto begin=vec.begin(),end=vec.end();beg!=end;){\n        size_t const remaining_size=end-begin;\n        size_t const this_chunk_size=std::min(remaining_size,chunk_size);\n        results.push_back(\n            std::async(process_chunk,begin,begin+this_chunk_size));\n        begin+=this_chunk_size;\n    }\n    return std::async([all_results=std::move(results)](){\n        std::vector<ChunkResult> v;\n        v.reserve(all_results.size());\n        for(auto& f: all_results)\n        {\n            v.push_back(f.get());   \n        }\n        return gather_results(v);\n    });\n}\nThis code spawns a new asynchronous task to wait for the results, and then processes\nthem when they are all available. However, because it waits for each task individually, it\nwill repeatedly be woken by the scheduler at B as each result becomes available, and\nthen go back to sleep again when it finds another result that is not yet ready. Not only\ndoes this occupy the thread doing the waiting, but it adds additional context switches\nas each future becomes ready, which adds additional overhead.\n With std::experimental::when_all, this waiting and switching can be avoided.\nYou pass the set of futures to be waited on to when_all, and it returns a new future\nthat becomes ready when all the futures in the set are ready. This future can then be\nused with continuations to schedule additional work when the all the futures are\nready. See, for example, the next listing.\nListing 4.22\nGathering results from futures using std::async\nb\n",
      "content_length": 2574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "115\nUsing synchronization of operations to simplify code\nstd::experimental::future<FinalResult> process_data(\n    std::vector<MyData>& vec)\n{\n    size_t const chunk_size=whatever;\n    std::vector<std::experimental::future<ChunkResult>> results;\n    for(auto begin=vec.begin(),end=vec.end();beg!=end;){\n        size_t const remaining_size=end-begin;\n        size_t const this_chunk_size=std::min(remaining_size,chunk_size);\n        results.push_back(\n            spawn_async(\n            process_chunk,begin,begin+this_chunk_size));\n        begin+=this_chunk_size;\n    }\n    return std::experimental::when_all(\n        results.begin(),results.end()).then(     \n        [](std::future<std::vector<\n             std::experimental::future<ChunkResult>>> ready_results)\n        {\n            std::vector<std::experimental::future<ChunkResult>>\n                all_results=ready_results .get();\n            std::vector<ChunkResult> v;\n            v.reserve(all_results.size());\n            for(auto& f: all_results)\n            {\n                v.push_back(f.get());    \n            }\n            return gather_results(v);\n        });\n}\nIn this case, you use when_all to wait for all the futures to become ready, and then\nschedule the function using .then rather than async B. Though the lambda is super-\nficially the same, it takes the results vector as a parameter (wrapped in a future)\nrather than as a capture, and the calls to get on the futures at c do not block, as all\nthe values are ready by the time execution gets there. This has the potential to reduce\nthe load on the system for little change to the code.\n To complement when_all, we also have when_any. This creates a future that\nbecomes ready when any of the supplied futures becomes ready. This works well for\nscenarios where you’ve spawned multiple tasks to take advantage of the available con-\ncurrency, but need to do something when the first one becomes ready.\n4.4.6\nWaiting for the first future in a set with when_any\nSuppose you are searching a large dataset for a value that meets particular criteria, but\nif there are multiple such values, then any will do. This is a prime target for parallel-\nism—you can spawn multiple threads, each of which checks a subset of the data; if a\ngiven thread finds a suitable value, then it sets a flag indicating that the other threads\nshould stop their search, and then sets the final return value. In this case, you want to\nListing 4.23\nGathering results from futures using std::experimental::when_all\nb\nc\n",
      "content_length": 2511,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "116\nCHAPTER 4\nSynchronizing concurrent operations\ndo the further processing when the first task completes its search, even if the other\ntasks haven’t finished cleaning up yet.\n Here, you can use std::experimental::when_any to gather the futures together,\nand provide a new future that is ready when at least one of the original set is ready.\nWhereas when_all gave you a future that wrapped the collection of futures you passed\nin, when_any adds a further layer, combining the collection with an index value that\nindicates which future triggered the combined future to be ready into an instance of\nthe std::experimental::when_any_result class template.\n An example of using when_any as described here is shown in the next listing.\nstd::experimental::future<FinalResult>\nfind_and_process_value(std::vector<MyData> &data)\n{\n    unsigned const concurrency = std::thread::hardware_concurrency();\n    unsigned const num_tasks = (concurrency > 0) ? concurrency : 2;\n    std::vector<std::experimental::future<MyData *>> results;\n    auto const chunk_size = (data.size() + num_tasks - 1) / num_tasks;\n    auto chunk_begin = data.begin();\n    std::shared_ptr<std::atomic<bool>> done_flag =\n        std::make_shared<std::atomic<bool>>(false);\n    for (unsigned i = 0; i < num_tasks; ++i) {        \n        auto chunk_end =\n            (i < (num_tasks - 1)) ? chunk_begin + chunk_size : data.end();\n        results.push_back(spawn_async([=] {               \n            for (auto entry = chunk_begin;\n                !*done_flag && (entry != chunk_end);\n                 ++entry) {\n                if (matches_find_criteria(*entry)) {\n                    *done_flag = true;\n                    return &*entry;\n                }\n            }\n            return (MyData *)nullptr;\n        }));\n        chunk_begin = chunk_end;\n    }\n    std::shared_ptr<std::experimental::promise<FinalResult>> final_result =\n        std::make_shared<std::experimental::promise<FinalResult>>();\n    struct DoneCheck {\n        std::shared_ptr<std::experimental::promise<FinalResult>>\n            final_result;\n        DoneCheck(\n            std::shared_ptr<std::experimental::promise<FinalResult>>\n                final_result_)\n            : final_result(std::move(final_result_)) {}\n        void operator()(                                     \n            std::experimental::future<std::experimental::when_any_result<\n                std::vector<std::experimental::future<MyData *>>>>\nListing 4.24\nUsing std::experimental::when_any to process the first value found\nb\nc\ne\n",
      "content_length": 2543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "117\nUsing synchronization of operations to simplify code\n                results_param) {\n            auto results = results_param.get();\n            MyData *const ready_result =\n                results.futures[results.index].get();     \n            if (ready_result)\n                final_result->set_value(                 \n                    process_found_value(*ready_result));\n            else {\n                results.futures.erase(\n                    results.futures.begin() + results.index);    \n                if (!results.futures.empty()) {\n                    std::experimental::when_any(                       \n                        results.futures.begin(), results.futures.end())\n                        .then(std::move(*this));\n            } else {\n                final_result->set_exception(\n                    std::make_exception_ptr(               \n                        std::runtime_error(“Not found”)));\n            }\n        }\n    };\n    std::experimental::when_any(results.begin(), results.end())\n        .then(DoneCheck(final_result));                          \n    return final_result->get_future();     \n}\nThe initial loop B spawns off num_tasks asynchronous tasks, each running the\nlambda function from c. This lambda captures by copying, so each task will have its\nown values for chunk_begin and chunk_end, as well as a copy of the shared pointer,\ndone_flag. This avoids any concerns over lifetime issues.\n Once all the tasks have been spawned, you want to handle the case that a task\nreturned. This is done by chaining a continuation on the when_any call d. This time\nyou write the continuation as a class because you want to reuse it recursively. When\none of the initial tasks is ready, the DoneCheck function call operator is invoked e.\nFirst, it extracts the value from the future that is ready f, and then if the value was\nfound, you process it and set the final result g. Otherwise, you drop the ready future\nfrom the collection h, and if there are still more futures to check, issue a new call to\nwhen_any i, that will trigger its continuation when the next future is ready. If there\nare no futures left, then none of them found the value, so store an exception instead\nj. The return value of the function is the future for the final result 1). There are\nalternative ways to solve this problem, but I hope this shows how one might use\nwhen_any.\n Both these examples of using when_all and when_any have used the iterator-range\noverloads, which take a pair of iterators denoting the beginning and end of a set of\nfutures to wait for. Both functions also come in variadic forms, where they accept a\nnumber of futures directly as parameters to the function. In this case, the result is a\nfuture holding a tuple (or a when_any_result holding a tuple) rather than a vector:\nf\ng\nh\ni\nj\nd\n1)\n",
      "content_length": 2826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "118\nCHAPTER 4\nSynchronizing concurrent operations\nstd::experimental::future<int> f1=spawn_async(func1);\nstd::experimental::future<std::string> f2=spawn_async(func2);\nstd::experimental::future<double> f3=spawn_async(func3);\nstd::experimental::future<\n    std::tuple<\n        std::experimental::future<int>,\n        std::experimental::future<std::string>,\n        std::experimental::future<double>>> result=\n    std::experimental::when_all(std::move(f1),std::move(f2),std::move(f3));\nThis example highlights something important about all the uses of when_any and\nwhen_all—they always move from any std::experimental::futures passed in via a\ncontainer, and they take their parameters by value, so you have to explicitly move the\nfutures in, or pass temporaries.\n Sometimes the event that you’re waiting for is for a set of threads to reach a partic-\nular point in the code, or to have processed a certain number of data items between\nthem. In these cases, you might be better served using a latch or a barrier rather than a\nfuture. Let’s look at the latches and barriers that are provided by the Concurrency TS.\n4.4.7\nLatches and barriers in the Concurrency TS\nFirst off, let’s consider what is meant when we talk of a latch or a barrier. A latch is a syn-\nchronization object that becomes ready when its counter is decremented to zero. Its\nname comes from the fact that it latches the output—once it is ready, it stays ready\nuntil it is destroyed. A latch is thus a lightweight facility for waiting for a series of\nevents to occur.\n On the other hand, a barrier is a reusable synchronization component used for\ninternal synchronization between a set of threads. Whereas a latch doesn’t care which\nthreads decrement the counter—the same thread can decrement the counter multi-\nple times, or multiple threads can each decrement the counter once, or some combi-\nnation of the two—with barriers, each thread can only arrive at the barrier once per\ncycle. When threads arrive at the barrier, they block until all of the threads involved\nhave arrived at the barrier, at which point they are all released. The barrier can then\nbe reused—the threads can then arrive at the barrier again to wait for all the threads\nfor the next cycle.\n Latches are inherently simpler than barriers, so let’s start with the latch type from\nthe Concurrency TS: std::experimental::latch.\n4.4.8\nA basic latch type: std::experimental::latch\nstd::experimental::latch comes from the <experimental/latch> header. When\nyou construct a std::experimental::latch, you specify the initial counter value as\nthe one and only argument to the constructor. Then, as the events that you are wait-\ning for occur, you call count_down on your latch object, and the latch becomes ready\nwhen that count reaches zero. If you need to wait for the latch to become ready, then\nyou can call wait on the latch; if you only need to check if it is ready, then you can call\n",
      "content_length": 2914,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "119\nUsing synchronization of operations to simplify code\nis_ready. Finally, if you need to both count down the counter and then wait for the\ncounter to reach zero, you can call count_down_and_wait. A basic example is shown\nin the following listing.\nvoid foo(){\n    unsigned const thread_count=...;\n    latch done(thread_count);         \n    my_data data[thread_count];\n    std::vector<std::future<void> > threads;\n    for(unsigned i=0;i<thread_count;++i)\n        threads.push_back(std::async(std::launch::async,[&,i]{    \n            data[i]=make_data(i);\n            done.count_down();   \n            do_more_stuff();     \n        }));\n    done.wait();                   \n    process_data(data,thread_count);   \n}     \nThis constructs done with the number of events that you need to wait for B, and then\nspawns the appropriate number of threads using std::async c. Each thread then\ncounts down the latch when it has generated the relevant chunk of data d before\ncontinuing on with further processing e. The main thread can wait for all the data to\nbe ready by waiting on the latch f before processing the generated data g. The data\nprocessing at g will potentially run concurrently with the final processing steps of\neach thread e—there is no guarantee that the threads have all completed until the\nstd::future destructors run at the end of the function h.\n One thing to note is that the lambda passed to std::async at c captures every-\nthing by reference except i, which is captured by value. This is because i is the loop\ncounter, and capturing that by reference would cause a data race and undefined\nbehavior, whereas data and done are things you need to share access to. Also, you only\nneed a latch at all in this scenario because the threads have additional processing to\ndo after the data is ready; otherwise you could wait for all the futures to ensure the\ntasks were complete before processing the data.\n It is safe to access data in the process_data call g, even though it is stored by\ntasks running in other threads, because the latch is a synchronization object, so\nchanges visible to a thread that call count_down are guaranteed to be visible to a\nthread that returns from a call to wait on the same latch object. Formally, the call to\ncount_down synchronizes with the call to wait—we’ll see what that means when we look\nat the low-level memory ordering and synchronization constraints in chapter 5.\n Alongside latches, the Concurrency TS gives us barriers—reusable synchronization\nobjects for synchronizing a group of threads. Let’s look at those next.\nListing 4.25\nWaiting for events with std::experimental::latch\nb\nc\nd\ne\nf\ng\nh\n",
      "content_length": 2643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "120\nCHAPTER 4\nSynchronizing concurrent operations\n4.4.9\nstd::experimental::barrier: a basic barrier\nThe Concurrency TS provides two types of barriers in the <experimental/barrier>\nheader: std::experimental::barrier and std::experimental::flex_barrier. The\nformer is more basic, and potentially therefore has lower overhead, whereas the latter\nis more flexible, but potentially has more overhead.\n Suppose you have a group of threads that are operating on some data. Each thread\ncan do its processing on the data independently of the others, so no synchronization\nis needed during the processing, but all the threads must have completed their pro-\ncessing before the next data item can be processed, or before the subsequent process-\ning can be done. std::experimental::barrier is targeted at precisely this scenario.\nYou construct a barrier with a count specifying the number of threads involved in the\nsynchronization group. As each thread is done with its processing, it arrives at the bar-\nrier and waits for the rest of the group by calling arrive_and_wait on the barrier\nobject. When the last thread in the group arrives, all the threads are released, and the\nbarrier is reset. The threads in the group can then resume their processing and either\nprocess the next data item or proceed with the next stage of processing, as appropriate.\n Whereas latches latch, so once they are ready they stay ready, barriers do not—bar-\nriers release the waiting threads and then reset so they can be used again. They also\nonly synchronize within a group of threads—a thread cannot wait for a barrier to be\nready unless it is one of the threads in the synchronization group. Threads can explic-\nitly drop out of the group by calling arrive_and_drop on the barrier, in which case\nthat thread cannot wait for the barrier to be ready anymore, and the count of threads\nthat must arrive in the next cycle is one less than the number of threads that had to\narrive in the current cycle.\nresult_chunk process(data_chunk);\nstd::vector<data_chunk>\ndivide_into_chunks(data_block data, unsigned num_threads);\nvoid process_data(data_source &source, data_sink &sink) {\n    unsigned const concurrency = std::thread::hardware_concurrency();\n    unsigned const num_threads = (concurrency > 0) ? concurrency : 2;\n    std::experimental::barrier sync(num_threads);\n    std::vector<joining_thread> threads(num_threads);\n    std::vector<data_chunk> chunks;\n    result_block result;\n    for (unsigned i = 0; i < num_threads; ++i) {\n        threads[i] = joining_thread([&, i] {\n            while (!source.done()) {            \n                if (!i) {                 \n                    data_block current_block =\n                        source.get_next_data_block();\nListing 4.26\nUsing std::experimental::barrier\ng\nb\n",
      "content_length": 2786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "121\nUsing synchronization of operations to simplify code\n                    chunks = divide_into_chunks(\n                        current_block, num_threads);\n                }\n                sync.arrive_and_wait();               \n                result.set_chunk(i, num_threads, process(chunks[i]));  \n                sync.arrive_and_wait();                 \n                if (!i) {                               \n                    sink.write_data(std::move(result));\n                }\n            }\n        });\n    }\n}       \nListing 4.26 shows an example of using a barrier to synchronize a group of threads.\nYou have data coming from source, and you’re writing the output to sink, but in\norder to make use of the available concurrency in the system, you’re splitting each\nblock of data into num_threads chunks. This has to be done serially, so you have an\ninitial block B that only runs on the thread for which i==0. All threads then wait on\nthe barrier for that serial code to complete c before you reach the parallel region,\nwhere each thread processes its individual chunk and updates the result with that d\nbefore synchronizing again e. You then have a second serial region where only\nthread 0 writes the result out to the sink f. All threads then keep looping until the\nsource reports that everything is done g. Note that as each thread loops round, the\nserial section at the bottom of the loop combines with the section at the top; because\nonly thread 0 has anything to do in either of these sections, this is OK, and all the\nthreads will synchronize together at the first use of the barrier c. When all the pro-\ncessing is done, then all the threads will exit the loop, and the destructors for the\njoining_thread objects will wait for them all to finish at the end of the outer func-\ntion h (joining_thread was introduced in chapter 2, listing 2.7).\n The key thing to note here is that the calls to arrive_and_wait are at the points in\nthe code where it is important that no threads proceed until all threads are ready. At\nthe first synchronization point, all the threads are waiting for thread 0 to arrive, but\nthe use of the barrier provides you with a clean line in the sand. At the second syn-\nchronization point, you have the reverse situation: it is thread 0 that is waiting for all\nthe other threads to arrive before it can write out the completed result to the sink.\n The Concurrency TS doesn’t just give you one barrier type; as well as std::experi-\nmental::barrier, you also get std::experimental::flex_barrier, which is more flex-\nible. One of the ways that it is more flexible is that it allows for a final serial region to be\nrun when all threads have arrived at the barrier, before they are all released again.\n4.4.10 std::experimental::flex_barrier—std::experimental::barrier’s \nflexible friend\nThe interface to std::experimental::flex_barrier differs from that of std::\nexperimental::barrier in only one way: there is an additional constructor that takes\nc\nd\ne\nf\nh\n",
      "content_length": 2997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "122\nCHAPTER 4\nSynchronizing concurrent operations\na completion function, as well as a thread count. This function is run on exactly one\nof the threads that arrived at the barrier, once all the threads have arrived at the bar-\nrier. Not only does it provide a means of specifying a chunk of code that must be run\nserially, it also provides a means of changing the number of threads that must arrive at\nthe barrier for the next cycle. The thread count can be changed to any number,\nwhether higher or lower than the previous count; it is up to the programmer who uses\nthis facility to ensure that the correct number of threads will arrive at the barrier the\nnext time round.\n The following listing shows how listing 4.26 could be rewritten to use std::\nexperimental::flex_barrier to manage the serial region.\nvoid process_data(data_source &source, data_sink &sink) {\n    unsigned const concurrency = std::thread::hardware_concurrency();\n    unsigned const num_threads = (concurrency > 0) ? concurrency : 2;\n    std::vector<data_chunk> chunks;\n    auto split_source = [&] {     \n        if (!source.done()) {\n            data_block current_block = source.get_next_data_block();\n            chunks = divide_into_chunks(current_block, num_threads);\n        }\n    };\n    split_source();    \n    result_block result;\n    \n    std::experimental::flex_barrier sync(num_threads, [&] {    \n        sink.write_data(std::move(result));\n        split_source();               \n        return -1;          \n    });\n    std::vector<joining_thread> threads(num_threads);\n    for (unsigned i = 0; i < num_threads; ++i) {\n        threads[i] = joining_thread([&, i] {\n            while (!source.done()) {               \n                result.set_chunk(i, num_threads, process(chunks[i]));\n                sync.arrive_and_wait();       \n            }\n        });\n    }\n}\nThe first difference between this code and listing 4.26 is that you’ve extracted a\nlambda that splits the next data block into chunks B. This is called before you start\nc, and encapsulates the code that was run on thread 0 at the start of each iteration.\nListing 4.27\nUsing std::flex_barrier to provide a serial region\nb\nc\nd\ne\nf\ng\nh\n",
      "content_length": 2182,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "123\nSummary\n The second difference is that your sync object is now a std::experimental::flex\n_barrier, and you are passing a completion function as well as a thread count d.\nThis completion function is run on one thread after each thread has arrived, and so\ncan encapsulate the code that was to be run on thread 0 at the end of each iteration,\nand then there’s a call to your newly-extracted split_source lambda that would have\nbeen called at the start of the next iteration e. The return value of -1 f indicates that\nthe number of participating threads is to remain unchanged; a return value of zero or\nmore would specify the number of participating threads in the next cycle.\n The main loop g is now simplified: it only contains the parallel portion of the\ncode, and thus only needs a single synchronization point h. The use of std::\nexperimental::flex_barrier has thus simplified the code.\n The use of the completion function to provide a serial section is quite powerful, as\nis the ability to change the number of participating threads. For example, this could\nbe used by pipeline style code where the number of threads is less during the initial\npriming of the pipeline and the final draining of the pipeline than it is during the\nmain processing, when all the stages of the pipeline are operating.\nSummary\nSynchronizing operations between threads is an important part of writing an applica-\ntion that uses concurrency: if there’s no synchronization, the threads are essentially\nindependent and might as well be written as separate applications that are run as a\ngroup because of their related activities. In this chapter, I’ve covered various ways of\nsynchronizing operations from the basic condition variables, through futures, prom-\nises, packaged tasks, latches, and barriers. I’ve also discussed ways of approaching the\nsynchronization issues: functional-style programming, where each task produces a\nresult entirely dependent on its input rather than on the external environment; mes-\nsage passing, where communication between threads is via asynchronous messages\nsent through a messaging subsystem that acts as an intermediary; and continuation\nstyle, where the follow-on tasks for each operation are specified, and the system takes\ncare of the scheduling.\n Having discussed many of the high-level facilities available in C++, it’s now time to\nlook at the low-level facilities that make it all work: the C++ memory model and atomic\noperations.\n",
      "content_length": 2456,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "124\nThe C++ memory model\nand operations on\natomic types\nOne of the most important features of the C++ Standard is something most pro-\ngrammers won’t even notice. It’s not the new syntax features, nor is it the new\nlibrary facilities, but the new multithreading-aware memory model. Without the\nmemory model to define exactly how the fundamental building blocks work, none\nof the facilities I’ve covered could be relied on to work. There’s a reason that most\nprogrammers won’t notice: if you use mutexes to protect your data and condition\nvariables, futures, latches, or barriers to signal events, the details of why they work\naren’t important. It’s only when you start trying to get “close to the machine” that\nthe precise details of the memory model matter.\n Whatever else it is, C++ is a systems programming language. One of the goals of\nthe Standards Committee is that there will be no need for a lower-level language\nThis chapter covers\nThe details of the C++ memory model\nThe atomic types provided by the C++ \nStandard Library\nThe operations that are available on those types\nHow those operations can be used to provide \nsynchronization between threads\n",
      "content_length": 1162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "125\nMemory model basics\nthan C++. Programmers should be provided with enough flexibility within C++ to do\nwhatever they need without the language getting in the way, allowing them to get\n“close to the machine” when the need arises. The atomic types and operations allow\njust that, providing facilities for low-level synchronization operations that will com-\nmonly reduce to one or two CPU instructions.\n In this chapter, I’ll start by covering the basics of the memory model, then move on\nto the atomic types and operations, and finally cover the various types of synchroniza-\ntion available with the operations on atomic types. This is quite complex: unless you’re\nplanning on writing code that uses the atomic operations for synchronization (such as\nthe lock-free data structures in chapter 7), you won’t need to know these details.\n Let’s ease into things with a look at the basics of the memory model.\n5.1\nMemory model basics\nThere are two aspects to the memory model: the basic structural aspects, which\nrelate to how things are laid out in memory, and the concurrency aspects. The struc-\ntural aspects are important for concurrency, particularly when you’re looking at low-\nlevel atomic operations, so I’ll start with those. In C++, it’s all about objects and\nmemory locations.\n5.1.1\nObjects and memory locations\nAll data in a C++ program is made up of objects. This is not to say that you can create a\nnew class derived from int, or that the fundamental types have member functions, or\nany of the other consequences often implied when people say “everything is an object”\nwhen discussing a language like Smalltalk or Ruby. It’s a statement about the building\nblocks of data in C++. The C++ Standard defines an object as “a region of storage,”\nalthough it goes on to assign properties to these objects, such as their type and lifetime.\n Some of these objects are simple values of a fundamental type such as int or\nfloat, whereas others are instances of user-defined classes. Some objects (such as\narrays, instances of derived classes, and instances of classes with non-static data\nmembers) have sub-objects, but others don’t.\n Whatever its type, an object is stored in one or more memory locations. Each mem-\nory location is either an object (or sub-object) of a scalar type such as unsigned short\nor my_class* or a sequence of adjacent bit fields. If you use bit fields, this is an\nimportant point to note: though adjacent bit fields are distinct objects, they’re still\ncounted as the same memory location. Figure 5.1 shows how a struct divides into\nobjects and memory locations.\n First, the entire struct is one object that consists of several sub-objects, one for\neach data member. The bf1 and bf2 bit fields share a memory location, and the\nstd::string object, s, consists of several memory locations internally, but otherwise\neach member has its own memory location. Note how the zero-length bit field bf3\n(the name is commented out because zero-length bit fields must be unnamed) sepa-\nrates bf4 into its own memory location, but doesn't have a memory location itself.\n",
      "content_length": 3081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "126\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nThere are four important things to take away from this:\nEvery variable is an object, including those that are members of other objects.\nEvery object occupies at least one memory location. \nVariables of fundamental types such as int or char occupy exactly one memory\nlocation, whatever their size, even if they’re adjacent or part of an array.\nAdjacent bit fields are part of the same memory location.\nI’m sure you’re wondering what this has to do with concurrency, so let’s take a look.\n5.1.2\nObjects, memory locations, and concurrency\nNow, here’s the part that’s crucial for multithreaded applications in C++: everything\nhinges on those memory locations. If two threads access separate memory locations,\nthere’s no problem: everything works fine. On the other hand, if two threads access\nthe same memory location, then you have to be careful. If neither thread is updating\nthe memory location, you’re fine; read-only data doesn’t need protection or synchro-\nnization. If either thread is modifying the data, there’s a potential for a race condi-\ntion, as described in chapter 3.\n In order to avoid the race condition, there has to be an enforced ordering\nbetween the accesses in the two threads. This could be a fixed ordering such that one\naccess is always before the other, or it could be an ordering that varies between runs of\nthe application, but guarantees that there is some defined ordering. One way to ensure\nthere’s a defined ordering is to use mutexes as described in chapter 3; if the same\nmutex is locked prior to both accesses, only one thread can access the memory location\nstruct my_data\n{\nint i;\ndouble d;\nunsigned bf1:10;\nint bf2:25;\nint bf3:0;\nint bf4:9;\nint i2;\nchar c1,c2;\nstd::string s;\n};\ns\nc1\ni2\nbf4\nbf3\nbf1\nbf2\ni\nc2\nd\nObject\nMemory Location\nFigure 5.1\nThe division of a struct into objects and memory locations\n",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "127\nMemory model basics\nat a time, so one must happen before the other (though, in general, you can't know in\nadvance which will be first). The other way is to use the synchronization properties of\natomic operations (see section 5.2 for the definition of atomic operations) either on\nthe same or other memory locations to enforce an ordering between the accesses in\nthe two threads. The use of atomic operations to enforce an ordering is described\nin section 5.3. If more than two threads access the same memory location, each pair of\naccesses must have a defined ordering.\n If there’s no enforced ordering between two accesses to a single memory location\nfrom separate threads, one or both of those accesses is not atomic, and if one or both\nis a write, then this is a data race and causes undefined behavior.\n This statement is crucially important: undefined behavior is one of the nastiest cor-\nners of C++. According to the language standard, once an application contains any\nundefined behavior, all bets are off; the behavior of the complete application is now\nundefined, and it may do anything at all. I know of one case where a particular\ninstance of undefined behavior caused someone’s monitor to catch fire. Although this\nis rather unlikely to happen to you, a data race is definitely a serious bug and should\nbe avoided at all costs.\n There’s another important point in that statement: you can also avoid the unde-\nfined behavior by using atomic operations to access the memory location involved\nin the race. This doesn’t prevent the race itself—which of the atomic operations\ntouches the memory location first is still not specified—but it does bring the program\nback into the realm of defined behavior.\n Before we look at atomic operations, there’s one more concept that’s important to\nunderstand about objects and memory locations: modification orders.\n5.1.3\nModification orders\nEvery object in a C++ program has a modification order composed of all the writes to\nthat object from all threads in the program, starting with the object’s initialization. In\nmost cases this order will vary between runs, but in any given execution of the pro-\ngram all threads in the system must agree on the order. If the object in question isn’t\none of the atomic types described in section 5.2, you’re responsible for making certain\nthat there’s sufficient synchronization to ensure that threads agree on the modifica-\ntion order of each variable. If different threads see distinct sequences of values for a\nsingle variable, you have a data race and undefined behavior (see section 5.1.2). If you\ndo use atomic operations, the compiler is responsible for ensuring that the necessary\nsynchronization is in place.\n This requirement means that certain kinds of speculative execution aren’t permit-\nted, because once a thread has seen a particular entry in the modification order, sub-\nsequent reads from that thread must return later values, and subsequent writes from\nthat thread to that object must occur later in the modification order. Also, a read of an\nobject that follows a write to that object in the same thread must either return the\nvalue written or another value that occurs later in the modification order of that\n",
      "content_length": 3220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "128\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nobject. Although all threads must agree on the modification orders of each individual\nobject in a program, they don’t necessarily have to agree on the relative order of oper-\nations on separate objects. See section 5.3.3 for more on the ordering of operations\nbetween threads.\n So, what constitutes an atomic operation, and how can these be used to enforce\nordering?\n5.2\nAtomic operations and types in C++\nAn atomic operation is an indivisible operation. You can’t observe such an operation\nhalf-done from any thread in the system; it’s either done or not done. If the load oper-\nation that reads the value of an object is atomic, and all modifications to that object are\nalso atomic, that load will retrieve either the initial value of the object or the value\nstored by one of the modifications.\n The flip side of this is that a non-atomic operation might be seen as half-done by\nanother thread. If the non-atomic operation is composed of atomic operations (for\nexample, assignment to a struct with atomic members), then other threads may\nobserve some subset of the constituent atomic operations as complete, but others as\nnot yet started, so you might observe or end up with a value that is a mixed-up combi-\nnation of the various values stored. In any case, unsynchronized accesses to non-\natomic variables form a simple problematic race condition, as described in chapter 3,\nbut at this level it may constitute a data race (see section 5.1) and cause undefined\nbehavior.\n In C++, you need to use an atomic type to get an atomic operation in most cases, so\nlet’s look at those.\n5.2.1\nThe standard atomic types\nThe standard atomic types can be found in the <atomic> header. All operations on such\ntypes are atomic, and only operations on these types are atomic in the sense of the lan-\nguage definition, although you can use mutexes to make other operations appear\natomic. In fact, the standard atomic types themselves might use such emulation: they\n(almost) all have an is_lock_free() member function, which allows the user to deter-\nmine whether operations on a given type are done directly with atomic instructions\n(x.is_lock_free() returns true) or done by using a lock internal to the compiler\nand library (x.is_lock_free() returns false).\n This is important to know in many cases—the key use case for atomic operations is\nas a replacement for an operation that would otherwise use a mutex for synchroniza-\ntion; if the atomic operations themselves use an internal mutex then the hoped-for\nperformance gains will probably not materialize, and you might be better off using\nthe easier-to-get-right mutex-based implementation instead. This is the case with lock-\nfree data structures such as those discussed in chapter 7.\n In fact, this is so important that the library provides a set of macros to identify at\ncompile time whether the atomic types for the various integral types are lock-free.\n",
      "content_length": 2965,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "129\nAtomic operations and types in C++\nSince C++17, all atomic types have a static constexpr member variable, X::is_\nalways_lock_free, which is true if and only if the atomic type X is lock-free for all\nsupported hardware that the output of the current compilation might run on. For\nexample, for a given target platform, std::atomic<int> might always be lock-free, so\nstd::atomic<int>::is_always_lock_free will be true, but std::atomic<uintmax_t>\nmight only be lock-free if the hardware the program ends up running on supports the\nnecessary instructions, so this is a run-time property, and std::atomic<uintmax_t>\n::is_always_lock_free would be false when compiling for that platform.\n The macros are ATOMIC_BOOL_LOCK_FREE, ATOMIC_CHAR_LOCK_FREE, ATOMIC_\nCHAR16_T_LOCK_FREE, \nATOMIC_CHAR32_T_LOCK_FREE, \nATOMIC_WCHAR_T_LOCK_FREE,\nATOMIC_SHORT_LOCK_FREE, ATOMIC_INT_LOCK_FREE, ATOMIC_LONG_LOCK_FREE, ATOMIC\n_LLONG_LOCK_FREE, and ATOMIC_POINTER_LOCK_FREE. They specify the lock-free status\nof the corresponding atomic types for the specified built-in types and their unsigned\ncounterparts (LLONG refers to long long, and POINTER refers to all pointer types).\nThey evaluate to the value 0 if the atomic type is never lock-free, to the value 2 if the\natomic type is always lock-free, and to the value 1 if the lock-free status of the corre-\nsponding atomic type is a runtime property as described previously.\n The only type that doesn’t provide an is_lock_free() member function is\nstd::atomic_flag. This type is a simple Boolean flag, and operations on this type are\nrequired to be lock-free; once you have a simple lock-free Boolean flag, you can use that\nto implement a simple lock and implement all the other atomic types using that as a\nbasis. When I said simple, I meant it: objects of the std::atomic_flag type are initial-\nized to clear, and they can then either be queried and set (with the test_and_set()\nmember function) or cleared (with the clear() member function). That’s it: no\nassignment, no copy construction, no test and clear, no other operations at all.\n The remaining atomic types are all accessed through specializations of the\nstd::atomic<> class template and are a bit more full-featured but may not be lock-\nfree (as explained previously). On most popular platforms it’s expected that the\natomic variants of all the built-in types (such as std::atomic<int> and std::atomic\n<void*>) are indeed lock-free, but it isn’t required. As you’ll see shortly, the inter-\nface of each specialization reflects the properties of the type; bitwise operations\nsuch as &= aren’t defined for plain pointers, so they aren’t defined for atomic point-\ners either, for example.\n In addition to using the std::atomic<> class template directly, you can use the set\nof names shown in table 5.1 to refer to the implementation-supplied atomic types.\nBecause of the history of how atomic types were added to the C++ Standard, if you\nhave an older compiler, these alternative type names may refer either to the corre-\nsponding std::atomic<> specialization or to a base class of that specialization,\nwhereas in a compiler that fully supports C++17, these are always aliases for the corre-\nsponding std::atomic<> specializations. Mixing these alternative names with the\ndirect naming of std::atomic<> specializations in the same program can therefore\nlead to nonportable code.\n",
      "content_length": 3368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "130\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nAs well as the basic atomic types, the C++ Standard Library also provides a set of\ntypedefs for the atomic types corresponding to the various non-atomic Standard\nLibrary typedefs such as std::size_t. These are shown in table 5.2.\nTable 5.1\nThe alternative names for the standard atomic types and their corresponding \nstd::atomic<> specializations\nAtomic type\nCorresponding specialization\natomic_bool\nstd::atomic<bool>\natomic_char\nstd::atomic<char>\natomic_schar\nstd::atomic<signed char>\natomic_uchar\nstd::atomic<unsigned char>\natomic_int\nstd::atomic<int>\natomic_uint\nstd::atomic<unsigned>\natomic_short\nstd::atomic<short>\natomic_ushort\nstd::atomic<unsigned short>\natomic_long\nstd::atomic<long>\natomic_ulong\nstd::atomic<unsigned long>\natomic_llong\nstd::atomic<long long>\natomic_ullong\nstd::atomic<unsigned long long>\natomic_char16_t\nstd::atomic<char16_t>\natomic_char32_t\nstd::atomic<char32_t>\natomic_wchar_t\nstd::atomic<wchar_t>\nTable 5.2\nThe standard atomic typedefs and their corresponding built-in typedefs\nAtomic typedef\nCorresponding Standard Library typedef\natomic_int_least8_t\nint_least8_t\natomic_uint_least8_t\nuint_least8_t\natomic_int_least16_t\nint_least16_t\natomic_uint_least16_t\nuint_least16_t\natomic_int_least32_t\nint_least32_t\natomic_uint_least32_t\nuint_least32_t\natomic_int_least64_t\nint_least64_t\natomic_uint_least64_t\nuint_least64_t\natomic_int_fast8_t\nint_fast8_t\n",
      "content_length": 1442,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "131\nAtomic operations and types in C++\nThat’s a lot of types! There’s a rather simple pattern to it; for a standard typedef T, the\ncorresponding atomic type is the same name with an atomic_ prefix: atomic_T. The\nsame applies to the built-in types, except that signed is abbreviated as s, unsigned as\nu, and long long as llong. It’s generally simpler to say std::atomic<T> for whichever\nT you want to work with, rather than use the alternative names.\n The standard atomic types are not copyable or assignable in the conventional\nsense, in that they have no copy constructors or copy assignment operators. They do,\nhowever, support assignment from and implicit conversion to the corresponding\nbuilt-in types as well as direct load() and store() member functions, exchange(),\ncompare_exchange_weak(), and compare_exchange_strong(). They also support the\ncompound assignment operators where appropriate: +=, -=, *=, |=, and so on, and\nthe integral types and std::atomic<> specializations for ++ and -- pointers support.\nThese operators also have corresponding named member functions with the same\nfunctionality: fetch_add(), fetch_or(), and so on. The return value from the assign-\nment operators and member functions is either the value stored (in the case of the\nassignment operators) or the value prior to the operation (in the case of the named\nfunctions). This avoids the potential problems that could stem from the usual habit of\nthese assignment operators returning a reference to the object being assigned to. In\norder to get the stored value from these references, the code would have to perform a\nseparate read, allowing another thread to modify the value between the assignment\nand the read and opening the door for a race condition.\natomic_uint_fast8_t\nuint_fast8_t\natomic_int_fast16_t\nint_fast16_t\natomic_uint_fast16_t\nuint_fast16_t\natomic_int_fast32_t\nint_fast32_t\natomic_uint_fast32_t\nuint_fast32_t\natomic_int_fast64_t\nint_fast64_t\natomic_uint_fast64_t\nuint_fast64_t\natomic_intptr_t\nintptr_t\natomic_uintptr_t\nuintptr_t\natomic_size_t\nsize_t\natomic_ptrdiff_t\nptrdiff_t\natomic_intmax_t\nintmax_t\natomic_uintmax_t\nuintmax_t\nTable 5.2\nThe standard atomic typedefs and their corresponding built-in typedefs (continued)\nAtomic typedef\nCorresponding Standard Library typedef\n",
      "content_length": 2277,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "132\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n The std::atomic<> class template isn’t only a set of specializations, though. It\ndoes have a primary template that can be used to create an atomic variant of a user-\ndefined type. Because it’s a generic class template, the operations are limited to\nload(), store() (and assignment from and conversion to the user-defined type),\nexchange(), compare_exchange_weak(), and compare_exchange_strong().\n Each of the operations on the atomic types has an optional memory-ordering argu-\nment which is one of the values of the std::memory_order enumeration. This argu-\nment is used to specify the required memory-ordering semantics. The std::memory\n_order enumeration has six possible values: std::memory_order_relaxed, std::\nmemory_order_acquire, std::memory_order_consume, std::memory_order_acq_rel,\nstd::memory_order_release, and std::memory_order_seq_cst. \n The permitted values for the memory ordering depend on the operation category.\nIf you don't specify an ordering value, then the default ordering is used, which is the\nstrongest ordering: std::memory_order_seq_cst. The precise semantics of the mem-\nory-ordering options are covered in section 5.3. For now, it suffices to know that the\noperations are divided into three categories:\nStore operations, which can have memory_order_relaxed, memory_order_release,\nor memory_order_seq_cst ordering \nLoad operations, which can have memory_order_relaxed, memory_order_consume,\nmemory_order_acquire, or memory_order_seq_cst ordering \nRead-modify-write operations, which can have memory_order_relaxed, memory_\norder_consume, memory_order_acquire, memory_order_release, memory_order\n_acq_rel, or memory_order_seq_cst ordering\nLet’s now look at the operations you can perform on each of the standard atomic\ntypes, starting with std::atomic_flag.\n5.2.2\nOperations on std::atomic_flag\nstd::atomic_flag is the simplest standard atomic type, which represents a Boolean\nflag. Objects of this type can be in one of two states: set or clear. It’s deliberately basic\nand is intended as a building block only. As such, I’d never expect to see it in use,\nexcept under special circumstances. Even so, it will serve as a starting point for discuss-\ning the other atomic types, because it shows some of the general policies that apply to\nthe atomic types.\n Objects of the std::atomic_flag type must be initialized with ATOMIC_FLAG_INIT.\nThis initializes the flag to a clear state. There’s no choice in the matter; the flag always\nstarts clear:\nstd::atomic_flag f=ATOMIC_FLAG_INIT;\nThis applies no matter where the object is declared and what scope it has. It’s the only\natomic type to require such special treatment for initialization, but it’s also the only\ntype guaranteed to be lock-free. If the std::atomic_flag object has static storage\n",
      "content_length": 2838,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "133\nAtomic operations and types in C++\nduration, it’s guaranteed to be statically initialized, which means that there are no\ninitialization-order issues; it will always be initialized by the time of the first operation\non the flag.\n Once you have your flag object initialized, there are only three things you can do\nwith it: destroy it, clear it, or set it and query the previous value. These correspond\nto the destructor, the clear() member function, and the test_and_set() member\nfunction, respectively. Both the clear() and test_and_set() member functions\ncan have a memory order specified. clear() is a store operation and so can’t have\nmemory_order_acquire or memory_order_acq_rel semantics, but test_and_set()\nis a read-modify-write operation and so can have any of the memory-ordering tags\napplied. As with every atomic operation, the default for both is memory_order_seq_cst.\nFor example:\nf.clear(std::memory_order_release);    \nbool x=f.test_and_set();           \nHere, the call to clear() B explicitly requests that the flag is cleared with release\nsemantics, whereas the call to test_and_set() c uses the default memory ordering\nfor setting the flag and retrieving the old value.\n You can’t copy-construct another std::atomic_flag object from the first, and\nyou can’t assign one std::atomic_flag to another. This isn’t something peculiar to\nstd::atomic_flag but something common with all the atomic types. All operations on\nan atomic type are defined as atomic, and assignment and copy-construction involve two\nobjects. A single operation on two distinct objects can’t be atomic. In the case of copy-\nconstruction or copy-assignment, the value must first be read from one object and then\nwritten to the other. These are two separate operations on two separate objects, and the\ncombination can’t be atomic. Therefore, these operations aren’t permitted.\n The limited feature set makes std::atomic_flag ideally suited to use as a spin-\nlock mutex. Initially, the flag is clear and the mutex is unlocked. To lock the mutex,\nloop on test_and_set() until the old value is false, indicating that this thread set the\nvalue to true. Unlocking the mutex is simply a matter of clearing the flag. This imple-\nmentation is shown in the following listing.\nclass spinlock_mutex\n{\n    std::atomic_flag flag;\npublic:\n    spinlock_mutex():\n        flag(ATOMIC_FLAG_INIT)\n    {}\n    void lock()\n    {\n        while(flag.test_and_set(std::memory_order_acquire));\n    }\nListing 5.1\nImplementation of a spinlock mutex using std::atomic_flag\nB\nc\n",
      "content_length": 2534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "134\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n    void unlock()\n    {\n        flag.clear(std::memory_order_release);\n    }\n};\nThis mutex is basic, but it’s enough to use with std::lock_guard<> (see chapter 3). By\nits nature it does a busy-wait in lock(), so it’s a poor choice if you expect there to be any\ndegree of contention, but it’s enough to ensure mutual exclusion. When we look at the\nmemory-ordering semantics, you’ll see how this guarantees the necessary enforced\nordering that goes with a mutex lock. This example is covered in section 5.3.6.\n std::atomic_flag is so limited that it can’t even be used as a general Boolean flag,\nbecause it doesn’t have a simple nonmodifying query operation. For that you’re better\noff using std::atomic<bool>, so I’ll cover that next.\n5.2.3\nOperations on std::atomic<bool>\nThe most basic of the atomic integral types is std::atomic<bool>. This is a more full-\nfeatured Boolean flag than std::atomic_flag, as you might expect. Although it’s still\nnot copy-constructible or copy-assignable, you can construct it from a non-atomic\nbool, so it can be initially true or false, and you can also assign to instances of\nstd::atomic<bool> from a non-atomic bool:\nstd::atomic<bool> b(true);\nb=false;\nOne other thing to note about the assignment operator from a non-atomic bool is\nthat it differs from the general convention of returning a reference to the object it’s\nassigned to: it returns a bool with the value assigned instead. This is another common\npattern with the atomic types: the assignment operators they support return values (of\nthe corresponding non-atomic type) rather than references. If a reference to the\natomic variable was returned, any code that depended on the result of the assignment\nwould then have to explicitly load the value, potentially getting the result of a modifi-\ncation by another thread. By returning the result of the assignment as a non-atomic\nvalue, you can avoid this additional load, and you know that the value obtained is the\nvalue stored.\n Rather than using the restrictive clear() function of std::atomic_flag, writes (of\neither true or false) are done by calling store(), although the memory-order\nsemantics can still be specified. Similarly, test_and_set() has been replaced with the\nmore general exchange() member function that allows you to replace the stored\nvalue with a new one of your choosing and atomically retrieve the original value.\nstd::atomic<bool> also supports a plain nonmodifying query of the value with an\nimplicit conversion to plain bool or with an explicit call to load(). As you might\nexpect, store() is a store operation, whereas load() is a load operation. exchange()\nis a read-modify-write operation:\n",
      "content_length": 2728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "135\nAtomic operations and types in C++\nstd::atomic<bool> b;\nbool x=b.load(std::memory_order_acquire);\nb.store(true);\nx=b.exchange(false,std::memory_order_acq_rel);\nexchange() isn’t the only read-modify-write operation supported by std::atomic<bool>;\nit also introduces an operation to store a new value if the current value is equal to an\nexpected value.\nSTORING A NEW VALUE (OR NOT) DEPENDING ON THE CURRENT VALUE\nThis new operation is called compare-exchange, and it comes in the form of the\ncompare_exchange_weak() and compare_exchange_strong() member functions. The\ncompare-exchange operation is the cornerstone of programming with atomic types;\nit compares the value of the atomic variable with a supplied expected value and\nstores the supplied desired value if they’re equal. If the values aren’t equal, the\nexpected value is updated with the value of the atomic variable. The return type of\nthe compare-exchange functions is a bool, which is true if the store was performed\nand false otherwise. The operation is said to succeed if the store was done (because\nthe values were equal), and fail otherwise; the return value is true for success, and\nfalse for failure.\n For compare_exchange_weak(), the store might not be successful even if the origi-\nnal value was equal to the expected value, in which case the value of the variable is\nunchanged and the return value of compare_exchange_weak() is false. This is most\nlikely to happen on machines that lack a single compare-and-exchange instruction, if\nthe processor can’t guarantee that the operation has been done atomically—possibly\nbecause the thread performing the operation was switched out in the middle of the\nnecessary sequence of instructions and another thread scheduled in its place by the\noperating system where there are more threads than processors. This is called a spuri-\nous failure, because the reason for the failure is a function of timing rather than the\nvalues of the variables.\n Because compare_exchange_weak() can fail spuriously, it must typically be used in\na loop:\nbool expected=false;\nextern atomic<bool> b; // set somewhere else\nwhile(!b.compare_exchange_weak(expected,true) && !expected);\nIn this case, you keep looping as long as expected is still false, indicating that the\ncompare_exchange_weak() call failed spuriously.\n On the other hand, compare_exchange_strong() is guaranteed to return false\nonly if the value wasn’t equal to the expected value. This can eliminate the need for\nloops like the one shown where you want to know whether you successfully changed a\nvariable or whether another thread got there first.\n If you want to change the variable whatever the initial value is (perhaps with an\nupdated value that depends on the current value), the update of expected becomes\n",
      "content_length": 2769,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "136\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nuseful; each time through the loop, expected is reloaded, so if no other thread modi-\nfies the value in the meantime, the compare_exchange_weak() or compare_exchange\n_strong() call should be successful the next time around the loop. If the calculation\nof the value to be stored is simple, it may be beneficial to use compare_exchange\n_weak() in order to avoid a double loop on platforms where compare_exchange_weak()\ncan fail spuriously (and so compare_exchange_strong() contains a loop). On the\nother hand, if the calculation of the value to be stored is time-consuming, it may make\nsense to use compare_exchange_strong() to avoid having to recalculate the value to\nstore when the expected value hasn’t changed. For std::atomic<bool> this isn’t so\nimportant—there are only two possible values after all—but for the larger atomic types\nthis can make a difference.\n The compare-exchange functions are also unusual in that they can take two memory-\nordering parameters. This allows for the memory-ordering semantics to differ in the\ncase of success and failure; it might be desirable for a successful call to have memory\n_order_acq_rel semantics, whereas a failed call has memory_order_relaxed seman-\ntics. A failed compare-exchange doesn’t do a store, so it can’t have memory_order\n_release or memory_order_acq_rel semantics. It’s therefore not permitted to supply\nthese values as the ordering for failure. You also can’t supply stricter memory ordering\nfor failure than for success; if you want memory_order_acquire or memory_order_\nseq_cst semantics for failure, you must specify those for success as well.\n If you don’t specify an ordering for failure, it’s assumed to be the same as that for\nsuccess, except that the release part of the ordering is stripped: memory_order_release\nbecomes memory_order_relaxed, and memory_order_acq_rel becomes memory_order\n_acquire. If you specify neither, they default to memory_order_seq_cst as usual,\nwhich provides the full sequential ordering for both success and failure. The following\ntwo calls to compare_exchange_weak() are equivalent:\nstd::atomic<bool> b;\nbool expected;\nb.compare_exchange_weak(expected,true,\n    memory_order_acq_rel,memory_order_acquire);\nb.compare_exchange_weak(expected,true,memory_order_acq_rel);\nI’ll leave the consequences of the choice of memory ordering to section 5.3.\n One further difference between std::atomic<bool> and std::atomic_flag is\nthat std::atomic<bool> may not be lock-free; the implementation may have to\nacquire a mutex internally in order to ensure the atomicity of the operations. For the\nrare case when this matters, you can use the is_lock_free() member function to\ncheck whether operations on std::atomic<bool> are lock-free. This is another fea-\nture common to all atomic types other than std::atomic_flag.\n The next simplest of the atomic types are the atomic pointer specializations\nstd::atomic<T*>, so we’ll look at those next.\n",
      "content_length": 2991,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "137\nAtomic operations and types in C++\n5.2.4\nOperations on std::atomic<T*>: pointer arithmetic\nThe atomic form of a pointer to some type T  is std::atomic<T*>, just as the atomic\nform of bool is std::atomic<bool>. The interface is the same, although it operates\non values of the corresponding pointer type rather than bool values. Like\nstd::atomic<bool>, it’s neither copy-constructible nor copy-assignable, although it\ncan be both constructed and assigned from the suitable pointer values. As well as the\nobligatory is_lock_free() member function, std::atomic<T*> also has load(),\nstore(), exchange(), compare_exchange_weak(), and compare_exchange_strong()\nmember functions, with similar semantics to those of std::atomic<bool>, again tak-\ning and returning T* rather than bool.\n The new operations provided by std::atomic<T*> are the pointer arithmetic\noperations. The basic operations are provided by the fetch_add() and fetch_sub()\nmember functions, which do atomic addition and subtraction on the stored address,\nand the += and -= operators, and both pre- and post-increment and decrement with\n++ and --, which provide convenient wrappers. The operators work as you’d expect\nfrom the built-in types: if x is std::atomic<Foo*> to the first entry of an array of Foo\nobjects, then x+=3 changes it to point to the fourth entry and returns a plain Foo* that\nalso points to that fourth entry. fetch_add() and fetch_sub() are slightly different in\nthat they return the original value (so x.fetch_add(3) will update x to point to the\nfourth value but return a pointer to the first value in the array). This operation is also\nknown as exchange-and-add, and it’s an atomic read-modify-write operation, like\nexchange() and compare_exchange_weak()/compare_exchange_strong(). As with\nthe other operations, the return value is a plain T* value rather than a reference to the\nstd::atomic<T*> object, so that the calling code can perform actions based on what\nthe previous value was:\nclass Foo{};\nFoo some_array[5];\nstd::atomic<Foo*> p(some_array);\nFoo* x=p.fetch_add(2);            \nassert(x==some_array);\nassert(p.load()==&some_array[2]);\nx=(p-=1);                         \nassert(x==&some_array[1]);\nassert(p.load()==&some_array[1]);\nThe function forms also allow the memory-ordering semantics to be specified as an\nadditional function call argument:\np.fetch_add(3,std::memory_order_release);\nBecause both fetch_add() and fetch_sub() are read-modify-write operations, they\ncan have any of the memory-ordering tags and can participate in a release sequence. Speci-\nfying the ordering semantics isn’t possible for the operator forms, because there’s no\nAdd 2 to p and \nreturn old value\nSubtract 1 from p and \nreturn new value\n",
      "content_length": 2716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "138\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nway of providing the information: these forms therefore always have memory_order_\nseq_cst semantics.\n The remaining basic atomic types are all the same: they’re all atomic integral types\nand have the same interface as each other, except that the associated built-in type is\ndifferent. We’ll look at them as a group.\n5.2.5\nOperations on standard atomic integral types\nAs well as the usual set of operations (load(), store(), exchange(), compare_\nexchange_weak(), and compare_exchange_strong()), the atomic integral types such\nas std::atomic<int> or std::atomic<unsigned long long> have quite a comprehen-\nsive set of operations available: fetch_add(), fetch_sub(), fetch_and(), fetch_or(),\nfetch_xor(), compound-assignment forms of these operations (+=, -=, &=, |=, and\n^=), and pre- and post-increment and decrement (++x, x++, --x, and x--). It’s not quite\nthe full set of compound-assignment operations you could do on a normal integral\ntype, but it’s close enough: only division, multiplication, and shift operators are miss-\ning. Because atomic integral values are typically used either as counters or as bitmasks,\nthis isn’t a particularly noticeable loss; additional operations can easily be done using\ncompare_exchange_weak() in a loop, if required.\n The semantics closely match those of fetch_add() and fetch_sub() for\nstd::atomic<T*>; the named functions atomically perform their operation and\nreturn the old value, whereas the compound-assignment operators return the new\nvalue. Pre- and post- increment and decrement work as usual: ++x increments the vari-\nable and returns the new value, whereas x++ increments the variable and returns the\nold value. As you’ll be expecting, the result is a value of the associated integral type in\nboth cases.\n We’ve now looked at all the basic atomic types; all that remains is the generic\nstd::atomic<> primary class template rather than the specializations, so let’s look at\nthat next.\n5.2.6\nThe std::atomic<> primary class template\nThe presence of the primary template allows a user to create an atomic variant of a\nuser-defined type, in addition to the standard atomic types. Given a user-defined type\nUDT, std::atomic<UDT> provides the same interface as std::atomic<bool> (as\ndescribed in section 5.2.3), except that the bool parameters and return types that\nrelate to the stored value (rather than the success/failure result of the compare-\nexchange operations) are UDT instead. You can’t use just any user-defined type with\nstd::atomic<>, though; the type has to fulfill certain criteria. In order to use\nstd::atomic<UDT> for some user-defined type UDT,, this type must have a trivial copy-\nassignment operator. This means that the type must not have any virtual functions or\nvirtual base classes and must use the compiler-generated copy-assignment operator.\nNot only that, but every base class and non-static data member of a user-defined type\nmust also have a trivial copy-assignment operator. This permits the compiler to use\n",
      "content_length": 3048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "139\nAtomic operations and types in C++\nmemcpy() or an equivalent operation for assignment operations, because there’s no\nuser-written code to run.\n Finally, it is worth noting that the compare-exchange operations do bitwise com-\nparison as if using memcmp, rather than using any comparison operator that may be\ndefined for UDT. If the type provides comparison operations that have different seman-\ntics, or the type has padding bits that do not participate in normal comparisons, then\nthis can lead to a compare-exchange operation failing, even though the values com-\npare equally.\n The reasoning behind these restrictions goes back to one of the guidelines from\nchapter 3: don’t pass pointers and references to protected data outside the scope of\nthe lock by passing them as arguments to user-supplied functions. In general, the\ncompiler isn’t going to be able to generate lock-free code for std::atomic<UDT>, so it\nwill have to use an internal lock for all the operations. If user-supplied copy-assignment\nor comparison operators were permitted, this would require passing a reference to\nthe protected data as an argument to a user-supplied function, violating the guideline.\nAlso, the library is entirely at liberty to use a single lock for all atomic operations that\nneed it, and allowing user-supplied functions to be called while holding that lock\nmight cause deadlock or cause other threads to block because a comparison opera-\ntion took a long time. Finally, these restrictions increase the chance that the compiler\nwill be able to make use of atomic instructions directly for std::atomic<UDT> (and\nmake a particular instantiation lock-free), because it can treat the user-defined type as\na set of raw bytes.\n Note that although you can use std::atomic<float> or std::atomic<double>,\nbecause the built-in floating point types do satisfy the criteria for use with memcpy and\nmemcmp, the behavior may be surprising in the case of compare_exchange_strong\n(compare_exchange_weak can always fail for arbitrary internal reasons, as described\npreviously). The operation may fail even though the old stored value was equal in\nvalue to the comparand, if the stored value had a different representation. Note that\nthere are no atomic arithmetic operations on floating-point values. You’ll get similar\nbehavior with compare_exchange_strong if you use std::atomic<> with a user-\ndefined type that has an equality-comparison operator defined, and that operator\ndiffers from the comparison using memcmp—the operation may fail because the\notherwise-equal values have a different representation.\n If your UDT is the same size as (or smaller than) an int or a void*, most common\nplatforms will be able to use atomic instructions for std::atomic<UDT>. Some plat-\nforms will also be able to use atomic instructions for user-defined types that are twice\nthe size of an int or void*. These platforms are typically those that support a so-called\ndouble-word-compare-and-swap (DWCAS) instruction corresponding to the compare_\nexchange_xxx functions. As you’ll see in chapter 7, such support can be helpful when\nwriting lock-free code.\n These restrictions mean that you can’t, for example, create std::atomic<std::\nvector<int>> (because it has a non-trivial copy constructor and copy assignment\n",
      "content_length": 3279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "140\nCHAPTER 5\nThe C++ memory model and operations on atomic types\noperator), but you can instantiate std::atomic<> with classes containing counters or\nflags or pointers or even arrays of simple data elements. This isn’t particularly a prob-\nlem; the more complex the data structure, the more likely you’ll want to do opera-\ntions on it other than simple assignment and comparison. If that’s the case, you’re\nbetter off using an std::mutex to ensure that the data is appropriately protected for\nthe desired operations, as described in chapter 3.\n As already mentioned, when instantiated with a user-defined type T, the interface\nof std::atomic<T> is limited to the set of operations available for std::atomic<bool>:\nload(), store(), exchange(), compare_exchange_weak(), compare_exchange_strong(),\nand assignment from and conversion to an instance of type T.\n Table 5.3 shows the operations available on each atomic type.\n5.2.7\nFree functions for atomic operations\nUp until now I’ve limited myself to describing the member function forms of the\noperations on the atomic types. But there are also equivalent nonmember functions\nfor all the operations on the various atomic types. For the most part, the nonmember\nfunctions are named after the corresponding member functions but with an atomic_\nprefix (for example, std::atomic_load()). These functions are then overloaded for\nTable 5.3\nThe operations available on atomic types\nOperation\natomic_\nflag\natomic\n<bool>\natomic\n<T*>\natomic\n<integral\n-type>\natomic\n<other-\ntype>\ntest_and_set\nY\nclear\nY\nis_lock_free\nY\nY\nY\nY\nload\nY\nY\nY\nY\nstore\nY\nY\nY\nY\nexchange\nY\nY\nY\nY\ncompare_exchange\n_weak, compare_ \nexchange_strong\nY\nY\nY\nY\nfetch_add, +=\nY\nY\nfetch_sub, -=\nY\nY\nfetch_or, |=\nY\nfetch_and, &=\nY\nfetch_xor, ^=\nY\n++, --\nY\nY\n",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "141\nAtomic operations and types in C++\neach of the atomic types. Where there’s opportunity for specifying a memory-ordering\ntag, they come in two varieties: one without the tag and one with an _explicit suffix\nand an additional parameter or parameters for the memory-ordering tag or tags (for\nexample, std::atomic_store(&atomic_var,new_value) versus std::atomic_store_\nexplicit(&atomic_var,new_value,std::memory_order_release). Whereas the atomic\nobject being referenced by the member functions is implicit, all the free functions\ntake a pointer to the atomic object as the first parameter.\n For example, std::atomic_is_lock_free() comes in one variety (though over-\nloaded for each type), and std::atomic_is_lock_free(&a) returns the same value as\na.is_lock_free() for an object of atomic type a. Likewise, std::atomic_load(&a) is\nthe same as a.load(), but the equivalent of a.load(std::memory_order_acquire) is\nstd::atomic_load_explicit(&a, std::memory_order_acquire).\n The free functions are designed to be C-compatible, so they use pointers rather\nthan references in all cases. For example, the first parameter of the compare_ex-\nchange_weak() and compare_exchange_strong() member functions (the expected\nvalue) is a reference, whereas the second parameter of std::atomic_compare_\nexchange_weak() (the first is the object pointer) is a pointer. std::atomic_compare\n_exchange_weak_explicit() also requires both the success and failure memory\norders to be specified, whereas the compare-exchange member functions have both a\nsingle memory order form (with a default of std::memory_order_seq_cst) and an\noverload that takes the success and failure memory orders separately.\n The operations on std::atomic_flag buck the trend in that they spell out the\nflag part in the names: std::atomic_flag_test_and_set(), std::atomic_flag_\nclear(). The additional variants that specify the memory ordering again have the\n_explicit suffix: std::atomic_flag_test_and_set_explicit() and std::atomic_\nflag_clear_explicit().\n The C++ Standard Library also provides free functions for accessing instances of\nstd::shared_ptr<> in an atomic fashion. This is a break from the principle that only\nthe atomic types support atomic operations, because std::shared_ptr<> is quite defi-\nnitely not an atomic type (accessing the same std::shared_ptr<T> object from multi-\nple threads without using the atomic access functions from all threads, or using\nsuitable other external synchronization, is a data race and undefined behavior). But\nthe C++ Standards Committee felt it was sufficiently important to provide these extra\nfunctions. The atomic operations available are load, store, exchange, and compare-\nexchange, which are provided as overloads of the same operations on the standard\natomic types, taking an std::shared_ptr<>* as the first argument:\nstd::shared_ptr<my_data> p;\nvoid process_global_data()\n{\n    std::shared_ptr<my_data> local=std::atomic_load(&p);\n    process_data(local);\n}\n",
      "content_length": 2967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "142\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nvoid update_global_data()\n{\n    std::shared_ptr<my_data> local(new my_data);\n    std::atomic_store(&p,local);\n}\nAs with the atomic operations on other types, the _explicit variants are also pro-\nvided to allow you to specify the desired memory ordering, and the std::atomic_\nis_lock_free() function can be used to check whether the implementation uses\nlocks to ensure the atomicity.\n The Concurrency TS also provides std::experimental::atomic_shared_ptr<T>,\nwhich is an atomic type. To use it you must include the <experimental/atomic>\nheader. It provides the same set of operations as std::atomic<UDT>: load, store,\nexchange, compare-exchange. It is provided as a separate type because that allows for\na lock-free implementation that does not impose an additional cost on plain\nstd::shared_ptr instances. But as with the std::atomic template, you still need to\ncheck whether it is lock-free on your platform, which can be tested with the is_lock_\nfree member function. Even if it is not lock-free, std::experimental::atomic_\nshared_ptr is to be recommended over using the atomic free functions on a plain\nstd::shared_ptr, as it is clearer in your code, ensures that all accesses are atomic,\nand avoids the potential for data races due to forgetting to use the atomic free func-\ntions. As with all uses of atomic types and operations, if you are using them for a\npotential speed gain, it is important to profile, and compare with using alternative syn-\nchronization mechanisms.\n As described in the introduction, the standard atomic types do more than avoid\nthe undefined behavior associated with a data race; they allow the user to enforce an\nordering of operations between threads. This enforced ordering is the basis of the\nfacilities for protecting data and synchronizing operations such as std::mutex and\nstd::future<>. With that in mind, let’s move on to the real meat of this chapter: the\ndetails of the concurrency aspects of the memory model and how atomic operations\ncan be used to synchronize data and enforce ordering.\n5.3\nSynchronizing operations and enforcing ordering\nSuppose you have two threads, one of which is populating a data structure to be read\nby the second. In order to avoid a problematic race condition, the first thread sets a\nflag to indicate that the data is ready, and the second thread doesn’t read the data\nuntil the flag is set. The following listing shows such a scenario.\n#include <vector>\n#include <atomic>\n#include <iostream>\nstd::vector<int> data;\nstd::atomic<bool> data_ready(false);\nListing 5.2\nReading and writing variables from different threads\n",
      "content_length": 2655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "143\nSynchronizing operations and enforcing ordering\nvoid reader_thread()\n{\n    while(!data_ready.load())   \n    {\n        std::this_thread::sleep(std::chrono::milliseconds(1));\n    }\n    std::cout<<”The answer=”<<data[0]<<”\\n”;    \n}\nvoid writer_thread()\n{\n    data.push_back(42);     \n    data_ready=true;     \n}\nSetting aside the inefficiency of the loop waiting for the data to be ready B, you\nneed this to work, because otherwise sharing data between threads becomes impracti-\ncal: every item of data is forced to be atomic. You’ve already learned that it’s unde-\nfined behavior to have non-atomic reads c and writes d accessing the same data\nwithout an enforced ordering, so for this to work there must be an enforced ordering\nsomewhere.\n The required enforced ordering comes from the operations on the std::\natomic<bool> variable, data_ready;, they provide the necessary ordering by virtue of\nthe memory model relations happens-before and synchronizes-with. The write of the data\nd happens before the write to the data_ready flag e, and the read of the flag B\nhappens before the read of the data c. When the value read from data_ready B is\ntrue, the write synchronizes with that read, creating a happens-before relationship.\nBecause happens-before is transitive, the write to the data d happens before the write\nto the flag e, which happens before the read of the true value from the flag B,\nwhich happens before the read of the data c, and you have an enforced ordering: the\nwrite of the data happens before the read of the data and everything is OK. Figure 5.2\nshows the important happens-before relationships in the two threads. I’ve added a\ncouple of iterations of the while loop from the reader thread.\n All this might seem fairly intuitive: the operation that writes a value happens before\nan operation that reads that value. With the default atomic operations, that’s indeed\ntrue (which is why this is the default), but it does need spelling out: the atomic opera-\ntions also have other options for the ordering requirements, which I’ll come to shortly.\n Now that you’ve seen happens-before and synchronizes-with in action, it’s time to\nlook at what they mean. I’ll start with synchronizes-with.\n5.3.1\nThe synchronizes-with relationship\nThe synchronizes-with relationship is something that you can get only between opera-\ntions on atomic types. Operations on a data structure (such as locking a mutex) might\nprovide this relationship if the data structure contains atomic types and the opera-\ntions on that data structure perform the appropriate atomic operations internally, but\nfundamentally it comes only from operations on atomic types.\nb\nc\nd\ne\n",
      "content_length": 2662,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "144\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nThe basic idea is this: a suitably-tagged atomic write operation, W, on a variable, x, syn-\nchronizes with a suitably-tagged atomic read operation on x that reads the value stored\nby either that write, W, or a subsequent atomic write operation on x by the same thread\nthat performed the initial write, W, or a sequence of atomic read-modify-write operations\non x (such as fetch_add() or compare_exchange_weak()) by any thread, where the\nvalue read by the first thread in the sequence is the value written by W (see section 5.3.4). \n Leave the “suitably-tagged” part aside for now, because all operations on atomic\ntypes are suitably tagged by default. This means what you might expect: if thread A\nstores a value and thread B reads that value, there’s a synchronizes-with relationship\nbetween the store in thread A and the load in thread B, as in listing 5.2. This is illus-\ntrated in figure 5.2.\n As I’m sure you’ve guessed, the nuances are all in the “suitably-tagged” part. The\nC++ memory model allows various ordering constraints to be applied to the opera-\ntions on atomic types, and this is the tagging to which I refer. The various options for\nmemory ordering and how they relate to the synchronizes-with relationship are cov-\nered in section 5.3.3. First, let’s step back and look at the happens-before relationship.\ndata.push_back(42)\ndata_ready=true\ndata_ready.load()\n(returns\n)\nfalse\ndata_ready.load()\n(returns\n)\nfalse\ndata_ready.load()\n(returns\n)\ntrue\ndata[0]\n(returns 42)\nWriter thread\nReader thread\nFigure 5.2\nEnforcing an ordering between non-atomic operations using atomic operations\n",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "145\nSynchronizing operations and enforcing ordering\n5.3.2\nThe happens-before relationship\nThe happens-before and strongly-happens-before relationships are the basic building blocks\nof operation ordering in a program; it specifies which operations see the effects of\nwhich other operations. For a single thread, it’s largely straightforward: if one opera-\ntion is sequenced before another, then it also happens before it, and strongly-happens-\nbefore it. This means that if one operation (A) occurs in a statement prior to another\n(B) in the source code, then A happens before B, and A strongly-happens-before B.\nYou saw that in listing 5.2: the write to data d happens before the write to\ndata_ready e. If the operations occur in the same statement, in general there’s no\nhappens-before relationship between them, because they’re unordered. This is\nanother way of saying that the ordering is unspecified. You know that the program in\nthe following listing will output “1,2” or “2,1”, but it’s unspecified which, because\nthe order of the two calls to get_num()is unspecified.\n#include <iostream>\nvoid foo(int a,int b)\n{\n    std::cout<<a<<”,”<<b<<std::endl;\n}\nint get_num()\n{\n    static int i=0;\n    return ++i;\n}\nint main()\n{\n    foo(get_num(),get_num());   \n}\nThere are circumstances where operations within a single statement are sequenced,\nsuch as where the built-in comma operator is used or where the result of one expres-\nsion is used as an argument to another expression. But in general, operations within a\nsingle statement are nonsequenced, and there’s no sequenced-before (and thus no\nhappens-before) relationship between them. All operations in a statement happen\nbefore all of the operations in the next statement.\n This is a restatement of the single-threaded sequencing rules you’re used to, so\nwhat’s new? The new part is the interaction between threads: if operation A on one\nthread inter-thread happens before operation B on another thread, then A happens\nbefore B. This doesn’t help much: you’ve added a new relationship (inter-thread\nhappens-before), but this is an important relationship when you’re writing multi-\nthreaded code.\n At the basic level, inter-thread happens-before is relatively simple and relies on the\nsynchronizes-with relationship introduced in section 5.3.1: if operation A in one\nthread synchronizes with operation B in another thread, then A inter-thread happens\nListing 5.3\nOrder of evaluation of arguments to a function call is unspecified\nCalls to get_num() \nare unordered.\n",
      "content_length": 2517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "146\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nbefore B. It’s also a transitive relation: if A inter-thread happens before B and B inter-\nthread happens before C, then A inter-thread happens before C. You saw this in list-\ning 5.2 as well.\n Inter-thread happens-before also combines with the sequenced-before relation: if\noperation A is sequenced before operation B, and operation B inter-thread happens\nbefore operation C, then A inter-thread happens before C. Similarly, if A synchronizes\nwith B and B is sequenced before C, then A inter-thread happens before C. These two\ntogether mean that if you make a series of changes to data in a single thread, you need\nonly one synchronizes-with relationship for the data to be visible to subsequent opera-\ntions on the thread that executed C.\n The strongly-happens-before relationship is slightly different, but in most cases\ncomes down the same. The same two rules described above apply: if operation A\nsynchronizes-with operation B, or operation A is sequenced-before operation B, then\nA strongly-happens-before B. Transitive ordering also applies: if A strongly-happens-\nbefore B, and B strongly-happens-before C, then A strongly-happens-before C. The\ndifference is that operations tagged with memory_order_consume (see section 5.3.3)\nparticipate in inter-thread-happens-before relationships (and thus happens-before\nrelationships), but not in strongly-happens-before relationships. Since the vast major-\nity of code should not be using memory_order_consume, this distinction is unlikely to\naffect you in practice. I will use “happens-before” in the rest of this book for brevity.\n These are the crucial rules that enforce the ordering of operations between\nthreads and make everything in listing 5.2 work. There are some additional nuances\nwith data dependency, as you’ll see shortly. In order for you to understand this, I need\nto cover the memory-ordering tags used for atomic operations and how they relate to\nthe synchronizes-with relation.\n5.3.3\nMemory ordering for atomic operations\nThere are six memory ordering options that can be applied to operations on atomic\ntypes: memory_order_relaxed, memory_order_consume, memory_order_acquire, memory\n_order_release, memory_order_acq_rel, and memory_order_seq_cst. Unless you\nspecify otherwise for a particular operation, the memory-ordering option for all oper-\nations on atomic types is memory_order_seq_cst, which is the most stringent of the\navailable options. Although there are six ordering options, they represent three mod-\nels: sequentially consistent ordering (memory_order_seq_cst), acquire-release ordering\n(memory_order_consume, memory_order_acquire, memory_order_release, and memo-\nry_order_acq_rel), and relaxed ordering (memory_order_relaxed).\n These distinct memory-ordering models can have varying costs on different CPU\narchitectures. For example, on systems based on architectures with fine control over\nthe visibility of operations by processors other than the one that made the change,\nadditional synchronization instructions can be required for sequentially consistent\nordering over acquire-release ordering or relaxed ordering and for acquire-release\nordering over relaxed ordering. If these systems have many processors, these additional\n",
      "content_length": 3281,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "147\nSynchronizing operations and enforcing ordering\nsynchronization instructions may take a significant amount of time, reducing the overall\nperformance of the system. On the other hand, CPUs that use the x86 or x8664 archi-\ntectures (such as the Intel and AMD processors common in desktop PCs) don’t require\nany additional instructions for acquire-release ordering beyond those necessary for\nensuring atomicity, and even sequentially-consistent ordering doesn’t require any spe-\ncial treatment for load operations, although there’s a small additional cost on stores.\n The availability of the distinct memory-ordering models allows experts to take\nadvantage of the increased performance of the more fine-grained ordering relation-\nships where they’re advantageous while allowing the use of the default sequentially-\nconsistent ordering (which is considerably easier to reason about than the others) for\nthose cases that are less critical.\n In order to choose which ordering model to use, or to understand the ordering\nrelationships in code that uses the different models, it’s important to know how the\nchoices affect the program behavior. Let’s therefore look at the consequences of each\nchoice for operation ordering and synchronizes-with.\nSEQUENTIALLY CONSISTENT ORDERING\nThe default ordering is named sequentially consistent because it implies that the behav-\nior of the program is consistent with a simple sequential view of the world. If all oper-\nations on instances of atomic types are sequentially consistent, the behavior of a\nmultithreaded program is as if all these operations were performed in some particular\nsequence by a single thread. This is by far the easiest memory ordering to understand,\nwhich is why it’s the default: all threads must see the same order of operations. This\nmakes it easy to reason about the behavior of code written with atomic variables. You\ncan write down all the possible sequences of operations by different threads, eliminate\nthose that are inconsistent, and verify that your code behaves as expected in the oth-\ners. It also means that operations can’t be reordered; if your code has one operation\nbefore another in one thread, that ordering must be seen by all other threads.\n From the point of view of synchronization, a sequentially consistent store synchro-\nnizes with a sequentially consistent load of the same variable that reads the value\nstored. This provides one ordering constraint on the operation of two (or more)\nthreads, but sequential consistency is more powerful than that. Any sequentially con-\nsistent atomic operations done after that load must also appear after the store to other\nthreads in the system using sequentially consistent atomic operations. The example in\nlisting 5.4 demonstrates this ordering constraint in action. This constraint doesn’t\ncarry forward to threads that use atomic operations with relaxed memory orderings;\nthey can still see the operations in a different order, so you must use sequentially con-\nsistent operations on all your threads in order to get the benefit.\n This ease of understanding can come at a price, though. On a weakly-ordered\nmachine with many processors, it can impose a noticeable performance penalty,\nbecause the overall sequence of operations must be kept consistent between the proces-\nsors, possibly requiring extensive (and expensive!) synchronization operations between\nthe processors. That said, some processor architectures (such as the common x86 and\n",
      "content_length": 3472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "148\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nx86-64 architectures) offer sequential consistency relatively cheaply, so if you’re con-\ncerned about the performance implications of using sequentially consistent ordering,\ncheck the documentation for your target processor architectures.\n The following listing shows sequential consistency in action. The loads and stores\nto x and y are explicitly tagged with memory_order_seq_cst, although this tag could\nbe omitted in this case because it’s the default. \n#include <atomic>\n#include <thread>\n#include <assert.h>\nstd::atomic<bool> x,y;\nstd::atomic<int> z;\nvoid write_x()\n{\n    x.store(true,std::memory_order_seq_cst);  \n}\nvoid write_y()\n{\n    y.store(true,std::memory_order_seq_cst);  \n}\nvoid read_x_then_y()\n{\n    while(!x.load(std::memory_order_seq_cst));\n    if(y.load(std::memory_order_seq_cst))     \n        ++z;\n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_seq_cst));\n    if(x.load(std::memory_order_seq_cst))     \n        ++z;\n}\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x);\n    std::thread b(write_y);\n    std::thread c(read_x_then_y);\n    std::thread d(read_y_then_x);\n    a.join();\n    b.join();\n    c.join();\n    d.join();\n    assert(z.load()!=0);   \n}\nThe assert f can never fire, because either the store to x B or the store to y c must\nhappen first, even though it’s not specified which. If the load of y in read_x_then_y d\nListing 5.4\nSequential consistency implies a total ordering\nb\nc\nd\ne\nf\n",
      "content_length": 1521,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "149\nSynchronizing operations and enforcing ordering\nreturns false, the store to x must occur before the store to y, in which case the load of\nx in read_y_then_x e must return true, because the while loop ensures that the y is\ntrue at this point. Because the semantics of memory_order_seq_cst require a single\ntotal ordering over all operations tagged memory_order_seq_cst, there’s an implied\nordering relationship between a load of y that returns false d and the store to y B.\nFor there to be a single total order, if one thread sees x==true and then subse-\nquently sees y==false, this implies that the store to x occurs before the store to y in\nthis total order.\n Because everything is symmetrical, it could also happen the other way around, with\nthe load of x e returning false, forcing the load of y d to return true. In both\ncases, z is equal to 1. Both loads can return true, leading to z being 2, but under no\ncircumstances can z be 0.\n The operations and happens-before relationships for the case that read_x_then_y\nsees x as true and y as false are shown in figure 5.3. The dashed line from the load of\ny in read_x_then_y to the store to y in write_y shows the implied ordering relation-\nship required in order to maintain sequential consistency: the load must occur before\nthe store in the global order of memory_order_seq_cst operations in order to achieve\nthe outcomes given here.\nSequential consistency is the most straightforward and intuitive ordering, but it’s also\nthe most expensive memory ordering because it requires global synchronization\nbetween all threads. On a multiprocessor system this may require extensive and time-\nconsuming communication between processors.\n In order to avoid this synchronization cost, you need to step outside the world of\nsequential consistency and consider using other memory orderings.\nNON-SEQUENTIALLY CONSISTENT MEMORY ORDERINGS\nOnce you step outside the nice sequentially-consistent world, things start to get com-\nplicated. The single biggest issue to get to grips with is probably the fact that there’s no\ny.store(true)\nInitially\n,\nx=false y=false\nx.store(true)\nx.load()\nreturns true\ny.load()\nreturns false\ny.load()\nreturns true\nx.load()\nreturns true\nwrite_x\nread_x_then_y\nread_y_then_x\nwrite_y\nFigure 5.3\nSequential consistency and happens-before\n",
      "content_length": 2305,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "150\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nlonger a single global order of events. This means that different threads can see different\nviews of the same operations, and any mental model you have of operations from dif-\nferent threads neatly interleaved one after the other must be thrown away. Not only\ndo you have to account for things happening truly concurrently, but threads don’t have\nto agree on the order of events. In order to write (or even to understand) any code that\nuses a memory ordering other than the default memory_order_seq_cst, it’s absolutely\nvital to get your head around this. It’s not just that the compiler can reorder the\ninstructions. Even if the threads are running the same bit of code, they can disagree\non the order of events because of operations in other threads in the absence of\nexplicit ordering constraints, because the different CPU caches and internal buffers\ncan hold different values for the same memory. It’s so important I’ll say it again:\nthreads don’t have to agree on the order of events.\n Not only do you have to throw out mental models based on interleaving opera-\ntions, you also have to throw out mental models based on the idea of the compiler or\nprocessor reordering the instructions. In the absence of other ordering constraints, the only\nrequirement is that all threads agree on the modification order of each individual variable. Oper-\nations on distinct variables can appear in different orders on different threads, pro-\nvided the values seen are consistent with any additional ordering constraints imposed.\n This is best demonstrated by stepping completely outside the sequentially consis-\ntent world and using memory_order_relaxed for all operations. Once you’ve come to\ngrips with that, you can move back to acquire-release ordering, which allows you to\nselectively introduce ordering relationships between operations and claw back some\nof your sanity.\nRELAXED ORDERING\nOperations on atomic types performed with relaxed ordering don’t participate in\nsynchronizes-with relationships. Operations on the same variable within a single thread\nstill obey happens-before relationships, but there’s almost no requirement on order-\ning relative to other threads. The only requirement is that accesses to a single atomic\nvariable from the same thread can’t be reordered; once a given thread has seen a par-\nticular value of an atomic variable, a subsequent read by that thread can’t retrieve\nan earlier value of the variable. Without any additional synchronization, the modifi-\ncation order of each variable is the only thing shared between threads that are using\nmemory_order_relaxed. \n To demonstrate how relaxed your relaxed operations can be, you need only two\nthreads, as shown in the following listing.\n#include <atomic>\n#include <thread>\n#include <assert.h>\nstd::atomic<bool> x,y;\nstd::atomic<int> z;\nvoid write_x_then_y()\nListing 5.5\nRelaxed operations have few ordering requirements\n",
      "content_length": 2964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "151\nSynchronizing operations and enforcing ordering\n{\n    x.store(true,std::memory_order_relaxed);   \n    y.store(true,std::memory_order_relaxed);  \n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_relaxed));   \n    if(x.load(std::memory_order_relaxed))     \n        ++z;\n}\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x_then_y);\n    std::thread b(read_y_then_x);\n    a.join();\n    b.join();\n    assert(z.load()!=0);    \n}\nThis time the assert f can fire, because the load of x e can read false, even though\nthe load of y d reads true and the store of x B happens before the store of y c. x\nand y are different variables, so there are no ordering guarantees relating to the visi-\nbility of values arising from operations on each.\n Relaxed operations on different variables can be freely reordered provided they\nobey any happens-before relationships they’re bound by (for example, within the\nsame thread). They don’t introduce synchronizes-with relationships. The happens-\nbefore relationships from listing 5.5 are shown in figure 5.4, along with a possible out-\ncome. Even though there’s a happens-before relationship between the stores and\nb\nc\nd\ne\nf\nInitially\n,\nx=false y=false\nx.store(true,\nrelaxed)\ny.load(relaxed)\nreturns true\nx.load(relaxed)\nreturns false\nwrite_x_then_y\nread_y_then_x\ny.store(true,\nrelaxed)\nFigure 5.4\nRelaxed atomics \nand happens-before\n",
      "content_length": 1399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "152\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nbetween the loads, there isn’t one between either store and either load, and so the\nloads can see the stores out of order.\n Let’s look at the slightly more complex example with three variables and five\nthreads in the next listing.\n#include <thread>\n#include <atomic>\n#include <iostream>\nstd::atomic<int> x(0),y(0),z(0);   \nstd::atomic<bool> go(false);    \nunsigned const loop_count=10;\nstruct read_values\n{\n    int x,y,z;\n};\nread_values values1[loop_count];\nread_values values2[loop_count];\nread_values values3[loop_count];\nread_values values4[loop_count];\nread_values values5[loop_count];\nvoid increment(std::atomic<int>* var_to_inc,read_values* values)\n{\n    while(!go)                    \n        std::this_thread::yield();\n    for(unsigned i=0;i<loop_count;++i)\n    {\n        values[i].x=x.load(std::memory_order_relaxed);\n        values[i].y=y.load(std::memory_order_relaxed);\n        values[i].z=z.load(std::memory_order_relaxed);\n        var_to_inc->store(i+1,std::memory_order_relaxed);  \n        std::this_thread::yield();\n    }\n}\nvoid read_vals(read_values* values)\n{\n    while(!go)                      \n        std::this_thread::yield();\n    for(unsigned i=0;i<loop_count;++i)\n    {\n        values[i].x=x.load(std::memory_order_relaxed);\n        values[i].y=y.load(std::memory_order_relaxed);\n        values[i].z=z.load(std::memory_order_relaxed);\n        std::this_thread::yield();\n    }\n}\nvoid print(read_values* v)\n{\n    for(unsigned i=0;i<loop_count;++i)\n    {\n        if(i)\n            std::cout<<\",\";\nListing 5.6\nRelaxed operations on multiple threads\nB\nc\nSpin, waiting \nfor the signal\nd\ne\nSpin, waiting \nfor the signal\nf\n",
      "content_length": 1706,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "153\nSynchronizing operations and enforcing ordering\n        std::cout<<\"(\"<<v[i].x<<\",\"<<v[i].y<<\",\"<<v[i].z<<\")\";\n    }\n    std::cout<<std::endl;\n}\nint main()\n{\n    std::thread t1(increment,&x,values1);\n    std::thread t2(increment,&y,values2);\n    std::thread t3(increment,&z,values3);\n    std::thread t4(read_vals,values4);\n    std::thread t5(read_vals,values5);\n    go=true;          \n    t5.join();\n    t4.join();\n    t3.join();\n    t2.join();\n    t1.join();\n    print(values1);    \n    print(values2);\n    print(values3);\n    print(values4);\n    print(values5);\n} \nThis is a simple program. You have three shared global atomic variables B and five\nthreads. Each thread loops 10 times, reading the values of the three atomic variables\nusing memory_order_relaxed and storing them in an array. Three of the threads each\nupdate one of the atomic variables each time through the loop e, whereas the other\ntwo threads read. Once all the threads have been joined, you print the values from the\narrays stored by each thread h.\n The go atomic variable c is used to ensure that the threads all start the loop as\nnear to the same time as possible. Launching a thread is an expensive operation, and\nwithout the explicit delay, the first thread may be finished before the last one has\nstarted. Each thread waits for go to become true before entering the main loop d\nand f, and go is set to true only once all the threads have started g.\n One possible output from this program is as follows:\n(0,0,0),(1,0,0),(2,0,0),(3,0,0),(4,0,0),(5,7,0),(6,7,8),(7,9,8),(8,9,8),\n(9,9,10)\n(0,0,0),(0,1,0),(0,2,0),(1,3,5),(8,4,5),(8,5,5),(8,6,6),(8,7,9),(10,8,9),\n(10,9,10)\n(0,0,0),(0,0,1),(0,0,2),(0,0,3),(0,0,4),(0,0,5),(0,0,6),(0,0,7),(0,0,8),\n(0,0,9)\n(1,3,0),(2,3,0),(2,4,1),(3,6,4),(3,9,5),(5,10,6),(5,10,8),(5,10,10),\n(9,10,10),(10,10,10)\n(0,0,0),(0,0,0),(0,0,0),(6,3,7),(6,5,7),(7,7,7),(7,8,7),(8,8,7),(8,8,9),\n(8,8,9)\nThe first three lines are the threads doing the updating, and the last two are the\nthreads doing the reading. Each triplet is a set of the variables x, y, and z, in that\nSignal to start \nexecution of main loop\ng\nPrints the \nfinal values\nh\n",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "154\nCHAPTER 5\nThe C++ memory model and operations on atomic types\norder, from one pass through the loop. There are a few things to notice from\nthis output:\nThe first set of values shows x increasing by one with each triplet, the second set\nhas y increasing by one, and the third has z increasing by one.\nThe x elements of each triplet only increase within a given set, as do the y and z\nelements, but the increments are uneven, and the relative orderings vary\nbetween all threads.\nThread 3 doesn’t see any of the updates to x or y; it sees only the updates it\nmakes to z. This doesn’t stop the other threads from seeing the updates to z\nmixed in with the updates to x and y, though.\nThis is a valid outcome for relaxed operations, but it’s not the only valid outcome. Any\nset of values that’s consistent with the three variables, each holding the values 0 to 10\nin turn, and that has the thread incrementing a given variable printing the values 0 to\n9 for that variable, is valid.\nUNDERSTANDING RELAXED ORDERING\nTo understand how this works, imagine that each variable is a man in a cubicle with a\nnotepad. On his notepad is a list of values. You can phone him and ask him to give you\na value, or you can tell him to write down a new value. If you tell him to write down a\nnew value, he writes it at the bottom of the list. If you ask him for a value, he reads you\na number from the list. \n The first time you talk to this man, if you ask him for a value, he may give you any\nvalue from the list he has on his pad at the time. If you then ask him for another value,\nhe may give you the same one again or a value from farther down the list. He’ll never\ngive you a value from farther up the list. If you tell him to write down a number and\nthen subsequently ask him for a value, he’ll give you either the number you told him\nto write down or a number below that on the list.\n Imagine for a moment that his list starts with the values 5, 10, 23, 3, 1, and 2. If you\nask for a value, you could get any of those. If he gives you 10, then the next time you ask\nhe could give you 10 again, or any of the later ones, but not 5. If you call him five\ntimes, he could say “10, 10, 1, 2, 2,” for example. If you tell him to write down 42, he’ll\nadd it to the end of the list. If you ask him for a number again, he’ll keep telling you\n“42” until he has another number on his list and he feels like telling it to you.\n Now, imagine your friend Carl also has this man’s number. Carl can also phone\nhim and either ask him to write down a number or ask for one, and he applies the\nsame rules to Carl as he does to you. He has only one phone, so he can only deal with\none of you at a time, so the list on his pad is a nice straightforward list. But just\nbecause you got him to write down a new number doesn’t mean he has to tell it to\nCarl, and vice versa. If Carl asked him for a number and was told “23,” then just\nbecause you asked the man to write down 42 doesn’t mean he’ll tell that to Carl next\ntime. He may tell Carl any of the numbers 23, 3, 1, 2, 42, or even the 67 that Fred told\nhim to write down after you called. He could very well tell Carl “23, 3, 3, 1, 67” without\n",
      "content_length": 3167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "155\nSynchronizing operations and enforcing ordering\nbeing inconsistent with what he told you. It’s like he keeps track of which number he\ntold to whom with a little moveable sticky note for each person, like in figure 5.5.\nNow imagine that there’s not just one man in a cubicle but a whole cubicle farm, with\nloads of men with phones and notepads. These are all our atomic variables. Each vari-\nable has its own modification order (the list of values on the pad), but there’s no rela-\ntionship between them at all. If each caller (you, Carl, Anne, Dave, and Fred) is a\nthread, then this is what you get when every operation uses memory_order_relaxed.\nThere are a few additional things you can tell the man in the cubicle, such as “Write\ndown this number, and tell me what was at the bottom of the list” (exchange) and\n“Write down this number if the number on the bottom of the list is that; otherwise tell\nme what I should have guessed” (compare_exchange_strong), but that doesn’t affect\nthe general principle.\n If you think about the program logic from listing 5.5, then write_x_then_y is\nlike some guy calling up the man in cubicle x and telling him to write true, then\ncalling up the man in cubicle y and telling him to write true. The thread running\nread_y_then_x repeatedly calls up the man in cubicle y asking for a value until he says\ntrue and then calls the man in cubicle x to ask for a value. The man in cubicle x is\nunder no obligation to tell you any specific value off his list and is quite within his\nrights to say false.\n This makes relaxed atomic operations difficult to deal with. They must be used in\ncombination with atomic operations that feature stronger ordering semantics in order\nto be useful for inter-thread synchronization. I strongly recommend avoiding relaxed\natomic operations unless they’re absolutely necessary, and even then using them only\nwith extreme caution. Given the unintuitive results that can be achieved with only two\nthreads and two variables in listing 5.5, it’s not hard to imagine the possible complex-\nity when more threads and more variables are involved.\n One way to achieve additional synchronization without the overhead of full-blown\nsequential consistency is to use acquire-release ordering.\nACQUIRE-RELEASE ORDERING\nAcquire-release ordering is a step up from relaxed ordering; there’s still no total order\nof operations, but it does introduce some synchronization. Under this ordering model,\natomic loads are acquire operations (memory_order_acquire), atomic stores are release\noperations (memory_order_release), and atomic read-modify-write operations (such\nFigure 5.5\nThe notebook for \nthe man in the cubicle\n",
      "content_length": 2666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "156\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nas fetch_add() or exchange()) are either acquire, release, or both (memory_order_\nacq_rel). Synchronization is pairwise between the thread that does the release and\nthe thread that does the acquire. A release operation synchronizes-with an acquire operation\nthat reads the value written. This means that different threads can still see different\norderings, but these orderings are restricted. The following listing is a reworking of\nlisting 5.4 using acquire-release semantics rather than sequentially-consistent ones.\n#include <atomic>\n#include <thread>\n#include <assert.h>\nstd::atomic<bool> x,y;\nstd::atomic<int> z;\nvoid write_x()\n{\n    x.store(true,std::memory_order_release);\n}\nvoid write_y()\n{\n    y.store(true,std::memory_order_release);\n}\nvoid read_x_then_y()\n{\n    while(!x.load(std::memory_order_acquire));\n    if(y.load(std::memory_order_acquire))     \n        ++z;\n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_acquire));\n    if(x.load(std::memory_order_acquire))     \n        ++z;\n}\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x);\n    std::thread b(write_y);\n    std::thread c(read_x_then_y);\n    std::thread d(read_y_then_x);\n    a.join();\n    b.join();\n    c.join();\n    d.join();\n    assert(z.load()!=0);   \n}\nIn this case the assert d can fire (like in the relaxed-ordering case), because it’s possi-\nble for both the load of x c and the load of y B to read false. x and y are written by\nListing 5.7\nAcquire-release doesn’t imply a total ordering\nb\nc\nd\n",
      "content_length": 1578,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "157\nSynchronizing operations and enforcing ordering\ndifferent threads, so the ordering from the release to the acquire in each case has no\neffect on the operations in the other threads.\n Figure 5.6 shows the happens-before relationships from listing 5.7, along with a\npossible outcome where the two reading threads each have a different view of the\nworld. This is possible because there’s no happens-before relationship to force an\nordering, as described previously.\nIn order to see the benefit of acquire-release ordering, you need to consider two\nstores from the same thread, like in listing 5.5. If you change the store to y to use mem-\nory_order_release and the load from y to use memory_order_acquire like in the fol-\nlowing listing, then you impose an ordering on the operations on x.\n#include <atomic>\n#include <thread>\n#include <assert.h>\nstd::atomic<bool> x,y;\nstd::atomic<int> z;\nvoid write_x_then_y()\n{\n    x.store(true,std::memory_order_relaxed);   \n    y.store(true,std::memory_order_release);   \n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_acquire));   \n    if(x.load(std::memory_order_relaxed))     \n        ++z;\n}\nListing 5.8\nAcquire-release operations can impose ordering on relaxed operations\ny.store(true,\nrelease)\nInitially\n,\nx=false y=false\nx.store(true,\nrelease)\nx.load(acquire)\nreturns true\ny.load(acquire)\nreturns false\ny.load(acquire)\nreturns true\nx.load(acquire)\nreturns false\nwrite_x\nread_x_then_y\nread_y_then_x\nwrite_y\nFigure 5.6\nAcquire-release and happens-before\nB\nc\nSpin, waiting for y \nto be set to true\nd\ne\n",
      "content_length": 1557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "158\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x_then_y);\n    std::thread b(read_y_then_x);\n    a.join();\n    b.join();\n    assert(z.load()!=0);    \n}\nEventually, the load from y, d will see true as written by the store c. Because the\nstore uses memory_order_release and the load uses memory_order_acquire, the store\nsynchronizes with the load. The store to x B happens before the store to y c because\nthey’re in the same thread. Because the store to y synchronizes with the load from y,\nthe store to x also happens before the load from y and by extension happens before\nthe load from x e. Thus, the load from x must read true, and the assert f can’t fire.\nIf the load from y wasn’t in a while loop, this wouldn’t necessarily be the case; the\nload from y might read false, in which case there’d be no requirement on the value\nread from x. In order to provide any synchronization, acquire and release operations\nmust be paired up. The value stored by a release operation must be seen by an acquire\noperation for either to have any effect. If either the store at c or the load at d was a\nrelaxed operation, there’d be no ordering on the accesses to x, so there’d be no guar-\nantee that the load at e would read true, and the assert could fire.\n You can still think about acquire-release ordering in terms of our men with note-\npads in their cubicles, but you have to add more to the model. First, imagine that\nevery store that’s done is part of some batch of updates, so when you call a man to tell\nhim to write down a number, you also tell him which batch this update is part of:\n“Please write down 99, as part of batch 423.” For the last store in a batch, you tell this\nto the man too: “Please write down 147, which is the last store in batch 423.” The\nman in the cubicle will then duly write down this information, along with who gave\nhim the value. This models a store-release operation. The next time you tell some-\none to write down a value, you increase the batch number: “Please write down 41, as\npart of batch 424.”\n When you ask for a value, you now have a choice: you can either ask for a value\n(which is a relaxed load), in which case the man only gives you the number, or you\ncan ask for a value and information about whether it’s the last in a batch (which mod-\nels a load-acquire). If you ask for the batch information, and the value wasn’t the last\nin a batch, the man will tell you something like, “The number is 987, which is a ‘normal’\nvalue,” whereas if it was the last in a batch, he’ll tell you something like “The number is\n987, which is the last number in batch 956 from Anne.” Now, here’s where the acquire-\nrelease semantics kick in: if you tell the man all the batches you know about when you\nask for a value, he’ll look down his list for the last value from any of the batches you\nknow about and either give you that number or one further down the list.\nf\n",
      "content_length": 2973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "159\nSynchronizing operations and enforcing ordering\n How does this model acquire-release semantics? Let’s look at our example and see.\nFirst off, thread a is running write_x_then_y and says to the man in cubicle x,\n“Please write true as part of batch 1 from thread a,” which he duly writes down.\nThread a then says to the man in cubicle y, “Please write true as the last write of\nbatch 1 from thread a,” which he duly writes down. In the meantime, thread b is\nrunning read_y_then_x. Thread b keeps asking the man in box y for a value with\nbatch information until he says “true.” It may have to ask many times, but eventually\nthe man will say “true.” The man in box y doesn’t only say “true” though; he also says,\n“This is the last write in batch 1 from thread a.” \n Now, thread b goes on to ask the man in box x for a value, but this time it says,\n“Please can I have a value, and by the way I know about batch 1 from thread a.” Now\nthe man from cubicle x has to look down his list for the last mention of batch 1 from\nthread a. The only mention he has is the value true, which is also the last value on his\nlist, so he must read out that value; otherwise, he’s breaking the rules of the game.\n If you look at the definition of inter-thread happens-before back in section 5.3.2,\none of the important properties is that it’s transitive: if A inter-thread happens before B\nand B inter-thread happens before C, then A inter-thread happens before C. This means that\nacquire-release ordering can be used to synchronize data across several threads, even\nwhen the “intermediate” threads haven’t touched the data.\nTRANSITIVE SYNCHRONIZATION WITH ACQUIRE-RELEASE ORDERING\nIn order to think about transitive ordering, you need at least three threads. The first\nthread modifies some shared variables and does a store-release to one of them. A sec-\nond thread then reads the variable subject to the store-release with a load-acquire and\nperforms a store-release on a second shared variable. Finally, a third thread does a\nload-acquire on that second shared variable. Provided that the load-acquire opera-\ntions see the values written by the store-release operations to ensure the synchronizes-\nwith relationships, this third thread can read the values of the other variables stored\nby the first thread, even if the intermediate thread didn’t touch any of them. This sce-\nnario is shown in the next listing.\nstd::atomic<int> data[5];\nstd::atomic<bool> sync1(false),sync2(false);\nvoid thread_1()\n{\n    data[0].store(42,std::memory_order_relaxed);\n    data[1].store(97,std::memory_order_relaxed);\n    data[2].store(17,std::memory_order_relaxed);\n    data[3].store(-141,std::memory_order_relaxed);\n    data[4].store(2003,std::memory_order_relaxed);\n    sync1.store(true,std::memory_order_release);    \n}\nvoid thread_2()\n{\n    while(!sync1.load(std::memory_order_acquire));  \nListing 5.9\nTransitive synchronization using acquire and release ordering\nSet sync1\nB\nLoop until \nsync1 is set\nc\n",
      "content_length": 2971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "160\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n    sync2.store(true,std::memory_order_release);  \n}\nvoid thread_3()\n{\n    while(!sync2.load(std::memory_order_acquire));      \n    assert(data[0].load(std::memory_order_relaxed)==42);\n    assert(data[1].load(std::memory_order_relaxed)==97);\n    assert(data[2].load(std::memory_order_relaxed)==17);\n    assert(data[3].load(std::memory_order_relaxed)==-141);\n    assert(data[4].load(std::memory_order_relaxed)==2003);\n}\nEven though thread_2 only touches the variables sync1 c and sync2 d, this is\nenough for synchronization between thread_1 and thread_3 to ensure that the asserts\ndon’t fire. First off, the stores to data from thread_1 happens before the store to sync1\nB because they’re sequenced before it in the same thread. Because the load from\nsync1 B is in a while loop, it will eventually see the value stored from thread_1 and\nform the second half of the release-acquire pair. Therefore, the store to sync1 happens\nbefore the final load from sync1 in the while loop. This load is sequenced before (and\nthus happens before) the store to sync2 d, which forms a release-acquire pair with the\nfinal load from the while loop in thread_3 e. The store to sync2 d thus happens\nbefore the load e, which happens before the loads from data. Because of the transitive\nnature of happens-before, you can chain it all together: the stores to data happen\nbefore the store to sync1 B, which happens before the load from sync1 c, which hap-\npens before the store to sync2 d, which happens before the load from sync2 e, which\nhappens before the loads from data. Thus the stores to data in thread_1 happen\nbefore the loads from data in thread_3, and the asserts can’t fire.\n In this case, you could combine sync1 and sync2 into a single variable by using a\nread-modify-write operation with memory_order_acq_rel in thread_2. One option\nwould be to use compare_exchange_strong() to ensure that the value is updated only\nonce the store from thread_1 has been seen:\nstd::atomic<int> sync(0);\nvoid thread_1()\n{\n    // ...\n    sync.store(1,std::memory_order_release);\n}\nvoid thread_2()\n{\n    int expected=1;\n    while(!sync.compare_exchange_strong(expected,2,\n                                        std::memory_order_acq_rel))\n        expected=1;\n}\nvoid thread_3()\n{\n    while(sync.load(std::memory_order_acquire)<2);\n    // ...\n}\nSet sync2\nd\nLoop until \nsync2 is set\ne\n",
      "content_length": 2420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "161\nSynchronizing operations and enforcing ordering\nIf you use read-modify-write operations, it’s important to pick which semantics you\ndesire. In this case, you want both acquire and release semantics, so memory_order\n_acq_rel is appropriate, but you can use other orderings too. A fetch_sub operation\nwith memory_order_acquire semantics doesn’t synchronize with anything, even though\nit stores a value, because it isn’t a release operation. Likewise, a store can’t synchronize\nwith a fetch_or with memory_order_release semantics, because the read part of the\nfetch_or isn’t an acquire operation. Read-modify-write operations with memory_order\n_acq_rel semantics behave as both an acquire and a release, so a prior store can syn-\nchronize with such an operation, and it can synchronize with a subsequent load, as is\nthe case in this example.\n If you mix acquire-release operations with sequentially consistent operations, the\nsequentially consistent loads behave like loads with acquire semantics, and sequen-\ntially consistent stores behave like stores with release semantics. Sequentially consis-\ntent read-modify-write operations behave as both acquire and release operations.\nRelaxed operations are still relaxed but are bound by the additional synchronizes-with\nand consequent happens-before relationships introduced through the use of acquire-\nrelease semantics.\n Despite the potentially non-intuitive outcomes, anyone who’s used locks has had to\ndeal with the same ordering issues: locking a mutex is an acquire operation, and\nunlocking the mutex is a release operation. With mutexes, you learn that you must\nensure that the same mutex is locked when you read a value as was locked when you\nwrote it, and the same applies here; your acquire and release operations have to be on\nthe same variable to ensure an ordering. If data is protected with a mutex, the exclu-\nsive nature of the lock means that the result is indistinguishable from what it would\nhave been had the lock and unlock been sequentially consistent operations. Similarly,\nif you use acquire and release orderings on atomic variables to build a simple lock,\nthen from the point of view of code that uses the lock, the behavior will appear\nsequentially consistent, even though the internal operations are not.\n If you don’t need the stringency of sequentially consistent ordering for your\natomic operations, the pairwise synchronization of acquire-release ordering has the\npotential for a much lower synchronization cost than the global ordering required for\nsequentially consistent operations. The trade-off here is the mental cost required to\nensure that the ordering works correctly and that the non-intuitive behavior across\nthreads isn’t problematic.\nDATA DEPENDENCY WITH ACQUIRE-RELEASE ORDERING AND MEMORY_ORDER_CONSUME\nIn the introduction to this section I said that memory_order_consume was part of the\nacquire-release ordering model, but it was conspicuously absent from the preceding\ndescription. This is because memory_order_consume is special: it’s all about data\ndependencies, and it introduces the data-dependency nuances to the inter-thread\nhappens-before relationship mentioned in section 5.3.2. It is also special in that the\nC++17 standard explicitly recommends that you do not use it. It is therefore only cov-\nered here for completeness: you should not use memory_order_consume in your code!\n",
      "content_length": 3384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "162\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n The concept of a data dependency is relatively straightforward: there is a data\ndependency between two operations if the second one operates on the result of the\nfirst. There are two new relations that deal with data dependencies: dependency-ordered-\nbefore and carries-a-dependency-to. Like sequenced-before, carries-a-dependency-to applies\nstrictly within a single thread and models the data dependency between operations; if\nthe result of an operation (A) is used as an operand for an operation (B), then A car-\nries a dependency to B. If the result of operation A is a value of a scalar type such as an\nint, then the relationship still applies if the result of A is stored in a variable, and that\nvariable is then used as an operand for operation B. This operation is also transitive,\nso if A carries a dependency to B, and B carries a dependency to C, then A carries a\ndependency to C. \n On the other hand, the dependency-ordered-before relationship can apply\nbetween threads. It’s introduced by using atomic load operations tagged with memory\n_order_consume. This is a special case of memory_order_acquire that limits the syn-\nchronized data to direct dependencies; a store operation (A) tagged with memory_\norder_release, memory_order_acq_rel, or memory_order_seq_cst is dependency-\nordered-before a load operation (B) tagged with memory_order_consume if the consume\nreads the value stored. This is as opposed to the synchronizes-with relationship you get\nif the load uses memory_order_acquire. If this operation (B) then carries a depen-\ndency to some operation (C), then A is also dependency-ordered-before C.\n This wouldn’t do you any good for synchronization purposes if it didn’t affect the\ninter-thread happens-before relation, but it does: if A is dependency-ordered-before\nB, then A also inter-thread happens-before B.\n One important use for this kind of memory ordering is where the atomic opera-\ntion loads a pointer to some data. By using memory_order_consume on the load and\nmemory_order_release on the prior store, you ensure that the pointed-to data is cor-\nrectly synchronized, without imposing any synchronization requirements on any other\nnondependent data. The following listing shows an example of this scenario.\nstruct X\n{\n    int i;\n    std::string s;\n};\nstd::atomic<X*> p;\nstd::atomic<int> a;\nvoid create_x()\n{\n    X* x=new X;\n    x->i=42;\n    x->s=”hello”;\n    a.store(99,std::memory_order_relaxed);   \n    p.store(x,std::memory_order_release);    \n}\nListing 5.10\nUsing std::memory_order_consume to synchronize data\nB\nc\n",
      "content_length": 2617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "163\nSynchronizing operations and enforcing ordering\nvoid use_x()\n{\n    X* x;\n    while(!(x=p.load(std::memory_order_consume)))       \n        std::this_thread::sleep(std::chrono::microseconds(1));\n    assert(x->i==42);                                  \n    assert(x->s==”hello”);                         \n    assert(a.load(std::memory_order_relaxed)==99);   \n}\nint main()\n{\n    std::thread t1(create_x);\n    std::thread t2(use_x);\n    t1.join();\n    t2.join();\n}\nEven though the store to a B is sequenced before the store to p c, and the store to p\nis tagged memory_order_release, the load of p d is tagged memory_order_consume.\nThis means that the store to p only happens before those expressions that are depen-\ndent on the value loaded from p. This means that the asserts on the data members\nof the X structure (e and f) are guaranteed not to fire, because the load of p car-\nries a dependency to those expressions through the variable x. On the other hand,\nthe assert on the value of a g may or may not fire; this operation isn’t dependent\non the value loaded from p, and so there’s no guarantee on the value that’s read.\nThis is particularly apparent because it’s tagged with memory_order_relaxed, as\nyou’ll see.\n Sometimes, you don’t want the overhead of carrying the dependency around. You\nwant the compiler to be able to cache values in registers and reorder operations to\noptimize the code rather than fussing about the dependencies. In these scenarios, you\ncan use std::kill_dependency() to explicitly break the dependency chain. std::\nkill_dependency() is a simple function template that copies the supplied argument to\nthe return value but breaks the dependency chain in doing so. For example, if you have\na global read-only array, and you use std::memory_order_consume when retrieving an\nindex into that array from another thread, you can use std::kill_dependency() to let\nthe compiler know that it doesn’t need to reread the contents of the array entry, as in\nthe following example:\nint global_data[]={ … };\nstd::atomic<int> index;\nvoid f()\n{\n    int i=index.load(std::memory_order_consume);\n    do_something_with(global_data[std::kill_dependency(i)]);\n}\nIn real code, you should always use memory_order_acquire where you might be\ntempted to use memory_order_consume, and std::kill_dependency is unnecessary.\nd\ne\nf\ng\n",
      "content_length": 2331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "164\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n Now that I’ve covered the basics of the memory orderings, it’s time to look at the\nmore complex parts of the synchronizes-with relation, which manifest in the form of\nrelease sequences.\n5.3.4\nRelease sequences and synchronizes-with\nBack in section 5.3.1, I mentioned that you could get a synchronizes-with relationship\nbetween a store to an atomic variable and a load of that atomic variable from another\nthread, even when there’s a sequence of read-modify-write operations between the\nstore and the load, provided all the operations are suitably tagged. Now that I’ve cov-\nered the possible memory-ordering “tags,” I can elaborate on this. If the store is tagged\nwith memory_order_release, memory_order_acq_rel, or memory_order_seq_cst, and\nthe load is tagged with memory_order_consume, memory_order_acquire, or memory\n_order_seq_cst, and each operation in the chain loads the value written by the pre-\nvious operation, then the chain of operations constitutes a release sequence and the ini-\ntial store synchronizes with (for memory_order_acquire or memory_order_seq_cst)\nor is dependency-ordered-before (for memory_order_consume) the final load. Any\natomic read-modify-write operations in the chain can have any memory ordering\n(even memory_order_relaxed).\n To see what this means and why it’s important, consider atomic<int> being used\nas a count of the number of items in a shared queue, as in the following listing.\n#include <atomic>\n#include <thread>\nstd::vector<int> queue_data;\nstd::atomic<int> count;\nvoid populate_queue()\n{\n    unsigned const number_of_items=20;\n    queue_data.clear();\n    for(unsigned i=0;i<number_of_items;++i)\n    {\n        queue_data.push_back(i);\n    }\n    \n    count.store(number_of_items,std::memory_order_release);   \n}\nvoid consume_queue_items()\n{\n    while(true)\n    {\n        int item_index;                               \n        if((item_index=count.fetch_sub(1,std::memory_order_acquire))<=0)  \n        {\n            wait_for_more_items();  \n            continue;\n        }\nListing 5.11\nReading values from a queue with atomic operations\nThe initial \nstore\nb\nAn RMW\noperation c\nWait for \nmore items.\nd\n",
      "content_length": 2212,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "165\nSynchronizing operations and enforcing ordering\n        process(queue_data[item_index-1]);    \n    }\n}\nint main()\n{\n    std::thread a(populate_queue);\n    std::thread b(consume_queue_items);\n    std::thread c(consume_queue_items);\n    a.join();\n    b.join();\n    c.join();\n}\nOne way to handle things would be to have the thread that’s producing the data store\nthe items in a shared buffer and then do count.store(number_of_items, memory_\norder_release) B to let the other threads know that data is available. The threads\nconsuming the queue items might then do count.fetch_sub(1,memory_order_\nacquire) c to claim an item from the queue, prior to reading the shared buffer e.\nOnce the count becomes zero, there are no more items, and the thread must wait d.\n If there’s one consumer thread, this is fine; fetch_sub() is a read with memory\n_order_acquire semantics, and the store had memory_order_release semantics, so\nthe store synchronizes with the load and the thread can read the item from the buffer.\nIf there are two threads reading, the second fetch_sub() will see the value written by\nthe first and not the value written by the store. Without the rule about the release\nsequence, this second thread wouldn’t have a happens-before relationship with the\nfirst thread, and it wouldn’t be safe to read the shared buffer unless the first fetch_\nsub() also had memory_order_release semantics, which would introduce unnecessary\nsynchronization between the two consumer threads. Without the release sequence\nrule or memory_order_release on the fetch_sub operations, there would be nothing\nto require that the stores to the queue_data were visible to the second consumer, and\nyou would have a data race. Thankfully, the first fetch_sub() does participate in the\nrelease sequence, and so the store() synchronizes with the second fetch_sub().\nThere’s still no synchronizes-with relationship between the two consumer threads.\nThis is shown in figure 5.7. The dotted lines in figure 5.7 show the release sequence,\nand the solid lines show the happens-before relationships.\n There can be any number of links in the chain, but provided they’re all read-\nmodify-write operations such as fetch_sub(), the store() will still synchronize with\neach one that’s tagged memory_order_acquire. In this example, all the links are the\nsame, and all are acquire operations, but they could be a mix of different operations\nwith different memory-ordering semantics.\n Although most of the synchronization relationships come from the memory-\nordering semantics applied to operations on atomic variables, it’s also possible to\nintroduce additional ordering constraints by using fences.\nReading \nqueue_data \nis safe.\ne\n",
      "content_length": 2695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "166\nCHAPTER 5\nThe C++ memory model and operations on atomic types\n5.3.5\nFences\nAn atomic operations library wouldn’t be complete without a set of fences. These are\noperations that enforce memory-ordering constraints without modifying any data and\nare typically combined with atomic operations that use the memory_order_relaxed\nordering constraints. Fences are global operations and affect the ordering of other\natomic operations in the thread that executed the fence. Fences are also commonly\ncalled memory barriers, and they get their name because they put a line in the code that\ncertain operations can’t cross. As you may recall from section 5.3.3, relaxed operations\non separate variables can usually be freely reordered by the compiler or the hardware.\nFences restrict this freedom and introduce happens-before and synchronizes-with\nrelationships that weren’t present before.\n Let’s start by adding a fence between the two atomic operations on each thread in\nlisting 5.5, as shown in the following listing.\n#include <atomic>\n#include <thread>\n#include <assert.h>\nstd::atomic<bool> x,y;\nstd::atomic<int> z;\nvoid write_x_then_y()\nListing 5.12\nRelaxed operations can be ordered with fences\ncount.store()\nrelease\ncount.fetch_sub()\nacquire\nProcess\nqueue_data\ncount.fetch_sub()\nacquire\nProcess\nqueue_data\npopulate_queue\nconsume_queue_items\nconsume_queue_items\nPopulate\nqueue_data\nFigure 5.7\nThe release sequence for the queue operations from listing 5.11\n",
      "content_length": 1454,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "167\nSynchronizing operations and enforcing ordering\n{\n    x.store(true,std::memory_order_relaxed);          \n    std::atomic_thread_fence(std::memory_order_release);    \n    y.store(true,std::memory_order_relaxed);          \n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_relaxed));     \n    std::atomic_thread_fence(std::memory_order_acquire);    \n    if(x.load(std::memory_order_relaxed))     \n        ++z;\n}\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x_then_y);\n    std::thread b(read_y_then_x);\n    a.join();\n    b.join();\n    assert(z.load()!=0);   \n}\nThe release fence c synchronizes with the acquire fence f because the load from y\nat e reads the value stored at d. This means that the store to x at B happens before\nthe load from x at g, so the value read must be true and the assert at h won’t fire.\nThis is in contrast to the original case without the fences where the store to and load\nfrom x weren’t ordered, and so the assert could fire. Note that both fences are neces-\nsary: you need a release in one thread and an acquire in another to get a synchronizes-\nwith relationship.\n In this case, the release fence c has the same effect as if the store to y d was\ntagged with memory_order_release rather than memory_order_relaxed. Likewise, the\nacquire fence f makes it as if the load from y e was tagged with memory_order_\nacquire. This is the general idea with fences: if an acquire operation sees the result of\na store that takes place after a release fence, the fence synchronizes with that acquire\noperation; and if a load that takes place before an acquire fence sees the result of a\nrelease operation, the release operation synchronizes with the acquire fence. You can\nhave fences on both sides, as in the example here, in which case if a load that takes\nplace before the acquire fence sees a value written by a store that takes place after the\nrelease fence, the release fence synchronizes with the acquire fence.\n Although the fence synchronization depends on the values read or written by\noperations before or after the fence, it’s important to note that the synchronization\npoint is the fence itself. If you take write_x_then_y from listing 5.12 and move the\nwrite to x after the fence as follows, the condition in the assert is no longer guaranteed\nto be true, even though the write to x comes before the write to y:\n \nb\nc\nd\ne\nf\ng\nh\n",
      "content_length": 2397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "168\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nvoid write_x_then_y()\n{\n    std::atomic_thread_fence(std::memory_order_release);\n    x.store(true,std::memory_order_relaxed);                    \n    y.store(true,std::memory_order_relaxed);                    \n}\nThese two operations are no longer separated by the fence and so are no longer\nordered. It’s only when the fence comes between the store to x and the store to y that\nit imposes an ordering. The presence or absence of a fence doesn’t affect any\nenforced orderings on happens-before relationships that exist because of other\natomic operations.\n This example, and almost every other example so far in this chapter, is built\nentirely from variables with an atomic type. But the real benefit of using atomic opera-\ntions to enforce an ordering is that they can enforce an ordering on non-atomic oper-\nations and avoid the undefined behavior of a data race, as you saw back in listing 5.2.\n5.3.6\nOrdering non-atomic operations with atomics\nIf you replace x from listing 5.12 with an ordinary non-atomic bool (as in the follow-\ning listing), the behavior is guaranteed to be the same.\n#include <atomic>\n#include <thread>\n#include <assert.h>\nbool x=false;          \nstd::atomic<bool> y;\nstd::atomic<int> z;\nvoid write_x_then_y()\n{\n    x=true;              \n    std::atomic_thread_fence(std::memory_order_release);\n    y.store(true,std::memory_order_relaxed);            \n}\nvoid read_y_then_x()\n{\n    while(!y.load(std::memory_order_relaxed));         \n    std::atomic_thread_fence(std::memory_order_acquire);\n    if(x)                    \n        ++z;\n}\nint main()\n{\n    x=false;\n    y=false;\n    z=0;\n    std::thread a(write_x_then_y);\n    std::thread b(read_y_then_x);\n    a.join();\nListing 5.13\nEnforcing ordering on non-atomic operations\nx is now a plain \nnon-atomic variable.\nb\nStore to x before \nthe fence\nc\nStore to y after \nthe fence\nd\nWait until you see \nthe write from c.\nThis will read the \nvalue written by B.\ne\n",
      "content_length": 1995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "169\nSynchronizing operations and enforcing ordering\n    b.join();\n    assert(z.load()!=0);   \n}\nThe fences still provide an enforced ordering of the store to x B and the store to y\nc, and the load from y d and the load from x e, and there’s still a happens-before\nrelationship between the store to x and the load from x, so the assert f still won’t fire.\nThe store to c and load from y d still have to be atomic; otherwise, there would be a\ndata race on y, but the fences enforce an ordering on the operations on x, once the\nreading thread has seen the stored value of y. This enforced ordering means that\nthere’s no data race on x, even though it’s modified by one thread and read by\nanother.\n It’s not only fences that can order non-atomic operations. You saw the ordering\neffects back in listing 5.10 with a memory_order_release/memory_order_consume pair\nordering non-atomic accesses to a dynamically allocated object, and many of the\nexamples in this chapter could be rewritten with some of the memory_order_relaxed\noperations replaced with plain non-atomic operations instead.\n5.3.7\nOrdering non-atomic operations\nOrdering of non-atomic operations through the use of atomic operations is where the\nsequenced-before part of happens-before becomes so important. If a non-atomic\noperation is sequenced before an atomic operation, and that atomic operation hap-\npens before an operation in another thread, the non-atomic operation also happens\nbefore that operation in the other thread. This is where the ordering of the opera-\ntions on x in listing 5.13 comes from and why the example in listing 5.2 works. This is\nalso the basis for the higher-level synchronization facilities in the C++ Standard\nLibrary, such as mutexes and condition variables. To see how this works, consider the\nsimple spinlock mutex from listing 5.1.\n The lock() operation is a loop on flag.test_and_set() using std::memory_\norder_acquire ordering, and the unlock() is a call to flag.clear() with std::memory\n_order_release ordering. When the first thread calls lock(), the flag is initially clear,\nso the first call to test_and_set() will set the flag and return false, indicating that\nthis thread now has the lock, and terminating the loop. The thread is then free to\nmodify any data protected by the mutex. Any other thread that calls lock() at this\ntime will find the flag already set and will be blocked in the test_and_set() loop.\n When the thread with the lock has finished modifying the protected data, it calls\nunlock(), which calls flag.clear() with std::memory_order_release semantics.\nThis then synchronizes (see section 5.3.1) with a subsequent call to flag.test\n_and_set() from an invocation of lock() on another thread, because this call has\nstd::memory_order_acquire semantics. Because the modification of the protected\ndata is necessarily sequenced before the unlock() call, this modification happens\nbefore the unlock() and thus happens before the subsequent lock() call from the\nsecond thread (because of the synchronizes with relationship between the unlock()\nThis assert won’t fire.\nf\n",
      "content_length": 3081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "170\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nand the lock()) and happens before any accesses to that data from this second thread\nonce it has acquired the lock.\n Although other mutex implementations will have different internal operations,\nthe basic principle is the same: lock() is an acquire operation on an internal memory\nlocation, and unlock() is a release operation on that same memory location.\n Each of the synchronization mechanisms described in chapters 2, 3, and 4 will pro-\nvide ordering guarantees in terms of the synchronizes-with relationship. This is what\nenables you to use them to synchronize your data, and provide ordering guarantees.\nThe following are the synchronization relationships provided by these facilities:\nstd::thread\nThe completion of the std::thread constructor synchronizes with the invoca-\ntion of the supplied function or callable object on the new thread.\nThe completion of a thread synchronizes with the return from a successful call\nto join on the std::thread object that owns that thread.\nstd::mutex, std::timed_mutex, std::recursive_mutex, std::recursive_timed_mutex\nAll calls to lock and unlock, and successful calls to try_lock, try_lock_for, or\ntry_lock_until, on a given mutex object form a single total order: the lock order\nof the mutex.\nA call to unlock on a given mutex object synchronizes with a subsequent call to\nlock, or a subsequent successful call to try_lock, try_lock_for, or try_\nlock_until, on that object in the lock order of the mutex.\nFailed calls to try_lock, try_lock_for, or try_lock_until do not participate\nin any synchronization relationships.\nstd::shared_mutex, std::shared_timed_mutex\nAll calls to lock, unlock, lock_shared, and unlock_shared, and successful calls\nto try_lock, try_lock_for, try_lock_until, try_lock_shared, try_lock_\nshared_for, or try_lock_shared_until, on a given mutex object form a single\ntotal order: the lock order of the mutex.\nA call to unlock on a given mutex object synchronizes with a subsequent call to\nlock or shared_lock, or a successful call to try_lock, try_lock_for, try_\nlock_until, try_lock_shared, try_lock_shared_for, or try_lock_shared\n_until, on that object in the lock order of the mutex.\nFailed calls to try_lock, try_lock_for, try_lock_until, try_lock_shared,\ntry_lock_shared_for, or try_lock_shared_until do not participate in any\nsynchronization relationships.\nstd::promise, std::future AND std::shared_future\nThe successful completion of a call to set_value or set_exception on a given\nstd::promise object synchronizes with a successful return from a call to wait\nor get, or a call to wait_for or wait_until that returns std::future_status::\nready on a future that shares the same asynchronous state as the promise.\n",
      "content_length": 2763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "171\nSynchronizing operations and enforcing ordering\nThe destructor of a given std::promise object that stores an std::future_error\nexception in the shared asynchronous state associated with the promise synchro-\nnizes with a successful return from a call to wait or get, or a call to wait_for or\nwait_until that returns std::future_status::ready on a future that shares the\nsame asynchronous state as the promise.\nstd::packaged_task, std::future AND std::shared_future\nThe successful completion of a call to the function call operator of a given\nstd::packaged_task object synchronizes with a successful return from a call to\nwait or get, or a call to wait_for or wait_until that returns std::future\n_status::ready on a future that shares the same asynchronous state as the\npackaged task.\nThe destructor of a given std::packaged_task object that stores an std::\nfuture_error exception in the shared asynchronous state associated with the\npackaged task synchronizes with a successful return from a call to wait or get,\nor a call to wait_for or wait_until that returns std::future_status::ready\non a future that shares the same asynchronous state as the packaged task.\nstd::async, std::future AND std::shared_future\nThe completion of the thread running a task launched via a call to std::async\nwith a policy of std::launch::async synchronizes with a successful return from\na call to wait or get, or a call to wait_for or wait_until that returns\nstd::future_status::ready on a future that shares the same asynchronous\nstate as the spawned task.\nThe completion of a task launched via a call to std::async with a policy of\nstd::launch::deferred synchronizes with a successful return from a call to wait\nor get, or a call to wait_for or wait_until that returns std::future_status\n::ready on a future that shares the same asynchronous state as the promise.\nstd::experimental::future, std::experimental::shared_future AND CONTINUATIONS\nThe event that causes an asynchronous shared state to become ready syn-\nchronizes with the invocation of a continuation function scheduled on that\nshared state.\nThe completion of a continuation function synchronizes with a successful\nreturn from a call to wait or get, or a call to wait_for or wait_until that\nreturns std::future_status::ready on a future that shares the same asyn-\nchronous state as the future returned from the call to then that scheduled the\ncontinuation, or the invocation of any continuation scheduled on that future.\nstd::experimental::latch\nThe invocation of each call to count_down or count_down_and_wait on a given\ninstance of std::experimental::latch synchronizes with the completion of\neach successful call to wait or count_down_and_wait on that latch.\n",
      "content_length": 2715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "172\nCHAPTER 5\nThe C++ memory model and operations on atomic types\nstd::experimental::barrier\nThe invocation of each call to arrive_and_wait or arrive_and_drop on a\ngiven instance of std::experimental::barrier synchronizes with the comple-\ntion of each subsequent successful call to arrive_and_wait on that barrier.\nstd::experimental::flex_barrier\nThe invocation of each call to arrive_and_wait or arrive_and_drop on a given\ninstance of std::experimental::flex_barrier synchronizes with the comple-\ntion of each subsequent successful call to arrive_and_wait on that barrier.\nThe invocation of each call to arrive_and_wait or arrive_and_drop on a\ngiven instance of std::experimental::flex_barrier synchronizes with the\nsubsequent invocation of the completion function on that barrier.\nThe return from the completion function on a given instance of std::\nexperimental::flex_barrier synchronizes with the completion of each call to\narrive_and_wait on that barrier that was blocked waiting for that barrier when\nthe completion function was invoked.\nstd::condition_variable AND std::condition_variable_any\nCondition variables do not provide any synchronization relationships. They are\noptimizations over busy-wait loops, and all the synchronization is provided by\nthe operations on the associated mutex.\nSummary\nIn this chapter I’ve covered the low-level details of the C++ memory model and the\natomic operations that provide the basis for synchronization between threads. This\nincludes the basic atomic types provided by specializations of the std::atomic<> class\ntemplate as well as the generic atomic interface provided by the primary std::atomic<>\ntemplate and the std::experimental::atomic_shared_ptr<> template, the opera-\ntions on these types, and the complex details of the various memory-ordering options.\n We’ve also looked at fences and how they can be paired with operations on atomic\ntypes to enforce an ordering. Finally, we’ve come back to the beginning with a look at\nhow the atomic operations can be used to enforce an ordering between non-atomic\noperations on separate threads, and the synchronization relationships provided by the\nhigher-level facilities.\n In the next chapter we’ll look at using the high-level synchronization facilities\nalongside atomic operations to design efficient containers for concurrent access, and\nwe’ll write algorithms that process data in parallel.\n",
      "content_length": 2397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "173\nDesigning lock-based\nconcurrent data structures\nIn the last chapter we looked at the low-level details of atomic operations and the\nmemory model. In this chapter we’ll take a break from the low-level details (although\nwe’ll need them for chapter 7) and think about data structures. \n The choice of data structure to use for a programming problem can be a key\npart of the overall solution, and parallel programming problems are no exception.\nIf a data structure is to be accessed from multiple threads, either it must be com-\npletely immutable so the data never changes and no synchronization is necessary,\nor the program must be designed to ensure that changes are correctly synchro-\nnized between threads. One option is to use a separate mutex and external locking\nto protect the data, using the techniques we looked at in chapters 3 and 4, and\nanother is to design the data structure itself for concurrent access.\nThis chapter covers\nWhat it means to design data structures for \nconcurrency\nGuidelines for doing so\nExample implementations of data structures \ndesigned for concurrency\n",
      "content_length": 1093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "174\nCHAPTER 6\nDesigning lock-based concurrent data structures\n When designing a data structure for concurrency, you can use the basic building\nblocks of multithreaded applications from earlier chapters, such as mutexes and con-\ndition variables. Indeed, you’ve already seen a couple of examples showing how to\ncombine these building blocks to write data structures that are safe for concurrent\naccess from multiple threads.\n In this chapter we’ll start by looking at some general guidelines for designing data\nstructures for concurrency. We’ll then take the basic building blocks of locks and con-\ndition variables and revisit the design of those basic data structures before moving on\nto more complex data structures. In chapter 7 we’ll look at how to go right back to\nbasics and use the atomic operations described in chapter 5 to build data structures\nwithout locks.\n So, without further ado, let’s look at what’s involved in designing a data structure\nfor concurrency.\n6.1\nWhat does it mean to design for concurrency?\nAt the basic level, designing a data structure for concurrency means that multiple\nthreads can access the data structure concurrently, either performing the same or dis-\ntinct operations, and each thread will see a self-consistent view of the data structure.\nNo data will be lost or corrupted, all invariants will be upheld, and there’ll be no prob-\nlematic race conditions. This data structure is said to be thread-safe. In general, a data\nstructure will be safe only for particular types of concurrent access. It may be possible\nto have multiple threads performing one type of operation on the data structure con-\ncurrently, whereas another operation requires exclusive access by a single thread.\nAlternatively, it may be safe for multiple threads to access a data structure concur-\nrently if they’re performing different actions, whereas multiple threads performing the\nsame action would be problematic.\n Truly designing for concurrency means more than that, though: it means provid-\ning the opportunity for concurrency to threads accessing the data structure. By its nature,\na mutex provides mutual exclusion: only one thread can acquire a lock on the mutex at\na time. A mutex protects a data structure by explicitly preventing true concurrent\naccess to the data it protects.\n This is called serialization: threads take turns accessing the data protected by the\nmutex; they must access it serially rather than concurrently. Consequently, you must\nput careful thought into the design of the data structure to enable true concurrent\naccess. Some data structures have more scope for true concurrency than others, but in\nall cases the idea is the same: the smaller the protected region, the fewer operations\nare serialized, and the greater the potential for concurrency.\n Before we look at some data structure designs, let’s have a quick look at some sim-\nple guidelines for what to consider when designing for concurrency.\n",
      "content_length": 2946,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "175\nWhat does it mean to design for concurrency?\n6.1.1\nGuidelines for designing data structures for concurrency\nAs I mentioned, you have two aspects to consider when designing data structures for\nconcurrent access: ensuring that the accesses are safe and enabling genuine concurrent\naccess. I covered the basics of how to make the data structure thread-safe back in\nchapter 3:\nEnsure that no thread can see a state where the invariants of the data structure\nhave been broken by the actions of another thread.\nTake care to avoid race conditions inherent in the interface to the data structure\nby providing functions for complete operations rather than for operation steps.\nPay attention to how the data structure behaves in the presence of exceptions to\nensure that the invariants are not broken.\nMinimize the opportunities for deadlock when using the data structure by\nrestricting the scope of locks and avoiding nested locks where possible.\nBefore you think about any of these details, it’s also important to think about what\nconstraints you want to put on the users of the data structure; if one thread is access-\ning the data structure through a particular function, which functions are safe to call\nfrom other threads?\n This is a crucial question to consider. Generally, constructors and destructors\nrequire exclusive access to the data structure, but it’s up to the user to ensure that\nthey’re not accessed before construction is complete or after destruction has started.\nIf the data structure supports assignment, swap(), or copy construction, then as the\ndesigner of the data structure, you need to decide whether these operations are safe\nto call concurrently with other operations or whether they require the user to ensure\nexclusive access even though the majority of functions for manipulating the data\nstructure may be called from multiple threads concurrently without any problems.\n The second aspect to consider is that of enabling genuine concurrent access. I\ncan’t offer much in the way of guidelines for this; instead, here’s a list of questions to\nask yourself as the data structure designer:\nCan the scope of locks be restricted to allow some parts of an operation to be\nperformed outside the lock?\nCan different parts of the data structure be protected with different mutexes?\nDo all operations require the same level of protection?\nCan a simple change to the data structure improve the opportunities for con-\ncurrency without affecting the operational semantics?\nAll these questions are guided by a single idea: how can you minimize the amount of\nserialization that must occur and enable the greatest amount of true concurrency?\nIt’s not uncommon for data structures to allow concurrent access from multiple\nthreads that merely read the data structure, whereas a thread that can modify the\ndata structure must have exclusive access. This is supported by using constructs like\n",
      "content_length": 2906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "176\nCHAPTER 6\nDesigning lock-based concurrent data structures\nstd::shared_mutex. Likewise, as you’ll see shortly, it’s quite common for a data\nstructure to support concurrent access from threads performing different operations\nwhile serializing threads that try to perform the same operation.\n The simplest thread-safe data structures typically use mutexes and locks to protect\nthe data. Although there are issues with this, as you saw in chapter 3, it’s relatively easy\nto ensure that only one thread is accessing the data structure at a time. To ease you\ninto the design of thread-safe data structures, we’ll stick to looking at such lock-based\ndata structures in this chapter and leave the design of concurrent data structures with-\nout locks for chapter 7.\n6.2\nLock-based concurrent data structures\nThe design of lock-based concurrent data structures is all about ensuring that the\nright mutex is locked when accessing the data and that the lock is held for the min-\nimum amount of time. This is hard enough when there’s just one mutex protecting\na data structure. You need to ensure that data can’t be accessed outside the protec-\ntion of the mutex lock and that there are no race conditions inherent in the inter-\nface, as you saw in chapter 3. If you use separate mutexes to protect separate parts\nof the data structure, these issues are compounded, and there’s now also the possi-\nbility of deadlock if the operations on the data structure require more than one\nmutex to be locked. You therefore need to consider the design of a data structure\nwith multiple mutexes even more carefully than the design of a data structure with a\nsingle mutex.\n In this section you’ll apply the guidelines from section 6.1.1 to the design of sev-\neral simple data structures, using mutexes and locks to protect the data. In each case\nyou’ll seek out opportunities for enabling greater concurrency while ensuring that the\ndata structure remains thread-safe.\n Let’s start by looking at the stack implementation from chapter 3; it’s one of the\nsimplest data structures around, and it uses only a single mutex. Is it thread-safe? How\ndoes it fare from the point of view of achieving true concurrency?\n6.2.1\nA thread-safe stack using locks\nThe thread-safe stack from chapter 3 is reproduced in the following listing. The intent\nis to write a thread-safe data structure akin to std::stack<>, which supports pushing\ndata items onto the stack and popping them off again.\n#include <exception>\nstruct empty_stack: std::exception\n{\n    const char* what() const throw();\n};\ntemplate<typename T>\nclass threadsafe_stack\nListing 6.1\nA class definition for a thread-safe stack\n",
      "content_length": 2651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "177\nLock-based concurrent data structures\n{\nprivate:\n    std::stack<T> data;\n    mutable std::mutex m;\npublic:\n    threadsafe_stack(){}\n    threadsafe_stack(const threadsafe_stack& other)\n    {\n        std::lock_guard<std::mutex> lock(other.m);\n        data=other.data;\n    }\n    threadsafe_stack& operator=(const threadsafe_stack&) = delete;\n    void push(T new_value)\n    {\n        std::lock_guard<std::mutex> lock(m);\n        data.push(std::move(new_value));    \n    }\n    std::shared_ptr<T> pop()\n    {\n        std::lock_guard<std::mutex> lock(m);\n        if(data.empty()) throw empty_stack();   \n        std::shared_ptr<T> const res(\n           std::make_shared<T>(std::move(data.top())));    \n        data.pop();       \n        return res;\n    }\n    void pop(T& value)\n    {\n        std::lock_guard<std::mutex> lock(m);\n        if(data.empty()) throw empty_stack();\n        value=std::move(data.top());        \n        data.pop();                \n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lock(m);\n        return data.empty();\n    }\n};\nLet’s look at each of the guidelines in turn and see how they apply here.\n First, as you can see, the basic thread safety is provided by protecting each member\nfunction with a lock on the mutex, m. This ensures that only one thread is accessing\nthe data at any one time, so provided each member function maintains the invariants,\nno thread can see a broken invariant.\n Second, there’s a potential for a race condition between empty() and either of the\npop() functions, but because the code explicitly checks for the contained stack being\nempty while holding the lock in pop(), this race condition isn’t problematic. By\nreturning the popped data item directly as part of the call to pop(), you avoid a poten-\ntial race condition that would be present with separate top() and pop() member\nfunctions such as those in std::stack<>.\nb\nc\nd\ne\nf\ng\n",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "178\nCHAPTER 6\nDesigning lock-based concurrent data structures\n Next, there are a few potential sources of exceptions. Locking a mutex may throw\nan exception, but not only is this likely to be exceedingly rare (because it indicates a\nproblem with the mutex or a lack of system resources), it’s also the first operation in\neach member function. Because no data has been modified, this is safe. Unlocking a\nmutex can’t fail, so that’s always safe, and the use of std::lock_guard<> ensures that\nthe mutex is never left locked.\n The call to data.push() B may throw an exception if either copying/moving the\ndata value throws an exception or not enough memory can be allocated to extend the\nunderlying data structure. Either way, std::stack<> guarantees it will be safe, so\nthat’s not a problem either. \n In the first overload of pop(), the code itself might throw an empty_stack excep-\ntion c, but nothing has been modified, so that’s safe. The creation of res d might\nthrow an exception, though, for a couple of reasons: the call to std::make_shared\nmight throw because it can’t allocate memory for the new object and the internal\ndata required for reference counting, or the copy constructor or move constructor\nof the data item to be returned might throw when copying/moving into the freshly-\nallocated memory. In both cases, the C++ runtime and Standard Library ensure that\nthere are no memory leaks and the new object (if any) is correctly destroyed. Because\nyou still haven’t modified the underlying stack, you’re OK. The call to data.pop() e\nis guaranteed not to throw, as is the return of the result, so this overload of pop() is\nexception-safe.\n The second overload of pop() is similar, except this time it’s the copy assignment\nor move assignment operator that can throw f, rather than the construction of a new\nobject and an std::shared_ptr instance. Again, you don’t modify the data structure\nuntil the call to data.pop() g, which is still guaranteed not to throw, so this overload\nis exception-safe too.\n Finally, empty() doesn’t modify any data, so that’s exception-safe.\n There are a couple of opportunities for deadlock here, because you call user code\nwhile holding a lock: the copy constructor or move constructor (B, d) and copy\nassignment or move assignment operator f on the contained data items, as well as\npotentially a user-defined operator new. If these functions either call member func-\ntions on the stack that the item is being inserted into or removed from or require a\nlock of any kind and another lock was held when the stack member function was\ninvoked, there’s the possibility of deadlock. But it’s sensible to require that users of\nthe stack be responsible for ensuring this; you can’t reasonably expect to add an item\nonto a stack or remove it from a stack without copying it or allocating memory for it.\n Because all the member functions use std::lock_guard<> to protect the data, it’s\nsafe for any number of threads to call the stack member functions. The only member\nfunctions that aren’t safe are the constructors and destructors, but this isn’t a prob-\nlem; the object can be constructed only once and destroyed only once. Calling mem-\nber functions on an incompletely constructed object or a partially destructed object is\nnever a good idea, whether done concurrently or not. As a consequence, the user must\n",
      "content_length": 3344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "179\nLock-based concurrent data structures\nensure that other threads aren’t able to access the stack until it’s fully constructed and\nmust ensure that all threads have ceased accessing the stack before it’s destroyed.\n Although it’s safe for multiple threads to call the member functions concurrently,\nbecause of the use of locks, only one thread is ever doing any work in the stack data\nstructure at a time. This serialization of threads can potentially limit the performance\nof an application where there’s significant contention on the stack: while a thread is\nwaiting for the lock, it isn’t doing any useful work. Also, the stack doesn’t provide any\nmeans of waiting for an item to be added, so if a thread needs to wait, it must periodi-\ncally call empty(), or call pop() and catch the empty_stack exceptions. This makes\nthis stack implementation a poor choice if such a scenario is required, because a wait-\ning thread must either consume precious resources checking for data or the user\nmust write external wait and notification code (for example, using condition vari-\nables), which might render the internal locking unnecessary and therefore wasteful.\nThe queue from chapter 4 shows a way of incorporating this waiting into the data struc-\nture itself using a condition variable inside the data structure, so let’s look at that next.\n6.2.2\nA thread-safe queue using locks and condition variables\nThe thread-safe queue from chapter 4 is reproduced in listing 6.2. Much like the stack\nwas modeled after std::stack<>, this queue is modeled after std::queue<>. Again,\nthe interface differs from that of the standard container adaptor because of the con-\nstraints of writing a data structure that’s safe for concurrent access from multiple\nthreads.\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    mutable std::mutex mut;\n    std::queue<T> data_queue;\n    std::condition_variable data_cond;\npublic:\n    threadsafe_queue()\n    {}\n    void push(T new_value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        data_queue.push(std::move(new_value));\n        data_cond.notify_one();         \n    }\n    void wait_and_pop(T& value)    \n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        value=std::move(data_queue.front());\n        data_queue.pop();\n    }\nListing 6.2\nThe full class definition for a thread-safe queue using condition variables\nb\nc\n",
      "content_length": 2435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "180\nCHAPTER 6\nDesigning lock-based concurrent data structures\n    std::shared_ptr<T> wait_and_pop()    \n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});    \n        std::shared_ptr<T> res(\n            std::make_shared<T>(std::move(data_queue.front())));\n        data_queue.pop();\n        return res;\n    }\n    bool try_pop(T& value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return false;\n        value=std::move(data_queue.front());\n        data_queue.pop();\n        return true;\n    }\n    std::shared_ptr<T> try_pop()\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return std::shared_ptr<T>();    \n        std::shared_ptr<T> res(\n            std::make_shared<T>(std::move(data_queue.front())));\n        data_queue.pop();\n        return res;\n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        return data_queue.empty();\n    }\n};\nThe structure of the queue implementation shown in listing 6.2 is similar to the stack\nfrom listing 6.1, except for the call to data_cond.notify_one() in push() B and the\nwait_and_pop() functions, c and d. The two overloads of try_pop() are almost\nidentical to the pop() functions from listing 6.1, except that they don’t throw an\nexception if the queue is empty. Instead, they return either a bool value indicating\nwhether a value was retrieved or a NULL pointer if no value could be retrieved by the\npointer-returning overload f. This would also have been a valid way of implementing\nthe stack. If you exclude the wait_and_pop() functions, the analysis you did for the\nstack applies just as well here.\n The new wait_and_pop() functions are a solution to the problem of waiting for a\nqueue entry that you saw with the stack; rather than continuously calling empty(), the\nwaiting thread can call wait_and_pop() and the data structure will handle the waiting\nwith a condition variable. The call to data_cond.wait() won’t return until the under-\nlying queue has at least one element, so you don’t have to worry about the possibility\nof an empty queue at this point in the code, and the data is still protected with the\nd\ne\nf\n",
      "content_length": 2256,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "181\nLock-based concurrent data structures\nlock on the mutex. These functions don’t therefore add any new race conditions or\npossibilities for deadlock, and the invariants will be upheld.\n There’s a slight twist with regard to exception safety in that if more than one\nthread is waiting when an entry is pushed onto the queue, only one thread will be\nwoken by the call to data_cond.notify_one(). But if that thread then throws an\nexception in wait_and_pop(), such as when the new std::shared_ptr<> is con-\nstructed e, none of the other threads will be woken. If this isn’t acceptable, the call is\nreadily replaced with data_cond.notify_all(), which will wake all the threads but at\nthe cost of most of them then going back to sleep when they find that the queue is\nempty after all. A second alternative is to have wait_and_pop() call notify_one() if\nan exception is thrown, so that another thread can attempt to retrieve the stored\nvalue. A third alternative is to move the std::shared_ptr<> initialization to the\npush() call and store std::shared_ptr<> instances rather than direct data values.\nCopying the std::shared_ptr<> out of the internal std::queue<> then can’t throw\nan exception, so wait_and_pop() is safe again. The following listing shows the queue\nimplementation revised with this in mind.\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    mutable std::mutex mut;\n    std::queue<std::shared_ptr<T> > data_queue;\n    std::condition_variable data_cond;\npublic:\n    threadsafe_queue()\n    {}\n    void wait_and_pop(T& value)\n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        value=std::move(*data_queue.front());     \n        data_queue.pop();\n    }\n    bool try_pop(T& value)\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return false;\n        value=std::move(*data_queue.front());     \n        data_queue.pop();\n        return true;\n    }\n    std::shared_ptr<T> wait_and_pop()\n    {\n        std::unique_lock<std::mutex> lk(mut);\n        data_cond.wait(lk,[this]{return !data_queue.empty();});\n        std::shared_ptr<T> res=data_queue.front();            \nListing 6.3\nA thread-safe queue holding std::shared_ptr<> instances\nb\nc\nd\n",
      "content_length": 2271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "182\nCHAPTER 6\nDesigning lock-based concurrent data structures\n        data_queue.pop();\n        return res;\n    }\n    std::shared_ptr<T> try_pop()\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        if(data_queue.empty())\n            return std::shared_ptr<T>();\n        std::shared_ptr<T> res=data_queue.front();    \n        data_queue.pop();\n        return res;\n    }\n    void push(T new_value)\n    {\n        std::shared_ptr<T> data(\n            std::make_shared<T>(std::move(new_value)));    \n        std::lock_guard<std::mutex> lk(mut);\n        data_queue.push(data);\n        data_cond.notify_one();\n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lk(mut);\n        return data_queue.empty();\n    }\n};\nThe basic consequences of holding the data by std::shared_ptr<> are straightfor-\nward: the pop functions that take a reference to a variable to receive the new value\nnow have to dereference the stored pointer, B and c, and the pop functions that\nreturn an std::shared_ptr<> instance can retrieve it from the queue, d and e,\nbefore returning it to the caller.\n If the data is held by std::shared_ptr<>, there’s an additional benefit: the alloca-\ntion of the new instance can now be done outside the lock in push() f, whereas in\nlisting 6.2 it had to be done while holding the lock in pop(). Because memory alloca-\ntion is typically quite an expensive operation, this can be beneficial for the perfor-\nmance of the queue, because it reduces the time the mutex is held, allowing other\nthreads to perform operations on the queue in the meantime.\n Just like in the stack example, the use of a mutex to protect the entire data struc-\nture limits the concurrency supported by this queue; although multiple threads might\nbe blocked on the queue in various member functions, only one thread can be doing\nany work at a time. But part of this restriction comes from the use of std::queue<> in\nthe implementation; by using the standard container you now have one data item\nthat’s either protected or not. By taking control of the detailed implementation of the\ndata structure, you can provide more fine-grained locking and allow a higher level of\nconcurrency.\ne\nf\n",
      "content_length": 2187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "183\nLock-based concurrent data structures\n6.2.3\nA thread-safe queue using fine-grained locks and \ncondition variables\nIn listings 6.2 and 6.3 you have one protected data item (data_queue) and therefore\none mutex. In order to use finer-grained locking, you need to look inside the queue at\nits constituent parts and associate one mutex with each distinct data item.\n The simplest data structure for a queue is a singly linked list, as shown in figure 6.1.\nThe queue contains a head pointer, which points to the first item in the list, and each\nitem then points to the next item. Data items are removed from the queue by replac-\ning the head pointer with the pointer to the next item and then returning the data\nfrom the old head.\nItems are added to the queue at the other end. In order to do this, the queue also con-\ntains a tail pointer, which refers to the last item in the list. New nodes are added by\nchanging the next pointer of the last item to point to the new node and then updat-\ning the tail pointer to refer to the new item. When the list is empty, both the head\nand tail pointers are NULL.\n The following listing shows a simple implementation of this queue based on a cut-\ndown version of the interface to the queue in listing 6.2; you have only one try_pop()\nfunction and no wait_and_pop() because this queue only supports single-threaded use.\ntemplate<typename T>\nclass queue\n{\nprivate:\n    struct node\n    {\n        T data;\n        std::unique_ptr<node> next;\n        node(T data_):\n            data(std::move(data_))\n        {}\n    };\n    std::unique_ptr<node> head;    \n    node* tail;                \nListing 6.4\nA simple single-threaded queue implementation\nTail\nHead\nFigure 6.1\nA queue represented using a single-linked list\nb\nc\n",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "184\nCHAPTER 6\nDesigning lock-based concurrent data structures\npublic:\n    queue(): tail(nullptr)\n    {}\n    queue(const queue& other)=delete;\n    queue& operator=(const queue& other)=delete;\n    std::shared_ptr<T> try_pop()\n    {\n        if(!head)\n        {\n            return std::shared_ptr<T>();\n        }\n        std::shared_ptr<T> const res(\n            std::make_shared<T>(std::move(head->data)));\n        std::unique_ptr<node> const old_head=std::move(head);\n        head=std::move(old_head->next);     \n        if(!head)\n            tail=nullptr;\n        return res;\n    }\n    void push(T new_value)\n    {\n        std::unique_ptr<node> p(new node(std::move(new_value)));\n        node* const new_tail=p.get();\n        if(tail)\n        {\n            tail->next=std::move(p);    \n        }\n        else\n        {\n            head=std::move(p);    \n        }\n        tail=new_tail;     \n    }\n};\nFirst off, note that listing 6.4 uses std::unique_ptr<node> to manage the nodes,\nbecause this ensures that they (and the data they refer to) get deleted when they’re no\nlonger needed, without having to write an explicit delete. This ownership chain is\nmanaged from head, with tail being a raw pointer to the last node, as it needs to\nrefer to a node already owned by std::unique_ptr<node>.\n Although this implementation works fine in a single-threaded context, a couple of\nthings will cause you problems if you try to use fine-grained locking in a multi-\nthreaded context. Given that you have two data items (head B and tail c), you\ncould in principle use two mutexes, one to protect head and one to protect tail, but\nthere are a couple of problems with that.\n The most obvious problem is that push() can modify both head f and tail g, so\nit would have to lock both mutexes. This isn’t too much of a problem, although it’s\nunfortunate, because locking both mutexes would be possible. The critical problem\nis that both push() and pop() access the next pointer of a node: push() updates\ntail->next e, and try_pop() reads head->next d. If there’s a single item in the\nd\ne\nf\ng\n",
      "content_length": 2073,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "185\nLock-based concurrent data structures\nqueue, then head==tail, so both head->next and tail->next are the same object,\nwhich therefore requires protection. Because you can’t tell if it’s the same object with-\nout reading both head and tail, you now have to lock the same mutex in both push()\nand try_pop(), so you’re no better off than before. Is there a way out of this dilemma?\nENABLING CONCURRENCY BY SEPARATING DATA\nYou can solve this problem by preallocating a dummy node with no data to ensure that\nthere’s always at least one node in the queue to separate the node being accessed at\nthe head from that being accessed at the tail. For an empty queue, head and tail now\nboth point to the dummy node rather than being NULL. This is fine, because\ntry_pop() doesn’t access head->next if the queue is empty. If you add a node to the\nqueue (so there’s one real node), then head and tail now point to separate nodes, so\nthere’s no race on head->next and tail->next. The downside is that you have to add\nan extra level of indirection to store the data by pointer in order to allow the dummy\nnodes. The following listing shows how the implementation looks now.\ntemplate<typename T>\nclass queue\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;     \n        std::unique_ptr<node> next;\n    };\n    std::unique_ptr<node> head;\n    node* tail;\npublic:\n    queue():\n        head(new node),tail(head.get())    \n    {}\n    queue(const queue& other)=delete;\n    queue& operator=(const queue& other)=delete;\n    std::shared_ptr<T> try_pop()\n    {\n        if(head.get()==tail)     \n        {\n            return std::shared_ptr<T>();\n        }\n        std::shared_ptr<T> const res(head->data);     \n        std::unique_ptr<node> old_head=std::move(head);\n        head=std::move(old_head->next);               \n        return res;                    \n    }\n    void push(T new_value)\n    {\n        std::shared_ptr<T> new_data(\n            std::make_shared<T>(std::move(new_value)));   \nListing 6.5\nA simple queue with a dummy node\nb\nc\nd\ne\nf\ng\nh\n",
      "content_length": 2048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "186\nCHAPTER 6\nDesigning lock-based concurrent data structures\n        std::unique_ptr<node> p(new node);            \n        tail->data=new_data;        \n        node* const new_tail=p.get();\n        tail->next=std::move(p);\n        tail=new_tail;\n    }\n};\nThe changes to try_pop() are fairly minimal. First, you’re comparing head against\ntail d, rather than checking for NULL, because the dummy node means that head is\nnever NULL. Because head is a std::unique_ptr<node>, you need to call head.get()\nto do the comparison. Second, because the node now stores the data by pointer B,\nyou can retrieve the pointer directly e, rather than having to construct a new\ninstance of T. The big changes are in push(): you must first create a new instance of T\non the heap and take ownership of it in a std::shared_ptr<> h (note the use of\nstd::make_shared to avoid the overhead of a second memory allocation for the refer-\nence count). The new node you create is going to be the new dummy node, so you\ndon’t need to supply the new_value to the constructor i. Instead, you set the data on\nthe old dummy node to your newly allocated copy of the new_value j. Finally, in\norder to have a dummy node, you have to create it in the constructor c.\n By now, I’m sure you’re wondering what these changes buy you and how they help\nwith making the queue thread-safe. Well, push() now  accesses only tail, not head,\nwhich is an improvement. try_pop() accesses both head and tail, but tail is needed\nonly for the initial comparison, so the lock is short-lived. The big gain is that the\ndummy node means try_pop() and push() are never operating on the same node, so\nyou no longer need an overarching mutex. You can have one mutex for head and one\nfor tail. Where do you put the locks?\n You’re aiming for the maximum number of opportunities for concurrency, so you\nwant to hold the locks for the shortest possible length of time. push() is easy: the\nmutex needs to be locked across all accesses to tail, which means you lock the mutex\nafter the new node is allocated i, and before you assign the data to the current tail\nnode j. The lock then needs to be held until the end of the function.\n try_pop() isn’t so easy. First off, you need to lock the mutex on head and hold it\nuntil you’re finished with head. This is the mutex to determine which thread does the\npopping, so you want to do that first. Once head is changed f, you can unlock\nthe mutex; it doesn’t need to be locked when you return the result g. That leaves the\naccess to tail needing a lock on the tail mutex. Because you need to access tail only\nonce, you can just acquire the mutex for the time it takes to do the read. This is best\ndone by wrapping it in a function. In fact, because the code that needs the head\nmutex locked is only a subset of the member, it’s clearer to wrap that in a function too.\nThe final code is shown here.\n \n \n \ni\nj\n",
      "content_length": 2883,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "187\nLock-based concurrent data structures\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::unique_ptr<node> next;\n    };\n    std::mutex head_mutex;\n    std::unique_ptr<node> head;\n    std::mutex tail_mutex;\n    node* tail;\n    node* get_tail()\n    {\n        std::lock_guard<std::mutex> tail_lock(tail_mutex);\n        return tail;\n    }\n    std::unique_ptr<node> pop_head()\n    {\n        std::lock_guard<std::mutex> head_lock(head_mutex);\n            \n        if(head.get()==get_tail())\n        {\n            return nullptr;\n        }\n        std::unique_ptr<node> old_head=std::move(head);\n        head=std::move(old_head->next);\n        return old_head;\n    }\npublic:\n    threadsafe_queue():\n        head(new node),tail(head.get())\n    {}\n    threadsafe_queue(const threadsafe_queue& other)=delete;\n    threadsafe_queue& operator=(const threadsafe_queue& other)=delete;\n    std::shared_ptr<T> try_pop()\n    {\n        std::unique_ptr<node> old_head=pop_head();\n        return old_head?old_head->data:std::shared_ptr<T>();\n    }\n    void push(T new_value)\n    {\n        std::shared_ptr<T> new_data(\n            std::make_shared<T>(std::move(new_value)));\n        std::unique_ptr<node> p(new node);\n        node* const new_tail=p.get();\n        std::lock_guard<std::mutex> tail_lock(tail_mutex);\n        tail->data=new_data;\n        tail->next=std::move(p);\n        tail=new_tail;\n    }\n};\nListing 6.6\nA thread-safe queue with fine-grained locking\n",
      "content_length": 1520,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "188\nCHAPTER 6\nDesigning lock-based concurrent data structures\nLet’s look at this code with a critical eye, thinking about the guidelines listed in sec-\ntion 6.1.1. Before you look for broken invariants, you should be sure what they are:\n\ntail->next==nullptr.\n\ntail->data==nullptr.\n\nhead==tail implies an empty list.\nA single element list has head->next==tail.\nFor each node x in the list, where x!=tail, x->data points to an instance of T\nand x->next points to the next node in the list. x->next==tail implies x is the\nlast node in the list.\nFollowing the next nodes from head will eventually yield tail.\nOn its own, push() is straightforward: the only modifications to the data structure are\nprotected by tail_mutex, and they uphold the invariant because the new tail node is\nan empty node and data and next are correctly set for the old tail node, which is now\nthe last real node in the list.\n The interesting part is try_pop(). It turns out that not only is the lock on\ntail_mutex necessary to protect the read of tail itself, but it’s also necessary to\nensure that you don’t get a data race reading the data from the head. If you didn’t\nhave that mutex, it would be quite possible for a thread to call try_pop() and a thread\nto call push() concurrently, and there’d be no defined ordering on their operations.\nEven though each member function holds a lock on a mutex, they hold locks on differ-\nent mutexes, and they potentially access the same data; all data in the queue originates\nfrom a call to push(), after all. Because the threads would be potentially accessing the\nsame data without a defined ordering, this would be a data race, as you saw in chapter 5,\nand undefined behavior. Thankfully the lock on tail_mutex in get_tail() solves\neverything. Because the call to get_tail() locks the same mutex as the call to push(),\nthere’s a defined order between the two calls. Either the call to get_tail() occurs\nbefore the call to push(), in which case it sees the old value of tail, or it occurs after\nthe call to push(), in which case it sees the new value of tail and the new data\nattached to the previous value of tail.\n It’s also important that the call to get_tail() occurs inside the lock on head_\nmutex. If it didn’t, the call to pop_head() could be stuck in between the call to\nget_tail() and the lock on the head_mutex, because other threads called try_pop()\n(and thus pop_head()) and acquired the lock first, preventing your initial thread\nfrom making progress:\n    std::unique_ptr<node> pop_head()    \n    {\n        node* const old_tail=get_tail();                  \n        std::lock_guard<std::mutex> head_lock(head_mutex);\n            \n        if(head.get()==old_tail)    \n        {\n            return nullptr;\n        }\nThis is a broken \nimplementation.\nGet old tail value \noutside lock on \nhead_mutex\nb\nc\n",
      "content_length": 2833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "189\nLock-based concurrent data structures\n        std::unique_ptr<node> old_head=std::move(head);\n        head=std::move(old_head->next);    \n        return old_head;\n    }\nIn this broken scenario, where the call to get_tail(0) B is made outside the scope\nof the lock, you might find that both head and tail have changed by the time your\ninitial thread can acquire the lock on head_mutex, and not only is the returned tail\nnode no longer the tail, but it’s no longer even part of the list. This could then\nmean that the comparison of head to old_tail c fails, even if head is the last\nnode. Consequently, when you update head d, you may end up moving head\nbeyond tail and off the end of the list, destroying the data structure. In the correct\nimplementation from listing 6.6, you keep the call to get_tail() inside the lock on\nhead_mutex. This ensures that no other threads can change head, and tail only\never moves further away (as new nodes are added in calls to push()), which is per-\nfectly safe. head can never pass the value returned from get_tail(), so the invari-\nants are upheld.\n Once pop_head() has removed the node from the queue by updating head, the\nmutex is unlocked, and try_pop() can extract the data and delete the node if there\nwas one (and return a NULL instance of std::shared_ptr<> if not), safe in the knowl-\nedge that it’s the only thread that can access this node.\n Next up, the external interface is a subset of that from listing 6.2, so the same anal-\nysis applies: there are no race conditions inherent in the interface.\n Exceptions are more interesting. Because you’ve changed the data allocation pat-\nterns, the exceptions can now come from different places. The only operations in\ntry_pop() that can throw exceptions are the mutex locks, and the data isn’t modified\nuntil the locks are acquired. Therefore try_pop() is exception-safe. On the other\nhand, push() allocates a new instance of T on the heap and a new instance of node,\neither of which might throw an exception. But both of the newly allocated objects are\nassigned to smart pointers, so they’ll be freed if an exception is thrown. Once the lock\nis acquired, none of the remaining operations in push() can throw an exception, so\nagain you’re home and dry and push() is exception-safe too.\n Because you haven’t changed the interface, there are no new external opportuni-\nties for deadlock. There are no internal opportunities, either; the only place that two\nlocks are acquired is in pop_head(), which always acquires the head_mutex, and then\nthe tail_mutex, so this will never deadlock.\n The remaining question concerns the possibilities for concurrency. This data struc-\nture has considerably more scope for concurrency than that from listing 6.2, because\nthe locks are more fine-grained and more is done outside the locks. For example, in\npush(), the new node and new data item are allocated with no locks held. This means\nthat multiple threads can be allocating new nodes and data items concurrently with-\nout a problem. Only one thread can add its new node to the list at a time, but the code\nto do so is only a few simple pointer assignments, so the lock isn’t held for much time\nd\n",
      "content_length": 3178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "190\nCHAPTER 6\nDesigning lock-based concurrent data structures\nat all compared to the std::queue<>-based implementation where the lock is held\naround all the memory allocation operations internal to the std::queue<>.\n Also, try_pop()holds the tail_mutex for only a short time, to protect a read from\ntail. Consequently, almost the entirety of a call to try_pop() can occur concurrently\nwith a call to push(). Also, the operations performed while holding the head_mutex\nare quite minimal; the expensive delete (in the destructor of the node pointer) is out-\nside the lock. This will increase the number of calls to try_pop() that can happen\nconcurrently; only one thread can call pop_head() at  a time, but multiple threads can\nthen delete their old nodes and return the data safely.\nWAITING FOR AN ITEM TO POP\nOK, so listing 6.6 provides a thread-safe queue with fine-grained locking, but it sup-\nports only try_pop() (and only one overload at that). What about the handy wait_\nand_pop() functions back in listing 6.2? Can you implement an identical interface\nwith your fine-grained locking?\n The answer is yes, but the real question is how. Modifying push() is easy: add the\ndata_cond.notify_one() call at the end of the function, like in listing 6.2. It’s not\nquite that simple; you’re using fine-grained locking because you want the maximum\npossible amount of concurrency. If you leave the mutex locked across the call to\nnotify_one() (as in listing 6.2), then if the notified thread wakes up before the\nmutex has been unlocked, it will have to wait for the mutex. On the other hand, if you\nunlock the mutex before you call notify_one(), then the mutex is available for the\nwaiting thread to acquire when it wakes up (assuming no other thread locks it first).\nThis is a minor improvement, but it might be important in some cases.\n wait_and_pop() is more complicated, because you have to decide where to wait,\nwhat the predicate is, and which mutex needs to be locked. The condition you’re wait-\ning for is “queue not empty,” which is represented by head!=tail. Written like that, it\nwould require both head_mutex and tail_mutex to be locked, but you’ve already\ndecided in listing 6.6 that you only need to lock tail_mutex for the read of tail and\nnot for the comparison itself, so you can apply the same logic here. If you make the\npredicate head!=get_tail(), you only need to hold head_mutex, so you can use your\nlock on that for the call to data_cond.wait(). Once you’ve added the wait logic, the\nimplementation is the same as try_pop().\n The second overload of try_pop() and the corresponding wait_and_pop() over-\nload require careful thought. If you replace the return of std::shared_ptr<>\nretrieved from old_head with a copy assignment to the value parameter, there’s a\npotential exception-safety issue. At this point, the data item has been removed from\nthe queue and the mutex unlocked; all that remains is to return the data to the caller.\nBut if the copy assignment throws an exception (as it might), the data item is lost\nbecause it can’t be returned to the queue in the same place.\n If the actual type T used for the template argument has a no-throw move-assignment\noperator or a no-throw swap operation, you could use that, but you’d prefer a general\nsolution that could be used for any type T. In this case, you have to move the potential\n",
      "content_length": 3354,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "191\nLock-based concurrent data structures\nthrowing operation inside the locked region before the node is removed from the list.\nThis means you need an extra overload of pop_head() that retrieves the stored value\nprior to modifying the list.\n In comparison, empty() is trivial: lock head_mutex and check for head== get_tail()\n(see listing 6.10). The final code for the queue is shown in listings 6.7, 6.8, 6.9, and 6.10.\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::unique_ptr<node> next;\n    };\n    std::mutex head_mutex;\n    std::unique_ptr<node> head;\n    std::mutex tail_mutex;\n    node* tail;\n    std::condition_variable data_cond;\npublic:\n    threadsafe_queue():\n        head(new node),tail(head.get())\n    {}\n    threadsafe_queue(const threadsafe_queue& other)=delete;\n    threadsafe_queue& operator=(const threadsafe_queue& other)=delete;\n    std::shared_ptr<T> try_pop();\n    bool try_pop(T& value);\n    std::shared_ptr<T> wait_and_pop();\n    void wait_and_pop(T& value);\n    void push(T new_value);\n    bool empty();\n};\nPushing new nodes onto the queue is fairly straightforward—the implementation\n(shown in the following listing) is close to that shown previously.\ntemplate<typename T>\nvoid threadsafe_queue<T>::push(T new_value)\n{\n    std::shared_ptr<T> new_data(\n        std::make_shared<T>(std::move(new_value)));\n    std::unique_ptr<node> p(new node);\n    {\n        std::lock_guard<std::mutex> tail_lock(tail_mutex);\n        tail->data=new_data;\n        node* const new_tail=p.get();\n        tail->next=std::move(p);\nListing 6.7\nA thread-safe queue with locking and waiting: internals and interface\nListing 6.8\nA thread-safe queue with locking and waiting: pushing new values\n",
      "content_length": 1768,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "192\nCHAPTER 6\nDesigning lock-based concurrent data structures\n        tail=new_tail;\n    }\n    data_cond.notify_one();\n}\nAs already mentioned, the complexity is all in the pop side, which makes use of a series\nof helper functions to simplify matters. The next listing shows the implementation of\nwait_and_pop() and the associated helper functions.\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    node* get_tail()\n    {\n        std::lock_guard<std::mutex> tail_lock(tail_mutex);\n        return tail;\n    }\n    std::unique_ptr<node> pop_head()    \n    {\n        std::unique_ptr<node> old_head=std::move(head);\n        head=std::move(old_head->next);\n        return old_head;\n    }\n    std::unique_lock<std::mutex> wait_for_data()    \n    {\n        std::unique_lock<std::mutex> head_lock(head_mutex);\n        data_cond.wait(head_lock,[&]{return head.get()!=get_tail();});\n        return std::move(head_lock);         \n    }\n    std::unique_ptr<node> wait_pop_head()\n    {\n        std::unique_lock<std::mutex> head_lock(wait_for_data());    \n        return pop_head();\n    }\n    std::unique_ptr<node> wait_pop_head(T& value)\n    {\n        std::unique_lock<std::mutex> head_lock(wait_for_data());    \n        value=std::move(*head->data);\n        return pop_head();\n    }\npublic:\n    std::shared_ptr<T> wait_and_pop()\n    {\n        std::unique_ptr<node> const old_head=wait_pop_head();\n        return old_head->data;\n    }\n    void wait_and_pop(T& value)\n    {\n        std::unique_ptr<node> const old_head=wait_pop_head(value);\n    }\n};\nListing 6.9\nA thread-safe queue with locking and waiting: wait_and_pop()\nb\nc\nd\ne\nf\n",
      "content_length": 1626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "193\nLock-based concurrent data structures\nThe implementation of the pop side shown in listing 6.9 has several little helper func-\ntions to simplify the code and reduce duplication, such as pop_head() B, which\nmodifies the list to remove the head item, and wait_for_data() c, which waits for\nthe queue to have some data to pop. wait_for_data() is particularly noteworthy,\nbecause not only does it wait on the condition variable using a lambda function for\nthe predicate, but it also returns the lock instance to the caller d. This is to ensure\nthat the same lock is held while the data is modified by the relevant wait_pop_head()\noverload, e and f. pop_head() is also reused by the try_pop() code shown in the\nnext listing.\ntemplate<typename T>\nclass threadsafe_queue\n{\nprivate:\n    std::unique_ptr<node> try_pop_head()\n    {\n        std::lock_guard<std::mutex> head_lock(head_mutex);\n        if(head.get()==get_tail())\n        {\n            return std::unique_ptr<node>();\n        }\n        return pop_head();\n    }\n    std::unique_ptr<node> try_pop_head(T& value)\n    {\n        std::lock_guard<std::mutex> head_lock(head_mutex);\n        if(head.get()==get_tail())\n        {\n            return std::unique_ptr<node>();\n        }\n        value=std::move(*head->data);\n        return pop_head();\n    }\npublic:\n    std::shared_ptr<T> try_pop()\n    {\n        std::unique_ptr<node> old_head=try_pop_head();\n        return old_head?old_head->data:std::shared_ptr<T>();\n    }\n    bool try_pop(T& value)\n    {\n        std::unique_ptr<node> const old_head=try_pop_head(value);\n        return old_head;\n    }\n    bool empty()\n    {\n        std::lock_guard<std::mutex> head_lock(head_mutex);\n        return (head.get()==get_tail());\n    }\n};\nListing 6.10\nA thread-safe queue with locking and waiting: try_pop() and empty()\n",
      "content_length": 1812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "194\nCHAPTER 6\nDesigning lock-based concurrent data structures\nThis queue implementation will serve as the basis for the lock-free queue covered in\nchapter 7. It’s an unbounded queue; threads can continue to push new values onto\nthe queue as long as there’s available memory, even if no values are removed. The\nalternative to an unbounded queue is a bounded queue, in which the maximum\nlength of the queue is fixed when the queue is created. Once a bounded queue is\nfull, attempts to push further elements onto the queue will either fail or block until\nan element has been popped from the queue to make room. Bounded queues can\nbe useful for ensuring an even spread of work when dividing work between threads\nbased on tasks to be performed (see chapter 8). This prevents the thread(s) popu-\nlating the queue from running too far ahead of the thread(s) reading items from\nthe queue.\n The unbounded queue implementation shown here can easily be extended to\nlimit the length of the queue by waiting on the condition variable in push(). Rather\nthan waiting for the queue to have items (as is done in pop()), you need to wait for\nthe queue to have fewer than the maximum number of items. Further discussion of\nbounded queues is outside the scope of this book; for now, let’s move beyond queues\nand on to more complex data structures.\n6.3\nDesigning more complex lock-based data structures\nStacks and queues are simple: the interface is exceedingly limited, and they’re tightly\nfocused on a specific purpose. Not all data structures are that simple; most data struc-\ntures support a variety of operations. In principle, this can then lead to greater oppor-\ntunities for concurrency, but it also makes the task of protecting the data that much\nharder because the multiple access patterns need to be taken into account. The pre-\ncise nature of the various operations that can be performed is important when design-\ning these data structures for concurrent access.\n To see some of the issues involved, let’s look at the design of a lookup table.\n6.3.1\nWriting a thread-safe lookup table using locks\nA lookup table or dictionary associates values of one type (the key type) with values of\neither the same or a different type (the mapped type). In general, the intention behind\nsuch a structure is to allow code to query the data associated with a given key. In the C++\nStandard Library, this facility is provided by the associative containers: std::map<>,\nstd::multimap<>, std::unordered_map<>, and std::unordered_multimap<>. \n A lookup table has a different usage pattern than a stack or a queue. Whereas\nalmost every operation on a stack or a queue modifies it in some way, either to add an\nelement or remove one, a lookup table might be modified rarely. The simple DNS\ncache in listing 3.13 is one example of this scenario, which features a greatly reduced\ninterface compared to std::map<>. As you saw with the stack and queue, the inter-\nfaces of the standard containers aren’t suitable when the data structure is to be\naccessed from multiple threads concurrently, because there are inherent race condi-\ntions in the interface design, so they need to be cut down and revised.\n",
      "content_length": 3168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "195\nDesigning more complex lock-based data structures\n The biggest problem with the std::map<> interface from a concurrency perspec-\ntive is the iterators. Although it’s possible to have an iterator that provides safe access\ninto a container even when other threads can access (and modify) the container, this\nis a tricky proposition. Correctly handling iterators requires you to deal with issues\nsuch as another thread deleting the element that the iterator is referring to, which\ncan get rather involved. For the first cut at a thread-safe lookup table interface, you’ll\nskip the iterators. Given that the interface to std::map<> (and the other associative\ncontainers in the standard library) is so heavily iterator-based, it’s probably worth set-\nting them aside and designing the interface from the ground up.\n There are only a few basic operations on a lookup table:\nAdd a new key/value pair.\nChange the value associated with a given key.\nRemove a key and its associated value.\nObtain the value associated with a given key, if any.\nThere are also a few container-wide operations that might be useful, such as a check\non whether the container is empty, a snapshot of the complete list of keys, or a snap-\nshot of the complete set of key/value pairs.\n If you stick to the simple thread-safety guidelines, such as not returning references,\nand put a simple mutex lock around the entirety of each member function, all of\nthese are safe; they either come before some modification from another thread or\nafter it. The biggest potential for a race condition is when a new key/value pair is\nbeing added; if two threads add a new value, only one will be first, and the second will\ntherefore fail. One possibility is to combine add and change into a single member\nfunction, as you did for the DNS cache in listing 3.13.\n The only other interesting point from an interface perspective is the if any part of\nobtaining an associated value. One option is to allow the user to provide a “default”\nresult that’s returned in the case when the key isn’t present:\nmapped_type get_value(key_type const& key, mapped_type default_value);\nIn this case, a default-constructed instance of mapped_type could be used if the\ndefault_value wasn’t explicitly provided. This could also be extended to return an\nstd::pair<mapped_type,bool> instead of just an instance of mapped_type, where\nthe bool indicates whether the value was present. Another option is to return a\nsmart pointer referring to the value; if the pointer value is NULL, there was no value\nto return.\n As already mentioned, once the interface has been decided, then (assuming no\ninterface race conditions) the thread safety could be guaranteed by using a single\nmutex and a simple lock around every member function to protect the underlying\ndata structure. But this would squander the possibilities for concurrency provided by\nthe separate functions for reading the data structure and modifying it. One option is\nto use a mutex that supports multiple reader threads or a single writer thread, such as\n",
      "content_length": 3045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "196\nCHAPTER 6\nDesigning lock-based concurrent data structures\nstd::shared_mutex used in listing 3.13. Although this would indeed improve the pos-\nsibilities for concurrent access, only one thread could modify the data structure at a\ntime. Ideally, you’d like to do better than that.\nDESIGNING A MAP DATA STRUCTURE FOR FINE-GRAINED LOCKING\nAs with the queue discussed in section 6.2.3, in order to permit fine-grained locking\nyou need to look carefully at the details of the data structure rather than wrapping a\npre-existing container such as std::map<>. There are three common ways of imple-\nmenting an associative container like your lookup table:\nA binary tree, such as a red-black tree\nA sorted array\nA hash table\nA binary tree doesn’t provide much scope for extending the opportunities for concur-\nrency; every lookup or modification has to start by accessing the root node, which\ntherefore has to be locked. Although this lock can be released as the accessing thread\nmoves down the tree, this isn’t much better than a single lock across the whole data\nstructure.\n A sorted array is even worse, because you can’t tell in advance where in the array a\ngiven data value is going to be, so you need a single lock for the whole array.\n That leaves the hash table. Assuming a fixed number of buckets, which bucket a\nkey belongs to is purely a property of the key and its hash function. This means you\ncan safely have a separate lock per bucket. If you again use a mutex that supports mul-\ntiple readers or a single writer, you increase the opportunities for concurrency N-fold,\nwhere N is the number of buckets. The downside is that you need a good hash func-\ntion for the key. The C++ Standard Library provides the std::hash<> template, which\nyou can use for this purpose. It’s already specialized for fundamental types such as int\nand common library types such as std::string, and the user can easily specialize it\nfor other key types. If you follow the lead of the standard unordered containers and\ntake the type of the function object to use for doing the hashing as a template param-\neter, the user can choose whether to specialize std::hash<> for their key type or pro-\nvide a separate hash function.\n So, let’s look at some code. What might the implementation of a thread-safe\nlookup table look like? One possibility is shown here.\ntemplate<typename Key,typename Value,typename Hash=std::hash<Key> >\nclass threadsafe_lookup_table\n{\nprivate:\n    class bucket_type\n    {\n    private:\n        typedef std::pair<Key,Value> bucket_value;\n        typedef std::list<bucket_value> bucket_data;\nListing 6.11\nA thread-safe lookup table\n",
      "content_length": 2635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "197\nDesigning more complex lock-based data structures\n        typedef typename bucket_data::iterator bucket_iterator;\n        bucket_data data;\n        mutable std::shared_mutex mutex;    \n        \n        bucket_iterator find_entry_for(Key const& key) const    \n        {\n            return std::find_if(data.begin(),data.end(),\n                                [&](bucket_value const& item)\n                                {return item.first==key;});\n        }\n    public:\n        Value value_for(Key const& key,Value const& default_value) const\n        {\n            std::shared_lock<std::shared_mutex> lock(mutex);     \n            bucket_iterator const found_entry=find_entry_for(key);\n            return (found_entry==data.end())?\n                default_value:found_entry->second;\n        }\n        void add_or_update_mapping(Key const& key,Value const& value)\n        {\n            std::unique_lock<std::shared_mutex> lock(mutex);      \n            bucket_iterator const found_entry=find_entry_for(key);\n            if(found_entry==data.end())\n            {\n                data.push_back(bucket_value(key,value));\n            }\n            else\n            {\n                found_entry->second=value;\n            }\n        }\n        void remove_mapping(Key const& key)\n        {\n            std::unique_lock<std::shared_mutex> lock(mutex);      \n            bucket_iterator const found_entry=find_entry_for(key);\n            if(found_entry!=data.end())\n            {\n                data.erase(found_entry);\n            }\n        }\n    };\n    std::vector<std::unique_ptr<bucket_type> > buckets;    \n    Hash hasher;\n    bucket_type& get_bucket(Key const& key) const    \n    {\n        std::size_t const bucket_index=hasher(key)%buckets.size();\n        return *buckets[bucket_index];\n    }\npublic:\n    typedef Key key_type;\n    typedef Value mapped_type;\n    typedef Hash hash_type;\n    threadsafe_lookup_table(\n        unsigned num_buckets=19,Hash const& hasher_=Hash()):\n        buckets(num_buckets),hasher(hasher_)\nb\nc\nd\ne\nf\ng\nh\n",
      "content_length": 2039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "198\nCHAPTER 6\nDesigning lock-based concurrent data structures\n    {\n        for(unsigned i=0;i<num_buckets;++i)\n        {\n            buckets[i].reset(new bucket_type);\n        }\n    }\n    threadsafe_lookup_table(threadsafe_lookup_table const& other)=delete;\n    threadsafe_lookup_table& operator=(\n        threadsafe_lookup_table const& other)=delete;\n    Value value_for(Key const& key,\n                    Value const& default_value=Value()) const\n    {\n        return get_bucket(key).value_for(key,default_value);    \n    }\n    void add_or_update_mapping(Key const& key,Value const& value)\n    {\n        get_bucket(key).add_or_update_mapping(key,value);    \n    }\n    void remove_mapping(Key const& key)\n    {\n        get_bucket(key).remove_mapping(key);    \n    }\n};\nThis implementation uses a std::vector<std::unique_ptr<bucket_type>> g to\nhold the buckets, which allows the number of buckets to be specified in the con-\nstructor. The default is 19, which is an arbitrary prime number; hash tables work\nbest with a prime number of buckets. Each bucket is protected with an instance of\nstd::shared_mutex B to allow many concurrent reads or a single call to either of the\nmodification functions per bucket.\n Because the number of buckets is fixed, the get_bucket() function h can be\ncalled without any locking (i, j, and 1)), and then the bucket mutex can be locked\neither for shared (read-only) ownership d, or unique (read/write) ownership, e\nand f, as appropriate for each function.\n All three functions make use of the find_entry_for() member function c on the\nbucket to determine whether the entry is in the bucket. Each bucket contains just an\nstd::list<> of key/value pairs, so adding and removing entries is easy.\n I’ve already covered the concurrency angle, and everything is suitably protected\nwith mutex locks, so what about exception safety? value_for doesn’t modify anything,\nso that’s fine; if it throws an exception, it won’t affect the data structure. remove_mapping\nmodifies the list with the call to erase, but this is guaranteed not to throw, so that’s\nsafe. This leaves add_or_update_mapping, which might throw in either of the two\nbranches of if. push_back is exception-safe and will leave the list in the original state\nif it throws, so that branch is fine. The only problem is with the assignment in the case\nwhere you’re replacing an existing value; if the assignment throws, you’re relying on it\nleaving the original unchanged. But this doesn’t affect the data structure as a whole\nand is entirely a property of the user-supplied type, so you can safely leave it up to the\nuser to handle this.\ni\nj\n1)\n",
      "content_length": 2630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "199\nDesigning more complex lock-based data structures\n At the beginning of this section, I mentioned that one nice-to-have feature of such\na lookup table would be the option of retrieving a snapshot of the current state into,\nfor example, a std::map<>. This would require locking the entire container in order\nto ensure that a consistent copy of the state is retrieved, which requires locking all the\nbuckets. Because the “normal” operations on the lookup table require a lock on only\none bucket at a time, this would be the only operation that requires a lock on all the\nbuckets. Therefore, provided you lock them in the same order every time (for exam-\nple, increasing bucket index), there’ll be no opportunity for deadlock. This imple-\nmentation is shown in the following listing.\nstd::map<Key,Value> threadsafe_lookup_table::get_map() const\n{\n    std::vector<std::unique_lock<std::shared_mutex> > locks;\n    for(unsigned i=0;i<buckets.size();++i)\n    {\n        locks.push_back(\n            std::unique_lock<std::shared_mutex>(buckets[i].mutex));\n    }\n    std::map<Key,Value> res;\n    for(unsigned i=0;i<buckets.size();++i)\n    {\n        for(bucket_iterator it=buckets[i].data.begin();\n            it!=buckets[i].data.end();\n            ++it)\n        {\n            res.insert(*it);\n        }\n    }\n    return res;\n}\nThe lookup table implementation from listing 6.11 increases the opportunity for con-\ncurrency of the lookup table as a whole by locking each bucket separately and by\nusing a std::shared_mutex to allow reader concurrency on each bucket. But what if\nyou could increase the potential for concurrency on a bucket by even finer-grained\nlocking? In the next section, you’ll do exactly that by using a thread-safe list container\nwith iterator support.\n6.3.2\nWriting a thread-safe list using locks\nA list is one of the most basic data structures, so it should be straightforward to write a\nthread-safe one, shouldn’t it? Well, that depends on what facilities you’re after, and\nyou need one that offers iterator support, something I shied away from adding to your\nmap on the basis that it was too complicated. The basic issue with STL-style iterator\nsupport is that the iterator must hold some kind of reference into the internal data\nstructure of the container. If the container can be modified from another thread, this\nreference must somehow remain valid, which requires that the iterator hold a lock on\nListing 6.12\nObtaining contents of a threadsafe_lookup_table as std::map<>\n",
      "content_length": 2493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "200\nCHAPTER 6\nDesigning lock-based concurrent data structures\nsome part of the structure. Given that the lifetime of an STL-style iterator is com-\npletely outside the control of the container, this is a bad idea.\n The alternative is to provide iteration functions such as for_each as part of the\ncontainer itself. This puts the container squarely in charge of the iteration and lock-\ning, but it does fall foul of the deadlock avoidance guidelines from chapter 3. In order\nfor for_each to do anything useful, it must call user-supplied code while holding the\ninternal lock. Not only that, but it must also pass a reference to each item to this user-\nsupplied code in order for the user-supplied code to work on this item. You could\navoid this by passing a copy of each item to the user-supplied code, but that would be\nexpensive if the data items were large.\n So, for now you’ll leave it up to the user to ensure that they don’t cause deadlock\nby acquiring locks in the user-supplied operations and don’t cause data races by stor-\ning the references for access outside the locks. In the case of the list being used by\nthe lookup table, this is perfectly safe, because you know you’re not going to do any-\nthing naughty.\n That leaves you with the question of which operations to supply for your list. If\nyou cast your eyes back to listings 6.11 and 6.12, you can see the sorts of operations\nyou require:\nAdd an item to the list.\nRemove an item from the list if it meets a certain condition.\nFind an item in the list that meets a certain condition.\nUpdate an item that meets a certain condition.\nCopy each item in the list to another container.\nFor this to be a good general-purpose list container, it would be helpful to add further\noperations, such as a positional insert, but this is unnecessary for your lookup table, so\nI’ll leave it as an exercise for the reader.\n The basic idea with fine-grained locking for a linked list is to have one mutex per\nnode. If the list gets big, that’s a lot of mutexes! The benefit here is that operations on\nseparate parts of the list are truly concurrent: each operation holds only the locks on\nthe nodes it’s interested in and unlocks each node as it moves on to the next. The\nnext listing shows an implementation of this list.\ntemplate<typename T>\nclass threadsafe_list\n{\n    struct node      \n    {\n        std::mutex m;\n        std::shared_ptr<T> data;\n        std::unique_ptr<node> next;\n        node():        \n            next()\nListing 6.13\nA thread-safe list with iteration support\nb\nc\n",
      "content_length": 2538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "201\nDesigning more complex lock-based data structures\n        {}\n        node(T const& value):              \n            data(std::make_shared<T>(value))\n        {}\n    };\n    node head;\npublic:\n    threadsafe_list()\n    {}\n    ~threadsafe_list()\n    {\n        remove_if([](node const&){return true;});\n    }\n    threadsafe_list(threadsafe_list const& other)=delete;\n    threadsafe_list& operator=(threadsafe_list const& other)=delete;\n    void push_front(T const& value)\n    {\n        std::unique_ptr<node> new_node(new node(value));   \n        std::lock_guard<std::mutex> lk(head.m);\n        new_node->next=std::move(head.next);    \n        head.next=std::move(new_node);     \n    }\n    template<typename Function>\n    void for_each(Function f)    \n    {\n        node* current=&head;\n        std::unique_lock<std::mutex> lk(head.m);   \n        while(node* const next=current->next.get())     \n        {\n            std::unique_lock<std::mutex> next_lk(next->m);    \n            lk.unlock();                                 \n            f(*next->data);     \n            current=next;\n            lk=std::move(next_lk);    \n        }\n    }\n    template<typename Predicate>\n    std::shared_ptr<T> find_first_if(Predicate p)     \n    {\n        node* current=&head;\n        std::unique_lock<std::mutex> lk(head.m);\n        while(node* const next=current->next.get())\n        {\n            std::unique_lock<std::mutex> next_lk(next->m);\n            lk.unlock();\n            if(p(*next->data))    \n            {\n                return next->data;    \n            }\n            current=next;\n            lk=std::move(next_lk);\n        }\n        return std::shared_ptr<T>();\n    }\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n1@\n1#\n1$\n1%\n1^\n",
      "content_length": 1709,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "202\nCHAPTER 6\nDesigning lock-based concurrent data structures\n    template<typename Predicate>\n    void remove_if(Predicate p)    \n    {\n        node* current=&head;\n        std::unique_lock<std::mutex> lk(head.m);\n        while(node* const next=current->next.get())\n        {\n            std::unique_lock<std::mutex> next_lk(next->m);\n            if(p(*next->data))                            \n            {\n                std::unique_ptr<node> old_next=std::move(current->next);\n                current->next=std::move(next->next);    \n                next_lk.unlock();\n            }                   \n            else\n            {\n                lk.unlock();     \n                current=next;\n                lk=std::move(next_lk);\n            }\n        }\n    }\n};\nThe threadsafe_list<> from listing 6.13 is a singly linked list, where each entry is a\nnode structure B. A default-constructed node is used for the head of the list, which\nstarts with a NULL next pointer c. New nodes are added with the push_front() func-\ntion; first a new node is constructed e, which allocates the stored data on the heap\nd, while leaving the next pointer as NULL. You then need to acquire the lock on the\nmutex for the head node in order to get the appropriate next value f and insert the\nnode at the front of the list by setting head.next to point to your new node g. So far,\nso good: you only need to lock one mutex in order to add a new item to the list, so\nthere’s no risk of deadlock. Also, the slow memory allocation happens outside the\nlock, so the lock is only protecting the update of a couple of pointer values that can’t\nfail. On to the iterative functions.\n First up, let’s look at for_each() h. This operation takes a Function of some type\nto apply to each element in the list; in common with most standard library algorithms,\nit takes this function by value and will work with either a genuine function or an\nobject of a type with a function call operator. In this case, the function must accept a\nvalue of type T as the sole parameter. Here’s where you do the hand-over-hand lock-\ning. To start with, you lock the mutex on the head node i. It’s then safe to obtain the\npointer to the next node (using get() because you’re not taking ownership of the\npointer). If that pointer isn’t NULL j, you lock the mutex on that node 1) in order to\nprocess the data. Once you have the lock on that node, you can release the lock on\nthe previous node 1! and call the specified function 1@. Once the function completes,\nyou can update the current pointer to the node you processed and move the owner-\nship of the lock from next_lk out to lk 1#. Because for_each passes each data item\n1&\n1*\n1(\n2)\n2!\n",
      "content_length": 2692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "203\nSummary\ndirectly to the supplied Function, you can use this to update the items if necessary,\nor copy them into another container, or whatever. This is entirely safe if the func-\ntion is well behaved, because the mutex for the node holding the data item is held\nacross the call.\n find_first_if() 1$ is similar to for_each(); the crucial difference is that the\nsupplied Predicate must return true to indicate a match or false to indicate no\nmatch 1%. Once you have a match, you return the found data 1^, rather than continu-\ning to search. You could do this with for_each(), but it would needlessly continue\nprocessing the rest of the list even once a match had been found.\n remove_if() 1& is slightly different, because this function has to update the list;\nyou can’t use for_each() for this. If the Predicate returns true 1*, you remove the\nnode from the list by updating current->next 1(. Once you’ve done that, you can\nrelease the lock held on the mutex for the next node. The node is deleted when the\nstd::unique_ptr<node> you moved it into goes out of scope 2). In this case, you don’t\nupdate current because you need to check the new next node. If the Predicate\nreturns false, you want to move on as before 2!.\n So, are there any deadlocks or race conditions with all these mutexes? The answer\nhere is quite definitely no, provided that the supplied predicates and functions are\nwell behaved. The iteration is always one way, always starting from the head node, and\nalways locking the next mutex before releasing the current one, so there’s no possibil-\nity of different lock orders in different threads. The only potential candidate for a\nrace condition is the deletion of the removed node in remove_if() 2), because you\ndo this after you’ve unlocked the mutex (it’s undefined behavior to destroy a locked\nmutex). But a few moments’ thought reveals that this is indeed safe, because you still\nhold the mutex on the previous node (current), so no new thread can try to acquire\nthe lock on the node you’re deleting.\n What about opportunities for concurrency? The whole point of this fine-grained\nlocking was to improve the possibilities for concurrency over a single mutex, so have\nyou achieved that? Yes, you have: different threads can be working on different nodes\nin the list at the same time, whether they’re processing each item with for_each(),\nsearching with find_first_if(), or removing items with remove_if(). But because\nthe mutex for each node must be locked in turn, the threads can’t pass each other. If\none thread is spending a long time processing a particular node, other threads will\nhave to wait when they reach that particular node.\nSummary\nThis chapter started by looking at what it means to design a data structure for con-\ncurrency and providing some guidelines for doing so. We then worked through sev-\neral common data structures (stack, queue, hash map, and linked list), looking at\nhow to apply those guidelines to implement them in a way designed for concurrent\naccess, using locks to protect the data and prevent data races. You should now be\n",
      "content_length": 3081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "204\nCHAPTER 6\nDesigning lock-based concurrent data structures\nable to look at the design of your own data structures to see where the opportunities\nfor concurrency lie and where there’s potential for race conditions.\n In chapter 7 we’ll look at ways of avoiding locks entirely, using the low-level atomic\noperations to provide the necessary ordering constraints, while sticking to the same\nset of guidelines.\n",
      "content_length": 409,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "205\nDesigning lock-free\nconcurrent data structures\nIn the last chapter we looked at general aspects of designing data structures for\nconcurrency, with guidelines for thinking about the design to ensure they’re safe.\nWe then examined several common data structures and looked at example imple-\nmentations that used mutexes and locks to protect the shared data. The first cou-\nple of examples used one mutex to protect the entire data structure, but later ones\nused more than one to protect various smaller parts of the data structure and allow\ngreater levels of concurrency in accesses to the data structure.\n Mutexes are powerful mechanisms for ensuring that multiple threads can safely\naccess a data structure without encountering race conditions or broken invariants.\nIt’s also relatively straightforward to reason about the behavior of code that uses\nthem: either the code has the lock on the mutex protecting the data or it doesn’t.\nBut it’s not all a bed of roses; you saw in chapter 3 how the incorrect use of locks\nThis chapter covers\nImplementations of data structures designed for \nconcurrency without using locks\nTechniques for managing memory in lock-free \ndata structures\nSimple guidelines to aid in the writing of lock-free \ndata structures\n",
      "content_length": 1257,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "206\nCHAPTER 7\nDesigning lock-free concurrent data structures\ncan lead to deadlock, and you’ve seen with the lock-based queue and lookup table\nexamples how the granularity of locking can affect the potential for true concurrency.\nIf you can write data structures that are safe for concurrent access without locks,\nthere’s the potential to avoid these problems. This data structure is called a lock-free\ndata structure.\n In this chapter we’ll look at how the memory-ordering properties of the atomic\noperations introduced in chapter 5 can be used to build lock-free data structures. It is\nvital for the understanding of this chapter that you have read and understood all of\nchapter 5. You need to take extreme care when designing these data structures,\nbecause they’re hard to get right, and the conditions that cause the design to fail may\noccur very rarely. We’ll start by looking at what it means for data structures to be lock-\nfree; then we’ll move on to the reasons for using them before working through some\nexamples and drawing out some general guidelines.\n7.1\nDefinitions and consequences\nAlgorithms and data structures that use mutexes, condition variables, and futures to\nsynchronize the data are called blocking data structures and algorithms. The applica-\ntion calls library functions that will suspend the execution of a thread until another\nthread performs an action. These library calls are termed blocking calls because the\nthread can’t progress past this point until the block is removed. Typically, the OS will\nsuspend a blocked thread completely (and allocate its time slices to another thread)\nuntil it’s unblocked by the appropriate action of another thread, whether that’s unlock-\ning a mutex, notifying a condition variable, or making a future ready.\n Data structures and algorithms that don’t use blocking library functions are said to\nbe nonblocking. Not all these data structures are lock-free, though, so let’s look at the\nvarious types of nonblocking data structures.\n7.1.1\nTypes of nonblocking data structures\nBack in chapter 5, we implemented a basic mutex using std::atomic_flag as a spin\nlock. The code is reproduced in the following listing.\nclass spinlock_mutex\n{\n    std::atomic_flag flag;\npublic:\n    spinlock_mutex():\n        flag(ATOMIC_FLAG_INIT)\n    {}\n    void lock()\n    {\n        while(flag.test_and_set(std::memory_order_acquire));\n    }\n    void unlock()\n    {\nListing 7.1\nImplementation of a spin-lock mutex using std::atomic_flag\n",
      "content_length": 2476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "207\nDefinitions and consequences\n        flag.clear(std::memory_order_release);\n    }\n};\nThis code doesn’t call any blocking functions; lock() keeps looping until the call to\ntest_and_set() returns false. This is why it gets the name spin lock—the code “spins”\naround the loop. There are no blocking calls, so any code that uses this mutex to pro-\ntect shared data is consequently nonblocking. It’s not lock-free, though. It’s still a mutex\nand can still be locked by only one thread at a time. For that reason, knowing something\nis nonblocking is not enough in most circumstances. Instead, you need to know which\n(if any) of the more specific terms defined here apply. These are\nObstruction-Free—If all other threads are paused, then any given thread will com-\nplete its operation in a bounded number of steps.\nLock-Free—If multiple threads are operating on a data structure, then after a\nbounded number of steps one of them will complete its operation.\nWait-Free—Every thread operating on a data structure will complete its opera-\ntion in a bounded number of steps, even if other threads are also operating on\nthe data structure.\nFor the most part, obstruction-free algorithms aren't particularly useful—it's not often\nthat all other threads are paused, so this is more useful as a characterization of a failed\nlock-free implementation. Let’s look more at what's involved in these characteriza-\ntions, starting with lock-free so you can see what kinds of data structures are covered.\n7.1.2\nLock-free data structures\nFor a data structure to qualify as lock-free, more than one thread must be able to\naccess the data structure concurrently. They don’t have to be able to do the same\noperations; a lock-free queue might allow one thread to push and one to pop but\nbreak if two threads try to push new items at the same time. Not only that, but if one of\nthe threads accessing the data structure is suspended by the scheduler midway\nthrough its operation, the other threads must still be able to complete their opera-\ntions without waiting for the suspended thread.\n Algorithms that use compare/exchange operations on the data structure often\nhave loops in them. The reason for using a compare/exchange operation is that\nanother thread might have modified the data in the meantime, in which case the code\nwill need to redo part of its operation before trying the compare/exchange again.\nThis code can still be lock-free if the compare/exchange would eventually succeed if\nthe other threads were suspended. If it didn’t, you’d have a spin lock, which is non-\nblocking but not lock-free.\n Lock-free algorithms with these loops can result in one thread being subject to star-\nvation. If another thread performs operations with the “wrong” timing, the other\nthread might make progress but the first thread continually has to retry its operation.\nData structures that avoid this problem are wait-free as well as lock-free.\n",
      "content_length": 2919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "208\nCHAPTER 7\nDesigning lock-free concurrent data structures\n7.1.3\nWait-free data structures\nA wait-free data structure is a lock-free data structure with the additional property that\nevery thread accessing the data structure can complete its operation within a bounded\nnumber of steps, regardless of the behavior of other threads. Algorithms that can\ninvolve an unbounded number of retries because of clashes with other threads are not\nwait-free. Most of the examples in this chapter have that property—they have a while\nloop on a compare_exchange_weak or compare_exchange_strong operation, with no\nupper bound on the number of times the loop can run. The scheduling of threads by\nthe OS may mean that a given thread can loop an exceedingly large number of times,\nbut other threads loop very few times. These operations are thus not wait-free.\n Writing wait-free data structures correctly is extremely hard. In order to ensure\nthat every thread can complete its operations within a bounded number of steps,\nyou have to ensure that each operation can be performed in a single pass and that\nthe steps performed by one thread don’t cause an operation on another thread to\nfail. This can make the overall algorithms for the various operations considerably\nmore complex.\n Given how hard it is to get a lock-free or wait-free data structure right, you need\nsome pretty good reasons to write one; you need to be sure that the benefit outweighs\nthe cost. Let’s therefore examine the points that affect the balance.\n7.1.4\nThe pros and cons of lock-free data structures\nWhen it comes down to it, the primary reason for using lock-free data structures is to\nenable maximum concurrency. With lock-based containers, there’s always the poten-\ntial for one thread to have to block and wait for another to complete its operation\nbefore the first thread can proceed; preventing concurrency through mutual exclu-\nsion is the entire purpose of a mutex lock. With a lock-free data structure, some thread\nmakes progress with every step. With a wait-free data structure, every thread can make\nforward progress, regardless of what the other threads are doing; there’s no need for\nwaiting. This is a desirable property to have but hard to achieve. It’s all too easy to end\nup writing what’s essentially a spin lock.\n A second reason to use lock-free data structures is robustness. If a thread dies while\nholding a lock, that data structure is broken forever. But if a thread dies partway\nthrough an operation on a lock-free data structure, nothing is lost except that thread’s\ndata; other threads can proceed normally.\n The flip side here is that if you can’t exclude threads from accessing the data struc-\nture, then you must be careful to ensure that the invariants are upheld or choose\nalternative invariants that can be upheld. Also, you must pay attention to the ordering\nconstraints you impose on the operations. To avoid the undefined behavior associated\nwith a data race, you must use atomic operations for the modifications. But that alone\nisn’t enough; you must ensure that changes become visible to other threads in the cor-\nrect order. All this means that writing thread-safe data structures without using locks is\nconsiderably harder than writing them with locks.\n",
      "content_length": 3254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "209\nExamples of lock-free data structures\n Because there aren’t any locks, deadlocks are impossible with lock-free data struc-\ntures, although there is the possibility of live locks instead. A live lock occurs when two\nthreads each try to change the data structure, but for each thread, the changes made\nby the other require the operation to be restarted, so both threads loop and try again.\nImagine two people trying to go through a narrow gap. If they both go at once, they\nget stuck, so they have to come out and try again. Unless someone gets there first\n(either by agreement, by being faster, or by sheer luck), the cycle will repeat. As in this\nsimple example, live locks are typically short-lived because they depend on the exact\nscheduling of threads. They therefore sap performance rather than cause long-term\nproblems, but they’re still something to watch out for. By definition, wait-free code\ncan’t suffer from live lock because there’s always an upper limit on the number of\nsteps needed to perform an operation. The flip side here is that the algorithm is likely\nmore complex than the alternative and may require more steps even when no other\nthread is accessing the data structure.\n This brings us to another downside of lock-free and wait-free code: although it can\nincrease the potential for concurrency of operations on a data structure and reduce\nthe time an individual thread spends waiting, it may well decrease overall performance.\nFirst, the atomic operations used for lock-free code can be much slower than non-\natomic operations, and there’ll likely be more of them in a lock-free data structure than\nin the mutex locking code for a lock-based data structure. Not only that, but the hard-\nware must synchronize data between threads that access the same atomic variables. As\nyou’ll see in chapter 8, the cache ping-pong associated with multiple threads accessing\nthe same atomic variables can be a significant performance drain. As with everything,\nit’s important to check the relevant performance aspects (whether that’s worst-case wait\ntime, average wait time, overall execution time, or something else) both with a lock-\nbased data structure and a lock-free one before committing either way.\n Now let’s look at some examples.\n7.2\nExamples of lock-free data structures\nIn order to demonstrate some of the techniques used in designing lock-free data\nstructures, we’ll look at the lock-free implementation of a series of simple data struc-\ntures. Not only will each example describe the implementation of a useful data\nstructure, but I’ll use the examples to highlight particular aspects of lock-free data\nstructure design.\n As already mentioned, lock-free data structures rely on the use of atomic opera-\ntions and the associated memory-ordering guarantees in order to ensure that data\nbecomes visible to other threads in the correct order. Initially, we’ll use the default\nmemory_order_seq_cst memory ordering for all atomic operations, because that’s the\neasiest to reason about (remember that all memory_order_seq_cst operations form a\ntotal order). But for later examples we’ll look at reducing some of the ordering con-\nstraints to memory_order_acquire, memory_order_release, or even memory_order_\nrelaxed. Although none of these examples use mutex locks directly, it’s worth bearing\n",
      "content_length": 3314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "210\nCHAPTER 7\nDesigning lock-free concurrent data structures\nin mind that only std::atomic_flag is guaranteed not to use locks in the implemen-\ntation. On some platforms, what appears to be lock-free code might be using locks\ninternal to the C++ Standard Library implementation (see chapter 5 for more details).\nOn these platforms, a simple lock-based data structure might be more appropriate,\nbut there’s more to it than that; before choosing an implementation, you must iden-\ntify your requirements and profile the various options that meet those requirements.\n So, back to the beginning with the simplest of data structures: a stack.\n7.2.1\nWriting a thread-safe stack without locks\nThe basic premise of a stack is relatively simple: nodes are retrieved in the reverse\norder to which they were added—last in, first out (LIFO). It’s therefore important\nto ensure that once a value is added to the stack, it can safely be retrieved immedi-\nately by another thread, and it’s also important to ensure that only one thread\nreturns a given value. The simplest stack is a linked list; the head pointer identifies\nthe first node (which will be the next to retrieve), and each node then points to the\nnext node in turn.\n Under this scheme, adding a node is relatively simple:\n1\nCreate a new node.\n2\nSet its next pointer to the current head node.\n3\nSet the head node to point to it.\nThis works fine in a single-threaded context, but if other threads are also modifying\nthe stack, it’s not enough. Crucially, if two threads are adding nodes, there’s a race\ncondition between steps 2 and 3: a second thread could modify the value of head\nbetween when your thread reads it in step 2 and you update it in step 3. This would\nthen result in the changes made by that other thread being discarded or something\neven worse. Before we look at addressing this race condition, it’s also important to\nnote that once head has been updated to point to your new node, another thread\ncould read that node. It’s therefore vital that your new node is thoroughly prepared\nbefore head is set to point to it; you can’t modify the node afterward.\n OK, so what can you do about this nasty race condition? The answer is to use an\natomic compare/exchange operation at step 3 to ensure that head hasn’t been modi-\nfied since you read it in step 2. If it has, you can loop and try again. The following list-\ning shows how you can implement a thread-safe push() without locks.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node\n    {\n        T data;\n        node* next;\nListing 7.2\nImplementing push() without locks\n",
      "content_length": 2597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "211\nExamples of lock-free data structures\n        node(T const& data_):    \n            data(data_)\n        {}\n    };\n    std::atomic<node*> head;\npublic:\n    void push(T const& data)\n    {\n        node* const new_node=new node(data);   \n        new_node->next=head.load();                     \n        while(!head.compare_exchange_weak(new_node->next,new_node));   \n    }\n};\nThis code neatly matches the preceding three-point plan: create a new node c, set\nthe node’s next pointer to the current head d, and set the head pointer to the new\nnode e. By populating the data in the node structure itself from the node construc-\ntor B, you’ve ensured that the node is ready to roll as soon as it’s constructed, so\nthat’s the easy problem solved. Then you use compare_exchange_weak() to ensure\nthat the head pointer still has the same value as you stored in new_node->next d, and\nyou set it to new_node if so. This bit of code also uses a nifty part of the com-\npare/exchange functionality: if it returns false to indicate that the comparison\nfailed (for example, because head was modified by another thread), the value sup-\nplied as the first parameter (new_node->next) is updated to the current value of\nhead. You therefore don’t have to reload head each time through the loop, because\nthe compiler does that for you. Also, because you’re looping directly on failure, you\ncan use compare_exchange_weak, which can result in more optimal code than\ncompare_exchange_strong on some architectures (see chapter 5).\n So, you might not have a pop() operation yet, but you can quickly check push()\nfor compliance with the guidelines. The only place that can throw an exception is\nthe construction of the new node B, but this will clean up after itself, and the list\nhasn’t been modified yet, so that’s perfectly safe. Because you build the data to be\nstored as part of the node, and you use compare_exchange_weak() to update the\nhead pointer, there are no problematic race conditions here. Once the compare/\nexchange succeeds, the node is on the list and ready for the taking. There are no\nlocks, so there’s no possibility of deadlock, and your push() function passes with fly-\ning colors.\n Now that you have a means of adding data to the stack, you need a way to remove\nit. On the face of it, this is quite simple:\n1\nRead the current value of head.\n2\nRead head->next.\n3\nSet head to head->next.\n4\nReturn the data from the retrieved node.\n5\nDelete the retrieved node.\nb\nc\nd\ne\n",
      "content_length": 2463,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "212\nCHAPTER 7\nDesigning lock-free concurrent data structures\nBut in the presence of multiple threads, this isn’t so simple. If there are two threads\nremoving items from the stack, they both might read the same value of head at step 1.\nIf one thread then proceeds all the way through to step 5 before the other gets to\nstep 2, the second thread will be dereferencing a dangling pointer. This is one of the\nbiggest issues in writing lock-free code, so for now you’ll leave out step 5 and leak\nthe nodes.\n This doesn’t resolve all the problems, though. There’s another problem: if two\nthreads read the same value of head, they’ll return the same node. This violates the\nintent of the stack data structure, so you need to avoid this. You can resolve this the\nsame way you resolved the race in push(): use compare/exchange to update head. If\nthe compare/exchange fails, either a new node has been pushed on or another\nthread popped the node you were trying to pop. Either way, you need to return to step 1\n(although the compare/exchange call rereads head for you).\n Once the compare/exchange call succeeds, you know you’re the only thread that’s\npopping the given node off the stack, so you can safely execute step 4. Here’s a first try\nat pop():\ntemplate<typename T>\nclass lock_free_stack\n{\npublic:\n    void pop(T& result)\n    {\n        node* old_head=head.load();\n        while(!head.compare_exchange_weak(old_head,old_head->next));\n        result=old_head->data;\n    }\n};\nAlthough this is nice and succinct, there are still a couple of problems aside from the\nleaking node. First, it doesn’t work on an empty list: if head is a null pointer, it will\ncause undefined behavior as it tries to read the next pointer. This is easily fixed by\nchecking for nullptr in the while loop and either throwing an exception on an\nempty stack or returning a bool to indicate success or failure.\n The second problem is an exception-safety issue. When we first introduced the\nthread-safe stack back in chapter 3, you saw how returning the object by value left you\nwith an exception safety issue: if an exception is thrown when copying the return\nvalue, the value is lost. In that case, passing in a reference to the result was an accept-\nable solution because you could ensure that the stack was left unchanged if an excep-\ntion was thrown. Unfortunately, here you don’t have that luxury; you can only safely\ncopy the data once you know you’re the only thread returning the node, which means\nthe node has already been removed from the queue. Consequently, passing in the tar-\nget for the return value by reference is no longer an advantage: you might as well\nreturn by value. If you want to return the value safely, you have to use the other option\nfrom chapter 3: return a (smart) pointer to the data value.\n",
      "content_length": 2789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "213\nExamples of lock-free data structures\n If you return a smart pointer, you can return nullptr to indicate that there’s no\nvalue to return, but this requires that the data be allocated on the heap. If you do the\nheap allocation as part of pop(), you’re still no better off, because the heap allocation\nmight throw an exception. Instead, you can allocate the memory when you push() the\ndata onto the stack—you have to allocate memory for the node anyway. Returning\nstd::shared_ptr<> won’t throw an exception, so pop() is now safe. Putting all this\ntogether gives the following listing.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;   \n        node* next;\n        node(T const& data_):\n            data(std::make_shared<T>(data_))    \n        {}\n    };\n    std::atomic<node*> head;\npublic:\n    void push(T const& data)\n    {\n        node* const new_node=new node(data);\n        new_node->next=head.load();\n        while(!head.compare_exchange_weak(new_node->next,new_node));\n    }\n    std::shared_ptr<T> pop()\n    {\n        node* old_head=head.load();\n        while(old_head &&                      \n            !head.compare_exchange_weak(old_head,old_head->next));\n        return old_head ? old_head->data : std::shared_ptr<T>();     \n    }\n};\nThe data is held by the pointer now B, so you have to allocate the data on the heap in\nthe node constructor c. You also have to check for a null pointer before you derefer-\nence old_head in the compare_exchange_weak() loop d. Finally, you either return\nthe data associated with your node, if there is one, or a null pointer if not e. Note that\nalthough this is lock-free, it’s not wait-free, because the while loops in both push() and\npop() could in theory loop forever if the compare_exchange_weak() keeps failing.\n If you have a garbage collector picking up after you (like in managed languages\nsuch as C# or Java), you’re finished; the old node will be collected and recycled once\nit’s no longer being accessed by any threads. But not many C++ compilers ship with a\ngarbage collector, so you generally have to tidy up after yourself. \nListing 7.3\nA lock-free stack that leaks nodes\nData is now held \nby pointer\nb\nCreate std::shared_ptr \nfor newly allocated T\nc\nCheck old_head is not \na null pointer before \nyou dereference it\nd\ne\n",
      "content_length": 2342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "214\nCHAPTER 7\nDesigning lock-free concurrent data structures\n7.2.2\nStopping those pesky leaks: managing memory in lock-free \ndata structures\nWhen you first looked at pop(), you opted to leak nodes in order to avoid the race\ncondition where one thread deletes a node while another thread still holds a pointer\nto it that it’s about to dereference. But leaking memory isn’t acceptable in any sensible\nC++ program, so you have to do something about that. Now it’s time to look at the\nproblem and work out a solution.\n The basic problem is that you want to free a node, but you can’t do so until you’re\nsure there are no other threads that still hold pointers to it. If only one thread ever\ncalls pop() on a particular stack instance, you’re home free. Nodes are created in calls\nto push(), and push() doesn't access the contents of existing nodes, so the only\nthreads that can access a given node are the thread that added that node to the stack,\nand any threads that call pop(). push() doesn’t touch the node once it’s been added\nto the stack, so that leaves the threads that call pop()—if there's only one of them,\nthen the thread that called pop() must be the only thread that can touch the node,\nand it can safely delete it.\n On the other hand, if you need to handle multiple threads calling pop() on the\nsame stack instance, you need some way to track when it’s safe to delete a node. This\nmeans you need to write a special-purpose garbage collector for nodes. Now, this\nmight sound scary, and although it’s certainly tricky, it’s not that bad: you’re only\nchecking for nodes, and you’re only checking for nodes accessed from pop(). You’re\nnot worried about nodes in push(), because they’re only accessible from one thread\nuntil they’re on the stack, whereas multiple threads might be accessing the same node\nin pop().\n If there are no threads calling pop(), it’s perfectly safe to delete all the nodes cur-\nrently awaiting deletion. Therefore, if you add the nodes to a “to be deleted” list when\nyou’ve extracted the data, then you can delete them all when there are no threads call-\ning pop(). How do you know there aren’t any threads calling pop()? Simple—count\nthem. If you increment a counter on entry and decrement that counter on exit, it’s\nsafe to delete the nodes from the “to be deleted” list when the counter is zero. It will\nhave to be an atomic counter so it can safely be accessed from multiple threads. The\nfollowing listing shows the amended pop() function, and listing 7.5 shows the sup-\nporting functions for this implementation.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    std::atomic<unsigned> threads_in_pop;    \n    void try_reclaim(node* old_head);\npublic:\n    std::shared_ptr<T> pop()\n    {\nListing 7.4\nReclaiming nodes when no threads are in pop()\nAtomic \nvariable\nb\n",
      "content_length": 2808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "215\nExamples of lock-free data structures\n        ++threads_in_pop;            \n        node* old_head=head.load();\n        while(old_head &&\n              !head.compare_exchange_weak(old_head,old_head->next));\n        std::shared_ptr<T> res;\n        if(old_head)\n        {\n            res.swap(old_head->data);   \n        }\n        try_reclaim(old_head);   \n        return res;\n    }\n};\nThe atomic variable threads_in_pop B is used to count the threads currently trying\nto pop an item off the stack. It’s incremented at the start of pop() c, and decre-\nmented inside try_reclaim(), which is called once the node has been removed e.\nBecause you’re going to potentially delay the deletion of the node itself, you can use\nswap() to remove the data from the node d rather than copying the pointer, so that\nthe data will be deleted automatically when you no longer need it rather than it being\nkept alive because there’s still a reference in a not-yet-deleted node. The next listing\nshows what goes into try_reclaim().\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    std::atomic<node*> to_be_deleted;\n    static void delete_nodes(node* nodes)\n    {\n        while(nodes)\n        {\n            node* next=nodes->next;\n            delete nodes;\n            nodes=next;\n         }\n    }\n    void try_reclaim(node* old_head)\n    {\n        if(threads_in_pop==1)    \n        {                                         \n            node* nodes_to_delete=to_be_deleted.exchange(nullptr); \n            if(!--threads_in_pop)                \n            {\n                delete_nodes(nodes_to_delete);    \n            }\n            else if(nodes_to_delete)    \n            {\n                chain_pending_nodes(nodes_to_delete);   \n            }\nListing 7.5\nThe reference-counted reclamation machinery\nIncrease counter before \ndoing anything else\nc\nReclaim deleted \nnodes if you can\nd\nExtract data from node \nrather than copying pointer\ne\nb\nClaim list of\nto-be-deleted\nnodes\nc\nAre you the only \nthread in pop()?\nd\ne\nf\ng\n",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "216\nCHAPTER 7\nDesigning lock-free concurrent data structures\n            delete old_head;    \n        }\n        else\n        {\n            chain_pending_node(old_head);   \n            --threads_in_pop;\n        }\n    }\n    void chain_pending_nodes(node* nodes)\n    {\n        node* last=nodes;\n        while(node* const next=last->next)    \n        {\n            last=next;\n        }\n        chain_pending_nodes(nodes,last);\n    }\n    void chain_pending_nodes(node* first,node* last)\n    {\n        last->next=to_be_deleted;                   \n        while(!to_be_deleted.compare_exchange_weak(   \n                  last->next,first));\n    }\n    void chain_pending_node(node* n)\n    {\n        chain_pending_nodes(n,n);   \n    }\n};\nIf the count of threads_in_pop is 1 when you’re trying to reclaim the node B,\nyou’re the only thread currently in pop(), which means it’s safe to delete the node\nyou just removed h, and it may also be safe to delete the pending nodes. If the count\nis not 1, it’s not safe to delete any nodes, so you have to add the node to the pending\nlist i.\n Assume for a moment that threads_in_pop is 1. You now need to try to reclaim\nthe pending nodes; if you don’t, they’ll stay pending until you destroy the stack. To do\nthis, you first claim the list for yourself with an atomic exchange operation c, and\nthen decrement the count of threads_in_pop d. If the count is zero after the decre-\nment, you know that no other thread can be accessing this list of pending nodes.\nThere may be new pending nodes, but you’re not bothered about them for now, as\nlong as it’s safe to reclaim your list. You can then call delete_nodes to iterate down\nthe list and delete them e.\n If the count is not zero after the decrement, it’s not safe to reclaim the nodes, so if\nthere are any f, you must chain them back onto the list of nodes pending deletion\ng. This can happen if there are multiple threads accessing the data structure concur-\nrently. Other threads might have called pop() in between the first test of threads_\nin_pop B and the “claiming” of the list c, potentially adding new nodes to the list\nthat are still being accessed by one or more of those other threads. In figure 7.1, thread\nC adds node Y to the to_be_deleted list, even though thread B is still referencing it as\nh\ni\nFollow the next pointer \nchain to the end.\nj\n1)\nLoop to guarantee that \nlast->next is correct.\n1!\n1@\n",
      "content_length": 2394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "217\nExamples of lock-free data structures\nhead\nZ\nthreads_in_pop == 2\nThread C calls pop() and runs until pop() returns\nA\nto_be_deleted\nY\n(Threads A and B. C is done)\nX\nY\nhead\nZ\nA\nto_be_deleted\nInitially\nthreads_in_pop == 0\nY\nhead\nZ\nthreads_in_pop == 2\nold_head\nThread B calls pop() and is preempted after the first read of head\nA\nto_be_deleted\n(Threads A and B)\nY\nhead\nZ\nthreads_in_pop == 1\nA\nto_be_deleted\n(Thread A)\nThread A calls\nand is preempted in\npop()\ntry_reclaim()\nthreads_in_pop\nafter first read of\nthreads_in_pop == 2\nA\nnodes_to_delete\nY\nto_be_deleted\nnullptr\nhead\nZ\nThread A resumes and is then preempted after only executing\nto_be_deleted.exchange(nullptr)\nIf we don't test\nagainnodes Y and A will bedeleted\nthreads_in_pop\nthreads_in_pop == 2\nY\nold_head\nhead\nZ\nThread B resumes and reads\nfor the\nold_head->next\ncall\ncompare_exchange_strong()\nNode Y is on A's\nlist\nto_be_deleted\nFigure 7.1\nThree threads call pop() concurrently, showing why you \nmust check threads_in_pop after claiming the nodes to be deleted \nin try_reclaim().\n",
      "content_length": 1041,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "218\nCHAPTER 7\nDesigning lock-free concurrent data structures\nold_head, and will try and read its next pointer. Thread A can’t therefore delete the\nnodes without potentially causing undefined behavior for thread B.\n To chain the nodes that are pending deletion onto the pending list, you reuse the\nnext pointer from the nodes to link them together. In the case of relinking an existing\nchain back onto the list, you traverse the chain to find the end j, replace the next\npointer from the last node with the current to_be_deleted pointer 1), and store the\nfirst node in the chain as the new to_be_deleted pointer 1!. You have to use\ncompare_exchange_weak in a loop here in order to ensure that you don’t leak any\nnodes that have been added by another thread. This has the benefit of updating the\nnext pointer from the end of the chain if it has been changed. Adding a single node\nonto the list is a special case where the first node in the chain to be added is the same\nas the last one 1@.\n This works reasonably well in low-load situations, where there are suitable quies-\ncent points at which no threads are in pop(). But this is potentially a transient situa-\ntion, which is why you need to test that the threads_in_pop count decrements to zero\nd before doing the reclaim and why this test occurs before you delete the just-\nremoved node h. Deleting a node is potentially a time-consuming operation, and you\nwant the window in which other threads can modify the list to be as small as possible.\nThe longer the time between when the thread first finds threads_in_pop to be equal\nto 1 and the attempt to delete the nodes, the more chance there is that another\nthread has called pop(), and that threads_in_pop is no longer equal to 1, preventing\nthe nodes from being deleted.\n In high-load situations, there may never be this quiescent state, because other\nthreads have entered pop() before all the threads initially in pop() have left. Under\nthis scenario, the to_be_deleted list would grow without bounds, and you’d be leak-\ning memory again. If there aren’t going to be any quiescent periods, you need to find\nan alternative mechanism for reclaiming the nodes. The key is to identify when no\nmore threads are accessing a particular node so that it can be reclaimed. By far the\neasiest such mechanism to reason about is the use of hazard pointers.\n7.2.3\nDetecting nodes that can’t be reclaimed using hazard pointers\nThe term hazard pointers is a reference to a technique discovered by Maged Michael.1\nThey are so called because deleting a node that might still be referenced by other\nthreads is hazardous. If other threads do indeed hold references to that node and\nproceed to access the node through that reference, you have undefined behavior.\nThe basic idea is that if a thread is going to access an object that another thread\nmight want to delete, it first sets a hazard pointer to reference the object, informing\nthe other thread that deleting the object would indeed be hazardous. Once the\nobject is no longer needed, the hazard pointer is cleared. If you’ve ever watched the\n1 “Safe Memory Reclamation for Dynamic Lock-Free Objects Using Atomic Reads and Writes,” Maged M.\nMichael, in PODC ’02: Proceedings of the Twenty-first Annual Symposium on Principles of Distributed Com-\nputing (2002), ISBN 1-58113-485-1.\n",
      "content_length": 3320,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "219\nExamples of lock-free data structures\nOxford/Cambridge boat race, you’ve seen a similar mechanism used when starting\nthe race: the cox of either boat can raise their hand to indicate that they aren’t ready.\nWhile either cox has their hand raised, the umpire may not start the race. If both\ncoxes have their hands down, the race may start, but a cox may raise their hand again\nif the race hasn’t started and they feel the situation has changed.\n When a thread wants to delete an object, it must first check the hazard pointers\nbelonging to the other threads in the system. If none of the hazard pointers reference\nthe object, it can safely be deleted. Otherwise, it must be left until later. Periodically,\nthe list of objects that have been left until later is checked to see if any of them can\nnow be deleted.\n Described at such a high level, it sounds relatively straightforward, so how do you\ndo this in C++?\n Well, first off you need a location in which to store the pointer to the object you’re\naccessing, the hazard pointer itself. This location must be visible to all threads, and\nyou need one of these for each thread that might access the data structure. Allocating\nthem correctly and efficiently can be a challenge, so you’ll leave that for later and\nassume you have a function get_hazard_pointer_for_current_thread() that returns\na reference to your hazard pointer. You then need to set it when you read a pointer\nthat you intend to dereference—in this case the head value from the list:\nstd::shared_ptr<T> pop()\n{\n    std::atomic<void*>& hp=get_hazard_pointer_for_current_thread();\n    node* old_head=head.load();     \n    node* temp;\n    do\n    {\n        temp=old_head;\n        hp.store(old_head);    \n        old_head=head.load();\n    } while(old_head!=temp);    \n    // ...\n}\nYou have to do this in a while loop to ensure that the node hasn’t been deleted\nbetween the reading of the old head pointer B and the setting of the hazard pointer\nc. During this window no other thread knows you’re accessing this particular node.\nFortunately, if the old head node is going to be deleted, head itself must have changed,\nso you can check this and keep looping until you know that the head pointer still has the\nsame value you set your hazard pointer to d. Using hazard pointers like this relies on\nthe fact that it's safe to use the value of a pointer after the object it references has been\ndeleted. This is technically undefined behavior if you are using the default implementa-\ntion of new and delete, so either you need to ensure that your implementation permits\nit, or you need to use a custom allocator that permits this usage.\n Now that you’ve set your hazard pointer, you can proceed with the rest of pop(),\nsafe in the knowledge that no other thread will delete the nodes from under you.\nb\nc\nd\n",
      "content_length": 2812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "220\nCHAPTER 7\nDesigning lock-free concurrent data structures\nWell, almost: every time you reload old_head, you need to update the hazard pointer\nbefore you dereference the freshly read pointer value. Once you’ve extracted a node\nfrom the list, you can clear your hazard pointer. If there are no other hazard pointers\nreferencing your node, you can safely delete it; otherwise, you have to add it to a list of\nnodes to be deleted later. The following listing shows a full implementation of pop()\nusing this scheme.\nstd::shared_ptr<T> pop()\n{\n    std::atomic<void*>& hp=get_hazard_pointer_for_current_thread();\n    node* old_head=head.load();\n    do\n    {\n        node* temp;\n        do              \n        {\n            temp=old_head;\n            hp.store(old_head);\n            old_head=head.load();\n        } while(old_head!=temp);\n    }\n    while(old_head &&\n          !head.compare_exchange_strong(old_head,old_head->next));\n    hp.store(nullptr);              \n    std::shared_ptr<T> res;\n    if(old_head)\n    {\n        res.swap(old_head->data);\n        if(outstanding_hazard_pointers_for(old_head))  \n        {\n            reclaim_later(old_head);    \n        }\n        else\n        {\n            delete old_head;     \n        }\n        delete_nodes_with_no_hazards();    \n    }\n    return res;\n}\nFirst off, you’ve moved the loop that sets the hazard pointer inside the outer loop for\nreloading old_head if the compare/exchange fails B. You’re using compare_exchange\n_strong() here because you’re doing work inside the while loop: a spurious failure\non compare_exchange_weak() would result in resetting the hazard pointer unneces-\nsarily. This ensures that the hazard pointer is correctly set before you dereference\nold_head. Once you’ve claimed the node as yours, you can clear your hazard pointer\nc. If you did get a node, you need to check the hazard pointers belonging to other\nthreads to see if they reference it d. If so, you can’t delete it yet, so you must put it on\nListing 7.6\nAn implementation of pop() using hazard pointers\nLoop until you’ve set the \nhazard pointer to head.\nb\nClear hazard pointer \nonce you’re finished\nc\nCheck for hazard \npointers referencing \na node before you \ndelete it.\nd\ne\nf\ng\n",
      "content_length": 2219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "221\nExamples of lock-free data structures\na list to be reclaimed later e; otherwise, you can delete it right away f. Finally, you\nput in a call to check for any nodes for which you had to call reclaim_later(). If\nthere are no longer any hazard pointers referencing those nodes, you can safely\ndelete them g. Any nodes for which there are still outstanding hazard pointers will be\nleft for the next thread that calls pop().\n There’s still a lot of detail hidden in these new functions—get_hazard_pointer_\nfor_current_thread(), reclaim_later(), outstanding_hazard_pointers_for(), and\ndelete_nodes_with_no_hazards()—so let’s draw back the curtain and look at how\nthey work.\n The exact scheme for allocating hazard pointer instances to threads used by\nget_hazard_pointer_for_current_thread() doesn’t matter for the program logic\n(although it can affect the efficiency, as you’ll see later). For now you’ll go with a sim-\nple structure: a fixed-size array of pairs of thread IDs and pointers. get_hazard_\npointer_for_current_thread() then searches through the array to find the first\nfree slot and sets the ID entry of that slot to the ID of the current thread. When the\nthread exits, the slot is freed by resetting the ID entry to a default-constructed\nstd::thread::id(). This is shown in the following listing.\nunsigned const max_hazard_pointers=100;\nstruct hazard_pointer\n{\n    std::atomic<std::thread::id> id;\n    std::atomic<void*> pointer;\n};\nhazard_pointer hazard_pointers[max_hazard_pointers];\nclass hp_owner\n{\n    hazard_pointer* hp;\n     \npublic:\n    hp_owner(hp_owner const&)=delete;\n    hp_owner operator=(hp_owner const&)=delete;\n    hp_owner():                                                        \n        hp(nullptr)\n    {\n        for(unsigned i=0;i<max_hazard_pointers;++i)\n        {\n            std::thread::id old_id;                  \n            if(hazard_pointers[i].id.compare_exchange_strong(  \n                old_id,std::this_thread::get_id()))\n            {\n                hp=&hazard_pointers[i];                                \n                break;\n            }\n        }\n        if(!hp)    \n        {\nListing 7.7\nA simple implementation of get_hazard_pointer_for_current\n_thread()\nTry to claim \nownership \nof a hazard \npointer.\nb\n",
      "content_length": 2260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "222\nCHAPTER 7\nDesigning lock-free concurrent data structures\n            throw std::runtime_error(\"No hazard pointers available\");\n        }\n    }\n    std::atomic<void*>& get_pointer()\n    {\n        return hp->pointer;\n    }\n    ~hp_owner()   \n    {\n        hp->pointer.store(nullptr);\n        hp->id.store(std::thread::id());\n    }\n};\nstd::atomic<void*>& get_hazard_pointer_for_current_thread()    \n{\n    thread_local static hp_owner hazard;   \n    return hazard.get_pointer();    \n}\nThe implementation of get_hazard_pointer_for_current_thread() itself is decep-\ntively simple d: it has a thread_local variable of type hp_owner e, which stores the\nhazard pointer for the current thread. It then returns the pointer from that object f.\nThis works as follows: the first time each thread calls this function, a new instance of\nhp_owner is created. The constructor for this new instance B then searches through\nthe table of owner/pointer pairs looking for an entry without an owner. It uses com-\npare_exchange_strong() to check for an entry without an owner and claim it in one\ngo c. If the compare_exchange_strong() fails, another thread owns that entry, so\nyou move on to the next. If the exchange succeeds, you’ve successfully claimed the\nentry for the current thread, so you store it and stop the search d. If you get to the\nend of the list without finding a free entry e, there are too many threads using haz-\nard pointers, so you throw an exception.\n Once the hp_owner instance has been created for a given thread, further accesses\nare much faster because the pointer is cached, so the table doesn’t have to be\nscanned again.\n When each thread exits, if an instance of hp_owner was created for that thread, then\nit’s destroyed. The destructor then resets the pointer to nullptr before setting the\nowner ID to std::thread::id(), allowing another thread to reuse the entry later f.\n With this implementation of get_hazard_pointer_for_current_thread(), the\nimplementation of outstanding_hazard_pointers_for() is simple—scan through\nthe hazard pointer table looking for entries:\nbool outstanding_hazard_pointers_for(void* p)\n{\n    for(unsigned i=0;i<max_hazard_pointers;++i)\n    {\n        if(hazard_pointers[i].pointer.load()==p)\n        {\n            return true;\n        }\nc\nd\nEach thread has its \nown hazard pointer.\ne\nf\n",
      "content_length": 2323,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "223\nExamples of lock-free data structures\n    }\n    return false;\n}\nIt’s not even worth checking whether each entry has an owner: unowned entries\nwill have a null pointer, so the comparison will return false anyway, and it simpli-\nfies the code.\n reclaim_later() and delete_nodes_with_no_hazards() can then work on a sim-\nple linked list; reclaim_later() adds nodes to the list, and delete_nodes_with_no_\nhazards() scans through the list, deleting entries with no outstanding hazards. The\nnext listing shows this implementation.\ntemplate<typename T>\nvoid do_delete(void* p)\n{\n    delete static_cast<T*>(p);\n}\nstruct data_to_reclaim\n{\n    void* data;\n    std::function<void(void*)> deleter;\n    data_to_reclaim* next;\n    template<typename T>\n    data_to_reclaim(T* p):    \n        data(p),\n        deleter(&do_delete<T>),\n        next(0)\n    {}\n    ~data_to_reclaim()\n    {\n        deleter(data);   \n    }\n};\nstd::atomic<data_to_reclaim*> nodes_to_reclaim;\nvoid add_to_reclaim_list(data_to_reclaim* node)    \n{\n    node->next=nodes_to_reclaim.load();\n    while(!nodes_to_reclaim.compare_exchange_weak(node->next,node));\n}\ntemplate<typename T>\nvoid reclaim_later(T* data)   \n{\n    add_to_reclaim_list(new data_to_reclaim(data));   \n}\nvoid delete_nodes_with_no_hazards()\n{\n    data_to_reclaim* current=nodes_to_reclaim.exchange(nullptr);   \n    while(current)\n    {\n        data_to_reclaim* const next=current->next;\n        if(!outstanding_hazard_pointers_for(current->data))   \nListing 7.8\nA simple implementation of the reclaim functions\nb\nc\nd\ne\nf\ng\nh\n",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "224\nCHAPTER 7\nDesigning lock-free concurrent data structures\n        {\n            delete current;   \n        }\n        else\n        {\n            add_to_reclaim_list(current);   \n        }\n        current=next;\n    }\n}\nFirst off, I expect you’ve spotted that reclaim_later() is a function template rather\nthan a plain function, e. This is because hazard pointers are a general-purpose util-\nity, so you don’t want to tie yourselves to stack nodes. You’ve been using std::\natomic<void*> to store the pointers already. You therefore need to handle any\npointer type, but you can’t use void* because you want to delete the data items when\nyou can, and delete requires the real type of the pointer. The constructor of\ndata_to_reclaim handles that nicely, as you’ll see in a minute; reclaim_later() cre-\nates a new instance of data_to_reclaim for your pointer and adds it to the reclaim list\nf. add_to_reclaim_list() itself d is a simple compare_exchange_weak() loop on\nthe list head like you’ve seen before.\n Back to the constructor of data_to_reclaim B: the constructor is also a tem-\nplate. It stores the data to be deleted as a void* in the data member and then stores\na pointer to the appropriate instantiation of do_delete()—a simple function that\ncasts the supplied void* to the chosen pointer type and then deletes the pointed-to\nobject. std::function<> wraps this function pointer safely, so that the destructor of\ndata_to_reclaim can then delete the data by invoking the stored function c.\n The destructor of data_to_reclaim isn’t called when you’re adding nodes to the\nlist; it’s called when there are no more hazard pointers to that node. This is the respon-\nsibility of delete_nodes_with_no_hazards().\n delete_nodes_with_no_hazards() first claims the entire list of nodes to be\nreclaimed for itself with a simple exchange() g. This simple but crucial step ensures\nthat this is the only thread trying to reclaim this particular set of nodes. Other threads\nare now free to add further nodes to the list or even try to reclaim them without\nimpacting the operation of this thread.\n Then, as long as there are still nodes left in the list, you check each node in turn to\nsee if there are any outstanding hazard pointers h. If there aren’t, you can safely\ndelete the entry (and clean up the stored data) i. Otherwise, you add the item back\non the list for reclaiming later j.\n Although this simple implementation does indeed safely reclaim the deleted\nnodes, it adds quite a bit of overhead to the process. Scanning the hazard pointer\narray requires checking max_hazard_pointers atomic variables, and this is done for\nevery pop() call. Atomic operations are inherently slow—often 100 times slower than\nan equivalent non-atomic operation on desktop CPUs—so this makes pop() an expen-\nsive operation. Not only do you scan the hazard pointer list for the node you’re about\ni\nj\n",
      "content_length": 2876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "225\nExamples of lock-free data structures\nto remove, but you also scan it for each node in the waiting list. Clearly this is a bad\nidea. There may well be max_hazard_pointers nodes in the list, and you’re checking\nall of them against max_hazard_pointers stored hazard pointers. Ouch! There has to\nbe a better way.\nBETTER RECLAMATION STRATEGIES USING HAZARD POINTERS\nThere is a better way. What I’ve shown here is a simple and naïve implementation of\nhazard pointers to help explain the technique. The first thing you can do is trade\nmemory for performance. Rather than checking every node on the reclamation list\nevery time you call pop(), you don’t try to reclaim any nodes at all unless there are\nmore than max_hazard_pointers nodes on the list. That way you’re guaranteed to be\nable to reclaim at least one node. If you wait until there are max_hazard_pointers+1\nnodes on the list, you’re not much better off. Once you get to max_hazard_pointers\nnodes, you’ll be trying to reclaim nodes for most calls to pop(), so you’re not doing\nmuch better. But if you wait until there are 2*max_hazard_pointers nodes on the list,\nthen at most max_hazard_pointers of those will still be active, so you’re guaranteed to\nbe able to reclaim at least max_hazard_pointers nodes, and it will then be at least\nmax_hazard_pointers calls to pop() before you try to reclaim any nodes again. This is\nmuch better. Rather than checking around max_hazard_pointers nodes every call to\npush() (and not necessarily reclaiming any), you’re checking 2*max_hazard_pointers\nnodes every max_hazard_pointers calls to pop() and reclaiming at least max_hazard_\npointers nodes. That’s effectively two nodes checked for every pop(), one of which\nis reclaimed.\n Even this has a downside (beyond the increased memory usage from the larger\nreclamation list, and the larger number of potentially reclaimable nodes): you now\nhave to count the nodes on the reclamation list, which means using an atomic count,\nand you still have multiple threads competing to access the reclamation list itself. If\nyou have memory to spare, you can trade increased memory usage for an even better\nreclamation scheme: each thread keeps its own reclamation list in a thread-local vari-\nable. There’s no need for atomic variables for the count or the list access. Instead, you\nhave max_hazard_pointers*max_hazard_pointers nodes allocated. If a thread exits\nbefore all its nodes have been reclaimed, they can be stored in the global list as before\nand added to the local list of the next thread doing a reclamation process.\n Another downside of hazard pointers is that they’re covered by a patent applica-\ntion submitted by IBM.2 Though I believe this patent has now expired, if you write\nsoftware for use in a country where the patents are valid, it is a good idea to get a pat-\nent lawyer to verify that for you, or you need to make sure you have a suitable licensing\narrangement in place. This is something common to many of the lock-free memory\nreclamation techniques; this is an active research area, so large companies are taking\nout patents where they can. You may be asking why I’ve devoted so many pages to a\n2 Maged M. Michael, U.S. Patent and Trademark Office application number 20040107227, “Method for effi-\ncient implementation of dynamic lock-free data structures with safe memory reclamation.” \n",
      "content_length": 3347,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "226\nCHAPTER 7\nDesigning lock-free concurrent data structures\ntechnique that people may be unable to use, and that’s a fair question. First, it may be\npossible to use the technique without paying for a license. For example, if you’re\ndeveloping free software licensed under the GPL,3 your software may be covered by\nIBM’s statement of non-assertion.4 Second, and most important, the explanation of\nthe techniques shows some of the things that are important to think about when writ-\ning lock-free code, such as the costs of atomic operations. Finally, there is a proposal\nto incorporate hazard pointers into a future revision of the C++ Standard,5 so it is\ngood to know how they work, even if you will hopefully be able to use your compiler\nvendor’s implementation in the future.\n So, are there any unpatented memory reclamation techniques that can be used\nwith lock-free code? Luckily, there are. One such mechanism is reference counting.\n7.2.4\nDetecting nodes in use with reference counting\nBack in section 7.2.2, you saw that the problem with deleting nodes is detecting which\nnodes are still being accessed by reader threads. If you could safely identify precisely\nwhich nodes were being referenced and when no threads were accessing these nodes,\nyou could delete them. Hazard pointers tackle the problem by storing a list of the\nnodes in use. Reference counting tackles the problem by storing a count of the num-\nber of threads accessing each node.\n This may seem nice and straightforward, but it’s quite hard to manage in practice.\nAt first, you might think that something like std::shared_ptr<> would be up to the\ntask; after all, it’s a reference-counted pointer. Unfortunately, although some opera-\ntions on std::shared_ptr<> are atomic, they aren’t guaranteed to be lock-free.\nAlthough by itself this is no different than any of the operations on the atomic types,\nstd::shared_ptr<> is intended for use in many contexts, and making the atomic\noperations lock-free would likely impose an overhead on all uses of the class. If your\nplatform supplies an implementation for which std::atomic_is_lock_free(&some_\nshared_ptr) returns true, the whole memory reclamation issue goes away. Use\nstd::shared_ptr<node> for the list, as in listing 7.9. Note the need to clear the next\npointer from the popped node in order to avoid the potential for deeply nested\ndestruction of nodes when the last std::shared_ptr referencing a given node is\ndestroyed.\n \n \n \n3 GNU General Public License http://www.gnu.org/licenses/gpl.html.\n4 IBM Statement of Non-Assertion of Named Patents Against OSS, http://www.ibm.com/ibm/licensing/pat-\nents/pledgedpatents.pdf.\n5 P0566: Proposed Wording for Concurrent Data Structures: Hazard Pointer and ReadCopyUpdate (RCU),\nMichael Wong, Maged M. Michael, Paul McKenney, Geoffrey Romer, Andrew Hunter, Arthur O'Dwyer, David\nS. Hollman, JF Bastien, Hans Boehm, David Goldblatt, Frank Birbacher http://www.open-std.org/jtc1/sc22/\nwg21/docs/papers/2018/p0566r5.pdf\n",
      "content_length": 2985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "227\nExamples of lock-free data structures\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::shared_ptr<node> next;\n        node(T const& data_):\n            data(std::make_shared<T>(data_))\n        {}\n    };\n    std::shared_ptr<node> head;\npublic:\n    void push(T const& data)\n    {\n        std::shared_ptr<node> const new_node=std::make_shared<node>(data);\n        new_node->next=std::atomic_load(&head);\n        while(!std::atomic_compare_exchange_weak(&head,\n                  &new_node->next,new_node));\n    }\n    std::shared_ptr<T> pop()\n    {\n        std::shared_ptr<node> old_head=std::atomic_load(&head);\n        while(old_head && !std::atomic_compare_exchange_weak(&head,\n                  &old_head,std::atomic_load(&old_head->next)));\n        if(old_head) {\n            std::atomic_store(&old_head->next,std::shared_ptr<node>());\n            return old_head->data;\n        }\n        return std::shared_ptr<T>();\n    }\n    ~lock_free_stack(){\n        while(pop());\n    }\n};\nNot only is it rare for an implementation to provide lock-free atomic operations on\nstd::shared_ptr<>, but remembering to use the atomic operations consistently is hard.\nThe Concurrency TS helps you out, if you have an implementation available, because it\nprovides std::experimental::atomic_shared_ptr<T> in the <experimental/atomic>\nheader. This is in many ways equivalent to a theoretical std::atomic<std::shared\n_ptr<T>>, except that std::shared_ptr<T> can't be used with std::atomic<>,\nbecause it has nontrivial copy semantics to ensure that the reference count is handled\ncorrectly.  std::experimental::atomic_shared_ptr<T> handles the reference count-\ning correctly, while still ensuring atomic operations. Like the other atomic types\ndescribed in chapter 5, it may or may not be lock-free on any given implementation.\nListing 7.9 can thus be rewritten as in listing 7.10. See how much simpler it is without\nhaving to remember to include the atomic_load and atomic_store calls.\nListing 7.9\nA lock-free stack using a lock-free std::shared_ptr<> implementation\n",
      "content_length": 2121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "228\nCHAPTER 7\nDesigning lock-free concurrent data structures\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::experimental::atomic_shared_ptr<node> next;\n        node(T const& data_):\n            data(std::make_shared<T>(data_))\n        {}\n    };\n    std::experimental::atomic_shared_ptr<node> head;\npublic:\n    void push(T const& data)\n    {\n        std::shared_ptr<node> const new_node=std::make_shared<node>(data);\n        new_node->next=head.load();\n        while(!head.compare_exchange_weak(new_node->next,new_node));\n    }\n    std::shared_ptr<T> pop()\n    {\n        std::shared_ptr<node> old_head=head.load();\n        while(old_head && !head.compare_exchange_weak(\n                  old_head,old_head->next.load()));\n        if(old_head) {\n            old_head->next=std::shared_ptr<node>();\n            return old_head->data;\n        }\n        return std::shared_ptr<T>();\n    }\n    ~lock_free_stack(){\n        while(pop());\n    }\n};\nIn the probable case that your std::shared_ptr<> implementation isn’t lock-free,\nand your implementation doesn’t provide a lock-free std::experimental::atomic_\nshared_ptr<> either, you need to manage the reference counting manually.\n One possible technique involves the use of not one but two reference counts for\neach node: an internal count and an external count. The sum of these values is the\ntotal number of references to the node. The external count is kept alongside the pointer\nto the node and is increased every time the pointer is read. When the reader is fin-\nished with the node, it decreases the internal count. A simple operation that reads the\npointer will leave the external count increased by one and the internal count decreased\nby one when it’s finished.\n When the external count/pointer pairing is no longer required (the node is no\nlonger accessible from a location accessible to multiple threads), the internal count is\nListing 7.10\nStack implementation using std::experimental::atomic\n_shared_ptr<>\n",
      "content_length": 2036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "229\nExamples of lock-free data structures\nincreased by the value of the external count minus one and the external counter is\ndiscarded. Once the internal count is equal to zero, there are no outstanding refer-\nences to the node and it can be safely deleted. It’s still important to use atomic opera-\ntions for updates of shared data. Let’s now look at an implementation of a lock-free\nstack that uses this technique to ensure that the nodes are reclaimed only when it’s\nsafe to do so.\n The following listing shows the internal data structure and the implementation of\npush(), which is nice and straightforward.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node;\n    struct counted_node_ptr   \n    {\n        int external_count;\n        node* ptr;\n    };\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::atomic<int> internal_count;    \n        counted_node_ptr next;           \n        node(T const& data_):\n            data(std::make_shared<T>(data_)),\n            internal_count(0)\n        {}\n    };\n    std::atomic<counted_node_ptr> head;   \npublic:\n    ~lock_free_stack()\n    {\n        while(pop());\n    }\n    void push(T const& data)   \n    {\n        counted_node_ptr new_node;\n        new_node.ptr=new node(data);\n        new_node.external_count=1;\n        new_node.ptr->next=head.load();\n        while(!head.compare_exchange_weak(new_node.ptr->next,new_node));\n    }\n};\nFirst, the external count is wrapped together with the node pointer in the counted_\nnode_ptr structure B. This can then be used for the next pointer in the node struc-\nture, d alongside the internal count c. Because counted_node_ptr is a simple\nstruct, you can use it with the std::atomic<> template for the head of the list e.\nListing 7.11\nPushing a node on a lock-free stack using split reference counts\nb\nc\nd\ne\nf\n",
      "content_length": 1830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "230\nCHAPTER 7\nDesigning lock-free concurrent data structures\n On those platforms that support a double-word-compare-and-swap operation, this\nstructure will be small enough for std::atomic<counted_node_ptr> to be lock-free. If\nit isn’t on your platform, you might be better off using the std::shared_ptr<> ver-\nsion from listing 7.9, because std::atomic<> will use a mutex to guarantee atomicity\nwhen the type is too large for the platform’s atomic instructions (rendering your\n“lock-free” algorithm lock-based after all). Alternatively, if you’re willing to limit the\nsize of the counter, and you know that your platform has spare bits in a pointer (for\nexample, because the address space is only 48 bits but a pointer is 64 bits), you can\nstore the count inside the spare bits of the pointer to fit it all back in a single machine\nword. These tricks require platform-specific knowledge and are thus outside the scope\nof this book.\n push() is relatively simple f. You construct a counted_node_ptr that refers to a\nfreshly allocated node with associated data and set the next value of the node to the\ncurrent value of head. You can then use compare_exchange_weak() to set the value of\nhead, as in the previous listings. The counts are set up so the internal_count is zero,\nand the external_count is one. Because this is a new node, there’s currently only one\nexternal reference to the node (the head pointer itself).\n As usual, the complexities come to light in the implementation of pop(), which is\nshown in the following listing.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    // other parts as in listing 7.11\n    void increase_head_count(counted_node_ptr& old_counter)\n    {\n        counted_node_ptr new_counter;\n        do\n        {\n            new_counter=old_counter;\n            ++new_counter.external_count;\n        }\n        while(!head.compare_exchange_strong(old_counter,new_counter));  \n        old_counter.external_count=new_counter.external_count;\n    }\npublic:\n    std::shared_ptr<T> pop()#\n    {\n        counted_node_ptr old_head=head.load();\n        for(;;)\n        {\n            increase_head_count(old_head);\n            node* const ptr=old_head.ptr;    \n            if(!ptr)\n            {\n                return std::shared_ptr<T>();\n            }\nListing 7.12\nPopping a node from a lock-free stack using split reference counts\nb\nc\n",
      "content_length": 2363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "231\nExamples of lock-free data structures\n            if(head.compare_exchange_strong(old_head,ptr->next))   \n            {\n                std::shared_ptr<T> res;\n                res.swap(ptr->data);        \n                int const count_increase=old_head.external_count-2;   \n                if(ptr->internal_count.fetch_add(count_increase)==   \n                   -count_increase)\n                {\n                    delete ptr;\n                }\n                return res;    \n            }\n            else if(ptr->internal_count.fetch_sub(1)==1)\n            {\n                delete ptr;    \n            }\n        }\n    }\n};\nThis time, once you’ve loaded the value of head, you must first increase the count of\nexternal references to the head node to indicate that you’re referencing it and to\nensure that it’s safe to dereference it. If you dereference the pointer before increasing\nthe reference count, another thread could free the node before you access it, leaving\nyou with a dangling pointer. This is the primary reason for using the split reference\ncount: by incrementing the external reference count, you ensure that the pointer\nremains valid for the duration of your access. The increment is done with a compare\n_exchange_strong() loop B, which compares and sets the whole structure to ensure\nthat the pointer hasn’t been changed by another thread in the meantime.\n Once the count has been increased, you can safely dereference the ptr field of\nthe value loaded from head in order to access the pointed-to node c. If the pointer\nis a null pointer, you’re at the end of the list: no more entries. If the pointer isn’t a\nnull pointer, you can try to remove the node by a compare_exchange_strong() call\non head d.\n If the compare_exchange_strong() succeeds, you’ve taken ownership of the node\nand can swap out the data in preparation for returning it e. This ensures that the\ndata isn’t kept alive just because other threads accessing the stack happen to still have\npointers to its node. Then you can add the external count to the internal count on the\nnode with an atomic fetch_add g. If the reference count is now zero, the previous\nvalue (which is what fetch_add returns) was the negative of what you added, in which\ncase you can delete the node. It’s important to note that the value you add is two less\nthan the external count f; you’ve removed the node from the list, so you drop one\noff the count for that, and you’re no longer accessing the node from this thread, so\nyou drop another off the count for that. Whether or not you deleted the node, you’ve\nfinished, so you can return the data h.\n If the compare/exchange d fails, another thread removed your node before you\ndid, or another thread added a new node to the stack. Either way, you need to start\nd\ne\nf\ng\nh\ni\n",
      "content_length": 2791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "232\nCHAPTER 7\nDesigning lock-free concurrent data structures\nagain with the fresh value of head returned by the compare/exchange call. But first\nyou must decrease the reference count on the node you were trying to remove. This\nthread won’t access it anymore. If you’re the last thread to hold a reference (because\nanother thread removed it from the stack), the internal reference count will be 1, so\nsubtracting 1 will set the count to zero. In this case, you can delete the node here\nbefore you loop i.\n So far, you’ve been using the default std::memory_order_seq_cst memory order-\ning for all your atomic operations. On most systems these are more expensive in terms\nof execution time and synchronization overhead than the other memory orderings,\nand on some systems considerably so. Now that you have the logic of your data struc-\nture right, you can think about relaxing some of these memory-ordering require-\nments; you don’t want to impose any unnecessary overhead on the users of the stack.\nBefore leaving your stack behind and moving on to the design of a lock-free queue,\nlet’s examine the stack operations and ask ourselves, can we use more relaxed mem-\nory orderings for some operations and still get the same level of safety?\n7.2.5\nApplying the memory model to the lock-free stack\nBefore you go about changing the memory orderings, you need to examine the oper-\nations and identify the required relationships between them. You can then go back\nand find the minimum memory orderings that provide these required relationships.\nIn order to do this, you’ll have to look at the situation from the point of view of\nthreads in several different scenarios. The simplest possible scenario has to be where\none thread pushes a data item onto the stack and another thread then pops that data\nitem off the stack some time later, so we’ll start from there.\n In this simple case, three important pieces of data are involved. First is the count-\ned_node_ptr used for transferring the data: head. Second is the node structure that\nhead refers to, and third is the data item pointed to by that node.\n The thread doing the push() first constructs the data item and the node and then\nsets head. The thread doing the pop() first loads the value of head, then does a com-\npare/exchange loop on head to increase the reference count, and then reads the\nnode structure to obtain the next value. Right here you can see a required relation-\nship; the next value is a plain non-atomic object, so in order to read this safely, there\nmust be a happens-before relationship between the store (by the pushing thread) and\nthe load (by the popping thread). Because the only atomic operation in the push() is\nthe compare_exchange_weak(), and you need a release operation to get a happens-\nbefore relationship between threads, the compare_exchange_weak() must be std::\nmemory_order_release or stronger. If the compare_exchange_weak() call fails, noth-\ning has changed and you keep looping, so you need only std::memory_order_\nrelaxed in that case:\nvoid push(T const& data)\n{\n    counted_node_ptr new_node;\n    new_node.ptr=new node(data);\n",
      "content_length": 3114,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "233\nExamples of lock-free data structures\n    new_node.external_count=1;\n    new_node.ptr->next=head.load(std::memory_order_relaxed)\n    while(!head.compare_exchange_weak(new_node.ptr->next,new_node,\n        std::memory_order_release,std::memory_order_relaxed));\n}\nWhat about the pop() code? In order to get the happens-before relationship you\nneed, you must have an operation that’s std::memory_order_acquire or stronger\nbefore the access to next. The pointer you dereference to access the next field is the\nold value read by the compare_exchange_strong() in increase_head_count(), so\nyou need the ordering on that if it succeeds. As with the call in push(), if the\nexchange fails, you just loop again, so you can use relaxed ordering on failure:\nvoid increase_head_count(counted_node_ptr& old_counter)\n{\n    counted_node_ptr new_counter;\n    do\n    {\n        new_counter=old_counter;\n        ++new_counter.external_count;\n    }\n    while(!head.compare_exchange_strong(old_counter,new_counter,\n        std::memory_order_acquire,std::memory_order_relaxed));\n    old_counter.external_count=new_counter.external_count;\n}\nIf the compare_exchange_strong() call succeeds, you know that the value read had\nthe ptr field set to what’s now stored in old_counter. Because the store in push() was\na release operation, and this compare_exchange_strong() is an acquire operation,\nthe store synchronizes with the load and you have a happens-before relationship. Con-\nsequently, the store to the ptr field in the push() happens before the ptr->next\naccess in pop(), and you’re safe.\n Note that the memory ordering on the initial head.load() didn’t matter to this\nanalysis, so you can safely use std::memory_order_relaxed for that.\n Next up, let’s consider the compare_exchange_strong() to set head to old_head\n.ptr->next. Do you need anything from this operation to guarantee the data integrity\nof this thread? If the exchange succeeds, you access ptr->data, so you need to ensure\nthat the store to ptr->data in the push() thread happens before the load. But you\nalready have that guarantee: the acquire operation in increase_head_count() ensures\nthat there’s a synchronizes-with relationship between the store in the push() thread\nand that compare/exchange. Because the store to data in the push() thread is\nsequenced before the store to head and the call to increase_head_count() is sequenced\nbefore the load of ptr->data, there’s a happens-before relationship, and all is well\neven if this compare/exchange in pop() uses std::memory_order_relaxed. The only\nother place where ptr->data is changed is the call to swap() that you’re looking at,\nand no other thread can be operating on the same node; that’s the whole point of the\ncompare/exchange.\n",
      "content_length": 2734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "234\nCHAPTER 7\nDesigning lock-free concurrent data structures\n If the compare_exchange_strong() fails, the new value of old_head isn’t touched\nuntil next time around the loop, and you already decided that the std::memory_order\n_acquire in increase_head_count() was enough, so std::memory_order_relaxed is\nenough there also.\n What about other threads? Do you need anything stronger here to ensure other\nthreads are still safe? The answer is no, because head is only ever modified by com-\npare/exchange operations. Because these are read-modify-write operations, they form\npart of the release sequence headed by the compare/exchange in push(). Therefore,\nthe compare_exchange_weak() in push() synchronizes with a call to compare_exchange\n_strong() in increase_head_count(), which reads the value stored, even if many\nother threads modify head in the meantime.\n You’ve nearly finished: the only remaining operations to deal with are the\nfetch_add() operations for modifying the reference count. The thread that got to\nreturn the data from this node can proceed, safe in the knowledge that no other\nthread can have modified the node data. But any thread that did not successfully\nretrieve the data knows that another thread did modify the node data; the successful\nthread used swap() to extract the referenced data item. Therefore you need to ensure\nthat swap() happens before the delete in order to avoid a data race. The easy way to\ndo this is to make the fetch_add() in the successful-return branch use std::memory_\norder_release and the fetch_add() in the loop-again branch use std::memory_order\n_acquire. But this is still overkill: only one thread does the delete (the one that sets\nthe count to zero), so only that thread needs to do an acquire operation. Thankfully,\nbecause fetch_add() is a read-modify-write operation, it forms part of the release\nsequence, so you can do that with an additional load(). If the loop-again branch\ndecreases the reference count to zero, it can reload the reference count with\nstd::memory_order_acquire in order to ensure the required synchronizes-with rela-\ntionship, and the fetch_add() itself can use std::memory_order_relaxed. The final\nstack implementation with the new version of pop() is shown here.\ntemplate<typename T>\nclass lock_free_stack\n{\nprivate:\n    struct node;\n    struct counted_node_ptr\n    {\n        int external_count;\n        node* ptr;\n    };\n    struct node\n    {\n        std::shared_ptr<T> data;\n        std::atomic<int> internal_count;\n        counted_node_ptr next;\n        node(T const& data_):\nListing 7.13\nA lock-free stack with reference counting and relaxed atomic operations\n",
      "content_length": 2643,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "235\nExamples of lock-free data structures\n            data(std::make_shared<T>(data_)),\n            internal_count(0)\n        {}\n    };\n    std::atomic<counted_node_ptr> head;\n    void increase_head_count(counted_node_ptr& old_counter)\n    {\n        counted_node_ptr new_counter;\n        do\n        {\n            new_counter=old_counter;\n            ++new_counter.external_count;\n        }\n        while(!head.compare_exchange_strong(old_counter,new_counter,\n                                            std::memory_order_acquire,\n                                            std::memory_order_relaxed));\n        old_counter.external_count=new_counter.external_count;\n    }\npublic:\n    ~lock_free_stack()\n    {\n        while(pop());\n    }\n    void push(T const& data)\n    {\n        counted_node_ptr new_node;\n        new_node.ptr=new node(data);\n        new_node.external_count=1;\n        new_node.ptr->next=head.load(std::memory_order_relaxed)\n        while(!head.compare_exchange_weak(new_node.ptr->next,new_node,\n                                          std::memory_order_release,\n                                          std::memory_order_relaxed));\n    }\n    std::shared_ptr<T> pop()\n    {\n        counted_node_ptr old_head=\n            head.load(std::memory_order_relaxed);\n        for(;;)\n        {\n            increase_head_count(old_head);\n            node* const ptr=old_head.ptr;\n            if(!ptr)\n            {\n                return std::shared_ptr<T>();\n            }\n            if(head.compare_exchange_strong(old_head,ptr->next,\n                                            std::memory_order_relaxed))\n            {\n                std::shared_ptr<T> res;\n                res.swap(ptr->data);\n                int const count_increase=old_head.external_count-2;\n                if(ptr->internal_count.fetch_add(count_increase,\n                       std::memory_order_release)==-count_increase)\n                {\n                    delete ptr;\n",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "236\nCHAPTER 7\nDesigning lock-free concurrent data structures\n                }\n                return res;\n            }\n            else if(ptr->internal_count.fetch_add(-1,\n                        std::memory_order_relaxed)==1)\n            {\n                ptr->internal_count.load(std::memory_order_acquire);\n                delete ptr;\n            }\n        }\n    }\n};\nThat was quite a workout, but you got there in the end, and the stack is better for it. By\nusing more relaxed operations in a carefully thought-out manner, the performance is\nimproved without impacting the correctness. As you can see, the implementation of\npop() is now 37 lines rather than the 8 lines of the equivalent pop() in the lock-based\nstack of listing 6.1 and the 7 lines of the basic lock-free stack without memory manage-\nment in listing 7.2. As we move on to look at writing a lock-free queue, you’ll see a sim-\nilar pattern: lots of the complexity in lock-free code comes from managing memory.\n7.2.6\nWriting a thread-safe queue without locks\nA queue offers a slightly different challenge to a stack, because the push() and pop()\noperations access different parts of the data structure in a queue, whereas they both\naccess the same head node for a stack. Consequently, the synchronization needs are\ndifferent. You need to ensure that changes made to one end are correctly visible to\naccesses at the other. But the structure of try_pop() for the queue in listing 6.6 isn’t\nthat far off that of pop() for the simple lock-free stack in listing 7.2, so you can reason-\nably assume that the lock-free code won’t be that dissimilar. Let’s see how.\n If you take listing 6.6 as a basis, you need two node pointers: one for the head of the\nlist and one for the tail. You’re going to be accessing these from multiple threads, so\nthey’d better be atomic in order to allow you to do away with the corresponding\nmutexes. Let’s start by making that small change and see where it gets you. The follow-\ning listing shows the result.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    struct node\n    {\n        std::shared_ptr<T> data;\n        node* next;\n        node():\n            next(nullptr)\n        {}\n    };\nListing 7.14\nA single-producer, single-consumer lock-free queue\n",
      "content_length": 2257,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "237\nExamples of lock-free data structures\n    std::atomic<node*> head;\n    std::atomic<node*> tail;\n    node* pop_head()\n    {\n        node* const old_head=head.load();\n        if(old_head==tail.load())       \n        {\n            return nullptr;\n        }\n        head.store(old_head->next);\n        return old_head;\n    }\npublic:\n    lock_free_queue():\n        head(new node),tail(head.load())\n    {}\n    lock_free_queue(const lock_free_queue& other)=delete;\n    lock_free_queue& operator=(const lock_free_queue& other)=delete;\n    ~lock_free_queue()\n    {\n        while(node* const old_head=head.load())\n        {\n            head.store(old_head->next);\n            delete old_head;\n        }\n    }\n    std::shared_ptr<T> pop()\n    {\n        node* old_head=pop_head();\n        if(!old_head)\n        {\n            return std::shared_ptr<T>();\n        }\n        std::shared_ptr<T> const res(old_head->data);   \n        delete old_head;\n        return res;\n    }\n    void push(T new_value)\n    {\n        std::shared_ptr<T> new_data(std::make_shared<T>(new_value));\n        node* p=new node;                          \n        node* const old_tail=tail.load();     \n        old_tail->data.swap(new_data);   \n        old_tail->next=p;       \n        tail.store(p);    \n    }\n};\nAt first glance, this doesn’t seem too bad, and if there’s only one thread calling push()\nat a time, and only one thread calling pop(), then this is perfectly fine. The important\nthing in that case is the happens-before relationship between the push() and the\npop() to ensure that it’s safe to retrieve the data. The store to tail h synchronizes\nwith the load from tail B; the store to the preceding node’s data pointer f is\nb\nc\nd\ne\nf\ng\nh\n",
      "content_length": 1715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "238\nCHAPTER 7\nDesigning lock-free concurrent data structures\nsequenced before the store to tail; and the load from tail is sequenced before the\nload from the data pointer c, so the store to data happens before the load, and\neverything is OK. This is therefore a perfectly serviceable single-producer, single-consumer\n(SPSC) queue. \n The problems come when multiple threads call push() concurrently or multiple\nthreads call pop() concurrently. Let’s look at push() first. If you have two threads call-\ning push() concurrently, they both allocate new nodes to be the new dummy node d,\nboth read the same value for tail e, and consequently both update the data mem-\nbers of the same node when setting the data and next pointers, f and g. This is a\ndata race!\n There are similar problems in pop_head(). If two threads call concurrently, they\nwill both read the same value of head, and both then overwrite the old value with the\nsame next pointer. Both threads will now think they’ve retrieved the same node—a\nrecipe for disaster. Not only do you have to ensure that only one thread uses pop()on\na given item, but you also need to ensure that other threads can safely access the next\nmember of the node they read from head. This is exactly the problem you saw with\npop() for your lock-free stack, so any of the solutions for that could be used here.\n So if pop() is a “solved problem,” what about push()? The problem here is that in\norder to get the required happens-before relationship between push() and pop(), you\nneed to set the data items on the dummy node before you update tail. But this\nmeans that concurrent calls to push() are racing over those same data items, because\nthey’ve read the same tail pointer.\nHANDLING MULTIPLE THREADS IN PUSH()\nOne option is to add a dummy node between the real nodes. This way, the only part of\nthe current tail node that needs updating is the next pointer, which could therefore\nbe made atomic. If a thread manages to successfully change the next pointer from\nnullptr to its new node, then it has successfully added the pointer; otherwise, it\nwould have to start again and reread the tail. This would then require a minor\nchange to pop() in order to discard nodes with a null data pointer and loop again.\nThe downside here is that every pop() call will typically have to remove two nodes, and\nthere are twice as many memory allocations.\n A second option is to make the data pointer atomic and set that with a call to com-\npare/exchange. If the call succeeds, this is your tail node, and you can safely set the\nnext pointer to your new node and then update tail. If the compare/exchange fails\nbecause another thread has stored the data, you loop around, reread tail, and start\nagain. If the atomic operations on std::shared_ptr<> are lock-free, you’re home\nfree. If not, you need an alternative. One possibility is to have pop() return\nstd::unique_ptr<> (after all, it’s the only reference to the object) and store the data\nas a plain pointer in the queue. This would allow you to store it as std::atomic<T*>,\nwhich would then support the necessary compare_exchange_strong() call. If you’re\nusing the reference-counting scheme from listing 7.12 to handle multiple threads in\npop(), push() now looks like this.\n",
      "content_length": 3247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "239\nExamples of lock-free data structures\nvoid push(T new_value)\n{\n    std::unique_ptr<T> new_data(new T(new_value));\n    counted_node_ptr new_next;\n    new_next.ptr=new node;\n    new_next.external_count=1;\n    for(;;)\n    {\n        node* const old_tail=tail.load();    \n        T* old_data=nullptr;\n        if(old_tail->data.compare_exchange_strong(\n            old_data,new_data.get()))    \n        {\n            old_tail->next=new_next;\n            tail.store(new_next.ptr);    \n            new_data.release();\n            break;\n        }\n    }\n}\nUsing the reference-counting scheme avoids this particular race, but it’s not the only\nrace in push(). If you look at the revised version of push() in listing 7.15, you’ll see a\npattern you saw in the stack: load an atomic pointer B and dereference that pointer\nc. In the meantime, another thread could update the pointer d, eventually leading\nto the node being deallocated (in pop()). If the node is deallocated before you deref-\nerence the pointer, you have undefined behavior. Ouch! It’s tempting to add an exter-\nnal count in tail the same as you did for head, but each node already has an external\ncount in the next pointer of the previous node in the queue. Having two external\ncounts for the same node requires a modification to the reference-counting scheme\nto avoid deleting the node too early. You can address this by also counting the number\nof external counters inside the node structure and decreasing this number when each\nexternal counter is destroyed (as well as adding the corresponding external count to\nthe internal count). If the internal count is zero and there are no external counters,\nyou know the node can safely be deleted. This is a technique I first encountered\nthrough Joe Seigh’s Atomic Ptr Plus Project (http://atomic-ptr-plus.sourceforge.net/).\nThe following listing shows how push() looks under this scheme.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    struct node;\n    struct counted_node_ptr\n    {\n        int external_count;\nListing 7.15\nA (broken) first attempt at revising push()\nListing 7.16\nImplementing push() for a lock-free queue with a reference-counted tail\nb\nc\nd\n",
      "content_length": 2173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "240\nCHAPTER 7\nDesigning lock-free concurrent data structures\n        node* ptr;\n    };\n    std::atomic<counted_node_ptr> head;\n    std::atomic<counted_node_ptr> tail;    \n    struct node_counter\n    {\n        unsigned internal_count:30;\n        unsigned external_counters:2;   \n    };\n    struct node\n    {\n        std::atomic<T*> data;\n        std::atomic<node_counter> count;   \n        counted_node_ptr next;\n        node()\n        {\n            node_counter new_count;\n            new_count.internal_count=0;\n            new_count.external_counters=2;   \n            count.store(new_count);\n            \n            next.ptr=nullptr;\n            next.external_count=0;\n        }\n    };\npublic:\n    void push(T new_value)\n    {\n        std::unique_ptr<T> new_data(new T(new_value));\n        counted_node_ptr new_next;\n        new_next.ptr=new node;\n        new_next.external_count=1;\n        counted_node_ptr old_tail=tail.load();\n        for(;;)\n        {\n            increase_external_count(tail,old_tail);    \n            T* old_data=nullptr;\n            if(old_tail.ptr->data.compare_exchange_strong(   \n               old_data,new_data.get()))\n            {\n                old_tail.ptr->next=new_next;\n                old_tail=tail.exchange(new_next);\n                free_external_counter(old_tail);    \n                new_data.release();\n                break;\n            }\n            old_tail.ptr->release_ref();\n        }\n    }\n};\nIn listing 7.16, tail is now atomic<counted_node_ptr>, the same as head B, and the\nnode structure has a count member to replace the internal_count from before d. This\ncount is a structure containing the internal_count and an additional external_counters\nb\nc\nd\ne\nf\ng\nh\n",
      "content_length": 1715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "241\nExamples of lock-free data structures\nmember c. Note that you need only 2 bits for the external_counters because there\nare at most two such counters. By using a bit field for this and specifying internal\n_count as a 30-bit value, you keep the total counter size to 32 bits. This gives you plenty\nof scope for large internal count values while ensuring that the whole structure fits\ninside a machine word on 32-bit and 64-bit machines. It’s important to update these\ncounts together as a single entity in order to avoid race conditions, as you’ll see shortly.\nKeeping the structure within a machine word makes it more likely that the atomic oper-\nations can be lock-free on many platforms.\n The node is initialized with the internal_count set to zero and the external_\ncounters set to 2 e, because every new node starts out referenced from tail and\nfrom the next pointer of the previous node once you’ve added it to the queue.\npush()itself is similar to listing 7.15, except that before you dereference the value\nloaded from tail in order to call to compare_exchange_strong() on the data mem-\nber of the node g, you call a new function increase_external_count() to increase\nthe count f, and then afterward you call free_external_counter() on the old tail\nvalue h.\n With the push() side dealt with, let’s take a look at pop(). This is shown in the fol-\nlowing listing and blends the reference-counting logic from the pop() implementa-\ntion in listing 7.12 with the queue-pop logic from listing 7.14.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    struct node\n    {\n        void release_ref();\n    };\npublic:\n    std::unique_ptr<T> pop()\n    {\n        counted_node_ptr old_head=head.load(std::memory_order_relaxed); \n        for(;;)\n        {\n            increase_external_count(head,old_head);   \n            node* const ptr=old_head.ptr;\n            if(ptr==tail.load().ptr)\n            {\n                ptr->release_ref();        \n                return std::unique_ptr<T>();\n            }\n            if(head.compare_exchange_strong(old_head,ptr->next))   \n            {\n                T* const res=ptr->data.exchange(nullptr);\n                free_external_counter(old_head);        \n                return std::unique_ptr<T>(res);\n            }\nListing 7.17\nPopping a node from a lock-free queue with a reference-counted tail\nb\nc\nd\ne\nf\n",
      "content_length": 2356,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "242\nCHAPTER 7\nDesigning lock-free concurrent data structures\n            ptr->release_ref();   \n        }\n    }\n};\nYou prime the pump by loading the old_head value before you enter the loop B, and\nbefore you increase the external count on the loaded value,c. If the head node is the\nsame as the tail node, you can release the reference d and return a null pointer\nbecause there’s no data in the queue. If there is data, you want to try to claim it for\nyourself, and you do this with the call to compare_exchange_strong() e. As with the\nstack in listing 7.12, this compares the external count and pointer as a single entity; if\neither changes, you need to loop again, after releasing the reference g. If the\nexchange succeeded, you’ve claimed the data in the node as yours, so you can return\nthat to the caller after you’ve released the external counter to the popped node f.\nOnce both the external reference counts have been freed and the internal count has\ndropped to zero, the node itself can be deleted. The reference-counting functions\nthat take care of all this are shown in listings 7.18, 7.19, and 7.20.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    struct node\n    {\n        void release_ref()\n        {\n            node_counter old_counter=\n                count.load(std::memory_order_relaxed);\n            node_counter new_counter;\n            do\n            {\n                new_counter=old_counter;\n                --new_counter.internal_count;    \n            }\n            while(!count.compare_exchange_strong(    \n                  old_counter,new_counter,\n                  std::memory_order_acquire,std::memory_order_relaxed));\n            if(!new_counter.internal_count && \n               !new_counter.external_counters)\n            {\n                delete this;    \n            }\n        }\n    };\n};\nThe implementation of node::release_ref() is only slightly changed from the equiv-\nalent code in the implementation of lock_free_stack::pop() from listing 7.12.\nListing 7.18\nReleasing a node reference in a lock-free queue\ng\nb\nc\nd\n",
      "content_length": 2064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "243\nExamples of lock-free data structures\nWhereas the code in listing 7.12 only has to handle a single external count so you\ncould use a simple fetch_sub, the whole count structure now has to be updated atomi-\ncally, even though you only want to modify the internal_count field B. This therefore\nrequires a compare/exchange loop c. Once you’ve decremented internal_count, if\nboth the internal and external counts are now zero, this is the last reference, so you\ncan delete the node d.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    static void increase_external_count(\n        std::atomic<counted_node_ptr>& counter,\n        counted_node_ptr& old_counter)\n    {\n        counted_node_ptr new_counter;\n        do\n        {\n            new_counter=old_counter;\n            ++new_counter.external_count;\n        }\n        while(!counter.compare_exchange_strong(\n              old_counter,new_counter,\n              std::memory_order_acquire,std::memory_order_relaxed));\n        old_counter.external_count=new_counter.external_count;\n    }\n};\nListing 7.19 is the other side. This time, rather than releasing a reference, you’re obtain-\ning a fresh one and increasing the external count. increase_external_count() is simi-\nlar to the increase_head_count() function from listing 7.13, except that it has been\nmade into a static member function that takes the external counter to update as the\nfirst parameter rather than operating on a fixed counter.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    static void free_external_counter(counted_node_ptr &old_node_ptr)\n    {\n        node* const ptr=old_node_ptr.ptr;\n        int const count_increase=old_node_ptr.external_count-2;\n        node_counter old_counter=\n            ptr->count.load(std::memory_order_relaxed);\n        node_counter new_counter;\n        do\n        {\n            new_counter=old_counter;\nListing 7.19\nObtaining a new reference to a node in a lock-free queue\nListing 7.20\nFreeing an external counter to a node in a lock-free queue\n",
      "content_length": 2015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "244\nCHAPTER 7\nDesigning lock-free concurrent data structures\n            --new_counter.external_counters;            \n            new_counter.internal_count+=count_increase;    \n        }\n        while(!ptr->count.compare_exchange_strong(    \n              old_counter,new_counter,\n              std::memory_order_acquire,std::memory_order_relaxed));\n        if(!new_counter.internal_count && \n           !new_counter.external_counters)\n        {\n            delete ptr;    \n        }\n    }\n};\nThe counterpart to increase_external_count() is free_external_counter(). This\nis similar to the equivalent code from lock_free_stack::pop() in listing 7.12, but\nmodified to handle the external_counters count. It updates the two counts using a\nsingle compare_exchange_strong() on the whole count structure d, as you did\nwhen decreasing the internal_count in release_ref(). The internal_count value\nis updated as in listing 7.12 c, and the external_counters value is decreased by\none B. If both the values are now zero, there are no more references to the node, so it\ncan be safely deleted e. This has to be done as a single action (which therefore\nrequires the compare/exchange loop) to avoid a race condition. If they’re updated\nseparately, two threads may both think they are the last one and both delete the node,\nresulting in undefined behavior.\n Although this now works and is race-free, there’s still a performance issue. Once\none thread has started a push() operation by successfully completing the compare_\nexchange_strong() on old_tail.ptr->data (f from listing 7.16), no other thread\ncan perform a push() operation. Any thread that tries will see the new value rather\nthan nullptr, which will cause the compare_exchange_strong() call to fail and\nmake that thread loop again. This is a busy wait, which consumes CPU cycles with-\nout achieving anything. Consequently, this is effectively a lock. The first push() call\nblocks other threads until it has completed, so this code is no longer lock-free. Not\nonly that, but whereas the operating system can give priority to the thread that holds\nthe lock on a mutex if there are blocked threads, it can’t do so in this case, so the\nblocked threads will waste CPU cycles until the first thread is done. This calls for the\nnext trick from the lock-free bag of tricks: the waiting thread can help the thread\nthat’s doing the push().\nMAKING THE QUEUE LOCK-FREE BY HELPING OUT ANOTHER THREAD\nIn order to restore the lock-free property of the code, you need to find a way for a\nwaiting thread to make progress even if the thread doing the push() is stalled. One\nway to do this is to help the stalled thread by doing its work for it.\n In this case, you know exactly what needs to be done: the next pointer on the tail\nnode needs to be set to a new dummy node, and then the tail pointer itself must be\nupdated. The thing about dummy nodes is that they’re all equivalent, so it doesn’t\nb\nc\nd\ne\n",
      "content_length": 2931,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "245\nExamples of lock-free data structures\nmatter if you use the dummy node created by the thread that successfully pushed the\ndata or the dummy node from one of the threads that’s waiting to push. If you make\nthe next pointer in a node atomic, you can then use compare_exchange_strong() to\nset the pointer. Once the next pointer is set, you can then use a compare_exchange_\nweak() loop to set the tail while ensuring that it’s still referencing the same original\nnode. If it isn’t, someone else has updated it, and you can stop trying and loop again.\nThis requires a minor change to pop() as well in order to load the next pointer; this is\nshown in the following listing.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    struct node\n    {\n        std::atomic<T*> data;\n        std::atomic<node_counter> count;\n        std::atomic<counted_node_ptr> next;    \n    };\npublic:\n    std::unique_ptr<T> pop()\n    {\n        counted_node_ptr old_head=head.load(std::memory_order_relaxed);\n        for(;;)\n        {\n            increase_external_count(head,old_head);\n            node* const ptr=old_head.ptr;\n            if(ptr==tail.load().ptr)\n            {\n                return std::unique_ptr<T>();\n            }\n            counted_node_ptr next=ptr->next.load();        \n            if(head.compare_exchange_strong(old_head,next))\n            {\n                T* const res=ptr->data.exchange(nullptr);\n                free_external_counter(old_head);\n                return std::unique_ptr<T>(res);\n            }\n            ptr->release_ref();\n        }\n    }\n};\nAs I mentioned, the changes here are simple: the next pointer is now atomic B, so\nthe load at c is atomic. In this example, you’re using the default memory_order_\nseq_cst ordering, so you could omit the explicit call to load() and rely on the load in\nthe implicit conversion to counted_node_ptr, but putting in the explicit call reminds\nyou where to add the explicit memory ordering later.\nListing 7.21\npop() modified to allow helping on the push() side\nb\nc\n",
      "content_length": 2031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "246\nCHAPTER 7\nDesigning lock-free concurrent data structures\n The code for push() is more involved and is shown here.\ntemplate<typename T>\nclass lock_free_queue\n{\nprivate:\n    void set_new_tail(counted_node_ptr &old_tail,      \n                      counted_node_ptr const &new_tail)\n    {\n        node* const current_tail_ptr=old_tail.ptr;\n        while(!tail.compare_exchange_weak(old_tail,new_tail) &&    \n              old_tail.ptr==current_tail_ptr);\n        if(old_tail.ptr==current_tail_ptr)      \n            free_external_counter(old_tail);    \n        else\n            current_tail_ptr->release_ref();   \n    }\npublic:\n    void push(T new_value)\n    {\n        std::unique_ptr<T> new_data(new T(new_value));\n        counted_node_ptr new_next;\n        new_next.ptr=new node;\n        new_next.external_count=1;\n        counted_node_ptr old_tail=tail.load();\n        for(;;)\n        {\n            increase_external_count(tail,old_tail);\n            T* old_data=nullptr;\n            if(old_tail.ptr->data.compare_exchange_strong(    \n                   old_data,new_data.get()))\n            {\n                counted_node_ptr old_next={0};\n                if(!old_tail.ptr->next.compare_exchange_strong(     \n                       old_next,new_next))\n                {\n                    delete new_next.ptr;    \n                    new_next=old_next;    \n                }\n                set_new_tail(old_tail, new_next);\n                new_data.release();\n                break;\n            }\n            else     \n            {\n                counted_node_ptr old_next={0};\n                if(old_tail.ptr->next.compare_exchange_strong(    \n                       old_next,new_next))\n                {\n                    old_next=new_next;     \n                    new_next.ptr=new node;     \n                }\nListing 7.22\nA sample push() with helping for a lock-free queue\nb\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n1@\n1#\n",
      "content_length": 1919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "247\nExamples of lock-free data structures\n                set_new_tail(old_tail, old_next);    \n            }\n        }\n    }\n};\nThis is similar to the original push() from listing 7.16, but there are a few crucial dif-\nferences. If you do set the data pointer g, you need to handle the case where another\nthread has helped you, and there’s now an else clause to do the helping 1).\n Having set the data pointer in the node g, this new version of push() updates the\nnext pointer using compare_exchange_strong() h. You use compare_exchange_\nstrong() to avoid looping. If the exchange fails, you know that another thread has\nalready set the next pointer, so you don’t need the new node you allocated at the\nbeginning, and you can delete it i. You also want to use the next value that the other\nthread set for updating tail j.\n The update of the tail pointer has been extracted into set_new_tail() B. This\nuses a compare_exchange_weak() loop c to update the tail, because if other threads\nare trying to push() a new node, the external_count part may have changed, and\nyou don’t want to lose it. But you also need to take care that you don’t replace the\nvalue if another thread has successfully changed it already; otherwise, you may end up\nwith loops in the queue, which would be a rather bad idea. Consequently, you need to\nensure that the ptr part of the loaded value is the same if the compare/exchange\nfails. If the ptr is the same once the loop has exited d, then you must have success-\nfully set the tail, so you need to free the old external counter e. If the ptr value is\ndifferent, then another thread will have freed the counter, so you need to release the\nsingle reference held by this thread f.\n If the thread calling push() failed to set the data pointer this time through the\nloop, it can help the successful thread to complete the update. First off, you try to\nupdate the next pointer to the new node allocated on this thread 1!. If this succeeds,\nyou want to use the node you allocated as the new tail node 1@, and you need to allo-\ncate another new node in anticipation of managing to push an item on the queue 1#.\nYou can then try to set the tail node by calling set_new_tail before looping around\nagain 1$.\n You may have noticed that there are a lot of new and delete calls for such a small\npiece of code, because new nodes are allocated on push() and destroyed in pop().\nThe efficiency of the memory allocator therefore has a considerable impact on the\nperformance of this code; a poor allocator can completely destroy the scalability prop-\nerties of a lock-free container like this. The selection and implementation of these\nallocators are beyond the scope of this book, but it’s important to bear in mind that\nthe only way to know that an allocator is better is to try it and measure the perfor-\nmance of the code before and after. Common techniques for optimizing memory allo-\ncation include having a separate memory allocator on each thread and using free lists\nto recycle nodes rather than returning them to the allocator. \n1$\n",
      "content_length": 3048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "248\nCHAPTER 7\nDesigning lock-free concurrent data structures\n That’s enough examples for now; instead, let’s look at extracting some guidelines\nfor writing lock-free data structures from the examples.\n7.3\nGuidelines for writing lock-free data structures\nIf you’ve followed through all the examples in this chapter, you’ll appreciate the com-\nplexities involved in getting lock-free code right. If you’re going to design your own\ndata structures, it helps to have some guidelines to focus on. The general guidelines\nregarding concurrent data structures from the beginning of chapter 6 still apply, but\nyou need more than that. I’ve pulled a few useful guidelines out from the examples,\nwhich you can then refer to when designing your own lock-free data structures.\n7.3.1\nGuideline: use std::memory_order_seq_cst for prototyping\nstd::memory_order_seq_cst is much easier to reason about than any other memory\nordering because all these operations form a total order. In all the examples in this\nchapter, you’ve started with std::memory_order_seq_cst and only relaxed the\nmemory-ordering constraints once the basic operations were working. In this sense,\nusing other memory orderings is an optimization, and as such you need to avoid doing\nit prematurely. In general, you can only determine which operations can be relaxed\nwhen you can see the full set of code that can operate on the guts of the data struc-\nture. Attempting to do otherwise makes your life harder. This is complicated by the\nfact that the code may work when tested but isn’t guaranteed. Unless you have an\nalgorithm checker that can systematically test all possible combinations of thread visi-\nbilities that are consistent with the specified ordering guarantees (and these things do\nexist), running the code isn’t enough.\n7.3.2\nGuideline: use a lock-free memory reclamation scheme\nOne of the biggest difficulties with lock-free code is managing memory. It’s essential\nto avoid deleting objects when other threads might still have references to them, but\nyou still want to delete the object as soon as possible in order to avoid excessive mem-\nory consumption. In this chapter you’ve seen three techniques for ensuring that mem-\nory can safely be reclaimed:\nWaiting until no threads are accessing the data structure and deleting all objects\nthat are pending deletion\nUsing hazard pointers to identify that a thread is accessing a particular object\nReference counting the objects so that they aren’t deleted until there are no\noutstanding references\nIn all cases, the key idea is to use some method to keep track of how many threads are\naccessing a particular object and only delete each object when it’s no longer refer-\nenced from anywhere. There are many other ways of reclaiming memory in lock-free\ndata structures. For example, this is the ideal scenario for using a garbage collector.\nIt’s much easier to write the algorithms if you know that the garbage collector will free\nthe nodes when they’re no longer used, but not before.\n",
      "content_length": 3002,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "249\nGuidelines for writing lock-free data structures\n Another alternative is to recycle nodes and only free them completely when the\ndata structure is destroyed. Because the nodes are reused, the memory never\nbecomes invalid, so some of the difficulties in avoiding undefined behavior go away.\nThe downside here is that another problem becomes more prevalent. This is the so-\ncalled ABA problem.\n7.3.3\nGuideline: watch out for the ABA problem\nThe ABA problem is something to be wary of in any compare/exchange–based algo-\nrithm. It goes like this:\n1\nThread 1 reads an atomic variable, x, and finds it has value A.\n2\nThread 1 performs some operation based on this value, such as dereferencing it\n(if it’s a pointer) or doing a lookup, or something.\n3\nThread 1 is stalled by the operating system.\n4\nAnother thread performs some operations on x that change its value to B.\n5\nA thread then changes the data associated with the value A such that the value\nheld by thread 1 is no longer valid. This may be as drastic as freeing the\npointed-to memory or changing an associated value. \n6\nA thread then changes x back to A based on this new data. If this is a pointer, it\nmay be a new object that happens to share the same address as the old one.\n7\nThread 1 resumes and performs a compare/exchange on x, comparing against\nA. The compare/exchange succeeds (because the value is indeed A), but this is\nthe wrong A value. The data originally read at step 2 is no longer valid, but\nthread 1 has no way of telling and will corrupt the data structure.\nNone of the algorithms presented here suffer from this problem, but it’s easy to write\nlock-free algorithms that do. The most common way to avoid this problem is to\ninclude an ABA counter alongside the variable x. The compare/exchange operation\nis then done on the combined structure of x plus the counter as a single unit. Every\ntime the value is replaced, the counter is incremented, so even if x has the same value,\nthe compare/exchange will fail if another thread has modified x.\n The ABA problem is particularly prevalent in algorithms that use free lists or other-\nwise recycle nodes rather than returning them to the allocator.\n7.3.4\nGuideline: identify busy-wait loops and help the other thread\nIn the final queue example, you saw how a thread performing a push operation had\nto wait for another thread also performing a push to complete its operation before it\ncould proceed. Left alone, this would have been a busy-wait loop, with the waiting\nthread wasting CPU time while failing to proceed. If you end up with a busy-wait loop,\nyou effectively have a blocking operation and might as well use mutexes and locks. By\nmodifying the algorithm so that the waiting thread performs the incomplete steps if\nit’s scheduled to run before the original thread completes the operation, you can\nremove the busy-wait and the operation is no longer blocking. In the queue example\n",
      "content_length": 2912,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "250\nCHAPTER 7\nDesigning lock-free concurrent data structures\nthis required changing a data member to be an atomic variable rather than a non-\natomic variable and using compare/exchange operations to set it, but in more com-\nplex data structures it might require more extensive changes.\nSummary\nFollowing from the lock-based data structures of chapter 6, this chapter has described\nsimple implementations of various lock-free data structures, starting with a stack and a\nqueue, as before. You saw how you must take care with the memory ordering on your\natomic operations to ensure that there are no data races and that each thread sees a\ncoherent view of the data structure. You also saw how memory management becomes\nmuch harder for lock-free data structures than lock-based ones and examined a cou-\nple of mechanisms for handling it. You also saw how to avoid creating wait loops by\nhelping the thread you’re waiting for to complete its operation.\n Designing lock-free data structures is a difficult task, and it’s easy to make mistakes,\nbut these data structures have scalability properties that are important in some situa-\ntions. Hopefully, by following through the examples in this chapter and reading the\nguidelines, you’ll be better equipped to design your own lock-free data structure,\nimplement one from a research paper, or find the bug in the one your former col-\nleague wrote before they left the company.\n Wherever data is shared between threads, you need to think about the data struc-\ntures used and how the data is synchronized between threads. By designing data struc-\ntures for concurrency, you can encapsulate that responsibility in the data structure\nitself, so the rest of the code can focus on the task it’s trying to perform with the data\nrather than the data synchronization. You’ll see this in action in chapter 8 as we move\non from concurrent data structures to concurrent code in general. Parallel algorithms\nuse multiple threads to improve their performance, and the choice of concurrent\ndata structure is crucial where the algorithms need their worker threads to share data.\n",
      "content_length": 2104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "251\nDesigning concurrent code\nMost of the preceding chapters have focused on the tools you have in your C++\ntoolbox for writing concurrent code. In chapters 6 and 7 we looked at how to\nuse those tools to design basic data structures that are safe for concurrent access\nby multiple threads. Much as a carpenter needs to know more than how to build\na hinge or a joint in order to make a cupboard or a table, there’s more to design-\ning concurrent code than the design and use of basic data structures. You now\nneed to look at the wider context so you can build bigger structures that per-\nform useful work. I’ll be using multithreaded implementations of some of the\nThis chapter covers\nTechniques for dividing data between threads\nFactors that affect the performance of concurrent \ncode \nHow performance factors affect the design of \ndata structures\nException safety in multithreaded code\nScalability\nExample implementations of several parallel \nalgorithms\n",
      "content_length": 961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "252\nCHAPTER 8\nDesigning concurrent code\nC++ Standard Library algorithms as examples, but the same principles apply at all\nscales of an application.\n Just as with any programming project, it’s vital to think carefully about the design\nof concurrent code. But with multithreaded code, there are even more factors to con-\nsider than with sequential code. Not only must you think about the usual factors, such\nas encapsulation, coupling, and cohesion (which are amply described in the many\nbooks on software design), but you also need to consider which data to share, how to\nsynchronize accesses to that data, which threads need to wait for which other threads\nto complete certain operations, and so on.\n In this chapter we’ll be focusing on these issues, from the high-level (but funda-\nmental) considerations of how many threads to use, which code to execute on which\nthread, and how this can affect the clarity of the code, to the low-level details of how to\nstructure the shared data for optimal performance.\n Let’s start by looking at techniques for dividing work between threads.\n8.1\nTechniques for dividing work between threads\nImagine for a moment that you’ve been tasked with building a house. In order to\ncomplete the job, you’ll need to dig the foundation, build walls, put in plumbing, add\nthe wiring, and so on. Theoretically, you could do it all yourself with sufficient train-\ning, but it would probably take a long time, and you’d be continually switching tasks as\nnecessary. Alternatively, you could hire a few other people to help out. You now have\nto choose how many people to hire and decide what skills they need. You could, for\nexample, hire a couple of people with general skills and have everybody chip in with\neverything. You’d still all switch tasks as necessary, but now things can be done more\nquickly because there are more of you.\n Alternatively, you could hire a team of specialists: a bricklayer, a carpenter, an\nelectrician, and a plumber, for example. Your specialists do whatever their specialty\nis, so if there’s no plumbing needed, your plumber sits around drinking tea or cof-\nfee. Things still get done more quickly than before, because there are more of you,\nand the plumber can put the toilet in while the electrician wires up the kitchen, but\nthere’s more waiting around when there’s no work for a particular specialist. Even\nwith the idle time, you might find that the work is done faster with specialists than\nwith a team of general handymen. Your specialists don’t need to keep changing\ntools, and they can probably each do their tasks quicker than the generalists can.\nWhether or not this is the case depends on the particular circumstances—you’d\nhave to try it and see.\n Even if you hire specialists, you can still choose to hire different numbers of each.\nIt might make sense to have more bricklayers than electricians, for example. Also, the\nmakeup of your team and the overall efficiency might change if you had to build more\nthan one house. Even though your plumber might not have lots of work to do on any\ngiven house, you might have enough work to keep him busy all the time if you’re\nbuilding many houses at once. Also, if you don’t have to pay your specialists when\n",
      "content_length": 3217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "253\nTechniques for dividing work between threads\nthere’s no work for them to do, you might be able to afford a larger team overall even\nif you have only the same number of people working at any one time.\n OK, enough about building; what does all this have to do with threads? Well, with\nthreads the same issues apply. You need to decide how many threads to use and what\ntasks they should be doing. You need to decide whether to have “generalist” threads\nthat do whatever work is necessary at any point in time or “specialist” threads that do\none thing well, or some combination. You need to make these choices whatever the\ndriving reason for using concurrency, and how you do this will have a critical effect on\nthe performance and clarity of the code. It’s therefore vital to understand the options\nso you can make an appropriately informed decision when designing the structure of\nyour application. In this section, we’ll look at several techniques for dividing the tasks,\nstarting with dividing data between threads before we do any other work. \n8.1.1\nDividing data between threads before processing begins\nThe easiest algorithms to parallelize are simple algorithms, such as std::for_each,\nthat perform an operation on each element in a data set. In order to parallelize this\nalgorithm, you can assign each element to one of the processing threads. How the ele-\nments are best divided for optimal performance depends on the details of the data\nstructure, as you’ll see later in this chapter when we look at performance issues.\n The simplest means of dividing the data is to allocate the first N elements to one\nthread, the next N elements to another thread, and so on, as shown in figure 8.1, but\nother patterns could be used too. No matter how the data is divided, each thread then\nprocesses the elements it has been assigned without any communication with the\nother threads until it has completed its processing.\n This structure will be familiar to anyone who has programmed using the Message\nPassing Interface (MPI, http://www.mpi-forum.org/) or OpenMP (http://www.openmp\n.org/) frameworks: a task is split into a set of parallel tasks, the worker threads run these\ntasks independently, and the results are combined in a final reduction step. It’s the\nThread 1\nThread 2\nThread m\nFigure 8.1\nDistributing consecutive chunks of data between threads\n",
      "content_length": 2353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "254\nCHAPTER 8\nDesigning concurrent code\napproach used by the accumulate example from section 2.4; in this case, both the par-\nallel tasks and the final reduction step are accumulations. For a simple for_each, the\nfinal step is a no-op because there are no results to reduce.\n Identifying this final step as a reduction is important; a naive implementation such\nas listing 2.9 will perform this reduction as a final serial step. But this step can often be\nparallelized as well; accumulate is a reduction operation, so listing 2.9 could be modi-\nfied to call itself recursively where the number of threads is larger than the minimum\nnumber of items to process on a thread, for example. Alternatively, the worker threads\ncould be made to perform some of the reduction steps as each one completes its task,\nrather than spawning new threads each time.\n Although this technique is powerful, it can’t be applied to everything. Sometimes\nthe data can’t be divided neatly up front because the necessary divisions become appar-\nent only as the data is processed. This is particularly apparent with recursive algo-\nrithms such as Quicksort; they therefore need a different approach.\n8.1.2\nDividing data recursively\nThe Quicksort algorithm has two basic steps: partition the data into items that come\nbefore or after one of the elements (the pivot) in the final sort order and recursively\nsort those two “halves.” You can’t parallelize this by dividing the data up front, because\nit’s only by processing the items that you know which “half” they go in. If you’re going\nto parallelize this algorithm, you need to make use of the recursive nature. With each\nlevel of recursion there are more calls to the quick_sort function, because you have to\nsort both the elements that belong before the pivot and those that belong after it.\nThese recursive calls are entirely independent, because they access separate sets of\nelements, and so are prime candidates for concurrent execution. Figure 8.2 shows this\nrecursive division.\n In chapter 4, you saw this implementation. Rather than performing two recursive\ncalls for the higher and lower chunks, you used std::async() to spawn asynchronous\nFigure 8.2\nRecursively dividing data\n",
      "content_length": 2208,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "255\nTechniques for dividing work between threads\ntasks for the lower chunk at each stage. By using std::async(), you ask the C++\nThread Library to decide when to run the task on a new thread and when to run it\nsynchronously.\n This is important: if you’re sorting a large set of data, spawning a new thread for\neach recursion would quickly result in a lot of threads. As you’ll see when we look at\nperformance, if you have too many threads, you might slow down the application.\nThere’s also a possibility of running out of threads if the data set is large. The idea of\ndividing the overall task in a recursive fashion like this is a good one; you just need to\nkeep a tighter rein on the number of threads. std::async() can handle this in simple\ncases, but it’s not the only choice.\n One alternative is to use the std::thread::hardware_concurrency() function to\nchoose the number of threads, as you did with the parallel version of accumulate()\nfrom listing 2.9. Then, rather than starting a new thread for the recursive calls, you\ncan push the chunk to be sorted onto a thread-safe stack, such as one of those\ndescribed in chapters 6 and 7. If a thread has nothing else to do, either because it has\nfinished processing all its chunks or because it’s waiting for a chunk to be sorted, it\ncan take a chunk from the stack and sort that.\n The following listing shows a sample implementation that uses this technique. As\nwith most of the examples, this is intended to demonstrate an idea rather than being\nproduction-ready code. If you're using a C++17 compiler and your library supports it,\nyou're better off using the parallel algorithms provided by Standard Library, as cov-\nered in chapter 10.\ntemplate<typename T>\nstruct sorter        \n{\n    struct chunk_to_sort\n    {\n        std::list<T> data;\n        std::promise<std::list<T> > promise;\n    };\n    thread_safe_stack<chunk_to_sort> chunks;   \n    std::vector<std::thread> threads;       \n    unsigned const max_thread_count;\n    std::atomic<bool> end_of_data;\n    sorter():\n        max_thread_count(std::thread::hardware_concurrency()-1),\n        end_of_data(false)\n    {}\n    ~sorter()   \n    {\n        end_of_data=true;   \n        for(unsigned i=0;i<threads.size();++i)\n        {\n            threads[i].join();   \n        }\nListing 8.1\nParallel Quicksort using a stack of pending chunks to sort\nb\nc\nd\ne\nf\ng\n",
      "content_length": 2361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "256\nCHAPTER 8\nDesigning concurrent code\n    }\n    void try_sort_chunk()\n    {\n        boost::shared_ptr<chunk_to_sort > chunk=chunks.pop();   \n        if(chunk)\n        {\n            sort_chunk(chunk);   \n        }\n    }\n    std::list<T> do_sort(std::list<T>& chunk_data)   \n    {\n        if(chunk_data.empty())\n        {\n            return chunk_data;\n        }\n        std::list<T> result;\n        result.splice(result.begin(),chunk_data,chunk_data.begin());\n        T const& partition_val=*result.begin();\n        typename std::list<T>::iterator divide_point=    \n            std::partition(chunk_data.begin(),chunk_data.end(),\n                           [&](T const& val){return val<partition_val;});\n        chunk_to_sort new_lower_chunk;\n        new_lower_chunk.data.splice(new_lower_chunk.data.end(),\n                                    chunk_data,chunk_data.begin(),\n                                    divide_point);\n        std::future<std::list<T> > new_lower=\n            new_lower_chunk.promise.get_future();\n        chunks.push(std::move(new_lower_chunk));     \n        if(threads.size()<max_thread_count)     \n        {\n            threads.push_back(std::thread(&sorter<T>::sort_thread,this));\n        }\n        std::list<T> new_higher(do_sort(chunk_data));\n        result.splice(result.end(),new_higher);\n        while(new_lower.wait_for(std::chrono::seconds(0)) !=\n              std::future_status::ready)  \n        {\n            try_sort_chunk();    \n        }\n        result.splice(result.begin(),new_lower.get());\n        return result;\n    }\n    void sort_chunk(boost::shared_ptr<chunk_to_sort > const& chunk)\n    {\n        chunk->promise.set_value(do_sort(chunk->data));   \n    }\n    void sort_thread()\n    {\n        while(!end_of_data)   \n        {\n            try_sort_chunk();   \n            std::this_thread::yield();    \n        }\n    }\n};\nh\ni\nj\n1)\n1!\n1@\n1#\n1$\n1%\n1^\n1&\n1*\n",
      "content_length": 1900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "257\nTechniques for dividing work between threads\ntemplate<typename T>\nstd::list<T> parallel_quick_sort(std::list<T> input)    \n{\n    if(input.empty())\n    {\n        return input;\n    }\n    sorter<T> s;\n    return s.do_sort(input);    \n}\nHere, the parallel_quick_sort function 1( delegates most of the functionality to\nthe sorter class B, which provides an easy way of grouping the stack of unsorted\nchunks c and the set of threads d. The main work is done in the do_sort member\nfunction j, which does the usual partitioning of the data 1). This time, rather than\nspawning a new thread for one chunk, it pushes it onto the stack 1! and spawns a new\nthread while you still have processors to spare 1@. Because the lower chunk might be\nhandled by another thread, you then have to wait for it to be ready 1#. In order to\nhelp things along (in case you’re the only thread or all the others are already busy),\nyou try to process chunks from the stack on this thread while you’re waiting 1$.\ntry_sort_chunk pops a chunk off the stack h, and sorts it i, storing the result in the\npromise, ready to be picked up by the thread that posted the chunk on the stack 1%.\n Your freshly spawned threads sit in a loop trying to sort chunks off the stack 1&,\nwhile the end_of_data flag isn’t set 1^. In between checking, they yield to other\nthreads 1* to give them a chance to put some more work on the stack. This code relies\non the destructor of your sorter class e to tidy up these threads. When all the data\nhas been sorted, do_sort will return (even though the worker threads are still run-\nning), so your main thread will return from parallel_quick_sort 2) and destroy\nyour sorter object. This sets the end_of_data flag f and waits for the threads to fin-\nish g. Setting the flag terminates the loop in the thread function 1^.\n With this approach you no longer have the problem of unbounded threads that\nyou have with a spawn_task that launches a new thread, and you’re no longer rely-\ning on the C++ Thread Library to choose the number of threads for you, as it does\nwith std::async(). Instead, you limit the number of threads to the value of std::\nthread::hardware_concurrency() in order to avoid excessive task switching. You do,\nhowever, have another potential problem: the management of these threads and the\ncommunication between them add quite a lot of complexity to the code. Also,\nalthough the threads are processing separate data elements, they all access the stack\nto add new chunks and to remove chunks for processing. This heavy contention can\nreduce performance, even if you use a lock-free (and hence nonblocking) stack, for\nreasons you’ll see shortly.\n This approach is a specialized version of a thread pool—that’s a set of threads that\neach take work to do from a list of pending work, do the work, and then go back to\nthe list for more. Some of the potential problems with thread pools (including the\ncontention on the work list) and ways of addressing them are covered in chapter 9.\n1(\n2)\n",
      "content_length": 2996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "258\nCHAPTER 8\nDesigning concurrent code\nThe problems of scaling your application to multiple processors are discussed in more\ndetail later in this chapter (see section 8.2.1).\n Both dividing the data before processing begins and dividing it recursively pre-\nsume that the data itself is fixed beforehand, and you’re looking at ways of dividing it.\nThis isn’t always the case; if the data is dynamically generated or is coming from exter-\nnal input, this approach doesn’t work. In this case, it might make more sense to divide\nthe work by task type rather than dividing based on the data.\n8.1.3\nDividing work by task type\nDividing work between threads by allocating different chunks of data to each thread\n(whether up front or recursively during processing) still rests on the assumption\nthat the threads are going to be doing the same work on each chunk of data. An\nalternative to dividing the work is to make the threads specialists, where each per-\nforms a distinct task, just as plumbers and electricians perform distinct tasks when\nbuilding a house. Threads may or may not work on the same data, but if they do, it’s\nfor different purposes.\n This is the sort of division of work that results from separating concerns with con-\ncurrency; each thread has a different task, which it carries out independently of other\nthreads. Occasionally other threads may give it data or trigger events that it needs to\nhandle, but in general each thread focuses on doing one thing well. In itself, this is\nbasic good design; each piece of code should have a single responsibility.\nDIVIDING WORK BY TASK TYPE TO SEPARATE CONCERNS\nA single-threaded application has to handle conflicts with the single responsibility\nprinciple where there are multiple tasks that need to be run continuously over a\nperiod of time, or where the application needs to be able to handle incoming events\n(such as user key presses or incoming network data) in a timely fashion, even while\nother tasks are ongoing. In the single-threaded world you end up manually writing\ncode that performs a bit of task A, a bit of task B, checks for key presses, checks for\nincoming network packets, and then loops back to perform another bit of task A. This\nmeans that the code for task A ends up being complicated by the need to save its state\nand return control to the main loop periodically. If you add too many tasks to the\nloop, things might slow down too much, and the user may find it takes too long to\nrespond to the key press. I’m sure you’ve all seen the extreme form of this in action\nwith some application or other: you set it to doing some task, and the interface freezes\nuntil it has completed the task.\n This is where threads come in. If you run each of the tasks in a separate thread, the\noperating system handles this for you. In the code for task A, you can focus on per-\nforming the task and not worry about saving state and returning to the main loop or\nhow long you spend before doing so. The operating system will automatically save the\nstate and switch to task B or C when appropriate, and if the target system has multiple\ncores or processors, tasks A and B may be able to run concurrently. The code for han-\ndling the key press or network packet will now be run in a timely fashion, and everybody\n",
      "content_length": 3270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "259\nTechniques for dividing work between threads\nwins: the user gets timely responses, and you, as the developer, have simpler code\nbecause each thread can focus on doing operations related directly to its responsibili-\nties, rather than getting mixed up with control flow and user interaction.\n That sounds like a nice, rosy vision. Can it be like that? As with everything, it\ndepends on the details. If everything is independent, and the threads have no need to\ncommunicate with each other, then it can be this easy. Unfortunately, the world is\nrarely like that. These nice background tasks are often doing something that the user\nrequested, and they need to let the user know when they’re done by updating the user\ninterface in some manner. Alternatively, the user might want to cancel the task, which\ntherefore requires the user interface to somehow send a message to the background\ntask telling it to stop. Both these cases require careful thought and design and suit-\nable synchronization, but the concerns are still separate. The user interface thread\nstill handles the user interface, but it might have to update it when asked to do so by\nother threads. Likewise, the thread running the background task still focuses on the\noperations required for that task; it just happens that one of them is “allow task to\nbe stopped by another thread.” In neither case do the threads care where the\nrequest came from, only that it was intended for them and relates directly to their\nresponsibilities.\n There are two big dangers with separating concerns with multiple threads. The\nfirst is that you’ll end up separating the wrong concerns. The symptoms to check for\nare that there is a lot of data shared between the threads or the different threads end\nup waiting for each other; both cases boil down to too much communication between\nthreads. If this happens, it’s worth looking at the reasons for the communication. If all\nthe communication relates to the same issue, maybe that should be the key responsi-\nbility of a single thread and extracted from all the threads that refer to it. Alterna-\ntively, if two threads are communicating a lot with each other but much less with other\nthreads, maybe they should be combined into a single thread.\n When dividing work across threads by task type, you don’t have to limit yourself to\ncompletely isolated cases. If multiple sets of input data require the same sequence of\noperations to be applied, you can divide the work so each thread performs one stage\nfrom the overall sequence.\nDIVIDING A SEQUENCE OF TASKS BETWEEN THREADS\nIf your task consists of applying the same sequence of operations to many indepen-\ndent data items, you can use a pipeline to exploit the available concurrency of your sys-\ntem. This is by analogy to a physical pipeline: data flows in at one end through a series\nof operations (pipes) and out at the other end.\n To divide the work this way, you create a separate thread for each stage in the\npipeline—one thread for each of the operations in the sequence. When the opera-\ntion is completed, the data element is put in a queue to be picked up by the next\nthread. This allows the thread performing the first operation in the sequence to\nstart on the next data element while the second thread in the pipeline is working on\nthe first element.\n",
      "content_length": 3309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "260\nCHAPTER 8\nDesigning concurrent code\n This is an alternative to dividing the data between threads, as described in sec-\ntion 8.1.1, and is appropriate in circumstances where the input data itself isn’t all\nknown when the operation is started. For example, the data might be coming in over\na network, or the first operation in the sequence might be to scan a filesystem in order\nto identify files to process.\n Pipelines are also good when each operation in the sequence is time-consuming;\nby dividing the tasks between threads rather than the data, you change the perfor-\nmance profile. Suppose you have 20 data items to process on 4 cores, and each data\nitem requires 4 steps, which take 3 seconds each. If you divide the data between four\nthreads, then each thread has five items to process. Assuming there’s no other pro-\ncessing that might affect the timings, after 12 seconds you’ll have 4 items processed,\nafter 24 seconds 8 items processed, and so on. All 20 items will be done after 1 minute.\nWith a pipeline, things work differently. Each of your four steps can be assigned to a\nprocessing core. Now the first item has to be processed by each core, so it still takes\nthe full 12 seconds. Indeed, after 12 seconds you only have 1 item processed, which\nisn’t as good as with the division by data. But once the pipeline is primed, things pro-\nceed a bit differently; after the first core has processed the first item, it moves on to\nthe second, so once the final core has processed the first item, it can perform its step\non the second. You now get 1 item processed every 3 seconds rather than having the\nitems processed in batches of 4 every 12 seconds.\n The overall time to process the entire batch takes longer because you have to wait\nnine seconds before the final core starts processing the first item. But smoother, more\nregular processing can be beneficial in some circumstances. Consider, for example, a\nsystem for watching high-definition digital videos. In order for the video to be watch-\nable, you typically need at least 25 frames per second and ideally more. Also, the\nviewer needs these to be evenly spaced to give the impression of continuous move-\nment; an application that can decode 100 frames per second is still of no use if it\npauses for a second, then displays 100 frames, then pauses for another second, and\ndisplays another 100 frames. On the other hand, viewers are probably happy to accept\na delay of a couple of seconds when they start watching a video. In this case, paralleliz-\ning using a pipeline that outputs frames at a nice steady rate is probably preferable.\n Having looked at various techniques for dividing the work between threads, let’s\ntake a look at the factors affecting the performance of a multithreaded system and\nhow that can impact your choice of techniques.\n8.2\nFactors affecting the performance of concurrent code\nIf you’re using concurrency to improve the performance of your code on systems with\nmultiple processors, you need to know what factors are going to affect the perfor-\nmance. Even if you’re using multiple threads to separate concerns, you need to\nensure that this doesn’t adversely affect the performance. Customers won’t thank you\nif your application runs more slowly on their shiny new 16-core machine than it did on\ntheir old single-core one.\n",
      "content_length": 3317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "261\nFactors affecting the performance of concurrent code\n As you’ll see shortly, many factors affect the performance of multithreaded code—\neven something as simple as changing which data elements are processed by each\nthread (while keeping everything else identical) can have a dramatic effect on perfor-\nmance. Without further ado, let’s look at some of these factors, starting with the obvi-\nous one: how many processors does your target system have?\n8.2.1\nHow many processors?\nThe number (and structure) of processors is the first big factor that affects the perfor-\nmance of a multithreaded application, and it’s a crucial one. In some cases you know\nexactly what the target hardware is and can design with this in mind, taking real mea-\nsurements on the target system or an exact duplicate. If so, you’re one of the lucky\nones; in general, you don’t have that luxury. You might be developing on a similar sys-\ntem, but the differences can be crucial. For example, you might be developing on a\ndual- or quad-core system, but your customers’ systems may have one multicore proces-\nsor (with any number of cores), or multiple single-core processors, or even multiple\nmulticore processors. The behavior and performance characteristics of a concurrent\nprogram can vary considerably under these different circumstances, so you need to\nthink carefully about what the impact may be and test things where possible.\n To a first approximation, a single 16-core processor is the same as 4 quad-core pro-\ncessors or 16 single-core processors: in each case the system can run 16 threads con-\ncurrently. If you want to take advantage of this, your application must have at least 16\nthreads. If it has fewer than 16, you’re leaving processor power on the table (unless\nthe system is running other applications too, but we’ll ignore that possibility for now).\nOn the other hand, if you have more than 16 threads ready to run (and not blocked,\nwaiting for something), your application will waste processor time switching between\nthe threads, as discussed in chapter 1. When this happens, the situation is called over-\nsubscription.\n To allow applications to scale the number of threads in line with the number of\nthreads the hardware can run concurrently, the C++11 Standard Thread Library pro-\nvides std::thread::hardware_concurrency(). You’ve already seen how that can be\nused to scale the number of threads to the hardware.\n Using std::thread::hardware_concurrency() directly requires care; your code\ndoesn’t take into account any of the other threads that are running on the system\nunless you explicitly share that information. In the worst-case scenario, if multiple\nthreads call a function that uses std::thread::hardware_concurrency() for scaling\nat the same time, there will be huge oversubscription. std::async() avoids this prob-\nlem because the library is aware of all calls and can schedule appropriately. Careful\nuse of thread pools can also avoid this problem.\n But even if you take into account all threads running in your application, you’re\nstill subject to the impact of other applications running at the same time. Although\nthe use of multiple CPU-intensive applications simultaneously is rare on single-user\nsystems, there are some domains where it’s more common. Systems designed to handle\n",
      "content_length": 3299,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "262\nCHAPTER 8\nDesigning concurrent code\nthis scenario typically offer mechanisms to allow each application to choose an appro-\npriate number of threads, although these mechanisms are outside the scope of the\nC++ Standard. One option is for a facility like std::async() to take into account the\ntotal number of asynchronous tasks run by all applications when choosing the num-\nber of threads. Another is to limit the number of processing cores that can be used by\na given application. I’d expect this limit to be reflected in the value returned by\nstd::thread::hardware_concurrency() on these platforms, although this isn’t guar-\nanteed. If you need to handle this scenario, consult your system documentation to see\nwhat options are available to you.\n One additional twist to this situation is that the ideal algorithm for a problem can\ndepend on the size of the problem compared to the number of processing units. If\nyou have a massively parallel system with many processing units, an algorithm that per-\nforms more operations overall may finish more quickly than one that performs fewer\noperations, because each processor performs only a few operations.\n As the number of processors increases, so does the likelihood and performance\nimpact of another problem: that of multiple processors trying to access the same data.\n8.2.2\nData contention and cache ping-pong\nIf two threads are executing concurrently on different processors and they’re both\nreading the same data, this usually won’t cause a problem; the data will be copied into\ntheir respective caches, and both processors can proceed. But if one of the threads\nmodifies the data, this change then has to propagate to the cache on the other core,\nwhich takes time. Depending on the nature of the operations on the two threads, and\nthe memory orderings used for the operations, this modification may cause the sec-\nond processor to stop in its tracks and wait for the change to propagate through the\nmemory hardware. In terms of CPU instructions, this can be a phenomenally slow oper-\nation, equivalent to many hundreds of individual instructions, although the exact tim-\ning depends primarily on the physical structure of the hardware.\n Consider the following simple piece of code:\nstd::atomic<unsigned long> counter(0);\nvoid processing_loop()\n{\n    while(counter.fetch_add(1,std::memory_order_relaxed)<100000000)\n    {\n        do_something();\n    }\n}\nThe counter is global, so any threads that call processing_loop() are modifying the\nsame variable. Therefore, for each increment the processor must ensure it has an\nup-to-date copy of counter in its cache, modify the value, and publish it to other pro-\ncessors. Even though you’re using std::memory_order_relaxed, so the compiler\ndoesn’t have to synchronize any other data, fetch_add is a read-modify-write opera-\ntion and therefore needs to retrieve the most recent value of the variable. If another\n",
      "content_length": 2909,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "263\nFactors affecting the performance of concurrent code\nthread on another processor is running the same code, the data for counter must\ntherefore be passed back and forth between the two processors and their correspond-\ning caches so that each processor has the latest value for counter when it does the\nincrement. If do_something() is short enough, or if there are too many processors\nrunning this code, the processors might find themselves waiting for each other; one\nprocessor is ready to update the value, but another processor is currently doing that,\nso it has to wait until the second processor has completed its update and the change\nhas propagated. This situation is called high contention. If the processors rarely have to\nwait for each other, you have low contention.\n In a loop like this one, the data for counter will be passed back and forth between\nthe caches many times. This is called cache ping-pong, and it can seriously impact the\nperformance of the application. If a processor stalls because it has to wait for a cache\ntransfer, it can’t do any work in the meantime, even if there are other threads waiting\nthat could do useful work, so this is bad news for the whole application.\n You might think that this won’t happen to you; after all, you don’t have any loops\nlike that. Are you sure? What about mutex locks? If you acquire a mutex in a loop,\nyour code is similar to the previous code from the point of view of data accesses. In\norder to lock the mutex, another thread must transfer the data that makes up the\nmutex to its processor and modify it. When it’s done, it modifies the mutex again to\nunlock it, and the mutex data has to be transferred to the next thread to acquire the\nmutex. This transfer time is in addition to any time that the second thread has to wait\nfor the first to release the mutex:\nstd::mutex m;\nmy_data data;\nvoid processing_loop_with_mutex()\n{\n    while(true)\n    {\n        std::lock_guard<std::mutex> lk(m);\n        if(done_processing(data)) break;\n    }\n}\nNow, here’s the worst part: if the data and mutex are accessed by more than one\nthread, then as you add more cores and processors to the system, it becomes more\nlikely that you will get high contention and one processor having to wait for another.\nIf you’re using multiple threads to process the same data more quickly, the threads are\ncompeting for the data and thus competing for the same mutex. The more of them\nthere are, the more likely they’ll try to acquire the mutex at the same time, or access\nthe atomic variable at the same time, and so forth.\n The effects of contention with mutexes are usually different from the effects of\ncontention with atomic operations for the simple reason that the use of a mutex natu-\nrally serializes threads at the operating system level rather than at the processor level.\nIf you have enough threads ready to run, the operating system can schedule another\n",
      "content_length": 2907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "264\nCHAPTER 8\nDesigning concurrent code\nthread to run while one thread is waiting for the mutex, whereas a processor stall pre-\nvents any threads from running on that processor. But it will still impact the perfor-\nmance of those threads that are competing for the mutex; they can only run one at a\ntime, after all.\n Back in chapter 3, you saw how a rarely updated data structure can be protected\nwith a single-writer, multiple-reader mutex (see section 3.3.2). Cache ping-pong\neffects can nullify the benefits of this mutex if the workload is unfavorable, because all\nthreads accessing the data (even reader threads) still have to modify the mutex itself.\nAs the number of processors accessing the data goes up, the contention on the mutex\nitself increases, and the cache line holding the mutex must be transferred between\ncores, potentially increasing the time taken to acquire and release locks to undesir-\nable levels. There are techniques to ameliorate this problem by spreading out the\nmutex across multiple cache lines, but unless you implement your own mutex, you are\nsubject to whatever your system provides.\n If this cache ping-pong is bad, how can you avoid it? As you’ll see later in the chap-\nter, the answer ties in nicely with general guidelines for improving the potential for\nconcurrency: do what you can to reduce the potential for two threads competing for\nthe same memory location.\n It’s not quite that simple, though; things never are. Even if a particular memory\nlocation is only ever accessed by one thread, you can still get cache ping-pong due to\nan effect known as false sharing.\n8.2.3\nFalse sharing\nProcessor caches don’t generally deal in individual memory locations; instead, they\ndeal in blocks of memory called cache lines. These blocks of memory are typically 32 or\n64 bytes in size, but the exact details depend on the particular processor model being\nused. Because the cache hardware only deals in cache-line-sized blocks of memory,\nsmall data items in adjacent memory locations will be in the same cache line. Sometimes\nthis is good: if a set of data accessed by a thread is in the same cache line, this is better\nfor the performance of the application than if the same set of data was spread over mul-\ntiple cache lines. But if the data items in a cache line are unrelated and need to be\naccessed by different threads, this can be a major cause of performance problems.\n Suppose you have an array of int values and a set of threads that each access their\nown entry in the array but do so repeatedly, including updates. Because an int is typi-\ncally much smaller than a cache line, quite a few of those array entries will be in the\nsame cache line. Consequently, even though each thread only accesses its own array\nentry, the cache hardware still has to play cache ping-pong. Every time the thread\naccessing entry 0 needs to update the value, ownership of the cache line needs to be\ntransferred to the processor running that thread, only to be transferred to the cache\nfor the processor running the thread for entry 1 when that thread needs to update its\ndata item. The cache line is shared, even though none of the data is, hence the term\nfalse sharing. The solution here is to structure the data so that data items to be accessed\n",
      "content_length": 3267,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "265\nFactors affecting the performance of concurrent code\nby the same thread are close together in memory (and thus more likely to be in the\nsame cache line), whereas those that are to be accessed by separate threads are far\napart in memory and thus more likely to be in separate cache lines. You’ll see how this\naffects the design of the code and data later in this chapter. The C++17 standard\ndefines std::hardware_destructive_interference_size in the header <new>, which\nspecifies the maximum number of consecutive bytes that may be subject to false shar-\ning for the current compilation target. If you ensure that your data is at least this num-\nber of bytes apart, then there will be no false sharing.\n If having multiple threads access data from the same cache line is bad, how does\nthe memory layout of data accessed by a single thread affect things?\n8.2.4\nHow close is your data?\nAlthough false sharing is caused by having data accessed by one thread too close to\ndata accessed by another thread, another pitfall associated with data layout directly\nimpacts the performance of a single thread on its own. The issue is data proximity: if\nthe data accessed by a single thread is spread out in memory, it’s likely that it lies on\nseparate cache lines. On the flip side, if the data accessed by a single thread is close\ntogether in memory, it’s more likely to lie on the same cache line. Consequently, if\ndata is spread out, more cache lines must be loaded from memory onto the processor\ncache, which can increase memory access latency and reduce performance compared\nto data that’s located close together.\n Also, if the data is spread out, there’s an increased chance that a given cache line\ncontaining data for the current thread also contains data that’s not for the current\nthread. At the extreme, there’ll be more data in the cache that you don’t care about\nthan data that you do. This wastes precious cache space and increases the chance that\nthe processor will experience a cache miss and have to fetch a data item from main\nmemory even if it once held it in the cache, because it had to remove the item from\nthe cache to make room for another.\n Now, this is important with single-threaded code, so why am I bringing it up here?\nThe reason is task switching. If there are more threads than cores in the system, each\ncore is going to be running multiple threads. This increases the pressure on the\ncache, as you try to ensure that different threads are accessing different cache lines in\norder to avoid false sharing. Consequently, when the processor switches threads, it’s\nmore likely to have to reload the cache lines if each thread uses data spread across\nmultiple cache lines than if each thread’s data is close together in the same cache line.\nThe C++17 standard specifies the constant std::hardware_constructive_interfer-\nence_size, also in the header <new>, which is the maximum number of consecutive\nbytes guaranteed to be on the same cache line (if suitably aligned). If you can fit data\nthat is needed together within this number of bytes, it will potentially reduce the num-\nber of cache misses.\n If there are more threads than cores or processors, the operating system might also\nchoose to schedule a thread on one core for one time slice and then on another core\n",
      "content_length": 3285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "266\nCHAPTER 8\nDesigning concurrent code\nfor the next time slice. This will therefore require transferring the cache lines for that\nthread’s data from the cache for the first core to the cache for the second; the more\ncache lines that need transferring, the more time-consuming this will be. Although\noperating systems typically avoid this when they can, it does happen and does impact\nperformance.\n Task-switching problems are particularly prevalent when lots of threads are ready to\nrun as opposed to waiting. This is an issue we’ve already touched on: oversubscription.\n8.2.5\nOversubscription and excessive task switching\nIn multithreaded systems, it’s typical to have more threads than processors, unless\nyou’re running on massively parallel hardware. But threads often spend time waiting\nfor external I/O to complete, blocked on mutexes, waiting for condition variables, and\nso forth, so this isn’t a problem. Having the extra threads enables the application to per-\nform useful work rather than having processors sitting idle while the threads wait.\n This isn’t always a good thing. If you have too many additional threads, there will\nbe more threads ready to run than there are available processors, and the operating\nsystem will have to start task switching quite heavily in order to ensure they all get a\nfair time slice. As you saw in chapter 1, this can increase the overhead of the task\nswitching as well as compound any cache problems resulting from lack of proximity.\nOversubscription can arise when you have a task that repeatedly spawns new threads\nwithout limits, as the recursive quick sort from chapter 4 did, or where the natural\nnumber of threads when you separate by task type is more than the number of proces-\nsors and the work is naturally CPU-bound rather than I/O-bound.\n If you’re spawning too many threads because of data division, you can limit the\nnumber of worker threads, as you saw in section 8.1.2. If the oversubscription is due to\nthe natural division of work, there’s not a lot you can do to ameliorate the problem\nsave choosing a different division. In that case, choosing the appropriate division may\nrequire more knowledge of the target platform than you have available and is only\nworth doing if performance is unacceptable and it can be demonstrated that chang-\ning the division of work does improve performance.\n Other factors can affect the performance of multithreaded code. The cost of cache\nping-pong can vary quite considerably between two single-core processors and a single\ndual-core processor, even if they’re the same CPU type and clock speed, for example,\nbut these are the major ones that will have a visible impact. Let’s now look at how that\naffects the design of the code and data structures.\n8.3\nDesigning data structures for multithreaded \nperformance\nIn section 8.1 we looked at various ways of dividing work between threads, and in sec-\ntion 8.2 we looked at various factors that can affect the performance of your code.\nHow can you use this information when designing data structures for multithreaded\nperformance? This is a different question than that addressed in chapters 6 and 7,\n",
      "content_length": 3147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "267\nDesigning data structures for multithreaded performance\nwhich were about designing data structures that are safe for concurrent access. As\nyou’ve seen in section 8.2, the layout of the data used by a single thread can have an\nimpact, even if that data isn’t shared with any other threads.\n The key things to bear in mind when designing your data structures for multi-\nthreaded performance are contention, false sharing, and data proximity. All three of these\ncan have a big impact on performance, and you can often improve things by altering\nthe data layout or changing which data elements are assigned to which thread. First\noff, let’s look at an easy win: dividing array elements between threads.\n8.3.1\nDividing array elements for complex operations\nSuppose you’re doing some heavy-duty math, and you need to multiply two large\nsquare matrices together. To multiply matrices, you multiply each element in the first\nrow of the first matrix with the corresponding element of the first column of the second\nmatrix and add up the products to give the top-left element of the result. You then\nrepeat this with the second row and the first column to give the second element in the\nfirst column of the result, and with the first row and second column to give the first\nelement in the second column of the result, and so forth. This is shown in figure 8.3;\nthe highlighting shows that the second row of the first matrix is paired with the third\ncolumn of the second matrix to give the entry in the second row of the third column\nof the result.\nNow let’s assume that these are large matrices with several thousand rows and col-\numns, in order to make it worthwhile to use multiple threads to optimize the multipli-\ncation. Typically, a nonsparse matrix is represented by a big array in memory, with all\nthe elements of the first row followed by all the elements of the second row, and so\nforth. To multiply your matrices you have three of these huge arrays. In order to get\noptimal performance, you need to pay careful attention to the data access patterns,\nparticularly the writes to the third array.\n There are many ways you can divide the work between threads. Assuming you have\nmore rows/columns than available processors, you could have each thread calculate\nthe values for a number of columns in the result matrix, or have each thread calculate\na1,1 a2,1 a3,1 a4,1\nan,1\na1,2 a2,2 a3,2 a4,2\nan,2\na1,3 a2,3 a3,3 a4,3\nan,3\na1,ma2,ma3,ma4,m\nan,m\nb1,1 b2,1 b3,1 b4,1\nbk,1\nb1,2 b2,2 b3,2 b4,2\nbk,2\nb1,3 b2,3 b3,3 b4,3\nbk,3\nb1,n b2,n b3,n b4,n\nbk,n\nc1,1 c2,1 c3,1 c4,1\nck,1\nc1,2 c2,2 c3,2 c4,2\nck,2\nc1,3 c2,3 c3,3 c4,3\nck,3\nc1,m c2,m c3,m c4,m\nck,m\n=\nFigure 8.3\nMatrix multiplication\n",
      "content_length": 2680,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "268\nCHAPTER 8\nDesigning concurrent code\nthe results for a number of rows, or even have each thread calculate the results for a\nrectangular subset of the matrix.\n Back in sections 8.2.3 and 8.2.4, you saw that it’s better to access contiguous ele-\nments from an array rather than values all over the place, because this reduces cache\nusage and the chance of false sharing. If you have each thread compute a set of col-\numns, it needs to read every value from the first matrix and the values from the corre-\nsponding columns in the second matrix, but you only have to write the column values.\nGiven that the matrices are stored with the rows contiguous, this means that you’re\naccessing N elements from the first row, N elements from the second, and so forth\n(where N is the number of columns you’re processing). Because other threads will be\naccessing the other elements of each row, it’s clear that you ought to be accessing adja-\ncent columns, so the N elements from each row are adjacent, and you minimize false\nsharing. If the space occupied by your N elements is an exact number of cache lines,\nthere’ll be no false sharing because threads will be working on separate cache lines.\n On the other hand, if you have each thread compute a set of rows, then it needs to\nread every value from the second matrix and the values from the corresponding rows of\nthe first matrix, but it only has to write the row values. Because the matrices are stored\nwith the rows contiguous, you’re now accessing all elements from N rows. If you again\nchoose adjacent rows, this means that the thread is now the only thread writing to\nthose N rows; it has a contiguous block of memory that’s not touched by any other\nthread. This is likely an improvement over having each thread compute a set of col-\numns, because the only possibility of false sharing is for the last few elements of one\nblock with the first few of the next, but it’s worth timing it on the target architecture\nto confirm.\n What about your third option—dividing into rectangular blocks? This can be\nviewed as dividing into columns and then dividing into rows. As such, it has the same\nfalse-sharing potential as division by columns. If you can choose the number of col-\numns in the block to avoid this possibility, there’s an advantage to rectangular division\nfrom the read side: you don’t need to read the entirety of either source matrix. You\nonly need to read the values corresponding to the rows and columns of the target rect-\nangle. To look at this in concrete terms, consider multiplying two matrices that have\n1,000 rows and 1,000 columns. That’s 1 million elements. If you have 100 processors,\nthey can compute 10 rows each for a nice round 10,000 elements. But to calculate the\nresults of those 10,000 elements, they need to access the entirety of the second matrix\n(1 million elements) plus the 10,000 elements from the corresponding rows in the\nfirst matrix, for a grand total of 1,010,000 elements. On the other hand, if they each\ncompute a block of 100 elements by 100 elements (which is still 10,000 elements\ntotal), they need to access the values from 100 rows of the first matrix (100 x 1,000 =\n100,000 elements) and 100 columns of the second matrix (another 100,000). This is\nonly 200,000 elements, which is a five-fold reduction in the number of elements read.\nIf you’re reading fewer elements, there’s less chance of a cache miss and the potential\nfor greater performance.\n",
      "content_length": 3441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "269\nDesigning data structures for multithreaded performance\n It may therefore be better to divide the result matrix into small, square or almost-\nsquare blocks rather than have each thread compute the entirety of a small number of\nrows. You can adjust the size of each block at runtime, depending on the size of the\nmatrices and the available number of processors. As ever, if performance is important,\nit’s vital to profile various options on the target architecture, and check the literature\nrelevant to the field—I make no claim that these are the only or best options if you are\ndoing matrix multiplication\n Chances are you’re not doing matrix multiplication, so how does this apply to you?\nThe same principles apply to any situation where you have large blocks of data to\ndivide between threads; look at all the aspects of the data access patterns carefully, and\nidentify the potential causes of performance hits. There may be similar circumstances\nin your problem domain where changing the division of work can improve perfor-\nmance without requiring any change to the basic algorithm.\n OK, so we’ve looked at how access patterns in arrays can affect performance. What\nabout other types of data structures?\n8.3.2\nData access patterns in other data structures\nFundamentally, the same considerations apply when trying to optimize the data access\npatterns of other data structures as when optimizing access to arrays:\nTry to adjust the data distribution between threads so that data that’s close\ntogether is worked on by the same thread.\nTry to minimize the data required by any given thread.\nTry to ensure that data accessed by separate threads is sufficiently far apart to\navoid false sharing using std::hardware_destructive_interference_size as\na guide.\nThat’s not easy to apply to other data structures. For example, binary trees are inher-\nently difficult to subdivide in any unit other than a subtree, which may or may not be\nuseful, depending on how balanced the tree is and how many sections you need to\ndivide it into. Also, the nature of the trees means that the nodes are likely dynamically\nallocated and thus end up in different places on the heap.\n Now, having data end up in different places on the heap isn’t a particular problem\nin itself, but it does mean that the processor has to keep more things in cache. This\ncan be beneficial. If multiple threads need to traverse the tree, then they all need to\naccess the tree nodes, but if the tree nodes only contain pointers to the real data held\nat the node, then the processor only has to load the data from memory if it’s\nneeded. If the data is being modified by the threads that need it, this can avoid the\nperformance hit of false sharing between the node data itself and the data that pro-\nvides the tree structure.\n There’s a similar issue with data protected by a mutex. Suppose you have a simple\nclass that contains a few data items and a mutex used to protect accesses from multiple\nthreads. If the mutex and the data items are close together in memory, this is ideal for\n",
      "content_length": 3049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "270\nCHAPTER 8\nDesigning concurrent code\na thread that acquires the mutex; the data it needs may already be in the processor\ncache, because it was loaded in order to modify the mutex. But there’s also a downside:\nif other threads try to lock the mutex while it’s held by the first thread, they’ll need\naccess to that memory. Mutex locks are typically implemented as a read-modify-write\natomic operation on a memory location within the mutex to try to acquire the mutex,\nfollowed by a call to the operating system kernel if the mutex is already locked. This\nread-modify-write operation may cause the data held in the cache by the thread that\nowns the mutex to be invalidated. As far as the mutex goes, this isn’t a problem; that\nthread isn’t going to touch the mutex until it unlocks it. But if the mutex shares a\ncache line with the data being used by the thread, the thread that owns the mutex can\ntake a performance hit because another thread tried to lock the mutex!\n One way to test whether this kind of false sharing is a problem is to add huge\nblocks of padding between the data elements that can be concurrently accessed by dif-\nferent threads. For example, you can use\nstruct protected_data\n{\n    std::mutex m;\n    char padding[std::hardware_destructive_interference_size];     \n    my_data data_to_protect;\n};\nto test the mutex contention issue or\nstruct my_data\n{\n    data_item1 d1;\n    data_item2 d2;\n    char padding[std::hardware_destructive_interference_size];\n};\nmy_data some_array[256];\nto test for false sharing of array data. If this improves the performance, you know that\nfalse sharing was a problem, and you can either leave the padding in or work to elimi-\nnate the false sharing in another way by rearranging the data accesses.\n There’s more than the data access patterns to consider when designing for concur-\nrency, so let’s look at some of these additional considerations.\n8.4\nAdditional considerations when designing for \nconcurrency\nSo far in this chapter we’ve looked at ways of dividing work between threads, factors\naffecting performance, and how these factors affect your choice of data access pat-\nterns and data structures. There’s more to designing code for concurrency than that,\nthough. You also need to consider things such as exception safety and scalability. Code\nis said to be scalable if the performance (whether in terms of reduced speed of execution\nIf std::hardware_destructive_interference_size is\nnot available with your compiler, you could use\nsomething like 65536 bytes which is likely to be\norders of magnitude larger than a cache line\n",
      "content_length": 2585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "271\nAdditional considerations when designing for concurrency\nor increased throughput) increases as more processing cores are added to the system.\nIdeally, the performance increase is linear, so a system with 100 processors performs\n100 times better than a system with one processor.\n Although code can work even if it isn’t scalable—a single-threaded application is\ncertainly not scalable, for example—exception safety is a matter of correctness. If your\ncode isn’t exception-safe, you can end up with broken invariants or race conditions, or\nyour application might terminate unexpectedly because an operation threw an excep-\ntion. With this in mind, we’ll look at exception safety first.\n8.4.1\nException safety in parallel algorithms\nException safety is an essential aspect of good C++ code, and code that uses concur-\nrency is no exception. In fact, parallel algorithms often require that you take more\ncare with exceptions than normal sequential algorithms. If an operation in a sequen-\ntial algorithm throws an exception, the algorithm only has to worry about ensuring\nthat it tidies up after itself to avoid resource leaks and broken invariants; it can mer-\nrily allow the exception to propagate to the caller for them to handle. By contrast, in\na parallel algorithm many of the operations will be running on separate threads. In\nthis case, the exception can’t be allowed to propagate because it’s on the wrong call\nstack. If a function spawned on a new thread exits with an exception, the application\nis terminated.\n As a concrete example, let’s revisit the parallel_accumulate function from list-\ning 2.9, which is reproduced here.\ntemplate<typename Iterator,typename T>\nstruct accumulate_block\n{\n    void operator()(Iterator first,Iterator last,T& result)\n    {\n        result=std::accumulate(first,last,result);    \n    }\n};\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);    \n    if(!length)\n        return init;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\n    std::vector<T> results(num_threads);           \nListing 8.2\nA naive parallel version of std::accumulate (from listing 2.9)\nb\nc\nd\n",
      "content_length": 2545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "272\nCHAPTER 8\nDesigning concurrent code\n    std::vector<std::thread>  threads(num_threads-1);    \n    Iterator block_start=first;      \n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_end=block_start;     \n        std::advance(block_end,block_size);\n        threads[i]=std::thread(             \n            accumulate_block<Iterator,T>(),\n            block_start,block_end,std::ref(results[i]));\n        block_start=block_end;  \n    }\n    accumulate_block<Iterator,T>()(\n        block_start,last,results[num_threads-1]);    \n    std::for_each(threads.begin(),threads.end(),\n        std::mem_fn(&std::thread::join));\n    return std::accumulate(results.begin(),results.end(),init);   \n}\nNow let’s go through and identify the places where an exception can be thrown: any-\nwhere where you call a function you know can throw or you perform an operation on\na user-defined type that may throw.\n First up, you have the call to distance c, which performs operations on the user-\nsupplied iterator type. Because you haven’t yet done any work, and this is on the call-\ning thread, it’s fine. Next up, you have the allocation of the results vector d and the\nthreads vector e. Again, these are on the calling thread, and you haven’t done any\nwork or spawned any threads, so this is fine. If the construction of threads throws, the\nmemory allocated for results will have to be cleaned up, but the destructor will take\ncare of that for you.\n Skipping over the initialization of block_start f, because that’s similarly safe,\nyou come to the operations in the thread-spawning loop, g, h, and i. Once you’ve\nbeen through the creation of the first thread at h, you’re in trouble if you throw any\nexceptions; the destructors of your new std::thread objects will call std::terminate\nand abort your program. This isn’t a good place to be.\n The call to accumulate_block j, can potentially throw, with similar conse-\nquences; your thread objects will be destroyed and call std::terminate. On the other\nhand, the final call to std::accumulate 1) can throw without causing any hardship,\nbecause all the threads have been joined by this point.\n That’s it for the main thread, but there’s more: the calls to accumulate_block on\nthe new threads might throw at B. There aren’t any catch blocks, so this exception\nwill be left unhandled and cause the library to call std::terminate() to abort the\napplication.\n In case it’s not glaringly obvious, this code isn’t exception-safe.\nADDING EXCEPTION SAFETY\nOK, so we’ve identified all the possible throw points and the nasty consequences of\nexceptions. What can you do about it? Let’s start by addressing the issue of the excep-\ntions thrown on your new threads.\ne\nf\ng\nh\ni\nj\n1)\n",
      "content_length": 2723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "273\nAdditional considerations when designing for concurrency\n You encountered the tool for this job in chapter 4. If you look carefully at what\nyou’re trying to achieve with new threads, it’s apparent that you’re trying to calculate a\nresult to return while allowing for the possibility that the code might throw an excep-\ntion. This is precisely what the combination of std::packaged_task and std::future\nis designed for. If you rearrange your code to use std::packaged_task, you end up\nwith the following code.\ntemplate<typename Iterator,typename T>\nstruct accumulate_block\n{\n    T operator()(Iterator first,Iterator last)   \n    {\n        return std::accumulate(first,last,T());    \n    }\n};\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return init;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\n    std::vector<std::future<T> > futures(num_threads-1);    \n    std::vector<std::thread> threads(num_threads-1);\n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_end=block_start;\n        std::advance(block_end,block_size);\n        std::packaged_task<T(Iterator,Iterator)> task(    \n            accumulate_block<Iterator,T>());\n        futures[i]=task.get_future();     \n        threads[i]=std::thread(std::move(task),block_start,block_end); \n        block_start=block_end;\n    }\n    T last_result=accumulate_block<Iterator,T>()(block_start,last);   \n    std::for_each(threads.begin(),threads.end(),\n        std::mem_fn(&std::thread::join));\n    T result=init;    \n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        result+=futures[i].get();   \n    }\nListing 8.3\nA parallel version of std::accumulate using std::packaged_task\nb\nc\nd\ne\nf\ng\nh\ni\nj\n",
      "content_length": 2186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "274\nCHAPTER 8\nDesigning concurrent code\n    result += last_result;    \n    return result;\n}\nThe first change is that the function call operator of accumulate_block now returns\nthe result directly, rather than taking a reference to somewhere to store it B. You’re\nusing std::packaged_task and std::future for the exception safety, so you can use it\nto transfer the result too. This does require that you explicitly pass a default-constructed\nT in the call to std::accumulate c, rather than reusing the supplied result value,\nbut that’s a minor change.\n The next change is that rather than having a vector of results, you have a vector of\nfutures d to store an std::future<T> for each spawned thread. In the thread-\nspawning loop, you first create a task for accumulate_block e. std::packaged\n_task<T(Iterator, Iterator)> declares a task that takes two Iterators and returns\na T, which is what your function does. You then get the future for that task f and run\nthat task on a new thread, passing in the start and end of the block to process g.\nWhen the task runs, the result will be captured in the future, as will any exception\nthrown.\n Because you’ve been using futures, you don’t have a result array, so you must store\nthe result from the final block in a variable h, rather than in a slot in the array. Also,\nbecause you have to get the values out of the futures, it’s now simpler to use a basic\nfor loop rather than std::accumulate, starting with the supplied initial value i and\nadding in the result from each future j. If the corresponding task threw an excep-\ntion, this will have been captured in the future and will now be thrown again by the\ncall to get(). Finally, you add the result from the last block 1) before returning the\noverall result to the caller.\n So, that’s removed one of the potential problems: exceptions thrown in the worker\nthreads are rethrown in the main thread. If more than one of the worker threads\nthrows an exception, only one will be propagated, but that’s not too big a deal. If it\nmatters, you can use something like std::nested_exception to capture all the excep-\ntions and throw that instead.\n The remaining problem is the leaking threads if an exception is thrown between\nwhen you spawn the first thread and when you’ve joined with them all. The simplest\nsolution is to catch any exceptions, join with the threads that are still joinable(), and\nrethrow the exception:\ntry\n{\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        // ... as before\n    }\n    T last_result=accumulate_block<Iterator,T>()(block_start,last);\n    std::for_each(threads.begin(),threads.end(),\n        std::mem_fn(&std::thread::join));\n}\n1)\n",
      "content_length": 2665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "275\nAdditional considerations when designing for concurrency\ncatch(...)\n{\n    for(unsigned long i=0;i<(num_thread-1);++i)\n    {\n        if(threads[i].joinable())\n            thread[i].join();\n    }\n    throw;\n}\nNow this works. All the threads will be joined, no matter how the code leaves the\nblock. But try-catch blocks are ugly, and you have duplicate code. You’re joining\nthe threads both in the “normal” control flow and in the catch block. Duplicate code\nis rarely a good thing, because it means more places to change. Instead, let’s extract\nthis out into the destructor of an object; it is, after all, the idiomatic way of cleaning up\nresources in C++. Here’s your class:\nclass join_threads\n{\n    std::vector<std::thread>& threads;\npublic:\n    explicit join_threads(std::vector<std::thread>& threads_):\n        threads(threads_)\n    {}\n    ~join_threads()\n    {\n        for(unsigned long i=0;i<threads.size();++i)\n        {\n            if(threads[i].joinable())\n                threads[i].join();\n        }\n    }\n};\nThis is similar to your thread_guard class from listing 2.3, except it’s extended for the\nwhole vector of threads. You can then simplify your code as follows.\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return init;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\nListing 8.4\nAn exception-safe parallel version of std::accumulate \n",
      "content_length": 1822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "276\nCHAPTER 8\nDesigning concurrent code\n    std::vector<std::future<T> > futures(num_threads-1);\n    std::vector<std::thread> threads(num_threads-1);\n    join_threads joiner(threads);    \n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_end=block_start;\n        std::advance(block_end,block_size);\n        std::packaged_task<T(Iterator,Iterator)> task(\n            accumulate_block<Iterator,T>());\n        futures[i]=task.get_future();\n        threads[i]=std::thread(std::move(task),block_start,block_end);\n        block_start=block_end;\n    }\n    T last_result=accumulate_block<Iterator,T>()(block_start,last);\n    T result=init;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        result+=futures[i].get();   \n    }\n    result += last_result;\n    return result;\n}\nOnce you’ve created your container of threads, you create an instance of your new\nclass B to join with all the threads on exit. You can then remove your explicit join\nloop, safe in the knowledge that the threads will be joined however the function exits.\nNote that the calls to futures[i].get() c will block until the results are ready, so\nyou don’t need to have explicitly joined with the threads at this point. This is unlike\nthe original from listing 8.2, where you needed to have joined with the threads to\nensure that the results vector was correctly populated. Not only do you get exception-\nsafe code, but your function is shorter because you’ve extracted the join code into\nyour new (reusable) class.\nEXCEPTION SAFETY WITH STD::ASYNC()\nNow that you’ve seen what’s required for exception safety when explicitly managing\nthe threads, let’s take a look at the same thing done with std::async(). As you’ve\nalready seen, in this case the library takes care of managing the threads for you, and\nany threads spawned are completed when the future is ready. The key thing to note\nfor exception safety is that if you destroy the future without waiting for it, the destruc-\ntor will wait for the thread to complete. This neatly avoids the problem of leaked\nthreads that are still executing and holding references to the data. The next listing\nshows an exception-safe implementation using std::async().\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);    \nListing 8.5\nAn exception-safe parallel version of std::accumulate using std::async\nb\nc\nb\n",
      "content_length": 2488,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "277\nAdditional considerations when designing for concurrency\n    unsigned long const max_chunk_size=25;\n    if(length<=max_chunk_size)\n    {\n        return std::accumulate(first,last,init);   \n    }\n    else\n    {\n    Iterator mid_point=first;\n        std::advance(mid_point,length/2);    \n        std::future<T> first_half_result=\n            std::async(parallel_accumulate<Iterator,T>,     \n                       first,mid_point,init);\n        T second_half_result=parallel_accumulate(mid_point,last,T());  \n        return first_half_result.get()+second_half_result;    \n    }\n}\nThis version uses a recursive division of the data rather than pre-calculating the divi-\nsion of the data into chunks, but it’s a whole lot simpler than the previous version, and\nit’s still exception-safe. As before, you start by finding the length of the sequence B, and\nif it’s smaller than the maximum chunk size, you resort to calling std::accumulate\ndirectly c. If there are more elements than your chunk size, you find the midpoint d\nand then spawn an asynchronous task to handle that half e. The second half of the\nrange is handled with a direct recursive call f, and then the results from the two\nchunks are added together g. The library ensures that the std::async calls make use\nof the hardware threads that are available without creating an overwhelming number\nof threads. Some of the “asynchronous” calls will be executed synchronously in the\ncall to get() g.\n The beauty of this is that not only can it take advantage of the hardware concur-\nrency, but it’s also trivially exception-safe. If an exception is thrown by the recursive\ncall f, the future created from the call to std::async e will be destroyed as the\nexception propagates. This will in turn wait for the asynchronous task to finish, avoid-\ning a dangling thread. On the other hand, if the asynchronous call throws, this is cap-\ntured by the future, and the call to get() g will rethrow the exception.\n What other considerations do you need to take into account when designing con-\ncurrent code? Let’s look at scalability. How much does the performance improve if you\nmove your code to a system with more processors?\n8.4.2\nScalability and Amdahl’s law\nScalability is all about ensuring that your application can take advantage of additional\nprocessors in the system it’s running on. At one extreme you have a single-threaded\napplication that’s completely unscalable; even if you add 100 processors to your sys-\ntem, the performance will remain unchanged. At the other extreme you have some-\nthing like the SETI@Home (http://setiathome.ssl.berkeley.edu/) project, which is\ndesigned to take advantage of thousands of additional processors (in the form of indi-\nvidual computers added to the network by users) as they become available.\nc\nd\ne\nf\ng\n",
      "content_length": 2800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "278\nCHAPTER 8\nDesigning concurrent code\n For any given multithreaded program, the number of threads that are performing\nuseful work will vary as the program runs. Even if every thread is doing useful work for\nthe entirety of its existence, the application may initially have only one thread, which\nwill then have the task of spawning all the others. But even that’s a highly unlikely sce-\nnario. Threads often spend time waiting for each other or waiting for I/O operations\nto complete.\n Every time one thread has to wait for something (whatever that something is),\nunless there’s another thread ready to take its place on the processor, you have a pro-\ncessor sitting idle that could be doing useful work.\n A simplified way of looking at this is to divide the program into “serial” sections\nwhere only one thread is doing any useful work and “parallel” sections where all the\navailable processors are doing useful work. If you run your application on a system\nwith more processors, the “parallel” sections will theoretically be able to complete\nmore quickly, because the work can be divided between more processors, whereas the\n“serial” sections will remain serial. Under such a simplified set of assumptions, you can\ntherefore estimate the potential performance gain to be achieved by increasing the\nnumber of processors: if the “serial” sections constitute a fraction, fs, of the program,\nthen the performance gain, P, from using N processors can be estimated as\nThis is Amdahl’s law, which is often cited when talking about the performance of con-\ncurrent code. If everything can be parallelized, so the serial fraction is 0, the speedup\nis N. Alternatively, if the serial fraction is one-third, even with an infinite number of\nprocessors you’re not going to get a speedup of more than 3.\n But this paints a naive picture, because tasks are rarely infinitely divisible in the way\nthat would be required for the equation to hold, and it’s also rare for everything to be\nCPU-bound in the way that’s assumed. As you’ve seen, threads may wait for many\nthings while executing.\n One thing that’s clear from Amdahl’s law is that when you’re using concurrency\nfor performance, it’s worth looking at the overall design of the application to maxi-\nmize the potential for concurrency and ensure that there’s always useful work for the\nprocessors to be doing. If you can reduce the size of the “serial” sections or reduce the\npotential for threads to wait, you can improve the potential for performance gains on\nsystems with more processors. Alternatively, if you can provide more data for the sys-\ntem to process, and thus keep the parallel sections primed with work, you can reduce\nthe serial fraction and increase the performance gain, P.\n Scalability is about reducing the time it takes to perform an action or increasing the amount\nof data that can be processed in a given time as more processors are added. Sometimes\nthese are equivalent (you can process more data if each element is processed faster),\nbut not always. Before choosing the techniques to use for dividing work between\nP\n1\nfs\n1 fs\n–\nN\n---------\n+\n-------------------\n=\n",
      "content_length": 3131,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "279\nAdditional considerations when designing for concurrency\nthreads, it’s important to identify which of these aspects of scalability are important\nto you.\n I mentioned at the beginning of this section that threads don’t always have useful\nwork to do. Sometimes they have to wait for other threads, or for I/O to complete, or\nfor something else. If you give the system something useful to do during this wait, you\ncan effectively “hide” the waiting.\n8.4.3\nHiding latency with multiple threads\nFor most of the discussions of the performance of multithreaded code, we’ve been\nassuming that the threads are running “flat out” and always have useful work to do\nwhen they’re running on a processor. This is not true; in application code, threads fre-\nquently block while waiting for something. For example, they may be waiting for some\nI/O to complete, waiting to acquire a mutex, waiting for another thread to complete\nsome operation and notify a condition variable or populate a future, or even sleeping\nfor a period of time.\n Whatever the reason for the waits, if you have only as many threads as there are\nphysical processing units in the system, having blocked threads means you’re wasting\nCPU time. The processor that would otherwise be running a blocked thread is instead\ndoing nothing. Consequently, if you know that one of your threads is likely to spend a\nconsiderable portion of its time waiting around, you can make use of that spare CPU\ntime by running one or more additional threads.\n Consider a virus-scanner application, which divides the work across threads using\na pipeline. The first thread searches the filesystem for files to check and puts them\nin a queue. Meanwhile, another thread takes filenames from the queue, loads the files,\nand scans them for viruses. You know that the thread searching the filesystem for files\nto scan is definitely going to be I/O-bound, so you make use of the “spare” CPU time\nby running an additional scanning thread. You’d then have one file-searching thread\nand as many scanning threads as there are physical cores or processors in the system.\nBecause the scanning thread may also have to read significant portions of the files off\nthe disk in order to scan them, it might make sense to have even more scanning\nthreads. But at some point there’ll be too many threads, and the system will slow down\nagain as it spends more and more time task switching, as described in section 8.2.5.\n As ever, this is an optimization, so it’s important to measure performance before\nand after any change in the number of threads; the optimal number of threads will be\nhighly dependent on the nature of the work being done and the percentage of time\nthe thread spends waiting.\n Depending on the application, it might be possible to use up this spare CPU time\nwithout running additional threads. For example, if a thread is blocked because it’s\nwaiting for an I/O operation to complete, it might make sense to use asynchronous\nI/O if that’s available, and then the thread can perform other useful work while the\nI/O is performed in the background. In other cases, if a thread is waiting for another\nthread to perform an operation, then rather than blocking, the waiting thread might\n",
      "content_length": 3213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "280\nCHAPTER 8\nDesigning concurrent code\nbe able to perform that operation itself, as you saw with the lock-free queue in chap-\nter 7. In an extreme case, if a thread is waiting for a task to be completed and that task\nhasn’t yet been started by any thread, the waiting thread might perform the task in\nentirety itself or another task that’s incomplete. You saw an example of this in listing 8.1,\nwhere the sort function repeatedly tries to sort outstanding chunks as long as the\nchunks it needs are not yet sorted.\n Rather than adding threads to ensure that all available processors are being used,\nsometimes it pays to add threads to ensure that external events are handled in a timely\nmanner to increase the responsiveness of the system.\n8.4.4\nImproving responsiveness with concurrency\nMost modern graphical user interface frameworks are event-driven; the user performs\nactions on the user interface by pressing keys or moving the mouse, which generate\na series of events or messages that the application then handles. The system may also\ngenerate messages or events on its own. In order to ensure that all events and mes-\nsages are correctly handled, the application typically has an event loop that looks\nlike this:\nwhile(true)\n{\n    event_data event=get_event();\n    if(event.type==quit)\n        break;\n    process(event);\n}\nObviously, the details of the API will vary, but the structure is generally the same: wait\nfor an event, do whatever processing is necessary to handle it, and then wait for the\nnext one. If you have a single-threaded application, this can make long-running tasks\nhard to write, as described in section 8.1.3. In order to ensure that user input is han-\ndled in a timely manner, get_event() and process() must be called with reasonable\nfrequency, whatever the application is doing. This means that either the task must\nperiodically suspend itself and return control to the event loop, or the get_event()/\nprocess() code must be called from within the code at convenient points. Either\noption complicates the implementation of the task.\n By separating the concerns with concurrency, you can put the lengthy task on a\nwhole new thread and leave a dedicated GUI thread to process the events. The threads\ncan then communicate through simple mechanisms rather than having to somehow\nmix the event-handling code in with the task code. The following listing shows a sim-\nple outline for this separation.\nstd::thread task_thread;\nstd::atomic<bool> task_cancelled(false);\nvoid gui_thread()\nListing 8.6\nSeparating GUI thread from task thread\n",
      "content_length": 2560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "281\nAdditional considerations when designing for concurrency\n{\n    while(true)\n    {\n        event_data event=get_event();\n        if(event.type==quit)\n            break;\n        process(event);\n    }\n}\nvoid task()\n{\n    while(!task_complete() && !task_cancelled)\n    {\n        do_next_operation();\n    }\n    if(task_cancelled)\n    {\n        perform_cleanup();\n    }\n    else\n    {\n        post_gui_event(task_complete);\n    }\n}\nvoid process(event_data const& event)\n{\n    switch(event.type)\n    {\n    case start_task:\n        task_cancelled=false;\n        task_thread=std::thread(task);\n        break;\n    case stop_task:\n        task_cancelled=true;\n        task_thread.join();\n        break;\n    case task_complete:\n        task_thread.join();\n        display_results();\n        break;\n    default:\n        //...\n    }\n}\nBy separating the concerns in this way, the user thread is always able to respond to the\nevents in a timely fashion, even if the task takes a long time. This responsiveness is\noften key to the user experience when using an application; applications that com-\npletely lock up whenever a particular operation is being performed (whatever that\nmay be) are inconvenient to use. By providing a dedicated event-handling thread, the\nGUI can handle GUI-specific messages (such as resizing or repainting the window)\nwithout interrupting the execution of the time-consuming processing, while still pass-\ning on the relevant messages where they do affect the long-running task.\n",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "282\nCHAPTER 8\nDesigning concurrent code\n So far in this chapter you’ve had a thorough look at the issues that need to be con-\nsidered when designing concurrent code. Taken as a whole, these can be quite over-\nwhelming, but as you get used to working with your “multithreaded programming\nhat” on, most of them will become second nature. If these considerations are new to\nyou, hopefully they’ll become clearer as you look at how they impact some concrete\nexamples of multithreaded code.\n8.5\nDesigning concurrent code in practice\nWhen designing concurrent code for a particular task, the extent to which you’ll need\nto consider each of the issues described previously will depend on the task. To demon-\nstrate how they apply, we’ll look at the implementation of parallel versions of three\nfunctions from the C++ Standard Library. This will give you a familiar basis on which\nto build, while providing a platform for looking at the issues. As a bonus, we’ll also\nhave usable implementations of the functions, which could be used to help with paral-\nlelizing a larger task.\n I’ve primarily selected these implementations to demonstrate particular tech-\nniques rather than to be state-of-the-art implementations; more advanced implemen-\ntations that make better use of the available hardware concurrency may be found in\nthe academic literature on parallel algorithms or in specialist multithreading libraries\nsuch as Intel’s Threading Building Blocks (http://threadingbuildingblocks.org/).\n Conceptually, the simplest parallel algorithm is a parallel version of std::for_\neach, so we’ll start with that.\n8.5.1\nA parallel implementation of std::for_each\nstd::for_each is simple in concept; it calls a user-supplied function on every ele-\nment in a range in turn. The big difference between a parallel implementation and\nthe sequential std::for_each is the order of the function calls. std::for_each calls\nthe function with the first element in the range, then the second, and so on, whereas\nwith a parallel implementation there’s no guarantee as to the order in which the\nelements will be processed, and they may (indeed, we hope they will) be processed\nconcurrently.\n To implement a parallel version of this, you need to divide the range into sets of ele-\nments to process on each thread. You know the number of elements in advance, so you\ncan divide the data before processing begins (section 8.1.1). We’ll assume that this is the\nonly parallel task running, so you can use std::thread::hardware_concurrency() to\ndetermine the number of threads. You also know that the elements can be processed\nentirely independently, so you can use contiguous blocks to avoid false sharing (sec-\ntion 8.2.3).\n This algorithm is similar in concept to the parallel version of std::accumulate\ndescribed in section 8.4.1, but rather than computing the sum of each element, you\nmerely have to apply the specified function. Although you might imagine this would\ngreatly simplify the code, because there’s no result to return, if you want to pass on\n",
      "content_length": 3024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "283\nDesigning concurrent code in practice\nexceptions to the caller, you still need to use the std::packaged_task and std::\nfuture mechanisms to transfer the exception between threads. A sample implementa-\ntion is shown here.\ntemplate<typename Iterator,typename Func>\nvoid parallel_for_each(Iterator first,Iterator last,Func f)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\n    std::vector<std::future<void> > futures(num_threads-1);    \n    std::vector<std::thread> threads(num_threads-1);\n    join_threads joiner(threads);\n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_end=block_start;\n        std::advance(block_end,block_size);\n        std::packaged_task<void(void)> task(   \n            [=]()\n            {\n                std::for_each(block_start,block_end,f);\n            });\n        futures[i]=task.get_future();\n        threads[i]=std::thread(std::move(task));    \n        block_start=block_end;\n    }\n    std::for_each(block_start,last,f);\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        futures[i].get();    \n    }\n}\nThe basic structure of the code is identical to that of listing 8.4, which is unsurprising.\nThe key difference is that the futures vector stores std::future<void> B, because\nthe worker threads don’t return a value, and a simple lambda function that invokes\nthe function f on the range from block_start to block_end is used for the task c.\nThis avoids having to pass the range into the thread constructor d. Because the\nworker threads don’t return a value, the calls to futures[i].get() e provide a\nmeans of retrieving any exceptions thrown on the worker threads; if you don’t want to\npass on the exceptions, you could omit this.\nListing 8.7\nA parallel version of std::for_each\nb\nc\nd\ne\n",
      "content_length": 2217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "284\nCHAPTER 8\nDesigning concurrent code\n Just as your parallel implementation of std::accumulate could be simplified using\nstd::async, so can your parallel_for_each. This implementation follows.\ntemplate<typename Iterator,typename Func>\nvoid parallel_for_each(Iterator first,Iterator last,Func f)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return;\n    unsigned long const min_per_thread=25;\n    if(length<(2*min_per_thread))\n    {\n        std::for_each(first,last,f);   \n    }\n    else\n    {\n        Iterator const mid_point=first+length/2;\n        std::future<void> first_half=                    \n            std::async(&parallel_for_each<Iterator,Func>,\n                       first,mid_point,f);\n        parallel_for_each(mid_point,last,f);  \n        first_half.get();                   \n    }\n}\nAs with your std::async-based parallel_accumulate from listing 8.5, you split the\ndata recursively rather than before execution, because you don’t know how many\nthreads the library will use. As before, you divide the data in half at each stage, run-\nning one half asynchronously c and the other directly d, until the remaining data is\ntoo small to be worth dividing, in which case you defer to std::for_each B. Again,\nthe use of std::async and the get() member function of std::future e provides\nthe exception propagation semantics.\n Let’s move on from algorithms that must perform the same operation on each ele-\nment (of which there are several; std::count and std::replace spring to mind, for\nstarters) to a slightly more complicated example in the shape of std::find.\n8.5.2\nA parallel implementation of std::find\nstd::find is a useful algorithm to consider next because it’s one of several algorithms\nthat can complete without every element having been processed. For example, if the\nfirst element in the range matches the search criterion, there’s no need to examine\nany other elements. As you’ll see shortly, this is an important property for performance,\nand it has direct consequences for the design of the parallel implementation. It’s a par-\nticular example of how data access patterns can affect the design of your code (section\n8.3.2). Other algorithms in this category include std::equal and std::any_of.\n If you and your partner were searching for an old photograph through the boxes\nof keepsakes in your attic, you wouldn’t let them continue searching if you found the\nListing 8.8\nA parallel version of std::for_each using std::async\nb\nc\nd\ne\n",
      "content_length": 2498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "285\nDesigning concurrent code in practice\nphotograph. Instead, you’d let them know you’d found the photograph (perhaps by\nshouting, “Found it!”), so that they could stop searching and move on to something\nelse. The nature of many algorithms requires that they process every element, so they\nhave no equivalent to shouting, “Found it!” For algorithms such as std::find, the\nability to complete “early” is an important property and not something to squander.\nYou therefore need to design your code to make use of it—to interrupt the other tasks\nin some way when the answer is known, so that the code doesn’t have to wait for the\nother worker threads to process the remaining elements.\n If you don’t interrupt the other threads, the serial version may outperform your\nparallel implementation, because the serial algorithm can stop searching and return\nonce a match is found. If, for example, the system can support four concurrent\nthreads, each thread will have to examine one quarter of the elements in the range,\nand your naive parallel implementation would take approximately one quarter of the\ntime a single thread would take to check every element. If the matching element lies\nin the first quarter of the range, the sequential algorithm will return first, because it\ndoesn’t need to check the remainder of the elements.\n One way in which you can interrupt the other threads is by making use of an\natomic variable as a flag and checking the flag after processing every element. If the\nflag is set, one of the other threads has found a match, so you can cease processing\nand return. By interrupting the threads in this way, you preserve the property that you\ndon’t have to process every value and improve the performance compared to the\nserial version in more circumstances. The downside to this is that atomic loads can be\nslow operations, so this can impede the progress of each thread.\n Now you have two choices as to how to return the values and how to propagate any\nexceptions. You can use an array of futures, std::packaged_task, for transferring the\nvalues and exceptions, and then process the results back in the main thread; or you\ncan use std::promise to set the final result directly from the worker threads. It all\ndepends on how you want to handle exceptions from the worker threads. If you want\nto stop on the first exception (even if you haven’t processed all elements), you can use\nstd::promise to set both the value and the exception. On the other hand, if you want\nto allow the other workers to keep searching, you can use std::packaged_task, store\nall the exceptions, and then rethrow one of them if a match isn’t found.\n In this case I’ve opted to use std::promise because the behavior matches that of\nstd::find more closely. One thing to watch out for here is the case where the ele-\nment being searched for isn’t in the supplied range. You therefore need to wait for all\nthe threads to finish before getting the result from the future. If you block on the\nfuture, you’ll be waiting forever if the value isn’t there. The result is shown here.\ntemplate<typename Iterator,typename MatchType>\nIterator parallel_find(Iterator first,Iterator last,MatchType match)\n{\n    struct find_element    \nListing 8.9\nAn implementation of a parallel find algorithm\nb\n",
      "content_length": 3272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "286\nCHAPTER 8\nDesigning concurrent code\n    {\n        void operator()(Iterator begin,Iterator end,\n                        MatchType match,\n                        std::promise<Iterator>* result,\n                        std::atomic<bool>* done_flag)\n        {\n            try\n            {\n                for(;(begin!=end) && !done_flag->load();++begin)    \n                {\n                    if(*begin==match)\n                    {\n                        result->set_value(begin);    \n                        done_flag->store(true);   \n                        return;\n                    }\n                }\n            }\n            catch(...)   \n            {\n                try\n                {\n                    result->set_exception(std::current_exception());   \n                    done_flag->store(true);\n                }\n                catch(...)   \n                {}\n            }\n        }\n    };\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return last;\n    unsigned long const min_per_thread=25;\n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\n    std::promise<Iterator> result;     \n    std::atomic<bool> done_flag(false);             \n    std::vector<std::thread> threads(num_threads-1);\n    {        \n        join_threads joiner(threads);\n        Iterator block_start=first;\n        for(unsigned long i=0;i<(num_threads-1);++i)\n        {\n            Iterator block_end=block_start;\n            std::advance(block_end,block_size);\n            threads[i]=std::thread(find_element(),    \n                                   block_start,block_end,match,\n                                   &result,&done_flag);\n            block_start=block_end;\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n",
      "content_length": 2015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "287\nDesigning concurrent code in practice\n        }\n        find_element()(block_start,last,match,&result,&done_flag);   \n    }\n    if(!done_flag.load())    \n    {\n        return last;\n    }\n    return result.get_future().get();    \n}\nThe main body of listing 8.9 is similar to the previous examples. This time, the work is\ndone in the function call operator of the local find_element class B. This loops\nthrough the elements in the block it’s been given, checking the flag at each step c. If\na match is found, it sets the final result value in the promise d, and then sets the\ndone_flag e before returning.\n If an exception is thrown, this is caught by the catchall handler f, and you try to\nstore the exception in the promise g before setting the done_flag. Setting the value\non the promise might throw an exception if the promise is already set, so you catch\nand discard any exceptions that happen here h.\n This means that if a thread calling find_element either finds a match or throws an\nexception, all other threads will see done_flag set and will stop. If multiple threads\nfind a match or throw at the same time, they’ll race to set the result in the promise.\nBut this is a benign race condition; whichever succeeds is nominally “first” and there-\nfore an acceptable result.\n Back in the main parallel_find function itself, you have the promise i and flag\nj used to stop the search, both of which are passed in to the new threads along with\nthe range to search 1!. The main thread also uses find_element to search the remain-\ning elements 1@. As already mentioned, you need to wait for all threads to finish\nbefore you check the result, because there might not be any matching elements. You\ndo this by enclosing the thread launching-and-joining code in a block 1) so all threads\nare joined when you check the flag to see whether a match was found 1#. If a match\nwas found, you can get the result or throw the stored exception by calling get() on\nthe std::future<Iterator> you can get from the promise 1$.\n Again, this implementation assumes that you’re going to be using all available\nhardware threads or that you have some other mechanism to determine the number\nof threads to use for the upfront division of work between threads. As before, you can\nuse std::async and recursive data division to simplify your implementation, while\nusing the automatic scaling facility of the C++ Standard Library. An implementation\nof parallel_find using std::async is shown in the following listing.\ntemplate<typename Iterator,typename MatchType>   \nIterator parallel_find_impl(Iterator first,Iterator last,MatchType match,\n                            std::atomic<bool>& done)\n{\nListing 8.10\nAn implementation of a parallel find algorithm using std::async\n1@\n1#\n1$\nb\n",
      "content_length": 2760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "288\nCHAPTER 8\nDesigning concurrent code\n    try\n    {\n        unsigned long const length=std::distance(first,last);\n        unsigned long const min_per_thread=25;   \n        if(length<(2*min_per_thread))        \n        {\n            for(;(first!=last) && !done.load();++first)    \n            {\n                if(*first==match)\n                {\n                    done=true;    \n                    return first;\n                }\n            }\n            return last;   \n        }\n        else\n        {\n            Iterator const mid_point=first+(length/2);   \n            std::future<Iterator> async_result=\n                std::async(&parallel_find_impl<Iterator,MatchType>,   \n                           mid_point,last,match,std::ref(done));\n            Iterator const direct_result=\n                    parallel_find_impl(first,mid_point,match,done);   \n            return (direct_result==mid_point)?\n                async_result.get():direct_result;   \n        }\n    }\n    catch(...)\n    {\n        done=true;   \n        throw;\n    }\n}\ntemplate<typename Iterator,typename MatchType>\nIterator parallel_find(Iterator first,Iterator last,MatchType match)\n{\n    std::atomic<bool> done(false);\n    return parallel_find_impl(first,last,match,done);   \n}\nThe desire to finish early if you find a match means that you need to introduce a flag\nthat is shared between all threads to indicate that a match has been found. This there-\nfore needs to be passed in to all recursive calls. The simplest way to achieve this is by\ndelegating to an implementation function B, which takes an additional parameter—\na reference to the done flag, which is passed in from the main entry point 1@.\n The core implementation then proceeds along familiar lines. In common with\nmany of the implementations here, you set a minimum number of items to process on\na single thread c; if you can’t cleanly divide into two halves of at least that size, you\nrun everything on the current thread d. The algorithm is a simple loop through\nthe specified range, looping until you reach the end of the range or the done flag is\nset e. If you do find a match, the done flag is set before returning f. If you stop\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n1@\n",
      "content_length": 2206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "289\nDesigning concurrent code in practice\nsearching either because you got to the end of the list, or because another thread set\nthe done flag, you return last to indicate that no match was found here g.\n If the range can be divided, you first find the midpoint h before using\nstd::async to run the search in the second half of the range i, being careful to use\nstd::ref to pass a reference to the done flag. In the meantime, you can search in the\nfirst half of the range by doing a direct recursive call j. Both the asynchronous call\nand the direct recursion may result in further subdivisions if the original range is\nbig enough.\n If the direct search returned mid_point, then it failed to find a match, so you need\nto get the result of the asynchronous search. If no result was found in that half, the\nresult will be last, which is the correct return value to indicate that the value was not\nfound 1). If the “asynchronous” call was deferred rather than truly asynchronous, it\nwill run here in the call to get(); in these circumstances, the search of the top half of\nthe range is skipped if the search in the bottom half was successful. If the asynchro-\nnous search is running on another thread, the destructor of the async_result vari-\nable will wait for the thread to complete, so you don’t have any leaking threads.\n As before, the use of std::async provides you with exception safety and exception-\npropagation features. If the direct recursion throws an exception, the future’s\ndestructor will ensure that the thread running the asynchronous call has terminated\nbefore the function returns, and if the asynchronous call throws, the exception is\npropagated through the get() call 1). The use of a try/catch block around the\nwhole thing is only there to set the done flag on an exception and ensure that all\nthreads terminate quickly if an exception is thrown 1!. The implementation would\nstill be correct without it but would keep checking elements until every thread\nwas finished.\n A key feature that both implementations of this algorithm share with the other\nparallel algorithms you’ve seen is that there’s no longer the guarantee that items are\nprocessed in the sequence that you get from std::find. This is essential if you’re\ngoing to parallelize the algorithm. You can’t process elements concurrently if the order\nmatters. If the elements are independent, it doesn’t matter for things like parallel\n_for_each, but it means that your parallel_find might return an element toward\nthe end of the range even when there’s a match toward the beginning, which might\nbe surprising if you’re not expecting it.\n OK, so you’ve managed to parallelize std::find. As I stated at the beginning of\nthis section, there are other similar algorithms that can complete without processing\nevery data element, and the same techniques can be used for those. We’ll also look\nfurther at the issue of interrupting threads in chapter 9.\n To complete our trio of examples, we’ll go in a different direction and look at\nstd::partial_sum. This algorithm doesn’t get a lot of press, but it’s an interesting\nalgorithm to parallelize and highlights some additional design choices.\n",
      "content_length": 3162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "290\nCHAPTER 8\nDesigning concurrent code\n8.5.3\nA parallel implementation of std::partial_sum\nstd::partial_sum calculates the running totals in a range, so each element is replaced\nby the sum of that element and all the elements prior to it in the original sequence.\nThus the sequence 1, 2, 3, 4, 5 becomes 1, (1+2)=3, (1+2+3)=6, (1+2+3+4)=10,\n(1+2+3+4+5)=15. This is interesting to parallelize because you can’t just divide the\nrange into chunks and calculate each chunk independently. For example, the initial\nvalue of the first element needs to be added to every other element.\n One approach to determining the partial sum of a range is to calculate the partial\nsum of individual chunks and then add the resulting value of the last element in the\nfirst chunk onto the elements in the next chunk, and so forth. If you have the ele-\nments 1, 2, 3, 4, 5, 6, 7, 8, 9 and you’re splitting into three chunks, you get {1, 3, 6},\n{4, 9, 15}, {7, 15, 24} in the first instance. If you then add 6 (the sum for the last element\nin the first chunk) onto the elements in the second chunk, you get {1, 3, 6}, {10, 15, 21},\n{7, 15, 24}. Then you add the last element of the second chunk (21) onto the elements\nin the third and final chunk to get the final result: {1, 3, 6}, {10, 15, 21}, {28, 36, 55}.\n As well as the original division into chunks, the addition of the partial sum from\nthe previous block can also be parallelized. If the last element of each block is\nupdated first, the remaining elements in a block can be updated by one thread while a\nsecond thread updates the next block, and so forth. This works well when there are\nmany more elements in the list than processing cores, because each core has a reason-\nable number of elements to process at each stage.\n If you have a lot of processing cores (as many or more than the number of ele-\nments), this doesn’t work so well. If you divide the work among the processors, you\nend up working in pairs of elements at the first step. Under these conditions, this for-\nward propagation of results means that many processors are left waiting, so you need\nto find some work for them to do. You can then take a different approach to the prob-\nlem. Rather than doing the full forward propagation of the sums from one chunk to\nthe next, you do a partial propagation: first sum adjacent elements as before, but then\nadd those sums to those two elements away, then add the next set of results to the\nresults from four elements away, and so forth. If you start with the same initial nine\nelements, you get 1, 3, 5, 7, 9, 11, 13, 15, 17 after the first round, which gives you the\nfinal results for the first two elements. After the second you then have 1, 3, 6, 10, 14,\n18, 22, 26, 30, which is correct for the first four elements. After round three you have\n1, 3, 6, 10, 15, 21, 28, 36, 44, which is correct for the first eight elements, and finally\nafter round four you have 1, 3, 6, 10, 15, 21, 28, 36, 45, which is the final answer.\nAlthough there are more total steps than in the first approach, there’s greater scope\nfor parallelism if you have many processors; each processor can update one entry with\neach step.\n Overall, the second approach takes log2(N) steps of approximately N operations\n(one per processor), where N is the number of elements in the list. This compares to\nthe first algorithm where each thread has to perform N/k operations for the initial\npartial sum of the chunk allocated to it and then further N/k operations to do the\n",
      "content_length": 3487,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "291\nDesigning concurrent code in practice\nforward propagation, where k is the number of threads. Thus the first approach is\nO(N), whereas the second is O(N log(N)) in terms of the total number of operations.\nBut if you have as many processors as list elements, the second approach requires only\nlog(N) operations per processor, whereas the first serializes the operations when k gets\nlarge, because of the forward propagation. For small numbers of processing units, the\nfirst approach will therefore finish faster, whereas for massively parallel systems, the sec-\nond will finish faster. This is an extreme example of the issues discussed in section 8.2.1.\n Anyway, efficiency issues aside, let’s look at some code. The following listing shows\nthe first approach.\ntemplate<typename Iterator>\nvoid parallel_partial_sum(Iterator first,Iterator last)\n{\n    typedef typename Iterator::value_type value_type;\n    \n    struct process_chunk     \n    {\n        void operator()(Iterator begin,Iterator last,\n                        std::future<value_type>* previous_end_value,\n                        std::promise<value_type>* end_value)\n        {\n            try\n            {\n                Iterator end=last;\n                ++end;\n                std::partial_sum(begin,end,begin);    \n                if(previous_end_value)    \n                {\n                    value_type& addend=previous_end_value->get();    \n                    *last+=addend;     \n                    if(end_value)\n                    {\n                        end_value->set_value(*last);   \n                    }\n                    std::for_each(begin,last,[addend](value_type& item) \n                                  {\n                                      item+=addend;\n                                  });\n                }\n                else if(end_value)\n                {\n                    end_value->set_value(*last);   \n                }\n            }\n            catch(...)    \n            {\n                if(end_value)\n                {\n                    end_value->set_exception(std::current_exception()); \n                }\nListing 8.11\nCalculating partial sums in parallel by dividing the problem\nb\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n",
      "content_length": 2216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "292\nCHAPTER 8\nDesigning concurrent code\n                else\n                {\n                    throw;    \n                }\n            }\n        }\n    };\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return;\n    unsigned long const min_per_thread=25;    \n    unsigned long const max_threads=\n        (length+min_per_thread-1)/min_per_thread;\n    unsigned long const hardware_threads=\n        std::thread::hardware_concurrency();\n    unsigned long const num_threads=\n        std::min(hardware_threads!=0?hardware_threads:2,max_threads);\n    unsigned long const block_size=length/num_threads;\n    typedef typename Iterator::value_type value_type;\n    std::vector<std::thread> threads(num_threads-1);    \n    std::vector<std::promise<value_type> > \n         end_values(num_threads-1);          \n    std::vector<std::future<value_type> > \n         previous_end_values;      \n    previous_end_values.reserve(num_threads-1);    \n    join_threads joiner(threads);\n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_threads-1);++i)\n    {\n        Iterator block_last=block_start;\n        std::advance(block_last,block_size-1);    \n        threads[i]=std::thread(process_chunk(),         \n                               block_start,block_last,\n                               (i!=0)?&previous_end_values[i-1]:0,\n                               &end_values[i]);\n        block_start=block_last;\n        ++block_start;      \n        previous_end_values.push_back(end_values[i].get_future());   \n    }\n    Iterator final_element=block_start;\n    std::advance(final_element,std::distance(block_start,last)-1);   \n    process_chunk()(block_start,final_element,                  \n                    (num_threads>1)?&previous_end_values.back():0,\n                    0);\n}\nIn this instance, the general structure is the same as with the previous algorithms,\ndividing the problem into chunks, with a minimum chunk size per thread 1@. In this\ncase, as well as the vector of threads 1#, you have a vector of promises 1$, which is used\nto store the value of the last element in the chunk, and a vector of futures 1%, which is\nused to retrieve the last value from the previous chunk. You can reserve the space for\nthe futures 1^ to avoid a reallocation while spawning threads, because you know how\nmany you’re going to have.\n1!\n1@\n1#\n1$\n1%\n1^\n1&\n1*\n1(\n2)\n2!\n2@\n",
      "content_length": 2392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "293\nDesigning concurrent code in practice\n The main loop is the same as before, except this time you want the iterator that\npoints to the last element in each block, rather than being the usual one past the end\n1&, so that you can do the forward propagation of the last element in each range. The\nprocessing is done in the process_chunk function object, which we’ll look at shortly;\nthe start and end iterators for this chunk are passed in as arguments alongside the\nfuture for the end value of the previous range (if any) and the promise to hold the end\nvalue of this range 1*.\n After you’ve spawned the thread, you can update the block start, remembering to\nadvance it past that last element 1(, and store the future for the last value in the cur-\nrent chunk into the vector of futures so it will be picked up next time around the\nloop 2).\n Before you process the final chunk, you need to get an iterator for the last element\n2!, which you can pass in to process_chunk 2@. std::partial_sum doesn’t return a\nvalue, so you don’t need to do anything once the final chunk has been processed. The\noperation is complete once all the threads have finished.\n OK, now it’s time to look at the process_chunk function object that does all the\nwork B. You start by calling std::partial_sum for the entire chunk, including the\nfinal element c, but then you need to know if you’re the first chunk or not d. If\nyou are not the first chunk, then there was a previous_end_value from the previ-\nous chunk, so you need to wait for that e. In order to maximize the parallelism of\nthe algorithm, you then update the last element first f, so you can pass the value\non to the next chunk (if there is one) g. Once you’ve done that, you can use\nstd::for_each and a simple lambda function h, to update all the remaining ele-\nments in the range.\n If there was not a previous_end_value, you’re the first chunk, so you can update\nthe end_value for the next chunk (again, if there is one—you might be the only\nchunk) i.\n Finally, if any of the operations threw an exception, you catch it j and store it in\nthe promise 1) so it will propagate to the next chunk when it tries to get the previous\nend value e. This will propagate all exceptions into the final chunk, which then\nrethrows 1!, because you know you’re running on the main thread.\n Because of the synchronization between the threads, this code isn’t readily amena-\nble to rewriting with std::async. The tasks wait on results made available partway\nthrough the execution of other tasks, so all tasks must be running concurrently.\n With the block-based, forward-propagation approach out of the way, let’s look at\nthe second approach to computing the partial sums of a range.\nIMPLEMENTING THE INCREMENTAL PAIRWISE ALGORITHM FOR PARTIAL SUMS\nThis second approach to calculating the partial sums by adding elements increasingly\nfurther away works best where your processors can execute the additions in lockstep.\nIn this case, no further synchronization is necessary because all the intermediate\nresults can be propagated directly to the next processor that needs them. But in prac-\ntice, you rarely have these systems to work with, except for those cases where a single\n",
      "content_length": 3197,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "294\nCHAPTER 8\nDesigning concurrent code\nprocessor can execute the same instruction across a small number of data elements\nsimultaneously with so-called Single-Instruction/Multiple-Data (SIMD) instructions.\nTherefore, you must design your code for the general case and explicitly synchronize\nthe threads at each step.\n One way to do this is to use a barrier—a synchronization mechanism that causes\nthreads to wait until the required number of threads has reached the barrier. Once all\nthe threads have reached the barrier, they’re all unblocked and may proceed. The\nC++11 Thread Library doesn’t offer this facility directly, so you have to design one\nyourself.\n Imagine a roller coaster at the fairground. If there’s a reasonable number of peo-\nple waiting, the fairground staff will ensure that every seat is filled before the roller\ncoaster leaves the platform. A barrier works the same way: you specify up front the\nnumber of “seats,” and threads have to wait until all the “seats” are filled. Once there\nare enough waiting threads, they can all proceed; the barrier is reset and starts waiting\nfor the next batch of threads. Often, this construct is used in a loop, where the same\nthreads come around and wait until next time. The idea is to keep the threads in lock-\nstep, so one thread doesn’t run away in front of the others and get out of step. For an\nalgorithm such as this one, that would be disastrous, because the runaway thread\nwould potentially modify data that was still being used by other threads or use data\nthat hadn’t been correctly updated yet.\n The following listing shows a simple implementation of a barrier.\nclass barrier\n{\n    unsigned const count;\n    std::atomic<unsigned> spaces;\n    std::atomic<unsigned> generation;\npublic:\n    explicit barrier(unsigned count_):         \n        count(count_),spaces(count),generation(0)\n    {}\n    void wait()\n    {\n        unsigned const my_generation=generation;   \n        if(!--spaces)                \n        {\n            spaces=count;      \n            ++generation;   \n        }\n        else\n        {\n            while(generation==my_generation)    \n                std::this_thread::yield();    \n        }\n    }\n};\nListing 8.12\nA simple barrier class\nb\nc\nd\ne\nf\ng\nh\n",
      "content_length": 2240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "295\nDesigning concurrent code in practice\nWith this implementation, you construct a barrier with the number of “seats” B,\nwhich is stored in the count variable. Initially, the number of spaces at the barrier is\nequal to this count. As each thread waits, the number of spaces is decremented d.\nWhen it reaches zero, the number of spaces is reset back to count e, and the\ngeneration is increased to signal to the other threads that they can continue f. If\nthe number of free spaces does not reach zero, you have to wait. This implementa-\ntion uses a simple spin lock g, checking the generation against the value you retrieved\nat the beginning of wait() c. Because the generation is only updated when all the\nthreads have reached the barrier f, you yield() while waiting h, so the waiting\nthread doesn’t hog the CPU in a busy wait.\n When I said this implementation was simple, I meant it: it uses a spin wait, so it’s\nnot ideal for cases where threads are likely to be waiting a long time, and it doesn’t\nwork if there’s more than count threads that can potentially call wait() at any one\ntime. If you need to handle either of those scenarios, you must use a more robust (but\nmore complex) implementation instead. I’ve also stuck to sequentially consistent\noperations on the atomic variables, because that makes everything easier to reason\nabout, but you could potentially relax some of the ordering constraints. This global\nsynchronization is expensive on massively parallel architectures, because the cache\nline holding the barrier state must be shuttled between all the processors involved\n(see the discussion of cache ping-pong in section 8.2.2), so you must take great care to\nensure that this is the best choice here. If your C++ Standard Library provides the\nfacilities from the Concurrency TS, you could use std::experimental::barrier\nhere. See chapter 4 for details.\n This is what you need here; you have a fixed number of threads that need to run in\na lockstep loop. Well, it’s almost a fixed number of threads. As you may remember, the\nitems at the beginning of the list acquire their final values after a couple of steps. This\nmeans that either you have to keep those threads looping until the entire range has\nbeen processed, or you need to allow your barrier to handle threads dropping out and\ndecreasing count. I opted for the latter option because it avoids having threads doing\nunnecessary work, looping until the final step is done.\n This means you have to change count to be an atomic variable, so you can update\nit from multiple threads without external synchronization:\nstd::atomic<unsigned> count;\nThe initialization remains the same, but now you have to explicitly load() from count\nwhen you reset the number of spaces:\nspaces=count.load();\nThese are all the changes that you need on the wait() front; now you need a new\nmember function to decrement count. Let’s call it done_waiting(), because a thread\nis declaring that it is done with waiting:\n",
      "content_length": 2967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "296\nCHAPTER 8\nDesigning concurrent code\nvoid done_waiting()\n{\n    --count;       \n    if(!--spaces)    \n    {\n        spaces=count.load();    \n        ++generation;\n    }\n}\nThe first thing you do is decrement the count B so that the next time spaces is reset\nit reflects the new lower number of waiting threads. Then you need to decrease the\nnumber of free spaces c. If you don’t do this, the other threads will be waiting for-\never, because spaces was initialized to the old, larger value. If you’re the last thread\nthrough on this batch, you need to reset the counter and increase the generation d,\nas you do in wait(). The key difference here is that if you’re the last thread in the\nbatch, you don’t have to wait. \n You’re now ready to write your second implementation of partial sum. At each\nstep, every thread calls wait() on the barrier to ensure the threads step through\ntogether, and once each thread is done, it calls done_waiting() on the barrier to dec-\nrement the count. If you use a second buffer alongside the original range, the barrier\nprovides all the synchronization you need. At each step, the threads read from either\nthe original range or the buffer and write the new value to the corresponding element\nof the other. If the threads read from the original range on one step, they read from\nthe buffer on the next, and vice versa. This ensures there are no race conditions\nbetween the reads and writes by separate threads. Once a thread has finished looping,\nit must ensure that the correct final value has been written to the original range. The\nfollowing listing pulls this all together.\nstruct barrier\n{\n    std::atomic<unsigned> count;\n    std::atomic<unsigned> spaces;\n    std::atomic<unsigned> generation;\n    barrier(unsigned count_):\n        count(count_),spaces(count_),generation(0)\n    {}\n    void wait()\n    {\n        unsigned const gen=generation.load();\n        if(!--spaces)\n        {\n            spaces=count.load();\n            ++generation;\n        }\n        else\n        {\nListing 8.13\nA parallel implementation of partial_sum by pairwise updates\nb\nc\nd\n",
      "content_length": 2092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "297\nDesigning concurrent code in practice\n            while(generation.load()==gen)\n            {\n                std::this_thread::yield();\n            }\n        }\n    }\n    void done_waiting()\n    {\n        --count;\n        if(!--spaces)\n        {\n            spaces=count.load();\n            ++generation;\n        }\n    }\n};\ntemplate<typename Iterator>\nvoid parallel_partial_sum(Iterator first,Iterator last)\n{\n    typedef typename Iterator::value_type value_type;\n    struct process_element       \n    {\n        void operator()(Iterator first,Iterator last,\n                        std::vector<value_type>& buffer,\n                        unsigned i,barrier& b)\n        {\n            value_type& ith_element=*(first+i);\n            bool update_source=false;\n            \n            for(unsigned step=0,stride=1;stride<=i;++step,stride*=2)\n            {\n                value_type const& source=(step%2)?    \n                    buffer[i]:ith_element;\n                value_type& dest=(step%2)?\n                    ith_element:buffer[i];\n                value_type const& addend=(step%2)?     \n                    buffer[i-stride]:*(first+i-stride);\n                dest=source+addend;     \n                update_source=!(step%2);\n                b.wait();              \n            }\n            if(update_source)    \n            {\n                ith_element=buffer[i];\n            }\n            b.done_waiting();   \n        }\n    };\n    unsigned long const length=std::distance(first,last);\n    if(length<=1)\n        return;\n    std::vector<value_type> buffer(length);\n    barrier b(length);\n    std::vector<std::thread> threads(length-1);    \n    join_threads joiner(threads);\nb\nc\nd\ne\nf\ng\nh\ni\n",
      "content_length": 1702,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "298\nCHAPTER 8\nDesigning concurrent code\n    Iterator block_start=first;\n    for(unsigned long i=0;i<(length-1);++i)\n    {\n        threads[i]=std::thread(process_element(),first,last,   \n                               std::ref(buffer),i,std::ref(b));\n    }\n    process_element()(first,last,buffer,length-1,b);   \n}\nThe overall structure of this code is probably becoming familiar by now. You have a\nclass with a function call operator (process_element) for doing the work B, which\nyou run on a bunch of threads j stored in a vector i, and which you also call from\nthe main thread 1). The key difference this time is that the number of threads is\ndependent on the number of items in the list rather than on std::thread::hardware\n_concurrency. As I said already, unless you’re on a massively parallel machine where\nthreads are cheap, this is probably a bad idea, but it shows the overall structure. It\nwould be possible to have fewer threads, with each thread handling several values\nfrom the source range, but there will come a point where there are sufficiently few\nthreads that this is less efficient than the forward-propagation algorithm.\n The key work is done in the function call operator of process_element. At each\nstep, you either take the ith element from the original range or the ith element from\nthe buffer c and add it to the value stride elements prior d, storing it in the buffer\nif you started in the original range or back in the original range if you started in the\nbuffer e. You then wait on the barrier f before starting the next step. You’ve fin-\nished when the stride takes you off the start of the range, in which case you need to\nupdate the element in the original range if your final result was stored in the buffer g.\nFinally, you tell the barrier that you’re done_waiting() h.\n Note that this solution isn’t exception-safe. If an exception is thrown in process-\n_element on one of the worker threads, it will terminate the application. You could\ndeal with this by using std::promise to store the exception, as you did for the\nparallel_find implementation from listing 8.9, or even using std::exception_ptr\nprotected by a mutex.\n That concludes our three examples. Hopefully, they’ve helped to crystallize some\nof the design considerations highlighted in sections 8.1, 8.2, 8.3, and 8.4, and have\ndemonstrated how these techniques can be brought to bear in real code.\nSummary\nWe’ve covered quite a lot of ground in this chapter. We started with various tech-\nniques for dividing work between threads, such as dividing the data beforehand or\nusing a number of threads to form a pipeline. We then looked at the issues sur-\nrounding the performance of multithreaded code from a low-level perspective, with\na look at false sharing and data contention before moving on to how the patterns of\ndata access can affect the performance of a bit of code. We then looked at addi-\ntional considerations in the design of concurrent code, such as exception safety and\nj\n1)\n",
      "content_length": 2983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "299\nSummary\nscalability. Finally, we ended with a number of examples of parallel algorithm imple-\nmentations, each of which highlighted particular issues that can occur when design-\ning multithreaded code.\n One item that has cropped up a couple of times in this chapter is the idea of a\nthread pool—a preconfigured group of threads that run tasks assigned to the pool.\nQuite a lot of thought goes into the design of a good thread pool, so we’ll look at\nsome of the issues in the next chapter, along with other aspects of advanced thread\nmanagement.\n",
      "content_length": 549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "300\nAdvanced thread\nmanagement\nIn earlier chapters, you’ve been explicitly managing threads by creating std::thread\nobjects for every thread. In a couple of places you’ve seen how this can be undesir-\nable, because you then have to manage the lifetime of the thread objects, deter-\nmine the number of threads appropriate to the problem and to the current\nhardware, and so forth. The ideal scenario would be that you could divide the code\ninto the smallest pieces that could be executed concurrently, pass them over to the\ncompiler and library, and say, “Parallelize this for optimal performance.” As we'll\nsee in chapter 10, there are cases where you can do this: if your code that requires\nparallelization can be expressed as a call to a standard library algorithm, then you\ncan ask the library to do the parallelization for you in most cases.\n Another recurring theme in several of the examples is that you might use sev-\neral threads to solve a problem but require that they finish early if some condition\nis met. This might be because the result has already been determined, or because\nThis chapter covers\nThread pools\nHandling dependencies between pool tasks\nWork stealing for pool threads\nInterrupting threads\n",
      "content_length": 1220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "301\nThread pools\nan error has occurred, or even because the user has explicitly requested that the oper-\nation be aborted. Whatever the reason, the threads need to be sent a “Please stop”\nrequest so that they can give up on the task they were given, tidy up, and finish as soon\nas possible.\n In this chapter, we’ll look at mechanisms for managing threads and tasks, starting\nwith the automatic management of the number of threads and the division of tasks\nbetween them.\n9.1\nThread pools\nIn many companies, employees who would normally spend their time in the office are\noccasionally required to visit clients or suppliers or to attend a trade show or confer-\nence. Although these trips might be necessary, and on any given day there might be\nseveral people making this trip, it may well be months or even years between these\ntrips for any particular employee. Because it would therefore be rather expensive and\nimpractical for each employee to have a company car, companies often offer a car pool\ninstead; they have a limited number of cars that are available to all employees. When\nan employee needs to make an off-site trip, they book one of the pool cars for the\nappropriate time and return it for others to use when they return to the office. If\nthere are no pool cars free on a given day, the employee will have to reschedule their\ntrip for a subsequent date.\n A thread pool is a similar idea, except that threads are being shared rather than\ncars. On most systems, it’s impractical to have a separate thread for every task that\ncan potentially be done in parallel with other tasks, but you’d still like to take advan-\ntage of the available concurrency where possible. A thread pool allows you to accom-\nplish this; tasks that can be executed concurrently are submitted to the pool, which\nputs them on a queue of pending work. Each task is then taken from the queue by\none of the worker threads, which executes the task before looping back to take another\nfrom the queue.\n There are several key design issues when building a thread pool, such as how many\nthreads to use, the most efficient way to allocate tasks to threads, and whether or not\nyou can wait for a task to complete. In this section we’ll look at some thread pool\nimplementations that address these design issues, starting with the simplest possible\nthread pool.\n9.1.1\nThe simplest possible thread pool\nAt its simplest, a thread pool is a fixed number of worker threads (typically the same\nnumber as the value returned by std::thread::hardware_concurrency()) that pro-\ncess work. When you have work to do, you call a function to put it on the queue of\npending work. Each worker thread takes work off the queue, runs the specified task,\nand then goes back to the queue for more work. In the simplest case there’s no way to\nwait for the task to complete. If you need to do this, you have to manage the synchro-\nnization yourself.\n",
      "content_length": 2896,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "302\nCHAPTER 9\nAdvanced thread management\n The following listing shows a sample implementation of this thread pool.\nclass thread_pool\n{\n    std::atomic_bool done;\n    threadsafe_queue<std::function<void()> > work_queue;     \n    std::vector<std::thread> threads;             \n    join_threads joiner;            \n    void worker_thread()\n    {\n        while(!done)     \n        {\n            std::function<void()> task;\n            if(work_queue.try_pop(task))    \n            {\n                task();    \n            }\n            else\n            {\n                std::this_thread::yield();    \n            }\n        }\n    }\npublic:\n    thread_pool():\n        done(false),joiner(threads)\n    {\n        unsigned const thread_count=std::thread::hardware_concurrency(); \n        try\n        {\n            for(unsigned i=0;i<thread_count;++i)\n            {\n                threads.push_back(\n                    std::thread(&thread_pool::worker_thread,this));   \n            }\n        }\n        catch(...)\n        {\n            done=true;    \n            throw;\n        }\n    }\n    ~thread_pool()\n    {\n        done=true;   \n    }\n    template<typename FunctionType>\n    void submit(FunctionType f)\n    {\n        work_queue.push(std::function<void()>(f));    \n    }\n};\nListing 9.1\nSimple thread pool\nb\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n1@\n",
      "content_length": 1326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "303\nThread pools\nThis implementation has a vector of worker threads c and uses one of the thread-safe\nqueues from chapter 6 B to manage the queue of work. In this case, users can’t wait for\nthe tasks, and they can’t return any values, so you can use std::function<void()> to\nencapsulate your tasks. The submit() function then wraps whatever function or call-\nable object is supplied inside an std::function<void()> instance and pushes it on\nthe queue 1@.\n The threads are started in the constructor: you use std::thread::hardware_\nconcurrency() to tell you how many concurrent threads the hardware can support\ni, and you create that many threads running your worker_thread() member func-\ntion j.\n Starting a thread can fail by throwing an exception, so you need to ensure that any\nthreads you’ve already started are stopped and cleaned up nicely in this case. This is\nachieved with a try-catch block that sets the done flag when an exception is thrown\n1), alongside an instance of the join_threads class from chapter 8 d to join all the\nthreads. This also works with the destructor: you can set the done flag 1!, and the\njoin_threads instance will ensure that all the threads have completed before the\npool is destroyed. Note that the order of declaration of the members is important:\nboth the done flag and the worker_queue must be declared before the threads vector,\nwhich must in turn be declared before the joiner. This ensures that the members are\ndestroyed in the right order; you can’t destroy the queue safely until all the threads\nhave stopped, for example.\n The worker_thread function itself is quite simple: it sits in a loop waiting until the\ndone flag is set e, pulling tasks off the queue f and executing them g in the mean-\ntime. If there are no tasks on the queue, the function calls std::this_thread::\nyield() to take a small break h and give another thread a chance to put some work\non the queue before it tries to take some off again the next time around.\n For many purposes this simple thread pool will suffice, especially if the tasks are\nentirely independent and don’t return any values or perform any blocking opera-\ntions. But there are also many circumstances where this simple thread pool may not\nadequately address your needs, and yet others where it can cause problems such as\ndeadlock. Also, in simple cases you may be better served using std::async as in\nmany of the examples in chapter 8. Throughout this chapter, we’ll look at more com-\nplex thread pool implementations that have additional features either to address\nuser needs or reduce the potential for problems. First up: waiting for the tasks we’ve\nsubmitted.\n9.1.2\nWaiting for tasks submitted to a thread pool\nIn the examples in chapter 8 that explicitly spawned threads, after dividing the work\nbetween threads, the master thread always waited for the newly spawned threads to\nfinish, to ensure that the overall task was complete before returning to the caller. With\nthread pools, you’d need to wait for the tasks submitted to the thread pool to com-\nplete, rather than the worker threads themselves. This is similar to the way that the\n",
      "content_length": 3127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "304\nCHAPTER 9\nAdvanced thread management\nstd::async-based examples in chapter 8 waited for the futures. With the simple\nthread pool from listing 9.1, you’d have to do this manually using the techniques from\nchapter 4: condition variables and futures. This adds complexity to the code; it would\nbe better if you could wait for the tasks directly.\n By moving that complexity into the thread pool itself, you can wait for the tasks\ndirectly. You can have the submit() function return a task handle of some description\nthat you can then use to wait for the task to complete. This task handle would wrap the\nuse of condition variables or futures, simplifying the code that uses the thread pool.\n A special case of having to wait for the spawned task to finish occurs when the main\nthread needs a result computed by the task. You’ve seen this in examples throughout\nthe book, such as the parallel_accumulate() function from chapter 2. In this case,\nyou can combine the waiting with the result transfer through the use of futures. List-\ning 9.2 shows the changes required to the simple thread pool that allow you to wait\nfor tasks to complete and then pass return values from the task to the waiting\nthread. Because std::packaged_task<> instances are not copyable, just movable, you\ncan no longer use std::function<> for the queue entries, because std::function<>\nrequires that the stored function objects are copy-constructible. Instead, you must\nuse a custom function wrapper that can handle move-only types. This is a simple\ntype-erasure class with a function call operator. You only need to handle functions\nthat take no parameters and return void, so this is a straightforward virtual call in\nthe implementation.\nclass function_wrapper\n{\n    struct impl_base {\n        virtual void call()=0;\n        virtual ~impl_base() {}\n    };\n    std::unique_ptr<impl_base> impl;\n    template<typename F>\n    struct impl_type: impl_base\n    {\n        F f;\n        impl_type(F&& f_): f(std::move(f_)) {}\n        void call() { f(); }\n    };\npublic:\n    template<typename F>\n    function_wrapper(F&& f):\n        impl(new impl_type<F>(std::move(f)))\n    {}\n    void operator()() { impl->call(); }\n    function_wrapper() = default;\n    function_wrapper(function_wrapper&& other):\n        impl(std::move(other.impl))\n    {}\nListing 9.2\nA thread pool with waitable tasks\n",
      "content_length": 2350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "305\nThread pools\n    function_wrapper& operator=(function_wrapper&& other)\n    {\n        impl=std::move(other.impl);\n        return *this;\n    }\n    function_wrapper(const function_wrapper&)=delete;\n    function_wrapper(function_wrapper&)=delete;\n    function_wrapper& operator=(const function_wrapper&)=delete;\n};\nclass thread_pool\n{\n    thread_safe_queue<function_wrapper> work_queue;   \n    void worker_thread()\n    {\n        while(!done)\n        {\n            function_wrapper task;                    \n            if(work_queue.try_pop(task))\n            {\n                task();\n            }\n            else\n            {\n                std::this_thread::yield();\n            }\n        }\n    }\npublic:\n    template<typename FunctionType>\n    std::future<typename std::result_of<FunctionType()>::type>   \n        submit(FunctionType f)\n    {\n        typedef typename std::result_of<FunctionType()>::type\n            result_type;                                      \n        std::packaged_task<result_type()> task(std::move(f));    \n        std::future<result_type> res(task.get_future());     \n        work_queue.push(std::move(task));    \n        return res;   \n    }\n    // rest as before\n};\nFirst, the modified submit() function B returns a std::future<> to hold the return\nvalue of the task and allow the caller to wait for the task to complete. This requires\nthat you know the return type of the supplied function f, which is where std::\nresult_of<> comes in: std::result_of<FunctionType()>::type is the type of the\nresult of invoking an instance of type FunctionType (such as f) with no arguments.\nYou use the same std::result_of<> expression for the result_type typedef c\ninside the function.\n You then wrap the function f in a std::packaged_task<result_type()> d,\nbecause f is a function or callable object that takes no parameters and returns an\ninstance of type result_type, as we deduced. You can now get your future from the\nUse function_\nwrapper rather \nthan std::function\n b\nc\nd\n e\nf\ng\n",
      "content_length": 2010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "306\nCHAPTER 9\nAdvanced thread management\nstd::packaged_task<> e before pushing the task onto the queue f and returning\nthe future g. Note that you have to use std::move() when pushing the task onto\nthe queue, because std::packaged_task<> isn’t copyable. The queue now stores\nfunction_wrapper objects rather than std::function<void()> objects in order to\nhandle this.\n This pool allows you to wait for your tasks and have them return results. The next\nlisting shows what the parallel_accumulate function looks like with this thread pool.\ntemplate<typename Iterator,typename T>\nT parallel_accumulate(Iterator first,Iterator last,T init)\n{\n    unsigned long const length=std::distance(first,last);\n    if(!length)\n        return init;\n    unsigned long const block_size=25;\n    unsigned long const num_blocks=(length+block_size-1)/block_size;   \n    std::vector<std::future<T> > futures(num_blocks-1);\n    thread_pool pool;\n    Iterator block_start=first;\n    for(unsigned long i=0;i<(num_blocks-1);++i)\n    {\n        Iterator block_end=block_start;\n        std::advance(block_end,block_size);\n        futures[i]=pool.submit([=]{\n            accumulate_block<Iterator,T>()(block_start,block_end);\n        });   \n        block_start=block_end;\n    }\n    T last_result=accumulate_block<Iterator,T>()(block_start,last);\n    T result=init;\n    for(unsigned long i=0;i<(num_blocks-1);++i)\n    {\n        result+=futures[i].get();\n    }\n    result += last_result;\n    return result;\n}\nWhen you compare this against listing 8.4, there are a couple of things to notice. First,\nyou’re working in terms of the number of blocks to use (num_blocks) B rather than\nthe number of threads. In order to make the most use of the scalability of your thread\npool, you need to divide the work into the smallest blocks that it’s worth working with\nconcurrently. When there are only a few threads in the pool, each thread will process\nmany blocks, but as the number of threads grows with the hardware, the number of\nblocks processed in parallel will also grow.\n You need to be careful when choosing the “smallest blocks worth working with con-\ncurrently.” There’s an inherent overhead to submitting a task to a thread pool, having\nthe worker thread run it, and passing the return value through a std::future<>, and\nListing 9.3\nparallel_accumulate using a thread pool with waitable tasks\nb\nc\n",
      "content_length": 2364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "307\nThread pools\nfor small tasks it’s not worth the payoff. If you choose too small a task size, the code\nmay run more slowly with a thread pool than with one thread.\n Assuming the block size is sensible, you don’t have to worry about packaging the\ntasks, obtaining the futures, or storing the std::thread objects so you can join with\nthe threads later; the thread pool takes care of that. All you need to do is call submit()\nwith your task c.\n The thread pool takes care of the exception safety too. Any exception thrown by\nthe task gets propagated through the future returned from submit(), and if the func-\ntion exits with an exception, the thread pool destructor abandons any not-yet-completed\ntasks and waits for the pool threads to finish.\n This works well for simple cases like this, where the tasks are independent. But it’s\nnot so good for situations where the tasks depend on other tasks also submitted to the\nthread pool.\n9.1.3\nTasks that wait for other tasks\nThe Quicksort algorithm is an example that I’ve used throughout this book. It’s sim-\nple in concept: the data to be sorted is partitioned into those items that go before a\npivot item and those that go after it in the sorted sequence. These two sets of items are\nrecursively sorted and then stitched back together to form a fully sorted set. When\nparallelizing this algorithm, you need to ensure that these recursive calls make use of\nthe available concurrency.\n Back in chapter 4, when I first introduced this example, you used std::async to\nrun one of the recursive calls at each stage, letting the library choose between running\nit on a new thread and running it synchronously when the relevant get() was called.\nThis works well, because each task is either running on its own thread or will be\ninvoked when required.\n When we revisited the implementation in chapter 8, you saw an alternative struc-\nture that used a fixed number of threads related to the available hardware concur-\nrency. In this case, you used a stack of pending chunks that needed sorting. As each\nthread partitioned the data it was sorting, it added a new chunk to the stack for one of\nthe sets of data and then sorted the other one directly. At this point, a straightforward\nwait for the sorting of the other chunk to complete would potentially deadlock,\nbecause you’d be consuming one of your limited number of threads waiting. It would\nbe easy to end up in a situation where all of the threads were waiting for chunks to be\nsorted and no threads were doing any sorting. We addressed this issue by having the\nthreads pull chunks off the stack and sort them while the particular chunk they were\nwaiting for was unsorted.\n You’d get the same problem if you substituted a simple thread pool like the ones\nyou’ve seen so far in this chapter, instead of std::async in the example from chapter 4.\nThere are now only a limited number of threads, and they might end up all waiting\nfor tasks that haven’t been scheduled because there are no free threads. You therefore\nneed to use a solution similar to the one you used in chapter 8: process outstanding\n",
      "content_length": 3091,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "308\nCHAPTER 9\nAdvanced thread management\nchunks while you’re waiting for your chunk to complete. If you’re using the thread\npool to manage the list of tasks and their association with threads—which is, after all,\nthe whole point of using a thread pool—you don’t have access to the task list to do\nthis. What you need to do is modify the thread pool to do this automatically.\n The simplest way to do this is to add a new function on thread_pool to run a task\nfrom the queue and manage the loop yourself, so we’ll go with that. Advanced thread\npool implementations might add logic into the wait function or additional wait func-\ntions to handle this case, possibly prioritizing the task being waited for. The following\nlisting shows the new run_pending_task() function, and a modified Quicksort to\nmake use of it is shown in listing 9.5.\nvoid thread_pool::run_pending_task()\n{\n    function_wrapper task;\n    if(work_queue.try_pop(task))\n    {\n        task();\n    }\n    else\n    {\n        std::this_thread::yield();\n    }\n}\nThis implementation of run_pending_task() is lifted straight out of the main loop of\nthe worker_thread() function, which can now be modified to call the extracted\nrun_pending_task(). This tries to take a task off the queue and run it if there is one;\notherwise, it yields to allow the OS to reschedule the thread. The Quicksort imple-\nmentation in listing 9.5 is a lot simpler than the corresponding version from listing\n8.1, because all the thread-management logic has been moved to the thread pool.\ntemplate<typename T>\nstruct sorter        \n{\n    thread_pool pool;    \n    \n    std::list<T> do_sort(std::list<T>& chunk_data)\n    {\n        if(chunk_data.empty())\n        {\n            return chunk_data;\n        }\n        std::list<T> result;\n        result.splice(result.begin(),chunk_data,chunk_data.begin());\n        T const& partition_val=*result.begin();\nListing 9.4\nAn implementation of run_pending_task()\nListing 9.5\nA thread-pool–based implementation of Quicksort\nb\nc\n",
      "content_length": 1999,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "309\nThread pools\n        typename std::list<T>::iterator divide_point=\n            std::partition(chunk_data.begin(),chunk_data.end(),\n                           [&](T const& val){return val<partition_val;});\n        std::list<T> new_lower_chunk;\n        new_lower_chunk.splice(new_lower_chunk.end(),\n                               chunk_data,chunk_data.begin(),\n                               divide_point);\n        std::future<std::list<T> > new_lower=    \n            pool.submit(std::bind(&sorter::do_sort,this,\n                                  std::move(new_lower_chunk)));\n        std::list<T> new_higher(do_sort(chunk_data));\n        result.splice(result.end(),new_higher);\n        while(new_lower.wait_for(std::chrono::seconds(0)) ==\n            std::future_status::timeout)\n        {\n            pool.run_pending_task();    \n        }\n        result.splice(result.begin(),new_lower.get());\n        return result;\n    }\n};\ntemplate<typename T>\nstd::list<T> parallel_quick_sort(std::list<T> input)\n{\n    if(input.empty())\n    {\n        return input;\n    }\n    sorter<T> s;\n    return s.do_sort(input);\n}\nAs in listing 8.1, you’ve delegated the real work to the do_sort() member function of\nthe sorter class template B, although in this case the class is only there to wrap the\nthread_pool instance c. \n Your thread and task management are now reduced to submitting a task to the\npool d and running pending tasks while waiting e. This is much simpler than in list-\ning 8.1, where you had to explicitly manage the threads and the stack of chunks to\nsort. When submitting the task to the pool, you use std::bind() to bind the this\npointer to do_sort() and to supply the chunk to sort. In this case, you call std::move()\non new_lower_chunk as you pass it in, to ensure that the data is moved rather than\ncopied.\n Although this has now addressed the crucial deadlock-causing problem with tasks\nthat wait for other tasks, this thread pool is still far from ideal. For starters, every call to\nsubmit() and every call to run_pending_task()accesses the same queue. You saw in\nchapter 8 how having a single set of data modified by multiple threads can have a det-\nrimental effect on performance, so you need to address this problem.\nd\ne\n",
      "content_length": 2235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "310\nCHAPTER 9\nAdvanced thread management\n9.1.4\nAvoiding contention on the work queue\nEvery time a thread calls submit() on a particular instance of the thread pool, it has\nto push a new item onto the single shared work queue. Likewise, the worker threads\nare continually popping items off the queue in order to run the tasks. This means that\nas the number of processors increases, there’s increasing contention on the queue.\nThis can be a real performance drain; even if you use a lock-free queue so there’s no\nexplicit waiting, cache ping-pong can be a substantial time sink.\n One way to avoid cache ping-pong is to use a separate work queue per thread.\nEach thread then posts new items to its own queue and takes work from the global\nwork queue only if there’s no work on its own individual queue. The following listing\nshows an implementation that makes use of a thread_local variable to ensure that\neach thread has its own work queue, as well as the global one.\nclass thread_pool\n{\n    threadsafe_queue<function_wrapper> pool_work_queue;\n    typedef std::queue<function_wrapper> local_queue_type;   \n    static thread_local std::unique_ptr<local_queue_type>\n        local_work_queue;    \n    void worker_thread()\n    {\n        local_work_queue.reset(new local_queue_type);   \n        \n        while(!done)\n        {\n            run_pending_task();\n        }\n    }\npublic:\n    template<typename FunctionType>\n    std::future<typename std::result_of<FunctionType()>::type>\n        submit(FunctionType f)\n    {\n        typedef typename std::result_of<FunctionType()>::type result_type;\n        std::packaged_task<result_type()> task(f);\n        std::future<result_type> res(task.get_future());\n        if(local_work_queue)         \n        {\n            local_work_queue->push(std::move(task));\n        }\n        else\n        {\n            pool_work_queue.push(std::move(task));   \n        }\n        return res;\n    }\n    void run_pending_task()\n    {\n        function_wrapper task;\nListing 9.6\nA thread pool with thread-local work queues\nb\nc\nd\ne\nf\n",
      "content_length": 2050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "311\nThread pools\n        if(local_work_queue && !local_work_queue->empty())    \n        {\n            task=std::move(local_work_queue->front());\n            local_work_queue->pop();\n            task();\n        }\n        else if(pool_work_queue.try_pop(task))    \n        {\n            task();\n        }\n        else\n        {\n            std::this_thread::yield();\n        }\n    }\n    // rest as before\n};\nYou’ve used a std::unique_ptr<> to hold the thread-local work queue c because\nyou don’t want other threads that aren't part of your thread pool to have one; this is\ninitialized in the worker_thread() function before the processing loop d. The destruc-\ntor of std::unique_ptr<> will ensure that the work queue is destroyed when the\nthread exits.\n submit() then checks to see if the current thread has a work queue e. If it does,\nit’s a pool thread, and you can put the task on the local queue; otherwise, you need to\nput the task on the pool queue as before f.\n There’s a similar check in run_pending_task() g, except this time you also need\nto check to see if there are any items on the local queue. If there are, you can take the\nfront one and process it; notice that the local queue can be a plain std::queue< B\nbecause it’s only ever accessed by the one thread. If there are no tasks on the local\nqueue, you try the pool queue as before h.\n This works fine for reducing contention, but when the distribution of work is\nuneven, it can easily result in one thread having a lot of work in its queue while the\nothers have no work do to. For example, with the Quicksort example, only the top-\nmost chunk would make it to the pool queue, because the remaining chunks would\nend up on the local queue of the worker thread that processed that one. This defeats\nthe purpose of using a thread pool.\n Thankfully, there is a solution to this: allow the threads to steal work from each\nother’s queues if there’s no work in their queue and no work in the global queue.\n9.1.5\nWork stealing\nIn order to allow a thread with no work to do to take work from another thread with a\nfull queue, the queue must be accessible to the thread doing the stealing from run_\npending_tasks(). This requires that each thread register its queue with the thread\npool or be given one by the thread pool. Also, you must ensure that the data in the work\nqueue is suitably synchronized and protected so that your invariants are protected.\ng\nh\n",
      "content_length": 2413,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "312\nCHAPTER 9\nAdvanced thread management\n It’s possible to write a lock-free queue that allows the owner thread to push and\npop at one end while other threads can steal entries from the other, but the imple-\nmentation of this queue is beyond the scope of this book. In order to demonstrate the\nidea, we’ll stick to using a mutex to protect the queue’s data. We hope work stealing is\na rare event, so there should be little contention on the mutex, and this simple queue\nshould therefore have minimal overhead. A simple lock-based implementation is\nshown here.\nclass work_stealing_queue\n{\nprivate:\n    typedef function_wrapper data_type;\n    std::deque<data_type> the_queue;    \n    mutable std::mutex the_mutex;\npublic:\n    work_stealing_queue()\n    {}\n    work_stealing_queue(const work_stealing_queue& other)=delete;\n    work_stealing_queue& operator=(\n        const work_stealing_queue& other)=delete;\n    void push(data_type data)   \n    {\n        std::lock_guard<std::mutex> lock(the_mutex);\n        the_queue.push_front(std::move(data));\n    }\n    bool empty() const\n    {\n        std::lock_guard<std::mutex> lock(the_mutex);\n        return the_queue.empty();\n    }\n    bool try_pop(data_type& res)   \n    {\n        std::lock_guard<std::mutex> lock(the_mutex);\n        if(the_queue.empty())\n        {\n            return false;\n        }\n        res=std::move(the_queue.front());\n        the_queue.pop_front();\n        return true;\n    }\n    bool try_steal(data_type& res)   \n    {\n        std::lock_guard<std::mutex> lock(the_mutex);\n        if(the_queue.empty())\n        {\n            return false;\n        }\n        res=std::move(the_queue.back());\n        the_queue.pop_back();\n        return true;\nListing 9.7\nLock-based queue for work stealing\nb\nc\nd\ne\n",
      "content_length": 1763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "313\nThread pools\n    }\n};\nThis queue is a simple wrapper around a std::deque<function_wrapper> B that\nprotects all accesses with a mutex lock. Both push() c and try_pop() d work on the\nfront of the queue, while try_steal() e works on the back.\n This means that this “queue” is a last-in-first-out stack for its own thread; the task\nmost recently pushed on is the first one off again. This can help improve performance\nfrom a cache perspective, because the data related to that task is more likely to still be\nin the cache than the data related to a task pushed on the queue previously. Also, it\nmaps nicely to algorithms such as Quicksort. In the previous implementation, each\ncall to do_sort() pushes one item on the stack and then waits for it. By processing the\nmost recent item first, you ensure that the chunk needed for the current call to com-\nplete is processed before the chunks needed for the other branches, reducing the\nnumber of active tasks and the total stack usage. try_steal() takes items from the\nopposite end of the queue to try_pop() in order to minimize contention; you could\npotentially use the techniques discussed in chapters 6 and 7 to enable concurrent calls\nto try_pop() and try_steal().\n OK, so you have your nice sparkly work queue that permits stealing; how do you\nuse it in your thread pool? Here’s one potential implementation.\nclass thread_pool\n{\n    typedef function_wrapper task_type;\n    std::atomic_bool done;\n    threadsafe_queue<task_type> pool_work_queue;\n    std::vector<std::unique_ptr<work_stealing_queue> > queues;   \n    std::vector<std::thread> threads;\n    join_threads joiner;\n    static thread_local work_stealing_queue* local_work_queue;   \n    static thread_local unsigned my_index;\n    void worker_thread(unsigned my_index_)\n    {\n        my_index=my_index_;\n        local_work_queue=queues[my_index].get();   \n        while(!done)\n        {\n            run_pending_task();\n        }\n    }\n    bool pop_task_from_local_queue(task_type& task)\n    {\n        return local_work_queue && local_work_queue->try_pop(task);\n    }\n    bool pop_task_from_pool_queue(task_type& task)\n    {\n        return pool_work_queue.try_pop(task);\n    }\nListing 9.8\nA thread pool that uses work stealing\nb\nc\nd\n",
      "content_length": 2239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "314\nCHAPTER 9\nAdvanced thread management\n    bool pop_task_from_other_thread_queue(task_type& task)   \n    {\n        for(unsigned i=0;i<queues.size();++i)\n        {\n            unsigned const index=(my_index+i+1)%queues.size();   \n            if(queues[index]->try_steal(task))\n            {\n                return true;\n            }\n        }\n        return false;\n    }\npublic:\n    thread_pool():\n        done(false),joiner(threads)\n    {\n        unsigned const thread_count=std::thread::hardware_concurrency();\n        try\n        {\n            for(unsigned i=0;i<thread_count;++i)\n            {\n                queues.push_back(std::unique_ptr<work_stealing_queue>(  \n                                     new work_stealing_queue));\n            }\n            for(unsigned i=0;i<thread_count;++i)\n            {\n                threads.push_back(\n                    std::thread(&thread_pool::worker_thread,this,i));\n            }\n        }\n        catch(...)\n        {\n            done=true;\n            throw;\n        }\n    }\n    ~thread_pool()\n    {\n        done=true;\n    }\n    template<typename FunctionType>\n    std::future<typename std::result_of<FunctionType()>::type> submit(\n        FunctionType f)\n    {\n        typedef typename std::result_of<FunctionType()>::type result_type;\n        std::packaged_task<result_type()> task(f);\n        std::future<result_type> res(task.get_future());\n        if(local_work_queue)\n        {\n            local_work_queue->push(std::move(task));\n        }\n        else\n        {\n            pool_work_queue.push(std::move(task));\n        }\ne\nf\ng\n",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "315\nInterrupting threads\n        return res;\n    }\n    void run_pending_task()\n    {\n        task_type task;\n        if(pop_task_from_local_queue(task) ||    \n           pop_task_from_pool_queue(task) ||      \n           pop_task_from_other_thread_queue(task))   \n        {\n            task();\n        }\n        else\n        {\n            std::this_thread::yield();\n        }\n    }\n};\nThis code is similar to listing 9.6. The first difference is that each thread has a\nwork_stealing_queue rather than a plain std::queue<> c. When each thread is cre-\nated, rather than allocating its own work queue, the pool constructor allocates one g,\nwhich is then stored in the list of work queues for this pool B. The index of the queue\nin the list is then passed in to the thread function and used to retrieve the pointer to\nthe queue d. This means that the thread pool can access the queue when trying to\nsteal a task for a thread that has no work to do. run_pending_task() will now try to\ntake a task from its thread’s own queue h, take a task from the pool queue i, or take\na task from the queue of another thread j.\n pop_task_from_other_thread_queue() e iterates through the queues belonging\nto all the threads in the pool, trying to steal a task from each in turn. In order to\navoid every thread trying to steal from the first thread in the list, each thread starts\nat the next thread in the list by offsetting the index of the queue to check by its own\nindex f.\n Now you have a working thread pool that’s good for many potential uses. There\nare still a myriad of ways to improve it for any particular usage, but that’s left as an\nexercise for the reader. One aspect that hasn’t been explored is the idea of dynami-\ncally resizing the thread pool to ensure that there’s optimal CPU usage even when\nthreads are blocked waiting for something such as I/O or a mutex lock.\n Next on the list of “advanced” thread-management techniques is interrupting\nthreads.\n9.2\nInterrupting threads\nIn many situations it’s desirable to signal to a long-running thread that it’s time to\nstop. This might be because it’s a worker thread for a thread pool and the pool is\nnow being destroyed, or because the work being done by the thread has been explic-\nitly canceled by the user, or a myriad of other reasons. Whatever the reason, the idea\nis the same: you need to signal from one thread that another should stop before it\nh\ni j\n",
      "content_length": 2403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "316\nCHAPTER 9\nAdvanced thread management\nreaches the natural end of its processing, and you need to do this in a way that\nallows that thread to terminate nicely rather than abruptly pulling the rug out from\nunder it.\n You could potentially design a separate mechanism for every case where you need\nto do this, but that would be overkill. Not only does a common mechanism make it\neasier to write the code on subsequent occasions, but it can allow you to write code\nthat can be interrupted, without having to worry about where that code is being used.\nThe C++11 Standard doesn’t provide this mechanism (though there is an active pro-\nposal for adding interrupt support to a future C++ standard1), but it’s relatively\nstraightforward to build one. Let’s look at how you can do that, starting from the\npoint of view of the interface for launching and interrupting a thread rather than that\nof the thread being interrupted.\n9.2.1\nLaunching and interrupting another thread\nTo start with, let’s look at the external interface. What do you need from an interrupt-\nible thread? At the basic level, all you need is the same interface as you have for\nstd::thread, with an additional interrupt() function:\nclass interruptible_thread\n{\npublic:\n    template<typename FunctionType>\n    interruptible_thread(FunctionType f);\n    void join();\n    void detach();\n    bool joinable() const;\n    void interrupt();\n};\nInternally, you can use std::thread to manage the thread itself and use some custom\ndata structure to handle the interruption. Now, what about from the point of view of\nthe thread itself? At the most basic level you want to be able to say “I can be inter-\nrupted here”—you want an interruption point. For this to be usable without having to\npass down additional data, it needs to be a simple function that can be called without\nany parameters: interruption_point(). This implies that the interruption-specific\ndata structure needs to be accessible through a thread_local variable that’s set when\nthe thread is started, so that when a thread calls your interruption_point() func-\ntion, it checks the data structure for the currently-executing thread. We’ll look at the\nimplementation of interruption_point() later.\n This thread_local flag is the primary reason you can’t use plain std::thread to\nmanage the thread; it needs to be allocated in a way that the interruptible_thread\ninstance can access, as well as the newly started thread. You can do this by wrapping\n1 P0660: A Cooperatively Interruptible Joining Thread, Rev 3, Nicolai Josuttis, Herb Sutter, Anthony Williams\nhttp://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0660r3.pdf.\n",
      "content_length": 2640,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "317\nInterrupting threads\nthe supplied function before you pass it to std::thread to launch the thread in the\nconstructor, as shown in the next listing.\nclass interrupt_flag\n{\npublic:\n    void set();\n    bool is_set() const;\n};\nthread_local interrupt_flag this_thread_interrupt_flag;    \nclass interruptible_thread\n{\n    std::thread internal_thread;\n    interrupt_flag* flag;\npublic:\n    template<typename FunctionType>\n    interruptible_thread(FunctionType f)\n    {\n        std::promise<interrupt_flag*> p;    \n        internal_thread=std::thread([f,&p]{           \n                p.set_value(&this_thread_interrupt_flag);\n                f();                         \n            });\n        flag=p.get_future().get();   \n    }\n    void interrupt()\n    {\n        if(flag)\n        {\n            flag->set();   \n        }\n    }\n};\nThe supplied function f is wrapped in a lambda function d, which holds a copy of f\nand a reference to the local promise, p c. The lambda sets the value of the promise\nto the address of the this_thread_interrupt_flag (which is declared thread_local\nB) for the new thread before invoking the copy of the supplied function e. The\ncalling thread then waits for the future associated with the promise to become ready\nand stores the result in the flag member variable f. Note that even though the\nlambda is running on the new thread and has a dangling reference to the local vari-\nable, p, this is OK because the interruptible_thread constructor waits until p is no\nlonger referenced by the new thread before returning. Note that this implementa-\ntion doesn’t take account of handling joining with the thread, or detaching it. You\nneed to ensure that the flag variable is cleared when the thread exits, or is detached,\nto avoid a dangling pointer.\n The interrupt() function is then relatively straightforward: if you have a valid\npointer to an interrupt flag, you have a thread to interrupt, so you can set the flag g.\nListing 9.9\nBasic implementation of interruptible_thread\nb\nc\nd\ne\nf\ng\n",
      "content_length": 2013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "318\nCHAPTER 9\nAdvanced thread management\nIt’s then up to the interrupted thread what it does with the interruption. Let’s explore\nthat next.\n9.2.2\nDetecting that a thread has been interrupted\nYou can now set the interruption flag, but that doesn’t do you any good if the thread\ndoesn’t check whether it’s being interrupted. In the simplest case you can do this with\nan interruption_point() function; you can call this function at a point where it’s\nsafe to be interrupted, and it throws a thread_interrupted exception if the flag is set:\nvoid interruption_point()\n{\n    if(this_thread_interrupt_flag.is_set())\n    {\n        throw thread_interrupted();\n    }\n}\nYou can use this function by calling it at convenient points within your code:\nvoid foo()\n{\n    while(!done)\n    {\n        interruption_point();\n        process_next_item();\n    }\n}\nAlthough this works, it’s not ideal. Some of the best places for interrupting a thread\nare where it’s blocked waiting for something, which means that the thread isn’t run-\nning in order to call interruption_point()! What you need here is a means for wait-\ning for something in an interruptible fashion.\n9.2.3\nInterrupting a condition variable wait\nOK, so you can detect interruptions at carefully chosen places in your code, with\nexplicit calls to interruption_point(), but that doesn’t help when you want to do a\nblocking wait, such as waiting for a condition variable to be notified. You need a new\nfunction—interruptible_wait()—which you can then overload for the various\nthings you might want to wait for, and you can work out how to interrupt the waiting.\nI’ve already mentioned that one thing you might be waiting for is a condition variable,\nso let’s start there: what do you need to do in order to be able to interrupt a wait on a\ncondition variable? The simplest thing that would work is to notify the condition vari-\nable once you’ve set the interrupt flag, and put an interruption point immediately\nafter the wait. But for this to work, you’d have to notify all threads waiting on the con-\ndition variable in order to ensure that your thread of interest wakes up. Waiters have\nto handle spurious wake-ups anyway, so other threads would handle this the same as a\nspurious wake-up—they wouldn’t be able to tell the difference. The interrupt_flag\n",
      "content_length": 2297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "319\nInterrupting threads\nstructure would need to be able to store a pointer to a condition variable so that it can\nbe notified in a call to set(). One possible implementation of interruptible_wait()\nfor condition variables might look like the following listing.\nvoid interruptible_wait(std::condition_variable& cv,\n                        std::unique_lock<std::mutex>& lk)\n{\n    interruption_point();\n    this_thread_interrupt_flag.set_condition_variable(cv);   \n    cv.wait(lk);                                      \n    this_thread_interrupt_flag.clear_condition_variable();   \n    interruption_point();\n}\nAssuming the presence of some functions for setting and clearing an association of a\ncondition variable with an interrupt flag, this code is nice and simple. It checks for\ninterruption, associates the condition variable with interrupt_flag for the current\nthread B, waits on the condition variable c, clears the association with the condition\nvariable d, and checks for interruption again. If the thread is interrupted during the\nwait on the condition variable, the interrupting thread will broadcast the condition\nvariable and wake you from the wait, so you can check for interruption. Unfortunately,\nthis code is broken: there are two problems with it. The first problem is relatively obvi-\nous if you have your exception safety hat on: std::condition_variable::wait() can\nthrow an exception, so you might exit the function without removing the association\nof the interrupt flag with the condition variable. This is easily fixed with a structure\nthat removes the association in its destructor.\n The second, less obvious problem is that there’s a race condition. If the thread is\ninterrupted after the initial call to interruption_point(), but before the call to\nwait(), then it doesn’t matter whether the condition variable has been associated with\nthe interrupt flag, because the thread isn’t waiting and so can’t be woken by a notify on the\ncondition variable. You need to ensure that the thread can’t be notified between the last\ncheck for interruption and the call to wait(). Without delving into the internals of\nstd::condition_variable, you have only one way of doing that: use the mutex held\nby lk to protect this too, which requires passing it in on the call to set_condition\n_variable(). Unfortunately, this creates its own problems: you’d be passing a refer-\nence to a mutex whose lifetime you don’t know to another thread (the thread doing\nthe interrupting) for that thread to lock (in the call to interrupt()), without know-\ning whether that thread has locked the mutex already when it makes the call. This has\nthe potential for deadlock and the potential to access a mutex after it has already been\ndestroyed, so it’s a nonstarter. It would be rather too restrictive if you couldn’t reliably\ninterrupt a condition variable wait—you can do almost as well without a special\ninterruptible_wait()—so what other options do you have? One option is to put a\nListing 9.10\nA broken version of interruptible_wait for std::condition\n_variable\nb\nc\nd\n",
      "content_length": 3061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "320\nCHAPTER 9\nAdvanced thread management\ntimeout on the wait; use wait_for() rather than wait() with a small timeout value\n(such as 1 ms). This puts an upper limit on how long the thread will have to wait\nbefore it sees the interruption (subject to the tick granularity of the clock). If you do\nthis, the waiting thread will see more “spurious” wakes resulting from the timeout, but\nit can’t easily be helped. This implementation is shown in the next listing, along with\nthe corresponding implementation of interrupt_flag.\nclass interrupt_flag\n{\n    std::atomic<bool> flag;\n    std::condition_variable* thread_cond;\n    std::mutex set_clear_mutex;\npublic:\n    interrupt_flag():\n        thread_cond(0)\n    {}\n    void set()\n    {\n        flag.store(true,std::memory_order_relaxed);\n        std::lock_guard<std::mutex> lk(set_clear_mutex);\n        if(thread_cond)\n        {\n            thread_cond->notify_all();\n        }\n    }\n    bool is_set() const\n    {\n        return flag.load(std::memory_order_relaxed);\n    }\n    void set_condition_variable(std::condition_variable& cv)\n    {\n        std::lock_guard<std::mutex> lk(set_clear_mutex);\n        thread_cond=&cv;\n    }\n    void clear_condition_variable()\n    {\n        std::lock_guard<std::mutex> lk(set_clear_mutex);\n        thread_cond=0;\n    }\n    struct clear_cv_on_destruct\n    {\n        ~clear_cv_on_destruct()\n        {\n            this_thread_interrupt_flag.clear_condition_variable();\n        }\n    };\n};\nvoid interruptible_wait(std::condition_variable& cv,\n                        std::unique_lock<std::mutex>& lk)\n{\nListing 9.11\nUsing a timeout in interruptible_wait for std::condition\n_variable\n",
      "content_length": 1659,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "321\nInterrupting threads\n    interruption_point();\n    this_thread_interrupt_flag.set_condition_variable(cv);          \n    interrupt_flag::clear_cv_on_destruct guard;\n    interruption_point();\n    cv.wait_for(lk,std::chrono::milliseconds(1));\n    interruption_point();\n}\nIf you have the predicate that’s being waited for, then the 1 ms timeout can be com-\npletely hidden inside the predicate loop:\ntemplate<typename Predicate>\nvoid interruptible_wait(std::condition_variable& cv,\n                        std::unique_lock<std::mutex>& lk,\n                        Predicate pred)\n{\n    interruption_point();\n    this_thread_interrupt_flag.set_condition_variable(cv);          \n    interrupt_flag::clear_cv_on_destruct guard;\n    while(!this_thread_interrupt_flag.is_set() && !pred())\n    {\n        cv.wait_for(lk,std::chrono::milliseconds(1));\n    }\n    interruption_point();\n}\nThis will result in the predicate being checked more often than it might otherwise be,\nbut it’s easily used in place of a plain call to wait(). The variants with timeouts are eas-\nily implemented: wait either for the time specified, or 1 ms, whichever is shortest. OK,\nso std::condition_variable waits are now taken care of; what about std::condition\n_variable_any? Is this the same, or can you do better?\n9.2.4\nInterrupting a wait on std::condition_variable_any\nstd::condition_variable_any differs from std::condition_variable in that it works\nwith any lock type rather than just std::unique_lock<std::mutex>. It turns out that\nthis makes things much easier, and you can do better with std::condition_variable\n_any than you could with std::condition_variable. Because it works with any lock\ntype, you can build your own lock type that locks/unlocks both the internal set_clear\n_mutex in your interrupt_flag and the lock supplied to the wait call, as shown here.\nclass interrupt_flag\n{\n    std::atomic<bool> flag;\n    std::condition_variable* thread_cond;\n    std::condition_variable_any* thread_cond_any;\n    std::mutex set_clear_mutex;\npublic:\n    interrupt_flag():\nListing 9.12\ninterruptible_wait for std::condition_variable_any\n",
      "content_length": 2109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "322\nCHAPTER 9\nAdvanced thread management\n        thread_cond(0),thread_cond_any(0)\n    {}\n    void set()\n    {\n        flag.store(true,std::memory_order_relaxed);\n        std::lock_guard<std::mutex> lk(set_clear_mutex);\n        if(thread_cond)\n        {\n            thread_cond->notify_all();\n        }\n        else if(thread_cond_any)\n        {\n            thread_cond_any->notify_all();\n        }\n    }\n    template<typename Lockable>\n    void wait(std::condition_variable_any& cv,Lockable& lk)\n    {\n        struct custom_lock\n        {\n            interrupt_flag* self;\n            Lockable& lk;\n            custom_lock(interrupt_flag* self_,\n                        std::condition_variable_any& cond,\n                        Lockable& lk_):\n                self(self_),lk(lk_)\n            {\n                self->set_clear_mutex.lock();   \n                self->thread_cond_any=&cond;   \n            }\n            void unlock()    \n            {\n                lk.unlock();\n                self->set_clear_mutex.unlock();\n            }\n            void lock()\n            {\n                std::lock(self->set_clear_mutex,lk);    \n            }\n            ~custom_lock()\n            {\n                self->thread_cond_any=0;   \n                self->set_clear_mutex.unlock();\n            }\n        };\n        custom_lock cl(this,cv,lk);\n        interruption_point();\n        cv.wait(cl);\n        interruption_point();\n    }\n    // rest as before    \n};\ntemplate<typename Lockable>\nvoid interruptible_wait(std::condition_variable_any& cv,\n                        Lockable& lk)\nB\nc\nd\ne\nf\n",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "323\nInterrupting threads\n{\n    this_thread_interrupt_flag.wait(cv,lk);\n}\nYour custom lock type acquires the lock on the internal set_clear_mutex when it’s\nconstructed B, and then sets the thread_cond_any pointer to refer to the std:: con-\ndition_variable_any passed in to the constructor c. The Lockable reference is\nstored for later; this must already be locked. You can now check for an interruption\nwithout worrying about races. If the interrupt flag is set at this point, it was set before\nyou acquired the lock on set_clear_mutex. When the condition variable calls your\nunlock() function inside wait(), you unlock the Lockable object and the internal\nset_clear_mutex d. This allows threads that are trying to interrupt you to acquire\nthe lock on set_clear_mutex and check the thread_cond_any pointer once you’re\ninside the wait() call but not before. This is exactly what you were after (but couldn’t\nmanage) with std::condition_variable. Once wait() has finished waiting (either\nbecause it was notified or because of a spurious wake), it will call your lock() func-\ntion, which again acquires the lock on the internal set_clear_mutex and the lock on\nthe Lockable object e. You can now check again for interruptions that happened\nduring the wait() call before clearing the thread_cond_any pointer in your cus-\ntom_lock destructor f, where you also unlock the set_clear_mutex.\n9.2.5\nInterrupting other blocking calls\nThat rounds up interrupting condition variable waits, but what about other blocking\nwaits: mutex locks, waiting for futures, and the like? In general you have to go for the\ntimeout option you used for std::condition_variable because there’s no way to\ninterrupt the wait short of fulfilling the condition being waited for, without access to\nthe internals of the mutex or future. But with those other things, you do know what\nyou’re waiting for, so you can loop within the interruptible_wait() function. As an\nexample, here’s an overload of interruptible_wait() for std::future<>:\ntemplate<typename T>\nvoid interruptible_wait(std::future<T>& uf)\n{\n   while(!this_thread_interrupt_flag.is_set())\n   {\n       if(uf.wait_for(lk,std::chrono::milliseconds(1))==\n           std::future_status::ready)\n           break;\n   }\n   interruption_point();\n}\nThis waits until either the interrupt flag is set or the future is ready but does a block-\ning wait on the future for 1 ms at a time. This means that on average it will be around\n0.5 ms before an interrupt request is acknowledged, assuming a high-resolution clock.\nThe wait_for will typically wait at least a whole clock tick, so if your clock ticks every\n",
      "content_length": 2620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "324\nCHAPTER 9\nAdvanced thread management\n15 ms, you’ll end up waiting around 15 ms rather than 1 ms. This may or may not be\nacceptable, depending on the circumstances. You can always reduce the timeout if\nnecessary (and if the clock supports it). The downside of reducing the timeout is that\nthe thread will wake more often to check the flag, and this will increase the task-\nswitching overhead.\n OK, we’ve looked at how you might detect interruption with the interruption\n_point() and interruptible_wait() functions, but how do you handle that?\n9.2.6\nHandling interruptions\nFrom the point of view of the thread being interrupted, an interruption is a thread\n_interrupted exception, which can therefore be handled like any other exception. In\nparticular, you can catch it in a standard catch block:\ntry\n{\n    do_something();\n}\ncatch(thread_interrupted&)\n{\n    handle_interruption();\n}\nThis means that you could catch the interruption, handle it in some way, and then\ncarry on regardless. If you do this, and another thread calls interrupt() again, your\nthread will be interrupted again the next time it calls an interruption point. You might\nwant to do this if your thread is performing a series of independent tasks; interrupting\none task will cause that task to be abandoned, and the thread can then move on to\nperforming the next task in the list.\n Because thread_interrupted is an exception, all the usual exception-safety precau-\ntions must also be taken when calling code that can be interrupted, in order to ensure\nthat resources aren’t leaked, and your data structures are left in a coherent state. Often,\nit will be desirable to let the interruption terminate the thread, so you can let the excep-\ntion propagate up. But if you let exceptions propagate out of the thread function\npassed to the std::thread constructor, std::terminate() will be called, and the\nwhole program will be terminated. In order to avoid having to remember to put a catch\n(thread_interrupted) handler in every function you pass to interruptible_thread,\nyou can instead put that catch block inside the wrapper you use for initializing the\ninterrupt_flag. This makes it safe to allow the interruption exception to propagate\nunhandled, because it will then terminate that individual thread. The initialization of\nthe thread in the interruptible_thread constructor now looks like this:\ninternal_thread=std::thread([f,&p]{\n        p.set_value(&this_thread_interrupt_flag);\n        try\n        {\n            f();\n",
      "content_length": 2490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "325\nInterrupting threads\n        }\n        catch(thread_interrupted const&)\n        {}\n    });\nLet’s now look at a concrete example where interruption is useful.\n9.2.7\nInterrupting background tasks on application exit\nConsider for a moment a desktop search application. As well as interacting with the\nuser, the application needs to monitor the state of the filesystem, identifying any\nchanges and updating its index. This processing is typically left to a background\nthread in order to avoid affecting the responsiveness of the GUI. This background\nthread needs to run for the entire lifetime of the application; it will be started as part\nof the application initialization and left to run until the application is shut down. For\nsuch an application this is typically only when the machine itself is being shut down,\nbecause the application needs to run the whole time in order to maintain an up-to-\ndate index. In any case, when the application is being shut down, you need to close\ndown the background threads in an orderly manner; one way to do this is by inter-\nrupting them.\n The following listing shows a sample implementation of the thread-management\nparts of this system.\nstd::mutex config_mutex;\nstd::vector<interruptible_thread> background_threads;\nvoid background_thread(int disk_id)\n{\n    while(true)\n    {\n        interruption_point();     \n        fs_change fsc=get_fs_changes(disk_id);   \n        if(fsc.has_changes())\n        {\n            update_index(fsc);   \n        }\n    }\n}\nvoid start_background_processing()\n{\n    background_threads.push_back(\n        interruptible_thread(background_thread,disk_1));\n    background_threads.push_back(\n        interruptible_thread(background_thread,disk_2));\n}\nint main()\n{\n    start_background_processing();   \n    process_gui_until_exit();                  \n    std::unique_lock<std::mutex> lk(config_mutex);\nListing 9.13\nMonitoring the filesystem in the background\nB\nc\nd\ne\nf\n",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "326\nCHAPTER 9\nAdvanced thread management\n    for(unsigned i=0;i<background_threads.size();++i)\n    {\n        background_threads[i].interrupt();    \n    }\n    for(unsigned i=0;i<background_threads.size();++i)\n    {\n        background_threads[i].join();   \n    }\n}\nAt startup, the background threads are launched d. The main thread then proceeds\nwith handling the GUI e. When the user has requested that the application exit, the\nbackground threads are interrupted f, and then the main thread waits for each back-\nground thread to complete before exiting g. The background threads sit in a loop,\nchecking for disk changes h and updating the index c. Every time around the loop\nthey check for interruption by calling interruption_point() B.\n Why do you interrupt all the threads before waiting for any? Why not interrupt\neach and then wait for it before moving on to the next? The answer is concurrency.\nThreads will likely not finish immediately when they’re interrupted, because they have\nto proceed to the next interruption point and then run any destructor calls and\nexception-handling code necessary before they exit. By joining with each thread\nimmediately, you therefore cause the interrupting thread to wait, even though it still\nhas useful work it could do—interrupt the other threads. Only when you have no\nmore work to do (all the threads have been interrupted) do you wait. This also allows\nall the threads being interrupted to process their interruptions in parallel and poten-\ntially finish sooner.\n This interruption mechanism could easily be extended to add further interrupt-\nible calls or to disable interruptions across a specific block of code, but this is left as an\nexercise for the reader.\nSummary\nIn this chapter, we’ve looked at various advanced thread-management techniques:\nthread pools and interrupting threads. You’ve seen how the use of local work queues\nand work stealing can reduce the synchronization overhead and potentially improve\nthe throughput of the thread pool and how running other tasks from the queue while\nwaiting for a subtask to complete can eliminate the potential for deadlock.\n We’ve also looked at various ways of allowing one thread to interrupt the process-\ning of another, such as the use of specific interruption points and functions that per-\nform what would otherwise be a blocking wait in a way that can be interrupted.\ng\nh\n",
      "content_length": 2378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "327\nParallel algorithms\nIn the last chapter we looked at advanced thread management and thread pools,\nand in chapter 8 we looked at designing concurrent code, using parallel versions of\nsome algorithms as examples. In this chapter, we’ll look at the parallel algorithms\nprovided by the C++17 standard, so let’s start, without further ado.\n10.1\nParallelizing the standard library algorithms\nThe C++17 standard added the concept of parallel algorithms to the C++ Standard\nLibrary. These are additional overloads of many of the functions that operate on\nranges, such as std::find, std::transform and std::reduce. The parallel ver-\nsions have the same signature as the “normal” single-threaded versions, except for\nthe addition of a new first parameter, which specifies the execution policy to use. For\nexample:\nstd::vector<int> my_data;\nstd::sort(std::execution::par,my_data.begin(),my_data.end());\nThe execution policy of std::execution::par indicates to the standard library that\nit is allowed to perform this call as a parallel algorithm, using multiple threads. Note\nThis chapter covers\nUsing the C++17 parallel algorithms\n",
      "content_length": 1125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "328\nCHAPTER 10\nParallel algorithms\nthat this is permission, not a requirement—the library may still execute the code on a sin-\ngle thread if it wishes. It is also important to note that by specifying an execution pol-\nicy, the requirements on the algorithm complexity have changed, and are usually\nslacker than the requirements for the normal serial algorithm. This is because parallel\nalgorithms often do more total work in order to take advantage of the parallelism of\nthe system — if you can divide the work across 100 processors, then you can still get an\noverall speed up to 50, even if the implementation does twice as much total work.\n Before we get onto the algorithms themselves, let’s take a look at the execution\npolicies.\n10.2\nExecution policies\nThe standard specifies three execution policies:\n\nstd::execution::sequenced_policy \n\nstd::execution::parallel_policy \n\nstd::execution::parallel_unsequenced_policy \nThese are classes defined in the <execution> header. The header also defines three\ncorresponding policy objects to pass to the algorithms:\n\nstd::execution::seq \n\nstd::execution::par \n\nstd::execution::par_unseq \nYou cannot rely on being able to construct objects from these policy classes yourself,\nexcept by copying these three objects, because they might have special initialization\nrequirements. Implementations may also define additional execution policies that have\nimplementation-specific behavior. You cannot define your own execution policies.\n The consequences of these policies on the behavior of the algorithms are described\nin section 10.2.1. Any given implementation is also allowed to provide additional exe-\ncution policies, with whatever semantics they want. Let’s now take a look at the conse-\nquences of using one of the standard execution policies, starting with the general\nchanges for all algorithm overloads that take an exception policy.\n10.2.1 General effects of specifying an execution policy\nIf you pass an execution policy to one of the standard library algorithms, then the\nbehavior of that algorithm is now governed by the execution policy. This affects sev-\neral aspects of the behavior:\nThe algorithm’s complexity\nThe behavior when an exception is thrown\nWhere, how, and when the steps of the algorithm are executed\nEFFECTS ON ALGORITHM COMPLEXITY\nIf an execution policy is supplied to an algorithm, then that algorithm’s complexity\nmay be changed: in addition to the scheduling overhead of managing the parallel\n",
      "content_length": 2474,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "329\nExecution policies\nexecution, many parallel algorithms will perform more of the core operations of the\nalgorithm (whether swaps, comparisons, or applications of a supplied function object), with\nthe intention that this provides an overall improvement in the performance in terms\nof total elapsed time.\n The precise details of the complexity change will vary with each algorithm, but the\ngeneral policy is that if an algorithm specifies something will happen exactly some-\nexpression times, or at most some-expression times, then the overload with an execution\npolicy will slacken that requirement to O(some-expression). This means that the overload\nwith an execution policy may perform some multiple of the number of operations\nperformed by its counterpart without an execution policy, where that multiple will\ndepend on the internals of the library and the platform, rather than the data supplied\nto the algorithm.\nEXCEPTIONAL BEHAVIOR\nIf an exception is thrown during execution of an algorithm with an execution policy,\nthen the consequences are determined by the execution policy. All the standard-\nsupplied execution policies will call std::terminate if there are any uncaught excep-\ntions. The only exception that may be thrown by a call to a standard library algorithm\nwith one of the standard execution policies is std::bad_alloc, which is thrown if the\nlibrary cannot obtain sufficient memory resources for its internal operations. For\nexample, the following call to std::for_each, without an execution policy, will propa-\ngate the exception\nstd::for_each(v.begin(),v.end(),[](auto x){ throw my_exception(); });\nwhereas the corresponding call with an execution policy will terminate the program:\nstd::for_each(\n    std::execution::seq,v.begin(),v.end(),\n    [](auto x){ throw my_exception(); });\nThis is one of the key differences between using std::execution::seq and not pro-\nviding an execution policy.\nWHERE AND WHEN ALGORITHM STEPS ARE EXECUTED\nThis is the fundamental aspect of an execution policy, and is the only aspect that dif-\nfers between the standard execution policies. The policy specifies which execution\nagents are used to perform the steps of the algorithm, be they “normal” threads, vec-\ntor streams, GPU threads, or anything else. The execution policy will also specify\nwhether there are any ordering constraints on how the algorithm steps are run:\nwhether or not they are run in any particular order, whether or not parts of separate\nalgorithm steps may be interleaved with each other, or run in parallel with each other,\nand so forth.\n The details for each of the standard execution policies are given in sections 10.2.2,\n10.2.3, and 10.2.4, starting with the most basic policy: std::execution::sequenced\n_policy.\n",
      "content_length": 2748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "330\nCHAPTER 10\nParallel algorithms\n10.2.2 std::execution::sequenced_policy\nThe sequenced policy is not a policy for parallelism: using it forces the implementa-\ntion to perform all operations on the thread that called the function, so there is no\nparallelism. But it is still an execution policy, and therefore has the same conse-\nquences on algorithmic complexity and the effect of exceptions as the other stan-\ndard policies.\n Not only must all operations be performed on the same thread, but they must be\nperformed in some definite order, so they are not interleaved. The precise order is\nunspecified, and may be different between different invocations of the function. In\nparticular, the order of execution of the operations is not guaranteed to be the same\nas that of the corresponding overload without an execution policy. For example, the\nfollowing call to std::for_each will populate the vector with the numbers 1-1,000, in\nan unspecified order. This is in contrast to the overload without an execution policy,\nwhich will store the numbers in order:\nstd::vector<int> v(1000);\nint count=0;\nstd::for_each(std::execution::seq,v.begin(),v.end(),\n    [&](int& x){ x=++count; });\nThe numbers may be stored in order, but you cannot rely on it.\n This means that the sequenced policy imposes few requirements on the iterators,\nvalues, and callable objects used with the algorithm: they may freely use synchroniza-\ntion mechanisms, and may rely on all operations being invoked on the same thread,\nthough they cannot rely on the order of these operations.\n10.2.3 std::execution::parallel_policy\nThe parallel policy provides basic parallel execution across a number of threads.\nOperations may be performed either on the thread that invoked the algorithm, or on\nthreads created by the library. Operations performed on a given thread must be per-\nformed in a definite order, and not interleaved, but the precise order is unspecified,\nand may vary between invocations. A given operation will run on a fixed thread for its\nentire duration.\n This imposes additional requirements on the iterators, values, and callable objects\nused with the algorithm over the sequenced policy: they must not cause data races if\ninvoked in parallel, and must not rely on being run on the same thread as any other\noperation, or indeed rely on not being run on the same thread as any other operation.\n You can use the parallel execution policy for the vast majority of cases where you\nwould have used a standard library algorithm without an execution policy. It’s only\nwhere there is specific ordering between elements that is required, or unsynchronized\naccess to shared data, that is problematic. Incrementing all the values in a vector can\nbe done in parallel:\nstd::for_each(std::execution::par,v.begin(),v.end(),[](auto& x){++x;});\n",
      "content_length": 2807,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "331\nThe parallel algorithms from the C++ Standard Library\nThe previous example of populating a vector is not OK if done with the parallel exe-\ncution policy; specifically, it is undefined behavior:\nstd::for_each(std::execution::par,v.begin(),v.end(),\n    [&](int& x){ x=++count; });\nHere, the variable count is modified from every invocation of the lambda, so if the\nlibrary were to execute the lambdas across multiple threads, this would be a data race,\nand thus undefined behavior. The requirements for std::execution::parallel_\npolicy pre-empt this: it is undefined behavior to make the preceding call, even if the\nlibrary doesn’t use multiple threads for this call. Whether or not something exhibits\nundefined behavior is a static property of the call, rather than dependent on imple-\nmentation details of the library. Synchronization between the function invocations is\npermitted, however, so you could make this defined behavior again either by making\ncount an std::atomic<int> rather than a plain int, or by using a mutex. In this case,\nthat would likely defeat the point of using the parallel execution policy, because that\nwould serialize all the calls, but in the general case it would allow for synchronized\naccess to a shared state.\n10.2.4 std::execution::parallel_unsequenced_policy\nThe parallel unsequenced policy provides the library with the greatest scope for paral-\nlelizing the algorithm in exchange for imposing the strictest requirements on the iter-\nators, values, and callable objects used with the algorithm.\n An algorithm invoked with the parallel unsequenced policy may perform the\nalgorithm steps on unspecified threads of execution, unordered and unsequenced\nwith respect to one another. This means that operations may now be interleaved with\neach other on a single thread, such that a second operation is started on the same\nthread before the first has finished, and may be migrated between threads, so a given\noperation may start on one thread, run further on a second thread, and complete on\na third.\n If you use the parallel unsequenced policy, then the operations invoked on the\niterators, values, and callable objects supplied to the algorithm must not use any form\nof synchronization or call any function that synchronizes with another, or any func-\ntion such that some other code synchronizes with it.\n This means that the operations must only operate on the relevant element, or any\ndata that can be accessed based on that element, and must not modify any state shared\nbetween threads, or between elements.\n We’ll flesh these out with some examples later. For now, let’s take a look at the par-\nallel algorithms themselves.\n10.3\nThe parallel algorithms from the C++ Standard Library\nMost of the algorithms from the <algorithm> and <numeric> headers have overloads\nthat take an execution policy. This comprises: all_of, any_of, none_of, for_each,\n",
      "content_length": 2883,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "332\nCHAPTER 10\nParallel algorithms\nfor_each_n, find, find_if, find_end, find_first_of, adjacent_find, count, count_if,\nmismatch, equal, search, search_n, copy, copy_n, copy_if, move, swap_ranges,\ntransform, replace, replace_if, replace_copy, replace_copy_if, fill, fill_n,\ngenerate, generate_n, remove, remove_if, remove_copy, remove_copy_if, unique,\nunique_copy, reverse, reverse_copy, rotate, rotate_copy, is_partitioned,\npartition, stable_partition, partition_copy, sort, stable_sort, partial_sort,\npartial_sort_copy, is_sorted, is_sorted_until, nth_element, merge, inplace\n_merge, includes, set_union, set_intersection, set_difference, set_symmetric\n_difference, is_heap, is_heap_until, min_element, max_element, minmax_element,\nlexicographical_compare, reduce, transform_reduce, exclusive_scan, inclusive\n_scan, transform_exclusive_scan, transform_inclusive_scan, and adjacent_\ndifference.\n That’s quite a list; pretty much every algorithm in the C++ Standard Library that\ncould be parallelized is in this list. Notable exceptions are things like std::accumulate,\nwhich is strictly a serial accumulation, but its generalized counterpart in std::reduce\ndoes appear in the list — with a suitable warning in the standard that if the reduction\noperation is not both associative and commutative, then the result may be nondeter-\nministic due to the unspecified order of operations.\n For each of the algorithms in the list, every “normal” overload has a new variant\nwhich takes an execution policy as the first argument—the corresponding arguments\nfor the “normal” overload then come after this execution policy. For example,\nstd::sort has two “normal” overloads without an execution policy:\ntemplate<class RandomAccessIterator>\nvoid sort(RandomAccessIterator first, RandomAccessIterator last);\ntemplate<class RandomAccessIterator, class Compare>\nvoid sort(\n    RandomAccessIterator first, RandomAccessIterator last, Compare comp);\nIt therefore also has two overloads with an execution policy:\ntemplate<class ExecutionPolicy, class RandomAccessIterator>\nvoid sort(\n    ExecutionPolicy&& exec,\n    RandomAccessIterator first, RandomAccessIterator last);\ntemplate<class ExecutionPolicy, class RandomAccessIterator, class Compare>\nvoid sort(\n    ExecutionPolicy&& exec,\n    RandomAccessIterator first, RandomAccessIterator last, Compare comp);\nThere is one important difference between the signatures with and without the execu-\ntion policy argument, which only impacts some algorithms: if the “normal” algorithm\nallows Input Iterators or Output Iterators, then the overloads with an execution policy\nrequire Forward Iterators instead. This is because Input Iterators are fundamentally\nsingle-pass: you can only access the current element, and you cannot store iterators to\n",
      "content_length": 2770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "333\nThe parallel algorithms from the C++ Standard Library\nprevious elements. Similarly, Output Iterators only allow writing to the current ele-\nment: you cannot advance them to write a later element, and then backtrack to write a\nprevious one.\nThus, given the “normal” signature for std::copy\ntemplate<class InputIterator, class OutputIterator>\nOutputIterator copy(\n    InputIterator first, InputIterator last, OutputIterator result);\nthe overload with an execution policy is\ntemplate<class ExecutionPolicy,\n    class ForwardIterator1, class ForwardIterator2>\nForwardIterator2 copy(\n    ExecutionPolicy&& policy,\n    ForwardIterator1 first, ForwardIterator1 last, \n    ForwardIterator2 result);\nThough the naming of the template parameters doesn’t carry any direct consequence\nfrom the compiler’s perspective, it does from the C++ Standard’s perspective: the\nnames of the template parameters for Standard Library algorithms denote semantic\nconstraints on the types, and the algorithms will rely on the operations implied by\nIterator categories in the C++ Standard Library\nThe C++ Standard Library defines five categories of iterators: Input Iterators, Output\nIterators, Forward Iterators, Bidirectional Iterators, and Random Access Iterators.\nInput Iterators are single-pass iterators for retrieving values. They are typically used\nfor things like input from a console or network, or generated sequences. Advancing\nan Input Iterator invalidates any copies of that iterator.\nOutput Iterators are single-pass iterators for writing values. They are typically used for\noutput to files, or adding values to a container. Advancing an Output Iterator invali-\ndates any copies of that iterator.\nForward Iterators are multipass iterators for one-way iteration through persistent\ndata. Though you can't make an iterator go back to a previous element, you can store\ncopies and use them to reference earlier elements. Forward Iterators return real ref-\nerences to the elements, and so can be used for both reading and writing (if the tar-\nget is non-const).\nBidirectional Iterators are multipass iterators like Forward Iterators, but they can also\nbe made to go backward to access previous elements.\nRandom Access Iterators are multipass iterators that can go forward and backward\nlike Bidirectional Iterators, but they can go forward and backward in steps larger than\na single element, and you can directly access elements at an offset, using the array\nindex operator.\n",
      "content_length": 2458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "334\nCHAPTER 10\nParallel algorithms\nthose constraints existing, with the specified semantics. In the case of Input Iterators\nvs. Forward Iterators, the former allows dereferencing the iterator to return a proxy\ntype, which is convertible to the value type of the iterator, whereas the latter requires\nthat dereferencing the iterator returns a real reference to the value and that all equal\niterators return a reference to the same value.\n This is important for parallelism: it means that the iterators can be freely copied\naround, and used equivalently. Also, the requirement that incrementing a Forward\nIterator does not invalidate other copies is important, as it means that separate\nthreads can operate on their own copies of the iterators, incrementing them when\nrequired, without concern about invalidating the iterators held by the other threads.\nIf the overload with an execution policy allowed use of Input Iterators, this would\nforce any threads to serialize access to the one and only iterator that was used for read-\ning from the source sequence, which obviously limits the potential for parallelism.\n Let’s have a look at some concrete examples.\n10.3.1 Examples of using parallel algorithms\nThe simplest possible example surely has to be the parallel loop: do something for\neach element of a container. This is the classic example of an embarrassingly parallel\nscenario: each item is independent, so you have the maximum possibility of parallel-\nism. With a compiler that supports OpenMP, you might write\n#pragma omp parallel for\nfor(unsigned i=0;i<v.size();++i){\n    do_stuff(v[i]);\n}\nWith the C++ Standard Library algorithms, you can instead write\nstd::for_each(std::execution::par,v.begin(),v.end(),do_stuff);\nThis will divide the elements of the range between the internal threads created by the\nlibrary, and invoke do_stuff(x) on each element x in the range. How those elements\nare divided between the threads is an implementation detail.\nCHOICE OF EXECUTION POLICY\nstd::execution::par is the policy that you’ll want to use most often, unless your\nimplementation provides a nonstandard policy better suited to your needs. If your\ncode is suitable for parallelization, then it should work with std::execution::par. In\nsome circumstances, you may be able to use std::execution::par_unseq instead.\nThis may do nothing at all (none of the standard execution policies make a guarantee\nabout the level of parallelism that will be attained), but it may give the library addi-\ntional scope to improve the performance of the code by reordering and interleaving\nthe tasks, in exchange for the tighter requirements on your code. Most notable of\nthese tighter requirements is that there is no synchronization used in accessing the\nelements, or performing the operations on the elements. This means that you cannot\n",
      "content_length": 2818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "335\nThe parallel algorithms from the C++ Standard Library\nuse mutexes or atomic variables, or any of the other mechanisms described in previ-\nous chapters, to ensure that accesses from multiple threads are safe; instead, you must\nrely on the algorithm itself not accessing the same element from multiple threads,\nand use external synchronization outside the call to the parallel algorithm to prevent\nother threads accessing the data.\n The example from listing 10.1 shows some code that can be used with std::\nexecution::par, but not std::execution::par_unseq. The use of the internal mutex\nfor synchronization means that attempting to use std::execution::par_unseq would\nbe undefined behavior.\nclass X{\n    mutable std::mutex m;\n    int data;\npublic:\n    X():data(0){}\n    int get_value() const{\n        std::lock_guard guard(m);\n        return data;\n    }\n    void increment(){\n        std::lock_guard guard(m);\n        ++data;\n    }\n};\nvoid increment_all(std::vector<X>& v){\n    std::for_each(std::execution::par,v.begin(),v.end(),\n        [](X& x){\n            x.increment();\n        });\n}\nThe next listing shows an alternative that can be used with std::execution::par_un-\nseq. In this case, the internal per-element mutex has been replaced with a whole-con-\ntainer mutex.\nclass Y{\n    int data;\npublic:\n    Y():data(0){}\n    int get_value() const{\n        return data;\n    }\n    void increment(){\n        ++data;\n    }\n};\nListing 10.1\nParallel algorithms on a class with internal synchronization\nListing 10.2\nParallel algorithms on a class without internal synchronization\n",
      "content_length": 1578,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "336\nCHAPTER 10\nParallel algorithms\nclass ProtectedY{\n    std::mutex m;\n    std::vector<Y> v;\npublic:\n   void lock(){\n         m.lock();\n     }\n   void unlock(){\n         m.unlock();\n     }\n     std::vector<Y>& get_vec(){\n         return v;\n     }\n};\nvoid increment_all(ProtectedY& data){\n    std::lock_guard guard(data);\n    auto& v=data.get_vec();\n    std::for_each(std::execution::par_unseq,v.begin(),v.end(),\n        [](Y& y){\n            y.increment();\n        });\n}\nThe element accesses in listing 10.2 now have no synchronization, and it is safe to use\nstd::execution::par_unseq. The downside is that concurrent accesses from other\nthreads outside the parallel algorithm invocation must now wait for the entire opera-\ntion to complete, rather than the per-element granularity of listing 10.1.\n Let’s now take a look at a more realistic example of how the parallel algorithms\nmight be used: counting visits to a website.\n10.3.2 Counting visits\nSuppose you run a busy website, such that the logs contain millions of entries, and you\nwant to process those logs to see aggregate data: how many visits per page, where do\nthose visits come from, which browsers were used to access the website, and so forth.\nAnalyzing these logs has two parts: processing each line to extract the relevant infor-\nmation, and aggregating the results together. This is an ideal scenario for using paral-\nlel algorithms, because processing each individual line is entirely independent of\neverything else, and aggregating the results can be done piecemeal, provided the final\ntotals are correct.\n In particular, this is the sort of task that transform_reduce is designed for. The\nfollowing listing shows how this could be used for this task.\n#include <vector>\n#include <string>\n#include <unordered_map>\n#include <numeric>\nListing 10.3\nUsing transform_reduce to count visits to pages of a website\n",
      "content_length": 1875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "337\nThe parallel algorithms from the C++ Standard Library\nstruct log_info {\n    std::string page;\n    time_t visit_time;\n    std::string browser;\n    // any other fields\n};\nextern log_info parse_log_line(std::string const &line);   \nusing visit_map_type= std::unordered_map<std::string, unsigned long long>;\nvisit_map_type\ncount_visits_per_page(std::vector<std::string> const &log_lines) {\n    struct combine_visits {\n        visit_map_type\n        operator()(visit_map_type lhs, visit_map_type rhs) const {   \n            if(lhs.size() < rhs.size())\n                std::swap(lhs, rhs);\n            for(auto const &entry : rhs) {\n                lhs[entry.first]+= entry.second;\n            }\n            return lhs;\n        }\n        visit_map_type operator()(log_info log,visit_map_type map) const{ \n            ++map[log.page];\n            return map;\n        }\n        visit_map_type operator()(visit_map_type map,log_info log) const{ \n            ++map[log.page];\n            return map;\n        }\n        visit_map_type operator()(log_info log1,log_info log2) const{ \n            visit_map_type map;\n            ++map[log1.page];\n            ++map[log2.page];\n            return map;\n        }\n    };\n    return std::transform_reduce(      \n        std::execution::par, log_lines.begin(), log_lines.end(),\n        visit_map_type(), combine_visits(), parse_log_line);\n}\nAssuming you’ve got some function parse_log_line to extract the relevant informa-\ntion from a log entry B, your count_visits_per_page function is a simple wrapper\naround a call to std::transform_reduce c. The complexity comes from the reduction\noperation: you need to be able to combine two log_info structures to produce a map,\na log_info structure and a map (either way around), and two maps. This therefore\nmeans that your combine_visits function object needs four overloads of the function\nb\nd\ne\nf\ng\nc\n",
      "content_length": 1882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "338\nCHAPTER 10\nParallel algorithms\ncall operator, d, e, f, and g, which precludes doing it with a simple lambda, even\nthough the implementation of these four overloads is simple.\n The implementation of std::transform_reduce will therefore use the available\nhardware to perform this calculation in parallel (because you passed std::execution\n::par). Writing this algorithm manually is nontrivial, as we saw in the previous chap-\nter, so this allows you to delegate the hard work of implementing the parallelism to\nthe Standard Library implementers, so you can focus on the required outcome.\nSummary\nIn this chapter we looked at the parallel algorithms available in the C++ Standard\nLibrary and how to use them. We looked at the various execution policies, the impact\nyour choice of execution policy has on the behavior of the algorithm, and the restric-\ntions it imposes on your code. We then looked at an example of how this algorithm\nmight be used in real code.\n",
      "content_length": 963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "339\nTesting and debugging\nmultithreaded applications\nUp to now, I’ve focused on what’s involved in writing concurrent code—the tools that\nare available, how to use them, and the overall design and structure of the code. But\nthere’s a crucial part of software development that I haven’t addressed yet: testing\nand debugging. If you’re reading this chapter hoping for an easy way to test concur-\nrent code, you’re going to be sorely disappointed. Testing and debugging concurrent\ncode is hard. What I am going to give you are some techniques that will make things\neasier, alongside some issues that are important to think about.\n Testing and debugging are like two sides of a coin—you subject your code to\ntests in order to find any bugs that might be there, and you debug it to remove\nthose bugs. With any luck, you only have to remove the bugs found by your own\ntests rather than bugs found by the end users of your application. Before we look at\neither testing or debugging, it’s important to understand the problems that might\narise, so let’s look at those.\nThis chapter covers\nConcurrency-related bugs\nLocating bugs through testing and code review\nDesigning multithreaded tests\nTesting the performance of multithreaded code\n",
      "content_length": 1231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "340\nCHAPTER 11\nTesting and debugging multithreaded applications\n11.1\nTypes of concurrency-related bugs\nYou can get any sort of bug in concurrent code; it’s not special in that regard. But\nsome types of bugs are directly related to the use of concurrency and therefore of par-\nticular relevance to this book. Typically, these concurrency-related bugs fall into two\ncategories:\nUnwanted blocking\nRace conditions\nThese are broad categories, so let’s divide them up a bit. First, let’s look at unwanted\nblocking.\n11.1.1 Unwanted blocking\nWhat do I mean by unwanted blocking? A thread is blocked when it’s unable to proceed\nbecause it’s waiting for something. This is typically something like a mutex, a condi-\ntion variable, or a future, but it could be waiting for I/O. This is a natural part of\nmultithreaded code, but it’s not always desirable—hence the problem of unwanted\nblocking. This leads us to the next question: why is this blocking unwanted? Typi-\ncally, this is because some other thread is also waiting for the blocked thread to per-\nform some action, and so that thread in turn is blocked. There are several variations\non this theme:\nDeadlock—As you saw in chapter 3, in the case of deadlock, one thread is waiting\nfor another, which is in turn waiting for the first. If your threads deadlock, the\ntasks they’re supposed to be doing won’t get done. In the most visible cases, one\nof the threads involved is the thread responsible for the user interface, in which\ncase the interface will cease to respond. In other cases, the interface will remain\nresponsive, but some required tasks won’t complete, such as a search not\nreturning or a document not printing.\nLivelock—Livelock is similar to deadlock in that one thread is waiting for\nanother, which is in turn waiting for the first. The key difference here is that the\nwait is not a blocking wait but an active checking loop, such as a spin lock. In\nserious cases, the symptoms are the same as deadlock (the app doesn’t make\nany progress), except that the CPU usage is high because threads are still run-\nning but blocking each other. In not-so-serious cases, the livelock will eventually\nresolve because of the random scheduling, but there will be a long delay in the\ntask that got livelocked, with a high CPU usage during that delay.\nBlocking on I/O or other external input—If your thread is blocked waiting for exter-\nnal input, it can’t proceed, even if the waited-for input is never going to come.\nIt’s therefore undesirable to block on external input from a thread that also\nperforms tasks that other threads may be waiting for.\nThat briefly covers unwanted blocking. What about race conditions?\n",
      "content_length": 2667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "341\nTypes of concurrency-related bugs\n11.1.2 Race conditions\nRace conditions are the most common cause of problems in multithreaded code—\nmany deadlocks and livelocks only manifest because of a race condition. Not all race\nconditions are problematic—a race condition occurs any time the behavior depends\non the relative scheduling of operations in separate threads. A large number of race\nconditions are entirely benign; for example, which worker thread processes the next\ntask in the task queue is largely irrelevant. But many concurrency bugs are due to race\nconditions. In particular, race conditions often cause the following types of problems:\nData races—A data race is the specific type of race condition that results in\nundefined behavior because of unsynchronized concurrent access to a shared\nmemory location. I introduced data races in chapter 5 when we looked at the\nC++ memory model. Data races usually occur through incorrect usage of atomic\noperations to synchronize threads or through access to shared data without\nlocking the appropriate mutex.\nBroken invariants—These can manifest as dangling pointers (because another\nthread deleted the data being accessed), random memory corruption (due to a\nthread reading inconsistent values resulting from partial updates), and double-\nfree (such as when two threads pop the same value from a queue, and so both\ndelete some associated data), among others. The invariants being broken can\nbe temporal- as well as value-based. If operations on separate threads are required\nto execute in a particular order, incorrect synchronization can lead to a race\ncondition in which the required order is sometimes violated.\nLifetime issues—Although you could bundle these problems in with broken\ninvariants, this is a separate category. The basic problem with bugs in this cate-\ngory is that the thread outlives the data that it accesses, so it is accessing data\nthat has been deleted or otherwise destroyed, and potentially the storage is\neven reused for another object. You typically get lifetime issues where a thread\nreferences local variables that go out of scope before the thread function has\ncompleted, but they aren’t limited to that scenario. Whenever the lifetime of\nthe thread and the data it operates on aren’t tied together in some way, there’s\nthe potential for the data to be destroyed before the thread has finished and for\nthe thread function to have the rug pulled out from under its feet. If you manu-\nally call join() in order to wait for the thread to complete, you need to ensure\nthat the call to join() can’t be skipped if an exception is thrown. This is basic\nexception safety applied to threads.\nIt’s the problematic race conditions that are the killers. With deadlock and livelock,\nthe application appears to hang and become completely unresponsive or takes too\nlong to complete a task. Often, you can attach a debugger to the running process to\nidentify which threads are involved in the deadlock or livelock and which synchroniza-\ntion objects they’re fighting over. With data races, broken invariants, and lifetime\nissues, the visible symptoms of the problem (such as random crashes or incorrect\n",
      "content_length": 3173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "342\nCHAPTER 11\nTesting and debugging multithreaded applications\noutput) can manifest anywhere in the code—the code may overwrite memory used by\nanother part of the system that isn’t touched until much later. The fault will then man-\nifest in code completely unrelated to the location of the buggy code, possibly much\nlater in the execution of the program. This is the true curse of shared memory sys-\ntems—however much you try to limit which data is accessible by which thread, and try\nto ensure that correct synchronization is used, any thread can overwrite the data\nbeing used by any other thread in the application.\n Now that we’ve briefly identified the sorts of problems we’re looking for, let’s look\nat what you can do to locate any instances in your code so you can fix them.\n11.2\nTechniques for locating concurrency-related bugs\nIn the previous section we looked at the types of concurrency-related bugs you might\nsee and how they might manifest in your code. With that information in mind, you\ncan then look at your code to see where bugs might lie and how you can attempt to\ndetermine whether there are any bugs in a particular section.\n Perhaps the most obvious and straightforward thing to do is look at the code.\nAlthough this might seem obvious, it’s difficult to do in a thorough way. When you\nread code you’ve written, it’s all too easy to read what you intended to write rather\nthan what’s there. Likewise, when reviewing code that others have written, it’s tempt-\ning to give it a quick read-through, check it off against your local coding standards,\nand highlight any glaringly obvious problems. What’s needed is to spend the time\ngoing through the code with a fine-tooth comb, thinking about the concurrency\nissues—and the nonconcurrency issues as well. (You might as well, while you’re doing\nit. After all, a bug is a bug.) We’ll cover specific things to think about when reviewing\ncode shortly.\n Even after thoroughly reviewing your code, you still might have missed some bugs,\nand in any case, you need to confirm that it does work, for peace of mind if nothing\nelse. Consequently, we’ll continue on from reviewing the code to a few techniques to\nemploy when testing multithreaded code.\n11.2.1 Reviewing code to locate potential bugs\nAs I’ve already mentioned, when reviewing multithreaded code to check for concurrency-\nrelated bugs, it’s important to review it thoroughly. If possible, get someone else to\nreview it. Because they haven’t written the code, they’ll have to think through how it\nworks, and this will help to uncover any bugs that may be there. It’s important that the\nreviewer have the time to do the review properly—not a casual two-minute quick\nglance, but a proper, considered review. Most concurrency bugs require more than a\nquick glance to spot—they usually rely on subtle timing issues to manifest.\n If you get one of your colleagues to review the code, they’ll be coming at it fresh.\nThey’ll therefore see things from a different point of view and may spot things that\nyou can’t. If you don’t have colleagues you can ask, ask a friend, or even post the\ncode on the internet (taking care not to upset your company lawyers). If you can’t\n",
      "content_length": 3183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "343\nTechniques for locating concurrency-related bugs\nget anybody to review your code for you, or they don’t find anything, don’t worry—\nthere’s still more you can do. For starters, it might be worth leaving the code alone\nfor a while—work on another part of the application, read a book, or go for a walk.\nIf you take a break, your subconscious can work on the problem in the background\nwhile you’re consciously focused on something else. Also, the code will be less famil-\niar when you come back to it—you might manage to look at it from a different per-\nspective yourself.\n An alternative to getting someone else to review your code is to do it yourself. One\nuseful technique is to try to explain how it works in detail to someone else. They don’t\neven have to be physically there—many teams have a bear or rubber chicken for this\npurpose, and I personally find that writing detailed notes can be hugely beneficial. As\nyou explain, think about each line, what could happen, which data it accesses, and so\nforth. Ask yourself questions about the code, and explain the answers. I find this to be\nan incredibly powerful technique—by asking myself these questions and thinking\ncarefully about the answers, the problem often reveals itself. These questions can be\nhelpful for any code review, not just when reviewing your own code.\nQUESTIONS TO THINK ABOUT WHEN REVIEWING MULTITHREADED CODE\nAs I’ve already mentioned, it can be useful for a reviewer (whether the code’s author\nor someone else) to think about specific questions relating to the code being\nreviewed. These questions can focus the reviewer’s mind on the relevant details of the\ncode and can help identify potential problems. The questions I like to ask include the\nfollowing, though this is most definitely not an exhaustive list. You might find other\nquestions that help you to focus better. Here are my questions:\nWhich data needs to be protected from concurrent access?\nHow do you ensure that the data is protected?\nWhere in the code could other threads be at this time?\nWhich mutexes does this thread hold?\nWhich mutexes might other threads hold?\nAre there any ordering requirements between the operations done in this\nthread and those done in another? How are those requirements enforced?\nIs the data loaded by this thread still valid? Could it have been modified by\nother threads?\nIf you assume that another thread could be modifying the data, what would that\nmean and how could you ensure that this never happens?\nThis last question is my favorite, because it makes me think about the relationships\nbetween the threads. By assuming the existence of a bug related to a particular line of\ncode, you can then act as a detective and track down the cause. In order to convince\nyourself that there’s no bug, you have to consider every corner case and possible\nordering. This is particularly useful where the data is protected by more than one\nmutex over its lifetime, such as with the thread-safe queue from chapter 6 where you\nhad separate mutexes for the head and tail of the queue: in order to be sure that an\n",
      "content_length": 3083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "344\nCHAPTER 11\nTesting and debugging multithreaded applications\naccess is safe while holding one mutex, you have to be certain that a thread holding\nthe other mutex can’t also access the same element. It also makes it obvious that public\ndata, or data for which other code can readily obtain a pointer or reference, has to\ncome under particular scrutiny.\n The penultimate question in the list is also important, because it addresses a mis-\ntake that is easy to make: if you release and then reacquire a mutex, you must assume\nthat other threads may have modified the shared data. Although this is obvious, if the\nmutex locks aren’t immediately visible—perhaps because they’re internal to an\nobject—you may unwittingly be doing exactly that. In chapter 6 you saw how this can\nlead to race conditions and bugs where the functions provided on a thread-safe data\nstructure are too fine-grained. Whereas for a non-thread-safe stack it makes sense to\nhave separate top() and pop() operations, for a stack that may be accessed by multi-\nple threads concurrently, this is no longer the case because the lock on the internal\nmutex is released between the two calls, and so another thread can modify the stack.\nAs you saw in chapter 6, the solution is to combine the two operations so they are both\nperformed under the protection of the same mutex lock, eliminating the potential\nrace condition.\n OK, so you’ve reviewed your code (or got someone else to review it). You’re sure\nthere are no bugs. The proof of the pudding is, as they say, in the eating—how can\nyou test your code to confirm or disprove your belief in its lack of bugs?\n11.2.2 Locating concurrency-related bugs by testing\nWhen developing single-threaded applications, testing your applications is relatively\nstraightforward, if time-consuming. You could, in principle, identify all the possible\nsets of input data (or at least all the interesting cases) and run them through the\napplication. If the application produced the correct behavior and output, you’d know\nit works for that given set of input. Testing for error states such as the handling of disk-\nfull errors is more complicated than that, but the idea is the same: set up the initial\nconditions and allow the application to run.\n Testing multithreaded code is an order of magnitude harder, because the precise\nscheduling of the threads is indeterminate and may vary from run to run. Conse-\nquently, even if you run the application with the same input data, it might work cor-\nrectly some of the time, and fail at other times if there’s a race condition lurking in\nthe code. Having a potential race condition doesn’t mean the code will fail always, just\nthat it might fail sometimes. \n Given the inherent difficulty of reproducing concurrency-related bugs, it pays to\ndesign your tests carefully. You want each test to run the smallest amount of code that\ncould potentially demonstrate a problem, so that you can best isolate the code that’s\nfaulty if the test fails—it’s better to test a concurrent queue directly to verify that con-\ncurrent pushes and pops work rather than testing it through a whole chunk of code\nthat uses the queue. It can help if you think about how code should be tested when\ndesigning it—see the section on designing for testability later in this chapter.\n",
      "content_length": 3297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "345\nTechniques for locating concurrency-related bugs\n It’s also worth eliminating the concurrency from the test in order to verify that the\nproblem is concurrency-related. If you have a problem when everything is running in\na single thread, it’s a common, or garden-variety, bug rather than a concurrency-\nrelated bug. This is particularly important when trying to track down a bug that occurs\n“in the wild” as opposed to being detected in your test harness. Just because a bug\noccurs in the multithreaded portion of your application doesn’t mean it’s automati-\ncally concurrency-related. If you’re using thread pools to manage the level of concur-\nrency, there’s usually a configuration parameter you can set to specify the number of\nworker threads. If you’re managing threads manually, you’ll have to modify the code\nto use a single thread for the test. Either way, if you can reduce your application to a\nsingle thread, you can eliminate concurrency as a cause. On the flip side, if the prob-\nlem goes away on a single-core system (even with multiple threads running) but is pres-\nent on multicore systems or multiprocessor systems, you have a race condition and\npossibly a synchronization or memory-ordering issue.\n There’s more to testing concurrent code than the structure of the code being\ntested; the structure of the test is just as important, as is the test environment. If you\ncontinue on with the example of testing a concurrent queue, you have to think about\nvarious scenarios:\nOne thread calling push() or pop() on its own to verify that the queue works at\na basic level\nOne thread calling push() on an empty queue while another thread calls pop()\nMultiple threads calling push() on an empty queue\nMultiple threads calling push() on a full queue\nMultiple threads calling pop() on an empty queue\nMultiple threads calling pop() on a full queue\nMultiple threads calling pop() on a partially full queue with insufficient items\nfor all threads\nMultiple threads calling push() while one thread calls pop() on an empty queue\nMultiple threads calling push() while one thread calls pop() on a full queue\nMultiple threads calling push() while multiple threads call pop() on an empty\nqueue\nMultiple threads calling push() while multiple threads call pop() on a full queue\nHaving thought about all these scenarios and more, you then need to consider addi-\ntional factors about the test environment:\nWhat you mean by “multiple threads” in each case (3, 4, 1,024?)\nWhether there are enough processing cores in the system for each thread to\nrun on its own core\nWhich processor architectures the tests should be run on\nHow you ensure suitable scheduling for the “while” parts of your tests\n",
      "content_length": 2703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "346\nCHAPTER 11\nTesting and debugging multithreaded applications\nThere are additional factors to think about specific to your particular situation. Of\nthese four environmental considerations, the first and last affect the structure of the\ntest itself (and are covered in section 11.2.5), whereas the other two are related to the\nphysical test system being used. The number of threads to use relates to the particular\ncode being tested, but there are various ways of structuring tests to obtain suitable\nscheduling. Before we look at these techniques, let’s look at how you can design your\napplication code to be easier to test.\n11.2.3 Designing for testability\nTesting multithreaded code is difficult, so you want to do what you can to make it eas-\nier. One of the most important things you can do is design the code for testability. A lot\nhas been written about designing single-threaded code for testability, and much of the\nadvice still applies. In general, code is easier to test if the following factors apply:\nThe responsibilities of each function and class are clear\nThe functions are short and to the point\nYour tests can take complete control of the environment surrounding the code\nbeing tested\nThe code that performs the particular operation being tested is close together\nrather than spread throughout the system\nYou thought about how to test the code before you wrote it\nAll of these are still true for multithreaded code. In fact, I’d argue that it’s even more\nimportant to pay attention to the testability of multithreaded code than for single-\nthreaded code, because it’s inherently that much harder to test. That last point is\nimportant: even if you don’t go as far as writing your tests before the code, it’s well\nworth thinking about how you can test the code before you write it—what inputs to\nuse, which conditions are likely to be problematic, how to stimulate the code in poten-\ntially problematic ways, and so on.\n One of the best ways to design concurrent code for testing is to eliminate the con-\ncurrency. If you can break down the code into those parts that are responsible for the\ncommunication paths between threads and those parts that operate on the communi-\ncated data within a single thread, then you’ve greatly reduced the problem. Those\nparts of the application that operate on data that’s being accessed by only that one\nthread can then be tested using the normal single-threaded techniques. The hard-to-\ntest concurrent code that deals with communicating between threads and ensuring\nthat only one thread at a time is accessing a particular block of data is now much\nsmaller and the testing more tractable.\n For example, if your application is designed as a multithreaded state machine, you\ncould split it into several parts. The state logic for each thread, which ensures that the\ntransitions and operations are correct for each possible set of input events, can be\ntested independently with single-threaded techniques, with the test harness providing\nthe input events that would be coming from other threads. Then, the core state\n",
      "content_length": 3074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "347\nTechniques for locating concurrency-related bugs\nmachine and message routing code that ensures that events are correctly delivered to\nthe right thread in the right order can be tested independently, but with multiple con-\ncurrent threads and simple state logic designed specifically for the tests.\n Alternatively, if you can divide your code into multiple blocks of read shared\ndata/transform data/update shared data, you can test the transform data portions using\nall the usual single-threaded techniques, because this is now single-threaded code.\nThe hard problem of testing a multithreaded transformation will be reduced to test-\ning the reading and updating of the shared data, which is much simpler.\n One thing to watch out for is that library calls can use internal variables to store state,\nwhich then becomes shared if multiple threads use the same set of library calls. This can\nbe a problem because it’s not immediately apparent that the code accesses shared data.\nBut with time you learn which library calls these are, and they stick out like sore thumbs.\nYou can then either add appropriate protection and synchronization or use an alternate\nfunction that’s safe for concurrent access from multiple threads.\n There’s more to designing multithreaded code for testability than structuring your\ncode to minimize the amount of code that needs to deal with concurrency-related\nissues and paying attention to the use of non-thread-safe library calls. It’s also helpful\nto bear in mind the same set of questions you ask yourself when reviewing the code,\nfrom section 11.2.1. Although these questions aren’t directly about testing and test-\nability, if you think about the issues with your “testing hat” on and consider how to test\nthe code, it will affect which design choices you make and will make testing easier.\n Now that we’ve looked at designing code to make testing easier, and potentially\nmodified the code to separate the “concurrent” parts (such as the thread-safe contain-\ners or state machine event logic) from the “single-threaded” parts (which may still\ninteract with other threads through the concurrent chunks), let’s look at the tech-\nniques for testing concurrency-aware code.\n11.2.4 Multithreaded testing techniques\nSo, you’ve thought through the scenario you want to test and written a small amount\nof code that exercises the functions being tested. How do you ensure that any poten-\ntially problematic scheduling sequences are exercised in order to flush out the bugs?\n Well, there are a few ways of approaching this, starting with brute-force testing, or\nstress testing.\nBRUTE-FORCE TESTING\nThe idea behind brute-force testing is to stress the code to see if it breaks. This typi-\ncally means running the code many times, possibly with many threads running at\nonce. If there’s a bug that manifests only when the threads are scheduled in a particu-\nlar fashion, then the more times the code is run, the more likely the bug is to appear.\nIf you run the test once and it passes, you might feel a bit of confidence that the code\nworks. If you run it ten times in a row and it passes every time, you’ll likely feel more\nconfident. If you run the test a billion times and it passes every time, you’ll feel more\nconfident still.\n",
      "content_length": 3253,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "348\nCHAPTER 11\nTesting and debugging multithreaded applications\n The confidence you have in the results does depend on the amount of code being\ntested by each test. If your tests are quite fine-grained, like the tests outlined previ-\nously for a thread-safe queue, this brute-force testing can give you a high degree of\nconfidence in your code. On the other hand, if the code being tested is considerably\nlarger, the number of possible scheduling permutations is so vast that even a billion\ntest runs might yield a low level of confidence.\n The downside to brute-force testing is that it might give you false confidence. If the\nway you’ve written the test means that the problematic circumstances can’t occur, you\ncan run the test as many times as you like and it won’t fail, even if it would fail every\ntime in slightly different circumstances. The worst example is where the problematic\ncircumstances can’t occur on your test system because of the way the particular system\nyou’re testing on happens to run. Unless your code is to run only on systems identical\nto the one being tested, the particular hardware and operating system combination\nmay not allow the circumstances that would cause a problem to arise.\n The classic example here is testing a multithreaded application on a single-processor\nsystem. Because every thread has to run on the same processor, everything is auto-\nmatically serialized, and many race conditions and cache ping-pong problems that\nyou may get with a true multiprocessor system evaporate. This isn’t the only variable,\nthough; different processor architectures provide different synchronization and\nordering facilities. For example, on x86 and x86-64 architectures, atomic load opera-\ntions are always the same, whether tagged memory_order_relaxed or memory_order\n_seq_cst (see section 5.3.3). This means that code written using relaxed memory\nordering may work on systems with an x86 architecture, where it would fail on a sys-\ntem with a finer-grained set of memory-ordering instructions, such as SPARC.\n If you need your application to be portable across a range of target systems, it’s\nimportant to test it on representative instances of those systems. This is why I listed the\nprocessor architectures being used for testing as a consideration in section 11.2.2.\n Avoiding the potential for false confidence is crucial to successful brute-force test-\ning. This requires careful thought over test design, not just with respect to the choice\nof unit for the code being tested but also with respect to the design of the test harness\nand the choice of testing environment. You need to ensure that you test as many of\nthe code paths and the possible thread interactions as feasible. Not only that, but you\nneed to know which options are covered and which are left untested.\n Although brute-force testing does give you some degree of confidence in your\ncode, it’s not guaranteed to find all the problems. There’s one technique that is guar-\nanteed to find the problems, if you have the time to apply it to your code and the\nappropriate software. I call it combination simulation testing.\nCOMBINATION SIMULATION TESTING\nThat’s a bit of a mouthful, so I’ll explain what I mean. The idea is that you run your\ncode with a special piece of software that simulates the real runtime environment of\nthe code. You may be aware of software that allows you to run multiple virtual machines\non a single physical computer, where the characteristics of the virtual machine and its\n",
      "content_length": 3498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "349\nTechniques for locating concurrency-related bugs\nhardware are emulated by the supervisor software. The idea here is similar, except\nrather than emulating the system, the simulation software records the sequences of\ndata accesses, locks, and atomic operations from each thread. It then uses the rules of\nthe C++ memory model to repeat the run with every permitted combination of opera-\ntions and identify race conditions and deadlocks.\n Although this exhaustive combination testing is guaranteed to find all the prob-\nlems the system is designed to detect, for anything but the most trivial of programs it\nwill take a huge amount of time, because the number of combinations increases\nexponentially with the number of threads and the number of operations performed\nby each thread. This technique is best reserved for fine-grained tests of individual\npieces of code rather than an entire application. The other obvious downside is that\nit relies on the availability of simulation software that can handle the operations used\nin your code.\n So, you have a technique that involves running your test many times under normal\nconditions but that might miss problems, and you have a technique that involves run-\nning your test many times under special conditions but that’s more likely to find any\nproblems that exist. Are there any other options?\n A third option is to use a library that detects problems as they occur in the running\nof the tests.\nDETECTING PROBLEMS EXPOSED BY TESTS WITH A SPECIAL LIBRARY\nAlthough this option doesn’t provide the exhaustive checking of a combination simu-\nlation test, you can identify many problems by using a special implementation of the\nlibrary synchronization primitives such as mutexes, locks, and condition variables. For\nexample, it’s common to require that all accesses to a piece of shared data be done\nwith a particular mutex locked. If you could check which mutexes were locked when\nthe data was accessed, you could verify that the appropriate mutex was indeed locked\nby the calling thread when the data was accessed and report a failure if this was not\nthe case. By marking your shared data in some way, you can allow the library to check\nthis for you.\n This library implementation can also record the sequence of locks if more than\none mutex is held by a particular thread at once. If another thread locks the same\nmutexes in a different order, this could be recorded as a potential deadlock even if the\ntest didn’t deadlock while running.\n Another type of special library that could be used when testing multithreaded\ncode is one where the implementations of the threading primitives such as mutexes\nand condition variables give the test writer control over which thread gets the lock\nwhen multiple threads are waiting or which thread is notified by a notify_one() call\non a condition variable. This would allow you to set up particular scenarios and verify\nthat your code works as expected in those scenarios.\n Some of these testing facilities would have to be supplied as part of the C++ Stan-\ndard Library implementation, whereas others can be built on top of the Standard\nLibrary as part of your test harness.\n",
      "content_length": 3160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "350\nCHAPTER 11\nTesting and debugging multithreaded applications\n Having looked at various ways of executing test code, let’s now look at ways of\nstructuring the code to achieve the scheduling you want.\n11.2.5 Structuring multithreaded test code\nBack in section 11.2.2, I said that you need to find ways of providing suitable scheduling\nfor the “while” part of your tests. Now it’s time to look at the issues involved in that.\n The basic issue is that you need to arrange for a set of threads to each be executing\na chosen piece of code at a time that you specify. In the most basic case you have two\nthreads, but this could easily be extended to more. In the first step, you need to iden-\ntify the distinct parts of each test:\nThe general setup code that must be executed before anything else\nThe thread-specific setup code that must run on each thread\nThe code for each thread that you want to run concurrently\nThe code to be run after the concurrent execution has finished, possibly\nincluding assertions on the state of the code\nTo explain further, let’s consider a specific example from the test list in section 11.2.2:\none thread calling push() on an empty queue while another thread calls pop().\n The general setup code is simple: you must create the queue. The thread executing\npop() has no thread-specific setup code. The thread-specific setup code for the thread\nexecuting push() depends on the interface to the queue and the type of object being\nstored. If the object being stored is expensive to construct or must be heap-allocated,\nyou want to do this as part of the thread-specific setup, so that it doesn’t affect the test.\nOn the other hand, if the queue is just storing plain ints, there’s nothing to be gained\nby constructing an int in the setup code. The code being tested is relatively straight-\nforward—a call to push() from one thread and a call to pop() from another—but\nwhat about the “after completion” code?\n In this case, it depends on what you want pop() to do. If it’s supposed to block\nuntil there is data, then clearly you want to see that the returned data is what was sup-\nplied to the push() call and that the queue is empty afterward. If pop() is not blocking\nand may complete even when the queue is empty, you need to test for two possibilities:\neither the pop() returned the data item supplied to the push() and the queue is\nempty or the pop() signaled that there was no data and the queue has one element.\nOne or the other must be true; what you want to avoid is the scenario that pop() sig-\nnaled “no data” but the queue is empty, or that pop() returned the value and the\nqueue is still not empty. In order to simplify the test, assume you have a blocking\npop(). The final code is therefore an assertion that the popped value is the pushed\nvalue and that the queue is empty.\n Now, having identified the various chunks of code, you need to do the best you can\nto ensure that everything runs as planned. One way to do this is to use a set of\nstd::promises to indicate when everything is ready. Each thread sets a promise to\nindicate that it’s ready and then waits on a (copy of a) std::shared_future obtained\n",
      "content_length": 3148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "351\nTechniques for locating concurrency-related bugs\nfrom a third std::promise; the main thread waits for all the promises from all the\nthreads to be set and then triggers the threads to go. This ensures that each thread has\nstarted and comes before the chunk of code that should be run concurrently; any\nthread-specific setup should be done before setting that thread’s promise. Finally, the\nmain thread waits for the threads to complete and checks the final state. You also need\nto be aware of exceptions and make sure you don’t have any threads left waiting for\nthe go signal when that’s not going to happen. The following listing shows one way of\nstructuring this test.\nvoid test_concurrent_push_and_pop_on_empty_queue()\n{\n    threadsafe_queue<int> q;                     \n    std::promise<void> go,push_ready,pop_ready;    \n    std::shared_future<void> ready(go.get_future());   \n    std::future<void> push_done;     \n    std::future<int> pop_done;\n    try\n    {\n        push_done=std::async(std::launch::async,     \n                             [&q,ready,&push_ready]()\n                             {\n                                 push_ready.set_value();\n                                 ready.wait();\n                                 q.push(42);\n                             }\n            );\n        pop_done=std::async(std::launch::async,    \n                            [&q,ready,&pop_ready]()\n                            {\n                                pop_ready.set_value();\n                                ready.wait();\n                                return q.pop();    \n                            }\n            );\n        push_ready.get_future().wait();   \n        pop_ready.get_future().wait();\n        go.set_value();      \n        push_done.get();           \n        assert(pop_done.get()==42);    \n        assert(q.empty());\n    }\n    catch(...)\n    {\n        go.set_value();   \n        throw;\n    }\n}\nThe structure is pretty much as described previously. First, you create your empty\nqueue as part of the general setup B. Then, you create all your promises for the\nListing 11.1\nAn example test for concurrent push() and pop() calls on a queue\nB\nc\nd\ne\nf\ng\nh\ni\nj\n1)\n1!\n1@\n",
      "content_length": 2195,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "352\nCHAPTER 11\nTesting and debugging multithreaded applications\n“ready” signals c and get std::shared_future for the go signal d. Then, you create\nthe futures you’ll use to indicate that the threads have finished e. These have to go\noutside the try block so that you can set the go signal on an exception without waiting\nfor the test threads to complete (which would deadlock—a deadlock in the test code\nwould be less than ideal).\n Inside the try block you can then start the threads, f and g—you use std::\nlaunch::async to guarantee that the tasks are each running on their own thread.\nNote that the use of std::async makes your exception-safety task easier than it would\nbe with plain std::thread because the destructor for the future will join with the\nthread. The lambda captures specify that each task will reference the queue and the\nrelevant promise for signaling readiness, while taking a copy of the ready future you\ngot from the go promise.\n As described previously, each task sets its own ready signal and then waits for the\ngeneral ready signal before running the test code. The main thread does the\nreverse—it waits for the signals from both threads i before signaling them to start\nthe real test j.\n Finally, the main thread calls get() on the futures from the async calls to wait for\nthe tasks to finish, 1) and 1!, and checks the results. Note that the pop task returns the\nretrieved value through the future h, so you can use that to get the result for the\nassert 1!.\n If an exception is thrown, you set the go signal to avoid any chance of a dangling\nthread and rethrow the exception 1@. The futures corresponding to the tasks e were\ndeclared last, so they’ll be destroyed first, and their destructors will wait for the tasks\nto complete, if they haven’t already.\n Although this seems like quite a lot of boilerplate to test two simple calls, it’s neces-\nsary to use something similar in order to have the best chance of testing what you want\nto test. For example, starting a thread can be quite a time-consuming process, so if you\ndidn’t make the threads wait for the go signal, then the push thread may have com-\npleted before the pop thread even started, which would completely defeat the point\nof the test. Using the futures in this way ensures that both threads are running and\nblocked on the same future. Unblocking the future then allows both threads to run.\nOnce you’re familiar with the structure, it should be relatively straightforward to cre-\nate new tests in the same pattern. For tests that require more than two threads, this\npattern is readily extended to additional threads.\n So far, we’ve been looking at the correctness of multithreaded code. Although this is\nthe most important issue, it’s not the only reason you test: it’s also important to test\nthe performance of multithreaded code, so let’s look at that next.\n11.2.6 Testing the performance of multithreaded code\nOne of the main reasons you might choose to use concurrency in an application is\nto make use of the increasing prevalence of multicore processors to improve the\nperformance of your applications. It’s therefore important to test your code to\n",
      "content_length": 3147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "353\nSummary\nconfirm that the performance does indeed improve, as you’d do with any other\nattempt at optimization.\n The particular issue with using concurrency for performance is the scalability—you\nwant code that runs approximately 24 times faster or processes 24 times as much data\non a 24-core machine as on a single-core machine, all else being equal. You don’t want\ncode that runs twice as fast on a dual-core machine but is slower on a 24-core\nmachine. As you saw in section 8.4.2, if a significant section of your code runs on only\none thread, this can limit the potential performance gain. It’s therefore worth looking\nat the overall design of the code before you start testing, so you know whether you’re\nhoping for a factor-of-24 improvement, or whether the serial portion of your code\nmeans you’re limited to a maximum of a factor of 3.\n As you’ve already seen in previous chapters, contention between processors for access\nto a data structure can have a big performance impact. Something that scales nicely with\nthe number of processors when that number is small may perform badly when the num-\nber of processors is much larger because of the huge increase in contention.\n Consequently, when testing for the performance of multithreaded code, it’s best to\ncheck the performance on systems with as many different configurations as possible, so\nyou get a picture of the scalability graph. At the very least, you ought to test on a single-\nprocessor system and a system with as many processing cores as are available to you.\nSummary\nIn this chapter, we looked at the various types of concurrency-related bugs that you\nmight encounter, from deadlocks and livelocks to data races and other problematic race\nconditions. We followed that with techniques for locating bugs. These included issues to\nthink about during code reviews, guidelines for writing testable code, and how to struc-\nture tests for concurrent code. Finally, we looked at some utility components that can\nhelp with testing.\n",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "354\nappendix A\nBrief reference for some\nC++11 language features\nThe new C++ Standard brings more than just concurrency support; there are a host\nof other language features and new libraries as well. In this appendix I give a brief\noverview of the new language features that are used in the Thread Library and the\nrest of the book. Aside from thread_local (which is covered in section A.8), none\nof them are directly related to concurrency, though they are important and/or use-\nful for multithreaded code. I’ve limited this list to those that are either necessary\n(such as rvalue references) or serve to make the code simpler or easier to under-\nstand. Code that uses these features may be difficult to understand at first because of\nlack of familiarity, but as you become familiar with them, they should generally make\ncode easier to understand. As the use of C++11 becomes more widespread, code mak-\ning use of these features will become more common.\n Without further ado, let’s start by looking at rvalue references, which are used\nextensively by the Thread Library to facilitate the transfer of ownership (of threads,\nlocks, or whatever) between objects.\nA.1\nRvalue references\nIf you’ve been doing C++ programming for any time, you’ll be familiar with refer-\nences; C++ references allow you to create a new name for an existing object. All\naccesses and modifications done through the new reference affect the original; for\nexample:\nint var=42;\nint& ref=var;   \nref=99;\nassert(var==99);       \nCreate a reference \nto var.\nOriginal updated because \nof assignment to reference\n",
      "content_length": 1578,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "355\nRvalue references\nThe only references that existed prior to C++11 are lvalue references—references to lval-\nues. The term lvalue comes from C and refers to things that can be on the left side of\nan assignment expression, named objects, objects allocated on the stack or heap, or\nmembers of other objects, all things with a defined storage location. The term rvalue\nalso comes from C and refers to things that can occur only on the right side of an\nassignment expression—literals and temporaries, for example. Lvalue references can\nonly be bound to lvalues, not rvalues. You can’t write\nint& i=42;    \nfor example, because 42 is an rvalue. OK, that’s not quite true; you’ve always been able\nto bind an rvalue to a const lvalue reference:\nint const& i=42;\nBut this is a deliberate exception on the part of the standard, introduced before we\nhad rvalue references in order to allow you to pass temporaries to functions taking ref-\nerences. This allows implicit conversions, so you can write things like this:\nvoid print(std::string const& s);\nprint(\"hello\");                  \nThe C++11 Standard introduced rvalue references, which bind only to rvalues, not to lval-\nues, and are declared with two ampersands rather than one:\nint&& i=42;\nint j=42;\nint&& k=j;      \nYou can use function overloading to determine whether function parameters are lval-\nues or rvalues by having one overload take an lvalue reference and another take an\nrvalue reference. This is the cornerstone of move semantics.\nA.1.1\nMove semantics\nRvalues are typically temporary and so can be freely modified; if you know that your\nfunction parameter is an rvalue, you can use it as temporary storage, or “steal” its con-\ntents without affecting program correctness. This means that rather than copying the\ncontents of an rvalue parameter, you can move the contents. For large dynamic struc-\ntures, this saves a lot of memory allocation and provides a lot of scope for optimiza-\ntion. Consider a function that takes an std::vector<int> as a parameter and needs\nto have an internal copy for modification, without touching the original. The old way\nof doing this would be to take the parameter as a const lvalue reference and make the\ncopy internally:\nvoid process_copy(std::vector<int> const& vec_)\n{\nWon’t compile\nCreate temporary \nstd::string object\nWon’t \ncompile\n",
      "content_length": 2334,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "356\nAPPENDIX A\nBrief reference for some C++11 language features\n     std::vector<int> vec(vec_);\n     vec.push_back(42);\n}\nThis allows the function to take both lvalues and rvalues but forces the copy in every\ncase. If you overload the function with a version that takes an rvalue reference, you\ncan avoid the copy in the rvalue case, because you know you can freely modify the\noriginal:\nvoid process_copy(std::vector<int> && vec)\n{\n     vec.push_back(42);\n}\nNow, if the function in question is the constructor of your class, you can pilfer the\ninnards of the rvalue and use them for your new instance. Consider the class in the\nfollowing listing. In the default constructor it allocates a large chunk of memory,\nwhich is freed in the destructor.\nclass X\n{\nprivate:\n    int* data;\npublic:\n    X():\n        data(new int[1000000])\n    {}\n    ~X()\n    {\n        delete [] data;\n    }\n    X(const X& other):     \n        data(new int[1000000])\n    {\n        std::copy(other.data,other.data+1000000,data);\n    }\n    X(X&& other):         \n        data(other.data)\n    {\n        other.data=nullptr;\n    }\n};\nThe copy constructor B is defined as you might expect: allocate a new block of memory\nand copy the data across. But you also have a new constructor that takes the old value\nby rvalue reference c. This is the move constructor. In this case you copy the pointer to\nthe data and leave the other instance with a null pointer, saving yourself a huge chunk\nof memory and time when creating variables from rvalues.\nListing A.1\nA class with a move constructor\nB\nc\n",
      "content_length": 1558,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "357\nRvalue references\n For class X the move constructor is an optimization, but in some cases it makes\nsense to provide a move constructor even when it doesn’t make sense to provide a\ncopy constructor. For example, the whole point of std::unique_ptr<> is that each\nnon-null instance is the one and only pointer to its object, so a copy constructor\nmakes no sense. But a move constructor allows ownership of the pointer to be trans-\nferred between instances and permits std::unique_ptr<> to be used as a function\nreturn value—the pointer is moved rather than copied.\n If you want to explicitly move from a named object that you know you’ll no longer\nuse, you can cast it to an rvalue either by using static_cast<X&&> or by calling\nstd::move():\nX x1;\nX x2=std::move(x1);\nX x3=static_cast<X&&>(x2);\nThis can be beneficial when you want to move the parameter value into a local or\nmember variable without copying, because although an rvalue reference parameter\ncan bind to rvalues, within the function it is treated as an lvalue:\nvoid do_stuff(X&& x_)\n{\n    X a(x_);            \n    X b(std::move(x_));    \n}\ndo_stuff(X());      \nX x;\ndo_stuff(x);     \nMove semantics are used extensively in the Thread Library, both where copies make\nno semantic sense but resources can be transferred, and as an optimization to avoid\nexpensive copies where the source is going to be destroyed anyway. You saw an example\nof this in section 2.2 where you used std::move() to transfer an std::unique_ptr<>\ninstance into a newly constructed thread, and then again in section 2.3 where we\nlooked at transferring the ownership of threads between std::thread instances. \n std::thread, std::unique_lock<>, std::future<>, std::promise<>, and std::\npackaged_task<> can’t be copied, but they all have move constructors to allow the\nassociated resource to be transferred between instances and support their use as func-\ntion return values. std::string and std::vector<> both can be copied as always, but\nthey also have move constructors and move-assignment operators to avoid copying\nlarge quantities of data from an rvalue.\n The C++ Standard Library never does anything with an object that has been explic-\nitly moved into another object, except destroy it or assign to it (either with a copy or,\nmore likely, a move). But it’s good practice to ensure that the invariant of the class\nencompasses the moved-from state. An std::thread instance that has been used as\nthe source of a move is equivalent to a default-constructed std::thread instance, for\nexample, and an instance of std::string that has been used as the source of a move\nCopies\nMoves\nOK; rvalue binds to \nrvalue reference\nError; lvalue can’t bind \nto rvalue reference\n",
      "content_length": 2700,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "358\nAPPENDIX A\nBrief reference for some C++11 language features\nwill still have a valid state, although no guarantees are made as to what that state is (in\nterms of how long the string is or what characters it contains).\nA.1.2\nRvalue references and function templates\nThere’s a final nuance when you use rvalue references for parameters to a function\ntemplate: if the function parameter is an rvalue reference to a template parameter,\nautomatic template argument type deduction deduces the type to be an lvalue refer-\nence if an lvalue is supplied or a plain unadorned type if an rvalue is supplied. That’s\na bit of a mouthful, so let’s look at an example. Consider the following function:\ntemplate<typename T>\nvoid foo(T&& t)\n{}\nIf you call it with an rvalue as follows, then T is deduced to be the type of the value:\nfoo(42);       \nfoo(3.14159);       \nfoo(std::string());    \nBut if you call foo with an lvalue, T is deduced to be an lvalue reference:\nint i=42;\nfoo(i);    \nBecause the function parameter is declared as T&&, this is therefore a reference to a ref-\nerence, which is treated as the original reference type. The signature of foo<int&>() is\nvoid foo<int&>(int& t);\nThis allows a single function template to accept both lvalue and rvalue parameters\nand is used by the std::thread constructor (sections 2.1 and 2.2) so that the supplied\ncallable object can be moved into internal storage rather than copied if the parameter\nis an rvalue.\nA.2\nDeleted functions\nSometimes it doesn’t make sense to allow a class to be copied. std::mutex is a prime\nexample of this—what would it mean if you did copy a mutex? std::unique_lock<> is\nanother—an instance is the one and only owner of the lock it holds. To truly copy it\nwould mean that the copy also held the lock, which doesn’t make sense. Moving own-\nership between instances, as described in section A.1.2, makes sense, but that’s not\ncopying. I’m sure you’ve seen other examples.\n The standard idiom for preventing copies of a class used to be declaring the copy\nconstructor and copy assignment operator private and then not providing an imple-\nmentation. This would cause a compile error if any code outside the class in question\nCalls foo<int>(42)\nCalls foo<double>(3.14159)\nCalls foo<std::string>(std::string())\nCalls foo<int&>(i)\n",
      "content_length": 2295,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "359\nDeleted functions\ntried to copy an instance and a link-time error (due to lack of an implementation) if\nany of the class’s member functions or friends tried to copy an instance: \nclass no_copies\n{\npublic:\n    no_copies(){}\nprivate:\n    no_copies(no_copies const&);               \n    no_copies& operator=(no_copies const&);    \n};\nno_copies a;\nno_copies b(a);    \nWith C++11, the committee realized that this was a common idiom but also realized\nthat it’s a bit of a hack. The committee therefore provided a more general mechanism\nthat can be applied in other cases too: you can declare a function as deleted by adding =\ndelete to the function declaration. no_copies can be written as\nclass no_copies\n{\npublic:\n    no_copies(){}\n    no_copies(no_copies const&) = delete;\n    no_copies& operator=(no_copies const&) = delete;\n};\nThis is much more descriptive than the original code and clearly expresses the intent.\nIt also allows the compiler to give more descriptive error messages and moves the\nerror from link time to compile time if you try to perform the copy within a member\nfunction of your class.\n If, in addition to deleting the copy constructor and copy-assignment operator, you\nalso explicitly write a move constructor and move-assignment operator, your class\nbecomes move-only, the same as std::thread and std::unique_lock<>. The follow-\ning listing shows an example of this move-only type.\nclass move_only\n{\n    std::unique_ptr<my_class> data;\npublic:\n    move_only(const move_only&) = delete;\n    move_only(move_only&& other):\n        data(std::move(other.data))\n    {}\n    move_only& operator=(const move_only&) = delete;\n    move_only& operator=(move_only&& other)\n    {\n        data=std::move(other.data);\n        return *this;\nListing A.2\nA simple move-only type\nNo implementation\nWon’t compile\n",
      "content_length": 1816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "360\nAPPENDIX A\nBrief reference for some C++11 language features\n    }\n};\nmove_only m1;\nmove_only m2(m1);            \nmove_only m3(std::move(m1));   \nMove-only objects can be passed as function parameters and returned from functions,\nbut if you want to move from an lvalue, you always have to be explicit and use\nstd::move() or a static_cast<T&&>.\n You can apply the = delete specifier to any function, not just copy constructors\nand assignment operators. This makes it clear that the function isn’t available. It does\na bit more than that too, though; a deleted function participates in overload resolu-\ntion in the normal way and only causes a compilation error if it’s selected. This can be\nused to remove specific overloads. For example, if your function takes a short param-\neter, you can prevent the narrowing of int values by writing an overload that takes an\nint and declaring it deleted:\nvoid foo(short);\nvoid foo(int) = delete;\nAny attempts to call foo with an int will now be met with a compilation error, and the\ncaller will have to explicitly cast supplied values to short:\nfoo(42);        \nfoo((short)42);    \nA.3\nDefaulted functions\nWhereas deleted functions allow you to explicitly declare that a function isn’t imple-\nmented, defaulted functions are the opposite extreme: they allow you to specify that\nthe compiler should write the function for you, with its “default” implementation. You\ncan only do this for functions that the compiler can autogenerate anyway: default con-\nstructors, destructors, copy constructors, move constructors, copy-assignment opera-\ntors, and move-assignment operators.\n Why would you want to do that? There are several reasons why you might:\n■\nIn order to change the accessibility of the function—By default, the compiler-generated\nfunctions are public. If you want to make them protected or even private,\nyou must write them yourself. By declaring them as defaulted, you can get the\ncompiler to write the function and change the access level.\n■\nAs documentation—If the compiler-generated version is sufficient, it might be\nworth explicitly declaring it as such so that when you or someone else looks at\nthe code later, it’s clear that this was intended.\n■\nIn order to force the compiler to generate the function when it would not otherwise have\ndone so—This is typically done with default constructors, which are only normally\nError; copy constructor \nis declared deleted\nOK; move constructor found\nError; int overload \ndeclared deleted\nOK\n",
      "content_length": 2487,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "361\nDefaulted functions\ncompiler-generated if there are no user-defined constructors. If you need to\ndefine a custom copy constructor (for example), you can still get a compiler-\ngenerated default constructor by declaring it as defaulted.\n■\nIn order to make a destructor virtual while leaving it as compiler-generated.\n■\nTo force a particular declaration of the copy constructor, such as having it take the source\nparameter by a non-const reference rather than by a const reference.\n■\nTo take advantage of the special properties of the compiler-generated function, which are\nlost if you provide an implementation—More on this in a moment.\nJust as deleted functions are declared by following the declaration with = delete,\ndefaulted functions are declared by following the declaration by = default; for example:\nclass Y\n{\nprivate:\n    Y() = default;   \npublic:\n    Y(Y&) = default;        \n    T& operator=(const Y&) = default;    \nprotected:\n    virtual ~Y() = default;   \n};\nI mentioned previously that compiler-generated functions can have special properties\nthat you can’t get from a user-defined version. The biggest difference is that a compiler-\ngenerated function can be trivial. This has a few consequences, including the following:\n■\nObjects with trivial copy constructors, trivial copy assignment operators, and\ntrivial destructors can be copied with memcpy or memmove.\n■\nLiteral types used for constexpr functions (see section A.4) must have a trivial\nconstructor, copy constructor, and destructor. \n■\nClasses with a trivial default constructor, copy constructor, copy assignment\noperator, and destructor can be used in a union with a user-defined constructor\nand destructor. \n■\nClasses with trivial copy assignment operators can be used with the std::atomic<>\nclass template (see section 5.2.6) in order to provide a value of that type with\natomic operations.\nJust declaring the function as = default doesn’t make it trivial—it will only be trivial if\nthe class also supports all the other criteria for the corresponding function to be triv-\nial—but explicitly writing the function in user code does prevent it from being trivial.\n The second difference between classes with compiler-generated functions and\nuser-supplied equivalents is that a class with no user-supplied constructors can be an\naggregate and thus can be initialized with an aggregate initializer:\nstruct aggregate\n{\n    aggregate() = default;\nChange access\nTake a non-const \nreference\nDeclare as defaulted \nfor documentation\nChange access \nand add virtual\n",
      "content_length": 2535,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "362\nAPPENDIX A\nBrief reference for some C++11 language features\n    aggregate(aggregate const&) = default;\n    int a;\n    double b;\n};\naggregate x={42,3.141};\nIn this case, x.a is initialized to 42 and x.b is initialized to 3.141.\n The third difference between a compiler-generated function and a user-supplied\nequivalent is quite esoteric and applies only to the default constructor and only to the\ndefault constructor of classes that meet certain criteria. Consider the following class:\nstruct X\n{\n    int a;\n};\nIf you create an instance of class X without an initializer, the contained int (a) is\ndefault initialized. If the object has static storage duration, it’s initialized to zero; other-\nwise, it has an indeterminate value that can potentially cause undefined behavior if it’s\naccessed before being assigned a new value:\nX x1;     \nIf, on the other hand, you initialize your instance of X by explicitly invoking the\ndefault constructor, then a is initialized to zero:\nX x2=X();     \nThis bizarre property also extends to base classes and members. If your class has a\ncompiler-generated default constructor and any of your data members and base\nclasses also have a compiler-generated default constructor, data members of those\nbases and members that are built-in types are also either left with an indeterminate\nvalue or initialized to zero, depending on whether or not the outer class has its default\nconstructor explicitly invoked.\n Although this rule is confusing and potentially error-prone, it does have its uses,\nand if you write the default constructor yourself, you lose this property; either data\nmembers like a are always initialized (because you specify a value or explicitly default\nconstruct) or always uninitialized (because you don’t):\nX::X():a(){}    \nX::X():a(42){}   \nX::X(){}      \nIf you omit the initialization of a from the constructor of X as in the third example B,\nthen a is left uninitialized for nonstatic instances of X and initialized to zero for\ninstances of X with static storage duration.\nx1.a has an \nindeterminate value.\nx2.a==0\na==0 always.\na==42 always.\nb\n",
      "content_length": 2101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "363\nconstexpr functions\n Under normal circumstances, if you write any other constructor manually, the\ncompiler will no longer generate the default constructor for you, so if you want one\nyou have to write it, which means you lose this bizarre initialization property. But by\nexplicitly declaring the constructor as defaulted, you can force the compiler to gener-\nate the default constructor for you, and this property is retained:\nX::X() = default;     \nThis property is used for the atomic types (see section 5.2), which have their default\nconstructor explicitly defaulted. Their initial value is always undefined unless either\n(a) they have static storage duration (and thus are statically initialized to zero), (b)\nyou explicitly invoke the default constructor to request zero initialization, or (c) you\nexplicitly specify a value. Note that in the case of the atomic types, the constructor for\ninitialization with a value is declared constexpr (see section A.4) in order to allow\nstatic initialization.\nA.4\nconstexpr functions\nInteger literals such as 42 are constant expressions, as are simple arithmetic expressions\nsuch as 23*2-4. You can even use const variables of integral type that are themselves\ninitialized with constant expressions as part of a new constant expression:\nconst int i=23;\nconst int two_i=i*2;\nconst int four=4;\nconst int forty_two=two_i-four;\nAside from using constant expressions to create variables that can be used in other\nconstant expressions, there are a few things you can only do with constant expressions:\n■\nSpecify the bounds of an array:\nint bounds=99;\nint array[bounds];      \nconst int bounds2=99;\nint array2[bounds2];     \n■\nSpecify the value of a nontype template parameter:\ntemplate<unsigned size>\nstruct test\n{};\ntest<bounds> ia;       \ntest<bounds2> ia2;         \n■\nProvide an initializer for a static const class data member of integral type in\nthe class definition:\nDefault initialization \nrules for a apply\nError bounds is not a \nconstant expression\nOK, bounds2 is a \nconstant expression.\nError bounds is not a \nconstant expression\nOK, bounds2 is a \nconstant expression.\n",
      "content_length": 2120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "364\nAPPENDIX A\nBrief reference for some C++11 language features\nclass X\n{\n    static const int the_answer=forty_two;\n};\n■\nProvide an initializer for a built-in type or aggregate that can be used for static\ninitialization:\nstruct my_aggregate\n{\n    int a;\n    int b;\n};\nstatic my_aggregate ma1={forty_two,123};    \nint dummy=257;\nstatic my_aggregate ma2={dummy,dummy};     \n■\nStatic initialization like this can be used to avoid order-of-initialization prob-\nlems and race conditions.\nNone of this is new—you could do all that with the 1998 edition of the C++ Stan-\ndard. But with the C++11 Standard what constitutes a constant expression has been\nextended with the introduction of the constexpr keyword. The C++14 and C++17\nstandards extend the constexpr facility further; a full primer is beyond the scope of\nthis appendix.\n The constexpr keyword is primarily a function modifier. If the parameter and\nreturn type of a function meet certain requirements and the body is sufficiently sim-\nple, a function can be declared constexpr, in which case it can be used in constant\nexpressions; for example:\nconstexpr int square(int x)\n{\n    return x*x;\n}\nint array[square(5)];\nIn this case, array will have 25 entries, because square is declared constexpr. Just\nbecause the function can be used in a constant expression doesn’t mean that all uses\nare automatically constant expressions:\nint dummy=4;\nint array[square(dummy)];     \nIn this example, dummy is not a constant expression B, so square(dummy) isn’t either—\nit’s a normal function call—and thus can’t be used to specify the bounds of array.\nStatic initialization\nDynamic \ninitialization\nError, dummy is not a \nconstant expression\nb\n",
      "content_length": 1683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "365\nconstexpr functions\nA.4.1\nconstexpr and user-defined types\nUp to now, all the examples have been with built-in types such as int. But the new C++\nStandard allows constant expressions to be of any type that satisfies the requirements\nfor a literal type. For a class type to be classified as a literal type, the following must all\nbe true:\n■\nIt must have a trivial copy constructor.\n■\nIt must have a trivial destructor.\n■\nAll non-static data members and base classes must be trivial types.\n■\nIt must have either a trivial default constructor or a constexpr constructor\nother than the copy constructor.\nWe’ll look at constexpr constructors shortly. For now, we’ll focus on classes with a triv-\nial default constructor, such as class CX in the following listing.\nclass CX\n{\nprivate:\n    int a;\n    int b;\npublic:\n    CX() = default;    \n    CX(int a_, int b_):    \n        a(a_),b(b_)\n    {}\n    int get_a() const\n    {\n        return a;\n    }\n    int get_b() const\n    {\n        return b;\n    }\n    int foo() const\n    {\n        return a+b;\n    }\n};\nNote that we’ve explicitly declared the default constructor B as defaulted (see section\nA.3) in order to preserve it as trivial in the face of the user-defined constructor c.\nThis type therefore fits all the qualifications for being a literal type, and you can use it\nin constant expressions. You can, for example, provide a constexpr function that cre-\nates new instances:\nconstexpr CX create_cx()\n{\n    return CX();\n}\nListing A.3\nA class with a trivial default constructor\nB\nc\n",
      "content_length": 1530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "366\nAPPENDIX A\nBrief reference for some C++11 language features\nYou can also create a simple constexpr function that copies its parameter:\nconstexpr CX clone(CX val)\n{\n    return val;\n}\nBut that’s about all you can do in C++11—a constexpr function can only call other\nconstexpr functions. In C++14, this restriction is lifted, and you can do almost any-\nthing in a constexpr function, provided it doesn’t modify any objects with non-local\nscope. What you can do, even in C++11, is apply constexpr to the member functions\nand constructor of CX:\nclass CX\n{\nprivate:\n    int a;\n    int b;\npublic:\n    CX() = default;\n    constexpr CX(int a_, int b_):\n        a(a_),b(b_)\n    {}\n    constexpr int get_a() const    \n    {\n        return a;\n    }\n    constexpr int get_b()    \n    {\n        return b;\n    }\n    constexpr int foo()\n    {\n        return a+b;\n    }\n};\nIn C++11, the const qualification on get_a() B is now superfluous, because it’s\nimplied by the use of constexpr, and get_b() is thus const even though the const\nqualification is omitted c. In C++14, this is changed (due to the extended capabilities\nof constexpr functions), so get_b() is no longer implicitly const. This now allows\nmore complex constexpr functions such as the following:\nconstexpr CX make_cx(int a)\n{\n    return CX(a,1);\n}\nconstexpr CX half_double(CX old)\n{\n    return CX(old.get_a()/2,old.get_b()*2);\n}\nconstexpr int foo_squared(CX val)\nb\nc\n",
      "content_length": 1419,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "367\nconstexpr functions\n{\n    return square(val.foo());\n}\nint array[foo_squared(half_double(make_cx(10)))];   \nInteresting though this is, it’s a lot of effort to go to if all you get is a fancy way of com-\nputing some array bounds or an integral constant. The key benefit of constant expres-\nsions and constexpr functions involving user-defined types is that objects of a literal\ntype initialized with a constant expression are statically initialized, and so their initial-\nization is free from race conditions and initialization order issues:\nCX si=half_double(CX(42,19));    \nThis covers constructors too. If the constructor is declared constexpr and the con-\nstructor parameters are constant expressions, the initialization is constant initialization\nand happens as part of the static initialization phase. This is one of the most important\nchanges in C++11 as far as concurrency goes: by allowing user-defined constructors that\ncan still undergo static initialization, you can avoid any race conditions over their ini-\ntialization, because they’re guaranteed to be initialized before any code is run.\n This is particularly relevant for things like std::mutex (see section 3.2.1) or\nstd::atomic<> (see section 5.2.6) where you might want to use a global instance to\nsynchronize access to other variables and avoid race conditions in that access. This\nwouldn’t be possible if the constructor of the mutex was subject to race conditions, so\nthe default constructor of std::mutex is declared constexpr to ensure that mutex ini-\ntialization is always done as part of the static initialization phase.\nA.4.2\nconstexpr objects\nSo far, we’ve looked at constexpr as applied to functions. constexpr can also be\napplied to objects. This is primarily for diagnostic purposes; it verifies that the object\nis initialized with a constant expression, constexpr constructor, or aggregate initial-\nizer made of constant expressions. It also declares the object as const:\nconstexpr int i=45;              \n    \nconstexpr std::string s(“hello”);      \nint foo();\nconstexpr int j=foo();   \nA.4.3\nconstexpr function requirements\nIn order to declare a function as constexpr it must meet a few requirements; if it\ndoesn’t meet these requirements, declaring it constexpr is a compilation error. In\nC++11, the requirements for a constexpr function were as follows:\n■\nAll parameters must be of a literal type.\n■\nThe return type must be a literal type.\n■\nThe function body must consist of a single return statement.\n49 elements\nStatically initialized\nOK\nError; std::string \nisn’t a literal type\nError; foo() isn’t \ndeclared constexpr\n",
      "content_length": 2610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "368\nAPPENDIX A\nBrief reference for some C++11 language features\n■\nThe expression in the return statement must qualify as a constant expression.\n■\nAny constructor or conversion operator used to construct the return value from\nthe expression must be constexpr.\nThis is straightforward; you must be able to inline the function into a constant expres-\nsion and it will still be a constant expression, and you must not modify anything.\nconstexpr functions are pure functions with no side effects.\n In C++14, the requirements were slackened quite considerably. Though the overall\nidea of a pure function with no side effects is preserved, the body is allowed to contain\nconsiderably more:\n■\nMultiple return statements are allowed.\n■\nObjects created within the function can be modified.\n■\nLoops, conditionals, and switch statements are allowed.\nFor constexpr class member functions there are additional requirements:\n■\nconstexpr member functions can’t be virtual.\n■\nThe class for which the function is a member must be a literal type.\nThe rules are different for constexpr constructors:\n■\nThe constructor body must be empty for a C++11 compiler; for a C++14 or later\ncompiler it must satisfy the requirements for a constexpr function.\n■\nEvery base class must be initialized.\n■\nEvery non-static data member must be initialized.\n■\nAny expressions used in the member initialization list must qualify as constant\nexpressions.\n■\nThe constructors chosen for the initialization of the data members and base\nclasses must be constexpr constructors.\n■\nAny constructor or conversion operator used to construct the data members\nand base classes from their corresponding initialization expression must be\nconstexpr.\nThis is the same set of rules as for functions, except that there’s no return value, so no\nreturn statement. Instead, the constructor initializes all the bases and data members\nin the member initialization list. Trivial copy constructors are implicitly constexpr.\nA.4.4\nconstexpr and templates\nWhen constexpr is applied to a function template, or to a member function of a class\ntemplate, it’s ignored if the parameters and return types of a particular instantiation\nof the template aren’t literal types. This allows you to write function templates that are\nconstexpr if the type of the template parameters is appropriate and just plain inline\nfunctions otherwise, for example:\ntemplate<typename T>\nconstexpr T sum(T a,T b)\n",
      "content_length": 2420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "369\nLambda functions\n{\n    return a+b;\n}\nconstexpr int i=sum(3,42);    \nstd::string s=\n    sum(std::string(\"hello\"),\n        std::string(\" world\"));     \nThe function must satisfy all the other requirements for a constexpr function. You\ncan’t declare a function with multiple statements constexpr just because it’s a func-\ntion template; that’s still a compilation error.\nA.5\nLambda functions\nLambda functions are one of the most exciting features of the C++11 Standard,\nbecause they have the potential to greatly simplify code and eliminate much of the\nboilerplate associated with writing callable objects. The C++11 lambda function syntax\nallows a function to be defined at the point where it’s needed in another expression.\nThis works well for things like predicates provided to the wait functions of std::\ncondition_variable (as in the example in section 4.1.1), because it allows the seman-\ntics to be quickly expressed in terms of the accessible variables rather than capturing\nthe necessary state in the member variables of a class with a function call operator.\n At its simplest, a lambda expression defines a self-contained function that takes no\nparameters and relies only on global variables and functions. It doesn’t even have to\nreturn a value. This lambda expression is a series of statements enclosed in brackets,\nprefixed with square brackets (the lambda introducer):\n[]{               \n    do_stuff();\n    do_more_stuff();\n}();               \nIn this example, the lambda expression is called by following it with parentheses, but\nthis is unusual. For one thing, if you’re going to call it directly, you could usually do\naway with the lambda and write the statements directly in the source. It’s more com-\nmon to pass it as a parameter to a function template that takes a callable object as one\nof its parameters, in which case it likely needs to take parameters or return a value or\nboth. If you need to take parameters, you can do this by following the lambda intro-\nducer with a parameter list like for a normal function. For example, the following\ncode writes all the elements of the vector to std::cout separated by newlines:\nstd::vector<int> data=make_data();\nstd::for_each(data.begin(),data.end(),[](int i){std::cout<<i<<\"\\n\";});\nReturn values are almost as easy. If your lambda function body consists of a single\nreturn statement, the return type of the lambda is the type of the expression being\nOK; sum<int> \nis constexpr.\nOK, but sum<std::string> \nisn’t constexpr.\nStart the lambda \nexpression with [].\nFinish the lambda, \nand call it.\n",
      "content_length": 2563,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "370\nAPPENDIX A\nBrief reference for some C++11 language features\nreturned. For example, you might use a simple lambda like this to wait for a flag to be\nset with std::condition_variable (see section 4.1.1), as in the following listing.\nstd::condition_variable cond;\nbool data_ready;\nstd::mutex m;\nvoid wait_for_data()\n{\n    std::unique_lock<std::mutex> lk(m);\n    cond.wait(lk,[]{return data_ready;});    \n}\nThe return type of the lambda passed to cond.wait() B is deduced from the type of\ndata_ready and is thus bool. Whenever the condition variable wakes from waiting, it\nthen calls the lambda with the mutex locked and only returns from the call to wait()\nonce data_ready is true.\n What if you can’t write your lambda body as a single return statement? In that case\nyou have to specify the return type explicitly. You can do this even if your body is a sin-\ngle return statement, but you have to do it if your lambda body is more complex. The\nreturn type is specified by following the lambda parameter list with an arrow (->) and\nthe return type. If your lambda doesn’t take any parameters, you must still include the\n(empty) parameter list in order to specify the return value explicitly. Your condition\nvariable predicate can be written\ncond.wait(lk,[]()->bool{return data_ready;});\nBy specifying the return type, you can expand the lambda to log messages or do some\nmore complex processing:\ncond.wait(lk,[]()->bool{\n    if(data_ready)\n    {\n        std::cout<<”Data ready”<<std::endl;\n        return true;\n    }\n    else\n    {\n        std::cout<<”Data not ready, resuming wait”<<std::endl;\n        return false;\n    }\n});\nAlthough simple lambdas like this are powerful and can simplify code quite a lot, the\nreal power of lambdas comes when they capture local variables.\nListing A.4\nA simple lambda with a deduced return type\nb\n",
      "content_length": 1833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "371\nLambda functions\nA.5.1\nLambda functions that reference local variables\nLambda functions with a lambda introducer of [] can’t reference any local variables\nfrom the containing scope; they can only use global variables and anything passed in\nas a parameter. If you want to access a local variable, you need to capture it. The sim-\nplest way to do this is to capture the entire set of variables within the local scope by\nusing a lambda introducer of [=]. That’s all there is to it—your lambda can now\naccess copies of the local variables at the time the lambda was created.\n To see this in action, consider the following simple function:\nstd::function<int(int)> make_offseter(int offset)\n{\n   return [=](int j){return offset+j;};\n}\nEvery call to make_offseter returns a new lambda function object through the\nstd::function<> function wrapper. This returned function adds the supplied offset\nto any parameter supplied. For example,\nint main()\n{\n    std::function<int(int)> offset_42=make_offseter(42);\n    std::function<int(int)> offset_123=make_offseter(123);\n    std::cout<<offset_42(12)<<”,“<<offset_123(12)<<std::endl;\n    std::cout<<offset_42(12)<<”,“<<offset_123(12)<<std::endl;\n}\nwill write out 54,135 twice because the function returned from the first call to make_\noffseter always adds 42 to the supplied argument, whereas the function returned\nfrom the second call to make_offseter always adds 123 to the supplied argument.\n This is the safest form of local variable capture; everything is copied, so you can\nreturn the lambda and call it outside the scope of the original function. It’s not the\nonly choice though; you can choose to capture everything by reference instead. In this\ncase it’s undefined behavior to call the lambda once the variables it references have\nbeen destroyed by exiting the function or block scope to which they belong, just as it’s\nundefined behavior to reference a variable that has already been destroyed in any\nother circumstance.\n A lambda function that captures all the local variables by reference is introduced\nusing [&], as in the following example:\nint main()\n{\n    int offset=42;     \n    std::function<int(int)> offset_a=[&](int j){return offset+j;};    \n    offset=123;                                                   \n    std::function<int(int)> offset_b=[&](int j){return offset+j;};    \n    std::cout<<offset_a(12)<<”,”<<offset_b(12)<<std::endl;         \n    offset=99;                                             \n    std::cout<<offset_a(12)<<”,”<<offset_b(12)<<std::endl;    \n}\nB\nc\nd\ne\nf\ng\nh\n",
      "content_length": 2547,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "372\nAPPENDIX A\nBrief reference for some C++11 language features\nWhereas in the make_offseter function from the previous example you used the [=]\nlambda introducer to capture a copy of the offset, the offset_a function in this exam-\nple uses the [&] lambda introducer to capture offset by reference c. It doesn’t mat-\nter that the initial value of offset is 42 B; the result of calling offset_a(12) will\nalways depend on the current value of offset. Even though the value of offset is\nthen changed to 123 d, before you produce the second (identical) lambda function,\noffset_b e, this second lambda again captures by reference, so the result depends\non the current value of offset. \n Now, when you print the first line of output f, offset is still 123, so the output is\n135,135. But at the second line of output h, offset has been changed to 99 g, so\nthis time the output is 111,111. Both offset_a and offset_b add the current value of\noffset (99) to the supplied argument (12).\n Now, C++ being C++, you’re not stuck with these all-or-nothing options; you\ncan choose to capture some variables by copy and some by reference, and you can\nchoose to capture only those variables you have explicitly chosen by tweaking the\nlambda introducer. If you want to copy all the used variables except for one or two,\nyou can use the [=] form of the lambda introducer but follow the equals sign with a\nlist of variables to capture by reference preceded with ampersands. The following\nexample will print 1239, because i is copied into the lambda, but j and k are cap-\ntured by reference:\nint main()\n{\n    int i=1234,j=5678,k=9;\n    std::function<int()> f=[=,&j,&k]{return i+j+k;};\n    i=1;\n    j=2;\n    k=3;\n    std::cout<<f()<<std::endl;\n}\nAlternatively, you can capture by reference by default but capture a specific subset of\nvariables by copying. In this case, you use the [&] form of the lambda introducer but\nfollow the ampersand with a list of variables to capture by copy. The following exam-\nple prints 5688 because i is captured by reference, but j and k are copied:\nint main()\n{\n    int i=1234,j=5678,k=9;\n    std::function<int()> f=[&,j,k]{return i+j+k;};\n    i=1;\n    j=2;\n    k=3;\n    std::cout<<f()<<std::endl;\n}\nIf you only want to capture the named variables, then you can omit the leading = or &\nand just list the variables to be captured, prefixing them with an ampersand to capture\n",
      "content_length": 2384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "373\nLambda functions\nby reference rather than copy. The following code will print 5682 because i and k are\ncaptured by reference, but j is copied:\nint main()\n{\n    int i=1234,j=5678,k=9;\n    std::function<int()> f=[&i,j,&k]{return i+j+k;};\n    i=1;\n    j=2;\n    k=3;\n    std::cout<<f()<<std::endl;\n}\nThis final variant allows you to ensure that only the intended variables are being cap-\ntured, because any reference to a local variable not in the capture list will cause a com-\npilation error. If you choose this option, you have to be careful when accessing class\nmembers if the function containing the lambda is a member function. Class members\ncan’t be captured directly; if you want to access class members from your lambda, you\nhave to capture the this pointer by adding it to the capture list. In the following\nexample, the lambda captures this to allow access to the some_data class member:\nstruct X\n{\n    int some_data;\n    void foo(std::vector<int>& vec)\n    {\n        std::for_each(vec.begin(),vec.end(),\n            [this](int& i){i+=some_data;});\n    }\n};\nIn the context of concurrency, lambdas are most useful as predicates for std::condition\n_variable::wait() (section 4.1.1) and with std::packaged_task<> (section 4.2.1)\nor thread pools for packaging small tasks. They can also be passed to the std::thread\nconstructor as a thread function (section 2.1.1) and as the function when using paral-\nlel algorithms such as parallel_for_each() (from section 8.5.1).\n Since C++14, lambdas can also be generic lamdas, where the parameter types are\ndeclared as auto rather than a specified type. In this case, the function call operator is\nimplicitly a template, and the type of the parameter is deduced from the supplied\nargument when the lambda is invoked; for example:\nauto f=[](auto x){ std::cout<<”x=”<<x<<std::endl;};\nf(42); // x is of type int; outputs “x=42”\nf(“hello”); // x is of type const char*; outputs “x=hello”\nC++14 also adds the concept of generalized captures, so you can capture the results of\nexpressions, rather than a direct copy of or reference to a local variable. Most com-\nmonly this can be used to capture move-only types by moving them, rather than hav-\ning to capture by reference; for example:\n",
      "content_length": 2230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "374\nAPPENDIX A\nBrief reference for some C++11 language features\nstd::future<int> spawn_async_task(){\n    std::promise<int> p;\n    auto f=p.get_future();\n    std::thread t([p=std::move(p)](){ p.set_value(find_the_answer());});\n    t.detach();\n    return f;\n}\nHere, the promise is moved into the lambda by the p=std::move(p) generalized cap-\nture, so it is safe to detach the thread, without the worry of a dangling reference to a\nlocal variable that has been destroyed. After the construction of the lambda, the origi-\nnal p is now in a moved-from state, which is why you had to get the future beforehand.\nA.6\nVariadic templates\nVariadic templates are templates with a variable number of parameters. Just as you’ve\nalways been able to have variadic functions, such as printf, that take a variable num-\nber of parameters, you can now have variadic templates that have a variable number of\ntemplate parameters. Variadic templates are used throughout the C++ Thread Library.\nFor example, the std::thread constructor for starting a thread (section 2.1.1) is a\nvariadic function template, and std::packaged_task<> (section 4.2.2) is a variadic\nclass template. From a user’s point of view, it’s enough to know that the template takes\nan unbounded number of parameters, but if you want to write this template, or if\nyou’re interested in how it all works, you need to know the details.\n Just as variadic functions are declared with an ellipsis (...) in the function param-\neter list, variadic templates are declared with an ellipsis in the template parameter list:\ntemplate<typename ... ParameterPack>\nclass my_template\n{};\nYou can use variadic templates for a partial specialization of a template too, even if the\nprimary template isn’t variadic. For example, the primary template for std::packaged\n_task<> (section 4.2.1) is a simple template with a single template parameter:\ntemplate<typename FunctionType>\nclass packaged_task;\nBut this primary template is never defined anywhere; it’s a placeholder for the partial\nspecialization:\ntemplate<typename ReturnType,typename ... Args>\nclass packaged_task<ReturnType(Args...)>;\nIt’s this partial specialization that contains the real definition of the class; you saw in\nchapter 4 that you can write std::packaged_task<int(std::string,double)> to\ndeclare a task that takes an std::string and a double as parameters when you call it\nand that provides the result through an std::future<int>.\n",
      "content_length": 2428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "375\nVariadic templates\n This declaration shows two additional features of variadic templates. The first\nfeature is relatively simple: you can have normal template parameters (such as\nReturnType) as well as variadic ones (Args) in the same declaration. The second fea-\nture demonstrated is the use of Args... in the template argument list of the special-\nization to show that the types that make up Args when the template is instantiated\nare to be listed here. Because this is a partial specialization, it works as a pattern\nmatch; the types that occur in this context in the instantiation are captured as Args.\nThe variadic parameter Args is called a parameter pack, and the use of Args... is\ncalled a pack expansion.\n Like with variadic functions, the variadic part may be an empty list or may have\nmany entries. For example, with std::packaged_task<my_class()> the ReturnType\nparameter is my_class, and the Args parameter pack is empty, whereas with\nstd::packaged_task<void(int,double,my_class&,std::string*)> the ReturnType\nis void, and Args is the list int, double, my_class&, std::string*.\nA.6.1\nExpanding the parameter pack\nThe power of variadic templates comes from what you can do with that pack expan-\nsion: you aren’t limited to expanding the list of types as is. First off, you can use a pack\nexpansion directly anywhere a list of types is required, such as in the argument list for\nanother template:\ntemplate<typename ... Params>\nstruct dummy\n{\n    std::tuple<Params...> data;\n};\nIn this case the single member variable data is an instantiation of std::tuple<>\ncontaining all the types specified, so dummy<int,double,char> has a member of\ntype std::tuple<int,double,char>. You can combine pack expansions with nor-\nmal types:\ntemplate<typename ... Params>\nstruct dummy2\n{\n    std::tuple<std::string,Params...> data;\n};\nThis time, the tuple has an additional (first) member of type std::string. The nifty\npart is that you can create a pattern with the pack expansion, which is then copied for\neach element in the expansion. You do this by putting the ... that marks the pack\nexpansion at the end of the pattern. For example, rather than just creating a tuple of\nthe elements supplied in your parameter pack, you can create a tuple of pointers to\nthe elements or even a tuple of std::unique_ptr<> to your elements:\ntemplate<typename ... Params>\nstruct dummy3\n",
      "content_length": 2369,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "376\nAPPENDIX A\nBrief reference for some C++11 language features\n{\n    std::tuple<Params* ...> pointers;\n    std::tuple<std::unique_ptr<Params> ...> unique_pointers;\n};\nThe type expression can be as complex as you like, provided the parameter pack\noccurs in the type expression, and provided the expression is followed by the ... that\nmarks the expansion. When the parameter pack is expanded, for each entry in the pack\nthat type is substituted into the type expression to generate the corresponding entry in\nthe resulting list. If your parameter pack Params contains the types int,int,char, then\nthe expansion of std::tuple<std::pair<std::unique_ptr<Params>,double> ... > is\nstd::tuple<std::pair<std::unique_ptr<int>,double>, std::pair<std::unique_\nptr<int>,double>, std::pair<std::unique_ptr<char>,double> >. If the pack expan-\nsion is used as a template argument list, that template doesn’t have to have variadic\nparameters, but if it doesn’t, the size of the pack must exactly match the number of\ntemplate parameters required:\ntemplate<typename ... Types>\nstruct dummy4\n{\n    std::pair<Types...> data;\n};\ndummy4<int,char> a;           \ndummy4<int> b;                \ndummy4<int,int,int> c;         \nThe second thing you can do with a pack expansion is use it to declare a list of func-\ntion parameters:\ntemplate<typename ... Args>\nvoid foo(Args ... args);\nThis creates a new parameter pack, args, which is a list of the function parameters\nrather than a list of types, which you can expand with ... as before. Now, you can use\na pattern with the pack expansion for declaring the function parameters, just as you\ncan use a pattern when you expand the pack elsewhere. For example, this is used by\nthe std::thread constructor to take all the function arguments by rvalue reference\n(see section A.1):\ntemplate<typename CallableType,typename ... Args>\nthread::thread(CallableType&& func,Args&& ... args);\nThe function parameter pack can then be used to call another function, by specifying\nthe pack expansion in the argument list of the called function. As with the type expan-\nsions, you can use a pattern for each expression in the resulting argument list. For\nexample, one common idiom with rvalue references is to use std::forward<> to pre-\nserve the rvalue-ness of the supplied function arguments:\nOK, data is \nstd::pair<int,char>.\nError; no second type.\nError; too many types.\n",
      "content_length": 2381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "377\nAutomatically deducing the type of a variable\ntemplate<typename ... ArgTypes>\nvoid bar(ArgTypes&& ... args)\n{\n    foo(std::forward<ArgTypes>(args)...);\n}\nNote that in this case, the pack expansion contains both the type pack ArgTypes and\nthe function parameter pack args, and the ellipsis follows the whole expression. If you\ncall bar like this,\nint i;\nbar(i,3.141,std::string(\"hello \"));\nthen the expansion becomes\ntemplate<>\nvoid bar<int&,double,std::string>(\n    int& args_1,\n    double&& args_2,\n    std::string&& args_3)\n{\n    foo(std::forward<int&>(args_1),\n        std::forward<double>(args_2),\n        std::forward<std::string>(args_3));\n}\nwhich correctly passes the first argument on to foo as an lvalue reference, while pass-\ning the others as rvalue references.\n The final thing you can do with a parameter pack is find its size with the sizeof...\noperator. This is quite simple: sizeof...(p) is the number of elements in the param-\neter pack p. It doesn’t matter whether this is a type parameter pack or a function argu-\nment parameter pack; the result is the same. This is probably the only case where you\ncan use a parameter pack and not follow it with an ellipsis; the ellipsis is already part\nof the sizeof... operator. The following function returns the number of arguments\nsupplied to it:\ntemplate<typename ... Args>\nunsigned count_args(Args ... args)\n{\n    return sizeof... (Args);\n}\nAs with the normal sizeof operator, the result of sizeof... is a constant expression,\nso it can be used for specifying array bounds and so forth.\nA.7\nAutomatically deducing the type of a variable\nC++ is a statically typed language: the type of every variable is known at compile time.\nNot only that, but as a programmer you have to specify the type of each variable. In\nsome cases this can lead to quite unwieldy names; for example:\n",
      "content_length": 1840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "378\nAPPENDIX A\nBrief reference for some C++11 language features\nstd::map<std::string,std::unique_ptr<some_data>> m;\nstd::map<std::string,std::unique_ptr<some_data>>::iterator\n    iter=m.find(\"my key\");\nTraditionally, the solution has been to use typedefs to reduce the length of a type\nidentifier and potentially eliminate problems due to inconsistent types. This still\nworks in C++11, but there’s now a new way: if a variable is initialized in its declaration\nfrom a value of the same type, then you can specify the type as auto. In this case, the\ncompiler will automatically deduce the type of the variable to be the same as the ini-\ntializer. The iterator example can be written as\nauto iter=m.find(\"my key\");\nNow, you’re not restricted to plain auto; you can embellish it to declare const vari-\nables or pointer or reference variables too. Here are a few variable declarations using\nauto and the corresponding type of the variable:\nauto i=42;        // int\nauto& j=i;        // int&\nauto const k=i;   // int const\nauto* const p=&i; // int * const\nThe rules for deducing the type of the variable are based on the rules for the only\nother place in the language where types are deduced: parameters of function tem-\nplates. In a declaration of the form\nsome-type-expression-involving-auto var=some-expression;\nthe type of var is the same as the type deduced for the parameter of a function tem-\nplate declared with the same type expression, except replacing auto with the name of\na template type parameter:\ntemplate<typename T>\nvoid f(type-expression var);\nf(some-expression);\nThis means that array types decay to pointers, and references are dropped unless the\ntype expression explicitly declares the variable as a reference; for example:\nint some_array[45];\nauto p=some_array;   // int*\nint& r=*p;\nauto x=r;            // int\nauto& y=r;           // int&\nThis can greatly simplify the declaration of variables, particularly where the full type\nidentifier is long or possibly not even known (for example, the type of the result of a\nfunction call in a template).\n",
      "content_length": 2064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "379\nThread-local variables\nA.8\nThread-local variables\nThread-local variables allow you to have a separate instance of a variable for each thread\nin your program. You mark a variable as being thread-local by declaring it with the\nthread_local keyword. Variables at namespace scope, static data members of classes,\nand local variables can be declared thread-local, and are said to have thread storage\nduration:\nthread_local int x;    \nclass X\n{\n    static thread_local std::string s;       \n};\nstatic thread_local std::string X::s;     \nvoid foo()\n{\n    thread_local std::vector<int> v;    \n}\nThread-local variables at namespace scope and thread-local static class data members\nare constructed before the first use of a thread-local variable from the same transla-\ntion unit, but it isn’t specified how much before. Some implementations may construct\nthread-local variables when the thread is started; others may construct them immedi-\nately before their first use on each thread, and others may construct them at other\ntimes, or in some combination depending on their usage context. Indeed, if none of\nthe thread-local variables from a given translation unit is used, there’s no guarantee\nthat they will be constructed at all. This allows for the dynamic loading of modules\ncontaining thread-local variables—these variables can be constructed on a given\nthread the first time that thread references a thread-local variable from the dynami-\ncally-loaded module.\n Thread-local variables declared inside a function are initialized the first time the\nflow of control passes through their declaration on a given thread. If the function is\nnot called by a given thread, any thread-local variables declared in that function are\nnot constructed. This is the same as the behavior for local static variables, except it\napplies separately to each thread.\n Thread-local variables share other properties with static variables—they’re zero-\ninitialized prior to any further initialization (such as dynamic initialization), and if the\nconstruction of a thread-local variable throws an exception, std::terminate() is called\nto abort the application.\n The destructors for all thread-local variables that have been constructed on a given\nthread are run when the thread function returns, in the reverse order of construction.\nBecause the order of initialization is unspecified, it’s important to ensure that there\nare no interdependencies between the destructors of these variables. If the destructor\nof a thread-local variable exits with an exception, std::terminate() is called, as for\nconstruction.\nA thread-local variable \nat namespace scope\nA thread-local static \nclass data member\nThe definition of X::s is \nrequired.\nA thread-local \nlocal variable\n",
      "content_length": 2735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "380\nAPPENDIX A\nBrief reference for some C++11 language features\n Thread-local variables are also destroyed for a thread if that thread calls\nstd::exit() or returns from main() (which is equivalent to calling std::exit() with\nthe return value of main()). If any other threads are still running when the applica-\ntion exits, the destructors of thread-local variables on those threads are not called.\n Though thread-local variables have a different address on each thread, you can\nstill obtain a normal pointer to this variable. The pointer then references the object in\nthe thread that took the address, and can be used to allow other threads to access that\nobject. It’s undefined behavior to access an object after it’s been destroyed (as always),\nso if you pass a pointer to a thread-local variable to another thread, you need to\nensure it’s not dereferenced once the owning thread has finished.\nA.9\nClass Template Argument Deduction\nC++17 extends the idea of automatically deducing types to template parameters: if you\nare declaring an object of a templated type, then in many cases the type of the tem-\nplate parameters can be deduced from the object initializer.\n Specifically, if an object is declared with the name of a class template, without spec-\nifying a template argument list, then constructors specified in the class template are\nused to deduce the template arguments from the object's initializer, as per the normal\ntype deduction rules for function templates.\n For example, std::lock_guard takes a single template parameter, which is the\ntype of the mutex. The constructor also takes a single parameter, which is a reference\nto that type. If you declare an object to be of type std::lock_guard, then the type\nparameter can be deduced from the type of the supplied mutex:\nstd::mutex m;\nstd::lock_guard guard(m); // deduces std::lock_guard<std::mutex>\nThe same applies to std::scoped_lock, except that it has multiple template parame-\nters, which can be deduced from multiple mutex arguments:\nstd::mutex m1;\nstd::shared_mutex m2;\nstd::scoped_lock guard(m1,m2);\n// deduces std::scoped_lock<std::mutex,std::shared_mutex>\nFor those templates where the constructors would lead to the wrong types being\ndeduced, the template author can write explicit deduction guides to ensure the cor-\nrect types are deduced. But these are beyond the scope of this book.\nSummary\nThis appendix has only scratched the surface of the new language features introduced\nwith the C++11 Standard, because we’ve only looked at those features that actively\naffect the usage of the Thread Library. Other new language features include static\nassertions, strongly typed enumerations, delegating constructors, Unicode support,\ntemplate aliases, and a new uniform initialization sequence, along with a host of\n",
      "content_length": 2787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "381\nSummary\nsmaller changes. Describing all the new features in detail is outside the scope of this\nbook; it would probably require a book in itself. There are also a considerable number\nof changes added with C++14 and C++17, but again these are outside the scope of this\nbook. The best overview of the entire set of changes to the standard at the time of writ-\ning is probably the documentation at cppreference.com,1 as well as Bjarne Strous-\ntrup’s C++11 FAQ,2 though popular C++ reference books will be revised to cover it in\ndue course.\n Hopefully the brief introduction to the new features covered in this appendix has\nprovided enough depth to show how they relate to the Thread Library and to enable\nyou to write and understand multithreaded code that uses these new features.\nAlthough this appendix should provide enough depth for simple uses of the features\ncovered, this is still only a brief introduction and not a complete reference or tutorial\nfor the use of these features. If you intend to make extensive use of them, I recom-\nmend acquiring a reference or tutorial in order to gain the most benefit from them. \n1 http://www.cppreference.com\n2 http://www.research.att.com/~bs/C++0xFAQ.html\n",
      "content_length": 1204,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "382\nappendix B\nBrief comparison of\nconcurrency libraries\nConcurrency and multithreading support in programming languages and libraries\naren’t something new, even though standardized support in C++ is new. For exam-\nple, Java has had multithreading support since it was first released, platforms that\nconform to the POSIX standard provide a C interface for multithreading, and\nErlang provides support for message-passing concurrency. There are even C++ class\nlibraries, such as Boost, that wrap the underlying programming interface for multi-\nthreading used on any given platform (whether it’s the POSIX C interface or some-\nthing else) to provide a portable interface across the supported platforms.\n For those who are already experienced in writing multithreaded applications\nand would like to use that experience to write code using the new C++ multithread-\ning facilities, this appendix provides a comparison between the facilities available in\nJava, POSIX C, C++ with the Boost Thread Library, and C++11, along with cross-\nreferences to the relevant chapters of this book.\n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 1099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "383\nFeature\nJava\nPOSIX C\nBoost threads\nC++11\nChapter\nreference\nStarting\nthreads\njava.lang.thread\nclass\npthread_t type and\nassociated API functions:\npthread_create(),\npthread_detach(), and\npthread_join()\nboost::thread class and\nmember functions\nstd::thread class and\nmember functions\nChapter 2\nMutual\nexclusion\nsynchronized\nblocks\npthread_mutex_t type\nand associated API functions:\npthread_mutex_lock(),\npthread_mutex_unlock(),\netc.\nboost::mutex class and\nmember functions,\nboost::lock_guard<>\nand boost::unique_lock<>\ntemplates\nstd::mutex class and\nmember functions,\nstd::lock_guard<> and\nstd::unique_lock<>\ntemplates\nChapter 3\nMonitors/\nwaits for a\npredicate\nwait()\nnotify()\nand\nmethods of the\njava.lang.Object class,\nused inside synchronized\nblocks\npthread_cond_t type\nand associated API functions:\npthread_cond_wait(),\npthread_cond_timed_\nwait(), etc.\nboost::condition_\nvariable and\nboost::condition_\nvariable_any classes and\nmember functions\nstd::condition_\nvariable and\nstd::condition_\nvariable_any classes and\nmember functions\nChapter 4\nAtomic opera-\ntions and\nconcurrency-\naware mem-\nory model\nvolatile variables,\nthe types in the\njava.util.concurrent\n.atomic package\nN/A\nN/A\nstd::atomic_xxx types,\nstd::atomic<> class\ntemplate,\nstd::atomic_thread_\nfence() function\nChapter 5\nThread-safe\ncontainers\nThe containers in the\njava.util.concurrent\npackage\nN/A\nN/A\nN/A\nChapters 6\nand 7\nFutures\njava.util.concurrent\n.future interface and\nassociated classes\nN/A\nboost::unique_future<>\nand\nboost::shared_future<>\nclass templates\nstd::future<>,\nstd::shared_future<>\nand\nstd::atomic_future<>\nclass templates\nChapter 4\nThread\npools\njava.util.concurrent\n.ThreadPoolExecutor\nclass\nN/A\nN/A\nN/A\nChapter 9\nThread\ninterruption\ninterrupt() method of\njava.lang.Thread\npthread_cancel()\ninterrupt() member function\nof\nclass\nboost::thread\nN/A\nChapter 9\n",
      "content_length": 1836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "384\nappendix C\nA message-passing\nframework and complete\nATM example\nBack in chapter 4, I presented an example of sending messages between threads\nusing a message-passing framework, using a simple implementation of the code in\nan ATM as an example. What follows is the complete code for this example, includ-\ning the message-passing framework.\n Listing C.1 shows the message queue. It stores a list of messages as pointers to a\nbase class; the specific message type is handled with a template class derived from\nthat base class. Pushing an entry constructs an appropriate instance of the wrapper\nclass and stores a pointer to it; popping an entry returns that pointer. Because the\nmessage_base class doesn’t have any member functions, the popping thread will\nneed to cast the pointer to a suitable wrapped_message<T> pointer before it can\naccess the stored message.\n#include <mutex>\n#include <condition_variable>\n#include <queue>\n#include <memory>\nnamespace messaging\n{\n    struct message_base      \n    {\n        virtual ~message_base()\n        {}\n    };\nListing C.1\nA simple message queue\nBase class of your \nqueue entries\n",
      "content_length": 1124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "385\n    template<typename Msg>\n    struct wrapped_message:      \n        message_base\n    {\n        Msg contents;\n        explicit wrapped_message(Msg const& contents_):\n            contents(contents_)\n        {}\n    };\n    class queue         \n    {\n        std::mutex m;\n        std::condition_variable c;\n        std::queue<std::shared_ptr<message_base> > q;   \n    public:\n        template<typename T>\n        void push(T const& msg)\n        {\n            std::lock_guard<std::mutex> lk(m);\n            q.push(std::make_shared<wrapped_message<T> >(msg));   \n            c.notify_all();\n        }\n        std::shared_ptr<message_base> wait_and_pop()\n        {\n            std::unique_lock<std::mutex> lk(m);\n            c.wait(lk,[&]{return !q.empty();});    \n            auto res=q.front();\n            q.pop();\n            return res;\n        }\n    };\n}\nSending messages is handled through an instance of the sender class shown in list-\ning C.2. This is a thin wrapper around a message queue that only allows messages to\nbe pushed. Copying instances of sender copies the pointer to the queue rather than\nthe queue itself.\nnamespace messaging\n{\n    class sender\n    {\n        queue*q;    \n    public:\n        sender():          \n            q(nullptr)\n        {}\n        explicit sender(queue*q_):     \n            q(q_)\n        {}\n        template<typename Message>\n        void send(Message const& msg)\nListing C.2\nThe sender class\nEach message type \nhas a specialization.\nYour message \nqueue\nInternal queue stores \npointers to message_base\nWrap posted \nmessage and \nstore pointer\nBlock until queue \nisn’t empty\nsender is a wrapper \naround the queue pointer.\nDefault-constructed \nsender has no queue\nAllow construction \nfrom pointer to queue\n",
      "content_length": 1748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "386\nAPPENDIX C\nA message-passing framework and complete ATM example\n        {\n            if(q)\n            {\n                q->push(msg);  \n            }\n        }\n    };\n}\nReceiving messages is a bit more complicated. Not only do you have to wait for a mes-\nsage from the queue, but you also have to check to see if the type matches any of the\nmessage types being waited on and call the appropriate handler function. This all\nstarts with the receiver class, shown in the following listing.\nnamespace messaging\n{\n    class receiver\n    {\n        queue q;     \n    public:\n        operator sender()      \n        {\n            return sender(&q);\n        }\n        dispatcher wait()        \n        {\n            return dispatcher(&q);\n        }\n    };\n}\nWhereas a sender references a message queue, a receiver owns it. You can obtain a\nsender that references the queue by using the implicit conversion. The complexity of\ndoing the message dispatch starts with a call to wait(). This creates a dispatcher\nobject that references the queue from the receiver. The dispatcher class is shown in\nthe next listing; as you can see, the work is done in the destructor. In this case, that\nwork consists of waiting for a message and dispatching it.\nnamespace messaging\n{\n    class close_queue      \n    {};\n    class dispatcher\n    {\n        queue* q;\n        bool chained;\n        dispatcher(dispatcher const&)=delete;       \n        dispatcher& operator=(dispatcher const&)=delete;\nListing C.3\nThe receiver class\nListing C.4\nThe dispatcher class\nSending pushes \nmessage on the queue\nA receiver owns \nthe queue.\nAllow implicit conversion \nto a sender that \nreferences the queue.\nWaiting for a queue \ncreates a dispatcher\nThe message for \nclosing the queue\ndispatcher instances \ncannot be copied.\n",
      "content_length": 1786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "387\n        template<\n            typename Dispatcher,\n            typename Msg,\n            typename Func>               \n        friend class TemplateDispatcher;\n        void wait_and_dispatch()\n        {\n            for(;;)          \n            {\n                auto msg=q->wait_and_pop();\n                dispatch(msg);\n            }\n        }\n        bool dispatch(                \n            std::shared_ptr<message_base> const& msg)\n        {\n            if(dynamic_cast<wrapped_message<close_queue>*>(msg.get()))\n            {\n                throw close_queue();\n            }\n            return false;\n        }\n    public:\n        dispatcher(dispatcher&& other):      \n            q(other.q),chained(other.chained)\n        {\n            other.chained=true;         \n        }\n        explicit dispatcher(queue* q_):          \n            q(q_),chained(false)\n        {}\n        template<typename Message,typename Func>\n        TemplateDispatcher<dispatcher,Message,Func> \n        handle(Func&& f)                             \n        {\n            return TemplateDispatcher<dispatcher,Message,Func>(\n                q,this,std::forward<Func>(f));\n        }\n        ~dispatcher() noexcept(false)   \n        {\n            if(!chained)\n            {\n                wait_and_dispatch();\n            }\n        }\n    };\n}\nThe dispatcher instance that’s returned from wait() will be destroyed immediately,\nbecause it’s temporary, and as mentioned, the destructor does the work. The destruc-\ntor calls wait_and_dispatch(), which is a loop B that waits for a message and passes it\nto dispatch(). dispatch() itself c is rather simple, it checks whether the message\nis a close_queue message and throws an exception if it is; otherwise, it returns false\nto indicate that the message was unhandled. This close_queue exception is why the\nAllow TemplateDispatcher \ninstances to access the \ninternals.\nLoop, waiting for, and \ndispatching messages\nb\ndispatch() checks \nfor a close_queue \nmessage, and throws.\nc\nDispatcher instances \ncan be moved.\nThe source shouldn’t \nwait for messages.\nHandle a specific type \nof message with a \nTemplateDispatcher.\nd\nThe destructor might \nthrow exceptions.\ne\n",
      "content_length": 2193,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "388\nAPPENDIX C\nA message-passing framework and complete ATM example\ndestructor is marked noexcept(false); without this annotation, the default exception\nspecification for the destructor would be noexcept(true) e, indicating that no excep-\ntions can be thrown, and the close_queue exception would terminate the program.\n It’s not often that you’re going to call wait() on its own, though; most of the time\nyou’ll want to handle a message. This is where the handle() member function d\ncomes in. It’s a template, and the message type isn’t deducible, so you must specify\nwhich message type to handle and pass in a function (or callable object) to handle it.\nhandle() itself passes the queue, the current dispatcher object, and the handler\nfunction to a new instance of the TemplateDispatcher class template, to handle mes-\nsages of the specified type, shown in listing C.5. This is why you test the chained value\nin the destructor before waiting for messages; not only does it prevent moved-from\nobjects waiting for messages, but it also allows you to transfer the responsibility of wait-\ning to your new TemplateDispatcher instance.\nnamespace messaging\n{\n    template<typename PreviousDispatcher,typename Msg,typename Func>\n    class TemplateDispatcher\n    {\n        queue* q;\n        PreviousDispatcher* prev;\n        Func f;\n        bool chained;\n        TemplateDispatcher(TemplateDispatcher const&)=delete;\n        TemplateDispatcher& operator=(TemplateDispatcher const&)=delete;\n        template<typename Dispatcher,typename OtherMsg,typename OtherFunc>\n        friend class TemplateDispatcher;    \n        void wait_and_dispatch()\n        {\n            for(;;)\n            {\n                auto msg=q->wait_and_pop();\n                if(dispatch(msg))            \n                    break;\n            }\n        }\n        bool dispatch(std::shared_ptr<message_base> const& msg)\n        {\n            if(wrapped_message<Msg>* wrapper=\n                dynamic_cast<wrapped_message<Msg>*>(msg.get()))   \n            {\n                f(wrapper->contents);    \n                return true;\n            }\n            else\n            {\n                return prev->dispatch(msg);    \n            }\n        }\nListing C.5\nThe TemplateDispatcher class template\nTemplateDispatcher \ninstantiations are \nfriends of each other.\nIf you handle the message, \nbreak out of the loop.\nB\nCheck the message type\nand call the function. c\nChain to the \nprevious dispatcher.\nd\n",
      "content_length": 2460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "389\n    public:\n        TemplateDispatcher(TemplateDispatcher&& other):\n            q(other.q),prev(other.prev),f(std::move(other.f)),\n            chained(other.chained)\n        {\n            other.chained=true;\n        }\n        TemplateDispatcher(queue* q_,PreviousDispatcher* prev_,Func&& f_):\n            q(q_),prev(prev_),f(std::forward<Func>(f_)),chained(false)\n        {\n            prev_->chained=true;\n        }\n        template<typename OtherMsg,typename OtherFunc>\n        TemplateDispatcher<TemplateDispatcher,OtherMsg,OtherFunc>\n        handle(OtherFunc&& of)                        \n        {\n            return TemplateDispatcher<\n                TemplateDispatcher,OtherMsg,OtherFunc>(\n                    q,this,std::forward<OtherFunc>(of));\n        }\n        ~TemplateDispatcher() noexcept(false)   \n        {\n            if(!chained)\n            {\n                wait_and_dispatch();\n            }\n        }\n    };\n}\nThe TemplateDispatcher<> class template is modeled on the dispatcher class and is\nalmost identical. In particular, the destructor still calls wait_and_dispatch() to wait\nfor a message.\n Because you don’t throw exceptions if you handle the message, you now need to\ncheck whether you did handle the message in your message loop B. Your message\nprocessing stops when you’ve successfully handled a message, so that you can wait for\na different set of messages next time. If you do get a match for the specified message\ntype, the supplied function is called c rather than throwing an exception (although\nthe handler function may throw an exception itself). If you don’t get a match, you\nchain to the previous dispatcher d. In the first instance, this will be a dispatcher, but\nif you chain calls to handle() e to allow multiple types of messages to be handled, this\nmay be a prior instantiation of TemplateDispatcher<>, which will in turn chain to the\nprevious handler if the message doesn’t match. Because any of the handlers might\nthrow an exception (including the dispatcher’s default handler for close_queue mes-\nsages), the destructor must once again be declared noexcept(false) f.\n This simple framework allows you to push any type of message on the queue and\nthen selectively match against messages you can handle on the receiving end. It also\nallows you to pass around a reference to the queue for pushing messages on, while\nkeeping the receiving end private.\nAdditional handlers \ncan be chained.\ne\nThe destructor is \nnoexcept(false) \nagain.\nf\n",
      "content_length": 2484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "390\nAPPENDIX C\nA message-passing framework and complete ATM example\n To complete the example from chapter 4, the messages are given in listing C.6, the\nvarious state machines in listings C.7, C.8, and C.9, and the driving code in listing C.10.\nstruct withdraw\n{\n    std::string account;\n    unsigned amount;\n    mutable messaging::sender atm_queue;\n    withdraw(std::string const& account_,\n             unsigned amount_,\n             messaging::sender atm_queue_):\n        account(account_),amount(amount_),\n        atm_queue(atm_queue_)\n    {}\n};\nstruct withdraw_ok\n{};\nstruct withdraw_denied\n{};\nstruct cancel_withdrawal\n{\n    std::string account;\n    unsigned amount;\n    cancel_withdrawal(std::string const& account_,\n                      unsigned amount_):\n        account(account_),amount(amount_)\n    {}\n};\nstruct withdrawal_processed\n{\n    std::string account;\n    unsigned amount;\n    withdrawal_processed(std::string const& account_,\n                         unsigned amount_):\n        account(account_),amount(amount_)\n    {}\n};\nstruct card_inserted\n{\n    std::string account;\n    explicit card_inserted(std::string const& account_):\n        account(account_)\n    {}\n    \n};\nstruct digit_pressed\n{\n    char digit;\n    explicit digit_pressed(char digit_):\n        digit(digit_)\n    {}\n    \n};\nListing C.6\nATM messages\n",
      "content_length": 1330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "391\nstruct clear_last_pressed\n{};\nstruct eject_card\n{};\nstruct withdraw_pressed\n{\n    unsigned amount;\n    explicit withdraw_pressed(unsigned amount_):\n        amount(amount_)\n    {}\n    \n};\nstruct cancel_pressed\n{};\nstruct issue_money\n{\n    unsigned amount;\n    issue_money(unsigned amount_):\n        amount(amount_)\n    {}\n};\nstruct verify_pin\n{\n    std::string account;\n    std::string pin;\n    mutable messaging::sender atm_queue;\n    verify_pin(std::string const& account_,std::string const& pin_,\n               messaging::sender atm_queue_):\n        account(account_),pin(pin_),atm_queue(atm_queue_)\n    {}\n};\nstruct pin_verified\n{};\nstruct pin_incorrect\n{};\nstruct display_enter_pin\n{};\nstruct display_enter_card\n{};\nstruct display_insufficient_funds\n{};\nstruct display_withdrawal_cancelled\n{};\nstruct display_pin_incorrect_message\n{};\nstruct display_withdrawal_options\n{};\nstruct get_balance\n{\n    std::string account;\n    mutable messaging::sender atm_queue;\n    get_balance(std::string const& account_,messaging::sender atm_queue_):\n        account(account_),atm_queue(atm_queue_)\n    {}\n};\n",
      "content_length": 1102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "392\nAPPENDIX C\nA message-passing framework and complete ATM example\nstruct balance\n{\n    unsigned amount;\n    \n    explicit balance(unsigned amount_):\n        amount(amount_)\n    {}\n};\nstruct display_balance\n{\n    unsigned amount;\n    explicit display_balance(unsigned amount_):\n        amount(amount_)\n    {}\n};\nstruct balance_pressed\n{};\nclass atm\n{\n    messaging::receiver incoming;\n    messaging::sender bank;\n    messaging::sender interface_hardware;\n    void (atm::*state)();\n    std::string account;\n    unsigned withdrawal_amount;\n    std::string pin;\n    void process_withdrawal()\n    {\n        incoming.wait()\n            .handle<withdraw_ok>(\n                [&](withdraw_ok const& msg)\n                {\n                    interface_hardware.send(\n                        issue_money(withdrawal_amount));\n                    bank.send(\n                        withdrawal_processed(account,withdrawal_amount));\n                    state=&atm::done_processing;\n                }\n                )\n            .handle<withdraw_denied>(\n                [&](withdraw_denied const& msg)\n                {\n                    interface_hardware.send(display_insufficient_funds());\n                    state=&atm::done_processing;\n                }\n                )\n            .handle<cancel_pressed>(\n                [&](cancel_pressed const& msg)\n                {\n                    bank.send(\n                        cancel_withdrawal(account,withdrawal_amount));\n                    interface_hardware.send(\nListing C.7\nThe ATM state machine\n",
      "content_length": 1555,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "393\n                        display_withdrawal_cancelled());\n                    state=&atm::done_processing;\n                }\n                );\n    }\n    void process_balance()\n    {\n        incoming.wait()\n            .handle<balance>(\n                [&](balance const& msg)\n                {\n                    interface_hardware.send(display_balance(msg.amount));\n                    state=&atm::wait_for_action;\n                }\n                )\n            .handle<cancel_pressed>(\n                [&](cancel_pressed const& msg)\n                {\n                    state=&atm::done_processing;\n                }\n                );\n    }\n    void wait_for_action()\n    {\n        interface_hardware.send(display_withdrawal_options());\n        incoming.wait()\n            .handle<withdraw_pressed>(\n                [&](withdraw_pressed const& msg)\n                {\n                    withdrawal_amount=msg.amount;\n                    bank.send(withdraw(account,msg.amount,incoming));\n                    state=&atm::process_withdrawal;\n                }\n                )\n            .handle<balance_pressed>(\n                [&](balance_pressed const& msg)\n                {\n                    bank.send(get_balance(account,incoming));\n                    state=&atm::process_balance;\n                }\n                )\n            .handle<cancel_pressed>(\n                [&](cancel_pressed const& msg)\n                {\n                    state=&atm::done_processing;\n                }\n                );\n    }\n    void verifying_pin()\n    {\n        incoming.wait()\n            .handle<pin_verified>(\n                [&](pin_verified const& msg)\n                {\n                    state=&atm::wait_for_action;\n",
      "content_length": 1732,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "394\nAPPENDIX C\nA message-passing framework and complete ATM example\n                }\n                )\n            .handle<pin_incorrect>(\n                [&](pin_incorrect const& msg)\n                {\n                    interface_hardware.send(\n                        display_pin_incorrect_message());\n                    state=&atm::done_processing;\n                }\n                )\n            .handle<cancel_pressed>(\n                [&](cancel_pressed const& msg)\n                {\n                    state=&atm::done_processing;\n                }\n                );\n    }\n    void getting_pin()\n    {\n        incoming.wait()\n            .handle<digit_pressed>(\n                [&](digit_pressed const& msg)\n                {\n                    unsigned const pin_length=4;\n                    pin+=msg.digit;\n                    if(pin.length()==pin_length)\n                    {\n                        bank.send(verify_pin(account,pin,incoming));\n                        state=&atm::verifying_pin;\n                    }\n                }\n                )\n            .handle<clear_last_pressed>(\n                [&](clear_last_pressed const& msg)\n                {\n                    if(!pin.empty())\n                    {\n                        pin.pop_back();\n                    }\n                }\n                )\n            .handle<cancel_pressed>(\n                [&](cancel_pressed const& msg)\n                {\n                    state=&atm::done_processing;\n                }\n                );\n    }\n    void waiting_for_card()\n    {\n        interface_hardware.send(display_enter_card());\n        incoming.wait()\n            .handle<card_inserted>(\n                [&](card_inserted const& msg)\n                {\n",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "395\n                    account=msg.account;\n                    pin=\"\";\n                    interface_hardware.send(display_enter_pin());\n                    state=&atm::getting_pin;\n                }\n                );\n    }\n    void done_processing()\n    {\n        interface_hardware.send(eject_card());\n        state=&atm::waiting_for_card;\n    }\n    atm(atm const&)=delete;\n    atm& operator=(atm const&)=delete;\npublic:\n    atm(messaging::sender bank_,\n        messaging::sender interface_hardware_):\n        bank(bank_),interface_hardware(interface_hardware_)\n    {}\n    void done()\n    {\n        get_sender().send(messaging::close_queue());\n    }\n    void run()\n    {\n        state=&atm::waiting_for_card;\n        try\n        {\n            for(;;)\n            {\n                (this->*state)();\n            }\n        }\n        catch(messaging::close_queue const&)\n        {\n        }\n    }\n    messaging::sender get_sender()\n    {\n        return incoming;\n    }\n};\nclass bank_machine\n{\n    messaging::receiver incoming;\n    unsigned balance;\npublic:\n    bank_machine():\n        balance(199)\n    {}\n    void done()\n    {\nListing C.8\nThe bank state machine\n",
      "content_length": 1164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "396\nAPPENDIX C\nA message-passing framework and complete ATM example\n        get_sender().send(messaging::close_queue());\n    }\n    void run()\n    {\n        try\n        {\n            for(;;)\n            {\n                incoming.wait()\n                    .handle<verify_pin>(\n                        [&](verify_pin const& msg)\n                        {\n                            if(msg.pin==\"1937\")\n                            {\n                                msg.atm_queue.send(pin_verified());\n                            }\n                            else\n                            {\n                                msg.atm_queue.send(pin_incorrect());\n                            }\n                        }\n                        )\n                    .handle<withdraw>(\n                        [&](withdraw const& msg)\n                        {\n                            if(balance>=msg.amount)\n                            {\n                                msg.atm_queue.send(withdraw_ok());\n                                balance-=msg.amount;\n                            }\n                            else\n                            {\n                                msg.atm_queue.send(withdraw_denied());\n                            }\n                        }\n                        )\n                    .handle<get_balance>(\n                        [&](get_balance const& msg)\n                        {\n                            msg.atm_queue.send(::balance(balance));\n                        }\n                        )\n                    .handle<withdrawal_processed>(\n                        [&](withdrawal_processed const& msg)\n                        {\n                        }\n                        )\n                    .handle<cancel_withdrawal>(\n                        [&](cancel_withdrawal const& msg)\n                        {\n                        }\n                        );\n            }\n        }\n        catch(messaging::close_queue const&)\n",
      "content_length": 1990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "397\n        {\n        }\n    }\n    \n    messaging::sender get_sender()\n    {\n        return incoming;\n    }\n};\nclass interface_machine\n{\n    messaging::receiver incoming;\npublic:\n    void done()\n    {\n        get_sender().send(messaging::close_queue());\n    }\n    void run()\n    {\n        try\n        {\n            for(;;)\n            {\n                incoming.wait()\n                    .handle<issue_money>(\n                        [&](issue_money const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Issuing \"\n                                         <<msg.amount<<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_insufficient_funds>(\n                        [&](display_insufficient_funds const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Insufficient funds\"<<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_enter_pin>(\n                        [&](display_enter_pin const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout\n                                    <<\"Please enter your PIN (0-9)\"\n                                    <<std::endl;\n                            }\nListing C.9\nThe user-interface state machine\n",
      "content_length": 1661,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "398\nAPPENDIX C\nA message-passing framework and complete ATM example\n                        }\n                        )\n                    .handle<display_enter_card>(\n                        [&](display_enter_card const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Please enter your card (I)\"\n                                         <<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_balance>(\n                        [&](display_balance const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout\n                                    <<\"The balance of your account is \"\n                                    <<msg.amount<<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_withdrawal_options>(\n                        [&](display_withdrawal_options const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Withdraw 50? (w)\"<<std::endl;\n                                std::cout<<\"Display Balance? (b)\"\n                                         <<std::endl;\n                                std::cout<<\"Cancel? (c)\"<<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_withdrawal_cancelled>(\n                        [&](display_withdrawal_cancelled const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Withdrawal cancelled\"\n                                         <<std::endl;\n                            }\n                        }\n                        )\n                    .handle<display_pin_incorrect_message>(\n                        [&](display_pin_incorrect_message const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"PIN incorrect\"<<std::endl;\n                            }\n                        }\n                        )\n                    .handle<eject_card>(\n",
      "content_length": 2543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "399\n                        [&](eject_card const& msg)\n                        {\n                            {\n                                std::lock_guard<std::mutex> lk(iom);\n                                std::cout<<\"Ejecting card\"<<std::endl;\n                            }\n                        }\n                        );\n            }\n        }\n        catch(messaging::close_queue&)\n        {\n        }\n    }\n    messaging::sender get_sender()\n    {\n        return incoming;\n    }    \n};\nint main()\n{\n    bank_machine bank;\n    interface_machine interface_hardware;\n    atm machine(bank.get_sender(),interface_hardware.get_sender());\n    std::thread bank_thread(&bank_machine::run,&bank);\n    std::thread if_thread(&interface_machine::run,&interface_hardware);\n    std::thread atm_thread(&atm::run,&machine);\n    messaging::sender atmqueue(machine.get_sender());\n    bool quit_pressed=false;\n    while(!quit_pressed)\n    {\n        char c=getchar();\n        switch(c)\n        {\n        case '0':\n        case '1':\n        case '2':\n        case '3':\n        case '4':\n        case '5':\n        case '6':\n        case '7':\n        case '8':\n        case '9':\n            atmqueue.send(digit_pressed(c));\n            break;\n        case 'b':\n            atmqueue.send(balance_pressed());\n            break;\n        case 'w':\n            atmqueue.send(withdraw_pressed(50));\n            break;\nListing C.10\nThe driving code\n",
      "content_length": 1434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "400\nAPPENDIX C\nA message-passing framework and complete ATM example\n        case 'c':\n            atmqueue.send(cancel_pressed());\n            break;\n        case 'q':\n            quit_pressed=true;\n            break;\n        case 'i':\n            atmqueue.send(card_inserted(\"acc1234\"));\n            break;\n        }\n    }\n    bank.done();\n    machine.done();\n    interface_hardware.done();\n    atm_thread.join();\n    bank_thread.join();\n    if_thread.join();\n}\n",
      "content_length": 463,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "401\nappendix D\nC++ Thread\nLibrary reference\nD.1\nThe <chrono> header \nThe <chrono> header provides classes for representing points in time, durations,\nand clock classes, which act as a source of time_points. Each clock has an is_steady\nstatic data member, which indicates whether it’s a steady clock that advances at a\nuniform rate (and can’t be adjusted). The std::chrono::steady_clock class is the\nonly clock guaranteed to be steady. \nHeader contents\nnamespace std\n{\n   namespace chrono\n   {\n       template<typename Rep,typename Period = ratio<1>>\n       class duration;\n       template<\n           typename Clock,\n           typename Duration = typename Clock::duration>\n       class time_point;\n       class system_clock;\n       class steady_clock;\n       typedef unspecified-clock-type high_resolution_clock;\n   }\n}\nD.1.1\nstd::chrono::duration class template \nThe std::chrono::duration class template provides a facility for representing\ndurations. The template parameters Rep and Period are the data type to store the\nduration value and an instantiation of the std::ratio class template indicating the\n",
      "content_length": 1108,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "402\nAPPENDIX D\nC++ Thread Library reference\nlength of time (as a fraction of a second) between successive “ticks,” respectively. Thus\nstd::chrono::duration<int, std::milli> is a count of milliseconds stored in a value\nof type int, whereas std::chrono::duration<short, std::ratio<1,50>> is a count of\nfiftieths of a second stored in a value of type short, and std::chrono:: d-uration\n<long long, std::ratio<60,1>> is a count of minutes stored in a value of type long\nlong. \nClass definition\ntemplate <class Rep, class Period=ratio<1> >\nclass duration\n{\npublic:\n    typedef Rep rep;\n    typedef Period period;\n    constexpr duration() = default;\n    ~duration() = default;\n    duration(const duration&) = default;\n    duration& operator=(const duration&) = default;\n    template <class Rep2>\n    constexpr explicit duration(const Rep2& r);\n    template <class Rep2, class Period2>\n    constexpr duration(const duration<Rep2, Period2>& d);\n    constexpr rep count() const;\n    constexpr duration operator+() const;\n    constexpr duration operator-() const;\n    duration& operator++();\n    duration operator++(int);\n    duration& operator--();\n    duration operator--(int);\n    duration& operator+=(const duration& d);\n    duration& operator-=(const duration& d);\n    duration& operator*=(const rep& rhs);\n    duration& operator/=(const rep& rhs);\n    duration& operator%=(const rep& rhs);\n    duration& operator%=(const duration& rhs);\n    static constexpr duration zero();\n    static constexpr duration min();\n    static constexpr duration max();\n};\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator==(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator!=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\n",
      "content_length": 1894,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "403\nThe <chrono> header\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator<(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator<=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator>(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator>=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\ntemplate <class ToDuration, class Rep, class Period>\nconstexpr ToDuration duration_cast(const duration<Rep, Period>& d);\nRequirements\nRep must be a built-in numeric type, or a number-like user-defined type. Period\nmust be an instantiation of std::ratio<>.\nSTD::CHRONO::DURATION::REP TYPEDEF \nThis is a typedef for the type used to hold the number of ticks in a duration value. \nDeclaration\ntypedef Rep rep;\nrep is the type of value used to hold the internal representation of the duration\nobject.\nSTD::CHRONO::DURATION::PERIOD TYPEDEF \nThis typedef is for an instantiation of the std::ratio class template that specifies the\nfraction of a second represented by the duration count. For example, if period is\nstd::ratio<1,50>, a duration value with a count() of N represents N fiftieths of\na second.\nDeclaration\ntypedef Period period;\nSTD::CHRONO::DURATION DEFAULT CONSTRUCTOR\nConstructs an std::chrono::duration instance with a default value.\nDeclaration\nconstexpr duration() = default;\nEffects\nThe internal value of the duration (of type rep) is default initialized.\n",
      "content_length": 1781,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "404\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::DURATION CONVERTING CONSTRUCTOR FROM A COUNT VALUE\nConstructs an std::chrono::duration instance with a specified count.\nDeclaration\ntemplate <class Rep2>\nconstexpr explicit duration(const Rep2& r);\nEffects\nThe internal value of the duration object is initialized with static_cast<rep>(r).\nRequirements\nThis constructor only participates in overload resolution if Rep2 is implicitly convert-\nible to Rep and either Rep is a floating point type or Rep2 is not a floating point type.\nPostcondition\nthis->count()==static_cast<rep>(r)\nSTD::CHRONO::DURATION CONVERTING CONSTRUCTOR FROM ANOTHER STD::CHRONO::DURATION VALUE\nConstructs an std::chrono::duration instance by scaling the count value of another\nstd::chrono::duration object.\nDeclaration\ntemplate <class Rep2, class Period2>\nconstexpr duration(const duration<Rep2,Period2>& d);\nEffects\nThe internal value of the duration object is initialized with duration_cast<duration\n<Rep,Period>>(d).count().\nRequirements\nThis constructor only participates in overload resolution if Rep is a floating point\ntype or Rep2 is not a floating point type and Period2 is a whole number multiple of\nPeriod (that is, ratio_divide<Period2,Period>::den==1). This avoids accidental\ntruncation (and corresponding loss of precision) from storing a duration with\nsmall periods in a variable representing a duration with a longer period.\nPostcondition\nthis->count()==duration_cast<duration<Rep,Period>>(d).count()\nExamples\nduration<int,ratio<1,1000>> ms(5);  \nduration<int,ratio<1,1>> s(ms);                   \nduration<double,ratio<1,1>> s2(ms);                \nduration<int,ratio<1,1000000>> us(ms);    \nSTD::CHRONO::DURATION::COUNT MEMBER FUNCTION\nRetrieves the value of the duration.\nDeclaration\nconstexpr rep count() const;\nReturns\nThe internal value of the duration object, as a value of type rep.\nFive milliseconds\nError: can’t store ms \nas integral seconds\nOK: s2.count()==0.005\nOK: us.count()==5000\n",
      "content_length": 1990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "405\nThe <chrono> header\nSTD::CHRONO::DURATION::OPERATOR+ UNARY PLUS OPERATOR\nThis is a no-op: it just returns a copy of *this.\nDeclaration\nconstexpr duration operator+() const;\nReturns\n*this\nSTD::CHRONO::DURATION::OPERATOR- UNARY MINUS OPERATOR\nReturns a duration such that the count() value is the negative value of this->\ncount().\nDeclaration\nconstexpr duration operator-() const;\nReturns\nduration(-this->count());\nSTD::CHRONO::DURATION::OPERATOR++ PRE-INCREMENT OPERATOR\nIncrements the internal count.\nDeclaration\nduration& operator++();\nEffects\n++this->internal_count;\nReturns\n*this\nSTD::CHRONO::DURATION::OPERATOR++ POST-INCREMENT OPERATOR\nIncrements the internal count and returns the value of *this prior to the increment.\nDeclaration\nduration operator++(int);\nEffects\nduration temp(*this);\n++(*this);\nreturn temp;\nSTD::CHRONO::DURATION::OPERATOR-- PRE-DECREMENT OPERATOR\nDecrements the internal count.\nDeclaration\nduration& operator--();\nEffects\n--this->internal_count;\nReturns\n*this\n",
      "content_length": 992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "406\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::DURATION::OPERATOR-- POST-DECREMENT OPERATOR\nDecrements the internal count and returns the value of *this prior to the decrement.\nDeclaration\nduration operator--(int);\nEffects\nduration temp(*this);\n--(*this);\nreturn temp;\nSTD::CHRONO::DURATION::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR\nAdds the count for another duration object to the internal count for *this.\nDeclaration\nduration& operator+=(duration const& other);\nEffects\ninternal_count+=other.count();\nReturns \n*this\nSTD::CHRONO::DURATION::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR\nSubtracts the count for another duration object from the internal count for *this.\nDeclaration\nduration& operator-=(duration const& other);\nEffects\ninternal_count-=other.count();\nReturns \n*this\nSTD::CHRONO::DURATION::OPERATOR*= COMPOUND ASSIGNMENT OPERATOR\nMultiplies the internal count for *this by the specified value.\nDeclaration\nduration& operator*=(rep const& rhs);\nEffects\ninternal_count*=rhs;\nReturns \n*this\nSTD::CHRONO::DURATION::OPERATOR/= COMPOUND ASSIGNMENT OPERATOR\nDivides the internal count for *this by the specified value.\nDeclaration\nduration& operator/=(rep const& rhs);\n",
      "content_length": 1181,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "407\nThe <chrono> header\nEffects\ninternal_count/=rhs;\nReturns \n*this\nSTD::CHRONO::DURATION::OPERATOR%= COMPOUND ASSIGNMENT OPERATOR\nAdjusts the internal count for *this to be the remainder when divided by the speci-\nfied value.\nDeclaration\nduration& operator%=(rep const& rhs);\nEffects\ninternal_count%=rhs;\nReturns \n*this\nSTD::CHRONO::DURATION::OPERATOR%= COMPOUND ASSIGNMENT OPERATOR\nAdjusts the internal count for *this to be the remainder when divided by the count of\nthe other duration object.\nDeclaration\nduration& operator%=(duration const& rhs);\nEffects\ninternal_count%=rhs.count();\nReturns \n*this\nSTD::CHRONO::DURATION::ZERO STATIC MEMBER FUNCTION\nReturns a duration object representing a value of zero.\nDeclaration\nconstexpr duration zero();\nReturns \nduration(duration_values<rep>::zero());\nSTD::CHRONO::DURATION::MIN STATIC MEMBER FUNCTION\nReturns a duration object holding the minimum possible value for the specified\ninstantiation.\nDeclaration\nconstexpr duration min();\nReturns \nduration(duration_values<rep>::min());\n",
      "content_length": 1029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "408\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::DURATION::MAX STATIC MEMBER FUNCTION\nReturns a duration object holding the maximum possible value for the specified\ninstantiation.\nDeclaration\nconstexpr duration max();\nReturns \nduration(duration_values<rep>::max());\nSTD::CHRONO::DURATION EQUALITY COMPARISON OPERATOR\nCompares two duration objects for equality, even if they have distinct representations\nand/or periods.\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator==(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly convert to the other, the expression is ill-formed. \nEffects\nIf CommonDuration is a synonym for std::common_type< duration< Rep1, Period1>,\nduration< Rep2, Period2>>::type, then lhs==rhs returns CommonDuration(lhs)\n.count()==CommonDuration(rhs).count().\nSTD::CHRONO::DURATION INEQUALITY COMPARISON OPERATOR\nCompares two duration objects for inequality, even if they have distinct representa-\ntions and/or periods.\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator!=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly convert to the other, the expression is ill-formed. \nReturns\n!(lhs==rhs)\nSTD::CHRONO::DURATION LESS-THAN COMPARISON OPERATOR\nCompares two duration objects to see if one is less than the other, even if they have\ndistinct representations and/or periods.\n",
      "content_length": 1870,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "409\nThe <chrono> header\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator<(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly converted to the other, the expression is ill-formed. \nEffects\nIf CommonDuration is a synonym for std::common_type< duration< Rep1, Period1>,\nduration< Rep2, Period2>>::type, then lhs<rhs returns CommonDuration(lhs)\n.count()<CommonDuration(rhs).count().\nSTD::CHRONO::DURATION GREATER-THAN COMPARISON OPERATOR\nCompares two duration objects to see if one is greater than the other, even if they\nhave distinct representations and/or periods.\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator>(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly convert to the other, the expression is ill-formed. \nReturns\nrhs<lhs\nSTD::CHRONO::DURATION LESS-THAN-OR-EQUALS COMPARISON OPERATOR\nCompares two duration objects to see if one is less than or equal to the other, even if\nthey have distinct representations and/or periods.\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator<=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly convert to the other, the expression is ill-formed. \nReturns\n!(rhs<lhs)\n",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "410\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::DURATION GREATER-THAN-OR-EQUALS COMPARISON OPERATOR\nCompares two duration objects to see if one is greater than or equal to the other,\neven if they have distinct representations and/or periods.\nDeclaration\ntemplate <class Rep1, class Period1, class Rep2, class Period2>\nconstexpr bool operator>=(\n    const duration<Rep1, Period1>& lhs, \n    const duration<Rep2, Period2>& rhs);\nRequirements\nEither lhs must be implicitly convertible to rhs, or vice versa. If neither can be\nimplicitly converted to the other, or they are distinct instantiations of duration but\neach can implicitly convert to the other, the expression is ill-formed. \nReturns\n!(lhs<rhs)\nSTD::CHRONO::DURATION_CAST NONMEMBER FUNCTION\nExplicitly converts an std::chrono::duration object to a specific std::chrono::\nduration instantiation.\nDeclaration\ntemplate <class ToDuration, class Rep, class Period>\nconstexpr ToDuration duration_cast(const duration<Rep, Period>& d);\nRequirements\nToDuration must be an instantiation of std::chrono::duration. \nReturns\nThe duration, d converted to the duration type specified by ToDuration. This is\ndone in such a way as to minimize any loss of precision resulting from conversions\nbetween different scales and representation types.\nD.1.2\nstd::chrono::time_point class template \nThe std::chrono::time_point class template represents a point in time, as measured by\na particular clock. It’s specified as a duration since the epoch of that particular clock. The\ntemplate parameter Clock identifies the clock (each distinct clock must have a unique\ntype), whereas the Duration template parameter is the type to use for measuring the\nduration since the epoch and must be an instantiation of the std::chrono::duration\nclass template. The Duration defaults to the default duration type of the Clock. \nClass definition\ntemplate <class Clock,class Duration = typename Clock::duration>\nclass time_point\n{\npublic:\n    typedef Clock clock;\n    typedef Duration duration;\n    typedef typename duration::rep rep;\n    typedef typename duration::period period;\n",
      "content_length": 2105,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": "411\nThe <chrono> header\n    time_point();\n    explicit time_point(const duration& d);\n    template <class Duration2>\n    time_point(const time_point<clock, Duration2>& t);\n    duration time_since_epoch() const;\n    time_point& operator+=(const duration& d);\n    time_point& operator-=(const duration& d);\n    static constexpr time_point min();\n    static constexpr time_point max();\n};\nSTD::CHRONO::TIME_POINT DEFAULT CONSTRUCTOR\nConstructs a time_point representing the epoch of the associated Clock; the internal\nduration is initialized with Duration::zero().\nDeclaration\ntime_point();\nPostcondition\nFor a newly default-constructed time_point object, tp, tp.time_since_epoch() ==\ntp::duration::zero(). \nSTD::CHRONO::TIME_POINT DURATION CONSTRUCTOR\nConstructs a time_point representing the specified duration since the epoch of the\nassociated Clock.\nDeclaration\nexplicit time_point(const duration& d);\nPostcondition\nFor a time_point object, tp, constucted with tp(d) for some duration, d, tp.time_\nsince_epoch()==d.\nSTD::CHRONO::TIME_POINT CONVERSION CONSTRUCTOR\nConstructs a time_point object from another time_point object with the same Clock\nbut a distinct Duration.\nDeclaration\ntemplate <class Duration2>\ntime_point(const time_point<clock, Duration2>& t);\nRequirements\nDuration2 shall be implicitly convertible to Duration.\nEffects\nAs-if time_point(t.time_since_epoch())\nThe value returned from t.time_since_epoch() is implicitly converted to an\nobject of the Duration type, and that value is stored in the newly constructed time_\npoint object.\n",
      "content_length": 1550,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "412\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::TIME_POINT::TIME_SINCE_EPOCH MEMBER FUNCTION\nRetrieves the duration since the clock epoch for a particular time_point object.\nDeclaration\nduration time_since_epoch() const;\nReturns\nThe duration value stored in *this.\nSTD::CHRONO::TIME_POINT::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR\nAdds the specified duration to the value stored in the specified time_point object.\nDeclaration\ntime_point& operator+=(const duration& d);\nEffects\nAdds d to the internal duration object of *this, as-if\nthis->internal_duration += d;\nReturns\n*this\nSTD::CHRONO::TIME_POINT::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR\nSubtracts the specified duration from the value stored in the specified time_point\nobject.\nDeclaration\ntime_point& operator-=(const duration& d);\nEffects\nSubtracts d from the internal duration object of *this, as-if\nthis->internal_duration -= d;\nReturns\n*this\nSTD::CHRONO::TIME_POINT::MIN STATIC MEMBER FUNCTION\nObtains a time_point object representing the minimum possible value for its type.\nDeclaration\nstatic constexpr time_point min();\nReturns\ntime_point(time_point::duration::min()) (see 11.1.1.15)\nSTD::CHRONO::TIME_POINT::MAX STATIC MEMBER FUNCTION\nObtains a time_point object representing the maximum possible value for its type.\nDeclaration\nstatic constexpr time_point max();\nReturns\ntime_point(time_point::duration::max()) (see 11.1.1.16)\n",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "413\nThe <chrono> header\nD.1.3\nstd::chrono::system_clock class \nThe std::chrono::system_clock class provides a means of obtaining the current\nwall-clock time from the system-wide real-time clock. The current time can be obtained\nby calling std::chrono::system_clock::now(). Instances of std::chrono::system_\nclock::time_point can be converted to and from time_t with the std::chrono::\nsystem_clock::to_time_t() and std::chrono::system_clock::to_time_point()\nfunctions. The system clock isn’t steady, so a subsequent call to std::chrono::system_\nclock::now() may return an earlier time than a previous call (for example, if the\noperating system clock is manually adjusted or synchronized with an external clock). \nClass definition\nclass system_clock\n{\npublic:\n    typedef unspecified-integral-type rep;\n    typedef std::ratio<unspecified,unspecified> period;\n    typedef std::chrono::duration<rep,period> duration;\n    typedef std::chrono::time_point<system_clock> time_point;\n    static const bool is_steady=unspecified;\n    static time_point now() noexcept;\n    static time_t to_time_t(const time_point& t) noexcept;\n    static time_point from_time_t(time_t t) noexcept;\n};\nSTD::CHRONO::SYSTEM_CLOCK::REP TYPEDEF \nA typedef for an integral type used to hold the number of ticks in a duration value. \nDeclaration\ntypedef unspecified-integral-type rep;\nSTD::CHRONO::SYSTEM_CLOCK::PERIOD TYPEDEF \nA typedef for an instantiation of the std::ratio class template that specifies the\nsmallest number of seconds (or fractions of a second) between distinct values of\nduration or time_point. The period specifies the precision of the clock, not the tick\nfrequency. \nDeclaration\ntypedef std::ratio<unspecified,unspecified> period;\nSTD::CHRONO::SYSTEM_CLOCK::DURATION TYPEDEF \nAn instantiation of the std::chrono::duration class template that can hold the dif-\nference between any two time points returned by the system-wide real-time clock. \nDeclaration\ntypedef std::chrono::duration<\n    std::chrono::system_clock::rep,\n    std::chrono::system_clock::period> duration;\n",
      "content_length": 2059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "414\nAPPENDIX D\nC++ Thread Library reference\nSTD::CHRONO::SYSTEM_CLOCK::TIME_POINT TYPEDEF \nAn instantiation of the std::chrono::time_point class template that can hold time\npoints returned by the system-wide real-time clock. \nDeclaration\ntypedef std::chrono::time_point<std::chrono::system_clock> time_point;\nSTD::CHRONO::SYSTEM_CLOCK::NOW STATIC MEMBER FUNCTION \nObtains the current wall-clock time from the system-wide real-time clock. \nDeclaration\ntime_point now() noexcept;\nReturns\nA time_point representing the current time of the system-wide real-time clock. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::CHRONO::SYSTEM_CLOCK::TO_TIME_T STATIC MEMBER FUNCTION \nConverts an instance of time_point to time_t. \nDeclaration\ntime_t to_time_t(time_point const& t) noexcept;\nReturns\nA time_t value that represents the same point in time as t, rounded or truncated\nto seconds precision. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::CHRONO::SYSTEM_CLOCK::FROM_TIME_T STATIC MEMBER FUNCTION \nConverts an instance of time_t to time_point. \nDeclaration\ntime_point from_time_t(time_t const& t) noexcept;\nReturns\nA time_point value that represents the same point in time as t. \nThrows\nAn exception of type std::system_error if an error occurs. \nD.1.4\nstd::chrono::steady_clock class \nThe std::chrono::steady_clock class provides access to the system-wide steady clock.\nThe current time can be obtained by calling std::chrono::steady_clock::now().\nThere is no fixed relationship between values returned by std::chrono::steady_\nclock::now() and wall-clock time. A steady clock can’t go backwards, so if one call to\nstd::chrono::steady_clock::now() happens-before another call to std::chrono\n::steady_clock::now(), the second call must return a time point equal to or later\nthan the first. The clock advances at a uniform rate as far as possible. \n",
      "content_length": 1892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "415\nThe <chrono> header\nClass definition\nclass steady_clock\n{\npublic:\n    typedef unspecified-integral-type rep;\n    typedef std::ratio<\n        unspecified,unspecified> period;\n    typedef std::chrono::duration<rep,period> duration;\n    typedef std::chrono::time_point<steady_clock>\n        time_point;\n    static const bool is_steady=true;\n    static time_point now() noexcept;\n};\nSTD::CHRONO::STEADY_CLOCK::REP TYPEDEF \nThis typedef is for an integral type used to hold the number of ticks in a duration\nvalue. \nDeclaration\ntypedef unspecified-integral-type rep;\nSTD::CHRONO::STEADY_CLOCK::PERIOD TYPEDEF \nThis is a typedef for an instantiation of the std::ratio class template that specifies\nthe smallest number of seconds (or fractions of a second) between distinct values of\nduration or time_point. The period specifies the precision of the clock, not the tick\nfrequency. \nDeclaration\ntypedef std::ratio<unspecified,unspecified> period;\nSTD::CHRONO::STEADY_CLOCK::DURATION TYPEDEF \nThis is an instantiation of the std::chrono::duration class template that can hold\nthe difference between any two time points returned by the system-wide steady clock. \nDeclaration\ntypedef std::chrono::duration<\n    std::chrono::steady_clock::rep,\n    std::chrono::steady_clock::period> duration;\nSTD::CHRONO::STEADY_CLOCK::TIME_POINT TYPEDEF \nThis instantiation of the std::chrono::time_point class template can hold time\npoints returned by the system-wide steady clock. \nDeclaration\ntypedef std::chrono::time_point<std::chrono::steady_clock> time_point;\nSTD::CHRONO::STEADY_CLOCK::NOW STATIC MEMBER FUNCTION \nObtains the current time from the system-wide steady clock. \nDeclaration\ntime_point now() noexcept;\n",
      "content_length": 1699,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "416\nAPPENDIX D\nC++ Thread Library reference\nReturns\nA time_point representing the current time of the system-wide steady clock. \nThrows\nAn exception of type std::system_error if an error occurs. \nSynchronization\nIf one call to std::chrono::steady_clock::now() happens-before another, the\ntime_point returned by the first call shall compare less-than or equal-to the time_\npoint returned by the second call. \nD.1.5\nstd::chrono::high_resolution_clock typedef \nThe std::chrono::high_resolution_clock class provides access to the system-wide\nclock with the highest resolution. As for all clocks, the current time can be obtained\nby calling std::chrono::high_resolution_clock::now(). std::chrono::high_\nresolution_clock may be a typedef for the std::chrono::system_clock class or the\nstd::chrono::steady_clock class, or it may be a separate type. \n Although std::chrono::high_resolution_clock has the highest resolution of all\nthe library-supplied clocks, std::chrono::high_resolution_clock::now() still takes\na finite amount of time. You must take care to account for the overhead of calling\nstd::chrono::high_resolution_clock::now() when timing short operations. \nClass definition\nclass high_resolution_clock\n{\npublic:\n    typedef unspecified-integral-type rep;\n    typedef std::ratio<\n        unspecified,unspecified> period;\n    typedef std::chrono::duration<rep,period> duration;\n    typedef std::chrono::time_point<\n        unspecified> time_point;\n    static const bool is_steady=unspecified;\n    static time_point now() noexcept;\n};\nD.2\n<condition_variable> header \nThe <condition_variable> header provides condition variables. These are basic-\nlevel synchronization mechanisms that allow a thread to block until notified that some\ncondition is true or a timeout period has elapsed. \nHeader contents\nnamespace std\n{\n    enum class cv_status { timeout, no_timeout };\n    class condition_variable;\n    class condition_variable_any;\n}\n",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "417\n<condition_variable> header\nD.2.1\nstd::condition_variable class \nThe std::condition_variable class allows a thread to wait for a condition to become\ntrue. Instances of std::condition_variable aren’t CopyAssignable, CopyConstruct-\nible, MoveAssignable, or MoveConstructible. \nClass definition\nclass condition_variable\n{\npublic:\n    condition_variable();\n    ~condition_variable();\n    condition_variable(condition_variable const& ) = delete;\n    condition_variable& operator=(condition_variable const& ) = delete;\n    void notify_one() noexcept;\n    void notify_all() noexcept;\n    void wait(std::unique_lock<std::mutex>& lock);\n    template <typename Predicate>\n    void wait(std::unique_lock<std::mutex>& lock,Predicate pred);\n    template <typename Clock, typename Duration>\n    cv_status wait_until(\n        std::unique_lock<std::mutex>& lock,\n        const std::chrono::time_point<Clock, Duration>& absolute_time);\n    template <typename Clock, typename Duration, typename Predicate>\n    bool wait_until(\n        std::unique_lock<std::mutex>& lock,\n        const std::chrono::time_point<Clock, Duration>& absolute_time,\n        Predicate pred);\n    template <typename Rep, typename Period>\n    cv_status wait_for(\n        std::unique_lock<std::mutex>& lock,\n        const std::chrono::duration<Rep, Period>& relative_time);\n    template <typename Rep, typename Period, typename Predicate>\n    bool wait_for(\n        std::unique_lock<std::mutex>& lock,\n        const std::chrono::duration<Rep, Period>& relative_time,\n        Predicate pred);\n};\nvoid notify_all_at_thread_exit(condition_variable&,unique_lock<mutex>);\nSTD::CONDITION_VARIABLE DEFAULT CONSTRUCTOR \nConstructs an std::condition_variable object. \nDeclaration\ncondition_variable();\nEffects\nConstructs a new std::condition_variable instance. \n",
      "content_length": 1811,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "418\nAPPENDIX D\nC++ Thread Library reference\nThrows\nAn exception of type std::system_error if the condition variable could not be\nconstructed. \nSTD::CONDITION_VARIABLE DESTRUCTOR \nDestroys an std::condition_variable object. \nDeclaration\n~condition_variable();\nPreconditions\nThere are no threads blocked on *this in a call to wait(), wait_for(), or\nwait_until(). \nEffects\nDestroys *this. \nThrows\nNothing. \nSTD::CONDITION_VARIABLE::NOTIFY_ONE MEMBER FUNCTION \nWakes one of the threads currently waiting on a std::condition_variable. \nDeclaration\nvoid notify_one() noexcept;\nEffects\nWakes one of the threads waiting on *this at the point of the call. If there are no\nthreads waiting, the call has no effect. \nThrows\nstd::system_error if the effects can’t be achieved. \nSynchronization \nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE::NOTIFY_ALL MEMBER FUNCTION \nWake all of the threads currently waiting on a std::condition_variable. \nDeclaration\nvoid notify_all() noexcept;\nEffects\nWakes all of the threads waiting on *this at the point of the call. If there are no\nthreads waiting, the call has no effect. \nThrows\nstd::system_error if the effects can’t be achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \n",
      "content_length": 1636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "419\n<condition_variable> header\nSTD::CONDITION_VARIABLE::WAIT MEMBER FUNCTION \nWaits until std::condition_variable is woken by a call to notify_one(), a call to\nnotify_all(), or a spurious wakeup. \nDeclaration\nvoid wait(std::unique_lock<std::mutex>& lock);\nPreconditions\nlock.owns_lock()is true, and the lock is owned by the calling thread. \nEffects\nAtomically unlocks the supplied lock object and block until the thread is woken by\na call to notify_one()or notify_all()by another thread, or the thread is woken\nspuriously. The lock object is locked again before the call to wait() returns. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait(), it’s locked again on exit, even if the function exits via an\nexception. \nNOTE\nThe spurious wakeups mean that a thread calling wait() may wake\neven though no thread has called notify_one() or notify_all(). It’s there-\nfore recommended that the overload of wait() that takes a predicate is used\nin preference where possible. Otherwise, it’s recommended that wait() be\ncalled in a loop that tests the predicate associated with the condition variable. \nSynchronization \nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE::WAIT MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWaits until std::condition_variable is woken by a call to notify_one() or notify_\nall(), and the predicate is true. \nDeclaration\ntemplate<typename Predicate>\nvoid wait(std::unique_lock<std::mutex>& lock,Predicate pred);\nPreconditions\nThe expression pred() shall be valid and shall return a value convertible to bool.\nlock.owns_lock() shall be true, and the lock shall be owned by the thread calling\nwait(). \nEffects\nAs-if \nwhile(!pred())\n{\n    wait(lock);\n}\n",
      "content_length": 1951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": "420\nAPPENDIX D\nC++ Thread Library reference\nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects couldn’t\nbe achieved. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex ref-\nerenced by lock locked, and the function shall return if (and only if) an eval-\nuation of (bool)pred() returns true. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for() and wait_until() on a\nsingle std::condition_variable instance are serialized. A call to notify_one() or\nnotify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE::WAIT_FOR MEMBER FUNCTION \nWaits until std::condition_variable is notified by a call to notify_one() or noti-\nfy_all(), or until a specified time period has elapsed or the thread is woken spuri-\nously. \nDeclaration\ntemplate<typename Rep,typename Period>\ncv_status wait_for(\n    std::unique_lock<std::mutex>& lock,\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nlock.owns_lock() is true, and the lock is owned by the calling thread. \nEffects\nAtomically unlocks the supplied lock object and block until the thread is woken by\na call to notify_one() or notify_all() by another thread, or the time period\nspecified by relative_time has elapsed, or the thread is woken spuriously. The\nlock object is locked again before the call to wait_for() returns. \nReturns\nstd::cv_status::no_timeout if the thread was woken by a call to notify_one(), a\ncall to notify_all(), or a spurious wakeup, std::cv_status::timeout otherwise. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait_for(), it’s locked again on exit, even if the function exits\nvia an exception. \nNOTE\nThe spurious wakeups mean that a thread calling wait_for() may\nwake even though no thread has called notify_one() or notify_all(). It’s\ntherefore recommended that the overload of wait_for() that takes a predi-\ncate is used in preference where possible. Otherwise, it’s recommended that\nwait_for() be called in a loop that tests the predicate associated with the con-\ndition variable. Care must be taken when doing this to ensure that the timeout\nis still valid; wait_until() may be more appropriate in many circumstances.\n",
      "content_length": 2363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": "421\n<condition_variable> header\nThe thread may be blocked for longer than the specified duration. Where\npossible, the elapsed time is determined by a steady clock. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE::WAIT_FOR MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWait until std::condition_variable is woken by a call to notify_one() or notify_\nall() and the predicate is true, or until the specified time period has elapsed. \nDeclaration\ntemplate<typename Rep,typename Period,typename Predicate>\nbool wait_for(\n    std::unique_lock<std::mutex>& lock,\n    std::chrono::duration<Rep,Period> const& relative_time,\n    Predicate pred);\nPreconditions\nThe expression pred() shall be valid and shall return a value that’s convertible to\nbool. lock.owns_lock() shall be true, and the lock shall be owned by the thread\ncalling wait(). \nEffects\nAs-if \ninternal_clock::time_point end=internal_clock::now()+relative_time;\nwhile(!pred())\n{\n    std::chrono::duration<Rep,Period> remaining_time=\n        end-internal_clock::now();\n    if(wait_for(lock,remaining_time)==std::cv_status::timeout)\n        return pred();\n}\nreturn true;\nReturns\ntrue if the most recent call to pred() returned true, false if the time period spec-\nified by relative_time has elapsed and pred() returned false. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex\nreferenced by lock locked, and the function shall return if (and only if) an\nevaluation of (bool)pred() returns true or the time period specified by\nrelative_time has elapsed. The thread may be blocked for longer than the\nspecified duration. Where possible, the elapsed time is determined by a\nsteady clock. \nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects couldn’t\nbe achieved. \n",
      "content_length": 2075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": "422\nAPPENDIX D\nC++ Thread Library reference\nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE::WAIT_UNTIL MEMBER FUNCTION \nWaits until std::condition_variable is notified by a call to notify_one() or notify\n_all(), until a specified time has been reached, or the thread is woken spuriously. \nDeclaration\ntemplate<typename Clock,typename Duration>\ncv_status wait_until(\n    std::unique_lock<std::mutex>& lock,\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nlock.owns_lock() is true, and the lock is owned by the calling thread. \nEffects \nAtomically unlocks the supplied lock object and block until the thread is woken by a\ncall to notify_one() or notify_all() by another thread, or Clock::now() returns a\ntime equal to or later than absolute_time or the thread is woken spuriously. The\nlock object is locked again before the call to wait_until() returns. \nReturns\nstd::cv_status::no_timeout if the thread was woken by a call to notify_one(), a\ncall to notify_all(), or a spurious wakeup, std::cv_status::timeout otherwise. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait_until(), it’s locked again on exit, even if the function exits\nvia an exception. \nNOTE\nThe spurious wakeups mean that a thread calling wait_until() may\nwake even though no thread has called notify_one() or notify_all(). It’s\ntherefore recommended that the overload of wait_until() that takes a pred-\nicate is used in preference where possible. Otherwise, it’s recommended that\nwait_until() be called in a loop that tests the predicate associated with the\ncondition variable. There’s no guarantee as to how long the calling thread\nwill be blocked, only that if the function returns false, then Clock::now()\nreturns a time equal to or later than absolute_time at the point at which the\nthread became unblocked. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one()\nor notify_all() will only wake threads that started waiting prior to that call. \n",
      "content_length": 2365,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": "423\n<condition_variable> header\nSTD::CONDITION_VARIABLE::WAIT_UNTIL MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWait until std::condition_variable is woken by a call to notify_one() or notify_\nall() and the predicate is true, or until the specified time has been reached. \nDeclaration\ntemplate<typename Clock,typename Duration,typename Predicate>\nbool wait_until(\n    std::unique_lock<std::mutex>& lock,\n    std::chrono::time_point<Clock,Duration> const& absolute_time,\n    Predicate pred);\nPreconditions\nThe expression pred() shall be valid and shall return a value convertible to bool.\nlock.owns_lock() shall be true, and the lock shall be owned by the thread calling\nwait(). \nEffects\nAs-if \nwhile(!pred())\n{\n    if(wait_until(lock,absolute_time)==std::cv_status::timeout)\n        return pred();\n}\nreturn true;\nReturns \ntrue if the most recent call to pred() returned true, false if a call to Clock::now()\nreturned a time equal to or later than the time specified by absolute_time and\npred() returned false. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex ref-\nerenced by lock locked, and the function shall return if (and only if) an eval-\nuation of (bool)pred() returns true or Clock::now() returns a time equal to\nor later than absolute_time. There’s no guarantee as to how long the calling\nthread will be blocked, only that if the function returns false, then Clock::\nnow() returns a time equal to or later than absolute_time at the point at\nwhich the thread became unblocked. \nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects couldn’t\nbe achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_until(), and wait_until() on\na single std::condition_variable instance are serialized. A call to notify_one() or\nnotify_all() will wake only threads that started waiting prior to that call. \n",
      "content_length": 1952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": "424\nAPPENDIX D\nC++ Thread Library reference\nSTD::NOTIFY_ALL_AT_THREAD_EXIT NONMEMBER FUNCTION \nWake all of the threads waiting on a specific a std::condition_variable when the\ncurrent thread exits. \nDeclaration\nvoid notify_all_at_thread_exit(\n    condition_variable& cv,unique_lock<mutex> lk);\nPreconditions\nlk.owns_lock() is true, and the lock is owned by the calling thread. lk.mutex()\nshall return the same value as for any of the lock objects supplied to wait(),\nwait_for(), or wait_until() on cv from concurrently waiting threads. \nEffects\nTransfers ownership of the lock held by lk into internal storage and schedules cv\nto be notified when the calling thread exits. This notification shall be as-if \nlk.unlock();\ncv.notify_all();\nThrows\nstd::system_error if the effects can’t be achieved. \nNOTE\nThe lock is held until the thread exits, so care must be taken to avoid\ndeadlock. It’s recommended that the calling thread should exit as soon as\npossible and that no blocking operations be performed on this thread. \nThe user should ensure that waiting threads don’t erroneously assume that the\nthread has exited when they are woken, particularly with the potential for spurious\nwakeups. This can be achieved by testing a predicate on the waiting thread that’s\nonly made true by the notifying thread under the protection of the mutex and\nwithout releasing the lock on the mutex prior to the call of notify_all_at_thread\n_exit.std::condition_variable_any class.\nD.2.2\nstd::condition_variable_any class\nThe std::condition_variable_any class allows a thread to wait for a condition to\nbecome true. Whereas std::condition_variable can be used only with std::unique_\nlock<std::mutex>, std::condition_variable_any can be used with any type that\nmeets the Lockable requirements. \n Instances of std::condition_variable_any aren’t CopyAssignable, Copy-\nConstructible, MoveAssignable, or MoveConstructible. \nClass definition\nclass condition_variable_any\n{\npublic:\n    condition_variable_any();\n    ~condition_variable_any();\n    condition_variable_any(\n        condition_variable_any const& ) = delete;\n",
      "content_length": 2095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": "425\n<condition_variable> header\n    condition_variable_any& operator=(\n        condition_variable_any const& ) = delete;\n    void notify_one() noexcept;\n    void notify_all() noexcept;\n    template<typename Lockable>\n    void wait(Lockable& lock);\n    template <typename Lockable, typename Predicate>\n    void wait(Lockable& lock, Predicate pred);\n    template <typename Lockable, typename Clock,typename Duration>\n    std::cv_status wait_until(\n        Lockable& lock,\n        const std::chrono::time_point<Clock, Duration>& absolute_time);\n    template <\n        typename Lockable, typename Clock,\n        typename Duration, typename Predicate>\n    bool wait_until(\n        Lockable& lock,\n        const std::chrono::time_point<Clock, Duration>& absolute_time,\n        Predicate pred);\n    template <typename Lockable, typename Rep, typename Period>\n    std::cv_status wait_for(\n        Lockable& lock,\n        const std::chrono::duration<Rep, Period>& relative_time);\n    template <\n        typename Lockable, typename Rep,\n        typename Period, typename Predicate>\n    bool wait_for(\n        Lockable& lock,\n        const std::chrono::duration<Rep, Period>& relative_time,\n        Predicate pred);\n};\nSTD::CONDITION_VARIABLE_ANY DEFAULT CONSTRUCTOR \nConstructs an std::condition_variable_any object. \nDeclaration\ncondition_variable_any();\nEffects\nConstructs a new std::condition_variable_any instance. \nThrows\nAn exception of type std::system_error if the condition variable couldn’t be con-\nstructed. \nSTD::CONDITION_VARIABLE_ANY DESTRUCTOR \nDestroys an std::condition_variable_any object. \nDeclaration\n~condition_variable_any();\n",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": "426\nAPPENDIX D\nC++ Thread Library reference\nPreconditions\nThere are no threads blocked on *this in a call to wait(), wait_for(), or wait_\nuntil(). \nEffects \nDestroys *this. \nThrows\nNothing. \nSTD::CONDITION_VARIABLE_ANY::NOTIFY_ONE MEMBER FUNCTION \nWakes one of the threads currently waiting on a specific a std::condition_variable\n_any. \nDeclaration\nvoid notify_one() noexcept;\nEffects\nWakes one of the threads waiting on *this at the point of the call. If there are no\nthreads waiting, the call has no effect. \nThrows\nstd::system_error if the effects can’t be achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE_ANY::NOTIFY_ALL MEMBER FUNCTION \nWakes all of the threads currently waiting on a specific a std::condition_variable\n_any. \nDeclaration\nvoid notify_all() noexcept;\nEffects\nWakes all of the threads waiting on *this at the point of the call. If there are no\nthreads waiting, the call has no effect. \nThrows\nstd::system_error if the effects can’t be achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE_ANY::WAIT MEMBER FUNCTION \nWaits until std::condition_variable_any is woken by a call to notify_one(), a call\nto notify_all(), or a spurious wakeup. \nDeclaration\ntemplate<typename Lockable>\nvoid wait(Lockable& lock);\n",
      "content_length": 1714,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": "427\n<condition_variable> header\nPreconditions\nLockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAtomically unlocks the supplied lock object and block until the thread is woken by\na call to notify_one() or notify_all() by another thread, or the thread is woken\nspuriously. The lock object is locked again before the call to wait() returns. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait(), it’s locked again on exit, even if the function exits via an\nexception. \nNOTE\nThe spurious wakeups mean that a thread calling wait() may wake\neven though no thread has called notify_one() or notify_all(). It’s there-\nfore recommended that the overload of wait() that takes a predicate is used\nin preference where possible. Otherwise, it’s recommended that wait() be\ncalled in a loop that tests the predicate associated with the condition variable. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE_ANY::WAIT MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWaits until std::condition_variable_any is woken by a call to notify_one() or\nnotify_all() and the predicate is true. \nDeclaration\ntemplate<typename Lockable,typename Predicate>\nvoid wait(Lockable& lock,Predicate pred);\nPreconditions\nThe expression pred() shall be valid and shall return a value that’s convertible to\nbool. Lockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAs-if \nwhile(!pred())\n{\n    wait(lock);\n}\nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects could\nnot be achieved. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex ref-\nerenced by lock locked, and the function shall return if (and only if) an eval-\nuation of (bool)pred() returns true. \n",
      "content_length": 2088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": "428\nAPPENDIX D\nC++ Thread Library reference\nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE_ANY::WAIT_FOR MEMBER FUNCTION \nWaits until std::condition_variable_any is notified by a call to notify_one() or\nnotify_all(), until a specified time period has elapsed, or the thread is woken spu-\nriously. \nDeclaration\ntemplate<typename Lockable,typename Rep,typename Period>\nstd::cv_status wait_for(\n    Lockable& lock,\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions \nLockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAtomically unlocks the supplied lock object and block until the thread is woken by\na call to notify_one() or notify_all() by another thread or the time period spec-\nified by relative_time has elapsed or the thread is woken spuriously. The lock\nobject is locked again before the call to wait_for() returns. \nReturns\nstd::cv_status::no_timeout if the thread was woken by a call to notify_one(), a\ncall to notify_all(), or a spurious wakeup, std::cv_status::timeout otherwise. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait_for(), it’s locked again on exit, even if the function exits\nvia an exception. \nNOTE\nThe spurious wakeups mean that a thread calling wait_for() may\nwake even though no thread has called notify_one() or notify_all(). It’s\ntherefore recommended that the overload of wait_for() that takes a predi-\ncate is used in preference where possible. Otherwise, it’s recommended that\nwait_for() be called in a loop that tests the predicate associated with the\ncondition variable. Care must be taken when doing this to ensure that the\ntimeout is still valid; wait_until() may be more appropriate in many circum-\nstances. The thread may be blocked for longer than the specified duration.\nWhere possible, the elapsed time is determined by a steady clock. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \n",
      "content_length": 2387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": "429\n<condition_variable> header\nSTD::CONDITION_VARIABLE_ANY::WAIT_FOR MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWaits until std::condition_variable_any is woken by a call to notify_one() or notify\n_all() and the predicate is true, or until the specified time period has elapsed. \nDeclaration\ntemplate<typename Lockable,typename Rep,\n    typename Period, typename Predicate>\nbool wait_for(\n    Lockable& lock,\n    std::chrono::duration<Rep,Period> const& relative_time,\n    Predicate pred);\nPreconditions\nThe expression pred() shall be valid and shall return a value that’s convertible to\nbool. Lockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAs-if \ninternal_clock::time_point end=internal_clock::now()+relative_time;\nwhile(!pred())\n{\n    std::chrono::duration<Rep,Period> remaining_time=\n        end-internal_clock::now();\n    if(wait_for(lock,remaining_time)==std::cv_status::timeout)\n        return pred();\n}\nreturn true;\nReturns\ntrue if the most recent call to pred() returned true, false if the time period spec-\nified by relative_time has elapsed and pred() returned false. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex\nreferenced by lock locked, and the function shall return if (and only if) an\nevaluation of (bool)pred() returns true or the time period specified by\nrelative_time has elapsed. The thread may be blocked for longer than the\nspecified duration. Where possible, the elapsed time is determined by a\nsteady clock. \nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects couldn’t\nbe achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \n",
      "content_length": 1934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": "430\nAPPENDIX D\nC++ Thread Library reference\nSTD::CONDITION_VARIABLE_ANY::WAIT_UNTIL MEMBER FUNCTION \nWaits until std::condition_variable_any is notified by a call to notify_one() or\nnotify_all(), until a specified time has been reached, or the thread is woken\nspuriously. \nDeclaration\ntemplate<typename Lockable,typename Clock,typename Duration>\nstd::cv_status wait_until(\n    Lockable& lock,\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nLockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAtomically unlocks the supplied lock object and block until the thread is woken by\na call to notify_one() or notify_all() by another thread, Clock::now() returns\na time equal to or later than absolute_time, or the thread is woken spuriously. The\nlock object is locked again before the call to wait_until() returns. \nReturns\nstd::cv_status::no_timeout if the thread was woken by a call to notify_one(), a\ncall to notify_all(), or a spurious wakeup, std::cv_status::timeout otherwise. \nThrows\nstd::system_error if the effects can’t be achieved. If the lock object is unlocked\nduring the call to wait_until(), it’s locked again on exit, even if the function exits\nvia an exception. \nNOTE\nThe spurious wakeups mean that a thread calling wait_until() may\nwake even though no thread has called notify_one() or notify_all(). It’s\ntherefore recommended that the overload of wait_until() that takes a pred-\nicate is used in preference where possible. Otherwise, it’s recommended that\nwait_until() be called in a loop that tests the predicate associated with the\ncondition variable. There’s no guarantee as to how long the calling thread\nwill be blocked, only that if the function returns false, then Clock::now()\nreturns a time equal to or later than absolute_time at the point at which the\nthread became unblocked. \nSynchronization \nCalls to notify_one(), notify_all(), wait(), wait_for(), and wait_until() on\na single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nSTD::CONDITION_VARIABLE_ANY::WAIT_UNTIL MEMBER FUNCTION OVERLOAD THAT TAKES A PREDICATE \nWaits until std::condition_variable_any is woken by a call to notify_one() or\nnotify_all() and the predicate is true, or until the specified time has been reached. \nDeclaration\ntemplate<typename Lockable,typename Clock,\n    typename Duration, typename Predicate>\n",
      "content_length": 2468,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": "431\n<atomic> header\nbool wait_until(\n    Lockable& lock,\n    std::chrono::time_point<Clock,Duration> const& absolute_time,\n    Predicate pred);\nPreconditions\nThe expression pred() shall be valid, and shall return a value that’s convertible to\nbool. Lockable meets the Lockable requirements, and lock owns a lock. \nEffects\nAs-if \nwhile(!pred())\n{\n    if(wait_until(lock,absolute_time)==std::cv_status::timeout)\n        return pred();\n}\nreturn true;\nReturns\ntrue if the most recent call to pred() returned true, false if a call to Clock::\nnow() returned a time equal to or later than the time specified by absolute_time,\nand pred() returned false. \nNOTE\nThe potential for spurious wakeups means that it’s unspecified how\nmany times pred will be called. pred will always be invoked with the mutex ref-\nerenced by lock locked, and the function shall return if (and only if) an eval-\nuation of (bool)pred() returns true or Clock::now() returns a time equal to\nor later than absolute_time. There’s no guarantee as to how long the calling\nthread will be blocked, only that if the function returns false, then Clock::\nnow() returns a time equal to or later than absolute_time at the point at\nwhich the thread became unblocked. \nThrows\nAny exception thrown by a call to pred, or std::system_error if the effects couldn’t\nbe achieved. \nSynchronization\nCalls to notify_one(), notify_all(), wait(), wait_until(), and wait_until()\non a single std::condition_variable_any instance are serialized. A call to notify_\none() or notify_all() will only wake threads that started waiting prior to that call. \nD.3\n<atomic> header \nThe <atomic> header provides the set of basic atomic types and operations on those\ntypes and a class template for constructing an atomic version of a user-defined type\nthat meets certain criteria. \nHeader contents\n#define ATOMIC_BOOL_LOCK_FREE see description\n#define ATOMIC_CHAR_LOCK_FREE see description\n#define ATOMIC_SHORT_LOCK_FREE see description\n",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": "432\nAPPENDIX D\nC++ Thread Library reference\n#define ATOMIC_INT_LOCK_FREE see description\n#define ATOMIC_LONG_LOCK_FREE see description\n#define ATOMIC_LLONG_LOCK_FREE see description\n#define ATOMIC_CHAR16_T_LOCK_FREE see description\n#define ATOMIC_CHAR32_T_LOCK_FREE see description\n#define ATOMIC_WCHAR_T_LOCK_FREE see description\n#define ATOMIC_POINTER_LOCK_FREE see description\n#define ATOMIC_VAR_INIT(value) see description\nnamespace std\n{\n    enum memory_order;\n    struct atomic_flag;\n    typedef see description atomic_bool;\n    typedef see description atomic_char;\n    typedef see description atomic_char16_t;\n    typedef see description atomic_char32_t;\n    typedef see description atomic_schar;\n    typedef see description atomic_uchar;\n    typedef see description atomic_short;\n    typedef see description atomic_ushort;\n    typedef see description atomic_int;\n    typedef see description atomic_uint;\n    typedef see description atomic_long;\n    typedef see description atomic_ulong;\n    typedef see description atomic_llong;\n    typedef see description atomic_ullong;\n    typedef see description atomic_wchar_t;\n    typedef see description atomic_int_least8_t;\n    typedef see description atomic_uint_least8_t;\n    typedef see description atomic_int_least16_t;\n    typedef see description atomic_uint_least16_t;\n    typedef see description atomic_int_least32_t;\n    typedef see description atomic_uint_least32_t;\n    typedef see description atomic_int_least64_t;\n    typedef see description atomic_uint_least64_t;\n    typedef see description atomic_int_fast8_t;\n    typedef see description atomic_uint_fast8_t;\n    typedef see description atomic_int_fast16_t;\n    typedef see description atomic_uint_fast16_t;\n    typedef see description atomic_int_fast32_t;\n    typedef see description atomic_uint_fast32_t;\n    typedef see description atomic_int_fast64_t;\n    typedef see description atomic_uint_fast64_t;\n    typedef see description atomic_int8_t;\n    typedef see description atomic_uint8_t;\n    typedef see description atomic_int16_t;\n    typedef see description atomic_uint16_t;\n    typedef see description atomic_int32_t;\n    typedef see description atomic_uint32_t;\n    typedef see description atomic_int64_t;\n    typedef see description atomic_uint64_t;\n    typedef see description atomic_intptr_t;\n    typedef see description atomic_uintptr_t;\n",
      "content_length": 2365,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": "433\n<atomic> header\n    typedef see description atomic_size_t;\n    typedef see description atomic_ssize_t;\n    typedef see description atomic_ptrdiff_t;\n    typedef see description atomic_intmax_t;\n    typedef see description atomic_uintmax_t;\n    template<typename T>\n    struct atomic;\n    extern \"C\" void atomic_thread_fence(memory_order order);\n    extern \"C\" void atomic_signal_fence(memory_order order);\n    template<typename T>\n    T kill_dependency(T);\n}\nD.3.1\nstd::atomic_xxx typedefs \nFor compatibility with the forthcoming C Standard, typedefs for the atomic integral\ntypes are provided. For C++17, these must be typedefs to the corresponding std::\natomic<T> specialization; for prior C++ standards, they may instead be a base class of\nthat specialization with the same interface. \nTable D.1\nAtomic typedefs and their corresponding std::atomic<> specializations\nstd::atomic_itype \nstd::atomic<> specialization\nstd::atomic_char \nstd::atomic<char> \nstd::atomic_schar \nstd::atomic<signed char> \nstd::atomic_uchar \nstd::atomic<unsigned char> \nstd::atomic_short \nstd::atomic<short> \nstd::atomic_ushort \nstd::atomic<unsigned short> \nstd::atomic_int \nstd::atomic<int> \nstd::atomic_uint \nstd::atomic<unsigned int> \nstd::atomic_long \nstd::atomic<long> \nstd::atomic_ulong \nstd::atomic<unsigned long> \nstd::atomic_llong \nstd::atomic<long long> \nstd::atomic_ullong \nstd::atomic<unsigned long long> \nstd::atomic_wchar_t \nstd::atomic<wchar_t> \nstd::atomic_char16_t \nstd::atomic<char16_t> \nstd::atomic_char32_t \nstd::atomic<char32_t> \n",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": "434\nAPPENDIX D\nC++ Thread Library reference\nD.3.2\nATOMIC_xxx_LOCK_FREE macros\nThese macros specify whether the atomic types corresponding to particular built-in\ntypes are lock-free. \nMacro declarations\n#define ATOMIC_BOOL_LOCK_FREE see description\n#define ATOMIC_CHAR_LOCK_FREE see description\n#define ATOMIC_SHORT_LOCK_FREE see description\n#define ATOMIC_INT_LOCK_FREE see description\n#define ATOMIC_LONG_LOCK_FREE see description\n#define ATOMIC_LLONG_LOCK_FREE see description\n#define ATOMIC_CHAR16_T_LOCK_FREE see description\n#define ATOMIC_CHAR32_T_LOCK_FREE see description\n#define ATOMIC_WCHAR_T_LOCK_FREE see description\n#define ATOMIC_POINTER_LOCK_FREE see description\nThe value of ATOMIC_xxx_LOCK_FREE is either 0, 1, or 2. A value of 0 means that\noperations on both the signed and unsigned atomic types corresponding to the\nnamed type are never lock-free, a value of 1 means that the operations may be lock-\nfree for particular instances of those types and not for others, and a value of 2\nmeans that the operations are always lock-free. For example, if ATOMIC_INT_\nLOCK_FREE is 2, operations on instances of std::atomic<int> and std::atomic\n<unsigned> are always lock-free. \nThe ATOMIC_POINTER_LOCK_FREE macro describes the lock-free property of oper-\nations on the atomic pointer specializations std::atomic<T*>. \nD.3.3\nATOMIC_VAR_INIT macro \nThe ATOMIC_VAR_INIT macro provides a means of initializing an atomic variable to a\nparticular value. \nDeclaration\n#define ATOMIC_VAR_INIT(value) see description\nThe macro expands to a token sequence that can be used to initialize one of the stan-\ndard atomic types with the specified value in an expression of the following form: \nstd::atomic<type> x = ATOMIC_VAR_INIT(val);\nThe specified value must be compatible with the nonatomic type corresponding to\nthe atomic variable; for example:\nstd::atomic<int> i = ATOMIC_VAR_INIT(42);\nstd::string s;\nstd::atomic<std::string*> p = ATOMIC_VAR_INIT(&s);\nThis initialization is not atomic, and any access by another thread to the variable\nbeing initialized where the initialization doesn’t happen-before that access is a data\nrace and thus undefined behavior. \n",
      "content_length": 2158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": "435\n<atomic> header\nD.3.4\nstd::memory_order enumeration \nThe std::memory_order enumeration is used to specify the ordering constraints of\natomic operations. \nDeclaration\ntypedef enum memory_order\n{\n    memory_order_relaxed,memory_order_consume,\n    memory_order_acquire,memory_order_release,\n    memory_order_acq_rel,memory_order_seq_cst\n} memory_order;\nOperations tagged with the various memory order values behave as follows (see\nchapter 5 for detailed descriptions of the ordering constraints). \nSTD::MEMORY_ORDER_RELAXED \nThe operation doesn’t provide any additional ordering constraints. \nSTD::MEMORY_ORDER_RELEASE \nThe operation is a release operation on the specified memory location. This therefore\nsynchronizes-with an acquire operation on the same memory location that reads the\nstored value. \nSTD::MEMORY_ORDER_ACQUIRE \nThe operation is an acquire operation on the specified memory location. If the stored\nvalue was written by a release operation, that store synchronizes-with this operation. \nSTD::MEMORY_ORDER_ACQ_REL \nThe operation must be a read-modify-write operation, and it behaves as both std::\nmemory_order_acquire and std::memory_order_release on the specified location. \nSTD::MEMORY_ORDER_SEQ_CST \nThe operation forms part of the single global total order of sequentially consistent\noperations. In addition, if it’s a store, it behaves like an std::memory_order_release\noperation; if it’s a load, it behaves like an std::memory_order_acquire operation;\nand if it’s a read-modify-write operation, it behaves as both std::memory_order_\nacquire and std::memory_order_release. This is the default for all operations. \nSTD::MEMORY_ORDER_CONSUME \nThe operation is a consume operation on the specified memory location. The C++17\nStandard states that this memory ordering should not be used.\nD.3.5\nstd::atomic_thread_fence function \nThe std::atomic_thread_fence() function inserts a “memory barrier” or “fence” in\nthe code to force memory-ordering constraints between operations. \nDeclaration\nextern \"C\" void atomic_thread_fence(std::memory_order order);\n",
      "content_length": 2069,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": "436\nAPPENDIX D\nC++ Thread Library reference\nEffects\nInserts a fence with the required memory-ordering constraints. \nA fence with an order of std::memory_order_release, std::memory_order_\nacq_rel, or std::memory_order_seq_cst synchronizes-with an acquire operation\non the same memory location if that acquire operation reads a value stored by an\natomic operation following the fence on the same thread as the fence. \nA release operation synchronizes-with a fence with an order of std::memory\n_order_acquire, std::memory_order_acq_rel, or std::memory_order_seq_cst if\nthat release operation stores a value that’s read by an atomic operation prior to the\nfence on the same thread as the fence. \nThrows\nNothing. \nD.3.6\nstd::atomic_signal_fence function \nThe std::atomic_signal_fence() function inserts a memory barrier or fence in the\ncode to force memory ordering constraints between operations on a thread and oper-\nations in a signal handler on that thread. \nDeclaration\nextern \"C\" void atomic_signal_fence(std::memory_order order);\nEffects\nInserts a fence with the required memory-ordering constraints. This is equivalent to\nstd::atomic_thread_fence(order) except that the constraints apply only between\na thread and a signal handler on the same thread. \nThrows\nNothing. \nD.3.7\nstd::atomic_flag class \nThe std::atomic_flag class provides a simple bare-bones atomic flag. It’s the only\ndata type that’s guaranteed to be lock-free by the C++11 Standard (although many\natomic types will be lock-free in most implementations).\n An instance of std::atomic_flag is either set or clear. \nClass definition\nstruct atomic_flag\n{\n    atomic_flag() noexcept = default;\n    atomic_flag(const atomic_flag&) = delete;\n    atomic_flag& operator=(const atomic_flag&) = delete;\n    atomic_flag& operator=(const atomic_flag&) volatile = delete;\n    bool test_and_set(memory_order = memory_order_seq_cst) volatile \nnoexcept;\n    bool test_and_set(memory_order = memory_order_seq_cst) noexcept;\n    void clear(memory_order = memory_order_seq_cst) volatile noexcept;\n    void clear(memory_order = memory_order_seq_cst) noexcept;\n};\n",
      "content_length": 2110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": "437\n<atomic> header\nbool atomic_flag_test_and_set(volatile atomic_flag*) noexcept;\nbool atomic_flag_test_and_set(atomic_flag*) noexcept;\nbool atomic_flag_test_and_set_explicit(\n    volatile atomic_flag*, memory_order) noexcept;\nbool atomic_flag_test_and_set_explicit(\n    atomic_flag*, memory_order) noexcept;\nvoid atomic_flag_clear(volatile atomic_flag*) noexcept;\nvoid atomic_flag_clear(atomic_flag*) noexcept;\nvoid atomic_flag_clear_explicit(\n    volatile atomic_flag*, memory_order) noexcept;\nvoid atomic_flag_clear_explicit(\n    atomic_flag*, memory_order) noexcept;\n#define ATOMIC_FLAG_INIT unspecified\nSTD::ATOMIC_FLAG DEFAULT CONSTRUCTOR \nIt’s unspecified whether a default-constructed instance of std::atomic_flag is clear\nor set. For objects of static storage duration, initialization shall be static initialization.\nDeclaration\nstd::atomic_flag() noexcept = default;\nEffects\nConstructs a new std::atomic_flag object in an unspecified state. \nThrows\nNothing.\nSTD::ATOMIC_FLAG INITIALIZATION WITH ATOMIC_FLAG_INIT \nAn instance of std::atomic_flag may be initialized using the ATOMIC_FLAG_INIT\nmacro, in which case it’s initialized into the clear state. For objects of static storage\nduration, initialization shall be static initialization.\nDeclaration\n#define ATOMIC_FLAG_INIT unspecified\nUsage\nstd::atomic_flag flag=ATOMIC_FLAG_INIT;\nEffects\nConstructs a new std::atomic_flag object in the clear state. \nThrows\nNothing. \nSTD::ATOMIC_FLAG::TEST_AND_SET MEMBER FUNCTION \nAtomically sets the flag and checks whether or not it was set. \nDeclaration\nbool test_and_set(memory_order order = memory_order_seq_cst) volatile \nnoexcept;\nbool test_and_set(memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically sets the flag. \n",
      "content_length": 1742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": "438\nAPPENDIX D\nC++ Thread Library reference\nReturns\ntrue if the flag was set at the point of the call, false if the flag was clear. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FLAG_TEST_AND_SET NONMEMBER FUNCTION \nAtomically sets the flag and checks whether or not it was set. \nDeclaration\nbool atomic_flag_test_and_set(volatile atomic_flag* flag) noexcept;\nbool atomic_flag_test_and_set(atomic_flag* flag) noexcept;\nEffects\nreturn flag->test_and_set();\nSTD::ATOMIC_FLAG_TEST_AND_SET_EXPLICIT NONMEMBER FUNCTION \nAtomically sets the flag and checks whether or not it was set. \nDeclaration\nbool atomic_flag_test_and_set_explicit(\n    volatile atomic_flag* flag, memory_order order) noexcept;\nbool atomic_flag_test_and_set_explicit(\n    atomic_flag* flag, memory_order order) noexcept;\nEffects\nreturn flag->test_and_set(order);\nSTD::ATOMIC_FLAG::CLEAR MEMBER FUNCTION \nAtomically clears the flag. \nDeclaration\nvoid clear(memory_order order = memory_order_seq_cst) volatile noexcept;\nvoid clear(memory_order order = memory_order_seq_cst) noexcept;\nPreconditions\nThe supplied order must be one of std::memory_order_relaxed, std::memory_\norder_release, or std::memory_order_seq_cst. \nEffects\nAtomically clears the flag. \nThrows\nNothing. \nNOTE\nThis is an atomic store operation for the memory location comprising\n*this. \n",
      "content_length": 1395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": "439\n<atomic> header\nSTD::ATOMIC_FLAG_CLEAR NONMEMBER FUNCTION \nAtomically clears the flag. \nDeclaration\nvoid atomic_flag_clear(volatile atomic_flag* flag) noexcept;\nvoid atomic_flag_clear(atomic_flag* flag) noexcept;\nEffects\nflag->clear();\nSTD::ATOMIC_FLAG_CLEAR_EXPLICIT NONMEMBER FUNCTION \nAtomically clears the flag. \nDeclaration\nvoid atomic_flag_clear_explicit(\n    volatile atomic_flag* flag, memory_order order) noexcept;\nvoid atomic_flag_clear_explicit(\n    atomic_flag* flag, memory_order order) noexcept;\nEffects\nreturn flag->clear(order);\nD.3.8\nstd::atomic class template \nThe std::atomic class provides a wrapper with atomic operations for any type that\nsatisfies the following requirements. \n The template parameter BaseType must \n■\nHave a trivial default constructor \n■\nHave a trivial copy-assignment operator \n■\nHave a trivial destructor \n■\nBe bitwise-equality comparable \nThis means that std::atomic<some-built-in-type> is fine, as is std::atomic<some-\nsimple-struct>, but things like std::atomic<std::string> are not. \n In addition to the primary template, there are specializations for the built-in inte-\ngral types and pointers to provide additional operations, such as x++. \n Instances of std::atomic are not CopyConstructible or CopyAssignable, because\nthese operations can’t be performed as a single atomic operation. \nClass definition\ntemplate<typename BaseType>\nstruct atomic\n{\n    using value_type = T;\n    static constexpr bool is_always_lock_free = implementation-defined ;\n    atomic() noexcept = default;\n    constexpr atomic(BaseType) noexcept;\n    BaseType operator=(BaseType) volatile noexcept;\n    BaseType operator=(BaseType) noexcept;\n    atomic(const atomic&) = delete;\n    atomic& operator=(const atomic&) = delete;\n    atomic& operator=(const atomic&) volatile = delete;\n",
      "content_length": 1808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": "440\nAPPENDIX D\nC++ Thread Library reference\n    bool is_lock_free() const volatile noexcept;\n    bool is_lock_free() const noexcept;\n    void store(BaseType,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    void store(BaseType,memory_order = memory_order_seq_cst) noexcept;\n    BaseType load(memory_order = memory_order_seq_cst)\n        const volatile noexcept;\n    BaseType load(memory_order = memory_order_seq_cst) const noexcept;\n    BaseType exchange(BaseType,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    BaseType exchange(BaseType,memory_order = memory_order_seq_cst)\n        noexcept;\n    bool compare_exchange_strong(\n        BaseType & old_value, BaseType new_value,\n        memory_order order = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_strong(\n        BaseType & old_value, BaseType new_value,\n        memory_order order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(\n        BaseType & old_value, BaseType new_value,\n        memory_order success_order,\n        memory_order failure_order) volatile noexcept;\n    bool compare_exchange_strong(\n        BaseType & old_value, BaseType new_value,\n        memory_order success_order,\n        memory_order failure_order) noexcept;\n    bool compare_exchange_weak(\n        BaseType & old_value, BaseType new_value,\n        memory_order order = memory_order_seq_cst)\n        volatile noexcept;\n    bool compare_exchange_weak(\n        BaseType & old_value, BaseType new_value,\n        memory_order order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_weak(\n        BaseType & old_value, BaseType new_value,\n        memory_order success_order,\n        memory_order failure_order) volatile noexcept;\n    bool compare_exchange_weak(\n        BaseType & old_value, BaseType new_value,\n        memory_order success_order,\n        memory_order failure_order) noexcept;\n    operator BaseType () const volatile noexcept;\n    operator BaseType () const noexcept;\n};\ntemplate<typename BaseType>\nbool atomic_is_lock_free(volatile const atomic<BaseType>*) noexcept;\ntemplate<typename BaseType>\nbool atomic_is_lock_free(const atomic<BaseType>*) noexcept;\ntemplate<typename BaseType>\nvoid atomic_init(volatile atomic<BaseType>*, void*) noexcept;\ntemplate<typename BaseType>\nvoid atomic_init(atomic<BaseType>*, void*) noexcept;\ntemplate<typename BaseType>\n",
      "content_length": 2390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": "441\n<atomic> header\nBaseType atomic_exchange(volatile atomic<BaseType>*, memory_order)\n    noexcept;\ntemplate<typename BaseType>\nBaseType atomic_exchange(atomic<BaseType>*, memory_order) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_exchange_explicit(\n    volatile atomic<BaseType>*, memory_order) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_exchange_explicit(\n    atomic<BaseType>*, memory_order) noexcept;\ntemplate<typename BaseType>\nvoid atomic_store(volatile atomic<BaseType>*, BaseType) noexcept;\ntemplate<typename BaseType>\nvoid atomic_store(atomic<BaseType>*, BaseType) noexcept;\ntemplate<typename BaseType>\nvoid atomic_store_explicit(\n    volatile atomic<BaseType>*, BaseType, memory_order) noexcept;\ntemplate<typename BaseType>\nvoid atomic_store_explicit(\n    atomic<BaseType>*, BaseType, memory_order) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load(volatile const atomic<BaseType>*) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load(const atomic<BaseType>*) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load_explicit(\n    volatile const atomic<BaseType>*, memory_order) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load_explicit(\n    const atomic<BaseType>*, memory_order) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong(\n    volatile atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong(\n    atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong_explicit(\n    volatile atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong_explicit(\n    atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak(\n    volatile atomic<BaseType>*,BaseType * old_value,BaseType new_value)\n    noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak(\n    atomic<BaseType>*,BaseType * old_value,BaseType new_value) noexcept;\n",
      "content_length": 2263,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": "442\nAPPENDIX D\nC++ Thread Library reference\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak_explicit(\n    volatile atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak_explicit(\n    atomic<BaseType>*,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\nNOTE\nAlthough the nonmember functions are specified as templates, they\nmay be provided as an overloaded set of functions, and explicit specification\nof the template arguments shouldn’t be used.\nSTD::ATOMIC DEFAULT CONSTRUCTOR \nConstructs an instance of std::atomic with a default-initialized value. \nDeclaration\natomic() noexcept;\nEffects\nConstructs a new std::atomic object with a default-initialized value. For objects\nwith static storage duration, this is static initialization.\nNOTE\nInstances of std::atomic with nonstatic storage duration initialized\nwith the default constructor can’t be relied on to have a predictable value. \nThrows\nNothing. \nSTD::ATOMIC_INIT NONMEMBER FUNCTION \nNonatomically stores the supplied value in an instance of std::atomic<BaseType>. \nDeclaration\ntemplate<typename BaseType>\nvoid atomic_init(atomic<BaseType> volatile* p, BaseType v) noexcept;\ntemplate<typename BaseType>\nvoid atomic_init(atomic<BaseType>* p, BaseType v) noexcept;\nEffects\nNonatomically stores the value of v in *p. Invoking atomic_init() on an instance\nof atomic<BaseType> that hasn’t been default constructed, or that has had any\noperations performed on it since construction, is undefined behavior.\nNOTE\nBecause this store is nonatomic, any concurrent access to the object\npointed to by p from another thread (even with atomic operations) consti-\ntutes a data race. \nThrows\nNothing. \n",
      "content_length": 1862,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": "443\n<atomic> header\nSTD::ATOMIC CONVERSION CONSTRUCTOR \nConstructs an instance of std::atomic with the supplied BaseType value. \nDeclaration\nconstexpr atomic(BaseType b) noexcept;\nEffects\nConstructs a new std::atomic object with a value of b. For objects with static stor-\nage duration, this is static initialization.\nThrows\nNothing. \nSTD::ATOMIC CONVERSION ASSIGNMENT OPERATOR \nStores a new value in *this. \nDeclaration\nBaseType operator=(BaseType b) volatile noexcept;\nBaseType operator=(BaseType b) noexcept;\nEffects\nreturn this->store(b);\nSTD::ATOMIC::IS_LOCK_FREE MEMBER FUNCTION \nDetermines if operations on *this are lock-free. \nDeclaration\nbool is_lock_free() const volatile noexcept;\nbool is_lock_free() const noexcept;\nReturns\ntrue if operations on *this are lock-free, false otherwise. \nThrows\nNothing. \nSTD::ATOMIC_IS_LOCK_FREE NONMEMBER FUNCTION \nDetermines if operations on *this are lock-free. \nDeclaration\ntemplate<typename BaseType>\nbool atomic_is_lock_free(volatile const atomic<BaseType>* p) noexcept;\ntemplate<typename BaseType>\nbool atomic_is_lock_free(const atomic<BaseType>* p) noexcept;\nEffects\nreturn p->is_lock_free();\nSTD::ATOMIC::IS_ALWAYS_LOCK_FREE STATIC DATA MEMBER \nDetermines if operations on all objects of this type are always lock-free. \nDeclaration\nstatic constexpr bool is_always_lock_free() = implementation-defined;\nValue\ntrue if operations on all objects of this type are always lock-free, false otherwise. \n",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": "444\nAPPENDIX D\nC++ Thread Library reference\nSTD::ATOMIC::LOAD MEMBER FUNCTION \nAtomically loads the current value of the std::atomic instance. \nDeclaration\nBaseType load(memory_order order = memory_order_seq_cst)\n    const volatile noexcept;\nBaseType load(memory_order order = memory_order_seq_cst) const noexcept;\nPreconditions\nThe supplied order must be one of std::memory_order_relaxed, std::memory_\norder_acquire, std::memory_order_consume, or std::memory_order_seq_cst.\nEffects\nAtomically loads the value stored in *this. \nReturns\nThe value stored in *this at the point of the call. \nThrows\nNothing. \nNOTE\nThis is an atomic load operation for the memory location comprising\n*this. \nSTD::ATOMIC_LOAD NONMEMBER FUNCTION \nAtomically loads the current value of the std::atomic instance. \nDeclaration\ntemplate<typename BaseType>\nBaseType atomic_load(volatile const atomic<BaseType>* p) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load(const atomic<BaseType>* p) noexcept;\nEffects\nreturn p->load();\nSTD::ATOMIC_LOAD_EXPLICIT NONMEMBER FUNCTION \nAtomically loads the current value of the std::atomic instance. \nDeclaration\ntemplate<typename BaseType>\nBaseType atomic_load_explicit(\n    volatile const atomic<BaseType>* p, memory_order order) noexcept;\ntemplate<typename BaseType>\nBaseType atomic_load_explicit(\n    const atomic<BaseType>* p, memory_order order) noexcept;\nEffects\nreturn p->load(order);\nSTD::ATOMIC::OPERATOR BASETYPE CONVERSION OPERATOR \nLoads the value stored in *this. \nDeclaration\noperator BaseType() const volatile noexcept;\noperator BaseType() const noexcept;\n",
      "content_length": 1590,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": "445\n<atomic> header\nEffects\nreturn this->load();\nSTD::ATOMIC::STORE MEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance.\nDeclaration\nvoid store(BaseType new_value,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nvoid store(BaseType new_value,memory_order order = memory_order_seq_cst)\n    noexcept;\nPreconditions\nThe supplied order must be one of std::memory_order_relaxed, std::memory_\norder_release, or std::memory_order_seq_cst. \nEffects\nAtomically stores new_value in *this. \nThrows\nNothing. \nNOTE\nThis is an atomic store operation for the memory location comprising\n*this. \nSTD::ATOMIC_STORE NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance.\nDeclaration\ntemplate<typename BaseType>\nvoid atomic_store(volatile atomic<BaseType>* p, BaseType new_value)\n    noexcept;\ntemplate<typename BaseType>\nvoid atomic_store(atomic<BaseType>* p, BaseType new_value) noexcept;\nEffects\np->store(new_value);\nSTD::ATOMIC_STORE_EXPLICIT NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance.\nDeclaration\ntemplate<typename BaseType>\nvoid atomic_store_explicit(\n    volatile atomic<BaseType>* p, BaseType new_value, memory_order order)\n    noexcept;\ntemplate<typename BaseType>\nvoid atomic_store_explicit(\n    atomic<BaseType>* p, BaseType new_value, memory_order order) noexcept;\nEffects\np->store(new_value,order);\n",
      "content_length": 1405,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": "446\nAPPENDIX D\nC++ Thread Library reference\nSTD::ATOMIC::EXCHANGE MEMBER FUNCTION \nAtomically stores a new value and reads the old one. \nDeclaration\nBaseType exchange(\n    BaseType new_value,\n    memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nEffects\nAtomically stores new_value in *this and retrieves the existing value of *this. \nReturns\nThe value of *this immediately prior to the store. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_EXCHANGE NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance and reads the prior\nvalue.\nDeclaration\ntemplate<typename BaseType>\nBaseType atomic_exchange(volatile atomic<BaseType>* p, BaseType new_value)\n    noexcept;\ntemplate<typename BaseType>\nBaseType atomic_exchange(atomic<BaseType>* p, BaseType new_value) noexcept;\nEffects\nreturn p->exchange(new_value);\nSTD::ATOMIC_EXCHANGE_EXPLICIT NONMEMBER FUNCTION \nAtomically stores a new value in an atomic<BaseType> instance and reads the prior\nvalue.\nDeclaration\ntemplate<typename BaseType>\nBaseType atomic_exchange_explicit(\n    volatile atomic<BaseType>* p, BaseType new_value, memory_order order)\n    noexcept;\ntemplate<typename BaseType>\nBaseType atomic_exchange_explicit(\n    atomic<BaseType>* p, BaseType new_value, memory_order order) noexcept;\nEffects\nreturn p->exchange(new_value,order);\n",
      "content_length": 1410,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": "447\n<atomic> header\nSTD::ATOMIC::COMPARE_EXCHANGE_STRONG MEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the values\nare equal. If the values aren’t equal, updates the expected value with the value read. \nDeclaration\nbool compare_exchange_strong(\n    BaseType& expected,BaseType new_value,\n    memory_order order = std::memory_order_seq_cst) volatile noexcept;\nbool compare_exchange_strong(\n    BaseType& expected,BaseType new_value,\n    memory_order order = std::memory_order_seq_cst) noexcept;\nbool compare_exchange_strong(\n    BaseType& expected,BaseType new_value,\n    memory_order success_order,memory_order failure_order)\n    volatile noexcept;\nbool compare_exchange_strong(\n    BaseType& expected,BaseType new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nPreconditions \nfailure_order shall not be std::memory_order_release or std::memory_order\n_acq_rel. \nEffects\nAtomically compares expected to the value stored in *this using bitwise compari-\nson and stores new_value in *this if equal; otherwise updates expected to the\nvalue read. \nReturns\ntrue if the existing value of *this was equal to expected, false otherwise. \nThrows\nNothing. \nNOTE\nThe three-parameter overload is equivalent to the four-parameter\noverload with success_order==order and failure_order==order, except\nthat if order is std::memory_order_acq_rel, then failure_order is std::\nmemory_order_acquire, and if order is std::memory_order_release, then\nfailure_order is std::memory_order_relaxed. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this if the result is true, with memory ordering success_order;\notherwise, it’s an atomic load operation for the memory location comprising\n*this with memory ordering failure_order. \nSTD::ATOMIC_COMPARE_EXCHANGE_STRONG NONMEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the values\nare equal. If the values aren’t equal, updates the expected value with the value read.\nDeclaration\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong(\n",
      "content_length": 2123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": "448\nAPPENDIX D\nC++ Thread Library reference\n    volatile atomic<BaseType>* p,BaseType * old_value,BaseType new_value)\n    noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong(\n    atomic<BaseType>* p,BaseType * old_value,BaseType new_value) noexcept;\nEffects\nreturn p->compare_exchange_strong(*old_value,new_value);\nSTD::ATOMIC_COMPARE_EXCHANGE_STRONG_EXPLICIT NONMEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the values\nare equal. If the values aren’t equal, updates the expected value with the value read.\nDeclaration\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong_explicit(\n    volatile atomic<BaseType>* p,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_strong_explicit(\n    atomic<BaseType>* p,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\nEffects\nreturn p->compare_exchange_strong(\n    *old_value,new_value,success_order,failure_order) noexcept;\nSTD::ATOMIC::COMPARE_EXCHANGE_WEAK MEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the val-\nues are equal and the update can be done atomically. If the values aren’t equal or the\nupdate can’t be done atomically, updates the expected value with the value read.\nDeclaration\nbool compare_exchange_weak(\n    BaseType& expected,BaseType new_value,\n    memory_order order = std::memory_order_seq_cst) volatile noexcept;\nbool compare_exchange_weak(\n    BaseType& expected,BaseType new_value,\n    memory_order order = std::memory_order_seq_cst) noexcept;\nbool compare_exchange_weak(\n    BaseType& expected,BaseType new_value,\n    memory_order success_order,memory_order failure_order)\n    volatile noexcept;\nbool compare_exchange_weak(\n    BaseType& expected,BaseType new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nPreconditions\nfailure_order shall not be std::memory_order_release or std::memory_order\n_acq_rel. \n",
      "content_length": 2118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": "449\n<atomic> header\nEffects\nAtomically compares expected to the value stored in *this using bitwise compari-\nson and stores new_value in *this if equal. If the values aren’t equal or the update\ncan’t be done atomically, updates expected to the value read. \nReturns\ntrue if the existing value of *this was equal to expected and new_value was suc-\ncessfully stored in *this, false otherwise. \nThrows \nNothing. \nNOTE\nThe three-parameter overload is equivalent to the four-parameter\noverload with success_order==order and failure_order==order, except\nthat if order is std::memory_order_acq_rel, then failure_order is std::\nmemory_order_acquire, and if order is std::memory_order_release, then\nfailure_order is std::memory_order_relaxed. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this if the result is true, with memory ordering success_order;\notherwise, it’s an atomic load operation for the memory location comprising\n*this with memory ordering failure_order. \nSTD::ATOMIC_COMPARE_EXCHANGE_WEAK NONMEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the val-\nues are equal and the update can be done atomically. If the values aren’t equal or the\nupdate can’t be done atomically, updates the expected value with the value read.\nDeclaration\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak(\n    volatile atomic<BaseType>* p,BaseType * old_value,BaseType new_value)\n    noexcept;\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak(\n    atomic<BaseType>* p,BaseType * old_value,BaseType new_value) noexcept;\nEffects\nreturn p->compare_exchange_weak(*old_value,new_value);\nSTD::ATOMIC_COMPARE_EXCHANGE_WEAK_EXPLICIT NONMEMBER FUNCTION \nAtomically compares the value to an expected value and stores a new value if the val-\nues are equal and the update can be done atomically. If the values aren’t equal or the\nupdate can’t be done atomically, updates the expected value with the value read.\nDeclaration\ntemplate<typename BaseType>\nbool atomic_compare_exchange_weak_explicit(\n    volatile atomic<BaseType>* p,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\ntemplate<typename BaseType>\n",
      "content_length": 2254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": "450\nAPPENDIX D\nC++ Thread Library reference\nbool atomic_compare_exchange_weak_explicit(\n    atomic<BaseType>* p,BaseType * old_value,\n    BaseType new_value, memory_order success_order,\n    memory_order failure_order) noexcept;\nEffects\nreturn p->compare_exchange_weak(\n    *old_value,new_value,success_order,failure_order);\nD.3.9\nSpecializations of the std::atomic template \nSpecializations of the std::atomic class template are provided for the integral types\nand pointer types. For the integral types, these specializations provide atomic addi-\ntion, subtraction, and bitwise operations in addition to the operations provided by the\nprimary template. For pointer types, the specializations provide atomic pointer arith-\nmetic in addition to the operations provided by the primary template. \n Specializations are provided for the following integral types: \nstd::atomic<bool>\nstd::atomic<char>\nstd::atomic<signed char>\nstd::atomic<unsigned char>\nstd::atomic<short>\nstd::atomic<unsigned short>\nstd::atomic<int>\nstd::atomic<unsigned>\nstd::atomic<long>\nstd::atomic<unsigned long>\nstd::atomic<long long>\nstd::atomic<unsigned long long>\nstd::atomic<wchar_t>\nstd::atomic<char16_t>\nstd::atomic<char32_t>\nand std::atomic<T*> for all types T. \nD.3.10 std::atomic<integral-type> specializations \nThe std::atomic<integral-type> specializations of the std::atomic class template\nprovide an atomic integral data type for each fundamental integer type, with a com-\nprehensive set of operations. \n The following description applies to these specializations of the std::atomic<>\nclass template:\nstd::atomic<char>\nstd::atomic<signed char>\nstd::atomic<unsigned char>\nstd::atomic<short>\nstd::atomic<unsigned short>\nstd::atomic<int>\nstd::atomic<unsigned>\nstd::atomic<long>\n",
      "content_length": 1753,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": "451\n<atomic> header\nstd::atomic<unsigned long>\nstd::atomic<long long>\nstd::atomic<unsigned long long>\nstd::atomic<wchar_t>\nstd::atomic<char16_t>\nstd::atomic<char32_t>\nInstances of these specializations are not CopyConstructible or CopyAssignable,\nbecause these operations can’t be performed as a single atomic operation. \nClass definition\ntemplate<>\nstruct atomic<integral-type>\n{\n    atomic() noexcept = default;\n    constexpr atomic(integral-type) noexcept;\n    bool operator=(integral-type) volatile noexcept;\n    atomic(const atomic&) = delete;\n    atomic& operator=(const atomic&) = delete;\n    atomic& operator=(const atomic&) volatile = delete;\n    bool is_lock_free() const volatile noexcept;\n    bool is_lock_free() const noexcept;\n    void store(integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    void store(integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type load(memory_order = memory_order_seq_cst)\n        const volatile noexcept;\n    integral-type load(memory_order = memory_order_seq_cst) const noexcept;\n    integral-type exchange(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type exchange(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(\n        integral-type & old_value,integral-type new_value,\n         memory_order order = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_strong(\n        integral-type & old_value,integral-type new_value,\n         memory_order order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(\n        integral-type & old_value,integral-type new_value,\n        memory_order success_order,memory_order failure_order)\n        volatile noexcept;\n    bool compare_exchange_strong(\n        integral-type & old_value,integral-type new_value,\n        memory_order success_order,memory_order failure_order) noexcept;\n    bool compare_exchange_weak(\n        integral-type & old_value,integral-type new_value,\n         memory_order order = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_weak(\n        integral-type & old_value,integral-type new_value,\n         memory_order order = memory_order_seq_cst) noexcept;\n",
      "content_length": 2273,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": "452\nAPPENDIX D\nC++ Thread Library reference\n    bool compare_exchange_weak(\n        integral-type & old_value,integral-type new_value,\n        memory_order success_order,memory_order failure_order)\n        volatile noexcept;\n    bool compare_exchange_weak(\n        integral-type & old_value,integral-type new_value,\n        memory_order success_order,memory_order failure_order) noexcept;\n    operator integral-type() const volatile noexcept;\n    operator integral-type() const noexcept;\n    integral-type fetch_add(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type fetch_add(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type fetch_sub(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type fetch_sub(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type fetch_and(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type fetch_and(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type fetch_or(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type fetch_or(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type fetch_xor(\n        integral-type,memory_order = memory_order_seq_cst)\n        volatile noexcept;\n    integral-type fetch_xor(\n        integral-type,memory_order = memory_order_seq_cst) noexcept;\n    integral-type operator++() volatile noexcept;\n    integral-type operator++() noexcept;\n    integral-type operator++(int) volatile noexcept;\n    integral-type operator++(int) noexcept;\n    integral-type operator--() volatile noexcept;\n    integral-type operator--() noexcept;\n    integral-type operator--(int) volatile noexcept;\n    integral-type operator--(int) noexcept;\n    integral-type operator+=(integral-type) volatile noexcept;\n    integral-type operator+=(integral-type) noexcept;\n    integral-type operator-=(integral-type) volatile noexcept;\n    integral-type operator-=(integral-type) noexcept;\n    integral-type operator&=(integral-type) volatile noexcept;\n    integral-type operator&=(integral-type) noexcept;\n    integral-type operator|=(integral-type) volatile noexcept;\n    integral-type operator|=(integral-type) noexcept;\n    integral-type operator^=(integral-type) volatile noexcept;\n    integral-type operator^=(integral-type) noexcept;\n};\n",
      "content_length": 2515,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": "453\n<atomic> header\nbool atomic_is_lock_free(volatile const atomic<integral-type>*) noexcept;\nbool atomic_is_lock_free(const atomic<integral-type>*) noexcept;\nvoid atomic_init(volatile atomic<integral-type>*,integral-type) noexcept;\nvoid atomic_init(atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_exchange(\n    volatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_exchange(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_exchange_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_exchange_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nvoid atomic_store(volatile atomic<integral-type>*,integral-type) noexcept;\nvoid atomic_store(atomic<integral-type>*,integral-type) noexcept;\nvoid atomic_store_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nvoid atomic_store_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_load(volatile const atomic<integral-type>*) noexcept;\nintegral-type atomic_load(const atomic<integral-type>*) noexcept;\nintegral-type atomic_load_explicit(\n    volatile const atomic<integral-type>*,memory_order) noexcept;\nintegral-type atomic_load_explicit(\n    const atomic<integral-type>*,memory_order) noexcept;\nbool atomic_compare_exchange_strong(\n    volatile atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value) noexcept;\nbool atomic_compare_exchange_strong(\n    atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value) noexcept;\nbool atomic_compare_exchange_strong_explicit(\n    volatile atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_strong_explicit(\n    atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_weak(\n    volatile atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value) noexcept;\nbool atomic_compare_exchange_weak(\n    atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value) noexcept;\nbool atomic_compare_exchange_weak_explicit(\n    volatile atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_weak_explicit(\n    atomic<integral-type>*,\n    integral-type * old_value,integral-type new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nintegral-type atomic_fetch_add(\n    volatile atomic<integral-type>*,integral-type) noexcept;\n",
      "content_length": 2801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": "454\nAPPENDIX D\nC++ Thread Library reference\nintegral-type atomic_fetch_add(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_add_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_add_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_sub(\n    volatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_sub(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_sub_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_sub_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_and(\n    volatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_and(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_and_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_and_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_or(\n    volatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_or(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_or_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_or_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_xor(\n    volatile atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_xor(\n    atomic<integral-type>*,integral-type) noexcept;\nintegral-type atomic_fetch_xor_explicit(\n    volatile atomic<integral-type>*,integral-type, memory_order) noexcept;\nintegral-type atomic_fetch_xor_explicit(\n    atomic<integral-type>*,integral-type, memory_order) noexcept;\nThose operations that are also provided by the primary template (see D.3.8) have\nthe same semantics.\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_ADD MEMBER FUNCTION \nAtomically loads a value and replaces it with the sum of that value and the supplied\nvalue i. \nDeclaration\nintegral-type fetch_add(\n    integral-type i,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nintegral-type fetch_add(\n    integral-type i,memory_order order = memory_order_seq_cst) noexcept;\n",
      "content_length": 2420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": "455\n<atomic> header\nEffects\nAtomically retrieves the existing value of *this and stores old-value + i in *this. \nReturns \nThe value of *this immediately prior to the store. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_ADD NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith that value plus the supplied value i.\nDeclaration\nintegral-type atomic_fetch_add(\n    volatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_add(\n    atomic<integral-type>* p, integral-type i) noexcept;\nEffects\nreturn p->fetch_add(i);\nSTD::ATOMIC_FETCH_ADD_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith that value plus the supplied value i.\nDeclaration\nintegral-type atomic_fetch_add_explicit(\n    volatile atomic<integral-type>* p, integral-type i,\n    memory_order order) noexcept;\nintegral-type atomic_fetch_add_explicit(\n    atomic<integral-type>* p, integral-type i, memory_order order)\n    noexcept;\nEffects\nreturn p->fetch_add(i,order);\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_SUB MEMBER FUNCTION \nAtomically loads a value and replaces it with the sum of that value and the supplied\nvalue i. \nDeclaration\nintegral-type fetch_sub(\n    integral-type i,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nintegral-type fetch_sub(\n    integral-type i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value - i in *this. \n",
      "content_length": 1627,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": "456\nAPPENDIX D\nC++ Thread Library reference\nReturns\nThe value of *this immediately prior to the store. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_SUB NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith that value minus the supplied value i.\nDeclaration\nintegral-type atomic_fetch_sub(\n    volatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_sub(\n    atomic<integral-type>* p, integral-type i) noexcept;\nEffects\nreturn p->fetch_sub(i);\nSTD::ATOMIC_FETCH_SUB_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith that value minus the supplied value i.\nDeclaration\nintegral-type atomic_fetch_sub_explicit(\n    volatile atomic<integral-type>* p, integral-type i,\n    memory_order order) noexcept;\nintegral-type atomic_fetch_sub_explicit(\n    atomic<integral-type>* p, integral-type i, memory_order order)\n    noexcept;\nEffects\nreturn p->fetch_sub(i,order);\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_AND MEMBER FUNCTION \nAtomically loads a value and replaces it with the bitwise-and of that value and the sup-\nplied value i. \nDeclaration\nintegral-type fetch_and(\n    integral-type i,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nintegral-type fetch_and(\n    integral-type i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value & i in *this. \nReturns\nThe value of *this immediately prior to the store. \n",
      "content_length": 1629,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": "457\n<atomic> header\nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_AND NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-and of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_and(\n    volatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_and(\n    atomic<integral-type>* p, integral-type i) noexcept;\nEffects\nreturn p->fetch_and(i);\nSTD::ATOMIC_FETCH_AND_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-and of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_and_explicit(\n    volatile atomic<integral-type>* p, integral-type i,\n    memory_order order) noexcept;\nintegral-type atomic_fetch_and_explicit(\n    atomic<integral-type>* p, integral-type i, memory_order order)\n    noexcept;\nEffects\nreturn p->fetch_and(i,order);\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_OR MEMBER FUNCTION \nAtomically loads a value and replaces it with the bitwise-or of that value and the sup-\nplied value i. \nDeclaration\nintegral-type fetch_or(\n    integral-type i,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nintegral-type fetch_or(\n    integral-type i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value | i in *this. \nReturns\nThe value of *this immediately prior to the store. \n",
      "content_length": 1575,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "458\nAPPENDIX D\nC++ Thread Library reference\nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_OR NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-or of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_or(\n    volatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_or(\n    atomic<integral-type>* p, integral-type i) noexcept;\nEffects\nreturn p->fetch_or(i);\nSTD::ATOMIC_FETCH_OR_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-or of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_or_explicit(\n    volatile atomic<integral-type>* p, integral-type i,\n    memory_order order) noexcept;\nintegral-type atomic_fetch_or_explicit(\n    atomic<integral-type>* p, integral-type i, memory_order order)\n    noexcept;\nEffects\nreturn p->fetch_or(i,order);\nSTD::ATOMIC<INTEGRAL-TYPE>::FETCH_XOR MEMBER FUNCTION \nAtomically loads a value and replaces it with the bitwise-xor of that value and the sup-\nplied value i. \nDeclaration\nintegral-type fetch_xor(\n    integral-type i,memory_order order = memory_order_seq_cst) \n    volatile noexcept;\nintegral-type fetch_xor(\n    integral-type i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value ^ i in *this. \nReturns\nThe value of *this immediately prior to the store. \nThrows\nNothing. \n",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "459\n<atomic> header\nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_XOR NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-xor of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_xor(\n    volatile atomic<integral-type>* p, integral-type i) noexcept;\nintegral-type atomic_fetch_xor(\n    atomic<integral-type>* p, integral-type i) noexcept;\nEffects\nreturn p->fetch_xor(i);\nSTD::ATOMIC_FETCH_XOR_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<integral-type> instance and replaces it\nwith the bitwise-xor of that value and the supplied value i.\nDeclaration\nintegral-type atomic_fetch_xor_explicit(\n    volatile atomic<integral-type>* p, integral-type i,\n    memory_order order) noexcept;\nintegral-type atomic_fetch_xor_explicit(\n    atomic<integral-type>* p, integral-type i, memory_order order)\n    noexcept;\nEffects\nreturn p->fetch_xor(i,order);\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR++ PREINCREMENT OPERATOR \nAtomically increments the value stored in *this and returns the new value. \nDeclaration\nintegral-type operator++() volatile noexcept;\nintegral-type operator++() noexcept;\nEffects\nreturn this->fetch_add(1) + 1; \nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR++ POSTINCREMENT OPERATOR \nAtomically increments the value stored in *this and returns the old value. \nDeclaration\nintegral-type operator++(int) volatile noexcept;\nintegral-type operator++(int) noexcept;\nEffects\nreturn this->fetch_add(1); \n",
      "content_length": 1576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "460\nAPPENDIX D\nC++ Thread Library reference\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR-- PREDECREMENT OPERATOR \nAtomically decrements the value stored in *this and returns the new value. \nDeclaration\nintegral-type operator--() volatile noexcept;\nintegral-type operator--() noexcept;\nEffects\nreturn this->fetch_sub(1) – 1; \nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR-- POSTDECREMENT OPERATOR \nAtomically decrements the value stored in *this and returns the old value. \nDeclaration\nintegral-type operator--(int) volatile noexcept;\nintegral-type operator--(int) noexcept;\nEffects\nreturn this->fetch_sub(1);\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR \nAtomically adds the supplied value to the value stored in *this and returns the new\nvalue. \nDeclaration\nintegral-type operator+=(integral-type i) volatile noexcept;\nintegral-type operator+=(integral-type i) noexcept;\nEffects\nreturn this->fetch_add(i) + i;\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR \nAtomically subtracts the supplied value from the value stored in *this and returns the\nnew value. \nDeclaration\nintegral-type operator-=(integral-type i) volatile noexcept;\nintegral-type operator-=(integral-type i) noexcept;\nEffects\nreturn this->fetch_sub(i,std::memory_order_seq_cst) – i;\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR&= COMPOUND ASSIGNMENT OPERATOR \nAtomically replaces the value stored in *this with the bitwise-and of the supplied\nvalue and the value stored in *this and returns the new value. \nDeclaration\nintegral-type operator&=(integral-type i) volatile noexcept;\nintegral-type operator&=(integral-type i) noexcept;\nEffects\nreturn this->fetch_and(i) & i;\n",
      "content_length": 1655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "461\n<atomic> header\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR|= COMPOUND ASSIGNMENT OPERATOR \nAtomically replaces the value stored in *this with the bitwise-or of the supplied value\nand the value stored in *this and returns the new value. \nDeclaration\nintegral-type operator|=(integral-type i) volatile noexcept;\nintegral-type operator|=(integral-type i) noexcept;\nEffects\nreturn this->fetch_or(i,std::memory_order_seq_cst) | i;\nSTD::ATOMIC<INTEGRAL-TYPE>::OPERATOR^= COMPOUND ASSIGNMENT OPERATOR \nAtomically replaces the value stored in *this with the bitwise-xor of the supplied\nvalue and the value stored in *this and returns the new value. \nDeclaration\nintegral-type operator^=(integral-type i) volatile noexcept;\nintegral-type operator^=(integral-type i) noexcept;\nEffects\nreturn this->fetch_xor(i,std::memory_order_seq_cst) ^ i; \nSTD::ATOMIC<T*> PARTIAL SPECIALIZATION \nThe std::atomic<T*> partial specialization of the std::atomic class template provides\nan atomic data type for each pointer type, with a comprehensive set of operations. \n Instances of std::atomic<T*> are not CopyConstructible or CopyAssignable,\nbecause these operations can’t be performed as a single atomic operation. \nClass definition\ntemplate<typename T>\nstruct atomic<T*>\n{\n    atomic() noexcept = default;\n    constexpr atomic(T*) noexcept;\n    bool operator=(T*) volatile;\n    bool operator=(T*);\n    atomic(const atomic&) = delete;\n    atomic& operator=(const atomic&) = delete;\n    atomic& operator=(const atomic&) volatile = delete;\n    bool is_lock_free() const volatile noexcept;\n    bool is_lock_free() const noexcept;\n    void store(T*,memory_order = memory_order_seq_cst) volatile noexcept;\n    void store(T*,memory_order = memory_order_seq_cst) noexcept;\n    T* load(memory_order = memory_order_seq_cst) const volatile noexcept;\n    T* load(memory_order = memory_order_seq_cst) const noexcept;\n    T* exchange(T*,memory_order = memory_order_seq_cst) volatile noexcept;\n    T* exchange(T*,memory_order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(\n        T* & old_value, T* new_value,\n        memory_order order = memory_order_seq_cst) volatile noexcept;\n",
      "content_length": 2163,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "462\nAPPENDIX D\nC++ Thread Library reference\n    bool compare_exchange_strong(\n        T* & old_value, T* new_value,\n        memory_order order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_strong(\n        T* & old_value, T* new_value,\n        memory_order success_order,memory_order failure_order)\n        volatile noexcept;\n    bool compare_exchange_strong(\n        T* & old_value, T* new_value,\n        memory_order success_order,memory_order failure_order) noexcept;\n    bool compare_exchange_weak(\n        T* & old_value, T* new_value,\n        memory_order order = memory_order_seq_cst) volatile noexcept;\n    bool compare_exchange_weak(\n        T* & old_value, T* new_value,\n        memory_order order = memory_order_seq_cst) noexcept;\n    bool compare_exchange_weak(\n        T* & old_value, T* new_value,\n        memory_order success_order,memory_order failure_order)\n        volatile noexcept;\n    bool compare_exchange_weak(\n        T* & old_value, T* new_value,\n        memory_order success_order,memory_order failure_order) noexcept;\n    operator T*() const volatile noexcept;\n    operator T*() const noexcept;\n    T* fetch_add(\n        ptrdiff_t,memory_order = memory_order_seq_cst) volatile noexcept;\n    T* fetch_add(\n        ptrdiff_t,memory_order = memory_order_seq_cst) noexcept;\n    T* fetch_sub(\n        ptrdiff_t,memory_order = memory_order_seq_cst) volatile noexcept;\n    T* fetch_sub(\n        ptrdiff_t,memory_order = memory_order_seq_cst) noexcept;\n    T* operator++() volatile noexcept;\n    T* operator++() noexcept;\n    T* operator++(int) volatile noexcept;\n    T* operator++(int) noexcept;\n    T* operator--() volatile noexcept;\n    T* operator--() noexcept;\n    T* operator--(int) volatile noexcept;\n    T* operator--(int) noexcept;\n    T* operator+=(ptrdiff_t) volatile noexcept;\n    T* operator+=(ptrdiff_t) noexcept;\n    T* operator-=(ptrdiff_t) volatile noexcept;\n    T* operator-=(ptrdiff_t) noexcept;\n};\nbool atomic_is_lock_free(volatile const atomic<T*>*) noexcept;\nbool atomic_is_lock_free(const atomic<T*>*) noexcept;\nvoid atomic_init(volatile atomic<T*>*, T*) noexcept;\nvoid atomic_init(atomic<T*>*, T*) noexcept;\nT* atomic_exchange(volatile atomic<T*>*, T*) noexcept;\nT* atomic_exchange(atomic<T*>*, T*) noexcept;\n",
      "content_length": 2265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "463\n<atomic> header\nT* atomic_exchange_explicit(volatile atomic<T*>*, T*, memory_order)\n    noexcept;\nT* atomic_exchange_explicit(atomic<T*>*, T*, memory_order) noexcept;\nvoid atomic_store(volatile atomic<T*>*, T*) noexcept;\nvoid atomic_store(atomic<T*>*, T*) noexcept;\nvoid atomic_store_explicit(volatile atomic<T*>*, T*, memory_order)\n    noexcept;\nvoid atomic_store_explicit(atomic<T*>*, T*, memory_order) noexcept;\nT* atomic_load(volatile const atomic<T*>*) noexcept;\nT* atomic_load(const atomic<T*>*) noexcept;\nT* atomic_load_explicit(volatile const atomic<T*>*, memory_order) noexcept;\nT* atomic_load_explicit(const atomic<T*>*, memory_order) noexcept;\nbool atomic_compare_exchange_strong(\n    volatile atomic<T*>*,T* * old_value,T* new_value) noexcept;\nbool atomic_compare_exchange_strong(\n    volatile atomic<T*>*,T* * old_value,T* new_value) noexcept;\nbool atomic_compare_exchange_strong_explicit(\n    atomic<T*>*,T* * old_value,T* new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_strong_explicit(\n    atomic<T*>*,T* * old_value,T* new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_weak(\n    volatile atomic<T*>*,T* * old_value,T* new_value) noexcept;\nbool atomic_compare_exchange_weak(\n    atomic<T*>*,T* * old_value,T* new_value) noexcept;\nbool atomic_compare_exchange_weak_explicit(\n    volatile atomic<T*>*,T* * old_value, T* new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nbool atomic_compare_exchange_weak_explicit(\n    atomic<T*>*,T* * old_value, T* new_value,\n    memory_order success_order,memory_order failure_order) noexcept;\nT* atomic_fetch_add(volatile atomic<T*>*, ptrdiff_t) noexcept;\nT* atomic_fetch_add(atomic<T*>*, ptrdiff_t) noexcept;\nT* atomic_fetch_add_explicit(\n    volatile atomic<T*>*, ptrdiff_t, memory_order) noexcept;\nT* atomic_fetch_add_explicit(\n    atomic<T*>*, ptrdiff_t, memory_order) noexcept;\nT* atomic_fetch_sub(volatile atomic<T*>*, ptrdiff_t) noexcept;\nT* atomic_fetch_sub(atomic<T*>*, ptrdiff_t) noexcept;\nT* atomic_fetch_sub_explicit(\n    volatile atomic<T*>*, ptrdiff_t, memory_order) noexcept;\nT* atomic_fetch_sub_explicit(\n    atomic<T*>*, ptrdiff_t, memory_order) noexcept;\nThose operations that are also provided by the primary template (see 11.3.8) have\nthe same semantics.\nSTD::ATOMIC<T*>::FETCH_ADD MEMBER FUNCTION \nAtomically loads a value and replaces it with the sum of that value and the supplied\nvalue i using standard pointer arithmetic rules, and returns the old value. \nDeclaration\nT* fetch_add(\n",
      "content_length": 2611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "464\nAPPENDIX D\nC++ Thread Library reference\n    ptrdiff_t i,memory_order order = memory_order_seq_cst)\n    volatile noexcept;\nT* fetch_add(\n    ptrdiff_t i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value + i in *this. \nReturns\nThe value of *this immediately prior to the store. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_ADD NONMEMBER FUNCTION \nAtomically reads the value from an atomic<T*> instance and replaces it with that\nvalue plus the supplied value i using standard pointer arithmetic rules.\nDeclaration\nT* atomic_fetch_add(volatile atomic<T*>* p, ptrdiff_t i) noexcept;\nT* atomic_fetch_add(atomic<T*>* p, ptrdiff_t i) noexcept;\nEffects\nreturn p->fetch_add(i);\nSTD::ATOMIC_FETCH_ADD_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<T*> instance and replaces it with that\nvalue plus the supplied value i using standard pointer arithmetic rules.\nDeclaration\nT* atomic_fetch_add_explicit(\n    volatile atomic<T*>* p, ptrdiff_t i,memory_order order) noexcept;\nT* atomic_fetch_add_explicit(\n    atomic<T*>* p, ptrdiff_t i, memory_order order) noexcept;\nEffects\nreturn p->fetch_add(i,order);\nSTD::ATOMIC<T*>::FETCH_SUB MEMBER FUNCTION \nAtomically loads a value and replaces it with that value minus the supplied value i\nusing standard pointer arithmetic rules, and returns the old value. \nDeclaration\nT* fetch_sub(\n    ptrdiff_t i,memory_order order = memory_order_seq_cst)\n    volatile noexcept;\nT* fetch_sub(\n    ptrdiff_t i,memory_order order = memory_order_seq_cst) noexcept;\nEffects\nAtomically retrieves the existing value of *this and stores old-value - i in *this. \n",
      "content_length": 1766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "465\n<atomic> header\nReturns\nThe value of *this immediately prior to the store. \nThrows\nNothing. \nNOTE\nThis is an atomic read-modify-write operation for the memory location\ncomprising *this. \nSTD::ATOMIC_FETCH_SUB NONMEMBER FUNCTION \nAtomically reads the value from an atomic<T*> instance and replaces it with that\nvalue minus the supplied value i using standard pointer arithmetic rules.\nDeclaration\nT* atomic_fetch_sub(volatile atomic<T*>* p, ptrdiff_t i) noexcept;\nT* atomic_fetch_sub(atomic<T*>* p, ptrdiff_t i) noexcept;\nEffects\nreturn p->fetch_sub(i);\nSTD::ATOMIC_FETCH_SUB_EXPLICIT NONMEMBER FUNCTION \nAtomically reads the value from an atomic<T*> instance and replaces it with that\nvalue minus the supplied value i using standard pointer arithmetic rules.\nDeclaration\nT* atomic_fetch_sub_explicit(\n    volatile atomic<T*>* p, ptrdiff_t i,memory_order order) noexcept;\nT* atomic_fetch_sub_explicit(\n    atomic<T*>* p, ptrdiff_t i, memory_order order) noexcept;\nEffects\nreturn p->fetch_sub(i,order);\nSTD::ATOMIC<T*>::OPERATOR++ PREINCREMENT OPERATOR \nAtomically increments the value stored in *this using standard pointer arithmetic\nrules and returns the new value. \nDeclaration\nT* operator++() volatile noexcept;\nT* operator++() noexcept;\nEffects\nreturn this->fetch_add(1) + 1;\nSTD::ATOMIC<T*>::OPERATOR++ POSTINCREMENT OPERATOR \nAtomically increments the value stored in *this and returns the old value. \nDeclaration\nT* operator++(int) volatile noexcept;\nT* operator++(int) noexcept;\nEffects\nreturn this->fetch_add(1);\n",
      "content_length": 1526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "466\nAPPENDIX D\nC++ Thread Library reference\nSTD::ATOMIC<T*>::OPERATOR-- PREDECREMENT OPERATOR \nAtomically decrements the value stored in *this using standard pointer arithmetic\nrules and returns the new value. \nDeclaration\nT* operator--() volatile noexcept;\nT* operator--() noexcept;\nEffects\nreturn this->fetch_sub(1) - 1;\nSTD::ATOMIC<T*>::OPERATOR-- POSTDECREMENT OPERATOR \nAtomically decrements the value stored in *this using standard pointer arithmetic\nrules and returns the old value. \nDeclaration\nT* operator--(int) volatile noexcept;\nT* operator--(int) noexcept;\nEffects\nreturn this->fetch_sub(1);\nSTD::ATOMIC<T*>::OPERATOR+= COMPOUND ASSIGNMENT OPERATOR \nAtomically adds the supplied value to the value stored in *this using standard pointer\narithmetic rules and returns the new value. \nDeclaration\nT* operator+=(ptrdiff_t i) volatile noexcept;\nT* operator+=(ptrdiff_t i) noexcept;\nEffects\nreturn this->fetch_add(i) + i;\nSTD::ATOMIC<T*>::OPERATOR-= COMPOUND ASSIGNMENT OPERATOR \nAtomically subtracts the supplied value from the value stored in *this using standard\npointer arithmetic rules and returns the new value. \nDeclaration\nT* operator-=(ptrdiff_t i) volatile noexcept;\nT* operator-=(ptrdiff_t i) noexcept;\nEffects\nreturn this->fetch_sub(i) - i;\nD.4\n<future> header \nThe <future> header provides facilities for handling asynchronous results from oper-\nations that may be performed on another thread. \nHeader contents\nnamespace std\n{\n    enum class future_status {\n        ready, timeout, deferred };\n",
      "content_length": 1514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "467\n<future> header\n    enum class future_errc\n    {\n        broken_promise,\n        future_already_retrieved,\n        promise_already_satisfied,\n        no_state\n    };\n    class future_error;\n    const error_category& future_category();\n    error_code make_error_code(future_errc e);\n    error_condition make_error_condition(future_errc e);\n    template<typename ResultType>\n    class future;\n    template<typename ResultType>\n    class shared_future;\n    template<typename ResultType>\n    class promise;\n    template<typename FunctionSignature>\n    class packaged_task; // no definition provided\n    template<typename ResultType,typename ... Args>\n    class packaged_task<ResultType (Args...)>;\n    enum class launch {\n        async, deferred\n    };\n    template<typename FunctionType,typename ... Args>\n    future<result_of<FunctionType(Args...)>::type>\n    async(FunctionType&& func,Args&& ... args);\n    template<typename FunctionType,typename ... Args>\n    future<result_of<FunctionType(Args...)>::type>\n    async(std::launch policy,FunctionType&& func,Args&& ... args);\n}\nD.4.1\nstd::future class template \nThe std::future class template provides a means of waiting for an asynchronous\nresult from another thread, in conjunction with the std::promise and std:: pack-\naged_task class templates and the std::async function template, which can be used\nto provide that asynchronous result. Only one std::future instance references any\ngiven asynchronous result at any time. \n Instances of std::future are MoveConstructible and MoveAssignable but not\nCopyConstructible or CopyAssignable. \nClass definition\ntemplate<typename ResultType>\nclass future\n{\n",
      "content_length": 1653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "468\nAPPENDIX D\nC++ Thread Library reference\npublic:\n    future() noexcept;\n    future(future&&) noexcept;\n    future& operator=(future&&) noexcept;\n    ~future();\n    future(future const&) = delete;\n    future& operator=(future const&) = delete;\n    shared_future<ResultType> share();\n    bool valid() const noexcept;\n    see description get();\n    void wait();\n    template<typename Rep,typename Period>\n    future_status wait_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock,typename Duration>\n    future_status wait_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n};\nSTD::FUTURE DEFAULT CONSTRUCTOR \nConstructs an std::future object without an associated asynchronous result. \nDeclaration\nfuture() noexcept;\nEffects\nConstructs a new std::future instance. \nPostconditions\nvalid() returns false. \nThrows\nNothing. \nSTD::FUTURE MOVE CONSTRUCTOR \nConstructs one std::future object from another, transferring ownership of the asyn-\nchronous result associated with the other std::future object to the newly con-\nstructed instance.\nDeclaration \nfuture(future&& other) noexcept;\nEffects\nMove-constructs a new std::future instance from other. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with the newly constructed std::future object. other has\nno associated asynchronous result. this->valid() returns the same value that\nother.valid() returned before the invocation of this constructor. other\n.valid() returns false. \n",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "469\n<future> header\nThrows\nNothing. \nSTD::FUTURE MOVE ASSIGNMENT OPERATOR \nTransfers ownership of the asynchronous result associated with the one std::future\nobject to another. \nDeclaration\nfuture(future&& other) noexcept;\nEffects\nTransfers ownership of an asynchronous state between std::future instances. \nPostconditions \nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with *this. other has no associated asynchronous result. The\nownership of the asynchronous state (if any) associated with *this prior to the call\nis released, and the state destroyed if this is the last reference. this->valid()\nreturns the same value that other.valid() returned before the invocation of this\nconstructor. other.valid() returns false. \nThrows\nNothing. \nSTD::FUTURE DESTRUCTOR \nDestroys an std::future object. \nDeclaration\n~future();\nEffects\nDestroys *this. If this is the last reference to the asynchronous result associated\nwith *this (if any), then destroy that asynchronous result. \nThrows\nNothing.\nSTD::FUTURE::SHARE MEMBER FUNCTION \nConstructs a new std::shared_future instance and transfers ownership of the asyn-\nchronous result associated with *this to this newly constructed std::shared_future\ninstance. \nDeclaration\nshared_future<ResultType> share();\nEffects\nAs-if shared_future<ResultType>(std::move(*this)). \nPostconditions\nThe asynchronous result associated with *this prior to the invocation of share()\n(if any) is associated with the newly constructed std::shared_future instance.\nthis->valid() returns false. \n",
      "content_length": 1571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "470\nAPPENDIX D\nC++ Thread Library reference\nThrows\nNothing. \nSTD::FUTURE::VALID MEMBER FUNCTION \nChecks if an std::future instance is associated with an asynchronous result. \nDeclaration\nbool valid() const noexcept;\nReturns\ntrue if *this has an associated asynchronous result, false otherwise. \nThrows \nNothing. \nSTD::FUTURE::WAIT MEMBER FUNCTION \nIf the state associated with *this contains a deferred function, invokes the deferred\nfunction. Otherwise, waits until the asynchronous result associated with an instance of\nstd::future is ready. \nDeclaration\nvoid wait();\nPreconditions \nthis->valid() would return true. \nEffects\nIf the associated state contains a deferred function, invokes the deferred function\nand stores the returned value or thrown exception as the asynchronous result. Oth-\nerwise, blocks until the asynchronous result associated with *this is ready. \nThrows\nNothing. \nSTD::FUTURE::WAIT_FOR MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::future is\nready or until a specified time period has elapsed. \nDeclaration\ntemplate<typename Rep,typename Period>\nfuture_status wait_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nthis->valid() would return true. \nEffects\nIf the asynchronous result associated with *this contains a deferred function arising\nfrom a call to std::async that hasn’t yet started execution, returns immediately with-\nout blocking. Otherwise blocks until the asynchronous result associated with *this is\nready or the time period specified by relative_time has elapsed. \nReturns\nstd::future_status::deferred if the asynchronous result associated with *this\ncontains a deferred function arising from a call to std::async that hasn’t yet\n",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "471\n<future> header\nstarted execution, std::future_status::ready if the asynchronous result associ-\nated with *this is ready, std::future_status::timeout if the time period speci-\nfied by relative_time has elapsed. \nNOTE\nThe thread may be blocked for longer than the specified duration.\nWhere possible, the elapsed time is determined by a steady clock. \nThrows\nNothing. \nSTD::FUTURE::WAIT_UNTIL MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::future is\nready or until a specified time period has elapsed.\nDeclaration\ntemplate<typename Clock,typename Duration>\nfuture_status wait_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions \nthis->valid() would return true. \nEffects\nIf the asynchronous result associated with *this contains a deferred function aris-\ning from a call to std::async that hasn’t yet started execution, returns immediately\nwithout blocking. Otherwise blocks until the asynchronous result associated with\n*this is ready or Clock::now() returns a time equal to or later than absolute_\ntime. \nReturns\nstd::future_status::deferred if the asynchronous result associated with *this\ncontains a deferred function arising from a call to std::async that hasn’t yet\nstarted execution, std::future_status::ready if the asynchronous result associ-\nated with *this is ready, std::future_status::timeout if Clock::now() returns a\ntime equal to or later than absolute_time. \nNOTE\nThere’s no guarantee as to how long the calling thread will be\nblocked, only that if the function returns std::future_status::timeout,\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked. \nThrows\nNothing. \nSTD::FUTURE::GET MEMBER FUNCTION \nIf the associated state contains a deferred function from a call to std::async, invokes\nthat function and returns the result; otherwise, waits until the asynchronous result\nassociated with an instance of std::future is ready, and then returns the stored value\nor throws the stored exception. \n",
      "content_length": 2062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "472\nAPPENDIX D\nC++ Thread Library reference\nDeclaration\nvoid future<void>::get();\nR& future<R&>::get();\nR future<R>::get();\nPreconditions\nthis->valid() would return true. \nEffects\nIf the state associated with *this contains a deferred function, invokes the deferred\nfunction and returns the result or propagates any thrown exception. \nOtherwise, blocks until the asynchronous result associated with *this is ready.\nIf the result is a stored exception, throws that exception. Otherwise, returns the\nstored value. \nReturns\nIf the associated state contains a deferred function, the result of the function invo-\ncation is returned. Otherwise, if ResultType is void, the call returns normally. If\nResultType is R& for some type R, the stored reference is returned. Otherwise, the\nstored value is returned. \nThrows\nThe exception thrown by the deferred exception or stored in the asynchronous\nresult, if any. \nPostcondition\nthis->valid()==false \nD.4.2\nstd::shared_future class template \nThe std::shared_future class template provides a means of waiting for an asynchro-\nnous result from another thread, in conjunction with the std::promise and std::\npackaged_task class templates and the std::async function template, which can be\nused to provide that asynchronous result. Multiple std::shared_future instances can\nreference the same asynchronous result. \n Instances of std::shared_future are CopyConstructible and CopyAssignable.\nYou can also move-construct a std::shared_future from a std::future with the\nsame ResultType. \n Accesses to a given instance of std::shared_future aren’t synchronized. It’s\ntherefore not safe for multiple threads to access the same std::shared_future\ninstance without external synchronization. But accesses to the associated state are syn-\nchronized, so it is safe for multiple threads to each access separate instances of std::\nshared_future that share the same associated state without external synchronization. \nClass definition\ntemplate<typename ResultType>\nclass shared_future\n{\npublic:\n    shared_future() noexcept;\n    shared_future(future<ResultType>&&) noexcept;\n",
      "content_length": 2096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "473\n<future> header\n    shared_future(shared_future&&) noexcept;\n    shared_future(shared_future const&);\n    shared_future& operator=(shared_future const&);\n    shared_future& operator=(shared_future&&) noexcept;\n    ~shared_future();\n    bool valid() const noexcept;\n    see description get() const;\n    void wait() const;\n    template<typename Rep,typename Period>\n    future_status wait_for(\n        std::chrono::duration<Rep,Period> const& relative_time) const;\n    template<typename Clock,typename Duration>\n    future_status wait_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time) const;\n};\nSTD::SHARED_FUTURE DEFAULT CONSTRUCTOR \nConstructs an std::shared_future object without an associated asynchronous result. \nDeclaration\nshared_future() noexcept;\nEffects\nConstructs a new std::shared_future instance. \nPostconditions\nvalid() returns false for the newly constructed instance. \nThrows\nNothing. \nSTD::SHARED_FUTURE MOVE CONSTRUCTOR \nConstructs one std::shared_future object from another, transferring ownership of\nthe asynchronous result associated with the other std::shared_future object to the\nnewly constructed instance. \nDeclaration\nshared_future(shared_future&& other) noexcept;\nEffects\nConstructs a new std::shared_future instance. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with the newly constructed std::shared_future object. other\nhas no associated asynchronous result. \nThrows\nNothing. \n",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "474\nAPPENDIX D\nC++ Thread Library reference\nSTD::SHARED_FUTURE MOVE-FROM-STD::FUTURE CONSTRUCTOR \nConstructs an std::shared_future object from astd::future, transferring owner-\nship of the asynchronous result associated with the std::future object to the newly\nconstructed std::shared_future. \nDeclaration\nshared_future(std::future<ResultType>&& other) noexcept;\nEffects\nConstructs a new std::shared_future instance. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with the newly constructed std::shared_future object. other\nhas no associated asynchronous result. \nThrows\nNothing. \nSTD::SHARED_FUTURE COPY CONSTRUCTOR \nConstructs one std::shared_future object from another, so that both the source and\nthe copy refer to the asynchronous result associated with the source std::shared_\nfuture object, if any. \nDeclaration\nshared_future(shared_future const& other);\nEffects\nConstructs a new std::shared_future instance. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with the newly constructed std::shared_future object and\nother. \nThrows\nNothing. \nSTD::SHARED_FUTURE DESTRUCTOR \nDestroys an std::shared_future object. \nDeclaration\n~shared_future();\nEffects\nDestroys *this. If there’s no longer an std::promise or std::packaged_task\ninstance associated with the asynchronous result associated with *this, and this is\nthe last std::shared_future instance associated with that asynchronous result,\ndestroys that asynchronous result. \nThrows\nNothing. \n",
      "content_length": 1593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": "475\n<future> header\nSTD::SHARED_FUTURE::VALID MEMBER FUNCTION \nChecks if an std::shared_future instance is associated with an asynchronous result. \nDeclaration\nbool valid() const noexcept;\nReturns\ntrue if *this has an associated asynchronous result, false otherwise. \nThrows\nNothing. \nSTD::SHARED_FUTURE::WAIT MEMBER FUNCTION \nIf the state associated with *this contains a deferred function, invokes the deferred\nfunction. Otherwise, waits until the asynchronous result associated with an instance of\nstd::shared_future is ready. \nDeclaration\nvoid wait() const;\nPreconditions\nthis->valid() would return true. \nEffects\nCalls to get() and wait() from multiple threads on std::shared_future instances\nthat share the same associated state are serialized. If the associated state contains a\ndeferred function, the first call to get() or wait() invokes the deferred function\nand stores the returned value or thrown exception as the asynchronous result. \nBlocks until the asynchronous result associated with *this is ready. \nThrows\nNothing. \nSTD::SHARED_FUTURE::WAIT_FOR MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::shared_\nfuture is ready or until a specified time period has elapsed. \nDeclaration\ntemplate<typename Rep,typename Period>\nfuture_status wait_for(\n    std::chrono::duration<Rep,Period> const& relative_time) const;\nPreconditions \nthis->valid() would return true. \nEffects \nIf the asynchronous result associated with *this contains a deferred function aris-\ning from a call to std::async that has not yet started execution, returns immedi-\nately without blocking. Otherwise, blocks until the asynchronous result associated\nwith *this is ready or the time period specified by relative_time has elapsed. \nReturns\nstd::future_status::deferred if the asynchronous result associated with *this\ncontains a deferred function arising from a call to std::async that hasn’t yet\n",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": "476\nAPPENDIX D\nC++ Thread Library reference\nstarted execution, std::future_status::ready if the asynchronous result associ-\nated with *this is ready, std::future_status::timeout if the time period speci-\nfied by relative_time has elapsed. \nNOTE\nThe thread may be blocked for longer than the specified duration.\nWhere possible, the elapsed time is determined by a steady clock. \nThrows\nNothing. \nSTD::SHARED_FUTURE::WAIT_UNTIL MEMBER FUNCTION \nWaits until the asynchronous result associated with an instance of std::shared_\nfuture is ready or until a specified time period has elapsed. \nDeclaration\ntemplate<typename Clock,typename Duration>\nbool wait_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time) const;\nPreconditions \nthis->valid() would return true. \nEffects\nIf the asynchronous result associated with *this contains a deferred function aris-\ning from a call to std::async that hasn’t yet started execution, returns immediately\nwithout blocking. Otherwise, blocks until the asynchronous result associated with\n*this is ready or Clock::now() returns a time equal to or later than absolute_\ntime. \nReturns\nstd::future_status::deferred if the asynchronous result associated with *this\ncontains a deferred function arising from a call to std::async that hasn’t yet\nstarted execution, std::future_status::ready if the asynchronous result associ-\nated with *this is ready, std::future_status::timeout if Clock::now() returns a\ntime equal to or later than absolute_time. \nNOTE\nThere’s no guarantee as to how long the calling thread will be\nblocked, only that if the function returns std::future_status::timeout,\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked. \nThrows\nNothing. \nSTD::SHARED_FUTURE::GET MEMBER FUNCTION \nIf the associated state contains a deferred function from a call to std::async, invokes\nthat function and return the result. Otherwise, waits until the asynchronous result\nassociated with an instance of std::shared_future is ready, and then returns the\nstored value or throws the stored exception. \n",
      "content_length": 2113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": "477\n<future> header\nDeclaration\nvoid shared_future<void>::get() const;\nR& shared_future<R&>::get() const;\nR const& shared_future<R>::get() const;\nPreconditions\nthis->valid() would return true. \nEffects\nCalls to get() and wait() from multiple threads on std::shared_future instances\nthat share the same associated state are serialized. If the associated state contains a\ndeferred function, the first call to get() or wait() invokes the deferred function\nand stores the returned value or thrown exception as the asynchronous result. \nBlocks until the asynchronous result associated with *this is ready. If the asyn-\nchronous result is a stored exception, throws that exception. Otherwise, returns the\nstored value. \nReturns\nIf ResultType is void, returns normally. If ResultType is R& for some type R, returns\nthe stored reference. Otherwise, returns a const reference to the stored value. \nThrows\nThe stored exception, if any. \nD.4.3\nstd::packaged_task class template \nThe std::packaged_task class template packages a function or other callable object\nso that when the function is invoked through the std::packaged_task instance, the\nresult is stored as an asynchronous result for retrieval through an instance of\nstd::future. \n Instances of std::packaged_task are MoveConstructible and MoveAssignable\nbut not CopyConstructible or CopyAssignable. \nClass definition\ntemplate<typename FunctionType>\nclass packaged_task; // undefined\ntemplate<typename ResultType,typename... ArgTypes>\nclass packaged_task<ResultType(ArgTypes...)>\n{\npublic:\n    packaged_task() noexcept;\n    packaged_task(packaged_task&&) noexcept;\n    ~packaged_task();\n    packaged_task& operator=(packaged_task&&) noexcept;\n    packaged_task(packaged_task const&) = delete;\n    packaged_task& operator=(packaged_task const&) = delete;\n    void swap(packaged_task&) noexcept;\n",
      "content_length": 1840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": null,
      "content": "478\nAPPENDIX D\nC++ Thread Library reference\n    template<typename Callable>\n    explicit packaged_task(Callable&& func);\n    template<typename Callable,typename Allocator>\n    packaged_task(std::allocator_arg_t, const Allocator&,Callable&&);\n    bool valid() const noexcept;\n    std::future<ResultType> get_future();\n    void operator()(ArgTypes...);\n    void make_ready_at_thread_exit(ArgTypes...);\n    void reset();\n};\nSTD::PACKAGED_TASK DEFAULT CONSTRUCTOR \nConstructs an std::packaged_task object. \nDeclaration\npackaged_task() noexcept;\nEffects \nConstructs an std::packaged_task instance with no associated task or asynchro-\nnous result. \nThrows\nNothing. \nSTD::PACKAGED_TASK CONSTRUCTION FROM A CALLABLE OBJECT \nConstructs an std::packaged_task object with an associated task and asynchronous\nresult. \nDeclaration\ntemplate<typename Callable>\npackaged_task(Callable&& func);\nPreconditions\nThe expression func(args...) shall be valid, where each element args-i in args...\nshall be a value of the corresponding type ArgTypes-i in ArgTypes.... The return\nvalue shall be convertible to ResultType. \nEffects\nConstructs an std::packaged_task instance with an associated asynchronous\nresult of type ResultType that isn’t ready and an associated task of type Callable\nthat’s a copy of func. \nThrows\nAn exception of type std::bad_alloc if the constructor is unable to allocate mem-\nory for the asynchronous result. Any exception thrown by the copy or move con-\nstructor of Callable. \nSTD::PACKAGED_TASK CONSTRUCTION FROM A CALLABLE OBJECT WITH AN ALLOCATOR\nConstructs an std::packaged_task object with an associated task and asynchronous\nresult, using the supplied allocator to allocate memory for the associated asynchro-\nnous result and task. \n",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": null,
      "content": "479\n<future> header\nDeclaration\ntemplate<typename Allocator,typename Callable>\npackaged_task(\n    std::allocator_arg_t, Allocator const& alloc,Callable&& func);\nPreconditions\nThe expression func(args...) shall be valid, where each element args-i in args...\nshall be a value of the corresponding type ArgTypes-i in ArgTypes.... The return\nvalue shall be convertible to ResultType. \nEffects\nConstructs an std::packaged_task instance with an associated asynchronous\nresult of type ResultType that isn’t ready and an associated task of type Callable\nthat’s a copy of func. The memory for the asynchronous result and task is allocated\nthrough the allocator alloc or a copy thereof.\nThrows\nAny exception thrown by the allocator when trying to allocate memory for the asyn-\nchronous result or task. Any exception thrown by the copy or move constructor of\nCallable. \nSTD::PACKAGED_TASK MOVE CONSTRUCTOR \nConstructs one std::packaged_task object from another, transferring ownership of\nthe asynchronous result and task associated with the other std::packaged_task\nobject to the newly constructed instance. \nDeclaration\npackaged_task(packaged_task&& other) noexcept;\nEffects\nConstructs a new std::packaged_task instance. \nPostconditions\nThe asynchronous result and task associated with other prior to the invocation of\nthe constructor is associated with the newly constructed std::packaged_task\nobject. other has no associated asynchronous result. \nThrows\nNothing. \nSTD::PACKAGED_TASK MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of the asynchronous result associated with one std::packaged_\ntask object to another. \nDeclaration\npackaged_task& operator=(packaged_task&& other) noexcept;\nEffects\nTransfers ownership of the asynchronous result and task associated with other to\n*this, and discards any prior asynchronous result, as-if std::packaged_task(other)\n.swap(*this).\n",
      "content_length": 1864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 503,
      "chapter": null,
      "content": "480\nAPPENDIX D\nC++ Thread Library reference\nPostconditions\nThe asynchronous result and task associated with other prior to the invocation of\nthe move-assignment operator is associated with *this. other has no associated\nasynchronous result. \nReturns\n*this \nThrows\nNothing. \nSTD::PACKAGED_TASK::SWAP MEMBER FUNCTION \nExchanges ownership of the asynchronous results associated with two std::packaged\n_task objects. \nDeclaration\nvoid swap(packaged_task& other) noexcept;\nEffects \nExchanges ownership of the asynchronous results and tasks associated with other\nand *this. \nPostconditions\nThe asynchronous result and task associated with other prior to the invocation of\nswap (if any) is associated with *this. The asynchronous result and task associated\nwith *this prior to the invocation of swap (if any) is associated with other. \nThrows\nNothing. \nSTD::PACKAGED_TASK DESTRUCTOR \nDestroys an std::packaged_task object. \nDeclaration\n~packaged_task();\nEffects\nDestroys *this. If *this has an associated asynchronous result, and that result\ndoesn’t have a stored task or exception, then that result becomes ready with an\nstd::future_error exception with an error code of std::future_errc::broken\n_promise. \nThrows\nNothing. \nSTD::PACKAGED_TASK::GET_FUTURE MEMBER FUNCTION \nRetrieves an std::future instance for the asynchronous result associated with *this. \nDeclaration\nstd::future<ResultType> get_future();\nPreconditions \n*this has an associated asynchronous result. \n",
      "content_length": 1463,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": null,
      "content": "481\n<future> header\nReturns\nAn std::future instance for the asynchronous result associated with *this. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::future_already_retrieved if a std::future has already been obtained for\nthis asynchronous result through a prior call to get_future(). \nSTD::PACKAGED_TASK::RESET MEMBER FUNCTION \nAssociates an std::packaged_task instance with a new asynchronous result for the\nsame task. \nDeclaration\nvoid reset();\nPreconditions\n*this has an associated asynchronous task. \nEffects\nAs-if *this=packaged_task(std::move(f)), where f is the stored task associated\nwith *this. \nThrows\nAn exception of type std::bad_alloc if memory couldn’t be allocated for the new\nasynchronous result. \nSTD::PACKAGED_TASK::VALID MEMBER FUNCTION \nChecks whether *this has an associated task and asynchronous result. \nDeclaration\nbool valid() const noexcept;\nReturns\ntrue if *this has an associated task and asynchronous result, false otherwise. \nThrows\nNothing. \nSTD::PACKAGED_TASK::OPERATOR() FUNCTION CALL OPERATOR \nInvokes the task associated with an std::packaged_task instance, and stores the return\nvalue or exception in the associated asynchronous result. \nDeclaration\nvoid operator()(ArgTypes... args);\nPreconditions\n*this has an associated task. \nEffects\nInvokes the associated task func as-if INVOKE(func,args...). If the invocation\nreturns normally, stores the return value in the asynchronous result associated with\n*this. If the invocation returns with an exception, stores the exception in the asyn-\nchronous result associated with *this. \n",
      "content_length": 1606,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": null,
      "content": "482\nAPPENDIX D\nC++ Thread Library reference\nPostconditions\nThe asynchronous result associated with *this is ready with a stored value or excep-\ntion. Any threads blocked waiting for the asynchronous result are unblocked. \nThrows\nAn exception of type std::future_error with an error code of std::future_errc::\npromise_already_satisfied if the asynchronous result already has a stored value\nor exception.\nSynchronization\nA successful call to the function call operator synchronizes-with a call to std::\nfuture<ResultType>::get() or std::shared_future<ResultType>::get(), which\nretrieves the value or exception stored. \nSTD::PACKAGED_TASK::MAKE_READY_AT_THREAD_EXIT MEMBER FUNCTION \nInvokes the task associated with an std::packaged_task instance, and stores the\nreturn value or exception in the associated asynchronous result without making the\nassociated asynchronous result ready until thread exit. \nDeclaration\nvoid make_ready_at_thread_exit(ArgTypes... args);\nPreconditions\n*this has an associated task. \nEffects\nInvokes the associated task func as-if INVOKE(func,args...). If the invocation\nreturns normally, stores the return value in the asynchronous result associated with\n*this. If the invocation returns with an exception, stores the exception in the asyn-\nchronous result associated with *this. Schedules the associated asynchronous state\nto be made ready when the current thread exits. \nPostconditions\nThe asynchronous result associated with *this has a stored value or exception but\nisn’t ready until the current thread exits. Threads blocked waiting for the asynchro-\nnous result will be unblocked when the current thread exits. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::promise_already_satisfied if the asynchronous result already has a stored\nvalue or exception. An exception of type std::future_error with an error code of\nstd::future_errc::no_state if *this has no associated asynchronous state. \nSynchronization\nThe completion of the thread that made a successful call to make_ready_at_thread_\nexit() synchronizes-with a call to std::future<ResultType>::get() or std::shared\n_future<ResultType>::get(), which retrieves the value or exception stored. \n",
      "content_length": 2214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": null,
      "content": "483\n<future> header\nD.4.4\nstd::promise class template \nThe std::promise class template provides a means of setting an asynchronous result,\nwhich may be retrieved from another thread through an instance of std::future. \n The ResultType template parameter is the type of the value that can be stored in\nthe asynchronous result. \n A std::future associated with the asynchronous result of a particular std::promise\ninstance can be obtained by calling the get_future() member function. The asyn-\nchronous result is set either to a value of type ResultType with the set_value() mem-\nber function or to an exception with the set_exception() member function. \n Instances of std::promise are MoveConstructible and MoveAssignable but not\nCopyConstructible or CopyAssignable. \nClass definition\ntemplate<typename ResultType>\nclass promise\n{\npublic:\n    promise();\n    promise(promise&&) noexcept;\n    ~promise();\n    promise& operator=(promise&&) noexcept;\n    template<typename Allocator>\n    promise(std::allocator_arg_t, Allocator const&);\n    promise(promise const&) = delete;\n    promise& operator=(promise const&) = delete;\n    void swap(promise& ) noexcept;\n    std::future<ResultType> get_future();\n    void set_value(see description);\n    void set_exception(std::exception_ptr p);\n};\nSTD::PROMISE DEFAULT CONSTRUCTOR \nConstructs an std::promise object. \nDeclaration\npromise();\nEffects \nConstructs an std::promise instance with an associated asynchronous result of\ntype ResultType that’s not ready. \nThrows\nAn exception of type std::bad_alloc if the constructor is unable to allocate mem-\nory for the asynchronous result. \nSTD::PROMISE ALLOCATOR CONSTRUCTOR \nConstructs an std::promise object, using the supplied allocator to allocate memory\nfor the associated asynchronous result. \n",
      "content_length": 1779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": null,
      "content": "484\nAPPENDIX D\nC++ Thread Library reference\nDeclaration\ntemplate<typename Allocator>\npromise(std::allocator_arg_t, Allocator const& alloc);\nEffects\nConstructs an std::promise instance with an associated asynchronous result of\ntype ResultType that isn’t ready. The memory for the asynchronous result is allo-\ncated through the allocator alloc. \nThrows\nAny exception thrown by the allocator when attempting to allocate memory for the\nasynchronous result. \nSTD::PROMISE MOVE CONSTRUCTOR \nConstructs one std::promise object from another, transferring ownership of the\nasynchronous result associated with the other std::promise object to the newly con-\nstructed instance. \nDeclaration\npromise(promise&& other) noexcept;\nEffects\nConstructs a new std::promise instance. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the con-\nstructor is associated with the newly constructed std::promise object. other has\nno associated asynchronous result. \nThrows\nNothing. \nSTD::PROMISE MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of the asynchronous result associated with one std::promise\nobject to another. \nDeclaration\npromise& operator=(promise&& other) noexcept;\nEffects\nTransfers ownership of the asynchronous result associated with other to *this. If\n*this already had an associated asynchronous result, that asynchronous result is\nmade ready with an exception of type std::future_error and an error code of\nstd::future_errc::broken_promise. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of the move-\nassignment operator is associated with *this. other has no associated asynchro-\nnous result. \nReturns\n*this \n",
      "content_length": 1681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": null,
      "content": "485\n<future> header\nThrows\nNothing. \nSTD::PROMISE::SWAP MEMBER FUNCTION \nExchanges ownership of the asynchronous results associated with two std::promise\nobjects. \nDeclaration\nvoid swap(promise& other);\nEffects\nExchanges ownership of the asynchronous results associated with other and *this. \nPostconditions\nThe asynchronous result associated with other prior to the invocation of swap (if\nany) is associated with *this. The asynchronous result associated with *this prior\nto the invocation of swap (if any) is associated with other.\nThrows\nNothing. \nSTD::PROMISE DESTRUCTOR \nDestroys an std::promise object. \nDeclaration\n~promise();\nEffects\nDestroys *this. If *this has an associated asynchronous result, and that result doesn’t\nhave a stored value or exception, that result becomes ready with an std::future_\nerror exception with an error code of std::future_errc::broken_promise. \nThrows\nNothing. \nSTD::PROMISE::GET_FUTURE MEMBER FUNCTION \nRetrieves an std::future instance for the asynchronous result associated with *this. \nDeclaration\nstd::future<ResultType> get_future();\nPreconditions\n*this has an associated asynchronous result. \nReturns\nAn std::future instance for the asynchronous result associated with *this. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::future_already_retrieved if a std::future has already been obtained for\nthis asynchronous result through a prior call to get_future(). \n \n",
      "content_length": 1448,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": null,
      "content": "486\nAPPENDIX D\nC++ Thread Library reference\nSTD::PROMISE::SET_VALUE MEMBER FUNCTION \nStores a value in the asynchronous result associated with *this. \nDeclaration\nvoid promise<void>::set_value();\nvoid promise<R&>::set_value(R& r);\nvoid promise<R>::set_value(R const& r);\nvoid promise<R>::set_value(R&& r);\nPreconditions\n*this has an associated asynchronous result. \nEffects\nStores r in the asynchronous result associated with *this if ResultType isn’t void. \nPostconditions\nThe asynchronous result associated with *this is ready with a stored value. Any\nthreads blocked waiting for the asynchronous result are unblocked. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::promise_already_satisfied if the asynchronous result already has a stored\nvalue or exception. Any exceptions thrown by the copy-constructor or move-con-\nstructor of r. \nSynchronization\nMultiple concurrent calls to set_value(), set_value_at_thread_exit(), set_\nexception(), and set_exception_at_thread_exit() are serialized. A successful\ncall to set_value() happens-before a call to std::future<ResultType>::get() or\nstd::shared_future<ResultType>::get(), which retrieves the value stored. \nSTD::PROMISE::SET_VALUE_AT_THREAD_EXIT MEMBER FUNCTION \nStores a value in the asynchronous result associated with *this without making that\nresult ready until the current thread exits. \nDeclaration\nvoid promise<void>::set_value_at_thread_exit();\nvoid promise<R&>::set_value_at_thread_exit(R& r);\nvoid promise<R>::set_value_at_thread_exit(R const& r);\nvoid promise<R>::set_value_at_thread_exit(R&& r);\nPreconditions\n*this has an associated asynchronous result. \nEffects\nStores r in the asynchronous result associated with *this if ResultType isn’t void.\nMarks the asynchronous result as having a stored value. Schedules the associated\nasynchronous result to be made ready when the current thread exits. \nPostconditions\nThe asynchronous result associated with *this has a stored value but isn’t ready\nuntil the current thread exits. Threads blocked waiting for the asynchronous result\nwill be unblocked when the current thread exits. \n",
      "content_length": 2132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": null,
      "content": "487\n<future> header\nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::promise_already_satisfied if the asynchronous result already has a stored\nvalue or exception. Any exceptions thrown by the copy-constructor or move-\nconstructor of r. \nSynchronization\nMultiple concurrent calls to set_value(), set_value_at_thread_exit(), set_\nexception(), and set_exception_at_thread_exit() are serialized. The comple-\ntion of the thread that made a successful call to set_value_at_thread_exit() hap-\npens-before a call to std::future<ResultType>::get() or std::shared_future\n<ResultType>::get(), which retrieves the stored exception. \nSTD::PROMISE::SET_EXCEPTION MEMBER FUNCTION \nStores an exception in the asynchronous result associated with *this. \nDeclaration\nvoid set_exception(std::exception_ptr e);\nPreconditions\n*this has an associated asynchronous result. (bool)e is true. \nEffects\nStores e in the asynchronous result associated with *this. \nPostconditions\nThe asynchronous result associated with *this is ready with a stored exception.\nAny threads blocked waiting for the asynchronous result are unblocked. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::promise_already_satisfied if the asynchronous result already has a stored\nvalue or exception. \nSynchronization\nMultiple concurrent calls to set_value() and set_exception() are serialized. A\nsuccessful call to set_exception() happens-before a call to std::future<Result-\nType>::get() or std::shared_future<ResultType>::get(), which retrieves the\nstored exception. \nSTD::PROMISE::SET_EXCEPTION_AT_THREAD_EXIT MEMBER FUNCTION \nStores an exception in the asynchronous result associated with *this without making\nthat result ready until the current thread exits. \nDeclaration\nvoid set_exception_at_thread_exit(std::exception_ptr e);\nPreconditions\n*this has an associated asynchronous result. (bool)e is true. \nEffects\nStores e in the asynchronous result associated with *this. Schedules the associated\nasynchronous result to be made ready when the current thread exits. \n",
      "content_length": 2094,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": null,
      "content": "488\nAPPENDIX D\nC++ Thread Library reference\nPostconditions\nThe asynchronous result associated with *this has a stored exception but isn’t\nready until the current thread exits. Threads blocked waiting for the asynchronous\nresult will be unblocked when the current thread exits. \nThrows\nAn exception of type std::future_error with an error code of std::future_\nerrc::promise_already_satisfied if the asynchronous result already has a stored\nvalue or exception. \nSynchronization\nMultiple concurrent calls to set_value(), set_value_at_thread_exit(), set_\nexception(), and set_exception_at_thread_exit() are serialized. The comple-\ntion of the thread that made a successful call to set_exception_at_thread_exit()\nhappens-before a call to std::future<ResultType>::get() or std::shared_\nfuture<ResultType>::get(), which retrieves the exception stored. \nD.4.5\nstd::async function template \nstd::async is a simple way of running self-contained asynchronous tasks to make use\nof the available hardware concurrency. A call to std::async returns a std::future\nthat will contain the result of the task. Depending on the launch policy, the task is\neither run asynchronously on its own thread or synchronously on whichever thread\ncalls the wait() or get() member functions on that future. \nDeclaration\nenum class launch\n{\n    async,deferred\n};\ntemplate<typename Callable,typename ... Args>\nfuture<result_of<Callable(Args...)>::type>\nasync(Callable&& func,Args&& ... args);\ntemplate<typename Callable,typename ... Args>\nfuture<result_of<Callable(Args...)>::type>\nasync(launch policy,Callable&& func,Args&& ... args);\nPreconditions\nThe expression INVOKE(func,args) is valid for the supplied values of func and\nargs. Callable and every member of Args are MoveConstructible. \nEffects\nConstructs copies of func and args... in internal storage (denoted by fff and\nxyz..., respectively). \nIf policy is std::launch::async, runs INVOKE(fff,xyz...) on its own thread.\nThe returned std::future will become ready when this thread is complete and will\nhold either the return value or the exception thrown by the function invocation.\nThe destructor of the last future object associated with the asynchronous state of\nthe returned std::future blocks until the future is ready. \n",
      "content_length": 2248,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": null,
      "content": "489\n<mutex> header\nIf policy is std::launch::deferred, fff and xyz... are stored in the returned\nstd::future as a deferred function call. The first call to the wait() or get() mem-\nber functions on a future that shares the same associated state will execute INVOKE\n(fff,xyz...) synchronously on the thread that called wait() or get(). \nThe value returned or exception thrown by the execution of INVOKE(fff,\nxyz...) will be returned from a call to get() on that std::future. \nIf policy is std::launch::async | std::launch::deferred or the policy\nargument is omitted, the behavior is as-if either std::launch::async or std::\nlaunch::deferred had been specified. The implementation will choose the behav-\nior on a call-by-call basis in order to take advantage of the available hardware con-\ncurrency without excessive oversubscription. \nIn all cases, the std::async call returns immediately. \nSynchronization\nThe completion of the function invocation happens-before a successful return\nfrom a call to wait(), get(), wait_for(), or wait_until() on any std::future or\nstd::shared_future instance that references the same associated state as the\nstd:: future object returned from the std::async call. In the case of a policy of\nstd::launch::async, the completion of the thread on which the function invoca-\ntion occurs also happens-before the successful return from these calls. \nThrows\nstd::bad_alloc if the required internal storage can’t be allocated, otherwise\nstd::future_error when the effects can’t be achieved, or any exception thrown\nduring the construction of fff or xyz.... \nD.5\n<mutex> header \nThe <mutex> header provides facilities for ensuring mutual exclusion: mutex types,\nlock types and functions, and a mechanism for ensuring an operation is performed\nexactly once. \nHeader contents\nnamespace std\n{\n    class mutex;\n    class recursive_mutex;\n    class timed_mutex;\n    class recursive_timed_mutex;\n    class shared_mutex;\n    class shared_timed_mutex;\n    struct adopt_lock_t;\n    struct defer_lock_t;\n    struct try_to_lock_t;\n    constexpr adopt_lock_t adopt_lock{};\n    constexpr defer_lock_t defer_lock{};\n    constexpr try_to_lock_t try_to_lock{};\n    template<typename LockableType>\n    class lock_guard;\n",
      "content_length": 2224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": null,
      "content": "490\nAPPENDIX D\nC++ Thread Library reference\n    template<typename LockableType>\n    class unique_lock;\n    template<typename LockableType>\n    class shared_lock;\n    template<typename ... LockableTypes>\n    class scoped_lock;\n    template<typename LockableType1,typename... LockableType2>\n    void lock(LockableType1& m1,LockableType2& m2...);\n    template<typename LockableType1,typename... LockableType2>\n    int try_lock(LockableType1& m1,LockableType2& m2...);\n    struct once_flag;\n    template<typename Callable,typename... Args>\n    void call_once(once_flag& flag,Callable func,Args args...);\n}\nD.5.1\nstd::mutex class \nThe std::mutex class provides a basic mutual exclusion and synchronization facility\nfor threads that can be used to protect shared data. Prior to accessing the data pro-\ntected by the mutex, the mutex must be locked by calling lock() or try_lock(). Only\none thread may hold the lock at a time, so if another thread also tries to lock the\nmutex, it will fail (try_lock()) or block (lock()) as appropriate. Once a thread is\ndone accessing the shared data, it then must call unlock() to release the lock and\nallow other threads to acquire it. \n std::mutex meets the Lockable requirements. \nClass definition \nclass mutex\n{\npublic:\n    mutex(mutex const&)=delete;\n    mutex& operator=(mutex const&)=delete;\n    constexpr mutex() noexcept;\n    ~mutex();\n    void lock();\n    void unlock();\n    bool try_lock();\n};\nSTD::MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::mutex object. \nDeclaration\nconstexpr mutex() noexcept;\nEffects\nConstructs an std::mutex instance. \n",
      "content_length": 1584,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": null,
      "content": "491\n<mutex> header\nPostconditions\nThe newly constructed std::mutex object is initially unlocked. \nThrows\nNothing.\nSTD::MUTEX DESTRUCTOR \nDestroys an std::mutex object. \nDeclaration\n~mutex();\nPreconditions\n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing.\nSTD::MUTEX::LOCK MEMBER FUNCTION \nAcquires a lock on an std::mutex object for the current thread. \nDeclaration\nvoid lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until a lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a lock on an std::mutex object for the current thread. \nDeclaration\nbool try_lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a lock on *this for the calling thread without blocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \n",
      "content_length": 1114,
      "extraction_method": "Direct"
    },
    {
      "page_number": 515,
      "chapter": null,
      "content": "492\nAPPENDIX D\nC++ Thread Library reference\nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::MUTEX::UNLOCK MEMBER FUNCTION \nReleases a lock on an std::mutex object held by the current thread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold a lock on *this. \nEffects\nReleases the lock on *this held by the current thread. If any threads are blocked\nwaiting to acquire a lock on *this, unblocks one of them. \nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nD.5.2\nstd::recursive_mutex class \nThe std::recursive_mutex class provides a basic mutual exclusion and synchroniza-\ntion facility for threads that can be used to protect shared data. Prior to accessing the\ndata protected by the mutex, the mutex must be locked by calling lock() or\ntry_lock(). Only one thread may hold the lock at a time, so if another thread also\ntries to lock the recursive_mutex, it will fail (try_lock) or block (lock) as appropri-\nate. Once a thread is done accessing the shared data, it then must call unlock() to\nrelease the lock and allow other threads to acquire it. \n This mutex is recursive so a thread that holds a lock on a particular std::recursive\n_mutex instance may make further calls to lock() or try_lock() to increase the lock\ncount. The mutex can’t be locked by another thread until the thread that acquired\nthe locks has called unlock once for each successful call to lock() or try_lock(). \n std::recursive_mutex meets the Lockable requirements. \nClass definition \nclass recursive_mutex\n{\npublic:\n    recursive_mutex(recursive_mutex const&)=delete;\n    recursive_mutex& operator=(recursive_mutex const&)=delete;\n    recursive_mutex() noexcept;\n    ~recursive_mutex();\n    void lock();\n    void unlock();\n    bool try_lock() noexcept;\n};\n",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": null,
      "content": "493\n<mutex> header\nSTD::RECURSIVE_MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::recursive_mutex object. \nDeclaration\nrecursive_mutex() noexcept;\nEffects\nConstructs an std::recursive_mutex instance. \nPostconditions\nThe newly constructed std::recursive_mutex object is initially unlocked. \nThrows\nAn exception of type std::system_error if unable to create a new std::recursive\n_mutex instance. \nSTD::RECURSIVE_MUTEX DESTRUCTOR \nDestroys an std::recursive_mutex object. \nDeclaration\n~recursive_mutex();\nPreconditions \n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing. \nSTD::RECURSIVE_MUTEX::LOCK MEMBER FUNCTION \nAcquires a lock on an std::recursive_mutex object for the current thread. \nDeclaration\nvoid lock();\nEffects\nBlocks the current thread until a lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread. If the calling thread already held a lock on\n*this, the lock count is increased by one. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::RECURSIVE_MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a lock on an std::recursive_mutex object for the current thread. \nDeclaration\nbool try_lock() noexcept;\nEffects\nAttempts to acquire a lock on *this for the calling thread without blocking. \n",
      "content_length": 1270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": null,
      "content": "494\nAPPENDIX D\nC++ Thread Library reference\nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\nA new lock on *this has been obtained for the calling thread if the function\nreturns true. \nThrows\nNothing. \nNOTE\nIf the calling thread already holds the lock on *this, the function\nreturns true and the count of locks on *this held by the calling thread is\nincreased by one. If the current thread doesn’t already hold a lock on *this,\nthe function may fail to acquire the lock (and return false) even if no other\nthread holds a lock on *this. \nSTD::RECURSIVE_MUTEX::UNLOCK MEMBER FUNCTION \nReleases a lock on an std::recursive_mutex object held by the current thread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold a lock on *this. \nEffects\nReleases a lock on *this held by the current thread. If this is the last lock on *this\nheld by the calling thread, any threads are blocked waiting to acquire a lock on\n*this. Unblocks one of them. \nPostconditions\nThe number of locks on *this held by the calling thread is reduced by one. \nThrows\nNothing. \nD.5.3\nstd::timed_mutex class \nThe std::timed_mutex class provides support for locks with timeouts on top of the\nbasic mutual exclusion and synchronization facility provided by std::mutex. Prior to\naccessing the data protected by the mutex, the mutex must be locked by calling lock(),\ntry_lock(), try_lock_for(), or try_lock_until(). If a lock is already held by\nanother thread, an attempt to acquire the lock will fail (try_lock()), block until the\nlock can be acquired (lock()), or block until the lock can be acquired or the lock\nattempt times out (try_lock_for() or try_lock_until()). Once a lock has been\nacquired (whichever function was used to acquire it), it must be released, by calling\nunlock(), before another thread can acquire the lock on the mutex. \n std::timed_mutex meets the TimedLockable requirements. \nClass definition \nclass timed_mutex\n{\n",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": null,
      "content": "495\n<mutex> header\npublic:\n    timed_mutex(timed_mutex const&)=delete;\n    timed_mutex& operator=(timed_mutex const&)=delete;\n    timed_mutex();\n    ~timed_mutex();\n    void lock();\n    void unlock();\n    bool try_lock();\n    template<typename Rep,typename Period>\n    bool try_lock_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock,typename Duration>\n    bool try_lock_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n};\nSTD::TIMED_MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::timed_mutex object. \nDeclaration\ntimed_mutex();\nEffects\nConstructs an std::timed_mutex instance. \nPostconditions\nThe newly constructed std::timed_mutex object is initially unlocked. \nThrows\nAn exception of type std::system_error if unable to create a new std::timed\n_mutex instance. \nSTD::TIMED_MUTEX DESTRUCTOR \nDestroys an std::timed_mutex object. \nDeclaration\n~timed_mutex();\nPreconditions\n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing.\nSTD::TIMED_MUTEX::LOCK MEMBER FUNCTION \nAcquires a lock on an std::timed_mutex object for the current thread. \nDeclaration\nvoid lock();\n",
      "content_length": 1153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": null,
      "content": "496\nAPPENDIX D\nC++ Thread Library reference\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until a lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::TIMED_MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a lock on an std::timed_mutex object for the current thread. \nDeclaration\nbool try_lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a lock on *this for the calling thread without blocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::TIMED_MUTEX::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire a lock on an std::timed_mutex object for the current thread. \nDeclaration\ntemplate<typename Rep,typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a lock on *this for the calling thread within the time specified\nby relative_time. If relative_time.count() is zero or negative, the call will\nreturn immediately, as if it was a call to try_lock(). Otherwise, the call blocks until\neither the lock has been acquired or the time period specified by relative_time\nhas elapsed. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \n",
      "content_length": 1661,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": null,
      "content": "497\n<mutex> header\nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. The thread may be blocked for longer\nthan the specified duration. Where possible, the elapsed time is determined\nby a steady clock. \nSTD::TIMED_MUTEX::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a lock on an std::timed_mutex object for the current thread. \nDeclaration\ntemplate<typename Clock,typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a lock on *this for the calling thread before the time specified\nby absolute_time. If absolute_time<=Clock::now() on entry, the call will return\nimmediately, as if it was a call to try_lock(). Otherwise, the call blocks until either\nthe lock has been acquired or Clock::now() returns a time equal to or later than\nabsolute_time. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. There’s no guarantee as to how long\nthe calling thread will be blocked, only that if the function returns false,\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked. \nSTD::TIMED_MUTEX::UNLOCK MEMBER FUNCTION \nReleases a lock on an std::timed_mutex object held by the current thread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold a lock on *this. \n",
      "content_length": 1796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": null,
      "content": "498\nAPPENDIX D\nC++ Thread Library reference\nEffects \nReleases the lock on *this held by the current thread. If any threads are blocked\nwaiting to acquire a lock on *this, unblocks one of them. \nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nD.5.4\nstd::recursive_timed_mutex class \nThe std::recursive_timed_mutex class provides support for locks with timeouts on\ntop of the mutual exclusion and synchronization facility provided by std::recursive_\nmutex. Prior to accessing the data protected by the mutex, the mutex must be locked by\ncalling lock(), try_lock(), try_lock_for(), or try_lock_until(). If a lock is already\nheld by another thread, an attempt to acquire the lock will fail (try_lock()), block\nuntil the lock can be acquired (lock()), or block until the lock can be acquired or the\nlock attempt times out (try_lock_for() or try_lock_until()). Once a lock has\nbeen acquired (whichever function was used to acquire it), it must be released by call-\ning unlock() before another thread can acquire the lock on the mutex. \n This mutex is recursive, so a thread that holds a lock on a particular instance of\nstd::recursive_timed_mutex may acquire additional locks on that instance through\nany of the lock functions. All of these locks must be released by a corresponding call\nto unlock() before another thread can acquire a lock on that instance. \n std::recursive_timed_mutex meets the TimedLockable requirements. \nClass definition \nclass recursive_timed_mutex\n{\npublic:\n    recursive_timed_mutex(recursive_timed_mutex const&)=delete;\n    recursive_timed_mutex& operator=(recursive_timed_mutex const&)=delete;\n    recursive_timed_mutex();\n    ~recursive_timed_mutex();\n    void lock();\n    void unlock();\n    bool try_lock() noexcept;\n    template<typename Rep,typename Period>\n    bool try_lock_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock,typename Duration>\n    bool try_lock_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n};\n",
      "content_length": 2046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": null,
      "content": "499\n<mutex> header\nSTD::RECURSIVE_TIMED_MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::recursive_timed_mutex object. \nDeclaration\nrecursive_timed_mutex();\nEffects\nConstructs an std::recursive_timed_mutex instance. \nPostconditions\nThe newly constructed std::recursive_timed_mutex object is initially unlocked. \nThrows\nAn exception of type std::system_error if unable to create a new std::recursive\n_timed_mutex instance. \nSTD::RECURSIVE_TIMED_MUTEX DESTRUCTOR \nDestroys an std::recursive_timed_mutex object. \nDeclaration\n~recursive_timed_mutex();\nPreconditions\n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing. \nSTD::RECURSIVE_TIMED_MUTEX::LOCK MEMBER FUNCTION \nAcquires a lock on an std::recursive_timed_mutex object for the current thread. \nDeclaration\nvoid lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until a lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread. If the calling thread already held a lock on\n*this, the lock count is increased by one. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::RECURSIVE_TIMED_MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a lock on an std::recursive_timed_mutex object for the current\nthread. \nDeclaration\nbool try_lock() noexcept;\n",
      "content_length": 1317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": null,
      "content": "500\nAPPENDIX D\nC++ Thread Library reference\nEffects\nAttempts to acquire a lock on *this for the calling thread without blocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nIf the calling thread already holds the lock on *this, the function\nreturns true and the count of locks on *this held by the calling thread is\nincreased by one. If the current thread doesn’t already hold a lock on *this,\nthe function may fail to acquire the lock (and return false) even if no other\nthread holds a lock on *this. \nSTD::RECURSIVE_TIMED_MUTEX::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire a lock on an std::recursive_timed_mutex object for the current\nthread. \nDeclaration\ntemplate<typename Rep,typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nEffects\nAttempts to acquire a lock on *this for the calling thread within the time specified\nby relative_time. If relative_time.count() is zero or negative, the call will\nreturn immediately, as if it was a call to try_lock(). Otherwise, the call blocks until\neither the lock has been acquired or the time period specified by relative_time\nhas elapsed. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nIf the calling thread already holds the lock on *this, the function\nreturns true and the count of locks on *this held by the calling thread is\nincreased by one. If the current thread doesn’t already hold a lock on *this,\nthe function may fail to acquire the lock (and return false) even if no other\nthread holds a lock on *this. The thread may be blocked for longer than the\nspecified duration. Where possible, the elapsed time is determined by a\nsteady clock. \n",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": null,
      "content": "501\n<mutex> header\nSTD::RECURSIVE_TIMED_MUTEX::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a lock on an std::recursive_timed_mutex object for the current\nthread. \nDeclaration\ntemplate<typename Clock,typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nEffects\nAttempts to acquire a lock on *this for the calling thread before the time specified\nby absolute_time. If absolute_time<=Clock::now() on entry, the call will return\nimmediately, as if it was a call to try_lock(). Otherwise, the call blocks until either\nthe lock has been acquired or Clock::now() returns a time equal to or later than\nabsolute_time. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nIf the calling thread already holds the lock on *this, the function\nreturns true and the count of locks on *this held by the calling thread is\nincreased by one. If the current thread doesn’t already hold a lock on *this,\nthe function may fail to acquire the lock (and return false) even if no other\nthread holds a lock on *this. There’s no guarantee as to how long the calling\nthread will be blocked, only that if the function returns false, then\nClock::now() returns a time equal to or later than absolute_time at the\npoint at which the thread became unblocked. \nSTD::RECURSIVE_TIMED_MUTEX::UNLOCK MEMBER FUNCTION \nReleases a lock on an std::recursive_timed_mutex object held by the current\nthread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold a lock on *this. \nEffects\nReleases a lock on *this held by the current thread. If this is the last lock on *this\nheld by the calling thread, any threads are blocked waiting to acquire a lock on\n*this. Unblocks one of them. \nPostconditions\nThe number of locks on *this held by the calling thread is reduced by one. \nThrows\nNothing. \n",
      "content_length": 1959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": null,
      "content": "502\nAPPENDIX D\nC++ Thread Library reference\nD.5.5\nstd::shared_mutex class \nThe std::shared_mutex class provides a mutual exclusion and synchronization facil-\nity for threads that can be used to protect shared data that is frequently read and\nrarely modified. It allows one thread to hold an exclusive lock, or one or more threads\nto hold a shared lock. Prior to modifying the data protected by the mutex, the mutex\nmust be locked with an exclusive lock by calling lock() or try_lock(). Only one\nthread may hold an exclusive lock at a time, so if another thread also tries to lock the\nmutex, it will fail (try_lock()) or block (lock()) as appropriate. Once a thread is\ndone modifying the shared data, it then must call unlock() to release the lock and\nallow other threads to acquire it. Threads that only want to read the protected data\nmay obtain a shared lock by calling lock_shared() or try_lock_shared(). Multiple\nthreads may hold a shared lock at a time, so if one thread holds a shared lock, then\nanother thread may also acquire a shared lock. If a thread tries to acquire an exclusive\nlock, that thread will wait. Once a thread that has acquired a shared lock is done\naccessing the protected data, it must call unlock_shared() to release the shared lock.\n std::shared_mutex meets the Lockable requirements. \nClass definition \nclass shared_mutex\n{\npublic:\n    shared_mutex(shared_mutex const&)=delete;\n    shared_mutex& operator=(shared_mutex const&)=delete;\n    shared_mutex() noexcept;\n    ~shared_mutex();\n    void lock();\n    void unlock();\n    bool try_lock();\n    void lock_shared();\n    void unlock_shared();\n    bool try_lock_shared();\n};\nSTD::SHARED_MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::shared_mutex object. \nDeclaration\nshared_mutex() noexcept;\nEffects\nConstructs an std::shared_mutex instance. \nPostconditions\nThe newly constructed std::shared_mutex object is initially unlocked. \nThrows\nNothing.\n",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": null,
      "content": "503\n<mutex> header\nSTD::SHARED_MUTEX DESTRUCTOR \nDestroys an std::shared_mutex object. \nDeclaration\n~shared_mutex();\nPreconditions\n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing.\nSTD::SHARED_MUTEX::LOCK MEMBER FUNCTION \nAcquires an exclusive lock on an std::shared_mutex object for the current thread. \nDeclaration\nvoid lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until an exclusive lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread with an exclusive lock. \nThrows\nAn exception ofif an error occurs. \nSTD::SHARED_MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire an exclusive lock on an std::shared_mutex object for the cur-\nrent thread. \nDeclaration\nbool try_lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire an exclusive lock on *this for the calling thread without\nblocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread with an exclusive lock if the function returns\ntrue. \n",
      "content_length": 1136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 527,
      "chapter": null,
      "content": "504\nAPPENDIX D\nC++ Thread Library reference\nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::SHARED_MUTEX::UNLOCK MEMBER FUNCTION \nReleases an exclusive lock on an std::shared_mutex object held by the current\nthread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold an exclusive lock on *this. \nEffects\nReleases the exclusive lock on *this held by the current thread. If any threads are\nblocked waiting to acquire a lock on *this, unblocks one thread waiting for an\nexclusive lock or some number of threads waiting for a shared lock. \nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nSTD::SHARED_MUTEX::LOCK_SHARED MEMBER FUNCTION \nAcquires a shared lock on an std::shared_mutex object for the current thread. \nDeclaration\nvoid lock_shared();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until a shared lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread with a shared lock. \nThrows\nAn exception ofif an error occurs. \nSTD::SHARED_MUTEX::TRY_LOCK_SHARED MEMBER FUNCTION \nAttempts to acquire a shared lock on an std::shared_mutex object for the current\nthread. \nDeclaration\nbool try_lock_shared();\nPreconditions\nThe calling thread must not hold a lock on *this. \n",
      "content_length": 1385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": null,
      "content": "505\n<mutex> header\nEffects\nAttempts to acquire a shared lock on *this for the calling thread without blocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread with a shared lock if the function returns\ntrue. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::SHARED_MUTEX::UNLOCK_SHARED MEMBER FUNCTION \nReleases a shared lock on an std::shared_mutex object held by the current thread. \nDeclaration\nvoid unlock_shared();\nPreconditions\nThe calling thread must hold a shared lock on *this. \nEffects\nReleases the shared lock on *this held by the current thread. If this is the last\nshared lock on *this, and any threads are blocked waiting to acquire a lock on\n*this, unblocks one thread waiting for an exclusive lock or some number of\nthreads waiting for a shared lock. \nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nD.5.6\nstd::shared_timed_mutex class \nThe std::shared_timed_mutex class provides a mutual exclusion and synchroniza-\ntion facility for threads that can be used to protect shared data that is frequently read\nand rarely modified. It allows one thread to hold an exclusive lock, or one or more\nthreads to hold a shared lock. Prior to modifying the data protected by the mutex, the\nmutex must be locked with an exclusive lock by calling lock() or try_lock(). Only\none thread may hold an exclusive lock at a time, so if another thread also tries to lock\nthe mutex, it will fail (try_lock()) or block (lock()) as appropriate. Once a thread is\ndone modifying the shared data, it then must call unlock() to release the lock and\nallow other threads to acquire it. Threads that only want to read the protected data\nmay obtain a shared lock by calling lock_shared() or try_lock_shared(). Multiple\nthreads may hold a shared lock at a time, so if one thread holds a shared lock, then\nanother thread may also acquire a shared lock. If a thread tries to acquire an exclusive\n",
      "content_length": 2074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": null,
      "content": "506\nAPPENDIX D\nC++ Thread Library reference\nlock, that thread will wait. Once a thread that has acquired a shared lock is done\naccessing the protected data, it must call unlock_shared() to release the shared lock.\n std::shared_timed_mutex meets the Lockable requirements. \nClass definition \nclass shared_timed_mutex\n{\npublic:\n    shared_timed_mutex(shared_timed_mutex const&)=delete;\n    shared_timed_mutex& operator=(shared_timed_mutex const&)=delete;\n    shared_timed_mutex() noexcept;\n    ~shared_timed_mutex();\n    void lock();\n    void unlock();\n    bool try_lock();\n    template<typename Rep,typename Period>\n    bool try_lock_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock,typename Duration>\n    bool try_lock_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n    void lock_shared();\n    void unlock_shared();\n    bool try_lock_shared();\n    template<typename Rep,typename Period>\n    bool try_lock_shared_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock,typename Duration>\n    bool try_lock_shared_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n};\nSTD::SHARED_TIMED_MUTEX DEFAULT CONSTRUCTOR \nConstructs an std::shared_timed_mutex object. \nDeclaration\nshared_timed_mutex() noexcept;\nEffects\nConstructs an std::shared_timed_mutex instance. \nPostconditions\nThe newly constructed std::shared_timed_mutex object is initially unlocked. \nThrows\nNothing.\n",
      "content_length": 1515,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": null,
      "content": "507\n<mutex> header\nSTD::SHARED_TIMED_MUTEX DESTRUCTOR \nDestroys an std::shared_timed_mutex object. \nDeclaration\n~shared_timed_mutex();\nPreconditions\n*this must not be locked. \nEffects\nDestroys *this. \nThrows\nNothing.\nSTD::SHARED_TIMED_MUTEX::LOCK MEMBER FUNCTION \nAcquires an exclusive lock on an std::shared_timed_mutex object for the current\nthread. \nDeclaration\nvoid lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until an exclusive lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread with an exclusive lock. \nThrows\nAn exception of type std::system_error if an error occurs. \nSTD::SHARED_TIMED_MUTEX::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire an exclusive lock on an std::shared_timed_mutex object for the\ncurrent thread. \nDeclaration\nbool try_lock();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire an exclusive lock on *this for the calling thread without\nblocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread with an exclusive lock if the function returns\ntrue. \nThrows\nNothing. \n",
      "content_length": 1217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": null,
      "content": "508\nAPPENDIX D\nC++ Thread Library reference\nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::SHARED_TIMED_MUTEX::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire an exclusive lock on an std::shared_timed_mutex object for the\ncurrent thread. \nDeclaration\ntemplate<typename Rep,typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire an exclusive lock on *this for the calling thread within the\ntime specified by relative_time. If relative_time.count() is zero or negative,\nthe call will return immediately, as if it was a call to try_lock(). Otherwise, the call\nblocks until either the lock has been acquired or the time period specified by\nrelative_time has elapsed. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. The thread may be blocked for longer\nthan the specified duration. Where possible, the elapsed time is determined\nby a steady clock. \nSTD::SHARED_TIMED_MUTEX::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire an exclusive lock on an std::shared_timed_mutex object for the\ncurrent thread. \nDeclaration\ntemplate<typename Clock,typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire an exclusive lock on *this for the calling thread before the\ntime specified by absolute_time. If absolute_time<=Clock::now() on entry, the\ncall will return immediately, as if it was a call to try_lock(). Otherwise, the call\n",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": null,
      "content": "509\n<mutex> header\nblocks until either the lock has been acquired or Clock::now() returns a time equal\nto or later than absolute_time. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. There’s no guarantee as to how long\nthe calling thread will be blocked, only that if the function returns false,\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked. \nSTD::SHARED_TIMED_MUTEX::UNLOCK MEMBER FUNCTION \nReleases an exclusive lock on an std::shared_timed_mutex object held by the cur-\nrent thread. \nDeclaration\nvoid unlock();\nPreconditions\nThe calling thread must hold an exclusive lock on *this. \nEffects\nReleases the exclusive lock on *this held by the current thread. If any threads are\nblocked waiting to acquire a lock on *this, unblocks one thread waiting for an\nexclusive lock or some number of threads waiting for a shared lock. \nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nSTD::SHARED_TIMED_MUTEX::LOCK_SHARED MEMBER FUNCTION \nAcquires a shared lock on an std::shared_timed_mutex object for the current thread. \nDeclaration\nvoid lock_shared();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nBlocks the current thread until a shared lock on *this can be obtained. \nPostconditions\n*this is locked by the calling thread with a shared lock. \nThrows\nAn exception of type std::system_error if an error occurs. \n",
      "content_length": 1680,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": null,
      "content": "510\nAPPENDIX D\nC++ Thread Library reference\nSTD::SHARED_TIMED_MUTEX::TRY_LOCK_SHARED MEMBER FUNCTION \nAttempts to acquire a shared lock on an std::shared_timed_mutex object for the cur-\nrent thread. \nDeclaration\nbool try_lock_shared();\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a shared lock on *this for the calling thread without blocking. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread with a shared lock if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. \nSTD::SHARED_TIMED_MUTEX::TRY_LOCK_SHARED_FOR MEMBER FUNCTION \nAttempts to acquire a shared lock on an std::shared_timed_mutex object for the cur-\nrent thread. \nDeclaration\ntemplate<typename Rep,typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a shared lock on *this for the calling thread within the time\nspecified by relative_time. If relative_time.count() is zero or negative, the call\nwill return immediately, as if it was a call to try_lock(). Otherwise, the call blocks\nuntil either the lock has been acquired or the time period specified by relative\n_time has elapsed. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. The thread may be blocked for longer\n",
      "content_length": 1752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": null,
      "content": "511\n<mutex> header\nthan the specified duration. Where possible, the elapsed time is determined\nby a steady clock. \nSTD::SHARED_TIMED_MUTEX::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a shared lock on an std::shared_timed_mutex object for the cur-\nrent thread. \nDeclaration\ntemplate<typename Clock,typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe calling thread must not hold a lock on *this. \nEffects\nAttempts to acquire a shared lock on *this for the calling thread before the time\nspecified by absolute_time. If absolute_time<=Clock::now() on entry, the call\nwill return immediately, as if it was a call to try_lock(). Otherwise, the call blocks\nuntil either the lock has been acquired or Clock::now() returns a time equal to or\nlater than absolute_time. \nReturns\ntrue if a lock was obtained for the calling thread, false otherwise. \nPostconditions\n*this is locked by the calling thread if the function returns true. \nThrows\nNothing. \nNOTE\nThe function may fail to acquire the lock (and return false) even if\nno other thread holds a lock on *this. There’s no guarantee as to how long\nthe calling thread will be blocked, only that if the function returns false,\nthen Clock::now() returns a time equal to or later than absolute_time at\nthe point at which the thread became unblocked. \nSTD::SHARED_TIMED_MUTEX::UNLOCK_SHARED MEMBER FUNCTION \nReleases a shared lock on an std::shared_timed_mutex object held by the current\nthread. \nDeclaration\nvoid unlock_shared();\nPreconditions\nThe calling thread must hold a shared lock on *this. \nEffects\nReleases the shared lock on *this held by the current thread. If this is the last shared\nlock on *this, and any threads are blocked waiting to acquire a lock on *this,\nunblocks one thread waiting for an exclusive lock or some number of threads wait-\ning for a shared lock. \n",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": null,
      "content": "512\nAPPENDIX D\nC++ Thread Library reference\nPostconditions\n*this is not locked by the calling thread. \nThrows\nNothing. \nD.5.7\nstd::lock_guard class template \nThe std::lock_guard class template provides a basic lock ownership wrapper. The\ntype of mutex being locked is specified by template parameter Mutex and must meet\nthe Lockable requirements. The specified mutex is locked in the constructor and\nunlocked in the destructor. This provides a simple means of locking a mutex for a\nblock of code and ensuring that the mutex is unlocked when the block is left, whether\nthat’s by running off the end, by the use of a control flow statement such as break or\nreturn, or by throwing an exception. \n Instances of std::lock_guard are not MoveConstructible, CopyConstructible,\nor CopyAssignable. \nClass definition\ntemplate <class Mutex>\nclass lock_guard\n{\npublic:\n    typedef Mutex mutex_type;\n    explicit lock_guard(mutex_type& m);\n    lock_guard(mutex_type& m, adopt_lock_t);\n    ~lock_guard();\n    lock_guard(lock_guard const& ) = delete;\n    lock_guard& operator=(lock_guard const& ) = delete;\n};\nSTD::LOCK_GUARD LOCKING CONSTRUCTOR \nConstructs an std::lock_guard instance that locks the supplied mutex. \nDeclaration\nexplicit lock_guard(mutex_type& m);\nEffects\nConstructs an std::lock_guard instance that references the supplied mutex. Calls\nm.lock(). \nThrows \nAny exceptions thrown by m.lock(). \nPostconditions \n*this owns a lock on m. \nSTD::LOCK_GUARD LOCK-ADOPTING CONSTRUCTOR \nConstructs an std::lock_guard instance that owns the lock on the supplied mutex. \nDeclaration\nlock_guard(mutex_type& m,std::adopt_lock_t);\n",
      "content_length": 1617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": null,
      "content": "513\n<mutex> header\nPreconditions\nThe calling thread must own a lock on m. \nEffects\nConstructs an std::lock_guard instance that references the supplied mutex and\ntakes ownership of the lock on m held by the calling thread. \nThrows\nNothing. \nPostconditions \n*this owns the lock on m held by the calling thread. \nSTD::LOCK_GUARD DESTRUCTOR \nDestroys an std::lock_guard instance and unlocks the corresponding mutex. \nDeclaration\n~lock_guard();\nEffects\nCalls m.unlock() for the mutex instance, m, supplied when *this was constructed. \nThrows\nNothing. \nD.5.8\nstd::scoped_lock class template \nThe std::scoped_lock class template provides a basic lock ownership wrapper for\nmultiple mutexes at once. The type of mutex being locked is specified by the template\nparameter pack Mutexes and each must meet the Lockable requirements. The speci-\nfied mutexes are locked in the constructor and unlocked in the destructor. This pro-\nvides a simple means of locking a set of mutexes for a block of code and ensuring that\nthe mutexes are unlocked when the block is left, whether that’s by running off the\nend, by the use of a control flow statement such as break or return, or by throwing an\nexception. \n Instances of std::scoped_lock are not MoveConstructible, CopyConstructible,\nor CopyAssignable. \nClass definition\ntemplate <class ... Mutexes>\nclass scoped_lock\n{\npublic:\n    explicit scoped_lock(Mutexes& ... m);\n    scoped_lock(Mutexes& ... m, adopt_lock_t);\n    ~scoped_lock();\n    scoped_lock(scoped_lock const& ) = delete;\n    scoped_lock& operator=(scoped_lock const& ) = delete;\n};\nSTD::SCOPED_LOCK LOCKING CONSTRUCTOR \nConstructs an std::scoped_lock instance that locks the supplied mutexes. \n",
      "content_length": 1686,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": null,
      "content": "514\nAPPENDIX D\nC++ Thread Library reference\nDeclaration\nexplicit scoped_lock(Mutexes& ... m);\nEffects\nConstructs an std::scoped_lock instance that references the supplied mutexes.\nUses a combination of calls to m.lock(), m.try_lock(), and m.unlock() on each\nof the mutexes, in order to avoid deadlock, using the same algorithm as the\nstd::lock() free function.\nThrows \nAny exceptions thrown by the m.lock() and m.try_lock() calls.\nPostconditions \n*this owns a lock on the supplied mutexes. \nSTD::SCOPED_LOCK LOCK-ADOPTING CONSTRUCTOR \nConstructs an std::scoped_lock instance that owns the lock on the supplied mutexes;\nthey must already be locked by the calling thread. \nDeclaration\nscoped_lock(Mutexes& ... m,std::adopt_lock_t);\nPreconditions\nThe calling thread must own a lock on the mutexes in m. \nEffects\nConstructs an std::scoped_lock instance that references the supplied mutexes\nand takes ownership of the lock on the mutexes in m held by the calling thread. \nThrows\nNothing. \nPostconditions \n*this owns the lock on the supplied mutexes held by the calling thread. \nSTD::SCOPED_LOCK DESTRUCTOR \nDestroys an std::scoped_lock instance and unlocks the corresponding mutexes. \nDeclaration\n~scoped_lock();\nEffects\nCalls m.unlock() for each of the mutex instances m supplied when *this was con-\nstructed. \nThrows\nNothing. \nD.5.9\nstd::unique_lock class template \nThe std::unique_lock class template provides a more general lock ownership wrap-\nper than std::lock_guard. The type of mutex being locked is specified by the template\nparameter Mutex, which must meet the BasicLockable requirements. In general, the\nspecified mutex is locked in the constructor and unlocked in the destructor, although\n",
      "content_length": 1697,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": null,
      "content": "515\n<mutex> header\nadditional constructors and member functions are provided to allow other possibili-\nties. This provides a means of locking a mutex for a block of code and ensuring that\nthe mutex is unlocked when the block is left, whether that’s by running off the end, by\nthe use of a control flow statement such as break or return, or by throwing an excep-\ntion. The wait functions of std::condition_variable require an instance of std::\nunique_lock<std::mutex>, and all instantiations of std::unique_lock are suitable\nfor use with the Lockable parameter for the std::condition_variable_any wait\nfunctions. \n If the supplied Mutex type meets the Lockable requirements, then std::unique_\nlock<Mutex> also meets the Lockable requirements. If, in addition, the supplied\nMutex type meets the TimedLockable requirements, then std::unique_lock<Mutex>\nalso meets the TimedLockable requirements. \n Instances of std::unique_lock are MoveConstructible and MoveAssignable but\nnot CopyConstructible or CopyAssignable. \nClass definition\ntemplate <class Mutex>\nclass unique_lock\n{\npublic:\n    typedef Mutex mutex_type;\n    unique_lock() noexcept;\n    explicit unique_lock(mutex_type& m);\n    unique_lock(mutex_type& m, adopt_lock_t);\n    unique_lock(mutex_type& m, defer_lock_t) noexcept;\n    unique_lock(mutex_type& m, try_to_lock_t);\n    template<typename Clock,typename Duration>\n    unique_lock(\n        mutex_type& m,\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n    template<typename Rep,typename Period>\n    unique_lock(\n        mutex_type& m,\n        std::chrono::duration<Rep,Period> const& relative_time);\n    ~unique_lock();\n    unique_lock(unique_lock const& ) = delete;\n    unique_lock& operator=(unique_lock const& ) = delete;\n    unique_lock(unique_lock&& );\n    unique_lock& operator=(unique_lock&& );\n    void swap(unique_lock& other) noexcept;\n    void lock();\n    bool try_lock();\n    template<typename Rep, typename Period>\n    bool try_lock_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock, typename Duration>\n    bool try_lock_until(\n",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": null,
      "content": "516\nAPPENDIX D\nC++ Thread Library reference\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n    void unlock();\n    explicit operator bool() const noexcept;\n    bool owns_lock() const noexcept;\n    Mutex* mutex() const noexcept;\n    Mutex* release() noexcept;\n};\nSTD::UNIQUE_LOCK DEFAULT CONSTRUCTOR \nConstructs an std::unique_lock instance with no associated mutex. \nDeclaration\nunique_lock() noexcept;\nEffects\nConstructs an std::unique_lock instance that has no associated mutex. \nPostconditions\nthis->mutex()==NULL, this->owns_lock()==false. \nSTD::UNIQUE_LOCK LOCKING CONSTRUCTOR \nConstructs an std::unique_lock instance that locks the supplied mutex. \nDeclaration\nexplicit unique_lock(mutex_type& m);\nEffects \nConstructs an std::unique_lock instance that references the supplied mutex.\nCalls m.lock(). \nThrows\nAny exceptions thrown by m.lock(). \nPostconditions\nthis->owns_lock()==true, this->mutex()==&m. \nSTD::UNIQUE_LOCK LOCK-ADOPTING CONSTRUCTOR \nConstructs an std::unique_lock instance that owns the lock on the supplied mutex. \nDeclaration\nunique_lock(mutex_type& m,std::adopt_lock_t);\nPreconditions\nThe calling thread must own a lock on m. \nEffects\nConstructs an std::unique_lock instance that references the supplied mutex and\ntakes ownership of the lock on m held by the calling thread. \nThrows\nNothing. \nPostconditions\nthis->owns_lock()==true, this->mutex()==&m. \n",
      "content_length": 1399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": null,
      "content": "517\n<mutex> header\nSTD::UNIQUE_LOCK DEFERRED-LOCK CONSTRUCTOR \nConstructs an std::unique_lock instance that doesn’t own the lock on the supplied\nmutex. \nDeclaration\nunique_lock(mutex_type& m,std::defer_lock_t) noexcept;\nEffects\nConstructs an std::unique_lock instance that references the supplied mutex. \nThrows \nNothing. \nPostconditions\nthis->owns_lock()==false, this->mutex()==&m. \nSTD::UNIQUE_LOCK TRY-TO-LOCK CONSTRUCTOR \nConstructs an std::unique_lock instance associated with the supplied mutex and\ntries to acquire a lock on that mutex. \nDeclaration\nunique_lock(mutex_type& m,std::try_to_lock_t);\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the Lockable\nrequirements. \nEffects\nConstructs an std::unique_lock instance that references the supplied mutex.\nCalls m.try_lock(). \nThrows\nNothing. \nPostconditions\nthis->owns_lock() returns the result of the m.try_lock() call, this->mutex()==&m. \nSTD::UNIQUE_LOCK TRY-TO-LOCK CONSTRUCTOR WITH A DURATION TIMEOUT \nConstructs an std::unique_lock instance associated with the supplied mutex and\ntries to acquire a lock on that mutex. \nDeclaration\ntemplate<typename Rep,typename Period>\nunique_lock(\n    mutex_type& m,\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the Timed-Lockable\nrequirements. \nEffects\nConstructs an std::unique_lock instance that references the supplied mutex. Calls\nm.try_lock_for(relative_time). \n",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": null,
      "content": "518\nAPPENDIX D\nC++ Thread Library reference\nThrows\nNothing. \nPostconditions\nthis->owns_lock() returns the result of the m.try_lock_for() call, this->mutex()\n==&m. \nSTD::UNIQUE_LOCK TRY-TO-LOCK CONSTRUCTOR WITH A TIME_POINT TIMEOUT \nConstructs an std::unique_lock instance associated with the supplied mutex and\ntries to acquire a lock on that mutex. \nDeclaration\ntemplate<typename Clock,typename Duration>\nunique_lock(\n    mutex_type& m,\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the Timed-Lockable\nrequirements. \nEffects\nConstructs an std::unique_lock instance that references the supplied mutex. Calls\nm.try_lock_until(absolute_time). \nThrows\nNothing. \nPostconditions\nthis->owns_lock() returns the result of the m.try_lock_until() call, this->\nmutex()==&m. \nSTD::UNIQUE_LOCK MOVE-CONSTRUCTOR \nTransfers ownership of a lock from one std::unique_lock object to a newly-created\nstd::unique_lock object. \nDeclaration\nunique_lock(unique_lock&& other) noexcept;\nEffects\nConstructs an std::unique_lock instance. If other owned a lock on a mutex prior\nto the constructor invocation, that lock is now owned by the newly created\nstd::unique_lock object. \nPostconditions\nFor a newly constructed std::unique_lock object, x, x.mutex() is equal to the value\nof other.mutex() prior to the constructor invocation, and x.owns_lock() is equal to\nthe value of other.owns_lock() prior to the constructor invocation. other.mutex()\n==NULL, other.owns_lock()==false. \nThrows\nNothing. \n",
      "content_length": 1565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": null,
      "content": "519\n<mutex> header\nNOTE\nstd::unique_lock objects are not CopyConstructible, so there’s no\ncopy constructor, only this move constructor.\nSTD::UNIQUE_LOCK MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of a lock from one std::unique_lock object to another std::\nunique_lock object. \nDeclaration\nunique_lock& operator=(unique_lock&& other) noexcept;\nEffects\nIf this->owns_lock()returns true prior to the call, calls this->unlock(). If other\nowned a lock on a mutex prior to the assignment, that lock is now owned by *this. \nPostconditions\nthis->mutex() is equal to the value of other.mutex() prior to the assignment, and\nthis->owns_lock() is equal to the value of other.owns_lock() prior to the assign-\nment. other.mutex()==NULL, other.owns_lock()==false. \nThrows\nNothing. \nNOTE\nstd::unique_lock objects are not CopyAssignable, so there’s no copy-\nassignment operator, only this move-assignment operator. \nSTD::UNIQUE_LOCK DESTRUCTOR \nDestroys an std::unique_lock instance and unlocks the corresponding mutex if it’s\nowned by the destroyed instance. \nDeclaration\n~unique_lock();\nEffects\nIf this->owns_lock()returns true, calls this->mutex()->unlock(). \nThrows\nNothing. \nSTD::UNIQUE_LOCK::SWAP MEMBER FUNCTION \nExchanges ownership of their associated unique_locks of execution between two\nstd::unique_lock objects. \nDeclaration\nvoid swap(unique_lock& other) noexcept;\nEffects\nIf other owns a lock on a mutex prior to the call, that lock is now owned by *this.\nIf *this owns a lock on a mutex prior to the call, that lock is now owned by other. \nPostconditions\nthis->mutex() is equal to the value of other.mutex() prior to the call. other\n.mutex() is equal to the value of this->mutex() prior to the call. this->owns_lock()\n",
      "content_length": 1718,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": null,
      "content": "520\nAPPENDIX D\nC++ Thread Library reference\nis equal to the value of other.owns_lock() prior to the call. other.owns_lock() is\nequal to the value of this->owns_lock() prior to the call. \nThrows\nNothing. \nSWAP NONMEMBER FUNCTION FOR STD::UNIQUE_LOCK\nExchanges ownership of their associated mutex locks between two std::unique_lock\nobjects. \nDeclaration\nvoid swap(unique_lock& lhs,unique_lock& rhs) noexcept;\nEffects\nlhs.swap(rhs) \nThrows\nNothing. \nSTD::UNIQUE_LOCK::LOCK MEMBER FUNCTION \nAcquires a lock on the mutex associated with *this. \nDeclaration\nvoid lock();\nPreconditions\nthis->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->lock(). \nThrows\nAny exceptions thrown by this->mutex()->lock(). std::system_error with an\nerror code of std::errc::operation_not_permitted if this->mutex()==NULL.\nstd::system_error with an error code of std::errc::resource_deadlock_would\n_occur if this->owns_lock()==true on entry. \nPostconditions\nthis->owns_lock()==true. \nSTD::UNIQUE_LOCK::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a lock on the mutex associated with *this. \nDeclaration\nbool try_lock();\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the Lockable\nrequirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock(). \n",
      "content_length": 1310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 544,
      "chapter": null,
      "content": "521\n<mutex> header\nReturns\ntrue if the call to this->mutex()->try_lock() returned true, false otherwise. \nThrows\nAny exceptions thrown by this->mutex()->try_lock(). std::system_error with\nan error code of std::errc::operation_not_permitted if this->mutex()==NULL.\nstd::system_error with an error code of std::errc::resource_deadlock_would\n_occur if this->owns_lock()==true on entry. \nPostconditions\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::UNIQUE_LOCK::UNLOCK MEMBER FUNCTION \nReleases a lock on the mutex associated with *this. \nDeclaration\nvoid unlock();\nPreconditions\nthis->mutex()!=NULL, this->owns_lock()==true. \nEffects\nCalls this->mutex()->unlock(). \nThrows\nAny exceptions thrown by this->mutex()->unlock(). std::system_error with an\nerror code of std::errc::operation_not_permitted if this->owns_lock()==false\non entry. \nPostconditions\nthis->owns_lock()==false. \nSTD::UNIQUE_LOCK::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire a lock on the mutex associated with *this within the time specified. \nDeclaration\ntemplate<typename Rep, typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the TimedLock-\nable requirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock_for(relative_time). \nReturns\ntrue if the call to this->mutex()->try_lock_for() returned true, false otherwise. \nThrows\nAny exceptions thrown by this->mutex()->try_lock_for(). std::system_error\nwith an error code of std::errc::operation_not_permitted if this->mutex()==\n",
      "content_length": 1663,
      "extraction_method": "Direct"
    },
    {
      "page_number": 545,
      "chapter": null,
      "content": "522\nAPPENDIX D\nC++ Thread Library reference\nNULL. std::system_error with an error code of std::errc::resource_deadlock_\nwould_occur if this->owns_lock()==true on entry. \nPostconditions\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::UNIQUE_LOCK::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a lock on the mutex associated with *this within the time speci-\nfied. \nDeclaration\ntemplate<typename Clock, typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe Mutex type used to instantiate std::unique_lock must meet the Timed-Lockable\nrequirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock_until(absolute_time). \nReturns\ntrue if the call to this->mutex()->try_lock_until() returned true, false other-\nwise. \nThrows\nAny exceptions thrown by this->mutex()->try_lock_until(). std::system_error\nwith an error code of std::errc::operation_not_permitted if this-> mutex()==\nNULL. std::system_error with an error code of std::errc::resource_deadlock\n_would_occur if this->owns_lock()==true on entry. \nPostcondition\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::UNIQUE_LOCK::OPERATOR BOOL MEMBER FUNCTION \nChecks whether or not *this owns a lock on a mutex. \nDeclaration\nexplicit operator bool() const noexcept;\nReturns\nthis->owns_lock(). \nThrows\nNothing. \nNOTE\nThis is an explicit conversion operator, so it’s only implicitly called\nin contexts where the result is used as a Boolean and not where the result\nwould be treated as an integer value of 0 or 1.\n",
      "content_length": 1670,
      "extraction_method": "Direct"
    },
    {
      "page_number": 546,
      "chapter": null,
      "content": "523\n<mutex> header\nSTD::UNIQUE_LOCK::OWNS_LOCK MEMBER FUNCTION \nChecks whether or not *this owns a lock on a mutex. \nDeclaration\nbool owns_lock() const noexcept;\nReturns\ntrue if *this owns a lock on a mutex, false otherwise. \nThrows\nNothing. \nSTD::UNIQUE_LOCK::MUTEX MEMBER FUNCTION \nReturns the mutex associated with *this if any. \nDeclaration\nmutex_type* mutex() const noexcept;\nReturns\nA pointer to the mutex associated with *this if any, NULL otherwise. \nThrows\nNothing. \nSTD::UNIQUE_LOCK::RELEASE MEMBER FUNCTION \nReturns the mutex associated with *this if any, and releases that association. \nDeclaration\nmutex_type* release() noexcept;\nEffects\nBreaks the association of the mutex with *this without unlocking any locks held. \nReturns\nA pointer to the mutex associated with *this prior to the call if any, NULL otherwise.\nPostconditions\nthis->mutex()==NULL, this->owns_lock()==false. \nThrows\nNothing. \nNOTE\nIf this->owns_lock() would have returned true prior to the call, the\ncaller would now be responsible for unlocking the mutex. \nD.5.10 std::shared_lock class template \nThe std::shared_lock class template provides an equivalent to std::unique_lock,\nexcept that it acquires a shared lock rather than an exclusive lock. The type of mutex\nbeing locked is specified by the template parameter Mutex, which must meet the\nSharedLockable requirements. In general, the specified mutex is locked in the con-\nstructor and unlocked in the destructor, although additional constructors and mem-\nber functions are provided to allow other possibilities. This provides a means of\nlocking a mutex for a block of code and ensuring that the mutex is unlocked when\n",
      "content_length": 1655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 547,
      "chapter": null,
      "content": "524\nAPPENDIX D\nC++ Thread Library reference\nthe block is left, whether that’s by running off the end, by the use of a control flow\nstatement such as break or return, or by throwing an exception. All instantiations of\nstd::shared_lock are suitable for use with the Lockable parameter for the std::\ncondition_variable_any wait functions. \n Every std::shared_lock<Mutex> meets the Lockable requirements. If, in addi-\ntion, the supplied Mutex type meets the SharedTimedLockable requirements, then\nstd::shared_lock<Mutex> also meets the TimedLockable requirements. \n Instances of std::shared_lock are MoveConstructible and MoveAssignable but\nnot CopyConstructible or CopyAssignable. \nClass definition\ntemplate <class Mutex>\nclass shared_lock\n{\npublic:\n    typedef Mutex mutex_type;\n    shared_lock() noexcept;\n    explicit shared_lock(mutex_type& m);\n    shared_lock(mutex_type& m, adopt_lock_t);\n    shared_lock(mutex_type& m, defer_lock_t) noexcept;\n    shared_lock(mutex_type& m, try_to_lock_t);\n    template<typename Clock,typename Duration>\n    shared_lock(\n        mutex_type& m,\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n    template<typename Rep,typename Period>\n    shared_lock(\n        mutex_type& m,\n        std::chrono::duration<Rep,Period> const& relative_time);\n    ~shared_lock();\n    shared_lock(shared_lock const& ) = delete;\n    shared_lock& operator=(shared_lock const& ) = delete;\n    shared_lock(shared_lock&& );\n    shared_lock& operator=(shared_lock&& );\n    void swap(shared_lock& other) noexcept;\n    void lock();\n    bool try_lock();\n    template<typename Rep, typename Period>\n    bool try_lock_for(\n        std::chrono::duration<Rep,Period> const& relative_time);\n    template<typename Clock, typename Duration>\n    bool try_lock_until(\n        std::chrono::time_point<Clock,Duration> const& absolute_time);\n    void unlock();\n    explicit operator bool() const noexcept;\n    bool owns_lock() const noexcept;\n    Mutex* mutex() const noexcept;\n",
      "content_length": 1996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 548,
      "chapter": null,
      "content": "525\n<mutex> header\n    Mutex* release() noexcept;\n};\nSTD::SHARED_LOCK DEFAULT CONSTRUCTOR \nConstructs an std::shared_lock instance with no associated mutex. \nDeclaration\nshared_lock() noexcept;\nEffects\nConstructs an std::shared_lock instance that has no associated mutex. \nPostconditions\nthis->mutex()==NULL, this->owns_lock()==false. \nSTD::SHARED_LOCK LOCKING CONSTRUCTOR \nConstructs an std::shared_lock instance that acquires a shared lock on the supplied\nmutex. \nDeclaration\nexplicit shared_lock(mutex_type& m);\nEffects \nConstructs an std::shared_lock instance that references the supplied mutex. Calls\nm.lock_shared(). \nThrows\nAny exceptions thrown by m.lock_shared(). \nPostconditions\nthis->owns_lock()==true, this->mutex()==&m. \nSTD::SHARED_LOCK LOCK-ADOPTING CONSTRUCTOR \nConstructs an std::shared_lock instance that owns the lock on the supplied mutex. \nDeclaration\nshared_lock(mutex_type& m,std::adopt_lock_t);\nPreconditions\nThe calling thread must own a shared lock on m. \nEffects\nConstructs an std::shared_lock instance that references the supplied mutex and\ntakes ownership of the shared lock on m held by the calling thread. \nThrows\nNothing. \nPostconditions\nthis->owns_lock()==true, this->mutex()==&m. \nSTD::SHARED_LOCK DEFERRED-LOCK CONSTRUCTOR \nConstructs an std::shared_lock instance that doesn’t own the lock on the supplied\nmutex. \n",
      "content_length": 1349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 549,
      "chapter": null,
      "content": "526\nAPPENDIX D\nC++ Thread Library reference\nDeclaration\nshared_lock(mutex_type& m,std::defer_lock_t) noexcept;\nEffects\nConstructs an std::shared_lock instance that references the supplied mutex. \nThrows \nNothing. \nPostconditions\nthis->owns_lock()==false, this->mutex()==&m. \nSTD::SHARED_LOCK TRY-TO-LOCK CONSTRUCTOR \nConstructs an std::shared_lock instance associated with the supplied mutex and\ntries to acquire a shared lock on that mutex. \nDeclaration\nshared_lock(mutex_type& m,std::try_to_lock_t);\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the Lockable\nrequirements. \nEffects\nConstructs an std::shared_lock instance that references the supplied mutex.\nCalls m.try_lock_shared(). \nThrows\nNothing. \nPostconditions\nthis->owns_lock() returns the result of the m.try_lock_shared() call, this->\nmutex()==&m. \nSTD::SHARED_LOCK TRY-TO-LOCK CONSTRUCTOR WITH A DURATION TIMEOUT \nConstructs an std::shared_lock instance associated with the supplied mutex and\ntries to acquire a shared lock on that mutex. \nDeclaration\ntemplate<typename Rep,typename Period>\nshared_lock(\n    mutex_type& m,\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nLockable requirements. \nEffects\nConstructs an std::shared_lock instance that references the supplied mutex.\nCalls m.try_lock_shared_for(relative_time). \nThrows\nNothing. \n",
      "content_length": 1440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 550,
      "chapter": null,
      "content": "527\n<mutex> header\nPostconditions\nthis->owns_lock() returns the result of the m.try_lock_shared_for() call, this->\nmutex()==&m. \nSTD::SHARED_LOCK TRY-TO-LOCK CONSTRUCTOR WITH A TIME_POINT TIMEOUT \nConstructs an std::shared_lock instance associated with the supplied mutex and\ntries to acquire a shared lock on that mutex. \nDeclaration\ntemplate<typename Clock,typename Duration>\nshared_lock(\n    mutex_type& m,\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nLockable requirements. \nEffects\nConstructs an std::shared_lock instance that references the supplied mutex.\nCalls m.try_lock_shared_until(absolute_time). \nThrows\nNothing. \nPostconditions\nthis->owns_lock() returns the result of the m.try_lock_shared_until() call, this\n->mutex()==&m. \nSTD::SHARED_LOCK MOVE-CONSTRUCTOR \nTransfers ownership of a shared lock from one std::shared_lock object to a newly\ncreated std::shared_lock object. \nDeclaration\nshared_lock(shared_lock&& other) noexcept;\nEffects\nConstructs an std::shared_lock instance. If other owned a shared lock on a\nmutex prior to the constructor invocation, that lock is now owned by the newly cre-\nated std::shared_lock object. \nPostconditions\nFor a newly-constructed std::shared_lock object, x, x.mutex() is equal to the value\nof other.mutex() prior to the constructor invocation, and x.owns_lock() is equal to\nthe value of other.owns_lock() prior to the constructor invocation. other.mutex()\n==NULL, other.owns_lock()==false. \nThrows\nNothing. \nNOTE\nstd::shared_lock objects are not CopyConstructible, so there’s no\ncopy constructor, only this move constructor.\n",
      "content_length": 1691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 551,
      "chapter": null,
      "content": "528\nAPPENDIX D\nC++ Thread Library reference\nSTD::SHARED_LOCK MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of a shared lock from one std::shared_lock object to another\nstd::shared_lock object. \nDeclaration\nshared_lock& operator=(shared_lock&& other) noexcept;\nEffects\nIf this->owns_lock()returns true prior to the call, calls this->unlock(). If other\nowned a shared lock on a mutex prior to the assignment, that lock is now owned by\n*this. \nPostconditions\nthis->mutex() is equal to the value of other.mutex() prior to the assignment, and\nthis->owns_lock() is equal to the value of other.owns_lock() prior to the assign-\nment. other.mutex()==NULL, other.owns_lock()==false. \nThrows\nNothing. \nNOTE\nstd::shared_lock objects are not CopyAssignable, so there’s no copy-\nassignment operator, only this move-assignment operator. \nSTD::SHARED_LOCK DESTRUCTOR \nDestroys an std::shared_lock instance and unlocks the corresponding mutex if it’s\nowned by the destroyed instance. \nDeclaration\n~shared_lock();\nEffects\nIf this->owns_lock()returns true, calls this->mutex()->unlock_shared(). \nThrows\nNothing. \nSTD::SHARED_LOCK::SWAP MEMBER FUNCTION \nExchanges ownership of their associated shared_locks of execution between two\nstd::shared_lock objects. \nDeclaration\nvoid swap(shared_lock& other) noexcept;\nEffects\nIf other owns a lock on a mutex prior to the call, that lock is now owned by *this.\nIf *this owns a lock on a mutex prior to the call, that lock is now owned by other. \nPostconditions\nthis->mutex() is equal to the value of other.mutex() prior to the call. other\n.mutex() is equal to the value of this->mutex() prior to the call. this->owns\n_lock() is equal to the value of other.owns_lock() prior to the call. other.owns\n_lock() is equal to the value of this->owns_lock() prior to the call. \n",
      "content_length": 1791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 552,
      "chapter": null,
      "content": "529\n<mutex> header\nThrows\nNothing. \nSWAP NONMEMBER FUNCTION FOR STD::SHARED_LOCK\nExchanges ownership of their associated mutex locks between two std::shared_lock\nobjects. \nDeclaration\nvoid swap(shared_lock& lhs,shared_lock& rhs) noexcept;\nEffects\nlhs.swap(rhs) \nThrows\nNothing. \nSTD::SHARED_LOCK::LOCK MEMBER FUNCTION \nAcquires a shared lock on the mutex associated with *this. \nDeclaration\nvoid lock();\nPreconditions\nthis->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->lock_shared(). \nThrows\nAny exceptions thrown by this->mutex()->lock_shared(). std::system_error\nwith an error code of std::errc::operation_not_permitted if this->mutex()\n==NULL. std::system_error with an error code of std::errc::resource_deadlock\n_would_occur if this->owns_lock()==true on entry. \nPostconditions\nthis->owns_lock()==true. \nSTD::SHARED_LOCK::TRY_LOCK MEMBER FUNCTION \nAttempts to acquire a shared lock on the mutex associated with *this. \nDeclaration\nbool try_lock();\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the Lockable\nrequirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock_shared(). \nReturns\ntrue if the call to this->mutex()->try_lock_shared() returned true, false\notherwise. \n",
      "content_length": 1272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 553,
      "chapter": null,
      "content": "530\nAPPENDIX D\nC++ Thread Library reference\nThrows\nAny exceptions thrown by this->mutex()->try_lock_shared(). std::system_error\nwith an error code of std::errc::operation_not_permitted if this->mutex()==\nNULL. std::system_error with an error code of std::errc::resource_deadlock\n_would_occur if this->owns_lock()==true on entry. \nPostconditions\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::SHARED_LOCK::UNLOCK MEMBER FUNCTION \nReleases a shared lock on the mutex associated with *this. \nDeclaration\nvoid unlock();\nPreconditions\nthis->mutex()!=NULL, this->owns_lock()==true. \nEffects\nCalls this->mutex()->unlock_shared(). \nThrows\nAny exceptions thrown by this->mutex()->unlock_shared(). std::system_error\nwith an error code of std::errc::operation_not_permitted if this->owns_lock()\n== false on entry. \nPostconditions\nthis->owns_lock()==false. \nSTD::SHARED_LOCK::TRY_LOCK_FOR MEMBER FUNCTION \nAttempts to acquire a shared lock on the mutex associated with *this within the time\nspecified. \nDeclaration\ntemplate<typename Rep, typename Period>\nbool try_lock_for(\n    std::chrono::duration<Rep,Period> const& relative_time);\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nLockable requirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock_shared_for(relative_time). \nReturns\ntrue if the call to this->mutex()->try_lock_shared_for() returned true, false\notherwise. \nThrows\nAny exceptions thrown by this->mutex()->try_lock_shared_for(). std::system\n_error with an error code of std::errc::operation_not_permitted if this->\n",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 554,
      "chapter": null,
      "content": "531\n<mutex> header\nmutex()==NULL. std::system_error with an error code of std::errc::resource\n_deadlock_would_occur if this->owns_lock()==true on entry. \nPostconditions\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::SHARED_LOCK::TRY_LOCK_UNTIL MEMBER FUNCTION \nAttempts to acquire a shared lock on the mutex associated with *this within the time\nspecified. \nDeclaration\ntemplate<typename Clock, typename Duration>\nbool try_lock_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nPreconditions\nThe Mutex type used to instantiate std::shared_lock must meet the SharedTimed-\nLockable requirements. this->mutex()!=NULL, this->owns_lock()==false. \nEffects\nCalls this->mutex()->try_lock_shared_until(absolute_time). \nReturns\ntrue if the call to this->mutex()->try_lock_shared_until() returned true, false\notherwise. \nThrows\nAny exceptions thrown by this->mutex()->try_lock_shared_until(). std::system\n_error with an error code of std::errc::operation_not_permitted if this->\nmutex()==NULL. std::system_error with an error code of std::errc::resource_\ndeadlock_would_occur if this->owns_lock()==true on entry. \nPostcondition\nIf the function returns true, this->owns_lock()==true, otherwise this->owns_\nlock()==false. \nSTD::SHARED_LOCK::OPERATOR BOOL MEMBER FUNCTION \nChecks whether or not *this owns a shared lock on a mutex. \nDeclaration\nexplicit operator bool() const noexcept;\nReturns\nthis->owns_lock(). \nThrows\nNothing. \nNOTE\nThis is an explicit conversion operator, so it’s only implicitly called\nin contexts where the result is used as a Boolean and not where the result\nwould be treated as an integer value of 0 or 1.\n",
      "content_length": 1692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 555,
      "chapter": null,
      "content": "532\nAPPENDIX D\nC++ Thread Library reference\nSTD::SHARED_LOCK::OWNS_LOCK MEMBER FUNCTION \nChecks whether or not *this owns a shared lock on a mutex. \nDeclaration\nbool owns_lock() const noexcept;\nReturns\ntrue if *this owns a shared lock on a mutex, false otherwise. \nThrows\nNothing. \nSTD::SHARED_LOCK::MUTEX MEMBER FUNCTION \nReturns the mutex associated with *this if any. \nDeclaration\nmutex_type* mutex() const noexcept;\nReturns\nA pointer to the mutex associated with *this if any, NULL otherwise. \nThrows\nNothing. \nSTD::SHARED_LOCK::RELEASE MEMBER FUNCTION \nReturns the mutex associated with *this if any, and releases that association. \nDeclaration\nmutex_type* release() noexcept;\nEffects\nBreaks the association of the mutex with *this without unlocking any locks held. \nReturns\nA pointer to the mutex associated with *this prior to the call if any, NULL otherwise. \nPostconditions\nthis->mutex()==NULL, this->owns_lock()==false. \nThrows\nNothing. \nNOTE\nIf this->owns_lock() would have returned true prior to the call, the\ncaller would now be responsible for unlocking the mutex. \n",
      "content_length": 1080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 556,
      "chapter": null,
      "content": "533\n<mutex> header\nD.5.11 std::lock function template \nThe std::lock function template provides a means of locking more than one mutex\nat the same time, without risk of deadlock resulting from inconsistent lock orders. \nDeclaration\ntemplate<typename LockableType1,typename... LockableType2>\nvoid lock(LockableType1& m1,LockableType2& m2...);\nPreconditions\nThe types of the supplied lockable objects, LockableType1, LockableType2, ...,\nshall conform to the Lockable requirements. \nEffects\nAcquires a lock on each of the supplied lockable objects, m1, m2, ..., by an unspeci-\nfied sequence of calls to the lock(), try_lock(), and unlock() members of those\ntypes that avoid deadlock. \nPostconditions\nThe current thread owns a lock on each of the supplied lockable objects. \nThrows\nAny exceptions thrown by the calls to lock(), try_lock(), and unlock(). \nNOTE\nIf an exception propagates out of the call to std::lock, then\nunlock() shall have been called for any of the objects m1, m2, ... for which a\nlock has been acquired in the function by a call to lock() or try_lock(). \nD.5.12 std::try_lock function template \nThe std::try_lock function template allows you to try to lock a set of lockable\nobjects in one go, so either they are all locked or none are locked. \nDeclaration\ntemplate<typename LockableType1,typename... LockableType2>\nint try_lock(LockableType1& m1,LockableType2& m2...);\nPreconditions\nThe types of the supplied lockable objects, LockableType1, LockableType2, ...,\nshall conform to the Lockable requirements. \nEffects\nTries to acquires a lock on each of the supplied lockable objects, m1, m2, ..., by call-\ning try_lock() on each in turn. If a call to try_lock() returns false or throws an\nexception, locks already acquired are released by calling unlock() on the corre-\nsponding lockable object. \nReturns\n-1 if all locks were acquired (each call to try_lock() returned true), otherwise the\nzero-based index of the object for which the call to try_lock() returned false. \nPostconditions\nIf the function returns -1, the current thread owns a lock on each of the supplied\nlockable objects. Otherwise, any locks acquired by this call have been released. \n",
      "content_length": 2167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 557,
      "chapter": null,
      "content": "534\nAPPENDIX D\nC++ Thread Library reference\nThrows\nAny exceptions thrown by the calls to try_lock(). \nNOTE\nIf an exception propagates out of the call to std::try_lock, then\nunlock() shall have been called for any of the objects, m1, m2, ..., for which a\nlock has been acquired in the function by a call to try_lock(). \nD.5.13 std::once_flag class \nInstances of std::once_flag are used with std::call_once to ensure that a particular\nfunction is called exactly once, even if multiple threads invoke the call concurrently. \n Instances of std::once_flag are not CopyConstructible, CopyAssignable, Move-\nConstructible, or MoveAssignable. \nClass definition\nstruct once_flag\n{\n    constexpr once_flag() noexcept;\n    once_flag(once_flag const& ) = delete;\n    once_flag& operator=(once_flag const& ) = delete;\n};\nSTD::ONCE_FLAG DEFAULT CONSTRUCTOR \nThe std::once_flag default constructor creates a new std::once_flag instance in a\nstate, which indicates that the associated function hasn’t been called. \nDeclaration\nconstexpr once_flag() noexcept;\nEffects\nConstructs a new std::once_flag instance in a state, which indicates that the asso-\nciated function hasn’t been called. Because this is a constexpr constructor, an\ninstance with static storage duration is constructed as part of the static initialization\nphase, which avoids race conditions and order-of-initialization problems. \nD.5.14 std::call_once function template \nstd::call_once is used with an instance of std::once_flag to ensure that a particular\nfunction is called exactly once, even if multiple threads invoke the call concurrently. \nDeclaration\ntemplate<typename Callable,typename... Args>\nvoid call_once(std::once_flag& flag,Callable func,Args args...);\nPreconditions\nThe expression INVOKE(func,args) is valid for the supplied values of func and\nargs. Callable and every member of Args are MoveConstructible. \nEffects\nInvocations of std::call_once on the same std::once_flag object are serialized.\nIf there has been no prior effective std::call_once invocation on the same\n",
      "content_length": 2036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 558,
      "chapter": null,
      "content": "535\n<ratio> header\nstd::once_flag object, the argument func (or a copy thereof) is called as-if by\nINVOKE(func,args), and the invocation of std::call_once is effective if and only\nif the invocation of func returns without throwing an exception. If an exception is\nthrown, the exception is propagated to the caller. If there has been a prior effective\nstd::call_once on the same std::once_flag object, the invocation of std::\ncall_once returns without invoking func. \nSynchronization\nThe completion of an effective std::call_once invocation on an std::once_flag\nobject happens-before all subsequent std::call_once invocations on the same\nstd::once_flag object. \nThrows\nstd::system_error when the effects can’t be achieved or for any exception propa-\ngated from the invocation of func. \nD.6\n<ratio> header \nThe <ratio> header provides support for compile-time rational arithmetic. \nHeader contents\nnamespace std\n{\n    template<intmax_t N,intmax_t D=1>\n    class ratio;\n    // ratio arithmetic\n    template <class R1, class R2>\n    using ratio_add = see description;\n    template <class R1, class R2>\n    using ratio_subtract = see description;\n    template <class R1, class R2>\n    using ratio_multiply = see description;\n    template <class R1, class R2>\n    using ratio_divide = see description;\n    // ratio comparison\n    template <class R1, class R2>\n    struct ratio_equal;\n    template <class R1, class R2>\n    struct ratio_not_equal;\n    template <class R1, class R2>\n    struct ratio_less;\n    template <class R1, class R2>\n    struct ratio_less_equal;\n    template <class R1, class R2>\n    struct ratio_greater;\n    template <class R1, class R2>\n    struct ratio_greater_equal;\n",
      "content_length": 1686,
      "extraction_method": "Direct"
    },
    {
      "page_number": 559,
      "chapter": null,
      "content": "536\nAPPENDIX D\nC++ Thread Library reference\n    typedef ratio<1, 1000000000000000000> atto;\n    typedef ratio<1, 1000000000000000> femto;\n    typedef ratio<1, 1000000000000> pico;\n    typedef ratio<1, 1000000000> nano;\n    typedef ratio<1, 1000000> micro;\n    typedef ratio<1, 1000> milli;\n    typedef ratio<1, 100> centi;\n    typedef ratio<1, 10> deci;\n    typedef ratio<10, 1> deca;\n    typedef ratio<100, 1> hecto;\n    typedef ratio<1000, 1> kilo;\n    typedef ratio<1000000, 1> mega;\n    typedef ratio<1000000000, 1> giga;\n    typedef ratio<1000000000000, 1> tera;\n    typedef ratio<1000000000000000, 1> peta;\n    typedef ratio<1000000000000000000, 1> exa;\n}\nD.6.1\nstd::ratio class template \nThe std::ratio class template provides a mechanism for compile-time arithmetic\ninvolving rational values such as one half (std::ratio<1,2>), two thirds (std::\nratio<2,3>), or fifteen forty-thirds (std::ratio<15,43>). It’s used within the C++\nStandard Library for specifying the period for instantiating the std::chrono::duration\nclass template. \nClass definition\ntemplate <intmax_t N, intmax_t D = 1>\nclass ratio\n{\npublic:\n    typedef ratio<num, den> type;\n    static constexpr intmax_t num= see below;\n    static constexpr intmax_t den= see below;\n};\nRequirements\nD may not be zero. \nDescription\nnum and den are the numerator and denominator of the fraction N/D reduced to\nlowest terms. den is always positive. If N and D are the same sign, num is positive;\notherwise num is negative.\nExamples \nratio<4,6>::num == 2\nratio<4,6>::den == 3\nratio<4,-6>::num == -2\nratio<4,-6>::den == 3\n",
      "content_length": 1578,
      "extraction_method": "Direct"
    },
    {
      "page_number": 560,
      "chapter": null,
      "content": "537\n<ratio> header\nD.6.2\nstd::ratio_add template alias\nThe std::ratio_add template alias provides a mechanism for adding two std::ratio\nvalues at compile time, using rational arithmetic. \nDefinition\ntemplate <class R1, class R2>\nusing ratio_add = std::ratio<see below>;\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nEffects\nratio_add<R1,R2> is defined as an alias for an instantiation of std::ratio that\nrepresents the sum of the fractions represented by R1 and R2 if that sum can be cal-\nculated without overflow. If the calculation of the result overflows, the program is\nill-formed. In the absence of arithmetic overflow, std::ratio_add<R1,R2> shall\nhave the same num and den values as std::ratio<R1::num * R2::den + R2::num *\nR1::den, R1::den * R2::den>. \nExamples\nstd::ratio_add<std::ratio<1,3>, std::ratio<2,5> >::num == 11\nstd::ratio_add<std::ratio<1,3>, std::ratio<2,5> >::den == 15\nstd::ratio_add<std::ratio<1,3>, std::ratio<7,6> >::num == 3\nstd::ratio_add<std::ratio<1,3>, std::ratio<7,6> >::den == 2\nD.6.3\nstd::ratio_subtract template alias\nThe std::ratio_subtract template alias provides a mechanism for subtracting two\nstd::ratio values at compile time, using rational arithmetic. \nDefinition\ntemplate <class R1, class R2>\nusing ratio_subtract = std::ratio<see below>;\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nEffects\nratio_subtract<R1,R2> is defined as an alias for an instantiation of std::ratio\nthat represents the difference of the fractions represented by R1 and R2 if that dif-\nference can be calculated without overflow. If the calculation of the result over-\nflows, the program is ill-formed. In the absence of arithmetic overflow, std::ratio\n_subtract<R1,R2> shall have the same num and den values as std::ratio<R1::num\n* R2::den - R2::num * R1::den, R1::den * R2::den>. \nExamples\nstd::ratio_subtract<std::ratio<1,3>, std::ratio<1,5> >::num == 2\nstd::ratio_subtract<std::ratio<1,3>, std::ratio<1,5> >::den == 15\nstd::ratio_subtract<std::ratio<1,3>, std::ratio<7,6> >::num == -5\nstd::ratio_subtract<std::ratio<1,3>, std::ratio<7,6> >::den == 6\n",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 561,
      "chapter": null,
      "content": "538\nAPPENDIX D\nC++ Thread Library reference\nD.6.4\nstd::ratio_multiply template alias\nThe std::ratio_multiply template alias provides a mechanism for multiplying two\nstd::ratio values at compile time, using rational arithmetic. \nDefinition\ntemplate <class R1, class R2>\nusing ratio_multiply = std::ratio<see below>;\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nEffects\nratio_multiply<R1,R2> is defined as an alias for an instantiation of std::ratio\nthat represents the product of the fractions represented by R1 and R2 if that prod-\nuct can be calculated without overflow. If the calculation of the result overflows, the\nprogram is ill-formed. In the absence of arithmetic overflow, std::ratio_multiply\n<R1,R2> shall have the same num and den values as std::ratio<R1::num * R2::num,\nR1::den * R2::den>. \nExamples\nstd::ratio_multiply<std::ratio<1,3>, std::ratio<2,5> >::num == 2\nstd::ratio_multiply<std::ratio<1,3>, std::ratio<2,5> >::den == 15\nstd::ratio_multiply<std::ratio<1,3>, std::ratio<15,7> >::num == 5\nstd::ratio_multiply<std::ratio<1,3>, std::ratio<15,7> >::den == 7\nD.6.5\nstd::ratio_divide template alias\nThe std::ratio_divide template alias provides a mechanism for dividing two std::\nratio values at compile time, using rational arithmetic. \nDefinition\ntemplate <class R1, class R2>\nusing ratio_divide = std::ratio<see below>;\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nEffects\nratio_divide<R1,R2> is defined as an alias for an instantiation of std::ratio that\nrepresents the result of dividing the fractions represented by R1 and R2 if that result\ncan be calculated without overflow. If the calculation overflows, the program is ill-\nformed. In the absence of arithmetic overflow, std::ratio_divide<R1,R2> shall\nhave the same num and den values as std::ratio<R1::num * R2::den, R1::den *\nR2::num>. \nExamples\nstd::ratio_divide<std::ratio<1,3>, std::ratio<2,5> >::num == 5\nstd::ratio_divide<std::ratio<1,3>, std::ratio<2,5> >::den == 6\nstd::ratio_divide<std::ratio<1,3>, std::ratio<15,7> >::num == 7\nstd::ratio_divide<std::ratio<1,3>, std::ratio<15,7> >::den == 45\n",
      "content_length": 2151,
      "extraction_method": "Direct"
    },
    {
      "page_number": 562,
      "chapter": null,
      "content": "539\n<ratio> header\nD.6.6\nstd::ratio_equal class template \nThe std::ratio_equal class template provides a mechanism for comparing two std::\nratio values for equality at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_equal:\n    public std::integral_constant<\n        bool,(R1::num == R2::num) && (R1::den == R2::den)>\n{};\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nExamples\nstd::ratio_equal<std::ratio<1,3>, std::ratio<2,6> >::value == true\nstd::ratio_equal<std::ratio<1,3>, std::ratio<1,6> >::value == false\nstd::ratio_equal<std::ratio<1,3>, std::ratio<2,3> >::value == false\nstd::ratio_equal<std::ratio<1,3>, std::ratio<1,3> >::value == true\nD.6.7\nstd::ratio_not_equal class template \nThe std::ratio_not_equal class template provides a mechanism for comparing two\nstd::ratio values for inequality at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_not_equal:\n    public std::integral_constant<bool,!ratio_equal<R1,R2>::value>\n{};\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nExamples\nstd::ratio_not_equal<std::ratio<1,3>, std::ratio<2,6> >::value == false\nstd::ratio_not_equal<std::ratio<1,3>, std::ratio<1,6> >::value == true\nstd::ratio_not_equal<std::ratio<1,3>, std::ratio<2,3> >::value == true\nstd::ratio_not_equal<std::ratio<1,3>, std::ratio<1,3> >::value == false\nD.6.8\nstd::ratio_less class template \nThe std::ratio_less class template provides a mechanism for comparing two std::\nratio values at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_less:\n    public std::integral_constant<bool,see below>\n{};\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template.\n",
      "content_length": 1829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 563,
      "chapter": null,
      "content": "540\nAPPENDIX D\nC++ Thread Library reference\nEffects\nstd::ratio_less<R1,R2> derives from std::integral_constant<bool, value >,\nwhere value is (R1::num * R2::den) < (R2::num * R1::den). Where possible, imple-\nmentations shall use a method of calculating the result that avoids overflow. If over-\nflow occurs, the program is ill-formed.\nExamples\nstd::ratio_less<std::ratio<1,3>, std::ratio<2,6> >::value == false\nstd::ratio_less<std::ratio<1,6>, std::ratio<1,3> >::value == true\nstd::ratio_less<\n    std::ratio<999999999,1000000000>, \n    std::ratio<1000000001,1000000000> >::value == true\nstd::ratio_less<\n    std::ratio<1000000001,1000000000>,\n    std::ratio<999999999,1000000000> >::value == false\nD.6.9\nstd::ratio_greater class template \nThe std::ratio_greater class template provides a mechanism for comparing two\nstd::ratio values at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_greater:\n    public std::integral_constant<bool,ratio_less<R2,R1>::value>\n{};\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nD.6.10 std::ratio_less_equal class template \nThe std::ratio_less_equal class template provides a mechanism for comparing two\nstd::ratio values at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_less_equal:\n    public std::integral_constant<bool,!ratio_less<R2,R1>::value>\n{};\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nD.6.11 std::ratio_greater_equal class template \nThe std::ratio_greater_equal class template provides a mechanism for comparing\ntwo std::ratio values at compile time, using rational arithmetic. \nClass definition\ntemplate <class R1, class R2>\nclass ratio_greater_equal:\n    public std::integral_constant<bool,!ratio_less<R1,R2>::value>\n{};\n",
      "content_length": 1847,
      "extraction_method": "Direct"
    },
    {
      "page_number": 564,
      "chapter": null,
      "content": "541\n<thread> header\nPreconditions\nR1 and R2 must be instantiations of the std::ratio class template. \nD.7\n<thread> header \nThe <thread> header provides facilities for managing and identifying threads and\nprovides functions for making the current thread sleep. \nHeader contents\nnamespace std\n{\n    class thread;\n    namespace this_thread\n    {\n        thread::id get_id() noexcept;\n        void yield() noexcept;\n        template<typename Rep,typename Period>\n        void sleep_for(\n            std::chrono::duration<Rep,Period> sleep_duration);\n        template<typename Clock,typename Duration>\n        void sleep_until(\n            std::chrono::time_point<Clock,Duration> wake_time);\n    }\n}\nD.7.1\nstd::thread class \nThe std::thread class is used to manage a thread of execution. It provides a means of\nstarting a new thread of execution and waiting for the completion of a thread of exe-\ncution. It also provides a means for identifying and provides other functions for man-\naging threads of execution. \nClass definition \nclass thread\n{\npublic:\n    // Types\n    class id;\n    typedef implementation-defined native_handle_type; // optional\n    // Construction and Destruction\n    thread() noexcept;\n    ~thread();\n    template<typename Callable,typename Args...>\n    explicit thread(Callable&& func,Args&&... args);\n    // Copying and Moving\n    thread(thread const& other) = delete;\n    thread(thread&& other) noexcept;\n    thread& operator=(thread const& other) = delete;\n    thread& operator=(thread&& other) noexcept;\n",
      "content_length": 1525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 565,
      "chapter": null,
      "content": "542\nAPPENDIX D\nC++ Thread Library reference\n    void swap(thread& other) noexcept;\n    void join();\n    void detach();\n    bool joinable() const noexcept;\n    id get_id() const noexcept;\n    native_handle_type native_handle();\n    static unsigned hardware_concurrency() noexcept;\n};\nvoid swap(thread& lhs,thread& rhs);\nSTD::THREAD::ID CLASS \nAn instance of std::thread::id identifies a particular thread of execution. \nClass definition\nclass thread::id\n{\npublic:\n    id() noexcept;\n};\nbool operator==(thread::id x, thread::id y) noexcept;\nbool operator!=(thread::id x, thread::id y) noexcept;\nbool operator<(thread::id x, thread::id y) noexcept;\nbool operator<=(thread::id x, thread::id y) noexcept;\nbool operator>(thread::id x, thread::id y) noexcept;\nbool operator>=(thread::id x, thread::id y) noexcept;\ntemplate<typename charT, typename traits>\nbasic_ostream<charT, traits>&\noperator<< (basic_ostream<charT, traits>&& out, thread::id id);\nNotes\nThe std::thread::id value that identifies a particular thread of execution shall be\ndistinct from the value of a default-constructed std::thread::id instance and\nfrom any value that represents another thread of execution. \nThe std::thread::id values for particular threads aren’t predictable and may\nvary between executions of the same program. \nstd::thread::id is CopyConstructible and CopyAssignable, so instances of\nstd::thread::id may be freely copied and assigned. \nSTD::THREAD::ID DEFAULT CONSTRUCTOR \nConstructs an std::thread::id object that doesn’t represent any thread of execution. \nDeclaration\nid() noexcept;\nEffects\nConstructs an std::thread::id instance that has the singular not any thread value. \nThrows\nNothing. \n",
      "content_length": 1679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 566,
      "chapter": null,
      "content": "543\n<thread> header\nNOTE\nAll default-constructed std::thread::id instances store the same value. \nSTD::THREAD::ID EQUALITY COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if they represent the same thread\nof execution. \nDeclaration\nbool operator==(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\ntrue if both lhs and rhs represent the same thread of execution or both have the\nsingular not any thread value. false if lhs and rhs represent different threads of\nexecution or one represents a thread of execution and the other has the singular\nnot any thread value. \nThrows \nNothing. \nSTD::THREAD::ID INEQUALITY COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if they represent different threads\nof execution. \nDeclaration\nbool operator!=(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\n!(lhs==rhs) \nThrows\nNothing. \nSTD::THREAD::ID LESS-THAN COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies before the other in the\ntotal ordering of thread ID values. \nDeclaration\nbool operator<(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\ntrue if the value of lhs occurs before the value of rhs in the total ordering of\nthread ID values. If lhs!=rhs, exactly one of lhs<rhs or rhs<lhs returns true and\nthe other returns false. If lhs==rhs, lhs<rhs and rhs<lhs both return false. \nThrows\nNothing. \nNOTE\nThe singular not any thread value held by a default-constructed std::\nthread::id instance compares less than any std::thread::id instance that\nrepresents a thread of execution. If two instances of std::thread::id are\nequal, neither is less than the other. Any set of distinct std::thread::id val-\nues forms a total order, which is consistent throughout an execution of a pro-\ngram. This order may vary between executions of the same program. \n",
      "content_length": 1841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 567,
      "chapter": null,
      "content": "544\nAPPENDIX D\nC++ Thread Library reference\nSTD::THREAD::ID LESS-THAN OR EQUAL COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies before the other in the\ntotal ordering of thread ID values or is equal to it. \nDeclaration\nbool operator<=(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\n!(rhs<lhs)\nThrows\nNothing.\nSTD::THREAD::ID GREATER-THAN COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies after the other in the\ntotal ordering of thread ID values. \nDeclaration\nbool operator>(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\nrhs<lhs \nThrows\nNothing.\nSTD::THREAD::ID GREATER-THAN OR EQUAL COMPARISON OPERATOR \nCompares two instances of std::thread::id to see if one lies after the other in the\ntotal ordering of thread ID values or is equal to it. \nDeclaration\nbool operator>=(std::thread::id lhs,std::thread::id rhs) noexcept;\nReturns\n!(lhs<rhs)\nThrows\nNothing. \nSTD::THREAD::ID STREAM INSERTION OPERATOR \nWrites a string representation of the std::thread::id value into the specified stream. \nDeclaration\ntemplate<typename charT, typename traits>\nbasic_ostream<charT, traits>&\noperator<< (basic_ostream<charT, traits>&& out, thread::id id);\nEffects\nInserts a string representation of the std::thread::id value into the specified stream. \nReturns\nout \nThrows\nNothing. \n",
      "content_length": 1355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 568,
      "chapter": null,
      "content": "545\n<thread> header\nNOTE\nThe format of the string representation isn’t specified. Instances of\nstd::thread::id that compare equal have the same representation, and\ninstances that aren’t equal have distinct representations. \nSTD::THREAD::NATIVE_HANDLE_TYPE TYPEDEF \nnative_handle_type is a typedef to a type that can be used with platform-specific APIs. \nDeclaration\ntypedef implementation-defined native_handle_type;\nNOTE\nThis typedef is optional. If present, the implementation should provide\na type that’s suitable for use with native platform-specific APIs.\nSTD::THREAD::NATIVE_HANDLE MEMBER FUNCTION \nReturns a value of type native_handle_type that represents the thread of execution\nassociated with *this. \nDeclaration\nnative_handle_type native_handle();\nNOTE\nThis function is optional. If present, the value returned should be suit-\nable for use with the native platform-specific APIs.\nSTD::THREAD DEFAULT CONSTRUCTOR \nConstructs an std::thread object without an associated thread of execution. \nDeclaration\nthread() noexcept;\nEffects\nConstructs an std::thread instance that has no associated thread of execution. \nPostconditions\nFor a newly constructed std::thread object, x, x.get_id()==id(). \nThrows\nNothing. \nSTD::THREAD CONSTRUCTOR \nConstructs an std::thread object associated with a new thread of execution. \nDeclaration\ntemplate<typename Callable,typename Args...>\nexplicit thread(Callable&& func,Args&&... args);\nPreconditions \nfunc and each element of args must be MoveConstructible. \nEffects\nConstructs an std::thread instance and associates it with a newly created thread of\nexecution. Copies or moves func and each element of args into internal storage\nthat persists for the lifetime of the new thread of execution. Performs INVOKE\n(copy-of-func,copy-of-args) on the new thread of execution.\n",
      "content_length": 1810,
      "extraction_method": "Direct"
    },
    {
      "page_number": 569,
      "chapter": null,
      "content": "546\nAPPENDIX D\nC++ Thread Library reference\nPostconditions\nFor a newly constructed std::thread object, x, x.get_id()!=id(). \nThrows\nAn exception of type std::system_error if unable to start the new thread. Any\nexception thrown by copying func or args into internal storage. \nSynchronization\nThe invocation of the constructor happens-before the execution of the supplied\nfunction on the newly created thread of execution. \nSTD::THREAD MOVE-CONSTRUCTOR \nTransfers ownership of a thread of execution from one std::thread object to a newly\ncreated std::thread object. \nDeclaration\nthread(thread&& other) noexcept;\nEffects\nConstructs an std::thread instance. If other has an associated thread of execution\nprior to the constructor invocation, that thread of execution is now associated with\nthe newly created std::thread object. Otherwise, the newly created std::thread\nobject has no associated thread of execution. \nPostconditions \nFor a newly constructed std::thread object, x, x.get_id() is equal to the value of\nother.get_id() prior to the constructor invocation. other.get_id()==id(). \nThrows \nNothing. \nNOTE\nstd::thread objects are not CopyConstructible, so there’s no copy\nconstructor, only this move constructor.\nSTD::THREAD DESTRUCTOR \nDestroys an std::thread object. \nDeclaration\n~thread();\nEffects\nDestroys *this. If *this has an associated thread of execution (this->joinable()\nwould return true), calls std::terminate() to abort the program. \nThrows\nNothing. \nSTD::THREAD MOVE-ASSIGNMENT OPERATOR \nTransfers ownership of a thread of execution from one std::thread object to another\nstd::thread object. \nDeclaration\nthread& operator=(thread&& other) noexcept;\n",
      "content_length": 1667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 570,
      "chapter": null,
      "content": "547\n<thread> header\nEffects\nIf this->joinable()returns true prior to the call, calls std::terminate() to abort\nthe program. If other has an associated thread of execution prior to the assign-\nment, that thread of execution is now associated with *this. Otherwise *this has\nno associated thread of execution. \nPostconditions\nthis->get_id() is equal to the value of other.get_id() prior to the call. other\n.get_id()==id(). \nThrows\nNothing. \nNOTE\nstd::thread objects are not CopyAssignable, so there’s no copy-\nassignment operator, only this move-assignment operator. \nSTD::THREAD::SWAP MEMBER FUNCTION \nExchanges ownership of their associated threads of execution between two std::\nthread objects. \nDeclaration\nvoid swap(thread& other) noexcept;\nEffects \nIf other has an associated thread of execution prior to the call, that thread of exe-\ncution is now associated with *this. Otherwise *this has no associated thread of\nexecution. If *this has an associated thread of execution prior to the call, that\nthread of execution is now associated with other. Otherwise other has no associ-\nated thread of execution. \nPostconditions\nthis->get_id() is equal to the value of other.get_id() prior to the call. other\n.get_id() is equal to the value of this->get_id() prior to the call. \nThrows\nNothing. \nSWAP NONMEMBER FUNCTION FOR STD::THREADS \nExchanges ownership of their associated threads of execution between two std::\nthread objects. \nDeclaration\nvoid swap(thread& lhs,thread& rhs) noexcept;\nEffects\nlhs.swap(rhs) \nThrows\nNothing.\nSTD::THREAD::JOINABLE MEMBER FUNCTION \nQueries whether or not *this has an associated thread of execution. \n",
      "content_length": 1634,
      "extraction_method": "Direct"
    },
    {
      "page_number": 571,
      "chapter": null,
      "content": "548\nAPPENDIX D\nC++ Thread Library reference\nDeclaration\nbool joinable() const noexcept;\nReturns\ntrue if *this has an associated thread of execution, false otherwise. \nThrows\nNothing. \nSTD::THREAD::JOIN MEMBER FUNCTION \nWaits for the thread of execution associated with *this to finish. \nDeclaration\nvoid join();\nPreconditions\nthis->joinable() would return true. \nEffects\nBlocks the current thread until the thread of execution associated with *this has\nfinished. \nPostconditions\nthis->get_id()==id(). The thread of execution associated with *this prior to the\ncall has finished. \nSynchronization\nThe completion of the thread of execution associated with *this prior to the call\nhappens-before the call to join() returns. \nThrows\nstd::system_error if the effects can’t be achieved or this->joinable() returns\nfalse. \nSTD::THREAD::DETACH MEMBER FUNCTION \nDetaches the thread of execution associated with *this to finish. \nDeclaration\nvoid detach();\nPreconditions\nthis->joinable()returns true.\nEffects\nDetaches the thread of execution associated with *this. \nPostconditions\nthis->get_id()==id(), this->joinable()==false\nThe thread of execution associated with *this prior to the call is detached and no\nlonger has an associated std::thread object. \nThrows\nstd::system_error if the effects can’t be achieved or this->joinable()returns\nfalse on invocation.\n",
      "content_length": 1352,
      "extraction_method": "Direct"
    },
    {
      "page_number": 572,
      "chapter": null,
      "content": "549\n<thread> header\nSTD::THREAD::GET_ID MEMBER FUNCTION \nReturns a value of type std::thread::id that identifies the thread of execution asso-\nciated with *this. \nDeclaration\nthread::id get_id() const noexcept;\nReturns\nIf *this has an associated thread of execution, returns an instance of std::\nthread::id that identifies that thread. Otherwise returns a default-constructed\nstd::thread::id. \nThrows\nNothing.\nSTD::THREAD::HARDWARE_CONCURRENCY STATIC MEMBER FUNCTION \nReturns a hint as to the number of threads that can run concurrently on the current\nhardware. \nDeclaration\nunsigned hardware_concurrency() noexcept;\nReturns\nThe number of threads that can run concurrently on the current hardware. This\nmay be the number of processors in the system, for example. Where this informa-\ntion is not available or well-defined, this function returns 0. \nThrows\nNothing.\nD.7.2\nNamespace this_thread \nThe functions in the std::this_thread namespace operate on the calling thread. \nSTD::THIS_THREAD::GET_ID NONMEMBER FUNCTION \nReturns a value of type std::thread::id that identifies the current thread of execution. \nDeclaration\nthread::id get_id() noexcept;\nReturns \nAn instance of std::thread::id that identifies the current thread. \nThrows\nNothing.\nSTD::THIS_THREAD::YIELD NONMEMBER FUNCTION \nUsed to inform the library that the thread that invoked the function doesn’t need to\nrun at the point of the call. Commonly used in tight loops to avoid consuming exces-\nsive CPU time. \nDeclaration\nvoid yield() noexcept;\n",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 573,
      "chapter": null,
      "content": "550\nAPPENDIX D\nC++ Thread Library reference\nEffects\nProvides the library an opportunity to schedule something else in place of the cur-\nrent thread. \nThrows\nNothing.\nSTD::THIS_THREAD::SLEEP_FOR NONMEMBER FUNCTION \nSuspends execution of the current thread for the specified duration. \nDeclaration\ntemplate<typename Rep,typename Period>\nvoid sleep_for(std::chrono::duration<Rep,Period> const& relative_time);\nEffects\nBlocks the current thread until the specified relative_time has elapsed. \nNOTE\nThe thread may be blocked for longer than the specified duration.\nWhere possible, the elapsed time is determined by a steady clock. \nThrows\nNothing.\nSTD::THIS_THREAD::SLEEP_UNTIL NONMEMBER FUNCTION \nSuspends execution of the current thread until the specified time point has been\nreached. \nDeclaration\ntemplate<typename Clock,typename Duration>\nvoid sleep_until(\n    std::chrono::time_point<Clock,Duration> const& absolute_time);\nEffects\nBlocks the current thread until the specified absolute_time has been reached for\nthe specified Clock. \nNOTE\nThere’s no guarantee as to how long the calling thread will be blocked\nfor, only that Clock::now() returned a time equal to or later than abso-\nlute_time at the point at which the thread became unblocked. \nThrows\nNothing.\n",
      "content_length": 1262,
      "extraction_method": "Direct"
    },
    {
      "page_number": 574,
      "chapter": null,
      "content": "551\nindex\nA\nABA problem 249\nabsolute_time function 550\nabstraction penalty 12\naccumulate operation 254\nacquire-release ordering 155–159\ndata dependency with 161–164\noverview of 146\ntransitive synchronization with 159–161\nActor model 107\nadd_or_update_mapping 198\nadd_to_list( ) function 41\nalgorithms\neffects of execution policies on complexity \nof 328–329\nincremental pairwise algorithms 293–299\noverview of 8\nparallelizing standard library algorithms\n327–328\nwhere and when steps are executed 329\nallocators 478\nAmdahl’s law 277–279\nArgs 375\nArgTypes 377\narguments, passing to thread functions\n24–27\narray elements 267–269\narrive_and_drop function 120\nasynchronous tasks 82\natomic operations 128–142\nfree functions for 140–142\nmemory ordering for 146–164\nacquire-release ordering 155–164\ndata dependency with \nmemory_order_consume 161–164\nnon-sequentially consistent memory \norderings 149–150\nrelaxed ordering 150–155\nsequentially consistent ordering 147–149\non standard atomic integral types 138\non std::atomic 134–137\non std::atomic_flag 132–134\non std::atomic<T*> 137–138\noverview of 127, 144\nstd::atomic<> primary class templates\n138–140\nAtomic Ptr Plus Project 239\natomic types 128, 130, 132–142\natomic variables 155\n<atomic> headers 431–466\nATOMIC_VAR_INIT macro 434\nspecializations of std::atomic templates 450\nstd::atomic class templates 439–449\nstd::atomic specializations 450–466\nstd::atomic_flag classes 436–439\nstd::atomic_signal_fence functions 436\nstd::atomic_thread_fence functions 435\nstd::atomic_xxx typedefs 433\nstd::memory_order enumeration 435\natomic_flag_init 437\natomic_load function 227\natomics 168–169\natomic_store function 227\nATOMIC_VAR_INIT macro 434\nB\nback( ) function 77\nbackground tasks\ninterrupting on application exit 325–326\nreturning values from 82–84\n",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 575,
      "chapter": null,
      "content": "INDEX\n552\nbarrier class 294\nbarriers in Concurrency TS 118\nBidirectional Iterators 333\nblocked threads 340\nblocking calls 323–324\nblocking, unwanted 340\nBoost Thread Library 11, 382\nbounded queue 194\nbroken invariants 37, 341\nbrute-force testing 347–348\nbusy-wait loops 249–250\nC\nC++ programming language\nC++11 11\nC++14 12\nC++17 12\nefficiency in Thread Library 12–13\nhistory of multithreading in 10–11\nC++ Standard Library, parallel algorithms \nfrom 331–338\ncounting visits 336–338\nexamples of using 334–336\nC++ Standards Committee 12\nC++ Thread Library 384, 401–550\n<atomic> header 431–466\n<chrono> header 401–416\n<condition_variable> header 416–431\nefficiency in 12–13\n<future> header 466–489\n<mutex> header 489–535\n<ratio> header 535–540\n<thread> header 541–549\ncache ping-pong 262, 264\ncallable objects 478\ncarries-a-dependency-to 162\nchaining continuations 110–113\nchar const 24\ncheck_for_hierarchy_violation( ) function 58\n<chrono> headers 401–416\nstd::chrono::duration class templates 401–410\nstd::chrono::high_resolution_clock \ntypedefs 416\nstd::chrono::steady_clock classes 414–416\nstd::chrono::system_clock classes 413–414\nstd::chrono::time_point class templates 410–412\nclass template argument deduction 380–381\nclear( ) function 133–134\nclocks 93–94\nclose_queue exception 387\ncode\nreviewing to locate potential bugs 342–344\nsimplifying with synchronization of \noperations 99–123\nbarriers in Concurrency TS 118\nchaining continuations 110–113\ncontinuation-style concurrency with Concur-\nrency TS 108–110\nFP with futures 99–104\nlatches in Concurrency TS 118\nstd::experimental::barrier 120–121\nstd::experimental::flex_barrier 121–123\nstd::experimental::latch 118–119\nsynchronizing operations with message \npassing 104–108\nwaiting for first future in set with \nwhen_any 115–118\nwaiting for more than one future 114–115\nstructuring for protecting shared data 42–43\ncombine_visits function 337\nCommonDuration 409\ncompare_exchange_strong( ) function 135, 160, \n208, 220\ncompare_exchange_weak( ) function 135, 138, \n208, 211\ncompiler vendors 10\nconcerns, separating 7–8\nconcurrency 126–127\napproaches to 4–6\ncomparison of libraries 382\nconcurrent operations, synchronizing 72–123\nto simplify code 99–123\nwaiting for conditions 73–81\nwaiting for events 73–81\nwaiting for one-off events with futures\n81–93\nwaiting with time limits 93–99\ncontinuation-style 108–110\ndesigning concurrent code 251–270, \n282–299\ndesigning data structures for multithreaded \nperformance 266–270\nexception safety in parallel algorithms\n271–277\nhiding latency with multiple threads\n279–280\nimproving responsiveness with \nconcurrency 280–282\nparallel implementation of std::find\n284–289\nparallel implementation of std::for_each\n282\nparallel implementation of \nstd::partial_sum 290–299\nscalability and Amdahl’s law 277–279\ntechniques for dividing work between \nthreads 252–260\ndesigning data structures for 175–176\ndesigning for 174–176\n",
      "content_length": 2912,
      "extraction_method": "Direct"
    },
    {
      "page_number": 576,
      "chapter": null,
      "content": "INDEX\n553\nconcurrency (continued)\ndisadvantages of 9–10\nenabling by separating data 185–190\nimproving responsiveness with 280–282\nin computer systems 2–4\noverview of 2–7\nparallelism vs. 6–7\nperformance of concurrent code, factors \naffecting 260–266\ncache ping-pong 262–264\ncontending data 262–264\ndata proximity 265–266\nexcessive task switching 266\nfalse sharing 264–265\nnumber of processors 261–262\noversubscription 266\nsupport in C++11 11\nsupport in C++14 12\nsupport in C++17 12\nTechnical Specification for 12\nuses for 7–10\nseparating concerns 7–8\ntask and data parallelism 8–9\nwith multiple processes 5\nwith multiple threads 6\nConcurrency TS\nbarriers in 118\ncontinuation-style concurrency with\n108–110\nlatches in 118\ncondition variables\nbuilding thread-safe queues with 76–81\nfine-grained 183–194\ninterrupting condition variable wait\n318–321\noverview of 72, 172\nthread-safe queues using 179–182\nwaiting for conditions with 74–76\nconditions, waiting for 73–81\nbuilding thread-safe queues with condition \nvariables 76–81\nwith condition variables 74–76\n<condition_variable> headers 416–431\nstd::condition_variable classes 417–424\nstd::condition_variable_any classes 424–431\nconstant initialization 367\nconstexpr functions 363–369\noverview of 361\nrequirements 367–368\ntemplates and 368–369\nuser-defined types and 365–367\nconstexpr keyword 364\nconstexpr objects 367\ncontention 262–264, 310–311\ncontext switch 3\ncontinuations, chaining 110–113\ncontinuation-style concurrency 108–110\nconveniently concurrent algorithms 8\nconverting std::chrono::duration constructors\nfrom count value 404\nfrom std::chrono::duration value 404\ncopy constructor 356\ncopyable types 61\nCopyAssignable 542\ncopy-assignment operator 138\nCopyConstructible 542, 546\ncount values 404\ncount_down function 118\ncount_down_and_wait function 119\ncounted_node_ptr 232, 245\ncounting\nreferences 226–232\nvisits 336–338\ncount_visits_per_page function 337\nCPU cycles 244, 279\ncustom_lock destructor 323\nD\ndaemon threads 23\ndata\naccess patterns in data structures 269–270\ncontending 262–264\ndividing between threads before processing \nbegins 253–254\ndividing recursively 254–260\nparallelism of 8–9\nproximity of 265–266\nseparating to enable concurrency 185–190\nsharing between threads 36–71\nproblems with 37–40\nprotecting shared data with mutexes\n40–64\ndata dependency\nwith acquire-release ordering 161–164\nwith memory_order_consume 161–164\ndata parallelism 8–9\ndata proximity 267\ndata race 39, 66, 90, 341\ndata structures\ndata access patterns in 269–270\ndesigning for concurrency 175–176\ndesigning for multithreaded performance\n266–270\nmap data structures 196–199\nrarely updated 68–70\nthread-safe 174\ndata_cond.notify_all( ) function 181\ndata_cond.notify_one( ) function 180, 190\ndata.pop( ) function 178\n",
      "content_length": 2761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 577,
      "chapter": null,
      "content": "INDEX\n554\ndata_ready flag 143\ndeadlocks 51–53\nguidelines for avoiding 53–59\nacquiring locks in fixed order 54–55\navoiding calling user-supplied code while \nholding locks 53–54\navoiding nested locks 53\nusing lock hierarchy 55–59\noverview of 51, 340\ndebugging 339–353\ntechniques for locating bugs 342–353\nby reviewing code 342–344\nby testing 344–346\ndesigning for testability 346–347\nmultithreaded testing techniques\n347–350\nstructuring multithreaded test code\n350–352\ntesting performance of multithreaded \ncode 352–353\ntypes of bugs 340–342\nrace conditions 341–342\nunwanted blocking 340\ndefault-constructible 33\ndelete_nodes_with_no_hazards( ) function\n221, 224\ndependency-ordered-before 162\ndetach( ) function 20, 23\ndetached threads 23\ndispatcher class 386\ndividing\ndata between threads before processing \nbegins 253–254\ndata recursively 254–260\nsequence of tasks between threads\n259–260\nwork between threads, techniques for\n252–260\nwork by task type 258–260\ndocumentation 360\ndo_delete( ) function 224\ndone flag 303\ndone( ) function 88\nDoneCheck function 117\ndone_waiting( ) function 295–296\ndo_something( ) function 45\ndo_sort( ) function 257, 309\ndouble-checked locking pattern 65\ndouble-word-compare-and-swap (DWCAS) 139\nDuration template parameter 410\nduration timeouts\nstd::shared_lock try-to-lock constructors \nwith 526\nstd::unique_lock try-to-lock constructors \nwith 517\nDuration::zero( ) function 411\nduration-based timeout 93\ndurations 94–96\nDWCAS (double-word-compare-and-swap) 139\nE\nedit_document function 24\nemplace( ) function 77\nempty( ) function 44, 49, 177\nempty_stack exception 178\nend_of_data flag 257\nenforcing ordering 142–172\nfences 166–168\nmemory ordering for atomic operations\n146–164\nordering non-atomic operations 169–172\nordering non-atomic operations with \natomics 168–169\nrelease sequences and synchronizes-with\n164–165\nErlang 5, 104\nevent driven frameworks 280\nevents\nwaiting for 73–81\nwaiting for one-off events with futures\n81–93\nassociating tasks with futures 84–87\npromises 87–89\nreturning values from background \ntasks 82–84\nsaving exceptions for future 89–90\nwaiting for multiple threads 90–93\nexception safety\nadding 272–276\nin parallel algorithms 271–277\noverview of 277\nwith std::async( ) 276–277\nexceptions 89–90\nexchange( ) function 131\nexchange-and-add operation 137\nexecution policies 328–331\nchoosing 334–336\ngeneral effects of specifying 328–329\neffects on algorithm complexity\n328–329\nexceptional behavior 329\nwhere and when algorithm steps are \nexecuted 329\noverview of 327\nstd::execution::parallel_policy 330–331\nstd::execution::parallel_unsequenced_policy\n331\nstd::execution::sequenced_policy 330\nexiting applications 325–326\n",
      "content_length": 2676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 578,
      "chapter": null,
      "content": "INDEX\n555\nexternal input 340\nexternal_count 230\nF\nf( ) function 21\nfacilities\nfor protecting shared data 64–71\nplatform-specific 13\nfalse sharing 264–265\nfences 166–168\nfetch_add( ) function 131, 137–138, 234\nfetch_or( ) function 131\nfetch_sub( ) function 137–138, 165\nf.get( ) function 89\nfind_element class 287\nfind_entry( ) function 70\nfind_entry_for( ) function 198\nfind_first_if( ) function 203\nfind_the_answer function 109\nfind_the_question function 109\nfine-grained condition variables, thread-safe \nqueues using 183–194\nenabling concurrency by separating \ndata 185–190\nwaiting for items to pop 190–194\nfine-grained locks\ndesigning map data structures for 196–199\nthread-safe queues using 183–194\nenabling concurrency by separating \ndata 185–190\nwaiting for items to pop 190–194\nFinite State Machine model 104\nflag.clear( ) function 169\nflag.test_and_set( ) function 169\nflexible locking 59–60\nfoo( ) function 43\nfor_each( ) function 200, 203\nforward iterators 33, 333\nFP (functional programming), with futures\n99–104\nparallel Quicksort 102–104\nQuicksort 100–102\nframeworks, event driven 280\nfree_external_counter( ) function 241\nfront( ) function 77\nfunc function 43\nfunction templates 358\nfunctions\naccepting timeouts 98–99\ndefaulted 360–363\ndeleted 358–360\nfree functions for atomic operations\n140–142\nfut.get( ) function 109\nfut.then( ) function 109\n<future> headers 466–489\nstd::async function templates 488\nstd::future class templates 467–472\nstd::packaged_task class templates 477–482\nstd::promise class templates 483–488\nstd::shared_future class templates 472–477\nfutures\nassociating tasks with 84–87\nFP with 99–104\nparallel Quicksort 102–104\nQuicksort 100–102\nwaiting for first in set with when_any 115–118\nwaiting for more than one 114–115\nwaiting for one-off events with 81–93\npromises 87–89\nreturning values from background \ntasks 82–84\nsaving exceptions for future 89–90\nwaiting for multiple threads 90–93\nG\ngeneralized captures 373\ngeneric lamdas 373\nget( ) function 274, 307\nget_bucket( ) function 198\nget_detail( ) function 64\nget_event( ) function 280\nget_future( ) function 85–86, 92\nget_hazard_pointer_for_current_thread( ) \nfunction 219, 222\nget_id( ) function 34\nget_lock( ) function 61\nget_my_class_instance( ) function 68\nget_num( ) function 145\nget_tail( ) function 188\ngo atomic variable 153\nH\nhandle( ) function 107–108, 388, 405\nhappens-before relationships 145–146\nhardware concurrency 3\nhardware threads 4\nHaskell 100\nhazard pointers\nfor detecting nodes 218–226\nreclamation strategies with 225–226\nhazard_pointer_for_current_thread( ) \nfunction 221\nhead pointer 183, 210\nhead.load( ) function 233\nHello World program 14–15\nhello( ) function 14\n",
      "content_length": 2680,
      "extraction_method": "Direct"
    },
    {
      "page_number": 579,
      "chapter": null,
      "content": "INDEX\n556\nhierarchical_mutex type 55–56, 58\nhigh contention 263\nI\nI/O operations 278\nimpure functions 100\nincrease_external_count( ) function 241, 243\nincrease_head_count( ) function 243\nincremental pairwise algorithms 293–299\ninfo_to_display.get( ) function 112\ninit parameter value 32\ninitial function 14\ninitialization\nof std::atomic_flag 437\noverview of 367\nprotecting shared data during 65–68\ninput iterators 33, 332\ninterfaces, race conditions inherent in 44–50\npassing references 47\nreturning pointers to popped items 47–48\nthread-safe stacks 48–50\ninternal_count 230\ninterrupt( ) function 316–317, 319\ninterruptible_wait( ) function 318, 323\ninterrupting\nbackground tasks on application exit\n325–326\nblocking calls 323–324\ncondition variable wait 318–321\nthreads 315–316, 318–326\ndetecting interrupted threads 318\nhandling interruptions 324–325\nlaunching threads 316–318\nwait on std::condition_variable_any 321–323\ninterruption_point( ) function 316, 318, 324\ninter-thread happens-before 159\nints 63, 350\ninvariants 37\nis_lock_free( ) function 128–129\nJ\njoin( ) function 20–22, 341\njoinable( ) function 20, 22, 274\njoining_thread class 29, 121\njoin_threads class 303\nL\nlambda expression 18\nlambda functions 369–374\nlast in, first out (LIFO) 210\nlatches in Concurrency TS 118\nlatency 279–280\nlazy initialization 65\nlhs.some_detail 64\nlibraries 382\nlifetime issues 341\nLIFO (last in, first out) 210\nlist_contains( ) function 41\nlivelock 209, 340\nload( ) function 131–132\nlock( ) function 41, 70, 169\nlock-based concurrent data structures 176–194\ndesigning 173–194, 204\nfor concurrency 174–176\nwriting thread-safe lists using locks\n199–204\nwriting thread-safe lookup tables using \nlocks 194–196\nthread-safe queues using fine-grained locks \nand condition variables 183–194\nenabling concurrency by separating \ndata 185–190\nwaiting for items to pop 190–194\nthread-safe queues using locks and condition \nvariables 179–182\nthread-safe stacks using locks 176–179\nlock-free concurrent data structures 205–250\nadvantages and disadvantages of 208–209\ndefinitions and consequences 206–209\ntypes of nonblocking data structures\n206–207\nwait-free data structures 208\nexamples of 209–248\napplying memory models to lock-free \nstacks 232–236\ndetecting nodes in use with reference \ncounting 226–232\ndetecting nodes using hazard pointers\n218–226\nmanaging memory in lock-free data \nstructures 214–218\nwriting thread-safe queues without \nlocks 236–248\nwriting thread-safe stacks without \nlocks 210–213\nguidelines for writing 248–250\nABA problem 249\nhelp other threads 249–250\nidentifying busy-wait loops 249–250\nlock-free memory reclamation schemes\n248–249\nstd::memory_order_seq_cst for \nprototyping 248\nmanaging memory in 214–218\nlock-free memory reclamation schemes\n248–249\nlock-free queues 244–248\n",
      "content_length": 2786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 580,
      "chapter": null,
      "content": "INDEX\n557\nlock-free stacks 232–236\nlocking\nflexible 59–60\nrecursive 70–71\nlocks\nacquiring in fixed order 54–55\ncalling user-supplied code while holding\n53–54\nfine-grained\ndesigning map data structures for 196–199\nthread-safe queues using 183–194\nhierarchy of 55–59\nnested 53\nthread-safe queues using 179–182\nthread-safe stacks using 176–179\nwriting thread-safe lists using 199–204\nwriting thread-safe lookup tables using\n194–199\nwriting thread-safe queues without 236–248\nhandling multiple threads in push( )\n238–244\nmaking queues lock-free by helping other \nthreads 244–248\nwriting thread-safe stacks without 210–213\nlog(N) operations 291\nlow contention 263\nlvalue 61, 355\nM\nmain( ) function 14, 17\nmake_offseter function 371\nmalicious_function 43\nmap data structures 196–199\nmathematical functions 99\nmatrix multiplication 267\nmax_hazard_pointers atomic variables 224\nmax_hazard_pointers nodes 225\nmemcpy( ) function 139\nmemory\nin lock-free data structures 214–218\nlocations of 125–127\nmemory barriers 166\nmemory locations 125\nmemory models 125–128\napplying to lock-free stacks 232–236\nconcurrency 126–127\nmemory locations 125–127\nmodification orders 127–128\nobjects 125–127\nmemory ordering for atomic operations\n146–164\nacquire-release ordering 155–164\ndata dependency with \nmemory_order_consume 161–164\nnon-sequentially consistent memory \norderings 149–150\nrelaxed ordering 150–155\nsequentially consistent ordering 147–149\nmemory_order_acq_rel 136, 146\nmemory_order_acquire 136, 146, 157\nmemory_order_consume 146, 161–164\nmemory-ordering parameters 136\nmemory_order_relaxed 136, 146, 150, 153, 348\nmemory_order_release 146, 157\nmemory_order_seq_cst 133, 146, 149, 245, 348\nmessage passing\nframeworks for 384–401\nsynchronizing operations with 104–108\nMessage Passing Interface (MPI) 104, 253\nmessage_base class 384\nmodification orders 127–128\nmove assignment operator 26\nmove constructors 26\nmove semantics 355–358\nmoveable types 61\nMoveConstructible 545\nmove-from-std::future constructor 474\nMPI (Message Passing Interface) 104, 253\nmultithreading\nconcurrency and 10–13\nconcurrency support in C++11 11\nefficiency in C++ Thread Library 12–13\nplatform-specific facilities 13\nsupport for concurrency and parallelism in \nC++14 and C++17 12\nhistory of 10–11\nreviewing multithreaded code 343–344\nstructuring test code 350–352\ntesting performance of multithreaded \ncode 352–353\ntesting techniques 347–350\nbrute-force testing 347–348\ncombination simulation testing 348–349\ndetecting problems exposed by tests with \nspecial library 349–350\n<mutex> headers 489–535\nstd::call_once function templates 534–535\nstd::lock function templates 533\nstd::lock_guard class templates 512–513\nstd::mutex classes 490–492\nstd::once_flag classes 534\nstd::recursive_mutex classes 492–494\nstd::recursive_timed_mutex classes\n498–501\nstd::scoped_lock class templates 513–514\nstd::shared_lock class templates 523–532\nstd::shared_mutex classes 502–505\nstd::shared_timed_mutex classes\n505–512\n",
      "content_length": 2964,
      "extraction_method": "Direct"
    },
    {
      "page_number": 581,
      "chapter": null,
      "content": "INDEX\n558\n<mutex> headers (continued)\nstd::timed_mutex classes 494–498\nstd::try_lock function templates 533\nstd::unique_lock class templates 514–523\nmutexes 41–42\nlocking 40\nprotecting shared data with 40–64\ndeadlock 51–59\nflexible locking with std::unique_lock\n59–60\nlocking at appropriate granularity\n62–64\nrace conditions inherent in interfaces\n44–50\nstructuring code for protecting shared \ndata 42–43\ntransferring ownership between scopes\n61–62\nunlocking 40\nmutual exclusion 174\nmy_thread function 18–19\nN\nN elements 253\nN/k operations 290\nnamespace this_thread 549–550\nnative_handle( ) function 13\nnaturally parallel algorithms 8\nnested locks 53\nnew_higher values 102\nnew_lower values 102\nnext pointer 183\nno_copies class 359\nnode pointer 190\nnodes\ndetecting with hazard pointers\n218–226\ndetecting with reference counting\n226–232\nnon-atomic operations, ordering 168–172\nnonblocking data structures 206–207\nnonmember functions\nswap for std::shared_lock 529\nswap for std::threads 547\nswap for std::unique_lock 520\nnonmodifying query operations 134\nnon-sequentially consistent memory \norderings 149–150\nnon-static data members 125\nnonwaiting functions 96\nnotify_all( ) function 76, 81\nnotify_one( ) function 75–76, 81, 349\nnow( ) function 93\nnullptr 212, 238\nO\nobjects\ncallable 478\nconstexpr 367\noverview of 125–127\nobstruction-free 207\nOpenMP 253\noperations\ndividing array elements for 267–269\nnon-atomic\nordering 169–172\nordering with atomics 168–169\non standard atomic integral types 138\non std::atomic 134–137\non std::atomic_flag 132–134\non std::atomic<T*> 137–138\nsynchronizing 142–172\nhappens-before relationships 145–146\nrelease sequences and synchronizes-with\n164–165\nsynchronizes-with relationships 143–144\nwith message passing 104–108\nordering\nacquire-release 155–159\ndata dependency with 161–164\ntransitive synchronization with 159–161\nenforcing 142–172\nfences 166–168\nmemory ordering for atomic operations\n146–164\nrelease sequences and synchronizes-with\n164–165\nnon-atomic operations 168–172\nrelaxed 150–155\nsequentially consistent 147–149\nOutput Iterators 332\noutstanding_ hazard_pointers_for( ) \nfunction 221–222\noversubscription 32, 261, 266\nownership\nof mutexes, transferring 61–62\nof threads, transferring 27–31\nP\npack expansion 375\nparallel algorithms 327–338\nexception safety in 271–277\nadding exception safety 272–276\nexception safety with std::async( )\n276–277\nexecution policies 328–331\ngeneral effects of specifying 328–329\nstd::execution::parallel_policy 330–331\n",
      "content_length": 2489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 582,
      "chapter": null,
      "content": "INDEX\n559\nparallel algorithms (continued)\nstd::execution::parallel_unsequenced_policy\n331\nstd::execution::sequenced_policy 330\nfrom C++ Standard Library 331–338\ncounting visits 336–338\nexamples of using 334–336\nparallelizing standard library algorithms\n327–328\nparallel implementation\nof std::find 284–289\nof std::for_each 282\nof std::partial_sum 290, 293–299\nparallel Quicksort 102–104\nparallel_ quick_sort( ) function 102\nparallel_accumulate( ) function 33, 271, 304, 306\nparallel_find function 287\nparallel_for_each function 289\nparallelism\nconcurrency vs. 6–7\nof data 8–9\nof standard library algorithms 327–328\nof tasks 8–9\nsupport in C++14 12\nsupport in C++17 12\nparallel_quick_sort function 257\nparameter packs 375–377\nparse_log_line function 337\npointers\nhazard pointers\nfor detecting nodes 218–226\nreclamation strategies with 225–226\nreturning to popped items 47–48\npop( ) function 44, 48, 50, 77, 177, 182, 214, 236, \n344–345\npop_head( ) function 188\npopped items\nreturning pointers to 47–48\nwaiting for 190–194\npop_task_from_other_thread_queue( ) \nfunction 315\nPOSIX C interface 382\npredicates\nstd::condition_variable::wait member function \noverloads taking 419\nstd::condition_variable::wait_for member \nfunction overloads taking 421\nstd::condition_variable::wait_until member \nfunction overloads taking 423\nstd::condition_variable_any::wait member \nfunction overloads taking 427\nstd::condition_variable_any::wait_for member \nfunction overloads taking 429\nstd::condition_variable_any::wait_until member \nfunction overloads taking 430\nprevious_end_value 293\nprimed pipelines 260\nprintf function 374\nprivate functions 360\nproblematic race condition 39\nprocess( ) function 63, 280\nprocess_chunk function 293\nprocess_connections( ) function 88\nprocess_data( ) function 61\nprocessing_loop( ) function 262\nprocessors, number of 261–262\nprotected data 43\nprotected functions 360\nprototyping 248\nproximity of data 265–266\nptr value 247\npublic functions 360\npure functions 100, 368\npush( ) function 44, 48, 77, 182, 210, 214, 236, \n238–244, 345\npush_front( ) function 202\nQ\nqueues\navoiding contention on 310–311\nlock-free by helping other threads\n244–248\nQuicksort\nFP-style 100–102\nparallel Quicksort 102–104\nquick_sort function 254\nR\nrace conditions 38–39, 341–342\ninherent in interfaces 44–50\npassing references 47\nrequiring no-throw copy constructors 47\nreturning pointers to popped items 47–48\nthread-safe stacks 48–50\nproblematic 39–40\nRAII (Resource Acquisition Is Initialization)\n11, 21\nRandom Access Iterators 333\n<ratio> headers 535–540\nreader-writer mutex 68\nread-modify-write operations 132\nreceive_data( ) function 67\nreceiver class 386\nreclaim_later( ) function 221, 224\nreclamation strategies with hazard pointers\n225–226\nrecursive locking 70–71\nreference counting 226–232\n",
      "content_length": 2788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 583,
      "chapter": null,
      "content": "INDEX\n560\nreferences\npassing 47\nRvalue references 354–358\nfunction templates and 358\nmove semantics 355–358\nrelaxed ordering 146, 150–155\nrelease operation 232\nrelease sequences 164–165\nremove_if( ) function 203\nResource Acquisition Is Initialization (RAII)\n11, 21\nresponsiveness, improving 280–282\nresults vector 33, 272, 276\nreturn statement 368\nrhs.some_detail 64\nrun( ) function 107\nrun_pending_task( ) function 308–309, 311\nruntime 31–33\nRvalue references 354–358\nfunction templates and 358\nmove semantics 355–358\nrvalues 25, 47, 61, 83\nS\nsame memory locations 126\nscalability 277–279, 353\nscoped_thread class 28\nscopes 61–62\nsend_data( ) function 67\nsender class 385\nseparate memory locations 126\nseparating\nconcerns 7–8, 259\ndata 185–190\nsequentially consistent ordering 147–149\nserialization 174, 179\nset_ condition_variable( ) function 319\nset_clear_mutex 323\nset_exception( ) function 89\nset_exception_at_thread_exit 110\nset_new_tail( ) function 247\nset_value( ) function 89\nset_value_at_thread_exit 110\nshared access 69\nshared data\nalternative facilities for protecting 64–71\nprotecting rarely updated data structures\n68–70\nrecursive locking 70–71\nprotecting during initialization 65–68\nprotecting with mutexes 40–64\ndeadlock 51–59\nflexible locking with std::unique_lock\n59–60\nlocking at appropriate granularity 62–64\nrace conditions inherent in interfaces 44–50\ntransferring mutex ownership between \nscopes 61–62\nusing mutexes in C++ 41–42\nstructuring code for protecting 42–43\nshared futures 81\nshared_timed_mutex 170\nSIMD (Single-Instruction/Multiple-Data) 294\nsimple state machine model 105\nsimulation testing 348–349\nsingle-core system 345\nSingle-Instruction/Multiple-Data (SIMD) 294\nsingle-producer, single-consumer (SPSC) 238\nsize( ) function 44\nsizeof operator 377\nsleep_for( ) function 98\nsleep_until( ) function 98\nslow operations 262\nsoftware transactional memory (STM) 40\nsome-expression times 328\nsort function 280\nspawn_task( ) function 103\nsplice( ) function 101\nsplit reference count 231\nSPSC (single-producer, single-consumer) 238\nspurious failure 135\nspurious wake 76\nsquare_root( ) function 89\nsstd::thread::hardware_concurrency( ) \nfunction 255\nstatic constexpr member variable 129\nstd::accumulate 31\nstd::adopt_lock parameter 52, 59\nstd::async function templates 488\nstd::async( ) function 82, 171, 254, 276–277\nstd::atomic 134–137\nclass templates 439–449\nconversion assignment operators 443\nconversion constructors 443\ndefault constructors 442\ntemplate specializations 450\nstd::atomic_ is_lock_free( ) function 142\nstd::atomic specializations 450\nstd::atomic::compare_exchange_strong member \nfunction 447\nstd::atomic::compare_exchange_weak member \nfunction 448\nstd::atomic::exchange member function 446\nstd::atomic::is_always_lock_free static data \nmember 443\nstd::atomic::is_lock_free member function 443\nstd::atomic::load member function 444\nstd::atomic::operator basetype conversion \noperator 444\n",
      "content_length": 2933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 584,
      "chapter": null,
      "content": "INDEX\n561\nstd::atomic::store member function 445\nstd::atomic<> primary class template 138–140\nstd::atomic_compare_exchange_strong \nnonmember function 447\nstd::atomic_compare_exchange_strong_explicit \nnonmember function 448\nstd::atomic_compare_exchange_weak \nnonmember function 449\nstd::atomic_compare_exchange_weak_explicit \nnonmember function 449\nstd::atomic_exchange nonmember function 446\nstd::atomic_exchange_explicit nonmember \nfunction 446\nstd::atomic_fetch_add nonmember function 455, \n464\nstd::atomic_fetch_add_explicit nonmember \nfunction 455, 464\nstd::atomic_fetch_and nonmember function 457\nstd::atomic_fetch_and_explicit nonmember \nfunction 457\nstd::atomic_fetch_or nonmember function 458\nstd::atomic_fetch_or_explicit nonmember \nfunction 458\nstd::atomic_fetch_sub nonmember function 456, \n465\nstd::atomic_fetch_sub_explicit nonmember \nfunction 456, 465\nstd::atomic_fetch_xor nonmember function 459\nstd::atomic_fetch_xor_explicit nonmember \nfunction 459\nstd::atomic_flag\nclasses 436–439\ndefault constructors 437\ninitialization with atomic_flag_init 437\noperations on 132–134\noverview of 129, 206\nstd::atomic_flag::clear member function 438\nstd::atomic_flag::test_and_set member \nfunction 437\nstd::atomic_flag_clear nonmember \nfunction 439\nstd::atomic_flag_clear_explicit nonmember \nfunction 439\nstd::atomic_flag_test_and_set nonmember \nfunction 438\nstd::atomic_flag_test_and_set_explicit \nnonmember function 438\nstd::atomic_init nonmember function 442\nstd::atomic<integral-type> specializations 466\nstd::atomic<integral-type>::fetch_add member \nfunction 454\nstd::atomic<integral-type>::fetch_and member \nfunction 456\nstd::atomic<integral-type>::fetch_or member \nfunction 457\nstd::atomic<integral-type>::fetch_sub member \nfunction 455\nstd::atomic<integral-type>::fetch_xor member \nfunction 458\nstd::atomic<integral-type>::operator--\npostdecrement operators 460\npredecrement operators 460\nstd::atomic<integral-type>::operator&= \ncompound assignment operator 460\nstd::atomic<integral-type>::operator++\npostincrement operators 459\npreincrement operators 459\nstd::atomic<integral-type>::operator+= compound \nassignment operator 460\nstd::atomic<integral-type>::operator-= compound \nassignment operator 460\nstd::atomic<integral-type>::operator= compound \nassignment operator 461\nstd::atomic<integral-type>::operator|= compound \nassignment operator 461\nstd::atomic_is_lock_free nonmember \nfunction 443\nstd::atomic_load nonmember function 444\nstd::atomic_load_explicit nonmember \nfunction 444\nstd::atomic_signal_fence function 436\nstd::atomic_store nonmember function 445\nstd::atomic_store_explicit nonmember \nfunction 445\nstd::atomic<T*> 137–138\nstd::atomic<t*> partial specialization 461–463\nstd::atomic<t*>::fetch_add member function 463\nstd::atomic<t*>::fetch_sub member function 464\nstd::atomic<t*>::operator--\npostdecrement operators 466\npredecrement operators 466\nstd::atomic<t*>::operator++\npostincrement operators 465\npreincrement operators 465\nstd::atomic<t*>::operator+= compound assign-\nment operator 466\nstd::atomic<t*>::operator-= compound assignment \noperator 466\nstd::atomic_thread_fence function 435\nstd::atomic_xxx typedefs 433\nstd::bad_alloc exception 46, 329\nstd::bind( ) function 26, 309\nstd::call_once function template 534–535\nstd::chrono::duration\nclass templates 401–410\nconverting constructors\nfrom count value 404\nfrom std::chrono::duration value 404\ndefault constructors 403\nequality comparison operators 408\n",
      "content_length": 3440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 585,
      "chapter": null,
      "content": "INDEX\n562\nstd::chrono::duration (continued)\ngreater-than comparison operators 409\ngreater-than-or-equals comparison \noperators 410\ninequality comparison operators 408\nless-than comparison operator 408\nless-than-or-equals comparison operator 409\nvalues 404\nstd::chrono::duration::count member \nfunction 404\nstd::chrono::duration::max static member \nfunction 408\nstd::chrono::duration::min static member \nfunction 407\nstd::chrono::duration::operator--\npost-decrement operators 406\npre-decrement operators 405\nstd::chrono::duration::operator- unary minus \noperator 405\nstd::chrono::duration::operator*= compound \nassignment operator 406\nstd::chrono::duration::operator/= compound \nassignment operators 406\nstd::chrono::duration::operator%= compound \nassignment operator 407\nstd::chrono::duration::operator+ unary plus \noperator 405\nstd::chrono::duration::operator++\npost-increment operators 405\npre-increment operators 388, 405\nstd::chrono::duration::operator+= compound \nassignment operator 406\nstd::chrono::duration::operator-= compound \nassignment operator 406\nstd::chrono::duration::period typedef 403\nstd::chrono::duration::rep typedef 403\nstd::chrono::duration::zero static member \nfunction 407\nstd::chrono::duration_cast nonmember \nfunction 410\nstd::chrono::high_resolution_clock typedef 416\nstd::chrono::steady_clock class 94, 401, 414–416\nstd::chrono::steady_clock::duration typedef 415\nstd::chrono::steady_clock::now static member \nfunction 415\nstd::chrono::steady_clock::period typedef 415\nstd::chrono::steady_clock::rep typedef 415\nstd::chrono::steady_clock::time_point \ntypedef 415\nstd::chrono::system_clock class 94, 413–414\nstd::chrono::system_clock::duration typedef 413\nstd::chrono::system_clock::from_time_t static \nmember function 414\nstd::chrono::system_clock::now static member \nfunction 414\nstd::chrono::system_clock::period typedef 413\nstd::chrono::system_clock::rep typedef 413\nstd::chrono::system_clock::time_point \ntypedef 414\nstd::chrono::system_clock::to_time_t static \nmember function 414\nstd::chrono::time_point\nclass templates 410–412\nconversion constructors 411\ndefault constructors 411\nduration constructors 411\nstd::chrono::time_point::max static member \nfunction 412\nstd::chrono::time_point::min static member \nfunction 412\nstd::chrono::time_point::operator+= compound \nassignment operator 412\nstd::chrono::time_point::operator-= compound \nassignment operator 412\nstd::chrono::time_point::time_since_epoch \nmember function 412\nstd::chrono_literals namespace 95\nstd::condition_variable\nclasses 417–424\ndefault constructors 417\ndestructors 418\noverview of 74, 93, 172, 369\nstd::condition_variable::notify_all member \nfunction 418\nstd::condition_variable::notify_one member \nfunction 418\nstd::condition_variable::wait member \nfunction 419\nstd::condition_variable::wait_for member \nfunction 420–421\nstd::condition_variable::wait_until member \nfunction 422–423\nstd::condition_variable_any\nclasses 424–431\ndefault constructors 425\ndestructors 425\ninterrupting wait on 321–323\noverview of 74, 172\nstd::condition_variable_any::notify_all member \nfunction 426\nstd::condition_variable_any::notify_one member \nfunction 426\nstd::condition_variable_any::wait member \nfunction 426–427\nstd::condition_variable_any::wait_for member \nfunction 428–429\nstd::condition_variable_any::wait_until member \nfunction 430\nstd::defer_lock argument 59\nstd::execution::par 327–328\n",
      "content_length": 3383,
      "extraction_method": "Direct"
    },
    {
      "page_number": 586,
      "chapter": null,
      "content": "INDEX\n563\nstd::execution::parallel_policy 328, 330–331\nstd::execution::parallel_unsequenced_policy 328, \n331\nstd::execution::par_unseq 328, 334\nstd::execution::seq 328\nstd::execution::sequenced_policy 328–330\nstd::experimental::atomic_shared_ptr 142, 172, \n227–228\nstd::experimental::barrier 120–121, 172, 295\nstd::experimental::flex_barrier 120–123, 172\nstd::experimental::future 108, 113, 171\nstd::experimental::latch 118–119, 171\nstd::experimental::shared_future 113, 171\nstd::experimental::when_all 114\nstd::experimental::when_any 116\nstd::find 284–289\nstd::future\nclass templates 467–472\ndefault constructors 468\ndestructors 469\nmove assignment operators 469\nmove constructors 468\noverview of 170\nstd::future::get member function 471\nstd::future::share member function 469\nstd::future::valid member function 470\nstd::future::wait member function 470\nstd::future::wait_for member function 470\nstd::future::wait_until member function 471\nstd::hardware_constructive_interference_size 265\nstd::hardware_destructive_interference_size 265, \n269\nstd::hash 196\nstd::ifstream 27\nstd::is_nothrow_copy_constructible 47\nstd::is_nothrow_move_ constructible 47\nstd::kill_dependency( ) function 163\nstd::launch::async function 84, 103\nstd::launch::deferred function 103\nstd::lock function template 51, 54, 533\nstd::lock_guard class 41, 59, 380\nclass templates 512–513\ndestructors 513\nlock-adapting constructors 512\nlocking constructors 512\nstd::map<> interface 195\nstd::memory_order enumeration 132, 435\nstd::memory_order_acq_rel 435\nstd::memory_order_acquire 233, 435\nstd::memory_order_consume 163, 435\nstd::memory_order_relaxed 435\nstd::memory_order_release 435\nstd::memory_order_seq_cst 232, 248\nstd::move( ) function 26, 309, 360\nstd::mutex 170\nclasses 490–492\ndefault constructors 490\ndestructors 491\nstd::mutex::lock member function 491\nstd::mutex::try_lock member function 491\nstd::mutex::unlock member function 492\nstd::notify_all_at_thread_exit nonmember \nfunction 424\nstd::once_flag\nclasses 534\ndefault constructors 534\noverview of 66\nstd::packaged_task\nclass templates 477–482\nconstruction from callable objects 478\nconstruction from callable objects with \nallocators 478\ndefault constructors 478\ndestructors 480\nmove constructors 479\nmove-assignment operators 479\noverview of 171, 273, 285\nstd::packaged_task::get_future member \nfunction 480\nstd::packaged_task::make_ready_at_thread_exit \nmember function 482\nstd::packaged_task::operator( ) function call \noperator 481\nstd::packaged_task::reset member function 481\nstd::packaged_task::swap member function 480\nstd::packaged_task::valid member function 481\nstd::partial_sum 290–299\nstd::partition( ) function 101\nstd::promise 87–89\nallocator constructors 483\nclass templates 483–488\ndefault constructors 483\ndestructors 485\nmove constructors 484\nmove-assignment operators 484\nstd::promise::get_future member function 485\nstd::promise::set_exception member \nfunction 487\nstd::promise::set_exception_at_thread_exit \nmember function 487\nstd::promise::set_value member function 486\nstd::promise::set_value_at_thread_exit member \nfunction 486\nstd::promise::swap member function 485\nstd::ratio class template 401, 536\nstd::ratio_add template alias 537\nstd::ratio_divide template alias 538\nstd::ratio_equal class template 539\nstd::ratio_greater class template 540\nstd::ratio_greater_equal class template 540\n",
      "content_length": 3352,
      "extraction_method": "Direct"
    },
    {
      "page_number": 587,
      "chapter": null,
      "content": "INDEX\n564\nstd::ratio_less class template 539\nstd::ratio_less_equal class template 540\nstd::ratio_multiply template alias 538\nstd::ratio_not_equal class template 539\nstd::ratio_subtract template alias 537\nstd::recursive_mutex\nclasses 492–494\ndefault constructors 493\ndestructors 493\noverview of 52, 170\nstd::recursive_mutex::lock member function 493\nstd::recursive_mutex::try_lock member \nfunction 493\nstd::recursive_mutex::unlock member \nfunction 494\nstd::recursive_timed_mutex\nclasses 498–501\ndefault constructors 499\ndestructors 499\noverview of 98, 170\nstd::recursive_timed_mutex::lock member \nfunction 499\nstd::recursive_timed_mutex::try_lock member \nfunction 499\nstd::recursive_timed_mutex::try_lock_for member \nfunction 500\nstd::recursive_timed_mutex::try_lock_until mem-\nber function 501\nstd::recursive_timed_mutex::unlock member \nfunction 501\nstd::reduce 31\nstd::scoped_lock\nclass templates 513–514\ndestructors 514\nlock-adopting constructors 514\nlocking constructors 513\noverview of 52, 60, 380\nstd::shared_future\nclass templates 472–477\ncopy constructors 474\ndefault constructors 473\ndestructors 474\nmove constructors 473\nmove-from-std::future constructor 474\noverview of 90, 170\nstd::shared_future::get member function 476\nstd::shared_future::valid member function 475\nstd::shared_future::wait member function 475\nstd::shared_future::wait_for member \nfunction 475\nstd::shared_future::wait_until member \nfunction 476\nstd::shared_lock\nclass templates 523–532\ndefault constructors 525\ndeferred-lock constructors 525\ndestructors 528\nlock-adopting constructors 525\nlocking constructors 525\nmove constructors 527\nmove-assignment operators 528\nswap nonmember functions for 529\ntry-to-lock constructors\nwith duration timeout 526\nwith time_point timeout 527\nstd::shared_lock::lock member function 529\nstd::shared_lock::mutex member function 532\nstd::shared_lock::operator bool member \nfunction 531\nstd::shared_lock::owns_lock member \nfunction 532\nstd::shared_lock::release member function 532\nstd::shared_lock::swap member function 528\nstd::shared_lock::try_lock member function 529\nstd::shared_lock::try_lock_for member \nfunction 530\nstd::shared_lock::try_lock_until member \nfunction 531\nstd::shared_lock::unlock member function 530\nstd::shared_mutex\nclasses 502–505\ndefault constructors 502\ndestructors 503\noverview of 68, 170, 176, 196\nstd::shared_mutex::lock member function 503\nstd::shared_mutex::lock_shared member \nfunction 504\nstd::shared_mutex::try_lock member \nfunction 503\nstd::shared_mutex::try_lock_shared member \nfunction 504\nstd::shared_mutex::unlock member function 504\nstd::shared_mutex::unlock_shared member \nfunction 505\nstd::shared_ptr 48\nstd::shared_timed_mutex\nclasses 505–512\ndefault constructors 506\ndestructors 507\noverview of 68\nstd::shared_timed_mutex::lock member \nfunction 507\nstd::shared_timed_mutex::lock_shared member \nfunction 509\nstd::shared_timed_mutex::try_lock member \nfunction 507\nstd::shared_timed_mutex::try_lock_for member \nfunction 508\nstd::shared_timed_mutex::try_lock_shared \nmember function 510\n",
      "content_length": 3040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 588,
      "chapter": null,
      "content": "INDEX\n565\nstd::shared_timed_mutex::try_lock_shared_for \nmember function 510\nstd::shared_timed_mutex::try_lock_until member \nfunction 508, 511\nstd::shared_timed_mutex::unlock member \nfunction 509\nstd::shared_timed_mutex::unlock_shared mem-\nber function 511\nstd::sort( ) function 101\nstd::stack container adapter 44\nstd::string object 25\nstd::terminate( ) function 22, 27, 272, 379\nstd::this_thread::get_id nonmember function 549\nstd::this_thread::sleep_for nonmember \nfunction 550\nstd::this_thread::sleep_until nonmember \nfunction 550\nstd::this_thread::yield nonmember function 549\nstd::thread\nclasses 541–549\nconstructors 545\ndefault constructors 545\ndestructors 546\nmove constructors 546\nmove-assignment operators 546\nswap nonmember functions for 547\nstd::thread::detach member function 548\nstd::thread::get_id member function 549\nstd::thread::hardware_ concurrency( ) \nfunction 31–32, 257, 261, 282, 301, 549\nstd::thread::id\nclasses 542\ndefault constructors 542\nequality comparison operators 543\ngreater-than comparison operators 544\ngreater-than or equal comparison \noperators 544\ninequality comparison operators 543\nless-than comparison operators 543\nless-than-or-equals comparison operator 544\nstream insertion operators 544\nstd::thread::join member function 548\nstd::thread::joinable member function 547\nstd::thread::native_handle member \nfunctions 545\nstd::thread::native_handle_type typedef 545\nstd::thread::swap member function 547\nstd::timed_mutex\nclasses 494–498\ndefault constructors 495\ndestructors 495\nstd::timed_mutex::lock member function 495\nstd::timed_mutex::try_lock member function 496\nstd::timed_mutex::try_lock_for member \nfunction 496\nstd::timed_mutex::try_lock_until member \nfunction 497\nstd::timed_mutex::unlock member function 497\nstd::try_lock function template 533\nstd::unique_lock\nclass templates 514–523\ndefault constructors 516\ndeferred-lock constructors 517\ndestructors 519\nflexible locking with 59–60\nlock-adopting constructors 516\nlocking constructors 516\nmove constructors 518\nmove-assignment operators 519\nswap nonmember functions for 520\ntry-to-lock constructors 517\nwith duration timeout 517\nwith time_point timeout 518\nstd::unique_lock::mutex member function 523\nstd::unique_lock::operator bool member \nfunction 522\nstd::unique_lock::owns_lock member \nfunction 523\nstd::unique_lock::release member function 523\nstd::unique_lock::swap member function 519\nstd::unique_lock::try_lock member \nfunction 520\nstd::unique_lock::try_lock_until member \nfunction 522\nstd::unique_lock::unlock member function 521\nstd::unique_ptr 26–27\nstd::vector 31\nstd::vector<int> parameter 355\nSTM (software transactional memory) 40\nstore operations 132\nstore( ) function 131, 134, 165\nstrongly-happens-before relationship 145\nstruct, division of 126\nsubmit( ) function 303–305, 307, 310\nsums, partial 293–299\nswap( ) function 48, 52, 175, 215, 234\nswapping nonmember functions\nfor std::shared_lock 529\nfor std::threads 547\nfor std::unique_lock 520\nswitching 265\nsynchronizes-with relationships 143–144, \n164–165\nsynchronizing\nconcurrent operations\nto simplify code 99–123\nwaiting for conditions 73–81\nwaiting for events 73–81\nwaiting for one-off events with futures 81–93\nwaiting with time limits 93–99\n",
      "content_length": 3217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 589,
      "chapter": null,
      "content": "INDEX\n566\nsynchronizing (continued)\noperations 142–172\nhappens-before relationships 145–146\nrelease sequences and synchronizes-with\n164–165\nsynchronizes-with relationships 143–144\nwith message passing 104–108\ntransitive synchronization with acquire-release \nordering 159–161\nT\ntail pointer 183\ntask parallelism 8\ntask switching 3, 265\ntasks\nassociating with futures 84–87\nbackground tasks\ninterrupting on application exit 325–326\nreturning values from 82–84\ndividing sequence of 259–260\ndividing work by types of 258–260\ndividing sequence of tasks between \nthreads 259–260\nto separate concerns 258–259\nexcessive switching 266\nparallelism of 8–9\npassing between threads 85–87\nsubmitted to thread pools 303–307\nwaiting for other tasks 307–309\nTemplateDispatcher class 388–389\ntemplates\nconstexpr functions and 368–369\nfunction templates and Rvalue references 358\nvariadic templates 374–377\ntest_and_set( ) function 129, 133–134, 207\ntesting\ndesigning for 346–347\nlocating bugs by 344–346\nmultithreaded testing techniques 347–350\nbrute-force testing 347–348\ncombination simulation testing 348–349\ndetecting problems exposed by tests with \nspecial library 349–350\nperformance of multithreaded code\n352–353\nstructuring multithreaded test code\n350–352\nthen( ) function 110\nthis_thread_hierarchy_value 58\nthis_thread_interrupt_flag 317\nthread pools 301–315\navoiding contention on work queues\n310–311\nsimple 301–303\ntasks waiting for other tasks 307–309\nwaiting for tasks submitted to 303–307\nwork stealing 311–315\nthread storage duration 379\n<thread> headers 541–549\nnamespace this_thread 549–550\nstd::thread class 541–549\nthread_cond_any pointer 323\nthread_guard class 22, 29, 275\nThreading Building Blocks, Intel 282\nthread_local flag 316\nthread_local variable 110, 222, 310, 316\nthread-local variables 379–380\nthreads 16\nblocked 340\nchoosing number of at runtime 31–33\ndesigning data structures for multithreaded \nperformance 266–270\ndata access patterns in data structures\n269–270\ndividing array elements for complex \noperations 267–269\ndividing data between before processing \nbegins 253–254\ndividing sequence of tasks between\n259–260\nhandling multiple in push( ) 238–244\nhelping 249–250\nidentifying 34–35\ninterrupting 315–316, 318–326\nbackground tasks on application exit\n325–326\nblocking calls 323–324\ncondition variable wait 318–321\ndetecting interrupted threads 318\nhandling interruptions 324–325\nwait on std::condition_variable_any\n321–323\nlaunching 17–20, 316–318\nmaking queues lock-free by helping\n244–248\nmanaging 17–24, 300–326\nmultiple\nconcurrency with 6\nhiding latency with 279–280\nwaiting from 90–93\npassing arguments to thread function 24–27\npassing tasks between 85–87\nrunning in background 22–24\nserialization of 179\nsharing data between 36–71\nalternative facilities for protecting shared \ndata 64–71\nproblems with 37–40\nprotecting shared data with mutexes 40–64\n",
      "content_length": 2876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 590,
      "chapter": null,
      "content": "INDEX\n567\nthreads (continued)\ntechniques for dividing work between\n252–260\ndividing data recursively 254–260\ndividing work by task type 258–260\ntransferring ownership of 27–31\nwaiting to complete 20–22\nthreads vector 272\nthread-safe data structures 174\nthread-safe lists 199–204\nthread-safe lookup tables 194–199\nthread-safe queues\nbuilding with condition variables 76–81\nusing condition variables 179–182\nusing fine-grained condition variables\n183–194\nenabling concurrency by separating \ndata 185–190\nwaiting for items to pop 190–194\nusing fine-grained locks 183–194\nenabling concurrency by separating \ndata 185–190\nwaiting for items to pop 190–194\nusing locks 179–182\nwriting without locks 236–248\nhandling multiple threads in push( )\n238–244\nmaking queues lock-free by helping other \nthreads 244–248\nthread-safe stacks\nexample definitions of 48–50\nusing locks 176–179\nwriting without locks 210–213\nthreads_in_pop variable 215, 218\nthread-specific setup code 350\ntime limits, waiting with 93–99\nclocks 93–94\ndurations 94–96\nfunctions accepting timeouts 98–99\ntime points 96–97\ntime points 96–97\ntimeouts 98–99\ntime_point timeouts\nstd::shared_lock try-to-lock constructors \nwith 527\nstd::unique_lock try-to-lock constructors \nwith 518\ntime_since_ epoch( ) function 96\nto_be_deleted pointer 218\ntop( ) function 44, 50, 344\ntransform_reduce function 336\ntransitive synchronization 159–161\ntrivial copy-assignment operator 138\ntry/catch blocks 21\ntry_lock( ) function 57\ntry_lock_for( ) function 98\ntry_lock_until( ) function 98\ntry_pop( ) function 78, 183, 188, 236, \n313\ntry_reclaim( ) function 215, 217\ntry_steal( ) function 313\ntry-to-lock constructors, std::shared_lock\nwith duration timeouts 517, 526\nwith time_point timeouts 518, 527\nt.time_since_epoch( ) function 411\ntypedefs 94, 130\nU\nunbounded queue 194\nundefined behavior 39, 70\nuniform rate 94\nunique futures 81\nunlock( ) function 41, 70, 169\nunlocking mutexes 40\nunsequenced policy 331\nupdate_data_for_widget 25\nupdate_or_add_entry( ) function 70\nuser-defined types 365–367\nuser-interface state machine 397\nuser-supplied code 53–54\nV\nvalues\nreturning from background tasks\n82–84\nstoring new depending on current\n135–136\nvariables\nautomatically deducing type of\n377–378\nlocal 371–374\nthread-local 379–380\nvariadic templates 52, 374–377\nvectors 46\nvisits, counting 336–338\nvoid( ) function 84\nW\nwait( ) function 75, 81, 108, 295–296, \n387\nwait_and_dispatch( ) function 387, 389\nwait_and_pop( ) function 180, 183\nwait_for( ) function 93, 97, 109\nwait_for_data( ) function 193\nwait_for_pop( ) function 78\nwait-free 207, 213\nwait-free data structures 208\n",
      "content_length": 2612,
      "extraction_method": "Direct"
    },
    {
      "page_number": 591,
      "chapter": null,
      "content": "INDEX\n568\nwaiting\nfor conditions 73–81\nbuilding thread-safe queues with condition \nvariables 76–81\nwith condition variables 74–76\nfor events 73–81\nfor first future in set with when_any\n115–118\nfor items to pop 190–194\nfor more than one future 114–115\nfor multiple threads 90–93\nfor one-off events with futures 81–93\nassociating tasks with futures 84–87\npromises 87–89\nreturning values from background tasks\n82–84\nsaving exceptions for future 89–90\nfor tasks 307–309\nfor tasks submitted to thread pools 303–307\ninterrupting condition variable wait 318–321\ninterrupting wait on \nstd::condition_variable_any 321–323\nwith time limits 93–99\nclocks 93–94\ndurations 94–96\nfunctions accepting timeouts 98–99\ntime points 96–97\nwait_until( ) function 93, 109\nwhen_all function 115\nwhen_any function 115–118\nwhile loop 158, 160, 212–213\nwork stealing 311–315\nworker_thread( ) function 301, 303, 308, 311\nwrapped_message pointer 384\nX\nx.is_lock_free( ) function 128\n",
      "content_length": 954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 592,
      "chapter": null,
      "content": "Anthony Williams\nY\nou choose C++ when your applications need to run fast. \nWell-designed concurrency makes them go even faster. \nC++17 delivers strong support for the multithreaded, \nmultiprocessor programming required for fast graphic process-\ning, machine learning, and other performance-sensitive tasks. \nThis exceptional book unpacks the features, patterns, and best \npractices of production-grade C++ concurrency.\nC++ Concurrency in Action, Second Edition is the deﬁ nitive guide \nto writing elegant multithreaded applications in C++. Updated \nfor C++17, it carefully addresses every aspect of concurrent \ndevelopment, from starting new threads to designing fully \nfunctional multithreaded algorithms and data structures. \nConcurrency master Anthony Williams presents examples and \npractical tasks in every chapter, including insights that will \ndelight even the most experienced developer.  \nWhat’s Inside\n● Full coverage of new C++17 features\n● Starting and managing threads\n● Synchronizing concurrent operations\n● Designing concurrent code\n● Debugging multithreaded applications\nWritten for intermediate C and C++ developers. No prior \nexperience with concurrency required.\nAnthony Williams has been an active member of the BSI C++  \nPanel since 2001 and is the developer of the just::thread Pro \nextensions to the C++11 thread library.\nTo download their free eBook in PDF, ePub, and Kindle formats, \nowners of this book should visit \nmanning.com/books/c-plus-plus-concurrency-in-action-second-edition\n$69.99 / Can $92.99  [INCLUDING eBOOK]\nC++ Concurrency IN ACTION\nPROGRAMMING LANGUAGES\nM A N N I N G\n“\nThis book should be on \nevery C++ programmer’s \ndesk. It’s clear, concise, \nand valuable.”\n \n—Rob Green\nBowling Green State University\n“\nA thorough presentation \nof C++ concurrency \ncapabilities.”\n—Maurizio Tomasi\nUniversity of Milan \n“\nHighly recommended for \nprogrammers who want to \nfurther their knowledge of \nthe latest C++ standard.”\n \n—Frédéric Flayol, 4Pro Web C++\n“\nThe guide contains \nsnippets for everyday use in \nyour own projects and to \nhelp take your concurrency \nC++ skills from the Padawan \nto the Jedi level.”\n \n—Jura Shikin, IVI Technologies\nSee first page\n",
      "content_length": 2189,
      "extraction_method": "Direct"
    }
  ]
}