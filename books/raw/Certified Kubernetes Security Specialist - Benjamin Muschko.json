{
  "metadata": {
    "title": "Certified Kubernetes Security Specialist - Benjamin Muschko",
    "author": "Benjamin Muschko",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 298,
    "conversion_date": "2025-12-19T17:20:42.146455",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Certified Kubernetes Security Specialist - Benjamin Muschko.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Exam Details and Resources",
      "start_page": 11,
      "end_page": 22,
      "detection_method": "regex_chapter_title",
      "content": "Let’s have a very brief look at the details for each certification to see if the CKS is the right fit for you.\n\nKubernetes and Cloud Native Associate (KCNA)\n\nKCNA is an entry-level certification program for anyone interested in cloud-native application development, runtime environments, and tooling. While the exam does cover Kubernetes, it does not expect you to actually solve problems in a practical manner. This exam is suitable for candidates interested in the topic with a broad exposure to the ecosystem.\n\nKubernetes and Cloud Native Security Associate (KCSA)\n\nThe certification focuses on basic knowledge of security concepts and their application in a Kubernetes cluster. The breadth and depth of the program is comparable to the KCNA, as it does not require solving problems hands-on.\n\nCertified Kubernetes Application Developer (CKAD)\n\nThe CKAD exam focuses on verifying your ability to build, configure, and deploy a microservices-based application to Kubernetes. You are not expected to actually implement an application; however, the exam is suitable for developers familiar with topics like application architecture, runtimes, and programming languages.\n\nCertified Kubernetes Administrator (CKA)\n\nThe target audience for the CKA exam are DevOps practitioners, system administrators, and site reliability engineers. This exam tests your ability to perform in the role of a Kubernetes administrator, which includes tasks like cluster, network, storage, and beginner-level security management, with a big emphasis on troubleshooting scenarios.\n\nCertified Kubernetes Security Specialist (CKS)\n\nThe CKS exam expands on the topics verified by the CKA exam. Passing the CKA is a prerequisite before you can even sign up for the CKS exam. For this certification, you are expected to have a deeper knowledge of Kubernetes security aspects. The curriculum covers topics like applying best practices for building containerized applications and ensuring a secure Kubernetes runtime environment.\n\nExam Objectives\n\nVulnerabilities in software and IT infrastructure, if exploited, can pose a major threat to organizations. The Cloud Native Computing Foundation (CNCF) developed the Certified Kubernetes Security Specialist (CKS) certification to verify a Kubernetes administrator’s proficiency to protect a Kubernetes cluster and the cloud native software operated in it. As part of the CKS exam, you are expected to understand Kubernetes core security features, as well as third-party tools and established practices for securing applications and infrastructure.\n\nKUBERNETES VERSION USED DURING THE EXAM\n\nAt the time of writing, the exam is based on Kubernetes 1.26. All content in this book will follow the features, APIs, and command-line support for that specific version. It’s certainly possible that future versions will break backward compatibility. While preparing for the certification, review the Kubernetes release notes and practice with the Kubernetes version used during the exam to avoid unpleasant surprises.\n\nIn this book, I am going to explain each of the security threats by providing a specific use case. We’ll start by talking about a scenario that allows an attacker to gain access to a cluster, inject malicious code, or use a vulnerability to hack into the system. Then, we’ll touch on the concepts, practices, and/or tools that will prevent that situation. With this approach, you’ll be able to evaluate the severity of a security risk and the need for implementing security measures.\n\nCurriculum\n\nThe following overview lists the high-level sections, also called domains, of the CKS exam and their scoring weights:\n\n10%: Cluster Setup\n\n15%: Cluster Hardening\n\n15%: System Hardening\n\n20%: Minimize Microservice Vulnerabilities\n\n20%: Supply Chain Security\n\n20%: Monitoring, Logging, and Runtime Security\n\nHOW THE BOOK WORKS\n\nThe outline of the book follows the CKS curriculum to a T. While there might be a more natural, didactical organization structure to learn Kubernetes in general, the curriculum outline will help test takers prepare for the exam by focusing on specific topics. As a result, you will find yourself cross-referencing other chapters of the book depending on your existing knowledge level.\n\nLet’s break down each domain in detail in the next sections.\n\nCluster Setup\n\nThis section covers Kubernetes concepts that have already been covered by the CKA exam; however, they assume that you already understand the basics and expect you to be able to go deeper. Here, you will be tested on network policies and their effects on disallowing and granting network communication between Pods within the same namespace and across multiple namespaces. The main focus will be on restricting communication to minimize the attack surface. Furthermore, the domain “cluster setup” will verify your knowledge of setting up an Ingress object with Transport Layer Security (TLS) termination.\n\nA big emphasis lies on identifying and fixing security vulnerabilities by inspecting the cluster setup. External tools like kube-bench can help with automating the process. As a result of executing the tool against your cluster, you will receive an actionable list of vulnerabilities. Changing the configuration settings of your cluster according to the recommendations can help with significantly reducing the security risk.\n\nLast, locking down cluster node endpoints, ports, and graphical user interfaces (GUIs) can help with making it harder for attackers to gain control of the cluster. You need to be aware of the default cluster settings so that you can limit access to them as much as possible. Kubernetes binaries and executables like kubectl, kubeadm, and the kubelet need to be checked against their checksum to ensure they haven’t been tampered with by a third\n\nparty. You need to understand how to retrieve the checksum file for binaries and how to use it verify the validity of the executable.\n\nCluster Hardening\n\nMost organizations start out with a cluster that allows developers and administrators alike to manage the Kubernetes installation, configuration, and management of any objects. While this is a convenient approach for teams getting comfortable with Kubernetes, it is not a safe and sound situation, as it poses the potential of opening the floodgates for attackers. Once access has been gained to the cluster, any malicious operation can be performed.\n\nRole-based access control (RBAC) maps permissions to users or processes. The exam requires deep knowledge of the API resources involved. The domain “cluster hardening” also focuses the topic of keeping the cluster version up-to-date to ensure the latest bug fixes are picked up. Kubernetes exposes the API server via endpoints. You should be aware of strategies for minimizing its exposure to the outside world.\n\nSystem Hardening\n\nThis domain is all about understanding how to minimize access to the host system and external network to reduce the attack surface. This is where OS- level tools like AppArmor and seccomp come into play. You will need to demonstrate their use to fulfill the requirement. The domain also touches on the use of AWS IAM roles for clusters running in Amazon’s cloud environment specifically.\n\nMinimize Microservice Vulnerabilities\n\nSecurity contexts define privilege and access control for containers. Platform and security teams can govern and enforce desired security measures on the organizational level. The exam requires you to understand Pod security policies and the OPA Gatekeeper for that purpose. Moreover,\n\nyou’ll be asked to demonstrate defining Secrets of different types and consuming them from Pods to inject sensitive runtime information.\n\nSometimes, you may want to experiment with container images from an unverified source or a potentially unsafe origin. Container runtime sandboxes like gVisor and Kata Containers can be configured in Kubernetes to execute a container image with very restricted permissions. Configuring and using such a container runtime sandbox is part of this domain. Further, you need to be aware of the benefits of mTLS Pod-to-Pod encryption and how to configure it.\n\nSupply Chain Security\n\nContainer security starts with the base image. You need to be aware of the best practices for building container images that minimize the risk of introducing security vulnerabilities from the get-go. Optimally, you will only allow pulling trusted container images from an organization-internal container registry that has already scanned the image for vulnerabilities before it can be used. Allowing only those registries is paramount and will be one of the topics important to this domain. Tools like Trivy can help with the task of scanning images for vulnerabilities and are listed as a requirement to pass the exam.\n\nMonitoring, Logging, and Runtime Security\n\nOne of the focus points of this domain is behavior analytics, the process of observing abnormal and malicious events. Falco is the primary tool to get familiar with in this section. A container should not be mutable after it has been started to avoid opening additional backdoors for attackers. You will need to be aware of best practices and demonstrate the ability to apply them in the configuration of a container.\n\nAudit logging can be helpful for a real-time view on cluster events or for debugging purposes. Configuring audit logging for a Kubernetes cluster is part of the exam.\n\nInvolved Kubernetes Primitives\n\nSome of the exam objectives can be covered by understanding the relevant core Kubernetes primitives. It is to be expected that the exam combines multiple concepts in a single problem. Refer to Figure 1-2 as a rough guide to the applicable Kubernetes resources and their relationships.\n\nFigure 1-2. Kubernetes primitives relevant to the exam\n\nIn addition to Kubernetes core primitives, you will also need to have a grasp of specific Custom Resource Definitions (CRDs) provided by open source projects. For example, the Open Policy Agent (OPA) Gatekeeper provides the primitives’ ConstraintTemplate.\n\nInvolved External Tools\n\nA significant portion of the exam requires you to demonstrate expertise with external, security-related tools. Some of the tools have been spelled out explicitly in the curriculum, but there are other tools that fall into the same functional category. At the very least, you will have to be familiar with the following tools:\n\nkube-bench\n\nAppArmor\n\nseccomp\n\ngVisor\n\nKata Containers\n\nTrivy\n\nFalco\n\nDocumentation\n\nDuring the exam, you are permitted to open a well-defined list of web pages as a reference. You can freely browse those pages and copy-paste code in the exam terminal. The Frequently Asked Questions (FAQ) for the CKS spells out a list of permitted URLs.\n\nThe official Kubernetes documentation includes the reference manual, the GitHub site, and the blog:\n\nReference manual: https://kubernetes.io/docs\n\nGitHub: https://github.com/kubernetes\n\nBlog: https://kubernetes.io/blog\n\nFor external tools, you are allowed to open and browse the following URL:\n\nTrivy: https://github.com/aquasecurity/trivy\n\nFalco: https://falco.org/docs\n\nAppArmor: https://gitlab.com/apparmor/apparmor/-/wikis/Documentation\n\nCandidate Skills\n\nThe CKS certification assumes that you already have an administrator-level understanding of Kubernetes. The CNCF requires you to acquire the CKA certificate as a prerequisite. Without those credentials, you won’t be able to sign up for the CKS exam. If you have not passed the CKA exam yet or if you want to brush up on the topics, I’d recommend having a look at my book Certified Kubernetes Administrator (CKA) Study Guide.\n\nFor the remainder of the book, I will simply assume that you already have the knowledge needed for the CKA. Therefore, I won’t repeat the basics on overlapping topics anymore. For convenience reasons, I will point you to the relevant information in the CKA book as needed. Please revisit the sections on the exam environment and time management in the CKA book. They equally apply to the CKS exam.\n\nPracticing and Practice Exams\n\nHands-on practice is extremely important when it comes to passing the exam. For that purpose, you’ll need a functioning Kubernetes cluster environment. The following options stand out:\n\nI found it useful to run one or many virtual machines using Vagrant and VirtualBox. Those tools help with creating an isolated Kubernetes environment that is easy to bootstrap and dispose on demand. Some of the practice exercises in this book use this setup as their starting point.\n\nIt is relatively easy to install a simple Kubernetes cluster on your developer machine. The Kubernetes documentation provides various installation options, depending on your operating system. Minikube is useful when it comes to experimenting with more advanced features like Ingress or storage classes, as it provides the necessary functionality as add-ons that can be installed with a single command.\n\nIf you’re a subscriber to the O’Reilly learning platform, you have unlimited access to labs running a Kubernetes environment. In addition, you can test your knowledge with the help of the CKS practice test in the form of interactive labs.\n\nYou may also want to try one of the following commercial learning and practice resources:\n\nThe book Certified Kubernetes Administrator (CKA) Study Guide covers the curriculum of the CKA certification. Revisit the book’s materials for a refresher on the foundations.\n\nKiller Shell is a simulator with sample exercises for all Kubernetes certifications.\n\nThe CKS practice exam from Study4exam offers a commercial, web-based test environment to assess your knowledge level.\n\nSummary\n\nThe CKS exam verifies your hands-on knowledge of security-related aspects in Kubernetes. You are expected to understand core Kubernetes primitives and concepts that can fulfill security requirements, such as RBAC, network policies, and Ingress. The exam also involves helpful third- party security tools. You need to demonstrate how to effectively use those tools. Passing the CKA exam is mandatory for the CKS. Make sure you pass the CKA first if you haven’t done so yet.\n\nThe following chapters align with the exam curriculum so that you can map the content to the learning objectives. At the end of each chapter, you will find sample exercises to practice your knowledge. The discussion of each domain concludes with a short summary of the most important aspects to learn.\n\nOceanofPDF.com\n\nChapter 2. Cluster Setup\n\nThe first domain of the exam deals with concerns related to Kubernetes cluster setup and configuration. In this chapter, we’ll only drill into the security-specific aspects and not the standard responsibilities of a Kubernetes administrator.\n\nAt a high level, this chapter covers the following concepts:\n\nUsing network policies to restrict Pod-to-Pod communication\n\nRunning CIS benchmark tooling to identify security risks for cluster components\n\nSetting up an Ingress object with TLS support\n\nProtecting node ports, API endpoints, and GUI access\n\nVerifying platform binaries against their checksums\n\nUsing Network Policies to Restrict Pod-to- Pod Communication\n\nFor a microservice architecture to function in Kubernetes, a Pod needs to be able to reach another Pod running on the same or on a different node without Network Address Translation (NAT). Kubernetes assigns a unique IP address to every Pod upon creation from the Pod CIDR range of its node. The IP address is ephemeral and therefore cannot be considered stable over time. Every restart of a Pod leases a new IP address. It’s recommended to use Pod-to-Service communication over Pod-to-Pod communication so that you can rely on a consistent network interface.\n\nThe IP address assigned to a Pod is unique across all nodes and namespaces. This is achieved by assigning a dedicated subnet to each node when registering it. When creating a new Pod on a node, the IP address is",
      "page_number": 11
    },
    {
      "number": 2,
      "title": "Cluster Setup",
      "start_page": 23,
      "end_page": 63,
      "detection_method": "regex_chapter_title",
      "content": "leased from the assigned subnet. This is handled by the Container Network Interface (CNI) plugin. As a result, Pods on a node can communicate with all other Pods running on any other node of the cluster.\n\nNetwork policies act similarly to firewall rules, but for Pod-to-Pod communication. Rules can include the direction of network traffic (ingress and/or egress) for one or many Pods within a namespace or across different namespaces, as well as their targeted ports. For a deep-dive coverage on the basics of network policies, refer to the book Certified Kubernetes Application Developer (CKAD) Study Guide (O’Reilly) or the Kubernetes documentation. The CKS exam primarily focuses on restricting cluster- level access with network policies.\n\nDefining the rules of network policies correctly can be challenging. The page networkpolicy.io provides a visual editor for network policies that renders a graphical representation in the browser.\n\nScenario: Attacker Gains Access to a Pod\n\nSay you are working for a company that operates a Kubernetes cluster with three worker nodes. Worker node 1 currently runs two Pods as part of a microservices architecture. Given Kubernetes default behavior for Pod-to- Pod network communication, Pod 1 can talk to Pod 2 unrestrictedly and vice versa.\n\nAs you can see in Figure 2-1, an attacker gained access to Pod 1. Without defining network policies, the attacker can simply talk to Pod 2 and cause additional damage. This vulnerability isn’t restricted to a single namespace. Pods 3 and 4 can be reached and compromised as well.\n\nFigure 2-1. An attacker who gained access to Pod 1 has network access to other Pods\n\nObserving the Default Behavior\n\nWe’ll set up three Pods to demonstrate the unrestricted Pod-to-Pod network communication in practice. As you can see in Example 2-1, the YAML manifest defines the Pods named backend and frontend in the namespace g04. The other Pod lives in the default namespace. Observe the label assignment for the namespace and Pods. We will reference them a little bit later in this chapter when defining network policies.\n\nExample 2-1. YAML manifest for three Pods in different namespaces apiVersion: v1 kind: Namespace metadata: labels: app: orion name: g04 --- apiVersion: v1 kind: Pod metadata: labels:\n\ntier: backend name: backend namespace: g04 spec: containers: - image: bmuschko/nodejs-hello-world:1.0.0 name: hello ports: - containerPort: 3000 restartPolicy: Never --- apiVersion: v1 kind: Pod metadata: labels: tier: frontend name: frontend namespace: g04 spec: containers: - image: alpine name: frontend args: - /bin/sh - -c - while true; do sleep 5; done; restartPolicy: Never --- apiVersion: v1 kind: Pod metadata: labels: tier: outside name: other spec: containers: - image: alpine name: other args: - /bin/sh - -c - while true; do sleep 5; done; restartPolicy: Never\n\nStart by creating the objects from the existing YAML manifest using the declarative kubectl apply command:\n\n$ kubectl apply -f setup.yaml namespace/g04 created pod/backend created pod/frontend created pod/other created\n\nLet’s verify that the namespace g04 runs the correct Pods. Use the -o wide CLI option to determine the virtual IP addresses assigned to the Pods. The backend Pod uses the IP address 10.0.0.43, and the frontend Pod uses the IP address 10.0.0.193:\n\n$ kubectl get pods -n g04 -o wide NAME READY STATUS RESTARTS AGE IP NODE \\ NOMINATED NODE READINESS GATES backend 1/1 Running 0 15s 10.0.0.43 minikube \\ <none> <none> frontend 1/1 Running 0 15s 10.0.0.193 minikube \\ <none> <none>\n\nThe default namespace handles a single Pod:\n\n$ kubectl get pods NAME READY STATUS RESTARTS AGE other 1/1 Running 0 4h45m\n\nThe frontend Pod can talk to the backend Pod as no communication restrictions have been put in place:\n\n$ kubectl exec frontend -it -n g04 -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000) remote file exists / # exit\n\nThe other Pod residing in the default namespace can communicate with the backend Pod without problems:\n\n$ kubectl exec other -it -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000)\n\nremote file exists / # exit\n\nIn the next section, we’ll talk about restricting Pod-to-Pod network communication to a maximum level with the help of deny-all network policy rules. We’ll then open up ingress and/or egress communication only for the kind of network communication required for the microservices architecture to function properly.\n\nDenying Directional Network Traffic\n\nThe best way to restrict Pod-to-Pod network traffic is with the principle of least privilege. Least privilege means that Pods should communicate with the lowest privilege for network communication. You’d usually start by disallowing traffic in any direction and then opening up the traffic needed by the application architecture.\n\nThe Kubernetes documentation provides a couple of helpful YAML manifest examples. Example 2-2 shows a network policy that denies ingress traffic to all Pods in the namespace g04.\n\nExample 2-2. A default deny-all ingress network policy apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress namespace: g04 spec: podSelector: {} policyTypes: - Ingress\n\nSelecting all Pods is denoted by the value {} assigned to the spec.podSelector attribute. The value attribute spec.policyTypes defines the denied direction of traffic. For incoming traffic, you can add Ingress to the array. Outgoing traffic can be specified by the value Egress. In this particular example, we disallow all ingress traffic. Egress traffic is still permitted.\n\nThe contents of the “deny-all” network policy have been saved in the file deny-all-ingress-network-policy.yaml. The following command creates the object from the file:\n\n$ kubectl apply -f deny-all-ingress-network-policy.yaml networkpolicy.networking.k8s.io/default-deny-ingress created\n\nLet’s see how this changed the runtime behavior for Pod-to-Pod network communication. The frontend Pod cannot talk to the backend Pod anymore, as observed by running the same wget command we used earlier. The network call times out after one second, as defined by the CLI option - -timeout:\n\n$ kubectl exec frontend -it -n g04 -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000) wget: download timed out / # exit\n\nFurthermore, Pods running in a different namespace cannot connect to the backend Pod anymore either. The following wget command makes a call from the other Pod running in the default namespace to the IP address of the backend Pod:\n\n$ kubectl exec other -it -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000) wget: download timed out\n\nThis call times out as well.\n\nAllowing Fine-Grained Incoming Traffic\n\nNetwork policies are additive. To grant more permissions for network communication, simply create another network policy with more fine- grained rules. Say we wanted to allow ingress traffic to the backend Pod only from the frontend Pod that lives in the same namespace. Ingress\n\ntraffic from all other Pods should be denied independently of the namespace they are running in.\n\nNetwork policies heavily work with label selection to define rules. Identify the labels of the g04 namespace and the Pod objects running in the same namespace so we can use them in the network policy:\n\n$ kubectl get ns g04 --show-labels NAME STATUS AGE LABELS g04 Active 12m app=orion,kubernetes.io/metadata.name=g04 $ kubectl get pods -n g04 --show-labels NAME READY STATUS RESTARTS AGE LABELS backend 1/1 Running 0 9m46s tier=backend frontend 1/1 Running 0 9m46s tier=frontend\n\nThe label assignment for the namespace g04 includes the key-value pair app=orion. The Pod backend label set includes the key-value pair tier=backend, and the frontend Pod the key-value pair tier=frontend.\n\nCreate a new network policy that allows the frontend Pod to talk to the backend Pod only on port 3000. No other communication should be allowed. The YAML manifest representation in Example 2-3 shows the full network policy definition.\n\nExample 2-3. Network policy that allows ingress traffic apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: backend-ingress namespace: g04 spec: podSelector: matchLabels: tier: backend policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: app: orion podSelector:\n\nmatchLabels: tier: frontend ports: - protocol: TCP port: 3000\n\nThe definition of the network policy has been stored in the file backend- ingress-network-policy.yaml. Create the object from the file:\n\n$ kubectl apply -f backend-ingress-network-policy.yaml networkpolicy.networking.k8s.io/backend-ingress created\n\nThe frontend Pod can now talk to the backend Pod:\n\n$ kubectl exec frontend -it -n g04 -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000) remote file exists / # exit\n\nPods running outside of the g04 namespace still can’t connect to the backend Pod. The wget command times out:\n\n$ kubectl exec other -it -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000) wget: download timed out\n\nApplying Kubernetes Component Security Best Practices\n\nManaging an on-premises Kubernetes cluster gives you full control over the configuration options applied to cluster components, such as the API server, etcd, the kubelet, and others. It’s not uncommon to simply go with the default configuration settings used by kubeadm when creating the cluster nodes. Some of those default settings may expose cluster components to unnecessary attack opportunities.\n\nHardening the security measures of a cluster is a crucial activity for any Kubernetes administrator seeking to minimize attack vectors. You can either perform this activity manually if you are aware of the best practices, or use an automated process.\n\nThe Center for Internet Security (CIS) is a not-for-profit organization that publishes cybersecurity best practices. Part of their best practices portfolio is the Kubernetes CIS Benchmark, a catalog of best practices for Kubernetes environments. You will find a detailed list of recommended security settings for cluster components on their web page.\n\nCIS BENCHMARKING FOR CLOUD PROVIDER KUBERNETES ENVIRONMENTS\n\nThe Kubernetes CIS Benchmark is geared toward a self-managed installation of Kubernetes. Cloud provider Kubernetes environments, such as Amazon Elastic Kubernetes Service (EKS) and Google Kubernetes Engine (GKE), provide a managed control plane accompanied by their own command line tools. Therefore, the security recommendations made by the Kubernetes CIS Benchmark may be less fitting. Some tools, like kube-bench, discussed next, provide verification checks specifically for cloud providers.\n\nUsing kube-bench\n\nYou can use the tool kube-bench to check Kubernetes cluster components against the CIS Benchmark best practices in an automated fashion. Kube- bench can be executed in a variety of ways. For example, you can install it as a platform-specific binary in the form of an RPM or Debian file. The most convenient and direct way to run the verification process is by running kube-bench in a Pod directly on the Kubernetes cluster. For that purpose, create a Job object with the help of a YAML manifest checked into the GitHub repository of the tool.\n\nStart by creating the Job from the file job-master.yaml, or job- node.yaml depending on whether you want to inspect a control plane node\n\nor a worker node. The following command runs the verification checks against the control plane node:\n\n$ kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/\\ main/job-master.yaml job.batch/kube-bench-master created\n\nUpon Job execution, the corresponding Pod running the verification process can be identified by its name in the default namespace. The Pod’s name starts with the prefix kube-bench, then appended with the type of the node plus a hash at the end. The following output uses the Pod named kube- bench-master-8f6qh:\n\n$ kubectl get pods NAME READY STATUS RESTARTS AGE kube-bench-master-8f6qh 0/1 Completed 0 45s\n\nWait until the Pod transitions into the “Completed” status to ensure that all verification checks have finished. You can have a look at the benchmark result by dumping the logs of the Pod:\n\n$ kubectl logs kube-bench-master-8f6qh\n\nSometimes, it may be more convenient to write the verification results to a file. You can redirect the output of the kubectl logs command to a file, e.g., with the command kubectl logs kube-bench-master-8f6qh > control-plane-kube-bench-results.txt.\n\nThe kube-bench Verification Result\n\nThe produced verification result can be lengthy and detailed, but it consists of these key elements: the type of the inspected node, the inspected components, a list of passed checks, a list of failed checks, a list of warnings, and a high-level summary:\n\n[INFO] 1 Control Plane Security Configuration [INFO] 1.1 Control Plane Node Configuration Files [PASS] 1.1.1 Ensure that the API server pod specification file permissions are \\ set to 644 or more restrictive (Automated) ... [INFO] 1.2 API Server [WARN] 1.2.1 Ensure that the --anonymous-auth argument is set to false \\ (Manual) ... [FAIL] 1.2.6 Ensure that the --kubelet-certificate-authority argument is set \\ as appropriate (Automated)\n\n== Remediations master == ... 1.2.1 Edit the API server pod specification file /etc/kubernetes/manifests/ \\ kube-apiserver.yaml on the control plane node and set the below parameter. --anonymous-auth=false ... 1.2.6 Follow the Kubernetes documentation and setup the TLS connection between\n\nthe apiserver and kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the control plane node and \\\n\nset the --kubelet-certificate-authority parameter to the path to the cert \\ file for the certificate authority. --kubelet-certificate-authority=<ca-string>\n\n... == Summary total == 42 checks PASS 9 checks FAIL 11 checks WARN 0 checks INFO\n\nThe inspected node, in this case the control plane node.\n\nA passed check. Here, the file permissions of the API server configuration file.\n\nA warning message that prompts you to manually check the value of an argument provided to the API server executable.\n\nA failed check. For example, the flag --kubelet-certificate- authority should be set for the API server executable.\n\nThe remediation action to take to fix a problem. The number, e.g., 1.2.1, of the failure or warning corresponds to the number assigned to the remediation action.\n\nThe summary of all passed and failed checks plus warning and informational messages.\n\nFixing Detected Security Issues\n\nThe list of reported warnings and failures can be a bit overwhelming at first. Keep in mind that you do not have to fix them all at once. Some checks are merely guidelines or prompts to verify an assigned value for a configuration. The following steps walk you through the process of eliminating a warning message.\n\nThe configuration files of the control plane components can be found in the directory /etc/kubernetes/manifests on the host system of the control plane node. Say you wanted to fix the warning 1.2.12 reported by kube- bench:\n\n[INFO] 1.2 API Server ... [WARN] 1.2.12 Ensure that the admission control plugin AlwaysPullImages is \\ set (Manual)\n\n== Remediations master == ... 1.2.12 Edit the API server pod specification file /etc/kubernetes/manifests/ \\ kube-apiserver.yaml on the control plane node and set the --enable-admission-plugins parameter \\ to include AlwaysPullImages. --enable-admission-plugins=...,AlwaysPullImages,...\n\nAs proposed by the remediation action, you are supposed to edit the configuration file for the API server and add the value AlwaysPullImages\n\nto the list of admission plugins. Go ahead and edit the file kube- apiserver.yaml:\n\n$ sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml\n\nAfter appending the value AlwaysPullImages to the argument --enable- admission-plugins, the result could look as follows:\n\napiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: \\ 192.168.56.10:6443 creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --advertise-address=192.168.56.10 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/etc/kubernetes/pki/ca.crt - --enable-admission-plugins=NodeRestriction,AlwaysPullImages ...\n\nSave the changes to the file. The Pod running the API server in the kube- system namespace will be restarted automatically. The startup process can take a couple of seconds. Therefore, executing the following command may take a while to succeed:\n\n$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE ... kube-apiserver-control-plane 1/1 Running 0 71m ...\n\nYou will need to delete the existing Job object before you can verify the changed result:\n\n$ kubectl delete job kube-bench-master job.batch \"kube-bench-master\" deleted\n\nThe verification check 1.2.12 now reports a passed result:\n\n$ kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/\\ main/job-master.yaml job.batch/kube-bench-master created $ kubectl get pods NAME READY STATUS RESTARTS AGE kube-bench-master-5gjdn 0/1 Completed 0 10s $ kubectl logs kube-bench-master-5gjdn | grep 1.2.12 [PASS] 1.2.12 Ensure that the admission control plugin AlwaysPullImages is \\ set (Manual)\n\nCreating an Ingress with TLS Termination\n\nAn Ingress routes HTTP and/or HTTPS traffic from outside of the cluster to one or many Services based on a matching URL context path. You can see its functionality in action in Figure 2-2.\n\nFigure 2-2. Managing external access to the Services via HTTP(S)\n\nThe Ingress has been configured to accept HTTP and HTTPS traffic from outside of the cluster. If the caller provides the context path /app, then the traffic is routed to Service 1. If the caller provides the context path /api,\n\nthen the traffic is routed to Service 2. It’s important to point out that the communication typically uses unencrypted HTTP network communication as soon as it passes the Ingress.\n\nGiven that the Ingress API resource is a part of the CKAD and CKA exam, we are not going to discuss the basics anymore here. For a detailed discussion, refer to the information in the Certified Kubernetes Administrator (CKA) Study Guide or the Kubernetes documentation.\n\nTHE ROLE OF AN INGRESS CONTROLLER\n\nRemember that an Ingress cannot work without an Ingress controller. The Ingress controller evaluates the collection of rules defined by an Ingress that determine traffic routing. One example of a production-grade Ingress controller is the F5 NGINX Ingress Controller or AKS Application Gateway Ingress Controller. You can find other options listed in the Kubernetes documentation. If you are using minikube, make sure to enable the Ingress add-on.\n\nThe primary focus of the CKS lies on setting up Ingress objects with TLS termination. Configuring the Ingress for HTTPS communication relieves you from having to deal with securing the network communication on the Service level. In this section of the book, you will learn how to create a TLS certificate and key, how to feed the certificate and key to a TLS-typed Secret object, and how to configure an Ingress object so that it supports HTTPS communication.\n\nSetting Up the Ingress Backend\n\nIn the context of an Ingress, a backend is the combination of Service name and port. Before creating the Ingress, we’ll take care of the Service, a Deployment, and the Pods running nginx so we can later on demonstrate the routing of HTTPS traffic to an actual application. All of those objects are supposed to exist in the namespace t75. Example 2-4 defines all of those resources in a single YAML manifest file setup.yaml as a means to quickly create the Ingress backend.\n\nExample 2-4. YAML manifest for exposing nginx through a Service apiVersion: v1 kind: Namespace metadata: name: t75 --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: t75 labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: accounting-service namespace: t75 spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80\n\nCreate the objects from the YAML file with the following command:\n\n$ kubectl apply -f setup.yaml namespace/t75 created\n\ndeployment.apps/nginx-deployment created service/accounting-service created\n\nLet’s quickly verify that the objects have been created properly, and the Pods have transitioned into the “Running” status. Upon executing the get all command, you should see a Deployment named nginx-deployment that controls three replicas, and a Service named accounting-service of type ClusterIP:\n\n$ kubectl get all -n t75 NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6595874d85-5rdrh 1/1 Running 0 108s pod/nginx-deployment-6595874d85-jmhvh 1/1 Running 0 108s pod/nginx-deployment-6595874d85-vtwxp 1/1 Running 0 108s\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) \\ AGE service/accounting-service ClusterIP 10.97.101.228 <none> 80/TCP \\ 108s\n\nNAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 108s\n\nCalling the Service endpoint from another Pod running on the same node should result in a successful response from the nginx Pod. Here, we are using the wget command to verify the behavior:\n\n$ kubectl run tmp --image=busybox --restart=Never -it --rm \\ -- wget 10.97.101.228:80 Connecting to 10.97.101.228:80 (10.97.101.228:80) saving to 'index.html' index.html 100% |**| 612 0:00:00 ETA 'index.html' saved pod \"tmp\" deleted\n\nWith those objects in place and functioning as expected, we can now concentrate on creating an Ingress with TLS termination.\n\nCreating the TLS Certificate and Key\n\nWe will need to generate a TLS certificate and key before we can create a TLS Secret. To do this, we will use the OpenSSL command. The resulting files are named accounting.crt and accounting.key:\n\n$ openssl req -nodes -new -x509 -keyout accounting.key -out accounting.crt \\ -subj \"/CN=accounting.tls\" Generating a 2048 bit RSA private key ...........................+ ..........................+ writing new private key to 'accounting.key' ----- $ ls accounting.crt accounting.key\n\nFor use in production environments, you’d generate a key file and use it to obtain a TLS certificate from a certificate authority (CA). For more information on creating a TLS certification and key, see the OpenSSL documentation.\n\nCreating the TLS-Typed Secret\n\nThe easiest way to create a Secret is with the help of an imperative command. This method of creation doesn’t require you to manually base64- encode the certificate and key values. The encoding happens automatically upon object creation. The following command uses the Secret option tls and assigns the certificate and key file name with the options --cert and - -key:\n\n$ kubectl create secret tls accounting-secret --cert=accounting.crt \\ --key=accounting.key -n t75 secret/accounting-secret created\n\nExample 2-5 shows the YAML representation of a TLS Secret if you want to create the object declaratively.\n\nExample 2-5. A Secret using the type kubernetes.io/tls\n\napiVersion: v1 kind: Secret metadata: name: accounting-secret namespace: t75 type: kubernetes.io/tls data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk... tls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk...\n\nMake sure to assign the values for the attributes tls.crt and tls.key as single-line, base64-encoded values. To produce the base64-encoded value, simply point the base64 command to the file name you want to convert the contents for. The following example base64-encoded the contents of the file accounting.crt:\n\n$ base64 accounting.crt LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNyakNDQ...\n\nCreating the Ingress\n\nYou can use the imperative method to create the Ingress with the help of a one-liner command shown in the following snippet. Crafting the value of the --rule argument is hard to get right. You will likely have to refer to the --help option for the create ingress command as it requires a specific expression. The information relevant to creating the connection between Ingress object and the TLS Secret is the appended argument tls=accounting-secret:\n\n$ kubectl create ingress accounting-ingress \\ --rule=\"accounting.internal.acme.com/*=accounting-service:80, \\ tls=accounting-secret\" -n t75 ingress.networking.k8s.io/accounting-ingress created\n\nExample 2-6 shows a YAML representation of an Ingress. The attribute for defining the TLS information is spec.tls[].\n\nExample 2-6. A YAML manifest for defining a TLS-terminated Ingress\n\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: accounting-ingress namespace: t75 spec: tls: - hosts: - accounting.internal.acme.com secretName: accounting-secret rules: - host: accounting.internal.acme.com http: paths: - path: / pathType: Prefix backend: service: name: accounting-service port: number: 80\n\nAfter creating the Ingress object with the imperative or declarative approach, you should be able to find it in the namespace t75. As you can see in the following output, the port 443 is listed in the “PORT” column, indicating that TLS termination has been enabled:\n\n$ kubectl get ingress -n t75 NAME CLASS HOSTS ADDRESS \\ PORTS AGE accounting-ingress nginx accounting.internal.acme.com 192.168.64.91 \\ 80, 443 55s\n\nDescribing the Ingress object shows that the backend could be mapped to the path / and will route traffic to the Pod via the Service named accounting-service:\n\n$ kubectl describe ingress accounting-ingress -n t75 Name: accounting-ingress Labels: <none> Namespace: t75 Address: 192.168.64.91 Ingress Class: nginx\n\nDefault backend: <default> TLS: accounting-secret terminates accounting.internal.acme.com Rules: Host Path Backends ---- ---- -------- accounting.internal.acme.com / accounting-service:80 \\ (172.17.0.5:80,172.17.0.6:80,172.17.0.7:80) Annotations: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 1s (x2 over 31s) nginx-ingress-controller Scheduled for sync\n\nCalling the Ingress\n\nTo test the behavior on a local Kubernetes cluster on your machine, you need to first find out the IP address of a node. The following command reveals the IP address in a minikube environment:\n\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP \\ EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane 3d19h v1.24.1 192.168.64.91 \\ <none> Buildroot 2021.02.12 5.10.57 docker://20.10.16\n\nNext, you’ll need to add the IP address to the hostname mapping to your /etc/hosts file:\n\n$ sudo vim /etc/hosts ... 192.168.64.91 accounting.internal.acme.com\n\nYou can now send HTTPS requests to the Ingress using the assigned domain name and receive an HTTP response code 200 in return:\n\n$ wget -O- https://accounting.internal.acme.com --no-check-certificate --2022-07-28 15:32:43-- https://accounting.internal.acme.com/ Resolving accounting.internal.acme.com (accounting.internal.acme.com)... \\ 192.168.64.91\n\nConnecting to accounting.internal.acme.com (accounting.internal.acme.com) \\ |192.168.64.91|:443... connected. WARNING: cannot verify accounting.internal.acme.com's certificate, issued \\ by ‘CN=Kubernetes Ingress Controller Fake Certificate,O=Acme Co’: Self-signed certificate encountered. WARNING: no certificate subject alternative name matches\n\nrequested host name ‘accounting.internal.acme.com’.\n\nHTTP request sent, awaiting response... 200 OK\n\nProtecting Node Metadata and Endpoints\n\nKubernetes clusters expose ports used to communicate with cluster components. For example, the API server uses the port 6443 by default to enable clients like kubectl to talk to it when executing commands.\n\nThe Kubernetes documentation lists those ports in “Ports and Protocols”. The following two tables show the default port assignments per node.\n\nTable 2-1 shows the default inbound ports on the cluster node.\n\nTable 2-1. Inbound control plane node ports\n\nPort range\n\nPurpose\n\n6643\n\nKubernetes API server\n\n2379–2380\n\netcd server client API\n\n10250\n\nKubelet API\n\n10259\n\nkube-scheduler\n\n10257\n\nkube-controller-manager\n\nMany of those ports are configurable. For example, you can modify the API server port by providing a different value with the flag --secure-port in\n\nthe configuration file /etc/kubernetes/manifests/kube- apiserver.yaml, as documented for the cluster component. For all other cluster components, please refer to their corresponding documentation.\n\nTable 2-2 lists the default inbound ports on a worker node.\n\nTable 2-2. Inbound worker node ports\n\nPort range\n\nPurpose\n\n10250\n\nKubelet API\n\n30000–32767\n\nNodePort Services\n\nTo secure the ports used by cluster components, set up firewall rules to minimize the attack surface area. For example, you could decide not to expose the API server to anyone outside of the intranet. Clients using kubectl would only be able to run commands against the Kubernetes cluster if logged into the VPN, making the cluster less vulnerable to attacks.\n\nCloud provider Kubernetes clusters (e.g., on AWS, Azure, or Google Cloud) expose so-called metadata services. Metadata services are APIs that can provide sensitive data like an authentication token for consumption from VMs or Pods without any additional authorization. For the CKS exam, you need to be aware of those node endpoints and cloud provider metadata services. Furthermore, you should have a high-level understanding of how to protect them from unauthorized access.\n\nScenario: A Compromised Pod Can Access the Metadata Server\n\nFigure 2-3 shows an attacker who gained access to a Pod running on a node within a cloud provider Kubernetes cluster.\n\nFigure 2-3. An attacker who gained access to the Pod has access to metadata server\n\nAccess to the metadata server has not been restricted in any form. The attacker can retrieve sensitive information, which could open other possibilities of intrusion.\n\nProtecting Metadata Server Access with Network Policies\n\nLet’s pick one of the cloud providers that exposes a metadata endpoint. In AWS, the metadata server can be reached with the IP address 169.254.169.254, as described in the AWS documentation. The endpoints exposed can provide access to EC2 instance metadata. For example, you can retrieve the local IP address of an instance to manage a connection to an external application or to contact the instance with the help of a script. See the corresponding documentation page for calls to those endpoints made with the curl command line tool.\n\nTo prevent any Pod in a namespace from reaching the IP address of the metadata server, set up a network policy that allows egress traffic to all IP addresses except 169.254.169.254. Example 2-7 demonstrates a YAML manifest with such a rule set.\n\nExample 2-7. A default deny-all egress to IP address 169.254.169.254 network policy\n\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-egress-metadata-server namespace: a12 spec: podSelector: {} policyTypes: - Egress egress: - to: - ipBlock: cidr: 0.0.0.0/0 except: - 169.254.169.254/32\n\nOnce the network policy has been created, Pods in the namespace a12 should not be able to reach the metadata endpoints anymore. For detailed examples that use the endpoints via curl, see the relevant AWS documentation.\n\nProtecting GUI Elements The kubectl tool isn’t the only user interface (UI) for managing a cluster. While kubectl allows for fine-grained operations, most organizations prefer a more convenient graphical user interface (GUI) for managing the objects of a cluster. You can choose from a variety of options. The Kubernetes Dashboard is a free, web-based application. Other GUI dashboards for Kubernetes like Portainer go beyond the basic functionality by adding tracing of events or visualizations of hardware resource consumption. In this section, we’ll focus on the Kubernetes Dashboard as it is easy to install and configure.\n\nScenario: An Attacker Gains Access to the Dashboard Functionality\n\nThe Kubernetes Dashboard runs as a Pod inside of the cluster. Installing the Dashboard also creates a Service of type ClusterIP that only allows access\n\nto the endpoint from within the cluster. To make the Dashboard accessible to end users, you’d have to expose the Service outside of the cluster. For example, you could switch to a NodePort Service type or stand up an Ingress. Figure 2-4 illustrates the high-level architecture of deploying and accessing the Dashboard.\n\nFigure 2-4. An attacker who gained access to the Dashboard\n\nAs soon as you expose the Dashboard to the outside world, attackers can potentially gain access to it. Without the right security settings, objects can be deleted, modified, or used for malicious purposes. The most prominent victim of such an attack was Tesla, which in 2018 fell prey to hackers who gained access to its unprotected Dashboard to mine cryptocurrencies. Since then, newer versions of the Dashboard changed default settings to make it more secure from the get-go.\n\nInstalling the Kubernetes Dashboard\n\nInstalling the Kubernetes Dashboard is straightforward. You can create the relevant objects with the help of the YAML manifest available in the project’s GitHub repository. The following command installs all necessary objects:\n\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/\\ v2.6.0/aio/deploy/recommended.yaml\n\nRENDERING METRICS IN DASHBOARD\n\nYou may also want to install the metrics server if you are interested in inspecting resource consumption metrics as part of the Dashboard functionality.\n\nYou can find the objects created by the manifest in the kubernetes- dashboard namespace. Among them are Deployments, Pods, and Services. The following command lists all of them:\n\n$ kubectl get deployments,pods,services -n kubernetes-dashboard NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/dashboard-metrics-scraper 1/1 1 1 11m deployment.apps/kubernetes-dashboard 1/1 1 1 11m\n\nNAME READY STATUS RESTARTS AGE pod/dashboard-metrics-scraper-78dbd9dbf5-f8z4x 1/1 Running 0 11m pod/kubernetes-dashboard-5fd5574d9f-ns7nl 1/1 Running 0 11m\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP \\ PORT(S) AGE service/dashboard-metrics-scraper ClusterIP 10.98.6.37 <none> \\ 8000/TCP 11m service/kubernetes-dashboard ClusterIP 10.102.234.158 <none> \\ 80/TCP 11m\n\nAccessing the Kubernetes Dashboard\n\nThe kubectl proxy command can help with temporarily creating a proxy that allows you to open the Dashboard in a browser. This functionality is only meant for troubleshooting purposes and is not geared toward production environments. You can find information about the proxy command in the documentation:\n\n$ kubectl proxy Starting to serve on 127.0.0.1:8001\n\nOpen the browser with the URL http://localhost:8001/api/v1/namespaces/kubernetes- dashboard/services/https:kubernetes-dashboard:/proxy. The Dashboard will ask you to provide an authentication method and credentials. The recommended way to configure the Dashboard is through bearer tokens.\n\nCreating a User with Administration Privileges\n\nBefore you can authenticate in the login screen, you need to create a ServiceAccount and ClusterRoleBinding object that grant admin permissions. Start by creating the file admin-user-serviceaccount.yaml and populate it with the contents shown in Example 2-8.\n\nExample 2-8. Service account for admin permissions apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard\n\nNext, store the contents of Example 2-9 in the file admin- user- clusterrole bind ing.yaml to map the ClusterRole named cluster-admin to the ServiceAccount.\n\nExample 2-9. ClusterRoleBinding for admin permissions apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard\n\nCreate both objects with the following declarative command:\n\n$ kubectl create -f admin-user-serviceaccount.yaml serviceaccount/admin-user created $ kubectl create -f admin-user-clusterrolebinding.yaml clusterrolebinding.rbac.authorization.k8s.io/admin-user created\n\nYou can now create the bearer token of the admin user with the following command. The command will generate a token for the provided ServiceAccount object and render it on the console:\n\n$ kubectl create token admin-user -n kubernetes-dashboard eyJhbGciOiJSUzI1NiIsImtpZCI6...\n\nEXPIRATION OF A SERVICE ACCOUNT TOKEN\n\nBy default, this token will expire after 24 hours. That means that the token object will be deleted automatically once the “time to live” (TTL) has passed. You can change the TTL of a token by providing the command line option --ttl. For example, a value of 40h will expire the token after 40 hours. A value of 0 indicates that the token should never expire.\n\nCopy the output of the command and paste it into the “Enter token” field of the login screen, as shown in Figure 2-5.\n\nFigure 2-5. The usage of the token in the Dashboard login screen\n\nPressing the “Sign in” button will bring you to the Dashboard shown in Figure 2-6.\n\nFigure 2-6. The Dashboard view of Pods in a specific namespace\n\nYou can now manage end user and cluster objects without any restrictions.\n\nCreating a User with Restricted Privileges\n\nIn the previous section, you learned how to create a user with cluster-wide administrative permissions. Most users of the Dashboard only need a restricted set of permissions, though. For example, developers implementing and operating cloud-native applications will likely only need a subset of administrative permissions to perform their tasks on a Kubernetes cluster. Creating a user for the Dashboard with restricted privileges consists of a three-step approach:\n\n1. Create a ServiceAccount object.\n\n2. Create a ClusterRole object that defines the permissions.\n\n3. Create a ClusterRoleBinding that maps the ClusterRole to the ServiceAccount.\n\nAs you can see, the process is very similar to the one we went through for the admin user. Step 2 is new, as we need to be specific about which permissions we want to grant. The YAML manifests that follow will model\n\na user working as a developer that should only be allowed read-only permissions (e.g., getting, listing, and watching resources).\n\nStart by creating the file restricted-user-serviceaccount.yaml and populate it with the contents shown in Example 2-10.\n\nExample 2-10. Service account for restricted permissions apiVersion: v1 kind: ServiceAccount metadata: name: developer-user namespace: kubernetes-dashboard\n\nThe ClusterRole in Example 2-11 only allows getting, listing, and watching resources. All other operations are not permitted. Store the contents in the file restricted-user-clusterrole.yaml.\n\nExample 2-11. ClusterRole for restricted permissions apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" name: cluster-developer rules: - apiGroups: - '*' resources: - '*' verbs: - get - list - watch - nonResourceURLs: - '*' verbs: - get - list - watch\n\nLast, map the ServiceAccount to the ClusterRole in the file restricted- user-clusterrolebinding.yaml, as shown in Example 2-12.\n\nExample 2-12. ClusterRoleBinding for restricted permissions\n\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: developer-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-developer subjects: - kind: ServiceAccount name: developer-user namespace: kubernetes-dashboard\n\nCreate all objects with the following declarative command:\n\n$ kubectl create -f restricted-user-serviceaccount.yaml serviceaccount/restricted-user created $ kubectl create -f restricted-user-clusterrole.yaml clusterrole.rbac.authorization.k8s.io/cluster-developer created $ kubectl create -f restricted-user-clusterrolebinding.yaml clusterrolebinding.rbac.authorization.k8s.io/developer-user created\n\nGenerate the bearer token of the restricted user with the following command:\n\n$ kubectl create token developer-user -n kubernetes-dashboard eyJhbGciOiJSUzI1NiIsImtpZCI6...\n\nOperations that are not allowed for the logged-in user will not be rendered as disabled options in the GUI. You can still select the option; however, an error message is rendered. Figure 2-7 illustrates the behavior of the Dashboard if you try to delete a Pod via the user that doesn’t have the permissions to perform the operation.\n\nFigure 2-7. An error message rendered when trying to invoke a permitted operation\n\nAvoiding Insecure Configuration Arguments\n\nSecuring the Dashboard in production environments involves the usage of execution arguments necessary for properly configuring authentication and authorization. By default, login functionality is enabled and the HTTPS endpoint will be exposed on port 8443. You can provide TLS certificates with the --tls-cert-file and --tls-cert-key command line options if you don’t want them to be auto-generated.\n\nAvoid setting the command line arguments --insecure-port to expose an HTTP endpoint and --enable-insecure-login to enable serving the login page over HTTP instead of HTTPS. Furthermore, make sure you don’t use the option --enable-skip-login as it would allow circumventing an authentication method by simply clicking a Skip button in the login screen.\n\nVerifying Kubernetes Platform Binaries\n\nThe Kubernetes project publishes client and server binaries with every release. The client binary refers to the executable kubectl. Server binaries include kubeadm, as well as the executable for the API server, the scheduler, and the kubelet. You can find those files under the “tags” sections of the Kubernetes GitHub repository or on the release page at https://dl.k8s.io.\n\nScenario: An Attacker Injected Malicious Code into Binary\n\nThe executables kubectl and kubeadm are essential for interacting with Kubernetes. kubectl lets you run commands against the API server, e.g., for managing objects. kubeadm is necessary for upgrading cluster nodes from one version to another. Say you are in the process of upgrading the cluster version from 1.23 to 1.24. As part of the process, you will need to upgrade the kubeadm binary as well. The official upgrade documentation is very specific about what commands to use for upgrading the binary.\n\nSay an attacker managed to modify the kubeadm executable for version 1.24 and coaxed you into thinking that you need to download that very binary from a location where the malicious binary was placed. As shown in Figure 2-8, you’d expose yourself to running malicious code every time you invoke the modified kubeadm executable. For example, you may be sending credentials to a server outside of your cluster, which would open new ways to infiltrate your Kubernetes environment.\n\nFigure 2-8. An attacker who injected malicious code into a binary\n\nVerifying a Binary Against Hash\n\nYou can verify the validity of a binary with the help of a hash code like MD5 or SHA. Kubernetes publishes SHA256 hash codes for each binary. You should run through a hash validation for individual binaries before using them for the first time. Should the generated hash code not match with the one you downloaded, then there’s something off with the binary. The binary may have been modified by a third party or you didn’t use the hash code for the correct binary type or version.\n\nYou can download the corresponding hash code for a binary from https://dl.k8s.io. The full URL for a hash code reflects the version, operating system, and architecture of the binary. The following list shows example URLs for platform binaries compatible with Linux AMD64:\n\nkubectl: https://dl.k8s.io/v1.26.1/bin/linux/amd64/kubectl.sha256\n\nkubeadm: https://dl.k8s.io/v1.26.1/bin/linux/amd64/kubeadm.sha256\n\nkubelet: https://dl.k8s.io/v1.26.1/bin/linux/amd64/kubelet.sha256\n\nkube-apiserver: https://dl.k8s.io/v1.26.1/bin/linux/amd64/kube- apiserver.sha256\n\nYou’ll have to use an operating system-specific hash code validation tool to check the validity of a binary. You may have to install the tool if you do not have it available on your machine yet. The following commands show the usage of the tool for different operating systems, as explained in the Kubernetes documentation:\n\nLinux: echo \"$(cat kubectl.sha256) kubectl\" | sha256sum --check\n\nMacOSX: echo \"$(cat kubectl.sha256) kubectl\" | shasum -a 256 --check\n\nWindows with Powershell: $($(CertUtil -hashfile .\\kubectl.exe SHA256)[1] -replace \" \", \"\") -eq $(type .\\kubectl.exe.sha256)\n\nThe following commands demonstrate downloading the kubeadm binary for version 1.26.1 and its corresponding SHA256 hash file:\n\n$ curl -LO \"https://dl.k8s.io/v1.26.1/bin/linux/amd64/kubeadm\" $ curl -LO \"https://dl.k8s.io/v1.26.1/bin/linux/amd64/kubeadm.sha256\"\n\nThe validation tool shasum can verify if the checksum matches:\n\n$ echo \"$(cat kubeadm.sha256) kubeadm\" | shasum -a 256 --check kubeadm: OK\n\nThe previous command returned with an “OK” message. The binary file wasn’t tampered with. Any other message indicates a potential security risk when executing the binary.\n\nSummary\n\nThe domain “cluster setup” dials in on security aspects relevant to setting up a Kubernetes cluster. Even though you might be creating a cluster from scratch with kubeadm, that doesn’t mean you are necessarily following best practices. Using kube-bench to detect potential security risks is a good start. Fix the issues reported on by the tool one by one. You may also want to check client and server binaries against their checksums to ensure that they haven’t been modified by an attacker. Some organizations use a Dashboard to manage the cluster and its objects. Ensure that authentication and authorization for the Dashboard restrict access to a small subset of stakeholders.\n\nAn important security aspect is network communication. Pod-to-Pod communication is unrestricted by default. Have a close look at your application architecture running inside of Kubernetes. Only allow directional network traffic from and to Pods to fulfill the requirements of your architecture. Deny all other network traffic. When exposing the application outside of the cluster, make sure that Ingress objects have been configured with TLS termination. This will ensure that the data is encrypted both ways so that attackers cannot observe sensitive information like passwords sent between a client and the Kubernetes cluster.\n\nExam Essentials\n\nUnderstand the purpose and effects of network policies\n\nBy default, Pod-to-Pod communication is unrestricted. Instantiate a default deny rule to restrict Pod-to-Pod network traffic with the principle of least privilege. The attribute spec.podSelector of a\n\nnetwork policy selects the target Pod the rules apply to based on label selection. The ingress and egress rules define Pods, namespaces, IP addresses, and ports for allowing incoming and outgoing traffic. Network policies can be aggregated. A default deny rule may disallow ingress and/or egress traffic. An additional network policy can open up those rules with a more fine-grained definition.\n\nPractice the use of kube-bench to detect cluster component vulnerabilities\n\nThe Kubernetes CIS Benchmark is a set of best practices for recommended security settings in a production Kubernetes environment. You can automate the process of detecting security risks with the help of the tool kube-bench. The generated report from running kube-bench describes detailed remediation actions to fix a detected issue. Learn how to interpret the results and how to mitigate the issue.\n\nKnow how to configure Ingress with TLS termination\n\nAn Ingress can be configured to send and receive encrypted data by exposing an HTTPS endpoint. For this to work, you need to create a TLS Secret object and assign it a TLS certificate and key. The Secret can then be consumed by the Ingress using the attribute spec.tls[].\n\nKnow how to configure GUI elements for secure access\n\nGUI elements, such as the Kubernetes Dashboard, provide a convenient way to manage objects. Attackers can cause harm to your cluster if the application isn’t protected from unauthorized access. For the exam, you need to know how to properly set up RBAC for specific stakeholders. Moreover, you are expected to have a rough understanding of security- related command line arguments. Practice the installation process for the Dashboard, learn how to tweak its command line arguments, and understand the effects of setting permissions for different users.\n\nKnow how to detect modified platform binaries\n\nPlatform binaries like kubectl and kubeadm can be verified against their corresponding hash code. Know where to find the hash file and how to use a validation tool to identify if the binary has been tempered with.\n\nSample Exercises\n\nSolutions to these exercises are available in the Appendix.\n\n1. Create a network policy that denies egress traffic to any domain outside of the cluster. The network policy applies to Pods with the label app=backend and also allows egress traffic for port 53 for UDP and TCP to Pods in any other namespace.\n\n2. Create a Pod named allowed that runs the busybox:1.36.0 image on port 80 and assign it the label app=frontend. Make a curl call to http://google.com. The network call should be allowed, as the network policy doesn’t apply to the Pod.\n\n3. Create another Pod named denied that runs the busybox:1.36.0 image on port 80 and assign it the label app=backend. Make a curl call to http://google.com. The network call should be blocked.\n\n4. Install the Kubernetes Dashboard or make sure that it is already installed. In the namespace kubernetes-dashboard, create a ServiceAccount named observer-user. Moreover, create the corresponding ClusterRole and ClusterRoleBinding. The ServiceAccount should only be allowed to view Deployments. All other operations should be denied. As an example, create the Deployment named deploy in the default namespace with the following command: kubectl create deployment deploy -- image=nginx --replicas=3.\n\n5. Create a token for the ServiceAccount named observer-user that will never expire. Log into the Dashboard using the token. Ensure that only Deployments can be viewed and not any other type of resource.\n\n6. Download the binary file of the API server with version 1.26.1 on Linux AMD64. Download the SH256 checksum file for the API- server executable of version 1.23.1. Run the OS-specific verification tool and observe the result.\n\nOceanofPDF.com\n\nChapter 3. Cluster Hardening\n\nThe domain “cluster hardening” touches on topics important to keep a cluster as secure as possible once it has been set up and configured initially. As part of the discussion of this chapter, you may notice that I will reference concepts and practices that usually fall into the hands of Kubernetes administrators. Where appropriate, I will provide links to the topics that have already been covered by the CKA exam.\n\nAt a high level, this chapter covers the following concepts:\n\nRestricting access to the Kubernetes API\n\nConfiguring role-based access control (RBAC) to minimize exposure\n\nExercising caution in using service accounts\n\nUpdating Kubernetes frequently\n\nInteracting with the Kubernetes API\n\nThe API server is the gateway to the Kubernetes cluster. Any human user, client (e.g., kubectl), cluster component, or service account will access the API server by making a RESTful API call via HTTPS. It is the central point for performing operations like creating a Pod, or deleting a Service.\n\nIn this section, we’ll only focus on the security-specific aspects relevant to the API server. For a detailed discussion on the inner workings of the API server and the usage of the Kubernetes API, refer to the book Managing Kubernetes by Brendan Burns and Craig Tracey (O’Reilly).\n\nProcessing a Request",
      "page_number": 23
    },
    {
      "number": 3,
      "title": "Cluster Hardening",
      "start_page": 64,
      "end_page": 90,
      "detection_method": "regex_chapter_title",
      "content": "Figure 3-1 illustrates the stages a request goes through when a call is made to the API server. For reference, you can find more information in the Kubernetes documentation.\n\nFigure 3-1. API server request processing\n\nThe first stage of request processing is authentication. Authentication validates the identity of the caller by inspecting the client certificates or bearer tokens. If the bearer token is associated with a service account, then it will be verified here.\n\nThe second stage determines if the identity provided in the first stage can access the verb and HTTP path request. Therefore, stage two deals with authorization of the request, which is implemented with the standard Kubernetes RBAC model. Here, we’d ensure that the service account is allowed to list Pods or create a new Service object if that’s what has been requested.\n\nThe third stage of request processing deals with admission control. Admission control verifies if the request is well-formed and potentially needs to be modified before the request is processed. An admission control policy could, for example, ensure that the request for creating a Pod includes the definition of a specific label. If it doesn’t define the label, then the request is rejected.\n\nThe last stage of the process ensures that the resource included in the request is valid. Request validation can be implemented as part of admission control but doesn’t have to be. For example, this stage ensures\n\nthat the name of a Service object sticks to the standard Kubernetes naming rules for provided DNS names.\n\nConnecting to the API Server\n\nIt’s easy to determine the endpoint for the API server by running the following:\n\n$ kubectl cluster-info Kubernetes control plane is running at https://172.28.40.5:6443 ...\n\nFor the given Kubernetes cluster, the API server has been exposed via the URL https://172.28.40.5:6443. Alternatively, you can also have a look at the command line options --advertise-address and --secure-port in the configuration file of the API server to determine the endpoint. You can find the API server configuration file at /etc/kubernetes/manifests/kube-apiserver.yaml.\n\nCONFIGURING AN INSECURE PORT FOR THE API SERVER\n\nThe ability to configure the API server to use an insecure port (e.g., 80) has been deprecated in Kubernetes 1.10. With version 1.24, the insecure port flags --port and -- insecure-port have been removed completely and therefore cannot be used to configure the API server anymore. See the release notes for more information.\n\nUsing the kubernetes Service\n\nKubernetes makes accessing the API server a little bit more convenient for specific use cases. For example, you may want to send a request to the Kubernetes API from a Pod. Instead of using the IP address and port for the API server, you can simply refer to the Service named kubernetes.default.svc instead. This special Service lives in the default namespace and is stood up by the cluster automatically. Deleting\n\nthe Service will automatically recreate it. You can easily find the Service with the following command:\n\n$ kubectl get service kubernetes NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 32s\n\nUpon inspection of the endpoints of this Service, you will see that it points to the IP address and port of the API server, as demonstrated by executing the following command:\n\n$ kubectl get endpoints kubernetes NAME ENDPOINTS AGE kubernetes 172.28.40.5:6443 4m3s\n\nThe IP address and port of the Service is also exposed to the Pod via environment variables. You can read the values of the environment variables from a program running inside of a container. The Service’s IP address is reflected by the environment variable KUBERNETES_SERVICE_HOST. The port can be accessed using the environment variable KUBERNETES_SERVICE_PORT. To render the environment, simply access the environment variables using the env command in a temporary Pod:\n\n$ kubectl run kubernetes-envs --image=alpine:3.16.2 -it --rm --restart=Never \\ -- env KUBERNETES_SERVICE_HOST=10.96.0.1 KUBERNETES_SERVICE_PORT=443\n\nWe will use the kubernetes Service in the section “Minimizing Permissions for a Service Account”.\n\nAnonymous access\n\nThe following command makes an anonymous call to the API using the curl command line tool to list all namespaces. The option -k avoids verifying the server’s TLS certificate:\n\n$ curl https://172.28.40.5:6443/api/v1/namespaces -k { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Failure\", \"message\": \"namespaces is forbidden: User \\\"system:anonymous\\\" cannot list \\ resource \\\"namespaces\\\" in API group \\\"\\\" at the cluster scope\", \"reason\": \"Forbidden\", \"details\": { \"kind\": \"namespaces\" }, \"code\": 403 }\n\nAs you can see from the JSON-formatted HTTP response body, anonymous calls are accepted by the API server but do not have the appropriate permissions for the operation. Internally, Kubernetes maps the call to the username system:anonymous, which effectively isn’t authorized to execute the operation.\n\nAccess with a client certificate\n\nTo make a request as an authorized user, you need to either create a new one or use the existing, default user with administrator permissions named kubernetes-admin. We won’t go through the process of creating a new user right now. For more information on creating a user, refer to “Restricting User Permissions”.\n\nThe following command lists all available users, including their client certificate and key:\n\n$ kubectl config view --raw apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tL... server: https://172.28.132.5:6443 name: kubernetes contexts: - context: cluster: kubernetes\n\nuser: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tL... client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktL...\n\nThe base64-encoded value of the certificate authority\n\nThe user entry with administrator permissions created by default\n\nThe base64-encoded value of the user’s client certificate\n\nThe base64-encoded value of the user’s private key\n\nFor making a call using the user kubernetes-admin, we’ll need to extract the base64-encoded values for the CA, client certificate, and private key into files as a base64-decoded value. The following command copies the base64-encoded value and uses the tool base64 to decode it before it is written to a file. The CA value will be stored in the file ca, the client certificate value in kubernetes-admin.crt, and the private key in kubernetes-admin.key:\n\n$ echo LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tL... | base64 -d > ca $ echo LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tL... | base64 -d > kubernetes-admin.crt $ echo LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktL... | base64 - \\ > kubernetes-admin.key\n\nYou can now point the curl command to those files with the relevant command line option. The request to the API server should properly authenticate and return all existing namespaces, as the kubernetes-admin has the appropriated permissions:\n\n$ curl --cacert ca --cert kubernetes-admin.crt --key kubernetes-admin.key \\ https://172.28.132.5:6443/api/v1/namespaces { \"kind\": \"NamespaceList\", \"apiVersion\": \"v1\", \"metadata\": { \"resourceVersion\": \"2387\" }, \"items\": [ ... ] }\n\nRestricting Access to the API Server\n\nIf you’re exposing the API server to the internet, ask yourself if it is necessary. Some cloud providers offer the option of creating a private cluster, which will limit or completely disable public access to the API server. For more information, see the documentation pages for EKS and GKE.\n\nIf you are operating an on-premises Kubernetes cluster, you will need to instantiate firewall rules that prevent access to the API server. Setting up firewall rules is out of scope for the exam and therefore won’t be discussed in this book.\n\nScenario: An Attacker Can Call the API Server from the Internet\n\nCloud providers sometimes expose the API server to the internet to simplify administrative access. An attacker can try to make an anonymous request to the API server endpoint by declining to provide a client certificate or bearer token. If the attacker is lucky enough to capture user credentials, then an authenticated call can be performed. Depending on the permissions assigned to the user, malicious operations can be executed. Figure 3-2 illustrates an attacker calling the API server from the internet.\n\nFigure 3-2. An attacker calls the API server from the internet\n\nIn this chapter, we will have a look at how to restrict access to the API server and how to implement RBAC with limited permissions by example. “Understanding Open Policy Agent (OPA) and Gatekeeper” will review admission control with the help of OPA Gateway.\n\nRestricting User Permissions\n\nWe’ve seen that we can use the credentials of the kubernetes-admin user to make calls to the Kubernetes API. This user should be used very sparingly, nor should the credentials be shared with a lot of humans. A lot of damage can be done if the credentials fall into the wrong hands. Reserve this user exclusively for humans in charge of cluster administration.\n\nFor other stakeholders of your Kubernetes cluster, you should set up a dedicated user with a limited set of permissions. You may have specific roles in your organization you can map to. For example, you may have a developer role that should be allowed to manage Deployments, Pods, ConfigMaps, Secrets, and Services, but nothing else. To create a new user and assign the relevant RBAC permissions, refer to the Kubernetes documentation. In a nutshell, there are four steps:\n\n1. Create a private key.\n\n2. Create and approve a CertificateSigningRequest.\n\n3. Create a Role and a RoleBinding.\n\n4. Add the user to the kubeconfig file.\n\nWe will cover the process in detail, but will come back to the RBAC concept in more detail for a service account in “Minimizing Permissions for a Service Account”.\n\nCreating a private key Create a private key using the openssl executable. Provide an expressive file name, such as <username>.key:\n\n$ openssl genrsa -out johndoe.key 2048 Generating RSA private key, 2048 bit long modulus ...+ ......................................................................+ e is 65537 (0x10001)\n\nCreate a certificate signing request (CSR) in a file with the extension .csr. You need to provide the private key from the previous step. The following command uses the username johndoe when asked for entering the “Common Name” value. All other input requests are optional and can be filled in as needed:\n\n$ openssl req -new -key johndoe.key -out johndoe.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) []: State or Province Name (full name) []: Locality Name (eg, city) []: Organization Name (eg, company) []: Organizational Unit Name (eg, section) []: Common Name (eg, fully qualified host name) []:johndoe Email Address []:\n\nPlease enter the following 'extra' attributes to be sent with your certificate request A challenge password []:\n\nRetrieve the base64-encoded value of the CSR file content with the following command. You will need it when creating the CertificateSigningRequest object in the next step:\n\n$ cat johndoe.csr | base64 | tr -d \"\\n\" LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tL...\n\nCreating and approving a CertificateSigningRequest\n\nThe following script creates a CertificateSigningRequest object. A CertificateSigningRequest resource is used to request that a certificate be signed by a denoted signer:\n\n$ cat <<EOF | kubectl apply -f - apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: johndoe spec: request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tL... signerName: kubernetes.io/kube-apiserver-client expirationSeconds: 86400 usages: - client auth EOF certificatesigningrequest.certificates.k8s.io/johndoe created\n\nThe value for kubernetes.io/kube-apiserver-client for the attribute spec.signerName signs certificates that will be honored as client certificates by the API server. Use the base64-encoded value from the previous step and assign it as a value to the attribute spec.request. Finally, the optional attribute spec.expirationSeconds determines the lifespan of the certificate. The assigned value 86400 makes the certificate valid for a one day. You will want to increase the expiration time depending on how long you want the certificate to last, or simply refrain from adding the attribute.\n\nAfter creating the CertificateSigningRequest object, the condition will be “Pending.” You will need to approve the signing request within 24 hours or\n\nthe object will be deleted automatically as a means to garbage-collect unnecessary objects in the cluster:\n\n$ kubectl get csr johndoe NAME AGE SIGNERNAME REQUESTOR \\ REQUESTEDDURATION CONDITION johndoe 6s kubernetes.io/kube-apiserver-client minikube-user \\ 24h Pending\n\nUse the certificate approve command to approve the signing request. As a result, the condition changes to “Approved,Issued”:\n\n$ kubectl certificate approve johndoe certificatesigningrequest.certificates.k8s.io/johndoe approved $ kubectl get csr johndoe NAME AGE SIGNERNAME REQUESTOR \\ REQUESTEDDURATION CONDITION johndoe 17s kubernetes.io/kube-apiserver-client minikube-user \\ 24h Approved,Issued\n\nFinally, export the issued certificate from the approved CertificateSigningRequest object:\n\n$ kubectl get csr johndoe -o jsonpath={.status.certificate}| base64 \\ -d > johndoe.crt\n\nCreating a Role and a RoleBinding\n\nIt’s time to assign RBAC permissions. In this step, you will create a Role and a RoleBinding for the user. The Role models an “application developer” role within the organization. A developer should only be allowed to get, list, update, and delete Pods. The following imperative command creates the Role object:\n\n$ kubectl create role developer --verb=create --verb=get --verb=list \\ --verb=update --verb=delete --resource=pods role.rbac.authorization.k8s.io/developer created\n\nNext, we’ll bind the Role to the user named johndoe. Use the imperative command create rolebinding to achieve that:\n\n$ kubectl create rolebinding developer-binding-johndoe --role=developer \\ --user=johndoe rolebinding.rbac.authorization.k8s.io/developer-binding-johndoe created\n\nAdding the user to the kubeconfig file\n\nIn this last step, you will need to add the user to the kubeconfig file and create the context for a user. Be aware that the cluster name is minikube in the following command, as we are trying this out in a minikube installation:\n\n$ kubectl config set-credentials johndoe --client-key=johndoe.key \\ --client-certificate=johndoe.crt --embed-certs=true User \"johndoe\" set. $ kubectl config set-context johndoe --cluster=minikube --user=johndoe Context \"johndoe\" created.\n\nVerifying the permissions It’s time to switch to the user context named johndoe:\n\n$ kubectl config use-context johndoe Switched to context \"johndoe\".\n\nUsing kubectl as the client that makes calls to the API server, we’ll verify that the operation should be allowed. The API call for listing all Pods in the default namespace was authenticated and authorized:\n\n$ kubectl get pods No resources found in default namespace.\n\nThe output of the command indicates that the default namespace doesn’t contain any Pod object at this time but the call was successful. Let’s also test the negative case. Listing namespaces is a non-permitted operation for the user. Executing the relevant kubectl command will return with an error message:\n\n$ kubectl get namespaces Error from server (Forbidden): namespaces is forbidden: User \"johndoe\" cannot \\ list resource \"namespaces\" in API group \"\" at the cluster scope\n\nOnce you are done with verifying permissions, you may want to switch back to the context with admin permissions:\n\n$ kubectl config use-context minikube Switched to context \"minikube\".\n\nScenario: An Attacker Can Call the API Server from a Service Account\n\nA user represents a real person who commonly interacts with the Kubernetes cluster using the kubectl executable or the UI dashboard. Under rare conditions, applications running inside of a Pod’s container need to interact with the Kubernetes API. A typical example for such a requirement is the package manager Helm. Helm manages Kubernetes resources based on the YAML manifests bundled in a Helm chart. Kubernetes uses a service account to authenticate the Helm service process with the API server through an authentication token. This service account can be assigned to a Pod and mapped to RBAC rules.\n\nAn attacker who gains access to the Pod will likely also be able to misuse the service account to make calls to the Kubernetes API, as shown in Figure 3-3.\n\nFigure 3-3. An attacker uses a service account to call the API server\n\nMinimizing Permissions for a Service Account\n\nIt’s important to limit the permissions to only those service accounts that are really necessary for the application to function. The next sections will explain how to achieve this to minimize the potential attack surface.\n\nFor this scenario to work, you’ll need to create a ServiceAccount object and assign it to the Pod. Service accounts can be tied in with RBAC and assigned a Role and RoleBinding to define what operations they should be allowed to perform.\n\nBinding the service account to a Pod\n\nAs a starting point, we are going to a set up a Pod that lists all Pods and Deployments in the namespace k97 by calling the Kubernetes API. The call is made as part of an infinite loop every ten seconds. The response from the API call will be written to standard output accessible via the Pod’s logs.\n\nTo authenticate against the API server, we’ll send a bearer token associated with the service account used by the Pod. The default behavior of a service account is to auto-mount API credentials on the path /var/run/secrets/kubernetes.io/serviceaccount/token. We’ll simply get the contents of the file using the cat command line tool and send them along as a header for the HTTP request. Example 3-1 defines the namespace, the service account, and the Pod in a single YAML manifest file setup.yaml.\n\nExample 3-1. YAML manifest for assigning a service account to a Pod apiVersion: v1 kind: Namespace metadata: name: k97 --- apiVersion: v1 kind: ServiceAccount metadata: name: sa-api namespace: k97 --- apiVersion: v1 kind: Pod metadata:\n\nname: list-objects namespace: k97 spec: serviceAccountName: sa-api containers: - name: pods image: alpine/curl:3.14 command: ['sh', '-c', 'while true; do curl -s -k -m 5 -H \\ \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/ \\ serviceaccount/token)\" https://kubernetes.default.svc.cluster. \\ local/api/v1/namespaces/k97/pods; sleep 10; done'] - name: deployments image: alpine/curl:3.14 command: ['sh', '-c', 'while true; do curl -s -k -m 5 -H \\ \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/ \\ serviceaccount/token)\" https://kubernetes.default.svc.cluster. \\ local/apis/apps/v1/namespaces/k97/deployments; sleep 10; done']\n\nCreate the objects from the YAML file with the following command:\n\n$ kubectl apply -f setup.yaml namespace/k97 created serviceaccount/sa-api created pod/list-objects created\n\nVerifying the default permissions The Pod named list-objects makes a call to the API server to retrieve the list of Pods and Deployments in dedicated containers. The container pods performs the call to list Pods. The container deployments sends a request to the API server to list Deployments.\n\nAs explained in the Kubernetes documentation, the default RBAC policies do not grant any permissions to service accounts outside of the kube- system namespace. The logs of the containers pods and deployments return an error message indicating that the service account sa-api is not authorized to list the resources:\n\n$ kubectl logs list-objects -c pods -n k97 { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {},\n\n\"status\": \"Failure\", \"message\": \"pods is forbidden: User \\\"system:serviceaccount:k97:sa-api\\\" \\ cannot list resource \\\"pods\\\" in API group \\\"\\\" in the \\ namespace \\\"k97\\\"\", \"reason\": \"Forbidden\", \"details\": { \"kind\": \"pods\" }, \"code\": 403 } $ kubectl logs list-objects -c deployments -n k97 { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Failure\", \"message\": \"deployments.apps is forbidden: User \\ \\\"system:serviceaccount:k97:sa-api\\\" cannot list resource \\ \\\"deployments\\\" in API group \\\"apps\\\" in the namespace \\ \\\"k97\\\"\", \"reason\": \"Forbidden\", \"details\": { \"group\": \"apps\", \"kind\": \"deployments\" }, \"code\": 403 }\n\nNext up, we’ll stand up a ClusterRole and RoleBinding object with the required API permissions to perform the necessary calls.\n\nCreating the ClusterRole Start by defining the ClusterRole named list-pods-clusterrole shown in Example 3-2 in the file clusterrole.yaml. The set of the rules only adds the Pod resource and the verb list.\n\nExample 3-2. YAML manifest for a ClusterRole that allows listing Pods apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: list-pods-clusterrole rules: - apiGroups: [\"\"]\n\nresources: [\"pods\"] verbs: [\"list\"]\n\nCreate the object by pointing to its corresponding YAML manifest file:\n\n$ kubectl apply -f clusterrole.yaml clusterrole.rbac.authorization.k8s.io/list-pods-clusterrole created\n\nCreating the RoleBinding\n\nExample 3-3 defines the YAML manifest for the RoleBinding in the file rolebinding.yaml. The RoleBinding maps the ClusterRole list-pods- clusterrole to the service account named sa-pod-api and only applies to the namespace k97.\n\nExample 3-3. YAML manifest for a RoleBinding attached to a service account apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: serviceaccount-pod-rolebinding namespace: k97 subjects: - kind: ServiceAccount name: sa-api roleRef: kind: ClusterRole name: list-pods-clusterrole apiGroup: rbac.authorization.k8s.io\n\nCreate both the RoleBinding object using the apply command:\n\n$ kubectl apply -f rolebinding.yaml rolebinding.rbac.authorization.k8s.io/serviceaccount-pod-rolebinding created\n\nVerifying the granted permissions With the granted list permissions, the service account can now properly retrieve all the Pods in the k97 namespace. The curl command in the pods container succeeds, as shown in the following output:\n\n$ kubectl logs list-objects -c pods -n k97 { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"resourceVersion\": \"628\" }, \"items\": [ { \"metadata\": { \"name\": \"list-objects\", \"namespace\": \"k97\", ... } ] }\n\nWe did not grant any permissions to the service account for other resources. Listing the Deployments in the k97 namespace still fails. The following output shows the response from the curl command in the deployments namespace:\n\n$ kubectl logs list-objects -c deployments -n k97 { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Failure\", \"message\": \"deployments.apps is forbidden: User \\ \\\"system:serviceaccount:k97:sa-api\\\" cannot list resource \\ \\\"deployments\\\" in API group \\\"apps\\\" in the namespace \\ \\\"k97\\\"\", \"reason\": \"Forbidden\", \"details\": { \"group\": \"apps\", \"kind\": \"deployments\" }, \"code\": 403 }\n\nFeel free to modify the ClusterRole object to allow listing Deployment objects as well.\n\nDisabling automounting of a service account token\n\nThe Pod described in the previous section used the service account’s token as a means to authenticate against the API server. Mounting the token file at /var/run/secrets/kubernetes.io/serviceaccount/token is the standard behavior of every service account. You will really only need the contents of the file if the Pod actually interacts with the Kubernetes API. In all other cases, this behavior poses a potential security risk as access to the Pod will directly lead an attacker to the token.\n\nYou can disable the automount behavior for a service account object by assigning the value false to the attribute automountServiceAccountToken, as shown in Example 3-4.\n\nExample 3-4. Opting out of a service account’s token automount behavior apiVersion: v1 kind: ServiceAccount metadata: name: sa-api namespace: k97 automountServiceAccountToken: false\n\nIf you want to disable the automount behavior for individual Pods, use the attribute spec.automountServiceAccountToken in the Pod definition. Example 3-5 shows a YAML manifest for a Pod.\n\nExample 3-5. Disabling token automounting for a service account in a Pod apiVersion: v1 kind: Pod metadata: name: list-objects namespace: k97 spec: serviceAccountName: sa-api automountServiceAccountToken: false ...\n\nGenerating a service account token\n\nThere are a variety of use cases that speak for wanting to create a service account that disables token automounting. For example, you may need\n\naccess to the Kubernetes API from an external tool or a continuous delivery pipeline to query for information about existing objects. Authenticating against the API server in those scenarios still requires a token. The scenarios listed do not necessarily run a Pod with an assigned service account, but simply perform a RESTful API call from a tool like curl.\n\nTo create a token manually, execute the create token command and provide the name of the service account as an argument. The output of the command renders the token:\n\n$ kubectl create token sa-api eyJhbGciOiJSUzI1NiIsImtpZCI6IjBtQkJzVWlsQjl...\n\nYou’ll need to store the token in a safe place, e.g., a password manager. You cannot retrieve the token again if you lose it. You can only recreate it with the same command, which will automatically invalidate the previous token. All references that use the token will have to be changed.\n\nFor automated processes, it might be helpful to generate a token with a limited lifespan. The --duration will automatically invalidate the token after the “time-to-life” runs out:\n\n$ kubectl create token sa-api --duration 10m eyJhbGciOiJSUzI1NiIsImtpZCI6IjBtQkJzVWlsQjl...\n\nCreating a Secret for a service account\n\nWith Kubernetes 1.24, a ServiceAccount object does not automatically create a corresponding Secret object containing the token anymore. See the release notes for more information. Listing the ServiceAccount object renders 0 for the number of Secrets. The object also doesn’t contain the secrets attribute anymore in the YAML representation:\n\n$ kubectl get serviceaccount sa-api -n k97 NAME SECRETS AGE sa-api 0 42m\n\nYou can either generate the token using the create token command, as described in “Generating a service account token”, or manually create a corresponding Secret. Example 3-6 shows a YAML manifest for such a Secret.\n\nExample 3-6. Creating a Secret for a service account manually apiVersion: v1 kind: Secret metadata: name: sa-api-secret namespace: k97 annotations: kubernetes.io/service-account.name: sa-api type: kubernetes.io/service-account-token\n\nTo assign the service account to the Secret, add the annotation with the key kubernetes.io/service-account.name. The following command creates the Secret object:\n\n$ kubectl create -f secret.yaml secret/sa-api-secret created\n\nYou can find the token in the “Data” section when describing the Secret object:\n\n$ kubectl describe secret sa-api-secret -n k97 ... Data ==== ca.crt: 1111 bytes namespace: 3 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjBtQkJzVWlsQjl...\n\nUpdating Kubernetes Frequently\n\nInstalling a Kubernetes cluster with a specific version is not a one-time fire- and-forget operation. Even if you used the latest Long-Term Support (LTS) Release at the time of installation, it does not guarantee that your cluster is without security vulnerabilities.\n\nAs time goes by, security-related bugs and weaknesses will be discovered. This statement includes the underlying operating system and the dependencies the cluster nodes run on. An attacker can easily look up security vulnerabilities in the publicly disclosed Common Vulnerabilities and Exposures (CVE) database and exploit them.\n\nVersioning Scheme\n\nIt’s up to the cluster administrator to update the Kubernetes version across all nodes on a regular basis. Kubernetes follows the semantic versioning scheme. A Semantic Version consists of a major version, minor version, and patch version. For example, for the Kubernetes version 1.24.3, the major version is 1, the minor version is 24, and the patch version is 3.\n\nEach portion of the version carries a specific meaning. A change to the major version portion indicates a breaking change. Incrementing the minor version portion means that new functionality has been added in a backward- compatible manner. The patch version portion simply fixes a bug.\n\nBREAKING CHANGES IN KUBERNETES WITH MINOR VERSION UPDATES\n\nIt’s important to mention that Kubernetes doesn’t always stick to the strict interpretation of semantic versioning. For example, the PodSecurityPolicy (PSP) admission controller has been replaced by the Pod Security Admission concept in version 1.25.0. Conventionally, those changes should only happen with a major version update. Reference the Kubernetes deprecation policy for a better understanding on how an API, a flag, or a feature is phased out.\n\nRelease Cadence\n\nYou can expect a new minor version release of Kubernetes every three months. The release may include new features and additional bug fixes. Security fixes may be implemented as needed for the latest release of Kubernetes and will be backported to the two minor releases before that. Always staying on top of updating to the latest releases for your own\n\ncluster(s) takes quite a bit of people-power. You will need to reserve time for those activities accordingly.\n\nPerforming the Upgrade Process\n\nIt is recommended to upgrade from a minor version to the next higher one (e.g., from 1.23 to 1.24), or from a patch version to a more recent one (e.g., from 1.24.1 to 1.24.3). Abstain from jumping up multiple minor versions to avoid unexpected side effects.\n\nYou can find a full description of the upgrade steps in the official Kubernetes documentation. Figure 3-4 illustrates the upgrade process on a high level.\n\nThe cluster version upgrade process is already part of the CKA exam. Given that you have to pass the CKA as a prerequisite, I would assume that you already know how to perform the process. For a detailed description, refer to Certified Kubernetes Administrator (CKA) Study Guide.\n\nFigure 3-4. Process for a cluster version upgrade\n\nSummary Users, clients applications (such as kubectl or curl), Pods using service accounts, and cluster components all communicate with the API server to\n\nmanage objects. It’s paramount to secure the API server to prevent access with malicious intent.\n\nTo minimize the attack surface area, avoid exposing the API server to the internet using firewall rules. For every user or service account, restrict the permissions to execute operations against the Kubernetes API to the bare minimum using RBAC rules. With minimized permissions, attackers can cause far less damage in case they can gain access to credentials.\n\nMake sure to upgrade the version of your Kubernetes cluster. Incorporating bug and security fixes will decrease the risk of exposing unnecessary vulnerabilities attackers can use to their advantage.\n\nExam Essentials\n\nPractice interacting with the Kubernetes API.\n\nThis chapter demonstrated the different ways to communicate with the Kubernetes API. We performed API requests by switching to a user context, and with the help of a RESTful API call using curl. You will need to understand how to determine the endpoint of the API server and how to use different authentication methods, e.g., client credentials and bearer tokens. Explore the Kubernetes API and its endpoints on your own for broader exposure.\n\nUnderstand the implications of defining RBAC rules for users and service accounts.\n\nAnonymous user requests to the Kubernetes API will not allow any substantial operations. For requests coming from a user or a service account, you will need to carefully analyze permissions granted to the subject. Learn the ins and outs of defining RBAC rules by creating the relevant objects to control permissions. Service accounts automount a token when used in a Pod. Only expose the token as a Volume if you are intending to make API calls from the Pod.\n\nBe aware of Kubernetes release cadence and the need for upgrading the cluster.\n\nA Kubernetes cluster needs to be cared for over time for security reasons. Attackers may try to take advantage of known vulnerabilities in outdated Kubernetes versions. The version upgrade process is part of every administrator’s job and shouldn’t be ignored.\n\nSample Exercises\n\nSolutions to these exercises are available in the Appendix.\n\n1. Create a client certificate and key for the user named jill in the group observer. With the admin context, create the context for the user jill.\n\n2. For the group (not the user!), define a Role and RoleBinding in the default namespace that allow the verbs get, list, and watch for the resources Pods, ConfigMaps, and Secrets. Create the objects.\n\n3. Switch to the user context and execute a kubectl command that allows one of the granted operations, and one kubectl command that should not be permitted. Switch back to the admin context.\n\n4. Create a Pod named service-list in the namespace t23. The container uses the image alpine/curl:3.14 and makes a curl call to the Kubernetes API that lists Service objects in the default namespace in an infinite loop. Create and attach the service account api-call. Inspect the container logs after the Pod has been started. What response do you expect to see from the curl command?\n\n5. Assign a ClusterRole and RoleBinding to the service account that only allows the operation needed by the Pod. Have a look at the response from the curl command.\n\n6. Configure the Pod so that automounting of the service account token is disabled. Retrieve the token value and use it directly with the curl command. Make sure that the curl command can still authorize the operation.\n\n7. Navigate to the directory app-a/ch03/upgrade-version of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. Upgrade all nodes of the cluster from Kubernetes 1.25.6 to 1.26.1. The cluster consists of a single control plane node named kube- control-plane, and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\nOceanofPDF.com\n\nChapter 4. System Hardening\n\nThe domain “system hardening” deals with security aspects relevant to the underlying host system running the Kubernetes cluster nodes. Topics discussed here touch on techniques and configuration options that are fundamentally Linux core functionality. This includes disabling services and removing packages, managing users and groups, disabling ports, and setting up firewall rules. Finally, this chapter discusses Linux kernel hardening tools that can restrict what operations a process running in a container can perform on a host level.\n\nAt a high level, this chapter covers the following concepts:\n\nMinimizing the host OS footprint\n\nMinimizing IAM roles\n\nMinimizing external access to the network\n\nUsing kernel hardening tools like AppArmor and seccomp\n\nMinimizing the Host OS Footprint\n\nCluster nodes run on physical or virtual machines. In most cases, the operating system on those machines is a Linux distribution. Evidently, the operating system can expose security vulnerabilities.\n\nOver time, you need to keep the version of the operating system up to date with the latest security fixes. This process could entail upgrading a node’s operating system from Ubuntu 18 to 22, for example. Upgrading the operating system is out of scope for this book; for more information, check the relevant Linux documentation.\n\nMany Linux distributions, such as Ubuntu, come with additional tools, applications, and services that are not necessarily required for operating the",
      "page_number": 64
    },
    {
      "number": 4,
      "title": "System Hardening",
      "start_page": 91,
      "end_page": 116,
      "detection_method": "regex_chapter_title",
      "content": "Kubernetes cluster. It is your job as an administrator to identify security risks, disable or remove any operating system-specific functionality that may expose vulnerabilities, and keep the operating system patched to incorporate the latest security fixes. The less functionality an operating system has, the smaller the risk.\n\nCIS BENCHMARK FOR UBUNTU LINUX\n\nAs a reference guide, you may want to compare your operating system’s configuration with the CIS benchmark for Ubuntu Linux.\n\nScenario: An Attacker Exploits a Package Vulnerability\n\nFigure 4-1 illustrates an attacker exploiting a vulnerability of a package installed on the system. For example, the application could be the package manager snapd. Assume that the attacker takes advantage of the known vulnerability USN-5292-1 that has the potential of exposing sensitive information to an attacker.\n\nFigure 4-1. An attacker exploits an OS-level vulnerability\n\nThe following section will explain how to minimize security risks for services and packages that are not really needed for operating Kubernetes by simply disabling or removing them.\n\nDisabling Services\n\nOn Linux, many applications run as services in the background. Services can be managed using the command line tool systemctl. The following systemctl command lists all running services:\n\n$ systemctl | grep running ... snapd.service loaded active running Snap Daemon\n\nOne of the services we will not need for operating a cluster node is the package manager snapd. For more details on the service, retrieve the status for it with the status subcommand:\n\n$ systemctl status snapd ● snapd.service - Snap Daemon Loaded: loaded (/lib/systemd/system/snapd.service; enabled; vendor \\ preset: enabled) Active: active (running) since Mon 2022-09-19 22:49:56 UTC; 30min ago TriggeredBy: ● snapd.socket Main PID: 704 (snapd) Tasks: 12 (limit: 2339) Memory: 45.9M CGroup: /system.slice/snapd.service └─704 /usr/lib/snapd/snapd\n\nYou can stop service using the systemctl subcommand stop:\n\n$ sudo systemctl stop snapd Warning: Stopping snapd.service, but it can still be activated by: snapd.socket\n\nExecute the disable subcommand to prevent the service from being started again upon a system restart:\n\n$ sudo systemctl disable snapd Removed /etc/systemd/system/multi-user.target.wants/snapd.service.\n\nThe service has now been stopped and disabled:\n\n$ systemctl status snapd ● snapd.service - Snap Daemon\n\nLoaded: loaded (/lib/systemd/system/snapd.service; disabled; vendor \\ preset: enabled) Active: inactive (dead) since Mon 2022-09-19 23:22:22 UTC; 4min 4s ago TriggeredBy: ● snapd.socket Main PID: 704 (code=exited, status=0/SUCCESS)\n\nRemoving Unwanted Packages\n\nNow that the service has been disabled, there’s no more point in keeping the package around. You can remove the package to free up additional disk space and memory. You can use the apt purge command to remove a package and its transitive packages, as demonstrated in the following:\n\n$ sudo apt purge --auto-remove snapd Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be REMOVED: snapd* squashfs-tools* 0 upgraded, 0 newly installed, 2 to remove and 116 not upgraded. After this operation, 147 MB disk space will be freed. Do you want to continue? [Y/n] y ...\n\nYou can use the same command even if the package isn’t controlled by a service. Identify the packages you don’t need and simply remove them. You should end up with a much slimmer footprint of your system.\n\nA potential attacker cannot use the snapd service anymore to exploit the system. You should repeat the process for any unwanted services. As a result, the snapd service ceases to exist on the system:\n\n$ systemctl status snapd Unit snapd.service could not be found.\n\nMinimizing IAM Roles\n\nIdentity and access management (IAM) on the system level involves management of Linux users, the groups they belong to, and the permissions\n\ngranted to them. Any directory and file will have file permissions assigned to a user.\n\nProper user and access management is a classic responsibility of every system administrator. While your role as a Kubernetes administrator may not directly involve system-level IAM, it’s important to understand the implications to security. You will likely have to work with a peer to harden the system running the Kubernetes cluster.\n\nThis section will provide a short introduction on how to manage users and groups. We will also discuss how to set file permissions and ownership to minimize access as much as possible. We will only scratch the surface of the topic in this book. For more information, refer to the Linux documentation of your choice.\n\nScenario: An Attacker Uses Credentials to Gain File Access\n\nA security breach can lead to stolen user credentials. Gaining access to valid user credentials opens the door for additional attack vectors. Figure 4- 2 shows an attacker who could log into a cluster node with stolen user credentials and can now interact with all files and directories with the permissions granted to the user.\n\nFigure 4-2. An attacker uses stolen credentials to access files\n\nIt’s recommended to follow the principle of least privilege. Only grant administrative permissions to a limited group of users. All other users should only be allowed to perform operations necessary to perform their jobs.\n\nUnderstanding User Management\n\nEvery user must authenticate to a system to use it. The authenticated user has access to resources based on the assigned permissions. This section will walk you through the primary operations required to manage users.\n\nListing users To list all users on the system, render the contents of the file /etc/passwd. Every entry follows the general pattern username:password:UID:GID:com ment: home:shell. Some of the fields within the pattern may be empty:\n\n$ cat /etc/passwd root:x:0:0:root:/root:/bin/bash nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin ...\n\nThe command output renders the user root in the first position of the output. The last portion of the string for the root user, /bin/bash, indicates that the user is allowed to log into the system with a bash shell. Other users might not be allowed to log in at all. For those users, you will find the string /usr/sbin/nologin assigned to the shell field.\n\nAt any given point of time, you can see which processes have been started by users. The following command shows all bash processes, including the corresponding user that started it:\n\n$ ps aux | grep bash root 956 0.0 0.4 22512 19200 pts/0 Ss 17:57 0:00 -bash root 7064 0.0 0.0 6608 2296 pts/0 S+ 18:08 0:00 grep \\ --color=auto bash\n\nAdding a user\n\nAt some point, you may want to give team members access to the machines running the cluster nodes, with limited permissions. You can add new users to the system with the adduser command. Add the flag --shell /sbin/nologin to disable shell access for the user. The following command creates the user ben:\n\n$ sudo adduser ben Adding user ‘ben’ ... Adding new group ‘ben’ (1001) ... Adding new user ‘ben’ (1001) with group ‘ben’ ... Creating home directory ‘/home/ben’ ... Copying files from ‘/etc/skel’ ... New password: Retype new password: ...\n\nThe user entry has been added to the file /etc/passwd:\n\n$ cat /etc/passwd ... ben:x:1001:1001:,,,:/home/ben:/bin/bash\n\nSwitching to a user You can change the user in a shell by using the su command. The following command switches to the user ben we created earlier. You will be asked to enter the user’s password:\n\n$ su ben Password: ben@controlplane:/root$ pwd /root\n\nThe shell will indicate the current user by its prompt. You will inherit the environment variables from the account you used when running the su command. To create a new environment, add the hyphen with the su command:\n\n$ su - ben ben@controlplane:~$ pwd /home/ben\n\nAnother way to temporarily switch the user is by using the sudo command. You will need to have elevated privileges to execute the command. Therefore, the sudo command is equivalent to “run this command as administrator”:\n\n$ sudo -u ben pwd /root\n\nDeleting a user\n\nTeam members, represented by users in the system, transition to other teams or may simply leave the company. You will want to revoke access to the user to prevent unauthorized use of the credentials. The following command deletes the user, including the user’s home directory:\n\n$ sudo userdel -r ben\n\nUnderstanding Group Management\n\nIt’s more convenient for a system administrator to group users with similar access requirements to control permissions on an individual user level. Linux systems offer the concept of a group as a way to organize users based on teams, or specific organizational roles. We’ll briefly touch on the most important aspects of group management.\n\nListing groups Groups can be listed by inspecting the contents of the file /etc/group. Every entry follows the general pattern groupname:password:GID:group members:\n\n$ cat /etc/group root:x:0: plugdev:x:46:packer\n\nnogroup:x:65534: ...\n\nAs you can see in the output, some of the fields may be empty. The only group with an assigned member is plugdev, whose name is packer.\n\nAdding a group Use the command groupadd to add a new group. The following example adds the group kube-developers:\n\n$ sudo groupadd kube-developers\n\nThe group will now be listed in the file /etc/group. Notice that the group identifier is 1004:\n\n$ cat /etc/group ... kube-developers:x:1004:\n\nAssigning a user to a group To assign a group to a user, use the usermod command. The following command adds the user ben to the group kube-developers:\n\n$ sudo usermod -g kube-developers ben\n\nThe group identifier 1004 acts as a stand-in for the group kube- developers:\n\n$ cat /etc/passwd | grep ben ben:x:1001:1004:,,,:/home/ben:/bin/bash\n\nDeleting a group\n\nSometimes you want to get rid of a group entirely. Maybe the organizational role referring to a Linux group that does not exist anymore.\n\nUse the groupdel command to delete a group. You will receive an error message if the members are still part of the group:\n\n$ sudo groupdel kube-developers groupdel: cannot remove the primary group of user ben\n\nBefore deleting a group, you should reassign group members to a different group using the usermod command. The following command changes the group from kube-developers to kube-admins. Assume that the group kube-admins has been created before:\n\n$ sudo usermod -g kube-admins ben $ sudo groupdel kube-developers\n\nUnderstanding File Permissions and Ownership\n\nAssigning the file permissions with as minimal access as possible is crucial to maximizing security. This is where Linux file permissions and ownership come into play. I am only going to discuss the relevant operations on a high level. Refer to the Linux Foundation’s blog post about Linux file permissions for more details.\n\nViewing file permissions and ownership\n\nEvery user can create new directories and files. For example, you could use the touch command to create an empty file. The following command creates a file with the name my-file in the current directory:\n\n$ touch my-file\n\nTo see the contents of a directory in the “long” format, use the ls command. The long format of the output requested by the -l command line parameter renders the file permissions and the file ownership:\n\n$ ls -l total 0 -rw-r--r-- 1 root root 0 Sep 26 17:53 my-file\n\nThe important portion of the output is -rw-r--r--. The first character is a special permission character that can vary per system, followed by three groupings with the notation rwx. The first three characters stand for the owner permissions, the second set of three characters is for the group permissions, and the last three characters represent the permissions for all users. The symbol r means read permissions, w stands for write permissions, and x refers to execution permissions. In the previous example, the user root can read and write the file, whereas the group and all other users can only read the file.\n\nChanging file ownership Use the chown command to change the user and group assignment for a file or directory. The syntax of the command follows the pattern chown owner:group filename. The following command changes the ownership of the file to the user ben but does not reassign a group. The user executing the chown command needs to have write permissions:\n\n$ chown ben my-file $ ls -l total 0 -rw-r--r-- 1 ben root 0 Sep 26 17:53 my-file\n\nChanging file permissions You can add or remove permissions with the chmod command in a variety of notations. For example, use the following command to remove write permissions for the file owner:\n\n$ chmod -w file1 $ ls -l total 0 -r--r--r-- 1 ben root 0 Sep 26 17:53 my-file\n\nMinimizing External Access to the Network\n\nExternal access to your cluster nodes should only be allowed for the ports necessary to operate Kubernetes. We already discussed the standard Kubernetes ports in “Protecting Node Metadata and Endpoints”. Access to all other ports should be blocked.\n\nIdentifying and Disabling Open Ports\n\nApplications like FTP servers, web servers, and file and print services such as Samba open ports as a means to expose a communication endpoint to clients. Running applications that open network communication can expose a security risk. You can eliminate the risk by simply disabling the service and deinstalling the application.\n\nLet’s say we installed the Apache 2 HTTP web server on a control plane node with the following commands:\n\n$ sudo apt update $ sudo apt install apache2\n\nUPDATE ABOUT NETSTAT COMMAND\n\nThe netstat command has been deprecated in favor of the faster, more human-readable ss command. For more information, refer to the documentation of the operating system you are using.\n\nWe can inspect all open ports using the command line tool ss, a utility with similar functionality to netstat. The following command renders all of the open ports, including their processes. Among them is port 80, exposed by Apache 2:\n\n$ sudo ss -ltpn State Recv-Q Send-Q Local Address:Port Peer Address:Port Process ... LISTEN 0 511 *:80 *:* users: \\ ((\"apache2\",pid=18435,fd=4),(\"apache2\",pid=18434,fd=4),(\"apache2\", ]\\ pid=18432,fd=4))\n\nYou may have only needed the web server temporarily and may have simply forgotten about installing it. The process is currently managed by a server. You can review the status of a service with the systemctl status command:\n\n$ sudo systemctl status apache2 ● apache2.service - The Apache HTTP Server Loaded: loaded (/lib/systemd/system/apache2.service; enabled; vendor \\ preset: enabled) Active: active (running) since Tue 2022-09-20 22:25:25 UTC; 39s ago Docs: https://httpd.apache.org/docs/2.4/ Main PID: 18432 (apache2) Tasks: 55 (limit: 2339) Memory: 5.6M CGroup: /system.slice/apache2.service ├─18432 /usr/sbin/apache2 -k start ├─18434 /usr/sbin/apache2 -k start └─18435 /usr/sbin/apache2 -k start\n\nApache 2 is not needed by Kubernetes. We decide to shut down the service and deinstall the package:\n\n$ sudo systemctl stop apache2 $ sudo systemctl disable apache2 Synchronizing state of apache2.service with SysV service script with \\ /lib/systemd/systemd-sysv-install. Executing: /lib/systemd/systemd-sysv-install disable apache2 Removed /etc/systemd/system/multi-user.target.wants/apache2.service. $ sudo apt purge --auto-remove apache2\n\nVerify that the port isn’t used anymore. The ss command doesn’t find an application exposing port 80 anymore:\n\n$ sudo ss -ltpn | grep :80\n\nSetting Up Firewall Rules\n\nAnother way to control ports is with the help of an operating-system-level firewall. On Linux, you could use the Uncomplicated Firewall (UFW). This\n\nsection will give you a very brief introduction on how to enable UFW and how to configure firewall rules.\n\nFollowing the principle of least privilege, it’s a good idea to start by enabling the firewall and setting up deny rules for any incoming and outgoing network traffic. The following commands demonstrate the steps to achieve that:\n\n$ sudo ufw allow ssh Rules updated Rules updated (v6) $ sudo ufw default deny outgoing Default outgoing policy changed to deny (be sure to update your rules accordingly) $ sudo ufw default deny incoming Default incoming policy changed to deny (be sure to update your rules accordingly) $ sudo ufw enable Command may disrupt existing ssh connections. Proceed with operation (y|n)? y Firewall is active and enabled on system startup\n\nYou will want to allow external tools like kubectl to connect to the API server running on port 6443. On the control plane node, execute the following command to allow access to the API server port:\n\n$ sudo ufw allow 6443 Rule added Rule added (v6)\n\nYou will have to repeat the same process to open up other ports on control plane and worker nodes. Ensure that all other ports not needed to operate Kubernetes are blocked.\n\nUsing Kernel Hardening Tools\n\nApplications or processes running inside of a container can make system calls. A typical example could be the curl command performing an HTTP request. A system call is a programmatic abstraction running in the user\n\nspace for requesting a service from the kernel. We can restrict which system calls are allowed to be made with the help of kernel hardening tools. The CKS exam explicitly mentions two tools in this space, AppArmor and seccomp. We’ll discuss both tools and the mechanics to integrate them with Kubernetes.\n\nUsing AppArmor\n\nAppArmor provides access control to programs running on a Linux system. The tool implements an additional security layer between the applications invoked in the user space and the underlying system functionality. For example, we can restrict network calls or filesystem interaction. Many Linux distributions (e.g., Debian, Ubuntu, openSUSE) already ship with AppArmor. Therefore, AppArmor doesn’t have to be installed manually. Linux distributions that do not support AppArmor use Security-Enhanced Linux (SELinux) instead, which takes a similar approach to AppArmor. Understanding SELinux is out of scope for the CKS exam.\n\nUnderstanding profiles\n\nThe rules that define what a program can or cannot do are defined in an AppArmor profile. Every profile needs to be loaded into AppArmor before it can take effect. AppArmor provides a command line tool for checking the profiles that have been loaded. Execute the command aa-status to see a summary of all loaded profiles. You will see that AppArmor already comes with a set of default application profiles to protect Linux services:\n\n$ sudo aa-status apparmor module is loaded. 31 profiles are loaded. 31 profiles are in enforce mode. /snap/snapd/15177/usr/lib/snapd/snap-confine ... 0 profiles are in complain mode. 14 processes have profiles defined. 14 processes are in enforce mode. /pause (11934) docker-default ...\n\n0 processes are in complain mode. 0 processes are unconfined but have a profile defined.\n\nThe profile mode determines the treatment of rules at runtime should a matching event happen. AppArmor distinguishes two types of profile modes:\n\nEnforce\n\nThe system enforces the rules, reports the violation, and writes it to the syslog. You will want to use this mode to prevent a program from making specific calls.\n\nComplain\n\nThe system does not enforce the rules but will write violations to the log. This mode is helpful if you want to discover the calls a program makes.\n\nExample 4-1 defines a custom profile in the file k8s-deny-write for restricting file write access. The file should be placed in the directory /etc/apparmor.d of every worker node that executes workload. It is out of scope of this book to explain all the rules in detail. For more information, have a look at the AppArmor wiki.\n\nExample 4-1. An AppArmor profile for restricting file write access #include <tunables/global>\n\nprofile k8s-deny-write flags=(attach_disconnected) { #include <abstractions/base>\n\nfile,\n\ndeny /** w, }\n\nThe identifier after the profile keyword is the name of the profile.\n\nApply to file operations.\n\nDeny all file writes.\n\nSetting a custom profile\n\nTo load the profile into AppArmor, run the following command on the worker node:\n\n$ sudo apparmor_parser /etc/apparmor.d/k8s-deny-write\n\nThe command uses the enforce mode by default. To load the profile in complain mode, use the -C option. The aa-status command will now list the profile in addition to the default profiles. As you can see in the output, the profile is listed in enforce mode:\n\n$ sudo aa-status apparmor module is loaded. 32 profiles are loaded. 32 profiles are in enforce mode. k8s-deny-write ...\n\nAppArmor supports additional convenience commands as part of a utilities package. You can manually install the package using the following commands if you want to use them:\n\n$ sudo apt-get update $ sudo apt-get install apparmor-utils\n\nOnce installed, you can use the command aa-enforce to load a profile in enforce mode, and aa-complain to load a profile in complain mode. For the exam, it’s likely easier to just go with the standard apparmor_parser command.\n\nApplying a profile to a container\n\nYou need to ensure a couple of prerequisites before using AppArmor rules in a Pod definition. First, the container runtime needs to support AppArmor\n\nto let rules take effect. In addition, AppArmor needs to be installed on the worker node that runs the Pod. Last, make sure you loaded the profile, as described in the previous section.\n\nExample 4-2 shows a YAML manifest for a Pod defined in the file pod.yaml. To apply a profile to the container, you will need to set a specific annotation. The annotation key needs to use the key in the format container.apparmor.security.beta. kuber netes.io/<container- name>. In our case, the container name is hello. The full key is container.apparmor.security.beta.kubernetes.io/hello. The value of the annotation follows the pattern localhost/<profile-name>. The custom profile we want to use here is k8s-deny-write. For more information on the configuration options, see the Kubernetes documentation.\n\nExample 4-2. A Pod applying an AppArmor profile to a container apiVersion: v1 kind: Pod metadata: name: hello-apparmor annotations: container.apparmor.security.beta.kubernetes.io/hello: \\ localhost/k8s-deny-write spec: containers: - name: hello image: busybox:1.28 command: [\"sh\", \"-c\", \"echo 'Hello AppArmor!' && sleep 1h\"]\n\nThe annotation key that consists of a hard-coded prefix and the container name separated by a slash character.\n\nThe profile name available on the current node indicated by localhost.\n\nThe container name.\n\nWe are ready to create the Pod. Run the apply command and point it to the YAML manifest. Wait until the Pod transitions into the “Running” status:\n\n$ kubectl apply -f pod.yaml pod/hello-apparmor created $ kubectl get pod hello-apparmor NAME READY STATUS RESTARTS AGE hello-apparmor 1/1 Running 0 4s\n\nYou can now shell into the container and perform a file write operation:\n\n$ kubectl exec -it hello-apparmor -- /bin/sh / # touch test.txt touch: test.txt: Permission denied\n\nAppArmor will prevent writing a file to the container’s filesystem. The message “Permission denied” will be rendered if you try to perform the operation.\n\nUsing seccomp\n\nSeccomp, short for “Secure Computing Mode,” is another Linux kernel feature that can restrict the calls made from the userspace into the kernel. A seccomp profile is the mechanism for defining the rules for restricting syscalls and their arguments. Using seccomp can reduce the risk of exploiting a Linux kernel vulnerability. For more information on seccomp on Kubernetes, see the documentation.\n\nApplying the default container runtime profile to a container\n\nContainer runtimes, such as Docker Engine or containerd, ship with a default seccomp profile. The default seccomp profile allows the most commonly used syscalls used by applications while at the same time forbidding the use of syscalls considered dangerous.\n\nKubernetes does not apply the default container runtime profile to containers when creating a Pod, but you can enable it using the SeccompDefault feature gate. Alternatively, you can opt into the feature on a Pod-by-Pod basis by setting the seccomp profile type to RuntimeDefault with the help of the security context attribute seccompProfile. Example 4- 3 demonstrates its use.\n\nExample 4-3. A Pod applying the default seccomp profile provided by the container runtime profile apiVersion: v1 kind: Pod metadata: name: hello-seccomp spec: securityContext: seccompProfile: type: RuntimeDefault containers: - name: hello image: busybox:1.28 command: [\"sh\", \"-c\", \"echo 'Hello seccomp!' && sleep 1h\"]\n\nApplies the default container runtime profile.\n\nYou can start the Pod using the apply command and point to the YAML manifest. The Pod should transition into the “Running” status:\n\n$ kubectl apply -f pod.yaml pod/hello-seccomp created $ kubectl get pod hello-seccomp NAME READY STATUS RESTARTS AGE hello-seccomp 1/1 Running 0 4s\n\nThe echo command executed in the container is considered unproblematic from a security perspective by the default seccomp profile. The following command inspects the logs of the container:\n\n$ kubectl logs hello-seccomp Hello seccomp!\n\nThe call was permitted and resulted in writing the message “Hello seccomp!” to standard output.\n\nSetting a custom profile\n\nYou can create and set your own custom profile in addition to the default container runtime profile. The standard directory for those files is\n\n/var/lib/kubelet/seccomp. We’ll organize our custom profiles in the subdirectory profiles. Create the directory if it doesn’t exist yet:\n\n$ sudo mkdir -p /var/lib/kubelet/seccomp/profiles\n\nWe decide to create our custom profile in the file mkdir-violation.json in the profile directory. Example 4-4 shows the details of the profile definition. In a nutshell, the rule set disallows the use of the mkdir syscall.\n\nExample 4-4. A seccomp profile that prevents executing a mkdir syscall { \"defaultAction\": \"SCMP_ACT_ALLOW\", \"architectures\": [ \"SCMP_ARCH_X86_64\", \"SCMP_ARCH_X86\", \"SCMP_ARCH_X32\" ], \"syscalls\": [ { \"names\": [ \"mkdir\" ], \"action\": \"SCMP_ACT_ERRNO\" } ] }\n\nThe default action applies to all system calls. Here we’ll allow all syscalls using SCMP_ACT_ALLOW.\n\nYou can filter for specific architectures the default action should apply to. The definition of the field is optional.\n\nThe default action can be overwritten by declaring more fine-grained rules. The SCMP_ACT_ERRNO action will prevent the execution of the mkdir syscall.\n\nPlacing a custom profile into the directory /var/lib/kubelet/seccomp doesn’t automatically apply the rules to a Pod. You still need to configure a\n\nPod to use it.\n\nApplying the custom profile to a container\n\nApplying a custom profile follows a similar pattern to applying the default container runtime profile, with minor differences. As you can see in Example 4-5, we point the seccompProfile attribute of the security profile to the file mkdir-violation.json and set the type to Localhost.\n\nExample 4-5. A Pod applying a custom seccomp profile prevents a mkdir syscall apiVersion: v1 kind: Pod metadata: name: hello-seccomp spec: securityContext: seccompProfile: type: Localhost localhostProfile: profiles/mkdir-violation.json containers: - name: hello image: busybox:1.28 command: [\"sh\", \"-c\", \"echo 'Hello seccomp!' && sleep 1h\"] securityContext: allowPrivilegeEscalation: false\n\nRefers to a profile on the current node.\n\nApplies the profile with the name mkdir-violation.json in the subdirectory profiles.\n\nCreate the Pod using the declarative apply command. Wait until the Pod transitions into the “Running” status:\n\n$ kubectl apply -f pod.yaml pod/hello-seccomp created $ kubectl get pod hello-seccomp NAME READY STATUS RESTARTS AGE hello-seccomp 1/1 Running 0 4s\n\nShell into the container to verify that seccomp properly enforced the applied rules:\n\n$ kubectl exec -it hello-seccomp -- /bin/sh / # mkdir test mkdir: can't create directory test: Operation not permitted\n\nAs you can see in output, the operation renders an error message when trying to execute the mkdir command. The rule in the custom profile has been violated.\n\nSummary\n\nAddressing security aspects isn’t limited to Kubernetes cluster components or workload. There’s plenty you can do on the host system level. We discussed different OS capabilities and how to use them to minimize potential security vulnerabilities.\n\nMany operating systems come with a wealth of packages and services to offer a more feature-rich experience to end users. It’s important to identify functionality not required to operate a Kubernetes cluster. Purge unnecessary packages and services rigorously and close ports you don’t need. You will also want to limited which users are allowed to have access to specific directories, files, and applications. Use Linux’s user management to restrict permissions.\n\nIt’s very common for applications and processes running in containers to make system calls. You can use Linux kernel hardening tools like AppArmor and seccomp to restrict those calls. Only allow system calls crucial to fulfill the needs of your application running the container.\n\nExam Essentials\n\nHave a basic understanding of Linux OS tooling.\n\nThe CKS exam primarily focuses on security functionality in Kubernetes. This domain crosses the boundary to Linux OS security features. It won’t hurt to explore Linux-specific tools and security aspects independent from the content covered in this chapter. On a high level, familiarize yourself with service, package, user, and network management on Linux.\n\nKnow how to integrate Linux kernel hardening tools with Kubernetes.\n\nAppArmor and seccomp are just some kernel hardening tools that can be integrated with Kubernetes to restrict system calls made from a container. Practice the process of loading a profile and applying it to a container. In order to expand your horizons, you may also want to explore other kernel functionality that works alongside Kubernetes, such as SELinux or sysctl.\n\nSample Exercises\n\nSolutions to these exercises are available in the Appendix.\n\n1. Navigate to the directory app-a/ch04/close-ports of the checked- out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f.\n\nIdentify the process listening on port 21 in the VM kube-worker- 1. You decided not to expose this port to reduce the risk of attackers exploiting the port. Close the port by shutting down the corresponding process.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\n2. Navigate to the directory app-a/ch04/apparmor of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane, and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f. Create an AppArmor profile named network-deny. The profile should prevent any incoming and outgoing network traffic. Add the profile to the set of AppArmor rules in enforce mode. Apply the profile to the Pod named network-call running in the default namespace. Check the logs of the Pod to ensure that network calls cannot be made anymore.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\n3. Navigate to the directory app-a/ch04/seccomp of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane, and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f. Create a seccomp profile file named audit.json that logs all syscalls in the standard seccomp directory. Apply the profile to the Pod named network-call running in the default namespace. Check the log file /var/log/syslog for log entries.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\n4. Create a new Pod named sysctl-pod with the image\n\nnginx:1.23.1. Set the sysctl parameters net.core.somaxconn to 1024 and debug.iotrace to 1. Check on the status of the Pod.\n\nOceanofPDF.com\n\nChapter 5. Minimizing Microservice Vulnerabilities\n\nApplication stacks operated in a Kubernetes cluster often follow a microservices architecture. The domain “minimize microservice vulnerabilities” covers governance and enforcement of security settings on the Pod level. We’ll touch on Kubernetes core features, as well as external tooling, that help with minimizing security vulnerabilities. Additionally, we’ll also talk about encrypted network communication between Pods running microservices.\n\nAt a high level, this chapter covers the following concepts:\n\nSetting up appropriate OS-level security domains with security contexts, Pod Security Admission (PSA), and Open Policy Agent Gatekeeper\n\nManaging Secrets\n\nUsing container runtime sandboxes, such as gVisor and Kata Containers\n\nImplementing Pod-to-Pod communication encryption via mutual Transport Layer Security (TLS)\n\nSetting Appropriate OS-Level Security Domains\n\nBoth core Kubernetes and the Kubernetes ecosystem offer solutions for defining, enforcing, and governing security settings on the Pod and container level. This section will discuss security contexts, Pod Security Admission, and Open Policy Agent Gatekeeper. You will learn how to",
      "page_number": 91
    },
    {
      "number": 5,
      "title": "Minimizing Microservice Vulnerabilities",
      "start_page": 117,
      "end_page": 152,
      "detection_method": "regex_chapter_title",
      "content": "apply each of the features and tools using examples that demonstrate their importance to security. Let’s begin by setting up a scenario.\n\nScenario: An Attacker Misuses root User Container Access\n\nBy default, containers run with root privileges. A vulnerability in the application could grant an attacker root access to the container. The container’s root user is the same as the root user on the host machine. Not only can the attacker then inspect or modify the application, but they can also potentially install additional tooling that allows the attacker to break out of the container and step into host namespace with root permissions. The attacker could also copy sensitive data from the host’s filesystem to the container. Figure 5-1 illustrates the scenario.\n\nFigure 5-1. An attacker misuses root user container access\n\nFor that reason, running a container with the default root user is a bad idea. The next sections will explain how to declare a security context for the container that enforces the use of a non-root user or a specific user and/or group identifier. We’ll also discuss other security context settings relevant to shielding host access from the container.\n\nUnderstanding Security Contexts\n\nKubernetes, as the container orchestration engine, can apply additional configuration to increase container security. You do so by defining a security context. A security context defines privilege and access control settings for a Pod or a container. The following list provides some examples:\n\nThe user ID that should be used to run the Pod and/or container\n\nThe group ID that should be used for filesystem access\n\nGranting a running process inside the container some privileges of the root user but not all of them\n\nThe security context is not a Kubernetes primitive. It is modeled as a set of attributes under the directive securityContext within the Pod specification. Security settings defined on the Pod level apply to all containers running in the Pod; however, container-level settings take precedence. For more information on Pod-level security attributes, see the PodSecurityContext API. Container-level security attributes can be found in the SecurityContext API.\n\nEnforcing the Usage of a Non-Root User\n\nWe’ll have a look at a use case to make the functionality more transparent. Some images, like the one for the open source reverse-proxy server nginx, must be run with the root user. Say you wanted to enforce that containers cannot be run as the root user as a means to support a more sensible security strategy. The YAML manifest file container-non-root-user- error.yaml shown in Example 5-1 defines the security configuration specifically for a container. This security context only applies to this very container, but not others if you were to define more.\n\nExample 5-1. Enforcing a non-root user on an image that needs to run with the root user\n\napiVersion: v1 kind: Pod metadata: name: non-root-error spec: containers: - image: nginx:1.23.1 name: nginx securityContext: runAsNonRoot: true\n\nThe container fails during the startup process with the status CreateContainer Confi gError. A look at the Pod’s event log reveals that the image tries to run with the root user. The configured security context does not allow it:\n\n$ kubectl apply -f container-non-root-user-error.yaml pod/non-root-error created $ kubectl get pod non-root-error NAME READY STATUS RESTARTS AGE non-root-error 0/1 CreateContainerConfigError 0 9s $ kubectl describe pod non-root-error ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 24s default-scheduler Successfully \\ assigned default/non-root to minikube Normal Pulling 24s kubelet Pulling image \\ \"nginx:1.23.1\" Normal Pulled 16s kubelet Successfully \\ pulled image \"nginx:1.23.1\" in 7.775950615s Warning Failed 4s (x3 over 16s) kubelet Error: container \\ has runAsNonRoot and image will run as root (pod: \"non-root-error_default \\ (6ed9ed71-1002-4dc2-8cb1-3423f86bd144)\", container: secured-container) Normal Pulled 4s (x2 over 16s) kubelet Container image \\ \"nginx:1.23.1\" already present on machine\n\nThere are alternative nginx container images available that are not required to run with the root user. An example is bitnami/nginx. Example 5-2 shows the contents of the file container-non-root-user-success.yaml. The major change in this file is the value assigned to the spec.containers[].image attribute.\n\nExample 5-2. Enforcing a non-root user on an image that supports running with a user ID apiVersion: v1 kind: Pod metadata: name: non-root-success spec: containers: - image: bitnami/nginx:1.23.1 name: nginx securityContext: runAsNonRoot: true\n\nStarting the container with the runAsNonRoot directive will work just fine. The container transitions into the “Running” status:\n\n$ kubectl apply -f container-non-root-user-success.yaml pod/non-root-success created $ kubectl get pod non-root-success NAME READY STATUS RESTARTS AGE non-root-success 1/1 Running 0 7s\n\nLet’s quickly check which user ID the container runs with. Shell into the container and run the id command. The output renders the user ID, the group ID, and the IDs of supplemental groups. The image bitnami/nginx sets the user ID to 1001 with the help of an instruction when the container image is built:\n\n$ kubectl exec non-root-success -it -- /bin/sh $ id uid=1001 gid=0(root) groups=0(root) $ exit\n\nSetting a Specific User and Group ID\n\nMany container images do not set an explicit user ID or group ID. Instead of running with the root default user, you can set the desired user ID and group ID to minimize potential security risks. The YAML manifest stored in the file container-user-id.yaml shown in Example 5-3 sets the user ID to 1000 and the group ID to 3000.\n\nExample 5-3. Running the container with a specific user and group ID apiVersion: v1 kind: Pod metadata: name: user-id spec: containers: - image: busybox:1.35.0 name: busybox command: [\"sh\", \"-c\", \"sleep 1h\"] securityContext: runAsUser: 1000 runAsGroup: 3000\n\nCreating the Pod will work without issues. The container transitions into the “Running” status:\n\n$ kubectl apply -f container-user-id.yaml pod/user-id created $ kubectl get pods user-id NAME READY STATUS RESTARTS AGE user-id 1/1 Running 0 6s\n\nYou can inspect the user ID and group ID after shelling into the container. The current user is not allowed to create files in the / directory. Creating a file in the /tmp directory will work, as most users have the permissions to write to it:\n\n$ kubectl exec user-id -it -- /bin/sh / $ id uid=1000 gid=3000 groups=3000 / $ touch test.txt touch: test.txt: Permission denied / $ touch /tmp/test.txt / $ exit\n\nAvoiding Privileged Containers\n\nKubernetes establishes a clear separation between the container namespace and the host namespace for processes, network, mounts, user ID, and more. You can configure the container’s security context to gain privileges to\n\ncertain aspects of the host namespace. Assume the following implications when using a privileged container:\n\nProcesses within a container almost have the same privileges as processes on the host.\n\nThe container has access to all devices on the host.\n\nThe root user in the container has similar privileges to the root user on the host.\n\nAll directories on the host’s filesystem can be mounted in the container.\n\nKernel settings can be changed, e.g., by using the sysctl command.\n\nUSING CONTAINERS IN PRIVILEGED MODE\n\nConfiguring a container to use privileged mode should be a rare occasion. Most applications and processes running in a container do not need elevated privileges beyond the container namespace. Should you encounter a Pod that has been configured to use privileged mode, contact the team or developer in charge to clarify, as it will open a loophole for attackers to gain access to the host system.\n\nLet’s compare the behavior of a non-privileged container with one configured to run in privileged mode. First, we are going to set up a regular Pod, as shown in Example 5-4. No security context has been set on the Pod or container level.\n\nExample 5-4. A Pod with a container in non-privileged mode apiVersion: v1 kind: Pod metadata: name: non-privileged spec: containers: - image: busybox:1.35.0\n\nname: busybox command: [\"sh\", \"-c\", \"sleep 1h\"]\n\nCreate the Pod and ensure that it comes up properly:\n\n$ kubectl apply -f non-privileged.yaml pod/non-privileged created $ kubectl get pods NAME READY STATUS RESTARTS AGE non-privileged 1/1 Running 0 6s\n\nTo demonstrate the isolation between the container namespace and host’s namespace, we’ll try to use the sysctl to change the hostname. As you can see in the output of the command, the container will clearly enforce the restricted privileges:\n\n$ kubectl exec non-privileged -it -- /bin/sh / # sysctl kernel.hostname=test sysctl: error setting key 'kernel.hostname': Read-only file system / # exit\n\nTo make a container privileged, simply assign the value true to the security context attribute privileged. The YAML manifest in Example 5-5 shows an example.\n\nExample 5-5. A Pod with a container configured to run in privileged mode apiVersion: v1 kind: Pod metadata: name: privileged spec: containers: - image: busybox:1.35.0 name: busybox command: [\"sh\", \"-c\", \"sleep 1h\"] securityContext: privileged: true\n\nCreate the Pod as usual. The Pod should transition into the “Running” status:\n\n$ kubectl apply -f privileged.yaml pod/privileged created $ kubectl get pod privileged NAME READY STATUS RESTARTS AGE privileged 1/1 Running 0 6s\n\nYou can now see that the same sysctl will allow you to change the hostname:\n\n$ kubectl exec privileged -it -- /bin/sh / # sysctl kernel.hostname=test kernel.hostname = test / # exit\n\nA container security context configuration related to privileged mode is the attribute allowPrivilegeEscalation. This attribute will allow a process running the container to gain more privileges than the parent process. The default value for the attribute is false, but if you see the attribute set to true, critically question its use. In most cases, you do not need the functionality.\n\nScenario: A Developer Doesn’t Follow Pod Security Best Practices\n\nIt is unfair to assume that developers of all trades and levels of seniority have extensive knowledge of Kubernetes features, especially the ones that apply to security best practices. In the previous section, we learned about the security context and which settings to avoid. Developers are probably unaware of those best practices without continued education and therefore may create Pods that use problematic security settings or none at all. Figure 5-2 shows a developer creating a Pod in privileged mode enabled by a copied manifest found on the internet. An attacker will gladly use this setup to their advantage.\n\nFigure 5-2. A developer creates a Pod with enabled privileged mode\n\nThis is where you, as the Kubernetes security specialist, come in. The Kubernetes ecosystem provides core features and external tooling for enforcing security standards for Pods so that objects without the right configuration will be rejected, or at least audited. The next section explores the Kubernetes core feature named Pod Security Admission.\n\nUnderstanding Pod Security Admission (PSA)\n\nOlder versions of Kubernetes shipped with a feature called Pod Security Policies (PSP). Pod Security Policies are a concept that help with enforcing security standards for Pod objects. Kubernetes 1.21 deprecated Pod Security Policies and introduced the replacement functionality Pod Security Admission. PSA determines which Pod Security Standard (PSS) to follow. A PSS defines a range of security policies from highly restrictive to highly permissive.\n\nHowever, the Kubernetes release 1.25 completely removed Pod Security Policies. You may still see the feature listed in older versions of the CKS curriculum. We will only focus on Pod Security Admission in this book. PSA is enabled by default with Kubernetes 1.23; however, you will need to declare which Pods should adhere to the security standards. All you need to do to opt into the PSA feature is to add a label with a specific format to a namespace. All Pods in that namespace will have to follow the standards declared.\n\nThe label consists of three parts: a prefix, a mode, and a level. The prefix always uses the hard-coded value pod-security.kubernetes.io followed by a slash. The mode determines the handling of a violation. Finally, the level dictates the degree of security standards to adhere to. An example of such a label could look as follows:\n\nmetadata: labels: pod-security.kubernetes.io/enforce: restricted\n\nThe mode allows for setting three different options, shown in Table 5-1.\n\nTable 5-1. Pod Security Admission modes\n\nMode\n\nBehavior\n\nenforce\n\nViolations will cause the Pod to be rejected.\n\naudit\n\nPod creation will be allowed. Violations will be appended to the audit log.\n\nwarn\n\nPod creation will be allowed. Violations will be rendered on the console.\n\nTable 5-2 illustrates the security policies determined by the level set for the PSA.\n\nTable 5-2. Pod Security Admission levels\n\nLevel\n\nBehavior\n\nprivileged\n\nFully unrestricted policy.\n\nbaseline\n\nMinimally restrictive policy that covers crucial standards.\n\nrestricted\n\nHeavily restricted policy following best practices for hardening Pods from a security perspective.\n\nSee the Kubernetes documentation for details on the PSA, including usage examples.\n\nEnforcing Pod Security Standards for a Namespace\n\nLet’s apply a PSA to a Pod in the namespace psa. Example 5-6 shows the definition of the namespace and the declaration of the relevant label. The label will enforce a PSS on the highest level of security standards.\n\nExample 5-6. A namespace enforcing the highest level of security standards apiVersion: v1 kind: Namespace metadata: name: psa labels: pod-security.kubernetes.io/enforce: restricted\n\nMake sure that the Pod is created in the namespace psa. Example 5-7 shows the YAML manifest for a simple Pod running the busybox image.\n\nExample 5-7. A Pod violating the PSA restrictions apiVersion: v1 kind: Pod metadata: name: busybox\n\nnamespace: psa spec: containers: - image: busybox:1.35.0 name: busybox command: [\"sh\", \"-c\", \"sleep 1h\"]\n\nViolations will be rendered in the console upon running a command to create a Pod in the namespace. As you can see in the following, the Pod wasn’t allowed to be created:\n\n$ kubectl create -f psa-namespace.yaml namespace/psa created $ kubectl apply -f psa-violating-pod.yaml Error from server (Forbidden): error when creating \"psa-pod.yaml\": pods \\ \"busybox\" is forbidden: violates PodSecurity \"restricted:latest\": \\ allowPrivilegeEscalation != false (container \"busybox\" must set \\ securityContext.allowPrivilegeEscalation=false), unrestricted \\ capabilities (container \"busybox\" must set securityContext. \\ capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \\ \"busybox\" must set securityContext.runAsNonRoot=true), seccompProfile \\ (pod or container \"busybox\" must set securityContext.seccompProfile. \\ type to \"RuntimeDefault\" or \"Localhost\") $ kubectl get pod -n psa No resources found in psa namespace.\n\nYou need to configure the Pod’s security context settings to follow the very restrictive standards. Example 5-8 shows an exemplary Pod definition that does not violate the Pod Security Standard.\n\nExample 5-8. A Pod following the PSS apiVersion: v1 kind: Pod metadata: name: busybox namespace: psa spec: containers: - image: busybox:1.35.0 name: busybox command: [\"sh\", \"-c\", \"sleep 1h\"] securityContext: allowPrivilegeEscalation: false capabilities:\n\ndrop: [\"ALL\"] runAsNonRoot: true runAsUser: 2000 runAsGroup: 3000 seccompProfile: type: RuntimeDefault\n\nCreating the Pod object now works as expected:\n\n$ kubectl apply -f psa-non-violating-pod.yaml pod/busybox created $ kubectl get pod busybox -n psa NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 10s\n\nPSA is a built-in, enabled-by-default feature in Kubernetes version 1.23 or higher. It’s easy to adopt, allows for picking and choosing a suitable policy standard, and can be configured to enforce or just log violations.\n\nUnfortunately, PSA only applies to Pods with a predescribed set of policies. You cannot write your own custom rules, change the messaging, or mutate the Pod object should it not adhere to a PSS. In the next section, we are going to have a look at tooling that goes beyond the functionality of PSA.\n\nUnderstanding Open Policy Agent (OPA) and Gatekeeper\n\nOpen Policy Agent (OPA) is an open source, general-purpose policy engine for enforcing rules. OPA is not specific to Kubernetes and can be used across other technology stacks. One of its benefits is the ability to define a policy in a very flexible fashion. You can write your own rules with the help of the query language named Rego. The validation logic written in Rego determines if the object is accepted or denied.\n\nGatekeeper is an extension to Kubernetes that uses OPA. Gatekeeper allows for defining and enforcing custom policies for any kind of Kubernetes API primitive. Therefore, it is far more versatile than PSA but requires more intricate knowledge on how to craft those rules. Gatekeeper gets involved in the admission control stage discussed in “Processing a Request”. The\n\nfollowing list of policies tries to give you an impression on what’s possible with Gatekeeper:\n\nEnsuring that all Service objects need to define a label with the key team\n\nEnsuring that all container images defined by Pods need to be pulled from a company-internal registry\n\nEnsuring that Deployments need to control at least three replicas\n\nAt the time of writing, Gatekeeper allows for enforcing policies by rejecting object creation if requirements haven’t been met. Future versions of Gatekeeper might also provide a mechanism for mutating an object upon creation. For example, you may want to add specific label key-value pairs for any object created. The mutation would take care of adding those labels automatically.\n\nInstalling Gatekeeper\n\nInstalling Gatekeeper is relatively easy. All you need to do is to create a bunch of Kubernetes objects from a YAML manifest provided by the Gatekeeper project. You need to have cluster admin permissions to properly install Gatekeeper. The following command shows the kubectl command used to apply the latest release. For more information, see the installation manual:\n\n$ kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/\\ gatekeeper/master/deploy/gatekeeper.yaml\n\nGatekeeper objects have been installed in the namespace gatekeeper- system. Make sure that all Pods in the namespace transition into the “Running” status before trying to use Gatekeeper:\n\n$ kubectl get namespaces NAME STATUS AGE default Active 29h\n\ngatekeeper-system Active 4s ...\n\nImplementing an OPA Policy\n\nWe’ll use a specific use case as an example to demonstrate the moving parts required to define a custom OPA policy. “Using Network Policies to Restrict Pod-to-Pod Communication” explained how to assign a label to a namespace so that it can be selected from a network policy. At its core, our custom OPA policy will determine that namespaces need to define at least one label with the key app to signify the application hosted by the namespace.\n\nGatekeeper requires us to implement two components for custom policy, the constraint template and the constraint. In a nutshell, the constraint template defines the rules with Rego and describes the schema for the constraint. Example 5-9 shows a constraint template definition for enforcing a label assignment.\n\nExample 5-9. An OPA constraint template requiring the definition of at least a single label apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: k8srequiredlabels spec: crd: spec: names: kind: K8sRequiredLabels validation: openAPIV3Schema: type: object properties: labels: type: array items: type: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiredlabels\n\nviolation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] { provided := {label | input.review.object.metadata.labels[label]} required := {label | label := input.parameters.labels[_]} missing := required - provided count(missing) > 0 msg := sprintf(\"you must provide labels: %v\", [missing]) }\n\nDeclares the kind to be used by the constraint.\n\nSpecifies the validation schema of the constraint. In this case, we allow to pass in a property named labels that captures the required label keys.\n\nUses Rego to check for the existence of labels and compares them to the list of required keys.\n\nThe constraint is essentially an implementation of the constraint template. It uses the kind defined by the constraint template and populates the data provided by the end user. In Example 5-10, the kind is K8sRequiredLabels, which we defined in the constraint template. We are matching on namespaces and expect them to define the label with the key app.\n\nExample 5-10. An OPA constraint that defines the “data” for the policy apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: ns-must-have-app-label-key spec: match: kinds: - apiGroups: [\"\"] kinds: [\"Namespace\"] parameters: labels: [\"app\"]\n\nUses the kind defined by the constraint template.\n\nDefines the API resources the constraint template should apply to.\n\nDeclares that the labels property expects the key app to exist.\n\nWith the relevant YAML manifests in place, let’s create the objects for the constraint template and the constraint. Assume that the constraint template was written to the file constraint-template-labels.yaml and the constraint to the file constraint-ns-labels.yaml:\n\n$ kubectl apply -f constraint-template-labels.yaml constrainttemplate.templates.gatekeeper.sh/k8srequiredlabels created $ kubectl apply -f constraint-ns-labels.yaml k8srequiredlabels.constraints.gatekeeper.sh/ns-must-have-app-label-key created\n\nYou can verify the validation behavior with a quick-to-run imperative command. The following command tries to create a new namespace without a label assignment. Gatekeeper will render an error message and prevent the creation of the object:\n\n$ kubectl create ns governed-ns Error from server (Forbidden): admission webhook \"validation.gatekeeper.sh\" \\ denied the request: [ns-must-have-app-label-key] you must provide labels: {\"app\"}\n\nLet’s make sure that we can actually create a namespace with the expected label assignment. Example 5-11 shows the YAML manifest of such a namespace.\n\nExample 5-11. YAML manifest for namespace with a label assignment apiVersion: v1 kind: Namespace metadata: labels: app: orion name: governed-ns\n\nThe following command creates the object from the YAML manifest file named namespace-app-label.yaml:\n\n$ kubectl apply -f namespace-app-label.yaml namespace/governed-ns created\n\nThis simple example demonstrated the usage of OPA Gatekeeper. You can find a lot of other examples in the OPA Gatekeeper Library. Despite it not being spelled out explicitly in the CKS curriculum, you may also want to check out the project Kyverno, which recently gained a lot of traction with the Kubernetes community.\n\nManaging Secrets\n\nNo discussion on security features in Kubernetes would be complete without bringing up the topic of Secrets. I would assume that you are already well familiar with the API primitive Secret to define sensitive data and the different options for consuming it in a Pod. Given that this topic is already part of the CKA exam, I will not reiterate it here. For more information, see the relevant section in the Certified Kubernetes Administrator (CKA) Study Guide or the Kubernetes documentation. I talk about security aspects when consuming ConfigMaps and Secrets in a container in “Configuring a Container with a ConfigMap or Secret”.\n\nThe CKS exam puts a stronger emphasis on more specialized aspects of Secret management. One of those scenarios, which we already touched on, was the handling of a Secret you can assign to a service account. Revisit “Creating a Secret for a service account” to refresh your memory on the topic. As we are not going to discuss all built-in Secret types here, you may want to read up on their purpose and creation in the relevant section of the Kubernetes documentation.\n\nThe central location for storing Secrets key-value pairs is etcd. Let’s have a look at potential issues that may arise if an attacker gains access to Kubernetes backing store for cluster data.\n\nScenario: An Attacker Gains Access to the Node Running etcd\n\nWhere etcd runs is dependent on the topology of your Kubernetes cluster. For the purpose of this scenario, we’ll assume that etcd runs on the control plane node. Any data stored in etcd exists in unencrypted form, so access to the control plane node allows for reading Secrets in plain text. Figure 5-3 shows an attacker gaining access to the control plane node and therefore the unencrypted Secrets in etcd.\n\nFigure 5-3. An attacker gains access to etcd to read Secrets\n\nOne way to mitigate the situation is by encrypting the data stored in etcd. Access to etcd, either using etcdctl or by reading the etcd data from the filesystem, would not expose human-readable, sensitive information anymore.\n\nAccessing etcd Data\n\nWe’ll start by showing how an attacker could read etcd data after being able to log into the control plane node. First, we need to create a Secret object to store in etcd. Use the following imperative command to create an entry:\n\n$ kubectl create secret generic app-config --from-literal=password=passwd123 secret/app-config created\n\nWe created a Secret with the key-value pair password=passwd123. Shell into the control plane node using SSH. You can easily use the etcd client tool etcdctl to read an entry from etcd.\n\nUSING THE ETCD CLIENT TOOL ETCDCTL\n\nIt’s very likely that you do not have etcdctl installed on the control plane node yet. Follow the installation manual to make the tool available. On Debian Linux, it can be installed with sudo apt install etcd-client. To authenticate against etcd, you will need to provide the mandatory command line options --cacert, --cert, and --key. You can find the corresponding values in the configuration file for the API server usually available at /etc/kubernetes/manifests/kube-apiserver.yaml. The parameters need to start with the prefix --etcd.\n\nThe following command uses the mandatory CLI options to read the contents from the Secret object named app-config. The following output displays the file contents in hexadecimal format. While not 100% obvious, you can still identify the key-value pair in plain text from the output:\n\n$ sudo ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/\\ etcd/server.key get /registry/secrets/default/app-config | hexdump -C 00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 |/registry/secret| 00000010 73 2f 64 65 66 61 75 6c 74 2f 61 70 70 2d 63 6f |s/default/app-co| 00000020 6e 66 69 67 0a 6b 38 73 00 0a 0c 0a 02 76 31 12 |nfig.k8s.....v1.| 00000030 06 53 65 63 72 65 74 12 d9 01 0a b7 01 0a 0a 61 |.Secret........a| 00000040 70 70 2d 63 6f 6e 66 69 67 12 00 1a 07 64 65 66 |pp-config....def| 00000050 61 75 6c 74 22 00 2a 24 36 38 64 65 65 34 34 38 |ault\".*$68dee448| 00000060 2d 34 39 62 37 2d 34 34 32 66 2d 39 62 32 66 2d |-49b7-442f-9b2f-| 00000070 33 66 39 62 39 62 32 61 66 66 36 64 32 00 38 00 |3f9b9b2aff6d2.8.| 00000080 42 08 08 97 f8 a4 9b 06 10 00 7a 00 8a 01 65 0a |B.........z...e.| 00000090 0e 6b 75 62 65 63 74 6c 2d 63 72 65 61 74 65 12 |.kubectl-create.| 000000a0 06 55 70 64 61 74 65 1a 02 76 31 22 08 08 97 f8 |.Update..v1\"....| 000000b0 a4 9b 06 10 00 32 08 46 69 65 6c 64 73 56 31 3a |.....2.FieldsV1:| 000000c0 31 0a 2f 7b 22 66 3a 64 61 74 61 22 3a 7b 22 2e |1./{\"f:data\":{\".| 000000d0 22 3a 7b 7d 2c 22 66 3a 70 61 73 73 77 6f 72 64 |\":{},\"f:password| 000000e0 22 3a 7b 7d 7d 2c 22 66 3a 74 79 70 65 22 3a 7b |\":{}},\"f:type\":{| 000000f0 7d 7d 42 00 12 15 0a 08 70 61 73 73 77 6f 72 64 |}}B.....password| 00000100 12 09 70 61 73 73 77 64 31 32 33 1a 06 4f 70 61 |..passwd123..Opa| 00000110 71 75 65 1a 00 22 00 0a |que..\"..|\n\nIn the next step, we’ll encrypt the Secret stored in etcd and then verify the existing entries with the same command.\n\nEncrypting etcd Data\n\nYou can control how API data is encrypted in etcd with the help of the command line option --encryption-provider-config provided to the API server process. The value assigned to the parameter needs to point to a configuration file that defines an EncryptionConfiguration object. We’ll first create the configuration file and then configure the API server process to consume it.\n\nGenerate a 32-byte random key and base64-encode it. The value is needed to configure a so-called provider in the encryption configuration:\n\n$ head -c 32 /dev/urandom | base64 W68xlPT/VXcOSEZJvWeIvkGJnGfQNFpvZYfT9e+ZYuY=\n\nNext up, we’ll use the base64-encoded key and assign it to a provider in the encryption configuration, as shown in Example 5-12. Save the contents in the file /etc/kubernetes/enc/enc.yaml.\n\nExample 5-12. YAML manifest for encryption configuration apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: W68xlPT/VXcOSEZJvWeIvkGJnGfQNFpvZYfT9e+ZYuY= - identity: {}\n\nDefines the API resource to be encrypted in etcd. We are only encrypting Secrets data here.\n\nThe base64-encoded key assigned to an AES-CBC encryption provider.\n\nEdit the manifest at /etc/kubernetes/manifests/kube- apiserver.yaml, the YAML manifest that defines how to run an API server in a Pod. Add the parameter --encryption-provider-config, and define the Volume and its mountpath for the configuration file as the following shows:\n\n$ sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: \\ 192.168.56.10:6443 creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml volumeMounts: ... - name: enc mountPath: /etc/kubernetes/enc readonly: true volumes: ... - name: enc hostPath: path: /etc/kubernetes/enc type: DirectoryOrCreate ...\n\nThe Pod running the API server should automatically restart. This process may take a couple of minutes. Once fully restarted, you should be able to query for it:\n\n$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE\n\n... kube-apiserver-control-plane 1/1 Running 0 69s\n\nNew Secrets will be encrypted automatically. Existing Secrets need to be updated. You can run the following command to perform an update on Secrets across all namespaces. This includes the Secret named app-config in the default namespace:\n\n$ kubectl get secrets --all-namespaces -o json | kubectl replace -f - ... secret/app-config replaced\n\nRunning the etcdctl command we used before will reveal that the aescbc provider has been used to encrypt the data. The password value cannot be read in plain text anymore:\n\n$ sudo ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/\\ etcd/server.key get /registry/secrets/default/app-config | hexdump -C 00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 |/registry/secret| 00000010 73 2f 64 65 66 61 75 6c 74 2f 61 70 70 2d 63 6f |s/default/app-co| 00000020 6e 66 69 67 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 |nfig.k8s:enc:aes| 00000030 63 62 63 3a 76 31 3a 6b 65 79 31 3a ae 26 e9 c2 |cbc:v1:key1:.&..| 00000040 7b fd a2 74 30 24 85 61 3c 18 1e 56 00 a1 24 65 |{..t0$.a<..V..$e| 00000050 52 3c 3f f1 24 43 9f 6d de 5f b0 84 32 18 84 47 |R<?.$C.m._..2..G| 00000060 d5 30 e9 64 84 22 f5 d0 0b 6f 02 af db 1d 51 34 |.0.d.\"...o....Q4| 00000070 db 57 c8 17 93 ed 9e 00 ea 9a 7b ec 0e 75 0c 49 |.W........{..u.I| 00000080 6a e9 97 cd 54 d4 ae 6b b6 cb 65 8a 5d 4c 3c 9c |j...T..k..e.]L<.| 00000090 db 9b ed bc ce bf 3c ef f6 2e cb 6d a2 53 25 49 |......<....m.S%I| 000000a0 d4 26 c5 4c 18 f3 65 bb a8 4c 0f 8d 6e be 7b d3 |.&.L..e..L..n.{.| 000000b0 24 9b a8 09 9c bb a3 f9 53 49 78 86 f5 24 e7 10 |$.......SIx..$..| 000000c0 ad 05 45 b8 cb 31 bd 38 b6 5c 00 02 b2 a4 62 13 |..E..1.8.\\....b.| 000000d0 d5 82 6b 73 79 97 7e fa 2f 5d 3b 91 a0 21 50 9d |..ksy.~./];..!P.| 000000e0 77 1a 32 44 e1 93 9b 9c be bf 49 d2 f9 dc 56 23 |w.2D......I...V#| 000000f0 07 a8 ca a5 e3 e7 d1 ae 9c 22 1f 98 b1 63 b8 73 |.........\"...c.s| 00000100 66 3f 9f a5 6a 45 60 a7 81 eb 32 e5 42 4d 2b fd |f?..jE`...2.BM+.| 00000110 65 6c c2 c7 74 9f 1d 6a 1c 24 32 0e 7a 94 a2 60 |el..t..j.$2.z..`| 00000120 22 77 58 c9 69 c3 55 72 e8 fb 0b 63 9d 7f 04 31 |\"wX.i.Ur...c...1| 00000130 00 a2 07 76 af 95 4e 03 0a 92 10 b8 bb 1e 89 94 |...v..N.........| 00000140 45 60 01 45 bf d7 95 df ff 2e 9e 31 0a |E`.E.......1.| 0000014d\n\nFor more details on encrypting etcd data, refer to the Kubernetes documentation. There, you will find additional information on other encryption providers, how to rotate the decryption key, and the process to consider for a high-availability (HA) cluster setup.\n\nUnderstanding Container Runtime Sandboxes\n\nContainers run in a container runtime isolated from the host environment. The process or application running in the container can interact with the kernel by making syscalls. Now, we can have multiple containers (as controlled by Pods) running on a single Kubernetes cluster node and therefore the same kernel. Under certain conditions, vulnerabilities can lead to a situation where a process running a container can “break out” of its isolated environment and access another container running on the same host machine. A container runtime sandbox runs side-by-side with the regular container runtime but adds an additional layer of security by tightening process isolation.\n\nThere are a couple of use cases where using a container runtime sandbox may make sense. For example, your Kubernetes cluster handles the workload of different customers with the same infrastructure, a so-called multi-tenant environment. Another reason for wanting to rely on stronger container isolation is that you may not trust the process or application running in a container image, as should be the case if you pulled the container image from a public registry and you can’t verify the creator or its runtime behavior.\n\nScenario: An Attacker Gains Access to Another Container\n\nIn this scenario, we are confronted with a developer that pulls a container image from a public registry, as referenced by a Pod. The container has not been scanned for security vulnerabilities. An attacker can push a new tag of\n\nthe container image executing malicious code. After instantiating a container from the image, the malicious code running in the kernel group of container 1 can access the process running in container 2. As you can see in Figure 5-4, both containers use the same kernel of the host system.\n\nFigure 5-4. An attacker gains access to another container\n\nGenerally speaking, it’s not a good idea to blindly trust public container images. One way to ensure that such a container image runs with more isolation is the container runtime sandbox. The next section will introduce you to two implementations, both of which are explicitly mentioned by the curriculum.\n\nAvailable Container Runtime Sandbox Implementations\n\nIn this book, we’ll only want to talk about two container runtime sandbox implementations, Kata Containers and gVisor. Kata containers achieves container isolation by running them in a lightweight virtual machine. gVisor\n\ntakes a different approach. It effectively implements a Linux kernel that runs on the host system. Therefore, syscalls are not shared anymore across all containers on the host system.\n\nA deeper discussion on the feature sets or specific use cases for those container runtime sandbox implementations goes beyond the scope of this book. We’ll simply learn how to use one solution as an example, gVisor, and how to tie it into Kubernetes. Have a look at the talk “Kata Containers and gVisor: a Quantitative Comparison” for an in-depth comparison.\n\nInstalling and Configuring gVisor\n\nThe following instructions describe the steps required to install gVisor on Linux using the apt package manager. You will want to repeat those steps on all host machines declared as worker nodes. For the exam, you will not be expected to install gVisor or Kata Containers. You can assume that the container runtime sandbox has already been installed and configured.\n\nStart by installing the dependencies for gVisor with the following command:\n\n$ sudo apt-get update && \\ sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg\n\nNext, configure the key used to sign archives and the repository. As you can see in the following commands, gVisor is hosted on Google storage:\n\n$ curl -fsSL https://gvisor.dev/archive.key | sudo gpg --dearmor -o /usr/share/\\ keyrings/gvisor-archive-keyring.gpg $ echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/\\ gvisor-archive-keyring.gpg] https://storage.googleapis.com/gvisor/releases \\ release main\" | sudo tee /etc/apt/sources.list.d/gvisor.list > /dev/null\n\ngVisor includes an Open Container Initiative (OCI) runtime called runsc. The runsc runtime integrates with tools like Docker and Kubernetes to run container runtime sandboxes. The following command installs the executable from the repository:\n\n$ sudo apt-get update && sudo apt-get install -y runsc\n\nLet’s assume we are using containerd as the container runtime. You need to add some configuration to containerd to make it aware of runsc. You can find similar instructions for other container runtimes in the gVisor documentation:\n\n$ cat <<EOF | sudo tee /etc/containerd/config.toml version = 2 [plugins.\"io.containerd.runtime.v1.linux\"] shim_debug = true [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] runtime_type = \"io.containerd.runc.v2\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runsc] runtime_type = \"io.containerd.runsc.v1\" EOF\n\nFinally, restart containerd to let the changes take effect:\n\n$ sudo systemctl restart containerd\n\nWe successfully installed gVisor and can now configure Pods to use it.\n\nCreating and Using a Runtime Class\n\nIt’s a two-step approach to use a container runtime sandbox in a Pod. First, you need to create a runtime class. A RuntimeClass is a Kubernetes API resource that defines the configuration of the container runtime. Example 5- 13 shows a YAML manifest of a container runtime that points to runsc as the handler we set in the containerd configuration file earlier.\n\nExample 5-13. YAML manifest for defining a runtime class using runsc handler\n\napiVersion: node.k8s.io/v1 kind: RuntimeClass metadata: name: gvisor handler: runsc\n\nWe can now reference the runtime class name, gvisor, in the configuration of a Pod. Example 5-14 shows a Pod definition that assigns the runtime class using the attribute spec.runtimeClassName.\n\nExample 5-14. YAML manifest for a Pod using a runtime class apiVersion: v1 kind: Pod metadata: name: nginx spec: runtimeClassName: gvisor containers: - name: nginx image: nginx:1.23.2\n\nCreate the runtime class and Pod object using the apply command:\n\n$ kubectl apply -f runtimeclass.yaml runtimeclass.node.k8s.io/gvisor created $ kubectl apply -f pod.yaml pod/nginx created\n\nYou can verify that the container is running with the container runtime sandbox. Simply execute the dmesg command to examine the kernel ring buffer. The output from the command should mention gVisor, as shown in the following:\n\n$ kubectl exec nginx -- dmesg [ 0.000000] Starting gVisor... [ 0.123202] Preparing for the zombie uprising... [ 0.415862] Rewriting operating system in Javascript... [ 0.593368] Reading process obituaries... [ 0.741642] Segmenting fault lines... [ 0.797360] Daemonizing children... [ 0.831010] Creating bureaucratic processes... [ 1.313731] Searching for needles in stacks... [ 1.455084] Constructing home...\n\n[ 1.834278] Gathering forks... [ 1.928142] Mounting deweydecimalfs... [ 2.109973] Setting up VFS... [ 2.157224] Ready!\n\nUnderstanding Pod-to-Pod Encryption with mTLS\n\nIn “Using Network Policies to Restrict Pod-to-Pod Communication”, we talked about Pod-to-Pod communication. One of the big takeaways was that every Pod can talk to any other Pod by targeting its virtual IP address unless you put a more restrictive network policy in place. The communication between two Pods is unencrypted by default.\n\nTLS provides encryption for network communication, often in conjunction with the HTTP protocol. That’s when we talk about using the HTTPS protocol for calls to web pages from the browser. As part of the authentication process, the client offers its client certificate to the server for proving its identity. The server does not authenticate the client, though.\n\nWhen loading a web page, the identity of the client, in this case the browser, usually doesn’t matter. The important part is that the web page proves its identity. Mutual TLS (mTLS) is like TLS, but both sides have to authenticate. This approach has the following benefits. First, you achieve secure communication through encryption. Second, you can verify the client identity. An attacker cannot easily impersonate another Pod.\n\nScenario: An Attacker Listens to the Communication Between Two Pods\n\nAn attacker can use the default, unencrypted Pod-to-Pod network communication behavior to their advantage. As you can see in Figure 5-5, an attacker doesn’t even need to break into a Pod. They can simply listen to the Pod-to-Pod communication by impersonating the sending or the receiving side, extract sensitive information, and then use it for more advanced attack vectors.\n\nFigure 5-5. An attacker listens to Pod-to-Pod communication\n\nYou can mitigate the situation by setting up mTLS. The next section will briefly touch on the options for making that happen.\n\nAdopting mTLS in Kubernetes\n\nThe tricky part about implementing mTLS in a Kubernetes cluster is the management of certificates. As you can imagine, we’ll have to deal with a lot of certificates when implementing a microservices architecture. Those certificates are usually generated by an official certificate authority (CA) to ensure that they can be trusted. Requesting a certificate involves sending a certificate signing request (CSR) to the CA. If the CA approves the request, it creates the certificate, then signs and returns it. It’s recommended to assign short lifespans to a certificate before it needs to be re-issued again. That process is called certificate rotation.\n\nIt’s somewhat unclear to what degree of detail the CKS exam requires you to understand mTLS. The general process of requesting and approving a certificate is described in the Kubernetes documentation.\n\nIn most cases, Kubernetes administrators rely on a Kubernetes service mesh to implement mTLS instead of implementing it manually. A Kubernetes service mesh, such as Linkerd or Istio, is a tool for adding cross-cutting functionality to your cluster, like observability and security.\n\nAnother option is to use transparent encryption to ensure that traffic doesn’t go on the wire unencrypted. Some of the popular CNI plugins, such as Calico and Cilium, have added support for WireGuard. WireGuard is an open source, lightweight, and secure Virtual Private Network (VPN) solution that doesn’t require the configuration or management of encryption\n\nkeys or certificates. Many teams prefer WireGuard over a service mesh as it is easier to manage.\n\nServices meshes and WireGuard are out of scope for the exam.\n\nSummary\n\nIt’s important to enforce security best practices for Pods. In this chapter, we reviewed different options. We looked at security contexts and how they can be defined on the Pod and container level. For example, we can configure a security context for a container to run with a non-root user and prevent the use of privileged mode. It’s usually the responsibility of a developer to define those settings. The Pod Security Admission is a Kubernetes feature that takes Pod security settings one step further. You can centrally configure a Pod such that it needs to adhere to certain security standards. The configured security standard can either be enforced, audited, or just logged to standard output. Gatekeeper is an open source project that implements the functionality of the Open Policy Agent for Kubernetes. Not only can you govern the configuration for Pod objects, you can also apply policies to other kinds of objects during creation time.\n\nKey-value pairs defined by Secrets are stored in etcd in plain text. You should configure encryption for etcd to ensure that attackers cannot read sensitive data from it. To enable encryption, create a YAML manifest for an EncryptionConfiguration, which you would then pass to the API server process with the command line option --encryption-provider-config.\n\nContainer runtime sandboxes help with isolating processes and applications to a stronger degree than the regular container runtime. The projects Kata Containers and gVisor are implementations of such a container runtime sandbox and can be installed and configured to work with Kubernetes. We tried gVisor. After installing and configuring gVisor, you will need to create a RuntimeClass object that points to runsc. In the Pod configuration, point to the RuntimeClass object by name.\n\nPod-to-Pod communication is unencrypted and unauthenticated by default. Mutual TLS makes the process more secure. Pods communicating with one another need to provide certificates to prove their identity. Implementing mTLS for a cluster with hundreds of microservices is a tedious task. Each Pod running a microservice needs to employ an approved certificate from a Client Authority. Services meshes help with adding mTLS as a feature to a Kubernetes cluster.\n\nExam Essentials\n\nPractice the use of core Kubernetes features and external tools to govern security settings.\n\nIn the course of this chapter, we looked at OS-level security settings and how to govern them with different core features and external tooling. You need to understand the different options, their benefits and limitations, and be able to apply them to implement contextual requirements. Practice the use of security contexts, Pod Security Admission, and Open Policy Agent Gatekeeper. The Kubernetes ecosystem offers more tooling in this space. Feel free to explore those on your own to expand your horizon.\n\nUnderstand how etcd manages Secrets data.\n\nThe CKA exam already covers the workflow of creating and using Secrets to inject sensitive configuration data into Pods. I am assuming that you already know how to do this. Every Secret key-value pair is stored in etcd. Expand your knowledge of Secret management by learning how to encrypt etcd so that an attacker with access to a host running etcd isn’t able to read information in plain text.\n\nKnow how to configure the use of a container runtime sandbox.\n\nContainer runtime sandboxes help with adding stricter isolation to containers. You will not be expected to install a container runtime sandbox, such as Kata Containers or gVisor. You do need to understand\n\nthe process for configuring a container runtime sandbox with the help of a RuntimeClass object and how to assign the RuntimeClass to a Pod by name.\n\nGain awareness of mTLS.\n\nSetting up mTLS for all microservices running in a Pod can be extremely tedious due to certificate management. For the exam, understand the general use case for wanting to set up mTLS for Pod-to- Pod communication. You are likely not expected to actually implement it manually, though. Production Kubernetes clusters use services meshes to provide mTLS as a feature.\n\nSample Exercises\n\nSolutions to these exercises are available in the Appendix.\n\n1. Create a Pod named busybox-security-context with the container image busybox:1.28 that runs the command sh -c sleep 1h. Add a Volume of type emptydir and mount it to the path /data/test. Configure a security context with the following attributes: runAsUser: 1000, runAsGroup: 3000, and fsGroup: 2000. Furthermore, set the attribute allowPrivilegeEscalation to false.\n\nShell into the container, navigate to the directory /data/test, and create the file named hello.txt. Check the group assigned to the file. What’s the value? Exit out of the container.\n\n2. Create a Pod Security Admission (PSA) rule. In the namespace called audited, create a Pod Security Standard (PSS) with the level baseline that should be rendered to the console. Try to create a Pod in the namespace that violates the PSS and produces a message on the console log. You can provide any name, container image, and security configuration you like. Will the Pod\n\nbe created? What PSA level needs to be configured to prevent the creation of the Pod?\n\n3. Install Gatekeeper on your cluster. Create a Gatekeeper ConstraintTemplate object that defines the minimum and maximum number of replicas controlled by a ReplicaSet. Instantiate a Constraint object that uses the ConstraintTemplate. Set the minimum number of replicas to 3 and the maximum number to 10.\n\nCreate a Deployment object that sets the number of replicas to 15. Gatekeeper should not allow the Deployment, ReplicaSet, and Pods to be created. An error message should be rendered. Try again to create the Deployment object but with a replica number of 7. Verify that all objects have been created successfully.\n\n4. Configure encryption for etcd using the aescbc provider. Create a new Secret object of type Opaque. Provide the key-value pair api- key=YZvkiWUkycvspyGHk3fQRAkt. Query for the value of the Secret using etcdctl. What’s the encrypted value?\n\n5. Navigate to the directory app-a/ch05/gvisor of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f. gVisor has been installed in the VM kube-worker-1. Shell into the VM and create a RuntimeClass object named container- runtime-sandbox with runsc as the handler. Then create a Pod with the name nginx and the container image nginx:1.23.2 and assign the RuntimeClass to it.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\nOceanofPDF.com\n\nChapter 6. Supply Chain Security\n\nEarlier chapters primarily focused on securing the Kubernetes cluster and its components, the OS infrastructure used to run cluster nodes, and the operational aspects for running workload on a cluster node with existing container images. This chapter takes a step back and drills into the process, best practices, and tooling for designing, building, and optimizing container images.\n\nSometimes, you do not want to create your own container image but instead consume an existing one produced by a different team or company. Scanning container images for known vulnerabilities in a manual or automated fashion should be part of your vetting process before using them to run your workload. We’ll talk through some options relevant to the CKS exam used to identify, analyze, and mitigate security risks for pre-built container images.\n\nAt a high level, this chapter covers the following concepts:\n\nMinimizing base image footprint\n\nSecuring the supply chain\n\nUsing static analysis of user workload\n\nScanning images for known vulnerabilities\n\nMinimizing the Base Image Footprint\n\nThe process for building a container image looks straightforward on the surface level; however, the devil is often in the details. It may not be obvious to someone new to the topic to refrain from building a container",
      "page_number": 117
    },
    {
      "number": 6,
      "title": "Supply Chain Security",
      "start_page": 153,
      "end_page": 187,
      "detection_method": "regex_chapter_title",
      "content": "image that is unnecessarily too large in size, riddled with vulnerabilities, and not optimized for container layer caching. We’ll address all of those aspects in the course of this chapter with the help of the container engine Docker.\n\nScenario: An Attacker Exploits Container Vulnerabilities\n\nOne of the first decisions you have to make when defining a Dockerfile is the selection of a base image. The base image provides the operating system and additional dependencies, and it may expose shell access.\n\nSome of the base images you can choose from on a public registry like Docker Hub are large in size and will likely contain functionality you don’t necessarily need to run your application inside of it. The operating system itself, as well as any dependencies available with the base image, can expose vulnerabilities.\n\nIn Figure 6-1, the attacker was able to figure out details about the container by gaining access to it. Those vulnerabilities can now be used as a launching pad for more advanced attacks.\n\nFigure 6-1. An attacker exploits container image vulnerabilities\n\nIt is recommended to use a base image with a minimal set of functionality and dependencies. The next couple of sections will explain the methods for creating a more optimized base image that’s faster to build, quicker to download from a container registry, and that will ultimately lead to a smaller attack surface simply by reducing the bloat. The next sections will touch on the most important techniques. You can find a more detailed list of best practices for writing Dockerfiles in the Docker documentation.\n\nPicking a Base Image Small in Size\n\nSome container images can have a size of a gigabyte or even more. Do you really need all the functionality bundled with such a container image? Unlikely. Thankfully, many container producers upload a wide range of variations of their container images for the same release. One of those variations is an alpine image, a small, lightweight, and less vulnerable Linux distribution. As you can see in the following output, the downloaded alpine container image with the tag 3.17.0 only has a size of 7.05MB:\n\n$ docker pull alpine:3.17.0 ... $ docker image ls alpine REPOSITORY TAG IMAGE ID CREATED SIZE alpine 3.17.0 49176f190c7e 3 weeks ago 7.05MB\n\nThe alpine container image comes with an sh shell you can use to troubleshoot the process running inside of the container. You can use the following command to open an interactive shell in a new container:\n\n$ docker run -it alpine:3.17.0 /bin/sh / # exit\n\nWhile runtime troubleshooting functionality can be useful, offering a shell as part of a container image increases the size of it and potentially opens the door for attackers. Additionally, the more software there is inside of a container image, the more vulnerabilities it will have.\n\nYou can further reduce the container image size and the attack surface by using a distroless image offered by Google. The following command downloads the latest tag of the container image gcr.io/distroless/static-debian11 and renders its details. The size of the container image is only 2.34MB:\n\n$ docker pull gcr.io/distroless/static-debian11 ... $ docker image ls gcr.io/distroless/static-debian11:latest REPOSITORY TAG IMAGE ID CREATED \\ SIZE gcr.io/distroless/static-debian11 latest 901590160d4d 53 years ago \\ 2.34MB\n\nA distroless container image does not ship with any shell, which you can observe by running the following command:\n\n$ docker run -it gcr.io/distroless/static-debian11:latest /bin/sh docker: Error response from daemon: failed to create shim task: OCI runtime \\ create failed: runc create failed: unable to start container process: exec: \\ \"/bin/sh\": stat /bin/sh: no such file or directory: unknown.\n\nKubernetes offers the concept of ephemeral containers for troubleshooting distroless containers. Those containers are meant to be disposable and can be deployed for troubleshooting minimal containers that would usually not allow opening a shell. Discussing ephemeral containers is out of scope of this book, but you can find more information about them in the Kubernetes documentation.\n\nUsing a Multi-Stage Approach for Building Container Images\n\nAs a developer, you can decide to build the application code as part of the instructions in a Dockerfile. This process may include compiling the code and building a binary that should become the entry point of the container image. Having all the necessarily tools and dependencies available to implement the process will automatically blow up the size of the container image, plus you won’t need those dependencies at runtime anymore.\n\nThe idea of a multi-stage build in Docker is that you separate the build stage from the runtime stage. As a result, all dependencies needed in the build stage will be discarded after the process has been performed and therefore do not end up in the final container image. This approach leads to a much smaller container image size by removing all the unnecessary cruft.\n\nWhile we won’t go into the details of crafting and fully understanding multi-stage Dockerfile, I want to show you the differences on a high level. We’ll start by showing you a Dockerfile that builds and tests a simple program using the programming language Go, as shown in Example 6-1. In essence, we are using a base image that includes Go 1.19.4. The Go runtime provides the go executes, which we’ll invoke to execute the tests and build the binary of the application.\n\nExample 6-1. Building and testing a Go program using a Go base image FROM golang:1.19.4-alpine WORKDIR /app\n\nCOPY go.mod . COPY go.sum . RUN go mod download\n\ng\n\nCOPY . . RUN CGO_ENABLED=0 go test -v RUN go build -o /go-sample-app . CMD [\"/go-sample-app\"]\n\nUses a Go base image\n\nExecutes the tests against the application code\n\nBuilds the binary of the Go application\n\nYou can produce the image using the docker build command, as shown in the following:\n\n$ docker build . -t go-sample-app:0.0.1 ...\n\nThe size of the result container image is pretty big, 348MB, and there’s a good reason for it. It includes the Go runtime, even though we don’t actually need it anymore when starting the container. The go build command produced the binary that we can run as the container’s entry point:\n\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE go-sample-app 0.0.1 88175f3ab0d3 44 seconds ago 358MB\n\nNext up, we’ll have a look at the multi-stage approach. In a multi-stage Dockerfile, you define at least two stages. In Example 6-2, we specify a stage aliased with build to run the tests and build the binary, similarly to what we’ve done earlier. A second stage copies the binary produced by the stage build into a dedicated directory; however, it uses the alpine base image to run it. When running the docker build command, the stage build will not be included in the final container image anymore.\n\nExample 6-2. Building and testing a Go program as part of a multi-stage Dockerfile FROM golang:1.19.4-alpine AS build RUN apk add --no-cache git WORKDIR /tmp/go-sample-app COPY go.mod . COPY go.sum . RUN go mod download COPY . .\n\nRUN CGO_ENABLED=0 go test -v RUN go build -o ./out/go-sample-app .\n\nFROM alpine:3.17.0 RUN apk add ca-certificates COPY --from=build /tmp/go-sample-app/out/go-sample-app /app/go-sample-app CMD [\"/app/go-sample-app\"]\n\nUses a Go base image for building and testing the program in the stage named build.\n\nExecutes the tests against the application code.\n\nBuilds the binary of the Go application.\n\nUses a much smaller base image for running the application in a container.\n\nCopies the application binary produced in the build stage and uses it as the command to run when the container is started.\n\nThe resulting container image size is significantly smaller when using the alpine base image, only 12MB. You can verify the outcome by running the docker build command again and inspecting the size of the container image by listing it:\n\n$ docker build . -t go-sample-app:0.0.1 ... $ docker images\n\nREPOSITORY TAG IMAGE ID CREATED SIZE go-sample-app 0.0.1 88175f3ab0d3 44 seconds ago 12MB\n\nNot only did we reduce the size, we also reduced the attack surface by including fewer dependencies. You can further reduce the size of the container image by incorporating a distroless base image instead of the alpine base image.\n\nReducing the Number of Layers\n\nEvery Dockerfile consists of an ordered list of instructions. Only some instructions create a read-only layer in the resulting container image. Those instructions are FROM, COPY, RUN, and CMD. All other instructions will not create a layer as they create temporary intermediate images. The more layers you add to the container image, the slower will be the build execution time and/or the bigger will be the size of the container image. Therefore, you need to be cautious about the instructions used in your Dockerfile to minimize the footprint of the container image.\n\nIt’s common practice to execute multiple commands in a row. You may define those commands using a list of RUN instructions on individual lines, as shown in Example 6-3.\n\nExample 6-3. A Dockerfile specifying multiple RUN instructions FROM ubuntu:22.10 RUN apt-get update -y RUN apt-get upgrade -y RUN apt-get install -y curl\n\nEach RUN instruction will create a layer, potentially adding to the size of the container image. It’s more efficient to string them together with && to ensure that only a single layer will be produced. Example 6-4 shows an example of this optimization technique.\n\nExample 6-4. A Dockerfile specifying multiple RUN instructions FROM ubuntu:22.10 RUN apt-get update -y && apt-get upgrade -y && apt-get install -y curl\n\nUsing Container Image Optimization Tools\n\nIt’s easy to forget about the optimization practices mentioned previously. The open source community developed a couple of tools that can help with inspecting a produced container image. Their functionalities provide useful tips for pairing down on unnecessary layers, files, and folders:\n\nDockerSlim\n\nDockerSlim will optimize and secure your container image by analyzing your application and its dependencies. You can find more information in the tool’s GitHub repository.\n\nDive\n\nDive is a tool for exploring the layers baked into a container image. It makes it easy to identify unnecessary layers, which you can further optimize on. The code and documentation for Dive are available in a GitHub repository.\n\nThis is only the short list of container image optimization tools. In “Static Analysis of Workload”, we’ll have a look at other tools that focus on the static analysis of Dockerfiles and Kubernetes manifests.\n\nSecuring the Supply Chain\n\nA supply chain automates the process of producing a container image and operating it in a runtime environment, in this case Kubernetes. We already touched on a couple of tools that can be integrated into the supply chain to support the aspect of container image optimization. In this section, we’ll expand on practices that target security aspects. Refer to the book Container Security by Liz Rice (O’Reilly) to learn more.\n\nSigning Container Images\n\nYou can sign a container image before pushing it to a container registry. Signing can be achieved with the docker trust sign command, which adds a signature to the container image, the so-called image digest. An image digest is derived from the contents of the container image and commonly represented in the form of SHA256. When consuming the container image, Kubernetes can compare the image digest with the contents of the image to ensure that it hasn’t been tampered with.\n\nScenario: An Attacker Injects Malicious Code into a Container Image\n\nThe Kubernetes component that verifies the image digest is the kubelet. If you configured the image pull policy to Always, the kubelet will query for the image digest from the container registry even though it may have downloaded and verified the container image before.\n\nAn attacker can try to modify the contents of an existing container image, inject malicious code, and upload it to the container registry with the same tag, as shown in Figure 6-2. The malicious code running in the container could then send sensitive information to a third-party server accessible by the attacker.\n\nFigure 6-2. An attacker injects malicious code into a container image\n\nIMAGE CHECKSUM VALIDATION IS NOT AUTOMATIC\n\nImage digest validation is an opt-in functionality in Kubernetes. When defining Pods, make sure you spell out the image digest explicitly for all container images.\n\nValidating Container Images\n\nIn Kubernetes, you’re able to append the SHA256 image digest to the specification of a container. For example, this can be achieved with the attribute spec.containers[].image. The image digest is generally available in the container registry. For example, Figure 6-3 shows the image digest for the container image alpine:3.17.0 on Docker Hub. In this example, the image digest is sha256:c0d488a800e4127c334ad20d61d7bc21b4097540327217dfab5226 2adc02380c.\n\nFigure 6-3. The image digest of the alpine:3.17.0 container image on Docker Hub\n\nLet’s see the image digest in action. Instead of using the tag, Example 6-5 specifies the container image by appending the image digest.\n\nExample 6-5. A Pod using a valid container image digest apiVersion: v1 kind: Pod metadata: name: alpine-valid spec: containers: - name: alpine image: alpine@sha256:c0d488a800e4127c334ad20d61d7bc21b40 \\ 97540327217dfab52262adc02380c command: [\"/bin/sh\"] args: [\"-c\", \"while true; do echo hello; sleep 10; done\"]\n\nCreating the Pod will work as expected. The image digest will be verified and the container transitions into the “Running” status:\n\n$ kubectl apply -f pod-valid-image-digest.yaml pod/alpine-valid created $ kubectl get pod alpine-valid NAME READY STATUS RESTARTS AGE alpine-valid 1/1 Running 0 6s\n\nExample 6-6 shows the same Pod definition; however, the image digest has been changed so that it does not match with the contents of the container image.\n\nExample 6-6. A Pod using an invalid container image digest apiVersion: v1 kind: Pod metadata: name: alpine-invalid spec: containers: - name: alpine image: alpine@sha256:d006a643bccb6e9adbabaae668533c7f2e5 \\ 111572fffb5c61cb7fcba7ef4150b command: [\"/bin/sh\"] args: [\"-c\", \"while true; do echo hello; sleep 10; done\"]\n\nYou will see that Kubernetes can still create the Pod object but it can’t properly validate the hash of the container image. This results in the status “ErrImagePull.” As you can see from the event log, the container image couldn’t even be pulled from the container registry:\n\n$ kubectl get pods NAME READY STATUS RESTARTS AGE alpine-invalid 0/1 ErrImagePull 0 29s $ kubectl describe pod alpine-invalid ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 13s default-scheduler Successfully assigned default \\ /alpine-invalid to minikube Normal Pulling 13s kubelet Pulling image \"alpine@sha256: \\ d006a643bccb6e9adbabaae668533c7f2e5111572fffb5c61cb7fcba7ef4150b\" Warning Failed 11s kubelet Failed to pull image \\ \"alpine@sha256:d006a643bccb6e9adbabaae668533c7f2e5111572fffb5c61cb7fcba7ef4 \\ 150b\": rpc error: code = Unknown desc = Error response from daemon: manifest \\ for alpine@sha256:d006a643bccb6e9adbabaae668533c7f2e5111572fffb5c61cb7fcba7e \\ f4150b not found: manifest unknown: manifest unknown Warning Failed 11s kubelet Error: ErrImagePull Normal BackOff 11s kubelet Back-off pulling image \\\n\n\"alpine@sha256:d006a643bccb6e9adbabaae668533c7f2e5111572fffb5c61cb7fcba7ef415 \\ 0b\" Warning Failed 11s kubelet Error: ImagePullBackOff\n\nUsing Public Image Registries\n\nWhenever a Pod is created, the container runtime engine will download the declared container image from a container registry if it isn’t available locally yet. This runtime behavior can be further tweaked using the image pull policy.\n\nThe prefix in the image name declares the domain name of the registry; e.g., gcr.io/google-containers/debian-base:v1.0.1 refers to the container image google-containers/debian-base:v1.0.1 in the Google Cloud container registry, denoted by gcr.io. The container runtime will try to resolve it from docker.io, the Docker Hub container registry if you leave off the domain name in the container image declaration.\n\nScenario: An Attacker Uploads a Malicious Container Image\n\nWhile it is convenient to resolve container images from public container registries, it doesn’t come without risks. Anyone with a login to those container registries can upload new images. Consuming container images usually doesn’t even require an account.\n\nAs shown in Figure 6-4, an attacker can upload a container image containing malicious code to a public registry using stolen credentials. Any container referencing the container image from that registry will automatically run the malicious code.\n\nFigure 6-4. An attacker uploads a malicious container image\n\nOn an enterprise level, you need to control which container registries you trust. It’s recommended to set up your own container registry within your company’s network, which you can fully control and govern. Alternatively, you can set up a private container registry in a cloud provider environment, not accessible by anyone else.\n\nOne of the prominent binary repository managers you can choose from is JFrog Artifactory. The product fully supports storing, scanning, and serving container images. Any consumer of container images should only be allowed to pull images from your whitelisted container registry. All other container registries should be denied.\n\nWhitelisting Allowed Image Registries with OPA GateKeeper\n\nOne way to govern container registry usage is with OPA Gatekeeper. We discussed the installation process and functionality of OPA Gatekeeper in “Understanding Open Policy Agent (OPA) and Gatekeeper”. Here, we’ll’ touch on the constraint template and constraint for allowing trusted container registries.\n\nExample 6-7 shows the constraint template I got directly from the OPA Gatekeeper library. As input properties, the constraint template defines an array of strings representing the prefixes of container registries. The Rego rules verify not only the assigned container images for the attribute spec.containers[] but also spec.initContainers[] and spec.ephemeralContainers[].\n\nExample 6-7. An OPA Gatekeeper constraint template for enforcing container registries apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: k8sallowedrepos annotations: metadata.gatekeeper.sh/title: \"Allowed Repositories\" metadata.gatekeeper.sh/version: 1.0.0 description: >- Requires container images to begin with a string from the specified list. spec: crd: spec: names: kind: K8sAllowedRepos validation: openAPIV3Schema: type: object properties: repos: description: The list of prefixes a container image is allowed to have. type: array items: type: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8sallowedrepos\n\nviolation[{\"msg\": msg}] { container := input.review.object.spec.containers[_] satisfied := [good | repo = input.parameters.repos[_] ; \\ good = startswith(container.image, repo)] not any(satisfied) msg := sprintf(\"container <%v> has an invalid image repo <%v>, allowed \\ repos are %v\", [container.name, container.image, input.parameters.repos]) }\n\nviolation[{\"msg\": msg}] { container := input.review.object.spec.initContainers[_] satisfied := [good | repo = input.parameters.repos[_] ; \\ good = startswith(container.image, repo)] not any(satisfied) msg := sprintf(\"initContainer <%v> has an invalid image repo <%v>, \\ allowed repos are %v\", [container.name, container.image, \\ input.parameters.repos]) }\n\nviolation[{\"msg\": msg}] { container := input.review.object.spec.ephemeralContainers[_] satisfied := [good | repo = input.parameters.repos[_] ; \\ good = startswith(container.image, repo)] not any(satisfied) msg := sprintf(\"ephemeralContainer <%v> has an invalid image repo <%v>, \\ allowed repos are %v\", [container.name, container.image, \\ input.parameters.repos]) }\n\nThe constraint shown in Example 6-8 is in charge of defining which container registries we want to allow. You’d usually go with a domain name of a server hosted in your company’s network. Here, we are going to use gcr.io/ for demonstration purposes.\n\nExample 6-8. An OPA Gatekeeper constraint assigning Google Cloud registry as trusted repository apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sAllowedRepos metadata: name: repo-is-gcr spec:\n\nmatch: kinds: - apiGroups: [\"\"] kinds: [\"Pod\"] parameters: repos: - \"gcr.io/\"\n\nLet’s create both objects using the apply command:\n\n$ kubectl apply -f allowed-repos-constraint-template.yaml constrainttemplate.templates.gatekeeper.sh/k8sallowedrepos created $ kubectl apply -f gcr-allowed-repos-constraint.yaml k8sallowedrepos.constraints.gatekeeper.sh/repo-is-gcr created\n\nIn the constraint, we didn’t specify a namespace that the rules should apply to. Therefore, they’ll apply across all namespaces in the cluster. The following commands verify that the whitelisting rules work as expected. The following command tries to create a Pod using the nginx container image from Docker Hub. The creation of the Pod is denied with an appropriate error message:\n\n$ kubectl run nginx --image=nginx:1.23.3 Error from server (Forbidden): admission webhook \"validation.gatekeeper.sh\" \\ denied the request: [repo-is-gcr] container <nginx> has an invalid image \\ repo <nginx:1.23.3>, allowed repos are [\"gcr.io/\"]\n\nThe next command creates a Pod with a container image from the Google Cloud container registry. The operation is permitted and the Pod object is created:\n\n$ kubectl run busybox --image=gcr.io/google-containers/busybox:1.27.2 pod/busybox created $ kubectl get pods NAME READY STATUS RESTARTS AGE busybox 0/1 Completed 1 (2s ago) 3s\n\nWhitelisting Allowed Image Registries with the ImagePolicyWebhook Admission Controller Plugin\n\nAnother way to validate the use of allowed image registries is to intercept a call to the API server when a Pod is about to be created. This mechanism can be achieved by enabling an admission controller plugin. Once configured, the logic of an admission controller plugin is automatically invoked when the API server receives the request, but after it could authenticate and authorize the caller. We already touched on the phases an API call has to go through in “Processing a Request”.\n\nThe admission controller provides a way to approve, deny, or mutate a request before the request takes effect. For example, we can inspect the data sent with the API request to create a Pod, iterate over the assigned container images, and execute custom logic to validate the container image notation. If the container image doesn’t stick to the expected conventions, we can deny the creation of the Pod. Figure 6-5 illustrates the high-level workflow.\n\nFigure 6-5. Intercepting a Pod-specific API call and handling it with a webhook\n\nThe ImagePolicyWebhook admission controller plugin is one of the plugins we can configure for intercepting Kubernetes API calls. You can probably derive the plugin’s functionality from its name. It defines a policy for all defined container images in a Pod. To compare container images with the defined policy, the plugin calls off to a service external to Kubernetes via HTTPS, a webhook. The external service then makes the decision on how to validate the data. In the context of the admission controller plugin, the external service is also referred to as backend.\n\nImplementing the Backend Application\n\nThe backend application can be implemented with a programming language of your choice. There are only three requirements it must fulfill:\n\n1. It’s a web application that can handle HTTPS requests.\n\n2. It can accept and parse JSON request payloads.\n\n3. It can send a JSON response payload.\n\nImplementing the backend application is not part of the CKS exam, but you can find a sample Go-based implementation in the GitHub repository of this book. Be aware that the code is not considered to be production-ready.\n\nThe following commands demonstrate the runtime behavior of the application accessible at https://localhost:8080/validate. You can find an example request and response JSON body in the Kubernetes documentation.\n\nThe following curl command calls the validation logic for the container image nginx:1.19.0. As you can see from the JSON response, the image is denied:\n\n$ curl -X POST -H \"Content-Type: application/json\" -k -d \\'{\"apiVersion\": \\ \"imagepolicy.k8s.io/v1alpha1\", \"kind\": \"ImageReview\", \"spec\": \\ {\"containers\": [{\"image\": \"nginx:1.19.0\"}]}}' https://localhost:8080/validate {\"apiVersion\": \"imagepolicy.k8s.io/v1alpha1\", \"kind\": \"ImageReview\", \\ \"status\": {\"allowed\": false, \"reason\": \"Denied request: [container 1 \\ has an invalid image repo nginx:1.19.0, allowed repos are [gcr.io/]]\"}}\n\nThe following curl command calls the validation logic for the container image gcr.io/nginx:1.19.0. As you can see from the JSON response, the image is allowed:\n\n$ curl -X POST -H \"Content-Type: application/json\" -k -d '{\"apiVersion\": \\ \"imagepolicy.k8s.io/v1alpha1\", \"kind\": \"ImageReview\", \"spec\": {\"containers\": \\ [{\"image\": \"gcr.io/nginx:1.19.0\"}]}}' https://localhost:8080/validate {\"apiVersion\": \"imagepolicy.k8s.io/v1alpha1\", \"kind\": \"ImageReview\", \\ \"status\": {\"allowed\": true, \"reason\": \"\"}}\n\nConfiguring the ImagePolicyWebhook Admission Controller Plugin\n\nFor the exam, you are expected to understand how to “wire” the ImagePolicyWebhook admission controller plugin. This section will walk you through the process. First, you’ll need to create a configuration file for the admission controller so it knows what plugins to use and how it should behave at runtime. Create the file /etc/kubernetes/admission- control/image-policy-webhook-admission-config.yaml and populate it with the content shown in Example 6-9.\n\nExample 6-9. The admission controller configuration file apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: ImagePolicyWebhook configuration: imagePolicy: kubeConfigFile: /etc/kubernetes/admission-control/ \\ imagepolicywebhook.kubeconfig allowTTL: 50 denyTTL: 50 retryBackoff: 500 defaultAllow: false\n\nProvides the configuration for the ImagePolicyWebhook plugin.\n\nPoints to the configuration file used to configure the backend.\n\nDenies an API request if the backend cannot be reached. The default is true but setting it to false is far more sensible.\n\nNext, create the file referenced by the plugins[].configuration.imagePolicy.kubeConfigFile attribute. The contents of the file point to the HTTPS URL of the external service. It also defines the client certificate and key file, as well as the CA file on disk. Example 6-10 shows the contents of the configuration file.\n\nExample 6-10. The image policy configuration file\n\napiVersion: v1 kind: Config preferences: {} clusters: - name: image-validation-webhook cluster: certificate-authority: /etc/kubernetes/admission-control/ca.crt server: https://image-validation-webhook:8080/validate contexts: - context: cluster: image-validation-webhook user: api-server-client name: image-validation-webhook current-context: image-validation-webhook users: - name: api-server-client user: client-certificate: /etc/kubernetes/admission-control/api-server-client.crt client-key: /etc/kubernetes/admission-control/api-server-client.key\n\nThe URL to the backend service. Must use the HTTPS protocol.\n\nEnable the ImagePolicyWebhook admission controller plugin for the API server and point the admission controller to the configuration file. To achieve this, edit the configuration file of the API server, usually found at /etc/kubernetes/manifests/kube-apiserver.yaml.\n\nFind the command line option --enable-admission-plugins and append the value ImagePolicyWebhook to the existing list of plugins, separated by a comma. Provide the command line option --admission-control- config-file if it doesn’t exist yet, and set the value to /etc/kubernetes/admission-control/image-policy-webhook- admission-configuration.yaml. Given that the configuration file lives in a new directory, you will have to define it as a Volume and mount it to the container. Example 6-11 shows the relevant options for the API server container.\n\nExample 6-11. The API server configuration file ... spec: containers:\n\ncommand: - kube-apiserver - --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook - --admission-control-config-file=/etc/kubernetes/admission-control/ \\ image-policy-webhook-admission-configuration.yaml ... volumeMounts: ... - name: admission-control mountPath: /etc/kubernetes/admission-control readonly: true volumes: ... - name: admission-control hostPath: path: /etc/kubernetes/admission-control type: DirectoryOrCreate ... The Pod running the API server should automatically restart. This process may take a couple of minutes. Restart the kubelet service with the command sudo systemctl restart kubelet should the API server not come up by itself. Once fully restarted, you should be able to query for it:\n\n$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE ... kube-apiserver-control-plane 1/1 Running 0 69s\n\nAny API call that requests the creation of a Pod will now be routed to the backend. Based on the validation result, the creation of the object will be allowed or denied.\n\nStatic Analysis of Workload\n\nThroughout this book, we talk about best practices for Dockerfiles and Kubernetes manifests. You can inspect those files manually, find undesired configurations, and fix them by hand. This approach requires a lot of intricate knowledge and is very tedious and error-prone. It is much more convenient and efficient to analyze workload files in an automated fashion\n\nwith proper tooling. The list of commercial and open source tooling for static analysis is long. In this section, we are going to present the functionality of two options, Haskell Dockerfile Linter and Kubesec.\n\nOn an enterprise level, where you have to process hundreds or even thousands of workload files, you’d do so with the help of a continuous delivery pipeline, as shown in Figure 6-6.\n\nFigure 6-6. An exemplary continuous delivery pipeline for Kubernetes\n\nRelevant static analysis tools can be invoked as a quality gate at an early stage of the pipeline even before a container image is built, pushed to a registry, or deployed to a Kubernetes cluster. For a deep dive on the principles and practices of continuous delivery, see the excellent book Continuous Delivery by Jez Humble and David Farley (Addison-Wesley Professional).\n\nUsing Hadolint for Analyzing Dockerfiles\n\nHaskell Dockerfile Linter, also called hadolint in short, is a linter for Dockerfiles. The tool inspects a Dockerfile based on best practices listed on the Docker documentation page. Example 6-12 shows an unoptimized Dockerfile for building a container image running a Go-based application.\n\nExample 6-12. An unoptimized Dockerfile FROM golang COPY main.go . RUN go build main.go CMD [\"./main\"]\n\nThe Haskell Dockerfile Linter supports a mode of operation that lets you point the hadolint executable to a Dockerfile on disk. You can see the command execution that follows, including the discovered warning messages produced by the analysis:\n\n$ hadolint Dockerfile Dockerfile:1 DL3006 warning: Always tag the version of an image explicitly Dockerfile:2 DL3045 warning: `COPY` to a relative destination without \\ `WORKDIR` set.\n\nThe output of the command suggests that you should assign a tag to the base image. The existing Dockerfile will pull the latest tag, which will resolve to the newest Go container image. This practice can result in an incompatibility between the Go runtime version and the application code. Defining a working directory for copying resources helps with keeping operating system-specific directories and files separate from application- specific directories and files. Example 6-13 fixes the warning messages found by the Haskell Dockerfile Linter.\n\nExample 6-13. An optimized Dockerfile FROM golang:1.19.4-alpine WORKDIR /app COPY main.go . RUN go build main.go CMD [\"./main\"]\n\nAnother execution against the modified Dockerfile leads to a successful exit code, and no additional warning messages will be rendered:\n\n$ hadolint Dockerfile\n\nThe Dockerfile now follows best practices, as perceived by hadolint.\n\nUsing Kubesec for Analyzing Kubernetes Manifests\n\nKubesec is a tool for analyzing Kubernetes manifests. It can be executed as a binary, Docker container, admission controller plugin, and even as a plugin for kubectl. To demonstrate its use, we’ll set up a YAML manifest file pod-initial-kubesec-test.yaml, shown in Example 6-14 as a starting point.\n\nExample 6-14. A Pod YAML manifest using initial security settings apiVersion: v1 kind: Pod\n\nmetadata: name: kubesec-demo spec: containers: - name: kubesec-demo image: gcr.io/google-samples/node-hello:1.0 securityContext: readOnlyRootFilesystem: true\n\nUpon inspecting the Pod configuration, you may already have some suggestions on what could be improved based on the content of the previous chapters. Let’s see what Kubesec is going to recommend.\n\nThe simplest way to invoke Kubesec is to run the logic in a container with the help of Docker. The following command feeds the contents of the YAML manifest to the standard input stream. The result, formatted in JSON, renders a score, provides a human-readable message of the outcome, and advises on changes to be made:\n\n$ docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin \\ < pod-initial-kubesec-test.yaml [ { \"object\": \"Pod/kubesec-demo.default\", \"valid\": true, \"message\": \"Passed with a score of 1 points\", \"score\": 1, \"scoring\": { \"advise\": [ { \"selector\": \".spec .serviceAccountName\", \"reason\": \"Service accounts restrict Kubernetes API access and \\ should be configured with least privilege\" }, { \"selector\": \".metadata .annotations .\\\"container.apparmor.security. \\ beta.kubernetes.io/nginx\\\"\", \"reason\": \"Well defined AppArmor policies may provide greater \\ protection from unknown threats. WARNING: NOT PRODUCTION \\ READY\" }, { \"selector\": \"containers[] .resources .requests .cpu\",\n\n\"reason\": \"Enforcing CPU requests aids a fair balancing of \\ resources across the cluster\" }, { \"selector\": \".metadata .annotations .\\\"container.seccomp.security. \\ alpha.kubernetes.io/pod\\\"\", \"reason\": \"Seccomp profiles set minimum privilege and secure against \\ unknown threats\" }, { \"selector\": \"containers[] .resources .limits .memory\", \"reason\": \"Enforcing memory limits prevents DOS via resource \\ exhaustion\" }, { \"selector\": \"containers[] .resources .limits .cpu\", \"reason\": \"Enforcing CPU limits prevents DOS via resource exhaustion\" }, { \"selector\": \"containers[] .securityContext .runAsNonRoot == true\", \"reason\": \"Force the running image to run as a non-root user to \\ ensure least privilege\" }, { \"selector\": \"containers[] .resources .requests .memory\", \"reason\": \"Enforcing memory requests aids a fair balancing of \\ resources across the cluster\" }, { \"selector\": \"containers[] .securityContext .capabilities .drop\", \"reason\": \"Reducing kernel capabilities available to a container \\ limits its attack surface\" }, { \"selector\": \"containers[] .securityContext .runAsUser -gt 10000\", \"reason\": \"Run as a high-UID user to avoid conflicts with the \\ host's user table\" }, { \"selector\": \"containers[] .securityContext .capabilities .drop | \\ index(\\\"ALL\\\")\", \"reason\": \"Drop all capabilities and add only those required to \\ reduce syscall attack surface\" } ] }\n\n} ]\n\nA touched-up version of the original YAML manifest can be found in Example 6-15. Here, we incorporated some of the recommended changes proposed by Kubesec.\n\nExample 6-15. A Pod YAML manifest using improved security settings apiVersion: v1 kind: Pod metadata: name: kubesec-demo spec: containers: - name: kubesec-demo image: gcr.io/google-samples/node-hello:1.0 resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 20000 capabilities: drop: [\"ALL\"]\n\nExecuting the same Docker command against the changed Pod YAML manifest will render an improved score and reduce the number of advised messages:\n\n$ docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin \\ < pod-improved-kubesec-test.yaml [ { \"object\": \"Pod/kubesec-demo.default\", \"valid\": true, \"message\": \"Passed with a score of 9 points\", \"score\": 9, \"scoring\": { \"advise\": [\n\n{ \"selector\": \".metadata .annotations .\\\"container.seccomp.security. \\ alpha.kubernetes.io/pod\\\"\", \"reason\": \"Seccomp profiles set minimum privilege and secure against \\ unknown threats\" }, { \"selector\": \".spec .serviceAccountName\", \"reason\": \"Service accounts restrict Kubernetes API access and should \\ be configured with least privilege\" }, { \"selector\": \".metadata .annotations .\\\"container.apparmor.security. \\ beta.kubernetes.io/nginx\\\"\", \"reason\": \"Well defined AppArmor policies may provide greater \\ protection from unknown threats. WARNING: NOT PRODUCTION \\ READY\" } ] } } ]\n\nScanning Images for Known Vulnerabilities\n\nOne of the top sources for logging and discovering security vulnerabilities is CVE Details. The page lists and ranks known vulnerabilities (CVEs) by score. Automated tooling can identify the software components embedded in a container image, compare those with the central vulnerabilities database, and flag issues by their severity.\n\nOne of the open source tools with this capability explicitly mentioned in the CKS curriculum is Trivy. Trivy can run in different modes of operation: as a command line tool, in a container, as a GitHub Action configurable in a continuous integration workflow, as a plugin for the IDE VSCode, and as a Kubernetes operator. For an overview of the available installation options and procedures, see the Trivy documentation. During the exam, you are not expected to install the tool. It will already be preconfigured for you. All you\n\nneed to understand is how to run it and how to interpret and fix the found vulnerabilities.\n\nSay you installed the command line implementation of Trivy. You can check the version of Trivy with the following command:\n\n$ trivy -v Version: 0.36.1 Vulnerability DB: Version: 2 UpdatedAt: 2022-12-13 12:07:14.884952254 +0000 UTC NextUpdate: 2022-12-13 18:07:14.884951854 +0000 UTC DownloadedAt: 2022-12-13 17:09:28.866739 +0000 UTC\n\nAs you can see in Figure 6-7, Trivy indicates the timestamp when a copy of known vulnerabilities has been downloaded from the central database. Trivy can scan container images in various forms. The subcommand image simply expects you to spell out the image name and tag, in this case python:3.4-alpine.\n\nFigure 6-7. Reporting generated by scanning a container image with Trivy\n\nThe most important information in the output consists of the library that contains a specific vulnerability, its severity, and the minimum version to\n\nuse that fixes the issue. Any found vulnerabilities with high or critical severity should be considered for a fix. If you do not have any control over the container image yourself, you can try to upgrade to a newer version.\n\nSummary\n\nKubernetes primary objective is to run applications in containers in a scalable and secure fashion. In this chapter, we looked at the process, best practices, and tooling that help to ensure that a container image is produced that is small in size and that ships with as few known security vulnerabilities as possible.\n\nWe reviewed some of the most efficient techniques to reduce the footprint of a container image to a minimum. Start by picking a small base image to start with. You can even go to the extreme and not provide a shell at all for additional gains in size reduction. If you are using Docker, you can leverage the multi-stage approach that lets you build the application inside of the container without bundling the compiler, runtime, and build tools necessary to achieve the goal.\n\nWhen consuming the container image in a Pod, make sure to only pull the container image from a trusted registry. It’s advisable to set up an in-house container registry to serve up container images, so that reaching out to public, internet-accessible registries becomes obsolete, to eliminate potential security risks. You can enforce the usage of a list of trusted container registries with the help of OPA Gatekeeper. Another measure of security can be enforced by using the SHA256 hash of a container image to validate its expected contents.\n\nThe process of building and scanning a container image can be incorporated into a CI/CD pipeline. Third-party tools can parse and analyze the resource files of your deployable artifacts even before you build them. We looked at Haskell Dockerfile Linter for Dockerfiles and Kubesec for Kubernetes manifests. The other use case that needs to be covered on security aspects is consuming an existing container image built by an entity external to you as a developer, or your company. Before running a container image in a\n\nKubernetes Pod, make sure to scan the contents for vulnerabilities. Trivy is one of those tools that can identify and report vulnerabilities in a container image to give you an idea of the security risks you are exposing yourself to in case you are planning to operate it in a container.\n\nExam Essentials\n\nBecome familiar with techniques that help with reducing the container image footprint.\n\nIn this section, we described some techniques for reducing the size of a container image when building it. I would suggest you read the best practices mentioned on the Docker web page and try to apply them to sample container images. Compare the size of the produced container image before and after applying a technique. You can try to challenge yourself by reducing a container image to the smallest size possible while at the same time avoiding the loss of crucial functionality.\n\nWalk through the process of governing where a container image can be resolved from.\n\nOPA Gatekeeper offers a way to define the registries users are allowed to resolve container images from. Set up the objects for the constraint template and constraint, and see if the rules apply properly for a Pod that defines a main application container, an init container, and an ephemeral container. To broaden your exposure, also look at other products in the Kubernetes spaces that provide similar functionality. One of those products is Kyverno.\n\nSign a container image and verify it using the hash.\n\nAfter building a container image, make sure to also create a digest for it. Publish the container image, as well as the digest, to a registry. Practice how to use the digest in a Pod definition and verify the behavior of Kubernetes upon pulling the container image.\n\nUnderstand how to configure the ImagePolicyWebhook admission controller plugin.\n\nYou are not expected to write a backend for an ImagePolicyWebhook. That’s out of scope for the exam and requires knowledge of a programming language. You do need to understand how to enable the plugin in the API server configuration, though. I would suggest you practice the workflow even if you don’t have a running backend application available.\n\nKnow how to fix warnings produced by static analysis tools.\n\nThe CKS curriculum doesn’t prescribe a specific tool for analyzing Dockerfiles and Kubernetes manifests. During the exam, you may be asked to run a specific command that will produce a list of error and/or warning messages. Understand how to interpret the messages, and how to fix them in the relevant resource files.\n\nPractice the use of Trivy to identify and fix security vulnerabilities.\n\nThe FAQ of the CKS lists the documentation page for Trivy. Therefore, it’s fair to assume that Trivy may come up in one of the questions. You will need to understand the different ways to invoke Trivy to scan a container image. The produced report will give a you clear indication on what needs to be fixed and the severity of the found vulnerability. Given that you can’t modify the container image easily, you will likely be asked to flag Pods that run container images with known vulnerabilities.\n\nSample Exercises\n\nSolutions to these exercises are available in the Appendix.\n\n1. Have a look at the following Dockerfile. Can you identify possibilities for reducing the footprint of the produced container image?\n\nFROM node:latest\n\nENV NODE_ENV development\n\nWORKDIR /app\n\nCOPY package.json .\n\nRUN npm install\n\nCOPY . .\n\nEXPOSE 3001\n\nCMD [\"node\", \"app.js\"]\n\nApply Dockerfile best practices to optimize the container image footprint. Run the docker build command before and after making optimizations. The resulting size of the container image should be smaller but the application should still be functioning.\n\n2. Install Kyverno in your Kubernetes cluster. You can find installation instructions in the documentation.\n\nApply the “Restrict Image Registries” policy described on the documentation page. The only allowed registry should be gcr.io. Usage of any other registry should be denied.\n\nCreate a Pod that defines the container image gcr.io/google- containers/busybox:1.27.2. Creation of the Pod should fail. Create another Pod using the container image busybox:1.27.2. Kyverno should allow the Pod to be created.\n\n3. Define a Pod using the container image nginx:1.23.3-alpine in the YAML manifest pod-validate-image.yaml. Retrieve the digest of the container image from Docker Hub. Validate the container image contents using the SHA256 hash. Create the Pod. Kubernetes should be able to successfully pull the container image.\n\n4. Use Kubesec to analyze the following contents of the YAML manifest file pod.yaml:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: hello-world\n\nspec:\n\nsecurityContext:\n\nrunAsUser: 0\n\ncontainers:\n\nname: linux\n\nimage: hello-world:linux\n\nInspect the suggestions generated by Kubesec. Ignore the suggestions on seccomp and AppArmor. Fix the root cause of all messages so that another execution of the tool will not report any additional suggestions.\n\n5. Navigate to the directory app-a/ch06/trivy of the checked-out GitHub repository bmuschko/cks-study-guide. Execute the command kubectl apply -f setup.yaml. The command creates three different Pods in the namespace r61. From the command line, execute Trivy against the container images used by the Pods. Delete all Pods that have “CRITICAL” vulnerabilities. Which of the Pods are still running?\n\nOceanofPDF.com\n\nChapter 7. Monitoring, Logging, and Runtime Security\n\nThe last domain of the curriculum primarily deals with detecting suspicious activity on the host and container level in a Kubernetes cluster. We’ll first define the term behavior analytics and how it applies to the realm of Kubernetes. With the theory out of the way, we’ll bring in the tool called Falco that can detect intrusion scenarios.\n\nOnce a container has been started, its runtime environment can still be modified. For example, as an operator you could decide to shell into the container in order to manually install additional tools or write files to the container’s temporary filesystem. Modifying a container after it has been started can open doors to security risks. You will want to aim for creating immutable containers, containers that cannot be modified after they have been started. We’ll learn how to configure a Pod with the right settings to make its containers immutable.\n\nLast, we’ll talk about capturing logs for events that occur in a Kubernetes cluster. Those logs can be used for troubleshooting purposes on the cluster level, to reconstruct when and how the cluster configuration was changed such that it led to an undesired or broken runtime behavior. Log entries can also be used to trace an attack that may be happening right now as a means to enacting countermeasures.\n\nAt a high level, this chapter covers the following concepts:\n\nPerforming behavior analytics to detect malicious activities\n\nPerforming deep analytical investigation and identification\n\nEnsuring immutability of containers at runtime\n\nUsing audit logs to monitor access",
      "page_number": 153
    },
    {
      "number": 7,
      "title": "Monitoring, Logging, and Runtime Security",
      "start_page": 188,
      "end_page": 298,
      "detection_method": "regex_chapter_title",
      "content": "Performing Behavior Analytics\n\nApart from managing and upgrading a Kubernetes cluster, the administrator is in charge of keeping an eye on potentially malicious activity. While you can perform this task manually by logging into cluster nodes and observing host- and container-level processes, it is a horribly inefficient undertaking.\n\nBehavior analytics is the process of observing the cluster nodes for any activity that seems out of the ordinary. An automated process helps with filtering, recording, and alerting events of specific interest.\n\nScenario: A Kubernetes Administrator Can Observe Actions Taken by an Attacker\n\nAn attacker gained access to a container by opening a shell running on a worker node to launch additional attacks throughout the Kubernetes cluster. An administrator can’t easily detect this event by manually checking each and every container. Figure 7-1 illustrates the scenario.\n\nFigure 7-1. Malicious events recorded by behavior analytics tool\n\nThe administrator decided to take matters in their own hands by installing a behavior analytics tool. The tool will continuously monitor certain events and record them almost instantaneously. The administrator now has an efficient mechanism for detecting intrusions and can act upon them.\n\nAmong the behavior analytics tools relevant to the exam are Falco, Tracee, and Tetragon. In this book, we’ll only focus on Falco, as it is listed among the links of documentation pages available during the exam.\n\nUnderstanding Falco\n\nFalco helps with detecting threats by observing host- and container-level activity. Here are a few examples of events Falco could watch for:\n\nReading or writing files at specific locations in the filesystem\n\nOpening a shell binary for a container, such as /bin/bash to open a bash shell\n\nAn attempt to make a network call to undesired URLs\n\nFalco deploys a set of sensors that listen for the configured events and conditions. Each sensor consists of a set of rules that map an event to a data source. An alert is produced when a rule matches a specific event. Alerts will be sent to an output channel to record the event, such as standard output, a file, or an HTTPS endpoint. Falco allows for enabling more than one output channel simultaneously. Figure 7-2 shows Falco’s high-level architecture.\n\nFigure 7-2. Falco’s high-level architecture\n\nFalco is a tool with a lot of features and configuration options. We won’t be able to discuss all features in this book, but I would suggest you spend some time on understanding Falco’s high-level concepts.\n\nAnother great learning resource on Falco can be found on the Sysdig training portal webpage. “Falco 101” is a free video course that explains all the bells and whistles of the product. All you need to do to get started is sign up for an account. Moreover, I’d suggest giving the book Practical Cloud Native Security with Falco by Loris Degioanni and Leonardo Grasso (O’Reilly) a read. The content takes a beginner-friendly approach to learning Falco.\n\nInstalling Falco\n\nFalco can be installed as a binary on the host system or as a DaemonSet object in Kubernetes. You can safely assume that Falco has been preinstalled for you in the exam environment. For more information on the installation process, have a look at the relevant portion of the Falco documentation. The following steps briefly explain the installation of the binary on an Ubuntu machine. Falco needs to be installed on all worker nodes of a Kubernetes cluster. Be aware that those instructions may change with a future version of Falco.\n\nFirst, you need to trust the Falco GPG key, configure the Falco-specific apt repository, and update the package list:\n\n$ curl -s https://falco.org/repo/falcosecurity-packages.asc | apt-key add - $ echo \"deb https://download.falco.org/packages/deb stable main\" | tee -a \\ /etc/apt/sources.list.d/falcosecurity.list $ apt-get update -y\n\nYou then install the kernel header with the following command:\n\n$ apt-get -y install linux-headers-$(uname -r)\n\nLast, you need to install the Falco apt package with version 0.33.1:\n\n$ apt-get install -y falco=0.33.1\n\nFalco has been installed successfully and is running as a systemd service in the background. Run the following command to check on the status of the\n\nFalco service:\n\n$ sudo systemctl status falco ● falco.service - Falco: Container Native Runtime Security Loaded: loaded (/lib/systemd/system/falco.service; enabled; vendor preset: \\ enabled) Active: active (running) since Tue 2023-01-24 15:42:31 UTC; 43min ago Docs: https://falco.org/docs/ Main PID: 8718 (falco) Tasks: 12 (limit: 1131) Memory: 30.2M CGroup: /system.slice/falco.service └─8718 /usr/bin/falco --pidfile=/var/run/falco.pid\n\nThe Falco service should be in the “active” status. It is already monitoring events in your system.\n\nConfiguring Falco\n\nThe Falco service provides an operational environment for monitoring the system with a set of default rules. Those rules live in YAML files in the /etc/falco directory. The list of files and subdirectories in /etc/falco is as follows:\n\n$ tree /etc/falco /etc/falco ├── aws_cloudtrail_rules.yaml ├── falco.yaml ├── falco_rules.local.yaml ├── falco_rules.yaml ├── k8s_audit_rules.yaml ├── rules.available │ └── application_rules.yaml └── rules.d\n\nOf those files, I want to describe the high-level purpose of the most important ones.\n\nFalco configuration file\n\nThe file named falco.yaml is the configuration file for the Falco process. It controls the channel that will be notified in case of an alert. A channel could be standard output or a file. Furthermore, the file defines the minimum log level of alerts to include in logs, and how to configure the embedded web server used to implement a health check for the Falco process. Refer to “Falco Configuration Options” for a full reference on all available options.\n\nDefault rules The file named falco_rules.yaml defines a set of preinstalled rules. Falco considers those rules to be applied by default. Among them are checks for creating an alert when a shell to a container is opened or when a write operation is performed to a system directory. You can find other examples and their corresponding rule definitions on the “Rules Examples” page.\n\nCustom rules The file named falco_rules.local.yaml lets you define custom rules and override default rules. With a fresh installation of Falco, the file only contains commented-out rules to provide you with a starting point for adding your own rules. You can find guidance on writing custom rules in the Falco documentation.\n\nKubernetes-specific rules The file k8s_audit_rules.yaml defines Kubernetes-specific rules in addition to logging system call events. Typical examples are “log an event when a namespace is deleted” or “log an event when a Role or ClusterRole object is created.”\n\nApplying configuration changes\n\nModifying the contents of configuration files will not automatically propagate them to the Falco process. You need to restart the Falco service, as demonstrated by the following command:\n\n$ sudo systemctl restart falco\n\nNext up, we’ll trigger some of the events covered by Falco’s default rules. Each of those events will create an alert written to the configured channel. The initial setup of Falco routes messages to standard output.\n\nGenerating Events and Inspecting Falco Logs\n\nLet’s see Falco alerts in action. One of the default rules added by Falco creates an alert whenever a user tries to open a shell to a container. We’ll need to perform this activity to see a log entry for it. To achieve that, create a new Pod named nginx, open a bash shell to the container, and then exit out of the container:\n\n$ kubectl run nginx --image=nginx:1.23.3 pod/nginx created $ kubectl exec -it nginx -- bash root@nginx:/# exit\n\nIdentify the cluster node the Pod runs on by inspecting its runtime details:\n\n$ kubectl get pod nginx -o jsonpath='{.spec.nodeName}' kube-worker-1\n\nThis Pod is running on the cluster node named kube-worker-1. You will need to inspect the Falco logs on that machine to find the relevant log entry. You can inspect logged events by using the journalctl command directly on the kube-worker-1 cluster node. The following command searches for entries that contain the keyword falco:\n\n$ sudo journalctl -fu falco ... Jan 24 18:03:37 kube-worker-1 falco[8718]: 18:03:14.632368639: Notice A shell \\ was spawned in a container with an attached terminal (user=root user_loginuid=0 \\ nginx (id=18b247adb3ca) shell=bash parent=runc cmdline=bash pid=47773 \\ terminal=34816 container_id=18b247adb3ca image=docker.io/library/nginx)\n\nYou will find that additional rules will kick in if you try to modify the container state. Say you’d installed the Git package using apt in the nginx container:\n\nroot@nginx:/# apt update root@nginx:/# apt install git\n\nFalco added log entries for those system-level operations. The following output renders the alerts:\n\n$ sudo journalctl -fu falco Jan 24 18:55:48 ubuntu-focal falco[8718]: 18:55:05.173895727: Error Package \\ management process launched in container (user=root user_loginuid=0 \\ command=apt update pid=60538 container_id=18b247adb3ca container_name=nginx \\ image=docker.io/library/nginx:1.23.3) Jan 24 18:55:48 ubuntu-focal falco[8718]: 18:55:11.050925982: Error Package \\ management process launched in container (user=root user_loginuid=0 \\ command=apt install git-all pid=60823 container_id=18b247adb3ca \\ container_name=nginx image=docker.io/library/nginx:1.23.3) ...\n\nIn the next section, we’ll inspect the Falco rules that trigger the creation of alerts, and their syntax.\n\nUnderstanding Falco Rule File Basics\n\nA Falco rules file usually consists of three elements defined in YAML: rules, macros, and lists. You’ll need to understand those elements on a high level and know how to modify them to achieve a certain goal.\n\nCRAFTING YOUR OWN FALCO RULES\n\nDuring the exam, you will likely not have to craft your own Falco rules. You’ll be provided with an existing set of rules. If you want to dive deeper into Falco rules, have a look at the relevant documentation page.\n\nRule\n\nA rule is the condition under which an alert should be generated. It also defines the output message of the alert. An output message can consist of a hard-coded message and incorporate built-in variables. The alert is recorded on the WARNING log level. Example 7-1 shows the YAML for a rule listening to events that try to access the machine’s camera from processes other than your traditional video-conferencing software, such as Skype or Webex.\n\nExample 7-1. A Falco rule that monitors camera access - rule: access_camera desc: a process other than skype/webex tries to access the camera condition: evt.type = open and fd.name = /dev/video0 and not proc.name in \\ (skype, webex) output: Unexpected process opening camera video device (command=%proc.cmdline) priority: WARNING\n\nMacro\n\nA macro is a reusable rule condition that helps with avoiding copy-pasting similar logic across multiple rules. Macros are useful if the rule file becomes lengthy and you want to improve on maintainability.\n\nSay you are in the process of simplifying a rules file. You notice that multiple rules use the same condition that listens for camera access. Example 7-2 shows how to break out the logic into a macro.\n\nExample 7-2. A Falco macro defining a reusable condition - macro: camera_process_access condition: evt.type = open and fd.name = /dev/video0 and not proc.name in \\ (skype, webex)\n\nThe macro can now be referenced by name in a rule definition, as shown in Example 7-3.\n\nExample 7-3. A Falco rule incorporating a macro - rule: access_camera desc: a process other than skype/webex tries to access the camera condition: camera_process_access output: Unexpected process opening camera video device (command=%proc.cmdline) priority: WARNING\n\nList\n\nA list is a collection of items that you can include in rules, macros, and other lists. Think of lists as an array in traditional programming languages. Example 7-4 creates a list of process names associated with video- conferencing software.\n\nExample 7-4. A Falco list - list: video_conferencing_software items: [skype, webex]\n\nExample 7-5 shows the usage of the list by name in a macro.\n\nExample 7-5. A Falco macro that uses a list - macro: camera_process_access condition: evt.type = open and fd.name = /dev/video0 and not proc.name in \\ (video_conferencing_software)\n\nDissecting an existing rule\n\nThe reason why Falco ships with default rules is to shorten the timespan to hit the ground running for a production cluster. Instead of having to come up with your own rules and the correct syntax, you can simply install Falco and benefit from best practices from the get-go.\n\nLet’s come back to one of the events we triggered in “Generating Events and Inspecting Falco Logs”. At the time of writing, I am using the Falco version 0.33.1. The rules file /etc/falco/falco_rules.yaml shipped with it contains a rule named “Terminal shell in container.” It observes the event of opening a shell to a container. You can easily find the rule by searching for a portion of the log message, e.g., “A shell was spawned in a container.” Example 7-6 shows the syntax of the rule definition, as well as the annotated portions of the YAML.\n\nExample 7-6. A Falco rule that monitors shell activity to a container - macro: spawned_process condition: (evt.type in (execve, execveat) and evt.dir=<)\n\nmacro: container condition: (container.id != host)\n\nmacro: container_entrypoint condition: (not proc.pname exists or proc.pname in (runc:[0:PARENT], \\\n\nrunc:[1:CHILD], runc, docker-runc, exe, docker-runc-cur))\n\nmacro: user_expected_terminal_shell_in_container_conditions condition: (never_true)\n\nrule: Terminal shell in container desc: A shell was used as the entrypoint/exec point into a container with an \\ attached terminal. condition: > spawned_process and container and shell_procs and proc.tty != 0 and container_entrypoint and not user_expected_terminal_shell_in_container_conditions output: > A shell was spawned in a container with an attached terminal (user=%user.name \\ user_loginuid=%user.loginuid %container.info shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline pid=%proc.pid \\ terminal=%proc.tty container_id=%container.id image=%container.image.repository) priority: NOTICE tags: [container, shell, mitre_execution] Defines a macro, a condition reusable across multiple rules referenced by name.\n\nSpecifies the name of the rule.\n\nThe aggregated condition composed of multiple macros.\n\nThe alerting message should the event happen. The message may use built-in fields to reference runtime value.\n\nIndicates how serious a violation of the rule it is.\n\nCategorizes the rule set into groups of related rules for ease of management.\n\nSometimes you may want to change an existing rule. The next section will explain how to best approach overriding default rules.\n\nOverriding Existing Rules\n\nInstead of modifying the rule definition directly in /etc/falco/falco_rules.yaml, I’d suggest you redefine the rule in /etc/falco/falco_rules.local.yaml. That’ll help with falling back to the original rule definition in case you want to get rid of the modifications or if you make any mistakes in the process. The rule definition needs to be changed on all worker nodes of the cluster.\n\nThe rule definition shown in Example 7-7 overrides the rule named “Terminal shell in container” by changing the priority to ALERT and the output to a new format by incorporating built-in fields.\n\nExample 7-7. The modified rule that monitors shell activity to a container - rule: Terminal shell in container desc: A shell was used as the entrypoint/exec point into a container with an \\ attached terminal. condition: > spawned_process and container and shell_procs and proc.tty != 0 and container_entrypoint and not user_expected_terminal_shell_in_container_conditions output: > Opened shell: %evt.time,%user.name,%container.name priority: ALERT tags: [container, shell, mitre_execution]\n\nSimplifies the log output rendered for a violation.\n\nTreats a violation of the rule with ALERT priority.\n\nAfter adding the rule to falco_rules.local.yaml, we need to let Falco pick up the changes. Restart the Falco service with the following command:\n\n$ sudo systemctl restart falco\n\nAs a result, any attempt that shells into a container will be logged with a different output format and priority, as the following shows:\n\n$ sudo journalctl -fu falco ... Jan 24 21:19:13 kube-worker-1 falco[100017]: 21:19:13.961970887: Alert Opened \\ shell: 21:19:13.961970887,<NA>,nginx\n\nIn addition to overriding existing Falco rules, you can also define your own custom rules in /etc/falco/falco_rules.local.yaml. Writing custom rules is out of scope for this book, but you should find plenty of information on the topic in the Falco documentation.\n\nEnsuring Container Immutability\n\nContainers are mutable by default. After the container has been started, you can open a shell to it, install a patch for existing software, modify files, or make changes to its configuration. Mutable containers allow attackers to gain access to the container by downloading or installing malicious software.\n\nTo counteract the situation, make sure that the container is operated in an immutable state. That means preventing write operations to the container’s filesystem or even disallowing shell access to the container. If you need to make any substantial changes to the container, such as updating a dependency or incorporating a new application feature, you should release a new tag of the container image instead of manually modifying the container itself. You’d then assign the new tag of the container image to the Pod without having to modify the internals directly.\n\nScenario: An Attacker Installs Malicious Software\n\nFigure 7-3 illustrates a scenario where an attacker gains access to a container using stolen credentials. The attacker downloads and installs malicious software given that the container allows write operations to the root filesystem. The malicious software keeps monitoring activities in the container; for example, it could parse the application logs for sensitive information. It may then send the information to a server outside of the\n\nKubernetes cluster. Consequently, the data is used as a means to log into other parts of the system.\n\nFigure 7-3. An attacker shells into a container and installs malicious software\n\nIn the next section, you’ll learn how to prevent the situation from happening by using a distroless container image, injecting configuration data with the help of a ConfigMap or Secret, and by configuring a read-only container filesystem. Those settings as a whole will make the container truly immutable.\n\nUsing a Distroless Container Image\n\nDistroless container images have become increasing popular in the world of containers. They only bundle your application and its runtime dependencies, while at the same time removing as much of the operating system as possible, e.g., shells, package managers, and libraries. Distroless container images are not just smaller in size; they are also more secure. An attacker cannot shell into the container, and therefore the filesystem cannot be misused to install malicious software. Using distroless container images is the first line of defense in creating an immutable container. We already covered distroless container images in “Picking a Base Image Small in Size”. Refer to that section for more information.\n\nConfiguring a Container with a ConfigMap or Secret\n\nIt is best practice to use the same container image for different deployment environments, even though their runtime configurations may be different.\n\nAny environment-specific configuration, such as credentials, and connection URLs to other parts of the system, should be externalized. In Kubernetes, you can inject configuration data into a container with the help of a ConfigMap or Secret as environment variables or files mounted via Volumes. Figure 7-4 shows the reuse of the same container image to configure a Pod in a development and production cluster. Configuration data specific to the environment is provided by a ConfigMap.\n\nFigure 7-4. Using the same container image across multiple environments\n\nAvoiding the creation of environment-specific container images simplifies the creation process, reduces the risk of introducing accidental security risks, and makes testing of the functionality easier. Injecting runtime values does not require changing the container after it has been started and therefore is key to making it immutable.\n\nWhen using Secrets as environment variables in a container, make sure to avoid accidentally logging the values to standard output, e.g., as a plain-text value when writing a log message. Anyone with access to the container logs would be able to parse them for Secrets values. As a spot check, identify the places in your application code where you use a Secret and assess their risk for exposure.\n\nTo brush up on creating, configuring, and consuming ConfigMaps and Secrets, revisit the Kubernetes documentation.\n\nConfiguring a Read-Only Container Root Filesystem\n\nAnother aspect of container immutability is to prevent write access to the container’s filesystem. You can configure this runtime behavior by assigning the value true to the attribute spec.containers[].securityContext.readOnlyRootFilesystem.\n\nThere are some applications that still require write access to fulfill their functional requirements. For example, nginx needs to write to the directories /var/run, /var/cache/nginx, and /usr/local/nginx. In combination with setting readOnlyRootFilesystem to true, you can declare Volumes that make those directories writable. Example 7-8 shows the YAML manifest of an immutable container running nginx.\n\nExample 7-8. A container disallowing write access to the root filesystem apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:1.21.6 securityContext: readOnlyRootFilesystem: true volumeMounts: - name: nginx-run mountPath: /var/run - name: nginx-cache mountPath: /var/cache/nginx - name: nginx-data mountPath: /usr/local/nginx volumes: - name: nginx-run emptyDir: {} - name: nginx-data emptyDir: {} - name: nginx-cache emptyDir: {}\n\nIdentify the filesystem read/write requirements of your application before creating a Pod. Configure write mountpaths with the help of Volumes. Any other filesystem path should become read-only.\n\nUsing Audit Logs to Monitor Access\n\nIt’s imperative for a Kubernetes administrator to have a record of events that have occurred in the cluster. Those records can help detect an intrusion in real-time, or they can be used to track down configuration changes for troubleshooting purposes. Audit logs provide a chronological view of events received by the API server.\n\nScenario: An Administrator Can Monitor Malicious Events in Real Time\n\nFigure 7-5 shows the benefits of monitoring Kubernetes API events. In this scenario, the attacker tries to make calls to the API server. Events of interest have been captured by the audit log mechanism. The administrator can view those events at any time, identify intrusion attempts, and take countermeasure.\n\nFigure 7-5. An attacker monitored by observing audit logs\n\nWe only reviewed one of the use cases here, the one that applies to security concerns. The ability to trace company-internal API requests should not be underestimated. By reviewing audit logs, the administrator can provide guidance to application developers trying to create Kubernetes objects, or reconstruct configuration changes that may have led to faulty cluster behavior.\n\nUnderstanding Audit Logs\n\nKubernetes can store records for events triggered by end users for any requests made to the API server or for events emitted by the control plane\n\nitself. Entries in the audit log exist in JSON Lines format and can consist of, but aren’t limited to, the following information:\n\nWhat event occurred?\n\nWho triggered the event?\n\nWhen was it triggered?\n\nWhich Kubernetes component handled the request?\n\nThe type of event and the corresponding request data to be recorded are defined by an audit policy. The audit policy is a YAML manifest specifying those rules and has to be provided to the API server process.\n\nThe audit backend is responsible for storing the recorded audit events, as defined by the audit policy. You have two configurable options for a backend:\n\nA log backend, which write the events to a file.\n\nA webhook backend, which sends the events to an external service via HTTP(S)—for example, for the purpose of integrating a centralized logging and monitoring system. Such a backend can help with debugging runtime issues like a crashed application.\n\nFigure 7-6 puts together all the pieces necessary to configure audit logging. The following sections will explain the details of configuring them.\n\nFigure 7-6. The high-level audit log architecture\n\nLet’s have a deeper look at the audit policy file and its configuration options.\n\nCreating the Audit Policy File\n\nThe audit policy file is effectively a YAML manifest for a Policy resource. Any event received by the API server is matched against the rules defined in the policy file in the order of definition. The event is logged with the declared audit level if a matching rule can be found. Table 7-1 lists all available audit levels.\n\nTable 7-1. Audit levels\n\nLevel\n\nEffect\n\nNone\n\nDo not log events matching this rule.\n\nMetadata\n\nOnly log request metadata for the event.\n\nRequest\n\nLog metadata and the request body for the event.\n\nRequestResponse\n\nLog metadata, request, and response body for the event.\n\nExample 7-9 shows an exemplary audit policy. The rules are specified as an array of items with the attribute named rules. Each rule declares a level, the resource type and API group it applies to, and an optional namespace.\n\nExample 7-9. Contents of an audit policy file apiVersion: audit.k8s.io/v1 kind: Policy omitStages: - \"RequestReceived\" rules: - level: RequestResponse resources: - group: \"\" resources: [\"pods\"] - level: Metadata resources: - group: \"\" resources: [\"pods/log\", \"pods/status\"]\n\nPrevents generating logs for all requests in the RequestReceived stage\n\nLogs Pod changes at RequestResponse level\n\nLogs specialized Pod events, e.g., log and status requests, at the Metadata level\n\nThe previous audit policy isn’t very extensive but should give you an impression of its format. Refer to the Kubernetes documentation for additional examples and more details.\n\nOnce the audit policy file has been created, it can be consumed by the API server process. Add the flag --audit-policy-file to the API server process in the file /etc/kubernetes/manifests/kube-apiserver.yaml. The value assigned to the parameter is the fully qualified path to the audit policy file.\n\nNext up, we’ll walk through the settings needed to configure audit logging for the API server for a file-based log backend and a webhook backend.\n\nConfiguring a Log Backend\n\nTo set up a file-based log backend, you will need to add three pieces of configuration to the file /etc/kubernetes/manifests/kube- apiserver.yaml. The following list summarizes the configuration:\n\n1. Provide two flags to the API server process: the flag --audit- policy-file points to the audit policy file; the flag --audit- log-path points to the log output file.\n\n2. Add a Volume mountpath for the audit log policy file and the log output directory.\n\n3. Add a Volume definition to the host path for the audit log policy file and the log output directory.\n\nExample 7-10 shows the modified content of the API server configuration file.\n\nExample 7-10. Configuring the audit policy file and audit log file ... spec: containers: - command: - kube-apiserver - --audit-policy-file=/etc/kubernetes/audit-policy.yaml - --audit-log-path=/var/log/kubernetes/audit/audit.log ... volumeMounts: - mountPath: /etc/kubernetes/audit-policy.yaml name: audit readOnly: true - mountPath: /var/log/kubernetes/audit/ name: audit-log readOnly: false ... volumes: - name: audit hostPath: path: /etc/kubernetes/audit-policy.yaml type: File - name: audit-log hostPath:\n\npath: /var/log/kubernetes/audit/ type: DirectoryOrCreate\n\nProvides the location of the policy file and log file to the API server process.\n\nMounts the policy file and the audit log directory to the given paths.\n\nDefines the Volumes for the policy file and the audit log directory.\n\nThe runtime behavior of the log backend can be further customized by passing additional flags to the API server process. For example, you can specify the maximum number of days to retain old audit log files by providing the flag --audit-log-maxage. Refer to the Kubernetes documentation to have a look at the complete list of flags.\n\nIt’s time to produce some log entries. The following kubectl command sends a request to the API server for creating a Pod named nginx:\n\n$ kubectl run nginx --image=nginx:1.21.6 pod/nginx created\n\nIn the previous step, we configured the audit log file at /var/log/kubernetes/audit/audit.log. Depending on the rules in the audit policy, the number of entries may be overwhelming, which makes finding a specific event hard. A simple way to filter configured events is by searching for the value audit.k8s.io/v1 assigned to the apiVersion attribute. The following command finds relevant log entries, one for the RequestResponse level, and another for the Metadata level:\n\n$ sudo grep 'audit.k8s.io/v1' /var/log/kubernetes/audit/audit.log ... {\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"RequestResponse\", \\ \"auditID\":\"285f4b99-951e-405b-b5de-6b66295074f4\",\"stage\":\"ResponseComplete\", \\ \"requestURI\":\"/api/v1/namespaces/default/pods/nginx\",\"verb\":\"get\", \\ \"user\":{\"username\":\"system:node:node01\",\"groups\":[\"system:nodes\", \\ \"system:authenticated\"]},\"sourceIPs\":[\"172.28.116.6\"],\"userAgent\": \\\n\n\"kubelet/v1.26.0 (linux/amd64) kubernetes/b46a3f8\",\"objectRef\": \\ {\"resource\":\"pods\",\"namespace\":\"default\",\"name\":\"nginx\",\"apiVersion\":\"v1\"}, \\ \"responseStatus\":{\"metadata\":{},\"code\":200},\"responseObject\":{\"kind\":\"Pod\", \\ \"apiVersion\":\"v1\",\"metadata\":{\"name\":\"nginx\",\"namespace\":\"default\", \\ ... {\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"Metadata\",\"auditID\": \\ \"5c8e5ecc-0ce0-49e0-8ab2-368284f2f785\",\"stage\":\"ResponseComplete\", \\ \"requestURI\":\"/api/v1/namespaces/default/pods/nginx/status\",\"verb\":\"patch\", \\ \"user\":{\"username\":\"system:node:node01\",\"groups\":[\"system:nodes\", \\ \"system:authenticated\"]},\"sourceIPs\":[\"172.28.116.6\"],\"userAgent\": \\ \"kubelet/v1.26.0 (linux/amd64) kubernetes/b46a3f8\",\"objectRef\": \\ {\"resource\":\"pods\",\"namespace\":\"default\",\"name\":\"nginx\",\"apiVersion\":\"v1\", \\ \"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200}, \\ ...\n\nConfiguring a Webhook Backend\n\nConfiguring a webhook backend looks different from configuring a log backend. We need to tell the API server to send an HTTP(S) request to an external service instead of the file. The configuration to the external service, the webhook, and the credentials needed to authenticate are defined in a kubeconfig file, similarly to what we’ve done in “Configuring the ImagePolicyWebhook Admission Controller Plugin”.\n\nAdd the flag --audit-webhook-config-file to the API server process in the file /etc/kubernetes/manifests/kube-apiserver.yaml, and point it to the location of the kubeconfig file. The flag --audit-webhook- initial-backoff defines the time to wait after the initial request to the external service before retrying. You will still have to assign the flag -- audit-policy-file to point to the audit policy file.\n\nSummary\n\nMonitoring and logging events in a Kubernetes cluster is an important duty of every administrator. We used Falco to identify and filter security-related events. You learned about the purpose and syntax of the different configuration files and how to find relevant alerts in the logs.\n\nIn addition to employing behavior analytics tools, you will want to set up audit logging for requests reaching the API server. Audit logging records configured events to a backend, either to a file on the control plane node or to an external service via an HTTP(S) call. We worked through the process of enabling audit logging for the API server process.\n\nA sensible step toward more secure containers is to make them immutable. An immutable container only supports a read-only filesystem, so that a potential attacker cannot install malicious software. Mount a Volume if the application running inside of the container needs to write data. Use a distroless container image to lock out attackers from being able to shell into the container.\n\nExam Essentials\n\nPractice how to configure and operate Falco.\n\nFalco is definitely going to come up as a topic during the exam. You will need to understand how to read and modify a rule in a configuration file. I would suggest you browse through the syntax and options in more detail in case you need to write one yourself. The main entry point for running Falco is the command line tool. It’s fair to assume that it will have been preinstalled in the exam environment.\n\nKnow how to identify immutable containers.\n\nImmutable containers are a central topic to this exam domain. Understand how to set the spec.containers[].securityContext.readOnlyRootFilesystem attribute for a Pod and how to mount a Volume to a specific path in case a write operation is required by the container process.\n\nDeeply understand audit log configuration options.\n\nSetting up audit logging consists of two steps. For one, you need to understand the syntax and structure of an audit policy file. The other\n\naspect is how to configure the API server to consume the audit policy file, provide a reference to a backend, and mount the relevant filesystem Volumes. Make sure to practice all of those aspects hands-on.\n\nSample Exercises\n\n1. Navigate to the directory app-a/ch07/falco of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f. Falco is already running as a systemd service.\n\nInspect the process running in the existing Pod named malicious. Have a look at the Falco logs and see if a rule created a log for the process.\n\nReconfigure the existing rule that creates a log for the event by changing the output to <timestamp>,<username>,<container- id>. Find the changed log entry in the Falco logs.\n\nReconfigure Falco to write logs to the file at /var/logs/falco.log. Disable the standard output channel. Ensure that Falco appends new messages to the log file.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\n2. Navigate to the directory app-a/ch07/immutable-container of the checked-out GitHub repository bmuschko/cks-study-guide. Execute the command kubectl apply -f setup.yaml.\n\nInspect the Pod created by the YAML manifest in the default namespace. Make relevant changes to the Pod so that its container can be considered immutable.\n\n3. Navigate to the directory app-a/ch07/audit-log of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f. Edit the existing audit policy file at /etc/kubernetes/audit/rules/audit-policy.yaml. Add a rule that logs events for ConfigMaps and Secrets at the Metadata level. Add another rule that logs events for Services at the Request level.\n\nConfigure the API server to consume the audit policy file. Logs should be written to the file /var/log/kubernetes/audit/logs/apiserver.log. Define a maximum number of five days to retain audit log files.\n\nEnsure that the log file has been created and contains at least one entry that matches the events configured.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\nOceanofPDF.com\n\nAppendix. Answers to Review Questions\n\nChapter 2, “Cluster Setup”\n\n1. Create a file with the name deny-egress-external.yaml for defining the network policy. The network policy needs to set the Pod selector to app=backend and define the Egress policy type. Make sure to allow the port 53 for the protocols UDP and TCP. The namespace selector for the egress policy needs to use {} to select all namespaces:\n\napiVersion: networking.k8s.io/v1\n\nkind: NetworkPolicy\n\nmetadata:\n\nname: deny-egress-external\n\nspec:\n\npodSelector:\n\nmatchLabels:\n\napp: backend\n\npolicyTypes:\n\nEgress\n\negress:\n\nto:\n\nnamespaceSelector: {}\n\nports:\n\nport: 53\n\nprotocol: UDP\n\nport: 53\n\nprotocol: TCP\n\nRun the apply command to instantiate the network policy object from the YAML file:\n\n$ kubectl apply -f deny-egress-external.yaml\n\n2. A Pod that does not match the label selection of the network policy can make a call to a URL outside of the cluster. In this case, the label assignment is app=frontend:\n\n$ kubectl run web --image=busybox:1.36.0 -l app=frontend --port=80 -\n\nit \\\n\n--rm --restart=Never -- wget http://google.com --timeout=5 --\n\ntries=1\n\nConnecting to google.com (142.250.69.238:80)\n\nConnecting to www.google.com (142.250.72.4:80)\n\nsaving to /'index.html'\n\nindex.html 100% |**| 13987 \\\n\n0:00:00 ETA\n\n/'index.html' saved\n\npod \"web\" deleted\n\n3. A Pod that does match the label selection of the network policy cannot make a call to a URL outside of the cluster. In this case, the label assignment is app=backend:\n\n$ kubectl run web --image=busybox:1.36.0 -l app=backend --port=80 -it\n\n\\\n\n--rm --restart=Never -- wget http://google.com --timeout=5 --\n\ntries=1\n\nwget: download timed out\n\npod \"web\" deleted\n\npod default/web terminated (Error)\n\n4. First, see if the Dashboard is already installed. You can check the namespace the Dashboard usually creates:\n\n$ kubectl get ns kubernetes-dashboard\n\nNAME STATUS AGE\n\nkubernetes-dashboard Active 109s\n\nIf the namespace does not exist, you can assume that the Dashboard has not been installed yet. Install it with the following command:\n\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/\\\n\ndashboard/v2.6.0/aio/deploy/recommended.yaml\n\nCreate the ServiceAccount, ClusterRole, and ClusterRoleBinding. Make sure that the ClusterRole only allows listing Deployment objects. The following YAML manifest has been saved in the file dashboard-observer-user.yaml:\n\napiVersion: v1\n\nkind: ServiceAccount\n\nmetadata:\n\nname: observer-user\n\nnamespace: kubernetes-dashboard\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\n\nkind: ClusterRole\n\nmetadata:\n\nannotations:\n\nrbac.authorization.kubernetes.io/autoupdate: \"true\"\n\nname: cluster-observer\n\nrules:\n\napiGroups:\n\n'apps'\n\nresources:\n\n'deployments'\n\nverbs:\n\nlist\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\n\nkind: ClusterRoleBinding\n\nmetadata:\n\nname: observer-user\n\nroleRef:\n\napiGroup: rbac.authorization.k8s.io\n\nkind: ClusterRole\n\nname: cluster-observer\n\nsubjects:\n\nkind: ServiceAccount\n\nname: observer-user\n\nnamespace: kubernetes-dashboard\n\nCreate the objects with the following command:\n\n$ kubectl apply -f dashboard-observer-user.yaml\n\n5. Run the following command to create a token for the\n\nServiceAccount. The option --duration 0s ensures that the token will never expire. Copy the token that was rendered in the console output of the command:\n\n$ kubectl create token observer-user -n kubernetes-dashboard \\\n\n--duration 0s\n\neyJhbGciOiJSUzI1NiIsImtpZCI6Ik5lNFMxZ1...\n\nRun the proxy command and open the link http://localhost:8001/api/v1/namespaces/kubernetes- dashboard/services/https:kubernetes-dashboard:/proxy in a browser:\n\n$ kubectl proxy\n\nSelect the “Token” authentication method and paste the token you copied before. Sign into the Dashboard. You should see that only Deployment objects are listable (see Figure A-1).\n\nAll other objects will say “There is nothing to display here.” Figure A-2 renders the list of Pods.\n\nFigure A-1. The Dashboard view of Deployments is allowed\n\nFigure A-2. The Dashboard view of Pods is not permitted\n\n6. Download the API server binary with the following command:\n\n$ curl -LO \"https://dl.k8s.io/v1.26.1/bin/linux/amd64/kube-apiserver\"\n\nNext, download the SHA256 file for the same binary, but a different version. The following command downloads the file for version 1.23.1:\n\n$ curl -LO \"https://dl.k8s.io/v1.23.1/bin/linux/amd64/\\\n\nkube-apiserver.sha256\"\n\nComparing the binary file with the checksum file results in a failure, as the versions do not match:\n\n$ echo \"$(cat kube-apiserver.sha256) kube-apiserver\" | shasum -a 256\n\n\\\n\n--check\n\nkube-apiserver: FAILED\n\nshasum: WARNING: 1 computed checksum did NOT match\n\nChapter 3, “Cluster Hardening”\n\n1. Create a private key using the openssl executable. Provide an expressive file name, such as jill.key. The -subj option provides the username (CN) and the group (O). The following command uses the username jill and the group named observer: $ openssl genrsa -out jill.key 2048\n\n$ openssl req -new -key jill.key -out jill.csr -subj \\\n\n\"/CN=jill/O=observer\"\n\nRetrieve the base64-encoded value of the CSR file content with the following command. You will need it when creating a the CertificateSigningRequest object in the next step:\n\n$ cat jill.csr | base64 | tr -d \"\\n\"\n\nLS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tL...\n\nThe following script creates a CertificateSigningRequest object:\n\n$ cat <<EOF | kubectl apply -f -\n\napiVersion: certificates.k8s.io/v1\n\nkind: CertificateSigningRequest\n\nmetadata:\n\nname: jill\n\nspec:\n\nrequest: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tL...\n\nsignerName: kubernetes.io/kube-apiserver-client\n\nexpirationSeconds: 86400\n\nusages:\n\nclient auth\n\nEOF\n\nUse the certificate approve command to approve the signing request and export the issued certificate:\n\n$ kubectl certificate approve jill\n\n$ kubectl get csr jill -o jsonpath={.status.certificate}| base64 \\\n\nd > jill.crt\n\nAdd the user to the kubeconfig file and add the context for the user. The cluster name used here is minikube. It might be different for your Kubernetes environment:\n\n$ kubectl config set-credentials jill --client-key=jill.key \\\n\n--client-certificate=jill.crt --embed-certs=true\n\n$ kubectl config set-context jill --cluster=minikube --user=jill\n\n2. Create the Role and RoleBinding. The following imperative commands assign the verbs get, list, and watch for Pods, ConfigMaps, and Secrets to the subject named observer of type group. The user jill is part of the group:\n\n$ kubectl create role observer --verb=create --verb=get --verb=list \\\n\n--verb=watch --resource=pods --resource=configmaps --\n\nresource=secrets\n\n$ kubectl create rolebinding observer-binding --role=observer \\\n\n--group=observer\n\n3. Switch to the user context:\n\n$ kubectl config use-context jill\n\nWe’ll pick one permitted operation, listing ConfigMap objects. The user is authorized to map the call:\n\n$ kubectl get configmaps\n\nNAME DATA AGE\n\nkube-root-ca.crt 1 16m\n\nListing nodes won’t be authorized. The user does not have the appropriate permissions:\n\n$ kubectl get nodes\n\nError from server (Forbidden): nodes is forbidden: User \"jill\" cannot\n\n\\\n\nlist resource \"nodes\" in API group \"\" at the cluster scope\n\nSwitch back to the admin context:\n\n$ kubectl config use-context minikube\n\n4. Create the namespace t23:\n\n$ kubectl create namespace t23\n\nCreate the service account api-call in the namespace:\n\n$ kubectl create serviceaccount api-call -n t23\n\nDefine a YAML manifest file with the name pod.yaml. The contents of the file define a Pod that makes an HTTPS GET call to the API server to retrieve the list of Services in the default namespace:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: service-list\n\nnamespace: t23\n\nspec:\n\nserviceAccountName: api-call\n\ncontainers:\n\nname: service-list\n\nimage: alpine/curl:3.14\n\ncommand: ['sh', '-c', 'while true; do curl -s -k -m 5 \\\n\nH \"Authorization: Bearer $(cat /var/run/secrets/\\\n\nkubernetes.io/serviceaccount/token)\"\n\nhttps://kubernetes.\\\n\ndefault.svc.cluster.local/api/v1/namespaces/default/\\\n\nservices; sleep 10; done']\n\nCreate the Pod with the following command:\n\n$ kubectl apply -f pod.yaml\n\nCheck the logs of the Pod. The API call is not authorized, as shown in the following log output:\n\n$ kubectl logs service-list -n t23\n\n{\n\n\"kind\": \"Status\",\n\n\"apiVersion\": \"v1\",\n\n\"metadata\": {},\n\n\"status\": \"Failure\",\n\n\"message\": \"services is forbidden: User \\\"system:serviceaccount:t23\n\n\\\n\n:api-call\\\" cannot list resource \\\"services\\\" in API \\\n\ngroup \\\"\\\" in the namespace \\\"default\\\"\",\n\n\"reason\": \"Forbidden\",\n\n\"details\": {\n\n\"kind\": \"services\"\n\n},\n\n\"code\": 403\n\n}\n\n5. Create the YAML manifest in the file clusterrole.yaml, as shown in the following:\n\napiVersion: rbac.authorization.k8s.io/v1\n\nkind: ClusterRole\n\nmetadata:\n\nname: list-services-clusterrole\n\nrules:\n\napiGroups: [\"\"]\n\nresources: [\"services\"]\n\nverbs: [\"list\"]\n\nReference the ClusterRole in a RoleBinding defined in the file rolebinding.yaml. The subject should list the service account api-call in the namespace t23:\n\napiVersion: rbac.authorization.k8s.io/v1\n\nkind: RoleBinding\n\nmetadata:\n\nname: serviceaccount-service-rolebinding\n\nsubjects:\n\nkind: ServiceAccount\n\nname: api-call\n\nnamespace: t23\n\nroleRef:\n\nkind: ClusterRole\n\nname: list-services-clusterrole\n\napiGroup: rbac.authorization.k8s.io\n\nCreate both objects from the YAML manifests:\n\n$ kubectl apply -f clusterrole.yaml\n\n$ kubectl apply -f rolebinding.yaml\n\nThe API call running inside of the container should now be authorized and be allowed to list the Service objects in the default namespace. As shown in the following output, the namespace currently hosts at least one Service object, the kubernetes.default Service:\n\n$ kubectl logs service-list -n t23\n\n{\n\n\"kind\": \"ServiceList\",\n\n\"apiVersion\": \"v1\",\n\n\"metadata\": {\n\n\"resourceVersion\": \"1108\"\n\n},\n\n\"items\": [\n\n{\n\n\"metadata\": {\n\n\"name\": \"kubernetes\",\n\n\"namespace\": \"default\",\n\n\"uid\": \"30eb5425-8f60-4bb7-8331-f91fe0999e20\",\n\n\"resourceVersion\": \"199\",\n\n\"creationTimestamp\": \"2022-09-08T18:06:52Z\",\n\n\"labels\": {\n\n\"component\": \"apiserver\",\n\n\"provider\": \"kubernetes\"\n\n},\n\n...\n\n}\n\n]\n\n}\n\n6. Create the token for the service account using the following command:\n\n$ kubectl create token api-call -n t23\n\neyJhbGciOiJSUzI1NiIsImtpZCI6IjBtQkJzVWlsQjl...\n\nChange the existing Pod definition by deleting and recreating the live object. Add the attribute that disables automounting the token, as shown in the following:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: service-list\n\nnamespace: t23\n\nspec:\n\nserviceAccountName: api-call\n\nautomountServiceAccountToken: false\n\ncontainers:\n\nname: service-list\n\nimage: alpine/curl:3.14\n\ncommand: ['sh', '-c', 'while true; do curl -s -k -m 5 \\\n\nH \"Authorization: Bearer\n\neyJhbGciOiJSUzI1NiIsImtpZCI6Ij \\\n\nBtQkJzVWlsQjl\" https://kubernetes.default.svc.cluster.\n\n\\\n\nlocal/api/v1/namespaces/default/services; sleep 10; \\\n\ndone']\n\nThe API server will allow the HTTPS request performed with the token of the service account to be authenticated and authorized:\n\n$ kubectl logs service-list -n t23\n\n{\n\n\"kind\": \"ServiceList\",\n\n\"apiVersion\": \"v1\",\n\n\"metadata\": {\n\n\"resourceVersion\": \"81194\"\n\n},\n\n\"items\": [\n\n{\n\n\"metadata\": {\n\n\"name\": \"kubernetes\",\n\n\"namespace\": \"default\",\n\n\"uid\": \"30eb5425-8f60-4bb7-8331-f91fe0999e20\",\n\n\"resourceVersion\": \"199\",\n\n\"creationTimestamp\": \"2022-09-08T18:06:52Z\",\n\n\"labels\": {\n\n\"component\": \"apiserver\",\n\n\"provider\": \"kubernetes\"\n\n},\n\n...\n\n}\n\n]\n\n}\n\n7. The solution to this sample exercise requires a lot of manual steps. The following commands do not render their output.\n\nOpen an interactive shell to the control plane node using Vagrant:\n\n$ vagrant ssh kube-control-plane\n\nUpgrade kubeadm to version 1.26.1 and apply it:\n\n$ sudo apt-mark unhold kubeadm && sudo apt-get update && sudo apt-get\n\n\\\n\ninstall -y kubeadm=1.26.1-00 && sudo apt-mark hold kubeadm\n\n$ sudo kubeadm upgrade apply v1.26.1\n\nDrain the node, upgrade the kubelet and kubectl, restart the kubelet, and uncordon the node:\n\n$ kubectl drain kube-control-plane --ignore-daemonsets\n\n$ sudo apt-get update && sudo apt-get install -y \\\n\n--allow-change-held-packages kubelet=1.26.1-00 kubectl=1.26.1-00\n\n$ sudo systemctl daemon-reload\n\n$ sudo systemctl restart kubelet\n\n$ kubectl uncordon kube-control-plane\n\nThe version of the node should now say v1.26.1. Exit the node:\n\n$ kubectl get nodes\n\n$ exit\n\nOpen an interactive shell to the first worker node using Vagrant. Repeat all of the following steps for the worker node:\n\n$ vagrant ssh kube-worker-1\n\nUpgrade kubeadm to version 1.26.1 and apply it to the node:\n\n$ sudo apt-get update && sudo apt-get install -y \\\n\n--allow-change-held-packages kubeadm=1.26.1-00\n\n$ sudo kubeadm upgrade node\n\nDrain the node, upgrade the kubelet and kubectl, restart the kubelet, and uncordon the node:\n\n$ kubectl drain kube-worker-1 --ignore-daemonsets\n\n$ sudo apt-get update && sudo apt-get install -y \\\n\n--allow-change-held-packages kubelet=1.26.1-00 kubectl=1.26.1-00\n\n$ sudo systemctl daemon-reload\n\n$ sudo systemctl restart kubelet\n\n$ kubectl uncordon kube-worker-1\n\nThe version of the node should now say v1.26.1. Exit out of the node:\n\n$ kubectl get nodes\n\n$ exit\n\nChapter 4, “System Hardening”\n\n1. Shell into the worker node with the following command:\n\n$ vagrant ssh kube-worker-1\n\nIdentify the process exposing port 21. One way to do this is by using the lsof command. The command that exposes the port is vsftpd:\n\n$ sudo lsof -i :21\n\nCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME\n\nvsftpd 10178 root 3u IPv6 56850 0t0 TCP *:ftp (LISTEN)\n\nAlternatively, you could also use the ss command, as shown in the following:\n\n$ sudo ss -at -pn '( dport = :21 or sport = :21 )'\n\nState Recv-Q Send-Q Local Address:Port \\\n\nPeer Address:Port Process\n\nLISTEN 0 32 *:21 \\\n\n:* users:((\"vsftpd\",pid=10178,fd=3))\n\nThe process vsftpd has been started as a service:\n\n$ sudo systemctl status vsftpd\n\nvsftpd.service - vsftpd FTP server Loaded: loaded (/lib/systemd/system/vsftpd.service; enabled; \\ vendor preset: enabled)\n\nActive: active (running) since Thu 2022-10-06 14:39:12 UTC; \\\n\n11min ago\n\nMain PID: 10178 (vsftpd)\n\nTasks: 1 (limit: 1131)\n\nMemory: 604.0K\n\nCGroup: /system.slice/vsftpd.service\n\n└─10178 /usr/sbin/vsftpd /etc/vsftpd.conf\n\nOct 06 14:39:12 kube-worker-1 systemd[1]: Starting vsftpd FTP\n\nserver...\n\nOct 06 14:39:12 kube-worker-1 systemd[1]: Started vsftpd FTP server.\n\nShut down the service and deinstall the package:\n\n$ sudo systemctl stop vsftpd\n\n$ sudo systemctl disable vsftpd\n\n$ sudo apt purge --auto-remove -y vsftpd\n\nChecking on the port, you will see that it is not listed anymore:\n\n$ sudo lsof -i :21\n\nExit out of the node:\n\n$ exit\n\n2. Shell into the worker node with the following command:\n\n$ vagrant ssh kube-worker-1\n\nCreate the AppArmor profile at /etc/apparmor.d/network-deny using the command sudo vim /etc/apparmor.d/network-deny. The contents of the file should look as follows:\n\n#include <tunables/global>\n\nprofile network-deny flags=(attach_disconnected) {\n\n#include <abstractions/base>\n\nnetwork,\n\n}\n\nEnforce the AppArmor profile by running the following command:\n\n$ sudo apparmor_parser /etc/apparmor.d/network-deny\n\nYou cannot modify the existing Pod object in order to add the annotation for AppArmor. You will need to delete the object first. Write the definition of the Pod to a file:\n\n$ kubectl get pod -o yaml > pod.yaml\n\n$ kubectl delete pod network-call\n\nEdit the pod.yaml file to add the AppArmor annotation. For the relevant annotation, use the name of the container network-call as part of the key suffix and localhost/network-deny as the value. The suffix network-deny refers to the name of the AppArmor profile. The final content could look as follows after a little bit of cleanup:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: network-call\n\nannotations:\n\ncontainer.apparmor.security.beta.kubernetes.io/network-call: \\\n\nlocalhost/network-deny\n\nspec:\n\ncontainers:\n\nname: network-call\n\nimage: alpine/curl:3.14\n\ncommand: [\"sh\", \"-c\", \"while true; do ping -c 1 google.com; \\\n\nsleep 5; done\"]\n\nCreate the Pod from the manifest. After a couple of seconds, the Pod should transition into the “Running” status:\n\n$ kubectl create -f pod.yaml\n\n$ kubectl get pod network-call\n\nNAME READY STATUS RESTARTS AGE\n\nnetwork-call 1/1 Running 0 27s\n\nAppArmor prevents the Pod from making a network call. You can check the logs to verify:\n\n$ kubectl logs network-call\n\n...\n\nsh: ping: Permission denied\n\nsh: sleep: Permission denied\n\nExit out of the node:\n\n$ exit\n\n3. Shell into the worker node with the following command:\n\n$ vagrant ssh kube-worker-1\n\nCreate the target directory for the seccomp profiles:\n\n$ sudo mkdir -p /var/lib/kubelet/seccomp/profiles\n\nAdd the file audit.json in the directory /var/lib/kubelet/seccomp/profiles with the following content:\n\n{\n\n\"defaultAction\": \"SCMP_ACT_LOG\"\n\n}\n\nYou cannot modify the existing Pod object in order to add the seccomp configuration via the security context. You will need to delete the object first. Write the definition of the Pod to a file:\n\n$ kubectl get pod -o yaml > pod.yaml\n\n$ kubectl delete pod network-call\n\nEdit the pod.yaml file. Point the seccomp profile to the definition. The final content could look as follows after a little bit of cleanup:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: network-call\n\nspec:\n\nsecurityContext:\n\nseccompProfile:\n\ntype: Localhost\n\nlocalhostProfile: profiles/audit.json\n\ncontainers:\n\nname: network-call\n\nimage: alpine/curl:3.14\n\ncommand: [\"sh\", \"-c\", \"while true; do ping -c 1 google.com; \\\n\nsleep 5; done\"]\n\nsecurityContext:\n\nallowPrivilegeEscalation: false\n\nCreate the Pod from the manifest. After a couple of seconds, the Pod should transition into the “Running” status:\n\n$ kubectl create -f pod.yaml\n\n$ kubectl get pod network-call\n\nNAME READY STATUS RESTARTS AGE\n\nnetwork-call 1/1 Running 0 27s\n\nYou should be able to find log entries for syscalls, e.g., for the sleep command:\n\n$ sudo cat /var/log/syslog\n\nOct 6 16:25:06 ubuntu-focal kernel: [ 2114.894122] audit: type=1326\n\n\\\n\naudit(1665073506.099:23761): auid=4294967295 uid=0 gid=0 \\\n\nses=4294967295 pid=19226 comm=\"sleep\" exe=\"/bin/busybox\" \\\n\nsig=0 arch=c000003e syscall=231 compat=0 ip=0x7fc026adbf0b \\\n\ncode=0x7ffc0000\n\nExit out of the node:\n\n$ exit\n\nCreate the Pod definition in the file pod.yaml:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: sysctl-pod\n\nspec:\n\nsecurityContext:\n\nsysctls:\n\nname: net.core.somaxconn\n\nvalue: \"1024\"\n\nname: debug.iotrace\n\nvalue: \"1\"\n\ncontainers:\n\nname: nginx\n\nimage: nginx:1.23.1\n\nCreate the Pod and then check on the status. You will see that the status is “SysctlForbidden”:\n\n$ kubectl create -f pod.yaml\n\n$ kubectl get pods\n\nNAME READY STATUS RESTARTS AGE\n\nsysctl-pod 0/1 SysctlForbidden 0 4s\n\nThe event log will tell you more about the reasoning:\n\n$ kubectl describe pod sysctl-pod\n\n...\n\nEvents:\n\nType Reason Age From \\\n\nMessage\n\n---- ------ ---- ---- \\\n\n-------\n\nWarning SysctlForbidden 2m48s kubelet \\\n\nforbidden sysctl: \"net.core.somaxconn\" \\\n\nnot allowlisted\n\nChapter 5, “Minimize Microservice Vulnerabilities”\n\n1. Define the Pod with the security settings in the file busybox- security-context.yaml. You can find the content of the following YAML manifest:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: busybox-security-context\n\nspec:\n\nsecurityContext:\n\nrunAsUser: 1000\n\nrunAsGroup: 3000\n\nfsGroup: 2000\n\nvolumes:\n\nname: vol\n\nemptyDir: {}\n\ncontainers:\n\nname: busybox\n\nimage: busybox:1.28\n\ncommand: [\"sh\", \"-c\", \"sleep 1h\"]\n\nvolumeMounts:\n\nname: vol\n\nmountPath: /data/test\n\nsecurityContext:\n\nallowPrivilegeEscalation: false\n\nCreate the Pod with the following command:\n\n$ kubectl apply -f busybox-security-context.yaml\n\n$ kubectl get pod busybox-security-context\n\nNAME READY STATUS RESTARTS AGE\n\nbusybox-security-context 1/1 Running 0 54s\n\nShell into the container and create the file. You will find that the file group is 2000, as defined by the security context:\n\n$ kubectl exec busybox-security-context -it -- /bin/sh\n\n/ $ cd /data/test\n\n/data/test $ touch hello.txt\n\n/data/test $ ls -l\n\ntotal 0\n\nrw-r--r-- 1 1000 2000 0 Nov 21 18:29 hello.txt\n\n/data/test $ exit\n\n2. Specify the namespace named audited in the file psa- namespace.yaml. Set the PSA label with baseline level and the warn mode:\n\napiVersion: v1\n\nkind: Namespace\n\nmetadata:\n\nname: audited\n\nlabels:\n\npod-security.kubernetes.io/warn: baseline\n\nCreate the namespace from the YAML manifest:\n\n$ kubectl apply -f psa-namespace.yaml\n\nYou can produce an error by using the following Pod configuration in the file psa-pod.yaml. The YAML manifest sets the attribute hostNetwork: true, which is not allowed for the baseline level:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: busybox\n\nnamespace: audited\n\nspec:\n\nhostNetwork: true\n\ncontainers:\n\nname: busybox\n\nimage: busybox:1.28\n\ncommand: [\"sh\", \"-c\", \"sleep 1h\"]\n\nCreating the Pod renders a warning message. The Pod will have been created nevertheless. You can prevent the creation of the Pod by configuring the PSA with the restricted level:\n\n$ kubectl apply -f psa-pod.yaml\n\nWarning: would violate PodSecurity \"baseline:latest\": host namespaces\n\n\\\n\n(hostNetwork=true)\n\npod/busybox created\n\n$ kubectl get pod busybox -n audited\n\nNAME READY STATUS RESTARTS AGE\n\nbusybox 1/1 Running 0 2m21s\n\n3. You can install Gatekeeper with the following command:\n\n$ kubectl apply -f https://raw.githubusercontent.com/open-policy-\n\nagent/\\\n\ngatekeeper/master/deploy/gatekeeper.yaml\n\nThe Gatekeeper library describes a ConstraintTemplate for defining replica limits. Inspect the YAML manifest described on the page. Apply the manifest with the following command:\n\n$ kubectl apply -f https://raw.githubusercontent.com/open-policy-\n\nagent/\\\n\ngatekeeper-library/master/library/general/replicalimits/template.yaml\n\nNow, define the Constraint with the YAML manifest in the file named replica-limits-constraint.yaml:\n\napiVersion: constraints.gatekeeper.sh/v1beta1\n\nkind: K8sReplicaLimits\n\nmetadata:\n\nname: replica-limits\n\nspec:\n\nmatch:\n\nkinds:\n\napiGroups: [\"apps\"]\n\nkinds: [\"Deployment\"]\n\nparameters:\n\nranges:\n\nmin_replicas: 3\n\nmax_replicas: 10\n\nCreate the Constraint with the following command:\n\n$ kubectl apply -f replica-limits-constraint.yaml\n\nYou can see that a Deployment can only be created if the provided number of replicas falls within the range of the Constraint:\n\n$ kubectl create deployment nginx --image=nginx:1.23.2 --replicas=15\n\nerror: failed to create deployment: admission webhook \\\n\n\"validation.gatekeeper.sh\" denied the request: [replica-limits] \\\n\nThe provided number of replicas is not allowed for deployment: nginx.\n\n\\\n\nAllowed ranges: {\"ranges\": [{\"max_replicas\": 10, \"min_replicas\": 3}]}\n\n$ kubectl create deployment nginx --image=nginx:1.23.2 --replicas=7\n\ndeployment.apps/nginx created\n\n4. Configure encryption for etcd, as described in “Encrypting etcd Data”. Next, create a new Secret with the following imperative command:\n\n$ kubectl create secret generic db-credentials \\\n\n--from-literal=api-key=YZvkiWUkycvspyGHk3fQRAkt\n\nYou can check the encrypted value of the Secret stored in etcd with the following command:\n\n$ sudo ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt\n\n\\\n\n--cert=/etc/kubernetes/pki/etcd/server.crt --\n\nkey=/etc/kubernetes/pki/\\\n\netcd/server.key get /registry/secrets/default/db-credentials |\n\nhexdump -C\n\n5. Open an interactive shell to the worker node using Vagrant:\n\n$ vagrant ssh kube-worker-1\n\nDefine the RuntimeClass with the following YAML manifest. The contents have been stored in the file runtime-class.yaml:\n\napiVersion: node.k8s.io/v1\n\nkind: RuntimeClass\n\nmetadata:\n\nname: container-runtime-sandbox\n\nhandler: runsc\n\nCreate the RuntimeClass object:\n\n$ kubectl apply -f runtime-class.yaml\n\nAssign the name of the RuntimeClass to the Pod using the spec.runtimeClassName attribute. The nginx Pod has been defined in the file pod.yaml:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: nginx\n\nspec:\n\nruntimeClassName: container-runtime-sandbox\n\ncontainers:\n\nname: nginx\n\nimage: nginx:1.23.2\n\nCreate the Pod object. The Pod will transition into the status “Running”:\n\n$ kubectl apply -f pod.yaml\n\n$ kubectl get pod nginx\n\nNAME READY STATUS RESTARTS AGE\n\nnginx 1/1 Running 0 2m21s\n\nExit out of the node:\n\n$ exit\n\nChapter 6, “Supply Chain Security”\n\n1. The initial container image built with the provided Dockerfile has a size of 998MB. You can produce and run the container image with the following commands. Run a quick curl command to see if the endpoint exposed by the application can be reached:\n\n$ docker build . -t node-app:0.0.1\n\n...\n\n$ docker images\n\nREPOSITORY TAG IMAGE ID CREATED SIZE\n\nnode-app 0.0.1 7ba99d4ba3af 3 seconds ago 998MB\n\n$ docker run -p 3001:3001 -d node-app:0.0.1\n\nc0c8a301eeb4ac499c22d10399c424e1063944f18fff70ceb5c49c4723af7969\n\n$ curl -L http://localhost:3001/\n\nHello World\n\nOne of the changes you can make is to avoid using a large base image. You could replace it with the alpine version of the node base image. Also, avoid pulling the latest image. Pick the Node.js version you actually want the application to run with. The following command uses a Dockerfile with the base image node:19-alpine, which reduces the container image size to 176MB:\n\n$ docker build . -t node-app:0.0.1\n\n...\n\n$ docker images\n\nREPOSITORY TAG IMAGE ID CREATED SIZE\n\nnode-app 0.0.1 ef2fbec41a75 2 seconds ago 176MB\n\n2. You can install Kyverno using Helm or by pointing to the YAML manifest available on the project’s GitHub repository. We’ll use the YAML manifest here:\n\n$ kubectl create -f https://raw.githubusercontent.com/kyverno/\\\n\nkyverno/main/config/install.yaml\n\nSet up a YAML manifest file named restrict-image- registries.yaml. Add the following contents to the file. The manifest represents a ClusterPolicy that only allows the use of container images that start with gcr.io/. Make sure to assign the value Enforce to the attribute spec.validationFailureAction:\n\napiVersion: kyverno.io/v1\n\nkind: ClusterPolicy\n\nmetadata:\n\nname: restrict-image-registries\n\nannotations:\n\npolicies.kyverno.io/title: Restrict Image Registries\n\npolicies.kyverno.io/category: Best Practices, EKS Best Practices\n\npolicies.kyverno.io/severity: medium\n\npolicies.kyverno.io/minversion: 1.6.0\n\npolicies.kyverno.io/subject: Pod\n\npolicies.kyverno.io/description: >-\n\nImages from unknown, public registries can be of dubious\n\nquality \\\n\nand may not be scanned and secured, representing a high degree\n\nof \\\n\nrisk. Requiring use of known, approved registries helps reduce\n\n\\\n\nthreat exposure by ensuring image pulls only come from them.\n\nThis \\\n\npolicy validates that container images only originate from the\n\n\\\n\nregistry `eu.foo.io` or `bar.io`. Use of this policy requires \\\n\ncustomization to define your allowable registries.\n\nspec:\n\nvalidationFailureAction: Enforce\n\nbackground: true\n\nrules:\n\nname: validate-registries\n\nmatch:\n\nany:\n\nresources:\n\nkinds:\n\nPod\n\nvalidate:\n\nmessage: \"Unknown image registry.\"\n\npattern:\n\nspec:\n\ncontainers:\n\nimage: \"gcr.io/*\"\n\nApply the manifest with the following command:\n\n$ kubectl apply -f restrict-image-registries.yaml\n\nRun the following commands to verify that the policy has become active. Any container image definition that doesn’t use the prefix gcr.io/ will be denied:\n\n$ kubectl run nginx --image=nginx:1.23.3\n\nError from server: admission webhook \"validate.kyverno.svc-fail\" \\\n\ndenied the request:\n\npolicy Pod/default/nginx for resource violation:\n\nrestrict-image-registries:\n\nvalidate-registries: 'validation error: Unknown image registry. \\\n\nrule validate-registries\n\nfailed at path /spec/containers/0/image/'\n\n$ kubectl run busybox --image=gcr.io/google-containers/busybox:1.27.2\n\npod/busybox created\n\n3. Find the SHA256 hash for the image nginx:1.23.3-alpine with the search functionality of Docker Hub. The search result will lead you to the tag of the image. On top of the page, you should find the digest sha256:c1b9fe3c0c015486cf1e4a0ecabe78d05864475e279638e 9713eb55f013f907f. Use the digest instead of the tag in the Pod definition. The result is the following YAML manifest: apiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: nginx\n\nspec:\n\ncontainers:\n\nname: nginx\n\nimage:\n\nnginx@sha256:c1b9fe3c0c015486cf1e4a0ecabe78d05864475e279638 \\\n\ne9713eb55f013f907f\n\nThe creation of the Pod should work:\n\n$ kubectl apply -f pod-validate-image.yaml\n\npod/nginx created\n\n$ kubectl get pods nginx\n\nNAME READY STATUS RESTARTS AGE\n\nnginx 1/1 Running 0 29s\n\nIf you modify the SHA256 hash in any form and try to recreate the Pod, then Kubernetes would not allow you to pull the image.\n\n4. Running Kubesec in a Docker container results in a whole bunch of suggestions, as shown in the following output:\n\n$ docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin < pod.yaml\n\n[\n\n{\n\n\"object\": \"Pod/hello-world.default\",\n\n\"valid\": true,\n\n\"message\": \"Passed with a score of 0 points\",\n\n\"score\": 0,\n\n\"scoring\": {\n\n\"advise\": [\n\n{\n\n\"selector\": \"containers[] .securityContext .capabilities \\\n\n.drop | index(\\\"ALL\\\")\",\n\n\"reason\": \"Drop all capabilities and add only those \\\n\nrequired to reduce syscall attack surface\"\n\n},\n\n{\n\n\"selector\": \"containers[] .resources .requests .cpu\",\n\n\"reason\": \"Enforcing CPU requests aids a fair balancing \\\n\nof resources across the cluster\"\n\n},\n\n{\n\n\"selector\": \"containers[] .securityContext .runAsNonRoot \\\n\n== true\",\n\n\"reason\": \"Force the running image to run as a non-root \\\n\nuser to ensure least privilege\"\n\n},\n\n{\n\n\"selector\": \"containers[] .resources .limits .cpu\",\n\n\"reason\": \"Enforcing CPU limits prevents DOS via resource \\\n\nexhaustion\"\n\n},\n\n{\n\n\"selector\": \"containers[] .securityContext .capabilities \\\n\n.drop\",\n\n\"reason\": \"Reducing kernel capabilities available to a \\\n\ncontainer limits its attack surface\"\n\n},\n\n{\n\n\"selector\": \"containers[] .resources .requests .memory\",\n\n\"reason\": \"Enforcing memory requests aids a fair balancing\n\n\\\n\nof resources across the cluster\"\n\n},\n\n{\n\n\"selector\": \"containers[] .resources .limits .memory\",\n\n\"reason\": \"Enforcing memory limits prevents DOS via\n\nresource \\\n\nexhaustion\"\n\n},\n\n{\n\n\"selector\": \"containers[] .securityContext \\\n\n.readOnlyRootFilesystem == true\",\n\n\"reason\": \"An immutable root filesystem can prevent\n\nmalicious \\\n\nbinaries being added to PATH and increase attack\n\n\\\n\ncost\"\n\n},\n\n{\n\n\"selector\": \".metadata .annotations .\\\"container.seccomp. \\\n\nsecurity.alpha.kubernetes.io/pod\\\"\",\n\n\"reason\": \"Seccomp profiles set minimum privilege and\n\nsecure \\\n\nagainst unknown threats\"\n\n},\n\n{\n\n\"selector\": \".metadata .annotations .\\\"container.apparmor.\n\n\\\n\nsecurity.beta.kubernetes.io/nginx\\\"\",\n\n\"reason\": \"Well defined AppArmor policies may provide\n\ngreater \\\n\nprotection from unknown threats. WARNING: NOT \\\n\nPRODUCTION READY\"\n\n},\n\n{\n\n\"selector\": \"containers[] .securityContext .runAsUser -gt \\\n\n10000\",\n\n\"reason\": \"Run as a high-UID user to avoid conflicts with \\\n\nthe host's user table\"\n\n},\n\n{\n\n\"selector\": \".spec .serviceAccountName\",\n\n\"reason\": \"Service accounts restrict Kubernetes API access\n\n\\\n\nand should be configured with least privilege\"\n\n}\n\n]\n\n}\n\n}\n\n]\n\nThe fixed-up YAML manifest could look like this:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: hello-world\n\nspec:\n\nserviceAccountName: default\n\ncontainers:\n\nname: linux\n\nimage: hello-world:linux\n\nresources:\n\nrequests:\n\nmemory: \"64Mi\"\n\ncpu: \"250m\"\n\nlimits:\n\nmemory: \"128Mi\"\n\ncpu: \"500m\"\n\nsecurityContext:\n\nreadOnlyRootFilesystem: true\n\nrunAsNonRoot: true\n\nrunAsUser: 20000\n\ncapabilities:\n\ndrop: [\"ALL\"]\n\n5. Executing the kubectl apply command against the existing setup.yaml manifest will create the Pods named backend, loop, and logstash in the namespace r61:\n\n$ kubectl apply -f setup.yaml\n\nnamespace/r61 created\n\npod/backend created\n\npod/loop created\n\npod/logstash created\n\nYou can check on them with the following command:\n\n$ kubectl get pods -n r61\n\nNAME READY STATUS RESTARTS AGE\n\nbackend 1/1 Running 0 115s\n\nlogstash 1/1 Running 0 115s\n\nloop 1/1 Running 0 115s\n\nCheck the images of each Pod in the namespace r61 using the kubectl describe command. The images used are bmuschko/nodejs-hello-world:1.0.0, alpine:3.13.4, and elastic/logstash:7.13.3:\n\n$ kubectl describe pod backend -n r61\n\n...\n\nContainers:\n\nhello:\n\nContainer ID: docker://eb0bdefc75e635d03b625140d1e \\\n\nb229ca2db7904e44787882147921c2bd9c365\n\nImage: bmuschko/nodejs-hello-world:1.0.0\n\n...\n\nUse the Trivy executable to check vulnerabilities for all images:\n\n$ trivy image bmuschko/nodejs-hello-world:1.0.0\n\n$ trivy image alpine:3.13.4\n\n$ trivy image elastic/logstash:7.13.3\n\nIf you look closely at the list of vulnerabilities, you will find that all images contain issues with “CRITICAL” severity. As a result, delete all Pods:\n\n$ kubectl delete pod backend -n r61\n\n$ kubectl delete pod logstash -n r61\n\n$ kubectl delete pod loop -n r61\n\nChapter 7, “Monitoring, Logging, and Runtime Security”\n\n1. Shell into the worker node with the following command:\n\n$ vagrant ssh kube-worker-1\n\nInspect the command and arguments of the running Pod named malicious. You will see that it tries to append a message to the file /etc/threat:\n\n$ kubectl get pod malicious -o jsonpath='{.spec.containers[0].args}'\n\n...\n\nspec:\n\ncontainers:\n\nargs:\n\n/bin/sh\n\n-c\n\nwhile true; do echo \"attacker intrusion\" >> /etc/threat; \\\n\nsleep 5; done\n\n...\n\nOne of Falco’s default rules monitors file operations that try to write to the /etc directory. You can find a message for every write attempt in standard output:\n\n$ sudo journalctl -fu falco\n\nJan 24 23:40:18 kube-worker-1 falco[8575]: 23:40:18.359740123: Error\n\n\\\n\nFile below /etc opened for writing (user=<NA> user_loginuid=-1 \\\n\ncommand=sh -c while true; do echo \"attacker intrusion\" >>\n\n/etc/threat; \\\n\nsleep 5; done pid=9763 parent=<NA> pcmdline=<NA> file=/etc/threat \\\n\nprogram=sh gparent=<NA> ggparent=<NA> gggparent=<NA> \\\n\ncontainer_id=e72a6dbb63b8 image=docker.io/library/alpine)\n\n...\n\nFind the rule that produces the message in /etc/falco/falco_rules.yaml by searching for the string “etc opened for writing.” The rule looks as follows:\n\nrule: Write below etc\n\ndesc: an attempt to write to any file below /etc\n\ncondition: write_etc_common\n\noutput: \"File below /etc opened for writing (user=%user.name \\\n\nuser_loginuid=%user.loginuid command=%proc.cmdline \\\n\npid=%proc.pid parent=%proc.pname pcmdline=%proc.pcmdline \\\n\nfile=%fd.name program=%proc.name gparent=%proc.aname[2] \\\n\nggparent=%proc.aname[3] gggparent=%proc.aname[4] \\\n\ncontainer_id=%container.id\n\nimage=%container.image.repository)\"\n\npriority: ERROR\n\ntags: [filesystem, mitre_persistence]\n\nCopy the rule to the file /etc/falco/falco_rules.local.yaml and modify the output definition, as follows:\n\nrule: Write below etc\n\ndesc: an attempt to write to any file below /etc\n\ncondition: write_etc_common\n\noutput: \"%evt.time,%user.name,%container.id\"\n\npriority: ERROR\n\ntags: [filesystem, mitre_persistence]\n\nRestart the Falco service, and find the changed output in the Falco logs:\n\n$ sudo systemctl restart falco\n\n$ sudo journalctl -fu falco\n\nJan 24 23:48:18 kube-worker-1 falco[17488]: 23:48:18.516903001: \\\n\nError 23:48:18.516903001,<NA>,e72a6dbb63b8\n\n...\n\nEdit the file /etc/falco/falco.yaml to change the output channel. Disable standard output, enable file output, and point the file_output attribute to the file /var/log/falco.log. The resulting configuration will look like the following:\n\nfile_output:\n\nenabled: true\n\nkeep_alive: false\n\nfilename: /var/log/falco.log\n\nstdout_output:\n\nenabled: false\n\nThe log file will now append Falco log:\n\n$ sudo tail -f /var/log/falco.log\n\n00:10:30.425084165: Error 00:10:30.425084165,<NA>,e72a6dbb63b8\n\n...\n\nExit out of the VM:\n\n$ exit\n\n2. Create the Pod named hash from the setup.yaml file. The command running in its container appends a hash to a file at /var/config/hash.txt in an infinite loop:\n\n$ kubectl apply -f setup.yaml\n\npod/hash created\n\n$ kubectl get pod hash\n\nNAME READY STATUS RESTARTS AGE\n\nhash 1/1 Running 0 27s\n\n$ kubectl exec -it hash -- /bin/sh\n\n/ # ls /var/config/hash.txt\n\n/var/config/hash.txt\n\nTo make the container immutable, you will have to add configuration to the existing Pod definition. You have to set the root filesystem to read-only access and mount a Volume to the path /var/config to allow writing to the file named hash.txt. The resulting YAML manifest could look as follows:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: hash\n\nspec:\n\ncontainers:\n\nname: hash\n\nimage: alpine:3.17.1\n\nsecurityContext:\n\nreadOnlyRootFilesystem: true\n\nvolumeMounts:\n\nname: hash-vol\n\nmountPath: /var/config\n\ncommand: [\"sh\", \"-c\", \"if [ ! -d /var/config ]; then mkdir -p \\\n\n/var/config; fi; while true; do echo $RANDOM | md5sum \\\n\n| head -c 20 >> /var/config/hash.txt; sleep 20; done\"]\n\nvolumes:\n\nname: hash-vol\n\nemptyDir: {}\n\n3. Shell into the control plane node with the following command:\n\n$ vagrant ssh kube-control-plane\n\nEdit the existing audit policy file at /etc/kubernetes/audit/rules/audit-policy.yaml. Add the rules asked about in the instructions. The content of the final audit policy file could look as follows:\n\napiVersion: audit.k8s.io/v1\n\nkind: Policy\n\nomitStages:\n\n\"RequestReceived\"\n\nrules:\n\nlevel: RequestResponse\n\nresources:\n\ngroup: \"\"\n\nresources: [\"pods\"]\n\nlevel: Metadata\n\nresources:\n\ngroup: \"\"\n\nresources: [\"secrets\", \"configmaps\"]\n\nlevel: Request\n\nresources:\n\ngroup: \"\"\n\nresources: [\"services\"]\n\nConfigure the API server to consume the audit policy file by editing the file /etc/kubernetes/manifests/kube- apiserver.yaml. Provide additional options, as requested. The relevant configuration needed is as follows:\n\n...\n\nspec:\n\ncontainers:\n\ncommand:\n\nkube-apiserver\n\n--audit-policy-file=/etc/kubernetes/audit/rules/audit-\n\npolicy.yaml\n\n--audit-log-path=/var/log/kubernetes/audit/logs/apiserver.log\n\n--audit-log-maxage=5\n\n...\n\nvolumeMounts:\n\nmountPath: /etc/kubernetes/audit/rules/audit-policy.yaml\n\nname: audit\n\nreadOnly: true\n\nmountPath: /var/log/kubernetes/audit/logs/\n\nname: audit-log\n\nreadOnly: false\n\n...\n\nvolumes:\n\nname: audit\n\nhostPath:\n\npath: /etc/kubernetes/audit/rules/audit-policy.yaml\n\ntype: File\n\nname: audit-log\n\nhostPath:\n\npath: /var/log/kubernetes/audit/logs/\n\ntype: DirectoryOrCreate\n\nOne of the logged resources is a ConfigMap on the Metadata level. The following command creates an exemplary ConfigMap object:\n\n$ kubectl create configmap db-user --from-literal=username=tom\n\nconfigmap/db-user created\n\nThe audit log file will now contain an entry for the event:\n\n$ sudo cat /var/log/kubernetes/audit/logs/apiserver.log\n\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"Metadata\", \\\n\n\"auditID\":\"1fbb409a-3815-4da8-8a5e-d71c728b98b1\",\"stage\": \\\n\n\"ResponseComplete\",\"requestURI\":\"/api/v1/namespaces/default/configmap\n\ns? \\\n\nfieldManager=kubectl-create\\u0026fieldValidation=Strict\",\"verb\": \\\n\n\"create\",\"user\":{\"username\":\"kubernetes-admin\",\"groups\": \\\n\n[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\": \\\n\n[\"192.168.56.10\"], \"userAgent\":\"kubectl/v1.24.4 (linux/amd64) \\\n\nkubernetes/95ee5ab\", \"objectRef\":{\"resource\":\"configmaps\", \\\n\n\"namespace\":\"default\", \"name\":\"db-user\",\"apiVersion\":\"v1\"}, \\\n\n\"responseStatus\":{\"metadata\": {},\"code\":201}, \\\n\n\"requestReceivedTimestamp\":\"2023-01-25T18:57:51.367219Z\", \\\n\n\"stageTimestamp\":\"2023-01-25T18:57:51.372094Z\",\"annotations\": \\\n\n{\"authorization.k8s.io/decision\":\"allow\", \\\n\n\"authorization.k8s.io/reason\":\"\"}}\n\nExit out of the VM:\n\n$ exit\n\nOceanofPDF.com\n\nIndex\n\nA\n\naa-complain command, Setting a custom profile\n\naa-enforce command, Setting a custom profile\n\naa-status command, Understanding profiles\n\naccess\n\nmonitoring using audit logs, Using Audit Logs to Monitor Access- Configuring a Webhook Backend\n\nrestricting to API Server, Restricting Access to the API Server- Creating a Secret for a service account\n\nadduser command, Adding a user\n\nadministration privileges, creating users with, Creating a User with Administration Privileges\n\nadmission control, as stage in request processing, Processing a Request\n\nAKS Application Gateway Ingress Controller, Creating an Ingress with TLS Termination\n\nanonymous access, Anonymous access\n\nanswers to review questions\n\nbehavior analytics, Chapter 7, “Monitoring, Logging, and Runtime Security”\n\ncluster hardening, Chapter 3, “Cluster Hardening”-Chapter 3, “Cluster Hardening”\n\ncluster setup, Chapter 2, “Cluster Setup”-Chapter 2, “Cluster Setup”\n\nmicroservice vulnerabilities, Chapter 5, “Minimize Microservice Vulnerabilities”-Chapter 5, “Minimize Microservice Vulnerabilities”\n\nsupply chain security, Chapter 6, “Supply Chain Security”-Chapter 6, “Supply Chain Security”\n\nsystem hardening, Chapter 4, “System Hardening”-Chapter 4, “System Hardening”\n\nAPI server\n\nconnecting to, Connecting to the API Server-Access with a client certificate\n\nrestricting access to, Restricting Access to the API Server-Creating a Secret for a service account\n\nAppArmor, Involved External Tools, Using AppArmor-Applying a profile to a container\n\napply command, Creating the RoleBinding, Applying the default container runtime profile to a container, Whitelisting Allowed Image Registries with OPA GateKeeper\n\napt purge command, Removing Unwanted Packages\n\nattack surface, reducing, Using a Multi-Stage Approach for Building Container Images\n\naudit backend, Understanding Audit Logs\n\naudit logs, Monitoring, Logging, and Runtime Security, Using Audit Logs to Monitor Access-Configuring a Webhook Backend\n\naudit mode, Understanding Pod Security Admission (PSA)\n\naudit policy, Understanding Audit Logs\n\nauthentication, as stage in request processing, Processing a Request\n\nauthorization, as stage in request processing, Processing a Request\n\nautomounting, disabling for service account tokens, Disabling automounting of a service account token\n\nB\n\nbase images\n\nminimizing footprint for, Minimizing the Base Image Footprint-Using Container Image Optimization Tools\n\nselecting, Picking a Base Image Small in Size\n\nbaseline level, Understanding Pod Security Admission (PSA)\n\nbehavior analytics\n\nanswers to review questions, Chapter 7, “Monitoring, Logging, and Runtime Security”\n\ndefined, Monitoring, Logging, and Runtime Security\n\nensuring container immutability, Ensuring Container Immutability\n\nperforming, Performing Behavior Analytics-Overriding Existing Rules\n\nsample exercises, Exam Essentials\n\nusing audit logs to monitor access, Using Audit Logs to Monitor Access-Configuring a Webhook Backend\n\nbinaries, verifying against hash, Verifying a Binary Against Hash\n\nblog (Kubernetes), Documentation\n\nBurns, Brendan, Managing Kubernetes, Interacting with the Kubernetes API\n\nC\n\nCA (certificate authority), Adopting mTLS in Kubernetes\n\nCalico, Adopting mTLS in Kubernetes\n\ncalling Ingress, Calling the Ingress\n\ncandidate skills, Candidate Skills\n\ncertificate approve command, Creating and approving a CertificateSigningRequest\n\nCertificateSigningRequest, creating and approving, Creating and approving a CertificateSigningRequest\n\ncertification, learning path for, Kubernetes Certification Learning Path\n\nCertified Kubernetes Administrator (CKA) Study Guide (Muschko), Who This Book Is For, Candidate Skills, Using Network Policies to Restrict Pod- to-Pod Communication, Creating an Ingress with TLS Termination, Performing the Upgrade Process, Managing Secrets\n\nchmod command, Changing file permissions\n\nchown command, Changing file ownership\n\nCilium, Adopting mTLS in Kubernetes\n\nCIS (Center for Internet Security), Applying Kubernetes Component Security Best Practices\n\nCIS benchmark, for Ubuntu Linux, Minimizing the Host OS Footprint\n\nCKA (Certified Kubernetes Administrator), Preface, Certified Kubernetes Administrator (CKA)\n\nCKAD (Certified Kubernetes Application Developer), Preface, Certified Kubernetes Application Developer (CKAD)\n\nCKS (Certified Kubernetes Security Specialist), Preface, Certified Kubernetes Security Specialist (CKS)\n\nclient certificates, access with, Access with a client certificate\n\ncluster hardening, Cluster Hardening-Sample Exercises\n\nabout, Cluster Hardening, Cluster Hardening\n\nanswers to review questions, Chapter 3, “Cluster Hardening”-Chapter 3, “Cluster Hardening”\n\ninteracting with Kubernetes API, Interacting with the Kubernetes API- Access with a client certificate\n\nrestricting access to API server, Restricting Access to the API Server- Creating a Secret for a service account\n\nsample exercises, Sample Exercises\n\nupdating Kubernetes, Updating Kubernetes Frequently-Summary\n\ncluster setup, Cluster Setup-Sample Exercises\n\nabout, Cluster Setup, Cluster Setup\n\nanswers to review questions, Chapter 2, “Cluster Setup”-Chapter 2, “Cluster Setup”\n\napplying Kubernetes component best security practices, Applying Kubernetes Component Security Best Practices-Fixing Detected Security Issues\n\ncreating ingress with TLS termination, Creating an Ingress with TLS Termination-Calling the Ingress\n\nprotecting GUI elements, Protecting GUI Elements-Avoiding Insecure Configuration Arguments\n\nprotecting node metadata and endpoints, Protecting Node Metadata and Endpoints-Protecting Metadata Server Access with Network Policies\n\nrestricting pod-to-pod communication using network policies, Using Network Policies to Restrict Pod-to-Pod Communication-Allowing Fine-Grained Incoming Traffic\n\nsample exercises, Sample Exercises\n\nverifying Kubernetes platform binaries, Verifying Kubernetes Platform Binaries\n\nClusterIP Service type, Scenario: An Attacker Gains Access to the Dashboard Functionality\n\nClusterRole, Creating a User with Restricted Privileges, Creating the ClusterRole\n\nClusterRoleBinding object, Creating a User with Administration Privileges\n\nCMD command, Reducing the Number of Layers\n\nCNCF (Cloud Native Computing Foundation), Exam Objectives\n\nCNI (Container Network Interface) plugin, Using Network Policies to Restrict Pod-to-Pod Communication\n\ncommands\n\naa-complain, Setting a custom profile\n\naa-enforce, Setting a custom profile\n\naa-status, Understanding profiles\n\nadduser, Adding a user\n\napply, Creating the RoleBinding, Applying the default container runtime profile to a container, Whitelisting Allowed Image Registries\n\nwith OPA GateKeeper\n\napt purge, Removing Unwanted Packages\n\ncertificate approve, Creating and approving a CertificateSigningRequest\n\nchmod, Changing file permissions\n\nchown, Changing file ownership\n\nCMD, Reducing the Number of Layers\n\nCOPY, Reducing the Number of Layers\n\ncreate ingress, Creating the Ingress\n\ncreate rolebinding, Creating a Role and a RoleBinding\n\ncreate token, Generating a service account token\n\ncurl, Anonymous access, Verifying the granted permissions, Implementing the Backend Application\n\ndisable, Disabling Services\n\ndocker build, Using a Multi-Stage Approach for Building Container Images\n\ndocker trust sign, Signing Container Images\n\necho, Applying the default container runtime profile to a container\n\netcdctl, Scenario: An Attacker Gains Access to the Node Running etcd, Encrypting etcd Data\n\nFROM, Reducing the Number of Layers\n\nget all, Setting Up the Ingress Backend\n\ngo build, Using a Multi-Stage Approach for Building Container Images\n\ngroupadd, Adding a group\n\ngroupdel, Deleting a group\n\njournalctl, Generating Events and Inspecting Falco Logs\n\nkubectl, Installing Gatekeeper, Configuring a Log Backend\n\nkubectl apply, Observing the Default Behavior\n\nkubectl logs, Using kube-bench\n\nls, Viewing file permissions and ownership\n\nmkdir, Applying the custom profile to a container\n\nnetstat, Identifying and Disabling Open Ports\n\nOpenSSL, Creating the TLS Certificate and Key\n\nRUN, Reducing the Number of Layers\n\nss, Identifying and Disabling Open Ports\n\nstatus, Disabling Services\n\nsu, Switching to a user\n\nsudo, Switching to a user\n\nsudo systemctl restart kubelet, Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nsysctl, Avoiding Privileged Containers\n\nsystemctl, Disabling Services\n\nsystemctl status, Identifying and Disabling Open Ports\n\ntouch, Viewing file permissions and ownership\n\nusermod, Deleting a group\n\nwget, Denying Directional Network Traffic\n\nComplain profile mode, Understanding profiles\n\ncomponent security best practices, applying, Applying Kubernetes Component Security Best Practices-Fixing Detected Security Issues\n\nConfigMap, Scenario: An Attacker Installs Malicious Software, Configuring a Container with a ConfigMap or Secret\n\nconfiguration file (Falco), Falco configuration file\n\nconfiguring\n\ncontainers with ConfigMap, Configuring a Container with a ConfigMap or Secret\n\ncontainers with Secrets, Configuring a Container with a ConfigMap or Secret\n\nFalco, Configuring Falco\n\ngVisor, Installing and Configuring gVisor\n\nImagePolicyWebhook Admission Controller plugin, Configuring the ImagePolicyWebhook Admission Controller Plugin-Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nlog backend, Configuring a Log Backend\n\nports for API server, Connecting to the API Server\n\nread-only container root filesystems, Configuring a Read-Only Container Root Filesystem\n\nwebhook backend, Configuring a Webhook Backend\n\nconnecting to API server, Connecting to the API Server-Access with a client certificate\n\nconstraint template, Involved Kubernetes Primitives, Implementing an OPA Policy, Whitelisting Allowed Image Registries with OPA GateKeeper\n\ncontainer images\n\noptimization tools for, Using Container Image Optimization Tools\n\nsigning, Signing Container Images\n\nusing a multi-stage approach for building, Using a Multi-Stage Approach for Building Container Images\n\nvalidating, Validating Container Images\n\ncontainer runtime sandboxes, Understanding Container Runtime Sandboxes-Creating and Using a Runtime Class\n\nContainer Security (Rice), Securing the Supply Chain\n\ncontainerd, Applying the default container runtime profile to a container\n\ncontainers\n\napplying custom profiles to, Applying the custom profile to a container\n\napplying profiles to, Applying a profile to a container\n\nconfiguring with ConfigMap or Secrets, Configuring a Container with a ConfigMap or Secret\n\nensuring immutability, Ensuring Container Immutability\n\nusing in privileged mode, Avoiding Privileged Containers\n\nContinuous Delivery (Humble and Farley), Static Analysis of Workload\n\nCOPY command, Reducing the Number of Layers\n\nCRDs (Custom Resource Definitions), Involved Kubernetes Primitives\n\ncreate ingress command, Creating the Ingress\n\ncreate rolebinding command, Creating a Role and a RoleBinding\n\ncreate token command, Generating a service account token\n\nCSR (certificate signing request), Creating a private key, Adopting mTLS in Kubernetes\n\ncurl command, Anonymous access, Verifying the granted permissions, Implementing the Backend Application\n\ncurriculum, Curriculum\n\ncustom profiles\n\napplying to containers, Applying the custom profile to a container\n\nsetting, Setting a custom profile, Setting a custom profile\n\ncustom rules, for Falco, Custom rules\n\nCVE (Common Vulnerabilities and Exposures) database, Updating Kubernetes Frequently\n\nCVE Details, Scanning Images for Known Vulnerabilities\n\nD\n\ndefault behavior, observing, Observing the Default Behavior\n\ndefault container runtime profile, applying to containers, Applying the default container runtime profile to a container\n\ndefault namespace, Observing the Default Behavior, Using kube-bench, Verifying the permissions\n\ndefault permissions, verifying, Verifying the default permissions\n\ndefault rules, for Falco, Default rules\n\nDegioanni, Loris, Practical Cloud Native Security with Falco, Understanding Falco\n\ndeny-all network policy, Denying Directional Network Traffic\n\ndeprecation policy (Kubernetes), Versioning Scheme\n\ndirectional network traffic, Denying Directional Network Traffic\n\ndisable command, Disabling Services\n\ndisabling\n\nautomounting for service account tokens, Disabling automounting of a service account token\n\nopen ports, Identifying and Disabling Open Ports\n\ndistroless image, Picking a Base Image Small in Size, Using a Distroless Container Image\n\nDive, Using Container Image Optimization Tools\n\ndocker build command, Using a Multi-Stage Approach for Building Container Images\n\nDocker Engine, Applying the default container runtime profile to a container\n\nDocker Hub, Scenario: An Attacker Exploits Container Vulnerabilities, Using Public Image Registries\n\ndocker trust sign command, Signing Container Images\n\nDocker, multi-stage build in, Using a Multi-Stage Approach for Building Container Images\n\nDockerfiles, Scenario: An Attacker Exploits Container Vulnerabilities, Using a Multi-Stage Approach for Building Container Images, Using Hadolint for Analyzing Dockerfiles\n\nDockerSlim, Using Container Image Optimization Tools\n\ndocumentation\n\nAWS, Protecting Metadata Server Access with Network Policies\n\nDocker, Scenario: An Attacker Exploits Container Vulnerabilities\n\nEKS, Restricting Access to the API Server\n\nFalco, Installing Falco, Understanding Falco Rule File Basics\n\nGKE, Restricting Access to the API Server\n\nKubernetes, What You Will Learn, Documentation, Using Network Policies to Restrict Pod-to-Pod Communication, Denying Directional Network Traffic, Creating an Ingress with TLS Termination, Verifying a Binary Against Hash, Processing a Request, Restricting User Permissions, Verifying the default permissions, Using seccomp, Managing Secrets, Encrypting etcd Data, Adopting mTLS in Kubernetes, Picking a Base Image Small in Size, Implementing the Backend Application, Configuring a Container with a ConfigMap or Secret, Creating the Audit Policy File, Configuring a Log Backend\n\nOpenSSL, Creating the TLS Certificate and Key\n\nTrivy, Scanning Images for Known Vulnerabilities\n\nE\n\necho command, Applying the default container runtime profile to a container\n\nEKS (Amazon Elastic Kubernetes Service), Applying Kubernetes Component Security Best Practices\n\nencrypting\n\netcd data, Encrypting etcd Data\n\nPod-to-Pod, Understanding Pod-to-Pod Encryption with mTLS\n\nendpoints, protecting, Protecting Node Metadata and Endpoints-Protecting Metadata Server Access with Network Policies\n\nenforce mode, Understanding Pod Security Admission (PSA)\n\nEnforce profile mode, Understanding profiles\n\netcd data\n\naccessing, Accessing etcd Data\n\nencrypting, Encrypting etcd Data\n\netcdctl command, Scenario: An Attacker Gains Access to the Node Running etcd, Encrypting etcd Data\n\nevents, generating in Falco, Generating Events and Inspecting Falco Logs\n\nexam objectives, Exam Objectives\n\nexecution arguments, Avoiding Insecure Configuration Arguments\n\nexternal access, minimizing to network, Minimizing External Access to the Network\n\nexternal tools, Involved External Tools\n\nF\n\nF5 NGINX Ingress Controller, Creating an Ingress with TLS Termination\n\nFalco\n\nabout, Monitoring, Logging, and Runtime Security, Involved External Tools, Monitoring, Logging, and Runtime Security, Understanding Falco\n\nconfiguration file, Falco configuration file\n\nconfiguring, Configuring Falco\n\ncustom rules for, Custom rules\n\ndefault rules for, Default rules\n\ngenerating events in, Generating Events and Inspecting Falco Logs\n\ninspecting logs, Generating Events and Inspecting Falco Logs\n\ninstalling, Installing Falco\n\noverriding existing rules, Overriding Existing Rules\n\nrule file basics, Understanding Falco Rule File Basics\n\nFalco 101 video course, Understanding Falco\n\nfalco.yaml file, Falco configuration file\n\nfalco_rules.local.yaml file, Custom rules\n\nfalco_rules.yaml file, Default rules\n\nFarley, David, Continuous Delivery, Static Analysis of Workload\n\nfeature gate, Applying the default container runtime profile to a container\n\nfile ownership, Understanding File Permissions and Ownership\n\nfile permissions, Understanding File Permissions and Ownership, Changing file permissions\n\nfine-grained incoming traffic, allowing, Allowing Fine-Grained Incoming Traffic\n\nfirewalls, setting up rules for, Setting Up Firewall Rules\n\nFROM command, Reducing the Number of Layers\n\nFTP servers, Identifying and Disabling Open Ports\n\nG\n\nget all command, Setting Up the Ingress Backend\n\nGitHub, Documentation, Using Container Image Optimization Tools, Implementing the Backend Application\n\nGKE (Google Kubernetes Engine), Applying Kubernetes Component Security Best Practices\n\ngo build command, Using a Multi-Stage Approach for Building Container Images\n\nGo runtime, Using a Multi-Stage Approach for Building Container Images\n\nGoogle Cloud container registry, Using Public Image Registries\n\nGoogle distroless image, Picking a Base Image Small in Size\n\ngranted permissions, verifying, Verifying the granted permissions\n\nGrasso, Leonardo, Practical Cloud Native Security with Falco, Understanding Falco\n\ngroup ID, setting, Setting a Specific User and Group ID\n\ngroup management, Understanding Group Management\n\ngroupadd command, Adding a group\n\ngroupdel command, Deleting a group\n\ngroups, Listing groups\n\nGUIs (graphical user interfaces), Cluster Setup, Protecting GUI Elements\n\ngVisor\n\nabout, Minimize Microservice Vulnerabilities, Involved External Tools, Available Container Runtime Sandbox Implementations\n\nconfiguring, Installing and Configuring gVisor\n\ninstalling, Installing and Configuring gVisor\n\nH\n\nhadolint (see Haskell Dockerfile Linter)\n\nhash, verifying binaries against, Verifying a Binary Against Hash\n\nHaskell Dockerfile Linter, Using Hadolint for Analyzing Dockerfiles\n\nHelm package manager, Scenario: An Attacker Can Call the API Server from a Service Account\n\nhelp option, Creating the Ingress\n\nhost OS footprint, minimizing, Minimizing the Host OS Footprint- Removing Unwanted Packages\n\nHumble, Jez, Continuous Delivery, Static Analysis of Workload\n\nI\n\nidentity and access management (IAM) roles, minimizing, Minimizing IAM Roles-Changing file permissions\n\nimage digest validation, Validating Container Images\n\nimage pull policy, Scenario: An Attacker Injects Malicious Code into a Container Image, Using Public Image Registries\n\nimage registries\n\nwhitelisting with ImagePolicyWebhook Admission Controller plugin, Whitelisting Allowed Image Registries with the ImagePolicyWebhook Admission Controller Plugin\n\nwhitelisting with OPA Gatekeeper, Whitelisting Allowed Image Registries with OPA GateKeeper-Whitelisting Allowed Image Registries with OPA GateKeeper\n\nImagePolicyWebhook Admission Controller plugin\n\nconfiguring, Configuring the ImagePolicyWebhook Admission Controller Plugin-Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nwhitelisting allowed image registries with, Whitelisting Allowed Image Registries with the ImagePolicyWebhook Admission Controller Plugin\n\nimages\n\nbase, Picking a Base Image Small in Size\n\ncontainer, Using a Multi-Stage Approach for Building Container Images, Using Container Image Optimization Tools, Validating Container Images\n\ndistroless, Picking a Base Image Small in Size, Using a Distroless Container Image\n\nscanning for known vulnerabilities, Scanning Images for Known Vulnerabilities\n\nimmutability, ensuring for containers, Ensuring Container Immutability\n\nimperative method, Creating the Ingress\n\ninbound control node ports, Protecting Node Metadata and Endpoints\n\ningress, creating with TLS termination, Creating an Ingress with TLS Termination-Calling the Ingress\n\ninsecure configuration arguments, avoiding, Avoiding Insecure Configuration Arguments\n\ninstalling\n\nFalco, Installing Falco\n\nGatekeeper, Installing Gatekeeper\n\ngVisor, Installing and Configuring gVisor\n\nKubernetes Dashboard, Installing the Kubernetes Dashboard\n\noptions for, Practicing and Practice Exams\n\nJ\n\nJFrog Artifactory, Scenario: An Attacker Uploads a Malicious Container Image\n\njournalctl command, Generating Events and Inspecting Falco Logs\n\nJSON Lines format, Understanding Audit Logs\n\nK\n\nk8s_audit_rules.yaml file, Kubernetes-specific rules\n\nKata Containers, Minimize Microservice Vulnerabilities, Involved External Tools, Available Container Runtime Sandbox Implementations\n\nKCNA (Kubernetes and Cloud Native Associate), Kubernetes and Cloud Native Associate (KCNA)\n\nKCSA (Kubernetes and Cloud Native Security Associate), Kubernetes and Cloud Native Security Associate (KCSA)\n\nkernel hardening tools, Using Kernel Hardening Tools-Applying the custom profile to a container\n\nKiller Shell, Practicing and Practice Exams\n\nknown vulnerabilities, scanning images for, Scanning Images for Known Vulnerabilities\n\nkube-bench, Involved External Tools, Using kube-bench, Using kube-bench\n\nkubeadm, Verifying Kubernetes Platform Binaries\n\nkubeconfig file, Adding the user to the kubeconfig file, Configuring a Webhook Backend\n\nkubectl apply command, Observing the Default Behavior\n\nkubectl command, Installing Gatekeeper, Configuring a Log Backend\n\nkubectl logs command, Using kube-bench\n\nkubectl tool, Protecting GUI Elements, Verifying Kubernetes Platform Binaries, Verifying the permissions, Using Kubesec for Analyzing Kubernetes Manifests\n\nKubernetes\n\nrelease notes, Exam Objectives\n\nupdating, Updating Kubernetes Frequently-Summary\n\nusing mTLS in, Adopting mTLS in Kubernetes\n\nKubernetes API, interacting with, Interacting with the Kubernetes API- Access with a client certificate\n\nKubernetes CIS Benchmark, Applying Kubernetes Component Security Best Practices\n\nKubernetes Dashboard\n\nabout, Protecting GUI Elements\n\naccessing, Accessing the Kubernetes Dashboard\n\ninstalling, Installing the Kubernetes Dashboard\n\nKubernetes Manifests, analyzing using Kubesec, Using Kubesec for Analyzing Kubernetes Manifests-Using Kubesec for Analyzing Kubernetes Manifests\n\nKubernetes primitives (see primitives)\n\nKubernetes Service, Using the kubernetes Service\n\nKubernetes-specific rules, Kubernetes-specific rules\n\nKubesec, Static Analysis of Workload, Using Kubesec for Analyzing Kubernetes Manifests-Using Kubesec for Analyzing Kubernetes Manifests\n\nKyverno, Implementing an OPA Policy\n\nL\n\nlayers, reducing number of, Reducing the Number of Layers\n\nlisting groups, Listing groups\n\nlisting users, Listing users\n\nlists (Falco), List\n\nlog backend, Understanding Audit Logs, Configuring a Log Backend\n\nlogging, Monitoring, Logging, and Runtime Security, Monitoring, Logging, and Runtime Security-Exam Essentials\n\n(see also behavior analytics)\n\nlogs, inspecting in Falco, Generating Events and Inspecting Falco Logs\n\nls command, Viewing file permissions and ownership\n\nLTS (Long-Term Support), Updating Kubernetes Frequently\n\nM\n\nmacros (Falco), Macro\n\nManaging Kubernetes (Burns and Tracey), Interacting with the Kubernetes API\n\nMD5, Verifying a Binary Against Hash\n\nMetadata audit level, Creating the Audit Policy File\n\nmetadata server access, protecting with network policies, Protecting Metadata Server Access with Network Policies\n\nmicroservice vulnerabilities, Minimizing Microservice Vulnerabilities- Exam Essentials\n\nabout, Minimize Microservice Vulnerabilities, Minimizing Microservice Vulnerabilities\n\nanswers to review questions, Chapter 5, “Minimize Microservice Vulnerabilities”-Chapter 5, “Minimize Microservice Vulnerabilities”\n\ncontainer runtime sandboxes, Understanding Container Runtime Sandboxes-Creating and Using a Runtime Class\n\nmanaging Secrets, Managing Secrets-Encrypting etcd Data\n\nPod-to-Pod encryption with mTLS, Understanding Pod-to-Pod Encryption with mTLS\n\nsample exercises, Sample Exercises\n\nsetting OS-level security domains, Setting Appropriate OS-Level Security Domains-Implementing an OPA Policy\n\nMinikube, Practicing and Practice Exams\n\nmkdir command, Applying the custom profile to a container\n\nmode, Understanding Pod Security Admission (PSA)\n\nmonitoring, Monitoring, Logging, and Runtime Security, Monitoring, Logging, and Runtime Security-Exam Essentials, Using Audit Logs to Monitor Access-Configuring a Webhook Backend\n\n(see also behavior analytics)\n\nmTLS Pod-to-Pod encryption, Minimize Microservice Vulnerabilities, Understanding Pod-to-Pod Encryption with mTLS\n\nmulti-stage approach, using for building container images, Using a Multi- Stage Approach for Building Container Images\n\nMuschko, Benjamin\n\nCertified Kubernetes Administrator (CKA) Study Guide, Who This Book Is For, Candidate Skills, Creating an Ingress with TLS Termination, Performing the Upgrade Process, Managing Secrets\n\nCertified Kubernetes Application Developer (CKAD) Study Guide, Using Network Policies to Restrict Pod-to-Pod Communication\n\nN\n\nnamespaces, enforcing pod security standards for, Enforcing Pod Security Standards for a Namespace\n\nNAT (Network Address Translation), Using Network Policies to Restrict Pod-to-Pod Communication\n\nnetstat command, Identifying and Disabling Open Ports\n\nnetwork policies\n\nprotecting metadata server access with, Protecting Metadata Server Access with Network Policies\n\nusing to restrict pod-to-pod communication, Using Network Policies to Restrict Pod-to-Pod Communication\n\nnetworkpolicy.io, Using Network Policies to Restrict Pod-to-Pod Communication\n\nnetworks, minimizing external access to, Minimizing External Access to the Network\n\nnginx, Configuring a Read-Only Container Root Filesystem\n\nnode metadata, protecting, Protecting Node Metadata and Endpoints\n\nNodePort Service type, Scenario: An Attacker Gains Access to the Dashboard Functionality\n\nnon-root users, enforcing usage of, Enforcing the Usage of a Non-Root User\n\nNone audit level, Creating the Audit Policy File\n\nO\n\nOCI (Open Container Initiative) runtime, Installing and Configuring gVisor\n\nOPA (Open Policy Agent), Understanding Open Policy Agent (OPA) and Gatekeeper\n\nOPA (Open Policy Agent) Gatekeeper\n\nabout, Involved Kubernetes Primitives, Setting Appropriate OS-Level Security Domains, Understanding Open Policy Agent (OPA) and Gatekeeper\n\ninstalling, Installing Gatekeeper\n\nLibrary, Implementing an OPA Policy\n\nwhitelisting allowed image registries with, Whitelisting Allowed Image Registries with OPA GateKeeper-Whitelisting Allowed Image Registries with OPA GateKeeper\n\nOpenSSL command, Creating the TLS Certificate and Key\n\noptimization tools, for container images, Using Container Image Optimization Tools\n\nO’Reilly learning platform, Practicing and Practice Exams\n\nP\n\npackages, removing unwanted, Removing Unwanted Packages\n\npermissions\n\nminimizing for Service Account, Minimizing Permissions for a Service Account-Creating a Secret for a service account\n\nverifying, Verifying the permissions\n\nplatform binaries, verifying, Verifying Kubernetes Platform Binaries\n\nPod Security Admission, Setting Appropriate OS-Level Security Domains\n\npod-to-pod communication, restricting using network policies, Using Network Policies to Restrict Pod-to-Pod Communication-Allowing Fine- Grained Incoming Traffic\n\nPod-to-Pod encryption, with mTLS, Understanding Pod-to-Pod Encryption with mTLS\n\nPods\n\nbinding service account to, Binding the service account to a Pod\n\nenforcing security standards for namespaces, Enforcing Pod Security Standards for a Namespace\n\nPodSecurityContext API, Understanding Security Contexts\n\npolicies (OPA), Implementing an OPA Policy\n\nPortainer, Protecting GUI Elements\n\nports\n\nconfiguring for API server, Connecting to the API Server\n\ndisabling open, Identifying and Disabling Open Ports\n\nidentifying open, Identifying and Disabling Open Ports\n\nopen, Identifying and Disabling Open Ports\n\nport 10250, Protecting Node Metadata and Endpoints\n\nport 10257, Protecting Node Metadata and Endpoints\n\nport 10259, Protecting Node Metadata and Endpoints\n\nport 2379-2380, Protecting Node Metadata and Endpoints\n\nport 30000-32767, Protecting Node Metadata and Endpoints\n\nport 6443, Protecting Node Metadata and Endpoints\n\nPractical Cloud Native Security with Falco (Degioanni and Grasso), Understanding Falco\n\npractice exams, Practicing and Practice Exams\n\nprefix, Understanding Pod Security Admission (PSA)\n\nprimitives, Involved Kubernetes Primitives\n\nprivate keys, creating, Creating a private key\n\nprivileged containers, avoiding, Avoiding Privileged Containers\n\nprivileged level, Understanding Pod Security Admission (PSA)\n\nprofiles\n\nAppArmor, Understanding profiles\n\napplying to containers, Applying a profile to a container\n\ncustom, Setting a custom profile, Setting a custom profile, Applying the custom profile to a container\n\nPSA (Pod Security Admission), Understanding Pod Security Admission (PSA)\n\nPSP (Pod Security Policies), Understanding Pod Security Admission (PSA)\n\nPSP (PodSecurityPolicy) admission controller, Versioning Scheme\n\nPSS (Pod Security Standard), Understanding Pod Security Admission (PSA)\n\npublic image registries, Using Public Image Registries\n\nR\n\nRBAC (role-based access control), Cluster Hardening\n\nread-only container root filesystems, configuring, Configuring a Read-Only Container Root Filesystem\n\nreducing attack surface/number of payers, Reducing the Number of Layers\n\nreference manual, for exam, Documentation\n\nregistries, public image, Using Public Image Registries\n\nRego, Understanding Open Policy Agent (OPA) and Gatekeeper, Whitelisting Allowed Image Registries with OPA GateKeeper\n\nrelease cadence, Release Cadence\n\nrelease notes, Exam Objectives\n\nRequest audit level, Creating the Audit Policy File\n\nRequestResponse audit level, Creating the Audit Policy File\n\nrequests, processing, Processing a Request\n\nrestricted level, Understanding Pod Security Admission (PSA)\n\nrestricted privileges, creating users with, Creating a User with Restricted Privileges\n\nrestricting\n\naccess to API server, Restricting Access to the API Server-Creating a Secret for a service account\n\npod-to-pod communication using network policies, Using Network Policies to Restrict Pod-to-Pod Communication-Allowing Fine- Grained Incoming Traffic\n\nuser permissions, Restricting User Permissions-Scenario: An Attacker Can Call the API Server from a Service Account\n\nRice, Liz, Container Security, Securing the Supply Chain\n\nRoleBinding, creating, Creating a Role and a RoleBinding, Creating the RoleBinding\n\nroles\n\ncreating, Creating a Role and a RoleBinding\n\nidentity and access management (IAM), Minimizing IAM Roles- Changing file permissions\n\nrule argument, Creating the Ingress\n\nrules\n\nFalco, Rule\n\nfor firewalls, Setting Up Firewall Rules\n\noverriding existing, Overriding Existing Rules\n\nrules files (Falco), Understanding Falco Rule File Basics\n\nRUN command, Reducing the Number of Layers\n\nrunsc, Installing and Configuring gVisor\n\nruntime class, creating, Creating and Using a Runtime Class\n\nruntime security, Monitoring, Logging, and Runtime Security, Monitoring, Logging, and Runtime Security-Exam Essentials\n\n(see also behavior analytics)\n\nS\n\nsample exercises\n\nbehavior analytics, Exam Essentials\n\ncluster hardening, Sample Exercises\n\ncluster setup, Sample Exercises\n\nmicroservice vulnerabilities, Sample Exercises\n\nsupply chain security, Sample Exercises\n\nsystem hardening, Sample Exercises\n\nsandboxes, container runtime, Understanding Container Runtime Sandboxes-Creating and Using a Runtime Class\n\nscanning images, for known vulnerabilities, Scanning Images for Known Vulnerabilities\n\nscenarios\n\nadministrator can monitor malicious events in real time, Scenario: An Administrator Can Monitor Malicious Events in Real Time\n\nattacker can call API Server from Internet, Scenario: An Attacker Can Call the API Server from the Internet\n\nattacker can call API Server from Service Account, Scenario: An Attacker Can Call the API Server from a Service Account\n\nattacker exploits container vulnerabilities, Scenario: An Attacker Exploits Container Vulnerabilities\n\nattacker exploits package vulnerabilities, Scenario: An Attacker Exploits a Package Vulnerability\n\nattacker gains access to another container, Scenario: An Attacker Gains Access to Another Container\n\nattacker gains access to Dashboard functionality, Scenario: An Attacker Gains Access to the Dashboard Functionality\n\nattacker gains access to node running etcd, Scenario: An Attacker Gains Access to the Node Running etcd\n\nattacker gains access to pods, Scenario: Attacker Gains Access to a Pod\n\nattacker injects malicious code into binary, Scenario: An Attacker Injected Malicious Code into Binary\n\nattacker injects malicious code into container images, Scenario: An Attacker Injects Malicious Code into a Container Image\n\nattacker installs malicious software, Scenario: An Attacker Installs Malicious Software\n\nattacker listens to communication between two pods, Scenario: An Attacker Listens to the Communication Between Two Pods\n\nattacker misuses root user container access, Scenario: An Attacker Misuses root User Container Access\n\nattacker uploads malicious container images, Scenario: An Attacker Uploads a Malicious Container Image\n\nattacker uses credentials to gain file access, Scenario: An Attacker Uses Credentials to Gain File Access\n\ncompromised Pod accessing metadata servers, Scenario: A Compromised Pod Can Access the Metadata Server\n\ndeveloper doesn't follow pod security best practices, Scenario: A Developer Doesn’t Follow Pod Security Best Practices\n\nKubernetes administrator can observe actions taken by attackers, Scenario: A Kubernetes Administrator Can Observe Actions Taken by an Attacker\n\nseccomp, Involved External Tools, Using seccomp-Applying the custom profile to a container\n\nSecrets\n\nabout, Scenario: An Attacker Installs Malicious Software\n\nconfiguring containers with, Configuring a Container with a ConfigMap or Secret\n\ncreating for service accounts, Creating a Secret for a service account\n\nmanaging, Managing Secrets-Encrypting etcd Data\n\nsecurity\n\nenforcing standards for namespaces, Enforcing Pod Security Standards for a Namespace\n\nfixing issues with, Fixing Detected Security Issues\n\nof supply chain, Supply Chain Security\n\n(see also supply chain security)\n\nruntime, Monitoring, Logging, and Runtime Security\n\nsetting OS-level domains, Setting Appropriate OS-Level Security Domains-Implementing an OPA Policy\n\nsecurity contexts, Understanding Security Contexts\n\nSecurityContext API, Understanding Security Contexts\n\nSELinux (Security-Enhanced Linux), Using AppArmor\n\nsemantic versioning scheme, Versioning Scheme\n\nservice account\n\nbinding to Pods, Binding the service account to a Pod\n\ncreating Secrets for, Creating a Secret for a service account\n\ndisabling automounting for tokens, Disabling automounting of a service account token\n\nexpiration of token, Creating a User with Administration Privileges\n\ngenerating tokens, Generating a service account token\n\nminimizing permissions for, Minimizing Permissions for a Service Account-Creating a Secret for a service account\n\nServiceAccount, Creating a User with Administration Privileges\n\nservices, disabling, Disabling Services\n\nSHA, Verifying a Binary Against Hash\n\nSHA256, Verifying a Binary Against Hash, Validating Container Images\n\nsigning container images, Signing Container Images\n\nsize, of base images, Picking a Base Image Small in Size\n\nsnapd package manager, Scenario: An Attacker Exploits a Package Vulnerability\n\nss command, Identifying and Disabling Open Ports\n\nstatic analysis, of workload, Static Analysis of Workload-Using Kubesec for Analyzing Kubernetes Manifests\n\nstatus command, Disabling Services\n\nStudy4exam, Practicing and Practice Exams\n\nsu command, Switching to a user\n\nsudo command, Switching to a user\n\nsudo systemctl restart kubelet command, Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nsupply chain security, Supply Chain Security-Sample Exercises\n\nabout, Supply Chain Security, Supply Chain Security\n\nanswers to review questions, Chapter 6, “Supply Chain Security”- Chapter 6, “Supply Chain Security”\n\nminimizing base image footprint, Minimizing the Base Image Footprint-Using Container Image Optimization Tools\n\nsample exercises, Sample Exercises\n\nscanning images for known vulnerabilities, Scanning Images for Known Vulnerabilities\n\nsecuring supply chain, Securing the Supply Chain-Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nstatic analysis of workload, Static Analysis of Workload-Using Kubesec for Analyzing Kubernetes Manifests\n\nsysctl command, Avoiding Privileged Containers\n\nsystem hardening, System Hardening-Sample Exercises\n\nabout, System Hardening, System Hardening\n\nanswers to review questions, Chapter 4, “System Hardening”-Chapter 4, “System Hardening”\n\nminimizing external access to networks, Minimizing External Access to the Network\n\nminimizing host OS footprint, Minimizing the Host OS Footprint- Removing Unwanted Packages\n\nminimizing IAM roles, Minimizing IAM Roles-Changing file permissions\n\nsample exercises, Sample Exercises\n\nusing kernel hardening tools, Using Kernel Hardening Tools-Applying the custom profile to a container\n\nsystemctl command, Disabling Services\n\nsystemctl status command, Identifying and Disabling Open Ports\n\nT\n\nTetragon, Scenario: A Kubernetes Administrator Can Observe Actions Taken by an Attacker\n\nTLS (Transport Layer Security)\n\ncreating an ingress with termination, Creating an Ingress with TLS Termination-Calling the Ingress\n\ncreating TLS certificate and key, Creating the TLS Certificate and Key\n\ntermination of, Cluster Setup\n\nTLS Secret, Creating the TLS-Typed Secret\n\nTLS-typed Secret, creating, Creating the TLS-Typed Secret\n\ntokens, service account, Disabling automounting of a service account token\n\ntools\n\nexternal, Involved External Tools\n\nkernel hardening, Using Kernel Hardening Tools-Applying the custom profile to a container\n\noptimization, Using Container Image Optimization Tools\n\ntouch command, Viewing file permissions and ownership\n\nTracee, Scenario: A Kubernetes Administrator Can Observe Actions Taken by an Attacker\n\nTracey, Craig, Managing Kubernetes, Interacting with the Kubernetes API\n\nTrivy, Supply Chain Security, Involved External Tools, Scanning Images for Known Vulnerabilities\n\nTTL (time to live), Creating a User with Administration Privileges\n\nU\n\nUbuntu Linux, CIS benchmark for, Minimizing the Host OS Footprint\n\nUFW (Uncomplicated Firewall), Setting Up Firewall Rules\n\nupdating Kubernetes, Updating Kubernetes Frequently-Summary\n\nuser ID, setting, Setting a Specific User and Group ID\n\nuser management, Understanding User Management\n\nuser permissions, restricting, Restricting User Permissions-Scenario: An Attacker Can Call the API Server from a Service Account\n\nusermod command, Deleting a group\n\nusers\n\nadding, Adding a user\n\nadding to groups, Assigning a user to a group\n\nadding to kubeconfig file, Adding the user to the kubeconfig file\n\ncreating with administration privileges, Creating a User with Administration Privileges\n\ncreating with restricted privileges, Creating a User with Restricted Privileges\n\ndeleting, Deleting a user\n\nlisting, Listing users\n\nswitching to, Switching to a user\n\nV\n\nVagrant, Practicing and Practice Exams\n\nvalidating\n\nas stage in request processing, Processing a Request\n\ncontainer images, Validating Container Images\n\nverifying\n\nbinaries against hash, Verifying a Binary Against Hash\n\ndefault permissions, Verifying the default permissions\n\ngranted permissions, Verifying the granted permissions\n\nKubernetes platform binaries, Verifying Kubernetes Platform Binaries\n\npermissions, Verifying the permissions\n\nplatform binaries, Verifying Kubernetes Platform Binaries\n\nversioning scheme, Versioning Scheme\n\nVirtualBox, Practicing and Practice Exams\n\nVPN (Virtual Private Network), Adopting mTLS in Kubernetes\n\nvulnerabilities, scanning images for known, Scanning Images for Known Vulnerabilities\n\nW\n\nwarn mode, Understanding Pod Security Admission (PSA)\n\nwebhook backend, Understanding Audit Logs, Configuring a Webhook Backend\n\nwget command, Denying Directional Network Traffic\n\nwhitelisting\n\nallowed image registries with ImagePolicyWebhook Admission Controller plugin, Whitelisting Allowed Image Registries with the ImagePolicyWebhook Admission Controller Plugin\n\nallowed image registries with OPA Gatekeeper, Whitelisting Allowed Image Registries with OPA GateKeeper-Whitelisting Allowed Image Registries with OPA GateKeeper\n\nWireGuard, Adopting mTLS in Kubernetes\n\nworkload, static analysis of, Static Analysis of Workload-Using Kubesec for Analyzing Kubernetes Manifests\n\nY\n\nYAML manifest, Observing the Default Behavior\n\nOceanofPDF.com\n\nAbout the Author\n\nBenjamin Muschko is a software engineer, consultant, and trainer with more than 20 years of experience in the industry. He’s passionate about project automation, testing, and continuous delivery. Ben is an author, a frequent speaker at conferences, and an avid open source advocate. He holds the CKAD, CKA, and CKS certifications and is a CNCF Ambassador Spring 2023.\n\nSoftware projects sometimes feel like climbing a mountain. In his free time, Ben loves hiking Colorado’s 14ers and enjoys conquering long-distance trails.\n\nOceanofPDF.com\n\nColophon\n\nThe animal on the cover of Certified Kubernetes Security Specialist (CKS) Study Guide is a domestic goose. These birds have been selectively bred from wild greylag (Anser anse) and swan geese (Anser cygnoides domesticus). They have been introduced to every continent except Antarctica. Archaeological evidence shows the geese have been domesticated since at least 4,000 years ago.\n\nWild geese range in size from 7 to 9 pounds, whereas domestic geese have been bred for size and can weigh up to 22 pounds. The distribution of their fat deposits gives the domestic goose a more upright posture compared to the horizontal posture of their wild ancestors. Their larger size also makes them less likely to fly, although the birds are capable of some flight.\n\nHistorically, geese have been domesticated for use of their meat, eggs, and feathers. In more recent times, geese have been kept as backyard pets or even for yard maintenance since they eat weeds and leaves. Due to the loud and aggressive nature of geese, they have also been used to safeguard property, since they will make a lot of noise if they perceive a threat or an intruder.\n\nDomestic animals are not assessed by the IUCN. Many of the animals on O’Reilly covers are endangered; all of them are important to the world.\n\nThe cover illustration is by Karen Montgomery. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n\nOceanofPDF.com",
      "page_number": 188
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Certified Kubernetes Security Specialist (CKS) Study Guide In-Depth Guidance and Practice\n\nBenjamin Muschko\n\nOceanofPDF.com",
      "content_length": 123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Certified Kubernetes Security Specialist (CKS) Study Guide\n\nby Benjamin Muschko\n\nCopyright © 2023 Automated Ascent, LLC. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (https://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nAcquisitions Editor: John Devins\n\nDevelopment Editor: Michele Cronin\n\nProduction Editor: Beth Kelly\n\nCopyeditor: Liz Wheeler\n\nProofreader: Amnet Systems, LLC\n\nIndexer: Potomac Indexing, LLC\n\nInterior Designer: David Futato\n\nCover Designer: Karen Montgomery\n\nIllustrator: Kate Dullea\n\nJune 2023: First Edition\n\nRevision History for the First Edition",
      "content_length": 898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "2023-06-08: First Release\n\nSee https://oreilly.com/catalog/errata.csp?isbn=9781098132972 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Certified Kubernetes Security Specialist (CKS) Study Guide, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nThe views expressed in this work are those of the author, and do not represent the publisher’s views. While the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-098-13297-2\n\n[LSI]\n\nOceanofPDF.com",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Preface\n\nThe Kubernetes certification program has been around since 2018, or five years as of this writing. During this time, security has become more and more important everywhere, including the Kubernetes world. Recently, the role of Certified Kubernetes Security Specialist (CKS) has been added to the certification track to address the need. Security can have different facets, and the way you address those concerns can be very diverse. That’s where the Kubernetes ecosystem comes into play. Apart from Kubernetes built-in security features, many tools have evolved that help with identifying and fixing security risks. As a Kubernetes administrator, you need to be familiar with the wide range of concepts and tools to harden your clusters and applications.\n\nThe CKS certification program was created to verify competence on security-based topics, and it requires a successful pass of the Certified Kubernetes Administrator (CKA) exam before you can register. If you are completely new to the Kubernetes certification program, then I would recommend exploring the CKA or Certified Kubernetes Application Developer (CKAD) program first.\n\nIn this study guide, I will explore the topics covered in the CKS exam to fully prepare you to pass the certification exam. We’ll look at determining when and how you should apply the core concepts of Kubernetes and external tooling to secure cluster components, cluster configuration, and applications running in a Pod. I will also offer tips to help you better prepare for the exam and share my personal experience with getting ready for all aspects of it.\n\nThe CKS is different from the typical multiple-choice format of other certifications. It’s completely performance based and requires you to demonstrate deep knowledge of the tasks at hand under immense time pressure. Are you ready to pass the test on the first go?",
      "content_length": 1867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Who This Book Is For\n\nThis book is for anyone who already passed the CKA exam and wants to broaden their knowledge in the realm of security. Given that you need to pass the CKA exam before signing up for the CKS, you should already be familiar with the format of the exam questions and environment. Chapter 1 only briefly recaps the general aspects of the exam curriculum, but it highlights the information specific to the CKS exam. If you have not taken the CKA exam yet, I recommend taking a step by reading the Certified Kubernetes Administrator (CKA) Study Guide (O’Reilly). The book will provide you with the foundation you need to get started with the CKS.\n\nWhat You Will Learn\n\nThe content of the book condenses the most important aspects relevant to the CKS exam. Cloud-provider-specific Kubernetes implementations like AKS or GKE do not need to be considered. Given the plethora of configuration options available in Kubernetes, it’s almost impossible to cover all use cases and scenarios without duplicating the official documentation. Test takers are encouraged to reference the Kubernetes documentation as the go-to compendium for broader exposure. External tools relevant to the CKS exam, such as Trivy or Falco, are only covered on a high level. Refer to their documentation to explore more features, functionality, and configuration options.\n\nStructure of This Book\n\nThe outline of the book follows the CKS curriculum to a T. While there might be a more natural, didactical structure for learning Kubernetes in general, the curriculum outline will help test takers prepare for the exam by focusing on specific topics. As a result, you will find yourself cross- referencing other chapters of the book depending on your existing knowledge level.",
      "content_length": 1758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Be aware that this book covers only the concepts relevant to the CKS exam. Foundational Kubernetes concepts and primitives are not discussed. Refer to the Kubernetes documentation or other books if you want to dive deeper.\n\nPractical experience with Kubernetes is key to passing the exam. Each chapter contains a section named “Sample Exercises” with practice questions. Solutions to those questions can be found in the Appendix.\n\nConventions Used in This Book\n\nThe following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, and email addresses.\n\nConstant width\n\nUsed for filenames, file extensions, and program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nTIP\n\nThis element signifies a tip or suggestion.\n\nNOTE\n\nThis element signifies a general note.",
      "content_length": 1014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "WARNING\n\nThis element indicates a warning or caution.\n\nUsing Code Examples Some code snippets in the book use the backslash character (\\) to break up a single line into multiple lines to make it fit the page. You will need to rectify the code manually if you are copy-pasting it directly from the book content to a terminal or editor. The better choice is to refer to the code book’s GitHub repository, which already has the proper formatting.\n\nThe GitHub repository is distributed under the Apache License 2.0. The code is free to use in commercial and open source projects. If you encounter an issue in the source code or if you have a question, open an issue in the GitHub issue tracker. I’ll be happy to have a conversation and fix any issues that might arise.\n\nThis book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require permission. We appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “Certified Kubernetes Security Specialist (CKS) Study Guide by Benjamin Muschko (O’Reilly). Copyright 2023 Automated Ascent, LLC, 978-1-098-13297-2.”\n\nIf you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.",
      "content_length": 1870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "O’Reilly Online Learning\n\nNOTE\n\nFor more than 40 years, O’Reilly Media has provided technology and business training, knowledge, and insight to help companies succeed.\n\nOur unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\n\nHow to Contact Us\n\nPlease address comments and questions concerning this book to the publisher:\n\nO’Reilly Media, Inc.\n\n1005 Gravenstein Highway North\n\nSebastopol, CA 95472\n\n800-889-8969 (in the United States or Canada)\n\n707-829-7019 (international or local)\n\n707-829-0104 (fax)\n\nsupport@oreilly.com",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "https://www.oreilly.com/about/contact.html\n\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/cks- study-guide.\n\nFor news and information about our books and courses, visit http://oreilly.com.\n\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media\n\nFollow us on Twitter: http://twitter.com/oreillymedia\n\nWatch us on YouTube: http://youtube.com/oreillymedia\n\nFollow the author on Twitter: https://twitter.com/bmuschko\n\nFollow the author on GitHub: https://github.com/bmuschko\n\nFollow the author’s blog: https://bmuschko.com\n\nAcknowledgments\n\nEvery book project is a long journey and would not be possible without the help of the editorial staff and technical reviewers. Special thanks go to Robin Smorenburg, Werner Dijkerman, Michael Kehoe, and Liz Rice for their detailed technical guidance and feedback. I would also like to thank the editors at O’Reilly Media, John Devins and Michele Cronin, for their continued support and encouragement.\n\nOceanofPDF.com",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Chapter 1. Exam Details and Resources\n\nThis introductory chapter addresses the most pressing questions candidates ask when preparing for the Certified Kubernetes Security Specialist (CKS) exam. We will discuss the target audience for the certification, the curriculum, and the exam environment, as well as tips and tricks and additional learning resources. If you’re already familiar with the certification program, you can directly jump to any of the chapters covering the technical concepts.\n\nKubernetes Certification Learning Path\n\nThe CNCF offers four different Kubernetes certifications. Figure 1-1 categorizes each of them by target audience. You will find that the CKS is the most advanced certification you can acquire. It is the only one with a prerequisite of passing another certification first; all others are standalone programs.\n\nFigure 1-1. Kubernetes certifications learning path",
      "content_length": 895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Let’s have a very brief look at the details for each certification to see if the CKS is the right fit for you.\n\nKubernetes and Cloud Native Associate (KCNA)\n\nKCNA is an entry-level certification program for anyone interested in cloud-native application development, runtime environments, and tooling. While the exam does cover Kubernetes, it does not expect you to actually solve problems in a practical manner. This exam is suitable for candidates interested in the topic with a broad exposure to the ecosystem.\n\nKubernetes and Cloud Native Security Associate (KCSA)\n\nThe certification focuses on basic knowledge of security concepts and their application in a Kubernetes cluster. The breadth and depth of the program is comparable to the KCNA, as it does not require solving problems hands-on.\n\nCertified Kubernetes Application Developer (CKAD)\n\nThe CKAD exam focuses on verifying your ability to build, configure, and deploy a microservices-based application to Kubernetes. You are not expected to actually implement an application; however, the exam is suitable for developers familiar with topics like application architecture, runtimes, and programming languages.\n\nCertified Kubernetes Administrator (CKA)\n\nThe target audience for the CKA exam are DevOps practitioners, system administrators, and site reliability engineers. This exam tests your ability to perform in the role of a Kubernetes administrator, which includes tasks like cluster, network, storage, and beginner-level security management, with a big emphasis on troubleshooting scenarios.\n\nCertified Kubernetes Security Specialist (CKS)",
      "content_length": 1604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "The CKS exam expands on the topics verified by the CKA exam. Passing the CKA is a prerequisite before you can even sign up for the CKS exam. For this certification, you are expected to have a deeper knowledge of Kubernetes security aspects. The curriculum covers topics like applying best practices for building containerized applications and ensuring a secure Kubernetes runtime environment.\n\nExam Objectives\n\nVulnerabilities in software and IT infrastructure, if exploited, can pose a major threat to organizations. The Cloud Native Computing Foundation (CNCF) developed the Certified Kubernetes Security Specialist (CKS) certification to verify a Kubernetes administrator’s proficiency to protect a Kubernetes cluster and the cloud native software operated in it. As part of the CKS exam, you are expected to understand Kubernetes core security features, as well as third-party tools and established practices for securing applications and infrastructure.\n\nKUBERNETES VERSION USED DURING THE EXAM\n\nAt the time of writing, the exam is based on Kubernetes 1.26. All content in this book will follow the features, APIs, and command-line support for that specific version. It’s certainly possible that future versions will break backward compatibility. While preparing for the certification, review the Kubernetes release notes and practice with the Kubernetes version used during the exam to avoid unpleasant surprises.\n\nIn this book, I am going to explain each of the security threats by providing a specific use case. We’ll start by talking about a scenario that allows an attacker to gain access to a cluster, inject malicious code, or use a vulnerability to hack into the system. Then, we’ll touch on the concepts, practices, and/or tools that will prevent that situation. With this approach, you’ll be able to evaluate the severity of a security risk and the need for implementing security measures.",
      "content_length": 1904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Curriculum\n\nThe following overview lists the high-level sections, also called domains, of the CKS exam and their scoring weights:\n\n10%: Cluster Setup\n\n15%: Cluster Hardening\n\n15%: System Hardening\n\n20%: Minimize Microservice Vulnerabilities\n\n20%: Supply Chain Security\n\n20%: Monitoring, Logging, and Runtime Security",
      "content_length": 316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "HOW THE BOOK WORKS\n\nThe outline of the book follows the CKS curriculum to a T. While there might be a more natural, didactical organization structure to learn Kubernetes in general, the curriculum outline will help test takers prepare for the exam by focusing on specific topics. As a result, you will find yourself cross-referencing other chapters of the book depending on your existing knowledge level.\n\nLet’s break down each domain in detail in the next sections.\n\nCluster Setup\n\nThis section covers Kubernetes concepts that have already been covered by the CKA exam; however, they assume that you already understand the basics and expect you to be able to go deeper. Here, you will be tested on network policies and their effects on disallowing and granting network communication between Pods within the same namespace and across multiple namespaces. The main focus will be on restricting communication to minimize the attack surface. Furthermore, the domain “cluster setup” will verify your knowledge of setting up an Ingress object with Transport Layer Security (TLS) termination.\n\nA big emphasis lies on identifying and fixing security vulnerabilities by inspecting the cluster setup. External tools like kube-bench can help with automating the process. As a result of executing the tool against your cluster, you will receive an actionable list of vulnerabilities. Changing the configuration settings of your cluster according to the recommendations can help with significantly reducing the security risk.\n\nLast, locking down cluster node endpoints, ports, and graphical user interfaces (GUIs) can help with making it harder for attackers to gain control of the cluster. You need to be aware of the default cluster settings so that you can limit access to them as much as possible. Kubernetes binaries and executables like kubectl, kubeadm, and the kubelet need to be checked against their checksum to ensure they haven’t been tampered with by a third",
      "content_length": 1959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "party. You need to understand how to retrieve the checksum file for binaries and how to use it verify the validity of the executable.\n\nCluster Hardening\n\nMost organizations start out with a cluster that allows developers and administrators alike to manage the Kubernetes installation, configuration, and management of any objects. While this is a convenient approach for teams getting comfortable with Kubernetes, it is not a safe and sound situation, as it poses the potential of opening the floodgates for attackers. Once access has been gained to the cluster, any malicious operation can be performed.\n\nRole-based access control (RBAC) maps permissions to users or processes. The exam requires deep knowledge of the API resources involved. The domain “cluster hardening” also focuses the topic of keeping the cluster version up-to-date to ensure the latest bug fixes are picked up. Kubernetes exposes the API server via endpoints. You should be aware of strategies for minimizing its exposure to the outside world.\n\nSystem Hardening\n\nThis domain is all about understanding how to minimize access to the host system and external network to reduce the attack surface. This is where OS- level tools like AppArmor and seccomp come into play. You will need to demonstrate their use to fulfill the requirement. The domain also touches on the use of AWS IAM roles for clusters running in Amazon’s cloud environment specifically.\n\nMinimize Microservice Vulnerabilities\n\nSecurity contexts define privilege and access control for containers. Platform and security teams can govern and enforce desired security measures on the organizational level. The exam requires you to understand Pod security policies and the OPA Gatekeeper for that purpose. Moreover,",
      "content_length": 1749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "you’ll be asked to demonstrate defining Secrets of different types and consuming them from Pods to inject sensitive runtime information.\n\nSometimes, you may want to experiment with container images from an unverified source or a potentially unsafe origin. Container runtime sandboxes like gVisor and Kata Containers can be configured in Kubernetes to execute a container image with very restricted permissions. Configuring and using such a container runtime sandbox is part of this domain. Further, you need to be aware of the benefits of mTLS Pod-to-Pod encryption and how to configure it.\n\nSupply Chain Security\n\nContainer security starts with the base image. You need to be aware of the best practices for building container images that minimize the risk of introducing security vulnerabilities from the get-go. Optimally, you will only allow pulling trusted container images from an organization-internal container registry that has already scanned the image for vulnerabilities before it can be used. Allowing only those registries is paramount and will be one of the topics important to this domain. Tools like Trivy can help with the task of scanning images for vulnerabilities and are listed as a requirement to pass the exam.\n\nMonitoring, Logging, and Runtime Security\n\nOne of the focus points of this domain is behavior analytics, the process of observing abnormal and malicious events. Falco is the primary tool to get familiar with in this section. A container should not be mutable after it has been started to avoid opening additional backdoors for attackers. You will need to be aware of best practices and demonstrate the ability to apply them in the configuration of a container.\n\nAudit logging can be helpful for a real-time view on cluster events or for debugging purposes. Configuring audit logging for a Kubernetes cluster is part of the exam.",
      "content_length": 1864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Involved Kubernetes Primitives\n\nSome of the exam objectives can be covered by understanding the relevant core Kubernetes primitives. It is to be expected that the exam combines multiple concepts in a single problem. Refer to Figure 1-2 as a rough guide to the applicable Kubernetes resources and their relationships.\n\nFigure 1-2. Kubernetes primitives relevant to the exam\n\nIn addition to Kubernetes core primitives, you will also need to have a grasp of specific Custom Resource Definitions (CRDs) provided by open source projects. For example, the Open Policy Agent (OPA) Gatekeeper provides the primitives’ ConstraintTemplate.",
      "content_length": 629,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Involved External Tools\n\nA significant portion of the exam requires you to demonstrate expertise with external, security-related tools. Some of the tools have been spelled out explicitly in the curriculum, but there are other tools that fall into the same functional category. At the very least, you will have to be familiar with the following tools:\n\nkube-bench\n\nAppArmor\n\nseccomp\n\ngVisor\n\nKata Containers\n\nTrivy\n\nFalco\n\nDocumentation\n\nDuring the exam, you are permitted to open a well-defined list of web pages as a reference. You can freely browse those pages and copy-paste code in the exam terminal. The Frequently Asked Questions (FAQ) for the CKS spells out a list of permitted URLs.\n\nThe official Kubernetes documentation includes the reference manual, the GitHub site, and the blog:\n\nReference manual: https://kubernetes.io/docs\n\nGitHub: https://github.com/kubernetes\n\nBlog: https://kubernetes.io/blog\n\nFor external tools, you are allowed to open and browse the following URL:",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Trivy: https://github.com/aquasecurity/trivy\n\nFalco: https://falco.org/docs\n\nAppArmor: https://gitlab.com/apparmor/apparmor/-/wikis/Documentation\n\nCandidate Skills\n\nThe CKS certification assumes that you already have an administrator-level understanding of Kubernetes. The CNCF requires you to acquire the CKA certificate as a prerequisite. Without those credentials, you won’t be able to sign up for the CKS exam. If you have not passed the CKA exam yet or if you want to brush up on the topics, I’d recommend having a look at my book Certified Kubernetes Administrator (CKA) Study Guide.\n\nFor the remainder of the book, I will simply assume that you already have the knowledge needed for the CKA. Therefore, I won’t repeat the basics on overlapping topics anymore. For convenience reasons, I will point you to the relevant information in the CKA book as needed. Please revisit the sections on the exam environment and time management in the CKA book. They equally apply to the CKS exam.\n\nPracticing and Practice Exams\n\nHands-on practice is extremely important when it comes to passing the exam. For that purpose, you’ll need a functioning Kubernetes cluster environment. The following options stand out:\n\nI found it useful to run one or many virtual machines using Vagrant and VirtualBox. Those tools help with creating an isolated Kubernetes environment that is easy to bootstrap and dispose on demand. Some of the practice exercises in this book use this setup as their starting point.",
      "content_length": 1489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "It is relatively easy to install a simple Kubernetes cluster on your developer machine. The Kubernetes documentation provides various installation options, depending on your operating system. Minikube is useful when it comes to experimenting with more advanced features like Ingress or storage classes, as it provides the necessary functionality as add-ons that can be installed with a single command.\n\nIf you’re a subscriber to the O’Reilly learning platform, you have unlimited access to labs running a Kubernetes environment. In addition, you can test your knowledge with the help of the CKS practice test in the form of interactive labs.\n\nYou may also want to try one of the following commercial learning and practice resources:\n\nThe book Certified Kubernetes Administrator (CKA) Study Guide covers the curriculum of the CKA certification. Revisit the book’s materials for a refresher on the foundations.\n\nKiller Shell is a simulator with sample exercises for all Kubernetes certifications.\n\nThe CKS practice exam from Study4exam offers a commercial, web-based test environment to assess your knowledge level.\n\nSummary\n\nThe CKS exam verifies your hands-on knowledge of security-related aspects in Kubernetes. You are expected to understand core Kubernetes primitives and concepts that can fulfill security requirements, such as RBAC, network policies, and Ingress. The exam also involves helpful third- party security tools. You need to demonstrate how to effectively use those tools. Passing the CKA exam is mandatory for the CKS. Make sure you pass the CKA first if you haven’t done so yet.",
      "content_length": 1596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "The following chapters align with the exam curriculum so that you can map the content to the learning objectives. At the end of each chapter, you will find sample exercises to practice your knowledge. The discussion of each domain concludes with a short summary of the most important aspects to learn.\n\nOceanofPDF.com",
      "content_length": 317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Chapter 2. Cluster Setup\n\nThe first domain of the exam deals with concerns related to Kubernetes cluster setup and configuration. In this chapter, we’ll only drill into the security-specific aspects and not the standard responsibilities of a Kubernetes administrator.\n\nAt a high level, this chapter covers the following concepts:\n\nUsing network policies to restrict Pod-to-Pod communication\n\nRunning CIS benchmark tooling to identify security risks for cluster components\n\nSetting up an Ingress object with TLS support\n\nProtecting node ports, API endpoints, and GUI access\n\nVerifying platform binaries against their checksums\n\nUsing Network Policies to Restrict Pod-to- Pod Communication\n\nFor a microservice architecture to function in Kubernetes, a Pod needs to be able to reach another Pod running on the same or on a different node without Network Address Translation (NAT). Kubernetes assigns a unique IP address to every Pod upon creation from the Pod CIDR range of its node. The IP address is ephemeral and therefore cannot be considered stable over time. Every restart of a Pod leases a new IP address. It’s recommended to use Pod-to-Service communication over Pod-to-Pod communication so that you can rely on a consistent network interface.\n\nThe IP address assigned to a Pod is unique across all nodes and namespaces. This is achieved by assigning a dedicated subnet to each node when registering it. When creating a new Pod on a node, the IP address is",
      "content_length": 1461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "leased from the assigned subnet. This is handled by the Container Network Interface (CNI) plugin. As a result, Pods on a node can communicate with all other Pods running on any other node of the cluster.\n\nNetwork policies act similarly to firewall rules, but for Pod-to-Pod communication. Rules can include the direction of network traffic (ingress and/or egress) for one or many Pods within a namespace or across different namespaces, as well as their targeted ports. For a deep-dive coverage on the basics of network policies, refer to the book Certified Kubernetes Application Developer (CKAD) Study Guide (O’Reilly) or the Kubernetes documentation. The CKS exam primarily focuses on restricting cluster- level access with network policies.\n\nDefining the rules of network policies correctly can be challenging. The page networkpolicy.io provides a visual editor for network policies that renders a graphical representation in the browser.\n\nScenario: Attacker Gains Access to a Pod\n\nSay you are working for a company that operates a Kubernetes cluster with three worker nodes. Worker node 1 currently runs two Pods as part of a microservices architecture. Given Kubernetes default behavior for Pod-to- Pod network communication, Pod 1 can talk to Pod 2 unrestrictedly and vice versa.\n\nAs you can see in Figure 2-1, an attacker gained access to Pod 1. Without defining network policies, the attacker can simply talk to Pod 2 and cause additional damage. This vulnerability isn’t restricted to a single namespace. Pods 3 and 4 can be reached and compromised as well.",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Figure 2-1. An attacker who gained access to Pod 1 has network access to other Pods\n\nObserving the Default Behavior\n\nWe’ll set up three Pods to demonstrate the unrestricted Pod-to-Pod network communication in practice. As you can see in Example 2-1, the YAML manifest defines the Pods named backend and frontend in the namespace g04. The other Pod lives in the default namespace. Observe the label assignment for the namespace and Pods. We will reference them a little bit later in this chapter when defining network policies.\n\nExample 2-1. YAML manifest for three Pods in different namespaces apiVersion: v1 kind: Namespace metadata: labels: app: orion name: g04 --- apiVersion: v1 kind: Pod metadata: labels:",
      "content_length": 710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "tier: backend name: backend namespace: g04 spec: containers: - image: bmuschko/nodejs-hello-world:1.0.0 name: hello ports: - containerPort: 3000 restartPolicy: Never --- apiVersion: v1 kind: Pod metadata: labels: tier: frontend name: frontend namespace: g04 spec: containers: - image: alpine name: frontend args: - /bin/sh - -c - while true; do sleep 5; done; restartPolicy: Never --- apiVersion: v1 kind: Pod metadata: labels: tier: outside name: other spec: containers: - image: alpine name: other args: - /bin/sh - -c - while true; do sleep 5; done; restartPolicy: Never\n\nStart by creating the objects from the existing YAML manifest using the declarative kubectl apply command:",
      "content_length": 681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "$ kubectl apply -f setup.yaml namespace/g04 created pod/backend created pod/frontend created pod/other created\n\nLet’s verify that the namespace g04 runs the correct Pods. Use the -o wide CLI option to determine the virtual IP addresses assigned to the Pods. The backend Pod uses the IP address 10.0.0.43, and the frontend Pod uses the IP address 10.0.0.193:\n\n$ kubectl get pods -n g04 -o wide NAME READY STATUS RESTARTS AGE IP NODE \\ NOMINATED NODE READINESS GATES backend 1/1 Running 0 15s 10.0.0.43 minikube \\ <none> <none> frontend 1/1 Running 0 15s 10.0.0.193 minikube \\ <none> <none>\n\nThe default namespace handles a single Pod:\n\n$ kubectl get pods NAME READY STATUS RESTARTS AGE other 1/1 Running 0 4h45m\n\nThe frontend Pod can talk to the backend Pod as no communication restrictions have been put in place:\n\n$ kubectl exec frontend -it -n g04 -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000) remote file exists / # exit\n\nThe other Pod residing in the default namespace can communicate with the backend Pod without problems:\n\n$ kubectl exec other -it -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000)",
      "content_length": 1211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "remote file exists / # exit\n\nIn the next section, we’ll talk about restricting Pod-to-Pod network communication to a maximum level with the help of deny-all network policy rules. We’ll then open up ingress and/or egress communication only for the kind of network communication required for the microservices architecture to function properly.\n\nDenying Directional Network Traffic\n\nThe best way to restrict Pod-to-Pod network traffic is with the principle of least privilege. Least privilege means that Pods should communicate with the lowest privilege for network communication. You’d usually start by disallowing traffic in any direction and then opening up the traffic needed by the application architecture.\n\nThe Kubernetes documentation provides a couple of helpful YAML manifest examples. Example 2-2 shows a network policy that denies ingress traffic to all Pods in the namespace g04.\n\nExample 2-2. A default deny-all ingress network policy apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress namespace: g04 spec: podSelector: {} policyTypes: - Ingress\n\nSelecting all Pods is denoted by the value {} assigned to the spec.podSelector attribute. The value attribute spec.policyTypes defines the denied direction of traffic. For incoming traffic, you can add Ingress to the array. Outgoing traffic can be specified by the value Egress. In this particular example, we disallow all ingress traffic. Egress traffic is still permitted.",
      "content_length": 1472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "The contents of the “deny-all” network policy have been saved in the file deny-all-ingress-network-policy.yaml. The following command creates the object from the file:\n\n$ kubectl apply -f deny-all-ingress-network-policy.yaml networkpolicy.networking.k8s.io/default-deny-ingress created\n\nLet’s see how this changed the runtime behavior for Pod-to-Pod network communication. The frontend Pod cannot talk to the backend Pod anymore, as observed by running the same wget command we used earlier. The network call times out after one second, as defined by the CLI option - -timeout:\n\n$ kubectl exec frontend -it -n g04 -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000) wget: download timed out / # exit\n\nFurthermore, Pods running in a different namespace cannot connect to the backend Pod anymore either. The following wget command makes a call from the other Pod running in the default namespace to the IP address of the backend Pod:\n\n$ kubectl exec other -it -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000) wget: download timed out\n\nThis call times out as well.\n\nAllowing Fine-Grained Incoming Traffic\n\nNetwork policies are additive. To grant more permissions for network communication, simply create another network policy with more fine- grained rules. Say we wanted to allow ingress traffic to the backend Pod only from the frontend Pod that lives in the same namespace. Ingress",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "traffic from all other Pods should be denied independently of the namespace they are running in.\n\nNetwork policies heavily work with label selection to define rules. Identify the labels of the g04 namespace and the Pod objects running in the same namespace so we can use them in the network policy:\n\n$ kubectl get ns g04 --show-labels NAME STATUS AGE LABELS g04 Active 12m app=orion,kubernetes.io/metadata.name=g04 $ kubectl get pods -n g04 --show-labels NAME READY STATUS RESTARTS AGE LABELS backend 1/1 Running 0 9m46s tier=backend frontend 1/1 Running 0 9m46s tier=frontend\n\nThe label assignment for the namespace g04 includes the key-value pair app=orion. The Pod backend label set includes the key-value pair tier=backend, and the frontend Pod the key-value pair tier=frontend.\n\nCreate a new network policy that allows the frontend Pod to talk to the backend Pod only on port 3000. No other communication should be allowed. The YAML manifest representation in Example 2-3 shows the full network policy definition.\n\nExample 2-3. Network policy that allows ingress traffic apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: backend-ingress namespace: g04 spec: podSelector: matchLabels: tier: backend policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: app: orion podSelector:",
      "content_length": 1319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "matchLabels: tier: frontend ports: - protocol: TCP port: 3000\n\nThe definition of the network policy has been stored in the file backend- ingress-network-policy.yaml. Create the object from the file:\n\n$ kubectl apply -f backend-ingress-network-policy.yaml networkpolicy.networking.k8s.io/backend-ingress created\n\nThe frontend Pod can now talk to the backend Pod:\n\n$ kubectl exec frontend -it -n g04 -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000) remote file exists / # exit\n\nPods running outside of the g04 namespace still can’t connect to the backend Pod. The wget command times out:\n\n$ kubectl exec other -it -- /bin/sh / # wget --spider --timeout=1 10.0.0.43:3000 Connecting to 10.0.0.43:3000 (10.0.0.43:3000) wget: download timed out\n\nApplying Kubernetes Component Security Best Practices\n\nManaging an on-premises Kubernetes cluster gives you full control over the configuration options applied to cluster components, such as the API server, etcd, the kubelet, and others. It’s not uncommon to simply go with the default configuration settings used by kubeadm when creating the cluster nodes. Some of those default settings may expose cluster components to unnecessary attack opportunities.",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Hardening the security measures of a cluster is a crucial activity for any Kubernetes administrator seeking to minimize attack vectors. You can either perform this activity manually if you are aware of the best practices, or use an automated process.\n\nThe Center for Internet Security (CIS) is a not-for-profit organization that publishes cybersecurity best practices. Part of their best practices portfolio is the Kubernetes CIS Benchmark, a catalog of best practices for Kubernetes environments. You will find a detailed list of recommended security settings for cluster components on their web page.\n\nCIS BENCHMARKING FOR CLOUD PROVIDER KUBERNETES ENVIRONMENTS\n\nThe Kubernetes CIS Benchmark is geared toward a self-managed installation of Kubernetes. Cloud provider Kubernetes environments, such as Amazon Elastic Kubernetes Service (EKS) and Google Kubernetes Engine (GKE), provide a managed control plane accompanied by their own command line tools. Therefore, the security recommendations made by the Kubernetes CIS Benchmark may be less fitting. Some tools, like kube-bench, discussed next, provide verification checks specifically for cloud providers.\n\nUsing kube-bench\n\nYou can use the tool kube-bench to check Kubernetes cluster components against the CIS Benchmark best practices in an automated fashion. Kube- bench can be executed in a variety of ways. For example, you can install it as a platform-specific binary in the form of an RPM or Debian file. The most convenient and direct way to run the verification process is by running kube-bench in a Pod directly on the Kubernetes cluster. For that purpose, create a Job object with the help of a YAML manifest checked into the GitHub repository of the tool.\n\nStart by creating the Job from the file job-master.yaml, or job- node.yaml depending on whether you want to inspect a control plane node",
      "content_length": 1859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "or a worker node. The following command runs the verification checks against the control plane node:\n\n$ kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/\\ main/job-master.yaml job.batch/kube-bench-master created\n\nUpon Job execution, the corresponding Pod running the verification process can be identified by its name in the default namespace. The Pod’s name starts with the prefix kube-bench, then appended with the type of the node plus a hash at the end. The following output uses the Pod named kube- bench-master-8f6qh:\n\n$ kubectl get pods NAME READY STATUS RESTARTS AGE kube-bench-master-8f6qh 0/1 Completed 0 45s\n\nWait until the Pod transitions into the “Completed” status to ensure that all verification checks have finished. You can have a look at the benchmark result by dumping the logs of the Pod:\n\n$ kubectl logs kube-bench-master-8f6qh\n\nSometimes, it may be more convenient to write the verification results to a file. You can redirect the output of the kubectl logs command to a file, e.g., with the command kubectl logs kube-bench-master-8f6qh > control-plane-kube-bench-results.txt.\n\nThe kube-bench Verification Result\n\nThe produced verification result can be lengthy and detailed, but it consists of these key elements: the type of the inspected node, the inspected components, a list of passed checks, a list of failed checks, a list of warnings, and a high-level summary:",
      "content_length": 1416,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "[INFO] 1 Control Plane Security Configuration [INFO] 1.1 Control Plane Node Configuration Files [PASS] 1.1.1 Ensure that the API server pod specification file permissions are \\ set to 644 or more restrictive (Automated) ... [INFO] 1.2 API Server [WARN] 1.2.1 Ensure that the --anonymous-auth argument is set to false \\ (Manual) ... [FAIL] 1.2.6 Ensure that the --kubelet-certificate-authority argument is set \\ as appropriate (Automated)\n\n== Remediations master == ... 1.2.1 Edit the API server pod specification file /etc/kubernetes/manifests/ \\ kube-apiserver.yaml on the control plane node and set the below parameter. --anonymous-auth=false ... 1.2.6 Follow the Kubernetes documentation and setup the TLS connection between\n\nthe apiserver and kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the control plane node and \\\n\nset the --kubelet-certificate-authority parameter to the path to the cert \\ file for the certificate authority. --kubelet-certificate-authority=<ca-string>\n\n... == Summary total == 42 checks PASS 9 checks FAIL 11 checks WARN 0 checks INFO\n\nThe inspected node, in this case the control plane node.\n\nA passed check. Here, the file permissions of the API server configuration file.\n\nA warning message that prompts you to manually check the value of an argument provided to the API server executable.",
      "content_length": 1381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "A failed check. For example, the flag --kubelet-certificate- authority should be set for the API server executable.\n\nThe remediation action to take to fix a problem. The number, e.g., 1.2.1, of the failure or warning corresponds to the number assigned to the remediation action.\n\nThe summary of all passed and failed checks plus warning and informational messages.\n\nFixing Detected Security Issues\n\nThe list of reported warnings and failures can be a bit overwhelming at first. Keep in mind that you do not have to fix them all at once. Some checks are merely guidelines or prompts to verify an assigned value for a configuration. The following steps walk you through the process of eliminating a warning message.\n\nThe configuration files of the control plane components can be found in the directory /etc/kubernetes/manifests on the host system of the control plane node. Say you wanted to fix the warning 1.2.12 reported by kube- bench:\n\n[INFO] 1.2 API Server ... [WARN] 1.2.12 Ensure that the admission control plugin AlwaysPullImages is \\ set (Manual)\n\n== Remediations master == ... 1.2.12 Edit the API server pod specification file /etc/kubernetes/manifests/ \\ kube-apiserver.yaml on the control plane node and set the --enable-admission-plugins parameter \\ to include AlwaysPullImages. --enable-admission-plugins=...,AlwaysPullImages,...\n\nAs proposed by the remediation action, you are supposed to edit the configuration file for the API server and add the value AlwaysPullImages",
      "content_length": 1485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "to the list of admission plugins. Go ahead and edit the file kube- apiserver.yaml:\n\n$ sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml\n\nAfter appending the value AlwaysPullImages to the argument --enable- admission-plugins, the result could look as follows:\n\napiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: \\ 192.168.56.10:6443 creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --advertise-address=192.168.56.10 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/etc/kubernetes/pki/ca.crt - --enable-admission-plugins=NodeRestriction,AlwaysPullImages ...\n\nSave the changes to the file. The Pod running the API server in the kube- system namespace will be restarted automatically. The startup process can take a couple of seconds. Therefore, executing the following command may take a while to succeed:\n\n$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE ... kube-apiserver-control-plane 1/1 Running 0 71m ...",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "You will need to delete the existing Job object before you can verify the changed result:\n\n$ kubectl delete job kube-bench-master job.batch \"kube-bench-master\" deleted\n\nThe verification check 1.2.12 now reports a passed result:\n\n$ kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/\\ main/job-master.yaml job.batch/kube-bench-master created $ kubectl get pods NAME READY STATUS RESTARTS AGE kube-bench-master-5gjdn 0/1 Completed 0 10s $ kubectl logs kube-bench-master-5gjdn | grep 1.2.12 [PASS] 1.2.12 Ensure that the admission control plugin AlwaysPullImages is \\ set (Manual)\n\nCreating an Ingress with TLS Termination\n\nAn Ingress routes HTTP and/or HTTPS traffic from outside of the cluster to one or many Services based on a matching URL context path. You can see its functionality in action in Figure 2-2.\n\nFigure 2-2. Managing external access to the Services via HTTP(S)\n\nThe Ingress has been configured to accept HTTP and HTTPS traffic from outside of the cluster. If the caller provides the context path /app, then the traffic is routed to Service 1. If the caller provides the context path /api,",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "then the traffic is routed to Service 2. It’s important to point out that the communication typically uses unencrypted HTTP network communication as soon as it passes the Ingress.\n\nGiven that the Ingress API resource is a part of the CKAD and CKA exam, we are not going to discuss the basics anymore here. For a detailed discussion, refer to the information in the Certified Kubernetes Administrator (CKA) Study Guide or the Kubernetes documentation.\n\nTHE ROLE OF AN INGRESS CONTROLLER\n\nRemember that an Ingress cannot work without an Ingress controller. The Ingress controller evaluates the collection of rules defined by an Ingress that determine traffic routing. One example of a production-grade Ingress controller is the F5 NGINX Ingress Controller or AKS Application Gateway Ingress Controller. You can find other options listed in the Kubernetes documentation. If you are using minikube, make sure to enable the Ingress add-on.\n\nThe primary focus of the CKS lies on setting up Ingress objects with TLS termination. Configuring the Ingress for HTTPS communication relieves you from having to deal with securing the network communication on the Service level. In this section of the book, you will learn how to create a TLS certificate and key, how to feed the certificate and key to a TLS-typed Secret object, and how to configure an Ingress object so that it supports HTTPS communication.\n\nSetting Up the Ingress Backend\n\nIn the context of an Ingress, a backend is the combination of Service name and port. Before creating the Ingress, we’ll take care of the Service, a Deployment, and the Pods running nginx so we can later on demonstrate the routing of HTTPS traffic to an actual application. All of those objects are supposed to exist in the namespace t75. Example 2-4 defines all of those resources in a single YAML manifest file setup.yaml as a means to quickly create the Ingress backend.",
      "content_length": 1901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Example 2-4. YAML manifest for exposing nginx through a Service apiVersion: v1 kind: Namespace metadata: name: t75 --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: t75 labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: accounting-service namespace: t75 spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80\n\nCreate the objects from the YAML file with the following command:\n\n$ kubectl apply -f setup.yaml namespace/t75 created",
      "content_length": 669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "deployment.apps/nginx-deployment created service/accounting-service created\n\nLet’s quickly verify that the objects have been created properly, and the Pods have transitioned into the “Running” status. Upon executing the get all command, you should see a Deployment named nginx-deployment that controls three replicas, and a Service named accounting-service of type ClusterIP:\n\n$ kubectl get all -n t75 NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6595874d85-5rdrh 1/1 Running 0 108s pod/nginx-deployment-6595874d85-jmhvh 1/1 Running 0 108s pod/nginx-deployment-6595874d85-vtwxp 1/1 Running 0 108s\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) \\ AGE service/accounting-service ClusterIP 10.97.101.228 <none> 80/TCP \\ 108s\n\nNAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 108s\n\nCalling the Service endpoint from another Pod running on the same node should result in a successful response from the nginx Pod. Here, we are using the wget command to verify the behavior:\n\n$ kubectl run tmp --image=busybox --restart=Never -it --rm \\ -- wget 10.97.101.228:80 Connecting to 10.97.101.228:80 (10.97.101.228:80) saving to 'index.html' index.html 100% |**| 612 0:00:00 ETA 'index.html' saved pod \"tmp\" deleted\n\nWith those objects in place and functioning as expected, we can now concentrate on creating an Ingress with TLS termination.",
      "content_length": 1357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Creating the TLS Certificate and Key\n\nWe will need to generate a TLS certificate and key before we can create a TLS Secret. To do this, we will use the OpenSSL command. The resulting files are named accounting.crt and accounting.key:\n\n$ openssl req -nodes -new -x509 -keyout accounting.key -out accounting.crt \\ -subj \"/CN=accounting.tls\" Generating a 2048 bit RSA private key ...........................+ ..........................+ writing new private key to 'accounting.key' ----- $ ls accounting.crt accounting.key\n\nFor use in production environments, you’d generate a key file and use it to obtain a TLS certificate from a certificate authority (CA). For more information on creating a TLS certification and key, see the OpenSSL documentation.\n\nCreating the TLS-Typed Secret\n\nThe easiest way to create a Secret is with the help of an imperative command. This method of creation doesn’t require you to manually base64- encode the certificate and key values. The encoding happens automatically upon object creation. The following command uses the Secret option tls and assigns the certificate and key file name with the options --cert and - -key:\n\n$ kubectl create secret tls accounting-secret --cert=accounting.crt \\ --key=accounting.key -n t75 secret/accounting-secret created\n\nExample 2-5 shows the YAML representation of a TLS Secret if you want to create the object declaratively.\n\nExample 2-5. A Secret using the type kubernetes.io/tls",
      "content_length": 1444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "apiVersion: v1 kind: Secret metadata: name: accounting-secret namespace: t75 type: kubernetes.io/tls data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk... tls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk...\n\nMake sure to assign the values for the attributes tls.crt and tls.key as single-line, base64-encoded values. To produce the base64-encoded value, simply point the base64 command to the file name you want to convert the contents for. The following example base64-encoded the contents of the file accounting.crt:\n\n$ base64 accounting.crt LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNyakNDQ...\n\nCreating the Ingress\n\nYou can use the imperative method to create the Ingress with the help of a one-liner command shown in the following snippet. Crafting the value of the --rule argument is hard to get right. You will likely have to refer to the --help option for the create ingress command as it requires a specific expression. The information relevant to creating the connection between Ingress object and the TLS Secret is the appended argument tls=accounting-secret:\n\n$ kubectl create ingress accounting-ingress \\ --rule=\"accounting.internal.acme.com/*=accounting-service:80, \\ tls=accounting-secret\" -n t75 ingress.networking.k8s.io/accounting-ingress created\n\nExample 2-6 shows a YAML representation of an Ingress. The attribute for defining the TLS information is spec.tls[].\n\nExample 2-6. A YAML manifest for defining a TLS-terminated Ingress",
      "content_length": 1452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: accounting-ingress namespace: t75 spec: tls: - hosts: - accounting.internal.acme.com secretName: accounting-secret rules: - host: accounting.internal.acme.com http: paths: - path: / pathType: Prefix backend: service: name: accounting-service port: number: 80\n\nAfter creating the Ingress object with the imperative or declarative approach, you should be able to find it in the namespace t75. As you can see in the following output, the port 443 is listed in the “PORT” column, indicating that TLS termination has been enabled:\n\n$ kubectl get ingress -n t75 NAME CLASS HOSTS ADDRESS \\ PORTS AGE accounting-ingress nginx accounting.internal.acme.com 192.168.64.91 \\ 80, 443 55s\n\nDescribing the Ingress object shows that the backend could be mapped to the path / and will route traffic to the Pod via the Service named accounting-service:\n\n$ kubectl describe ingress accounting-ingress -n t75 Name: accounting-ingress Labels: <none> Namespace: t75 Address: 192.168.64.91 Ingress Class: nginx",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Default backend: <default> TLS: accounting-secret terminates accounting.internal.acme.com Rules: Host Path Backends ---- ---- -------- accounting.internal.acme.com / accounting-service:80 \\ (172.17.0.5:80,172.17.0.6:80,172.17.0.7:80) Annotations: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 1s (x2 over 31s) nginx-ingress-controller Scheduled for sync\n\nCalling the Ingress\n\nTo test the behavior on a local Kubernetes cluster on your machine, you need to first find out the IP address of a node. The following command reveals the IP address in a minikube environment:\n\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP \\ EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane 3d19h v1.24.1 192.168.64.91 \\ <none> Buildroot 2021.02.12 5.10.57 docker://20.10.16\n\nNext, you’ll need to add the IP address to the hostname mapping to your /etc/hosts file:\n\n$ sudo vim /etc/hosts ... 192.168.64.91 accounting.internal.acme.com\n\nYou can now send HTTPS requests to the Ingress using the assigned domain name and receive an HTTP response code 200 in return:\n\n$ wget -O- https://accounting.internal.acme.com --no-check-certificate --2022-07-28 15:32:43-- https://accounting.internal.acme.com/ Resolving accounting.internal.acme.com (accounting.internal.acme.com)... \\ 192.168.64.91",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Connecting to accounting.internal.acme.com (accounting.internal.acme.com) \\ |192.168.64.91|:443... connected. WARNING: cannot verify accounting.internal.acme.com's certificate, issued \\ by ‘CN=Kubernetes Ingress Controller Fake Certificate,O=Acme Co’: Self-signed certificate encountered. WARNING: no certificate subject alternative name matches\n\nrequested host name ‘accounting.internal.acme.com’.\n\nHTTP request sent, awaiting response... 200 OK\n\nProtecting Node Metadata and Endpoints\n\nKubernetes clusters expose ports used to communicate with cluster components. For example, the API server uses the port 6443 by default to enable clients like kubectl to talk to it when executing commands.\n\nThe Kubernetes documentation lists those ports in “Ports and Protocols”. The following two tables show the default port assignments per node.\n\nTable 2-1 shows the default inbound ports on the cluster node.\n\nTable 2-1. Inbound control plane node ports\n\nPort range\n\nPurpose\n\n6643\n\nKubernetes API server\n\n2379–2380\n\netcd server client API\n\n10250\n\nKubelet API\n\n10259\n\nkube-scheduler\n\n10257\n\nkube-controller-manager\n\nMany of those ports are configurable. For example, you can modify the API server port by providing a different value with the flag --secure-port in",
      "content_length": 1254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "the configuration file /etc/kubernetes/manifests/kube- apiserver.yaml, as documented for the cluster component. For all other cluster components, please refer to their corresponding documentation.\n\nTable 2-2 lists the default inbound ports on a worker node.\n\nTable 2-2. Inbound worker node ports\n\nPort range\n\nPurpose\n\n10250\n\nKubelet API\n\n30000–32767\n\nNodePort Services\n\nTo secure the ports used by cluster components, set up firewall rules to minimize the attack surface area. For example, you could decide not to expose the API server to anyone outside of the intranet. Clients using kubectl would only be able to run commands against the Kubernetes cluster if logged into the VPN, making the cluster less vulnerable to attacks.\n\nCloud provider Kubernetes clusters (e.g., on AWS, Azure, or Google Cloud) expose so-called metadata services. Metadata services are APIs that can provide sensitive data like an authentication token for consumption from VMs or Pods without any additional authorization. For the CKS exam, you need to be aware of those node endpoints and cloud provider metadata services. Furthermore, you should have a high-level understanding of how to protect them from unauthorized access.\n\nScenario: A Compromised Pod Can Access the Metadata Server\n\nFigure 2-3 shows an attacker who gained access to a Pod running on a node within a cloud provider Kubernetes cluster.",
      "content_length": 1384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Figure 2-3. An attacker who gained access to the Pod has access to metadata server\n\nAccess to the metadata server has not been restricted in any form. The attacker can retrieve sensitive information, which could open other possibilities of intrusion.\n\nProtecting Metadata Server Access with Network Policies\n\nLet’s pick one of the cloud providers that exposes a metadata endpoint. In AWS, the metadata server can be reached with the IP address 169.254.169.254, as described in the AWS documentation. The endpoints exposed can provide access to EC2 instance metadata. For example, you can retrieve the local IP address of an instance to manage a connection to an external application or to contact the instance with the help of a script. See the corresponding documentation page for calls to those endpoints made with the curl command line tool.\n\nTo prevent any Pod in a namespace from reaching the IP address of the metadata server, set up a network policy that allows egress traffic to all IP addresses except 169.254.169.254. Example 2-7 demonstrates a YAML manifest with such a rule set.\n\nExample 2-7. A default deny-all egress to IP address 169.254.169.254 network policy",
      "content_length": 1175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-egress-metadata-server namespace: a12 spec: podSelector: {} policyTypes: - Egress egress: - to: - ipBlock: cidr: 0.0.0.0/0 except: - 169.254.169.254/32\n\nOnce the network policy has been created, Pods in the namespace a12 should not be able to reach the metadata endpoints anymore. For detailed examples that use the endpoints via curl, see the relevant AWS documentation.\n\nProtecting GUI Elements The kubectl tool isn’t the only user interface (UI) for managing a cluster. While kubectl allows for fine-grained operations, most organizations prefer a more convenient graphical user interface (GUI) for managing the objects of a cluster. You can choose from a variety of options. The Kubernetes Dashboard is a free, web-based application. Other GUI dashboards for Kubernetes like Portainer go beyond the basic functionality by adding tracing of events or visualizations of hardware resource consumption. In this section, we’ll focus on the Kubernetes Dashboard as it is easy to install and configure.\n\nScenario: An Attacker Gains Access to the Dashboard Functionality\n\nThe Kubernetes Dashboard runs as a Pod inside of the cluster. Installing the Dashboard also creates a Service of type ClusterIP that only allows access",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "to the endpoint from within the cluster. To make the Dashboard accessible to end users, you’d have to expose the Service outside of the cluster. For example, you could switch to a NodePort Service type or stand up an Ingress. Figure 2-4 illustrates the high-level architecture of deploying and accessing the Dashboard.\n\nFigure 2-4. An attacker who gained access to the Dashboard\n\nAs soon as you expose the Dashboard to the outside world, attackers can potentially gain access to it. Without the right security settings, objects can be deleted, modified, or used for malicious purposes. The most prominent victim of such an attack was Tesla, which in 2018 fell prey to hackers who gained access to its unprotected Dashboard to mine cryptocurrencies. Since then, newer versions of the Dashboard changed default settings to make it more secure from the get-go.\n\nInstalling the Kubernetes Dashboard\n\nInstalling the Kubernetes Dashboard is straightforward. You can create the relevant objects with the help of the YAML manifest available in the project’s GitHub repository. The following command installs all necessary objects:\n\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/\\ v2.6.0/aio/deploy/recommended.yaml",
      "content_length": 1234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "RENDERING METRICS IN DASHBOARD\n\nYou may also want to install the metrics server if you are interested in inspecting resource consumption metrics as part of the Dashboard functionality.\n\nYou can find the objects created by the manifest in the kubernetes- dashboard namespace. Among them are Deployments, Pods, and Services. The following command lists all of them:\n\n$ kubectl get deployments,pods,services -n kubernetes-dashboard NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/dashboard-metrics-scraper 1/1 1 1 11m deployment.apps/kubernetes-dashboard 1/1 1 1 11m\n\nNAME READY STATUS RESTARTS AGE pod/dashboard-metrics-scraper-78dbd9dbf5-f8z4x 1/1 Running 0 11m pod/kubernetes-dashboard-5fd5574d9f-ns7nl 1/1 Running 0 11m\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP \\ PORT(S) AGE service/dashboard-metrics-scraper ClusterIP 10.98.6.37 <none> \\ 8000/TCP 11m service/kubernetes-dashboard ClusterIP 10.102.234.158 <none> \\ 80/TCP 11m\n\nAccessing the Kubernetes Dashboard\n\nThe kubectl proxy command can help with temporarily creating a proxy that allows you to open the Dashboard in a browser. This functionality is only meant for troubleshooting purposes and is not geared toward production environments. You can find information about the proxy command in the documentation:",
      "content_length": 1264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "$ kubectl proxy Starting to serve on 127.0.0.1:8001\n\nOpen the browser with the URL http://localhost:8001/api/v1/namespaces/kubernetes- dashboard/services/https:kubernetes-dashboard:/proxy. The Dashboard will ask you to provide an authentication method and credentials. The recommended way to configure the Dashboard is through bearer tokens.\n\nCreating a User with Administration Privileges\n\nBefore you can authenticate in the login screen, you need to create a ServiceAccount and ClusterRoleBinding object that grant admin permissions. Start by creating the file admin-user-serviceaccount.yaml and populate it with the contents shown in Example 2-8.\n\nExample 2-8. Service account for admin permissions apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard\n\nNext, store the contents of Example 2-9 in the file admin- user- clusterrole bind ing.yaml to map the ClusterRole named cluster-admin to the ServiceAccount.\n\nExample 2-9. ClusterRoleBinding for admin permissions apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard",
      "content_length": 1267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Create both objects with the following declarative command:\n\n$ kubectl create -f admin-user-serviceaccount.yaml serviceaccount/admin-user created $ kubectl create -f admin-user-clusterrolebinding.yaml clusterrolebinding.rbac.authorization.k8s.io/admin-user created\n\nYou can now create the bearer token of the admin user with the following command. The command will generate a token for the provided ServiceAccount object and render it on the console:\n\n$ kubectl create token admin-user -n kubernetes-dashboard eyJhbGciOiJSUzI1NiIsImtpZCI6...\n\nEXPIRATION OF A SERVICE ACCOUNT TOKEN\n\nBy default, this token will expire after 24 hours. That means that the token object will be deleted automatically once the “time to live” (TTL) has passed. You can change the TTL of a token by providing the command line option --ttl. For example, a value of 40h will expire the token after 40 hours. A value of 0 indicates that the token should never expire.\n\nCopy the output of the command and paste it into the “Enter token” field of the login screen, as shown in Figure 2-5.\n\nFigure 2-5. The usage of the token in the Dashboard login screen",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Pressing the “Sign in” button will bring you to the Dashboard shown in Figure 2-6.\n\nFigure 2-6. The Dashboard view of Pods in a specific namespace\n\nYou can now manage end user and cluster objects without any restrictions.\n\nCreating a User with Restricted Privileges\n\nIn the previous section, you learned how to create a user with cluster-wide administrative permissions. Most users of the Dashboard only need a restricted set of permissions, though. For example, developers implementing and operating cloud-native applications will likely only need a subset of administrative permissions to perform their tasks on a Kubernetes cluster. Creating a user for the Dashboard with restricted privileges consists of a three-step approach:\n\n1. Create a ServiceAccount object.\n\n2. Create a ClusterRole object that defines the permissions.\n\n3. Create a ClusterRoleBinding that maps the ClusterRole to the ServiceAccount.\n\nAs you can see, the process is very similar to the one we went through for the admin user. Step 2 is new, as we need to be specific about which permissions we want to grant. The YAML manifests that follow will model",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "a user working as a developer that should only be allowed read-only permissions (e.g., getting, listing, and watching resources).\n\nStart by creating the file restricted-user-serviceaccount.yaml and populate it with the contents shown in Example 2-10.\n\nExample 2-10. Service account for restricted permissions apiVersion: v1 kind: ServiceAccount metadata: name: developer-user namespace: kubernetes-dashboard\n\nThe ClusterRole in Example 2-11 only allows getting, listing, and watching resources. All other operations are not permitted. Store the contents in the file restricted-user-clusterrole.yaml.\n\nExample 2-11. ClusterRole for restricted permissions apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" name: cluster-developer rules: - apiGroups: - '*' resources: - '*' verbs: - get - list - watch - nonResourceURLs: - '*' verbs: - get - list - watch\n\nLast, map the ServiceAccount to the ClusterRole in the file restricted- user-clusterrolebinding.yaml, as shown in Example 2-12.\n\nExample 2-12. ClusterRoleBinding for restricted permissions",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: developer-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-developer subjects: - kind: ServiceAccount name: developer-user namespace: kubernetes-dashboard\n\nCreate all objects with the following declarative command:\n\n$ kubectl create -f restricted-user-serviceaccount.yaml serviceaccount/restricted-user created $ kubectl create -f restricted-user-clusterrole.yaml clusterrole.rbac.authorization.k8s.io/cluster-developer created $ kubectl create -f restricted-user-clusterrolebinding.yaml clusterrolebinding.rbac.authorization.k8s.io/developer-user created\n\nGenerate the bearer token of the restricted user with the following command:\n\n$ kubectl create token developer-user -n kubernetes-dashboard eyJhbGciOiJSUzI1NiIsImtpZCI6...\n\nOperations that are not allowed for the logged-in user will not be rendered as disabled options in the GUI. You can still select the option; however, an error message is rendered. Figure 2-7 illustrates the behavior of the Dashboard if you try to delete a Pod via the user that doesn’t have the permissions to perform the operation.",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Figure 2-7. An error message rendered when trying to invoke a permitted operation\n\nAvoiding Insecure Configuration Arguments\n\nSecuring the Dashboard in production environments involves the usage of execution arguments necessary for properly configuring authentication and authorization. By default, login functionality is enabled and the HTTPS endpoint will be exposed on port 8443. You can provide TLS certificates with the --tls-cert-file and --tls-cert-key command line options if you don’t want them to be auto-generated.\n\nAvoid setting the command line arguments --insecure-port to expose an HTTP endpoint and --enable-insecure-login to enable serving the login page over HTTP instead of HTTPS. Furthermore, make sure you don’t use the option --enable-skip-login as it would allow circumventing an authentication method by simply clicking a Skip button in the login screen.\n\nVerifying Kubernetes Platform Binaries\n\nThe Kubernetes project publishes client and server binaries with every release. The client binary refers to the executable kubectl. Server binaries include kubeadm, as well as the executable for the API server, the scheduler, and the kubelet. You can find those files under the “tags” sections of the Kubernetes GitHub repository or on the release page at https://dl.k8s.io.",
      "content_length": 1294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Scenario: An Attacker Injected Malicious Code into Binary\n\nThe executables kubectl and kubeadm are essential for interacting with Kubernetes. kubectl lets you run commands against the API server, e.g., for managing objects. kubeadm is necessary for upgrading cluster nodes from one version to another. Say you are in the process of upgrading the cluster version from 1.23 to 1.24. As part of the process, you will need to upgrade the kubeadm binary as well. The official upgrade documentation is very specific about what commands to use for upgrading the binary.\n\nSay an attacker managed to modify the kubeadm executable for version 1.24 and coaxed you into thinking that you need to download that very binary from a location where the malicious binary was placed. As shown in Figure 2-8, you’d expose yourself to running malicious code every time you invoke the modified kubeadm executable. For example, you may be sending credentials to a server outside of your cluster, which would open new ways to infiltrate your Kubernetes environment.",
      "content_length": 1041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Figure 2-8. An attacker who injected malicious code into a binary\n\nVerifying a Binary Against Hash\n\nYou can verify the validity of a binary with the help of a hash code like MD5 or SHA. Kubernetes publishes SHA256 hash codes for each binary. You should run through a hash validation for individual binaries before using them for the first time. Should the generated hash code not match with the one you downloaded, then there’s something off with the binary. The binary may have been modified by a third party or you didn’t use the hash code for the correct binary type or version.\n\nYou can download the corresponding hash code for a binary from https://dl.k8s.io. The full URL for a hash code reflects the version, operating system, and architecture of the binary. The following list shows example URLs for platform binaries compatible with Linux AMD64:",
      "content_length": 854,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "kubectl: https://dl.k8s.io/v1.26.1/bin/linux/amd64/kubectl.sha256\n\nkubeadm: https://dl.k8s.io/v1.26.1/bin/linux/amd64/kubeadm.sha256\n\nkubelet: https://dl.k8s.io/v1.26.1/bin/linux/amd64/kubelet.sha256\n\nkube-apiserver: https://dl.k8s.io/v1.26.1/bin/linux/amd64/kube- apiserver.sha256\n\nYou’ll have to use an operating system-specific hash code validation tool to check the validity of a binary. You may have to install the tool if you do not have it available on your machine yet. The following commands show the usage of the tool for different operating systems, as explained in the Kubernetes documentation:\n\nLinux: echo \"$(cat kubectl.sha256) kubectl\" | sha256sum --check\n\nMacOSX: echo \"$(cat kubectl.sha256) kubectl\" | shasum -a 256 --check\n\nWindows with Powershell: $($(CertUtil -hashfile .\\kubectl.exe SHA256)[1] -replace \" \", \"\") -eq $(type .\\kubectl.exe.sha256)\n\nThe following commands demonstrate downloading the kubeadm binary for version 1.26.1 and its corresponding SHA256 hash file:\n\n$ curl -LO \"https://dl.k8s.io/v1.26.1/bin/linux/amd64/kubeadm\" $ curl -LO \"https://dl.k8s.io/v1.26.1/bin/linux/amd64/kubeadm.sha256\"\n\nThe validation tool shasum can verify if the checksum matches:\n\n$ echo \"$(cat kubeadm.sha256) kubeadm\" | shasum -a 256 --check kubeadm: OK",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "The previous command returned with an “OK” message. The binary file wasn’t tampered with. Any other message indicates a potential security risk when executing the binary.\n\nSummary\n\nThe domain “cluster setup” dials in on security aspects relevant to setting up a Kubernetes cluster. Even though you might be creating a cluster from scratch with kubeadm, that doesn’t mean you are necessarily following best practices. Using kube-bench to detect potential security risks is a good start. Fix the issues reported on by the tool one by one. You may also want to check client and server binaries against their checksums to ensure that they haven’t been modified by an attacker. Some organizations use a Dashboard to manage the cluster and its objects. Ensure that authentication and authorization for the Dashboard restrict access to a small subset of stakeholders.\n\nAn important security aspect is network communication. Pod-to-Pod communication is unrestricted by default. Have a close look at your application architecture running inside of Kubernetes. Only allow directional network traffic from and to Pods to fulfill the requirements of your architecture. Deny all other network traffic. When exposing the application outside of the cluster, make sure that Ingress objects have been configured with TLS termination. This will ensure that the data is encrypted both ways so that attackers cannot observe sensitive information like passwords sent between a client and the Kubernetes cluster.\n\nExam Essentials\n\nUnderstand the purpose and effects of network policies\n\nBy default, Pod-to-Pod communication is unrestricted. Instantiate a default deny rule to restrict Pod-to-Pod network traffic with the principle of least privilege. The attribute spec.podSelector of a",
      "content_length": 1764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "network policy selects the target Pod the rules apply to based on label selection. The ingress and egress rules define Pods, namespaces, IP addresses, and ports for allowing incoming and outgoing traffic. Network policies can be aggregated. A default deny rule may disallow ingress and/or egress traffic. An additional network policy can open up those rules with a more fine-grained definition.\n\nPractice the use of kube-bench to detect cluster component vulnerabilities\n\nThe Kubernetes CIS Benchmark is a set of best practices for recommended security settings in a production Kubernetes environment. You can automate the process of detecting security risks with the help of the tool kube-bench. The generated report from running kube-bench describes detailed remediation actions to fix a detected issue. Learn how to interpret the results and how to mitigate the issue.\n\nKnow how to configure Ingress with TLS termination\n\nAn Ingress can be configured to send and receive encrypted data by exposing an HTTPS endpoint. For this to work, you need to create a TLS Secret object and assign it a TLS certificate and key. The Secret can then be consumed by the Ingress using the attribute spec.tls[].\n\nKnow how to configure GUI elements for secure access\n\nGUI elements, such as the Kubernetes Dashboard, provide a convenient way to manage objects. Attackers can cause harm to your cluster if the application isn’t protected from unauthorized access. For the exam, you need to know how to properly set up RBAC for specific stakeholders. Moreover, you are expected to have a rough understanding of security- related command line arguments. Practice the installation process for the Dashboard, learn how to tweak its command line arguments, and understand the effects of setting permissions for different users.\n\nKnow how to detect modified platform binaries",
      "content_length": 1851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Platform binaries like kubectl and kubeadm can be verified against their corresponding hash code. Know where to find the hash file and how to use a validation tool to identify if the binary has been tempered with.\n\nSample Exercises\n\nSolutions to these exercises are available in the Appendix.\n\n1. Create a network policy that denies egress traffic to any domain outside of the cluster. The network policy applies to Pods with the label app=backend and also allows egress traffic for port 53 for UDP and TCP to Pods in any other namespace.\n\n2. Create a Pod named allowed that runs the busybox:1.36.0 image on port 80 and assign it the label app=frontend. Make a curl call to http://google.com. The network call should be allowed, as the network policy doesn’t apply to the Pod.\n\n3. Create another Pod named denied that runs the busybox:1.36.0 image on port 80 and assign it the label app=backend. Make a curl call to http://google.com. The network call should be blocked.\n\n4. Install the Kubernetes Dashboard or make sure that it is already installed. In the namespace kubernetes-dashboard, create a ServiceAccount named observer-user. Moreover, create the corresponding ClusterRole and ClusterRoleBinding. The ServiceAccount should only be allowed to view Deployments. All other operations should be denied. As an example, create the Deployment named deploy in the default namespace with the following command: kubectl create deployment deploy -- image=nginx --replicas=3.",
      "content_length": 1472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "5. Create a token for the ServiceAccount named observer-user that will never expire. Log into the Dashboard using the token. Ensure that only Deployments can be viewed and not any other type of resource.\n\n6. Download the binary file of the API server with version 1.26.1 on Linux AMD64. Download the SH256 checksum file for the API- server executable of version 1.23.1. Run the OS-specific verification tool and observe the result.\n\nOceanofPDF.com",
      "content_length": 447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Chapter 3. Cluster Hardening\n\nThe domain “cluster hardening” touches on topics important to keep a cluster as secure as possible once it has been set up and configured initially. As part of the discussion of this chapter, you may notice that I will reference concepts and practices that usually fall into the hands of Kubernetes administrators. Where appropriate, I will provide links to the topics that have already been covered by the CKA exam.\n\nAt a high level, this chapter covers the following concepts:\n\nRestricting access to the Kubernetes API\n\nConfiguring role-based access control (RBAC) to minimize exposure\n\nExercising caution in using service accounts\n\nUpdating Kubernetes frequently\n\nInteracting with the Kubernetes API\n\nThe API server is the gateway to the Kubernetes cluster. Any human user, client (e.g., kubectl), cluster component, or service account will access the API server by making a RESTful API call via HTTPS. It is the central point for performing operations like creating a Pod, or deleting a Service.\n\nIn this section, we’ll only focus on the security-specific aspects relevant to the API server. For a detailed discussion on the inner workings of the API server and the usage of the Kubernetes API, refer to the book Managing Kubernetes by Brendan Burns and Craig Tracey (O’Reilly).\n\nProcessing a Request",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Figure 3-1 illustrates the stages a request goes through when a call is made to the API server. For reference, you can find more information in the Kubernetes documentation.\n\nFigure 3-1. API server request processing\n\nThe first stage of request processing is authentication. Authentication validates the identity of the caller by inspecting the client certificates or bearer tokens. If the bearer token is associated with a service account, then it will be verified here.\n\nThe second stage determines if the identity provided in the first stage can access the verb and HTTP path request. Therefore, stage two deals with authorization of the request, which is implemented with the standard Kubernetes RBAC model. Here, we’d ensure that the service account is allowed to list Pods or create a new Service object if that’s what has been requested.\n\nThe third stage of request processing deals with admission control. Admission control verifies if the request is well-formed and potentially needs to be modified before the request is processed. An admission control policy could, for example, ensure that the request for creating a Pod includes the definition of a specific label. If it doesn’t define the label, then the request is rejected.\n\nThe last stage of the process ensures that the resource included in the request is valid. Request validation can be implemented as part of admission control but doesn’t have to be. For example, this stage ensures",
      "content_length": 1452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "that the name of a Service object sticks to the standard Kubernetes naming rules for provided DNS names.\n\nConnecting to the API Server\n\nIt’s easy to determine the endpoint for the API server by running the following:\n\n$ kubectl cluster-info Kubernetes control plane is running at https://172.28.40.5:6443 ...\n\nFor the given Kubernetes cluster, the API server has been exposed via the URL https://172.28.40.5:6443. Alternatively, you can also have a look at the command line options --advertise-address and --secure-port in the configuration file of the API server to determine the endpoint. You can find the API server configuration file at /etc/kubernetes/manifests/kube-apiserver.yaml.\n\nCONFIGURING AN INSECURE PORT FOR THE API SERVER\n\nThe ability to configure the API server to use an insecure port (e.g., 80) has been deprecated in Kubernetes 1.10. With version 1.24, the insecure port flags --port and -- insecure-port have been removed completely and therefore cannot be used to configure the API server anymore. See the release notes for more information.\n\nUsing the kubernetes Service\n\nKubernetes makes accessing the API server a little bit more convenient for specific use cases. For example, you may want to send a request to the Kubernetes API from a Pod. Instead of using the IP address and port for the API server, you can simply refer to the Service named kubernetes.default.svc instead. This special Service lives in the default namespace and is stood up by the cluster automatically. Deleting",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "the Service will automatically recreate it. You can easily find the Service with the following command:\n\n$ kubectl get service kubernetes NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 32s\n\nUpon inspection of the endpoints of this Service, you will see that it points to the IP address and port of the API server, as demonstrated by executing the following command:\n\n$ kubectl get endpoints kubernetes NAME ENDPOINTS AGE kubernetes 172.28.40.5:6443 4m3s\n\nThe IP address and port of the Service is also exposed to the Pod via environment variables. You can read the values of the environment variables from a program running inside of a container. The Service’s IP address is reflected by the environment variable KUBERNETES_SERVICE_HOST. The port can be accessed using the environment variable KUBERNETES_SERVICE_PORT. To render the environment, simply access the environment variables using the env command in a temporary Pod:\n\n$ kubectl run kubernetes-envs --image=alpine:3.16.2 -it --rm --restart=Never \\ -- env KUBERNETES_SERVICE_HOST=10.96.0.1 KUBERNETES_SERVICE_PORT=443\n\nWe will use the kubernetes Service in the section “Minimizing Permissions for a Service Account”.\n\nAnonymous access\n\nThe following command makes an anonymous call to the API using the curl command line tool to list all namespaces. The option -k avoids verifying the server’s TLS certificate:",
      "content_length": 1413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "$ curl https://172.28.40.5:6443/api/v1/namespaces -k { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Failure\", \"message\": \"namespaces is forbidden: User \\\"system:anonymous\\\" cannot list \\ resource \\\"namespaces\\\" in API group \\\"\\\" at the cluster scope\", \"reason\": \"Forbidden\", \"details\": { \"kind\": \"namespaces\" }, \"code\": 403 }\n\nAs you can see from the JSON-formatted HTTP response body, anonymous calls are accepted by the API server but do not have the appropriate permissions for the operation. Internally, Kubernetes maps the call to the username system:anonymous, which effectively isn’t authorized to execute the operation.\n\nAccess with a client certificate\n\nTo make a request as an authorized user, you need to either create a new one or use the existing, default user with administrator permissions named kubernetes-admin. We won’t go through the process of creating a new user right now. For more information on creating a user, refer to “Restricting User Permissions”.\n\nThe following command lists all available users, including their client certificate and key:\n\n$ kubectl config view --raw apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tL... server: https://172.28.132.5:6443 name: kubernetes contexts: - context: cluster: kubernetes",
      "content_length": 1312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tL... client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktL...\n\nThe base64-encoded value of the certificate authority\n\nThe user entry with administrator permissions created by default\n\nThe base64-encoded value of the user’s client certificate\n\nThe base64-encoded value of the user’s private key\n\nFor making a call using the user kubernetes-admin, we’ll need to extract the base64-encoded values for the CA, client certificate, and private key into files as a base64-decoded value. The following command copies the base64-encoded value and uses the tool base64 to decode it before it is written to a file. The CA value will be stored in the file ca, the client certificate value in kubernetes-admin.crt, and the private key in kubernetes-admin.key:\n\n$ echo LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tL... | base64 -d > ca $ echo LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tL... | base64 -d > kubernetes-admin.crt $ echo LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktL... | base64 - \\ > kubernetes-admin.key\n\nYou can now point the curl command to those files with the relevant command line option. The request to the API server should properly authenticate and return all existing namespaces, as the kubernetes-admin has the appropriated permissions:",
      "content_length": 1441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "$ curl --cacert ca --cert kubernetes-admin.crt --key kubernetes-admin.key \\ https://172.28.132.5:6443/api/v1/namespaces { \"kind\": \"NamespaceList\", \"apiVersion\": \"v1\", \"metadata\": { \"resourceVersion\": \"2387\" }, \"items\": [ ... ] }\n\nRestricting Access to the API Server\n\nIf you’re exposing the API server to the internet, ask yourself if it is necessary. Some cloud providers offer the option of creating a private cluster, which will limit or completely disable public access to the API server. For more information, see the documentation pages for EKS and GKE.\n\nIf you are operating an on-premises Kubernetes cluster, you will need to instantiate firewall rules that prevent access to the API server. Setting up firewall rules is out of scope for the exam and therefore won’t be discussed in this book.\n\nScenario: An Attacker Can Call the API Server from the Internet\n\nCloud providers sometimes expose the API server to the internet to simplify administrative access. An attacker can try to make an anonymous request to the API server endpoint by declining to provide a client certificate or bearer token. If the attacker is lucky enough to capture user credentials, then an authenticated call can be performed. Depending on the permissions assigned to the user, malicious operations can be executed. Figure 3-2 illustrates an attacker calling the API server from the internet.",
      "content_length": 1376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Figure 3-2. An attacker calls the API server from the internet\n\nIn this chapter, we will have a look at how to restrict access to the API server and how to implement RBAC with limited permissions by example. “Understanding Open Policy Agent (OPA) and Gatekeeper” will review admission control with the help of OPA Gateway.\n\nRestricting User Permissions\n\nWe’ve seen that we can use the credentials of the kubernetes-admin user to make calls to the Kubernetes API. This user should be used very sparingly, nor should the credentials be shared with a lot of humans. A lot of damage can be done if the credentials fall into the wrong hands. Reserve this user exclusively for humans in charge of cluster administration.\n\nFor other stakeholders of your Kubernetes cluster, you should set up a dedicated user with a limited set of permissions. You may have specific roles in your organization you can map to. For example, you may have a developer role that should be allowed to manage Deployments, Pods, ConfigMaps, Secrets, and Services, but nothing else. To create a new user and assign the relevant RBAC permissions, refer to the Kubernetes documentation. In a nutshell, there are four steps:\n\n1. Create a private key.\n\n2. Create and approve a CertificateSigningRequest.\n\n3. Create a Role and a RoleBinding.\n\n4. Add the user to the kubeconfig file.",
      "content_length": 1344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "We will cover the process in detail, but will come back to the RBAC concept in more detail for a service account in “Minimizing Permissions for a Service Account”.\n\nCreating a private key Create a private key using the openssl executable. Provide an expressive file name, such as <username>.key:\n\n$ openssl genrsa -out johndoe.key 2048 Generating RSA private key, 2048 bit long modulus ...+ ......................................................................+ e is 65537 (0x10001)\n\nCreate a certificate signing request (CSR) in a file with the extension .csr. You need to provide the private key from the previous step. The following command uses the username johndoe when asked for entering the “Common Name” value. All other input requests are optional and can be filled in as needed:\n\n$ openssl req -new -key johndoe.key -out johndoe.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) []: State or Province Name (full name) []: Locality Name (eg, city) []: Organization Name (eg, company) []: Organizational Unit Name (eg, section) []: Common Name (eg, fully qualified host name) []:johndoe Email Address []:\n\nPlease enter the following 'extra' attributes to be sent with your certificate request A challenge password []:",
      "content_length": 1548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Retrieve the base64-encoded value of the CSR file content with the following command. You will need it when creating the CertificateSigningRequest object in the next step:\n\n$ cat johndoe.csr | base64 | tr -d \"\\n\" LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tL...\n\nCreating and approving a CertificateSigningRequest\n\nThe following script creates a CertificateSigningRequest object. A CertificateSigningRequest resource is used to request that a certificate be signed by a denoted signer:\n\n$ cat <<EOF | kubectl apply -f - apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: johndoe spec: request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tL... signerName: kubernetes.io/kube-apiserver-client expirationSeconds: 86400 usages: - client auth EOF certificatesigningrequest.certificates.k8s.io/johndoe created\n\nThe value for kubernetes.io/kube-apiserver-client for the attribute spec.signerName signs certificates that will be honored as client certificates by the API server. Use the base64-encoded value from the previous step and assign it as a value to the attribute spec.request. Finally, the optional attribute spec.expirationSeconds determines the lifespan of the certificate. The assigned value 86400 makes the certificate valid for a one day. You will want to increase the expiration time depending on how long you want the certificate to last, or simply refrain from adding the attribute.\n\nAfter creating the CertificateSigningRequest object, the condition will be “Pending.” You will need to approve the signing request within 24 hours or",
      "content_length": 1575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "the object will be deleted automatically as a means to garbage-collect unnecessary objects in the cluster:\n\n$ kubectl get csr johndoe NAME AGE SIGNERNAME REQUESTOR \\ REQUESTEDDURATION CONDITION johndoe 6s kubernetes.io/kube-apiserver-client minikube-user \\ 24h Pending\n\nUse the certificate approve command to approve the signing request. As a result, the condition changes to “Approved,Issued”:\n\n$ kubectl certificate approve johndoe certificatesigningrequest.certificates.k8s.io/johndoe approved $ kubectl get csr johndoe NAME AGE SIGNERNAME REQUESTOR \\ REQUESTEDDURATION CONDITION johndoe 17s kubernetes.io/kube-apiserver-client minikube-user \\ 24h Approved,Issued\n\nFinally, export the issued certificate from the approved CertificateSigningRequest object:\n\n$ kubectl get csr johndoe -o jsonpath={.status.certificate}| base64 \\ -d > johndoe.crt\n\nCreating a Role and a RoleBinding\n\nIt’s time to assign RBAC permissions. In this step, you will create a Role and a RoleBinding for the user. The Role models an “application developer” role within the organization. A developer should only be allowed to get, list, update, and delete Pods. The following imperative command creates the Role object:\n\n$ kubectl create role developer --verb=create --verb=get --verb=list \\ --verb=update --verb=delete --resource=pods role.rbac.authorization.k8s.io/developer created",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Next, we’ll bind the Role to the user named johndoe. Use the imperative command create rolebinding to achieve that:\n\n$ kubectl create rolebinding developer-binding-johndoe --role=developer \\ --user=johndoe rolebinding.rbac.authorization.k8s.io/developer-binding-johndoe created\n\nAdding the user to the kubeconfig file\n\nIn this last step, you will need to add the user to the kubeconfig file and create the context for a user. Be aware that the cluster name is minikube in the following command, as we are trying this out in a minikube installation:\n\n$ kubectl config set-credentials johndoe --client-key=johndoe.key \\ --client-certificate=johndoe.crt --embed-certs=true User \"johndoe\" set. $ kubectl config set-context johndoe --cluster=minikube --user=johndoe Context \"johndoe\" created.\n\nVerifying the permissions It’s time to switch to the user context named johndoe:\n\n$ kubectl config use-context johndoe Switched to context \"johndoe\".\n\nUsing kubectl as the client that makes calls to the API server, we’ll verify that the operation should be allowed. The API call for listing all Pods in the default namespace was authenticated and authorized:\n\n$ kubectl get pods No resources found in default namespace.\n\nThe output of the command indicates that the default namespace doesn’t contain any Pod object at this time but the call was successful. Let’s also test the negative case. Listing namespaces is a non-permitted operation for the user. Executing the relevant kubectl command will return with an error message:",
      "content_length": 1516,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "$ kubectl get namespaces Error from server (Forbidden): namespaces is forbidden: User \"johndoe\" cannot \\ list resource \"namespaces\" in API group \"\" at the cluster scope\n\nOnce you are done with verifying permissions, you may want to switch back to the context with admin permissions:\n\n$ kubectl config use-context minikube Switched to context \"minikube\".\n\nScenario: An Attacker Can Call the API Server from a Service Account\n\nA user represents a real person who commonly interacts with the Kubernetes cluster using the kubectl executable or the UI dashboard. Under rare conditions, applications running inside of a Pod’s container need to interact with the Kubernetes API. A typical example for such a requirement is the package manager Helm. Helm manages Kubernetes resources based on the YAML manifests bundled in a Helm chart. Kubernetes uses a service account to authenticate the Helm service process with the API server through an authentication token. This service account can be assigned to a Pod and mapped to RBAC rules.\n\nAn attacker who gains access to the Pod will likely also be able to misuse the service account to make calls to the Kubernetes API, as shown in Figure 3-3.\n\nFigure 3-3. An attacker uses a service account to call the API server\n\nMinimizing Permissions for a Service Account",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "It’s important to limit the permissions to only those service accounts that are really necessary for the application to function. The next sections will explain how to achieve this to minimize the potential attack surface.\n\nFor this scenario to work, you’ll need to create a ServiceAccount object and assign it to the Pod. Service accounts can be tied in with RBAC and assigned a Role and RoleBinding to define what operations they should be allowed to perform.\n\nBinding the service account to a Pod\n\nAs a starting point, we are going to a set up a Pod that lists all Pods and Deployments in the namespace k97 by calling the Kubernetes API. The call is made as part of an infinite loop every ten seconds. The response from the API call will be written to standard output accessible via the Pod’s logs.\n\nTo authenticate against the API server, we’ll send a bearer token associated with the service account used by the Pod. The default behavior of a service account is to auto-mount API credentials on the path /var/run/secrets/kubernetes.io/serviceaccount/token. We’ll simply get the contents of the file using the cat command line tool and send them along as a header for the HTTP request. Example 3-1 defines the namespace, the service account, and the Pod in a single YAML manifest file setup.yaml.\n\nExample 3-1. YAML manifest for assigning a service account to a Pod apiVersion: v1 kind: Namespace metadata: name: k97 --- apiVersion: v1 kind: ServiceAccount metadata: name: sa-api namespace: k97 --- apiVersion: v1 kind: Pod metadata:",
      "content_length": 1537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "name: list-objects namespace: k97 spec: serviceAccountName: sa-api containers: - name: pods image: alpine/curl:3.14 command: ['sh', '-c', 'while true; do curl -s -k -m 5 -H \\ \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/ \\ serviceaccount/token)\" https://kubernetes.default.svc.cluster. \\ local/api/v1/namespaces/k97/pods; sleep 10; done'] - name: deployments image: alpine/curl:3.14 command: ['sh', '-c', 'while true; do curl -s -k -m 5 -H \\ \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/ \\ serviceaccount/token)\" https://kubernetes.default.svc.cluster. \\ local/apis/apps/v1/namespaces/k97/deployments; sleep 10; done']\n\nCreate the objects from the YAML file with the following command:\n\n$ kubectl apply -f setup.yaml namespace/k97 created serviceaccount/sa-api created pod/list-objects created\n\nVerifying the default permissions The Pod named list-objects makes a call to the API server to retrieve the list of Pods and Deployments in dedicated containers. The container pods performs the call to list Pods. The container deployments sends a request to the API server to list Deployments.\n\nAs explained in the Kubernetes documentation, the default RBAC policies do not grant any permissions to service accounts outside of the kube- system namespace. The logs of the containers pods and deployments return an error message indicating that the service account sa-api is not authorized to list the resources:\n\n$ kubectl logs list-objects -c pods -n k97 { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {},",
      "content_length": 1535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "\"status\": \"Failure\", \"message\": \"pods is forbidden: User \\\"system:serviceaccount:k97:sa-api\\\" \\ cannot list resource \\\"pods\\\" in API group \\\"\\\" in the \\ namespace \\\"k97\\\"\", \"reason\": \"Forbidden\", \"details\": { \"kind\": \"pods\" }, \"code\": 403 } $ kubectl logs list-objects -c deployments -n k97 { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Failure\", \"message\": \"deployments.apps is forbidden: User \\ \\\"system:serviceaccount:k97:sa-api\\\" cannot list resource \\ \\\"deployments\\\" in API group \\\"apps\\\" in the namespace \\ \\\"k97\\\"\", \"reason\": \"Forbidden\", \"details\": { \"group\": \"apps\", \"kind\": \"deployments\" }, \"code\": 403 }\n\nNext up, we’ll stand up a ClusterRole and RoleBinding object with the required API permissions to perform the necessary calls.\n\nCreating the ClusterRole Start by defining the ClusterRole named list-pods-clusterrole shown in Example 3-2 in the file clusterrole.yaml. The set of the rules only adds the Pod resource and the verb list.\n\nExample 3-2. YAML manifest for a ClusterRole that allows listing Pods apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: list-pods-clusterrole rules: - apiGroups: [\"\"]",
      "content_length": 1163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "resources: [\"pods\"] verbs: [\"list\"]\n\nCreate the object by pointing to its corresponding YAML manifest file:\n\n$ kubectl apply -f clusterrole.yaml clusterrole.rbac.authorization.k8s.io/list-pods-clusterrole created\n\nCreating the RoleBinding\n\nExample 3-3 defines the YAML manifest for the RoleBinding in the file rolebinding.yaml. The RoleBinding maps the ClusterRole list-pods- clusterrole to the service account named sa-pod-api and only applies to the namespace k97.\n\nExample 3-3. YAML manifest for a RoleBinding attached to a service account apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: serviceaccount-pod-rolebinding namespace: k97 subjects: - kind: ServiceAccount name: sa-api roleRef: kind: ClusterRole name: list-pods-clusterrole apiGroup: rbac.authorization.k8s.io\n\nCreate both the RoleBinding object using the apply command:\n\n$ kubectl apply -f rolebinding.yaml rolebinding.rbac.authorization.k8s.io/serviceaccount-pod-rolebinding created\n\nVerifying the granted permissions With the granted list permissions, the service account can now properly retrieve all the Pods in the k97 namespace. The curl command in the pods container succeeds, as shown in the following output:",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "$ kubectl logs list-objects -c pods -n k97 { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"resourceVersion\": \"628\" }, \"items\": [ { \"metadata\": { \"name\": \"list-objects\", \"namespace\": \"k97\", ... } ] }\n\nWe did not grant any permissions to the service account for other resources. Listing the Deployments in the k97 namespace still fails. The following output shows the response from the curl command in the deployments namespace:\n\n$ kubectl logs list-objects -c deployments -n k97 { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Failure\", \"message\": \"deployments.apps is forbidden: User \\ \\\"system:serviceaccount:k97:sa-api\\\" cannot list resource \\ \\\"deployments\\\" in API group \\\"apps\\\" in the namespace \\ \\\"k97\\\"\", \"reason\": \"Forbidden\", \"details\": { \"group\": \"apps\", \"kind\": \"deployments\" }, \"code\": 403 }\n\nFeel free to modify the ClusterRole object to allow listing Deployment objects as well.",
      "content_length": 920,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Disabling automounting of a service account token\n\nThe Pod described in the previous section used the service account’s token as a means to authenticate against the API server. Mounting the token file at /var/run/secrets/kubernetes.io/serviceaccount/token is the standard behavior of every service account. You will really only need the contents of the file if the Pod actually interacts with the Kubernetes API. In all other cases, this behavior poses a potential security risk as access to the Pod will directly lead an attacker to the token.\n\nYou can disable the automount behavior for a service account object by assigning the value false to the attribute automountServiceAccountToken, as shown in Example 3-4.\n\nExample 3-4. Opting out of a service account’s token automount behavior apiVersion: v1 kind: ServiceAccount metadata: name: sa-api namespace: k97 automountServiceAccountToken: false\n\nIf you want to disable the automount behavior for individual Pods, use the attribute spec.automountServiceAccountToken in the Pod definition. Example 3-5 shows a YAML manifest for a Pod.\n\nExample 3-5. Disabling token automounting for a service account in a Pod apiVersion: v1 kind: Pod metadata: name: list-objects namespace: k97 spec: serviceAccountName: sa-api automountServiceAccountToken: false ...\n\nGenerating a service account token\n\nThere are a variety of use cases that speak for wanting to create a service account that disables token automounting. For example, you may need",
      "content_length": 1482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "access to the Kubernetes API from an external tool or a continuous delivery pipeline to query for information about existing objects. Authenticating against the API server in those scenarios still requires a token. The scenarios listed do not necessarily run a Pod with an assigned service account, but simply perform a RESTful API call from a tool like curl.\n\nTo create a token manually, execute the create token command and provide the name of the service account as an argument. The output of the command renders the token:\n\n$ kubectl create token sa-api eyJhbGciOiJSUzI1NiIsImtpZCI6IjBtQkJzVWlsQjl...\n\nYou’ll need to store the token in a safe place, e.g., a password manager. You cannot retrieve the token again if you lose it. You can only recreate it with the same command, which will automatically invalidate the previous token. All references that use the token will have to be changed.\n\nFor automated processes, it might be helpful to generate a token with a limited lifespan. The --duration will automatically invalidate the token after the “time-to-life” runs out:\n\n$ kubectl create token sa-api --duration 10m eyJhbGciOiJSUzI1NiIsImtpZCI6IjBtQkJzVWlsQjl...\n\nCreating a Secret for a service account\n\nWith Kubernetes 1.24, a ServiceAccount object does not automatically create a corresponding Secret object containing the token anymore. See the release notes for more information. Listing the ServiceAccount object renders 0 for the number of Secrets. The object also doesn’t contain the secrets attribute anymore in the YAML representation:\n\n$ kubectl get serviceaccount sa-api -n k97 NAME SECRETS AGE sa-api 0 42m",
      "content_length": 1625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "You can either generate the token using the create token command, as described in “Generating a service account token”, or manually create a corresponding Secret. Example 3-6 shows a YAML manifest for such a Secret.\n\nExample 3-6. Creating a Secret for a service account manually apiVersion: v1 kind: Secret metadata: name: sa-api-secret namespace: k97 annotations: kubernetes.io/service-account.name: sa-api type: kubernetes.io/service-account-token\n\nTo assign the service account to the Secret, add the annotation with the key kubernetes.io/service-account.name. The following command creates the Secret object:\n\n$ kubectl create -f secret.yaml secret/sa-api-secret created\n\nYou can find the token in the “Data” section when describing the Secret object:\n\n$ kubectl describe secret sa-api-secret -n k97 ... Data ==== ca.crt: 1111 bytes namespace: 3 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjBtQkJzVWlsQjl...\n\nUpdating Kubernetes Frequently\n\nInstalling a Kubernetes cluster with a specific version is not a one-time fire- and-forget operation. Even if you used the latest Long-Term Support (LTS) Release at the time of installation, it does not guarantee that your cluster is without security vulnerabilities.",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "As time goes by, security-related bugs and weaknesses will be discovered. This statement includes the underlying operating system and the dependencies the cluster nodes run on. An attacker can easily look up security vulnerabilities in the publicly disclosed Common Vulnerabilities and Exposures (CVE) database and exploit them.\n\nVersioning Scheme\n\nIt’s up to the cluster administrator to update the Kubernetes version across all nodes on a regular basis. Kubernetes follows the semantic versioning scheme. A Semantic Version consists of a major version, minor version, and patch version. For example, for the Kubernetes version 1.24.3, the major version is 1, the minor version is 24, and the patch version is 3.\n\nEach portion of the version carries a specific meaning. A change to the major version portion indicates a breaking change. Incrementing the minor version portion means that new functionality has been added in a backward- compatible manner. The patch version portion simply fixes a bug.\n\nBREAKING CHANGES IN KUBERNETES WITH MINOR VERSION UPDATES\n\nIt’s important to mention that Kubernetes doesn’t always stick to the strict interpretation of semantic versioning. For example, the PodSecurityPolicy (PSP) admission controller has been replaced by the Pod Security Admission concept in version 1.25.0. Conventionally, those changes should only happen with a major version update. Reference the Kubernetes deprecation policy for a better understanding on how an API, a flag, or a feature is phased out.\n\nRelease Cadence\n\nYou can expect a new minor version release of Kubernetes every three months. The release may include new features and additional bug fixes. Security fixes may be implemented as needed for the latest release of Kubernetes and will be backported to the two minor releases before that. Always staying on top of updating to the latest releases for your own",
      "content_length": 1884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "cluster(s) takes quite a bit of people-power. You will need to reserve time for those activities accordingly.\n\nPerforming the Upgrade Process\n\nIt is recommended to upgrade from a minor version to the next higher one (e.g., from 1.23 to 1.24), or from a patch version to a more recent one (e.g., from 1.24.1 to 1.24.3). Abstain from jumping up multiple minor versions to avoid unexpected side effects.\n\nYou can find a full description of the upgrade steps in the official Kubernetes documentation. Figure 3-4 illustrates the upgrade process on a high level.\n\nThe cluster version upgrade process is already part of the CKA exam. Given that you have to pass the CKA as a prerequisite, I would assume that you already know how to perform the process. For a detailed description, refer to Certified Kubernetes Administrator (CKA) Study Guide.",
      "content_length": 837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Figure 3-4. Process for a cluster version upgrade\n\nSummary Users, clients applications (such as kubectl or curl), Pods using service accounts, and cluster components all communicate with the API server to",
      "content_length": 204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "manage objects. It’s paramount to secure the API server to prevent access with malicious intent.\n\nTo minimize the attack surface area, avoid exposing the API server to the internet using firewall rules. For every user or service account, restrict the permissions to execute operations against the Kubernetes API to the bare minimum using RBAC rules. With minimized permissions, attackers can cause far less damage in case they can gain access to credentials.\n\nMake sure to upgrade the version of your Kubernetes cluster. Incorporating bug and security fixes will decrease the risk of exposing unnecessary vulnerabilities attackers can use to their advantage.\n\nExam Essentials\n\nPractice interacting with the Kubernetes API.\n\nThis chapter demonstrated the different ways to communicate with the Kubernetes API. We performed API requests by switching to a user context, and with the help of a RESTful API call using curl. You will need to understand how to determine the endpoint of the API server and how to use different authentication methods, e.g., client credentials and bearer tokens. Explore the Kubernetes API and its endpoints on your own for broader exposure.\n\nUnderstand the implications of defining RBAC rules for users and service accounts.\n\nAnonymous user requests to the Kubernetes API will not allow any substantial operations. For requests coming from a user or a service account, you will need to carefully analyze permissions granted to the subject. Learn the ins and outs of defining RBAC rules by creating the relevant objects to control permissions. Service accounts automount a token when used in a Pod. Only expose the token as a Volume if you are intending to make API calls from the Pod.",
      "content_length": 1710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Be aware of Kubernetes release cadence and the need for upgrading the cluster.\n\nA Kubernetes cluster needs to be cared for over time for security reasons. Attackers may try to take advantage of known vulnerabilities in outdated Kubernetes versions. The version upgrade process is part of every administrator’s job and shouldn’t be ignored.\n\nSample Exercises\n\nSolutions to these exercises are available in the Appendix.\n\n1. Create a client certificate and key for the user named jill in the group observer. With the admin context, create the context for the user jill.\n\n2. For the group (not the user!), define a Role and RoleBinding in the default namespace that allow the verbs get, list, and watch for the resources Pods, ConfigMaps, and Secrets. Create the objects.\n\n3. Switch to the user context and execute a kubectl command that allows one of the granted operations, and one kubectl command that should not be permitted. Switch back to the admin context.\n\n4. Create a Pod named service-list in the namespace t23. The container uses the image alpine/curl:3.14 and makes a curl call to the Kubernetes API that lists Service objects in the default namespace in an infinite loop. Create and attach the service account api-call. Inspect the container logs after the Pod has been started. What response do you expect to see from the curl command?\n\n5. Assign a ClusterRole and RoleBinding to the service account that only allows the operation needed by the Pod. Have a look at the response from the curl command.",
      "content_length": 1511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "6. Configure the Pod so that automounting of the service account token is disabled. Retrieve the token value and use it directly with the curl command. Make sure that the curl command can still authorize the operation.\n\n7. Navigate to the directory app-a/ch03/upgrade-version of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. Upgrade all nodes of the cluster from Kubernetes 1.25.6 to 1.26.1. The cluster consists of a single control plane node named kube- control-plane, and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\nOceanofPDF.com",
      "content_length": 759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Chapter 4. System Hardening\n\nThe domain “system hardening” deals with security aspects relevant to the underlying host system running the Kubernetes cluster nodes. Topics discussed here touch on techniques and configuration options that are fundamentally Linux core functionality. This includes disabling services and removing packages, managing users and groups, disabling ports, and setting up firewall rules. Finally, this chapter discusses Linux kernel hardening tools that can restrict what operations a process running in a container can perform on a host level.\n\nAt a high level, this chapter covers the following concepts:\n\nMinimizing the host OS footprint\n\nMinimizing IAM roles\n\nMinimizing external access to the network\n\nUsing kernel hardening tools like AppArmor and seccomp\n\nMinimizing the Host OS Footprint\n\nCluster nodes run on physical or virtual machines. In most cases, the operating system on those machines is a Linux distribution. Evidently, the operating system can expose security vulnerabilities.\n\nOver time, you need to keep the version of the operating system up to date with the latest security fixes. This process could entail upgrading a node’s operating system from Ubuntu 18 to 22, for example. Upgrading the operating system is out of scope for this book; for more information, check the relevant Linux documentation.\n\nMany Linux distributions, such as Ubuntu, come with additional tools, applications, and services that are not necessarily required for operating the",
      "content_length": 1498,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Kubernetes cluster. It is your job as an administrator to identify security risks, disable or remove any operating system-specific functionality that may expose vulnerabilities, and keep the operating system patched to incorporate the latest security fixes. The less functionality an operating system has, the smaller the risk.\n\nCIS BENCHMARK FOR UBUNTU LINUX\n\nAs a reference guide, you may want to compare your operating system’s configuration with the CIS benchmark for Ubuntu Linux.\n\nScenario: An Attacker Exploits a Package Vulnerability\n\nFigure 4-1 illustrates an attacker exploiting a vulnerability of a package installed on the system. For example, the application could be the package manager snapd. Assume that the attacker takes advantage of the known vulnerability USN-5292-1 that has the potential of exposing sensitive information to an attacker.\n\nFigure 4-1. An attacker exploits an OS-level vulnerability\n\nThe following section will explain how to minimize security risks for services and packages that are not really needed for operating Kubernetes by simply disabling or removing them.\n\nDisabling Services",
      "content_length": 1122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "On Linux, many applications run as services in the background. Services can be managed using the command line tool systemctl. The following systemctl command lists all running services:\n\n$ systemctl | grep running ... snapd.service loaded active running Snap Daemon\n\nOne of the services we will not need for operating a cluster node is the package manager snapd. For more details on the service, retrieve the status for it with the status subcommand:\n\n$ systemctl status snapd ● snapd.service - Snap Daemon Loaded: loaded (/lib/systemd/system/snapd.service; enabled; vendor \\ preset: enabled) Active: active (running) since Mon 2022-09-19 22:49:56 UTC; 30min ago TriggeredBy: ● snapd.socket Main PID: 704 (snapd) Tasks: 12 (limit: 2339) Memory: 45.9M CGroup: /system.slice/snapd.service └─704 /usr/lib/snapd/snapd\n\nYou can stop service using the systemctl subcommand stop:\n\n$ sudo systemctl stop snapd Warning: Stopping snapd.service, but it can still be activated by: snapd.socket\n\nExecute the disable subcommand to prevent the service from being started again upon a system restart:\n\n$ sudo systemctl disable snapd Removed /etc/systemd/system/multi-user.target.wants/snapd.service.\n\nThe service has now been stopped and disabled:\n\n$ systemctl status snapd ● snapd.service - Snap Daemon",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Loaded: loaded (/lib/systemd/system/snapd.service; disabled; vendor \\ preset: enabled) Active: inactive (dead) since Mon 2022-09-19 23:22:22 UTC; 4min 4s ago TriggeredBy: ● snapd.socket Main PID: 704 (code=exited, status=0/SUCCESS)\n\nRemoving Unwanted Packages\n\nNow that the service has been disabled, there’s no more point in keeping the package around. You can remove the package to free up additional disk space and memory. You can use the apt purge command to remove a package and its transitive packages, as demonstrated in the following:\n\n$ sudo apt purge --auto-remove snapd Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be REMOVED: snapd* squashfs-tools* 0 upgraded, 0 newly installed, 2 to remove and 116 not upgraded. After this operation, 147 MB disk space will be freed. Do you want to continue? [Y/n] y ...\n\nYou can use the same command even if the package isn’t controlled by a service. Identify the packages you don’t need and simply remove them. You should end up with a much slimmer footprint of your system.\n\nA potential attacker cannot use the snapd service anymore to exploit the system. You should repeat the process for any unwanted services. As a result, the snapd service ceases to exist on the system:\n\n$ systemctl status snapd Unit snapd.service could not be found.\n\nMinimizing IAM Roles\n\nIdentity and access management (IAM) on the system level involves management of Linux users, the groups they belong to, and the permissions",
      "content_length": 1524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "granted to them. Any directory and file will have file permissions assigned to a user.\n\nProper user and access management is a classic responsibility of every system administrator. While your role as a Kubernetes administrator may not directly involve system-level IAM, it’s important to understand the implications to security. You will likely have to work with a peer to harden the system running the Kubernetes cluster.\n\nThis section will provide a short introduction on how to manage users and groups. We will also discuss how to set file permissions and ownership to minimize access as much as possible. We will only scratch the surface of the topic in this book. For more information, refer to the Linux documentation of your choice.\n\nScenario: An Attacker Uses Credentials to Gain File Access\n\nA security breach can lead to stolen user credentials. Gaining access to valid user credentials opens the door for additional attack vectors. Figure 4- 2 shows an attacker who could log into a cluster node with stolen user credentials and can now interact with all files and directories with the permissions granted to the user.\n\nFigure 4-2. An attacker uses stolen credentials to access files",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "It’s recommended to follow the principle of least privilege. Only grant administrative permissions to a limited group of users. All other users should only be allowed to perform operations necessary to perform their jobs.\n\nUnderstanding User Management\n\nEvery user must authenticate to a system to use it. The authenticated user has access to resources based on the assigned permissions. This section will walk you through the primary operations required to manage users.\n\nListing users To list all users on the system, render the contents of the file /etc/passwd. Every entry follows the general pattern username:password:UID:GID:com ment: home:shell. Some of the fields within the pattern may be empty:\n\n$ cat /etc/passwd root:x:0:0:root:/root:/bin/bash nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin ...\n\nThe command output renders the user root in the first position of the output. The last portion of the string for the root user, /bin/bash, indicates that the user is allowed to log into the system with a bash shell. Other users might not be allowed to log in at all. For those users, you will find the string /usr/sbin/nologin assigned to the shell field.\n\nAt any given point of time, you can see which processes have been started by users. The following command shows all bash processes, including the corresponding user that started it:\n\n$ ps aux | grep bash root 956 0.0 0.4 22512 19200 pts/0 Ss 17:57 0:00 -bash root 7064 0.0 0.0 6608 2296 pts/0 S+ 18:08 0:00 grep \\ --color=auto bash",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Adding a user\n\nAt some point, you may want to give team members access to the machines running the cluster nodes, with limited permissions. You can add new users to the system with the adduser command. Add the flag --shell /sbin/nologin to disable shell access for the user. The following command creates the user ben:\n\n$ sudo adduser ben Adding user ‘ben’ ... Adding new group ‘ben’ (1001) ... Adding new user ‘ben’ (1001) with group ‘ben’ ... Creating home directory ‘/home/ben’ ... Copying files from ‘/etc/skel’ ... New password: Retype new password: ...\n\nThe user entry has been added to the file /etc/passwd:\n\n$ cat /etc/passwd ... ben:x:1001:1001:,,,:/home/ben:/bin/bash\n\nSwitching to a user You can change the user in a shell by using the su command. The following command switches to the user ben we created earlier. You will be asked to enter the user’s password:\n\n$ su ben Password: ben@controlplane:/root$ pwd /root\n\nThe shell will indicate the current user by its prompt. You will inherit the environment variables from the account you used when running the su command. To create a new environment, add the hyphen with the su command:",
      "content_length": 1147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "$ su - ben ben@controlplane:~$ pwd /home/ben\n\nAnother way to temporarily switch the user is by using the sudo command. You will need to have elevated privileges to execute the command. Therefore, the sudo command is equivalent to “run this command as administrator”:\n\n$ sudo -u ben pwd /root\n\nDeleting a user\n\nTeam members, represented by users in the system, transition to other teams or may simply leave the company. You will want to revoke access to the user to prevent unauthorized use of the credentials. The following command deletes the user, including the user’s home directory:\n\n$ sudo userdel -r ben\n\nUnderstanding Group Management\n\nIt’s more convenient for a system administrator to group users with similar access requirements to control permissions on an individual user level. Linux systems offer the concept of a group as a way to organize users based on teams, or specific organizational roles. We’ll briefly touch on the most important aspects of group management.\n\nListing groups Groups can be listed by inspecting the contents of the file /etc/group. Every entry follows the general pattern groupname:password:GID:group members:\n\n$ cat /etc/group root:x:0: plugdev:x:46:packer",
      "content_length": 1195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "nogroup:x:65534: ...\n\nAs you can see in the output, some of the fields may be empty. The only group with an assigned member is plugdev, whose name is packer.\n\nAdding a group Use the command groupadd to add a new group. The following example adds the group kube-developers:\n\n$ sudo groupadd kube-developers\n\nThe group will now be listed in the file /etc/group. Notice that the group identifier is 1004:\n\n$ cat /etc/group ... kube-developers:x:1004:\n\nAssigning a user to a group To assign a group to a user, use the usermod command. The following command adds the user ben to the group kube-developers:\n\n$ sudo usermod -g kube-developers ben\n\nThe group identifier 1004 acts as a stand-in for the group kube- developers:\n\n$ cat /etc/passwd | grep ben ben:x:1001:1004:,,,:/home/ben:/bin/bash\n\nDeleting a group\n\nSometimes you want to get rid of a group entirely. Maybe the organizational role referring to a Linux group that does not exist anymore.",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Use the groupdel command to delete a group. You will receive an error message if the members are still part of the group:\n\n$ sudo groupdel kube-developers groupdel: cannot remove the primary group of user ben\n\nBefore deleting a group, you should reassign group members to a different group using the usermod command. The following command changes the group from kube-developers to kube-admins. Assume that the group kube-admins has been created before:\n\n$ sudo usermod -g kube-admins ben $ sudo groupdel kube-developers\n\nUnderstanding File Permissions and Ownership\n\nAssigning the file permissions with as minimal access as possible is crucial to maximizing security. This is where Linux file permissions and ownership come into play. I am only going to discuss the relevant operations on a high level. Refer to the Linux Foundation’s blog post about Linux file permissions for more details.\n\nViewing file permissions and ownership\n\nEvery user can create new directories and files. For example, you could use the touch command to create an empty file. The following command creates a file with the name my-file in the current directory:\n\n$ touch my-file\n\nTo see the contents of a directory in the “long” format, use the ls command. The long format of the output requested by the -l command line parameter renders the file permissions and the file ownership:\n\n$ ls -l total 0 -rw-r--r-- 1 root root 0 Sep 26 17:53 my-file",
      "content_length": 1420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "The important portion of the output is -rw-r--r--. The first character is a special permission character that can vary per system, followed by three groupings with the notation rwx. The first three characters stand for the owner permissions, the second set of three characters is for the group permissions, and the last three characters represent the permissions for all users. The symbol r means read permissions, w stands for write permissions, and x refers to execution permissions. In the previous example, the user root can read and write the file, whereas the group and all other users can only read the file.\n\nChanging file ownership Use the chown command to change the user and group assignment for a file or directory. The syntax of the command follows the pattern chown owner:group filename. The following command changes the ownership of the file to the user ben but does not reassign a group. The user executing the chown command needs to have write permissions:\n\n$ chown ben my-file $ ls -l total 0 -rw-r--r-- 1 ben root 0 Sep 26 17:53 my-file\n\nChanging file permissions You can add or remove permissions with the chmod command in a variety of notations. For example, use the following command to remove write permissions for the file owner:\n\n$ chmod -w file1 $ ls -l total 0 -r--r--r-- 1 ben root 0 Sep 26 17:53 my-file\n\nMinimizing External Access to the Network",
      "content_length": 1376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "External access to your cluster nodes should only be allowed for the ports necessary to operate Kubernetes. We already discussed the standard Kubernetes ports in “Protecting Node Metadata and Endpoints”. Access to all other ports should be blocked.\n\nIdentifying and Disabling Open Ports\n\nApplications like FTP servers, web servers, and file and print services such as Samba open ports as a means to expose a communication endpoint to clients. Running applications that open network communication can expose a security risk. You can eliminate the risk by simply disabling the service and deinstalling the application.\n\nLet’s say we installed the Apache 2 HTTP web server on a control plane node with the following commands:\n\n$ sudo apt update $ sudo apt install apache2\n\nUPDATE ABOUT NETSTAT COMMAND\n\nThe netstat command has been deprecated in favor of the faster, more human-readable ss command. For more information, refer to the documentation of the operating system you are using.\n\nWe can inspect all open ports using the command line tool ss, a utility with similar functionality to netstat. The following command renders all of the open ports, including their processes. Among them is port 80, exposed by Apache 2:\n\n$ sudo ss -ltpn State Recv-Q Send-Q Local Address:Port Peer Address:Port Process ... LISTEN 0 511 *:80 *:* users: \\ ((\"apache2\",pid=18435,fd=4),(\"apache2\",pid=18434,fd=4),(\"apache2\", ]\\ pid=18432,fd=4))",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "You may have only needed the web server temporarily and may have simply forgotten about installing it. The process is currently managed by a server. You can review the status of a service with the systemctl status command:\n\n$ sudo systemctl status apache2 ● apache2.service - The Apache HTTP Server Loaded: loaded (/lib/systemd/system/apache2.service; enabled; vendor \\ preset: enabled) Active: active (running) since Tue 2022-09-20 22:25:25 UTC; 39s ago Docs: https://httpd.apache.org/docs/2.4/ Main PID: 18432 (apache2) Tasks: 55 (limit: 2339) Memory: 5.6M CGroup: /system.slice/apache2.service ├─18432 /usr/sbin/apache2 -k start ├─18434 /usr/sbin/apache2 -k start └─18435 /usr/sbin/apache2 -k start\n\nApache 2 is not needed by Kubernetes. We decide to shut down the service and deinstall the package:\n\n$ sudo systemctl stop apache2 $ sudo systemctl disable apache2 Synchronizing state of apache2.service with SysV service script with \\ /lib/systemd/systemd-sysv-install. Executing: /lib/systemd/systemd-sysv-install disable apache2 Removed /etc/systemd/system/multi-user.target.wants/apache2.service. $ sudo apt purge --auto-remove apache2\n\nVerify that the port isn’t used anymore. The ss command doesn’t find an application exposing port 80 anymore:\n\n$ sudo ss -ltpn | grep :80\n\nSetting Up Firewall Rules\n\nAnother way to control ports is with the help of an operating-system-level firewall. On Linux, you could use the Uncomplicated Firewall (UFW). This",
      "content_length": 1456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "section will give you a very brief introduction on how to enable UFW and how to configure firewall rules.\n\nFollowing the principle of least privilege, it’s a good idea to start by enabling the firewall and setting up deny rules for any incoming and outgoing network traffic. The following commands demonstrate the steps to achieve that:\n\n$ sudo ufw allow ssh Rules updated Rules updated (v6) $ sudo ufw default deny outgoing Default outgoing policy changed to deny (be sure to update your rules accordingly) $ sudo ufw default deny incoming Default incoming policy changed to deny (be sure to update your rules accordingly) $ sudo ufw enable Command may disrupt existing ssh connections. Proceed with operation (y|n)? y Firewall is active and enabled on system startup\n\nYou will want to allow external tools like kubectl to connect to the API server running on port 6443. On the control plane node, execute the following command to allow access to the API server port:\n\n$ sudo ufw allow 6443 Rule added Rule added (v6)\n\nYou will have to repeat the same process to open up other ports on control plane and worker nodes. Ensure that all other ports not needed to operate Kubernetes are blocked.\n\nUsing Kernel Hardening Tools\n\nApplications or processes running inside of a container can make system calls. A typical example could be the curl command performing an HTTP request. A system call is a programmatic abstraction running in the user",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "space for requesting a service from the kernel. We can restrict which system calls are allowed to be made with the help of kernel hardening tools. The CKS exam explicitly mentions two tools in this space, AppArmor and seccomp. We’ll discuss both tools and the mechanics to integrate them with Kubernetes.\n\nUsing AppArmor\n\nAppArmor provides access control to programs running on a Linux system. The tool implements an additional security layer between the applications invoked in the user space and the underlying system functionality. For example, we can restrict network calls or filesystem interaction. Many Linux distributions (e.g., Debian, Ubuntu, openSUSE) already ship with AppArmor. Therefore, AppArmor doesn’t have to be installed manually. Linux distributions that do not support AppArmor use Security-Enhanced Linux (SELinux) instead, which takes a similar approach to AppArmor. Understanding SELinux is out of scope for the CKS exam.\n\nUnderstanding profiles\n\nThe rules that define what a program can or cannot do are defined in an AppArmor profile. Every profile needs to be loaded into AppArmor before it can take effect. AppArmor provides a command line tool for checking the profiles that have been loaded. Execute the command aa-status to see a summary of all loaded profiles. You will see that AppArmor already comes with a set of default application profiles to protect Linux services:\n\n$ sudo aa-status apparmor module is loaded. 31 profiles are loaded. 31 profiles are in enforce mode. /snap/snapd/15177/usr/lib/snapd/snap-confine ... 0 profiles are in complain mode. 14 processes have profiles defined. 14 processes are in enforce mode. /pause (11934) docker-default ...",
      "content_length": 1691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "0 processes are in complain mode. 0 processes are unconfined but have a profile defined.\n\nThe profile mode determines the treatment of rules at runtime should a matching event happen. AppArmor distinguishes two types of profile modes:\n\nEnforce\n\nThe system enforces the rules, reports the violation, and writes it to the syslog. You will want to use this mode to prevent a program from making specific calls.\n\nComplain\n\nThe system does not enforce the rules but will write violations to the log. This mode is helpful if you want to discover the calls a program makes.\n\nExample 4-1 defines a custom profile in the file k8s-deny-write for restricting file write access. The file should be placed in the directory /etc/apparmor.d of every worker node that executes workload. It is out of scope of this book to explain all the rules in detail. For more information, have a look at the AppArmor wiki.\n\nExample 4-1. An AppArmor profile for restricting file write access #include <tunables/global>\n\nprofile k8s-deny-write flags=(attach_disconnected) { #include <abstractions/base>\n\nfile,\n\ndeny /** w, }\n\nThe identifier after the profile keyword is the name of the profile.\n\nApply to file operations.",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Deny all file writes.\n\nSetting a custom profile\n\nTo load the profile into AppArmor, run the following command on the worker node:\n\n$ sudo apparmor_parser /etc/apparmor.d/k8s-deny-write\n\nThe command uses the enforce mode by default. To load the profile in complain mode, use the -C option. The aa-status command will now list the profile in addition to the default profiles. As you can see in the output, the profile is listed in enforce mode:\n\n$ sudo aa-status apparmor module is loaded. 32 profiles are loaded. 32 profiles are in enforce mode. k8s-deny-write ...\n\nAppArmor supports additional convenience commands as part of a utilities package. You can manually install the package using the following commands if you want to use them:\n\n$ sudo apt-get update $ sudo apt-get install apparmor-utils\n\nOnce installed, you can use the command aa-enforce to load a profile in enforce mode, and aa-complain to load a profile in complain mode. For the exam, it’s likely easier to just go with the standard apparmor_parser command.\n\nApplying a profile to a container\n\nYou need to ensure a couple of prerequisites before using AppArmor rules in a Pod definition. First, the container runtime needs to support AppArmor",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "to let rules take effect. In addition, AppArmor needs to be installed on the worker node that runs the Pod. Last, make sure you loaded the profile, as described in the previous section.\n\nExample 4-2 shows a YAML manifest for a Pod defined in the file pod.yaml. To apply a profile to the container, you will need to set a specific annotation. The annotation key needs to use the key in the format container.apparmor.security.beta. kuber netes.io/<container- name>. In our case, the container name is hello. The full key is container.apparmor.security.beta.kubernetes.io/hello. The value of the annotation follows the pattern localhost/<profile-name>. The custom profile we want to use here is k8s-deny-write. For more information on the configuration options, see the Kubernetes documentation.\n\nExample 4-2. A Pod applying an AppArmor profile to a container apiVersion: v1 kind: Pod metadata: name: hello-apparmor annotations: container.apparmor.security.beta.kubernetes.io/hello: \\ localhost/k8s-deny-write spec: containers: - name: hello image: busybox:1.28 command: [\"sh\", \"-c\", \"echo 'Hello AppArmor!' && sleep 1h\"]\n\nThe annotation key that consists of a hard-coded prefix and the container name separated by a slash character.\n\nThe profile name available on the current node indicated by localhost.\n\nThe container name.\n\nWe are ready to create the Pod. Run the apply command and point it to the YAML manifest. Wait until the Pod transitions into the “Running” status:",
      "content_length": 1471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "$ kubectl apply -f pod.yaml pod/hello-apparmor created $ kubectl get pod hello-apparmor NAME READY STATUS RESTARTS AGE hello-apparmor 1/1 Running 0 4s\n\nYou can now shell into the container and perform a file write operation:\n\n$ kubectl exec -it hello-apparmor -- /bin/sh / # touch test.txt touch: test.txt: Permission denied\n\nAppArmor will prevent writing a file to the container’s filesystem. The message “Permission denied” will be rendered if you try to perform the operation.\n\nUsing seccomp\n\nSeccomp, short for “Secure Computing Mode,” is another Linux kernel feature that can restrict the calls made from the userspace into the kernel. A seccomp profile is the mechanism for defining the rules for restricting syscalls and their arguments. Using seccomp can reduce the risk of exploiting a Linux kernel vulnerability. For more information on seccomp on Kubernetes, see the documentation.\n\nApplying the default container runtime profile to a container\n\nContainer runtimes, such as Docker Engine or containerd, ship with a default seccomp profile. The default seccomp profile allows the most commonly used syscalls used by applications while at the same time forbidding the use of syscalls considered dangerous.\n\nKubernetes does not apply the default container runtime profile to containers when creating a Pod, but you can enable it using the SeccompDefault feature gate. Alternatively, you can opt into the feature on a Pod-by-Pod basis by setting the seccomp profile type to RuntimeDefault with the help of the security context attribute seccompProfile. Example 4- 3 demonstrates its use.",
      "content_length": 1594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Example 4-3. A Pod applying the default seccomp profile provided by the container runtime profile apiVersion: v1 kind: Pod metadata: name: hello-seccomp spec: securityContext: seccompProfile: type: RuntimeDefault containers: - name: hello image: busybox:1.28 command: [\"sh\", \"-c\", \"echo 'Hello seccomp!' && sleep 1h\"]\n\nApplies the default container runtime profile.\n\nYou can start the Pod using the apply command and point to the YAML manifest. The Pod should transition into the “Running” status:\n\n$ kubectl apply -f pod.yaml pod/hello-seccomp created $ kubectl get pod hello-seccomp NAME READY STATUS RESTARTS AGE hello-seccomp 1/1 Running 0 4s\n\nThe echo command executed in the container is considered unproblematic from a security perspective by the default seccomp profile. The following command inspects the logs of the container:\n\n$ kubectl logs hello-seccomp Hello seccomp!\n\nThe call was permitted and resulted in writing the message “Hello seccomp!” to standard output.\n\nSetting a custom profile\n\nYou can create and set your own custom profile in addition to the default container runtime profile. The standard directory for those files is",
      "content_length": 1148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "/var/lib/kubelet/seccomp. We’ll organize our custom profiles in the subdirectory profiles. Create the directory if it doesn’t exist yet:\n\n$ sudo mkdir -p /var/lib/kubelet/seccomp/profiles\n\nWe decide to create our custom profile in the file mkdir-violation.json in the profile directory. Example 4-4 shows the details of the profile definition. In a nutshell, the rule set disallows the use of the mkdir syscall.\n\nExample 4-4. A seccomp profile that prevents executing a mkdir syscall { \"defaultAction\": \"SCMP_ACT_ALLOW\", \"architectures\": [ \"SCMP_ARCH_X86_64\", \"SCMP_ARCH_X86\", \"SCMP_ARCH_X32\" ], \"syscalls\": [ { \"names\": [ \"mkdir\" ], \"action\": \"SCMP_ACT_ERRNO\" } ] }\n\nThe default action applies to all system calls. Here we’ll allow all syscalls using SCMP_ACT_ALLOW.\n\nYou can filter for specific architectures the default action should apply to. The definition of the field is optional.\n\nThe default action can be overwritten by declaring more fine-grained rules. The SCMP_ACT_ERRNO action will prevent the execution of the mkdir syscall.\n\nPlacing a custom profile into the directory /var/lib/kubelet/seccomp doesn’t automatically apply the rules to a Pod. You still need to configure a",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Pod to use it.\n\nApplying the custom profile to a container\n\nApplying a custom profile follows a similar pattern to applying the default container runtime profile, with minor differences. As you can see in Example 4-5, we point the seccompProfile attribute of the security profile to the file mkdir-violation.json and set the type to Localhost.\n\nExample 4-5. A Pod applying a custom seccomp profile prevents a mkdir syscall apiVersion: v1 kind: Pod metadata: name: hello-seccomp spec: securityContext: seccompProfile: type: Localhost localhostProfile: profiles/mkdir-violation.json containers: - name: hello image: busybox:1.28 command: [\"sh\", \"-c\", \"echo 'Hello seccomp!' && sleep 1h\"] securityContext: allowPrivilegeEscalation: false\n\nRefers to a profile on the current node.\n\nApplies the profile with the name mkdir-violation.json in the subdirectory profiles.\n\nCreate the Pod using the declarative apply command. Wait until the Pod transitions into the “Running” status:\n\n$ kubectl apply -f pod.yaml pod/hello-seccomp created $ kubectl get pod hello-seccomp NAME READY STATUS RESTARTS AGE hello-seccomp 1/1 Running 0 4s",
      "content_length": 1122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Shell into the container to verify that seccomp properly enforced the applied rules:\n\n$ kubectl exec -it hello-seccomp -- /bin/sh / # mkdir test mkdir: can't create directory test: Operation not permitted\n\nAs you can see in output, the operation renders an error message when trying to execute the mkdir command. The rule in the custom profile has been violated.\n\nSummary\n\nAddressing security aspects isn’t limited to Kubernetes cluster components or workload. There’s plenty you can do on the host system level. We discussed different OS capabilities and how to use them to minimize potential security vulnerabilities.\n\nMany operating systems come with a wealth of packages and services to offer a more feature-rich experience to end users. It’s important to identify functionality not required to operate a Kubernetes cluster. Purge unnecessary packages and services rigorously and close ports you don’t need. You will also want to limited which users are allowed to have access to specific directories, files, and applications. Use Linux’s user management to restrict permissions.\n\nIt’s very common for applications and processes running in containers to make system calls. You can use Linux kernel hardening tools like AppArmor and seccomp to restrict those calls. Only allow system calls crucial to fulfill the needs of your application running the container.\n\nExam Essentials\n\nHave a basic understanding of Linux OS tooling.",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "The CKS exam primarily focuses on security functionality in Kubernetes. This domain crosses the boundary to Linux OS security features. It won’t hurt to explore Linux-specific tools and security aspects independent from the content covered in this chapter. On a high level, familiarize yourself with service, package, user, and network management on Linux.\n\nKnow how to integrate Linux kernel hardening tools with Kubernetes.\n\nAppArmor and seccomp are just some kernel hardening tools that can be integrated with Kubernetes to restrict system calls made from a container. Practice the process of loading a profile and applying it to a container. In order to expand your horizons, you may also want to explore other kernel functionality that works alongside Kubernetes, such as SELinux or sysctl.\n\nSample Exercises\n\nSolutions to these exercises are available in the Appendix.\n\n1. Navigate to the directory app-a/ch04/close-ports of the checked- out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f.\n\nIdentify the process listening on port 21 in the VM kube-worker- 1. You decided not to expose this port to reduce the risk of attackers exploiting the port. Close the port by shutting down the corresponding process.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.",
      "content_length": 1545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "2. Navigate to the directory app-a/ch04/apparmor of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane, and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f. Create an AppArmor profile named network-deny. The profile should prevent any incoming and outgoing network traffic. Add the profile to the set of AppArmor rules in enforce mode. Apply the profile to the Pod named network-call running in the default namespace. Check the logs of the Pod to ensure that network calls cannot be made anymore.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\n3. Navigate to the directory app-a/ch04/seccomp of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane, and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f. Create a seccomp profile file named audit.json that logs all syscalls in the standard seccomp directory. Apply the profile to the Pod named network-call running in the default namespace. Check the log file /var/log/syslog for log entries.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\n4. Create a new Pod named sysctl-pod with the image\n\nnginx:1.23.1. Set the sysctl parameters net.core.somaxconn to 1024 and debug.iotrace to 1. Check on the status of the Pod.",
      "content_length": 1655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Chapter 5. Minimizing Microservice Vulnerabilities\n\nApplication stacks operated in a Kubernetes cluster often follow a microservices architecture. The domain “minimize microservice vulnerabilities” covers governance and enforcement of security settings on the Pod level. We’ll touch on Kubernetes core features, as well as external tooling, that help with minimizing security vulnerabilities. Additionally, we’ll also talk about encrypted network communication between Pods running microservices.\n\nAt a high level, this chapter covers the following concepts:\n\nSetting up appropriate OS-level security domains with security contexts, Pod Security Admission (PSA), and Open Policy Agent Gatekeeper\n\nManaging Secrets\n\nUsing container runtime sandboxes, such as gVisor and Kata Containers\n\nImplementing Pod-to-Pod communication encryption via mutual Transport Layer Security (TLS)\n\nSetting Appropriate OS-Level Security Domains\n\nBoth core Kubernetes and the Kubernetes ecosystem offer solutions for defining, enforcing, and governing security settings on the Pod and container level. This section will discuss security contexts, Pod Security Admission, and Open Policy Agent Gatekeeper. You will learn how to",
      "content_length": 1204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "apply each of the features and tools using examples that demonstrate their importance to security. Let’s begin by setting up a scenario.\n\nScenario: An Attacker Misuses root User Container Access\n\nBy default, containers run with root privileges. A vulnerability in the application could grant an attacker root access to the container. The container’s root user is the same as the root user on the host machine. Not only can the attacker then inspect or modify the application, but they can also potentially install additional tooling that allows the attacker to break out of the container and step into host namespace with root permissions. The attacker could also copy sensitive data from the host’s filesystem to the container. Figure 5-1 illustrates the scenario.\n\nFigure 5-1. An attacker misuses root user container access\n\nFor that reason, running a container with the default root user is a bad idea. The next sections will explain how to declare a security context for the container that enforces the use of a non-root user or a specific user and/or group identifier. We’ll also discuss other security context settings relevant to shielding host access from the container.",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Understanding Security Contexts\n\nKubernetes, as the container orchestration engine, can apply additional configuration to increase container security. You do so by defining a security context. A security context defines privilege and access control settings for a Pod or a container. The following list provides some examples:\n\nThe user ID that should be used to run the Pod and/or container\n\nThe group ID that should be used for filesystem access\n\nGranting a running process inside the container some privileges of the root user but not all of them\n\nThe security context is not a Kubernetes primitive. It is modeled as a set of attributes under the directive securityContext within the Pod specification. Security settings defined on the Pod level apply to all containers running in the Pod; however, container-level settings take precedence. For more information on Pod-level security attributes, see the PodSecurityContext API. Container-level security attributes can be found in the SecurityContext API.\n\nEnforcing the Usage of a Non-Root User\n\nWe’ll have a look at a use case to make the functionality more transparent. Some images, like the one for the open source reverse-proxy server nginx, must be run with the root user. Say you wanted to enforce that containers cannot be run as the root user as a means to support a more sensible security strategy. The YAML manifest file container-non-root-user- error.yaml shown in Example 5-1 defines the security configuration specifically for a container. This security context only applies to this very container, but not others if you were to define more.\n\nExample 5-1. Enforcing a non-root user on an image that needs to run with the root user",
      "content_length": 1696,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "apiVersion: v1 kind: Pod metadata: name: non-root-error spec: containers: - image: nginx:1.23.1 name: nginx securityContext: runAsNonRoot: true\n\nThe container fails during the startup process with the status CreateContainer Confi gError. A look at the Pod’s event log reveals that the image tries to run with the root user. The configured security context does not allow it:\n\n$ kubectl apply -f container-non-root-user-error.yaml pod/non-root-error created $ kubectl get pod non-root-error NAME READY STATUS RESTARTS AGE non-root-error 0/1 CreateContainerConfigError 0 9s $ kubectl describe pod non-root-error ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 24s default-scheduler Successfully \\ assigned default/non-root to minikube Normal Pulling 24s kubelet Pulling image \\ \"nginx:1.23.1\" Normal Pulled 16s kubelet Successfully \\ pulled image \"nginx:1.23.1\" in 7.775950615s Warning Failed 4s (x3 over 16s) kubelet Error: container \\ has runAsNonRoot and image will run as root (pod: \"non-root-error_default \\ (6ed9ed71-1002-4dc2-8cb1-3423f86bd144)\", container: secured-container) Normal Pulled 4s (x2 over 16s) kubelet Container image \\ \"nginx:1.23.1\" already present on machine\n\nThere are alternative nginx container images available that are not required to run with the root user. An example is bitnami/nginx. Example 5-2 shows the contents of the file container-non-root-user-success.yaml. The major change in this file is the value assigned to the spec.containers[].image attribute.",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "Example 5-2. Enforcing a non-root user on an image that supports running with a user ID apiVersion: v1 kind: Pod metadata: name: non-root-success spec: containers: - image: bitnami/nginx:1.23.1 name: nginx securityContext: runAsNonRoot: true\n\nStarting the container with the runAsNonRoot directive will work just fine. The container transitions into the “Running” status:\n\n$ kubectl apply -f container-non-root-user-success.yaml pod/non-root-success created $ kubectl get pod non-root-success NAME READY STATUS RESTARTS AGE non-root-success 1/1 Running 0 7s\n\nLet’s quickly check which user ID the container runs with. Shell into the container and run the id command. The output renders the user ID, the group ID, and the IDs of supplemental groups. The image bitnami/nginx sets the user ID to 1001 with the help of an instruction when the container image is built:\n\n$ kubectl exec non-root-success -it -- /bin/sh $ id uid=1001 gid=0(root) groups=0(root) $ exit\n\nSetting a Specific User and Group ID\n\nMany container images do not set an explicit user ID or group ID. Instead of running with the root default user, you can set the desired user ID and group ID to minimize potential security risks. The YAML manifest stored in the file container-user-id.yaml shown in Example 5-3 sets the user ID to 1000 and the group ID to 3000.",
      "content_length": 1327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Example 5-3. Running the container with a specific user and group ID apiVersion: v1 kind: Pod metadata: name: user-id spec: containers: - image: busybox:1.35.0 name: busybox command: [\"sh\", \"-c\", \"sleep 1h\"] securityContext: runAsUser: 1000 runAsGroup: 3000\n\nCreating the Pod will work without issues. The container transitions into the “Running” status:\n\n$ kubectl apply -f container-user-id.yaml pod/user-id created $ kubectl get pods user-id NAME READY STATUS RESTARTS AGE user-id 1/1 Running 0 6s\n\nYou can inspect the user ID and group ID after shelling into the container. The current user is not allowed to create files in the / directory. Creating a file in the /tmp directory will work, as most users have the permissions to write to it:\n\n$ kubectl exec user-id -it -- /bin/sh / $ id uid=1000 gid=3000 groups=3000 / $ touch test.txt touch: test.txt: Permission denied / $ touch /tmp/test.txt / $ exit\n\nAvoiding Privileged Containers\n\nKubernetes establishes a clear separation between the container namespace and the host namespace for processes, network, mounts, user ID, and more. You can configure the container’s security context to gain privileges to",
      "content_length": 1162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "certain aspects of the host namespace. Assume the following implications when using a privileged container:\n\nProcesses within a container almost have the same privileges as processes on the host.\n\nThe container has access to all devices on the host.\n\nThe root user in the container has similar privileges to the root user on the host.\n\nAll directories on the host’s filesystem can be mounted in the container.\n\nKernel settings can be changed, e.g., by using the sysctl command.\n\nUSING CONTAINERS IN PRIVILEGED MODE\n\nConfiguring a container to use privileged mode should be a rare occasion. Most applications and processes running in a container do not need elevated privileges beyond the container namespace. Should you encounter a Pod that has been configured to use privileged mode, contact the team or developer in charge to clarify, as it will open a loophole for attackers to gain access to the host system.\n\nLet’s compare the behavior of a non-privileged container with one configured to run in privileged mode. First, we are going to set up a regular Pod, as shown in Example 5-4. No security context has been set on the Pod or container level.\n\nExample 5-4. A Pod with a container in non-privileged mode apiVersion: v1 kind: Pod metadata: name: non-privileged spec: containers: - image: busybox:1.35.0",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "name: busybox command: [\"sh\", \"-c\", \"sleep 1h\"]\n\nCreate the Pod and ensure that it comes up properly:\n\n$ kubectl apply -f non-privileged.yaml pod/non-privileged created $ kubectl get pods NAME READY STATUS RESTARTS AGE non-privileged 1/1 Running 0 6s\n\nTo demonstrate the isolation between the container namespace and host’s namespace, we’ll try to use the sysctl to change the hostname. As you can see in the output of the command, the container will clearly enforce the restricted privileges:\n\n$ kubectl exec non-privileged -it -- /bin/sh / # sysctl kernel.hostname=test sysctl: error setting key 'kernel.hostname': Read-only file system / # exit\n\nTo make a container privileged, simply assign the value true to the security context attribute privileged. The YAML manifest in Example 5-5 shows an example.\n\nExample 5-5. A Pod with a container configured to run in privileged mode apiVersion: v1 kind: Pod metadata: name: privileged spec: containers: - image: busybox:1.35.0 name: busybox command: [\"sh\", \"-c\", \"sleep 1h\"] securityContext: privileged: true\n\nCreate the Pod as usual. The Pod should transition into the “Running” status:",
      "content_length": 1135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "$ kubectl apply -f privileged.yaml pod/privileged created $ kubectl get pod privileged NAME READY STATUS RESTARTS AGE privileged 1/1 Running 0 6s\n\nYou can now see that the same sysctl will allow you to change the hostname:\n\n$ kubectl exec privileged -it -- /bin/sh / # sysctl kernel.hostname=test kernel.hostname = test / # exit\n\nA container security context configuration related to privileged mode is the attribute allowPrivilegeEscalation. This attribute will allow a process running the container to gain more privileges than the parent process. The default value for the attribute is false, but if you see the attribute set to true, critically question its use. In most cases, you do not need the functionality.\n\nScenario: A Developer Doesn’t Follow Pod Security Best Practices\n\nIt is unfair to assume that developers of all trades and levels of seniority have extensive knowledge of Kubernetes features, especially the ones that apply to security best practices. In the previous section, we learned about the security context and which settings to avoid. Developers are probably unaware of those best practices without continued education and therefore may create Pods that use problematic security settings or none at all. Figure 5-2 shows a developer creating a Pod in privileged mode enabled by a copied manifest found on the internet. An attacker will gladly use this setup to their advantage.",
      "content_length": 1403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Figure 5-2. A developer creates a Pod with enabled privileged mode\n\nThis is where you, as the Kubernetes security specialist, come in. The Kubernetes ecosystem provides core features and external tooling for enforcing security standards for Pods so that objects without the right configuration will be rejected, or at least audited. The next section explores the Kubernetes core feature named Pod Security Admission.\n\nUnderstanding Pod Security Admission (PSA)\n\nOlder versions of Kubernetes shipped with a feature called Pod Security Policies (PSP). Pod Security Policies are a concept that help with enforcing security standards for Pod objects. Kubernetes 1.21 deprecated Pod Security Policies and introduced the replacement functionality Pod Security Admission. PSA determines which Pod Security Standard (PSS) to follow. A PSS defines a range of security policies from highly restrictive to highly permissive.\n\nHowever, the Kubernetes release 1.25 completely removed Pod Security Policies. You may still see the feature listed in older versions of the CKS curriculum. We will only focus on Pod Security Admission in this book. PSA is enabled by default with Kubernetes 1.23; however, you will need to declare which Pods should adhere to the security standards. All you need to do to opt into the PSA feature is to add a label with a specific format to a namespace. All Pods in that namespace will have to follow the standards declared.",
      "content_length": 1439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "The label consists of three parts: a prefix, a mode, and a level. The prefix always uses the hard-coded value pod-security.kubernetes.io followed by a slash. The mode determines the handling of a violation. Finally, the level dictates the degree of security standards to adhere to. An example of such a label could look as follows:\n\nmetadata: labels: pod-security.kubernetes.io/enforce: restricted\n\nThe mode allows for setting three different options, shown in Table 5-1.\n\nTable 5-1. Pod Security Admission modes\n\nMode\n\nBehavior\n\nenforce\n\nViolations will cause the Pod to be rejected.\n\naudit\n\nPod creation will be allowed. Violations will be appended to the audit log.\n\nwarn\n\nPod creation will be allowed. Violations will be rendered on the console.\n\nTable 5-2 illustrates the security policies determined by the level set for the PSA.",
      "content_length": 835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Table 5-2. Pod Security Admission levels\n\nLevel\n\nBehavior\n\nprivileged\n\nFully unrestricted policy.\n\nbaseline\n\nMinimally restrictive policy that covers crucial standards.\n\nrestricted\n\nHeavily restricted policy following best practices for hardening Pods from a security perspective.\n\nSee the Kubernetes documentation for details on the PSA, including usage examples.\n\nEnforcing Pod Security Standards for a Namespace\n\nLet’s apply a PSA to a Pod in the namespace psa. Example 5-6 shows the definition of the namespace and the declaration of the relevant label. The label will enforce a PSS on the highest level of security standards.\n\nExample 5-6. A namespace enforcing the highest level of security standards apiVersion: v1 kind: Namespace metadata: name: psa labels: pod-security.kubernetes.io/enforce: restricted\n\nMake sure that the Pod is created in the namespace psa. Example 5-7 shows the YAML manifest for a simple Pod running the busybox image.\n\nExample 5-7. A Pod violating the PSA restrictions apiVersion: v1 kind: Pod metadata: name: busybox",
      "content_length": 1049,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "namespace: psa spec: containers: - image: busybox:1.35.0 name: busybox command: [\"sh\", \"-c\", \"sleep 1h\"]\n\nViolations will be rendered in the console upon running a command to create a Pod in the namespace. As you can see in the following, the Pod wasn’t allowed to be created:\n\n$ kubectl create -f psa-namespace.yaml namespace/psa created $ kubectl apply -f psa-violating-pod.yaml Error from server (Forbidden): error when creating \"psa-pod.yaml\": pods \\ \"busybox\" is forbidden: violates PodSecurity \"restricted:latest\": \\ allowPrivilegeEscalation != false (container \"busybox\" must set \\ securityContext.allowPrivilegeEscalation=false), unrestricted \\ capabilities (container \"busybox\" must set securityContext. \\ capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \\ \"busybox\" must set securityContext.runAsNonRoot=true), seccompProfile \\ (pod or container \"busybox\" must set securityContext.seccompProfile. \\ type to \"RuntimeDefault\" or \"Localhost\") $ kubectl get pod -n psa No resources found in psa namespace.\n\nYou need to configure the Pod’s security context settings to follow the very restrictive standards. Example 5-8 shows an exemplary Pod definition that does not violate the Pod Security Standard.\n\nExample 5-8. A Pod following the PSS apiVersion: v1 kind: Pod metadata: name: busybox namespace: psa spec: containers: - image: busybox:1.35.0 name: busybox command: [\"sh\", \"-c\", \"sleep 1h\"] securityContext: allowPrivilegeEscalation: false capabilities:",
      "content_length": 1480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "drop: [\"ALL\"] runAsNonRoot: true runAsUser: 2000 runAsGroup: 3000 seccompProfile: type: RuntimeDefault\n\nCreating the Pod object now works as expected:\n\n$ kubectl apply -f psa-non-violating-pod.yaml pod/busybox created $ kubectl get pod busybox -n psa NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 10s\n\nPSA is a built-in, enabled-by-default feature in Kubernetes version 1.23 or higher. It’s easy to adopt, allows for picking and choosing a suitable policy standard, and can be configured to enforce or just log violations.\n\nUnfortunately, PSA only applies to Pods with a predescribed set of policies. You cannot write your own custom rules, change the messaging, or mutate the Pod object should it not adhere to a PSS. In the next section, we are going to have a look at tooling that goes beyond the functionality of PSA.\n\nUnderstanding Open Policy Agent (OPA) and Gatekeeper\n\nOpen Policy Agent (OPA) is an open source, general-purpose policy engine for enforcing rules. OPA is not specific to Kubernetes and can be used across other technology stacks. One of its benefits is the ability to define a policy in a very flexible fashion. You can write your own rules with the help of the query language named Rego. The validation logic written in Rego determines if the object is accepted or denied.\n\nGatekeeper is an extension to Kubernetes that uses OPA. Gatekeeper allows for defining and enforcing custom policies for any kind of Kubernetes API primitive. Therefore, it is far more versatile than PSA but requires more intricate knowledge on how to craft those rules. Gatekeeper gets involved in the admission control stage discussed in “Processing a Request”. The",
      "content_length": 1672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "following list of policies tries to give you an impression on what’s possible with Gatekeeper:\n\nEnsuring that all Service objects need to define a label with the key team\n\nEnsuring that all container images defined by Pods need to be pulled from a company-internal registry\n\nEnsuring that Deployments need to control at least three replicas\n\nAt the time of writing, Gatekeeper allows for enforcing policies by rejecting object creation if requirements haven’t been met. Future versions of Gatekeeper might also provide a mechanism for mutating an object upon creation. For example, you may want to add specific label key-value pairs for any object created. The mutation would take care of adding those labels automatically.\n\nInstalling Gatekeeper\n\nInstalling Gatekeeper is relatively easy. All you need to do is to create a bunch of Kubernetes objects from a YAML manifest provided by the Gatekeeper project. You need to have cluster admin permissions to properly install Gatekeeper. The following command shows the kubectl command used to apply the latest release. For more information, see the installation manual:\n\n$ kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/\\ gatekeeper/master/deploy/gatekeeper.yaml\n\nGatekeeper objects have been installed in the namespace gatekeeper- system. Make sure that all Pods in the namespace transition into the “Running” status before trying to use Gatekeeper:\n\n$ kubectl get namespaces NAME STATUS AGE default Active 29h",
      "content_length": 1480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "gatekeeper-system Active 4s ...\n\nImplementing an OPA Policy\n\nWe’ll use a specific use case as an example to demonstrate the moving parts required to define a custom OPA policy. “Using Network Policies to Restrict Pod-to-Pod Communication” explained how to assign a label to a namespace so that it can be selected from a network policy. At its core, our custom OPA policy will determine that namespaces need to define at least one label with the key app to signify the application hosted by the namespace.\n\nGatekeeper requires us to implement two components for custom policy, the constraint template and the constraint. In a nutshell, the constraint template defines the rules with Rego and describes the schema for the constraint. Example 5-9 shows a constraint template definition for enforcing a label assignment.\n\nExample 5-9. An OPA constraint template requiring the definition of at least a single label apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: k8srequiredlabels spec: crd: spec: names: kind: K8sRequiredLabels validation: openAPIV3Schema: type: object properties: labels: type: array items: type: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiredlabels",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] { provided := {label | input.review.object.metadata.labels[label]} required := {label | label := input.parameters.labels[_]} missing := required - provided count(missing) > 0 msg := sprintf(\"you must provide labels: %v\", [missing]) }\n\nDeclares the kind to be used by the constraint.\n\nSpecifies the validation schema of the constraint. In this case, we allow to pass in a property named labels that captures the required label keys.\n\nUses Rego to check for the existence of labels and compares them to the list of required keys.\n\nThe constraint is essentially an implementation of the constraint template. It uses the kind defined by the constraint template and populates the data provided by the end user. In Example 5-10, the kind is K8sRequiredLabels, which we defined in the constraint template. We are matching on namespaces and expect them to define the label with the key app.\n\nExample 5-10. An OPA constraint that defines the “data” for the policy apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: ns-must-have-app-label-key spec: match: kinds: - apiGroups: [\"\"] kinds: [\"Namespace\"] parameters: labels: [\"app\"]\n\nUses the kind defined by the constraint template.",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Defines the API resources the constraint template should apply to.\n\nDeclares that the labels property expects the key app to exist.\n\nWith the relevant YAML manifests in place, let’s create the objects for the constraint template and the constraint. Assume that the constraint template was written to the file constraint-template-labels.yaml and the constraint to the file constraint-ns-labels.yaml:\n\n$ kubectl apply -f constraint-template-labels.yaml constrainttemplate.templates.gatekeeper.sh/k8srequiredlabels created $ kubectl apply -f constraint-ns-labels.yaml k8srequiredlabels.constraints.gatekeeper.sh/ns-must-have-app-label-key created\n\nYou can verify the validation behavior with a quick-to-run imperative command. The following command tries to create a new namespace without a label assignment. Gatekeeper will render an error message and prevent the creation of the object:\n\n$ kubectl create ns governed-ns Error from server (Forbidden): admission webhook \"validation.gatekeeper.sh\" \\ denied the request: [ns-must-have-app-label-key] you must provide labels: {\"app\"}\n\nLet’s make sure that we can actually create a namespace with the expected label assignment. Example 5-11 shows the YAML manifest of such a namespace.\n\nExample 5-11. YAML manifest for namespace with a label assignment apiVersion: v1 kind: Namespace metadata: labels: app: orion name: governed-ns\n\nThe following command creates the object from the YAML manifest file named namespace-app-label.yaml:",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "$ kubectl apply -f namespace-app-label.yaml namespace/governed-ns created\n\nThis simple example demonstrated the usage of OPA Gatekeeper. You can find a lot of other examples in the OPA Gatekeeper Library. Despite it not being spelled out explicitly in the CKS curriculum, you may also want to check out the project Kyverno, which recently gained a lot of traction with the Kubernetes community.\n\nManaging Secrets\n\nNo discussion on security features in Kubernetes would be complete without bringing up the topic of Secrets. I would assume that you are already well familiar with the API primitive Secret to define sensitive data and the different options for consuming it in a Pod. Given that this topic is already part of the CKA exam, I will not reiterate it here. For more information, see the relevant section in the Certified Kubernetes Administrator (CKA) Study Guide or the Kubernetes documentation. I talk about security aspects when consuming ConfigMaps and Secrets in a container in “Configuring a Container with a ConfigMap or Secret”.\n\nThe CKS exam puts a stronger emphasis on more specialized aspects of Secret management. One of those scenarios, which we already touched on, was the handling of a Secret you can assign to a service account. Revisit “Creating a Secret for a service account” to refresh your memory on the topic. As we are not going to discuss all built-in Secret types here, you may want to read up on their purpose and creation in the relevant section of the Kubernetes documentation.\n\nThe central location for storing Secrets key-value pairs is etcd. Let’s have a look at potential issues that may arise if an attacker gains access to Kubernetes backing store for cluster data.\n\nScenario: An Attacker Gains Access to the Node Running etcd",
      "content_length": 1769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Where etcd runs is dependent on the topology of your Kubernetes cluster. For the purpose of this scenario, we’ll assume that etcd runs on the control plane node. Any data stored in etcd exists in unencrypted form, so access to the control plane node allows for reading Secrets in plain text. Figure 5-3 shows an attacker gaining access to the control plane node and therefore the unencrypted Secrets in etcd.\n\nFigure 5-3. An attacker gains access to etcd to read Secrets\n\nOne way to mitigate the situation is by encrypting the data stored in etcd. Access to etcd, either using etcdctl or by reading the etcd data from the filesystem, would not expose human-readable, sensitive information anymore.\n\nAccessing etcd Data\n\nWe’ll start by showing how an attacker could read etcd data after being able to log into the control plane node. First, we need to create a Secret object to store in etcd. Use the following imperative command to create an entry:\n\n$ kubectl create secret generic app-config --from-literal=password=passwd123 secret/app-config created",
      "content_length": 1052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "We created a Secret with the key-value pair password=passwd123. Shell into the control plane node using SSH. You can easily use the etcd client tool etcdctl to read an entry from etcd.\n\nUSING THE ETCD CLIENT TOOL ETCDCTL\n\nIt’s very likely that you do not have etcdctl installed on the control plane node yet. Follow the installation manual to make the tool available. On Debian Linux, it can be installed with sudo apt install etcd-client. To authenticate against etcd, you will need to provide the mandatory command line options --cacert, --cert, and --key. You can find the corresponding values in the configuration file for the API server usually available at /etc/kubernetes/manifests/kube-apiserver.yaml. The parameters need to start with the prefix --etcd.\n\nThe following command uses the mandatory CLI options to read the contents from the Secret object named app-config. The following output displays the file contents in hexadecimal format. While not 100% obvious, you can still identify the key-value pair in plain text from the output:\n\n$ sudo ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/\\ etcd/server.key get /registry/secrets/default/app-config | hexdump -C 00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 |/registry/secret| 00000010 73 2f 64 65 66 61 75 6c 74 2f 61 70 70 2d 63 6f |s/default/app-co| 00000020 6e 66 69 67 0a 6b 38 73 00 0a 0c 0a 02 76 31 12 |nfig.k8s.....v1.| 00000030 06 53 65 63 72 65 74 12 d9 01 0a b7 01 0a 0a 61 |.Secret........a| 00000040 70 70 2d 63 6f 6e 66 69 67 12 00 1a 07 64 65 66 |pp-config....def| 00000050 61 75 6c 74 22 00 2a 24 36 38 64 65 65 34 34 38 |ault\".*$68dee448| 00000060 2d 34 39 62 37 2d 34 34 32 66 2d 39 62 32 66 2d |-49b7-442f-9b2f-| 00000070 33 66 39 62 39 62 32 61 66 66 36 64 32 00 38 00 |3f9b9b2aff6d2.8.| 00000080 42 08 08 97 f8 a4 9b 06 10 00 7a 00 8a 01 65 0a |B.........z...e.| 00000090 0e 6b 75 62 65 63 74 6c 2d 63 72 65 61 74 65 12 |.kubectl-create.| 000000a0 06 55 70 64 61 74 65 1a 02 76 31 22 08 08 97 f8 |.Update..v1\"....| 000000b0 a4 9b 06 10 00 32 08 46 69 65 6c 64 73 56 31 3a |.....2.FieldsV1:| 000000c0 31 0a 2f 7b 22 66 3a 64 61 74 61 22 3a 7b 22 2e |1./{\"f:data\":{\".| 000000d0 22 3a 7b 7d 2c 22 66 3a 70 61 73 73 77 6f 72 64 |\":{},\"f:password| 000000e0 22 3a 7b 7d 7d 2c 22 66 3a 74 79 70 65 22 3a 7b |\":{}},\"f:type\":{| 000000f0 7d 7d 42 00 12 15 0a 08 70 61 73 73 77 6f 72 64 |}}B.....password| 00000100 12 09 70 61 73 73 77 64 31 32 33 1a 06 4f 70 61 |..passwd123..Opa| 00000110 71 75 65 1a 00 22 00 0a |que..\"..|",
      "content_length": 2596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "In the next step, we’ll encrypt the Secret stored in etcd and then verify the existing entries with the same command.\n\nEncrypting etcd Data\n\nYou can control how API data is encrypted in etcd with the help of the command line option --encryption-provider-config provided to the API server process. The value assigned to the parameter needs to point to a configuration file that defines an EncryptionConfiguration object. We’ll first create the configuration file and then configure the API server process to consume it.\n\nGenerate a 32-byte random key and base64-encode it. The value is needed to configure a so-called provider in the encryption configuration:\n\n$ head -c 32 /dev/urandom | base64 W68xlPT/VXcOSEZJvWeIvkGJnGfQNFpvZYfT9e+ZYuY=\n\nNext up, we’ll use the base64-encoded key and assign it to a provider in the encryption configuration, as shown in Example 5-12. Save the contents in the file /etc/kubernetes/enc/enc.yaml.\n\nExample 5-12. YAML manifest for encryption configuration apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: W68xlPT/VXcOSEZJvWeIvkGJnGfQNFpvZYfT9e+ZYuY= - identity: {}\n\nDefines the API resource to be encrypted in etcd. We are only encrypting Secrets data here.\n\nThe base64-encoded key assigned to an AES-CBC encryption provider.",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "Edit the manifest at /etc/kubernetes/manifests/kube- apiserver.yaml, the YAML manifest that defines how to run an API server in a Pod. Add the parameter --encryption-provider-config, and define the Volume and its mountpath for the configuration file as the following shows:\n\n$ sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: \\ 192.168.56.10:6443 creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml volumeMounts: ... - name: enc mountPath: /etc/kubernetes/enc readonly: true volumes: ... - name: enc hostPath: path: /etc/kubernetes/enc type: DirectoryOrCreate ...\n\nThe Pod running the API server should automatically restart. This process may take a couple of minutes. Once fully restarted, you should be able to query for it:\n\n$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE",
      "content_length": 1087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "... kube-apiserver-control-plane 1/1 Running 0 69s\n\nNew Secrets will be encrypted automatically. Existing Secrets need to be updated. You can run the following command to perform an update on Secrets across all namespaces. This includes the Secret named app-config in the default namespace:\n\n$ kubectl get secrets --all-namespaces -o json | kubectl replace -f - ... secret/app-config replaced\n\nRunning the etcdctl command we used before will reveal that the aescbc provider has been used to encrypt the data. The password value cannot be read in plain text anymore:\n\n$ sudo ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/\\ etcd/server.key get /registry/secrets/default/app-config | hexdump -C 00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 |/registry/secret| 00000010 73 2f 64 65 66 61 75 6c 74 2f 61 70 70 2d 63 6f |s/default/app-co| 00000020 6e 66 69 67 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 |nfig.k8s:enc:aes| 00000030 63 62 63 3a 76 31 3a 6b 65 79 31 3a ae 26 e9 c2 |cbc:v1:key1:.&..| 00000040 7b fd a2 74 30 24 85 61 3c 18 1e 56 00 a1 24 65 |{..t0$.a<..V..$e| 00000050 52 3c 3f f1 24 43 9f 6d de 5f b0 84 32 18 84 47 |R<?.$C.m._..2..G| 00000060 d5 30 e9 64 84 22 f5 d0 0b 6f 02 af db 1d 51 34 |.0.d.\"...o....Q4| 00000070 db 57 c8 17 93 ed 9e 00 ea 9a 7b ec 0e 75 0c 49 |.W........{..u.I| 00000080 6a e9 97 cd 54 d4 ae 6b b6 cb 65 8a 5d 4c 3c 9c |j...T..k..e.]L<.| 00000090 db 9b ed bc ce bf 3c ef f6 2e cb 6d a2 53 25 49 |......<....m.S%I| 000000a0 d4 26 c5 4c 18 f3 65 bb a8 4c 0f 8d 6e be 7b d3 |.&.L..e..L..n.{.| 000000b0 24 9b a8 09 9c bb a3 f9 53 49 78 86 f5 24 e7 10 |$.......SIx..$..| 000000c0 ad 05 45 b8 cb 31 bd 38 b6 5c 00 02 b2 a4 62 13 |..E..1.8.\\....b.| 000000d0 d5 82 6b 73 79 97 7e fa 2f 5d 3b 91 a0 21 50 9d |..ksy.~./];..!P.| 000000e0 77 1a 32 44 e1 93 9b 9c be bf 49 d2 f9 dc 56 23 |w.2D......I...V#| 000000f0 07 a8 ca a5 e3 e7 d1 ae 9c 22 1f 98 b1 63 b8 73 |.........\"...c.s| 00000100 66 3f 9f a5 6a 45 60 a7 81 eb 32 e5 42 4d 2b fd |f?..jE`...2.BM+.| 00000110 65 6c c2 c7 74 9f 1d 6a 1c 24 32 0e 7a 94 a2 60 |el..t..j.$2.z..`| 00000120 22 77 58 c9 69 c3 55 72 e8 fb 0b 63 9d 7f 04 31 |\"wX.i.Ur...c...1| 00000130 00 a2 07 76 af 95 4e 03 0a 92 10 b8 bb 1e 89 94 |...v..N.........| 00000140 45 60 01 45 bf d7 95 df ff 2e 9e 31 0a |E`.E.......1.| 0000014d",
      "content_length": 2372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "For more details on encrypting etcd data, refer to the Kubernetes documentation. There, you will find additional information on other encryption providers, how to rotate the decryption key, and the process to consider for a high-availability (HA) cluster setup.\n\nUnderstanding Container Runtime Sandboxes\n\nContainers run in a container runtime isolated from the host environment. The process or application running in the container can interact with the kernel by making syscalls. Now, we can have multiple containers (as controlled by Pods) running on a single Kubernetes cluster node and therefore the same kernel. Under certain conditions, vulnerabilities can lead to a situation where a process running a container can “break out” of its isolated environment and access another container running on the same host machine. A container runtime sandbox runs side-by-side with the regular container runtime but adds an additional layer of security by tightening process isolation.\n\nThere are a couple of use cases where using a container runtime sandbox may make sense. For example, your Kubernetes cluster handles the workload of different customers with the same infrastructure, a so-called multi-tenant environment. Another reason for wanting to rely on stronger container isolation is that you may not trust the process or application running in a container image, as should be the case if you pulled the container image from a public registry and you can’t verify the creator or its runtime behavior.\n\nScenario: An Attacker Gains Access to Another Container\n\nIn this scenario, we are confronted with a developer that pulls a container image from a public registry, as referenced by a Pod. The container has not been scanned for security vulnerabilities. An attacker can push a new tag of",
      "content_length": 1792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "the container image executing malicious code. After instantiating a container from the image, the malicious code running in the kernel group of container 1 can access the process running in container 2. As you can see in Figure 5-4, both containers use the same kernel of the host system.\n\nFigure 5-4. An attacker gains access to another container\n\nGenerally speaking, it’s not a good idea to blindly trust public container images. One way to ensure that such a container image runs with more isolation is the container runtime sandbox. The next section will introduce you to two implementations, both of which are explicitly mentioned by the curriculum.\n\nAvailable Container Runtime Sandbox Implementations\n\nIn this book, we’ll only want to talk about two container runtime sandbox implementations, Kata Containers and gVisor. Kata containers achieves container isolation by running them in a lightweight virtual machine. gVisor",
      "content_length": 929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "takes a different approach. It effectively implements a Linux kernel that runs on the host system. Therefore, syscalls are not shared anymore across all containers on the host system.\n\nA deeper discussion on the feature sets or specific use cases for those container runtime sandbox implementations goes beyond the scope of this book. We’ll simply learn how to use one solution as an example, gVisor, and how to tie it into Kubernetes. Have a look at the talk “Kata Containers and gVisor: a Quantitative Comparison” for an in-depth comparison.\n\nInstalling and Configuring gVisor\n\nThe following instructions describe the steps required to install gVisor on Linux using the apt package manager. You will want to repeat those steps on all host machines declared as worker nodes. For the exam, you will not be expected to install gVisor or Kata Containers. You can assume that the container runtime sandbox has already been installed and configured.\n\nStart by installing the dependencies for gVisor with the following command:\n\n$ sudo apt-get update && \\ sudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg\n\nNext, configure the key used to sign archives and the repository. As you can see in the following commands, gVisor is hosted on Google storage:\n\n$ curl -fsSL https://gvisor.dev/archive.key | sudo gpg --dearmor -o /usr/share/\\ keyrings/gvisor-archive-keyring.gpg $ echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/\\ gvisor-archive-keyring.gpg] https://storage.googleapis.com/gvisor/releases \\ release main\" | sudo tee /etc/apt/sources.list.d/gvisor.list > /dev/null",
      "content_length": 1621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "gVisor includes an Open Container Initiative (OCI) runtime called runsc. The runsc runtime integrates with tools like Docker and Kubernetes to run container runtime sandboxes. The following command installs the executable from the repository:\n\n$ sudo apt-get update && sudo apt-get install -y runsc\n\nLet’s assume we are using containerd as the container runtime. You need to add some configuration to containerd to make it aware of runsc. You can find similar instructions for other container runtimes in the gVisor documentation:\n\n$ cat <<EOF | sudo tee /etc/containerd/config.toml version = 2 [plugins.\"io.containerd.runtime.v1.linux\"] shim_debug = true [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] runtime_type = \"io.containerd.runc.v2\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runsc] runtime_type = \"io.containerd.runsc.v1\" EOF\n\nFinally, restart containerd to let the changes take effect:\n\n$ sudo systemctl restart containerd\n\nWe successfully installed gVisor and can now configure Pods to use it.\n\nCreating and Using a Runtime Class\n\nIt’s a two-step approach to use a container runtime sandbox in a Pod. First, you need to create a runtime class. A RuntimeClass is a Kubernetes API resource that defines the configuration of the container runtime. Example 5- 13 shows a YAML manifest of a container runtime that points to runsc as the handler we set in the containerd configuration file earlier.\n\nExample 5-13. YAML manifest for defining a runtime class using runsc handler",
      "content_length": 1512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "apiVersion: node.k8s.io/v1 kind: RuntimeClass metadata: name: gvisor handler: runsc\n\nWe can now reference the runtime class name, gvisor, in the configuration of a Pod. Example 5-14 shows a Pod definition that assigns the runtime class using the attribute spec.runtimeClassName.\n\nExample 5-14. YAML manifest for a Pod using a runtime class apiVersion: v1 kind: Pod metadata: name: nginx spec: runtimeClassName: gvisor containers: - name: nginx image: nginx:1.23.2\n\nCreate the runtime class and Pod object using the apply command:\n\n$ kubectl apply -f runtimeclass.yaml runtimeclass.node.k8s.io/gvisor created $ kubectl apply -f pod.yaml pod/nginx created\n\nYou can verify that the container is running with the container runtime sandbox. Simply execute the dmesg command to examine the kernel ring buffer. The output from the command should mention gVisor, as shown in the following:\n\n$ kubectl exec nginx -- dmesg [ 0.000000] Starting gVisor... [ 0.123202] Preparing for the zombie uprising... [ 0.415862] Rewriting operating system in Javascript... [ 0.593368] Reading process obituaries... [ 0.741642] Segmenting fault lines... [ 0.797360] Daemonizing children... [ 0.831010] Creating bureaucratic processes... [ 1.313731] Searching for needles in stacks... [ 1.455084] Constructing home...",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "[ 1.834278] Gathering forks... [ 1.928142] Mounting deweydecimalfs... [ 2.109973] Setting up VFS... [ 2.157224] Ready!\n\nUnderstanding Pod-to-Pod Encryption with mTLS\n\nIn “Using Network Policies to Restrict Pod-to-Pod Communication”, we talked about Pod-to-Pod communication. One of the big takeaways was that every Pod can talk to any other Pod by targeting its virtual IP address unless you put a more restrictive network policy in place. The communication between two Pods is unencrypted by default.\n\nTLS provides encryption for network communication, often in conjunction with the HTTP protocol. That’s when we talk about using the HTTPS protocol for calls to web pages from the browser. As part of the authentication process, the client offers its client certificate to the server for proving its identity. The server does not authenticate the client, though.\n\nWhen loading a web page, the identity of the client, in this case the browser, usually doesn’t matter. The important part is that the web page proves its identity. Mutual TLS (mTLS) is like TLS, but both sides have to authenticate. This approach has the following benefits. First, you achieve secure communication through encryption. Second, you can verify the client identity. An attacker cannot easily impersonate another Pod.\n\nScenario: An Attacker Listens to the Communication Between Two Pods\n\nAn attacker can use the default, unencrypted Pod-to-Pod network communication behavior to their advantage. As you can see in Figure 5-5, an attacker doesn’t even need to break into a Pod. They can simply listen to the Pod-to-Pod communication by impersonating the sending or the receiving side, extract sensitive information, and then use it for more advanced attack vectors.",
      "content_length": 1739,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Figure 5-5. An attacker listens to Pod-to-Pod communication\n\nYou can mitigate the situation by setting up mTLS. The next section will briefly touch on the options for making that happen.\n\nAdopting mTLS in Kubernetes\n\nThe tricky part about implementing mTLS in a Kubernetes cluster is the management of certificates. As you can imagine, we’ll have to deal with a lot of certificates when implementing a microservices architecture. Those certificates are usually generated by an official certificate authority (CA) to ensure that they can be trusted. Requesting a certificate involves sending a certificate signing request (CSR) to the CA. If the CA approves the request, it creates the certificate, then signs and returns it. It’s recommended to assign short lifespans to a certificate before it needs to be re-issued again. That process is called certificate rotation.\n\nIt’s somewhat unclear to what degree of detail the CKS exam requires you to understand mTLS. The general process of requesting and approving a certificate is described in the Kubernetes documentation.\n\nIn most cases, Kubernetes administrators rely on a Kubernetes service mesh to implement mTLS instead of implementing it manually. A Kubernetes service mesh, such as Linkerd or Istio, is a tool for adding cross-cutting functionality to your cluster, like observability and security.\n\nAnother option is to use transparent encryption to ensure that traffic doesn’t go on the wire unencrypted. Some of the popular CNI plugins, such as Calico and Cilium, have added support for WireGuard. WireGuard is an open source, lightweight, and secure Virtual Private Network (VPN) solution that doesn’t require the configuration or management of encryption",
      "content_length": 1714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "keys or certificates. Many teams prefer WireGuard over a service mesh as it is easier to manage.\n\nServices meshes and WireGuard are out of scope for the exam.\n\nSummary\n\nIt’s important to enforce security best practices for Pods. In this chapter, we reviewed different options. We looked at security contexts and how they can be defined on the Pod and container level. For example, we can configure a security context for a container to run with a non-root user and prevent the use of privileged mode. It’s usually the responsibility of a developer to define those settings. The Pod Security Admission is a Kubernetes feature that takes Pod security settings one step further. You can centrally configure a Pod such that it needs to adhere to certain security standards. The configured security standard can either be enforced, audited, or just logged to standard output. Gatekeeper is an open source project that implements the functionality of the Open Policy Agent for Kubernetes. Not only can you govern the configuration for Pod objects, you can also apply policies to other kinds of objects during creation time.\n\nKey-value pairs defined by Secrets are stored in etcd in plain text. You should configure encryption for etcd to ensure that attackers cannot read sensitive data from it. To enable encryption, create a YAML manifest for an EncryptionConfiguration, which you would then pass to the API server process with the command line option --encryption-provider-config.\n\nContainer runtime sandboxes help with isolating processes and applications to a stronger degree than the regular container runtime. The projects Kata Containers and gVisor are implementations of such a container runtime sandbox and can be installed and configured to work with Kubernetes. We tried gVisor. After installing and configuring gVisor, you will need to create a RuntimeClass object that points to runsc. In the Pod configuration, point to the RuntimeClass object by name.",
      "content_length": 1961,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Pod-to-Pod communication is unencrypted and unauthenticated by default. Mutual TLS makes the process more secure. Pods communicating with one another need to provide certificates to prove their identity. Implementing mTLS for a cluster with hundreds of microservices is a tedious task. Each Pod running a microservice needs to employ an approved certificate from a Client Authority. Services meshes help with adding mTLS as a feature to a Kubernetes cluster.\n\nExam Essentials\n\nPractice the use of core Kubernetes features and external tools to govern security settings.\n\nIn the course of this chapter, we looked at OS-level security settings and how to govern them with different core features and external tooling. You need to understand the different options, their benefits and limitations, and be able to apply them to implement contextual requirements. Practice the use of security contexts, Pod Security Admission, and Open Policy Agent Gatekeeper. The Kubernetes ecosystem offers more tooling in this space. Feel free to explore those on your own to expand your horizon.\n\nUnderstand how etcd manages Secrets data.\n\nThe CKA exam already covers the workflow of creating and using Secrets to inject sensitive configuration data into Pods. I am assuming that you already know how to do this. Every Secret key-value pair is stored in etcd. Expand your knowledge of Secret management by learning how to encrypt etcd so that an attacker with access to a host running etcd isn’t able to read information in plain text.\n\nKnow how to configure the use of a container runtime sandbox.\n\nContainer runtime sandboxes help with adding stricter isolation to containers. You will not be expected to install a container runtime sandbox, such as Kata Containers or gVisor. You do need to understand",
      "content_length": 1786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "the process for configuring a container runtime sandbox with the help of a RuntimeClass object and how to assign the RuntimeClass to a Pod by name.\n\nGain awareness of mTLS.\n\nSetting up mTLS for all microservices running in a Pod can be extremely tedious due to certificate management. For the exam, understand the general use case for wanting to set up mTLS for Pod-to- Pod communication. You are likely not expected to actually implement it manually, though. Production Kubernetes clusters use services meshes to provide mTLS as a feature.\n\nSample Exercises\n\nSolutions to these exercises are available in the Appendix.\n\n1. Create a Pod named busybox-security-context with the container image busybox:1.28 that runs the command sh -c sleep 1h. Add a Volume of type emptydir and mount it to the path /data/test. Configure a security context with the following attributes: runAsUser: 1000, runAsGroup: 3000, and fsGroup: 2000. Furthermore, set the attribute allowPrivilegeEscalation to false.\n\nShell into the container, navigate to the directory /data/test, and create the file named hello.txt. Check the group assigned to the file. What’s the value? Exit out of the container.\n\n2. Create a Pod Security Admission (PSA) rule. In the namespace called audited, create a Pod Security Standard (PSS) with the level baseline that should be rendered to the console. Try to create a Pod in the namespace that violates the PSS and produces a message on the console log. You can provide any name, container image, and security configuration you like. Will the Pod",
      "content_length": 1552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "be created? What PSA level needs to be configured to prevent the creation of the Pod?\n\n3. Install Gatekeeper on your cluster. Create a Gatekeeper ConstraintTemplate object that defines the minimum and maximum number of replicas controlled by a ReplicaSet. Instantiate a Constraint object that uses the ConstraintTemplate. Set the minimum number of replicas to 3 and the maximum number to 10.\n\nCreate a Deployment object that sets the number of replicas to 15. Gatekeeper should not allow the Deployment, ReplicaSet, and Pods to be created. An error message should be rendered. Try again to create the Deployment object but with a replica number of 7. Verify that all objects have been created successfully.\n\n4. Configure encryption for etcd using the aescbc provider. Create a new Secret object of type Opaque. Provide the key-value pair api- key=YZvkiWUkycvspyGHk3fQRAkt. Query for the value of the Secret using etcdctl. What’s the encrypted value?\n\n5. Navigate to the directory app-a/ch05/gvisor of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f. gVisor has been installed in the VM kube-worker-1. Shell into the VM and create a RuntimeClass object named container- runtime-sandbox with runsc as the handler. Then create a Pod with the name nginx and the container image nginx:1.23.2 and assign the RuntimeClass to it.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.",
      "content_length": 1669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Chapter 6. Supply Chain Security\n\nEarlier chapters primarily focused on securing the Kubernetes cluster and its components, the OS infrastructure used to run cluster nodes, and the operational aspects for running workload on a cluster node with existing container images. This chapter takes a step back and drills into the process, best practices, and tooling for designing, building, and optimizing container images.\n\nSometimes, you do not want to create your own container image but instead consume an existing one produced by a different team or company. Scanning container images for known vulnerabilities in a manual or automated fashion should be part of your vetting process before using them to run your workload. We’ll talk through some options relevant to the CKS exam used to identify, analyze, and mitigate security risks for pre-built container images.\n\nAt a high level, this chapter covers the following concepts:\n\nMinimizing base image footprint\n\nSecuring the supply chain\n\nUsing static analysis of user workload\n\nScanning images for known vulnerabilities\n\nMinimizing the Base Image Footprint\n\nThe process for building a container image looks straightforward on the surface level; however, the devil is often in the details. It may not be obvious to someone new to the topic to refrain from building a container",
      "content_length": 1326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "image that is unnecessarily too large in size, riddled with vulnerabilities, and not optimized for container layer caching. We’ll address all of those aspects in the course of this chapter with the help of the container engine Docker.\n\nScenario: An Attacker Exploits Container Vulnerabilities\n\nOne of the first decisions you have to make when defining a Dockerfile is the selection of a base image. The base image provides the operating system and additional dependencies, and it may expose shell access.\n\nSome of the base images you can choose from on a public registry like Docker Hub are large in size and will likely contain functionality you don’t necessarily need to run your application inside of it. The operating system itself, as well as any dependencies available with the base image, can expose vulnerabilities.\n\nIn Figure 6-1, the attacker was able to figure out details about the container by gaining access to it. Those vulnerabilities can now be used as a launching pad for more advanced attacks.",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Figure 6-1. An attacker exploits container image vulnerabilities\n\nIt is recommended to use a base image with a minimal set of functionality and dependencies. The next couple of sections will explain the methods for creating a more optimized base image that’s faster to build, quicker to download from a container registry, and that will ultimately lead to a smaller attack surface simply by reducing the bloat. The next sections will touch on the most important techniques. You can find a more detailed list of best practices for writing Dockerfiles in the Docker documentation.\n\nPicking a Base Image Small in Size\n\nSome container images can have a size of a gigabyte or even more. Do you really need all the functionality bundled with such a container image? Unlikely. Thankfully, many container producers upload a wide range of variations of their container images for the same release. One of those variations is an alpine image, a small, lightweight, and less vulnerable Linux distribution. As you can see in the following output, the downloaded alpine container image with the tag 3.17.0 only has a size of 7.05MB:",
      "content_length": 1119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "$ docker pull alpine:3.17.0 ... $ docker image ls alpine REPOSITORY TAG IMAGE ID CREATED SIZE alpine 3.17.0 49176f190c7e 3 weeks ago 7.05MB\n\nThe alpine container image comes with an sh shell you can use to troubleshoot the process running inside of the container. You can use the following command to open an interactive shell in a new container:\n\n$ docker run -it alpine:3.17.0 /bin/sh / # exit\n\nWhile runtime troubleshooting functionality can be useful, offering a shell as part of a container image increases the size of it and potentially opens the door for attackers. Additionally, the more software there is inside of a container image, the more vulnerabilities it will have.\n\nYou can further reduce the container image size and the attack surface by using a distroless image offered by Google. The following command downloads the latest tag of the container image gcr.io/distroless/static-debian11 and renders its details. The size of the container image is only 2.34MB:\n\n$ docker pull gcr.io/distroless/static-debian11 ... $ docker image ls gcr.io/distroless/static-debian11:latest REPOSITORY TAG IMAGE ID CREATED \\ SIZE gcr.io/distroless/static-debian11 latest 901590160d4d 53 years ago \\ 2.34MB\n\nA distroless container image does not ship with any shell, which you can observe by running the following command:\n\n$ docker run -it gcr.io/distroless/static-debian11:latest /bin/sh docker: Error response from daemon: failed to create shim task: OCI runtime \\ create failed: runc create failed: unable to start container process: exec: \\ \"/bin/sh\": stat /bin/sh: no such file or directory: unknown.",
      "content_length": 1604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Kubernetes offers the concept of ephemeral containers for troubleshooting distroless containers. Those containers are meant to be disposable and can be deployed for troubleshooting minimal containers that would usually not allow opening a shell. Discussing ephemeral containers is out of scope of this book, but you can find more information about them in the Kubernetes documentation.\n\nUsing a Multi-Stage Approach for Building Container Images\n\nAs a developer, you can decide to build the application code as part of the instructions in a Dockerfile. This process may include compiling the code and building a binary that should become the entry point of the container image. Having all the necessarily tools and dependencies available to implement the process will automatically blow up the size of the container image, plus you won’t need those dependencies at runtime anymore.\n\nThe idea of a multi-stage build in Docker is that you separate the build stage from the runtime stage. As a result, all dependencies needed in the build stage will be discarded after the process has been performed and therefore do not end up in the final container image. This approach leads to a much smaller container image size by removing all the unnecessary cruft.\n\nWhile we won’t go into the details of crafting and fully understanding multi-stage Dockerfile, I want to show you the differences on a high level. We’ll start by showing you a Dockerfile that builds and tests a simple program using the programming language Go, as shown in Example 6-1. In essence, we are using a base image that includes Go 1.19.4. The Go runtime provides the go executes, which we’ll invoke to execute the tests and build the binary of the application.\n\nExample 6-1. Building and testing a Go program using a Go base image FROM golang:1.19.4-alpine WORKDIR /app\n\nCOPY go.mod . COPY go.sum . RUN go mod download",
      "content_length": 1882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "g\n\nCOPY . . RUN CGO_ENABLED=0 go test -v RUN go build -o /go-sample-app . CMD [\"/go-sample-app\"]\n\nUses a Go base image\n\nExecutes the tests against the application code\n\nBuilds the binary of the Go application\n\nYou can produce the image using the docker build command, as shown in the following:\n\n$ docker build . -t go-sample-app:0.0.1 ...\n\nThe size of the result container image is pretty big, 348MB, and there’s a good reason for it. It includes the Go runtime, even though we don’t actually need it anymore when starting the container. The go build command produced the binary that we can run as the container’s entry point:\n\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE go-sample-app 0.0.1 88175f3ab0d3 44 seconds ago 358MB\n\nNext up, we’ll have a look at the multi-stage approach. In a multi-stage Dockerfile, you define at least two stages. In Example 6-2, we specify a stage aliased with build to run the tests and build the binary, similarly to what we’ve done earlier. A second stage copies the binary produced by the stage build into a dedicated directory; however, it uses the alpine base image to run it. When running the docker build command, the stage build will not be included in the final container image anymore.",
      "content_length": 1237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Example 6-2. Building and testing a Go program as part of a multi-stage Dockerfile FROM golang:1.19.4-alpine AS build RUN apk add --no-cache git WORKDIR /tmp/go-sample-app COPY go.mod . COPY go.sum . RUN go mod download COPY . .\n\nRUN CGO_ENABLED=0 go test -v RUN go build -o ./out/go-sample-app .\n\nFROM alpine:3.17.0 RUN apk add ca-certificates COPY --from=build /tmp/go-sample-app/out/go-sample-app /app/go-sample-app CMD [\"/app/go-sample-app\"]\n\nUses a Go base image for building and testing the program in the stage named build.\n\nExecutes the tests against the application code.\n\nBuilds the binary of the Go application.\n\nUses a much smaller base image for running the application in a container.\n\nCopies the application binary produced in the build stage and uses it as the command to run when the container is started.\n\nThe resulting container image size is significantly smaller when using the alpine base image, only 12MB. You can verify the outcome by running the docker build command again and inspecting the size of the container image by listing it:\n\n$ docker build . -t go-sample-app:0.0.1 ... $ docker images",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "REPOSITORY TAG IMAGE ID CREATED SIZE go-sample-app 0.0.1 88175f3ab0d3 44 seconds ago 12MB\n\nNot only did we reduce the size, we also reduced the attack surface by including fewer dependencies. You can further reduce the size of the container image by incorporating a distroless base image instead of the alpine base image.\n\nReducing the Number of Layers\n\nEvery Dockerfile consists of an ordered list of instructions. Only some instructions create a read-only layer in the resulting container image. Those instructions are FROM, COPY, RUN, and CMD. All other instructions will not create a layer as they create temporary intermediate images. The more layers you add to the container image, the slower will be the build execution time and/or the bigger will be the size of the container image. Therefore, you need to be cautious about the instructions used in your Dockerfile to minimize the footprint of the container image.\n\nIt’s common practice to execute multiple commands in a row. You may define those commands using a list of RUN instructions on individual lines, as shown in Example 6-3.\n\nExample 6-3. A Dockerfile specifying multiple RUN instructions FROM ubuntu:22.10 RUN apt-get update -y RUN apt-get upgrade -y RUN apt-get install -y curl\n\nEach RUN instruction will create a layer, potentially adding to the size of the container image. It’s more efficient to string them together with && to ensure that only a single layer will be produced. Example 6-4 shows an example of this optimization technique.\n\nExample 6-4. A Dockerfile specifying multiple RUN instructions FROM ubuntu:22.10 RUN apt-get update -y && apt-get upgrade -y && apt-get install -y curl",
      "content_length": 1664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "Using Container Image Optimization Tools\n\nIt’s easy to forget about the optimization practices mentioned previously. The open source community developed a couple of tools that can help with inspecting a produced container image. Their functionalities provide useful tips for pairing down on unnecessary layers, files, and folders:\n\nDockerSlim\n\nDockerSlim will optimize and secure your container image by analyzing your application and its dependencies. You can find more information in the tool’s GitHub repository.\n\nDive\n\nDive is a tool for exploring the layers baked into a container image. It makes it easy to identify unnecessary layers, which you can further optimize on. The code and documentation for Dive are available in a GitHub repository.\n\nThis is only the short list of container image optimization tools. In “Static Analysis of Workload”, we’ll have a look at other tools that focus on the static analysis of Dockerfiles and Kubernetes manifests.\n\nSecuring the Supply Chain\n\nA supply chain automates the process of producing a container image and operating it in a runtime environment, in this case Kubernetes. We already touched on a couple of tools that can be integrated into the supply chain to support the aspect of container image optimization. In this section, we’ll expand on practices that target security aspects. Refer to the book Container Security by Liz Rice (O’Reilly) to learn more.\n\nSigning Container Images",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "You can sign a container image before pushing it to a container registry. Signing can be achieved with the docker trust sign command, which adds a signature to the container image, the so-called image digest. An image digest is derived from the contents of the container image and commonly represented in the form of SHA256. When consuming the container image, Kubernetes can compare the image digest with the contents of the image to ensure that it hasn’t been tampered with.\n\nScenario: An Attacker Injects Malicious Code into a Container Image\n\nThe Kubernetes component that verifies the image digest is the kubelet. If you configured the image pull policy to Always, the kubelet will query for the image digest from the container registry even though it may have downloaded and verified the container image before.\n\nAn attacker can try to modify the contents of an existing container image, inject malicious code, and upload it to the container registry with the same tag, as shown in Figure 6-2. The malicious code running in the container could then send sensitive information to a third-party server accessible by the attacker.",
      "content_length": 1133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Figure 6-2. An attacker injects malicious code into a container image\n\nIMAGE CHECKSUM VALIDATION IS NOT AUTOMATIC\n\nImage digest validation is an opt-in functionality in Kubernetes. When defining Pods, make sure you spell out the image digest explicitly for all container images.\n\nValidating Container Images",
      "content_length": 307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "In Kubernetes, you’re able to append the SHA256 image digest to the specification of a container. For example, this can be achieved with the attribute spec.containers[].image. The image digest is generally available in the container registry. For example, Figure 6-3 shows the image digest for the container image alpine:3.17.0 on Docker Hub. In this example, the image digest is sha256:c0d488a800e4127c334ad20d61d7bc21b4097540327217dfab5226 2adc02380c.\n\nFigure 6-3. The image digest of the alpine:3.17.0 container image on Docker Hub\n\nLet’s see the image digest in action. Instead of using the tag, Example 6-5 specifies the container image by appending the image digest.\n\nExample 6-5. A Pod using a valid container image digest apiVersion: v1 kind: Pod metadata: name: alpine-valid spec: containers: - name: alpine image: alpine@sha256:c0d488a800e4127c334ad20d61d7bc21b40 \\ 97540327217dfab52262adc02380c command: [\"/bin/sh\"] args: [\"-c\", \"while true; do echo hello; sleep 10; done\"]\n\nCreating the Pod will work as expected. The image digest will be verified and the container transitions into the “Running” status:\n\n$ kubectl apply -f pod-valid-image-digest.yaml pod/alpine-valid created $ kubectl get pod alpine-valid NAME READY STATUS RESTARTS AGE alpine-valid 1/1 Running 0 6s",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "Example 6-6 shows the same Pod definition; however, the image digest has been changed so that it does not match with the contents of the container image.\n\nExample 6-6. A Pod using an invalid container image digest apiVersion: v1 kind: Pod metadata: name: alpine-invalid spec: containers: - name: alpine image: alpine@sha256:d006a643bccb6e9adbabaae668533c7f2e5 \\ 111572fffb5c61cb7fcba7ef4150b command: [\"/bin/sh\"] args: [\"-c\", \"while true; do echo hello; sleep 10; done\"]\n\nYou will see that Kubernetes can still create the Pod object but it can’t properly validate the hash of the container image. This results in the status “ErrImagePull.” As you can see from the event log, the container image couldn’t even be pulled from the container registry:\n\n$ kubectl get pods NAME READY STATUS RESTARTS AGE alpine-invalid 0/1 ErrImagePull 0 29s $ kubectl describe pod alpine-invalid ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 13s default-scheduler Successfully assigned default \\ /alpine-invalid to minikube Normal Pulling 13s kubelet Pulling image \"alpine@sha256: \\ d006a643bccb6e9adbabaae668533c7f2e5111572fffb5c61cb7fcba7ef4150b\" Warning Failed 11s kubelet Failed to pull image \\ \"alpine@sha256:d006a643bccb6e9adbabaae668533c7f2e5111572fffb5c61cb7fcba7ef4 \\ 150b\": rpc error: code = Unknown desc = Error response from daemon: manifest \\ for alpine@sha256:d006a643bccb6e9adbabaae668533c7f2e5111572fffb5c61cb7fcba7e \\ f4150b not found: manifest unknown: manifest unknown Warning Failed 11s kubelet Error: ErrImagePull Normal BackOff 11s kubelet Back-off pulling image \\",
      "content_length": 1607,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "\"alpine@sha256:d006a643bccb6e9adbabaae668533c7f2e5111572fffb5c61cb7fcba7ef415 \\ 0b\" Warning Failed 11s kubelet Error: ImagePullBackOff\n\nUsing Public Image Registries\n\nWhenever a Pod is created, the container runtime engine will download the declared container image from a container registry if it isn’t available locally yet. This runtime behavior can be further tweaked using the image pull policy.\n\nThe prefix in the image name declares the domain name of the registry; e.g., gcr.io/google-containers/debian-base:v1.0.1 refers to the container image google-containers/debian-base:v1.0.1 in the Google Cloud container registry, denoted by gcr.io. The container runtime will try to resolve it from docker.io, the Docker Hub container registry if you leave off the domain name in the container image declaration.\n\nScenario: An Attacker Uploads a Malicious Container Image\n\nWhile it is convenient to resolve container images from public container registries, it doesn’t come without risks. Anyone with a login to those container registries can upload new images. Consuming container images usually doesn’t even require an account.\n\nAs shown in Figure 6-4, an attacker can upload a container image containing malicious code to a public registry using stolen credentials. Any container referencing the container image from that registry will automatically run the malicious code.",
      "content_length": 1376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "Figure 6-4. An attacker uploads a malicious container image\n\nOn an enterprise level, you need to control which container registries you trust. It’s recommended to set up your own container registry within your company’s network, which you can fully control and govern. Alternatively, you can set up a private container registry in a cloud provider environment, not accessible by anyone else.\n\nOne of the prominent binary repository managers you can choose from is JFrog Artifactory. The product fully supports storing, scanning, and serving container images. Any consumer of container images should only be allowed to pull images from your whitelisted container registry. All other container registries should be denied.\n\nWhitelisting Allowed Image Registries with OPA GateKeeper",
      "content_length": 779,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "One way to govern container registry usage is with OPA Gatekeeper. We discussed the installation process and functionality of OPA Gatekeeper in “Understanding Open Policy Agent (OPA) and Gatekeeper”. Here, we’ll’ touch on the constraint template and constraint for allowing trusted container registries.\n\nExample 6-7 shows the constraint template I got directly from the OPA Gatekeeper library. As input properties, the constraint template defines an array of strings representing the prefixes of container registries. The Rego rules verify not only the assigned container images for the attribute spec.containers[] but also spec.initContainers[] and spec.ephemeralContainers[].\n\nExample 6-7. An OPA Gatekeeper constraint template for enforcing container registries apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: k8sallowedrepos annotations: metadata.gatekeeper.sh/title: \"Allowed Repositories\" metadata.gatekeeper.sh/version: 1.0.0 description: >- Requires container images to begin with a string from the specified list. spec: crd: spec: names: kind: K8sAllowedRepos validation: openAPIV3Schema: type: object properties: repos: description: The list of prefixes a container image is allowed to have. type: array items: type: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8sallowedrepos",
      "content_length": 1345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "violation[{\"msg\": msg}] { container := input.review.object.spec.containers[_] satisfied := [good | repo = input.parameters.repos[_] ; \\ good = startswith(container.image, repo)] not any(satisfied) msg := sprintf(\"container <%v> has an invalid image repo <%v>, allowed \\ repos are %v\", [container.name, container.image, input.parameters.repos]) }\n\nviolation[{\"msg\": msg}] { container := input.review.object.spec.initContainers[_] satisfied := [good | repo = input.parameters.repos[_] ; \\ good = startswith(container.image, repo)] not any(satisfied) msg := sprintf(\"initContainer <%v> has an invalid image repo <%v>, \\ allowed repos are %v\", [container.name, container.image, \\ input.parameters.repos]) }\n\nviolation[{\"msg\": msg}] { container := input.review.object.spec.ephemeralContainers[_] satisfied := [good | repo = input.parameters.repos[_] ; \\ good = startswith(container.image, repo)] not any(satisfied) msg := sprintf(\"ephemeralContainer <%v> has an invalid image repo <%v>, \\ allowed repos are %v\", [container.name, container.image, \\ input.parameters.repos]) }\n\nThe constraint shown in Example 6-8 is in charge of defining which container registries we want to allow. You’d usually go with a domain name of a server hosted in your company’s network. Here, we are going to use gcr.io/ for demonstration purposes.\n\nExample 6-8. An OPA Gatekeeper constraint assigning Google Cloud registry as trusted repository apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sAllowedRepos metadata: name: repo-is-gcr spec:",
      "content_length": 1519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "match: kinds: - apiGroups: [\"\"] kinds: [\"Pod\"] parameters: repos: - \"gcr.io/\"\n\nLet’s create both objects using the apply command:\n\n$ kubectl apply -f allowed-repos-constraint-template.yaml constrainttemplate.templates.gatekeeper.sh/k8sallowedrepos created $ kubectl apply -f gcr-allowed-repos-constraint.yaml k8sallowedrepos.constraints.gatekeeper.sh/repo-is-gcr created\n\nIn the constraint, we didn’t specify a namespace that the rules should apply to. Therefore, they’ll apply across all namespaces in the cluster. The following commands verify that the whitelisting rules work as expected. The following command tries to create a Pod using the nginx container image from Docker Hub. The creation of the Pod is denied with an appropriate error message:\n\n$ kubectl run nginx --image=nginx:1.23.3 Error from server (Forbidden): admission webhook \"validation.gatekeeper.sh\" \\ denied the request: [repo-is-gcr] container <nginx> has an invalid image \\ repo <nginx:1.23.3>, allowed repos are [\"gcr.io/\"]\n\nThe next command creates a Pod with a container image from the Google Cloud container registry. The operation is permitted and the Pod object is created:\n\n$ kubectl run busybox --image=gcr.io/google-containers/busybox:1.27.2 pod/busybox created $ kubectl get pods NAME READY STATUS RESTARTS AGE busybox 0/1 Completed 1 (2s ago) 3s\n\nWhitelisting Allowed Image Registries with the ImagePolicyWebhook Admission Controller Plugin",
      "content_length": 1426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Another way to validate the use of allowed image registries is to intercept a call to the API server when a Pod is about to be created. This mechanism can be achieved by enabling an admission controller plugin. Once configured, the logic of an admission controller plugin is automatically invoked when the API server receives the request, but after it could authenticate and authorize the caller. We already touched on the phases an API call has to go through in “Processing a Request”.\n\nThe admission controller provides a way to approve, deny, or mutate a request before the request takes effect. For example, we can inspect the data sent with the API request to create a Pod, iterate over the assigned container images, and execute custom logic to validate the container image notation. If the container image doesn’t stick to the expected conventions, we can deny the creation of the Pod. Figure 6-5 illustrates the high-level workflow.\n\nFigure 6-5. Intercepting a Pod-specific API call and handling it with a webhook\n\nThe ImagePolicyWebhook admission controller plugin is one of the plugins we can configure for intercepting Kubernetes API calls. You can probably derive the plugin’s functionality from its name. It defines a policy for all defined container images in a Pod. To compare container images with the defined policy, the plugin calls off to a service external to Kubernetes via HTTPS, a webhook. The external service then makes the decision on how to validate the data. In the context of the admission controller plugin, the external service is also referred to as backend.\n\nImplementing the Backend Application",
      "content_length": 1628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "The backend application can be implemented with a programming language of your choice. There are only three requirements it must fulfill:\n\n1. It’s a web application that can handle HTTPS requests.\n\n2. It can accept and parse JSON request payloads.\n\n3. It can send a JSON response payload.\n\nImplementing the backend application is not part of the CKS exam, but you can find a sample Go-based implementation in the GitHub repository of this book. Be aware that the code is not considered to be production-ready.\n\nThe following commands demonstrate the runtime behavior of the application accessible at https://localhost:8080/validate. You can find an example request and response JSON body in the Kubernetes documentation.\n\nThe following curl command calls the validation logic for the container image nginx:1.19.0. As you can see from the JSON response, the image is denied:\n\n$ curl -X POST -H \"Content-Type: application/json\" -k -d \\'{\"apiVersion\": \\ \"imagepolicy.k8s.io/v1alpha1\", \"kind\": \"ImageReview\", \"spec\": \\ {\"containers\": [{\"image\": \"nginx:1.19.0\"}]}}' https://localhost:8080/validate {\"apiVersion\": \"imagepolicy.k8s.io/v1alpha1\", \"kind\": \"ImageReview\", \\ \"status\": {\"allowed\": false, \"reason\": \"Denied request: [container 1 \\ has an invalid image repo nginx:1.19.0, allowed repos are [gcr.io/]]\"}}\n\nThe following curl command calls the validation logic for the container image gcr.io/nginx:1.19.0. As you can see from the JSON response, the image is allowed:\n\n$ curl -X POST -H \"Content-Type: application/json\" -k -d '{\"apiVersion\": \\ \"imagepolicy.k8s.io/v1alpha1\", \"kind\": \"ImageReview\", \"spec\": {\"containers\": \\ [{\"image\": \"gcr.io/nginx:1.19.0\"}]}}' https://localhost:8080/validate {\"apiVersion\": \"imagepolicy.k8s.io/v1alpha1\", \"kind\": \"ImageReview\", \\ \"status\": {\"allowed\": true, \"reason\": \"\"}}",
      "content_length": 1806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nFor the exam, you are expected to understand how to “wire” the ImagePolicyWebhook admission controller plugin. This section will walk you through the process. First, you’ll need to create a configuration file for the admission controller so it knows what plugins to use and how it should behave at runtime. Create the file /etc/kubernetes/admission- control/image-policy-webhook-admission-config.yaml and populate it with the content shown in Example 6-9.\n\nExample 6-9. The admission controller configuration file apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: ImagePolicyWebhook configuration: imagePolicy: kubeConfigFile: /etc/kubernetes/admission-control/ \\ imagepolicywebhook.kubeconfig allowTTL: 50 denyTTL: 50 retryBackoff: 500 defaultAllow: false\n\nProvides the configuration for the ImagePolicyWebhook plugin.\n\nPoints to the configuration file used to configure the backend.\n\nDenies an API request if the backend cannot be reached. The default is true but setting it to false is far more sensible.\n\nNext, create the file referenced by the plugins[].configuration.imagePolicy.kubeConfigFile attribute. The contents of the file point to the HTTPS URL of the external service. It also defines the client certificate and key file, as well as the CA file on disk. Example 6-10 shows the contents of the configuration file.\n\nExample 6-10. The image policy configuration file",
      "content_length": 1477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "apiVersion: v1 kind: Config preferences: {} clusters: - name: image-validation-webhook cluster: certificate-authority: /etc/kubernetes/admission-control/ca.crt server: https://image-validation-webhook:8080/validate contexts: - context: cluster: image-validation-webhook user: api-server-client name: image-validation-webhook current-context: image-validation-webhook users: - name: api-server-client user: client-certificate: /etc/kubernetes/admission-control/api-server-client.crt client-key: /etc/kubernetes/admission-control/api-server-client.key\n\nThe URL to the backend service. Must use the HTTPS protocol.\n\nEnable the ImagePolicyWebhook admission controller plugin for the API server and point the admission controller to the configuration file. To achieve this, edit the configuration file of the API server, usually found at /etc/kubernetes/manifests/kube-apiserver.yaml.\n\nFind the command line option --enable-admission-plugins and append the value ImagePolicyWebhook to the existing list of plugins, separated by a comma. Provide the command line option --admission-control- config-file if it doesn’t exist yet, and set the value to /etc/kubernetes/admission-control/image-policy-webhook- admission-configuration.yaml. Given that the configuration file lives in a new directory, you will have to define it as a Volume and mount it to the container. Example 6-11 shows the relevant options for the API server container.\n\nExample 6-11. The API server configuration file ... spec: containers:",
      "content_length": 1499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "command: - kube-apiserver - --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook - --admission-control-config-file=/etc/kubernetes/admission-control/ \\ image-policy-webhook-admission-configuration.yaml ... volumeMounts: ... - name: admission-control mountPath: /etc/kubernetes/admission-control readonly: true volumes: ... - name: admission-control hostPath: path: /etc/kubernetes/admission-control type: DirectoryOrCreate ... The Pod running the API server should automatically restart. This process may take a couple of minutes. Restart the kubelet service with the command sudo systemctl restart kubelet should the API server not come up by itself. Once fully restarted, you should be able to query for it:\n\n$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE ... kube-apiserver-control-plane 1/1 Running 0 69s\n\nAny API call that requests the creation of a Pod will now be routed to the backend. Based on the validation result, the creation of the object will be allowed or denied.\n\nStatic Analysis of Workload\n\nThroughout this book, we talk about best practices for Dockerfiles and Kubernetes manifests. You can inspect those files manually, find undesired configurations, and fix them by hand. This approach requires a lot of intricate knowledge and is very tedious and error-prone. It is much more convenient and efficient to analyze workload files in an automated fashion",
      "content_length": 1400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "with proper tooling. The list of commercial and open source tooling for static analysis is long. In this section, we are going to present the functionality of two options, Haskell Dockerfile Linter and Kubesec.\n\nOn an enterprise level, where you have to process hundreds or even thousands of workload files, you’d do so with the help of a continuous delivery pipeline, as shown in Figure 6-6.\n\nFigure 6-6. An exemplary continuous delivery pipeline for Kubernetes\n\nRelevant static analysis tools can be invoked as a quality gate at an early stage of the pipeline even before a container image is built, pushed to a registry, or deployed to a Kubernetes cluster. For a deep dive on the principles and practices of continuous delivery, see the excellent book Continuous Delivery by Jez Humble and David Farley (Addison-Wesley Professional).\n\nUsing Hadolint for Analyzing Dockerfiles\n\nHaskell Dockerfile Linter, also called hadolint in short, is a linter for Dockerfiles. The tool inspects a Dockerfile based on best practices listed on the Docker documentation page. Example 6-12 shows an unoptimized Dockerfile for building a container image running a Go-based application.\n\nExample 6-12. An unoptimized Dockerfile FROM golang COPY main.go . RUN go build main.go CMD [\"./main\"]\n\nThe Haskell Dockerfile Linter supports a mode of operation that lets you point the hadolint executable to a Dockerfile on disk. You can see the command execution that follows, including the discovered warning messages produced by the analysis:",
      "content_length": 1520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "$ hadolint Dockerfile Dockerfile:1 DL3006 warning: Always tag the version of an image explicitly Dockerfile:2 DL3045 warning: `COPY` to a relative destination without \\ `WORKDIR` set.\n\nThe output of the command suggests that you should assign a tag to the base image. The existing Dockerfile will pull the latest tag, which will resolve to the newest Go container image. This practice can result in an incompatibility between the Go runtime version and the application code. Defining a working directory for copying resources helps with keeping operating system-specific directories and files separate from application- specific directories and files. Example 6-13 fixes the warning messages found by the Haskell Dockerfile Linter.\n\nExample 6-13. An optimized Dockerfile FROM golang:1.19.4-alpine WORKDIR /app COPY main.go . RUN go build main.go CMD [\"./main\"]\n\nAnother execution against the modified Dockerfile leads to a successful exit code, and no additional warning messages will be rendered:\n\n$ hadolint Dockerfile\n\nThe Dockerfile now follows best practices, as perceived by hadolint.\n\nUsing Kubesec for Analyzing Kubernetes Manifests\n\nKubesec is a tool for analyzing Kubernetes manifests. It can be executed as a binary, Docker container, admission controller plugin, and even as a plugin for kubectl. To demonstrate its use, we’ll set up a YAML manifest file pod-initial-kubesec-test.yaml, shown in Example 6-14 as a starting point.\n\nExample 6-14. A Pod YAML manifest using initial security settings apiVersion: v1 kind: Pod",
      "content_length": 1532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "metadata: name: kubesec-demo spec: containers: - name: kubesec-demo image: gcr.io/google-samples/node-hello:1.0 securityContext: readOnlyRootFilesystem: true\n\nUpon inspecting the Pod configuration, you may already have some suggestions on what could be improved based on the content of the previous chapters. Let’s see what Kubesec is going to recommend.\n\nThe simplest way to invoke Kubesec is to run the logic in a container with the help of Docker. The following command feeds the contents of the YAML manifest to the standard input stream. The result, formatted in JSON, renders a score, provides a human-readable message of the outcome, and advises on changes to be made:\n\n$ docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin \\ < pod-initial-kubesec-test.yaml [ { \"object\": \"Pod/kubesec-demo.default\", \"valid\": true, \"message\": \"Passed with a score of 1 points\", \"score\": 1, \"scoring\": { \"advise\": [ { \"selector\": \".spec .serviceAccountName\", \"reason\": \"Service accounts restrict Kubernetes API access and \\ should be configured with least privilege\" }, { \"selector\": \".metadata .annotations .\\\"container.apparmor.security. \\ beta.kubernetes.io/nginx\\\"\", \"reason\": \"Well defined AppArmor policies may provide greater \\ protection from unknown threats. WARNING: NOT PRODUCTION \\ READY\" }, { \"selector\": \"containers[] .resources .requests .cpu\",",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "\"reason\": \"Enforcing CPU requests aids a fair balancing of \\ resources across the cluster\" }, { \"selector\": \".metadata .annotations .\\\"container.seccomp.security. \\ alpha.kubernetes.io/pod\\\"\", \"reason\": \"Seccomp profiles set minimum privilege and secure against \\ unknown threats\" }, { \"selector\": \"containers[] .resources .limits .memory\", \"reason\": \"Enforcing memory limits prevents DOS via resource \\ exhaustion\" }, { \"selector\": \"containers[] .resources .limits .cpu\", \"reason\": \"Enforcing CPU limits prevents DOS via resource exhaustion\" }, { \"selector\": \"containers[] .securityContext .runAsNonRoot == true\", \"reason\": \"Force the running image to run as a non-root user to \\ ensure least privilege\" }, { \"selector\": \"containers[] .resources .requests .memory\", \"reason\": \"Enforcing memory requests aids a fair balancing of \\ resources across the cluster\" }, { \"selector\": \"containers[] .securityContext .capabilities .drop\", \"reason\": \"Reducing kernel capabilities available to a container \\ limits its attack surface\" }, { \"selector\": \"containers[] .securityContext .runAsUser -gt 10000\", \"reason\": \"Run as a high-UID user to avoid conflicts with the \\ host's user table\" }, { \"selector\": \"containers[] .securityContext .capabilities .drop | \\ index(\\\"ALL\\\")\", \"reason\": \"Drop all capabilities and add only those required to \\ reduce syscall attack surface\" } ] }",
      "content_length": 1370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "} ]\n\nA touched-up version of the original YAML manifest can be found in Example 6-15. Here, we incorporated some of the recommended changes proposed by Kubesec.\n\nExample 6-15. A Pod YAML manifest using improved security settings apiVersion: v1 kind: Pod metadata: name: kubesec-demo spec: containers: - name: kubesec-demo image: gcr.io/google-samples/node-hello:1.0 resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 20000 capabilities: drop: [\"ALL\"]\n\nExecuting the same Docker command against the changed Pod YAML manifest will render an improved score and reduce the number of advised messages:\n\n$ docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin \\ < pod-improved-kubesec-test.yaml [ { \"object\": \"Pod/kubesec-demo.default\", \"valid\": true, \"message\": \"Passed with a score of 9 points\", \"score\": 9, \"scoring\": { \"advise\": [",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "{ \"selector\": \".metadata .annotations .\\\"container.seccomp.security. \\ alpha.kubernetes.io/pod\\\"\", \"reason\": \"Seccomp profiles set minimum privilege and secure against \\ unknown threats\" }, { \"selector\": \".spec .serviceAccountName\", \"reason\": \"Service accounts restrict Kubernetes API access and should \\ be configured with least privilege\" }, { \"selector\": \".metadata .annotations .\\\"container.apparmor.security. \\ beta.kubernetes.io/nginx\\\"\", \"reason\": \"Well defined AppArmor policies may provide greater \\ protection from unknown threats. WARNING: NOT PRODUCTION \\ READY\" } ] } } ]\n\nScanning Images for Known Vulnerabilities\n\nOne of the top sources for logging and discovering security vulnerabilities is CVE Details. The page lists and ranks known vulnerabilities (CVEs) by score. Automated tooling can identify the software components embedded in a container image, compare those with the central vulnerabilities database, and flag issues by their severity.\n\nOne of the open source tools with this capability explicitly mentioned in the CKS curriculum is Trivy. Trivy can run in different modes of operation: as a command line tool, in a container, as a GitHub Action configurable in a continuous integration workflow, as a plugin for the IDE VSCode, and as a Kubernetes operator. For an overview of the available installation options and procedures, see the Trivy documentation. During the exam, you are not expected to install the tool. It will already be preconfigured for you. All you",
      "content_length": 1493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "need to understand is how to run it and how to interpret and fix the found vulnerabilities.\n\nSay you installed the command line implementation of Trivy. You can check the version of Trivy with the following command:\n\n$ trivy -v Version: 0.36.1 Vulnerability DB: Version: 2 UpdatedAt: 2022-12-13 12:07:14.884952254 +0000 UTC NextUpdate: 2022-12-13 18:07:14.884951854 +0000 UTC DownloadedAt: 2022-12-13 17:09:28.866739 +0000 UTC\n\nAs you can see in Figure 6-7, Trivy indicates the timestamp when a copy of known vulnerabilities has been downloaded from the central database. Trivy can scan container images in various forms. The subcommand image simply expects you to spell out the image name and tag, in this case python:3.4-alpine.\n\nFigure 6-7. Reporting generated by scanning a container image with Trivy\n\nThe most important information in the output consists of the library that contains a specific vulnerability, its severity, and the minimum version to",
      "content_length": 955,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "use that fixes the issue. Any found vulnerabilities with high or critical severity should be considered for a fix. If you do not have any control over the container image yourself, you can try to upgrade to a newer version.\n\nSummary\n\nKubernetes primary objective is to run applications in containers in a scalable and secure fashion. In this chapter, we looked at the process, best practices, and tooling that help to ensure that a container image is produced that is small in size and that ships with as few known security vulnerabilities as possible.\n\nWe reviewed some of the most efficient techniques to reduce the footprint of a container image to a minimum. Start by picking a small base image to start with. You can even go to the extreme and not provide a shell at all for additional gains in size reduction. If you are using Docker, you can leverage the multi-stage approach that lets you build the application inside of the container without bundling the compiler, runtime, and build tools necessary to achieve the goal.\n\nWhen consuming the container image in a Pod, make sure to only pull the container image from a trusted registry. It’s advisable to set up an in-house container registry to serve up container images, so that reaching out to public, internet-accessible registries becomes obsolete, to eliminate potential security risks. You can enforce the usage of a list of trusted container registries with the help of OPA Gatekeeper. Another measure of security can be enforced by using the SHA256 hash of a container image to validate its expected contents.\n\nThe process of building and scanning a container image can be incorporated into a CI/CD pipeline. Third-party tools can parse and analyze the resource files of your deployable artifacts even before you build them. We looked at Haskell Dockerfile Linter for Dockerfiles and Kubesec for Kubernetes manifests. The other use case that needs to be covered on security aspects is consuming an existing container image built by an entity external to you as a developer, or your company. Before running a container image in a",
      "content_length": 2094,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Kubernetes Pod, make sure to scan the contents for vulnerabilities. Trivy is one of those tools that can identify and report vulnerabilities in a container image to give you an idea of the security risks you are exposing yourself to in case you are planning to operate it in a container.\n\nExam Essentials\n\nBecome familiar with techniques that help with reducing the container image footprint.\n\nIn this section, we described some techniques for reducing the size of a container image when building it. I would suggest you read the best practices mentioned on the Docker web page and try to apply them to sample container images. Compare the size of the produced container image before and after applying a technique. You can try to challenge yourself by reducing a container image to the smallest size possible while at the same time avoiding the loss of crucial functionality.\n\nWalk through the process of governing where a container image can be resolved from.\n\nOPA Gatekeeper offers a way to define the registries users are allowed to resolve container images from. Set up the objects for the constraint template and constraint, and see if the rules apply properly for a Pod that defines a main application container, an init container, and an ephemeral container. To broaden your exposure, also look at other products in the Kubernetes spaces that provide similar functionality. One of those products is Kyverno.\n\nSign a container image and verify it using the hash.\n\nAfter building a container image, make sure to also create a digest for it. Publish the container image, as well as the digest, to a registry. Practice how to use the digest in a Pod definition and verify the behavior of Kubernetes upon pulling the container image.",
      "content_length": 1736,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Understand how to configure the ImagePolicyWebhook admission controller plugin.\n\nYou are not expected to write a backend for an ImagePolicyWebhook. That’s out of scope for the exam and requires knowledge of a programming language. You do need to understand how to enable the plugin in the API server configuration, though. I would suggest you practice the workflow even if you don’t have a running backend application available.\n\nKnow how to fix warnings produced by static analysis tools.\n\nThe CKS curriculum doesn’t prescribe a specific tool for analyzing Dockerfiles and Kubernetes manifests. During the exam, you may be asked to run a specific command that will produce a list of error and/or warning messages. Understand how to interpret the messages, and how to fix them in the relevant resource files.\n\nPractice the use of Trivy to identify and fix security vulnerabilities.\n\nThe FAQ of the CKS lists the documentation page for Trivy. Therefore, it’s fair to assume that Trivy may come up in one of the questions. You will need to understand the different ways to invoke Trivy to scan a container image. The produced report will give a you clear indication on what needs to be fixed and the severity of the found vulnerability. Given that you can’t modify the container image easily, you will likely be asked to flag Pods that run container images with known vulnerabilities.\n\nSample Exercises\n\nSolutions to these exercises are available in the Appendix.\n\n1. Have a look at the following Dockerfile. Can you identify possibilities for reducing the footprint of the produced container image?",
      "content_length": 1597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "FROM node:latest\n\nENV NODE_ENV development\n\nWORKDIR /app\n\nCOPY package.json .\n\nRUN npm install\n\nCOPY . .\n\nEXPOSE 3001\n\nCMD [\"node\", \"app.js\"]\n\nApply Dockerfile best practices to optimize the container image footprint. Run the docker build command before and after making optimizations. The resulting size of the container image should be smaller but the application should still be functioning.\n\n2. Install Kyverno in your Kubernetes cluster. You can find installation instructions in the documentation.\n\nApply the “Restrict Image Registries” policy described on the documentation page. The only allowed registry should be gcr.io. Usage of any other registry should be denied.\n\nCreate a Pod that defines the container image gcr.io/google- containers/busybox:1.27.2. Creation of the Pod should fail. Create another Pod using the container image busybox:1.27.2. Kyverno should allow the Pod to be created.\n\n3. Define a Pod using the container image nginx:1.23.3-alpine in the YAML manifest pod-validate-image.yaml. Retrieve the digest of the container image from Docker Hub. Validate the container image contents using the SHA256 hash. Create the Pod. Kubernetes should be able to successfully pull the container image.\n\n4. Use Kubesec to analyze the following contents of the YAML manifest file pod.yaml:",
      "content_length": 1303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "apiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: hello-world\n\nspec:\n\nsecurityContext:\n\nrunAsUser: 0\n\ncontainers:\n\nname: linux\n\nimage: hello-world:linux\n\nInspect the suggestions generated by Kubesec. Ignore the suggestions on seccomp and AppArmor. Fix the root cause of all messages so that another execution of the tool will not report any additional suggestions.\n\n5. Navigate to the directory app-a/ch06/trivy of the checked-out GitHub repository bmuschko/cks-study-guide. Execute the command kubectl apply -f setup.yaml. The command creates three different Pods in the namespace r61. From the command line, execute Trivy against the container images used by the Pods. Delete all Pods that have “CRITICAL” vulnerabilities. Which of the Pods are still running?\n\nOceanofPDF.com",
      "content_length": 771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Chapter 7. Monitoring, Logging, and Runtime Security\n\nThe last domain of the curriculum primarily deals with detecting suspicious activity on the host and container level in a Kubernetes cluster. We’ll first define the term behavior analytics and how it applies to the realm of Kubernetes. With the theory out of the way, we’ll bring in the tool called Falco that can detect intrusion scenarios.\n\nOnce a container has been started, its runtime environment can still be modified. For example, as an operator you could decide to shell into the container in order to manually install additional tools or write files to the container’s temporary filesystem. Modifying a container after it has been started can open doors to security risks. You will want to aim for creating immutable containers, containers that cannot be modified after they have been started. We’ll learn how to configure a Pod with the right settings to make its containers immutable.\n\nLast, we’ll talk about capturing logs for events that occur in a Kubernetes cluster. Those logs can be used for troubleshooting purposes on the cluster level, to reconstruct when and how the cluster configuration was changed such that it led to an undesired or broken runtime behavior. Log entries can also be used to trace an attack that may be happening right now as a means to enacting countermeasures.\n\nAt a high level, this chapter covers the following concepts:\n\nPerforming behavior analytics to detect malicious activities\n\nPerforming deep analytical investigation and identification\n\nEnsuring immutability of containers at runtime\n\nUsing audit logs to monitor access",
      "content_length": 1625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Performing Behavior Analytics\n\nApart from managing and upgrading a Kubernetes cluster, the administrator is in charge of keeping an eye on potentially malicious activity. While you can perform this task manually by logging into cluster nodes and observing host- and container-level processes, it is a horribly inefficient undertaking.\n\nBehavior analytics is the process of observing the cluster nodes for any activity that seems out of the ordinary. An automated process helps with filtering, recording, and alerting events of specific interest.\n\nScenario: A Kubernetes Administrator Can Observe Actions Taken by an Attacker\n\nAn attacker gained access to a container by opening a shell running on a worker node to launch additional attacks throughout the Kubernetes cluster. An administrator can’t easily detect this event by manually checking each and every container. Figure 7-1 illustrates the scenario.\n\nFigure 7-1. Malicious events recorded by behavior analytics tool\n\nThe administrator decided to take matters in their own hands by installing a behavior analytics tool. The tool will continuously monitor certain events and record them almost instantaneously. The administrator now has an efficient mechanism for detecting intrusions and can act upon them.\n\nAmong the behavior analytics tools relevant to the exam are Falco, Tracee, and Tetragon. In this book, we’ll only focus on Falco, as it is listed among the links of documentation pages available during the exam.",
      "content_length": 1475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Understanding Falco\n\nFalco helps with detecting threats by observing host- and container-level activity. Here are a few examples of events Falco could watch for:\n\nReading or writing files at specific locations in the filesystem\n\nOpening a shell binary for a container, such as /bin/bash to open a bash shell\n\nAn attempt to make a network call to undesired URLs\n\nFalco deploys a set of sensors that listen for the configured events and conditions. Each sensor consists of a set of rules that map an event to a data source. An alert is produced when a rule matches a specific event. Alerts will be sent to an output channel to record the event, such as standard output, a file, or an HTTPS endpoint. Falco allows for enabling more than one output channel simultaneously. Figure 7-2 shows Falco’s high-level architecture.\n\nFigure 7-2. Falco’s high-level architecture\n\nFalco is a tool with a lot of features and configuration options. We won’t be able to discuss all features in this book, but I would suggest you spend some time on understanding Falco’s high-level concepts.",
      "content_length": 1071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "Another great learning resource on Falco can be found on the Sysdig training portal webpage. “Falco 101” is a free video course that explains all the bells and whistles of the product. All you need to do to get started is sign up for an account. Moreover, I’d suggest giving the book Practical Cloud Native Security with Falco by Loris Degioanni and Leonardo Grasso (O’Reilly) a read. The content takes a beginner-friendly approach to learning Falco.\n\nInstalling Falco\n\nFalco can be installed as a binary on the host system or as a DaemonSet object in Kubernetes. You can safely assume that Falco has been preinstalled for you in the exam environment. For more information on the installation process, have a look at the relevant portion of the Falco documentation. The following steps briefly explain the installation of the binary on an Ubuntu machine. Falco needs to be installed on all worker nodes of a Kubernetes cluster. Be aware that those instructions may change with a future version of Falco.\n\nFirst, you need to trust the Falco GPG key, configure the Falco-specific apt repository, and update the package list:\n\n$ curl -s https://falco.org/repo/falcosecurity-packages.asc | apt-key add - $ echo \"deb https://download.falco.org/packages/deb stable main\" | tee -a \\ /etc/apt/sources.list.d/falcosecurity.list $ apt-get update -y\n\nYou then install the kernel header with the following command:\n\n$ apt-get -y install linux-headers-$(uname -r)\n\nLast, you need to install the Falco apt package with version 0.33.1:\n\n$ apt-get install -y falco=0.33.1\n\nFalco has been installed successfully and is running as a systemd service in the background. Run the following command to check on the status of the",
      "content_length": 1705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Falco service:\n\n$ sudo systemctl status falco ● falco.service - Falco: Container Native Runtime Security Loaded: loaded (/lib/systemd/system/falco.service; enabled; vendor preset: \\ enabled) Active: active (running) since Tue 2023-01-24 15:42:31 UTC; 43min ago Docs: https://falco.org/docs/ Main PID: 8718 (falco) Tasks: 12 (limit: 1131) Memory: 30.2M CGroup: /system.slice/falco.service └─8718 /usr/bin/falco --pidfile=/var/run/falco.pid\n\nThe Falco service should be in the “active” status. It is already monitoring events in your system.\n\nConfiguring Falco\n\nThe Falco service provides an operational environment for monitoring the system with a set of default rules. Those rules live in YAML files in the /etc/falco directory. The list of files and subdirectories in /etc/falco is as follows:\n\n$ tree /etc/falco /etc/falco ├── aws_cloudtrail_rules.yaml ├── falco.yaml ├── falco_rules.local.yaml ├── falco_rules.yaml ├── k8s_audit_rules.yaml ├── rules.available │ └── application_rules.yaml └── rules.d\n\nOf those files, I want to describe the high-level purpose of the most important ones.\n\nFalco configuration file",
      "content_length": 1116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "The file named falco.yaml is the configuration file for the Falco process. It controls the channel that will be notified in case of an alert. A channel could be standard output or a file. Furthermore, the file defines the minimum log level of alerts to include in logs, and how to configure the embedded web server used to implement a health check for the Falco process. Refer to “Falco Configuration Options” for a full reference on all available options.\n\nDefault rules The file named falco_rules.yaml defines a set of preinstalled rules. Falco considers those rules to be applied by default. Among them are checks for creating an alert when a shell to a container is opened or when a write operation is performed to a system directory. You can find other examples and their corresponding rule definitions on the “Rules Examples” page.\n\nCustom rules The file named falco_rules.local.yaml lets you define custom rules and override default rules. With a fresh installation of Falco, the file only contains commented-out rules to provide you with a starting point for adding your own rules. You can find guidance on writing custom rules in the Falco documentation.\n\nKubernetes-specific rules The file k8s_audit_rules.yaml defines Kubernetes-specific rules in addition to logging system call events. Typical examples are “log an event when a namespace is deleted” or “log an event when a Role or ClusterRole object is created.”\n\nApplying configuration changes\n\nModifying the contents of configuration files will not automatically propagate them to the Falco process. You need to restart the Falco service, as demonstrated by the following command:",
      "content_length": 1645,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "$ sudo systemctl restart falco\n\nNext up, we’ll trigger some of the events covered by Falco’s default rules. Each of those events will create an alert written to the configured channel. The initial setup of Falco routes messages to standard output.\n\nGenerating Events and Inspecting Falco Logs\n\nLet’s see Falco alerts in action. One of the default rules added by Falco creates an alert whenever a user tries to open a shell to a container. We’ll need to perform this activity to see a log entry for it. To achieve that, create a new Pod named nginx, open a bash shell to the container, and then exit out of the container:\n\n$ kubectl run nginx --image=nginx:1.23.3 pod/nginx created $ kubectl exec -it nginx -- bash root@nginx:/# exit\n\nIdentify the cluster node the Pod runs on by inspecting its runtime details:\n\n$ kubectl get pod nginx -o jsonpath='{.spec.nodeName}' kube-worker-1\n\nThis Pod is running on the cluster node named kube-worker-1. You will need to inspect the Falco logs on that machine to find the relevant log entry. You can inspect logged events by using the journalctl command directly on the kube-worker-1 cluster node. The following command searches for entries that contain the keyword falco:\n\n$ sudo journalctl -fu falco ... Jan 24 18:03:37 kube-worker-1 falco[8718]: 18:03:14.632368639: Notice A shell \\ was spawned in a container with an attached terminal (user=root user_loginuid=0 \\ nginx (id=18b247adb3ca) shell=bash parent=runc cmdline=bash pid=47773 \\ terminal=34816 container_id=18b247adb3ca image=docker.io/library/nginx)",
      "content_length": 1550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "You will find that additional rules will kick in if you try to modify the container state. Say you’d installed the Git package using apt in the nginx container:\n\nroot@nginx:/# apt update root@nginx:/# apt install git\n\nFalco added log entries for those system-level operations. The following output renders the alerts:\n\n$ sudo journalctl -fu falco Jan 24 18:55:48 ubuntu-focal falco[8718]: 18:55:05.173895727: Error Package \\ management process launched in container (user=root user_loginuid=0 \\ command=apt update pid=60538 container_id=18b247adb3ca container_name=nginx \\ image=docker.io/library/nginx:1.23.3) Jan 24 18:55:48 ubuntu-focal falco[8718]: 18:55:11.050925982: Error Package \\ management process launched in container (user=root user_loginuid=0 \\ command=apt install git-all pid=60823 container_id=18b247adb3ca \\ container_name=nginx image=docker.io/library/nginx:1.23.3) ...\n\nIn the next section, we’ll inspect the Falco rules that trigger the creation of alerts, and their syntax.\n\nUnderstanding Falco Rule File Basics\n\nA Falco rules file usually consists of three elements defined in YAML: rules, macros, and lists. You’ll need to understand those elements on a high level and know how to modify them to achieve a certain goal.\n\nCRAFTING YOUR OWN FALCO RULES\n\nDuring the exam, you will likely not have to craft your own Falco rules. You’ll be provided with an existing set of rules. If you want to dive deeper into Falco rules, have a look at the relevant documentation page.\n\nRule",
      "content_length": 1496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "A rule is the condition under which an alert should be generated. It also defines the output message of the alert. An output message can consist of a hard-coded message and incorporate built-in variables. The alert is recorded on the WARNING log level. Example 7-1 shows the YAML for a rule listening to events that try to access the machine’s camera from processes other than your traditional video-conferencing software, such as Skype or Webex.\n\nExample 7-1. A Falco rule that monitors camera access - rule: access_camera desc: a process other than skype/webex tries to access the camera condition: evt.type = open and fd.name = /dev/video0 and not proc.name in \\ (skype, webex) output: Unexpected process opening camera video device (command=%proc.cmdline) priority: WARNING\n\nMacro\n\nA macro is a reusable rule condition that helps with avoiding copy-pasting similar logic across multiple rules. Macros are useful if the rule file becomes lengthy and you want to improve on maintainability.\n\nSay you are in the process of simplifying a rules file. You notice that multiple rules use the same condition that listens for camera access. Example 7-2 shows how to break out the logic into a macro.\n\nExample 7-2. A Falco macro defining a reusable condition - macro: camera_process_access condition: evt.type = open and fd.name = /dev/video0 and not proc.name in \\ (skype, webex)\n\nThe macro can now be referenced by name in a rule definition, as shown in Example 7-3.\n\nExample 7-3. A Falco rule incorporating a macro - rule: access_camera desc: a process other than skype/webex tries to access the camera condition: camera_process_access output: Unexpected process opening camera video device (command=%proc.cmdline) priority: WARNING\n\nList",
      "content_length": 1735,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "A list is a collection of items that you can include in rules, macros, and other lists. Think of lists as an array in traditional programming languages. Example 7-4 creates a list of process names associated with video- conferencing software.\n\nExample 7-4. A Falco list - list: video_conferencing_software items: [skype, webex]\n\nExample 7-5 shows the usage of the list by name in a macro.\n\nExample 7-5. A Falco macro that uses a list - macro: camera_process_access condition: evt.type = open and fd.name = /dev/video0 and not proc.name in \\ (video_conferencing_software)\n\nDissecting an existing rule\n\nThe reason why Falco ships with default rules is to shorten the timespan to hit the ground running for a production cluster. Instead of having to come up with your own rules and the correct syntax, you can simply install Falco and benefit from best practices from the get-go.\n\nLet’s come back to one of the events we triggered in “Generating Events and Inspecting Falco Logs”. At the time of writing, I am using the Falco version 0.33.1. The rules file /etc/falco/falco_rules.yaml shipped with it contains a rule named “Terminal shell in container.” It observes the event of opening a shell to a container. You can easily find the rule by searching for a portion of the log message, e.g., “A shell was spawned in a container.” Example 7-6 shows the syntax of the rule definition, as well as the annotated portions of the YAML.\n\nExample 7-6. A Falco rule that monitors shell activity to a container - macro: spawned_process condition: (evt.type in (execve, execveat) and evt.dir=<)\n\nmacro: container condition: (container.id != host)\n\nmacro: container_entrypoint condition: (not proc.pname exists or proc.pname in (runc:[0:PARENT], \\",
      "content_length": 1733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "runc:[1:CHILD], runc, docker-runc, exe, docker-runc-cur))\n\nmacro: user_expected_terminal_shell_in_container_conditions condition: (never_true)\n\nrule: Terminal shell in container desc: A shell was used as the entrypoint/exec point into a container with an \\ attached terminal. condition: > spawned_process and container and shell_procs and proc.tty != 0 and container_entrypoint and not user_expected_terminal_shell_in_container_conditions output: > A shell was spawned in a container with an attached terminal (user=%user.name \\ user_loginuid=%user.loginuid %container.info shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline pid=%proc.pid \\ terminal=%proc.tty container_id=%container.id image=%container.image.repository) priority: NOTICE tags: [container, shell, mitre_execution] Defines a macro, a condition reusable across multiple rules referenced by name.\n\nSpecifies the name of the rule.\n\nThe aggregated condition composed of multiple macros.\n\nThe alerting message should the event happen. The message may use built-in fields to reference runtime value.\n\nIndicates how serious a violation of the rule it is.\n\nCategorizes the rule set into groups of related rules for ease of management.\n\nSometimes you may want to change an existing rule. The next section will explain how to best approach overriding default rules.",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Overriding Existing Rules\n\nInstead of modifying the rule definition directly in /etc/falco/falco_rules.yaml, I’d suggest you redefine the rule in /etc/falco/falco_rules.local.yaml. That’ll help with falling back to the original rule definition in case you want to get rid of the modifications or if you make any mistakes in the process. The rule definition needs to be changed on all worker nodes of the cluster.\n\nThe rule definition shown in Example 7-7 overrides the rule named “Terminal shell in container” by changing the priority to ALERT and the output to a new format by incorporating built-in fields.\n\nExample 7-7. The modified rule that monitors shell activity to a container - rule: Terminal shell in container desc: A shell was used as the entrypoint/exec point into a container with an \\ attached terminal. condition: > spawned_process and container and shell_procs and proc.tty != 0 and container_entrypoint and not user_expected_terminal_shell_in_container_conditions output: > Opened shell: %evt.time,%user.name,%container.name priority: ALERT tags: [container, shell, mitre_execution]\n\nSimplifies the log output rendered for a violation.\n\nTreats a violation of the rule with ALERT priority.\n\nAfter adding the rule to falco_rules.local.yaml, we need to let Falco pick up the changes. Restart the Falco service with the following command:\n\n$ sudo systemctl restart falco\n\nAs a result, any attempt that shells into a container will be logged with a different output format and priority, as the following shows:",
      "content_length": 1523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "$ sudo journalctl -fu falco ... Jan 24 21:19:13 kube-worker-1 falco[100017]: 21:19:13.961970887: Alert Opened \\ shell: 21:19:13.961970887,<NA>,nginx\n\nIn addition to overriding existing Falco rules, you can also define your own custom rules in /etc/falco/falco_rules.local.yaml. Writing custom rules is out of scope for this book, but you should find plenty of information on the topic in the Falco documentation.\n\nEnsuring Container Immutability\n\nContainers are mutable by default. After the container has been started, you can open a shell to it, install a patch for existing software, modify files, or make changes to its configuration. Mutable containers allow attackers to gain access to the container by downloading or installing malicious software.\n\nTo counteract the situation, make sure that the container is operated in an immutable state. That means preventing write operations to the container’s filesystem or even disallowing shell access to the container. If you need to make any substantial changes to the container, such as updating a dependency or incorporating a new application feature, you should release a new tag of the container image instead of manually modifying the container itself. You’d then assign the new tag of the container image to the Pod without having to modify the internals directly.\n\nScenario: An Attacker Installs Malicious Software\n\nFigure 7-3 illustrates a scenario where an attacker gains access to a container using stolen credentials. The attacker downloads and installs malicious software given that the container allows write operations to the root filesystem. The malicious software keeps monitoring activities in the container; for example, it could parse the application logs for sensitive information. It may then send the information to a server outside of the",
      "content_length": 1812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Kubernetes cluster. Consequently, the data is used as a means to log into other parts of the system.\n\nFigure 7-3. An attacker shells into a container and installs malicious software\n\nIn the next section, you’ll learn how to prevent the situation from happening by using a distroless container image, injecting configuration data with the help of a ConfigMap or Secret, and by configuring a read-only container filesystem. Those settings as a whole will make the container truly immutable.\n\nUsing a Distroless Container Image\n\nDistroless container images have become increasing popular in the world of containers. They only bundle your application and its runtime dependencies, while at the same time removing as much of the operating system as possible, e.g., shells, package managers, and libraries. Distroless container images are not just smaller in size; they are also more secure. An attacker cannot shell into the container, and therefore the filesystem cannot be misused to install malicious software. Using distroless container images is the first line of defense in creating an immutable container. We already covered distroless container images in “Picking a Base Image Small in Size”. Refer to that section for more information.\n\nConfiguring a Container with a ConfigMap or Secret\n\nIt is best practice to use the same container image for different deployment environments, even though their runtime configurations may be different.",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Any environment-specific configuration, such as credentials, and connection URLs to other parts of the system, should be externalized. In Kubernetes, you can inject configuration data into a container with the help of a ConfigMap or Secret as environment variables or files mounted via Volumes. Figure 7-4 shows the reuse of the same container image to configure a Pod in a development and production cluster. Configuration data specific to the environment is provided by a ConfigMap.\n\nFigure 7-4. Using the same container image across multiple environments\n\nAvoiding the creation of environment-specific container images simplifies the creation process, reduces the risk of introducing accidental security risks, and makes testing of the functionality easier. Injecting runtime values does not require changing the container after it has been started and therefore is key to making it immutable.\n\nWhen using Secrets as environment variables in a container, make sure to avoid accidentally logging the values to standard output, e.g., as a plain-text value when writing a log message. Anyone with access to the container logs would be able to parse them for Secrets values. As a spot check, identify the places in your application code where you use a Secret and assess their risk for exposure.\n\nTo brush up on creating, configuring, and consuming ConfigMaps and Secrets, revisit the Kubernetes documentation.\n\nConfiguring a Read-Only Container Root Filesystem",
      "content_length": 1460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "Another aspect of container immutability is to prevent write access to the container’s filesystem. You can configure this runtime behavior by assigning the value true to the attribute spec.containers[].securityContext.readOnlyRootFilesystem.\n\nThere are some applications that still require write access to fulfill their functional requirements. For example, nginx needs to write to the directories /var/run, /var/cache/nginx, and /usr/local/nginx. In combination with setting readOnlyRootFilesystem to true, you can declare Volumes that make those directories writable. Example 7-8 shows the YAML manifest of an immutable container running nginx.\n\nExample 7-8. A container disallowing write access to the root filesystem apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:1.21.6 securityContext: readOnlyRootFilesystem: true volumeMounts: - name: nginx-run mountPath: /var/run - name: nginx-cache mountPath: /var/cache/nginx - name: nginx-data mountPath: /usr/local/nginx volumes: - name: nginx-run emptyDir: {} - name: nginx-data emptyDir: {} - name: nginx-cache emptyDir: {}\n\nIdentify the filesystem read/write requirements of your application before creating a Pod. Configure write mountpaths with the help of Volumes. Any other filesystem path should become read-only.",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Using Audit Logs to Monitor Access\n\nIt’s imperative for a Kubernetes administrator to have a record of events that have occurred in the cluster. Those records can help detect an intrusion in real-time, or they can be used to track down configuration changes for troubleshooting purposes. Audit logs provide a chronological view of events received by the API server.\n\nScenario: An Administrator Can Monitor Malicious Events in Real Time\n\nFigure 7-5 shows the benefits of monitoring Kubernetes API events. In this scenario, the attacker tries to make calls to the API server. Events of interest have been captured by the audit log mechanism. The administrator can view those events at any time, identify intrusion attempts, and take countermeasure.\n\nFigure 7-5. An attacker monitored by observing audit logs\n\nWe only reviewed one of the use cases here, the one that applies to security concerns. The ability to trace company-internal API requests should not be underestimated. By reviewing audit logs, the administrator can provide guidance to application developers trying to create Kubernetes objects, or reconstruct configuration changes that may have led to faulty cluster behavior.\n\nUnderstanding Audit Logs\n\nKubernetes can store records for events triggered by end users for any requests made to the API server or for events emitted by the control plane",
      "content_length": 1357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "itself. Entries in the audit log exist in JSON Lines format and can consist of, but aren’t limited to, the following information:\n\nWhat event occurred?\n\nWho triggered the event?\n\nWhen was it triggered?\n\nWhich Kubernetes component handled the request?\n\nThe type of event and the corresponding request data to be recorded are defined by an audit policy. The audit policy is a YAML manifest specifying those rules and has to be provided to the API server process.\n\nThe audit backend is responsible for storing the recorded audit events, as defined by the audit policy. You have two configurable options for a backend:\n\nA log backend, which write the events to a file.\n\nA webhook backend, which sends the events to an external service via HTTP(S)—for example, for the purpose of integrating a centralized logging and monitoring system. Such a backend can help with debugging runtime issues like a crashed application.\n\nFigure 7-6 puts together all the pieces necessary to configure audit logging. The following sections will explain the details of configuring them.",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Figure 7-6. The high-level audit log architecture\n\nLet’s have a deeper look at the audit policy file and its configuration options.\n\nCreating the Audit Policy File\n\nThe audit policy file is effectively a YAML manifest for a Policy resource. Any event received by the API server is matched against the rules defined in the policy file in the order of definition. The event is logged with the declared audit level if a matching rule can be found. Table 7-1 lists all available audit levels.\n\nTable 7-1. Audit levels\n\nLevel\n\nEffect\n\nNone\n\nDo not log events matching this rule.\n\nMetadata\n\nOnly log request metadata for the event.\n\nRequest\n\nLog metadata and the request body for the event.\n\nRequestResponse\n\nLog metadata, request, and response body for the event.",
      "content_length": 758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "Example 7-9 shows an exemplary audit policy. The rules are specified as an array of items with the attribute named rules. Each rule declares a level, the resource type and API group it applies to, and an optional namespace.\n\nExample 7-9. Contents of an audit policy file apiVersion: audit.k8s.io/v1 kind: Policy omitStages: - \"RequestReceived\" rules: - level: RequestResponse resources: - group: \"\" resources: [\"pods\"] - level: Metadata resources: - group: \"\" resources: [\"pods/log\", \"pods/status\"]\n\nPrevents generating logs for all requests in the RequestReceived stage\n\nLogs Pod changes at RequestResponse level\n\nLogs specialized Pod events, e.g., log and status requests, at the Metadata level\n\nThe previous audit policy isn’t very extensive but should give you an impression of its format. Refer to the Kubernetes documentation for additional examples and more details.\n\nOnce the audit policy file has been created, it can be consumed by the API server process. Add the flag --audit-policy-file to the API server process in the file /etc/kubernetes/manifests/kube-apiserver.yaml. The value assigned to the parameter is the fully qualified path to the audit policy file.\n\nNext up, we’ll walk through the settings needed to configure audit logging for the API server for a file-based log backend and a webhook backend.",
      "content_length": 1320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Configuring a Log Backend\n\nTo set up a file-based log backend, you will need to add three pieces of configuration to the file /etc/kubernetes/manifests/kube- apiserver.yaml. The following list summarizes the configuration:\n\n1. Provide two flags to the API server process: the flag --audit- policy-file points to the audit policy file; the flag --audit- log-path points to the log output file.\n\n2. Add a Volume mountpath for the audit log policy file and the log output directory.\n\n3. Add a Volume definition to the host path for the audit log policy file and the log output directory.\n\nExample 7-10 shows the modified content of the API server configuration file.\n\nExample 7-10. Configuring the audit policy file and audit log file ... spec: containers: - command: - kube-apiserver - --audit-policy-file=/etc/kubernetes/audit-policy.yaml - --audit-log-path=/var/log/kubernetes/audit/audit.log ... volumeMounts: - mountPath: /etc/kubernetes/audit-policy.yaml name: audit readOnly: true - mountPath: /var/log/kubernetes/audit/ name: audit-log readOnly: false ... volumes: - name: audit hostPath: path: /etc/kubernetes/audit-policy.yaml type: File - name: audit-log hostPath:",
      "content_length": 1172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "path: /var/log/kubernetes/audit/ type: DirectoryOrCreate\n\nProvides the location of the policy file and log file to the API server process.\n\nMounts the policy file and the audit log directory to the given paths.\n\nDefines the Volumes for the policy file and the audit log directory.\n\nThe runtime behavior of the log backend can be further customized by passing additional flags to the API server process. For example, you can specify the maximum number of days to retain old audit log files by providing the flag --audit-log-maxage. Refer to the Kubernetes documentation to have a look at the complete list of flags.\n\nIt’s time to produce some log entries. The following kubectl command sends a request to the API server for creating a Pod named nginx:\n\n$ kubectl run nginx --image=nginx:1.21.6 pod/nginx created\n\nIn the previous step, we configured the audit log file at /var/log/kubernetes/audit/audit.log. Depending on the rules in the audit policy, the number of entries may be overwhelming, which makes finding a specific event hard. A simple way to filter configured events is by searching for the value audit.k8s.io/v1 assigned to the apiVersion attribute. The following command finds relevant log entries, one for the RequestResponse level, and another for the Metadata level:\n\n$ sudo grep 'audit.k8s.io/v1' /var/log/kubernetes/audit/audit.log ... {\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"RequestResponse\", \\ \"auditID\":\"285f4b99-951e-405b-b5de-6b66295074f4\",\"stage\":\"ResponseComplete\", \\ \"requestURI\":\"/api/v1/namespaces/default/pods/nginx\",\"verb\":\"get\", \\ \"user\":{\"username\":\"system:node:node01\",\"groups\":[\"system:nodes\", \\ \"system:authenticated\"]},\"sourceIPs\":[\"172.28.116.6\"],\"userAgent\": \\",
      "content_length": 1714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "\"kubelet/v1.26.0 (linux/amd64) kubernetes/b46a3f8\",\"objectRef\": \\ {\"resource\":\"pods\",\"namespace\":\"default\",\"name\":\"nginx\",\"apiVersion\":\"v1\"}, \\ \"responseStatus\":{\"metadata\":{},\"code\":200},\"responseObject\":{\"kind\":\"Pod\", \\ \"apiVersion\":\"v1\",\"metadata\":{\"name\":\"nginx\",\"namespace\":\"default\", \\ ... {\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"Metadata\",\"auditID\": \\ \"5c8e5ecc-0ce0-49e0-8ab2-368284f2f785\",\"stage\":\"ResponseComplete\", \\ \"requestURI\":\"/api/v1/namespaces/default/pods/nginx/status\",\"verb\":\"patch\", \\ \"user\":{\"username\":\"system:node:node01\",\"groups\":[\"system:nodes\", \\ \"system:authenticated\"]},\"sourceIPs\":[\"172.28.116.6\"],\"userAgent\": \\ \"kubelet/v1.26.0 (linux/amd64) kubernetes/b46a3f8\",\"objectRef\": \\ {\"resource\":\"pods\",\"namespace\":\"default\",\"name\":\"nginx\",\"apiVersion\":\"v1\", \\ \"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200}, \\ ...\n\nConfiguring a Webhook Backend\n\nConfiguring a webhook backend looks different from configuring a log backend. We need to tell the API server to send an HTTP(S) request to an external service instead of the file. The configuration to the external service, the webhook, and the credentials needed to authenticate are defined in a kubeconfig file, similarly to what we’ve done in “Configuring the ImagePolicyWebhook Admission Controller Plugin”.\n\nAdd the flag --audit-webhook-config-file to the API server process in the file /etc/kubernetes/manifests/kube-apiserver.yaml, and point it to the location of the kubeconfig file. The flag --audit-webhook- initial-backoff defines the time to wait after the initial request to the external service before retrying. You will still have to assign the flag -- audit-policy-file to point to the audit policy file.\n\nSummary\n\nMonitoring and logging events in a Kubernetes cluster is an important duty of every administrator. We used Falco to identify and filter security-related events. You learned about the purpose and syntax of the different configuration files and how to find relevant alerts in the logs.",
      "content_length": 2022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "In addition to employing behavior analytics tools, you will want to set up audit logging for requests reaching the API server. Audit logging records configured events to a backend, either to a file on the control plane node or to an external service via an HTTP(S) call. We worked through the process of enabling audit logging for the API server process.\n\nA sensible step toward more secure containers is to make them immutable. An immutable container only supports a read-only filesystem, so that a potential attacker cannot install malicious software. Mount a Volume if the application running inside of the container needs to write data. Use a distroless container image to lock out attackers from being able to shell into the container.\n\nExam Essentials\n\nPractice how to configure and operate Falco.\n\nFalco is definitely going to come up as a topic during the exam. You will need to understand how to read and modify a rule in a configuration file. I would suggest you browse through the syntax and options in more detail in case you need to write one yourself. The main entry point for running Falco is the command line tool. It’s fair to assume that it will have been preinstalled in the exam environment.\n\nKnow how to identify immutable containers.\n\nImmutable containers are a central topic to this exam domain. Understand how to set the spec.containers[].securityContext.readOnlyRootFilesystem attribute for a Pod and how to mount a Volume to a specific path in case a write operation is required by the container process.\n\nDeeply understand audit log configuration options.\n\nSetting up audit logging consists of two steps. For one, you need to understand the syntax and structure of an audit policy file. The other",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "aspect is how to configure the API server to consume the audit policy file, provide a reference to a backend, and mount the relevant filesystem Volumes. Make sure to practice all of those aspects hands-on.\n\nSample Exercises\n\n1. Navigate to the directory app-a/ch07/falco of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f. Falco is already running as a systemd service.\n\nInspect the process running in the existing Pod named malicious. Have a look at the Falco logs and see if a rule created a log for the process.\n\nReconfigure the existing rule that creates a log for the event by changing the output to <timestamp>,<username>,<container- id>. Find the changed log entry in the Falco logs.\n\nReconfigure Falco to write logs to the file at /var/logs/falco.log. Disable the standard output channel. Ensure that Falco appends new messages to the log file.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\n2. Navigate to the directory app-a/ch07/immutable-container of the checked-out GitHub repository bmuschko/cks-study-guide. Execute the command kubectl apply -f setup.yaml.\n\nInspect the Pod created by the YAML manifest in the default namespace. Make relevant changes to the Pod so that its container can be considered immutable.",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "3. Navigate to the directory app-a/ch07/audit-log of the checked-out GitHub repository bmuschko/cks-study-guide. Start up the VMs running the cluster using the command vagrant up. The cluster consists of a single control plane node named kube-control- plane and one worker node named kube-worker-1. Once done, shut down the cluster using vagrant destroy -f. Edit the existing audit policy file at /etc/kubernetes/audit/rules/audit-policy.yaml. Add a rule that logs events for ConfigMaps and Secrets at the Metadata level. Add another rule that logs events for Services at the Request level.\n\nConfigure the API server to consume the audit policy file. Logs should be written to the file /var/log/kubernetes/audit/logs/apiserver.log. Define a maximum number of five days to retain audit log files.\n\nEnsure that the log file has been created and contains at least one entry that matches the events configured.\n\nPrerequisite: This exercise requires the installation of the tools Vagrant and VirtualBox.\n\nOceanofPDF.com",
      "content_length": 1014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "Appendix. Answers to Review Questions",
      "content_length": 37,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Chapter 2, “Cluster Setup”\n\n1. Create a file with the name deny-egress-external.yaml for defining the network policy. The network policy needs to set the Pod selector to app=backend and define the Egress policy type. Make sure to allow the port 53 for the protocols UDP and TCP. The namespace selector for the egress policy needs to use {} to select all namespaces:\n\napiVersion: networking.k8s.io/v1\n\nkind: NetworkPolicy\n\nmetadata:\n\nname: deny-egress-external\n\nspec:\n\npodSelector:\n\nmatchLabels:\n\napp: backend\n\npolicyTypes:\n\nEgress\n\negress:\n\nto:\n\nnamespaceSelector: {}\n\nports:\n\nport: 53\n\nprotocol: UDP\n\nport: 53\n\nprotocol: TCP\n\nRun the apply command to instantiate the network policy object from the YAML file:\n\n$ kubectl apply -f deny-egress-external.yaml",
      "content_length": 755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "2. A Pod that does not match the label selection of the network policy can make a call to a URL outside of the cluster. In this case, the label assignment is app=frontend:\n\n$ kubectl run web --image=busybox:1.36.0 -l app=frontend --port=80 -\n\nit \\\n\n--rm --restart=Never -- wget http://google.com --timeout=5 --\n\ntries=1\n\nConnecting to google.com (142.250.69.238:80)\n\nConnecting to www.google.com (142.250.72.4:80)\n\nsaving to /'index.html'\n\nindex.html 100% |**| 13987 \\\n\n0:00:00 ETA\n\n/'index.html' saved\n\npod \"web\" deleted\n\n3. A Pod that does match the label selection of the network policy cannot make a call to a URL outside of the cluster. In this case, the label assignment is app=backend:\n\n$ kubectl run web --image=busybox:1.36.0 -l app=backend --port=80 -it\n\n\\\n\n--rm --restart=Never -- wget http://google.com --timeout=5 --\n\ntries=1\n\nwget: download timed out\n\npod \"web\" deleted\n\npod default/web terminated (Error)\n\n4. First, see if the Dashboard is already installed. You can check the namespace the Dashboard usually creates:\n\n$ kubectl get ns kubernetes-dashboard\n\nNAME STATUS AGE\n\nkubernetes-dashboard Active 109s",
      "content_length": 1122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "If the namespace does not exist, you can assume that the Dashboard has not been installed yet. Install it with the following command:\n\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/\\\n\ndashboard/v2.6.0/aio/deploy/recommended.yaml\n\nCreate the ServiceAccount, ClusterRole, and ClusterRoleBinding. Make sure that the ClusterRole only allows listing Deployment objects. The following YAML manifest has been saved in the file dashboard-observer-user.yaml:\n\napiVersion: v1\n\nkind: ServiceAccount\n\nmetadata:\n\nname: observer-user\n\nnamespace: kubernetes-dashboard\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\n\nkind: ClusterRole\n\nmetadata:\n\nannotations:\n\nrbac.authorization.kubernetes.io/autoupdate: \"true\"\n\nname: cluster-observer\n\nrules:\n\napiGroups:\n\n'apps'\n\nresources:\n\n'deployments'\n\nverbs:\n\nlist\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\n\nkind: ClusterRoleBinding",
      "content_length": 880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "metadata:\n\nname: observer-user\n\nroleRef:\n\napiGroup: rbac.authorization.k8s.io\n\nkind: ClusterRole\n\nname: cluster-observer\n\nsubjects:\n\nkind: ServiceAccount\n\nname: observer-user\n\nnamespace: kubernetes-dashboard\n\nCreate the objects with the following command:\n\n$ kubectl apply -f dashboard-observer-user.yaml\n\n5. Run the following command to create a token for the\n\nServiceAccount. The option --duration 0s ensures that the token will never expire. Copy the token that was rendered in the console output of the command:\n\n$ kubectl create token observer-user -n kubernetes-dashboard \\\n\n--duration 0s\n\neyJhbGciOiJSUzI1NiIsImtpZCI6Ik5lNFMxZ1...\n\nRun the proxy command and open the link http://localhost:8001/api/v1/namespaces/kubernetes- dashboard/services/https:kubernetes-dashboard:/proxy in a browser:\n\n$ kubectl proxy\n\nSelect the “Token” authentication method and paste the token you copied before. Sign into the Dashboard. You should see that only Deployment objects are listable (see Figure A-1).",
      "content_length": 995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "All other objects will say “There is nothing to display here.” Figure A-2 renders the list of Pods.\n\nFigure A-1. The Dashboard view of Deployments is allowed\n\nFigure A-2. The Dashboard view of Pods is not permitted\n\n6. Download the API server binary with the following command:\n\n$ curl -LO \"https://dl.k8s.io/v1.26.1/bin/linux/amd64/kube-apiserver\"\n\nNext, download the SHA256 file for the same binary, but a different version. The following command downloads the file for version 1.23.1:\n\n$ curl -LO \"https://dl.k8s.io/v1.23.1/bin/linux/amd64/\\\n\nkube-apiserver.sha256\"\n\nComparing the binary file with the checksum file results in a failure, as the versions do not match:\n\n$ echo \"$(cat kube-apiserver.sha256) kube-apiserver\" | shasum -a 256\n\n\\",
      "content_length": 743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "--check\n\nkube-apiserver: FAILED\n\nshasum: WARNING: 1 computed checksum did NOT match",
      "content_length": 83,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "Chapter 3, “Cluster Hardening”\n\n1. Create a private key using the openssl executable. Provide an expressive file name, such as jill.key. The -subj option provides the username (CN) and the group (O). The following command uses the username jill and the group named observer: $ openssl genrsa -out jill.key 2048\n\n$ openssl req -new -key jill.key -out jill.csr -subj \\\n\n\"/CN=jill/O=observer\"\n\nRetrieve the base64-encoded value of the CSR file content with the following command. You will need it when creating a the CertificateSigningRequest object in the next step:\n\n$ cat jill.csr | base64 | tr -d \"\\n\"\n\nLS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tL...\n\nThe following script creates a CertificateSigningRequest object:\n\n$ cat <<EOF | kubectl apply -f -\n\napiVersion: certificates.k8s.io/v1\n\nkind: CertificateSigningRequest\n\nmetadata:\n\nname: jill\n\nspec:\n\nrequest: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tL...\n\nsignerName: kubernetes.io/kube-apiserver-client\n\nexpirationSeconds: 86400\n\nusages:\n\nclient auth\n\nEOF",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Use the certificate approve command to approve the signing request and export the issued certificate:\n\n$ kubectl certificate approve jill\n\n$ kubectl get csr jill -o jsonpath={.status.certificate}| base64 \\\n\nd > jill.crt\n\nAdd the user to the kubeconfig file and add the context for the user. The cluster name used here is minikube. It might be different for your Kubernetes environment:\n\n$ kubectl config set-credentials jill --client-key=jill.key \\\n\n--client-certificate=jill.crt --embed-certs=true\n\n$ kubectl config set-context jill --cluster=minikube --user=jill\n\n2. Create the Role and RoleBinding. The following imperative commands assign the verbs get, list, and watch for Pods, ConfigMaps, and Secrets to the subject named observer of type group. The user jill is part of the group:\n\n$ kubectl create role observer --verb=create --verb=get --verb=list \\\n\n--verb=watch --resource=pods --resource=configmaps --\n\nresource=secrets\n\n$ kubectl create rolebinding observer-binding --role=observer \\\n\n--group=observer\n\n3. Switch to the user context:\n\n$ kubectl config use-context jill\n\nWe’ll pick one permitted operation, listing ConfigMap objects. The user is authorized to map the call:",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "$ kubectl get configmaps\n\nNAME DATA AGE\n\nkube-root-ca.crt 1 16m\n\nListing nodes won’t be authorized. The user does not have the appropriate permissions:\n\n$ kubectl get nodes\n\nError from server (Forbidden): nodes is forbidden: User \"jill\" cannot\n\n\\\n\nlist resource \"nodes\" in API group \"\" at the cluster scope\n\nSwitch back to the admin context:\n\n$ kubectl config use-context minikube\n\n4. Create the namespace t23:\n\n$ kubectl create namespace t23\n\nCreate the service account api-call in the namespace:\n\n$ kubectl create serviceaccount api-call -n t23\n\nDefine a YAML manifest file with the name pod.yaml. The contents of the file define a Pod that makes an HTTPS GET call to the API server to retrieve the list of Services in the default namespace:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: service-list\n\nnamespace: t23",
      "content_length": 817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "spec:\n\nserviceAccountName: api-call\n\ncontainers:\n\nname: service-list\n\nimage: alpine/curl:3.14\n\ncommand: ['sh', '-c', 'while true; do curl -s -k -m 5 \\\n\nH \"Authorization: Bearer $(cat /var/run/secrets/\\\n\nkubernetes.io/serviceaccount/token)\"\n\nhttps://kubernetes.\\\n\ndefault.svc.cluster.local/api/v1/namespaces/default/\\\n\nservices; sleep 10; done']\n\nCreate the Pod with the following command:\n\n$ kubectl apply -f pod.yaml\n\nCheck the logs of the Pod. The API call is not authorized, as shown in the following log output:\n\n$ kubectl logs service-list -n t23\n\n{\n\n\"kind\": \"Status\",\n\n\"apiVersion\": \"v1\",\n\n\"metadata\": {},\n\n\"status\": \"Failure\",\n\n\"message\": \"services is forbidden: User \\\"system:serviceaccount:t23\n\n\\\n\n:api-call\\\" cannot list resource \\\"services\\\" in API \\\n\ngroup \\\"\\\" in the namespace \\\"default\\\"\",\n\n\"reason\": \"Forbidden\",\n\n\"details\": {\n\n\"kind\": \"services\"\n\n},",
      "content_length": 866,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "\"code\": 403\n\n}\n\n5. Create the YAML manifest in the file clusterrole.yaml, as shown in the following:\n\napiVersion: rbac.authorization.k8s.io/v1\n\nkind: ClusterRole\n\nmetadata:\n\nname: list-services-clusterrole\n\nrules:\n\napiGroups: [\"\"]\n\nresources: [\"services\"]\n\nverbs: [\"list\"]\n\nReference the ClusterRole in a RoleBinding defined in the file rolebinding.yaml. The subject should list the service account api-call in the namespace t23:\n\napiVersion: rbac.authorization.k8s.io/v1\n\nkind: RoleBinding\n\nmetadata:\n\nname: serviceaccount-service-rolebinding\n\nsubjects:\n\nkind: ServiceAccount\n\nname: api-call\n\nnamespace: t23\n\nroleRef:\n\nkind: ClusterRole\n\nname: list-services-clusterrole\n\napiGroup: rbac.authorization.k8s.io\n\nCreate both objects from the YAML manifests:",
      "content_length": 753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "$ kubectl apply -f clusterrole.yaml\n\n$ kubectl apply -f rolebinding.yaml\n\nThe API call running inside of the container should now be authorized and be allowed to list the Service objects in the default namespace. As shown in the following output, the namespace currently hosts at least one Service object, the kubernetes.default Service:\n\n$ kubectl logs service-list -n t23\n\n{\n\n\"kind\": \"ServiceList\",\n\n\"apiVersion\": \"v1\",\n\n\"metadata\": {\n\n\"resourceVersion\": \"1108\"\n\n},\n\n\"items\": [\n\n{\n\n\"metadata\": {\n\n\"name\": \"kubernetes\",\n\n\"namespace\": \"default\",\n\n\"uid\": \"30eb5425-8f60-4bb7-8331-f91fe0999e20\",\n\n\"resourceVersion\": \"199\",\n\n\"creationTimestamp\": \"2022-09-08T18:06:52Z\",\n\n\"labels\": {\n\n\"component\": \"apiserver\",\n\n\"provider\": \"kubernetes\"\n\n},\n\n...\n\n}\n\n]\n\n}",
      "content_length": 750,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "6. Create the token for the service account using the following command:\n\n$ kubectl create token api-call -n t23\n\neyJhbGciOiJSUzI1NiIsImtpZCI6IjBtQkJzVWlsQjl...\n\nChange the existing Pod definition by deleting and recreating the live object. Add the attribute that disables automounting the token, as shown in the following:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: service-list\n\nnamespace: t23\n\nspec:\n\nserviceAccountName: api-call\n\nautomountServiceAccountToken: false\n\ncontainers:\n\nname: service-list\n\nimage: alpine/curl:3.14\n\ncommand: ['sh', '-c', 'while true; do curl -s -k -m 5 \\\n\nH \"Authorization: Bearer\n\neyJhbGciOiJSUzI1NiIsImtpZCI6Ij \\\n\nBtQkJzVWlsQjl\" https://kubernetes.default.svc.cluster.\n\n\\\n\nlocal/api/v1/namespaces/default/services; sleep 10; \\\n\ndone']\n\nThe API server will allow the HTTPS request performed with the token of the service account to be authenticated and authorized:\n\n$ kubectl logs service-list -n t23\n\n{",
      "content_length": 936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "\"kind\": \"ServiceList\",\n\n\"apiVersion\": \"v1\",\n\n\"metadata\": {\n\n\"resourceVersion\": \"81194\"\n\n},\n\n\"items\": [\n\n{\n\n\"metadata\": {\n\n\"name\": \"kubernetes\",\n\n\"namespace\": \"default\",\n\n\"uid\": \"30eb5425-8f60-4bb7-8331-f91fe0999e20\",\n\n\"resourceVersion\": \"199\",\n\n\"creationTimestamp\": \"2022-09-08T18:06:52Z\",\n\n\"labels\": {\n\n\"component\": \"apiserver\",\n\n\"provider\": \"kubernetes\"\n\n},\n\n...\n\n}\n\n]\n\n}\n\n7. The solution to this sample exercise requires a lot of manual steps. The following commands do not render their output.\n\nOpen an interactive shell to the control plane node using Vagrant:\n\n$ vagrant ssh kube-control-plane\n\nUpgrade kubeadm to version 1.26.1 and apply it:\n\n$ sudo apt-mark unhold kubeadm && sudo apt-get update && sudo apt-get\n\n\\\n\ninstall -y kubeadm=1.26.1-00 && sudo apt-mark hold kubeadm\n\n$ sudo kubeadm upgrade apply v1.26.1",
      "content_length": 820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "Drain the node, upgrade the kubelet and kubectl, restart the kubelet, and uncordon the node:\n\n$ kubectl drain kube-control-plane --ignore-daemonsets\n\n$ sudo apt-get update && sudo apt-get install -y \\\n\n--allow-change-held-packages kubelet=1.26.1-00 kubectl=1.26.1-00\n\n$ sudo systemctl daemon-reload\n\n$ sudo systemctl restart kubelet\n\n$ kubectl uncordon kube-control-plane\n\nThe version of the node should now say v1.26.1. Exit the node:\n\n$ kubectl get nodes\n\n$ exit\n\nOpen an interactive shell to the first worker node using Vagrant. Repeat all of the following steps for the worker node:\n\n$ vagrant ssh kube-worker-1\n\nUpgrade kubeadm to version 1.26.1 and apply it to the node:\n\n$ sudo apt-get update && sudo apt-get install -y \\\n\n--allow-change-held-packages kubeadm=1.26.1-00\n\n$ sudo kubeadm upgrade node\n\nDrain the node, upgrade the kubelet and kubectl, restart the kubelet, and uncordon the node:\n\n$ kubectl drain kube-worker-1 --ignore-daemonsets\n\n$ sudo apt-get update && sudo apt-get install -y \\\n\n--allow-change-held-packages kubelet=1.26.1-00 kubectl=1.26.1-00\n\n$ sudo systemctl daemon-reload",
      "content_length": 1100,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "$ sudo systemctl restart kubelet\n\n$ kubectl uncordon kube-worker-1\n\nThe version of the node should now say v1.26.1. Exit out of the node:\n\n$ kubectl get nodes\n\n$ exit",
      "content_length": 166,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Chapter 4, “System Hardening”\n\n1. Shell into the worker node with the following command:\n\n$ vagrant ssh kube-worker-1\n\nIdentify the process exposing port 21. One way to do this is by using the lsof command. The command that exposes the port is vsftpd:\n\n$ sudo lsof -i :21\n\nCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME\n\nvsftpd 10178 root 3u IPv6 56850 0t0 TCP *:ftp (LISTEN)\n\nAlternatively, you could also use the ss command, as shown in the following:\n\n$ sudo ss -at -pn '( dport = :21 or sport = :21 )'\n\nState Recv-Q Send-Q Local Address:Port \\\n\nPeer Address:Port Process\n\nLISTEN 0 32 *:21 \\\n\n:* users:((\"vsftpd\",pid=10178,fd=3))\n\nThe process vsftpd has been started as a service:\n\n$ sudo systemctl status vsftpd\n\nvsftpd.service - vsftpd FTP server Loaded: loaded (/lib/systemd/system/vsftpd.service; enabled; \\ vendor preset: enabled)\n\nActive: active (running) since Thu 2022-10-06 14:39:12 UTC; \\\n\n11min ago\n\nMain PID: 10178 (vsftpd)\n\nTasks: 1 (limit: 1131)",
      "content_length": 966,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Memory: 604.0K\n\nCGroup: /system.slice/vsftpd.service\n\n└─10178 /usr/sbin/vsftpd /etc/vsftpd.conf\n\nOct 06 14:39:12 kube-worker-1 systemd[1]: Starting vsftpd FTP\n\nserver...\n\nOct 06 14:39:12 kube-worker-1 systemd[1]: Started vsftpd FTP server.\n\nShut down the service and deinstall the package:\n\n$ sudo systemctl stop vsftpd\n\n$ sudo systemctl disable vsftpd\n\n$ sudo apt purge --auto-remove -y vsftpd\n\nChecking on the port, you will see that it is not listed anymore:\n\n$ sudo lsof -i :21\n\nExit out of the node:\n\n$ exit\n\n2. Shell into the worker node with the following command:\n\n$ vagrant ssh kube-worker-1\n\nCreate the AppArmor profile at /etc/apparmor.d/network-deny using the command sudo vim /etc/apparmor.d/network-deny. The contents of the file should look as follows:\n\n#include <tunables/global>\n\nprofile network-deny flags=(attach_disconnected) {\n\n#include <abstractions/base>",
      "content_length": 877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "network,\n\n}\n\nEnforce the AppArmor profile by running the following command:\n\n$ sudo apparmor_parser /etc/apparmor.d/network-deny\n\nYou cannot modify the existing Pod object in order to add the annotation for AppArmor. You will need to delete the object first. Write the definition of the Pod to a file:\n\n$ kubectl get pod -o yaml > pod.yaml\n\n$ kubectl delete pod network-call\n\nEdit the pod.yaml file to add the AppArmor annotation. For the relevant annotation, use the name of the container network-call as part of the key suffix and localhost/network-deny as the value. The suffix network-deny refers to the name of the AppArmor profile. The final content could look as follows after a little bit of cleanup:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: network-call\n\nannotations:\n\ncontainer.apparmor.security.beta.kubernetes.io/network-call: \\\n\nlocalhost/network-deny\n\nspec:\n\ncontainers:\n\nname: network-call\n\nimage: alpine/curl:3.14",
      "content_length": 933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "command: [\"sh\", \"-c\", \"while true; do ping -c 1 google.com; \\\n\nsleep 5; done\"]\n\nCreate the Pod from the manifest. After a couple of seconds, the Pod should transition into the “Running” status:\n\n$ kubectl create -f pod.yaml\n\n$ kubectl get pod network-call\n\nNAME READY STATUS RESTARTS AGE\n\nnetwork-call 1/1 Running 0 27s\n\nAppArmor prevents the Pod from making a network call. You can check the logs to verify:\n\n$ kubectl logs network-call\n\n...\n\nsh: ping: Permission denied\n\nsh: sleep: Permission denied\n\nExit out of the node:\n\n$ exit\n\n3. Shell into the worker node with the following command:\n\n$ vagrant ssh kube-worker-1\n\nCreate the target directory for the seccomp profiles:\n\n$ sudo mkdir -p /var/lib/kubelet/seccomp/profiles\n\nAdd the file audit.json in the directory /var/lib/kubelet/seccomp/profiles with the following content:",
      "content_length": 830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "{\n\n\"defaultAction\": \"SCMP_ACT_LOG\"\n\n}\n\nYou cannot modify the existing Pod object in order to add the seccomp configuration via the security context. You will need to delete the object first. Write the definition of the Pod to a file:\n\n$ kubectl get pod -o yaml > pod.yaml\n\n$ kubectl delete pod network-call\n\nEdit the pod.yaml file. Point the seccomp profile to the definition. The final content could look as follows after a little bit of cleanup:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: network-call\n\nspec:\n\nsecurityContext:\n\nseccompProfile:\n\ntype: Localhost\n\nlocalhostProfile: profiles/audit.json\n\ncontainers:\n\nname: network-call\n\nimage: alpine/curl:3.14\n\ncommand: [\"sh\", \"-c\", \"while true; do ping -c 1 google.com; \\\n\nsleep 5; done\"]\n\nsecurityContext:\n\nallowPrivilegeEscalation: false\n\nCreate the Pod from the manifest. After a couple of seconds, the Pod should transition into the “Running” status:",
      "content_length": 907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "$ kubectl create -f pod.yaml\n\n$ kubectl get pod network-call\n\nNAME READY STATUS RESTARTS AGE\n\nnetwork-call 1/1 Running 0 27s\n\nYou should be able to find log entries for syscalls, e.g., for the sleep command:\n\n$ sudo cat /var/log/syslog\n\nOct 6 16:25:06 ubuntu-focal kernel: [ 2114.894122] audit: type=1326\n\n\\\n\naudit(1665073506.099:23761): auid=4294967295 uid=0 gid=0 \\\n\nses=4294967295 pid=19226 comm=\"sleep\" exe=\"/bin/busybox\" \\\n\nsig=0 arch=c000003e syscall=231 compat=0 ip=0x7fc026adbf0b \\\n\ncode=0x7ffc0000\n\nExit out of the node:\n\n$ exit\n\nCreate the Pod definition in the file pod.yaml:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: sysctl-pod\n\nspec:\n\nsecurityContext:\n\nsysctls:\n\nname: net.core.somaxconn\n\nvalue: \"1024\"\n\nname: debug.iotrace\n\nvalue: \"1\"\n\ncontainers:",
      "content_length": 764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "name: nginx\n\nimage: nginx:1.23.1\n\nCreate the Pod and then check on the status. You will see that the status is “SysctlForbidden”:\n\n$ kubectl create -f pod.yaml\n\n$ kubectl get pods\n\nNAME READY STATUS RESTARTS AGE\n\nsysctl-pod 0/1 SysctlForbidden 0 4s\n\nThe event log will tell you more about the reasoning:\n\n$ kubectl describe pod sysctl-pod\n\n...\n\nEvents:\n\nType Reason Age From \\\n\nMessage\n\n---- ------ ---- ---- \\\n\n-------\n\nWarning SysctlForbidden 2m48s kubelet \\\n\nforbidden sysctl: \"net.core.somaxconn\" \\\n\nnot allowlisted",
      "content_length": 519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Chapter 5, “Minimize Microservice Vulnerabilities”\n\n1. Define the Pod with the security settings in the file busybox- security-context.yaml. You can find the content of the following YAML manifest:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: busybox-security-context\n\nspec:\n\nsecurityContext:\n\nrunAsUser: 1000\n\nrunAsGroup: 3000\n\nfsGroup: 2000\n\nvolumes:\n\nname: vol\n\nemptyDir: {}\n\ncontainers:\n\nname: busybox\n\nimage: busybox:1.28\n\ncommand: [\"sh\", \"-c\", \"sleep 1h\"]\n\nvolumeMounts:\n\nname: vol\n\nmountPath: /data/test\n\nsecurityContext:\n\nallowPrivilegeEscalation: false\n\nCreate the Pod with the following command:\n\n$ kubectl apply -f busybox-security-context.yaml\n\n$ kubectl get pod busybox-security-context",
      "content_length": 699,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "NAME READY STATUS RESTARTS AGE\n\nbusybox-security-context 1/1 Running 0 54s\n\nShell into the container and create the file. You will find that the file group is 2000, as defined by the security context:\n\n$ kubectl exec busybox-security-context -it -- /bin/sh\n\n/ $ cd /data/test\n\n/data/test $ touch hello.txt\n\n/data/test $ ls -l\n\ntotal 0\n\nrw-r--r-- 1 1000 2000 0 Nov 21 18:29 hello.txt\n\n/data/test $ exit\n\n2. Specify the namespace named audited in the file psa- namespace.yaml. Set the PSA label with baseline level and the warn mode:\n\napiVersion: v1\n\nkind: Namespace\n\nmetadata:\n\nname: audited\n\nlabels:\n\npod-security.kubernetes.io/warn: baseline\n\nCreate the namespace from the YAML manifest:\n\n$ kubectl apply -f psa-namespace.yaml\n\nYou can produce an error by using the following Pod configuration in the file psa-pod.yaml. The YAML manifest sets the attribute hostNetwork: true, which is not allowed for the baseline level:",
      "content_length": 921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "apiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: busybox\n\nnamespace: audited\n\nspec:\n\nhostNetwork: true\n\ncontainers:\n\nname: busybox\n\nimage: busybox:1.28\n\ncommand: [\"sh\", \"-c\", \"sleep 1h\"]\n\nCreating the Pod renders a warning message. The Pod will have been created nevertheless. You can prevent the creation of the Pod by configuring the PSA with the restricted level:\n\n$ kubectl apply -f psa-pod.yaml\n\nWarning: would violate PodSecurity \"baseline:latest\": host namespaces\n\n\\\n\n(hostNetwork=true)\n\npod/busybox created\n\n$ kubectl get pod busybox -n audited\n\nNAME READY STATUS RESTARTS AGE\n\nbusybox 1/1 Running 0 2m21s\n\n3. You can install Gatekeeper with the following command:\n\n$ kubectl apply -f https://raw.githubusercontent.com/open-policy-\n\nagent/\\\n\ngatekeeper/master/deploy/gatekeeper.yaml\n\nThe Gatekeeper library describes a ConstraintTemplate for defining replica limits. Inspect the YAML manifest described on the page. Apply the manifest with the following command:",
      "content_length": 965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "$ kubectl apply -f https://raw.githubusercontent.com/open-policy-\n\nagent/\\\n\ngatekeeper-library/master/library/general/replicalimits/template.yaml\n\nNow, define the Constraint with the YAML manifest in the file named replica-limits-constraint.yaml:\n\napiVersion: constraints.gatekeeper.sh/v1beta1\n\nkind: K8sReplicaLimits\n\nmetadata:\n\nname: replica-limits\n\nspec:\n\nmatch:\n\nkinds:\n\napiGroups: [\"apps\"]\n\nkinds: [\"Deployment\"]\n\nparameters:\n\nranges:\n\nmin_replicas: 3\n\nmax_replicas: 10\n\nCreate the Constraint with the following command:\n\n$ kubectl apply -f replica-limits-constraint.yaml\n\nYou can see that a Deployment can only be created if the provided number of replicas falls within the range of the Constraint:\n\n$ kubectl create deployment nginx --image=nginx:1.23.2 --replicas=15\n\nerror: failed to create deployment: admission webhook \\\n\n\"validation.gatekeeper.sh\" denied the request: [replica-limits] \\\n\nThe provided number of replicas is not allowed for deployment: nginx.\n\n\\\n\nAllowed ranges: {\"ranges\": [{\"max_replicas\": 10, \"min_replicas\": 3}]}",
      "content_length": 1043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "$ kubectl create deployment nginx --image=nginx:1.23.2 --replicas=7\n\ndeployment.apps/nginx created\n\n4. Configure encryption for etcd, as described in “Encrypting etcd Data”. Next, create a new Secret with the following imperative command:\n\n$ kubectl create secret generic db-credentials \\\n\n--from-literal=api-key=YZvkiWUkycvspyGHk3fQRAkt\n\nYou can check the encrypted value of the Secret stored in etcd with the following command:\n\n$ sudo ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt\n\n\\\n\n--cert=/etc/kubernetes/pki/etcd/server.crt --\n\nkey=/etc/kubernetes/pki/\\\n\netcd/server.key get /registry/secrets/default/db-credentials |\n\nhexdump -C\n\n5. Open an interactive shell to the worker node using Vagrant:\n\n$ vagrant ssh kube-worker-1\n\nDefine the RuntimeClass with the following YAML manifest. The contents have been stored in the file runtime-class.yaml:\n\napiVersion: node.k8s.io/v1\n\nkind: RuntimeClass\n\nmetadata:\n\nname: container-runtime-sandbox\n\nhandler: runsc\n\nCreate the RuntimeClass object:",
      "content_length": 1008,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "$ kubectl apply -f runtime-class.yaml\n\nAssign the name of the RuntimeClass to the Pod using the spec.runtimeClassName attribute. The nginx Pod has been defined in the file pod.yaml:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: nginx\n\nspec:\n\nruntimeClassName: container-runtime-sandbox\n\ncontainers:\n\nname: nginx\n\nimage: nginx:1.23.2\n\nCreate the Pod object. The Pod will transition into the status “Running”:\n\n$ kubectl apply -f pod.yaml\n\n$ kubectl get pod nginx\n\nNAME READY STATUS RESTARTS AGE\n\nnginx 1/1 Running 0 2m21s\n\nExit out of the node:\n\n$ exit",
      "content_length": 550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "Chapter 6, “Supply Chain Security”\n\n1. The initial container image built with the provided Dockerfile has a size of 998MB. You can produce and run the container image with the following commands. Run a quick curl command to see if the endpoint exposed by the application can be reached:\n\n$ docker build . -t node-app:0.0.1\n\n...\n\n$ docker images\n\nREPOSITORY TAG IMAGE ID CREATED SIZE\n\nnode-app 0.0.1 7ba99d4ba3af 3 seconds ago 998MB\n\n$ docker run -p 3001:3001 -d node-app:0.0.1\n\nc0c8a301eeb4ac499c22d10399c424e1063944f18fff70ceb5c49c4723af7969\n\n$ curl -L http://localhost:3001/\n\nHello World\n\nOne of the changes you can make is to avoid using a large base image. You could replace it with the alpine version of the node base image. Also, avoid pulling the latest image. Pick the Node.js version you actually want the application to run with. The following command uses a Dockerfile with the base image node:19-alpine, which reduces the container image size to 176MB:\n\n$ docker build . -t node-app:0.0.1\n\n...\n\n$ docker images\n\nREPOSITORY TAG IMAGE ID CREATED SIZE\n\nnode-app 0.0.1 ef2fbec41a75 2 seconds ago 176MB\n\n2. You can install Kyverno using Helm or by pointing to the YAML manifest available on the project’s GitHub repository. We’ll use the YAML manifest here:",
      "content_length": 1264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "$ kubectl create -f https://raw.githubusercontent.com/kyverno/\\\n\nkyverno/main/config/install.yaml\n\nSet up a YAML manifest file named restrict-image- registries.yaml. Add the following contents to the file. The manifest represents a ClusterPolicy that only allows the use of container images that start with gcr.io/. Make sure to assign the value Enforce to the attribute spec.validationFailureAction:\n\napiVersion: kyverno.io/v1\n\nkind: ClusterPolicy\n\nmetadata:\n\nname: restrict-image-registries\n\nannotations:\n\npolicies.kyverno.io/title: Restrict Image Registries\n\npolicies.kyverno.io/category: Best Practices, EKS Best Practices\n\npolicies.kyverno.io/severity: medium\n\npolicies.kyverno.io/minversion: 1.6.0\n\npolicies.kyverno.io/subject: Pod\n\npolicies.kyverno.io/description: >-\n\nImages from unknown, public registries can be of dubious\n\nquality \\\n\nand may not be scanned and secured, representing a high degree\n\nof \\\n\nrisk. Requiring use of known, approved registries helps reduce\n\n\\\n\nthreat exposure by ensuring image pulls only come from them.\n\nThis \\\n\npolicy validates that container images only originate from the\n\n\\\n\nregistry `eu.foo.io` or `bar.io`. Use of this policy requires \\\n\ncustomization to define your allowable registries.\n\nspec:\n\nvalidationFailureAction: Enforce",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "background: true\n\nrules:\n\nname: validate-registries\n\nmatch:\n\nany:\n\nresources:\n\nkinds:\n\nPod\n\nvalidate:\n\nmessage: \"Unknown image registry.\"\n\npattern:\n\nspec:\n\ncontainers:\n\nimage: \"gcr.io/*\"\n\nApply the manifest with the following command:\n\n$ kubectl apply -f restrict-image-registries.yaml\n\nRun the following commands to verify that the policy has become active. Any container image definition that doesn’t use the prefix gcr.io/ will be denied:\n\n$ kubectl run nginx --image=nginx:1.23.3\n\nError from server: admission webhook \"validate.kyverno.svc-fail\" \\\n\ndenied the request:\n\npolicy Pod/default/nginx for resource violation:\n\nrestrict-image-registries:\n\nvalidate-registries: 'validation error: Unknown image registry. \\\n\nrule validate-registries\n\nfailed at path /spec/containers/0/image/'",
      "content_length": 786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "$ kubectl run busybox --image=gcr.io/google-containers/busybox:1.27.2\n\npod/busybox created\n\n3. Find the SHA256 hash for the image nginx:1.23.3-alpine with the search functionality of Docker Hub. The search result will lead you to the tag of the image. On top of the page, you should find the digest sha256:c1b9fe3c0c015486cf1e4a0ecabe78d05864475e279638e 9713eb55f013f907f. Use the digest instead of the tag in the Pod definition. The result is the following YAML manifest: apiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: nginx\n\nspec:\n\ncontainers:\n\nname: nginx\n\nimage:\n\nnginx@sha256:c1b9fe3c0c015486cf1e4a0ecabe78d05864475e279638 \\\n\ne9713eb55f013f907f\n\nThe creation of the Pod should work:\n\n$ kubectl apply -f pod-validate-image.yaml\n\npod/nginx created\n\n$ kubectl get pods nginx\n\nNAME READY STATUS RESTARTS AGE\n\nnginx 1/1 Running 0 29s\n\nIf you modify the SHA256 hash in any form and try to recreate the Pod, then Kubernetes would not allow you to pull the image.",
      "content_length": 957,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "4. Running Kubesec in a Docker container results in a whole bunch of suggestions, as shown in the following output:\n\n$ docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin < pod.yaml\n\n[\n\n{\n\n\"object\": \"Pod/hello-world.default\",\n\n\"valid\": true,\n\n\"message\": \"Passed with a score of 0 points\",\n\n\"score\": 0,\n\n\"scoring\": {\n\n\"advise\": [\n\n{\n\n\"selector\": \"containers[] .securityContext .capabilities \\\n\n.drop | index(\\\"ALL\\\")\",\n\n\"reason\": \"Drop all capabilities and add only those \\\n\nrequired to reduce syscall attack surface\"\n\n},\n\n{\n\n\"selector\": \"containers[] .resources .requests .cpu\",\n\n\"reason\": \"Enforcing CPU requests aids a fair balancing \\\n\nof resources across the cluster\"\n\n},\n\n{\n\n\"selector\": \"containers[] .securityContext .runAsNonRoot \\\n\n== true\",\n\n\"reason\": \"Force the running image to run as a non-root \\\n\nuser to ensure least privilege\"\n\n},\n\n{\n\n\"selector\": \"containers[] .resources .limits .cpu\",\n\n\"reason\": \"Enforcing CPU limits prevents DOS via resource \\\n\nexhaustion\"\n\n},",
      "content_length": 981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "{\n\n\"selector\": \"containers[] .securityContext .capabilities \\\n\n.drop\",\n\n\"reason\": \"Reducing kernel capabilities available to a \\\n\ncontainer limits its attack surface\"\n\n},\n\n{\n\n\"selector\": \"containers[] .resources .requests .memory\",\n\n\"reason\": \"Enforcing memory requests aids a fair balancing\n\n\\\n\nof resources across the cluster\"\n\n},\n\n{\n\n\"selector\": \"containers[] .resources .limits .memory\",\n\n\"reason\": \"Enforcing memory limits prevents DOS via\n\nresource \\\n\nexhaustion\"\n\n},\n\n{\n\n\"selector\": \"containers[] .securityContext \\\n\n.readOnlyRootFilesystem == true\",\n\n\"reason\": \"An immutable root filesystem can prevent\n\nmalicious \\\n\nbinaries being added to PATH and increase attack\n\n\\\n\ncost\"\n\n},\n\n{\n\n\"selector\": \".metadata .annotations .\\\"container.seccomp. \\\n\nsecurity.alpha.kubernetes.io/pod\\\"\",\n\n\"reason\": \"Seccomp profiles set minimum privilege and\n\nsecure \\\n\nagainst unknown threats\"\n\n},",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "{\n\n\"selector\": \".metadata .annotations .\\\"container.apparmor.\n\n\\\n\nsecurity.beta.kubernetes.io/nginx\\\"\",\n\n\"reason\": \"Well defined AppArmor policies may provide\n\ngreater \\\n\nprotection from unknown threats. WARNING: NOT \\\n\nPRODUCTION READY\"\n\n},\n\n{\n\n\"selector\": \"containers[] .securityContext .runAsUser -gt \\\n\n10000\",\n\n\"reason\": \"Run as a high-UID user to avoid conflicts with \\\n\nthe host's user table\"\n\n},\n\n{\n\n\"selector\": \".spec .serviceAccountName\",\n\n\"reason\": \"Service accounts restrict Kubernetes API access\n\n\\\n\nand should be configured with least privilege\"\n\n}\n\n]\n\n}\n\n}\n\n]\n\nThe fixed-up YAML manifest could look like this:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: hello-world\n\nspec:\n\nserviceAccountName: default",
      "content_length": 717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "containers:\n\nname: linux\n\nimage: hello-world:linux\n\nresources:\n\nrequests:\n\nmemory: \"64Mi\"\n\ncpu: \"250m\"\n\nlimits:\n\nmemory: \"128Mi\"\n\ncpu: \"500m\"\n\nsecurityContext:\n\nreadOnlyRootFilesystem: true\n\nrunAsNonRoot: true\n\nrunAsUser: 20000\n\ncapabilities:\n\ndrop: [\"ALL\"]\n\n5. Executing the kubectl apply command against the existing setup.yaml manifest will create the Pods named backend, loop, and logstash in the namespace r61:\n\n$ kubectl apply -f setup.yaml\n\nnamespace/r61 created\n\npod/backend created\n\npod/loop created\n\npod/logstash created\n\nYou can check on them with the following command:\n\n$ kubectl get pods -n r61\n\nNAME READY STATUS RESTARTS AGE\n\nbackend 1/1 Running 0 115s\n\nlogstash 1/1 Running 0 115s\n\nloop 1/1 Running 0 115s",
      "content_length": 722,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Check the images of each Pod in the namespace r61 using the kubectl describe command. The images used are bmuschko/nodejs-hello-world:1.0.0, alpine:3.13.4, and elastic/logstash:7.13.3:\n\n$ kubectl describe pod backend -n r61\n\n...\n\nContainers:\n\nhello:\n\nContainer ID: docker://eb0bdefc75e635d03b625140d1e \\\n\nb229ca2db7904e44787882147921c2bd9c365\n\nImage: bmuschko/nodejs-hello-world:1.0.0\n\n...\n\nUse the Trivy executable to check vulnerabilities for all images:\n\n$ trivy image bmuschko/nodejs-hello-world:1.0.0\n\n$ trivy image alpine:3.13.4\n\n$ trivy image elastic/logstash:7.13.3\n\nIf you look closely at the list of vulnerabilities, you will find that all images contain issues with “CRITICAL” severity. As a result, delete all Pods:\n\n$ kubectl delete pod backend -n r61\n\n$ kubectl delete pod logstash -n r61\n\n$ kubectl delete pod loop -n r61",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "Chapter 7, “Monitoring, Logging, and Runtime Security”\n\n1. Shell into the worker node with the following command:\n\n$ vagrant ssh kube-worker-1\n\nInspect the command and arguments of the running Pod named malicious. You will see that it tries to append a message to the file /etc/threat:\n\n$ kubectl get pod malicious -o jsonpath='{.spec.containers[0].args}'\n\n...\n\nspec:\n\ncontainers:\n\nargs:\n\n/bin/sh\n\n-c\n\nwhile true; do echo \"attacker intrusion\" >> /etc/threat; \\\n\nsleep 5; done\n\n...\n\nOne of Falco’s default rules monitors file operations that try to write to the /etc directory. You can find a message for every write attempt in standard output:\n\n$ sudo journalctl -fu falco\n\nJan 24 23:40:18 kube-worker-1 falco[8575]: 23:40:18.359740123: Error\n\n\\\n\nFile below /etc opened for writing (user=<NA> user_loginuid=-1 \\\n\ncommand=sh -c while true; do echo \"attacker intrusion\" >>\n\n/etc/threat; \\\n\nsleep 5; done pid=9763 parent=<NA> pcmdline=<NA> file=/etc/threat \\",
      "content_length": 955,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "program=sh gparent=<NA> ggparent=<NA> gggparent=<NA> \\\n\ncontainer_id=e72a6dbb63b8 image=docker.io/library/alpine)\n\n...\n\nFind the rule that produces the message in /etc/falco/falco_rules.yaml by searching for the string “etc opened for writing.” The rule looks as follows:\n\nrule: Write below etc\n\ndesc: an attempt to write to any file below /etc\n\ncondition: write_etc_common\n\noutput: \"File below /etc opened for writing (user=%user.name \\\n\nuser_loginuid=%user.loginuid command=%proc.cmdline \\\n\npid=%proc.pid parent=%proc.pname pcmdline=%proc.pcmdline \\\n\nfile=%fd.name program=%proc.name gparent=%proc.aname[2] \\\n\nggparent=%proc.aname[3] gggparent=%proc.aname[4] \\\n\ncontainer_id=%container.id\n\nimage=%container.image.repository)\"\n\npriority: ERROR\n\ntags: [filesystem, mitre_persistence]\n\nCopy the rule to the file /etc/falco/falco_rules.local.yaml and modify the output definition, as follows:\n\nrule: Write below etc\n\ndesc: an attempt to write to any file below /etc\n\ncondition: write_etc_common\n\noutput: \"%evt.time,%user.name,%container.id\"\n\npriority: ERROR\n\ntags: [filesystem, mitre_persistence]\n\nRestart the Falco service, and find the changed output in the Falco logs:",
      "content_length": 1169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "$ sudo systemctl restart falco\n\n$ sudo journalctl -fu falco\n\nJan 24 23:48:18 kube-worker-1 falco[17488]: 23:48:18.516903001: \\\n\nError 23:48:18.516903001,<NA>,e72a6dbb63b8\n\n...\n\nEdit the file /etc/falco/falco.yaml to change the output channel. Disable standard output, enable file output, and point the file_output attribute to the file /var/log/falco.log. The resulting configuration will look like the following:\n\nfile_output:\n\nenabled: true\n\nkeep_alive: false\n\nfilename: /var/log/falco.log\n\nstdout_output:\n\nenabled: false\n\nThe log file will now append Falco log:\n\n$ sudo tail -f /var/log/falco.log\n\n00:10:30.425084165: Error 00:10:30.425084165,<NA>,e72a6dbb63b8\n\n...\n\nExit out of the VM:\n\n$ exit\n\n2. Create the Pod named hash from the setup.yaml file. The command running in its container appends a hash to a file at /var/config/hash.txt in an infinite loop:",
      "content_length": 860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "$ kubectl apply -f setup.yaml\n\npod/hash created\n\n$ kubectl get pod hash\n\nNAME READY STATUS RESTARTS AGE\n\nhash 1/1 Running 0 27s\n\n$ kubectl exec -it hash -- /bin/sh\n\n/ # ls /var/config/hash.txt\n\n/var/config/hash.txt\n\nTo make the container immutable, you will have to add configuration to the existing Pod definition. You have to set the root filesystem to read-only access and mount a Volume to the path /var/config to allow writing to the file named hash.txt. The resulting YAML manifest could look as follows:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: hash\n\nspec:\n\ncontainers:\n\nname: hash\n\nimage: alpine:3.17.1\n\nsecurityContext:\n\nreadOnlyRootFilesystem: true\n\nvolumeMounts:\n\nname: hash-vol\n\nmountPath: /var/config\n\ncommand: [\"sh\", \"-c\", \"if [ ! -d /var/config ]; then mkdir -p \\\n\n/var/config; fi; while true; do echo $RANDOM | md5sum \\\n\n| head -c 20 >> /var/config/hash.txt; sleep 20; done\"]\n\nvolumes:\n\nname: hash-vol\n\nemptyDir: {}",
      "content_length": 935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "3. Shell into the control plane node with the following command:\n\n$ vagrant ssh kube-control-plane\n\nEdit the existing audit policy file at /etc/kubernetes/audit/rules/audit-policy.yaml. Add the rules asked about in the instructions. The content of the final audit policy file could look as follows:\n\napiVersion: audit.k8s.io/v1\n\nkind: Policy\n\nomitStages:\n\n\"RequestReceived\"\n\nrules:\n\nlevel: RequestResponse\n\nresources:\n\ngroup: \"\"\n\nresources: [\"pods\"]\n\nlevel: Metadata\n\nresources:\n\ngroup: \"\"\n\nresources: [\"secrets\", \"configmaps\"]\n\nlevel: Request\n\nresources:\n\ngroup: \"\"\n\nresources: [\"services\"]\n\nConfigure the API server to consume the audit policy file by editing the file /etc/kubernetes/manifests/kube- apiserver.yaml. Provide additional options, as requested. The relevant configuration needed is as follows:\n\n...\n\nspec:",
      "content_length": 821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "containers:\n\ncommand:\n\nkube-apiserver\n\n--audit-policy-file=/etc/kubernetes/audit/rules/audit-\n\npolicy.yaml\n\n--audit-log-path=/var/log/kubernetes/audit/logs/apiserver.log\n\n--audit-log-maxage=5\n\n...\n\nvolumeMounts:\n\nmountPath: /etc/kubernetes/audit/rules/audit-policy.yaml\n\nname: audit\n\nreadOnly: true\n\nmountPath: /var/log/kubernetes/audit/logs/\n\nname: audit-log\n\nreadOnly: false\n\n...\n\nvolumes:\n\nname: audit\n\nhostPath:\n\npath: /etc/kubernetes/audit/rules/audit-policy.yaml\n\ntype: File\n\nname: audit-log\n\nhostPath:\n\npath: /var/log/kubernetes/audit/logs/\n\ntype: DirectoryOrCreate\n\nOne of the logged resources is a ConfigMap on the Metadata level. The following command creates an exemplary ConfigMap object:\n\n$ kubectl create configmap db-user --from-literal=username=tom\n\nconfigmap/db-user created\n\nThe audit log file will now contain an entry for the event:",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "$ sudo cat /var/log/kubernetes/audit/logs/apiserver.log\n\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"Metadata\", \\\n\n\"auditID\":\"1fbb409a-3815-4da8-8a5e-d71c728b98b1\",\"stage\": \\\n\n\"ResponseComplete\",\"requestURI\":\"/api/v1/namespaces/default/configmap\n\ns? \\\n\nfieldManager=kubectl-create\\u0026fieldValidation=Strict\",\"verb\": \\\n\n\"create\",\"user\":{\"username\":\"kubernetes-admin\",\"groups\": \\\n\n[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\": \\\n\n[\"192.168.56.10\"], \"userAgent\":\"kubectl/v1.24.4 (linux/amd64) \\\n\nkubernetes/95ee5ab\", \"objectRef\":{\"resource\":\"configmaps\", \\\n\n\"namespace\":\"default\", \"name\":\"db-user\",\"apiVersion\":\"v1\"}, \\\n\n\"responseStatus\":{\"metadata\": {},\"code\":201}, \\\n\n\"requestReceivedTimestamp\":\"2023-01-25T18:57:51.367219Z\", \\\n\n\"stageTimestamp\":\"2023-01-25T18:57:51.372094Z\",\"annotations\": \\\n\n{\"authorization.k8s.io/decision\":\"allow\", \\\n\n\"authorization.k8s.io/reason\":\"\"}}\n\nExit out of the VM:\n\n$ exit\n\nOceanofPDF.com",
      "content_length": 941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Index\n\nA\n\naa-complain command, Setting a custom profile\n\naa-enforce command, Setting a custom profile\n\naa-status command, Understanding profiles\n\naccess\n\nmonitoring using audit logs, Using Audit Logs to Monitor Access- Configuring a Webhook Backend\n\nrestricting to API Server, Restricting Access to the API Server- Creating a Secret for a service account\n\nadduser command, Adding a user\n\nadministration privileges, creating users with, Creating a User with Administration Privileges\n\nadmission control, as stage in request processing, Processing a Request\n\nAKS Application Gateway Ingress Controller, Creating an Ingress with TLS Termination\n\nanonymous access, Anonymous access\n\nanswers to review questions\n\nbehavior analytics, Chapter 7, “Monitoring, Logging, and Runtime Security”\n\ncluster hardening, Chapter 3, “Cluster Hardening”-Chapter 3, “Cluster Hardening”",
      "content_length": 864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "cluster setup, Chapter 2, “Cluster Setup”-Chapter 2, “Cluster Setup”\n\nmicroservice vulnerabilities, Chapter 5, “Minimize Microservice Vulnerabilities”-Chapter 5, “Minimize Microservice Vulnerabilities”\n\nsupply chain security, Chapter 6, “Supply Chain Security”-Chapter 6, “Supply Chain Security”\n\nsystem hardening, Chapter 4, “System Hardening”-Chapter 4, “System Hardening”\n\nAPI server\n\nconnecting to, Connecting to the API Server-Access with a client certificate\n\nrestricting access to, Restricting Access to the API Server-Creating a Secret for a service account\n\nAppArmor, Involved External Tools, Using AppArmor-Applying a profile to a container\n\napply command, Creating the RoleBinding, Applying the default container runtime profile to a container, Whitelisting Allowed Image Registries with OPA GateKeeper\n\napt purge command, Removing Unwanted Packages\n\nattack surface, reducing, Using a Multi-Stage Approach for Building Container Images\n\naudit backend, Understanding Audit Logs\n\naudit logs, Monitoring, Logging, and Runtime Security, Using Audit Logs to Monitor Access-Configuring a Webhook Backend\n\naudit mode, Understanding Pod Security Admission (PSA)\n\naudit policy, Understanding Audit Logs",
      "content_length": 1204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "authentication, as stage in request processing, Processing a Request\n\nauthorization, as stage in request processing, Processing a Request\n\nautomounting, disabling for service account tokens, Disabling automounting of a service account token\n\nB\n\nbase images\n\nminimizing footprint for, Minimizing the Base Image Footprint-Using Container Image Optimization Tools\n\nselecting, Picking a Base Image Small in Size\n\nbaseline level, Understanding Pod Security Admission (PSA)\n\nbehavior analytics\n\nanswers to review questions, Chapter 7, “Monitoring, Logging, and Runtime Security”\n\ndefined, Monitoring, Logging, and Runtime Security\n\nensuring container immutability, Ensuring Container Immutability\n\nperforming, Performing Behavior Analytics-Overriding Existing Rules\n\nsample exercises, Exam Essentials\n\nusing audit logs to monitor access, Using Audit Logs to Monitor Access-Configuring a Webhook Backend\n\nbinaries, verifying against hash, Verifying a Binary Against Hash\n\nblog (Kubernetes), Documentation\n\nBurns, Brendan, Managing Kubernetes, Interacting with the Kubernetes API",
      "content_length": 1071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "C\n\nCA (certificate authority), Adopting mTLS in Kubernetes\n\nCalico, Adopting mTLS in Kubernetes\n\ncalling Ingress, Calling the Ingress\n\ncandidate skills, Candidate Skills\n\ncertificate approve command, Creating and approving a CertificateSigningRequest\n\nCertificateSigningRequest, creating and approving, Creating and approving a CertificateSigningRequest\n\ncertification, learning path for, Kubernetes Certification Learning Path\n\nCertified Kubernetes Administrator (CKA) Study Guide (Muschko), Who This Book Is For, Candidate Skills, Using Network Policies to Restrict Pod- to-Pod Communication, Creating an Ingress with TLS Termination, Performing the Upgrade Process, Managing Secrets\n\nchmod command, Changing file permissions\n\nchown command, Changing file ownership\n\nCilium, Adopting mTLS in Kubernetes\n\nCIS (Center for Internet Security), Applying Kubernetes Component Security Best Practices\n\nCIS benchmark, for Ubuntu Linux, Minimizing the Host OS Footprint\n\nCKA (Certified Kubernetes Administrator), Preface, Certified Kubernetes Administrator (CKA)\n\nCKAD (Certified Kubernetes Application Developer), Preface, Certified Kubernetes Application Developer (CKAD)",
      "content_length": 1166,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "CKS (Certified Kubernetes Security Specialist), Preface, Certified Kubernetes Security Specialist (CKS)\n\nclient certificates, access with, Access with a client certificate\n\ncluster hardening, Cluster Hardening-Sample Exercises\n\nabout, Cluster Hardening, Cluster Hardening\n\nanswers to review questions, Chapter 3, “Cluster Hardening”-Chapter 3, “Cluster Hardening”\n\ninteracting with Kubernetes API, Interacting with the Kubernetes API- Access with a client certificate\n\nrestricting access to API server, Restricting Access to the API Server- Creating a Secret for a service account\n\nsample exercises, Sample Exercises\n\nupdating Kubernetes, Updating Kubernetes Frequently-Summary\n\ncluster setup, Cluster Setup-Sample Exercises\n\nabout, Cluster Setup, Cluster Setup\n\nanswers to review questions, Chapter 2, “Cluster Setup”-Chapter 2, “Cluster Setup”\n\napplying Kubernetes component best security practices, Applying Kubernetes Component Security Best Practices-Fixing Detected Security Issues\n\ncreating ingress with TLS termination, Creating an Ingress with TLS Termination-Calling the Ingress\n\nprotecting GUI elements, Protecting GUI Elements-Avoiding Insecure Configuration Arguments",
      "content_length": 1180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "protecting node metadata and endpoints, Protecting Node Metadata and Endpoints-Protecting Metadata Server Access with Network Policies\n\nrestricting pod-to-pod communication using network policies, Using Network Policies to Restrict Pod-to-Pod Communication-Allowing Fine-Grained Incoming Traffic\n\nsample exercises, Sample Exercises\n\nverifying Kubernetes platform binaries, Verifying Kubernetes Platform Binaries\n\nClusterIP Service type, Scenario: An Attacker Gains Access to the Dashboard Functionality\n\nClusterRole, Creating a User with Restricted Privileges, Creating the ClusterRole\n\nClusterRoleBinding object, Creating a User with Administration Privileges\n\nCMD command, Reducing the Number of Layers\n\nCNCF (Cloud Native Computing Foundation), Exam Objectives\n\nCNI (Container Network Interface) plugin, Using Network Policies to Restrict Pod-to-Pod Communication\n\ncommands\n\naa-complain, Setting a custom profile\n\naa-enforce, Setting a custom profile\n\naa-status, Understanding profiles\n\nadduser, Adding a user\n\napply, Creating the RoleBinding, Applying the default container runtime profile to a container, Whitelisting Allowed Image Registries",
      "content_length": 1147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "with OPA GateKeeper\n\napt purge, Removing Unwanted Packages\n\ncertificate approve, Creating and approving a CertificateSigningRequest\n\nchmod, Changing file permissions\n\nchown, Changing file ownership\n\nCMD, Reducing the Number of Layers\n\nCOPY, Reducing the Number of Layers\n\ncreate ingress, Creating the Ingress\n\ncreate rolebinding, Creating a Role and a RoleBinding\n\ncreate token, Generating a service account token\n\ncurl, Anonymous access, Verifying the granted permissions, Implementing the Backend Application\n\ndisable, Disabling Services\n\ndocker build, Using a Multi-Stage Approach for Building Container Images\n\ndocker trust sign, Signing Container Images\n\necho, Applying the default container runtime profile to a container\n\netcdctl, Scenario: An Attacker Gains Access to the Node Running etcd, Encrypting etcd Data\n\nFROM, Reducing the Number of Layers\n\nget all, Setting Up the Ingress Backend\n\ngo build, Using a Multi-Stage Approach for Building Container Images",
      "content_length": 967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "groupadd, Adding a group\n\ngroupdel, Deleting a group\n\njournalctl, Generating Events and Inspecting Falco Logs\n\nkubectl, Installing Gatekeeper, Configuring a Log Backend\n\nkubectl apply, Observing the Default Behavior\n\nkubectl logs, Using kube-bench\n\nls, Viewing file permissions and ownership\n\nmkdir, Applying the custom profile to a container\n\nnetstat, Identifying and Disabling Open Ports\n\nOpenSSL, Creating the TLS Certificate and Key\n\nRUN, Reducing the Number of Layers\n\nss, Identifying and Disabling Open Ports\n\nstatus, Disabling Services\n\nsu, Switching to a user\n\nsudo, Switching to a user\n\nsudo systemctl restart kubelet, Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nsysctl, Avoiding Privileged Containers\n\nsystemctl, Disabling Services\n\nsystemctl status, Identifying and Disabling Open Ports\n\ntouch, Viewing file permissions and ownership\n\nusermod, Deleting a group",
      "content_length": 891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "wget, Denying Directional Network Traffic\n\nComplain profile mode, Understanding profiles\n\ncomponent security best practices, applying, Applying Kubernetes Component Security Best Practices-Fixing Detected Security Issues\n\nConfigMap, Scenario: An Attacker Installs Malicious Software, Configuring a Container with a ConfigMap or Secret\n\nconfiguration file (Falco), Falco configuration file\n\nconfiguring\n\ncontainers with ConfigMap, Configuring a Container with a ConfigMap or Secret\n\ncontainers with Secrets, Configuring a Container with a ConfigMap or Secret\n\nFalco, Configuring Falco\n\ngVisor, Installing and Configuring gVisor\n\nImagePolicyWebhook Admission Controller plugin, Configuring the ImagePolicyWebhook Admission Controller Plugin-Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nlog backend, Configuring a Log Backend\n\nports for API server, Connecting to the API Server\n\nread-only container root filesystems, Configuring a Read-Only Container Root Filesystem\n\nwebhook backend, Configuring a Webhook Backend\n\nconnecting to API server, Connecting to the API Server-Access with a client certificate",
      "content_length": 1119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "constraint template, Involved Kubernetes Primitives, Implementing an OPA Policy, Whitelisting Allowed Image Registries with OPA GateKeeper\n\ncontainer images\n\noptimization tools for, Using Container Image Optimization Tools\n\nsigning, Signing Container Images\n\nusing a multi-stage approach for building, Using a Multi-Stage Approach for Building Container Images\n\nvalidating, Validating Container Images\n\ncontainer runtime sandboxes, Understanding Container Runtime Sandboxes-Creating and Using a Runtime Class\n\nContainer Security (Rice), Securing the Supply Chain\n\ncontainerd, Applying the default container runtime profile to a container\n\ncontainers\n\napplying custom profiles to, Applying the custom profile to a container\n\napplying profiles to, Applying a profile to a container\n\nconfiguring with ConfigMap or Secrets, Configuring a Container with a ConfigMap or Secret\n\nensuring immutability, Ensuring Container Immutability\n\nusing in privileged mode, Avoiding Privileged Containers\n\nContinuous Delivery (Humble and Farley), Static Analysis of Workload\n\nCOPY command, Reducing the Number of Layers\n\nCRDs (Custom Resource Definitions), Involved Kubernetes Primitives\n\ncreate ingress command, Creating the Ingress",
      "content_length": 1213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "create rolebinding command, Creating a Role and a RoleBinding\n\ncreate token command, Generating a service account token\n\nCSR (certificate signing request), Creating a private key, Adopting mTLS in Kubernetes\n\ncurl command, Anonymous access, Verifying the granted permissions, Implementing the Backend Application\n\ncurriculum, Curriculum\n\ncustom profiles\n\napplying to containers, Applying the custom profile to a container\n\nsetting, Setting a custom profile, Setting a custom profile\n\ncustom rules, for Falco, Custom rules\n\nCVE (Common Vulnerabilities and Exposures) database, Updating Kubernetes Frequently\n\nCVE Details, Scanning Images for Known Vulnerabilities\n\nD\n\ndefault behavior, observing, Observing the Default Behavior\n\ndefault container runtime profile, applying to containers, Applying the default container runtime profile to a container\n\ndefault namespace, Observing the Default Behavior, Using kube-bench, Verifying the permissions\n\ndefault permissions, verifying, Verifying the default permissions\n\ndefault rules, for Falco, Default rules\n\nDegioanni, Loris, Practical Cloud Native Security with Falco, Understanding Falco",
      "content_length": 1135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "deny-all network policy, Denying Directional Network Traffic\n\ndeprecation policy (Kubernetes), Versioning Scheme\n\ndirectional network traffic, Denying Directional Network Traffic\n\ndisable command, Disabling Services\n\ndisabling\n\nautomounting for service account tokens, Disabling automounting of a service account token\n\nopen ports, Identifying and Disabling Open Ports\n\ndistroless image, Picking a Base Image Small in Size, Using a Distroless Container Image\n\nDive, Using Container Image Optimization Tools\n\ndocker build command, Using a Multi-Stage Approach for Building Container Images\n\nDocker Engine, Applying the default container runtime profile to a container\n\nDocker Hub, Scenario: An Attacker Exploits Container Vulnerabilities, Using Public Image Registries\n\ndocker trust sign command, Signing Container Images\n\nDocker, multi-stage build in, Using a Multi-Stage Approach for Building Container Images\n\nDockerfiles, Scenario: An Attacker Exploits Container Vulnerabilities, Using a Multi-Stage Approach for Building Container Images, Using Hadolint for Analyzing Dockerfiles\n\nDockerSlim, Using Container Image Optimization Tools\n\ndocumentation",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "AWS, Protecting Metadata Server Access with Network Policies\n\nDocker, Scenario: An Attacker Exploits Container Vulnerabilities\n\nEKS, Restricting Access to the API Server\n\nFalco, Installing Falco, Understanding Falco Rule File Basics\n\nGKE, Restricting Access to the API Server\n\nKubernetes, What You Will Learn, Documentation, Using Network Policies to Restrict Pod-to-Pod Communication, Denying Directional Network Traffic, Creating an Ingress with TLS Termination, Verifying a Binary Against Hash, Processing a Request, Restricting User Permissions, Verifying the default permissions, Using seccomp, Managing Secrets, Encrypting etcd Data, Adopting mTLS in Kubernetes, Picking a Base Image Small in Size, Implementing the Backend Application, Configuring a Container with a ConfigMap or Secret, Creating the Audit Policy File, Configuring a Log Backend\n\nOpenSSL, Creating the TLS Certificate and Key\n\nTrivy, Scanning Images for Known Vulnerabilities\n\nE\n\necho command, Applying the default container runtime profile to a container\n\nEKS (Amazon Elastic Kubernetes Service), Applying Kubernetes Component Security Best Practices\n\nencrypting\n\netcd data, Encrypting etcd Data\n\nPod-to-Pod, Understanding Pod-to-Pod Encryption with mTLS",
      "content_length": 1229,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "endpoints, protecting, Protecting Node Metadata and Endpoints-Protecting Metadata Server Access with Network Policies\n\nenforce mode, Understanding Pod Security Admission (PSA)\n\nEnforce profile mode, Understanding profiles\n\netcd data\n\naccessing, Accessing etcd Data\n\nencrypting, Encrypting etcd Data\n\netcdctl command, Scenario: An Attacker Gains Access to the Node Running etcd, Encrypting etcd Data\n\nevents, generating in Falco, Generating Events and Inspecting Falco Logs\n\nexam objectives, Exam Objectives\n\nexecution arguments, Avoiding Insecure Configuration Arguments\n\nexternal access, minimizing to network, Minimizing External Access to the Network\n\nexternal tools, Involved External Tools\n\nF\n\nF5 NGINX Ingress Controller, Creating an Ingress with TLS Termination\n\nFalco\n\nabout, Monitoring, Logging, and Runtime Security, Involved External Tools, Monitoring, Logging, and Runtime Security, Understanding Falco\n\nconfiguration file, Falco configuration file\n\nconfiguring, Configuring Falco",
      "content_length": 992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "custom rules for, Custom rules\n\ndefault rules for, Default rules\n\ngenerating events in, Generating Events and Inspecting Falco Logs\n\ninspecting logs, Generating Events and Inspecting Falco Logs\n\ninstalling, Installing Falco\n\noverriding existing rules, Overriding Existing Rules\n\nrule file basics, Understanding Falco Rule File Basics\n\nFalco 101 video course, Understanding Falco\n\nfalco.yaml file, Falco configuration file\n\nfalco_rules.local.yaml file, Custom rules\n\nfalco_rules.yaml file, Default rules\n\nFarley, David, Continuous Delivery, Static Analysis of Workload\n\nfeature gate, Applying the default container runtime profile to a container\n\nfile ownership, Understanding File Permissions and Ownership\n\nfile permissions, Understanding File Permissions and Ownership, Changing file permissions\n\nfine-grained incoming traffic, allowing, Allowing Fine-Grained Incoming Traffic\n\nfirewalls, setting up rules for, Setting Up Firewall Rules\n\nFROM command, Reducing the Number of Layers\n\nFTP servers, Identifying and Disabling Open Ports\n\nG",
      "content_length": 1037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "get all command, Setting Up the Ingress Backend\n\nGitHub, Documentation, Using Container Image Optimization Tools, Implementing the Backend Application\n\nGKE (Google Kubernetes Engine), Applying Kubernetes Component Security Best Practices\n\ngo build command, Using a Multi-Stage Approach for Building Container Images\n\nGo runtime, Using a Multi-Stage Approach for Building Container Images\n\nGoogle Cloud container registry, Using Public Image Registries\n\nGoogle distroless image, Picking a Base Image Small in Size\n\ngranted permissions, verifying, Verifying the granted permissions\n\nGrasso, Leonardo, Practical Cloud Native Security with Falco, Understanding Falco\n\ngroup ID, setting, Setting a Specific User and Group ID\n\ngroup management, Understanding Group Management\n\ngroupadd command, Adding a group\n\ngroupdel command, Deleting a group\n\ngroups, Listing groups\n\nGUIs (graphical user interfaces), Cluster Setup, Protecting GUI Elements\n\ngVisor\n\nabout, Minimize Microservice Vulnerabilities, Involved External Tools, Available Container Runtime Sandbox Implementations\n\nconfiguring, Installing and Configuring gVisor\n\ninstalling, Installing and Configuring gVisor",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "H\n\nhadolint (see Haskell Dockerfile Linter)\n\nhash, verifying binaries against, Verifying a Binary Against Hash\n\nHaskell Dockerfile Linter, Using Hadolint for Analyzing Dockerfiles\n\nHelm package manager, Scenario: An Attacker Can Call the API Server from a Service Account\n\nhelp option, Creating the Ingress\n\nhost OS footprint, minimizing, Minimizing the Host OS Footprint- Removing Unwanted Packages\n\nHumble, Jez, Continuous Delivery, Static Analysis of Workload\n\nI\n\nidentity and access management (IAM) roles, minimizing, Minimizing IAM Roles-Changing file permissions\n\nimage digest validation, Validating Container Images\n\nimage pull policy, Scenario: An Attacker Injects Malicious Code into a Container Image, Using Public Image Registries\n\nimage registries\n\nwhitelisting with ImagePolicyWebhook Admission Controller plugin, Whitelisting Allowed Image Registries with the ImagePolicyWebhook Admission Controller Plugin\n\nwhitelisting with OPA Gatekeeper, Whitelisting Allowed Image Registries with OPA GateKeeper-Whitelisting Allowed Image Registries with OPA GateKeeper\n\nImagePolicyWebhook Admission Controller plugin",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "configuring, Configuring the ImagePolicyWebhook Admission Controller Plugin-Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nwhitelisting allowed image registries with, Whitelisting Allowed Image Registries with the ImagePolicyWebhook Admission Controller Plugin\n\nimages\n\nbase, Picking a Base Image Small in Size\n\ncontainer, Using a Multi-Stage Approach for Building Container Images, Using Container Image Optimization Tools, Validating Container Images\n\ndistroless, Picking a Base Image Small in Size, Using a Distroless Container Image\n\nscanning for known vulnerabilities, Scanning Images for Known Vulnerabilities\n\nimmutability, ensuring for containers, Ensuring Container Immutability\n\nimperative method, Creating the Ingress\n\ninbound control node ports, Protecting Node Metadata and Endpoints\n\ningress, creating with TLS termination, Creating an Ingress with TLS Termination-Calling the Ingress\n\ninsecure configuration arguments, avoiding, Avoiding Insecure Configuration Arguments\n\ninstalling\n\nFalco, Installing Falco\n\nGatekeeper, Installing Gatekeeper",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "gVisor, Installing and Configuring gVisor\n\nKubernetes Dashboard, Installing the Kubernetes Dashboard\n\noptions for, Practicing and Practice Exams\n\nJ\n\nJFrog Artifactory, Scenario: An Attacker Uploads a Malicious Container Image\n\njournalctl command, Generating Events and Inspecting Falco Logs\n\nJSON Lines format, Understanding Audit Logs\n\nK\n\nk8s_audit_rules.yaml file, Kubernetes-specific rules\n\nKata Containers, Minimize Microservice Vulnerabilities, Involved External Tools, Available Container Runtime Sandbox Implementations\n\nKCNA (Kubernetes and Cloud Native Associate), Kubernetes and Cloud Native Associate (KCNA)\n\nKCSA (Kubernetes and Cloud Native Security Associate), Kubernetes and Cloud Native Security Associate (KCSA)\n\nkernel hardening tools, Using Kernel Hardening Tools-Applying the custom profile to a container\n\nKiller Shell, Practicing and Practice Exams\n\nknown vulnerabilities, scanning images for, Scanning Images for Known Vulnerabilities\n\nkube-bench, Involved External Tools, Using kube-bench, Using kube-bench\n\nkubeadm, Verifying Kubernetes Platform Binaries",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "kubeconfig file, Adding the user to the kubeconfig file, Configuring a Webhook Backend\n\nkubectl apply command, Observing the Default Behavior\n\nkubectl command, Installing Gatekeeper, Configuring a Log Backend\n\nkubectl logs command, Using kube-bench\n\nkubectl tool, Protecting GUI Elements, Verifying Kubernetes Platform Binaries, Verifying the permissions, Using Kubesec for Analyzing Kubernetes Manifests\n\nKubernetes\n\nrelease notes, Exam Objectives\n\nupdating, Updating Kubernetes Frequently-Summary\n\nusing mTLS in, Adopting mTLS in Kubernetes\n\nKubernetes API, interacting with, Interacting with the Kubernetes API- Access with a client certificate\n\nKubernetes CIS Benchmark, Applying Kubernetes Component Security Best Practices\n\nKubernetes Dashboard\n\nabout, Protecting GUI Elements\n\naccessing, Accessing the Kubernetes Dashboard\n\ninstalling, Installing the Kubernetes Dashboard\n\nKubernetes Manifests, analyzing using Kubesec, Using Kubesec for Analyzing Kubernetes Manifests-Using Kubesec for Analyzing Kubernetes Manifests\n\nKubernetes primitives (see primitives)",
      "content_length": 1064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "Kubernetes Service, Using the kubernetes Service\n\nKubernetes-specific rules, Kubernetes-specific rules\n\nKubesec, Static Analysis of Workload, Using Kubesec for Analyzing Kubernetes Manifests-Using Kubesec for Analyzing Kubernetes Manifests\n\nKyverno, Implementing an OPA Policy\n\nL\n\nlayers, reducing number of, Reducing the Number of Layers\n\nlisting groups, Listing groups\n\nlisting users, Listing users\n\nlists (Falco), List\n\nlog backend, Understanding Audit Logs, Configuring a Log Backend\n\nlogging, Monitoring, Logging, and Runtime Security, Monitoring, Logging, and Runtime Security-Exam Essentials\n\n(see also behavior analytics)\n\nlogs, inspecting in Falco, Generating Events and Inspecting Falco Logs\n\nls command, Viewing file permissions and ownership\n\nLTS (Long-Term Support), Updating Kubernetes Frequently\n\nM\n\nmacros (Falco), Macro\n\nManaging Kubernetes (Burns and Tracey), Interacting with the Kubernetes API\n\nMD5, Verifying a Binary Against Hash",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "Metadata audit level, Creating the Audit Policy File\n\nmetadata server access, protecting with network policies, Protecting Metadata Server Access with Network Policies\n\nmicroservice vulnerabilities, Minimizing Microservice Vulnerabilities- Exam Essentials\n\nabout, Minimize Microservice Vulnerabilities, Minimizing Microservice Vulnerabilities\n\nanswers to review questions, Chapter 5, “Minimize Microservice Vulnerabilities”-Chapter 5, “Minimize Microservice Vulnerabilities”\n\ncontainer runtime sandboxes, Understanding Container Runtime Sandboxes-Creating and Using a Runtime Class\n\nmanaging Secrets, Managing Secrets-Encrypting etcd Data\n\nPod-to-Pod encryption with mTLS, Understanding Pod-to-Pod Encryption with mTLS\n\nsample exercises, Sample Exercises\n\nsetting OS-level security domains, Setting Appropriate OS-Level Security Domains-Implementing an OPA Policy\n\nMinikube, Practicing and Practice Exams\n\nmkdir command, Applying the custom profile to a container\n\nmode, Understanding Pod Security Admission (PSA)\n\nmonitoring, Monitoring, Logging, and Runtime Security, Monitoring, Logging, and Runtime Security-Exam Essentials, Using Audit Logs to Monitor Access-Configuring a Webhook Backend\n\n(see also behavior analytics)",
      "content_length": 1224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "mTLS Pod-to-Pod encryption, Minimize Microservice Vulnerabilities, Understanding Pod-to-Pod Encryption with mTLS\n\nmulti-stage approach, using for building container images, Using a Multi- Stage Approach for Building Container Images\n\nMuschko, Benjamin\n\nCertified Kubernetes Administrator (CKA) Study Guide, Who This Book Is For, Candidate Skills, Creating an Ingress with TLS Termination, Performing the Upgrade Process, Managing Secrets\n\nCertified Kubernetes Application Developer (CKAD) Study Guide, Using Network Policies to Restrict Pod-to-Pod Communication\n\nN\n\nnamespaces, enforcing pod security standards for, Enforcing Pod Security Standards for a Namespace\n\nNAT (Network Address Translation), Using Network Policies to Restrict Pod-to-Pod Communication\n\nnetstat command, Identifying and Disabling Open Ports\n\nnetwork policies\n\nprotecting metadata server access with, Protecting Metadata Server Access with Network Policies\n\nusing to restrict pod-to-pod communication, Using Network Policies to Restrict Pod-to-Pod Communication\n\nnetworkpolicy.io, Using Network Policies to Restrict Pod-to-Pod Communication\n\nnetworks, minimizing external access to, Minimizing External Access to the Network",
      "content_length": 1198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "nginx, Configuring a Read-Only Container Root Filesystem\n\nnode metadata, protecting, Protecting Node Metadata and Endpoints\n\nNodePort Service type, Scenario: An Attacker Gains Access to the Dashboard Functionality\n\nnon-root users, enforcing usage of, Enforcing the Usage of a Non-Root User\n\nNone audit level, Creating the Audit Policy File\n\nO\n\nOCI (Open Container Initiative) runtime, Installing and Configuring gVisor\n\nOPA (Open Policy Agent), Understanding Open Policy Agent (OPA) and Gatekeeper\n\nOPA (Open Policy Agent) Gatekeeper\n\nabout, Involved Kubernetes Primitives, Setting Appropriate OS-Level Security Domains, Understanding Open Policy Agent (OPA) and Gatekeeper\n\ninstalling, Installing Gatekeeper\n\nLibrary, Implementing an OPA Policy\n\nwhitelisting allowed image registries with, Whitelisting Allowed Image Registries with OPA GateKeeper-Whitelisting Allowed Image Registries with OPA GateKeeper\n\nOpenSSL command, Creating the TLS Certificate and Key\n\noptimization tools, for container images, Using Container Image Optimization Tools\n\nO’Reilly learning platform, Practicing and Practice Exams",
      "content_length": 1104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "P\n\npackages, removing unwanted, Removing Unwanted Packages\n\npermissions\n\nminimizing for Service Account, Minimizing Permissions for a Service Account-Creating a Secret for a service account\n\nverifying, Verifying the permissions\n\nplatform binaries, verifying, Verifying Kubernetes Platform Binaries\n\nPod Security Admission, Setting Appropriate OS-Level Security Domains\n\npod-to-pod communication, restricting using network policies, Using Network Policies to Restrict Pod-to-Pod Communication-Allowing Fine- Grained Incoming Traffic\n\nPod-to-Pod encryption, with mTLS, Understanding Pod-to-Pod Encryption with mTLS\n\nPods\n\nbinding service account to, Binding the service account to a Pod\n\nenforcing security standards for namespaces, Enforcing Pod Security Standards for a Namespace\n\nPodSecurityContext API, Understanding Security Contexts\n\npolicies (OPA), Implementing an OPA Policy\n\nPortainer, Protecting GUI Elements\n\nports\n\nconfiguring for API server, Connecting to the API Server\n\ndisabling open, Identifying and Disabling Open Ports\n\nidentifying open, Identifying and Disabling Open Ports",
      "content_length": 1091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "open, Identifying and Disabling Open Ports\n\nport 10250, Protecting Node Metadata and Endpoints\n\nport 10257, Protecting Node Metadata and Endpoints\n\nport 10259, Protecting Node Metadata and Endpoints\n\nport 2379-2380, Protecting Node Metadata and Endpoints\n\nport 30000-32767, Protecting Node Metadata and Endpoints\n\nport 6443, Protecting Node Metadata and Endpoints\n\nPractical Cloud Native Security with Falco (Degioanni and Grasso), Understanding Falco\n\npractice exams, Practicing and Practice Exams\n\nprefix, Understanding Pod Security Admission (PSA)\n\nprimitives, Involved Kubernetes Primitives\n\nprivate keys, creating, Creating a private key\n\nprivileged containers, avoiding, Avoiding Privileged Containers\n\nprivileged level, Understanding Pod Security Admission (PSA)\n\nprofiles\n\nAppArmor, Understanding profiles\n\napplying to containers, Applying a profile to a container\n\ncustom, Setting a custom profile, Setting a custom profile, Applying the custom profile to a container\n\nPSA (Pod Security Admission), Understanding Pod Security Admission (PSA)\n\nPSP (Pod Security Policies), Understanding Pod Security Admission (PSA)",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "PSP (PodSecurityPolicy) admission controller, Versioning Scheme\n\nPSS (Pod Security Standard), Understanding Pod Security Admission (PSA)\n\npublic image registries, Using Public Image Registries\n\nR\n\nRBAC (role-based access control), Cluster Hardening\n\nread-only container root filesystems, configuring, Configuring a Read-Only Container Root Filesystem\n\nreducing attack surface/number of payers, Reducing the Number of Layers\n\nreference manual, for exam, Documentation\n\nregistries, public image, Using Public Image Registries\n\nRego, Understanding Open Policy Agent (OPA) and Gatekeeper, Whitelisting Allowed Image Registries with OPA GateKeeper\n\nrelease cadence, Release Cadence\n\nrelease notes, Exam Objectives\n\nRequest audit level, Creating the Audit Policy File\n\nRequestResponse audit level, Creating the Audit Policy File\n\nrequests, processing, Processing a Request\n\nrestricted level, Understanding Pod Security Admission (PSA)\n\nrestricted privileges, creating users with, Creating a User with Restricted Privileges\n\nrestricting",
      "content_length": 1029,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "access to API server, Restricting Access to the API Server-Creating a Secret for a service account\n\npod-to-pod communication using network policies, Using Network Policies to Restrict Pod-to-Pod Communication-Allowing Fine- Grained Incoming Traffic\n\nuser permissions, Restricting User Permissions-Scenario: An Attacker Can Call the API Server from a Service Account\n\nRice, Liz, Container Security, Securing the Supply Chain\n\nRoleBinding, creating, Creating a Role and a RoleBinding, Creating the RoleBinding\n\nroles\n\ncreating, Creating a Role and a RoleBinding\n\nidentity and access management (IAM), Minimizing IAM Roles- Changing file permissions\n\nrule argument, Creating the Ingress\n\nrules\n\nFalco, Rule\n\nfor firewalls, Setting Up Firewall Rules\n\noverriding existing, Overriding Existing Rules\n\nrules files (Falco), Understanding Falco Rule File Basics\n\nRUN command, Reducing the Number of Layers\n\nrunsc, Installing and Configuring gVisor\n\nruntime class, creating, Creating and Using a Runtime Class",
      "content_length": 999,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "runtime security, Monitoring, Logging, and Runtime Security, Monitoring, Logging, and Runtime Security-Exam Essentials\n\n(see also behavior analytics)\n\nS\n\nsample exercises\n\nbehavior analytics, Exam Essentials\n\ncluster hardening, Sample Exercises\n\ncluster setup, Sample Exercises\n\nmicroservice vulnerabilities, Sample Exercises\n\nsupply chain security, Sample Exercises\n\nsystem hardening, Sample Exercises\n\nsandboxes, container runtime, Understanding Container Runtime Sandboxes-Creating and Using a Runtime Class\n\nscanning images, for known vulnerabilities, Scanning Images for Known Vulnerabilities\n\nscenarios\n\nadministrator can monitor malicious events in real time, Scenario: An Administrator Can Monitor Malicious Events in Real Time\n\nattacker can call API Server from Internet, Scenario: An Attacker Can Call the API Server from the Internet\n\nattacker can call API Server from Service Account, Scenario: An Attacker Can Call the API Server from a Service Account\n\nattacker exploits container vulnerabilities, Scenario: An Attacker Exploits Container Vulnerabilities",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "attacker exploits package vulnerabilities, Scenario: An Attacker Exploits a Package Vulnerability\n\nattacker gains access to another container, Scenario: An Attacker Gains Access to Another Container\n\nattacker gains access to Dashboard functionality, Scenario: An Attacker Gains Access to the Dashboard Functionality\n\nattacker gains access to node running etcd, Scenario: An Attacker Gains Access to the Node Running etcd\n\nattacker gains access to pods, Scenario: Attacker Gains Access to a Pod\n\nattacker injects malicious code into binary, Scenario: An Attacker Injected Malicious Code into Binary\n\nattacker injects malicious code into container images, Scenario: An Attacker Injects Malicious Code into a Container Image\n\nattacker installs malicious software, Scenario: An Attacker Installs Malicious Software\n\nattacker listens to communication between two pods, Scenario: An Attacker Listens to the Communication Between Two Pods\n\nattacker misuses root user container access, Scenario: An Attacker Misuses root User Container Access\n\nattacker uploads malicious container images, Scenario: An Attacker Uploads a Malicious Container Image\n\nattacker uses credentials to gain file access, Scenario: An Attacker Uses Credentials to Gain File Access\n\ncompromised Pod accessing metadata servers, Scenario: A Compromised Pod Can Access the Metadata Server",
      "content_length": 1349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "developer doesn't follow pod security best practices, Scenario: A Developer Doesn’t Follow Pod Security Best Practices\n\nKubernetes administrator can observe actions taken by attackers, Scenario: A Kubernetes Administrator Can Observe Actions Taken by an Attacker\n\nseccomp, Involved External Tools, Using seccomp-Applying the custom profile to a container\n\nSecrets\n\nabout, Scenario: An Attacker Installs Malicious Software\n\nconfiguring containers with, Configuring a Container with a ConfigMap or Secret\n\ncreating for service accounts, Creating a Secret for a service account\n\nmanaging, Managing Secrets-Encrypting etcd Data\n\nsecurity\n\nenforcing standards for namespaces, Enforcing Pod Security Standards for a Namespace\n\nfixing issues with, Fixing Detected Security Issues\n\nof supply chain, Supply Chain Security\n\n(see also supply chain security)\n\nruntime, Monitoring, Logging, and Runtime Security\n\nsetting OS-level domains, Setting Appropriate OS-Level Security Domains-Implementing an OPA Policy\n\nsecurity contexts, Understanding Security Contexts\n\nSecurityContext API, Understanding Security Contexts",
      "content_length": 1104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "SELinux (Security-Enhanced Linux), Using AppArmor\n\nsemantic versioning scheme, Versioning Scheme\n\nservice account\n\nbinding to Pods, Binding the service account to a Pod\n\ncreating Secrets for, Creating a Secret for a service account\n\ndisabling automounting for tokens, Disabling automounting of a service account token\n\nexpiration of token, Creating a User with Administration Privileges\n\ngenerating tokens, Generating a service account token\n\nminimizing permissions for, Minimizing Permissions for a Service Account-Creating a Secret for a service account\n\nServiceAccount, Creating a User with Administration Privileges\n\nservices, disabling, Disabling Services\n\nSHA, Verifying a Binary Against Hash\n\nSHA256, Verifying a Binary Against Hash, Validating Container Images\n\nsigning container images, Signing Container Images\n\nsize, of base images, Picking a Base Image Small in Size\n\nsnapd package manager, Scenario: An Attacker Exploits a Package Vulnerability\n\nss command, Identifying and Disabling Open Ports\n\nstatic analysis, of workload, Static Analysis of Workload-Using Kubesec for Analyzing Kubernetes Manifests\n\nstatus command, Disabling Services",
      "content_length": 1151,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Study4exam, Practicing and Practice Exams\n\nsu command, Switching to a user\n\nsudo command, Switching to a user\n\nsudo systemctl restart kubelet command, Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nsupply chain security, Supply Chain Security-Sample Exercises\n\nabout, Supply Chain Security, Supply Chain Security\n\nanswers to review questions, Chapter 6, “Supply Chain Security”- Chapter 6, “Supply Chain Security”\n\nminimizing base image footprint, Minimizing the Base Image Footprint-Using Container Image Optimization Tools\n\nsample exercises, Sample Exercises\n\nscanning images for known vulnerabilities, Scanning Images for Known Vulnerabilities\n\nsecuring supply chain, Securing the Supply Chain-Configuring the ImagePolicyWebhook Admission Controller Plugin\n\nstatic analysis of workload, Static Analysis of Workload-Using Kubesec for Analyzing Kubernetes Manifests\n\nsysctl command, Avoiding Privileged Containers\n\nsystem hardening, System Hardening-Sample Exercises\n\nabout, System Hardening, System Hardening\n\nanswers to review questions, Chapter 4, “System Hardening”-Chapter 4, “System Hardening”\n\nminimizing external access to networks, Minimizing External Access to the Network",
      "content_length": 1200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "minimizing host OS footprint, Minimizing the Host OS Footprint- Removing Unwanted Packages\n\nminimizing IAM roles, Minimizing IAM Roles-Changing file permissions\n\nsample exercises, Sample Exercises\n\nusing kernel hardening tools, Using Kernel Hardening Tools-Applying the custom profile to a container\n\nsystemctl command, Disabling Services\n\nsystemctl status command, Identifying and Disabling Open Ports\n\nT\n\nTetragon, Scenario: A Kubernetes Administrator Can Observe Actions Taken by an Attacker\n\nTLS (Transport Layer Security)\n\ncreating an ingress with termination, Creating an Ingress with TLS Termination-Calling the Ingress\n\ncreating TLS certificate and key, Creating the TLS Certificate and Key\n\ntermination of, Cluster Setup\n\nTLS Secret, Creating the TLS-Typed Secret\n\nTLS-typed Secret, creating, Creating the TLS-Typed Secret\n\ntokens, service account, Disabling automounting of a service account token\n\ntools\n\nexternal, Involved External Tools\n\nkernel hardening, Using Kernel Hardening Tools-Applying the custom profile to a container",
      "content_length": 1040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "optimization, Using Container Image Optimization Tools\n\ntouch command, Viewing file permissions and ownership\n\nTracee, Scenario: A Kubernetes Administrator Can Observe Actions Taken by an Attacker\n\nTracey, Craig, Managing Kubernetes, Interacting with the Kubernetes API\n\nTrivy, Supply Chain Security, Involved External Tools, Scanning Images for Known Vulnerabilities\n\nTTL (time to live), Creating a User with Administration Privileges\n\nU\n\nUbuntu Linux, CIS benchmark for, Minimizing the Host OS Footprint\n\nUFW (Uncomplicated Firewall), Setting Up Firewall Rules\n\nupdating Kubernetes, Updating Kubernetes Frequently-Summary\n\nuser ID, setting, Setting a Specific User and Group ID\n\nuser management, Understanding User Management\n\nuser permissions, restricting, Restricting User Permissions-Scenario: An Attacker Can Call the API Server from a Service Account\n\nusermod command, Deleting a group\n\nusers\n\nadding, Adding a user\n\nadding to groups, Assigning a user to a group\n\nadding to kubeconfig file, Adding the user to the kubeconfig file\n\ncreating with administration privileges, Creating a User with Administration Privileges",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "creating with restricted privileges, Creating a User with Restricted Privileges\n\ndeleting, Deleting a user\n\nlisting, Listing users\n\nswitching to, Switching to a user\n\nV\n\nVagrant, Practicing and Practice Exams\n\nvalidating\n\nas stage in request processing, Processing a Request\n\ncontainer images, Validating Container Images\n\nverifying\n\nbinaries against hash, Verifying a Binary Against Hash\n\ndefault permissions, Verifying the default permissions\n\ngranted permissions, Verifying the granted permissions\n\nKubernetes platform binaries, Verifying Kubernetes Platform Binaries\n\npermissions, Verifying the permissions\n\nplatform binaries, Verifying Kubernetes Platform Binaries\n\nversioning scheme, Versioning Scheme\n\nVirtualBox, Practicing and Practice Exams\n\nVPN (Virtual Private Network), Adopting mTLS in Kubernetes\n\nvulnerabilities, scanning images for known, Scanning Images for Known Vulnerabilities",
      "content_length": 897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "W\n\nwarn mode, Understanding Pod Security Admission (PSA)\n\nwebhook backend, Understanding Audit Logs, Configuring a Webhook Backend\n\nwget command, Denying Directional Network Traffic\n\nwhitelisting\n\nallowed image registries with ImagePolicyWebhook Admission Controller plugin, Whitelisting Allowed Image Registries with the ImagePolicyWebhook Admission Controller Plugin\n\nallowed image registries with OPA Gatekeeper, Whitelisting Allowed Image Registries with OPA GateKeeper-Whitelisting Allowed Image Registries with OPA GateKeeper\n\nWireGuard, Adopting mTLS in Kubernetes\n\nworkload, static analysis of, Static Analysis of Workload-Using Kubesec for Analyzing Kubernetes Manifests\n\nY\n\nYAML manifest, Observing the Default Behavior\n\nOceanofPDF.com",
      "content_length": 745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "About the Author\n\nBenjamin Muschko is a software engineer, consultant, and trainer with more than 20 years of experience in the industry. He’s passionate about project automation, testing, and continuous delivery. Ben is an author, a frequent speaker at conferences, and an avid open source advocate. He holds the CKAD, CKA, and CKS certifications and is a CNCF Ambassador Spring 2023.\n\nSoftware projects sometimes feel like climbing a mountain. In his free time, Ben loves hiking Colorado’s 14ers and enjoys conquering long-distance trails.\n\nOceanofPDF.com",
      "content_length": 557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Colophon\n\nThe animal on the cover of Certified Kubernetes Security Specialist (CKS) Study Guide is a domestic goose. These birds have been selectively bred from wild greylag (Anser anse) and swan geese (Anser cygnoides domesticus). They have been introduced to every continent except Antarctica. Archaeological evidence shows the geese have been domesticated since at least 4,000 years ago.\n\nWild geese range in size from 7 to 9 pounds, whereas domestic geese have been bred for size and can weigh up to 22 pounds. The distribution of their fat deposits gives the domestic goose a more upright posture compared to the horizontal posture of their wild ancestors. Their larger size also makes them less likely to fly, although the birds are capable of some flight.\n\nHistorically, geese have been domesticated for use of their meat, eggs, and feathers. In more recent times, geese have been kept as backyard pets or even for yard maintenance since they eat weeds and leaves. Due to the loud and aggressive nature of geese, they have also been used to safeguard property, since they will make a lot of noise if they perceive a threat or an intruder.\n\nDomestic animals are not assessed by the IUCN. Many of the animals on O’Reilly covers are endangered; all of them are important to the world.\n\nThe cover illustration is by Karen Montgomery. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n\nOceanofPDF.com",
      "content_length": 1534,
      "extraction_method": "Unstructured"
    }
  ]
}