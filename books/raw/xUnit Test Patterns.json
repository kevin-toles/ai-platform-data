{
  "metadata": {
    "title": "xUnit Test Patterns",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 948,
    "conversion_date": "2025-12-25T18:23:27.730197",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "xUnit Test Patterns.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "of Refactoring [Ref], Martin Fowler writes:",
      "start_page": 24,
      "end_page": 65,
      "detection_method": "regex_chapter",
      "content": "Preface\n\nThe Value of Self-Testing Code\n\nIn Chapter 4 of Refactoring [Ref], Martin Fowler writes:\n\nIf you look at how most programmers spend their time, you’ll ﬁ nd that writing code is actually a small fraction. Some time is spent ﬁ guring out what ought to be going on, some time is spent designing, but most time is spent debugging. I’m sure every reader can remember long hours of debugging, often long into the night. Every programmer can tell a story of a bug that took a whole day (or more) to ﬁ nd. Fixing the bug is usually pretty quick, but ﬁ nding it is a nightmare. And then when you do ﬁ x a bug, there’s always a chance that anther one will appear and that you might not even notice it until much later. Then you spend ages ﬁ nding that bug.\n\nSome software is very difﬁ cult to test manually. In these cases, we are often forced into writing test programs.\n\nI recall a project I was working on in 1996. My task was to build an event framework that would let client software register for an event and be notiﬁ ed when some other software raised that event (the Observer [GOF] pattern). I could not think of a way to test this framework without writing some sample client software. I had about 20 different scenarios I needed to test, so I coded up each scenario with the requisite number of observers, events, and event raisers. At ﬁ rst, I logged what was occurring in the console and scanned it manually. This scanning became very tedious very quickly.\n\nBeing quite lazy, I naturally looked for an easier way to perform this test- ing. For each test I populated a Dictionary indexed by the expected event and the expected receiver of it with the name of the receiver as the value. When a particular receiver was notiﬁ ed of the event, it looked in the Dictionary for the entry indexed by itself and the event it had just received. If this entry existed, the receiver removed the entry. If it didn’t, the receiver added the entry with an error message saying it was an unexpected event notiﬁ cation.\n\nAfter running all the tests, the test program merely looked in the Dictionary and printed out its contents if it was not empty. As a result, running all of my tests had a nearly zero cost. The tests either passed quietly or spewed a list of test failures. I had unwittingly discovered the concept of a Mock Object (page 544) and a Test Automation Framework (page 298) out of necessity!\n\nxxi\n\nwww.it-ebooks.info\n\nxxii\n\nPreface\n\nMy First XP Project\n\nIn late 1999, I attended the OOPSLA conference, where I picked up a copy of Kent Beck’s new book, eXtreme Programming Explained [XPE]. I was used to doing iterative and incremental development and already believed in the value of automated unit testing, although I had not tried to apply it universally. I had a lot of respect for Kent, whom I had known since the ﬁ rst PLoP1 conference in 1994. For all these reasons, I decided that it was worth trying to apply eXtreme Programming on a ClearStream Consulting project. Shortly after OOPSLA, I was fortunate to come across a suitable project for trying out this develop- ment approach—namely, an add-on application that interacted with an existing database but had no user interface. The client was open to developing software in a different way.\n\nWe started doing eXtreme Programming “by the book” using pretty much all of the practices it recommended, including pair programming, collective owner- ship, and test-driven development. Of course, we encountered a few challenges in ﬁ guring out how to test some aspects of the behavior of the application, but we still managed to write tests for most of the code. Then, as the project pro- gressed, I started to notice a disturbing trend: It was taking longer and longer to implement seemingly similar tasks.\n\nI explained the problem to the developers and asked them to record on each task card how much time had been spent writing new tests, modifying existing tests, and writing the production code. Very quickly, a trend emerged. While the time spent writing new tests and writing the production code seemed to be staying more or less constant, the amount of time spent modifying existing tests was increasing and the developers’ estimates were going up as a result. When a developer asked me to pair on a task and we spent 90% of the time modify- ing existing tests to accommodate a relatively minor change, I knew we had to change something, and soon!\n\nWhen we analyzed the kinds of compile errors and test failures we were experiencing as we introduced the new functionality, we discovered that many of the tests were affected by changes to methods of the system under test (SUT). This came as no surprise, of course. What was surprising was that most of the impact was felt during the ﬁ xture setup part of the test and that the changes were not affecting the core logic of the tests.\n\nThis revelation was an important discovery because it showed us that we had the knowledge about how to create the objects of the SUT scattered across most of the tests. In other words, the tests knew too much about nonessential\n\n1 The Pattern Languages of Programs conference.\n\nwww.it-ebooks.info\n\nPreface\n\nparts of the behavior of the SUT. I say “nonessential” because most of the af- fected tests did not care about how the objects in the ﬁ xture were created; they were interested in ensuring that those objects were in the correct state. Upon further examination, we found that many of the tests were creating identical or nearly identical objects in their test ﬁ xtures.\n\nThe obvious solution to this problem was to factor out this logic into a small\n\nset of Test Utility Methods (page 599). There were several variations:\n\nWhen we had a bunch of tests that needed identical objects, we simply created a method that returned that kind of object ready to use. We now call these Creation Methods (page 415).\n\nSome tests needed to specify different values for some attribute of the object. In these cases, we passed that attribute as a parameter to the Parameterized Creation Method (see Creation Method).\n\nSome tests wanted to create a malformed object to ensure that the SUT would reject it. Writing a separate Parameterized Creation Method for each attribute cluttered the signature of our Test Helper (page 643), so we created a valid object and then replaced the value of the One Bad Attribute (see Derived Value on page 718).\n\nWe had discovered what would become2 our ﬁ rst test automation patterns.\n\nLater, when tests started failing because the database did not like the fact that we were trying to insert another object with the same key that had a unique constraint, we added code to generate the unique key programmatically. We called this variant an Anonymous Creation Method (see Creation Method) to indicate the presence of this added behavior.\n\nIdentifying the problem that we now call a Fragile Test (page 239) was an im- portant event on this project, and the subsequent deﬁ nition of its solution pat- terns saved this project from possible failure. Without this discovery we would, at best, have abandoned the automated unit tests that we had already built. At worst, the tests would have reduced our productivity so much that we would have been unable to deliver on our commitments to the client. As it turned out, we were able to deliver what we had promised and with very good quality. Yes, the testers3 still found bugs in our code because we were deﬁ nitely missing some tests. Introducing the changes needed to ﬁ x those bugs, once we had ﬁ gured\n\n2 Technically, they are not truly patterns until they have been discovered by three inde- pendent project teams. 3 The testing function is sometimes referred to as “Quality Assurance.” This usage is, strictly speaking, incorrect.\n\nwww.it-ebooks.info\n\nxxiii\n\nxxiv\n\nPreface\n\nout what the missing tests needed to look like, was a relatively straightforward process, however.\n\nWe were hooked. Automated unit testing and test-driven development really\n\ndid work, and we have been using them consistently ever since.\n\nAs we applied the practices and patterns on subsequent projects, we have run into new problems and challenges. In each case, we have “peeled the on- ion” to ﬁ nd the root cause and come up with ways to address it. As these tech- niques have matured, we have added them to our repertoire of techniques for automated unit testing.\n\nWe ﬁ rst described some of these patterns in a paper presented at XP2001. In discussions with other participants at that and subsequent conferences, we discovered that many of our peers were using the same or similar techniques. That elevated our methods from “practice” to “pattern” (a recurring solution to a recurring problem in a context). The ﬁ rst paper on test smells [RTC] was presented at the same conference, building on the concept of code smells ﬁ rst described in [Ref].\n\nMy Motivation\n\nI am a great believer in the value of automated unit testing. I practiced software development without it for the better part of two decades, and I know that my professional life is much better with it than without it. I believe that the xUnit framework and the automated tests it enables are among the truly great ad- vances in software development. I ﬁ nd it very frustrating when I see companies trying to adopt automated unit testing but being unsuccessful because of a lack of key information and skills.\n\nAs a software development consultant with ClearStream Consulting, I see a lot of projects. Sometimes I am called in early on a project to help clients make sure they “do things right.” More often than not, however, I am called in when things are already off the rails. As a result, I see a lot of “worst practices” that result in test smells. If I am lucky and I am called early enough, I can help the client recover from the mistakes. If not, the client will likely muddle through less than satisﬁ ed with how TDD and automated unit testing worked—and the word goes out that automated unit testing is a waste of time.\n\nIn hindsight, most of these mistakes and best practices are easily avoid- able given the right knowledge at the right time. But how do you obtain that knowledge without making the mistakes for yourself? At the risk of sounding self-serving, hiring someone who has the knowledge is the most time-efﬁ cient way of learning any new practice or technology. According to Gerry Weinberg’s\n\nwww.it-ebooks.info\n\nPreface\n\n“Law of Raspberry Jam” [SoC],4 taking a course or reading a book is a much less effective (though less expensive) alternative. I hope that by writing down a lot of these mistakes and suggesting ways to avoid them, I can save you a lot of grief on your project, whether it is fully agile or just more agile than it has been in the past—the “Law of Raspberry Jam” not withstanding.\n\nWho This Book Is For\n\nI have written this book primarily for software developers (programmers, designers, and architects) who want to write better tests and for the managers and coaches who need to understand what the developers are doing and why the developers need to be cut enough slack so they can learn to do it even bet- ter! The focus here is on developer tests and customer tests that are automated using xUnit. In addition, some of the higher-level patterns apply to tests that are automated using technologies other than xUnit. Rick Mugridge and Ward Cun- ningham have written an excellent book on Fit [FitB], and they advocate many of the same practices.\n\nDevelopers will likely want to read the book from cover to cover, but they should focus on skimming the reference chapters rather than trying to read them word for word. The emphasis should be on getting an overall idea of which pat- terns exist and how they work. Developers can then return to a particular pat- tern when the need for it arises. The ﬁ rst few elements (up to and include the “When to Use It” section) of each pattern should provide this overview.\n\nManagers and coaches might prefer to focus on reading Part I, The Nar- ratives, and perhaps Part II, The Test Smells. They might also need to read Chapter 18, Test Strategy Patterns, as these are decisions they need to under- stand and provide support to the developers as they work their way through these patterns. At a minimum, managers should read Chapter 3, Goals of Test Automation.\n\nAbout the Cover Photo\n\nEvery book in the Martin Fowler Signature Series features a picture of a bridge on the cover. One of the thoughts I had when Martin Fowler asked if he could “steal me for his series” was “Which bridge should I put on the cover?” I thought about the ability of testing to avoid catastrophic failures of software\n\n4 The Law of Raspberry Jam: “The wider you spread it, the thinner it gets.”\n\nwww.it-ebooks.info\n\nxxv\n\nxxvi\n\nPreface\n\nand how that related to bridges. Several famous bridge failures immediately came to mind, including “Galloping Gertie” (the Tacoma Narrows bridge) and the Iron Workers Memorial Bridge in Vancouver (named for the iron workers who died when a part of it collapsed during construction).\n\nAfter further reﬂ ection, it just did not seem right to claim that testing might have prevented these failures, so I chose a bridge with a more personal con- nection. The picture on the cover shows the New River Gorge bridge in West Virginia. I ﬁ rst passed over and subsequently paddled under this bridge on a whitewater kayaking trip in the late 1980s. The style of the bridge is also rel- evant to this book’s content: The complex arch structure underneath the bridge is largely hidden from those who use it to get to the other side of the gorge. The road deck is completely level and four lanes wide, resulting in a very smooth passage. In fact, at night it is quite possible to remain completely oblivious to the fact that one is thousands of feet above the valley ﬂ oor. A good test automa- tion infrastructure has the same effect: Writing tests is easy because most of the complexity lies hidden beneath the road bed.\n\nColophon\n\nThis book’s manuscript was written using XML, which I published to HTML for previewing on my Web site. I edited the XML using Eclipse and the XML Buddy plug-in. The HTML was generated using a Ruby program that I ﬁ rst obtained from Martin Fowler and which I then evolved quite extensively as I evolved my custom markup language. Code samples were written, compiled, and executed in (mostly) Eclipse and were inserted into the HTML automati- cally by XML tag handlers (one of the main reasons for using Ruby instead of XSLT). This gave me the ability to “publish early, publish often” to the Web site. I could also generate a single Word or PDF document for reviewers from the source, although this required some manual steps.\n\nwww.it-ebooks.info\n\nAcknowledgments\n\nWhile this book is largely a solo writing effort, many people have contributed to it in their own ways. Apologies in advance to anyone whom I may have missed.\n\nPeople who know me well may wonder how I found enough time to write a book like this. When I am not working, I am usually off doing various (some would say “extreme”) outdoor sports, such as back-country (extreme) skiing, whitewater (extreme) kayaking, and mountain (extreme) biking. Personally, I do not agree with this application of the “extreme” adjective to my activities any more than I agree with its use for highly iterative and incremental (extreme) programming. Nevertheless, the question of where I found the time to write this book is a valid one. I must give special thanks to my friend Heather Armitage, with whom I engage in most of the above activities. She has driven many long hours on the way to or from these adventures with me hunched over my laptop computer in the passenger seat working on this book. Also, thanks go to Alf Skrastins, who loves to drive all his friends to back-country skiing venues west of Calgary in his Previa. Also, thanks to the operators of the various back-country ski lodges who let me recharge my laptop from their generators so I could work on the book while on vacation—Grania Devine at Selkirk Lodge, Tannis Dakin at Sorcerer Lodge, and Dave Flear and Aaron Cooperman at Sol Mountain Touring. Without their help, this book would have taken much longer to write!\n\nAs usual, I’d like to thank all my reviewers, both ofﬁ cial and unofﬁ cial. Rob- ert C. (“Uncle Bob”) Martin reviewed an early draft. The ofﬁ cial reviewers of the ﬁ rst “ofﬁ cial” draft were Lisa Crispin and Rick Mugridge. Lisa Crispin, Jer- emy Miller, Alistair Duguid, Michael Hedgpeth, and Andrew Stopford reviewed the second draft.\n\nThanks to my “shepherds” from the various PLoP conferences who provided feedback on drafts of these patterns—Michael Stahl, Danny Dig, and especially Joe Yoder; they provided expert comments on my experiments with the pattern form. I would also like to thank the members of the PLoP workshop group on Pattern Languages at PLoP 2004 and especially Eugene Wallingford, Ralph Johnson, and Joseph Bergin. Brian Foote and the SAG group at UIUC posted several gigabytes of MP3’s of the review sessions in which they discussed the early drafts of the book. Their comments caused me to rewrite from scratch at least one of the narrative chapters.\n\nMany people e-mailed me comments about the material posted on my Web site at http://xunitpatterns.com or posted comments on the Yahoo! group; they\n\nxxvii\n\nwww.it-ebooks.info\n\nxxviii\n\nAcknowledgments\n\nprovided very timely feedback on the sometimes very draft-like material I had posted there. These folks included Javid Jamae, Philip Nelson, Tomasz Gajewski, John Hurst, Sven Gorts, Bradley T. Landis, Cédric Beust, Joseph Pelrine, Sebas- tian Bergmann, Kevin Rutherford, Scott W. Ambler, J. B. Rainsberger, Oli Bye, Dale Emery, David Nunn, Alex Chaffee, Burkhardt Hufnagel, Johannes Brod- wall, Bret Pettichord, Clint Shank, Sunil Joglekar, Rachel Davies, Nat Pryce, Paul Hodgetts, Owen Rogers, Amir Kolsky, Kevin Lawrence, Alistair Cockburn, Michael Feathers, and Joe Schmetzer. Special thanks go to Neal Norwitz, Markus Gaelli, Stephane Ducasse, and Stefan Reichhart, who provided copious feedback as unofﬁ cial reviewers.\n\nQuite a few people sent me e-mails describing their favorite pattern or special feature from their member of the xUnit family. Most of these were variations on patterns I had already documented; I’ve included them in this book as aliases or implementation variations as appropriate. A few were more esoteric patterns that I had to leave out for space reasons—for that, I apologize.\n\nMany of the ideas described in this book came from projects I worked on with my colleagues from ClearStream Consulting. We all pushed one another to ﬁ nd better ways of doing things back in the early days of eXtreme Program- ming when few—if any—resources were available. It was this single-minded determination that led to many of the more useful techniques described here. Those colleagues are Jennitta Andrea, Ralph Bohnet, Dave Braat, Russel Bryant, Greg Cook, Geoff Hardy, Shaun Smith, and Thomas (T2) Tannahill. Many of them also provided early reviews of various chapters. Greg also provided many of the code samples in Chapter 25, Database Patterns, while Ralph set up my CVS repository and automated build process for the Web site. I would also like to thank my bosses at ClearStream, who let me take time off from consulting engagements to work on the book and for permission to use the code-based exercises from our two-day “Testing for Developers” course as the basis for many of the code samples. Thanks, Denis Clelland and Luke McFarlane!\n\nSeveral people encouraged me to keep working on the book when the going got tough. They were always willing to take a phone call to discuss some sticky issue I was grappling with. Foremost among these individuals were Joshua Kerievsky and Martin Fowler.\n\nI’d like to especially thank Shaun Smith for helping me get started on this book and for the technical support he provided throughout the early part of writing it. He hosted my Web site, created the ﬁ rst CSS style sheets, taught me Ruby, set up a wiki for discussing the patterns, and even provided some of the early content before personal and work demands forced him to pull out of the writing side of the project. Whenever I say “we” when I talk about experi- ences, I am probably referring to Shaun and myself, although other coworkers may also share the same opinion.\n\nwww.it-ebooks.info\n\nIntroduction\n\nIt has been said before but it bears repeating: Writing defect-free software is exceedingly difﬁ cult. Proof of correctness of real systems is still well beyond our abilities, and speciﬁ cation of behavior is equally challenging. Predicting future needs is a hit or miss affair—we’d all be getting rich on the stock market instead of building software systems if we were any good at it!\n\nAutomated veriﬁ cation of software behavior is one of the biggest advances in development methods in the last few decades. This very developer-friendly prac- tice has huge beneﬁ ts in terms of increasing productivity, improving quality, and keeping software from becoming brittle. The very fact that so many developers are now doing it of their own free will speaks for its effectiveness.\n\nThis chapter introduces the concept of test automation using a variety of tools (including xUnit), describes why you would do it, and explains what makes it difﬁ cult to do test automation well.\n\nFeedback\n\nFeedback is a very important element in many activities. Feedback tells us whether our actions are having the right effect. The sooner we get feedback, the more quickly we can react. A good example of this kind of feedback is the rumble strips now being ground into many highways between the main driving surface and the shoulders. Yes, driving off the shoulder gives us feedback that we have left the road. But getting feedback earlier (when our wheels ﬁ rst enter the shoulder) gives us more time to correct our course and reduces the likeli- hood that we will drive off the road at all.\n\nTesting is all about getting feedback on software. For this reason, feedback is one of the essential elements of “agile” or “lean” software development. Hav- ing feedback loops in the development process is what gives us conﬁ dence in the software that we write. It lets us work more quickly and with less paranoia. It lets us focus on the new functionality we are adding by having the tests tell us whenever we break old functionality.\n\nxxix\n\nwww.it-ebooks.info\n\nxxx\n\nIntroduction\n\nTesting\n\nThe traditional deﬁ nition of “testing” comes from the world of quality assurance. We test software because we are sure it has bugs in it! So we test and we test and we test some more, until we cannot prove there are still bugs in the software. Traditionally, this testing occurs after the software is complete. As a result, it is a way of measuring quality—not a way of building quality into the product. In many organizations, testing is done by someone other than the software developers. The feedback provided by this kind of testing is very valuable, but it comes so late in the development cycle that its value is greatly diminished. It also has the nasty effect of extending the schedule as the problems found are sent back to development for rework, to be followed by another round of testing. So what kind of testing should software developers do to get feedback earlier?\n\nDeveloper Testing\n\nRare is the software developer who believes he or she can write code that works “ﬁ rst time, every time.” In fact, most of us are pleasantly surprised when some- thing does work the ﬁ rst time. (I hope I am not shattering any illusions for the nondeveloper readers out there!)\n\nSo developers do testing, too. We want to prove to ourselves that the soft- ware works as we intended it to. Some developers might do their testing the same way as testers do it: by testing the whole system as a single entity. Most developers, however, prefer to test their software unit by unit. The “units” may be larger-grained components or they may be individual classes, methods, or functions. The key thing that distinguishes these tests from the ones that the testers write is that the units being tested are a consequence of the design of the software, rather than being a direct translation of the requirements.1\n\nAutomated Testing\n\nAutomated testing has been around for several decades. When I worked on telephone switching systems at Nortel’s R&D subsidiary Bell-Northern Research in the early 1980s, we did automated regression and load testing of\n\n1 A small percentage of the unit tests may correspond directly to the business logic described in the requirements and the customer tests, but a large majority tests the code that surrounds the business logic.\n\nwww.it-ebooks.info\n\nIntroduction\n\nthe software/hardware that we were building. This testing was done primarily in the context of the “System Test” organization using specialized hardware and software that were programmed with test scripts. The test machines con- nected to the switch being tested as though it were a bunch of telephones and other telephone switches; it made telephone calls and exercised the myriad of telephone features. Of course, this automated testing infrastructure was not suitable for unit testing, nor was it generally available to the developers because of the huge amounts of hardware involved.\n\nIn the last decade, more general-purpose test automation tools have become available for testing applications through their user interfaces. Some of these tools use scripting languages to deﬁ ne the tests; the sexier tools rely on the “robot user” or “record and playback” metaphor for test automation. Unfor- tunately, many of the early experiences with these latter tools left the testers and test managers less than satisﬁ ed. The cause was high test maintenance costs caused by the “fragile test” problem.\n\nThe “Fragile Test” Problem\n\nTest automation using commercial “record and playback” or “robot user” tools has gained a bad reputation among early users of these tools. Tests automated using this approach often fail for seemingly trivial reasons. It is important to understand the limitations of this style of test automation to avoid falling vic- tim to the pitfalls commonly associated with it—namely, behavior sensitivity, interface sensitivity, data sensitivity, and context sensitivity.\n\nBehavior Sensitivity\n\nIf the behavior of the system is changed (e.g., if the requirements are changed and the system is modiﬁ ed to meet the new requirements), any tests that exer- cise the modiﬁ ed functionality will most likely fail when replayed.2 This is a basic reality of testing regardless of the test automation approach used. The real problem is that we often need to use that functionality to maneuver the system into the right state to start a test. As a consequence, behavioral changes have a much larger impact on the testing process than one might expect.\n\n2 A change in behavior could occur because the system is doing something different or because it is doing the same thing with different timing or sequencing.\n\nwww.it-ebooks.info\n\nxxxi\n\nxxxii\n\nIntroduction\n\nInterface Sensitivity\n\nTesting the business logic inside the system under test (SUT) via the user inter- face is a bad idea. Even minor changes to the interface can cause tests to fail, even though a human user might say the test should still pass. Such unintended interface sensitivity is partly what gave test automation tools such a bad name in the past decade. Although the problem occurs regardless of which user inter- face technology is being used, it seems to be worse with some types of interfaces than with others. Graphical user interfaces (GUIs) are a particularly challeng- ing way to interact with the business logic inside the system. The recent shift to Web-based (HTML) user interfaces has made some aspects of test automation easier but has introduced yet another problem because of the executable code needed within the HTML to provide a rich user experience.\n\nData Sensitivity\n\nAll tests assume some starting point, called the test ﬁ xture; this test context is sometimes called the “pre-conditions” or “before picture” of the test. Most commonly, this test ﬁ xture is deﬁ ned in terms of data that is already in the sys- tem. If the data changes, the tests may fail unless great effort has been expended to make them insensitive to the data being used.\n\nContext Sensitivity\n\nThe behavior of the system may be affected by the state of things outside the system. These external factors could include the states of devices (e.g., printers, servers), other applications, or even the system clock (e.g., the time and/or date of execution of the test). Any tests that are affected by this context will be dif- ﬁ cult to repeat deterministically without getting control over the context.\n\nOvercoming the Four Sensitivities\n\nThe four sensitivities exist regardless of which technology we use to automate the tests. Of course, some technologies give us ways to work around these sen- sitivities, while others force us down a particular path. The xUnit family of test automation frameworks gives us a large degree of control; we just have to learn how to use it effectively.\n\nwww.it-ebooks.info\n\nIntroduction\n\nUses of Automated Tests\n\nThus far, most of the discussion here has centered on regression testing of applications. This is a very valuable form of feedback when modifying existing applications because it helps us catch defects that we have introduced inadver- tently.\n\nTests as Speciﬁ cation\n\nA completely different use of automated testing is seen in test-driven devel- opment (TDD), which is one of the core practices of agile methods such as eXtreme Programming. This use of automated testing is more about speciﬁ cation of the behavior of the software yet to be written than it is about regression testing. The effectiveness of TDD comes from the way it lets us separate our thinking about software into two separate phases: what it should do, and how it should do it.\n\nHold on a minute! Don’t the proponents of agile software development eschew waterfall-style development? Yes, indeed. Agilists prefer to design and build a system feature by feature, with working software being available at every step to prove that each feature works before they move on to develop the next feature. That does not mean we do not do design; it simply means we do “continuous design”! Taking this to the extreme results in “emergent design,” where very little design is done upfront. But development does not have to be done that way. We can combine high-level design (or architecture) upfront with detailed design on a feature-by-feature basis. Either way, it can be useful to delay thinking about how to achieve the behavior of a speciﬁ c class or method for a few minutes while we capture what that behavior should be in the form of an executable speciﬁ cation. After all, most of us have trouble concentrating on one thing at a time, let alone several things simultaneously.\n\nOnce we have ﬁ nished writing the tests and verifying that they fail as expected, we can switch our perspective and focus on making them pass. The tests are now acting as a progress measurement. If we implement the functionality incremen- tally, we can see each test pass one by one as we write more code. As we work, we keep running all of the previously written tests as regression tests to make sure our changes have not had any unexpected side effects. This is where the true value of automated unit testing lies: in its ability to “pin down” the functionality of the SUT so that the functionality is not changed accidentally. That is what al- lows us to sleep well at night!\n\nwww.it-ebooks.info\n\nxxxiii\n\nxxxiv\n\nIntroduction\n\nTest-Driven Development\n\nMany books have been written recently on the topic of test-driven develop- ment, so this one will not devote a lot of space to that topic. This book focuses on what the code in the tests looks like, rather than when we wrote the tests. The closest we will get to talking about how the tests come into being is when we investigate refactoring of tests and learn how to refactor tests written using one pattern into tests that use a pattern with different characteristics.\n\nI am trying to stay “development process agnostic” in this book because au- tomated testing can help any team regardless of whether its members are doing TDD, test-ﬁ rst development, or test-last development. Also, once people learn how to automate tests in a “test last” environment, they are likely to be more inclined to experiment with a “test ﬁ rst” approach. Nevertheless, we do ex- plore some parts of the development process because they affect how easily we can do test automation. There are two key aspects of this investigation: (1) the interplay between Fully Automated Tests (see page 26) and our development in- tegration process and tools, and (2) the way in which the development process affects the testability of our designs.\n\nPatterns\n\nIn preparing to write this book, I read a lot of conference papers and books on xUnit-based test automation. Not surprisingly, each author seems to have a particular area of interest and favorite techniques. While I do not always agree with their practices, I am always trying to understand why these authors do things a particular way and when it would be more appropriate to use their techniques than the ones I already use.\n\nThis level of understanding is one of the major differences between examples and prose that merely explain the “how to” of a technique and a pattern. A pat- tern helps readers understand the why behind the practice, allowing them to make intelligent choices between the alternative patterns and thereby avoid any unexpected nasty consequences in the future.\n\nSoftware patterns have been around for a decade, so most readers should at least be aware of the concept. A pattern is a “solution to a recurring problem.” Some problems are bigger than others and, therefore, too big to solve with a single pattern. That is where the pattern language comes into play; this collec- tion (or grammar) of patterns leads the reader from an overall problem step by step to a detailed solution. In a pattern language, some of the patterns will nec- essarily be of higher levels of abstraction, while others will focus on lower-level details. To be useful, there must be linkages between the patterns so that we\n\nwww.it-ebooks.info\n\nIntroduction\n\ncan work our way down from the higher-level “strategy” patterns to the more detailed “design patterns” and the most detailed “coding idioms.”\n\nPatterns versus Principles versus Smells\n\nThis book includes three kinds of patterns. The most traditional kind of pat- tern is the “recurring solution to a common problem”; most of the patterns in this book fall into this general category. I do distinguish between three different levels:\n\n“Strategy”-level patterns have far-reaching consequences. The decision to use a Shared Fixture (page 317) rather than a Fresh Fixture (page 311) takes us down a very different path and leads to a different set of test design patterns. Each of the strategy patterns has its own write-up in the “Strategy Patterns” chapter in the reference section of the book.\n\nTest “design”-level patterns are used when developing tests for speciﬁ c functionality. They focus on how we organize our test logic. An exam- ple that should be familiar to most readers is the Mock Object pattern (page 544). Each test design pattern has its own write-up and the pat- terns are grouped into chapters in the reference section of the book based on topics such as Test Double patterns.\n\nTest “coding idioms” describe different ways to code a speciﬁ c test. Many of these are language speciﬁ c; examples include using block closures for Expected Exception Tests (see Test Method on page 348) in Smalltalk and anonymous inner classes for Mock Objects in Java. Some, such as Simple Success Test (see Test Method), are fairly generic in that they have analogs in each language. These idioms are typically listed as implementation variations or examples within the write-up of a “test design pattern.”\n\nOften, several alternative patterns could be used at each level. Of course, I almost always have a preference for which patterns to use, but one person’s “anti- pattern” may be another person’s “best practice pattern.” As a result, this book includes patterns that I do not necessarily advocate. It describes the advantages and disadvantages of each of those patterns, allowing readers to make informed decisions about their use. I have tried to provide linkages to those alternatives in each of the pattern descriptions as well as in the introductory narratives.\n\nThe nice thing about patterns is that they provide enough information to make an intelligent decision between several alternatives. The pattern we choose may be affected by the goals we have for test automation. The goals describe\n\nwww.it-ebooks.info\n\nxxxv\n\nxxxvi\n\nIntroduction\n\ndesired outcomes of the test automation efforts. These goals are supported by a number of principles that codify a belief system about what makes automated tests “good.” In this book, the goals of test automation are described in Chapter 3, Goals of Test Automation, and the principles are described in Chapter 5, Prin- ciples of Test Automation.\n\nThe ﬁ nal kind of pattern is more of an anti-pattern [AP]. These test smells describe recurring problems that our patterns help us address in terms of the symptoms we might observe and the root causes of those symptoms. Code smells were ﬁ rst popularized in Martin Fowler’s book [Ref] and applied to xUnit-based testing as test smells in a paper presented at XP2001 [RTC]. The test smells are cross-referenced with the patterns that can be used to banish them as well as the patterns3 that are more likely to lead to them.4 In addition, the test smells are covered in depth in their own section: Part II, The Test Smells.\n\nPattern Form\n\nThis book includes my descriptions of patterns. The patterns themselves existed before I started cataloging them, by virtue of having been invented indepen- dently by at least three different test automaters. I took it upon myself to write them down as a way of making the knowledge more easily distributable. But to do so, I had to choose a pattern description form.\n\nPattern descriptions come in many shapes and sizes. Some have a very rigid structure deﬁ ned by many headings that help the reader ﬁ nd the various sec- tions. Others read more like literature but may be more difﬁ cult to use as a ref- erence. Nevertheless, all patterns have a common core of information, however it is presented.\n\nMy Pattern Form\n\nI have really enjoyed reading the works of Martin Fowler, and I attribute much of that enjoyment to the pattern form that he uses. As the saying goes, “Imita- tion is the sincerest form of ﬂ attery”: I have copied his format shamelessly with only a few minor modiﬁ cations.\n\nThe template begins with the problem statement, the summary statement, and a sketch. The italicized problem statement summarizes the core of the problem\n\n3 Some might want to call these patterns “anti-patterns.” Just because a pattern often has negative consequences, it does not imply that the pattern is always bad. For this reason, I prefer not to call these anti-patterns; I just do not use them very often. 4 In a few cases, there are even a pattern and a smell with similar names.\n\nwww.it-ebooks.info\n\nIntroduction\n\nthat the pattern addresses. It is often stated as a question: “How do we . . . ?” The boldface summary statement captures the essence of the pattern in one or two sentences, while the sketch provides a visual representation of the pattern. The untitled section of text immediately after the sketch summarizes why we might want to use the pattern in just a few sentences. It elaborates on the problem statement and includes both the “Problem” and “Context” sections from the tra- ditional pattern template. A reader should be able to get a sense of whether he or she wants to read any further by skimming this section.\n\nThe next three sections provide the meat of the pattern. The “How It Works” section describes the essence of how the pattern is structured and what it is about. It also includes information about the “resulting context” when there are several ways to implement some important aspect of the pattern. This sec- tion corresponds to the “Solution” or “Therefore” sections of more traditional pattern forms. The “When to Use It” section describes the circumstances in which you should consider using the pattern. This section corresponds to the “Problem,” “Forces,” “Context,” and “Related Patterns” sections of traditional pattern templates. It also includes information about the “Resulting Context,” when this information might affect whether you would want to use this pattern. I also include any “test smells” that might suggest that you should use this pat- tern. The “Implementation Notes” section describes the nuts and bolts of how to implement the pattern. Subheadings within this section indicate key compo- nents of the pattern or variations in how the pattern can be implemented.\n\nMost of the concrete patterns include three additional sections. The “Moti- vating Example” section provides examples of what the test code might have looked like before this pattern was applied. The section titled “Example: {Pat- tern Name}” shows what the test would look like after the pattern was applied. The “Refactoring Notes” section provides more detailed instructions on how to get from the “Motivating Example” to the “Example: {Pattern Name}.”\n\nIf the pattern is written up elsewhere, the description may include a section titled “Further Reading.” A “Known Uses” section appears when there is some- thing particularly interesting about those applications. Most of these patterns have been seen in many systems, of course, so picking three uses to substantiate them would be arbitrary and meaningless.\n\nWhere a number of related techniques exist, they are often presented here as a single pattern with several variations. If the variations are different ways to implement the same fundamental pattern (namely, solving the same prob- lem the same general way), the variations and the differences between them are listed in the “Implementation Notes” section. If the variations are primarily a different reason for using the pattern, the variations are listed in the “When to Use It” section.\n\nwww.it-ebooks.info\n\nxxxvii\n\nxxxviii\n\nIntroduction\n\nHistorical Patterns and Smells\n\nI struggled mightily when trying to come up with a concise enough list of pat- terns and smells while still keeping historical names whenever possible. I often list the historical name as an alias for the pattern or smell. In some cases, it made more sense to consider the historical version of the pattern as a speciﬁ c variation of a larger pattern. In such a case, I usually include the historical pat- tern as a named variation in the “Implementation Notes” section.\n\nMany of the historical smells did not pass the “sniff test”—that is, the smell described a root cause rather than a symptom.5 Where an historical test smell describes a cause and not a symptom, I have chosen to move it into the cor- responding symptom-based smell as a special kind of variation titled “Cause.” Mystery Guest (see Obscure Test on page 186) is a good example.\n\nReferring to Patterns and Smells\n\nI also struggled to come up with a good way to refer to patterns and smells, espe- cially the historical ones. I wanted to be able to use both the historical names when appropriate and the new aggregate names, whichever was more appropriate. I also wanted the reader to be able to see which was which. In the online version of this book, hyperlinks were used for this purpose. For the printed version, however, I needed a way to represent this linkage as a page number annotation of the refer- ence without cluttering up the entire text with references. The solution I landed on after several tries includes the page number where the pattern or smell can be found the ﬁ rst time it is referenced in a chapter, pattern, or smell. If the reference is to a pattern variation or the cause of a smell, I include the aggregate pattern or smell name the ﬁ rst time. Note how this second reference to the Mystery Guest cause of Obscure Test shows up without the smell name, whereas references to other causes of Obscure Test such as Irrelevant Information (see Obscure Test) include the aggregate smell name but not the page number.\n\nRefactoring\n\nRefactoring is a relatively new concept in software development. While people have always had a need to modify existing code, refactoring is a highly\n\n5 The “sniff test” is based on the diaper story in [Ref] wherein Kent Beck asks Grandma Beck, “How do I know that it is time to change the diaper?” “If it stinks, change it!” was her response. Smells are named based on the “stink,” not the cause of the stink.\n\nwww.it-ebooks.info\n\nIntroduction\n\ndisciplined approach to changing the design without changing the behavior of the code. It goes hand-in-hand with automated testing because it is very difﬁ cult to do refactoring without having the safety net of automated tests to prove that you have not broken anything during your redesign.\n\nMany of the modern integrated development environments (IDEs) have built-in support for refactoring. Most of them automate the refactoring steps of at least a few of the refactorings described in Martin Fowler’s book [Ref]. Unfortunately, the tools do not tell us when or why we should use refactoring. We will have to get a copy of Martin’s book for that! Another piece of mandatory reading on this topic is Joshua Kerievsky’s book [RtP].\n\nRefactoring tests differs a bit from refactoring production code because we do not have automated tests for our automated tests! If a test fails after a refac- toring of the test, did the failure occur because we made a mistake during the refactoring? Just because a test passes after a test refactoring, can we be sure it will still fail when appropriate? To address this issue, many test refactorings are very conservative, “safe refactorings” that minimize the chance of introducing a change of behavior into the test. We also try to avoid having to do major refac- torings of tests by adopting an appropriate test strategy, as described in Chapter 6, Test Automation Strategy.\n\nThis book focuses more on the target of the refactoring than on the mechanics of this endeavor. A short summary of the refactorings does appear in Appendix A, but the process of refactoring is not the primary focus of this book. The patterns themselves are new enough that we have not yet had time to agree on their names, content, or applicability, let alone reach consensus on the best way to refactor to them. A further complication is that there are potentially many starting points for each refactoring target (pattern), and attempting to provide detailed refactoring instructions would make this already large book much larger.\n\nAssumptions\n\nIn writing this book, I assumed that the reader is somewhat familiar with object technology (also known as “object-oriented programming”); object technology seemed to be a prerequisite for automated unit testing to become popular. That does not mean we cannot perform testing in procedural or functional languages, but use of these languages may make it more challenging (or at least different). Different people have different learning styles. Some need to start with the “big picture” abstractions and work down to “just enough” detail. Others can understand only the details and have no need for the “big picture.” Some learn best by hearing or reading words; others need pictures to help them visualize\n\nwww.it-ebooks.info\n\nxxxix\n\nxl\n\nIntroduction\n\na concept. Still others learn programming concepts best by reading code. I’ve tried to accommodate all of these learning styles by providing a summary, a detailed description, code samples, and a picture wherever possible. These items should be Skippable Sections [PLOPD3] for those readers who won’t beneﬁ t from that style of learning.\n\nTerminology\n\nThis book brings together terminology from two different domains: software development and software testing. As a consequence, some terminology will inevitably be unfamiliar to some readers. Readers should refer to the glossary when they encounter any terms that they do not understand. I will, however, point out one or two terms here, because becoming familiar with these terms is essential to understanding most of the material in this book.\n\nTesting Terminology\n\nSoftware developers will probably ﬁ nd the term “system under test” (abbrevi- ated throughout this book as SUT) unfamiliar. It is short for “whatever thing we are testing.” When we are writing unit tests, the SUT is whatever class or method(s) we are testing; when we are writing customer tests, the SUT is prob- ably the entire application (or at least a major subsystem of it).\n\nAny part of the application or system we are building that is not included in the SUT may still be required to run our test because it is called by the SUT or because it sets up prerequisite data that the SUT will use as we exercise it. The former type of element is called a depended-on component (DOC), and both types are part of the test ﬁ xture. This is illustrated in Figure I.1.\n\nLanguage-Speciﬁ c xUnit Terminology\n\nAlthough this book includes examples in a variety of languages and xUnit fam- ily members, JUnit ﬁ gures prominently in this coverage. JUnit is the language and xUnit framework that most people are at least somewhat familiar with. Many of the translations of JUnit to other languages are relatively faithful ports, with only minor changes in class and method names needed to accom- modate the differences in the underlying language. Where this isn’t the case, Appendix B, xUnit Terminology Cross-Reference, often includes the appropri- ate mapping.\n\nwww.it-ebooks.info\n\nIntroduction\n\nUnit1 Unit1 Test Test\n\nExercise Exercise\n\nUnit1 Unit1 SUT SUT\n\nUnit2 Unit2 Test Test\n\nExercise Exercise\n\nUnit2 Unit2 SUT SUT\n\nUses Uses\n\nComp1 Comp1 Test Test\n\nExercise Exercise\n\nComp1 Comp1 SUT SUT\n\nComp2 Comp2 Test Test\n\nExercise Exercise\n\nComp2 Comp2 SUT SUT\n\nUses Uses\n\nApp1 App1 Test Test\n\nExercise Exercise\n\nApp1 App1 SUT SUT\n\nFigure I.1. A range of tests each with its own SUT. An application, component, or unit is only the SUT with respect to a speciﬁ c set of tests. The “Unit1 SUT” plays the role of DOC (part of the ﬁ xture) to “Unit2 Test” and is part of the “Comp1 SUT” and the “App1 SUT.”\n\nUsing Java as the main sample language also means that in some discussions we will refer to the JUnit name of a method and will not list the corresponding method names in each of the xUnit frameworks. For example, a discussion may refer to JUnit’s assertTrue method without mentioning that the NUnit equiva- lent is Assert.IsTrue, the SUnit equivalent is should:, and the VbUnit equivalent is verify. Readers are expected to do the mental swap of method names to the SUnit, VbUnit, Test::Unit, and other equivalents with which they may be most familiar. The Intent-Revealing Names [SBPP] of the JUnit methods should be clear enough for the purposes of our discussion.\n\nCode Samples\n\nSample code is always a problem. Samples of code from real projects are typi- cally much too large to include and are usually covered by nondisclosure agree- ments that preclude their publication. “Toy programs” do not get much respect because “they aren’t real.” A book such as this one has little choice except to use “toy programs,” but I have tried to make them as representative as possible of real projects.\n\nwww.it-ebooks.info\n\nxli\n\nxlii\n\nIntroduction\n\nAlmost all of the code samples presented here came from “real” compilable and executable code, so they should not (knock on wood) contain any compile errors unless they were introduced during the editing process. Most of the Ruby examples come from the XML-based publishing system I used to prepare this book, while many of the Java and C# samples came from courseware that we use at ClearStream to teach these concepts to ClearStream’s clients.\n\nI have tried to use a variety of languages to illustrate the nearly universal application of the patterns across the members of the xUnit family. In some cases, the speciﬁ c pattern dictated the use of language because of speciﬁ c features of either the language or the xUnit family member. In other cases, the language was dictated by the availability of third-party extensions for a speciﬁ c member of the xUnit family. Otherwise, the default language for examples is Java with some C# because most people have at least reading-level familiarity with them. Formatting code for a book is a particular challenge due to the recommended line length of just 65 characters. I have taken some liberties in shortening vari- able and class names simply to reduce the number of lines that wrap. I’ve also invented some line-wrapping conventions to minimize the vertical size of these samples. You can take solace in the fact that your test code should look a lot “shorter” than mine because you have to wrap many fewer lines!\n\nDiagramming Notation\n\n“A picture is worth a thousand words.” Wherever possible, I have tried to include a sketch of each pattern or smell. I’ve based the sketches loosely on the Uniﬁ ed Modeling Language (UML) but took a few liberties to make them more expres- sive. For example, I use the aggregation symbol (diamond) and the inheritance symbol (a triangle) of UML class diagrams, but I mix classes and objects on the same diagram along with associations and object interactions. Most of the nota- tion is introduced in the patterns in Chapter 19, xUnit Basics Patterns, so you may ﬁ nd it worthwhile to skim this chapter just to look at the pictures.\n\nAlthough I have tried to make this notation “discoverable” simply through comparing sketches, a few conventions are worth pointing out. Objects have shadows; classes and methods do not. Classes have square corners, in keep- ing with UML; methods have round corners. Large exclamation marks are as- sertions (potential test failures), and a starburst is an error or exception being raised. The ﬁ xture is a cloud, reﬂ ecting its nebulous nature, and any compo- nents the SUT depends on are superimposed on the cloud. Whatever the sketch is trying to illustrate is highlighted with heavier lines and darker shading. As a result, you should be able to compare two sketches of related concepts and quickly determine what is emphasized in each.\n\nwww.it-ebooks.info\n\nIntroduction\n\nLimitations\n\nAs you use these patterns, please keep in mind that I could not have seen every test automation problem and every solution to every problem; there may well be other, possibly better, ways to solve some of these problems. These solutions are just the ones that have worked for me and for the people I have been com- municating with. Accept everyone’s advice with a grain of salt!\n\nMy hope is that these patterns will give you a starting point for writing good, robust automated tests. With luck, you will avoid many of the mistakes we made on our ﬁ rst attempts and will go on to invent even better ways of auto- mating tests. I’d love to hear about them!\n\nwww.it-ebooks.info\n\nxliii\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nRefactoring a Test\n\nWhy Refactor Tests?\n\nTests can quickly become a bottleneck in an agile development process. This may not be immediately obvious to those who have never experienced the difference between simple, easily understood tests and complex, obtuse, hard-to-maintain tests. The productivity difference can be staggering!\n\nThis section of the book acts as a “motivating example” for the entire book by showing you how much of a difference refactoring tests can make. It walks you through an example starting with a complex test and, step by step, refac- tors it to a simple, easily understood test. Along the way, I will point out some key smells and the patterns that we can use to remove them. Ideally, this exer- cise will whet your appetite for more.\n\nA Complex Test\n\nHere is a test that is not atypical of some of the tests I have seen on various projects:\n\npublic void testAddItemQuantity_severalQuantity_v1(){ Address billingAddress = null; Address shippingAddress = null; Customer customer = null; Product product = null; Invoice invoice = null; try { // Set up ﬁxture billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\",\"Canada\"); shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); customer = new Customer(99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); product = new Product(88, \"SomeWidget\", new BigDecimal(\"19.99\")); invoice = new Invoice(customer);\n\nxlv\n\nwww.it-ebooks.info\n\nxlvi\n\nRefactoring a Test\n\n// Exercise SUT invoice.addItemQuantity(product, 5); // Verify outcome List lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"inv\", invoice, actItem.getInv()); assertEquals(\"prod\", product, actItem.getProd()); assertEquals(\"quant\", 5, actItem.getQuantity()); assertEquals(\"discount\", new BigDecimal(\"30\"), actItem.getPercentDiscount()); assertEquals(\"unit price\",new BigDecimal(\"19.99\"), actItem.getUnitPrice()); assertEquals(\"extended\", new BigDecimal(\"69.96\"), actItem.getExtendedPrice()); } else { assertTrue(\"Invoice should have 1 item\", false); } } ﬁnally { // Teardown deleteObject(invoice); deleteObject(product); deleteObject(customer); deleteObject(billingAddress); deleteObject(shippingAddress); } }\n\nThis test is quite long1 and is much more complicated than it needs to be. This Obscure Test (page 186) is difﬁ cult to understand because the sheer number of lines in the test makes it hard to see the big picture. It also suffers from a num- ber of other problems that we will address individually.\n\nCleaning Up the Test\n\nLet’s look at each of the various parts of the test.\n\nCleaning Up the Veriﬁ cation Logic\n\nFirst, let’s focus on the part that veriﬁ es the expected outcome. Maybe we can infer from the assertions which test conditions this test is trying to verify.\n\n1 While the need to wrap lines to keep them at 65 characters makes this code look even longer than it really is, it is still unnecessarily long. It contains 25 executable statements including initialized declarations, 6 lines of control statements, 4 in-line comments, and 2lines to declare the test method—giving a total of 37 lines of unwrapped source code.\n\nwww.it-ebooks.info\n\nCleaning Up the Test\n\nList lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"inv\", invoice, actItem.getInv()); assertEquals(\"prod\", product, actItem.getProd()); assertEquals(\"quant\", 5, actItem.getQuantity()); assertEquals(\"discount\", new BigDecimal(\"30\"), actItem.getPercentDiscount()); assertEquals(\"unit price\",new BigDecimal(\"19.99\"), actItem.getUnitPrice()); assertEquals(\"extended\", new BigDecimal(\"69.96\"), actItem.getExtendedPrice()); } else { assertTrue(\"Invoice should have 1 item\", false); }\n\nA simple problem to ﬁ x is the obtuse assertion on the very last line. Calling assertTrue with an argument of false should always result in a test failure, so why don’t we say so directly? Let’s change this to a call to fail:\n\nList lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"inv\", invoice, actItem.getInv()); assertEquals(\"prod\", product, actItem.getProd()); assertEquals(\"quant\", 5, actItem.getQuantity()); assertEquals(\"discount\", new BigDecimal(\"30\"), actItem.getPercentDiscount()); assertEquals(\"unit price\",new BigDecimal(\"19.99\"), actItem.getUnitPrice()); assertEquals(\"extended\", new BigDecimal(\"69.96\"), actItem.getExtendedPrice()); } else { fail(\"Invoice should have exactly one line item\"); }\n\nWe can think of this move as an Extract Method [Fowler] refactoring, because we are replacing the Stated Outcome Assertion (see Assertion Method on page 362) with a hard-coded parameter with a more intent-revealing call to a Single Out- come Assertion (see Assertion Method) method that encapsulates the call.\n\nOf course, this set of assertions suffers from several more problems. For exam- ple, why do we need so many of them? It turns out that many of these assertions are testing ﬁ elds set by the constructor for the LineItem, which is itself covered by another unit test. So why repeat these assertions here? It will just create more test code to maintain when the logic changes.\n\nOne solution is to use a single assertion on an Expected Object (see State Veri- ﬁ cation on page 462) instead of one assertion per object ﬁ eld. First, we deﬁ ne an object that looks exactly how we expect the result to look. In this case, we create\n\nwww.it-ebooks.info\n\nxlvii\n\nxlviii\n\nRefactoring a Test\n\nan expected LineItem with the ﬁ elds ﬁ lled in with the expected values, including the unitPrice and extendedPrice initialized from the product.\n\nList lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"invoice\", expected.getInv(), actItem.getInv()); assertEquals(\"product\", expected.getProd(), actItem.getProd()); assertEquals(\"quantity\",expected.getQuantity(), actItem.getQuantity()); assertEquals(\"discount\", expected.getPercentDiscount(), actItem.getPercentDiscount()); assertEquals(\"unit pr\", new BigDecimal(\"19.99\"), actItem.getUnitPrice()); assertEquals(\"extend pr\",new BigDecimal(\"69.96\"), actItem.getExtendedPrice()); } else { fail(\"Invoice should have exactly one line item\"); }\n\nOnce we have created our Expected Object, we can then assert on it using assertEquals:\n\nList lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem expected = new LineItem(invoice, product,5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"invoice\", expected, actItem); } else { fail(\"Invoice should have exactly one line item\"); }\n\nClearly, the Preserve Whole Object [Fowler] refactoring makes the code a lot simpler and more obvious. But wait! Why do we have an if statement in a test? If there are several paths through a test, how do we know which one is actually being executed? It would be a lot better if we could eliminate this Conditional Test Logic (page 200). Luckily for us, the pattern Guard Assertion (page 490) is designed to handle exactly this case. We simply use a Replace Conditional with Guard Clause [Fowler] refactoring to replace the if ... else fail() ... sequence with an assertion on the same condition. This Guard Assertion halts execution if the condition is not met without introducing Conditional Test Logic.\n\nwww.it-ebooks.info\n\nCleaning Up the Test\n\nList lineItems = invoice.getLineItems(); assertEquals(\"number of items\", 1,lineItems.size()); LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"invoice\", expected, actItem);\n\nSo far, we have reduced 11 lines of veriﬁ cation code to just 4, and those 4 lines are a lot simpler code to boot.2 Some people might suggest that this refactor- ing is good enough. But can’t we make this assertion even more obvious? What are we really trying to verify? We are trying to say that there should be only one line item and it should look exactly like our expectedLineItem. We can say this explicitly by using an Extract Method refactoring to deﬁ ne a Custom Asser- tion (page 474).\n\nLineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected);\n\nThat is better! Now we have the veriﬁ cation part of the test down to just two lines. Let’s review what the whole test looks like:\n\npublic void testAddItemQuantity_severalQuantity_v6(){ Address billingAddress = null; Address shippingAddress = null; Customer customer = null; Product product = null; Invoice invoice = null; try { // Set up ﬁxture billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); customer = new Customer(99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); product = new Product(88, \"SomeWidget\", new BigDecimal(\"19.99\")); invoice = new Invoice(customer); // Exercise SUT invoice.addItemQuantity(product, 5);\n\n2 It’s a good thing we are not being rewarded for the number of lines of code we write! This is yet another example of why KLOC is such a poor measure of productivity.\n\nwww.it-ebooks.info\n\nxlix\n\nl\n\nRefactoring a Test\n\n// Verify outcome LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); } ﬁnally { // Teardown deleteObject(invoice); deleteObject(product); deleteObject(customer); deleteObject(billingAddress); deleteObject(shippingAddress); } }\n\nCleaning Up the Fixture Teardown Logic\n\nNow that we have cleaned up the result veriﬁ cation logic, let’s turn our atten- tion to the ﬁ nally block at the end of the test. What is this code doing?\n\n} ﬁnally { // Teardown deleteObject(invoice); deleteObject(product); deleteObject(customer); deleteObject(billingAddress); deleteObject(shippingAddress); }\n\nMost modern languages have an equivalent construct to the try/ﬁ nally block that can be used to ensure that code gets run even when an error or exception occurs. In a Test Method (page 348), the ﬁ nally block ensures that any cleanup code gets run regardless of whether the test passed or failed. A failed assertion throws an exception, which would transfer control back to the Test Automation Framework’s (page 298) exception-handling code, so we use the ﬁ nally block to clean up ﬁ rst. This approach means that we avoid having to catch the exception and then rethrow it.\n\nIn this test, the ﬁ nally block calls the deleteObject method on each of the objects created by the test. Unfortunately, this code suffers from a fatal ﬂ aw. Have you noticed it yet?\n\nThings could go wrong during the teardown itself. What happens if the ﬁ rst call to deleteObject throws an exception? As coded here, none of the other calls to deleteObject would be executed. The solution is to use a nested try/ﬁ nally block around this ﬁ rst call, thereby ensuring that the second call to deleteObject always executes. But what if the second call fails? In this case, we would need a total\n\nwww.it-ebooks.info\n\nCleaning Up the Test\n\nof six nested try/ﬁ nally blocks to make this maneuver work. That would almost double the length of the test, and we cannot afford to write and maintain so much code in each test.\n\n} ﬁnally { // Teardown try { deleteObject(invoice); } ﬁnally { try { deleteObject(product); } ﬁnally { try { deleteObject(customer); } ﬁnally { try { deleteObject(billingAddress); } ﬁnally { deleteObject(shippingAddress); } } } }\n\nThe problem is that we now have a Complex Teardown (see Obscure Test). What are the chances of getting this code right? And how do we test the test code? Clearly, our current approach is not going to be very effective.\n\nOf course, we could move this code into the tearDown method. That would have the advantage of removing it from the Test Method. Also, because the tearDown method acts as a ﬁ nally block, we would get rid of the outermost try/ ﬁ nally. Unfortunately, this strategy doesn’t address the root of the problem: the need to write detailed teardown code in each test.\n\nWe could try to avoid creating the objects in the ﬁ rst place by using a Shared Fixture (page 317) that is not torn down between tests. Unfortunately, this approach is likely to lead to a number of test smells, including Unrepeatable Test (see Erratic Test on page 228) and Interacting Tests (see Erratic Test), caused by interactions via the shared ﬁ xture. Another issue is that the references to objects used from the shared ﬁ xture are often Mystery Guests (see Obscure Test).3\n\nThe best solution is to use a Fresh Fixture (page 311) but to avoid writ- ing teardown code for every test. To do so, we can use an in-memory ﬁ xture that is automatically garbage collected. This approach won’t work, however, if the objects we create are persistent (e.g., if they are saved in a database). While it is best to construct the system architecture so that most of our tests can\n\n3 The test reader cannot see the objects being used by the test.\n\nwww.it-ebooks.info\n\nli\n\nlii\n\nRefactoring a Test\n\nbe executed without the database, we almost always have some tests that need it. In these cases, we can extend the Test Automation Framework to do most of the work for us. We can add a means to register each object we create with the framework so that it can do the deleting for us.\n\nFirst, we need to register each object as we create it:\n\n// Set up ﬁxture billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); registerTestObject(billingAddress); shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\",\"T2N 2V2\", \"Canada\"); registerTestObject(shippingAddress); customer = new Customer(99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); registerTestObject(shippingAddress); product = new Product(88, \"SomeWidget\", new BigDecimal(\"19.99\")); registerTestObject(shippingAddress); invoice = new Invoice(customer); registerTestObject(shippingAddress);\n\nRegistration consists of adding the object to a collection of test objects:\n\nList testObjects;\n\nprotected void setUp() throws Exception { super.setUp(); testObjects = new ArrayList(); }\n\nprotected void registerTestObject(Object testObject) { testObjects.add(testObject); }\n\nIn the tearDown method, we iterate through the list of test objects and delete each one:\n\npublic void tearDown() { Iterator i = testObjects.iterator(); while (i.hasNext()) { try { deleteObject(i.next()); } catch (RuntimeException e) { // Nothing to do; we just want to make sure // we continue on to the next object in the list } } }\n\nwww.it-ebooks.info\n\nCleaning Up the Test\n\nNow our test looks like this:\n\npublic void testAddItemQuantity_severalQuantity_v8(){ Address billingAddress = null; Address shippingAddress = null; Customer customer = null; Product product = null; Invoice invoice = null; // Set up ﬁxture billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); registerTestObject(billingAddress); shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\",\"T2N 2V2\", \"Canada\"); registerTestObject(shippingAddress); customer = new Customer(99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); registerTestObject(shippingAddress); product = new Product(88, \"SomeWidget\", new BigDecimal(\"19.99\")); registerTestObject(shippingAddress); invoice = new Invoice(customer); registerTestObject(shippingAddress); // Exercise SUT invoice.addItemQuantity(product, 5); // Verify outcome LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\nWe have been able to remove the try/ﬁ nally block and, except for the additional calls to registerTestObject, our code is much simpler. But we can still clean this code up a bit more. Why, for example, do we need to declare the variables and initialize them to null, only to reinitialize them later? This action was needed with the original test because they had to be accessible in the ﬁ nally block; now that we have removed this block, we can combine the declaration with the initialization:\n\npublic void testAddItemQuantity_severalQuantity_v9(){ // Set up ﬁxture Address billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); registerTestObject(billingAddress); Address shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\");\n\nwww.it-ebooks.info\n\nliii\n\nliv\n\nRefactoring a Test\n\nregisterTestObject(shippingAddress); Customer customer = new Customer(99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); registerTestObject(shippingAddress); Product product = new Product(88, \"SomeWidget\", new BigDecimal(\"19.99\")); registerTestObject(shippingAddress); Invoice invoice = new Invoice(customer); registerTestObject(shippingAddress); // Exercise SUT invoice.addItemQuantity(product, 5); // Verify outcome LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.95\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\nCleaning Up the Fixture Setup\n\nNow that we have cleaned up the assertions and the ﬁ xture teardown, let’s turn our attention to the ﬁ xture setup. One obvious “quick ﬁ x” would be to take each of the calls to a constructor, take the subsequent call to registerTestObject, and use an Extract Method refactoring to deﬁ ne a Creation Method (page 415). This will make the test a bit simpler to read and write. The use of Creation Methods has another advantage: They encapsulate the API of the SUT and reduce the test maintenance effort when the various object constructors change by allowing us to modify only a single place rather than having to change each test.\n\npublic void testAddItemQuantity_severalQuantity_v10(){ // Set up ﬁxture Address billingAddress = createAddress( \"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); Address shippingAddress = createAddress( \"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); Customer customer = createCustomer( 99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); Product product = createProduct( 88,\"SomeWidget\",new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, 5);\n\nwww.it-ebooks.info\n\nCleaning Up the Test\n\n// Verify outcome LineItem expected = new LineItem(invoice, product,5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\nThis ﬁ xture setup logic still suffers from several problems. The ﬁ rst problem is that it is difﬁ cult to tell how the ﬁ xture is related to the expected outcome of the test. Do the customer’s particulars affect the outcome in some way? Does the customer’s address affect the outcome? What is this test really verifying?\n\nThe other problem is that this test exhibits Hard-Coded Test Data (see Obscure Test). Given that our SUT persists all objects we create in a database, the use of Hard-Coded Test Data may result in an Unrepeatable Test, an Interacting Test, or a Test Run War (see Erratic Test) if any of the ﬁ elds of the customer, product, or invoice must be unique.\n\nWe can solve this problem by generating a unique value for each test and then using that value to seed the attributes of the objects we create for the test. This approach will ensure that the test creates different objects each time the test is run. Because we have already moved the object creation logic into Creation Meth- ods, this step is relatively easy; we just put this logic into the Creation Method and remove the corresponding parameters. This is another application of the Extract Method refactoring, in which we create a new, parameterless version of the Cre- ation Method.\n\npublic void testAddItemQuantity_severalQuantity_v11(){ ﬁnal int QUANTITY = 5; // Set up ﬁxture Address billingAddress = createAnAddress(); Address shippingAddress = createAnAddress(); Customer customer = createACustomer(new BigDecimal(\"30\"), billingAddress, shippingAddress); Product product = createAProduct(new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, QUANTITY); // Verify outcome LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); } private Product createAProduct(BigDecimal unitPrice) { BigDecimal uniqueId = getUniqueNumber(); String uniqueString = uniqueId.toString(); return new Product(uniqueId.toBigInteger().intValue(), uniqueString, unitPrice); }\n\nwww.it-ebooks.info\n\nlv\n\nlvi\n\nRefactoring a Test\n\nWe call this pattern an Anonymous Creation Method (see Creation Method) because we are declaring that we don’t care about the particulars of the object. If the expected behavior of the SUT depends on a particular value, we can either pass the value as a parameter or imply it in the name of the creation method.\n\nThis test looks a lot better now, but we are not done yet. Does the expected outcome depend in any way on the addresses of the customer? If not, we can hide their construction completely by using an Extract Method refactoring (again!) to create a version of the createACustomer method that fabricates them for us.\n\npublic void testAddItemQuantity_severalQuantity_v12(){ // Set up ﬁxture Customer cust = createACustomer(new BigDecimal(\"30\")); Product prod = createAProduct(new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(cust); // Exercise SUT invoice.addItemQuantity(prod, 5); // Verify outcome LineItem expected = new LineItem(invoice, prod, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\nBy moving the calls that create the addresses into the method that creates the customer, we have made it clear that the addresses do not affect the logic that we are verifying in this test. The outcome does depend on the customer’s dis- count, however, so we pass the discount percentage to the customer creation method.\n\nWe still have one or two things to clean up. For example, the Hard-Coded Test Data for the unit price, quantity, and customer’s discount is repeated twice in the test. We can clarify the meaning of these numbers by using a Replace Magic Number with Symbolic Constant [Fowler] refactoring to give them role- describing names. Also, the constructor we are using to create the LineItem is not used anywhere in the SUT itself because the LineItem normally calculates the extendedCost when it is constructed. We should turn this test-speciﬁ c code into a Foreign Method [Fowler] implemented within the test harness. We have already seen examples of how to do so with the Customer and Product: We use a Param- eterized Creation Method (see Creation Method) to return the expected LineItem based on only those values of interest.\n\npublic void testAddItemQuantity_severalQuantity_v13(){ ﬁnal int QUANTITY = 5; ﬁnal BigDecimal UNIT_PRICE = new BigDecimal(\"19.99\"); ﬁnal BigDecimal CUST_DISCOUNT_PC = new BigDecimal(\"30\");\n\nwww.it-ebooks.info\n\nThe Cleaned-Up Test\n\n// Set up ﬁxture Customer customer = createACustomer(CUST_DISCOUNT_PC); Product product = createAProduct( UNIT_PRICE); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, QUANTITY); // Verify outcome ﬁnal BigDecimal EXTENDED_PRICE = new BigDecimal(\"69.96\"); LineItem expected = new LineItem(invoice, product, QUANTITY, CUST_DISCOUNT_PC, EXTENDED_PRICE); assertContainsExactlyOneLineItem(invoice, expected); }\n\nOne ﬁ nal point: Where did the value “69.96” come from? If this value comes from the output of some reference system, we should say so. Because it was just manually calculated and typed into the test, we can show the calculation in the test for the test reader’s beneﬁ t.\n\nThe Cleaned-Up Test\n\nHere is the ﬁ nal cleaned-up version of the test:\n\npublic void testAddItemQuantity_severalQuantity_v14(){ ﬁnal int QUANTITY = 5; ﬁnal BigDecimal UNIT_PRICE = new BigDecimal(\"19.99\"); ﬁnal BigDecimal CUST_DISCOUNT_PC = new BigDecimal(\"30\"); // Set up ﬁxture Customer customer = createACustomer(CUST_DISCOUNT_PC); Product product = createAProduct( UNIT_PRICE); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, QUANTITY); // Verify outcome ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE.multiply(new BigDecimal(QUANTITY)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUST_DISCOUNT_PC.movePointLeft(2))); LineItem expected = createLineItem(QUANTITY, CUST_DISCOUNT_PC, EXTENDED_PRICE, product, invoice); assertContainsExactlyOneLineItem(invoice, expected); }\n\nWe have used an Introduce Explaining Variable [Fowler] refactoring to better document the calculation of the BASE_PRICE (price*quantity) and EXTENDED_PRICE (the price with discount). The revised test is now much smaller and clearer than\n\nwww.it-ebooks.info\n\nlvii\n\nlviii\n\nRefactoring a Test\n\nthe bulky code we started with. It fulﬁ lls the role of Tests as Documentation (see page 23) very well. So what did we discover that this test veriﬁ es? It con- ﬁ rms that the line items added to an invoice are, indeed, added to the invoice and that the extended cost is based on the product price, the customer’s dis- count, and the quantity ordered.\n\nWriting More Tests\n\nIt seemed like we went to a lot of effort to refactor this test to make it clearer. Will we have to spend so much effort on every test?\n\nI should hope not! Much of the effort here related to the discovery of which Test Utility Methods (page 599) were required for writing the test. We deﬁ ned a Higher-Level Language (see page 41) for testing our application. Once we have those methods in place, writing other tests becomes much simpler. For example, if we want to write a test that veriﬁ es that the extended cost is recalculated when we change the quantity of a LineItem, we can reuse most of the Test Utility Methods.\n\npublic void testAddLineItem_quantityOne(){ ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE; ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE; // Set up ﬁxture Customer customer = createACustomer(NO_CUST_DISCOUNT); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(PRODUCT, QUAN_ONE); // Verify outcome LineItem expected = createLineItem( QUAN_ONE, NO_CUST_DISCOUNT, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\npublic void testChangeQuantity_severalQuantity(){ ﬁnal int ORIGINAL_QUANTITY = 3; ﬁnal int NEW_QUANTITY = 5; ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE.multiply( new BigDecimal(NEW_QUANTITY)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUST_DISCOUNT_PC.movePointLeft(2))); // Set up ﬁxture Customer customer = createACustomer(CUST_DISCOUNT_PC); Invoice invoice = createInvoice(customer); Product product = createAProduct( UNIT_PRICE); invoice.addItemQuantity(product, ORIGINAL_QUANTITY);\n\nwww.it-ebooks.info\n\nFurther Compaction\n\n// Exercise SUT invoice.changeQuantityForProduct(product, NEW_QUANTITY); // Verify outcome LineItem expected = createLineItem( NEW_QUANTITY, CUST_DISCOUNT_PC, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\nThis test was written in about two minutes and did not require adding any new Test Utility Methods. Contrast that with how long it would have taken to write a completely new test in the original style. And the effort saved in writing the tests is just part of the equation—we also need to consider the effort we saved understanding existing tests each time we need to revisit them. Over the course of a development project and the subsequent maintenance activity, this cost sav- ings will really add up.\n\nFurther Compaction\n\nWriting these additional tests revealed a few more sources of Test Code Duplication (page 213). For example, it seems that we always create both a Customer and an Invoice. Why not combine these two lines? Similarly, we continually deﬁ ne and initialize the QUANTITY and CUSTOMER_DISCOUNT_PC constants inside our test methods. Why can’t we do these tasks just once? The Product does not seem to play any roles in these tests; we always create it exactly the same way. Can we factor this responsibility out, too? Certainly! We just apply an Extract Method refactoring to each set of duplicated code to create more powerful Creation Methods.\n\npublic void testAddItemQuantity_severalQuantity_v15(){ // Set up ﬁxture Invoice invoice = createCustomerInvoice(CUST_DISCOUNT_PC); // Exercise SUT invoice.addItemQuantity(PRODUCT, SEVERAL); // Verify outcome ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE.multiply(new BigDecimal(SEVERAL)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUST_DISCOUNT_PC.movePointLeft(2))); LineItem expected = createLineItem( SEVERAL, CUST_DISCOUNT_PC, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem(invoice, expected); }\n\npublic void testAddLineItem_quantityOne_v2(){ ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE; ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE;\n\nwww.it-ebooks.info\n\nlix\n\nlx\n\nRefactoring a Test\n\n// Set up ﬁxture Invoice invoice = createCustomerInvoice(NO_CUST_DISCOUNT); // Exercise SUT invoice.addItemQuantity(PRODUCT, QUAN_ONE); // Verify outcome LineItem expected = createLineItem( SEVERAL, CUST_DISCOUNT_PC, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\npublic void testChangeQuantity_severalQuantity_V2(){ ﬁnal int NEW_QUANTITY = SEVERAL + 2; ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE.multiply( new BigDecimal(NEW_QUANTITY)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUST_DISCOUNT_PC.movePointLeft(2))); // Set up ﬁxture Invoice invoice = createCustomerInvoice(CUST_DISCOUNT_PC); invoice.addItemQuantity(PRODUCT, SEVERAL); // Exercise SUT invoice.changeQuantityForProduct(PRODUCT, NEW_QUANTITY); // Verify outcome LineItem expected = createLineItem( NEW_QUANTITY, CUST_DISCOUNT_PC, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\nWe have now reduced the number of lines of code we need to understand from 35 statements in the original test to just 6 statements.4 We are left with just a bit more than one sixth of the original code to maintain! We could go further by factoring out the ﬁ xture setup into a setUp method, but that effort would be worthwhile only if a lot of tests needed the same Customer/Discount/Invoice conﬁ guration. If we wanted to reuse these Test Utility Methods from other Testcase Classes (page 373), we could use an Extract Superclass [Fowler] refactoring to create a Testcase Super- class (page 638), and then use a Pull Up Method [Fowler] refactoring to move the Test Utility Methods to it so they can be reused.\n\n4 Ignoring wrapped lines, we have 6 executable statements surrounded by the two lines of method declarations/end.\n\nwww.it-ebooks.info\n\nPART I\n\nThe Narratives\n\nwww.it-ebooks.info\n\nThis page intentionally left blank\n\nwww.it-ebooks.info",
      "page_number": 24
    },
    {
      "number": 2,
      "title": "A Brief Tour",
      "start_page": 66,
      "end_page": 71,
      "detection_method": "regex_chapter",
      "content": "Chapter 1\n\nA Brief Tour\n\nAbout This Chapter\n\nThere are a lot of principles, patterns, and smells in this book—and even more pat- terns that couldn’t ﬁ t into the book. Do you need to learn them all? Do you need to use them all? Probably not! This chapter provides an abbreviated introduction to the bulk of the material in the entire book. You can use it as a quick tour of the material before diving into particular patterns or smells of interest. You can also use it as a warm-up before exploring the more detailed narrative chapters.\n\nThe Simplest Test Automation Strategy That Could Possibly Work\n\nThere is a simple test automation strategy that will work for many, many projects. This section describes this minimal test strategy. The principles, pat- terns, and smells referenced here are the core patterns that will serve us well in the long run. If we learn to apply them effectively, we will probably be success- ful in our test automation endeavors. If we ﬁ nd that we really cannot make the minimal test strategy work on our project by using these patterns, we can fall back to the alternative patterns listed in the full descriptions of these patterns and in the other narratives.\n\nI have laid out this simple strategy in ﬁ ve parts:\n\nDevelopment Process: How the process we use to develop the code\n\naffects our tests.\n\nCustomer Tests: The ﬁ rst tests we should write as the ultimate deﬁ ni-\n\ntion of “what done looks like.”\n\n3\n\nwww.it-ebooks.info\n\n4\n\nChapter 1 A Brief Tour\n\nUnit Tests: The tests that help our design emerge incrementally and\n\nensure that all our code is tested.\n\nDesign for Testability: The patterns that make our design easier to test,\n\nthereby reducing the cost of test automation.\n\nTest Organization: How we can organize our Test Methods (page 348)\n\nand Testcase Classes (page 373).\n\nDevelopment Process\n\nFirst things ﬁ rst: When do we write our tests? Writing tests before we write our software has several beneﬁ ts. In particular, it gives us an agreed-upon deﬁ nition of what success looks like.1\n\nWhen doing new software development, we strive to do storytest-driven development by ﬁ rst automating a suite of customer tests that verify the func- tionality provided by the application. To ensure that all of our software is tested, we augment these tests with a suite of unit tests that verify all code paths or, at a minimum, all the code paths that are not covered by the customer tests. We can use code coverage tools to discover which code is not being exercised and then retroﬁ t unit tests to accommodate the untested code.2\n\nBy organizing the unit tests and customer tests into separate test suites, we ensure that we can run just the unit tests or just the customer tests if neces- sary. The unit tests should always pass before we check them in; this is what we mean by the phrase “keep the bar green.” To ensure that the unit tests are run frequently, we can include them in the Smoke Tests [SCM] that are run as part of the Integration Build [SCM]. Although many of the customer tests will fail until the corresponding functionality is built, it is nevertheless useful to run all the passing customer tests as part of the integration build phase—but only if this step does not slow the build down too much. In that case, we can leave them out of the check-in build and simply run them every night.\n\nWe can ensure that our software is testable by doing test-driven development (TDD). That is, we write the unit tests before we write the code, and we use the tests to help us deﬁ ne the software’s design. This strategy helps concentrate all the business logic that needs veriﬁ cation in well-deﬁ ned objects that can be tested independently of the database. Although we should also have unit tests\n\n1 If our customer cannot deﬁ ne the tests before we have built the software, we have every reason to be worried! 2 We will likely ﬁ nd fewer Missing Unit Tests (see Production Bugs on page 268) when we practice test-driven development than if we adopt a “test last” policy. Even so, there is still value in running the code coverage tools with TDD.\n\nwww.it-ebooks.info\n\nThe Simplest Test Automation Strategy That Could Possibly Work\n\nfor the data access layer and the database, we try to keep the dependency on the database to a minimum in the unit tests for the business logic.\n\nCustomer Tests\n\nThe customer tests should capture the essence of what the customer wants the system to do. Enumerating the tests before we begin their development is an important step whether or not we actually automate the tests, because it helps the development team understand what the customer really wants; these tests deﬁ ne what success looks like. We can automate the tests using Scripted Tests (page 285) or Data-Driven Tests (page 288) depending on who is pre- paring the tests; customers can take part in test automation if we use Data- Driven Tests. On rare occasions, we might even use Recorded Tests (page 278) for regression testing an existing application while we refactor the application to improve its testability. Of course, we usually discard these tests once we have developed other tests that cover the functionality, because Recorded Tests tend to be Fragile Tests (page 239).\n\nDuring their development, we strive to make our customer tests represen- tative of how the system is really used. Unfortunately, this goal often conﬂ icts with attempts to keep the tests from becoming too long, because long tests are often Obscure Tests (page 186) and tend not to provide very good Defect Localization (see page 22) when they fail partway through the test. We can also use well-written Tests as Documentation (see page 23) to identify how the system is supposed to work. To keep the tests simple and easy to understand, we can bypass the user interface by performing Subcutaneous Testing (see Layer Test on page 337) against one or more Service Facades [CJ2EEP]. Service Facades encap- sulate all of the business logic behind a simple interface that is also used by the presentation layer.\n\nEvery test needs a starting point. As part of our testing plan, we take care that each test sets up this starting point, known as the test ﬁ xture, each time the test is run. This Fresh Fixture (page 311) helps us avoid Interacting Tests (see Erratic Test on page 228) by ensuring that tests do not depend on anything they did not set up themselves. We avoid using a Shared Fixture (page 317), unless it is an Immutable Shared Fixture, to avoid starting down the slippery slope to Erratic Tests.\n\nIf our application normally interacts with other applications, we may need to isolate it from any applications that we do not have in our development en- vironment by using some form of Test Double (page 522) for the objects that act as interfaces to the other applications. If the tests run too slowly because of database access or other slow components, we can replace them with functionally\n\nwww.it-ebooks.info\n\n5\n\n6\n\nChapter 1 A Brief Tour\n\nequivalent Fake Objects (page 551) to speed up our tests, thereby encouraging developers to run them more regularly. If at all possible, we avoid using Chained Tests (page 454)—they are just the test smell Interacting Tests in disguise.\n\nUnit Tests\n\nFor our unit tests to be effective, each one should be a Fully Automated Test (page 26) that does a round-trip test against a class through its public interface. We can strive for Defect Localization by ensuring that each test is a Single- Condition Test (see page 45) that exercises a single method or object in a single scenario. We should also write our tests so that each part of the Four-Phase Test (page 358) is easily recognizable, which enables us to use the Tests as Docu- mentation.\n\nWe use a Fresh Fixture strategy so that we do not have to worry about In- teracting Tests or ﬁ xture teardown. We begin by creating a Testcase Class for each class we are testing (see Testcase Class per Class on page 617), with each test being a separate Test Method on that class. Each Test Method can use Del- egated Setup (page 411) to build a Minimal Fixture (page 302) that makes the tests easily understood by calling well-named Creation Methods (page 415) to build the objects required for each test ﬁ xture.\n\nTo make the tests self-checking (Self-Checking Test; see page 26), we express the expected outcome of each test as one or more Expected Objects (see State Veriﬁ cation on page 462) and compare them with the actual objects returned by the system under test (SUT) using the built-in Equality Assertions (see Assertion Method on page 362) or Custom Assertions (page 474) that implement our own test-speciﬁ c equality. If several tests are expected to result in the same outcome, we can factor out the veriﬁ cation logic into an outcome- describing Veriﬁ cation Method (see Custom Assertion) that the test reader can more easily recognize.\n\nIf we have Untested Code (see Production Bugs on page 268) because we cannot ﬁ nd a way to execute the path through the code, we can use a Test Stub (page 529) to gain control of the indirect inputs of the SUT. If there are Untested Requirements (see Production Bugs) because not all of the system’s behavior is observable via its public interface, we can use a Mock Object (page 544) to intercept and verify the indirect outputs of the SUT.\n\nwww.it-ebooks.info\n\nThe Simplest Test Automation Strategy That Could Possibly Work\n\nDesign for Testability\n\nAutomated testing is much simpler if we adopt a Layered Architecture [DDD, PEAA, WWW]. At a minimum, we should separate our business logic from the database and the user interface, thereby enabling us to test it easily using either Subcutaneous Tests or Service Layer Tests (see Layer Test). We can minimize any dependence on a Database Sandbox (page 650) by doing most—if not all—of our testing using in-memory objects only. This scheme lets the runtime environ- ment implement Garbage-Collected Teardown (page 500) for us automatically, meaning that we can avoid writing potentially complex, error-prone teardown logic (a sure source of Resource Leakage; see Erratic Test). It also helps us avoid Slow Tests (page 253) by reducing disk I/O, which is much slower than memory manipulation.\n\nIf we are building a GUI, we should try to keep the complex GUI logic out of the visual classes. Using a Humble Dialog (see Humble Object on page 695) that delegates all decision making to nonvisual classes allows us to write unit tests for the GUI logic (e.g., enabling/disabling buttons) without having to instantiate the graphical objects or the framework on which they depend.\n\nIf the application is complex enough or if we are expected to build compo- nents that will be reused by other projects, we can augment the unit tests with component tests that verify the behavior of each component in isolation. We will probably need to use Test Doubles to replace any components on which our component depends. To install the Test Doubles at runtime, we can use either Dependency Injection (page 678), Dependency Lookup (page 686), or a Subclassed Singleton (see Test-Speciﬁ c Subclass on page 579).\n\nTest Organization\n\nIf we end up with too many Test Methods on our Testcase Class, we can con- sider splitting the class based on either the methods (or features) veriﬁ ed by the tests or their ﬁ xture needs. These patterns are called Testcase Class per Fea- ture (page 624) and Testcase Class per Fixture (page 631), respectively. Testcase Class per Fixture allows us to move all of the ﬁ xture setup code into the setUp method, an approach called Implicit Setup (page 424). We can then aggregate the Test Suite Objects (page 387) for the resulting Testcase Classes into a single Test Suite Object, resulting in a Suite of Suites (see Test Suite Object) containing all the tests from the original Testcase Class. This Test Suite Object can, in turn, be added to the Test Suite Object for the containing package or namespace. We can then run all of the tests or just a subset that is relevant to the area of the software in which we are working.\n\nwww.it-ebooks.info\n\n7\n\n8\n\nChapter 1 A Brief Tour\n\nWhat’s Next?\n\nThis whirlwind tour of the most important goals, principles, patterns, and smells is just a brief introduction to test automation. Chapters 2 through 14 give a more detailed overview of each area touched upon here. If you have already spotted some patterns or smells you want to learn more about, you can certainly proceed directly to the detailed descriptions in Parts II and III. Other- wise, your next step is to delve into the subsequent narratives, which provide a somewhat more in-depth examination of these patterns and the alternatives to them. First up is Chapter 2, Test Smells, which describes some common “test smells” that motivate much of the refactoring we do on our tests.\n\nwww.it-ebooks.info",
      "page_number": 66
    },
    {
      "number": 3,
      "title": "Test Smells",
      "start_page": 72,
      "end_page": 81,
      "detection_method": "regex_chapter",
      "content": "Chapter 2\n\nTest Smells\n\nAbout This Chapter\n\nChapter 1, A Brief Tour, provided a very quick introduction to the core patterns and smells covered in this book. This chapter provides a more detailed examina- tion of the “test smells” we are likely to encounter on our projects. We explore the basic concept of test smells ﬁ rst, and then move on to investigate the smells in three broad categories: test code smells, automated test behavior smells, and project smells related to automated testing.\n\nAn Introduction to Test Smells\n\nIn his book Refactoring: Improving the Design of Existing Code, Martin Fowler documented a number of ways that the design of code can be changed without actually changing what the code does. The motivation for this refactoring was the identiﬁ cation of “bad smells” that frequently occur in object-oriented code. These code smells were described in a chapter coauthored by Kent Beck that started with the famous quote from Grandma Beck: “If it stinks, change it.” The context of this quote was the question, “How do you know you need to change a baby’s diaper?” And so a new term was added to the programmer’s lexicon.\n\nThe code smells described in Refactoring focused on problems commonly found in production code. Many of us had long suspected that there were smells unique to automated test scripts. At XP2001, the paper “Refactoring Test Code” [RTC] conﬁ rmed these suspicions by identifying a number of “bad smells” that occur speciﬁ cally in test code. The authors also recommended a set of refactorings that can be applied to the tests to remove the noxious smells.\n\nThis chapter provides an overview of these test smells. More detailed ex-\n\namples of each test smell can be found in the reference section.\n\n9\n\nwww.it-ebooks.info\n\n10\n\nChapter 2 Test Smells\n\nWhat’s a Test Smell?\n\nA smell is a symptom of a problem. A smell doesn’t necessarily tell us what is wrong, because a particular smell may originate from any of several sources. Most of the smells in this book have several different named causes; some causes even appear under several smells. That’s because a root cause may reveal itself through several different symptoms (i.e., smells).\n\nNot all problems are considered smells, and some problems may even be the root cause of several smells. The “Occam’s razor” test for deciding whether something really is a smell (versus just a problem) is the “sniffability test.” That is, the smell must grab us by the nose and say, “Something is wrong here.” As discussed in the next section, I have classiﬁ ed the smells based on the kinds of symptoms they exhibit (how they “grab us by the nose”).\n\nBased on the “sniffability” criteria, I have demoted some of the test smells listed in prior papers and articles to “cause” status. I have mostly left their names unchanged so that we can still refer to them when talking about a par- ticular side effect of applying a pattern. In this case, it is more appropriate to refer directly to the cause rather than to the more general but sniffable smell.\n\nKinds of Test Smells\n\nOver the years we have discovered that there are at least two different kinds of smells: code smells, which must be recognized when looking at code, and behav- ior smells, which affect the outcome of tests as they execute.\n\nCode smells are coding-level anti-patterns that a developer, tester, or coach may notice while reading or writing test code. That is, the code just doesn’t look quite right or doesn’t communicate its intent very clearly. Code smells must ﬁ rst be recognized before we can take any action, and the need for action may not be equally obvious to everyone. Code smells apply to all kinds of tests, including both Scripted Tests (page 285) and Recorded Tests (page 278). They become particu- larly relevant for Recorded Tests when we must maintain the recorded code. Un- fortunately, most Recorded Tests suffer from Obscure Tests (page 186), because they are recorded by a tool that doesn’t know what is relevant to the human reader. Behavior smells, by contrast, are much more difﬁ cult to ignore because they cause tests to fail (or not compile at all) at the most inopportune times, such as when we are trying to integrate our code into a crucial build; we are forced to unearth the problems before we can “make the bar green.” Like code smells, behavior smells are relevant to both Scripted Tests and Recorded Tests.\n\nwww.it-ebooks.info\n\nAn Introduction To Test Smells\n\nDevelopers typically notice code and behavior smells when they automate, maintain, and run tests. More recently, we have identiﬁ ed a third kind of smell—a smell that is usually noticed by the project manager or the customer, who does not look at the test code or run the tests. These project smells are in- dicators of the overall health of a project.\n\nWhat to Do about Smells?\n\nSome smells are inevitable simply because they take too much effort to elimi- nate. The important thing is that we are aware of the smells and know what causes them. We can then make a conscious decision about which ones we must address to keep the project running efﬁ ciently.\n\nThe decision of which smells must be eliminated comes down to the balance between cost and beneﬁ t. Some smells are harder to stamp out than others; some smells cause more grief than others. We need to eradicate those smells that cause us the most grief because they will keep us from being suc- cessful. That being said, many smells can be avoided by selecting a sound test automation strategy and by following good test automation coding standards.\n\nWhile we carefully delineated the various types of smells, it is important to note that very often we will observe symptoms of each kind of smell at the same time. Project smells, for example, are the project-level symptoms of some underlying cause. That cause may show up as a behavior smell but ultimately there is probably an underlying code smell that is the root cause of the problem. The good news: We have three different ways to identify a problem. The bad news: It is easy to focus on the symptom at one level and to try to solve that problem directly without understanding the root cause.\n\nA very effective technique for identifying the root cause is the “Five Why’s” [TPS]. First, we ask why something is occurring. Once we have identiﬁ ed the factors that led to it, we next ask why each of those factors occurred. We repeat this process until no new information is forthcoming. In practice, asking why ﬁ ve times is usually enough—hence the name “Five Why’s.”1\n\nIn the rest of this chapter, we will look at the test-related smells that we are most likely to encounter on our projects. We will begin with the project smells, and then work our way down to the behavior smells and code smells that cause them.\n\n1 This practice is also called “root cause analysis” or “peeling the onion” in some circles.\n\nwww.it-ebooks.info\n\n11\n\n12\n\nChapter 2 Test Smells\n\nA Catalog of Smells\n\nNow that we have a better understanding of test smells and their role in projects that use automated testing, let’s look at some smells. Based on the “sniffability” criteria outlined earlier, this section focuses on introducing the smells. Discus- sions of their causes and the individual smell descriptions appear in Part II of this book.\n\nThe Project Smells\n\nProject smells are symptoms that something has gone wrong on the project. Their root cause is likely to be one or more of the code or behavior smells. Because proj- ect managers rarely run or write tests, however, project smells are likely to be the ﬁ rst hint they get that something may be less than perfect in test automation land. Project managers focus most on functionality, quality, resources, and cost. For this reason, the project-level smells tend to cluster around these issues. The most obvious metric a project manager is likely to encounter as a smell is the quality of the software as measured in defects found in formal testing or by users/customers. If the number of Production Bugs (page 268) is higher than expected, the project manager must ask, “Why are all of these bugs getting through our safety net of automated tests?”\n\nThe project manager may be monitoring the number of times the daily in- tegration build fails as a way of getting an early indication of software quality and adherence to the team’s development process. The manager may become worried if the build fails too frequently, and especially if it takes more than a few minutes to ﬁ x the build. Root cause analysis of the failures may indicate that many of the test failures are not the result of buggy software but rather derive from Buggy Tests (page 260). This is an example in which the tests cry “Wolf!” and consume a lot of resources as part of their correction, but do not actually increase the quality of the production code.\n\nBuggy Tests are just one contributor to the more general problem of High Test Maintenance Cost (page 265), which can severely affect the productivity of the team if not addressed quickly. If the tests need to be modiﬁ ed too often (e.g., every time the SUT is modiﬁ ed) or if the cost of modifying tests is too high due to Obscure Tests, the project manager may decide that the effort and ex- pense being directed toward writing the automated tests would be better spent\n\nwww.it-ebooks.info\n\nA Catalog of Smells\n\non writing more production code or doing manual testing. At this point, the manager is likely to tell the developers to stop writing tests.2\n\nAlternatively, the project manager may decide that the Production Bugs are caused by Developers Not Writing Tests (page 263). This pronouncement is likely to come during a process retrospective or as part of a root cause analysis session. Developers Not Writing Tests may be caused by an overly aggressive development schedule, supervisors who tell developers not to “waste time writ- ing tests,” or developers who do not have the skills to write tests. Other poten- tial causes might include an imposed design that is not conducive to testing or a test environment that leads to Fragile Tests (page 239). Finally, this problem could result from Lost Tests (see Production Bugs)—tests that exist but are not included in the AllTests Suite (see Named Test Suite on page 592) used by devel- opers during check-in or by the automated build tool.\n\nThe Behavior Smells\n\nBehavior smells are encountered when we compile or run tests. We don’t have to be particularly observant to notice them, as these smells will take the form of compile errors or test failures.\n\nThe most common behavior smell is Fragile Tests. It arises when tests that once passed begin failing for some reason. The Fragile Test problem has given test automation a bad name in many circles, especially when commercial “record and playback” test tools fail to deliver on their promise of easy test automation. Once recorded, these tests are very susceptible to breakage. Often the only remedy is to rerecord them because the test recordings are difﬁ cult to understand or modify by hand.\n\nThe root causes of Fragile Tests can be classiﬁ ed into four broad categories:\n\nInterface Sensitivity (see Fragile Test) occurs when tests are broken by changes to the test programming API or the user interface used to au- tomate the tests. Commercial Record and Playback Test (see Recorded Test) tools typically interact with the system via the user interface. Even minor changes to the interface can cause tests to fail, even in circum- stances in which a human user would say that the test should still pass.\n\n2 It can be hard enough to get project managers to buy into letting developers write automated tests. It is crucial that we don’t squander this opportunity by being sloppy or inefﬁ cient. The need for this balancing act is, in a nutshell, why I started writing this book: to help developers succeed and avoid giving the pessimistic project manager an excuse for calling a halt to automated unit testing.\n\nwww.it-ebooks.info\n\n13\n\n14\n\nChapter 2 Test Smells\n\nBehavior Sensitivity (see Fragile Test) occurs when tests are broken by changes to the behavior of the SUT. This may seem like a “no-brainer” (of course, the tests should break if we change the SUT!) but the issue is that only a few tests should be broken by any one change. If many or most of the tests break, we have a problem.\n\nData Sensitivity (see Fragile Test) occurs when tests are broken by changes to the data already in the SUT. This issue is particularly a problem for applications that use databases. Data Sensitivity is a spe- cial case of Context Sensitivity (see Fragile Test) where the context in question is the database.\n\nContext Sensitivity occurs when tests are broken by differences in the environment surrounding the SUT. The most common example is when tests depend on the time or date, but this problem can also arise when tests rely on the state of devices such as servers, printers, or monitors.\n\nData Sensitivity and Context Sensitivity are examples of a special kind of Frag- ile Test, known as a Fragile Fixture, in which changes to a commonly used test ﬁ xture cause multiple existing tests to fail. This scenario increases the cost of extending the Standard Fixture (page 305) to support new tests and, in turn, discourages good test coverage. Although Fragile Fixture’s root cause is poor test design, the problem actually appears when the ﬁ xture is changed rather than when the SUT is changed.\n\nMost agile projects use some form of daily or continuous integration that includes two steps: compiling the latest version of the code and running all of the automated tests against the newly compiled build. Assertion Rou- lette (page 224) can make it difﬁ cult to determine how and why tests failed during the integration build because the failure log does not include sufﬁ - cient information to clearly identify which assertion failed. Troubleshooting of the build failures may proceed slowly, because the failure must be repro- duced in the development environment before we can speculate on the cause of the failure.\n\nA common cause of grief is tests that fail for no apparent reason. That is, neither the tests nor the production code has been modiﬁ ed, yet the tests sud- denly begin failing. When we try to reproduce these results in the development environment, the tests may or may not fail. These Erratic Tests (page 228) are both very annoying and time-consuming to ﬁ x, because they have numerous possible causes. A few are listed here:\n\nwww.it-ebooks.info\n\nA Catalog of Smells\n\nInteracting Tests arise when several tests use a Shared Fixture (page 317). They make it hard to run tests individually or to run several test suites as part of a larger Suite of Suites (see Test Suite Object on page 387). They can also cause cascading failures (where a single test failure leaves the Shared Fixture in a state that causes many other tests to fail).\n\nTest Run Wars occur when several Test Runners (page 377) run tests against a Shared Fixture at the same time. They invariably happen at the worst possible time, such as when you are trying to ﬁ x the last few bugs before a release.\n\nUnrepeatable Tests provide a different result between the ﬁ rst and subsequent test runs. They may force the test automater to perform a Manual Intervention (page 250) between test runs.\n\nAnother productivity-sapping smell is Frequent Debugging (page 248). Auto- mated unit tests should obviate the need to use a debugger in all but rare cases, because the set of tests that are failing should make it obvious why the failure is occurring. Frequent Debugging is a sign that the unit tests are lacking in cover- age or are trying to test too much functionality at once.\n\nThe real value of having Fully Automated Tests (page 26) is being able to run them frequently. Agile developers who are doing test-driven development often run (at least a subset of) the tests every few minutes. This behavior should be encouraged because it shortens the feedback loop, thereby reducing the cost of any defects introduced into the code. When tests require Manual Intervention each time they are run, developers tend to run the tests less frequently. This practice increases the cost of ﬁ nding all defects introduced since the tests were last run, because more changes will have been made to the software since it was last tested.\n\nAnother smell that has the same net impact on productivity is Slow Tests (page 253). When tests take more than approximately 30 seconds to run, developers stop running them after every individual code change, instead wait- ing for a “logical time” to run them—for example, before a coffee break, lunch, or a meeting. This delayed feedback results in a loss of “ﬂ ow” and increases the time between when a defect is introduced and when it is identiﬁ ed by a test. The most frequently used solution to Slow Tests is also the most problematic; a Shared Fixture can result in many behavior smells and should be the solution of last resort.\n\nwww.it-ebooks.info\n\n15\n\n16\n\nChapter 2 Test Smells\n\nThe Code Smells\n\nCode smells are the “classic” bad smells that were ﬁ rst described by Martin Fowler in Refactoring [Ref]. Indeed, most of the smells identiﬁ ed by Fowler are code smells. These smells must be recognized by test automaters as they main- tain test code. Although code smells typically affect maintenance cost of tests, they may also be early warning signs of behavior smells to follow.\n\nWhen reading tests, a fairly obvious—albeit often overlooked—smell is Obscure Test. It can take many forms, but all versions have the same impact: It becomes difﬁ cult to tell what the test is trying to do, because the test does not Communicate Intent (page 41). This ambiguity increases the cost of test main- tenance and can lead to Buggy Tests when a test maintainer makes the wrong change to the test.\n\nA related smell is Conditional Test Logic (page 200). Tests should be simple, linear sequences of statements. When tests have multiple execution paths, we cannot be sure exactly how the test will execute in a speciﬁ c case.\n\nHard-Coded Test Data (see Obscure Test) can be insidious for several rea- sons. First, it makes tests more problematic to understand: We need to look at each value and guess whether it is related to any of the other values to under- stand how the SUT is supposed to behave. Second, it creates challenges when we are testing a SUT that includes a database. Hard-Coded Test Data can lead to Erratic Tests (if tests happen to use the same database key) or Fragile Fix- tures (if the values refer to records in the database that have been changed).\n\nHard-to-Test Code (page 209) may be a contributing factor to a number of other code and behavior smells. This problem is most obvious to the person who is writing a test and cannot ﬁ nd a way to set up the ﬁ xture, exercise the SUT, or verify the expected outcome. The test automater may then be forced to test more software (a larger SUT consisting of many classes) than he or she would like. When reading a test, Hard-to-Test Code tends to show up as an Obscure Test because of the hoops the test automater had to jump through to interact with the SUT.\n\nTest Code Duplication (page 213) is a poor practice because it increases the cost of maintaining tests. We have more test code to maintain and that code is more challenging to maintain because it often coincides with an Obscure Test. Duplication often arises when the automated tester clones tests and does not put enough thought into how to reuse test logic intelligently.3 As testing needs emerge, it is important that the test automater factor out commonly used sequences of statements into Test Utility Methods (page 599) that can be reused\n\n3 Note that I said “reuse test logic” and not “reuse Test Methods.”\n\nwww.it-ebooks.info\n\nWhat’s Next\n\nby various Test Methods (page 348).4 This practice reduces the maintenance cost of tests in several ways.\n\nTest Logic in Production (page 217) is undesirable because there is no way to ensure that it will not run accidentally.5 It also makes the production code larger and more complicated. Finally, this error may cause other software com- ponents or libraries to be included in the executable.\n\nWhat’s Next?\n\nIn this chapter, we saw a plethora of things that can go wrong when automating tests. Chapter 3, Goals of Test Automation, describes the goals we need to keep in mind while automating tests so that we can have an effective test automation experience. That understanding will prepare us to look at the principles that will help us steer clear of many of the problems described in this chapter.\n\n4 It is equally important that we do not reuse Test Methods, as that practice results in Flexible Tests (see Conditional Test Logic). 5 See the sidebar on Ariane (page 218) for a cautionary tale.\n\nwww.it-ebooks.info\n\n17\n\nThis page intentionally left blank\n\nwww.it-ebooks.info",
      "page_number": 72
    },
    {
      "number": 4,
      "title": "Goals of Test Automation",
      "start_page": 82,
      "end_page": 101,
      "detection_method": "regex_chapter",
      "content": "Chapter 3\n\nGoals of Test Automation\n\nAbout This Chapter\n\nChapter 2, Test Smells, introduced the various “test smells” that can act as symptoms of problems with automated testing. This chapter describes the goals we should be striving to reach to ensure successful automated unit tests and customer tests. It begins with a general discussion of why we automate tests, then turns to a description of the overall goals of test automation, including re- ducing costs, improving quality, and improving the understanding of code. Each of these areas has more detailed named goals that are discussed brieﬂ y here as well. This chapter doesn’t describe how to achieve these goals; that explanation will come in subsequent chapters where these goals are used as the rationale for many of the principles and patterns.\n\nWhy Test?\n\nMuch has been written about the need for automated unit and acceptance tests as part of agile software development. Writing good test code is hard, and main- taining obtuse test code is even harder. Because test code is optional (i.e., it is not what the customer is paying for), there is a strong temptation to abandon testing when the tests become difﬁ cult or expensive to maintain. Once we have given up on the principle of “keep the bar green to keep the code clean,” much of the value of the automated tests is lost.\n\nOver a series of projects, the teams I have worked with have faced a number of challenges to automated testing. The cost of writing and maintaining test suites has been a particular challenge, especially on projects with thousands of tests. Fortunately, as the cliché says, “Necessity is the mother of invention.” My teams, and others, have developed a number of solutions to address these challenges. I have since spent a lot of time reﬂ ecting on these solutions to ask why they are good\n\n19\n\nwww.it-ebooks.info\n\n20\n\nChapter 3 Goals of Test Automation\n\nsolutions. Along the way, I have divided the components of successful solutions into goals (things to achieve) and principles (ways to achieve them). Adherence to these goals and principles will result in automated tests that are easier to write, read, and maintain.\n\nEconomics of Test Automation\n\nOf course, there is always a cost incurred in building and maintaining an auto- mated test suite. Ardent test automation advocates will argue that it is worth spending more to have the ability to change the software later. Unfortunately, this “pay me now so you don’t have to pay me later” argument doesn’t go very far in a tough economic climate.1\n\nOur goal should be to make the decision to do test automation a “no-brainer” by ensuring that it does not increase the cost of software development. Thus the additional cost of building and maintaining automated tests must be offset by savings through reduced manual unit testing and debugging/troubleshooting as well as the remediation cost of the defects that would have gone undetected until the formal test phase of the project or early production usage of the application. Figure 3.1 shows how the cost of automation is offset by the savings received from automation.\n\nEffort Effort Spent on Spent on Automating Automating Tests Tests Development Development Effort on Effort on Production Production Code Code\n\nInitial Initial Effort Effort\n\nIncreased Increased Effort Effort (Hump) (Hump)\n\nReduced Reduced Effort Effort\n\nSaved Effort Saved Effort\n\nTime Time\n\nFigure 3.1 An automated unit test project with a good return on investment. The cost-beneﬁ t trade-off when the total cost is reduced by good test practices.\n\nInitially, the cost of learning the new technology and practices takes additional effort. Once we get over this “hump,” however, we should settle down to a steady state where the added cost (the part above the line) is fully offset by the\n\n1 The argument that the quality improvement is worth the extra cost also doesn’t go very far in these days of “just good enough” software quality.\n\nwww.it-ebooks.info\n\nGoals of Test Automation\n\nsavings (the part below the line). If tests are difﬁ cult to write, are difﬁ cult to understand, and require frequent, expensive maintenance, the total cost of soft- ware development (the heights of the vertical arrows) goes up as illustrated in Figure 3.2.\n\nEffort Effort Spent on Spent on Automating Automating Tests Tests\n\nDevelopment Development Effort on Effort on Production Production Code Code\n\nInitial Initial Effort Effort\n\nIncreased Increased Effort Effort (Hump) (Hump)\n\nOngoing Ongoing Effort Effort\n\nSaved Effort Saved Effort\n\nTime Time\n\nFigure 3.2 An automated unit test project with a poor return on investment. The cost-beneﬁ t trade-off when the total cost is increased by poor test practices.\n\nNote how the added work above the line in Figure 3.2 is more than that seen in Figure 3.1 and continues to increase over time. Also, the saved effort below the line is reduced. This reﬂ ects the increase in overall effort, which exceeds the original effort without test automation.\n\nGoals of Test Automation\n\nWe all come to test automation with some notion of why having automated tests would be a “good thing.” Here are some high-level objectives that might apply:\n\nTests should help us improve quality.\n\nTests should help us understand the SUT.\n\nTests should reduce (and not introduce) risk.\n\nTests should be easy to run.\n\nTests should be easy to write and maintain.\n\nTests should require minimal maintenance as the system evolves\n\naround them.\n\nwww.it-ebooks.info\n\n21\n\n22\n\nAlso known as: Executable Speciﬁ cation\n\nChapter 3 Goals of Test Automation\n\nThe ﬁ rst three objectives demonstrate the value provided by the tests, whereas the last three objectives focus on the characteristics of the tests themselves. Most of these objectives can be decomposed into more concrete (and measurable) goals. I have given these short catchy names so that I can refer to them as moti- vators of speciﬁ c principles or patterns.\n\nTests Should Help Us Improve Quality\n\nThe traditional reason given for doing testing is for quality assurance (QA). What, precisely, do we mean by this? What is quality? Traditional deﬁ nitions distinguish two main categories of quality based on the following questions: (1) Is the software built correctly? and (2) Have we built the right software?\n\nGoal: Tests as Speciﬁ cation\n\nIf we are doing test-driven development or test-ﬁ rst development, the tests give us a way to capture what the SUT should be doing before we start building it. They enable us to specify the behavior in various scenarios captured in a form that we can then execute (essentially an “executable speciﬁ cation”). To ensure that we are “building the right software,” we must ensure that our tests reﬂ ect how the SUT will actually be used. This effort can be facilitated by developing user interface mockups that capture just enough detail about how the applica- tion appears and behaves so that we can write our tests.\n\nThe very act of thinking through various scenarios in enough detail to turn them into tests helps us identify those areas where the requirements are ambigu- ous or self-contradictory. Such analysis improves the quality of the speciﬁ ca- tion, which improves the quality of the software so speciﬁ ed.\n\nGoal: Bug Repellent\n\nYes, tests ﬁ nd bugs—but that really isn’t what automated testing is about. Auto- mated testing tries to prevent bugs from being introduced. Think of automated tests as “bug repellent” that keeps nasty little bugs from crawling back into our software after we have made sure it doesn’t contain any bugs. Wherever we have regression tests, we won’t have bugs because the tests will point the bugs out before we even check in our code. (We are running all the tests before every check-in, aren’t we?)\n\nGoal: Defect Localization\n\nMistakes happen! Of course, some mistakes are much more expensive to pre- vent than to ﬁ x. Suppose a bug does slip through somehow and shows up in\n\nwww.it-ebooks.info\n\nGoals of Test Automation\n\nthe Integration Build [SCM]. If our unit tests are fairly small (i.e., we test only a single behavior in each one), we should be able to pinpoint the bug quickly based on which test fails. This speciﬁ city is one of the major advantages that unit tests enjoy over customer tests. The customer tests tell us that some behavior expected by the customer isn’t working; the unit tests tell us why. We call this phenomenon Defect Localization. If a customer test fails but no unit tests fail, it indicates a Missing Unit Test (see Production Bugs on page 268).\n\nAll of these beneﬁ ts are wonderful—but we cannot achieve them if we don’t write tests for all possible scenarios that each unit of software needs to cover. Nor will we realize these beneﬁ ts if the tests themselves contain bugs. Clearly, it is crucial that we keep the tests as simple as possible so that they can be easily seen to be correct. While writing unit tests for our unit tests is not a practical solution, we can—and should—write unit tests for any Test Utility Method (page 599) to which we delegate complex algorithms needed by the test methods.\n\nTests Should Help Us Understand the SUT\n\nRepelling bugs isn’t the only thing the tests can do for us. They can also show the test reader how the code is supposed to work. Black box component tests are—in effect—describing the requirements of that of software component.\n\nGoal: Tests as Documentation\n\nWithout automated tests, we would need to pore over the SUT code trying to answer the question, “What should be the result if . . . ?” With automated tests, we simply use the corresponding Tests as Documentation; they tell us what the result should be (recall that a Self-Checking Test states the expected outcome in one or more assertions). If we want to know how the system does something, we can turn on the debugger, run the test, and single-step through the code to see how it works. In this sense, the automated tests act as a form of documentation for the SUT.\n\nTests Should Reduce (and Not Introduce) Risk\n\nAs mentioned earlier, tests should improve the quality of our software by help- ing us better document the requirements and prevent bugs from creeping in dur- ing incremental development. This is certainly one form of risk reduction. Other forms of risk reduction involve verifying the software’s behavior in the “impos- sible” circumstances that cannot be induced when doing traditional customer testing of the entire application as a black box. It is a very useful exercise to\n\nwww.it-ebooks.info\n\n23\n\n24\n\nAlso known as: Safety Net\n\nAlso known as: No Test Risk\n\nChapter 3 Goals of Test Automation\n\nreview all of the project’s risks and brainstorm about which kinds of risks could be at least partially mitigated through the use of Fully Automated Tests.\n\nGoal: Tests as Safety Net\n\nWhen working on legacy code, I always feel nervous. By deﬁ nition, legacy code doesn’t have a suite of automated regression tests. Changing this kind of code is risky because we never know what we might break, and we have no way of know- ing whether we have broken something! As a consequence, we must work very slowly and carefully, doing a lot of manual analysis before making any changes.\n\nWhen working with code that has a regression test suite, by contrast, we can work much more quickly. We can adopt a more experimental style of changing the software: “I wonder what would happen if I changed this? Which tests fail? Interesting! So that’s what this parameter is for.” In this way, the automated tests act as a safety net that allows us to take chances.2\n\nThe effectiveness of the safety net is determined by how completely our tests verify the behavior of the system. Missing tests are like holes in the safety net. Incomplete assertions are like broken strands. Each gap in the safety net can let bugs of various sizes through.\n\nThe effectiveness of the safety net is ampliﬁ ed by the version-control capabil- ities of modern software development environments. A source code repository [SCM] such as CVS, Subversion, or SourceSafe lets us roll back our changes to a known point if our tests suggest that the current set of changes is affecting the code too extensively. The built-in “undo” or “local history” features of the IDE let us turn the clock back 5 seconds, 5 minutes, or even 5 hours.\n\nGoal: Do No Harm\n\nNaturally, there is a ﬂ ip side to this discussion: How might automated tests in- troduce risk? We must be careful not to introduce new kinds of problems into the SUT as a result of doing automated testing. The Keep Test Logic Out of Production Code principle directs us to avoid putting test-speciﬁ c hooks into the SUT. It is certainly desirable to design the system for testability, but any test- speciﬁ c code should be plugged in by the test and only in the test environment; it should not exist in the SUT when it is in production.\n\nAnother form of risk is believing that some code is reliable because it has been thoroughly tested when, in fact, it has not. A common mistake made by developers new to the use of Test Doubles (page 522) is replacing too much of\n\n2 Imagine trying to learn to be a trapeze artist in the circus without having that big net that allows you to make mistakes. You would never progress beyond swinging back and forth!\n\nwww.it-ebooks.info\n\nGoals of Test Automation\n\nthe SUT with a Test Double. This leads to another important principle: Don’t Modify the SUT. That is, we must be clear about which SUT we are testing and avoid replacing the parts we are testing with test-speciﬁ c logic (Figure 3.3).\n\nUnit1 Unit1 Test Test\n\nExercise Exercise\n\nUnit1 Unit1 SUT SUT\n\nUnit2 Unit2 Test Test\n\nExercise Exercise\n\nUnit2 Unit2 SUT SUT\n\nuses uses\n\nComp1 Comp1 Test Test\n\nExercise Exercise\n\nComp1 Comp1 SUT SUT\n\nComp2 Comp2 Test Test\n\nExercise Exercise\n\nComp2 Comp2 SUT SUT\n\nuses uses\n\nApp1 App1 Test Test\n\nExercise Exercise\n\nApp1 App1 SUT SUT\n\nFigure 3.3 A range of tests, each with its own SUT. An application, component, or unit is only the SUT with respect to a speciﬁ c set of tests. The “Unit1 SUT” plays the role of DOC (part of the ﬁ xture) to the “Unit2 Test” and is part of the “Comp1 SUT.”\n\nTests Should Be Easy to Run\n\nMost software developers just want to write code; testing is simply a necessary evil in our line of work. Automated tests provide a nice safety net so that we can write code more quickly,3 but we will run the automated tests frequently only if they are really easy to run.\n\nWhat makes tests easy to run? Four speciﬁ c goals answer this question:\n\nThey must be Fully Automated Tests so they can be run without any\n\neffort.\n\n3 “With less paranoia” is probably more accurate!\n\nwww.it-ebooks.info\n\n25\n\n26\n\nChapter 3 Goals of Test Automation\n\nThey must be Self-Checking Tests so they can detect and report any\n\nerrors without manual inspection.\n\nThey must be Repeatable Tests so they can be run multiple times with\n\nthe same result.\n\nIdeally, each test should be an Independent Test that can be run by itself.\n\nWith these four goals satisﬁ ed, one click of a button (or keyboard shortcut) is all it should take to get the valuable feedback the tests provide. Let’s look at these goals in a bit more detail.\n\nGoal: Fully Automated Test\n\nA test that can be run without any Manual Intervention (page 250) is a Fully Automated Test. Satisfying this criterion is a prerequisite to meeting many of the other goals. Yes, it is possible to write Fully Automated Tests that don’t check the results and that can be run only once. The main() program that runs the code and directs print statements to the console is a good example of such a test. I consider these two aspects of test automation to be so important in making tests easy to run that I have made them separate goals: Self-Checking Test and Repeatable Test.\n\nGoal: Self-Checking Test\n\nA Self-Checking Test has encoded within it everything that the test needs to verify that the expected outcome is correct. Self-Checking Tests apply the Holly- wood principle (“Don’t call us; we’ll call you”) to running tests. That is, the Test Runner (page 377) “calls us” only when a test did not pass; as a consequence, a clean test run requires zero manual effort. Many members of the xUnit fam- ily provide a Graphical Test Runner (see Test Runner) that uses a green bar to signal that everything is “A-okay”; a red bar indicates that a test has failed and warrants further investigation.\n\nGoal: Repeatable Test\n\nA Repeatable Test can be run many times in a row and will produce exactly the same results without any human intervention between runs. Unrepeatable Tests (see Erratic Test on page 228) increase the overhead of running tests signiﬁ cantly. This outcome is very undesirable because we want all developers to be able to run the tests very frequently—as often as after every “save.” Unrepeatable Tests can be run only once before whoever is running the tests must perform a Manual Interven- tion. Just as bad are Nondeterministic Tests (see Erratic Test) that produce different results at different times; they force us to spend lots of time chasing down failing tests. The power of the red bar diminishes signiﬁ cantly when we see it regularly\n\nwww.it-ebooks.info\n\nGoals of Test Automation\n\nwithout good reason. All too soon, we begin ignoring the red bar, assuming that it will go away if we wait long enough. Once this happens, we have lost a lot of the value of our automated tests, because the feedback indicating that we have intro- duced a bug and should ﬁ x it right away disappears. The longer we wait, the more effort it takes to ﬁ nd the source of the failing test.\n\nTests that run only in memory and that use only local variables or ﬁ elds are usually repeatable without us expending any additional effort. Unrepeatable Tests usually come about because we are using a Shared Fixture (page 317) of some sort (this deﬁ nition includes any persistence of data implemented within the SUT). In such a case, we must ensure that our tests are “self-cleaning” as well. When cleaning is necessary, the most consistent and foolproof strategy is to use a generic Automated Teardown (page 503) mechanism. Although it is possible to write teardown code for each test, this approach can result in Erratic Tests when it is not implemented correctly in every test.\n\nTests Should Be Easy to Write and Maintain\n\nCoding is a fundamentally difﬁ cult activity because we must keep a lot of in- formation in our heads as we work. When we are writing tests, we should stay focused on testing rather than coding of the tests. This means that tests must be simple—simple to read and simple to write. They need to be simple to read and understand because testing the automated tests themselves is a complicated endeavor. They can be tested properly only by introducing the very bugs that they are intended to detect into the SUT; this is hard to do in an automated way so it is usually done only once (if at all), when the test is ﬁ rst written. For these reasons, we need to rely on our eyes to catch any problems that creep into the tests, and that means we must keep the tests simple enough to read quickly.\n\nOf course, if we are changing the behavior of part of the system, we should expect a small number of tests to be affected by our modiﬁ cations. We want to Minimize Test Overlap so that only a few tests are affected by any one change. Contrary to popular opinion, having more tests pass through the same code doesn’t improve the quality of the code if most of the tests do exactly the same thing.\n\nTests become complicated for two reasons:\n\nWe try to verify too much functionality in a single test.\n\nToo large an “expressiveness gap” separates the test scripting language (e.g., Java) and the before/after relationships between domain concepts that we are trying to express in the test.\n\nwww.it-ebooks.info\n\n27\n\n28\n\nChapter 3 Goals of Test Automation\n\nGoal: Simple Tests\n\nTo avoid “biting off more than they can chew,” our tests should be small and test one thing at a time. Keeping tests simple is particularly important during test- driven development because code is written to pass one test at a time and we want each test to introduce only one new bit of behavior into the SUT. We should strive to Verify One Condition per Test by creating a separate Test Method (page 348) for each unique combination of pre-test state and input. Each Test Method should drive the SUT through a single code path.4\n\nThe major exception to the mandate to keep Test Methods short occurs with customer tests that express real usage scenarios of the application. Such extend- ed tests offer a useful way to document how a potential user of the software would go about using it; if these interactions involve long sequences of steps, the Test Methods should reﬂ ect this reality.\n\nGoal: Expressive Tests\n\nThe “expressiveness gap” can be addressed by building up a library of Test Utility Methods that constitute a domain-speciﬁ c testing language. Such a col- lection of methods allows test automaters to express the concepts that they wish to test without having to translate their thoughts into much more detailed code. Creation Methods (page 415) and Custom Assertion (page 474) are good examples of the building blocks that make up such a Higher-Level Language.\n\nThe key to solving this dilemma is avoiding duplication within tests. The DRY principle—“Don’t repeat yourself”—of the Pragmatic Programmers (http://www. pragmaticprogrammer.com) should be applied to test code in the same way it is applied to production code. There is, however, a counterforce at play. Because the tests should Communicate Intent, it is best to keep the core test logic in each Test Method so it can be seen in one place. Nevertheless, this idea doesn’t pre- clude moving a lot of supporting code into Test Utility Methods, where it needs to be modiﬁ ed in only one place if it is affected by a change in the SUT.\n\nGoal: Separation of Concerns\n\nSeparation of Concerns applies in two dimensions: (1) We want to keep test code separate from our production code (Keep Test Logic Out of Production Code) and (2) we want each test to focus on a single concern (Test Concerns Separately) to avoid Obscure Tests (page 186). A good example of what not to do is testing the business logic in the same tests as the user interface, because it involves testing\n\n4 There should be at least one Test Method for each unique path through the code; often there will be several, one for each boundary value of the equivalence class.\n\nwww.it-ebooks.info\n\nWhat’s Next?\n\ntwo concerns at the same time. If either concern is modiﬁ ed (e.g., the user inter- face changes), all the tests would need to be modiﬁ ed as well. Testing one concern at a time may require separating the logic into different components. This is a key aspect of design for testability, a consideration that is explored further in Chapter 11, Using Test Doubles.\n\nTests Should Require Minimal Maintenance as the System Evolves Around Them\n\nChange is a fact of life. Indeed, we write automated tests mostly to make change easier, so we should strive to ensure that our tests don’t inadvertently make change more difﬁ cult.\n\nSuppose we want to change the signature of some method on a class. When we add a new parameter, suddenly 50 tests no longer compile. Does that result en- courage us to make the change? Probably not. To counter this problem, we intro- duce a new method with the parameter and arrange to have the old method call the new method, defaulting the missing parameter to some value. Now all of the tests compile but 30 of them still fail! Are the tests helping us make the change?\n\nGoal: Robust Test\n\nInevitably, we will want to make many kinds of changes to the code as a project unfolds and its requirements evolve. For this reason, we want to write our tests in such a way that the number of tests affected by any one change is quite small. That means we need to minimize overlap between tests. We also need to ensure that changes to the test environment don’t affect our tests; we do this by isolat- ing the SUT from the environment as much as possible. This results in much more Robust Tests.\n\nWe should strive to Verify One Condition per Test. Ideally, only one kind of change should cause a test to require maintenance. System changes that affect ﬁ xture setup or teardown code can be encapsulated behind Test Utility Methods to further reduce the number of tests directly affected by the change.\n\nWhat’s Next?\n\nThis chapter discussed why we have automated tests and speciﬁ c goals we should try to achieve when writing Fully Automated Tests. Before moving on to Chapter 5, Principles of Test Automation, we need to take a short side-trip to Chapter 4, Philosophy of Test Automation, to understand the different mindsets of various kinds of test automaters.\n\nwww.it-ebooks.info\n\n29\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nChapter 4\n\nPhilosophy of Test Automation\n\nAbout This Chapter\n\nChapter 3, Goals of Test Automation, described many of the goals and beneﬁ ts of having an effective test automation program in place. This chapter introduces some differences in the way people think about design, construction, and testing that change the way they might naturally apply these patterns. The “big picture” questions include whether we write tests ﬁ rst or last, whether we think of them as tests or examples, whether we build the software from the inside-out or from the outside-in, whether we verify state or behavior, and whether we design the ﬁ xture upfront or test by test.\n\nWhy Is Philosophy Important?\n\nWhat’s philosophy got to do with test automation? A lot! Our outlook on life (and testing) strongly affects how we go about automating tests. When I was discussing an early draft of this book with Martin Fowler (the series editor), we came to the conclusion that there were philosophical differences between how different people approached xUnit-based test automation. These differences lie at the heart of why, for example, some people use Mock Objects (page 544) sparingly and others use them everywhere.\n\nSince that eye-opening discussion, I have been on the lookout for other phil- osophical differences among test automaters. These alternative viewpoints tend to come up as a result of someone saying, “I never (ﬁ nd a need to) use that pat- tern” or “I never run into that smell.” By questioning these statements, I can learn a lot about the testing philosophy of the speaker. Out of these discussions have come the following philosophical differences:\n\n31\n\nwww.it-ebooks.info\n\n32\n\nChapter 4 Philosophy of Test Automation\n\n“Test after” versus “test ﬁ rst”\n\nTest-by-test versus test all-at-once\n\n“Outside-in” versus “inside-out” (applies independently to design and\n\ncoding)\n\nBehavior veriﬁ cation versus state veriﬁ cation\n\n“Fixture designed test-by-test” versus “big ﬁ xture design upfront”\n\nSome Philosophical Differences\n\nTest First or Last?\n\nTraditional software development prepares and executes tests after all software is designed and coded. This order of steps holds true for both customer tests and unit tests. In contrast, the agile community has made writing the tests ﬁ rst the standard way of doing things. Why is the order in which testing and develop- ment take place important? Anyone who has tried to retroﬁ t Fully Automated Tests (page 22) onto a legacy system will tell you how much more difﬁ cult it is to write automated tests after the fact. Just having the discipline to write auto- mated unit tests after the software is “already ﬁ nished” is challenging, whether or not the tests themselves are easy to construct. Even if we design for testability, the likelihood that we can write the tests easily and naturally without modifying the production code is low. When tests are written ﬁ rst, however, the design of the system is inherently testable.\n\nWriting the tests ﬁ rst has some other advantages. When tests are written ﬁ rst and we write only enough code to make the tests pass, the production code tends to be more minimalist. Functionality that is optional tends not to be written; no extra effort goes into fancy error-handling code that doesn’t work. The tests tend to be more robust because only the necessary methods are provided on each object based on the tests’ needs.\n\nAccess to the state of the object for the purposes of ﬁ xture setup and result veriﬁ cation comes much more naturally if the software is written “test ﬁ rst.” For example, we may avoid the test smell Sensitive Equality (see Fragile Test on page 239) entirely because the correct attributes of objects are used in assertions rather than comparing the string representations of those objects. We may even ﬁ nd that we don’t need to implement a String representation at all because we\n\nwww.it-ebooks.info\n\nSome Philosophical Differences\n\nhave no real need for it. The ability to substitute dependencies with Test Doubles (page 522) for the purpose of verifying the outcome is also greatly enhanced be- cause substitutable dependency is designed into the software from the start.\n\nTests or Examples?\n\nWhenever I mention the concept of writing automated tests for software before the software has been written, some listeners get strange looks on their faces. They ask, “How can you possibly write tests for software that doesn’t exist?” In these cases, I follow Brian Marrick’s lead by reframing the discussion to talk about “examples” and example-driven development (EDD). It seems that examples are much easier for some people to envision writing before code than are “tests.” The fact that the examples are executable and reveal whether the requirements have been satisﬁ ed can be left for a later discussion or a discussion with people who have a bit more imagination.\n\nBy the time this book is in your hands, a family of EDD frameworks is likely to have emerged. The Ruby-based RSpec kicked off the reframing of TDD to EDD, and the Java-based JBehave followed shortly thereafter. The basic design of these “unit test frameworks” is the same as xUnit but the terminology has changed to reﬂ ect the Executable Speciﬁ cation (see Goals of Test Automation on page 21) mindset.\n\nAnother popular alternative for specifying components that contain business logic is to use Fit tests. These will invariably be more readable by nontechnical people than something written in a programming language regardless of how “business friendly” we make the programming language syntax!\n\nTest-by-Test or Test All-at-Once?\n\nThe test-driven development process encourages us to “write a test” and then “write some code” to pass that test. This process isn’t a case of all tests being written before any code, but rather the writing of tests and code being inter- leaved in a very ﬁ ne-grained way. “Test a bit, code a bit, test a bit more”—this is incremental development at its ﬁ nest. Is this approach the only way to do things? Not at all! Some developers prefer to identify all tests needed by the current feature before starting any coding. This strategy enables them to “think like a client” or “think like a tester” and lets developers avoid being sucked into “solution mode” too early.\n\nTest-driven purists argue that we can design more incrementally if we build the software one test at a time. “It’s easier to stay focused if only a single test is failing,” they say. Many test drivers report not using the debugger very much\n\nwww.it-ebooks.info\n\n33\n\n34\n\nChapter 4 Philosophy of Test Automation\n\nbecause the ﬁ ne-grained testing and incremental development leave little doubt about why tests are failing; the tests provide Defect Localization (see Goals of Test Automation on page 22) while the last change we made (which caused the problem) is still fresh in our minds.\n\nThis consideration is especially relevant when we are talking about unit tests because we can choose when to enumerate the detailed requirements (tests) of each object or method. A reasonable compromise is to identify all unit tests at the beginning of a task—possibly roughing in empty Test Method (page 348) skeletons, but coding only a single Test Method body at a time. We could also code all Test Method bodies and then disable all but one of the tests so that we can focus on building the production code one test at a time.\n\nWith customer tests, we probably don’t want to feed the tests to the devel- oper one by one within a user story. Therefore, it makes sense to prepare all the tests for a single story before we begin development of that story. Some teams prefer to have the customer tests for the story identiﬁ ed—although not neces- sarily ﬂ eshed out—before they are asked to estimate the effort needed to build the story, because the tests help frame the story.\n\nOutside-In or Inside-Out?\n\nDesigning the software from the outside inward implies that we think ﬁ rst about black-box customer tests (also known as storytests) for the entire system and then think about unit tests for each piece of software we design. Along the way, we may also implement component tests for the large-grained components we decide to build.\n\nEach of these sets of tests inspires us to “think like the client” well before we start thinking like a software developer. We focus ﬁ rst on the interface provided to the user of the software, whether that user is a person or another piece of software. The tests capture these usage patterns and help us enumerate the vari- ous scenarios we need to support. Only when we have identiﬁ ed all the tests are we “ﬁ nished” with the speciﬁ cation. Some people prefer to design outside-in but then code inside-out to avoid dealing with the “dependency problem.” This tactic requires anticipating the needs of the outer software when writing the tests for the inner software. It also means that we don’t actually test the outer software in isolation from the inner software. Figure 4.1 illustrates this concept. The top-to-bottom progression in the diagram implies the order in which we write the software. Tests for the middle and lower classes can take advantage of the already-built classes above them—a strategy that avoids the need for Test Stubs (page 529) or Mock Objects in many of the tests. We may still need to use Test Stubs in those tests where the inner components could potentially return\n\nwww.it-ebooks.info\n\nSome Philosophical Differences\n\nspeciﬁ c values or throw exceptions, but cannot be made to do so on cue. In such a case, a Saboteur (see Test Stub) comes in very handy.\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nUses Uses\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nUses Uses\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nFigure 4.1 “Inside-out” development of functionality. Development starts with the innermost components and proceeds toward the user interface, building on the previously constructed components.\n\nOther test drivers prefer to design and code from the outside-in. Writing the code outside-in forces us to deal with the “dependency problem.” We can use Test Stubs to stand in for the software we haven’t yet written, so that the outer layer of software can be executed and tested. We can also use Test Stubs to inject “impossible” indirect inputs (return values, out parameters, or exceptions) into the SUT to verify that it handles these cases correctly.\n\nIn Figure 4.2, we have reversed the order in which we build our classes. Be- cause the subordinate classes don’t exist yet, we used Test Doubles to stand in for them.\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nTest Test Double Double\n\nUses Uses\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nTest Test Double Double\n\nUses Uses\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nFigure 4.2 “Outside-in” development of functionality supported by Test Doubles. Development starts at the outside using Test Doubles in place of the depended-on components (DOCs) and proceeds inward as requirements for each DOC are identiﬁ ed.\n\nwww.it-ebooks.info\n\n35\n\n36\n\nChapter 4 Philosophy of Test Automation\n\nOnce the subordinate classes have been built, we could remove the Test Doubles from many of the tests. Keeping them provides better Defect Localization at the cost of potentially higher test maintenance cost.\n\nState or Behavior Veriﬁ cation?\n\nFrom writing code outside-in, it is but a small step to verifying behavior rather than just state. The “statist” view suggests that it is sufﬁ cient to put the SUT into a speciﬁ c state, exercise it, and verify that the SUT is in the expected state at the end of the test. The “behaviorist” view says that we should specify not only the start and end states of the SUT, but also the calls the SUT makes to its dependencies. That is, we should specify the details of the calls to the “outgoing interfaces” of the SUT. These indirect outputs of the SUT are outputs just like the values returned by functions, except that we must use special measures to trap them because they do not come directly back to the client or test.\n\nThe behaviorist school of thought is sometimes called behavior-driven development. It is evidenced by the copious use of Mock Objects or Test Spies (page 538) throughout the tests. Behavior verification does a better job of testing each unit of software in isolation, albeit at a possible cost of more difficult refactoring. Martin Fowler provides a detailed discussion of the statist and behaviorist approaches in [MAS].\n\nFixture Design Upfront or Test-by-Test?\n\nIn the traditional test community, a popular approach is to deﬁ ne a “test bed” consisting of the application and a database already populated with a variety of test data. The content of the database is carefully designed to allow many differ- ent test scenarios to be exercised.\n\nWhen the ﬁ xture for xUnit tests is approached in a similar manner, the test automater may deﬁ ne a Standard Fixture (page 305) that is then used for all the Test Methods of one or more Testcase Classes (page 373). This ﬁ xture may be set up as a Fresh Fixture (page 311) in each Test Method using Delegated Setup (page 411) or in the setUp method using Implicit Setup (page 424). Alter- natively, it can be set up as a Shared Fixture (page 317) that is reused by many tests. Either way, the test reader may ﬁ nd it difﬁ cult to determine which parts of the ﬁ xture are truly pre-conditions for a particular Test Method.\n\nThe more agile approach is to custom design a Minimal Fixture (page 302) for each Test Method. With this perspective, there is no “big ﬁ xture design up- front” activity. This approach is most consistent with using a Fresh Fixture.\n\nwww.it-ebooks.info\n\nWhat’s Next?\n\nWhen Philosophies Differ\n\nWe cannot always persuade the people we work with to adopt our philosophy, of course. Even so, understanding that others subscribe to a different philosophy helps us appreciate why they do things differently. It’s not that these individuals don’t share the same goals as ours;1 it’s just that they make the decisions about how to achieve those goals using a different philosophy. Understanding that dif- ferent philosophies exist and recognizing which ones we subscribe to are good ﬁ rst steps toward ﬁ nding some common ground between us.\n\nMy Philosophy\n\nIn case you were wondering what my personal philosophy is, here it is:\n\nWrite the tests ﬁ rst!\n\nTests are examples!\n\nI usually write tests one at a time, but sometimes I list all the tests I can\n\nthink of as skeletons upfront.\n\nOutside-in development helps clarify which tests are needed for the\n\nnext layer inward.\n\nI use primarily State Veriﬁ cation (page 462) but will resort to Behavior\n\nVeriﬁ cation (page 468) when needed to get good code coverage.\n\nI perform ﬁ xture design on a test-by-test basis.\n\nThere! Now you know where I’m coming from.\n\nWhat’s Next?\n\nThis chapter introduced the philosophies that anchor software design, construc- tion, testing, and test automation. Chapter 5, Principles of Test Automation, describes key principles that will help us achieve the goals described in Chapter 3, Goals of Test Automation. We will then be ready to start looking at the over- all test automation strategy and the individual patterns.\n\n1 For example, high-quality software, ﬁ t for purpose, on time, under budget.\n\nwww.it-ebooks.info\n\n37\n\nThis page intentionally left blank\n\nwww.it-ebooks.info",
      "page_number": 82
    },
    {
      "number": 5,
      "title": "Principles of Test Automation",
      "start_page": 102,
      "end_page": 111,
      "detection_method": "regex_chapter",
      "content": "Chapter 5\n\nPrinciples of Test Automation\n\nAbout This Chapter\n\nChapter 3, Goals of Test Automation, described the goals we should strive to achieve to help us be successful at automating our unit tests and customer tests. Chapter 4, Philosophy of Test Automation, discussed some of the differences in the way people approach software design, construction, and testing. This pro- vides the background for the principles that experienced test automaters follow while automating their tests. I call them “principles” for two reasons: They are too high level to be patterns and they represent a value system that not everyone will share. A different value system may cause you to choose different patterns than the ones presented in this book. Making this value system explicit will, I hope, accelerate the process of understanding where we disagree and why.\n\nThe Principles\n\nWhen Shaun Smith and I came up with the list in the original Test Automation Manifesto [TAM], we considered what was driving us to write tests the way we did. The Manifesto is a list of the qualities we would like to see in a test—not a set of patterns that can be directly applied. However, those principles have led us to identify a number of somewhat more concrete principles, some of which are described in this chapter. What makes these principles different from the goals is that there is more debate about them.\n\nPrinciples are more “prescriptive” than patterns and higher level in nature. Un- like patterns, they don’t have alternatives, but rather are presented in a “do this because” fashion. To distinguish them from patterns, I have given them imperative names rather than the noun-phrase names I use for goals, patterns, and smells.\n\n39\n\nwww.it-ebooks.info\n\n40\n\nAlso known as: Test-Driven Development, Test-First Development\n\nAlso known as: Front Door First\n\nChapter 5 Principles of Test Automation\n\nFor the most part, these principles apply equally well to unit tests and story- tests. A possible exception is the principle Verify One Condition per Test, which may not be practical for customer tests that exercise more involved chunks of functionality. It is, however, still worth striving to follow these principles and to deviate from them only when you are fully cognizant of the consequences.\n\nPrinciple: Write the Tests First\n\nTest-driven development is very much an acquired habit. Once one has “gotten the hang of it,” writing code in any other way can seem just as strange as TDD seems to those who have never done it. There are two major arguments in favor of doing TDD:\n\n1. The unit tests save us a lot of debugging effort—effort that often fully\n\noffsets the cost of automating the tests.\n\n2. Writing the tests before we write the code forces the code to be designed for testability. We don’t need to think about testability as a separate design condition; it just happens because we have written tests.\n\nPrinciple: Design for Testability\n\nGiven the last principle, this principle may seem redundant. For developers who choose to ignore Write the Tests First, Design for Testability becomes an even more important principle because they won’t be able to write automated tests after the fact if the testability wasn’t designed in. Anyone who has tried to retroﬁ t automated unit tests onto legacy software can testify to the difﬁ culty this raises. Mike Feathers talks about special techniques for introducing tests in this case in [WEwLC].\n\nPrinciple: Use the Front Door First\n\nObjects have several kinds of interfaces. There is the “public” interface that clients are expected to use. There may also be a “private” interface that only close friends should use. Many objects also have an “outgoing interface” consisting of the used part of the interfaces of any objects on which they depend.\n\nThe types of interfaces we use inﬂ uence the robustness of our tests. The use of Back Door Manipulation (page 327) to set up the ﬁ xture or verify the expected outcome or a test can result in Overcoupled Software (see Fragile Test on page 239) that needs more frequent test maintenance. Overuse of Behavior Veriﬁ ca- tion (page 468) and Mock Objects (page 544) can result in Overspeciﬁ ed Software (see Fragile Test) and tests that are more brittle and may discourage developers from doing desirable refactorings.\n\nwww.it-ebooks.info\n\nThe Principles\n\nWhen all choices are equally effective, we should use round-trip tests to test our SUT. To do so, we test an object through its public interface and use State Veriﬁ cation (page 462) to determine whether it behaved correctly. If this is not suf- ﬁ cient to accurately describe the expected behavior, we can make our tests layer- crossing tests and use Behavior Veriﬁ cation to verify the calls the SUT makes to depended-on components (DOCs). If we must replace a slow or unavailable DOC with a faster Test Double (page 522), using a Fake Object (page 551) is preferable because it encodes fewer assumptions into the test (the only assumption is that the component that the Fake Object replaces is actually needed).\n\nPrinciple: Communicate Intent\n\nFully Automated Tests, especially Scripted Tests (page 285), are programs. They need to be syntactically correct to compile and semantically correct to run success- fully. They need to implement whatever detailed logic is required to put the SUT into the appropriate starting state and to verify that the expected outcome has occurred. While these characteristics are necessary, they are not sufﬁ cient because they neglect the single most important interpreter of the tests: the test maintainer.\n\nTests that contain a lot of code1 or Conditional Test Logic (page 200) are usually Obscure Tests (page 186). They are much harder to understand because we need to infer the “big picture” from all the details. This reverse engineering of meaning takes extra time whenever we need to revisit the test either to main- tain it or to use the Tests as Documentation. It also increases the cost of owner- ship of the tests and reduces their return on investment.\n\nTests can be made easier to understand and maintain if we Communi- cate Intent. We can do so by calling Test Utility Methods (page 599) with Intent-Revealing Names [SBPP] to set up our test ﬁ xture and to verify that the expected outcome has been realized. It should be readily apparent within the Test Method (page 348) how the test ﬁ xture inﬂ uences the expected outcome of each test—that is, which inputs result in which outputs. A rich library of Test Utility Methods also makes tests easier to write because we don’t have to code the details into every test.\n\nPrinciple: Don’t Modify the SUT\n\nEffective testing often requires us to replace a part of the application with a Test Double or override part of its behavior using a Test-Speciﬁ c Subclass (page 579). This may be because we need to gain control over its indirect inputs or because we need to perform Behavior Veriﬁ cation by intercepting its indirect outputs. It may\n\n1 Anything more than about ten lines is getting to be too much.\n\nwww.it-ebooks.info\n\n41\n\nAlso known as: Higher-Level Language, Single-Glance Readable\n\n42\n\nAlso known as: Independent Test\n\nChapter 5 Principles of Test Automation\n\nalso be because parts of the application’s behavior have unacceptable side effects or dependencies that are impossible to satisfy in our development or test environment. Modifying the SUT is a dangerous thing whether we are putting in Test Hooks (page 709), overriding behavior in a Test-Speciﬁ c Subclass, or replacing a DOC with a Test Double. In any of these circumstances, we may no longer actually be testing the code we plan to put into production.\n\nWe need to ensure that we are testing the software in a conﬁ guration that is truly representative of how it will be used in production. If we do need to replace something the SUT depends on to get better control of the context surrounding the SUT, we must make sure that we are doing so in a representative way. Otherwise, we may end up replacing part of the SUT that we think we are testing. Suppose, for example, that we are writing tests for objects X, Y, and Z, where object X depends on object Y, which in turn depends on object Z. When writing tests for X, it is reasonable to replace Y and Z with a Test Double. When testing Y, we can replace Z with a Test Double. When testing Z, however, we cannot replace it with a Test Double because Z is what we are testing! This consideration is particularly salient when we have to refactor the code to improve its testability.\n\nWhen we use a Test-Speciﬁ c Subclass to override part of the behavior of an object to allow testing, we have to be careful that we override only those meth- ods that the test speciﬁ cally needs to null out or use to inject indirect inputs. If we choose to reuse a Test-Speciﬁ c Subclass created for another test, we must ensure that it does not override any of the behavior that this test is verifying.\n\nAnother way of looking at this principle is as follows: The term SUT is rela- tive to the tests we are writing. In our “X uses Y uses Z” example, the SUT for some component tests might be the aggregate of X, Y, and Z; for unit testing purposes, it might be just X for some tests, just Y for other tests, and just Z for yet other tests. Just about the only time we consider the entire application to be the SUT is when we are doing user acceptance testing using the user interface and going all the way back to the database. Even here, we might be testing only one module of the entire application (e.g., the “Customer Management Mod- ule”). Thus “SUT” rarely equals “application.”\n\nPrinciple: Keep Tests Independent\n\nWhen doing manual testing, it is common practice to have long test procedures that verify many aspects of the SUT’s behavior in a single test. This aggregation of tasks is necessary because the steps involved in setting up the starting state of the system for one test may simply repeat the steps used to verify other parts of its behavior. When tests are executed manually, this repetition is not cost-effective. In addition, human testers have the ability to recognize when a test failure should preclude continuing\n\nwww.it-ebooks.info\n\nThe Principles\n\nexecution of the test, when it should cause certain tests to be skipped, or when the failure is immaterial to subsequent tests (though it may still count as a failed test.)\n\nIf tests are interdependent and (even worse) order dependent, we will deprive ourselves of the useful feedback that individual test failures provide. Interacting Tests (see Erratic Test on page 228) tend to fail in a group. The failure of a test that moved the SUT into the state required by the dependent test will lead to the failure of the dependent test, too. With both tests failing, how can we tell whether the failure reﬂ ects a problem in code that both tests rely on in some way or whether it signals a problem in code that only the ﬁ rst test relies on? When both tests fail, we can’t tell. And we are talking about only two tests in this case—imagine how much worse matters would be with tens or even hun- dreds of Interacting Tests.\n\nAn Independent Test can be run by itself. It sets up its own Fresh Fix- ture (page 311) to put the SUT into a state that lets it verify the behavior it is testing. Tests that build a Fresh Fixture are much more likely to be independent than tests that use a Shared Fixture (page 317). The latter can lead to various kinds of Erratic Tests, including Lonely Tests, Interacting Tests, and Test Run Wars. With independent tests, unit test failures give us Defect Localization to help us pinpoint the source of the failure.\n\nPrinciple: Isolate the SUT\n\nSome pieces of software depend on nothing but the (presumably correct) run- time system or operating system. Most pieces of software build on other pieces of software developed by us or by others. When our software depends on other software that may change over time, our tests may suddenly start failing because the behavior of the other software has changed. This problem, which is called Context Sensitivity (see Fragile Test), is a form of Fragile Test.\n\nWhen our software depends on other software whose behavior we cannot control, we may ﬁ nd it difﬁ cult to verify that our software behaves properly with all possible return values. This is likely to lead to Untested Code (see Pro- duction Bugs on page 268) or Untested Requirements (see Production Bugs). To avoid this problem, we need to be able to inject all possible reactions of the DOC into our software under the complete control of our tests.\n\nWhatever application, component, class, or method we are testing, we should strive to isolate it as much as possible from all other parts of the software that we choose not to test. This isolation of elements allows us to Test Concerns Separately and allows us to Keep Tests Independent of one another. It also helps us create a Robust Test by reducing the likelihood of Context Sensitivity caused by too much coupling between our SUT and the software that surrounds it.\n\nwww.it-ebooks.info\n\n43\n\n44\n\nChapter 5 Principles of Test Automation\n\nWe can satisfy this principle by designing our software such that each piece of depended-on software can be replaced with a Test Double using Dependency Injection (page 678) or Dependency Lookup (page 686) or overridden with a Test-Speciﬁ c Subclass that gives us control of the indirect inputs of the SUT. This design for testability makes our tests more repeatable and robust.\n\nPrinciple: Minimize Test Overlap\n\nMost applications have lots of functionality to verify. Proving that all of the functionality works correctly in all possible combinations and interaction sce- narios is nearly impossible. Therefore, picking the tests to write is an exercise in risk management.\n\nWe should structure our tests so that as few tests as possible depend on a particular piece of functionality. This may seem counter-intuitive at ﬁ rst be- cause one would think that we would want to improve test coverage by testing the software as often as possible. Unfortunately, tests that verify the same func- tionality typically fail at the same time. They also tend to need the same mainte- nance when the functionality of the SUT is modiﬁ ed. Having several tests verify the same functionality is likely to increase test maintenance costs and probably won’t improve quality very much.\n\nWe do want to ensure that all test conditions are covered by the tests that we do use. Each test condition should be covered by exactly one test—no more, no less. If it seems to provide value to test the code in several different ways, we may have identiﬁ ed several different test conditions.\n\nPrinciple: Minimize Untestable Code\n\nSome kinds of code are difﬁ cult to test using Fully Automated Tests. GUI com- ponents, multithreaded code, and Test Methods immediately spring to mind as “untestable” code. All of these kinds of code share the same problem: They are embedded in a context that makes it hard to instantiate or interact with them from automated tests.\n\nUntestable code simply won’t have any Fully Automated Tests to protect it from those nefarious little bugs that can creep into code when we aren’t look- ing. That makes it more difﬁ cult to refactor this code safely and more danger- ous to modify existing functionality or introduce new functionality.\n\nIt is highly desirable to minimize the amount of untestable code that we have to maintain. We can refactor the untestable code to improve its testability by moving the logic we want to test out of the class that is causing the lack of test- ability. For active objects and multithreaded code, we can refactor to Humble Executable (see Humble Object on page 695). For user interface objects, we\n\nwww.it-ebooks.info\n\nThe Principles\n\ncan refactor to Humble Dialog (see Humble Object). Even Test Methods can have much of their untestable code extracted into Test Utility Methods, which can then be tested.\n\nWhen we Minimize Untestable Code, we improve the overall test coverage of our code. In so doing, we also improve our conﬁ dence in the code and extend our ability to refactor at will. The fact that this technique improves the quality of the code is yet another beneﬁ t.\n\nPrinciple: Keep Test Logic Out of Production Code\n\nWhen the production code hasn’t been designed for testability (whether as a result of test-driven development or otherwise), we may be tempted to put “hooks” into the production code to make it easier to test. These hooks typi- cally take the form of if testing then ... and may either run alternative logic or prevent certain logic from running.\n\nTesting is about verifying the behavior of a system. If the system behaves dif- ferently when under test, then how can we be certain that the production code actually works? Even worse, the test hooks could cause the software to fail in production!\n\nThe production code should not contain any conditional statements of the if testing then sort. Likewise, it should not contain any test logic. A well-designed system (from a testing perspective) is one that allows for the isolation of func- tionality. Object-oriented systems are particularly amenable to testing because they are composed of discrete objects. Unfortunately, even object-oriented sys- tems can be built in such a way as to be difﬁ cult to test, and we may still en- counter code with embedded test logic.\n\nPrinciple: Verify One Condition per Test\n\nMany tests require a starting state other than the default state of the SUT, and many operations of the SUT leave it in a different state from its original state. There is a strong temptation to reuse the end state of one test condition as the starting state of the next test condition by combining the veriﬁ cation of the two test conditions into a single Test Method because this makes testing more efﬁ - cient. This approach is not recommended, however, because when one assertion fails, the rest of the test will not be executed. As a consequence, it becomes more difﬁ cult to achieve Defect Localization.\n\nVerifying multiple conditions in a single test makes sense when we execute tests manually because of the high overhead of test setup and because the live- ware can adapt to test failures. It is too much work to set up the ﬁ xture for a large number of manual tests, so human testers naturally tend to write long\n\nwww.it-ebooks.info\n\n45\n\nAlso known as: No Test Logic in Production Code\n\nAlso known as: Single-Condition Test\n\n46\n\nChapter 5 Principles of Test Automation\n\nmultiple-condition tests.2 They also have the intelligence to work around any issues they encounter so that all is not lost if a single step fails. In contrast, with automated tests, a single failed assertion will cause the test to stop running and the rest of the test will provide no data on what works and what doesn’t.\n\nEach Scripted Test should verify a single test condition. This single-mindedness is possible because the test ﬁ xture is set up programmatically rather than by a human. Programs can set up ﬁ xtures very quickly and they don’t have trouble ex- ecuting exactly the same sequence of steps hundreds of times! If several tests need the same test ﬁ xture, either we can move the Test Methods into a single Testcase Class per Fixture (page 631) so we can use Implicit Setup (page 424) or we can call Test Utility Methods to set up the ﬁ xture using Delegated Setup (page 411). We design each test to have four distinct phases (see Four-Phase Test on page 358) that are executed in sequence: ﬁ xture setup, exercise SUT, result veriﬁ cation, and ﬁ xture teardown.\n\nIn the ﬁ rst phase, we set up the test ﬁ xture (the “before” picture) that is required for the SUT to exhibit the expected behavior as well as any- thing we need to put in place to observe the actual outcome (such as using a Test Double).\n\nIn the second phase, we interact with the SUT to exercise whatever behavior we are trying to verify. This should be a single, distinct behav- ior; if we try to exercise several parts of the SUT, we are not writing a Single-Condition Test.\n\nIn the third phase, we do whatever is necessary to determine whether the expected outcome has been obtained and fail the test if it has not.\n\nIn the fourth phase, we tear down the test ﬁ xture and put the world\n\nback into the state in which we found it.\n\nNote that there is a single exercise SUT phase and a single result veriﬁ cation phase. We avoid having a series of such alternating calls (exercise, verify, exercise, verify) because that approach would be trying to verify several distinct condi- tions—something that is better handled via distinct Test Methods.\n\nOne possibly contentious aspect of Verify One Condition per Test is what we mean by “one condition.” Some test drivers insist on one assertion per test. This insistence may be based on using a Testcase Class per Fixture organization of the Test Methods and naming each test based on what the one assertion is\n\n2 Clever testers often use automated test scripts to put the SUT into the correct starting state for their manual tests, thereby avoiding long manual test scripts.\n\nwww.it-ebooks.info\n\nThe Principles\n\nverifying.3 Having one assertion per test makes such naming very easy but also leads to many more test methods if we have to assert on many output ﬁ elds. Of course, we can often comply with this interpretation by extracting a Custom Assertion (page 474) or Veriﬁ cation Method (see Custom Assertion) that allows us to reduce the multiple assertion method calls to a single call. Sometimes that approach makes the test more readable. When it doesn’t, I wouldn’t be too dog- matic about insisting on a single assertion.\n\nPrinciple: Test Concerns Separately\n\nThe behavior of a complex application consists of the aggregate of a large num- ber of smaller behaviors. Sometimes several of these behaviors are provided by the same component. Each of these behaviors is a different concern and may have a signiﬁ cant number of scenarios in which it needs to be veriﬁ ed.\n\nThe problem with testing several concerns in a single Test Method is that this method will be broken whenever any of the tested concerns is modiﬁ ed. Even worse, it won’t be obvious which concern is the one at fault. Identify- ing the real culprit typically requires Manual Debugging (see Frequent Debug- ging on page 248) because of the lack of Defect Localization. The net effect is that more tests will fail and each test will take longer to troubleshoot and ﬁ x. Refactoring is also made more difﬁ cult by testing several concerns in the same test; it will be harder to “tease apart” the eager class into several independent classes, each of which implements a single concern, because the tests will need extensive redesign.\n\nTesting our concerns separately allows a failure to tell us that we have a problem in a speciﬁ c part of our system rather than simply saying that we have a problem somewhere. This approach to testing also makes it easier to understand the behavior now and to separate the concerns in subsequent refactorings. That is, we should just be able to move a subset of the tests to a different Testcase Class (page 373) that veriﬁ es the newly created class; it shouldn’t be necessary to modify the test much more than changing the class name of the SUT.\n\nPrinciple: Ensure Commensurate Effort and Responsibility\n\nThe amount of effort it takes to write or modify tests should not exceed the effort it takes to implement the corresponding functionality. Likewise, the tools required to write or maintain the test should require no more expertise than the tools used to implement the functionality. For example, if we can conﬁ gure the\n\n3 For example, AwaitingApprovalFlight.validApproverRequestShouldBeApproved.\n\nwww.it-ebooks.info\n\n47\n\n48\n\nChapter 5 Principles of Test Automation\n\nbehavior of a SUT using metadata and we want to write tests that verify that the metadata is set up correctly, we should not have to write code to do so. A Data-Driven Test (page 288) would be much more appropriate in these circum- stances.\n\nWhat’s Next?\n\nPrevious chapters covered the common pitfalls (in the form of test smells) and goals of test automation. This chapter made the value system we use while choosing patterns explicit. In Chapter 6, Test Automation Strategy, we will examine the “hard to change” decisions that we should try to get right early in the project.\n\nwww.it-ebooks.info",
      "page_number": 102
    },
    {
      "number": 6,
      "title": "Test Automation Strategy",
      "start_page": 112,
      "end_page": 137,
      "detection_method": "regex_chapter",
      "content": "Chapter 6\n\nTest Automation Strategy\n\nAbout This Chapter\n\nIn previous chapters, we saw some of the problems we might encounter with test automation. In Chapter 5, Principles of Test Automation, we learned about some of the principles we can apply to help address those problems. This chapter gets a bit more concrete but still focuses at the 30,000-foot level. In the logical sequence of things, test strategy comes before ﬁ xture setup but is a somewhat more advanced topic. If you are new to test automation using xUnit, you may want to skip this chapter and come back after reading more about the basics of xUnit in Chapter 7, xUnit Basics, and about ﬁ xture setup and teardown in Chapter 8, Transient Fixture Management, and subsequent chapters.\n\nWhat’s Strategic?\n\nAs the story in the preface amply demonstrates, it is easy to get off on the wrong foot. This is especially true when you lack experience in test automation and when this testing strategy is adopted “bottom up.” If we catch the problems early enough, the cost of refactoring the tests to eliminate the problems can be manage- able. If, however, the problems are left to fester for too long or the wrong approach is taken to address them, a very large amount of effort can be wasted. This is not to suggest that we should follow a “big design upfront” (BDUF) approach to test automation. BDUF is almost always the wrong answer. Rather, it is helpful to be aware of the strategic decisions necessary and to make them “just in time” rather than “much too late.” This chapter gives a “head’s up” about some of the strategic issues we want to keep in mind so that we don’t get blindsided by them later.\n\nWhat makes a decision “strategic”? A decision is strategic if it is “hard to change.” That is, a strategic decision affects a large number of tests, especially such that many or all the tests would need to be converted to a different approach\n\n49\n\nwww.it-ebooks.info\n\n50\n\nChapter 6 Test Automation Strategy\n\nat the same time. Put another way, any decision that could cost a large amount of effort to change is strategic.\n\nCommon strategic decisions include the following considerations:\n\nWhich kinds of tests to automate?\n\nWhich tools to use to automate them?\n\nHow to manage the test ﬁ xture?\n\nHow to ensure that the system is easily tested and how the tests interact\n\nwith the SUT?\n\nEach of these decisions can have far-reaching consequences, so they are best made consciously, at the right time, and based on the best available information.\n\nThe strategies and more detailed patterns described in this book are equally applicable regardless of the kind of Test Automation Framework (page 298) we choose to use. Most of my experience is with xUnit, so it is the focus of this book. But “don’t throw out the baby with the bath water”: If you ﬁ nd yourself using a different kind of Test Automation Framework, remember that most of what you learn in regard to xUnit may still be applicable.\n\nWhich Kinds of Tests Should We Automate?\n\nRoughly speaking, we can divide tests into the following two categories:\n\nPer-functionality tests (also known as functional tests) verify the behavior\n\nof the SUT in response to a particular stimulus.\n\nCross-functional tests verify various aspects of the system’s behavior\n\nthat cut across speciﬁ c functionality.\n\nFigure 6.1 shows these two basic kinds of tests as two columns, each of which is further subdivided into more speciﬁ c kinds of tests.\n\nPer-Functionality Tests\n\nPer-functionality tests verify the directly observable behavior of a piece of soft- ware. The functionality can be business related (e.g., the principal use cases of the system) or related to operational requirements (e.g., system maintenance and speciﬁ c fault-tolerance scenarios). Most of these requirements can also be expressed as use cases, features, user stories, or test scenarios.\n\nPer-functionality tests can be characterized by whether the functionality is\n\nbusiness (or user) facing and by the size of the SUT on which they operate.\n\nwww.it-ebooks.info\n\nWhich Kinds of Tests Should We Automate?\n\nAutomated Automated Automated Automated various various\n\nBusiness Business Facing Facing\n\nAutomated Automated Automated Automated xUnit xUnit\n\nTechnology Technology Facing Facing\n\nAutomated Automated Automated Automated xUnit xUnit\n\nKind of Behavior Kind of Behavior\n\nPer Functionality Per Functionality Customer Customer Tests Tests Business Intent Business Intent (Executable Specification) (Executable Specification) Component Component Tests Tests Architect Intent Architect Intent (Design of the System) (Design of the System) Unit Unit Tests Tests Developer Intent Developer Intent (Design of the Code) (Design of the Code)\n\nSupport Support Development Development\n\nCross-Functional Cross-Functional Usability Usability Testing Testing Is it pleasurable? Is it pleasurable?\n\nExploratory Exploratory Testing Testing Is it self-consistent? Is it self-consistent?\n\nProperty Property Testing Testing Is it responsive, Is it responsive, secure, scalable? secure, scalable? Critique Critique Product Product\n\nManual Manual Manual Manual\n\nManual Manual Manual Manual\n\nDiagram adapted Diagram adapted from Mary from Mary Poppendieck and Poppendieck and Brian Marick Brian Marick\n\nSpecial-Purpose Special-Purpose Tool - Tool - Based Based Tool-Based Tool-Based\n\nPurpose of Tests Purpose of Tests\n\nFigure 6.1 A summary of the kinds of tests we write and why. The left column contains the tests we write that describe the functionality of the product at various levels of granularity; we perform these tests to support development. The right column contains tests that span speciﬁ c chunks of functionality; we execute these tests to critique the product. The bottom of each cell describes what we are trying to communicate or verify.\n\nCustomer Tests\n\nCustomer tests verify the behavior of the entire system or application. They typi- cally correspond to scenarios of one or more use cases, features, or user stories. These tests often go by other names such as functional tests, acceptance tests, or end-user tests. Although they may be automated by developers, their key char- acteristic is that an end user should be able to recognize the behavior speciﬁ ed by the test even if the user cannot read the test representation.\n\nUnit Tests\n\nUnit tests verify the behavior of a single class or method that is a consequence of a design decision. This behavior is typically not directly related to the require- ments except when a key chunk of business logic is encapsulated within the class or method in question. These tests are written by developers for their own use; they help developers describe what “done looks like” by summarizing the behavior of the unit in the form of tests.\n\nwww.it-ebooks.info\n\n51\n\n52\n\nChapter 6 Test Automation Strategy\n\nComponent Tests\n\nComponent tests verify components consisting of groups of classes that collec- tively provide some service. They ﬁ t somewhere between unit tests and customer tests in terms of the size of the SUT being veriﬁ ed. Although some people call these “integration tests” or “subsystem tests,” those terms can mean something entirely different from “tests of a speciﬁ c larger-grained subcomponent of the overall system.”\n\nFault Insertion Tests\n\nFault insertion tests typically show up at all three levels of granularity within these functional tests, with different kinds of faults being inserted at each level. From a test automation strategy point of view, fault insertion is just another set of tests at the unit and component test levels. Things get more interesting at the whole-application level, however. Inserting faults here can be hard to automate because it is challenging to automate insertion of the faults without replacing parts of the application.\n\nCross-Functional Tests\n\nProperty Tests\n\nPerformance tests verify various “nonfunctional” (also known as “extra-functional” or “cross-functional”) requirements of the system. These requirements are different in that they span the various kinds of functionality. They often correspond to the architectural “-ilities.” These kinds of tests include\n\nResponse time tests\n\nCapacity tests\n\nStress tests\n\nFrom a test automation perspective, many of these tests must be automated (at least partially) because human testers would have a hard time creating enough load to verify the behavior under stress. While we can run the same test many times in a row in xUnit, the xUnit framework is not particularly well suited to automating performance tests.\n\nOne advantage of agile methods is that we can start running these kinds of tests quite early in the project—as soon as the key components of the architecture have been roughed in and the skeleton of the functionality is executable. The same tests can then be run continuously throughout the project as new features are added to the system skeleton.\n\nwww.it-ebooks.info\n\nWhich Tools Do We Use to Automate Which Tests?\n\nUsability Tests\n\nUsability tests verify “ﬁ tness for purpose” by conﬁ rming that real users can use the software application to achieve the stated goals. These tests are very difﬁ cult to automate because they require subjective assessment by people regarding how easy it is to use the SUT. For this reason, usability tests are rarely automated and will not be discussed further in this book.\n\nExploratory Testing\n\nExploratory testing is a way to determine whether the product is self-consistent. The testers use the product, observe how it behaves, form hypotheses, design tests to verify those hypotheses, and exercise the product with them. By its very nature, exploratory testing cannot be automated, although automated tests can be used to set up the SUT in preparation for doing exploratory testing.\n\nWhich Tools Do We Use to Automate Which Tests?\n\nChoosing the right tool for the job is as important as having good skills with the tools selected for use. A wide array of tools are available in the marketplace, and it is easy to be seduced by the features of a particular tool. The choice of tool is a strategic decision: Once we have invested a lot of time and effort in learning a tool and automating many tests using that tool, it becomes much more difﬁ cult to change to a different tool.\n\nThere are two fundamentally different approaches to automating tests (Figure 6.2). The Recorded Test (page 278) approach involves the use of tools that monitor our interactions with the SUT while we test it manually. This information is then saved to a ﬁ le or database and becomes the script for re- playing this test against another (or even the same) version of the SUT. The main problem with Recorded Tests is the level of granularity they record. Most commercial tools record actions at the user interface (UI) element level, which results in Fragile Tests (page 239).\n\nThe second approach to automating tests, Hand-Scripted Tests (see Scripted Test on page 285), involves the hand-coding of test programs (“scripts”) that ex- ercise the system. While xUnit is probably the most commonly used Test Automation Framework for preparing Hand-Scripted Tests, they may be pre- pared in other ways, including “batch” ﬁ les, macro languages, and commercial or open-source test tools. Some of the better-known open-source tools for preparing Scripted Tests are Watir (test scripts coded in Ruby and run inside Internet Ex- plorer), Canoo WebTest (tests scripted in XML and run using the WebTest tool),\n\nwww.it-ebooks.info\n\n53\n\n54\n\nChapter 6 Test Automation Strategy\n\nand the ever-popular Fit (and its wiki-based sibling FitNesse). Some of these tools even provide a test capture capability, thereby blurring the lines between Scripted Tests and Recorded Tests.\n\nGranularity Granularity SUT SUT\n\nunit unit Unit Unit component component Component Component\n\nsystem system System System\n\nMeans of Means of Test – SUT Test – SUT Interaction Interaction\n\nAPI API\n\nUI UI\n\nRecorded Recorded Way of Capturing Tests Way of Capturing Tests\n\nScripted Scripted\n\nFigure 6.2 A summary of the three dimensions of test automation choices. The left side shows the two ways of interacting with the SUT. The bottom edge enumerates how we create the test scripts. The front-to-back dimension categorizes the different sizes of SUT we may choose to test.\n\nChoosing which test automation tools to use is a large part of the test strategy decision. A full survey of the different kinds of tools available is beyond the scope of this book, but a somewhat more detailed treatment of the topic is avail- able in [ARTRP]. The following sections summarize the information here to provide an overview of the strengths and weaknesses of each approach.\n\nTest Automation Ways and Means\n\nFigure 6.3 depicts the decision-making possibilities as a matrix. In theory, there are 2 × 2 × 3 possible combinations in this matrix, but it is possible to under- stand the primary differences between the approaches by looking at the front face of the cube. Some of the four quadrants are applicable to all levels of granu- larity; others are primarily used for automating customer tests.\n\nwww.it-ebooks.info\n\nWhich Tools Do We Use to Automate Which Tests?\n\nBuilt-in Built-in Built-in Built-in R&PB R&PB R&PB R&PB\n\nMeans of Means of Test – SUT Test – SUT Interaction Interaction\n\nRobot Robot Robot Robot User User User User\n\nAPI API\n\nUI UI\n\n+ Somewhat robust + Somewhat robust - Less maintainable? - Less maintainable? - Cannot be prebuilt - Cannot be prebuilt + Fewer skills required + Fewer skills required - API required - API required - Few COTS tools - Few COTS tools - Very fragile - Very fragile - Not maintainable - Not maintainable - Cannot be pre-built - Cannot be pre-built + No special skills + No special skills + API not required + API not required - Mostly complex, flaky, - Mostly complex, flaky,\n\nexpensive tools expensive tools Recorded Tests Recorded Tests\n\n+ Robust + Robust + More maintainable + More maintainable + Can be prebuilt + Can be prebuilt - - - More skills required - More skills required - - - API required - API required + Simple, cheap tools + Simple, cheap tools - - - Somewhat fragile - Somewhat fragile - - - High maintenance - High maintenance + Can be prebuilt + Can be prebuilt - More skills required - More skills required + UI is the API + UI is the API + Mostly open- + Mostly open- source tools source tools Scripted Tests Scripted Tests\n\nModern Modern Modern Modern xUnit xUnit XUnit XUnit\n\nScripted Scripted Scripted Scripted UI Tests UI Tests UI Tests UI Tests\n\nWay of Capturing Tests Way of Capturing Tests\n\nFigure 6.3 The choices on the front face of the cube. A more detailed look at the front face of the cube in Figure 6.2 along with the advantages (+) and disadvantages of each (–).\n\nUpper-Right Quadrant: Modern xUnit\n\nThe upper-right quadrant of the front face of the cube is dominated by the xUnit family of testing frameworks. These frameworks involve hand-scripting tests that exercise the system at all three levels of granularity (system, component, and unit) via internal interfaces. A good example is unit tests automated using JUnit or NUnit.\n\nLower-Right Quadrant: Scripted UI Tests\n\nThis quadrant represents a variation on the “modern xUnit” approach, with the most common examples being the use of HttpUnit, JFCUnit, Watir, or similar tools to hand-script tests using the UI. It is also possible to hand-script tests using commercial Recorded Test tools such as QTP. These approaches all reside within the lower-right quadrant at various levels of SUT granularity. For example, when used for customer tests, these tools would perform at the system test level of granularity. They could also be used to test just the UI component of the system (or possibly even some UI units such as custom widgets), although this effort would require stubbing out the actual system behind the UI.\n\nLower-Left Quadrant: Robot User\n\nThe “robot user” quadrant focuses on recording tests that interact with the system via the UI. Most commercial test automation tools follow this approach. It applies primarily at the “whole system” granularity but, like scripted UI Tests,\n\nwww.it-ebooks.info\n\n55\n\n56\n\nChapter 6 Test Automation Strategy\n\ncould be applied to the UI components or units if the rest of the system can be stubbed out.\n\nUpper-Left Quadrant: Internal Recording\n\nFor completeness, the upper-left quadrant involves creating Recorded Tests via an API somewhere behind the UI by recording all inputs and responses as the SUT is exercised. It may even involve inserting observation points between the SUT (at whatever granularity we are testing) and any DOCs. During test play- back, the test APIs inject the inputs recorded earlier and compare the results with what was recorded\n\nThis quadrant is not well populated with commercial tools1 but is a feasible\n\noption when building a Recorded Test mechanism into the application itself.\n\nIntroducing xUnit\n\nThe xUnit family of Test Automation Frameworks is designed for use in auto- mating programmer tests. Its design is intended to meet the following goals:\n\nMake it easy for developers to write tests without needing to learn a new programming language. xUnit is available in most languages in use today.\n\nMake it easy to test individual classes and objects without needing to have the rest of the application available. xUnit is designed to allow us to test the software from the inside; we just have to design for testability to take advantage of this capability.\n\nMake it easy to run one test or many tests with a single action. xUnit includes the concept of a test suite and Suite of Suites (see Test Suite Object on page 387) to support this kind of test execution.\n\nMinimize the cost of running the tests so programmers aren’t discour- aged from running the existing tests. For this reason, each test should be a Self-Checking Test (page 26) that implements the Hollywood principle.2\n\n1 Most of the tools in this quadrant focus on recording regression tests by inserting obser- vation points into a component-based application and recording the (remote) method calls and responses between the components. This approach is becoming more popular with the advent of service-oriented architecture (SOA). 2 The name is derived from what directors in Hollywood tell aspiring applicants at mass casting calls: “Don’t call us; we’ll call you (if we want you).”\n\nwww.it-ebooks.info\n\nWhich Tools Do We Use to Automate Which Tests?\n\nThe xUnit family has been extraordinarily successful at meeting its goals. I cannot imagine that Erich Gamma and Kent Beck could have possibly antici- pated just how big an impact that ﬁ rst version of JUnit would have on software development!3 The same characteristics that make xUnit particularly well suited to automating programmer tests, however, may make it less suitable for writing some other kinds of tests. In particular, the “stop on ﬁ rst failure” behavior of as- sertions in xUnit has often been criticized (or overridden) by people who want to use xUnit for automating multistep customer tests so that they can see the whole score (what worked and what didn’t) rather than merely the ﬁ rst deviation from the expected results. This disagreement points out several things:\n\n“Stop on ﬁ rst failure” is a tool philosophy, not a characteristic of unit tests. It so happens that most test automaters prefer to have their unit tests stop on ﬁ rst failure, and most recognize that customer tests must necessarily be longer than unit tests.\n\nIt is possible to change the fundamental behavior of xUnit to satisfy speciﬁ c needs; this ﬂ exibility is just one advantage of open-source tools.\n\nSeeing a need to change the fundamental behavior of xUnit should probably be interpreted as a trigger for considering whether some other tool might possibly be a better ﬁ t.\n\nFor example, the Fit framework has been designed speciﬁ cally for the purpose of running customer tests. It overcomes the limitations of xUnit that lead to the “stop on ﬁ rst failure” behavior by communicating the pass/fail status of each step of a test using color coding. Another option for Java developers is TestNG, which provides capabilities for explicitly sequencing Chained Tests (page 454).\n\nHaving said this, choosing a different tool doesn’t eliminate the need to make many of the strategic decisions unless the tool constrains that decision making in some way. For example, we still need to set up the test ﬁ xture for a Fit test. Some patterns—such as Chained Tests, where one test sets up the ﬁ xture for a subsequent test—are difﬁ cult to automate and may therefore be less attractive in Fit than in xUnit. And isn’t it ironic that the very ﬂ exibility of xUnit is what al- lows test automaters to get themselves into so much trouble by creating Obscure Tests (page 186) that result in High Test Maintenance Cost (page 265)?\n\n3 Technically, SUnit came ﬁ rst but it took JUnit and the “Test Infected” article [TI] to really get things rolling.\n\nwww.it-ebooks.info\n\n57\n\n58\n\nChapter 6 Test Automation Strategy\n\nThe xUnit Sweet Spot\n\nThe xUnit family works best when we can organize our tests as a large set of small tests, each of which requires a small test ﬁ xture that is relatively easy to set up. This allows us to create a separate test for each test scenario of each object. The test ﬁ xture should be managed using a Fresh Fixture (page 311) strategy by setting up a new Minimal Fixture (page 302) for each test.\n\nxUnit works best when we write tests against software APIs and then test single classes or small groups of classes in isolation. This approach allows us to build small test ﬁ xtures that can be instantiated quickly.\n\nWhen doing customer tests, xUnit works best if we deﬁ ne a Higher-Level Language (page 41) with which to describe our tests. This choice moves the level of abstraction higher, away from the nitty-gritty of the technology and closer to the business concepts that customers understand. From here, it is a very small step to convert these tests to Data-Driven Tests (page 288) imple- mented in xUnit or Fit.\n\nNote that many of the higher-level patterns and principles described in this book apply equally well to both Fit tests and xUnit tests. I have also found them to be useful when working with commercial GUI-based testing tools, which typically use a “record and playback” metaphor. The ﬁ xture management patterns are particularly salient in this arena, as are reusable “test components” that may be strung together to form a variety of test scripts. This is entirely analogous to the xUnit practice of single-purpose Test Methods (page 348) calling reusable Test Utility Methods (page 599) to reduce their coupling to the SUT’s API.\n\nWhich Test Fixture Strategy Do We Use?\n\nThe test ﬁ xture management strategy is strategic because it has a large impact on the execution time and robustness of the tests. The effects of picking the wrong strategy won’t be felt immediately because it takes at least a few hundred tests before the Slow Tests (page 253) smell becomes evident and probably several months of development before the High Test Maintenance Cost smell starts to emerge. Once these smells appear, however, the need to change the test automa- tion strategy will become apparent—and its cost will be signiﬁ cant because of the number of tests affected.\n\nwww.it-ebooks.info\n\nWhich Test Fixture Strategy Do We Use?\n\nWhat Is a Fixture?\n\nEvery test consists of four parts, as described in Four-Phase Test (page 358). In the ﬁ rst phase, we create the SUT and everything it depends on and put them into the state required to exercise the SUT. In xUnit, we call everything we need in place to exercise the SUT the test ﬁ xture, and we call the part of the test logic that we execute to set it up the ﬁ xture setup phase of the test.\n\nAt this point, a word of caution is in order. The term “ﬁ xture” means many\n\nthings to many people:\n\nSome variants of xUnit keep the concept of the ﬁ xture separate from the Testcase Class (page 373) that creates it. JUnit and its direct ports fall into this category.\n\nOther members of the xUnit family assume that an instance of the Test-\n\ncase Class “is a” ﬁ xture. NUnit is a good example.\n\nA third camp uses an entirely different name for the ﬁ xture. For example, RSpec captures the pre-conditions of the test in a test con- text class that holds the Test Methods (same idea as NUnit but with different terminology).\n\nThe term “ﬁ xture” is used to mean entirely different things in other kinds of test automation. In Fit, for example, it means the custom-built parts of the Data-Driven Test Interpreter [GOF] that we use to deﬁ ne our Higher-Level Language.\n\nThe “class ‘is a’ ﬁ xture” approach assumes the Testcase Class per Fixture (page 631) approach to organizing the tests. When we choose a different way of organizing the tests, such as Testcase Class per Class (page 617) or Testcase Class per Fea- ture (page 624), this merging of the concepts of test ﬁ xture and Testcase Class can be confusing. Throughout this book, I use “test ﬁ xture”—or just “ﬁ xture”—to mean “the pre-conditions of the test” and Testcase Class to mean “the class that contains the Test Methods and any code needed to set up the test ﬁ xture.”\n\nThe most common way to set up the ﬁ xture is to use front door ﬁ xture setup by calling the appropriate methods on the SUT to construct the objects. When the state of the SUT is stored in other objects or components, we can do Back Door Setup (see Back Door Manipulation on page 327) by inserting the neces- sary records directly into the other component on which the behavior of the SUT depends. We use Back Door Setup most often with databases or when we need to use a Mock Object (page 544) or Test Double (page 522); these concepts are covered in more detail in Chapter 13, Testing with Databases, and Chapter 11, Using Test Doubles.\n\nwww.it-ebooks.info\n\n59\n\n60\n\nChapter 6 Test Automation Strategy\n\nMajor Fixture Strategies\n\nThere are probably many ways to classify just about anything. For the purposes of this discussion, we will classify our test ﬁ xture strategies based on what kinds of test development work we need to do for each one.\n\nThe ﬁ rst and simplest ﬁ xture management strategy requires us to worry only how we will organize the code to build the ﬁ xture for each test. That is, do we put this code in our Test Methods, factor it into Test Utility Methods that we call from our Test Methods, or put it into a setUp method in our Testcase Class? This strategy involves the use of Transient Fresh Fixtures (see Fresh Fixture). These ﬁ xtures live only in memory and very conveniently disappear as soon as we are done with them.\n\nA second strategy involves the use of Fresh Fixtures that, for one reason or another, persist beyond the single Test Method that uses it. To keep them from turning into Shared Fixtures (page 317), these Persistent Fresh Fixtures (see Fresh Fixture) require explicit code to tear them down at the end of each test. This requirement brings into play the ﬁ xture teardown patterns.\n\nA third strategy involves persistent ﬁ xtures that are deliberately reused across many tests. This Shared Fixture strategy is often used to improve the execu- tion speed of tests that use a Persistent Fresh Fixture but comes with a fair amount of baggage. These tests require the use of one of the ﬁ xture construc- tion and teardown triggering patterns. They also involve tests that interact with one another, whether by design or by consequence, which often leads to Erratic Tests (page 228) and High Test Maintenance Costs.\n\nTable 6.1 summarizes the ﬁ xture management overhead associated with each\n\nof the three styles of ﬁ xtures.\n\nTable 6.1 A Summary of the Fixture Setup and Teardown Requirements of the Various Test Fixture Strategies\n\nSet Up Code\n\nTear Down Code\n\nSetup/Teardown Triggering\n\nTransient Fresh Fixture\n\nYes\n\nPersistent Fresh Fixture\n\nYes\n\nYes\n\nShared Fixture\n\nYes\n\nYes\n\nYes\n\nNote: The Shared Fixture row assumes we are building a new Shared Fixture each test\n\nrun rather than using a Prebuilt Fixture (page 429).\n\nwww.it-ebooks.info\n\nWhich Test Fixture Strategy Do We Use?\n\nFigure 6.4 illustrates the interaction between our goals, freshness of ﬁ xtures or ﬁ xture reuse, and ﬁ xture persistence. It also illustrates a few variations of the Shared Fixture.\n\nTransient Transient\n\nFresh Fresh Fixture Fixture\n\nPersistent Persistent\n\nShared Shared Fixture Fixture\n\nImmutable Immutable Shared Shared Fixture Fixture\n\nFigure 6.4 A summary of the main test ﬁ xture strategies. Fresh Fixtures can be either transient or persistent; Shared Fixtures must be persistent. An Immutable Shared Fixture (see Shared Fixture) must not be modiﬁ ed by any test. As a consequence, most tests augment the Shared Fixture with a Fresh Fixture that they can modify.\n\nThe relationship between persistence and freshness is reasonably obvious for two of these combinations. The persistent Fresh Fixture is discussed in more detail later in this chapter. The transient Shared Fixture is inherently transient— how we hold references to these ﬁ xtures is what makes them persist. Other than this distinction, transient Shared Fixtures can be treated exactly like persistent Shared Fixtures.\n\nTransient Fresh Fixtures\n\nIn this approach, each test creates a temporary Fresh Fixture as it runs. Any objects or records it requires are created by the test itself (though not necessarily inside the Test Method). Because the test ﬁ xture visibility is restricted to the one test alone, we ensure that each test is completely independent because it cannot depend, either accidentally or on purpose, on the output of any other tests that use the same ﬁ xture.\n\nwww.it-ebooks.info\n\n61\n\n62\n\nChapter 6 Test Automation Strategy\n\nWe call this approach Fresh Fixture because each test starts with a clean slate and builds from there. It does not “inherit” or “reuse” any part of the ﬁ xture from other tests or from a Prebuilt Fixture (page 429). Every object or record used by the SUT is “fresh,” “brand new,” and not “previously enjoyed.”\n\nThe main disadvantage of using the Fresh Fixture approach is the additional CPU cycles it takes to create all the objects for each test. As a consequence, the tests may run more slowly than under a Shared Fixture approach, especially if we use a Persistent Fresh Fixture.\n\nPersistent Fresh Fixtures\n\nA Persistent Fresh Fixture sounds a bit oxymoronic. We want the ﬁ xture to be fresh, yet it persists beyond the lifetime of a single test! What kind of strategy is that? Some might say “stupid,” but sometimes one has to do this.\n\nWe are “forced” into this strategy when we are testing components that are tightly coupled to a database or other persistence mechanism. The obvious so- lution is that we should not let the coupling be so tight, but rather make the database a substitutable dependency of the component we are testing. This step may not be practical when testing legacy software, however—yet we may still want to partake of the beneﬁ ts of a Fresh Fixture. Hence the existence of the Persistent Fresh Fixture strategy. The key difference between this strategy and the Transient Fresh Fixture is the need for code to tear down the ﬁ xture after each test. Persistent Fresh Fixtures can result in Slow Tests if the persistence of the ﬁ xture is caused by the use of a database, ﬁ le system, or other high-latency dependency.\n\nWe can at least partially address the resulting Slow Tests by applying one or\n\nmore of the following patterns:\n\n1. Construct a Minimal Fixture (the smallest ﬁ xture possible).\n\n2. Speed up the construction by using a Test Double to replace the pro-\n\nvider of any data that takes too long to set up.\n\n3.\n\nIf the tests still are not fast enough, minimize the size of the part of the ﬁ xture we need to destroy and reconstruct each time by using an Immutable Shared Fixture for any objects that are referenced but not modiﬁ ed.\n\nThe project teams with which I have worked have found that, on average, our tests run 50 times faster (yes, they take 2% as long) when we use Dependency Injection (page 678) or Dependency Lookup (page 686) to replace the entire database with a Fake Database (see Fake Object on page 551) that uses a set of\n\nwww.it-ebooks.info\n\nWhich Test Fixture Strategy Do We Use?\n\nhash tables instead of tables. Each test may require many, many database opera- tions to set up and tear down the ﬁ xture required by a single query in the SUT. There is a lot to be said for minimizing the size and complexity of the test ﬁ xture. A Minimal Fixture (see Minimal Fixture) is much easier to understand and helps highlight the cause–effect relationship between the ﬁ xture and the ex- pected outcome. In this regard, it is a major enabler of Tests as Documentation (page 23). In some cases, we can make the test ﬁ xture much smaller by using Entity Chain Snipping (see Test Stub on page 529) to eliminate the need to in- stantiate those objects on which our test depends only indirectly. This tactic will certainly speed up the instantiation of our test ﬁ xture.\n\nShared Fixture Strategies\n\nSometimes we cannot—or choose not to—use a Fresh Fixture strategy. In these cases, we can use a Shared Fixture. In this approach, many tests reuse the same instance of a test ﬁ xture.\n\nThe major advantage of Shared Fixtures is that we save a lot of execution time in setting up and tearing down the ﬁ xture. The main disadvantage is con- veyed by one of its aliases, Stale Fixture, and by the test smell that describes its most common side effects, Interacting Tests (see Erratic Test). Although Shared Fixtures do have other beneﬁ ts, most can be realized by applying other patterns to Fresh Fixtures; Standard Fixture (page 305) avoids the ﬁ xture design and coding effort for every test without actually sharing the ﬁ xture.\n\nNow, if Shared Fixtures are so bad, why even discuss them? Because every- one seems to go down this road at least once in his or her career—so we might as well share the best available information about them should you venture down that path. Mind you, this discussion isn’t meant to encourage anyone to go down this path unnecessarily because it is paved with broken glass, infested with poisonous snakes, and . . . well, you get my drift.\n\nGiven that we have decided to use a Shared Fixture (we did investigate every possible alternative, didn’t we?), what are our options? We can make the fol- lowing adjustments (Figure 6.5):\n\nHow far and wide we share a ﬁ xture (e.g., a Testcase Class, all tests in\n\na test suite, all test run by a particular user)\n\nHow often we recreate the ﬁ xture\n\nwww.it-ebooks.info\n\n63\n\n64\n\nChapter 6 Test Automation Strategy\n\nShared Shared Fixture Fixture\n\nPrebuilt Prebuilt Fixture Fixture\n\nLazy Lazy Setup Setup\n\nSetup Setup Decorator Decorator\n\nSuite Suite Fixture Fixture Setup Setup\n\nChained Chained Tests Tests\n\nShared Fixture Setup Shared Fixture Setup\n\nFigure 6.5 The various ways we can manage a Shared Fixture. The strategies are ordered by the length of the ﬁ xture’s lifetime, with the longestlasting ﬁ xture appearing on the left.\n\nThe more tests that share a ﬁ xture, the more likely that one of them will make a mess of things and spoil everything for all the tests that follow it. The less often we reconstruct the ﬁ xture, the longer the effects of a messed-up ﬁ xture will per- sist. For example, a Prebuilt Fixture can be set up outside the test run, thereby avoiding the entire cost of setting up the ﬁ xture as part of the test run; unfor- tunately, it can also result in Unrepeatable Tests (see Erratic Test) if tests don’t clean up after themselves properly. This strategy is most commonly used with a Database Sandbox (page 650) that is initialized using a database script; once the ﬁ xture is corrupted, it must be reinitialized by rerunning the script. If the Shared Fixture is accessible to more than one Test Runner (page 377), we may end up in a Test Run War (see Erratic Test), in which tests fail randomly as they try to use the same ﬁ xture resource at the same time as some other test.\n\nWe can avoid both Unrepeatable Tests and Test Run Wars by setting up the ﬁ xture each time the test suite is run. xUnit provides several ways to do so, including Lazy Setup (page 435), Suite Fixture Setup (page 441), and Setup Decorator (page 447). The concept of “lazy initialization” should be familiar to most object-oriented developers; here we just apply the concept to the construc- tion of the test ﬁ xture. The latter two choices provide a way to tear down the test ﬁ xture when the test run is ﬁ nished because they call a setUp method and a corresponding tearDown at the appropriate times; Lazy Setup does not give us a way to do this.\n\nChained Tests represent another option for setting up a Shared Fixture, one that involves running the tests in a predeﬁ ned order and letting each test use the previ ous test’s results as its test ﬁ xture. Unfortunately, once one test fails, many of the tests that follow will provide erratic results because their pre-conditions have not\n\nwww.it-ebooks.info\n\nHow Do We Ensure Testability?\n\nbeen satisﬁ ed. This problem can be made easier to diagnose by having each test use Guard Assertions (page 490) to verify that its pre-conditions have been met.4\n\nAs mentioned earlier, an Immutable Shared Fixture is a strategy for speeding up tests that use a Fresh Fixture. We can also use an Immutable Shared Fixture to make tests based on a Shared Fixture less erratic by restricting changes to a smaller, mutable part of a Shared Fixture.\n\nHow Do We Ensure Testability?\n\nThe last strategic concern touched on in this chapter is ensuring testability. The discussion here isn’t intended to be a complete treatment of the topic—it is too large to cover in a single chapter on test strategy. Nevertheless, we shouldn’t sweep this issue under the carpet either, because it deﬁ nitely has a major impact on test automation. But ﬁ rst, I must climb onto my soapbox for a short digres- sion into the development process.\n\nTest Last—at Your Peril\n\nAnyone who has tried to retroﬁ t unit tests onto an existing application has prob- ably experienced a lot of pain! This is the hardest kind of test automation we can do as well as the least productive. A lot of the beneﬁ t of automated tests is derived during the “debugging phase” of software development, when such tests can reduce the amount of time spent working with debugging tools. Tackling a test retroﬁ t on legacy software as your ﬁ rst attempt at automated unit testing is the last thing you want to try, as it is sure to discourage even the most deter- mined developers and project managers.\n\nDesign for Testability—Upfront\n\nBDUF5 design for testability is hard because it is difﬁ cult to know what the tests will need in the way of control points and observation points on the SUT. We can easily build software that is difﬁ cult to test. We can also spend a lot of time designing in testability mechanisms that are either insufﬁ cient or unnecessary. Either way, we will have spent a lot of effort with nothing to show for it.\n\n4 Unfortunately, this may result in slower tests when the ﬁ xture is in a database. Never- theless, it will still be many times faster than if each test had to insert all the records it needed. 5 “Big Design Upfront” (also known as “waterfall design”) is the opposite of emergent design (“just-in-time design”).\n\nwww.it-ebooks.info\n\n65\n\n66\n\nChapter 6 Test Automation Strategy\n\nTest-Driven Testability\n\nThe nice thing about building our software driven by tests is that we don’t have to think very much about design for testability; we just write the tests and that forces us to build for testability. The act of writing the test deﬁ nes the control points and observation points that the SUT needs to provide. Once we have passed the tests, we know we have a testable design.\n\nNow that I’ve done my bit promoting TDD as a “design for testability” pro- cess, let’s get on with our discussion of the mechanics of actually making our software testable.\n\nControl Points and Observation Points\n\nA test interacts with the software6 through one or more interfaces or interaction points. From the test’s point of view, these interfaces can act as either control points or observation points (Figure 6.6).\n\nFixture Fixture\n\nSetup Setup\n\nInitialize Initialize\n\nDirect Inputs Direct Inputs (Control Points) (Control Points)\n\nSUT SUT\n\nIndirect Output Indirect Output (Observation Point) (Observation Point)\n\nGet Something Get Something (with return value) (with return value)\n\nDOC DOC\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise (with return value) (with return value)\n\nDirect Outputs Direct Outputs (Observation Points) (Observation Points)\n\nGet State Get State\n\nA A\n\nB B\n\nC C\n\nIndirect Input Indirect Input (Control Point) (Control Point)\n\nDo Something Do Something (no return value) (no return value)\n\nIndirect Output Indirect Output (Observation Point) (Observation Point)\n\nTeardown Teardown\n\nFigure 6.6 Control points and observation points. The test interacts with the SUT through interaction points. Direct interaction points are synchronous method calls made by the test; indirect interaction points require some form of Back Door Manipulation. Control points have arrows pointing toward the SUT; observation points have arrows pointing away from the SUT.\n\n6 I am deliberately not saying “SUT” here because it interacts with more than just the SUT.\n\nwww.it-ebooks.info\n\nHow Do We Ensure Testability?\n\nA control point is how the test asks the software to do something for it. This could be for the purpose of putting the software into a speciﬁ c state as part of setting up or tearing down the test ﬁ xture, or it could be to exercise the SUT. Some control points are provided strictly for the tests; they should not be used by the production code because they bypass input validation or short-circuit the normal life cycle of the SUT or some object on which it depends.\n\nAn observation point is how the test ﬁ nds out about the SUT’s behavior dur- ing the result veriﬁ cation phase of the test. Observation points can be used to retrieve the post-test state of the SUT or a DOC. They can also be used to spy on the interactions between the SUT and any components with which it is expected to interact while it is being exercised. Verifying these indirect outputs is an example of Back Door Veriﬁ cation (see Back Door Manipulation).\n\nBoth control points and observation points can be provided by the SUT as synchronous method calls; we call this “going in the front door.” Some inter- action points may be via a “back door” to the SUT; we call this Back Door Manipulation. In the diagrams that follow, control points are represented by the arrowheads that point to the SUT, whether from the test or from a DOC. Observation points are represented by the arrows whose heads point back to the test itself. These arrows typically start at the SUT or DOC7 or start at the test and interact with either the SUT or DOC before returning to the test.8\n\nInteraction Styles and Testability Patterns\n\nWhen testing a particular piece of software, our tests can take one of two basic forms.\n\nA round-trip test interacts with the SUT in question only through its public interface—that is, its “front door” (Figure 6.7). Both the control points and the observation points in a typical round-trip test are simple method calls. The nice thing about this approach is that it does not violate encapsulation. The test needs to know only the public interface of the software; it doesn’t need to know anything about how it is built.\n\nThe main alternative is the layer-crossing test (Figure 6.8), in which we exer- cise the SUT through the API and keep an eye on what comes out the back door using some form of Test Double such as a Test Spy (page 538) or Mock Object. This can be a very powerful testing technique for verifying certain kinds of mostly architectural requirements. Unfortunately, this approach can also result in Overspeciﬁ ed Software (see Fragile Test) if it is overused because changes in how the software implements its responsibilities can cause tests to fail.\n\n7 An asynchronous observation point. 8 A synchronous observation point.\n\nwww.it-ebooks.info\n\n67\n\n68\n\nChapter 6 Test Automation Strategy\n\nRealComponentTest RealComponentTest\n\nFakeComponentTest FakeComponentTest\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\ntestMethod_2 testMethod_2\n\nInstallation Installation\n\nSUT SUT\n\nCreation Creation\n\nSUT SUT\n\nDOC DOC\n\nFake Fake Object Object\n\nDOC DOC\n\nFigure 6.7 A round-trip test interacts with the SUT only via the front door. The test on the right replaces a DOC with a Fake Object to improve its repeatability or performance.\n\nIndirectInputTest IndirectInputTest\n\nIndirectOutputTest IndirectOutputTest\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\ntestMethod_2 testMethod_2\n\nInstallation Installation\n\nInstallation Installation\n\nCreation Creation and and Configuration Configuration\n\nSUT SUT\n\nIndirect Indirect Inputs Inputs\n\nCreation Creation and and Configuration Configuration\n\nSUT SUT\n\nIndirect Indirect Outputs Outputs\n\nTest Stub Test Stub\n\nMock Object Mock Object\n\nDOC DOC\n\nDOC DOC\n\nFigure 6.8 A layer-crossing test can interact with the SUT via a “back door.” The test on the left controls the SUT’s indirect inputs using a Test Stub. The test on the right veriﬁ es its indirect outputs using a Mock Object.\n\nwww.it-ebooks.info\n\nHow Do We Ensure Testability?\n\nIn Figure 6.8, the test on the right uses a Mock Object that stands in for the DOC as the observation point. The test on the left uses a Test Stub that stands in for the DOC as a control point. Testing in this style implies a Layered Architecture [DDD, PEAA, WWW], which in turn opens the door to using Layer Tests (page 337) to test each layer of the architecture independently (Figure 6.9). An even more general concept is the use of Component Tests (see Layer Test) to test each com- ponent within a layer independently.\n\nLayer1TestcaseClass Layer1TestcaseClass testMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nLayernTestcaseClass LayernTestcaseClass testMethod_1 testMethod_1 testMethod_2 testMethod_2\n\nLayer n Layer n\n\nLayer 1 Layer 1\n\nTest Double Test Double\n\nTest Double Test Double\n\nDOC DOC\n\nFigure 6.9 A pair of Layer Tests each testing a different layer of the system. Each layer of a layered architecture can be tested independently using a distinct set of Layer Tests. This ensures good separation of concerns, and the tests reinforce the layered architecture.\n\nWhenever we want to write layer-crossing tests, we need to ensure that we have built in a substitutable dependency mechanism for any components on which the SUT depends but that we want to test independently. The leading contend- ers include any of the variations of Dependency Injection (Figure 6.10) or some form of Dependency Lookup such as Object Factory or Service Locator. These dependency substitution mechanisms can be hand-coded or we can use an in- version of control (IOC) framework if one is available in our programming environment. The fallback plan is to use a Test-Speciﬁ c Subclass (page 579) of the SUT or the DOC in question. This subclass can be used to override the dependency access or construction mechanism within the SUT or to replace the behavior of the DOC with test-speciﬁ c behavior.\n\nwww.it-ebooks.info\n\n69\n\n70\n\nChapter 6 Test Automation Strategy\n\nThe “solution of last resort” is the Test Hook (page 709).9 These constructs do have utility as temporary measures that allow us to automate tests to act as a Safety Net (page 24) while refactoring to retroﬁ t testability. We deﬁ nitely shouldn’t make a habit of using them, however, as continued use of Test Hooks will result in Test Logic in Production (page 217).\n\nClient Client\n\nCreation Creation\n\nDOC DOC\n\nUsage Usage\n\nUsage Usage\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nExercise Exercise\n\nCreation Creation\n\nSUT SUT\n\nUsage Usage\n\nTest Test Double Double\n\nFigure 6.10 A Test Double being “injected” into a SUT by a test. A test can use Dependency Injection to replace a DOC with an appropriate Test Double. The DOC is passed to the SUT by the test as or after it has been created.\n\nA third kind of test worth mentioning is the asynchronous test, in which the test interacts with the SUT through real messaging. Because the responses to these requests also come asynchronously, these tests must include some kind of inter- process synchronization such as calls to wait. Unfortunately, the need to wait for message responses that might never arrive can cause these tests to take much, much longer to execute. This style of testing should be avoided at all costs in unit and component tests.\n\nFortunately, the Humble Executable pattern (see Humble Object on page 695) can remove the need to conduct unit tests this way (Figure 6.11). It involves putting the logic that handles the incoming message into a separate class or component, which can then be tested synchronously using either a round-trip or layer-crossing style.\n\nA related issue is the testing of business logic through a UI. In general, such Indirect Testing (see Obscure Test) is a bad idea because changes to the UI code will break tests that are trying to verify the business logic behind it. Because the UI tends to change frequently, especially on agile projects, this strategy will greatly increase test maintenance costs. Another reason this is a bad idea is that\n\n9 These typically take the form of if (testing) then ... else ... endif.\n\nwww.it-ebooks.info\n\nHow Do We Ensure Testability?\n\nUIs are inherently asynchronous. Tests that exercise the system through the UI have to be asynchronous tests along with all the issues that come with them.\n\nAsynchronous Asynchronous Interface Interface\n\nHumble Humble Executable Executable\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nSynchronous Synchronous Interface Interface\n\nTestable Testable\n\nVerify Verify\n\nComponent Component\n\nTeardown Teardown\n\nFigure 6.11 A Humble Executable making testing easier. The Humble Executable pattern can improve the repeatability and speed of verifying logic that would otherwise have to be veriﬁ ed via asynchronous tests.\n\nDivide and Test\n\nWe can turn almost any Hard-to-Test Code (page 209) into easily tested code through refactoring as long as we have enough tests in place to ensure that we do not introduce bugs during this refactoring.\n\nWe can avoid using the UI for customer tests by writing those tests as Subcu- taneous Tests (see Layer Test). These tests bypass the UI layer of the system and exercise the business logic via a Service Facade [CJ2EEP] that exposes the neces- sary synchronous interaction points to the test. The UI relies on the same facade, enabling us to verify that the business logic works correctly even before we hook up the UI logic. The layered architecture also enables us to test the UI logic before the business logic is ﬁ nished; we can replace the Service Facade with a Test Double that provides completely deterministic behavior that our tests can depend on.10\n\n10 This Test Double can be either hard-coded or ﬁ le driven. Either way, it should be inde- pendent of the real implementation so that the UI tests need to know only which data to use to evoke speciﬁ c behaviors from the Service Facade, not the logic behind it.\n\nwww.it-ebooks.info\n\n71\n\n72\n\nChapter 6 Test Automation Strategy\n\nWhen conducting unit testing of nontrivial UIs,11 we can use a Humble Dialog (see Humble Object) to move the logic that makes decisions about the UI out of the visual layer, which is difﬁ cult to test synchronously, and into a layer of supporting objects, which can be veriﬁ ed with standard unit-testing techniques (Figure 6.12). This approach allows the presentation logic behavior to be tested as thoroughly as the business logic behavior.\n\nMock Mock Dialog Dialog\n\nHumble Dialog Humble Dialog Abc Abc def def ghi ghi\n\nOK OK\n\nCancel Cancel\n\nGUI GUI Frame- Frame- work work\n\nSetup Setup\n\nExercise Exercise\n\nTestable Testable\n\nVerify Verify\n\nGUI Logic GUI Logic\n\nComponent Component\n\nTeardown Teardown\n\nFigure 6.12 A Humble Dialog reducing the dependency of the test on the UI framework. The logic that controls the state of UI components can be very difﬁ cult to test. Extracting it into a testable component leaves behind a Humble Dialog that requires very little testing.\n\nFrom a test automation strategy perspective, the key thing is to make the decision about which test–SUT interaction styles should be used and which ones should be avoided, and to ensure that the software is designed to support that decision.\n\n11 Any UI that contains state information or supports conditional display or enabling of elements should be considered nontrivial.\n\nwww.it-ebooks.info\n\nWhat’s Next?\n\nWhat’s Next?\n\nThis concludes our introduction to the hard-to-change decisions we must make as we settle upon our test automation strategy. Given that you are still reading, I will assume that you have decided xUnit is an appropriate tool for doing your test automation. The following chapters introduce the detailed patterns for im- plementing our chosen ﬁ xture strategy, whether it involves a Fresh Fixture or a Shared Fixture. First, we will explore the simplest case, a Transient Fresh Fixture, in Chapter 8, Transient Fixture Management. We will then investigate the use of persistent ﬁ xtures in Chapter 9, Persistent Fixture Management. But ﬁ rst, we must establish the basic xUnit terminology and notation that is used throughout this book in Chapter 7, xUnit Basics.\n\nwww.it-ebooks.info\n\n73\n\nThis page intentionally left blank\n\nwww.it-ebooks.info",
      "page_number": 112
    },
    {
      "number": 7,
      "title": "xUnit Basics",
      "start_page": 138,
      "end_page": 147,
      "detection_method": "regex_chapter",
      "content": "Chapter 7\n\nxUnit Basics\n\nAbout This Chapter\n\nChapter 6, Test Automation Strategy, introduced the “hard to change” decisions that we need to get right early in the project. The current chapter serves two purposes. First, it introduces the xUnit terminology and diagramming notation used throughout this book. Second, it explains how the xUnit framework oper- ates beneath the covers and why it was built that way. This knowledge can help the builder of a new Test Automation Framework (page 298) understand how to port xUnit. It can also help test automaters understand how to use certain features of xUnit.\n\nAn Introduction to xUnit\n\nThe term xUnit is how we refer to any member of the family of Test Automa- tion Frameworks used for automating Hand-Scripted Tests (see Scripted Test on page 285) that share the common set of features described here. Most pro- gramming languages in widespread use today have at least one implementation of xUnit; Hand-Scripted Tests are usually automated using the same program- ming language as is used for building the SUT. Although this is not necessarily the case, this strategy is usually much easier because our tests have easy access to the SUT API. By using a programming language with which the developers are familiar, less effort is required to learn how to automate Fully Automated Tests (page 26).1\n\n1 See the sidebar “Testing Stored Procs with JUnit” (page 657) for an example of using a testing framework in one language to test an SUT in another language.\n\n75\n\nwww.it-ebooks.info\n\n76\n\nChapter 7 xUnit Basics\n\nCommon Features\n\nGiven that most members of the xUnit family are implemented using an object- oriented programming language (OOPL), they are described here ﬁ rst and then places where the non-OOPL members of the family differ are noted.\n\nAll members of the xUnit family implement a basic set of features. They all\n\nprovide a way to perform the following tasks:\n\nSpecify a test as a Test Method (page 348)\n\nSpecify the expected results within the test method in the form of calls\n\nto Assertion Methods (page 362)\n\nAggregate the tests into test suites that can be run as a single operation\n\nRun one or more tests to get a report on the results of the test run\n\nBecause many members of the xUnit family support Test Method Discovery (see Test Discovery on page 393), we do not have to use Test Enumeration (page 399) in these members to manually add each Test Method we want to run to a test suite. Some members also support some form of Test Selection (page 403) to run subsets of test methods based on some criteria.\n\nThe Bare Minimum\n\nHere is the bare minimum we need to understand about how xUnit operates (Figure 7.1):\n\nHow we deﬁ ne tests using Test Methods on Testcase Classes (page 373)\n\nHow we can build up arbitrary Suites of Suites (see Test Suite Object on page 387)2\n\nHow we run the tests\n\nHow we interpret the test results\n\nDeﬁ ning Tests\n\nEach test is represented by a Test Method that implements a single Four-Phase Test (page 358) by following these steps:\n\n2 Even those xUnit variants that don’t have an explicit Suite class or method still build Test Suite Objects behind the scene.\n\nwww.it-ebooks.info\n\nThe Bare Minimum\n\nSetting up the test ﬁ xture using either In-line Setup (page 408), Delegated\n\nSetup (page 411), or Implicit Setup (page 424)\n\nExercising the SUT by interacting with methods in its public or private\n\ninterface\n\nVerifying that the expected outcome has occurred using calls to Assertion\n\nMethods\n\nTearing down the test ﬁ xture using either Garbage-Collected Tear- down (page 500), In-line Teardown (page 509), Implicit Teardown (page 516), or Automated Teardown (page 503)\n\nTest Test Suite Suite Factory Factory\n\ntestMethod_1 testMethod_1\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\nSuite Suite\n\nTest Runner Test Runner\n\nCreate Create\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite\n\nSetup Setup\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nObject Object\n\nTeardown Teardown\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nFigure 7.1 The static test structure as seen by a test automater. The test automater sees only the static structure as he or she reads or writes tests. The test automater writes one Test Method with four distinct phases for each test in the Testcase Class. The Test Suite Factory (see Test Enumeration) is used only for Test Enumeration. The runtime structure (shown grayed out) is left to the test automater’s imagination.\n\nThe most common types of tests are the Simple Success Test (see Test Method), which veriﬁ es that the SUT has behaved correctly with valid inputs, and the Expected Exception Test (see Test Method), which veriﬁ es that the SUT raises an exception when used incorrectly. A special type of test, the Constructor Test (see Test Method), veriﬁ es that the object constructor logic builds new objects cor- rectly. Both “simple success” and “expected exception” forms of the Constructor Test may be needed. The Test Methods that contain our test logic need to live\n\nwww.it-ebooks.info\n\n77\n\n78\n\nChapter 7 xUnit Basics\n\nsomewhere, so we deﬁ ne them as methods of a Testcase Class.3 We then pass the name of this Testcase Class (or the module or assembly in which it resides) to the Test Runner (page 377) to run our tests. This may be done explicitly—such as when invoking the Test Runner on a command line—or implicitly by the integrated development environment (IDE) that we are using.\n\nWhat’s a Fixture?\n\nThe test ﬁ xture is everything we need to have in place to exercise the SUT. Typi- cally, it includes at least an instance of the class whose method we are testing. It may also include other objects on which the SUT depends. Note that some mem- bers of the xUnit family call the Testcase Class the test ﬁ xture—a preference that likely reﬂ ects an assumption that all Test Methods on the Testcase Class should use the same ﬁ xture. This unfortunate name collision makes discussing test ﬁ xtures particularly problematic. In this book, I have used different names for the Testcase Class and the test ﬁ xture it creates. I trust that the reader will translate this termi- nology to the terminology of his or her particular member of the xUnit family.\n\nDeﬁ ning Suites of Tests\n\nMost Test Runners “auto-magically” construct a test suite containing all of the Test Methods in the Testcase Class. Often, this is all we need. Sometimes we want to run all the tests for an entire application; at other times we want to run just those tests that focus on a speciﬁ c subset of the functionality. Some mem- bers of the xUnit family and some third-party tools implement Testcase Class Discovery (see Test Discovery) in which the Test Runner ﬁ nds the test suites by searching either the ﬁ le system or an executable for test suites.\n\nIf we do not have this capability, we need to use Test Suite Enumeration (see Test Enumeration), in which we deﬁ ne the overall test suite for the entire system or application as an aggregate of several smaller test suites. To do so, we must deﬁ ne a special Test Suite Factory class whose suite method returns a Test Suite Object containing the collection of Test Methods and other Test Suite Objects to run.\n\nThis collection of test suites into increasingly larger Suites of Suites is com- monly used as a way to include the unit test suite for a class into the test suite for the package or module, which is in turn included in the test suite for the entire system. Such a hierarchical organization supports the running of test suites with varying degrees of completeness and provides a practical way for developers to run that subset of the tests that is most relevant to the software of\n\n3 This scheme is called a test ﬁ xture in some variants of xUnit, probably because the creators assumed we would have a single Testcase Class per Fixture (page 631).\n\nwww.it-ebooks.info\n\nThe Bare Minimum\n\ninterest. It also allows them to run all the tests that exist with a single command before they commit their changes into the source code repository [SCM].\n\nRunning Tests\n\nTests are run by using a Test Runner. Several different kinds of Test Runners are available for most members of the xUnit family.\n\nA Graphical Test Runner (see Test Runner) provides a visual way for the user to specify, invoke, and observe the results of running a test suite. Some Graphi- cal Test Runners allow the user to specify a test by typing in the name of a Test Suite Factory; others provide a graphical Test Tree Explorer (see Test Runner) that can be used to select a speciﬁ c Test Method to execute from within a tree of test suites, where the Test Methods serve as the tree’s leaves. Many Graphical Test Runners are integrated into an IDE to make running tests as easy as select- ing the Run As Test command from a context menu.\n\nA Command-Line Test Runner (see Test Runner) can be used to execute tests when running the test suite from the command line, as in Figure 7.2. The name of the Test Suite Factory that should be used to create the test suite is included as a command-line parameter. Command-Line Test Runners are most common- ly used when invoking the Test Runner from Integration Build [SCM] scripts or sometimes from within an IDE.\n\n>ruby testrunner.rb c:/examples/tests/SmellHandlerTest.rb Loaded suite SmellHandlerTest Started ..... Finished in 0.016 seconds. 5 tests, 6 assertions, 0 failures, 0 errors >Exit code: 0\n\nFigure 7.2 Using a Command-Line Test Runner to run tests from the command line.\n\nTest Results\n\nNaturally, the main reason for running automated tests is to determine the re- sults. For the results to be meaningful, we need a standard way to describe them. In general, members of the xUnit family follow the Hollywood principle (“Don’t call us; we’ll call you”). In other words, “No news is good news”; the tests will “call you” when a problem occurs. Thus we can focus on the test failures rather than inspecting a bunch of passing tests as they roll by.\n\nTest results are classiﬁ ed into one of three categories, each of which is trea- ted slightly differently. When a test runs without any errors or failures, it is\n\nwww.it-ebooks.info\n\n79\n\n80\n\nChapter 7 xUnit Basics\n\nconsidered to be successful. In general, xUnit does not do anything special for successful tests—there should be no need to examine any output when a Self- Checking Test (page 26) passes.\n\nA test is considered to have failed when an assertion fails. That is, the test asserts that something should be true by calling an Assertion Method, but that assertion turns out not to be the case. When it fails, an Assertion Method throws an assertion failure exception (or whatever facsimile the programming language supports). The Test Automation Framework increments a counter for each failure and adds the failure details to a list of failures; this list can be ex- amined more closely later, after the test run is complete. The failure of a single test, while signiﬁ cant, does not prevent the remaining tests from being run; this is in keeping with the principle Keep Tests Independent (see page 42).\n\nA test is considered to have an error when either the SUT or the test itself fails in an unexpected way. Depending on the language being used, this problem could consist of an uncaught exception, a raised error, or something else. As with assertion failures, the Test Automation Framework increments a counter for each error and adds the error details to a list of errors, which can then be examined after the test run is complete.\n\nFor each test error or test failure, xUnit records information that can be ex- amined to help understand exactly what went wrong. As a minimum, the name of the Test Method and Testcase Class are recorded, along with the nature of the problem (whether it was a failed assertion or a software error). In most Graphical Test Runners that are integrated with an IDE, one merely has to (double-) click on the appropriate line in the traceback to see the source code that emitted the failure or caused the error.\n\nBecause the name test error sounds more drastic than a test failure, some test automaters try to catch all errors raised by the SUT and turn them into test failures. This is simply unnecessary. Ironically, in most cases it is easier to deter- mine the cause of a test error than the cause of a test failure: The stack trace for a test error will typically pinpoint the problem code within the SUT, whereas the stack track for a test failure merely shows the location in the test where the failed assertion was made. It is, however, worthwhile using Guard Asser- tions (page 490) to avoid executing code within the Test Method that would result in a test error being raised from within the Test Method4 itself; this is just a normal part of verifying the expected outcome of exercising the SUT and does not remove useful diagnostic tracebacks.\n\n4 For example, before executing an assertion on the contents of a ﬁ eld of an object returned by the SUT, it is worthwhile to assertNotNull on the object reference so as to avoid a “null reference” error.\n\nwww.it-ebooks.info\n\nUnder the xUnit Covers\n\nUnder the xUnit Covers\n\nThe description thus far has focused on Test Methods and Testcase Classes with the odd mention of test suites. This simpliﬁ ed “compile time” view is enough for most people to get started writing automated unit tests in xUnit. It is pos- sible to use xUnit without any further understanding of how the Test Automa- tion Framework operates—but the lack of more extensive knowledge is likely to lead to confusion when building and reusing test ﬁ xtures. Thus it is better to understand how xUnit actually runs the Test Methods. In most5 members of the xUnit family, each Test Method is represented at runtime by a Testcase Object (page 382) because it is a lot easier to manipulate tests if they are “ﬁ rst- class” objects (Figure 7.3). The Testcase Objects are aggregated into Test Suite Objects, which can then be used to run many tests with a single user action.\n\nTest Test Suite Suite Factory Factory\n\nCreate Create\n\ntestMethod_1 testMethod_1\n\nTestcase Testcase Object Object testMethod_1 testMethod_1\n\nSuite Suite\n\nTest Runner Test Runner\n\nCreate Create\n\nRun Run\n\nTest Test Suite Suite Object Object\n\nRun Run\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nRun Testcase Run Testcase Object Object testMethod_n testMethod_n\n\nExercise Exercise\n\nTestcase Testcase Class Class\n\nFigure 7.3 The runtime test structure as seen by the Test Automation Framework. At runtime, the Test Runner asks the Testcase Class or a Test Suite Factory to instantiate one Testcase Object for each Test Method, with the objects being wrapped up in a single Test Suite Object. The Test Runner tells this Composite [GOF] object to run its tests and collect the results. Each Testcase Object runs one Test Method.\n\n5 NUnit is a known exception and others may exist. See the sidebar “There’s Always an Exception” (page 384) for more information.\n\nwww.it-ebooks.info\n\n81\n\n82\n\nChapter 7 xUnit Basics\n\nTest Commands\n\nThe Test Runner cannot possibly know how to call each Test Method individu- ally. To avoid the need for this, most members of the xUnit family convert each Test Method into a Command [GOF] object with a run method. To create these Testcase Objects, the Test Runner calls the suite method of the Testcase Class to get a Test Suite Object. It then calls the run method via the standard test inter- face. The run method of a Testcase Object executes the speciﬁ c Test Method for which it was instantiated and reports whether it passed or failed. The run method of a Test Suite Object iterates over all the members of the collection of tests, keeping track of how many tests were run and which ones failed.\n\nTest Suite Objects\n\nA Test Suite Object is a Composite object that implements the same standard test interface that all Testcase Objects implement. That interface (implicit in lan- guages lacking a type or interface construct) requires provision of a run method. The expectation is that when run is invoked, all of the tests contained in the receiver will be run. In the case of a Testcase Object, it is itself a “test” and will run the corresponding Test Method. In the case of a Test Suite Object, that means invoking run on all of the Testcase Objects it contains. The value of using a Composite Command is that it turns the processes of running one test and running many tests into exactly the same process.\n\nTo this point, we have assumed that we already have the Test Suite Object instantiated. But where did it come from? By convention, each Testcase Class acts as a Test Suite Factory. The Test Suite Factory provides a class method called suite that returns a Test Suite Object containing one Testcase Object for each Test Method in the class. In languages that support some form of reﬂ ec- tion, xUnit may use Test Method Discovery to discover the test methods and automatically construct the Test Suite Object containing them. Other mem- bers of the xUnit family require test automaters to implement the suite method themselves; this kind of Test Enumeration takes more effort and is more likely to lead to Lost Tests (see Production Bugs on page 268).\n\nxUnit in the Procedural World\n\nTest Automation Frameworks and test-driven development became popular only after object-oriented programming became commonplace. Most members of the xUnit family are implemented in object-oriented programming languages\n\nwww.it-ebooks.info\n\nWhat’s Next?\n\nthat support the concept of a Testcase Object. Although the lack of objects should not keep us from testing procedural code, it does make writing Self- Checking Tests more labor-intensive and building generic, reusable Test Runners more difﬁ cult.\n\nIn the absence of objects or classes, we must treat Test Methods as global (public static) procedures. These methods are typically stored in ﬁ les or mod- ules (or whatever modularity mechanism the language supports). If the language supports the concept of procedure variables (also known as function pointers), we can deﬁ ne a generic Test Suite Procedure (see Test Suite Object) that takes an array of Test Methods (commonly called “test procedures”) as an argument. Typically, the Test Methods must be aggregated into the arrays using Test Enu- meration because very few non-object-oriented programming languages sup- port reﬂ ection.\n\nIf the language does not support any way of treating Test Methods as data, we must deﬁ ne the test suites by writing Test Suite Procedures that make explicit calls to Test Methods and/or other Test Suite Procedures. Test runs may be initi- ated by deﬁ ning a main method on the module.\n\nA ﬁ nal option is to encode the tests as data in a ﬁ le and use a single Data- Driven Test (page 288) interpreter to execute them. The main disadvantage of this approach is that it restricts the kinds of tests that can be run to those imple- mented by the Data-Driven Test interpreter, which must itself be written anew for each SUT. This strategy does have the advantage of moving the coding of the actual tests out of the developer arena and into the end-user or tester arena, which makes it particularly appropriate for customer tests.\n\nWhat’s Next?\n\nIn this chapter we established the basic terminology for talking about how xUnit tests are put together. Now we turn our attention to a new task—constructing our ﬁ rst test ﬁ xture in Chapter 8, Transient Fixture Management.\n\nwww.it-ebooks.info\n\n83\n\nThis page intentionally left blank\n\nwww.it-ebooks.info",
      "page_number": 138
    },
    {
      "number": 8,
      "title": "Transient Fixture Management",
      "start_page": 148,
      "end_page": 157,
      "detection_method": "regex_chapter",
      "content": "Chapter 8\n\nTransient Fixture Management\n\nAbout This Chapter\n\nChapter 6, Test Automation Strategy, looked at the strategic decisions that we need to make. That included the deﬁ nition of the term “ﬁ xture” and the selection of a test ﬁ xture strategy. Chapter 7, xUnit Basics, established our basic xUnit terminol- ogy and diagramming notation. This chapter builds on both of these earlier chap- ters by focusing on the mechanics of implementing the chosen ﬁ xture strategy.\n\nThere are several different ways to set up a Fresh Fixture (page 311), and our decision will affect how much effort it takes to write the tests, how much effort\n\nTransient\n\nFresh Fixture\n\nPersistent\n\nShared Fixture\n\nImmutable Shared Fixture\n\nFigure 8.1 Transient Fresh Fixture. Fresh Fixtures come in two ﬂ avors: Transient and Persistent. Both require ﬁ xture setup; the latter also requires ﬁ xture teardown.\n\n85\n\nwww.it-ebooks.info\n\n86\n\nChapter 8 Transient Fixture Management\n\nit takes to maintain our tests, and whether we achieve Tests as Documentation (see page 23). Persistent Fresh Fixtures (see Fresh Fixture) are set up the same way as Transient Fresh Fixtures (see Fresh Fixture), albeit with some additional factors to consider related to ﬁ xture teardown (Figure 8.1). Shared Fixtures (page 317) introduce another set of considerations. Persistent Fresh Fixtures and Shared Fixtures are discussed in detail in Chapter 9.\n\nTest Fixture Terminology\n\nBefore we can talk about setting up a ﬁ xture, we need to agree what a ﬁ xture is.\n\nWhat Is a Fixture?\n\nEvery test consists of four parts, as described in Four-Phase Test (page 358). The ﬁ rst part is where we create the SUT and everything it depends on and where we put those elements into the state required to exercise the SUT. In xUnit, we call everything we need in place to exercise the SUT the test ﬁ xture and the part of the test logic that we execute to set it up the ﬁ xture setup.\n\nThe most common way to set up the ﬁ xture is using front door ﬁ xture set- up—that is, to call the appropriate methods on the SUT to put it into the start- ing state. This may require constructing other objects and passing them to the SUT as arguments of method calls. When the state of the SUT is stored in other objects or components, we can do Back Door Setup (see Back Door Manipulation on page 327)—that is, we can insert the necessary records directly into the other component on which the behavior of the SUT depends. We use Back Door Setup most often with databases or when we need to use a Mock Object (page 544) or Test Double (page 522). These possibilities are covered in Chapter 13, Testing with Databases, and Chapter 11, Using Test Doubles, respectively.\n\nIt is worth noting that the term “ﬁ xture” is used to mean different things in different kinds of test automation. The xUnit variants for the Microsoft lan- guages call the Testcase Class (page 373) the test ﬁ xture. Most other variants of xUnit distinguish between the Testcase Class and the test ﬁ xture (or test con- text) it sets up. In Fit [FitB], the term “ﬁ xture” is used to mean the custom-built parts of the Data-Driven Test (page 288) interpreter that we use to deﬁ ne our Higher-Level Language (see page 41). Whenever this book says “test ﬁ xture” without further qualifying this term, it refers to the stuff we set up before ex- ercising the SUT. To refer to the class that hosts the Test Methods (page 348), whether it be in Java or C#, Ruby or VB, this book uses Testcase Class.\n\nwww.it-ebooks.info\n\nTest Fixture Terminology\n\nWhat Is a Fresh Fixture?\n\nIn a Fresh Fixture strategy, we set up a brand-new ﬁ xture for every test we run (Figure 8.2). That is, each Testcase Object (page 382) builds its own ﬁ xture be- fore exercising the SUT and does so every time it is rerun. That is what makes the ﬁ xture “fresh.” As a result, we completely avoid the problems associated with Interacting Tests (see Erratic Test on page 228).\n\nSetup Setup\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nVerify Verify\n\nFixture Fixture\n\nSUT SUT\n\nExercise Exercise\n\nTeardown Teardown\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nFigure 8.2 A pair of Fresh Fixtures, each with its creator. A Fresh Fixture is built speciﬁ cally for a single test, used once, and then retired.\n\nWhat Is a Transient Fresh Fixture?\n\nWhen our ﬁ xture is an in-memory ﬁ xture referenced only by local variables or instance variables,1 the ﬁ xture just “disappears” after every test courtesy of Garbage-Collected Teardown (page 500). When ﬁ xtures are persistent, this is not the case. Thus we have some decisions to make about how we implement the Fresh Fixture strategy. In particular, we have two different ways to keep them “fresh.” The obvious option is tear down the ﬁ xture after each test. The less obvious option is to leave the old ﬁ xture around and then build a new ﬁ xture in such a way that it does not collide with the old ﬁ xture.\n\n1 See the sidebar “There’s Always an Exception” (page 384).\n\nwww.it-ebooks.info\n\n87\n\n88\n\nChapter 8 Transient Fixture Management\n\nMost Fresh Fixtures we build are transient, so we will cover that case ﬁ rst.\n\nWe will then come back to managing Persistent Fresh Fixtures in Chapter 9.\n\nBuilding Fresh Fixtures\n\nWhether we are building a Transient Fresh Fixture or a Persistent Fresh Fixture, the choices we have for how to construct it are pretty much the same. The ﬁ xture setup logic includes the code needed to instantiate the SUT,2 the code to put the SUT into the appropriate starting state, and the code to create and initialize the state of anything the SUT depends on or that will be passed to it as an argument. The most obvious way to set up a Fresh Fixture is through In-line Setup (page 408), in which all ﬁ xture setup logic is contained within the Test Method. This type of ﬁ xture can also be constructed by using Delegated Setup (page 411), which involves calling Test Utility Methods (page 599). Finally, we can use Implicit Setup (page 424), in which the Test Automation Framework (page 298) calls a special setUp method we provide on our Testcase Class. We can also use a combination of these three approaches. Let’s look at each possibility individually.\n\nIn-line Fixture Setup\n\nIn In-line Setup, the test handles all of the fixture setup within the body of the Test Method. We construct objects, call methods on them, construct the SUT, and call methods on it to put into a specific state. We perform all of these tasks from within our Test Method. Think of In-line Setup as the do- it-yourself approach to fixture creation.\n\npublic void testStatus_initial() { // In-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); // Exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // tearDown: // Garbage-collected }\n\n2 This discussion assumes that the SUT is an object and not just static methods on a class.\n\nwww.it-ebooks.info\n\nBuilding Fresh Fixtures\n\nThe main drawback of In-line Setup is that it tends to lead to Test Code Dupli- cation (page 213) because each Test Method needs to construct the SUT. Many of the Test Methods also need to perform similar ﬁ xture setup. This Test Code Duplication leads, in turn, to High Test Maintenance Cost (page 265) caused by Fragile Tests (page 239). If the work to create the ﬁ xture is complex, it can also lead to Obscure Tests (page 186). A related problem is that In-line Setup tends to encourage Hard-Coded Test Data (see Obscure Test) within each Test Method because creating a local variable with an Intent-Revealing Name [SBPP] may seem like too much work for the beneﬁ t yielded.\n\nWe can prevent these test smells by moving the code that sets up the ﬁ xture out of the Test Method. The location where we move it determines which of the alternative ﬁ xture setup strategies we have used.\n\nDelegated Fixture Setup\n\nA quick and easy way to reduce Test Code Duplication and the resulting Obscure Tests is to refactor our Test Methods to use Delegated Setup. We can use an Extract Method [Fowler] refactoring to move a sequence of statements used in several Test Methods into a Test Utility Method that we then call from those Test Methods. This is a very simple and safe refactoring, especially when we let the IDE do all the heavy lifting for us. When the extracted method contains logic to create an object on which our test depends, we call it a Creation Method (page 415). Creation Methods3 with Intent-Revealing Names make the test’s pre-conditions readily apparent to the reader while avoiding unnecessary Test Code Duplication. They allow both the test reader and the test automater to focus on what is being created without being distracted by how it is created. The Creation Methods act as reusable building blocks for test ﬁ xture construction.\n\npublic void testGetStatus_inital() { // Setup Flight ﬂight = createAnonymousFlight(); // Exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // Teardown // Garbage-collected }\n\nOne goal of these Creation Methods is to eliminate the need for every test to know the details of how the objects it requires are created. This stream- lining goes a long way toward preventing Fragile Tests caused by changes to\n\n3 When referenced via a Test Helper (page 643) class, they are often called the Object Mother pattern (see Test Helper on page 643).\n\nwww.it-ebooks.info\n\n89\n\n90\n\nChapter 8 Transient Fixture Management\n\nconstructor method signatures or semantics. When a test does not care about the speciﬁ c identity of the objects it is creating, we can use Anonymous Cre- ation Methods (see Creation Method). These methods generate any unique keys required by the object being created. By using a Distinct Generated Value (see Generated Value on page 723), we can guarantee that no other test instance that requires a similar object will accidentally use the same object as this test. This safeguard prevents many forms of the behavior smell Erratic Test, includ- ing Unrepeatable Tests, Interacting Tests, and Test Run Wars, even if we hap- pen to be using a persistent object repository that supports Shared Fixtures.\n\nWhen a test does care about the attributes of the object being created, we use a Parameterized Anonymous Creation Method (see Creation Method). This method is passed any attributes that the test cares about (i.e., attributes that are important to the test outcome), leaving all other attributes to be defaulted by the implementation of the Creation Method. My motto is this:\n\nWhen it is not important for something to be seen in the test method, it is important that it not be seen in the test method!\n\nDelegated Setup is often used when we write input validation tests for SUT methods that are expected to validate the attributes of an object argu- ment. In such a case, we need to write a separate test for each invalid at- tribute that should be detected. Building all of these slightly invalid objects would be a lot of work using In-line Setup. We can reduce the effort and the amount of Test Code Duplication dramatically by using the pattern One Bad Attribute (see Derived Value on page 718). That is, we first call a Creation Method to create a valid object, and then we replace one attri- bute with an invalid value that should be rejected by the SUT. Similarly, we might create an object in the correct state by using a Named State Reaching Method (see Creation Method).\n\nSome people prefer to Reuse Tests for Fixture Setup (see Creation Method) as an alternative to using Chained Tests (page 454). That is, they call other tests directly within the setup portion of their test. This approach is not an unreasonable one as long as the test reader can readily identify what the other test is setting up for the current test. Unfortunately, very few tests are named in such a way as to convey this intention. For this reason, if we value Tests as Documentation, we will want to con- sider wrapping the called test with a Creation Method that has an Intent-Revealing Name so that test reader can get a sense of what the ﬁ xture looks like.\n\nThe Creation Methods can be kept as private methods on the Testcase Class, pulled up to a Testcase Superclass (page 638), or moved to a Test Help- er (page 643). The “mother of all creation methods” is Object Mother (see Test\n\nwww.it-ebooks.info\n\nBuilding Fresh Fixtures\n\nHelper). This strategy-level pattern describes a family of approaches that center on the use of Creation Methods on one or more Test Helpers and may include Automated Teardown (page 503) as well.\n\nImplicit Fixture Setup\n\nMost members of the xUnit family provide a convenient hook for calling code that needs to be run before every Test Method. Some members call a method with a speciﬁ c name (e.g., setUp). Others call a method that has a speciﬁ c annota- tion (e.g., “@before” in JUnit) or method attribute (e.g., “[Setup]” in NUnit). To avoid repeating these alternative ways every time we need to refer to this mecha- nism, this book simply calls it the setUp method regardless of how we indicate this fact to the Test Automation Framework. The setUp method is optional or an empty default implementation is provided by the framework, so we do not have to provide one in each Testcase Class.\n\nIn Implicit Setup, we take advantage of this framework “hook” by putting all of the ﬁ xture creation logic into the setUp method. Because every Test Method on the Testcase Class shares this ﬁ xture setup logic, all Test Methods need to be able to use the ﬁ xture it creates. This tactic certainly addresses the Test Code Duplica- tion problem but it does have several consequences. What does the following test actually verify?\n\nAirport departureAirport; Airport destinationAirport; Flight ﬂight;\n\npublic void testGetStatus_inital() { // Implicit setup // Exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); }\n\nThe ﬁ rst consequence is that this approach can make the tests more difﬁ cult to understand because we cannot see how the pre-conditions of the test (the test ﬁ xture) correlate with the expected outcome within the Test Method; we have to look in the setUp method to see this relationship.\n\npublic void setUp() throws Exception{ super.setUp(); departureAirport = new Airport(\"Calgary\", \"YYC\"); destinationAirport = new Airport(\"Toronto\", \"YYZ\"); BigDecimal ﬂightNumber = new BigDecimal(\"999\"); ﬂight = new Flight( ﬂightNumber , departureAirport, destinationAirport); }\n\nwww.it-ebooks.info\n\n91\n\n92\n\nChapter 8 Transient Fixture Management\n\nWe can mitigate this problem by naming our Testcase Class based on the test ﬁ xture created in the setUp method. Of course, this makes sense only if all of the Test Methods really need the same ﬁ xture—it is an example of Testcase Class per Fixture (page 631). As mentioned earlier, several members of the xUnit family (VbUnit and NUnit, to name two) use the term “test ﬁ xture” to describe what this book calls the Testcase Class. This nomenclature is probably based on the assumption that we are using a Testcase Class per Fixture strategy.\n\nAnother consequence of using Implicit Setup is that we cannot use local vari- ables to hold references to the objects in our ﬁ xture. Instead, we are forced to use instance variables to refer to any objects that are constructed in the setUp method and that are needed either when exercising the SUT, when verifying the expected outcome, or when tearing down the ﬁ xture. These instance vari- ables act as global variables between the parts of the test. As long as we stick to instance variables rather than class variables, however, the test ﬁ xture will be newly constructed for each test case in the Testcase Class. Most members of xUnit provide isolation between the ﬁ xture created for each Test Method but at least one (NUnit) does not; see the sidebar “There’s Always an Excep- tion” (page 384) for more information. In any event, we should deﬁ nitely give the variables Intent-Revealing Names so that we do not need to keep referring back to the setUp method to understand what they hold.\n\nMisuse of the SetUp Method\n\nWhen you have a new hammer, everything looks like a nail!\n\nLike any feature of any system, the setUp method can be abused. We should not feel obligated to use it just because it is provided. It is one of several code reuse mechanisms that are available for our application. When object-oriented languages were ﬁ rst introduced, programmers were enamored with inheritance and tried to apply it in all possible reuse scenarios. Over time, we learned when inheritance was appropriate and when we should resort to other mechanisms such as delegation. The setUp method is xUnit’s inheritance.\n\nThe setUp method is most prone to misuse when it is applied to build a Gen- eral Fixture (see Obscure Test) with multiple distinct parts, each of which is dedicated to a different Test Method. This can lead to Slow Tests (page 253) if we are building a Persistent Fresh Fixture. More importantly, it can lead to Obscure Tests by hiding the cause–effect relationship between the ﬁ xture and the expected outcome of exercising the SUT.\n\nIf we do not adopt the practice of grouping the Test Methods into Testcase Classes based on identical ﬁ xtures but we do use the setUp method, we should build only the lowest common denominator part of the ﬁ xture in the setUp\n\nwww.it-ebooks.info\n\nTearing Down Transient Fresh Fixtures\n\nmethod. That is, only the setup logic that will not cause problems in any of the tests should be placed in the setUp method. Even the ﬁ xture setup code that does not cause problems for any of the Test Methods can still cause other problems if we use the setUp method to build a General Fixture instead of a Minimal Fixture (page 302). A General Fixture is a common cause of Slow Tests because each test spends much more time than necessary building the test ﬁ xture. It also tends to produce Obscure Tests because the test reader cannot easily see which part of the ﬁ xture a particular Test Method depends on. A General Fixture often evolves into a Fragile Fixture (see Fragile Test) as the relationship between its various elements and the tests that use them is forgotten over time. Changes made to the ﬁ xture to support a newly added test may then cause existing tests to fail.\n\nNote that if we use a class variable to hold the object, we may have crossed the line into the world of Persistent Fresh Fixtures. Use of Lazy Setup (page 435) to populate the variable, by contrast, carries us into the world of Shared Fix- tures because later tests within the test suite may reuse the object(s) created in earlier tests and thus may become dependent on the changes the other test (should have) made to it.\n\nHybrid Fixture Setup\n\nThis chapter has presented the three styles of ﬁ xture construction as strict alter- natives to one another. In practice, there is value in combining them. Test auto- maters often call some Creation Methods from within the Test Method but then do some additional setup on an in-line basis. The readability of the setUp method can also be improved if it calls Creation Methods to construct the ﬁ xture. An additional beneﬁ t is that the Creation Methods can be unit-tested much more easily than either in-line ﬁ xture construction logic or the setUp method. These methods can also be located on a class outside the Testcase Class hierarchy such as a Test Helper.\n\nTearing Down Transient Fresh Fixtures\n\nOne really nice thing about Transient Fresh Fixtures is that ﬁ xture teardown requires very little effort. Most members of the xUnit family are implemented in languages that support garbage collection. As long as our references to the ﬁ xture are held in variables that go out of scope, we can count on Garbage-Collected Teardown to do all the work for us. See the sidebar “There’s Always an Exception” on page 384 for a description of why the same is not true in NUnit.\n\nwww.it-ebooks.info\n\n93\n\n94\n\nChapter 8 Transient Fixture Management\n\nIf we are using one of the few members of the xUnit family that does not sup-\n\nport garbage collection, we may have to treat all Fresh Fixtures as persistent.\n\nWhat’s Next?\n\nThis chapter introduced techniques for setting up and tearing down an in-memory Fresh Fixture. With some planning and a bit of luck, they are all you should need for the majority of your tests. Managing Fresh Fixtures is more complicated when the ﬁ xture is persisted either by the SUT or by the test itself. Chapter 9, Persistent Fixture Management, introduces additional techniques needed for managing persistent ﬁ x- tures, including Persistent Fresh Fixtures and Shared Fixtures.\n\nwww.it-ebooks.info",
      "page_number": 148
    },
    {
      "number": 9,
      "title": "Persistent Fixture Management",
      "start_page": 158,
      "end_page": 169,
      "detection_method": "regex_chapter",
      "content": "Chapter 9\n\nPersistent Fixture Management\n\nAbout This Chapter\n\nIn Chapter 8, Transient Fixture Management, we saw how we can go about building in-memory Fresh Fixtures (page 311). We noted in that chapter that managing Fresh Fixtures is more complicated when the ﬁ xture is persisted either by the SUT or by the test itself. This chapter introduces the additional patterns required to manage persistent ﬁ xtures, including both Persistent Fresh Fixtures (see Fresh Fixture) and Shared Fixtures (page 317).\n\nManaging Persistent Fresh Fixtures\n\nThe term Persistent Fresh Fixture might sound like an oxymoron but it is actually not as large a contradiction as it might ﬁ rst seem. The Fresh Fixture strat- egy means that each run of each Test Method (page 348) uses a newly created ﬁ x- ture. The name speaks to its intent: We do not reuse the ﬁ xture! It does not need to imply that the ﬁ xture is transient—only that it is not reused (Figure 9.1). Per- sistent Fresh Fixtures present several challenges not encountered with Transient Fresh Fixtures. In this chapter, we focus on the challenge posed by Unrepeatable Tests (see Erratic Test on page 228) caused by leftover Persistent Fresh Fixtures and Slow Tests (page 253) caused by Shared Fixtures (page 317).\n\nWhat Makes Fixtures Persistent?\n\nA ﬁ xture, fresh or otherwise, can become persistent for one of two reasons. The ﬁ rst reason is that the SUT is a stateful object and “remembers” how it was used in the past. This scenario most often occurs when the SUT includes a database,\n\n95\n\nwww.it-ebooks.info\n\n96\n\nChapter 9 Persistent Fixture Management\n\nTransient Transient\n\nFresh Fresh Fixture Fixture\n\nPersistent Persistent\n\nShared Shared Fixture Fixture\n\nImmutable Immutable Shared Shared Fixture Fixture\n\nFigure 9.1 A Fresh Fixture can be either transient or persistent. We can apply a Fresh Fixture strategy even if the test ﬁ xture is naturally persistent but we must have a way to tear it down after each test.\n\nbut it can occur simply because the SUT uses class variables to hold some of its data. The second reason is that the Testcase Class (page 373) holds a reference to an otherwise Transient Fresh Fixture in a way that makes it survive across Test Method invocations.\n\nSome members of the xUnit family provide a mechanism to reload all classes at the beginning of each test run. This behavior may appear as an option—a check box labeled “Reload Classes”—or it may be automatic. Such a feature keeps the ﬁ xture from becoming persistent when it is referenced from a class variable; it does not prevent the Fresh Fixture from becoming persistent if either the SUT or the test puts the ﬁ xture into the ﬁ le system or a database.\n\nIssues Caused by Persistent Fresh Fixtures\n\nWhen ﬁ xtures are persistent, we may ﬁ nd that subsequent runs of the same Test Method try to recreate a ﬁ xture that already exists. This behavior may cause conﬂ icts between the preexisting and newly created resources. Although violat- ing unique key constraints in the database is the most common example of this problem, the conﬂ ict could be as simple as trying to create a ﬁ le with the same name as one that already exists. One way to avoid these Unrepeatable Tests is to tear down the ﬁ xture at the end of each test; another is to use Distinct Gen- erated Values (see Generated Value on page 723) for any identiﬁ ers that might cause conﬂ icts.\n\nwww.it-ebooks.info\n\nManaging Persistent Fresh Fixtures\n\nTearing Down Persistent Fresh Fixtures\n\nUnlike ﬁ xture setup code, which should help us understand the pre-conditions of the test, ﬁ xture teardown code is purely a matter of good housekeeping. It does not help us understand the behavior of the SUT but it has the potential to obscure the intent of the test or at least make it more difﬁ cult to understand. Therefore, the best kind of teardown code is the nonexistent kind. We should avoid writing teardown code whenever we can, which is why Garbage-Collected Teardown (page 500) is so preferable. Unfortunately, we cannot take advantage of Garbage-Collected Teardown if our Fresh Fixture is persistent.\n\nHand-Coded Teardown\n\nOne way to ensure that the ﬁ xture is destroyed after we are done with it is to include test-speciﬁ c teardown code within our Test Methods. This teardown mechanism might seem simple, but it is actually more complex than immediately meets the eye. Consider the following example:\n\npublic void testGetFlightsByOriginAirport_NoFlights()\n\nthrows Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); facade.removeAirport(outboundAirport); }\n\nThis Naive In-line Teardown (see In-line Teardown on page 509) will tear down the ﬁ xture when the test passes—but it won’t tear down the ﬁ xture if the test fails or ends with an error. This is because the calls to the Assertion Meth- ods (page 362) throw an exception; therefore, we may never make it to the teardown code. To ensure that the In-line Teardown code always executes, we must surround everything in the Test Method that might raise an exception with a try/catch control structure. Here’s the same test suitably modiﬁ ed:\n\npublic void testGetFlightsByOriginAirport_NoFlights_td() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); try { // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport);\n\nwww.it-ebooks.info\n\n97\n\n98\n\nChapter 9 Persistent Fixture Management\n\n// Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { facade.removeAirport(outboundAirport); } }\n\nUnfortunately, the mechanism to ensure that the teardown code always runs in- troduces a fair bit of complication into the Test Method. Matters become even more complicated when we must tear down several resources: Even if our attempt to clean up one resource fails, we want to ensure that the other resources are still cleaned up. We can address part of this problem by using Extract Method [Fowler] refactoring to move the teardown code into a Test Utility Method (page 599) that we call from inside the error-handling construct. Although this Delegated Teardown (see In-line Teardown) hides the complexity of dealing with teardown errors, we still need to ensure that the method gets called even when test errors or test failures occur.\n\nMost members of the xUnit family solve this problem by supporting Implicit Teardown (page 516). The Test Automation Framework (page 298) calls a spe- cial tearDown method after each Test Method regardless of whether the test passed or failed. This approach avoids placing the error-handling code within the Test Method but imposes two requirements on our tests. First, the ﬁ xture must be accessible from the tearDown method, so we must use instance variables (pre- ferred), class variables, or global variables to hold the ﬁ xture. Second, we must ensure that the tearDown method works properly with each of the Test Methods regardless of which ﬁ xture it sets up.1\n\nMatching Setup with Teardown Code Organization\n\nGiven the three ways of organizing our setup code—In-line Setup (page 408), Delegated Setup (page 411), and Implicit Setup (page 424)—and the three ways of organizing our teardown code—In-line Teardown, Delegated Teardown, and Implicit Teardown—nine different combinations are available to us. Choosing the right one turns out to be an easy decision because it is not important for the teardown code to be visible to the test reader. We simply choose the most appropriate setup code organization and either the equivalent or more hidden version of teardown (Table 9.1). For example, it is appropriate to use Implicit Teardown even with In-line Setup or Delegated Setup; it is almost never a good\n\n1 This is less of an issue with Testcase Class per Fixture (page 631) because the ﬁ xture should always be the same. With other Testcase Class organizations, we may need to include Teardown Guard Clauses (see In-line Teardown) within the tearDown method to ensure that it doesn’t produce errors when it runs.\n\nwww.it-ebooks.info\n\nManaging Persistent Fresh Fixtures\n\nidea to use In-line Teardown with anything other than In-line Setup, and even then it should probably be avoided!\n\nTable 9.1 The Compatibility of Various Fixture Setup and Teardown Strate- gies for Persistent Test Fixtures\n\nTeardown Mechanism\n\nSetup Mechanism\n\nIn-line Teardown Delegated Teardown\n\nImplicit Teardown\n\nIn-line Setup\n\nNot recommended Acceptable\n\nRecommended\n\nDelegated Setup\n\nNot recommended Acceptable\n\nRecommended\n\nImplicit Setup\n\nNot recommended Not recommended\n\nRecommended\n\nAutomated Teardown\n\nHand-coded teardown is associated with two problems: Extra work is required to write the tests, and the teardown code is hard to get right and even harder to test. When the teardown goes wrong, it may lead to Erratic Tests caused by Resource Leakage because the test that fails as a result is often different from the one that didn’t clean up properly.\n\nIn languages that support garbage collection, tearing down a Transient Fresh Fixture should be pretty much automatic. As long as our ﬁ xtures are referenced only by instance variables that go out of scope when our Testcase Object (page 382) is destroyed, garbage collection will clean them up. Garbage collection won’t work, however, if we use class variables or if our ﬁ xtures include persistent objects such as ﬁ les or database rows. In those cases, we need to perform our own cleanup.\n\nNot surprisingly, this situation may inspire the lazy but creative programmer to come up with a way to automate the teardown logic. The important thing to note is that teardown code doesn’t help us understand the test so it is better for it to remain hidden.2 We can eliminate the need to write hand-crafted teardown code for each Test Method or Testcase Class by building an Automated Tear- down (page 503) mechanism. It consists of three parts:\n\n1. A well-tested mechanism to iterate over a list of objects that need to be deleted and catch/report any errors it encounters while ensuring that all the deletions are attempted.\n\n2. A dispatching mechanism that invokes the deletion code appropriate to the kind of object to be deleted. This mechanism is often imple- mented as a Command [GOF] object that wraps each object to be\n\n2 Unlike setup code, which is often very important for understanding the test.\n\nwww.it-ebooks.info\n\n99\n\n100\n\nChapter 9 Persistent Fixture Management\n\ndeleted, but could be as simple as calling a delete method on the object itself or using a switch statement based on the object’s class.\n\n3. A registration mechanism to add newly created objects (suitably\n\nwrapped if necessary) to the list of objects to be deleted.\n\nOnce we have built our Automated Teardown mechanism, we can simply in- voke the registration method from our Creation Methods (page 415) and the cleanup method from the tearDown method. The latter operation can be speciﬁ ed in a Testcase Superclass (page 638) that all of our Testcase Classes inherit from. We can even extend this mechanism to delete objects created by the SUT as it is exercised. To do so, we use an observable Object Factory (see Dependency Lookup on page 686) inside the SUT and have our Testcase Superclass register itself as an Observer [GOF] of object creation.\n\nDatabase Teardown\n\nWhen our persistent Fresh Fixture has been built entirely in a relational database, we can take advantage of certain features of the database to implement its tear- down. Table Truncation Teardown (page 661) is a brute-force way to blow away the entire contents of a table with a single database command. Of course, it is appropriate only when each Test Runner (page 377) has its own Database Sand- box (page 650). A somewhat less drastic approach is to use Transaction Rollback Teardown (page 668) to undo all changes made within the context of the current test. This mechanism relies on the SUT having been designed using the Humble Transaction Controller pattern (see Humble Object on page 695) so that we can invoke the business logic from the test without having the SUT commit the trans- action automatically. Both of these database-speciﬁ c teardown patterns are most commonly implemented using Implicit Teardown to keep the teardown logic out of the Test Methods.\n\nAvoiding the Need for Teardown\n\nSo far, we have looked at ways to do ﬁ xture teardown. Now, let us look at ways to avoid ﬁ xture teardown.\n\nAvoiding Fixture Collisions\n\nWe need to do ﬁ xture teardown for three reasons:\n\n1. The accumulation of leftover ﬁ xture objects can cause tests to run\n\nslowly.\n\nwww.it-ebooks.info\n\nManaging Persistent Fresh Fixtures\n\n2. The leftover ﬁ xture objects can cause the SUT to behave differently or\n\nour assertions to report incorrect results.\n\n3. The leftover ﬁ xture objects can prevent us from creating the Fresh Fix-\n\nture required by our test.\n\nThe issue that is easiest to address is the ﬁ rst one: We can schedule a periodic cleansing of the persistence mechanism back to a known, minimalist state. Un- fortunately, this tactic is useful only if we can get the tests to run correctly in the presence of accumulated test detritus.\n\nThe second issue can be addressed by using Delta Assertions (page 485) rather than “absolute” assertions. Delta Assertions work by taking a snapshot of the ﬁ xture before the test is run and verifying that the expected differences have appeared after we exercise the SUT.\n\nThe third issue can be addressed by ensuring that each test generates a differ- ent set of ﬁ xture objects each time it is run. Thus any objects that the test needs to create must be given totally unique identiﬁ ers—that is, unique ﬁ lenames, unique keys, and so on. To do so, we can build a simple unique ID generator and create a new ID at the beginning of each test. We can then use that unique ID as part of the identity of each newly created ﬁ xture object. If the ﬁ xture is shared beyond a single Test Runner, we may need to include something about the user in the unique identiﬁ ers we create; the currently logged-in user ID is usually sufﬁ cient. Using Distinct Generated Values as keys offers another ben- eﬁ t: It allows us to implement a Database Partitioning Scheme (see Database Sandbox) in which we can use absolute assertions despite the presence of left- over ﬁ xture objects.\n\nAvoiding Fixture Persistence\n\nWe seem to be going to a lot of trouble to undo the side effects caused by a persistent Fresh Fixture. Wouldn’t it be nice if we could avoid all of this work? The good news is that we can. The bad news is that we need to make our Fresh Fixture nonpersistent to do so. When the SUT is to blame for the persistence of the ﬁ xture, one possibility is to replace the persistence mechanism with a Test Double (page 522) that the test can wipe out at will. A good example of this ap- proach is the use of a Fake Database (see Fake Object on page 551). When the test is to blame for ﬁ xture persistence, the solution is even easier: Just use a less persistent ﬁ xture reference mechanism.\n\nwww.it-ebooks.info\n\n101\n\n102\n\nChapter 9 Persistent Fixture Management\n\nDealing with Slow Tests\n\nA major drawback of using a Persistent Fresh Fixture is speed or, more precisely, the lack thereof. File systems and databases are much slower than the processors used in modern computers. As a consequence, tests that interact with databases tend to run much more slowly than tests that run entirely in memory. Part of this difference arises because the SUT is accessing the ﬁ xture from disk—but this issue turns out to be only a small part of the reason for the slowdown. Setting up the Fresh Fixture at the beginning of each test and tearing it down at the end of each test typically takes many more disk accesses than those used by the SUT to access the ﬁ xture. As a result, tests that access the database often take 50 to 100 times3 longer to run than tests that run entirely in memory, all other things being equal.\n\nThe typical reaction to slow tests caused by Persistent Fresh Fixtures is to eliminate the ﬁ xture setup and teardown overhead by reusing the ﬁ xture across many tests. Assuming we have ﬁ ve disk accesses to set up and tear down the ﬁ xture for every disk access performed by the SUT, the absolute best4 we can do by switching to a Shared Fixture is somewhere around ten times as slow. Of course, this outcome is still too slow in most situations and it comes with a hefty price: The tests are no longer independent. That means we will likely have Interacting Tests (see Erratic Test), Lonely Tests (see Erratic Test), and Unre- peatable Tests on top of our Slow Tests!\n\nA much better solution is to eliminate the need to have a disk-based database under the application. With a small amount of effort, we should be able to re- place the disk-based database with an In-Memory Database (see Fake Object) or a Fake Database. This decision is best made early in the project while the effort is still low. Yes, there are some challenges, such as dealing with stored procedures, but they are all surmountable.\n\nThis tactic isn’t the only way to deal with Slow Tests, of course. The side- bar “Faster Tests Without Shared Fixtures” (page 319) explores some other strategies.\n\n3 This is two orders of magnitude! 4 Your mileage may vary.\n\nwww.it-ebooks.info\n\nManaging Shared Fixtures\n\nManaging Shared Fixtures\n\nManaging Shared Fixtures has a lot in common with managing Persistent Fresh Fixtures, except that we deliberately choose not to tear the ﬁ xture down after every test so that we can reuse it in subsequent tests (Figure 9.2). This implies two things. First, we must be able to access the ﬁ xture in the other tests. Second, we must have a way of triggering both the construction and the teardown of the ﬁ xture.\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nExercise Exercise\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nVerify Verify\n\nTeardown Teardown\n\nFigure 9.2 A Shared Fixture with two Test Methods that share it. A Shared Fixture is set up once and used by two or more tests that may interact, either deliberately or accidentally, as a result. Note the lack of a ﬁ xture setup phase for the second test.\n\nAccessing Shared Fixtures\n\nRegardless of how and when we choose to build the Shared Fixture, the tests need a way to ﬁ nd the test ﬁ xture they are to reuse. The choices available to us depend on the nature of the ﬁ xture. When the ﬁ xture is stored in a database (the most common usage of a Shared Fixture), tests may access it directly with- out making direct references to the ﬁ xture objects as long as they know about the database. There may be a temptation to use Hard-Coded Values (see Literal Value on page 714) in database lookups to access the ﬁ xture objects. This is al- most always a bad idea, however, because it leads to a close coupling between tests and the ﬁ xture implementation and because it has poor documentation value (Obscure Test; page 186). To avoid these potential problems, we can use Finder Methods (see Test Utility Method) with Intent-Revealing Names [SBPP] to access\n\nwww.it-ebooks.info\n\n103\n\n104\n\nChapter 9 Persistent Fixture Management\n\nthe ﬁ xture. These Finder Methods may have names that are very similar to those of Creation Methods, but they return references to existing ﬁ xture objects rather than building brand new ones.\n\nWe have a range of possible solutions when the ﬁ xture is stored in memory. If all tests that need to share the ﬁ xture are in the same Testcase Class, we can use a ﬁ xture holding class variable to hold the reference to the ﬁ xture. As long as we give the variable an Intent-Revealing Name, the test reader should be able to understand the pre-conditions of the test. Another alternative is to use a Finder Method.\n\nIf we need to share the ﬁ xture across many Testcase Classes, we must use a more sophisticated technique. We could, of course, let one class declare the ﬁ xture holding class variable and have the other tests access the ﬁ xture via that variable. Unfortunately, this approach may create unnecessary coupling between the tests. Another alternative is to move the declaration to a well-known object—namely, a Test Fixture Registry (see Test Helper on page 643). This Registry [PEAA] object could be something like a test database or it could merely be a class. It can expose various parts of a ﬁ xture via discrete ﬁ xture holding class variables or via Finder Methods. When the Test Fixture Registry has only Finder Methods that know how to access the objects but don’t hold references to them, we call it a Test Helper.\n\nTriggering Shared Fixture Construction\n\nFor a test ﬁ xture to be shared, it must be built before any Test Method needs it. This construction could take place as late as right before the Test Method’s logic is run, just before the entire test suite is run, or at some earlier time (Figure 9.3). This leads us to the basic patterns of Shared Fixture creation.\n\nShared Shared Fixture Fixture\n\nPrebuilt Prebuilt Fixture Fixture\n\nLazy Lazy Setup Setup\n\nSetup Setup Decorator Decorator\n\nSuite Suite Fixture Fixture Setup Setup\n\nChained Chained Tests Tests\n\nShared Fixture Setup Shared Fixture Setup\n\nFigure 9.3 The plethora of ways to manage a Shared Fixture. A Shared Fixture can be set up at a variety of times; the decision is based on how many tests need to reuse the ﬁ xture and how many times they need to do so.\n\nwww.it-ebooks.info\n\nManaging Shared Fixtures\n\nIf we are happy with the idea of creating the test ﬁ xture the ﬁ rst time any test needs it, we can use Lazy Setup (page 435) in the setUp method of the corre- sponding Testcase Class to create it as part of running the ﬁ rst test. Subsequent tests will then see that the ﬁ xture already exists and reuse it. Because there is no obvious signal that the last test in a test suite (or Suite of Suites; see Test Suite Object on page 387) has been run, we won’t know when to tear down the ﬁ x- ture after each test run. This can lead to Unrepeatable Tests because the ﬁ xture may survive across test runs (depending on how the various tests access it).\n\nIf we need to share the ﬁ xture more broadly, we could include a Fixture Set- up Testcase at the beginning of the test suite. This is a special case of Chained Tests and suffers from the same problem as Lazy Setup—speciﬁ cally, we don’t know when it is time to tear down the ﬁ xture. It also depends on the ordering of tests within a suite, so it works best with Test Enumeration (page 399).\n\nIf we need to be able to tear down the test ﬁ xture after running a test suite, we must use a ﬁ xture management mechanism that tells us when the last test has been run. Several members of the xUnit family support the concept of a setUp method that runs just once for the test suite created from a single Testcase Class. This Suite Fixture Setup (page 441) method has a corresponding tearDown method that is called when the last Test Method has ﬁ nished running.5 We can then guarantee that a new ﬁ xture is built for each test run. The ﬁ xture is not left over to cause problems with subsequent test runs, which prevents Unrepeatable Tests; it does not prevent Interacting Tests within the test run, however. This capability could be added as an extension to any member of the xUnit family. When it isn’t supported or when we need to share the ﬁ xture beyond a single Testcase Class, we can resort to using a Setup Decorator (page 447) to bracket the running of a test suite with the execution of the ﬁ xture setUp and tearDown logic. The biggest drawback of Setup Decorator is that tests that depend on the decorator cannot be run by themselves; they are Lonely Tests.\n\nThe ﬁ nal option is to build the ﬁ xture well before the tests are run—that is, to employ a Prebuilt Fixture (page 429). This approach offers the most options regarding how the test ﬁ xture is actually constructed because the ﬁ xture setup need not be executable from within xUnit. For example, it could be set up manu- ally, by using database scripts, by copying a “golden” database, or by running a data generation program. The major disadvantage with a Prebuilt Fixture is that if any tests are Unrepeatable Tests, we will need to perform a Manual Intervention (page 250) before each test run. As a result, a Prebuilt Fixture is of- ten used in combination with a Fresh Fixture to construct an Immutable Shared Fixture (see Shared Fixture).\n\n5 Think of it as a built-in decorator for a single Testcase Class.\n\nwww.it-ebooks.info\n\n105\n\n106\n\nChapter 9 Persistent Fixture Management\n\nWhat’s Next?\n\nNow that we’ve determined how we will set up and tear down our ﬁ xtures, we are ready to turn our attention to exercising the SUT and verifying that the expected outcome has occurred using calls to Assertion Methods. This process is described in more detail in Chapter 10, Result Veriﬁ cation.\n\nwww.it-ebooks.info",
      "page_number": 158
    },
    {
      "number": 10,
      "title": "Result Veriﬁ cation",
      "start_page": 170,
      "end_page": 187,
      "detection_method": "regex_chapter",
      "content": "Chapter 10\n\nResult Veriﬁ cation\n\nAbout This Chapter\n\nChapter 8, Transient Fixture Management, and Chapter 9, Persistent Fixture Management, described how to set up the test ﬁ xture and how to tear it down after exercising the SUT. This chapter introduces a variety of options for verify- ing that the SUT has behaved correctly, including exercising the SUT and com- paring the actual outcome with the expected outcome.\n\nMaking Tests Self-Checking\n\nOne of the key characteristics of tests automated using xUnit is that they can be (and should be) Self-Checking Tests (see Goals of Test Automation on page 21). This characteristic makes them cost-effective enough to be run very frequently. Most members of the xUnit family come with a collection of built-in Assertion Methods (page 362) and some documentation that tells us which one to use when. On the surface this sounds pretty simple—but there’s a lot more to writ- ing good tests than just calling the built-in Assertion Methods. We also need to learn key techniques for making tests easy to understand and for avoiding and removing Test Code Duplication (page 213).\n\nA key challenge in coding the assertions is getting access to the information we want to compare with the expected results. This is where observation points come into play; they provide a window into the state or behavior of the SUT so that we can pass it to the Assertion Methods. Observation points for infor- mation accessible via synchronous method calls are relatively straightforward; observation points for other kinds of information can be quite challenging, which is precisely what makes automated unit testing so interesting.\n\nAssertions are usually—but not always—called from within the Test Method (page 348) body right after the SUT has been exercised. Some test automaters put\n\n107\n\nwww.it-ebooks.info\n\n108\n\nChapter 10 Result Verification\n\nassertions after the ﬁ xture setup phase of the test to ensure that the ﬁ xture is set up correctly. This practice almost always contributes to Obscure Tests (page 186), so I would rather write unit tests for the Test Utility Methods (page 599).1 Some styles of testing do require us to set up our expectations before we exercise the SUT; this topic is discussed in more detail in Chapter 11, Using Test Doubles. We’ll see several examples of calling Assertion Methods from within Test Utility Methods in this chapter.\n\nOne possible—though rarely used—place to put calls to Assertion Methods is in the tearDown method used in Implicit Teardown (page 516). Because this method is run for every test, whether that test passed or failed (as long as the setUp method succeeded), one can put assertions here. This scheme involves the same trade-off as usingImplicit Setup (page 424) for building our test ﬁ xture; it’s less visible but done automatically. See the sidebar “Using Delta Assertions to Detect Data Leakage” (page 487) for an example of putting assertions in the tearDown method used by Implicit Teardown of a superclass to detect when tests leave leftover test objects in the database.\n\nVerify State or Behavior?\n\nUltimately, test automation is about verifying the behavior of the SUT. Some aspects of the SUT’s behavior can be veriﬁ ed directly; the value returned by a function is a good example. Other aspects of the behavior are more easily veri- ﬁ ed indirectly by looking at the state of some object. We can verify the actual behavior of the SUT in our tests in two ways:\n\n1. We can verify the states of various objects affected by the SUT by extracting each state using an observation point and using assertions to compare it to the expected state.\n\n2. We can verify the behavior of the SUT directly by using observation points inserted between the SUT and its depended-on component (DOC) to monitor its interactions (in the form of the method calls it makes) and comparing those method calls with what we expected.\n\nState Veriﬁ cation (page 462) is done using assertions and is the simpler of the two approaches. Behavior Veriﬁ cation (page 468) is more complicated and builds on the assertion techniques we use for verifying state.\n\n1 The one exception is when we must use a Shared Fixture (page 317); it may be worth- while to use a Guard Assertion (page 490) to document what the test requires from it and to produce a test failure if the ﬁ xture is corrupted. We could also do so from within the Finder Methods (see Test Utility Method) that we use to retrieve the objects in the Shared Fixture (page 317) we will use in our tests.\n\nwww.it-ebooks.info\n\nState Verification\n\nState Veriﬁ cation\n\nThe “normal” way to verify the expected outcome has occurred is called State Veriﬁ cation (Figure 10.1). First we exercise the SUT; then we examine the post- exercise state of the SUT using assertions. We may also examine anything returned by the SUT as a result of the method call we made to exercise it. What is most notable is what we do not do: We do not instrument the SUT in any way to detect how it interacts with other components of the system. That is, we inspect only direct outputs and we use only direct method calls as our observation points.\n\nSetup Setup\n\nFixture Fixture Behavior Behavior (Indirect (Indirect Outputs) Outputs)\n\nDOC DOC\n\nExercise Exercise\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nGet State Get State\n\nB B\n\nA A\n\nC C\n\nTeardown Teardown\n\nFigure 10.1 State Veriﬁ cation. In State Veriﬁ cation, we assert that the SUT and any objects it returns are in the expected state after we have exercised the SUT. We “pay no attention to the man behind the curtain.”\n\nState Veriﬁ cation can be done in two slightly different ways. Procedural State Veriﬁ cation (see State Veriﬁ cation) involves writing a sequence of assertions that pick apart the end state of the SUT and verify that it is as expected. Expected Object (see State Veriﬁ cation) is a way of describing the expected state in such a way that it can be compared with a single Assertion Method call; this approach minimizes Test Code Duplication and increases test clarity (more on this later in this chapter). With both strategies, we can use either “built-in” assertions or Custom Assertions (page 474).\n\nwww.it-ebooks.info\n\n109\n\n110\n\nChapter 10 Result Verification\n\nUsing Built-in Assertions\n\nWe use the assertions provided by our testing framework to specify what should be and depend on them to tell us when it isn’t so! But simply using the built-in assertions is only a small part of the story.\n\nThe simplest form of result veriﬁ cation is the assertion in which we specify what should be true. Most members of the xUnit family support a range of dif- ferent Assertion Methods, including the following:\n\nStated Outcome Assertions (see Assertion Method) such as assertTrue (aBooleanExpression)\n\nSimple Equality Assertions such as assertEquals(expected, actual)\n\nFuzzy Equality Assertions such as assertEquals(expected, actual, tolerance), which are used for comparing ﬂ oats\n\nOf course, the test programming language has some inﬂ uence on the nature of the assertions. In JUnit, SUnit, CppUnit, NUnit, and CsUnit, most of the Equal- ity Assertions take a pair of Objects as their parameters. Some languages support “overloading” of method parameter types so we can have different implemen- tations of an assertion for different types of objects. Some languages—C, for example—don’t support objects, so we cannot compare objects, only values.\n\nThere are several issues to consider when using Assertion Methods. Naturally, the ﬁ rst priority is the veriﬁ cation of all things that should be true. The better our assertions, the ﬁ ner our Safety Net (see page 24) and the higher our conﬁ - dence in our code. The second priority is the documentation value of the asser- tions. Each test should make it very clear that “When the system is in state S1 and I do X, the result should be R and the system should be in state S2.” We put the system into state S1 in our ﬁ xture setup logic. “I do X” corresponds to the exercise SUT phase of the test. “The result is R” and “the system is in state S2” are implemented using assertions. Thus we want to write our assertions in such a way that they succinctly describe “R” and “S2.”\n\nAnother thing to consider is that when the test fails, we want the failure message to tell us enough to enable us to identify the problem.2 Therefore, we should almost always include an Assertion Message (page 370) as the optional message parameter (assuming our xUnit family member has one!). This tactic avoids the possibility of us playing Assertion Roulette (page 224), in which we cannot even tell which assertion is failing without running the test interactively;\n\n2 In his book [TDD-APG], Dave Astels claims he never/rarely used the Eclipse Debugger while writing the code samples because the assertions always told him enough about what was wrong. This is what we strive for!\n\nwww.it-ebooks.info\n\nState Verification\n\nit makes Integration Build [SCM] failures much easier to reproduce and ﬁ x. It also makes troubleshooting broken tests easier by telling us what should have happened; the actual outcome tells us what did happen!\n\nWhen we use a Stated Outcome Assertion (such as JUnit’s assertTrue), the failure messages tend to be unhelpful (e.g., “Assertion failed”). We can make the assertion output much more speciﬁ c by using an Argument-Describing Mes- sage (see Assertion Message) constructed by incorporating useful bits of data into the message. A good start is to include each of the values in the expression passed as the Assertion Method’s arguments.\n\nDelta Assertions\n\nWhen using a Shared Fixture (page 317), we may ﬁ nd that we have Interacting Tests (see Erratic Test on page 228) because each test adds more objects/rows into the database and we can never be certain exactly what should be there af- ter the SUT has been exercised. One way to deal with this uncertainty is to use Delta Assertions (page 485) to verify only the newly added objects/rows. In this approach, we take some sort of “snapshot” of the relevant tables/classes at the beginning of the test; we then remove these tables/classes from the collection of actual objects/rows produced at the end of the test before comparing them to the Expected Objects. Although this tactic can introduce signiﬁ cant extra complexity into the tests, the added complexity can be refactored into Custom Assertions and/or Veriﬁ cation Methods (see Custom Assertion). The “before” snapshot may be taken on an in-line basis within the Test Method or in the setUp method if all setup occurs before the Test Method is invoked [e.g., Implicit Setup, a Shared Fixture, or a Prebuilt Fixture (page 429)].\n\nExternal Result Veriﬁ cation\n\nThus far we have described only conventional “in-memory” veriﬁ cation of the expected results. In fact, another approach is possible—one that involves storing the expected and actual results in ﬁ les and using an external comparison pro- gram to report on any differences. This is, in effect, a form of Custom Assertion that uses a “deep compare” on two ﬁ le references. The comparison program often needs to be told which parts of the ﬁ les to ignore (or these parts need to be stripped out ﬁ rst), effectively making this a Fuzzy Equality Assertion.\n\nExternal result veriﬁ cation is particularly appropriate for automating accep- tance tests for regression-testing an application that hasn’t changed very much. The major disadvantage of this approach is that we almost always end up with a Mystery Guest (see Obscure Test) from the test reader’s perspective because the\n\nwww.it-ebooks.info\n\n111\n\n112\n\nChapter 10 Result Verification\n\nexpected results are not visible inside the test. One way to avoid this problem is to have the test write the contents of the expected ﬁ le, thereby making the con- tents visible to the test reader. This step is practical only if the amount of data is quite small—another argument in favor of a Minimal Fixture (page 302).\n\nVerifying Behavior\n\nVerifying behavior is more complicated than verifying state because behavior is dynamic. We have to catch the SUT “in the act” as it generates indirect outputs to the objects it depends on (Figure 10.2). Two basic styles of behavior veriﬁ ca- tion are worth discussing: Procedural Behavior Veriﬁ cation and Expected Behavior. Both require a mechanism to access the outgoing method calls of the SUT (its indirect outputs). This and other uses of Test Doubles (page 522) are described in more detail in Chapter 11, Using Test Doubles.\n\nSetup Setup\n\nFixture Fixture Behavior Behavior (Indirect (Indirect Outputs) Outputs)\n\nDOC DOC\n\nExercise Exercise\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nA A\n\nB B\n\nC C\n\ny y f f i i r r e e V V\n\nTeardown Teardown\n\nFigure 10.2 Behavior Veriﬁ cation. In Behavior Veriﬁ cation, we focus our assertions on the indirect outputs (outgoing interfaces) of the SUT. This typically involves replacing the DOC with something that facilitates observing and verifying the outgoing calls.\n\nwww.it-ebooks.info\n\nVerifying Behavior\n\nProcedural Behavior Veriﬁ cation\n\nIn Procedural Behavior Veriﬁ cation, we capture the behavior of the SUT as it executes and save that data for later retrieval. The test then compares each out- put of the SUT (one by one) with the corresponding expected output. Thus, in Procedural Behavior Veriﬁ cation, the test executes a procedure (a set of steps) to verify the behavior.\n\npublic void testRemoveFlightLogging_recordingTestStub() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // Test Double setup AuditLogSpy logSpy = new AuditLogSpy(); facade.setAuditLog(logSpy); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertEquals(\"number of calls\", 1, logSpy.getNumberOfCalls()); assertEquals(\"action code\", Helper.REMOVE_FLIGHT_ACTION_CODE, logSpy.getActionCode()); assertEquals(\"date\", helper.getTodaysDateWithoutTime(), logSpy.getDate()); assertEquals(\"user\", Helper.TEST_USER_NAME, logSpy.getUser()); assertEquals(\"detail\", expectedFlightDto.getFlightNumber(), logSpy.getDetail()); }\n\nThe key challenge in Procedural Behavior Veriﬁ cation is capturing the behavior as it occurs and saving it until the test is ready to use this information. This task is accomplished by conﬁ guring the SUT to use a Test Spy (page 538) or a Self Shunt (see Hard-Coded Test Double on page 568)3 instead of the depended-on class. After the SUT has been exercised, the test retrieves the recording of the behavior and veriﬁ es it using assertions.\n\nExpected Behavior Speciﬁ cation\n\nIf we can build an Expected Object and compare it with the actual object returned by the SUT for verifying state, can we do something similar for verifying\n\n3 A Test Spy built into the Testcase Class (page 373).\n\nwww.it-ebooks.info\n\n113\n\n114\n\nChapter 10 Result Verification\n\nbehavior? Yes, we can and do. Expected Behavior is often used in conjunction with layer-crossing tests to verify the indirect outputs of an object or compo- nent. We conﬁ gure a Mock Object (page 544) with the method calls we expect the SUT to make to it and install this object before exercising the SUT.\n\npublic void testRemoveFlight_JMock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // mock conﬁguration Mock mockLog = mock(AuditLog.class); mockLog.expects(once()).method(\"logMessage\") .with(eq(helper.getTodaysDateWithoutTime()), eq(Helper.TEST_USER_NAME), eq(Helper.REMOVE_FLIGHT_ACTION_CODE), eq(expectedFlightDto.getFlightNumber())); // mock installation facade.setAuditLog((AuditLog) mockLog.proxy()); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify // verify() method called automatically by JMock }\n\nReducing Test Code Duplication\n\nOne of the most common test smells is Test Code Duplication. With every test we write, there is a good chance we have introduced some duplication, but especially if we used “cut and paste” to create a new test from an existing test. Some will argue that duplication in test code is not nearly as bad as duplication in production code. Test Code Duplication is bad if it leads to some other smell such as Fragile Test (page 239), Fragile Fixture (see Fragile Test), or High Test Maintenance Cost (page 265) because too many tests are too closely coupled to the Standard Fixture (page 305) or the API of the SUT. In addition, Test Code Duplication may sometimes be a symptom of another problem—namely, the intent of the tests being obscured by too much code (i.e., an Obscure Test).\n\nIn result veriﬁ cation logic, Test Code Duplication usually shows up as a set of repeated assertions. Several techniques are available to reduce the number of assertions in such cases:\n\nwww.it-ebooks.info\n\nReducing Test Code Duplication\n\nExpected Objects\n\nCustom Assertions\n\nVeriﬁ cation Methods\n\nExpected Objects\n\nOften, we will ﬁ nd ourselves doing a series of assertions on different ﬁ elds of the same object. If we begin repeating this group of assertions (whether multiple times in a single test or in multiple tests), we should look for a way to reduce the Test Code Duplication. The next listing shows one Test Method that compares several attributes of a single object. Many other Test Methods probably require the same sequence of assertions.\n\npublic void testInvoice_addLineItem7() { LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\nThe most obvious alternative is to use a single Equality Assertion to compare two whole objects to each other rather than using many Equality Assertion calls to compare them ﬁ eld by ﬁ eld. If the values are stored in individual variables, we may need to create a new object of the appropriate class and initialize its ﬁ elds with those values. This technique works as long as we have an equals method that compares only those ﬁ elds and we have the ability to create the Expected Object at will.\n\npublic void testInvoice_addLineItem8() { LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); LineItem actual = (LineItem)lineItems.get(0); assertEquals(\"Item\", expItem, actual); }\n\nBut what if we don’t want to compare all the ﬁ elds in an object or the equals method looks for identity rather than equality? What if we want test-speciﬁ c\n\nwww.it-ebooks.info\n\n115\n\n116\n\nChapter 10 Result Verification\n\nequality? What if we cannot create an instance of the Expected Object because no constructor exists? In this scenario, we have two options: We can implement a Custom Assertion that deﬁ nes equality the way we want it or we can imple- ment our test-speciﬁ c equality in the equals method of the class of the Expected Object we pass to the Assertion Method. This class doesn’t need to be the same class as that of the actual object; it just needs to implement equals to compare itself with an instance of the actual object’s class. Therefore, it can be a simple Data Transfer Object [CJ2EEP] or it can be a Test-Speciﬁ c Subclass (page 579) of the real (production) class with just the equals method overridden.\n\nSome test automaters don’t think we should ever rely on the equals method of the SUT when making assertions because it could change, thereby causing tests that depend on this method to fail (or to miss important differences). I pre- fer to be pragmatic about this decision. If it seems reasonable to use the equals deﬁ nition supplied by the SUT, then I do so. If I need something else, I deﬁ ne a Custom Assertion or a test-speciﬁ c Expected Object class. I also ask myself how hard it would be to change my strategy if the equals method should later change. For example, in statically typed languages that support parameter type overloading (such as Java), we can add a Custom Assertion that uses different parameter types to override the default implementation when speciﬁ c types are used. This code can often be retroﬁ tted quite easily if a change to equals causes problems at a later date.\n\nCustom Assertions\n\nA Custom Assertion is a domain-speciﬁ c assertion we write ourselves. Custom Assertions hide the procedure for verifying the results behind a declarative name, making our result veriﬁ cation logic more intent-revealing. They also prevent Obscure Tests by eliminating of a lot of potentially distracting code. Another beneﬁ t of moving the code into a Custom Assertion is that the assertion logic can now be unit-tested by writing Custom Assertion Tests (see Custom Asser- tion). The assertions are no longer Untestable Test Code (see Hard-to-Test Code on page 209)!\n\nstatic void assertLineItemsEqual( String msg, LineItem exp, LineItem act) { assertEquals (msg+\" Inv\", exp.getInv(), act.getInv()); assertEquals (msg+\" Prod\", exp.getProd(), act.getProd()); assertEquals (msg+\" Quan\", exp.getQuantity(), act.getQuantity()); }\n\nThere are two ways to create Custom Assertions: (1) by refactoring existing complex test code to reduce Test Code Duplication and (2) by coding calls to\n\nwww.it-ebooks.info\n\nReducing Test Code Duplication\n\nnonexistent Assertion Methods as we write tests and then ﬁ lling in the method bodies with the appropriate logic once we land on the suite of Custom Assertions needed by a set of Test Methods. The latter technique is a good way of reminding ourselves what we expect the outcome of exercising the SUT to be, even though we haven’t yet written the code to verify it. Either way, the deﬁ nition of a set of Custom Assertions is the ﬁ rst step toward creating a Higher-Level Language (see page 41) for specifying our tests.\n\nWhen refactoring to Custom Assertions, we simply use Extract Method [Fowler] on the repeated assertions and give the new method an Intent-Revealing Name [SBPP]. We pass in the objects used by the existing veriﬁ cation logic as arguments and include an Assertion Message to differentiate between calls to the same assertion method.\n\nOutcome-Describing Veriﬁ cation Method\n\nAnother technique that is born from ruthless refactoring of test code is the “out- come-describing” Veriﬁ cation Method. Suppose we ﬁ nd that a group of tests all have identical exercise SUT and verify outcome sections. Only the setup portion is different for each test. If we do an Extract Method refactoring on the common code and give it a meaningful name, we need less code, achieve more understand- able tests, and produce testable veriﬁ cation logic all at the same time! If this isn’t a worthwhile reason for refactoring code, then I don’t know what else could be.\n\nvoid assertInvoiceContainsOnlyThisLineItem( Invoice inv, LineItem expItem) { List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\",expItem, actual); }\n\nThe major difference between a Veriﬁ cation Method and a Custom Assertion is that the latter only makes assertions, while the former also interacts with the SUT (typically for the purpose of exercising it). Another difference is that Custom Assertions typically have a standard Equality Assertion signature: assertSomething(message, expected, actual). In contrast, Veriﬁ cation Methods may have completely arbitrary parameters because they require additional param- eters to pass into the SUT. They are, in essence, halfway between a Custom Assertion and a Parameterized Test (page 607).\n\nwww.it-ebooks.info\n\n117\n\n118\n\nChapter 10 Result Verification\n\nParameterized and Data-Driven Tests\n\nWe can go even further in factoring out the commonality between tests. If the logic to set up the test ﬁ xture is the same but uses different data, we can extract the common ﬁ xture setup, exercise SUT, and verify outcome phases of the test into a new Parameterized Test method. This Parameterized Test is not called automatically by the Test Automation Framework (page 298) because it requires arguments; instead, we deﬁ ne very simple Test Methods for each test, which then call the Parameterized Test and pass in the data required to make this test unique. This data may include that required for ﬁ xture setup, exercising the SUT, and the corresponding expected result. In the following tests, the method generateAndVerifyHtml is the Parameterized Test.\n\ndef test_extref sourceXml = \"<extref id='abc' />\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<extref>\") end\n\ndef test_testterm_normal sourceXml = \"<testterm id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<testterm>\") end\n\ndef test_testterm_plural sourceXml = \"<testterms id='abc'/>\" expectedHtml = \"<a href='abc.html'>abcs</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<plural>\") end\n\nIn a Data-Driven Test (page 288), the test case is completely generic and directly executable by the framework; it reads the arguments from a test data ﬁ le as it executes. Think of a Data-Driven Test as a Parameterized Test turned inside out: A Test Method passes test-speciﬁ c data to a Parameterized Test; a Data-Driven Test is the Test Method and reads the test-speciﬁ c data from a ﬁ le. The contents of the ﬁ le are a Higher-Level Language for testing; the Data-Driven Test method is the Interpreter [GOF] of that language. This scheme is the xUnit equivalent of a Fit test. A simple example of a Data-Driven Test method is shown in this code sample written in Ruby:\n\ndef test_crossref executeDataDrivenTest \"CrossrefHandlerTest.txt\" end\n\ndef executeDataDrivenTest ﬁlename dataFile = File.open(ﬁlename)\n\nwww.it-ebooks.info\n\nAvoiding Conditional Test Logic\n\ndataFile.each_line do | line | desc, action, part2 = line.split(\",\") sourceXml, expectedHtml, leftOver = part2.split(\",\") if \"crossref\"==action.strip generateAndVerifyHtml sourceXml, expectedHtml, desc else # new \"verbs\" go before here as elsif's report_error( \"unknown action\" + action.strip ) end end end\n\nHere is the comma-delimited data ﬁ le that the Data-Driven Test method reads:\n\nID, Action, SourceXml, ExpectedHtml Extref,crossref,<extref id='abc'/>,<a href='abc.html'>abc</a> TTerm,crossref,<testterm id='abc'/>,<a href='abc.html'>abc</a> TTerms,crossref,<testterms id='abc'/>,<a href='abc.html'>abcs</a>\n\nAvoiding Conditional Test Logic\n\nAnother thing we want to avoid in our tests is conditional logic. Conditional Test Logic (page 200) is bad because the same test may execute differently in different circumstances. Conditional Test Logic reduces our trust in the tests because the code in our Test Methods is Untestable Test Code. Why is this important? Because the only way we can verify our Test Method is to manually edit the SUT so that it produces the error we want to be detected. If the Test Method has many paths through it, we need to make sure each path is coded correctly. Isn’t it so much simpler just to have only one possible execution path through the test? Let us look at some reasons why we might include conditional logic in our tests:\n\nWe don’t want to execute certain assertions because their execution doesn’t make sense given what we have already discovered at this point in the test (typically a failure condition).\n\nWe have to allow for various situations in the actual results that we are\n\ncomparing to the expected results.\n\nWe are trying to reuse a Test Method in several different circumstances\n\n(essentially merging several tests into a single Test Method).\n\nThe problem with using Conditional Test Logic in the ﬁ rst two cases is that it makes the code hard to read and may mask cases of reusing test methods via Flexible Tests (see Conditional Test Logic). The last “reason” is just a bad idea,\n\nwww.it-ebooks.info\n\n119\n\n120\n\nChapter 10 Result Verification\n\nplain and simple. There are much better ways of reusing test logic than trying to reuse the Test Method itself. We have already seen some of these reuse tech- niques elsewhere in this chapter (in Reducing Test Code Duplication), and we will see other ways elsewhere in this book. Just say “no”!\n\nThe good news is that it is relatively straightforward to remove all legitimate\n\nuses of Conditional Test Logic from our tests.\n\nEliminating “if” Statements\n\nWhat should we do when we don’t want to execute an assertion because we know it will result in a test error and we would prefer to have a more meaning- ful test failure message? The normal reaction is to place the assertion inside an “if” statement, as shown in the following listing. Unfortunately, this approach results in Conditional Test Logic, which we would dearly like to avoid because we want exactly the same code to run each time we run the test.\n\nList lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem expected = new LineItem(invoice, product,5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"invoice\", expected, actItem); } else { fail(\"Invoice should have exactly one line item\"); }\n\nThe preferred solution is to use a Guard Assertion (page 490) as shown in this revised version of the test code:\n\nList lineItems = invoice.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"invoice\", expected, actItem);\n\nThe nice thing about Guard Assertions is that they keep us from hitting the as- sertion that would cause a test error but without introducing Conditional Test Logic. Once we get used to them, these assertions are fairly obvious and intuitive to read. We may even ﬁ nd ourselves wanting to assert the pre-conditions of our methods in our production code!\n\nwww.it-ebooks.info\n\nOther Techniques\n\nEliminating Loops\n\nConditional Test Logic may also appear as loops that verify the content of a col- lection returned by the SUT matches what we expected. Putting loops directly into the Test Method creates three problems:\n\nIt introduces Untestable Test Code because the looping code, which is part of the test, cannot be tested with Fully Automated Tests (see page 26).\n\nIt leads to Obscure Tests because all that looping code obscures the real\n\nintent: Does or doesn’t the collection match?\n\nIt can lead to the project-level smell Developers Not Writing Tests (page 263) because the complexity of writing the loops may dis- courage the developer from writing the Self-Checking Test.\n\nA better solution is to delegate this logic to a Test Utility Method with an Intent- Revealing Name, which can be both tested and reused.\n\nOther Techniques\n\nThis section outlines some other techniques for writing easy-to-understand tests.\n\nWorking Backward, Outside-In\n\nA useful little trick for writing very intent-revealing code is to work backward. This is an application of Stephen Covey’s idea, “Start with the end in mind.” To do so, we write the last line of the function or test ﬁ rst. For a function, its whole reason for existence is to return a value; for a procedure, it is to produce one or more side effects by modifying something. For a test, the raison d’ tre is to verify that the expected outcome has occurred (by making assertions).\n\nWorking backward means we write these assertions ﬁ rst. We assert on the values of suitably named local variables to ensure that the assertion is intent-revealing. The rest of writing the test simply consists of ﬁ lling in whatever is needed to execute those assertions: We declare variables to hold the assertion arguments and initialize them with the appropriate content. Because at least one argument should have been retrieved from the SUT, we must, of course, invoke the SUT. To do so, we may need some variables to use as SUT arguments. Declaring and initializing a variable after it has been used forces us to understand the variable better when we introduce it. This scheme also results in better variable names and avoids meaningless names like invoice1 and invoice2.\n\nwww.it-ebooks.info\n\n121\n\n122\n\nChapter 10 Result Verification\n\nWorking “outside-in” (or “top-down” as it is sometimes called) means staying at a consistent level of abstraction. The Test Method should focus on what we need to have in place to induce the relevant behavior in the SUT. The mechanics of how we reach that place should be delegated to a “lower layer” of test soft- ware. In practice, we code this behavior as calls to Test Utility Methods, which allows us to stay focused on the requirements of the SUT as we write each Test Method. We don’t need to worry about how we will create that object or verify that outcome; we merely need to describe what that object or outcome should be. The utility method we just used but haven’t yet deﬁ ned acts as a placeholder for the unﬁ nished test automation logic.4 We can move on to writing the other tests we need for this SUT while they are still fresh in our minds. Later, we can switch to our “toolsmith” hat and implement the Test Utility Methods.\n\nUsing Test-Driven Development to Write Test Utility Methods\n\nOnce we are ﬁ nished writing the Test Method(s) that used the Test Utility Method, we can start the process of writing the Test Utility Method itself. Along the way, we can take advantage of test-driven development by writing Test Utility Tests (see Test Utility Method). It doesn’t take very long to write these unit tests that verify the behavior of our Test Utility Methods and we will have much more conﬁ dence in them.\n\nWe start with the simple case (say, asserting the equality of two identical collections that hold the same item) and work up to the most complicated case that the Test Methods actually require (say, two collections that contain the same two items but in different order). TDD helps us ﬁ nd the minimal implemen- tation of the Test Utility Method, which may be much simpler than a complete generic solution. There is no point in writing generic logic that handles cases that aren’t actually needed but it may be worthwhile to include a Guard Assertion or two inside the Custom Assertion to fail tests in cases it doesn’t support.\n\nWhere to Put Reusable Veriﬁ cation Logic?\n\nSuppose we have decided to use Extract Method refactorings to create some reus- able Custom Assertions or we have decided to write our tests in an intent-revealing way using Veriﬁ cation Methods. Where should we put these bits of reusable test\n\n4 We should always give this method an Intent-Revealing Name and stub it out with a call to the fail assertion to remind ourselves that we still need to write the method’s body.\n\nwww.it-ebooks.info\n\nWhat’s Next?\n\nlogic? The most obvious place is in the Testcase Class (page 373) itself. We can allow this logic to be reused more broadly by using a Pull-Up Method [Fowler] refactoring to move them up to a Testcase Superclass (page 638) or a Move Method [Fowler] refactoring to move them into a Test Helper (page 643). This issue is dis- cussed in more detail in Chapter 12, Organizing Our Tests.\n\nWhat’s Next?\n\nThis discussion of techniques for verifying the expected outcome concludes our introduction to the basic techniques of automating tests using xUnit. Chapter 11, Using Test Doubles, introduces some advanced techniques involving the use of Test Doubles.\n\nwww.it-ebooks.info\n\n123\n\nThis page intentionally left blank\n\nwww.it-ebooks.info",
      "page_number": 170
    },
    {
      "number": 11,
      "title": "Using Test Doubles",
      "start_page": 188,
      "end_page": 215,
      "detection_method": "regex_chapter",
      "content": "Chapter 11\n\nUsing Test Doubles\n\nAbout This Chapter\n\nThe last few chapters concluding with Chapter 10, Result Veriﬁ cation, intro- duced the basic mechanisms of running tests using the xUnit family of Test Automation Frameworks (page 298). For the most part we assumed that the SUT was designed such that it could be tested easily in isolation of other pieces of soft- ware. When a class does not depend on any other classes, testing it is relatively straightforward and the techniques described in this chapter are unnecessary. When a class does depend on other classes, we have two choices: We can test it together with all the other classes it depends on or we can try to isolate it from the other classes so that we can test it by itself. This chapter introduces techniques for isolating the SUT from the other software components on which it depends.\n\nWhat Are Indirect Inputs and Outputs?\n\nThe problem with testing classes in groups or clusters is that it becomes very hard to cover all the paths through the code. The depended-on component (DOC) may return values or throw exceptions that affect the behavior of the SUT, but it may prove difﬁ cult or impossible to cause certain cases to occur. The indirect inputs received from the DOC may be unpredictable (such as the system clock or cal- endar). In other cases, the DOC may not be available in the test environment or may not even exist. How can we test dependent classes in these circumstances?\n\nIn other cases, we need to verify that certain side effects of executing the SUT have, indeed, occurred. If it is too difﬁ cult to monitor these indirect outputs of the SUT (or if it is too expensive to retrieve them), the effectiveness of our automated testing may be compromised.\n\nAs you will no doubt have guessed from the title of this chapter, the solution to these problems is often the use of a Test Double (page 522). We will start by\n\n125\n\nwww.it-ebooks.info\n\n126\n\nChapter 11 Using Test Doubles\n\nlooking at how we can use Test Doubles to test indirect inputs and outputs. We will then describe a few other uses of these helpful mechanisms.\n\nWhy Do We Care about Indirect Inputs?\n\nCalls to DOCs often return objects or values, update their arguments or even throw exceptions. Many of the execution paths within the SUT are intended to deal with these return values and to handle the possible exceptions. Leaving these paths un- tested leads to Untested Code (see Production Bugs on page 268). These paths can be the most challenging to test effectively but are also among the most likely to lead to catastrophic failures if exercised for the very ﬁ rst time in production.\n\nWe certainly would rather not have the exception-handling code execute for the ﬁ rst time in production. What if it was coded incorrectly? Clearly, it would be high- ly desirable to have automated tests for such code. The testing challenge is to some- how cause the DOC to throw an exception so that the error path can be tested. The exception we expect the DOC to throw is a good example of an indirect input test condition (Figure 11.1). Our means of injecting this input is a control point.\n\nFixture Fixture\n\nSetup Setup\n\nDOC DOC\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nIndirect Indirect Input Input\n\nTeardown Teardown\n\nFigure 11.1 An indirect input being received by the SUT from a DOC. Not all inputs of the SUT come from the test. Some indirect inputs come from other components called by the SUT in the form of return values, updated parameters, or exceptions thrown.\n\nWhy Do We Care about Indirect Outputs?\n\nThe concept of encapsulation often directs us to not care about how some- thing is implemented. After all, that is the whole purpose of encapsulation—to alleviate the need for clients of our interface to care about our implementation.\n\nwww.it-ebooks.info\n\nWhat Are Indirect Inputs and Outputs?\n\nWhen testing, we try to verify the implementation precisely so our clients do not have to care about it.\n\nConsider for a moment a component that has a method in its API that returns nothing—or at least nothing that can be used to determine whether it has performed its function correctly. In this situation, we have no choice but to test through the back door. A good example of this is a message logging system. Calls to the API of a logger rarely return anything that indicates it did its job correctly. The only way to determine whether the message logging system is working as expected is to interact with it through some other interface—one that allows us to retrieve the logged messages.\n\nA client of the logger may specify that the logger be called when certain con- ditions are met. These calls will not be visible on the client’s interface but would typically be a requirement that the client needs to satisfy and, therefore, would be something we want to test. The circumstances that should result in a messag- ing being logged are indirect output test conditions (Figure 11.2) for which we need to write tests so that we can avoid having Untested Requirements (see Pro- duction Bugs). Our means of seeing this output is an observation point.\n\nFixture Fixture\n\nSetup Setup\n\nDOC DOC\n\nExercise Exercise\n\nSUT SUT\n\nIndirect Indirect Output Output\n\nVerify Verify\n\nTeardown Teardown\n\nFigure 11.2 An indirect output being received by the SUT. Not all outputs of the SUT are directly visible to the test. Some indirect outputs are sent to other components in the form of method calls or messages.\n\nIn other cases, the SUT does produce visible behavior that can be veriﬁ ed through the front door but also has some expected side effects. Both outputs need to be veriﬁ ed in our tests. Sometimes this testing is simply a matter of adding assertions for the indirect outputs to the existing tests to verify the Untested Requirement.\n\nwww.it-ebooks.info\n\n127\n\n128\n\nChapter 11 Using Test Doubles\n\nHow Do We Control Indirect Inputs?\n\nTesting with indirect inputs is a bit simpler than testing with indirect outputs because the techniques used to test outputs build on those used to test inputs. Let’s delve into indirect inputs ﬁ rst.\n\nTo test the SUT with indirect inputs, we must be able to control th e DOC well enough to cause it to return every possible kind of return value. That implies the availability of a suitable control point.\n\nExamples of the kinds of indirect inputs we want to be able to induce via this\n\ncontrol point include\n\nReturn values of methods/functions\n\nValues of updatable arguments\n\nExceptions that could be thrown\n\nOften, the test can interact with the DOC to set up how it will respond to requests. For example, if a component provides access to data in a database, then we can use Back Door Setup (see Back Door Manipulation on page 327) to insert speciﬁ c values into a database that cause the component to respond in the desired ways (e.g., no items found, one item found, many items found). (See Figure 11.3.) In this speciﬁ c case, we can use the database itself as a control point.\n\nSetup Setup Setup Setup\n\nExercise Exercise Exercise Exercise\n\nSUT SUT SUT SUT\n\nData Data Data Data Fixture Fixture Fixture Fixture\n\nVerify Verify Verify Verify\n\nTeardown Teardown Teardown Teardown\n\nFigure 11.3 Using Back Door Manipulation to indirectly control and observe the SUT. When the SUT stores its state in another component, we may be able to manipulate that state by having the test interact directly with the other com- ponent via a “back door.”\n\nIn most cases, however, this approach is neither practical nor even possible. We might not be able to use the real component for the following reasons:\n\nwww.it-ebooks.info\n\nWhat Are Indirect Inputs and Outputs?\n\nThe real component cannot be manipulated to produce the desired indirect input. Only a true software error within the real component would result in the desired input to the SUT.\n\nThe real component could be manipulated to make the input occur but\n\ndoing so would not be cost-effective.\n\nThe real component could be manipulated to make the input occur but\n\ndoing so could have unacceptable side effects.\n\nThe real component is not yet available for use.\n\nIf we cannot use the real component as a control point, then we have to replace it with one that we can control. This replacement can be done in a number of different ways, which are the focus of the section Installing the Test Double later in this chapter. The most common approach is to conﬁ gure a Test Stub (page 529) with a set of values to return from its functions and then to install this Test Stub into the SUT. During execution of the SUT, the Test Stub receives the calls and returns the previously conﬁ gured responses (Figure 11.4). It has become our control point.\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nCreation Creation\n\nTest Test Stub Stub\n\nInstallation Installation\n\nExercise Exercise\n\nSUT SUT\n\nReturn Return Values Values\n\nVerify Verify\n\nIndirect Indirect Input Input\n\nTeardown Teardown\n\nFigure 11.4 Using a Test Stub as a control point for indirect inputs. One way to use a control point to inject indirect inputs into the SUT is to install a Test Stub in place of the DOC. Before exercising the SUT, we tell the Test Stub what it should return to the SUT when it is called. This strategy allows us to force the SUT through all its code paths.\n\nwww.it-ebooks.info\n\n129\n\n130\n\nChapter 11 Using Test Doubles\n\nHow Do We Verify Indirect Outputs?\n\nIn normal usage, as the SUT is exercised, it interacts naturally with the component(s) upon which it depends. To test the indirect outputs, we must be able to observe the calls that the SUT makes to the API of the DOC (Figure 11.5). Furthermore, if we need the test to progress beyond that point, we need to be able to control the val- ues returned (as was discussed in the discussion of indirect inputs).\n\nSetup Setup\n\nFixture Fixture Behavior Behavior (Indirect (Indirect Outputs) Outputs)\n\nDOC DOC\n\nExercise Exercise\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nA A\n\nB B\n\nC C\n\ny y f f i i r r e e V V\n\nTeardown Teardown\n\nFigure 11.5 Using Behavior Veriﬁ cation to verify the indirect outputs of the SUT. When we care about exactly what calls our SUT makes to other components, we may have to do Behavior Veriﬁ cation rather than simply verifying the post-test state of the SUT.\n\nIn many cases, the test can use the DOC as an observation point to ﬁ nd out how it has been used. For example:\n\nWe can ask the ﬁ le system for the contents of a ﬁ le that the SUT has writ- ten to verify that it exists and was written with the expected contents.\n\nWe can ask the database for the contents of a table or speciﬁ c record to\n\nverify that the SUT wrote the expected records to the database.\n\nWe can interact directly with the e-mail sending component to ask\n\nwhether the SUT had asked it to send a particular e-mail.\n\nThese are all examples of Back Door Veriﬁ cation (see Back Door Manipulation on page 327). Some DOCs allow us to conﬁ gure their behavior in such a way that we can use them to keep the test informed of how they are being used:\n\nwww.it-ebooks.info\n\nWhat Are Indirect Inputs and Outputs?\n\nWe can ask the ﬁ le system to notify the test whenever a ﬁ le is created or\n\nmodiﬁ ed so we can verify its contents.\n\nWe can use a database trigger to notify the test when a record is written\n\nor deleted.\n\nWe can conﬁ gure the e-mail sending component to deliver all outgoing\n\ne-mail to the test.\n\nSometimes, as we have seen with indirect inputs, it is not practical to use the real component as an observation point. When all else fails, we may need to replace the real component with a test-speciﬁ c alternative. For example, we might need to do this for the following reasons:\n\nThe calls to (or the internal state of) the DOC cannot be queried.\n\nThe real component can be queried but doing so is cost-prohibitive.\n\nThe real component can be queried but doing so has unacceptable side\n\neffects.\n\nThe real component is not yet available for use.\n\nThe replacement of the real component can be done in a number of different ways, as will be discussed in Installing the Test Double.\n\nTwo basic styles of indirect output veriﬁ cation are available. Procedural Behav- ior Veriﬁ cation (see Behavior Veriﬁ cation) captures the calls to a DOC (or their re- sults) during SUT execution and then compares them with the expected calls after the SUT has ﬁ nished executing. This veriﬁ cation involves replacing a substitutable dependency with a Test Spy (page 538). During execution of the SUT, the Test Spy receives the calls and records them. After the Test Method (page 348) has ﬁ nished exercising the SUT, it retrieves the actual calls from the Test Spy and uses Assertion Methods (page 362) to compare them with the expected calls (Figure 11.6).\n\nExpected Behavior (see Behavior Veriﬁ cation) involves building a “behavior speciﬁ cation” during the ﬁ xture setup phase of the test and then comparing the actual behavior with this Expected Behavior. It is typically done by loading a Mock Object (page 544) with a set of expected procedure call descriptions and installing this object into the SUT (Figure 11.7). During execution of the SUT, the Mock Object receives the calls and compares them to the previously deﬁ ned expected calls (the “behavior speciﬁ cation”). As the test proceeds, if the Mock Object receives an unexpected call, it fails the test immediately. The test failure traceback will show the exact location in the SUT where the problem occurred because the Assertion Methods are called from the Mock Object, which is in turn called by the SUT. We can also see exactly where in the Test Method the SUT was being exercised.\n\nwww.it-ebooks.info\n\n131\n\n132\n\nChapter 11 Using Test Doubles\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nCreation Creation\n\nTest Spy Test Spy\n\nExercise Exercise\n\nInstallation Installation\n\nExercise Exercise\n\nSUT SUT\n\nIndirect Indirect Output Output\n\nIndirect Indirect Outputs Outputs\n\nVerify Verify\n\nTeardown Teardown\n\nFigure 11.6 Using a Test Spy as an observation point for indirect outputs of the SUT. One way to implement Behavior Veriﬁ cation is to install a Test Spy in place of the target of the indirect outputs. After exercising the SUT, the test asks the Test Spy for information about how it was used and compares that information to the expected behavior using assertions.\n\nSetup Setup\n\nCreation Creation\n\nFixture Fixture\n\nMock Mock Object Object\n\nDOC DOC\n\nInstallation Installation\n\nExpectations Expectations\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nSUT SUT\n\nFinal Verification Final Verification\n\nIndirect Indirect Output Output\n\ny y f f i i r r e e V V\n\nTeardown Teardown\n\nFigure 11.7 Using a Mock Object as an observation point for indirect outputs of the SUT. Another way to implement Behavior Veriﬁ cation is to install a Mock Object in place of the target of the indirect outputs. As the SUT makes calls to the DOC, the Mock Object uses assertions to compare the actual calls and arguments with the expected calls and arguments.\n\nwww.it-ebooks.info\n\nTesting with Doubles\n\nWhen we use a Test Spy or a Mock Object, we may also have to employ it as a control point for any indirect inputs on which the SUT depends after the Test Spy or Mock Object has been called to allow test execution to continue.\n\nTesting with Doubles\n\nBy now you are probably wondering about how to replace those inﬂ exible and uncooperative real components with something that makes it easier to control the indirect inputs and to verify the indirect outputs.\n\nAs we have seen, to test the indirect inputs, we must be able to control the DOC well enough to cause it to return every possible kind of return value (valid, invalid, and exception). To test indirect outputs, we must be able to track the calls the SUT makes to other components. A Test Double is a type of object that is much more cooperative and lets us write tests the way we want to.\n\nTypes of Test Doubles\n\nA Test Double is any object or component that we install in place of the real component for the express purpose of running a test. Depending on the reason why we are using it, a Test Double can behave in one of four ways (summarized in Figure 11.8):\n\nA Dummy Object (page 728) is a placeholder object that is passed to the SUT as an argument (or an attribute of an argument) but is never actually used.\n\nA Test Stub is an object that replaces a real component on which the SUT depends so that the test can control the indirect inputs of the SUT. It allows the test to force the SUT down paths it might not otherwise exercise. A Test Spy, which is a more capable version of a Test Stub, can be used to verify the indirect outputs of the SUT by giving the test a way to inspect them after exercising the SUT.\n\nA Mock Object is an object that replaces a real component on which\n\nthe SUT depends so that the test can verify its indirect outputs.\n\nwww.it-ebooks.info\n\n133\n\n134\n\nChapter 11 Using Test Doubles\n\nA Fake Object (page 551) (or just “Fake” for short) is an object that replaces the functionality of the real DOC with an alternative imple- mentation of the same functionality.\n\nTest Test Double Double\n\nDummy Dummy Object Object\n\nTest Test Stub Stub\n\nTest Test Spy Spy\n\nMock Mock Object Object\n\nFake Fake Object Object\n\nConfigurable Configurable Test Double Test Double\n\nHard-Coded Hard-Coded Test Double Test Double\n\nFigure 11.8 Several kinds of Test Doubles exist. Dummy Objects are really an alternative to the value patterns. Test Stubs are used to verify indirect inputs; Test Spies and Mock Objects are used to verify indirect outputs. Fake objects emulate the behavior of the real depended-on component, but with test-friendly characteristics.\n\nDummy Objects\n\nDummy Objects are a degenerate form of Test Double. They exist solely so that they can be passed around from method to method; they are never used. That is, Dummy Objects are not expected to do anything except exist. Often, we can get away with using “null” (or “nil” or “nothing”); at other times, we may be forced to create a real object because the code expects something non-null. In dynamically typed languages, almost any real object will do; in statically typed languages, we must make sure that the Dummy Object is “type-compatible” with the parameter it is being passed as or the variable to which it is being assigned.\n\nIn the following example, we pass an instance of DummyCustomer to the Invoice constructor to satisfy a mandatory argument. We do not expect the DummyCustomer to be used by the code we are testing here.\n\nwww.it-ebooks.info\n\nTesting with Doubles\n\npublic void testInvoice_addLineItem_DO() { ﬁnal int QUANTITY = 1; Product product = new Product(\"Dummy Product Name\", getUniqueNumber()); Invoice inv = new Invoice( new DummyCustomer() ); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\", expItem, actual); }\n\nNote that a Dummy Object is not the same as a Null Object [PLOPD3]. A Dummy Object is not used by the SUT, so its behavior is irrelevant. By contrast, a Null Object is used by the SUT but is designed to do nothing. That’s a small but very important distinction!\n\nDummy Objects are in a different league than the other Test Doubles; they are really an alternative to the attribute value patterns such as Literal Value (page 714), Generated Value (page 723), and Derived Value (page 718). Therefore, we don’t need to “conﬁ gure” them or “install” them. In fact, almost nothing we say about the other Test Doubles applies to Dummy Objects, so we won’t mention them again in this chapter.\n\nTest Stubs\n\nA Test Stub is an object that acts as a control point to deliver indirect inputs to the SUT when the Test Stub’s methods are called. Its use allows us to exercise Untested Code paths in the SUT that might otherwise be impossible to traverse during testing. A Responder (see Test Stub) is a basic Test Stub that is used to inject valid and invalid indirect inputs into the SUT via normal returns from method calls. A Saboteur (see Test Stub) is a special Test Stub that raises exceptions or errors to inject abnormal indirect inputs into the SUT. Because procedural programming languages do not support objects, they force us to use Procedural Test Stubs (see Test Stub).\n\nIn the following example, the Saboteur—implemented as an anonymous inner class in Java—throws an exception when the SUT calls the getTime method to allow us to verify that the SUT behaves correctly in this case:\n\npublic void testDisplayCurrentTime_exception() throws Exception { // Fixture setup\n\nwww.it-ebooks.info\n\n135\n\n136\n\nChapter 11 Using Test Doubles\n\n// Deﬁne and instantiate Test Stub TimeProvider testStub = new TimeProvider() { // Anonymous inner Test Stub public Calendar getTime() throws TimeProviderEx { throw new TimeProviderEx(\"Sample\"); } }; // Instantiate SUT TimeDisplay sut = new TimeDisplay(); sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"error\\\">Invalid Time</span>\"; assertEquals(\"Exception\", expectedTimeString, result); }\n\nIn procedural programming languages, a Procedural Test Stub is either (1) a Test Stub implemented as a stand-in for an as-yet-unwritten procedure or (2) an alternative implementation of a procedure linked into the program instead of the real implementation of the procedure. Traditionally, Procedural Test Stubs are introduced to allow debugging to proceed while we are waiting for other code to be ready. They are rarely “swapped in” at runtime—this is hard to do in most procedural languages. If we do not mind introducing Test Logic in Pro- duction (page 217) code, we can implement a Procedural Test Stub using Test Hooks (page 709) such as if testing then ... else in the SUT. This is illustrated in the following listing:\n\npublic Calendar getTime() throws TimeProviderEx { Calendar theTime = new GregorianCalendar(); if (TESTING) { theTime.set(Calendar.HOUR_OF_DAY, 0); theTime.set(Calendar.MINUTE, 0);} else { // just return the calendar } return theTime; };\n\nThe key exception occurs in languages that support procedure variables.1 These variables allow us to implement dynamic binding as long as the client code ac- cesses the procedure to be replaced via a procedure variable.\n\n1 Also called function pointers.\n\nwww.it-ebooks.info\n\nTesting with Doubles\n\nTest Spies\n\nA Test Spy is an object that can act as an observation point for the indirect outputs of the SUT. To the capabilities of a Test Stub, it adds the ability to quietly record all calls made to its methods by the SUT. The veriﬁ cation part of the test performs Procedural Behavior Veriﬁ cation on those calls by using a series of assertions to compare the actual calls received by the Test Spy with the expected calls.\n\nThe following example uses the Retrieval Interface (see Test Spy) on the Test Spy to verify that the correct information was passed as arguments in the call to the logMessage method by the SUT (the removeFlight method of the facade).\n\npublic void testRemoveFlightLogging_recordingTestStub() throws Exception { // Fixture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // Test Double setup AuditLogSpy logSpy = new AuditLogSpy(); facade.setAuditLog(logSpy); // Exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // Verify state assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); // Verify indirect outputs using retrieval interface of spy assertEquals(\"number of calls\", 1, logSpy.getNumberOfCalls()); assertEquals(\"action code\", Helper.REMOVE_FLIGHT_ACTION_CODE, logSpy.getActionCode()); assertEquals(\"date\", helper.getTodaysDateWithoutTime(), logSpy.getDate()); assertEquals(\"user\", Helper.TEST_USER_NAME, logSpy.getUser()); assertEquals(\"detail\", expectedFlightDto.getFlightNumber(), logSpy.getDetail()); }\n\nMock Objects\n\nA Mock Object is also an object that can act as an observation point for the indirect outputs of the SUT. Like a Test Stub, it may need to return information in response to method calls. Also like a Test Spy, a Mock Object pays attention to how it was called by the SUT. It differs from a Test Spy, however, in that the\n\nwww.it-ebooks.info\n\n137\n\n138\n\nChapter 11 Using Test Doubles\n\nMock Object compares actual calls received with the previously deﬁ ned expec- tations using assertions and fails the test on behalf of the Test Method. As a consequence, we can reuse the logic employed to verify the indirect outputs of the SUT across all tests that use the same Mock Object. Mock Objects come in two basic ﬂ avors:\n\nA strict Mock Object fails the test if the correct calls are received in a\n\ndifferent order than was speciﬁ ed.\n\nA lenient2 Mock Object tolerates out-of-order calls. Some lenient Mock Objects tolerate or even ignore unexpected calls or missed calls. That is, the Mock Object may verify only those actual calls that correspond to expected ones.\n\nThe following test conﬁ gures a Mock Object with the arguments of the expected call to logMessage. When the SUT (the removeFlight method) calls logMessage, the Mock Object asserts that each of the actual arguments equals the expected argu- ment. If it discovers that any wrong arguments were passed, the test fails.\n\npublic void testRemoveFlight_Mock() throws Exception { // Fixture setup FlightDto expectedFlightDto = createAnonRegFlight(); // Mock conﬁguration ConﬁgurableMockAuditLog mockLog = new ConﬁgurableMockAuditLog(); mockLog.setExpectedLogMessage( helper.getTodaysDateWithoutTime(), Helper.TEST_USER_NAME, Helper.REMOVE_FLIGHT_ACTION_CODE, expectedFlightDto.getFlightNumber()); mockLog.setExpectedNumberCalls(1); // Mock installation FlightManagementFacade facade = new FlightManagementFacadeImpl(); facade.setAuditLog(mockLog); // Exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // Verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); mockLog.verify(); }\n\n2 Lenient Mock Objects are sometimes called “nice,” but “lenient” is a more precise adjective.\n\nwww.it-ebooks.info\n\nTesting with Doubles\n\nLike Test Stubs, Mock Objects often support conﬁ guration with any indirect inputs required to allow the SUT to advance to the point where it would generate the indirect outputs they are verifying.\n\nFake Objects\n\nA Fake Object is quite different from a Test Stub or a Mock Object in that it is nei- ther directly controlled nor observed by the test. The Fake Object is used to replace the functionality of the real DOC in a test for reasons other than veriﬁ cation of indi- rect inputs and outputs. Typically, a Fake Object implements the same functionality or a subset of the functionality of the real DOC, albeit in a much simpler way. The most common reasons for using a Fake Object are that the real DOC has not yet been built, is too slow, or is not available in the test environment.\n\nThe sidebar “Faster Tests without Shared Fixtures” (page 319) describes how my team encapsulated all database access behind a persistence layer interface and then replaced the persistence layer component with one that used in-memory hash tables instead of a real database, thereby making our tests run 50 times faster. To do so, we used a Fake Database (see Fake Object) that was something like this one:\n\npublic class InMemoryDatabase implements FlightDao{ private List airports = new Vector(); public Airport createAirport(String airportCode, String name, String nearbyCity) throws DataException, InvalidArgumentException { assertParamtersAreValid( airportCode, name, nearbyCity); assertAirportDoesntExist( airportCode); Airport result = new Airport(getNextAirportId(), airportCode, name, createCity(nearbyCity)); airports.add(result); return result; } public Airport getAirportByPrimaryKey(BigDecimal airportId) throws DataException, InvalidArgumentException { assertAirportNotNull(airportId);\n\nAirport result = null; Iterator i = airports.iterator(); while (i.hasNext()) { Airport airport = (Airport) i.next(); if (airport.getId().equals(airportId)) { return airport; } } throw new DataException(\"Airport not found:\"+airportId); }\n\nwww.it-ebooks.info\n\n139\n\n140\n\nChapter 11 Using Test Doubles\n\nProviding the Test Double\n\nThere are two approaches to providing a Test Double: a Hand-Built Test Dou- ble (see Conﬁ gurable Test Double on page 558), which is coded by the test automater, or a Dynamically Generated Test Double (see Conﬁ gurable Test Double), which is generated at runtime using a framework or toolkit provided by some other developer.3 All generated Test Doubles must be, by their very nature, Conﬁ gurable Test Doubles; these components are covered in more detail in the next section. Hand-Built Test Doubles, by contrast, tend to be Hard-Coded Test Doubles (page 568) but can also be made conﬁ gurable with some additional effort. The following code sample illustrates a hand-coded Inner Test Double (see Hard-Coded Test Double) that uses Java’s anonymous inner class construct:\n\npublic void testDisplayCurrentTime_AtMidnight_PS() throws Exception { // Fixture setup // Deﬁne and instantiate Test Stub TimeProvider testStub = new PseudoTimeProvider() { // Anonymous inner stub public Calendar getTime(String timeZone) { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.MINUTE, 0); myTime.set(Calendar.HOUR_OF_DAY, 0); return myTime; } }; // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Inject Test Stub into SUT sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nWe can greatly simplify the development of Hand-Built Test Doubles in statically typed languages such as Java and C# by providing a set of base classes called Pseudo-Objects (see Hard-Coded Test Double) from which to create sub- classes. Pseudo-Objects can reduce the number of methods we need to implement\n\n3 JMock and its ports to other languages are good examples of such toolkits. Other toolkits, such as EasyMock, implement Statically Generated Test Doubles (see Conﬁ gurable Test Double) by generating code that is then compiled just like a Hand-Built Test Double.\n\nwww.it-ebooks.info\n\nTesting with Doubles\n\nin each Test Stub, Test Spy, or Mock Object to just the ones we expect to be called. They are especially helpful when we are using Inner Test Doubles or Self Shunts (see Hard-Coded Test Double). The class deﬁ nition for the Pseudo-Object used in the previous example looks like this:\n\n/** * Base class for hand-coded Test Stubs and Mock Objects */ public class PseudoTimeProvider implements ComplexTimeProvider {\n\npublic Calendar getTime() throws TimeProviderEx { throw new PseudoClassException(); }\n\npublic Calendar getTimeDifference(Calendar baseTime, Calendar otherTime) throws TimeProviderEx { throw new PseudoClassException(); }\n\npublic Calendar getTime( String timeZone ) throws TimeProviderEx { throw new PseudoClassException(); } }\n\nConﬁ guring the Test Double\n\nSome Test Doubles (speciﬁ cally, Test Stubs and Mock Objects) need to be told which values to return and/or which values to expect. A Hard-Coded Test Double receives these instructions at design time from the test automater; a Conﬁ gurable Test Double is told this information at runtime by the test (Figure 11.9). A Test Stub or Test Spy needs to be conﬁ gured only with the values that will be returned by the methods that the SUT is expected to invoke. A Mock Object also needs to be conﬁ gured with the names and arguments of all methods we expect the SUT to invoke on it. In all cases, the test automater ultimately decides with which values to conﬁ gure the Test Double. Not surprisingly, the primary considerations when making this deci- sion are the understandability of the test and the potential reusability of the Test Double code.\n\nFake Objects do not need to be “conﬁ gured” at runtime because they are just used by the SUT; later outputs depend on the earlier calls by the SUT. Similarly, Dummy Objects do not need to be “conﬁ gured” because they should never be\n\nwww.it-ebooks.info\n\n141\n\n142\n\nChapter 11 Using Test Doubles\n\nexecuted.4 Procedural Test Stubs are typically built as Hard-Coded Test Doubles. That is, they are hard-coded to return a particular value when the function is called—thus they are the simplest form of Test Double.\n\nExpectations, Expectations, Return Values Return Values\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nConfiguration Configuration\n\nInstallation Installation\n\nExpectations Expectations\n\nExercise Exercise\n\nSUT SUT\n\nReturn Return Values Values\n\nVerify Verify\n\nTeardown Teardown\n\nTest Test Double Double\n\nFigure 11.9 A Test Double being conﬁ gured by the test. We can avoid a proliferation of Hard-Coded Test Doubles classes by passing return values or expectation to the Conﬁ gurable Test Double at runtime.\n\nA Conﬁ gurable Test Double can provide either a Conﬁ guration Interface (see Conﬁ gurable Test Double) or a Conﬁ guration Mode (see Conﬁ gurable Test Double) that the test can use to conﬁ gure the Test Double with the values to return or expect. As a consequence, Conﬁ gurable Test Doubles are reusable across many tests. Use of these Conﬁ gurable Test Doubles also makes tests more understandable because the values used by the Test Double are visible within the test, thus avoiding the smell of a Mystery Guest (see Obscure Test on page 186).\n\nSo where should this conﬁ guration take place? The installation of the Test Double should be treated just like any other part of ﬁ xture setup. Alternatives such as In-line Setup (page 408), Implicit Setup (page 424), and Delegated Setup (page 411) are all available.\n\n4 A Dummy Object can be used as an observation point to verify that it was never used by ensuring that the Dummy Object throws an exception if any of its methods are called.\n\nwww.it-ebooks.info\n\nTesting with Doubles\n\nInstalling the Test Double\n\nBefore we exercise the SUT, we need to “install” any Test Doubles on which our test depends. The term “install” here serves as a generic way to describe the process of telling the SUT to use our Test Double, regardless of the exact details regarding how we do it. The normal sequence is to instantiate the Test Double, conﬁ gure it if it is a Conﬁ gurable Test Double, and then tell the SUT to use the Test Double either before or as we exercise the SUT. There are several distinct ways to “install” the Test Double, and the choice between them may be as much a matter of style as of necessity if we are designing the SUT for testability. Our choices may be much more constrained, however, when we try to retroﬁ t our tests to an existing design.\n\nThe basic choices boil down to Dependency Injection (page 678), in which the client software tells the SUT which DOC to use; Dependency Lookup (page 686), in which the SUT delegates the construction or retrieval of the DOC to another object; and Test Hook, in which the DOC or the calls to it within the SUT are modiﬁ ed.\n\nIf an inversion of control framework is available in our language, our tests can substitute dependencies without much additional work on our part. This removes the need for building in the Dependency Injection or Dependency Lookup mechanism.\n\nDependency Injection\n\nDependency Injection is a class of design decoupling in which the client tells the SUT which DOC to use at runtime (Figure 11.10). The test-driven development (TDD) movement has greatly increased its popularity because Dependency Injec- tion makes for more easily tested designs. This pattern also makes it possible to reuse the SUT more broadly because it removes knowledge of the dependency from the SUT; often the SUT will be aware of only a generic interface that the DOC must implement. Dependency Injection comes in several speciﬁ c ﬂ avors, with the choice between them being largely a matter of taste:\n\nSetter Injection (see Dependency Injection): The SUT accesses the DOC through a public attribute (i.e., a variable or property). The test explicitly sets the attribute after instantiating the SUT to installing the Test Double. The SUT may have previously initialized the attribute with the real DOC in its constructor (in which case the test is replac- ing it) or the SUT may use Lazy Initialization [SBPP] to initialize the attribute (in which case the SUT will not bother to install the real DOC).\n\nwww.it-ebooks.info\n\n143\n\n144\n\nChapter 11 Using Test Doubles\n\nClient Client\n\nCreation Creation\n\nDOC DOC\n\nUsage Usage\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nExercise Exercise\n\nCreation Creation\n\nSUT SUT\n\nUsage Usage\n\nTest Test Double Double\n\nFigure 11.10 A Test Double being “injected” into the SUT by a test. Using Test Doubles requires a means to replace the DOC. Using Dependency Injection involves having the caller supply the dependency to the SUT before or as it is used.\n\nConstructor Injection (see Dependency Injection): The SUT accesses the DOC through a private attribute. The test passes the Test Dou- ble to the SUT via a constructor that takes the DOC to be used as an explicit argument and initializes the attribute from it. This may be the primary constructor used by production code clients or it may be an alternative constructor. In the latter case, the primary constructor should call this constructor, passing the default DOC to it as an argument.\n\nParameter Injection (see Dependency Injection): The SUT receives the DOC as a method parameter. The test passes in a Test Double, whereas the production code passes in the real object.5 This approach works well when the API of the SUT takes as a parameter the object we need to replace. Although Mock Object aﬁ cionados might argue that designing APIs in this way improves the design of the SUT, it is not always possible or practical to pass everything required to each method.\n\nDependency Lookup\n\nWhen software is not designed for testability or when Dependency Injection is not appropriate, we may ﬁ nd it convenient to use Dependency Lookup. This pattern also removes the knowledge of exactly which DOC should be used from\n\n5 This approach was advocated in the original paper on Mock Objects [ET]. In this paper, Mock Objects passed as parameters to methods are called “Smart Handlers.”\n\nwww.it-ebooks.info\n\nTesting with Doubles\n\nthe SUT, but it does so by having the SUT ask another piece of software to create or ﬁ nd the DOC on its behalf (Figure 11.11). This opens the door to changing the DOC at runtime without modifying the SUT’s code. We do have to modify the behavior of the intermediary somehow, and this is where the speciﬁ c variants of Dependency Lookup differ from one another:\n\nConfiguration Configuration with Test Double with Test Double\n\nSetup Setup\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nFind or Create Find or Create\n\nCreation Creation\n\nor or\n\nDOC DOC\n\nTeardown Teardown\n\nCreation Creation\n\nClient Client\n\nUsage Usage\n\nSUT SUT\n\nUsage Usage\n\nUsage Usage\n\nTest Test Double Double\n\nFigure 11.11 A Service Locator being “conﬁ gured” by a test to return a Test Double to the SUT. Using Test Doubles requires a means to replace the DOC. Using Dependency Lookup involves having the SUT ask a well-known object to provide a reference to the DOC; the test can provide the Service Locator with a Test Double to return.\n\nObject Factory (see Dependency Lookup): The SUT creates the DOC by calling a Factory Method [GOF] on a well-known object instead of using an object constructor to create the DOC directly. The test explic- itly tells the Object Factory to create a Test Double instead of a normal DOC whenever this method is called .\n\nService Locator (see Dependency Lookup): The SUT retrieves a previ- ously created service object by asking a well-known Registry [PEAA] object for it. The test conﬁ gures the Service Locator to return the Test Double when the SUT requests the DOC.\n\nThe line between these two patterns can become quite blurry when we use Lazy Initialization to create the object being returned by a Service Locator. Should it be called an Object Factory instead? Does it really matter which label we apply? Probably not—hence the generic name of Dependency Lookup.\n\nwww.it-ebooks.info\n\n145\n\n146\n\nChapter 11 Using Test Doubles\n\nRetroﬁ tting Testability Using a Test-Speciﬁ c Subclass\n\nEven when none of these mechanisms is built into the SUT, we may be able to retroﬁ t them relatively easily by using a Test-Speciﬁ c Subclass.\n\nThe use of Singletons [GOF] speciﬁ cally to act as an Object Factory or Service Locator is common. If the Singleton has hard-coded behavior, we may have to turn it into a Substitutable Singleton (see Test-Speciﬁ c Subclass on page 579) to enable overriding the normally returned DOC with our Test Double. The use of Singletons can be avoided through the use of an IOC tool or a manually coded Dependency Injection mechanism. Both of these choices are preferable because they make the test’s dependency on a Test Double more obvious. Singletons used for other purposes almost always cause headaches when we are writing tests and should be avoided if possible.\n\nOur test can instantiate a Test-Speciﬁ c Subclass of the SUT to add a Depen- dency Injection mechanism or to replace other methods of the SUT with test-spe- ciﬁ c behavior; see Figure 11.12. We can override any logic used to access a DOC, thereby making it possible to return a Test Double instead of the normal DOC without modifying the production code. We can also replace the implementations of any methods being called from the method we are testing with Test Stub-like behavior, thereby turning the SUT into its own Subclassed Test Double (see Test- Speciﬁ c Subclass). This is one way to inject indirect inputs into the SUT.\n\nSUT SUT\n\nExercise Exercise\n\nMethod Under Test Method Under Test\n\nSetup Setup\n\nCreate Create\n\nInternal Method Internal Method\n\nExercise Exercise\n\nVerify Verify\n\nSet State Set State\n\nGet State Get State\n\nTest- Test- Specific Specific Subclass Subclass\n\nInternal Method Internal Method\n\nOverridden Overridden Self Call Self Call\n\nTeardown Teardown\n\nFigure 11.12 Using a Test-Speciﬁ c Subclass of the SUT. When all else fails, we can always try subclassing the SUT to change or expose functionality we need to enable testing\n\nwww.it-ebooks.info\n\nTesting with Doubles\n\nThe main prerequisite of using a Test-Speciﬁ c Subclass of the SUT is that the SUT must use Self-Calls [WWW] to nonprivate methods that implement any functionality we need to override from the test. Small, single-purpose methods rule! The main drawback of this approach is that it is possible to accidentally override parts of the behavior we are intending to test.\n\nWe can also subclass the DOC to insert test-speciﬁ c behavior, effectively turning it into a Subclassed Test Double (Figure 11.13). This strategy is some- what safer than subclassing the SUT because it avoids the possibility of acci- dentally overriding those parts of the SUT that we are testing. The trick, however, is to get the SUT to use the Test-Speciﬁ c Subclass instead of the DOC. In practice, this implies that we must use one of the Dependency Injection or Dependency Lookup techniques, unless the DOC is a Singleton. When the SUT uses a Singleton by calling a static soleInstance method on a hard-coded class name, the test can cause the soleInstance method to return an instance of a Test Double by subclassing the Singleton class and initializing the real Singleton’s soleInstance class variable to hold an instance of the Test Double. The returned Test Double may need to be a Subclassed Test Double if the type of the vari- able used to hold the Singleton’s sole instance is hard-coded as the Singleton’s class. Although we often use this technique to get a Service Locator to return a different service, but we can also use a Subclassed Test Double directly with- out an intermediary Service Locator.\n\nFixture Fixture\n\nSetup Setup\n\nDOC DOC\n\nExercise Exercise\n\nVerify Verify\n\nTeardown Teardown\n\nSUT SUT\n\nSub- Sub- classed classed Test Test Double Double\n\nFigure 11.13 Using A Test Double subclassed from the DOC. One way to build a Test Double is to subclass the real class and override the implementation of any methods we need to control the indirect inputs or verify indirect outputs.\n\nwww.it-ebooks.info\n\n147\n\n148\n\nChapter 11 Using Test Doubles\n\nOther Ways of Retroﬁ tting Testability\n\nAll is not lost when none of the techniques described thus far can be used to introduce testability. We still have a few tricks left up our sleeves.\n\nTest Hooks are the “elephant in the room” that no one wants to talk about because they may lead to Test Logic in Production. Test Hooks, however, are a perfectly legitimate way to get legacy code under test when it is too hard or dangerous to introduce one of the techniques described earlier. They are best used as a “transition” strategy to allow Scripted Tests (page 285) or Recorded Tests (page 278) to be automated to provide a Safety Net (see page 24) while large-scale refactoring is undertaken to improve testability. Ideally, once the code has been made more testable, better tests can be prepared using the tech- niques described earlier and the Test Hooks can be removed.\n\nMichael Feathers [WEwLC] has described several other techniques to replace dependencies with test-speciﬁ c code under the general heading of ﬁ nd- ing “object seams.” For example, we can replace a depended-on library with a library designed speciﬁ cally for testing. A seemingly hard-coded dependency can be broken this way. Most of these techniques are less applicable when we need to dynamically replace dependencies within individual tests than either Dependency Injection or Dependency Lookup because they require changes to the environment. Object seams are, however, an excellent way to place legacy code under test so that it can be refactored to introduce either of the previously mentioned dependency-breaking techniques.\n\nWe can use aspect-oriented programming (AOP) to install the Test Double behavior by deﬁ ning a test point-cut that matches the place where the SUT calls the DOC and we would rather have it call the Test Double. Although we need an AOP-enabled development environment to do this, we do not need to deploy the AOP-generated code into a production environ- ment. As a consequence, this technique may be used even in AOP-hostile environments.\n\nOther Uses of Test Doubles\n\nSo far, we have covered the testing of indirect inputs and indirect outputs. Now let’s look at some other uses of Test Doubles.\n\nwww.it-ebooks.info\n\nOther Uses of Test Doubles\n\nEndoscopic Testing\n\nTim Mackinnon et al. introduced the concept of endoscopic testing [ET] in their initial Mock Objects paper. Endoscopic testing focuses on testing the SUT from the inside by passing in a Mock Object as an argument to the method under test. This allows veriﬁ cation of certain internal behaviors of the SUT that may not always be visible from the outside.\n\nThe classic example that Mackinnon and colleagues cite is the use of a mock collection class preloaded with all of the expected members of the collection. When the SUT tries to add an unexpected member, the mock collection’s asser- tion fails. The full stack trace of the internal call stack then becomes visible in the xUnit failure report. If our IDE supports breaking on speciﬁ ed exceptions, we can also inspect the local variables at the point of failure.\n\nNeed-Driven Development\n\nA reﬁ nement of endoscopic testing is “need-driven development” [MRNO], in which the dependencies of the SUT are deﬁ ned as the tests are written. This “outside-in” approach to writing and testing software combines the conceptual elegance of the traditional “top-down” approach to writing code with modern TDD techniques supported by Mock Objects. It allows us to build and test the software layer by layer, starting at the outermost layer before we have imple- mented the lower layers.\n\nNeed-driven development combines the beneﬁ ts of test-driven development (specifying all software with tests before we build them) with a highly incre- mental approach to design that removes the need for any speculation about how a depended-on class might be used.\n\nSpeeding Up Fixture Setup\n\nAnother application of Test Doubles is to reduce the runtime cost of Fresh Fix- ture (page 311) setup. When the SUT needs to interact with other objects that are difﬁ cult to create because they have many dependencies, a single Test Dou- ble can be created instead of the complex network of objects. When applied to networks of entity objects, this technique is called Entity Chain Snipping (see Test Stub).\n\nwww.it-ebooks.info\n\n149\n\n150\n\nChapter 11 Using Test Doubles\n\nSpeeding Up Test Execution\n\nTest Doubles may also be used to speed up tests by replacing slow compo- nents with faster ones. Replacing a relational database with an in-memory Fake Object, for example, can reduce test execution times by an order of magnitude! The extra effort required to code the Fake Database is more than offset by the re- duced waiting time and the quality improvement due to the more timely feedback that comes from running the tests more frequently. Refer to the sidebar “Faster Tests without Shared Fixtures” on page 319 for a more detailed discussion of this issue.\n\nOther Considerations\n\nBecause many of our tests will involve replacing a real DOC with a Test Double, how do we know that the production code will work properly when it uses the real DOC? Of course, we would expect our customer tests to verify behavior with the real DOCs in place (except, possibly, when the real DOCs are interfaces to other systems that need to be stubbed out during single-system testing). We should write a special form of Constructor Test (see Test Method)— a “substitutable initialization test”—to verify that the real DOC is installed properly. The trigger for writing this test is performing the ﬁ rst test that replaces the DOC with a Test Double—that point is often when the Test Double installation mechanism is introduced.\n\nFinally, we want to be careful that we don’t fall into the “new hammer trap.”6 Overuse of Test Doubles (and especially Mock Objects or Test Stubs) can lead to Overspeciﬁ ed Software (see Fragile Test on page 239) by encoding implementation-speciﬁ c information about the design in our tests. The design may be then much more difﬁ cult to change if many tests are affected by the change simply because they use a Test Double that has been affected by the design change.\n\n6 “When you have a new hammer, everything looks like a nail.”\n\nwww.it-ebooks.info\n\nWhat’s Next?\n\nWhat’s Next?\n\nIn this chapter, we examined techniques for testing software with indirect inputs and indirect outputs. In particular, we explored the concept of Test Doubles and various techniques for installing them. In Chapter 12, Organizing Our Tests, we will turn our attention to strategies for organizing the test code into Test Methods and Test Utility Methods (page 599) implemented on Testcase Classes (page 373) and Test Helpers (page 643).\n\nwww.it-ebooks.info\n\n151\n\nThis page intentionally left blank\n\nwww.it-ebooks.info",
      "page_number": 188
    },
    {
      "number": 12,
      "title": "Organizing Our Tests",
      "start_page": 216,
      "end_page": 229,
      "detection_method": "regex_chapter",
      "content": "Chapter 12\n\nOrganizing Our Tests\n\nAbout This Chapter\n\nIn the chapters concluding with Chapter 11, Using Test Doubles, we looked at various techniques for interacting with the SUT for the purpose of verifying its behavior. In this chapter, we turn our attention to the question of how to orga- nize the test code to make it easy to ﬁ nd and understand.\n\nThe basic unit of test code organization is the Test Method (page 348). Deciding what to put in the Test Method and where to put it is central to the topic of test organization. When we have only a few tests, how we organize them isn’t terribly important. By contrast, when we have hundreds of tests, test organization becomes a critical factor in keeping our tests easy to understand and ﬁ nd.\n\nThis chapter begins by discussing what we should and should not include in a Test Method. Next, it explores how we can decide on which Testcase Classes (page 373) to put our Test Methods. Test naming depends heavily on how we have organized our tests, so we will talk about this issue next. We will then consider how to organize the Testcase Classes into test suites and where to put test code. The ﬁ nal topic is test code reuse—speciﬁ cally, where to put reusable test code.\n\nBasic xUnit Mechanisms\n\nThe xUnit family of Test Automation Frameworks (page 298) provides a num- ber of features to help us organize our tests. The basic question, “Where do I code my tests?”, is answered by putting our test code into a Test Method on a Testcase Class. We then use either Test Discovery (page 393) or Test Enumera- tion (page 399) to create a Test Suite Object (page 387) containing all the tests from the Testcase Class. The Test Runner (page 377) invokes a method on the Test Suite Object to run all the Test Methods.\n\n153\n\nwww.it-ebooks.info\n\n154\n\nChapter 12 Organizing Our Tests\n\nRight-Sizing Test Methods\n\nA test condition is something we need to prove the SUT really does; it can be described in terms of what the starting state of the SUT is, how we exercise the SUT, how we expect the SUT to respond, and what the ending state of the SUT is expected to be. A Test Method is a sequence of statements in our test scripting language that exercises one or more test conditions (Figure 12.1). What should we include in a single Test Method?\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nTest Test\n\nSetup Setup\n\nExercise Exercise\n\nExercise Exercise\n\nFixture Fixture\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nSuite Suite Object Object\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nFigure 12.1 The four phases of a typical test. Each Test Method implements a Four-Phase Test (page 358) that ideally veriﬁ es a single test condition. Not all phases of the Four-Phase Test need be in the Test Method.\n\nMany xUnit purists prefer to Verify One Condition per Test (see page 45) because it gives them good Defect Localization (see page 22). That is, when a test fails, they know exactly what is wrong in the SUT because each test veriﬁ es exactly one test condition. This is very much in contrast with manual testing, where one tends to build long, involved multiple-condition tests because of the overhead involved in setting up each test’s pre-conditions. When creating xUnit- based automated tests, we have many ways of dealing with this frequently re- peated ﬁ xture setup (as described in Chapter 8, Transient Fixture Management), so we tend to Verify One Condition per Test. We call a test that veriﬁ es too many test conditions an Eager Test (see Assertion Roulette on page 224) and consider it a code smell.\n\nA test that veriﬁ es a single test condition executes a single code path through the SUT and it should execute exactly the same path each time it runs; that is what makes it a Repeatable Test (see page 26). Yes, that means we need as\n\nwww.it-ebooks.info\n\nTest Methods and Testcase Classes\n\nmany test methods as we have paths through the code—but how else can we expect to achieve full code coverage? What makes this pattern manageable is that we Isolate the SUT (see page 43) when we write unit tests for each class so we only have to focus on paths through a single object. Also, because each test should verify only a single path through the code, each test method should con- sist of strictly sequential statements that describe what should happen on that one path.1 Another reason we Verify One Condition per Test (see page 45) is to Minimize Test Overlap (see page 44) so that we have fewer tests to modify if we later modify the behavior of the SUT.\n\nBrian Marrick has developed an interesting compromise that I call “While We’re at It,”2 which leverages the test ﬁ xture we already have set up to run some additional checks and assertions. Marrick clearly marks these elements with comments to indicate that if changes to the SUT obsolete that part of the test, they can be easily deleted. This strategy minimizes the effort needed to maintain the extra test code.\n\nTest Methods and Testcase Classes\n\nA Test Method needs to live on a Testcase Class. Should we put all our Test Methods onto a single Testcase Class for the application? Or should we create a Testcase Class for each Test Method? Of course, the right answer lies somewhere between these two extremes, and it will change over the life of our project.\n\nTestcase Class per Class\n\nWhen we write our ﬁ rst few Test Methods, we can put them all onto a single Testcase Class. As the number of Test Methods increases, we will likely want to split the Testcase Class so that one Testcase Class per Class (page 617) is tested, which reduces the number of Test Methods per class (Figure 12.2). As those Testcase Classes get too big, we usually split the classes further. In that case, we need to decide which Test Methods to include in each Testcase Class.\n\n1 A Test Method that contains Conditional Test Logic (page 200) is a sign of a test trying to accommodate different circumstances because it does not have control of all indirect inputs of the SUT or because it is trying to verify complex expected states on an in-line basis within the Test Method. 2 He calls it “Just for Laughs” but I don’t ﬁ nd that name very intent-revealing.\n\nwww.it-ebooks.info\n\n155\n\n156\n\nChapter 12 Organizing Our Tests\n\nTestcaseClass TestcaseClass\n\nCreation Creation\n\ntestMethod_A_1 testMethod_A_1\n\nFixture A Fixture A\n\ntestMethod_A_2 testMethod_A_2\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nExercise Exercise\n\nfeature_2 feature_2\n\ntestMethod_B_1 testMethod_B_1\n\ntestMethod_B_2 testMethod_B_2\n\nCreation Creation\n\nFixture B Fixture B\n\nFigure 12.2 A production class with a single Testcase Class. With the Testcase Class per Class pattern, a single Testcase Class holds all the Test Methods for all the behavior of our SUT class. Each Test Method may need to create a different ﬁ xture either in-line or by delegating that task to a Creation Method (page 415).\n\nTestcase Class per Feature\n\nOne school of thought is to put all Test Methods that verify a particular feature of the SUT—where a “feature” is deﬁ ned as one or more methods and attributes that collectively implement some capability of the SUT—into a single Testcase Class (Figure 12.3). This makes it easy to see all test conditions for that feature. (Use of appropriate Test Naming Conventions helps achieve this clarity.) It can, however, result in similar ﬁ xture setup code being required in each Testcase Class.\n\nTestcase Class per Fixture\n\nThe opposing view is that one should group all Test Methods that require the same test ﬁ xture (same pre-conditions) into one Testcase Class per Fixture (page 631; see Figure 12.4). This facilitates putting the test ﬁ xture setup code into the setUp method (Implicit Setup; see page 424) but can result in scattering of the test conditions for each feature across many Testcase Classes.\n\nwww.it-ebooks.info\n\nTest Methods and Testcase Classes\n\nFeature1TestcaseClass Feature1TestcaseClass\n\nCreation Creation\n\ntestMethod_A testMethod_A\n\nFixture A Fixture A\n\ntestMethod_B testMethod_B\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nFeature2TestcaseClass Feature2TestcaseClass\n\nExercise Exercise\n\nfeature_2 feature_2\n\ntestMethod_A testMethod_A\n\ntestMethod_B testMethod_B\n\nCreation Creation\n\nFixture B Fixture B\n\nFigure 12.3 A production class with one Testcase Class for each feature. With the Testcase Class per Feature pattern, we have one Testcase Class for each major capability or feature supported by our SUT class. The Test Methods on that test class exercise various aspects of that feature after building whatever test ﬁ xture they require.\n\nFixtureATestcaseClass FixtureATestcaseClass FixtureATestcaseClass FixtureATestcaseClass\n\nsetUp setUp\n\ntestMethod_1 testMethod_1\n\nCreation Creation\n\nFixture A Fixture A\n\ntestMethod_2 testMethod_2\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nFixtureBTestcaseClass FixtureBTestcaseClass\n\nExercise Exercise\n\nfeature_2 feature_2\n\nsetUp setUp\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nCreation Creation\n\nFixture B Fixture B\n\nFigure 12.4 A production class with one Testcase Class for each ﬁ xture. With the Testcase Class per Fixture pattern, we have one Testcase Class for each possible test ﬁ xture (test pre-condition) of our SUT class. The Test Methods on that test class exercise various features from the common starting point.\n\nwww.it-ebooks.info\n\n157\n\n158\n\nChapter 12 Organizing Our Tests\n\nChoosing a Test Method Organization Strategy\n\nClearly, there is no single “best practice” we can always follow; the best prac- tice is the one that is most appropriate for the particular circumstance. Testcase Class per Fixture is commonly used when we are writing unit tests for stateful objects and each method needs to be tested in each state of the object. Testcase Class per Feature (page 624) is more appropriate when we are writing customer tests against a Service Facade [CJ2EEP]; it enables us to keep all the tests for a customer-recognizable feature together. This pattern is also more commonly used when we rely on a Prebuilt Fixture (page 429) because ﬁ xture setup logic is not required in each test. When each test needs a slightly different ﬁ xture, the right answer may be to select the Testcase Class per Feature pattern and use a Delegated Setup (page 411) to facilitate setting up the ﬁ xtures.\n\nTest Naming Conventions\n\nThe names we give to our Testcase Classes and Test Methods are crucial in mak- ing our tests easy to ﬁ nd and understand. We can make the test coverage more obvious by naming each Test Method systematically based on which test condi- tion it veriﬁ es. Regardless of which test method organization scheme we use, we would like the combination of the names of the test package, the Testcase Class, and the Test Method to convey at least the following information:\n\nThe name of the SUT class\n\nThe name of the method or feature being exercised\n\nThe important characteristics of any input values related to the exercising\n\nof the SUT\n\nAnything relevant about the state of the SUT or its dependencies\n\nThese items are the “input” part of the test condition. Obviously, this is a lot to communicate in just two names but the reward is high if we can achieve it: We can tell exactly what test conditions we have tests for merely by looking at the names of the classes and methods in an outline view of our IDE. Figure 12.5 provides an example.\n\nwww.it-ebooks.info\n\nTest Naming Conventions\n\nFigure 12.5 A production class with one Testcase Class for each test ﬁ xture. When we use the Testcase Class per Fixture pattern, the class name can describe the ﬁ xture, leaving the method name available for describing the inputs and expected outputs.\n\nFigure 12.5 also shows how useful it is to include the “expectations” side of the test condition:\n\nThe outputs (responses) expected when exercising the SUT\n\nThe expected post-exercise state of the SUT and its dependencies\n\nThis information can be included in the name of the Test Method preﬁ xed by “should.” If this nomenclature makes the names too long,3 we can always access the expected outcome by looking at the body of the Test Method.\n\n3 Many xUnit variants “encourage” us to start all our Test Method names with “test” so that these methods can be automatically detected and added to the Test Suite Object. This constrains our naming somewhat compared to variants that indicate test methods via method attributes or annotations.\n\nwww.it-ebooks.info\n\n159\n\n160\n\nChapter 12 Organizing Our Tests\n\nOrganizing Test Suites\n\nThe Testcase Class acts as a Test Suite Factory (see Test Enumeration) when it returns a Test Suite Object containing a collection of Testcase Objects (page 382), each representing a Test Method (Figure 12.6). This is the default organization mechanism provided by xUnit. Most Test Runners allow any class to act as a Test Suite Factory by implementing a Factory Method [GOF], which is typi- cally called suite.\n\nTestcase Class Testcase Class\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nFixture Fixture\n\ntestMethod_1 testMethod_1\n\nImplicit tearDown Implicit tearDown\n\nTest Test Suite Suite Factory Factory\n\nCreation Creation\n\nTest Test Suite Suite Object Object\n\nSUT SUT\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\ntestMethod_n testMethod_n\n\nImplicit tearDown Implicit tearDown\n\nFigure 12.6 A Testcase Class acting as a Test Suite Factory. By default, the Testcase Class acts as a Test Suite Factory to produce the Test Suite Object that the Test Runner requires to execute our tests. We can also enumerate a speciﬁ c set of tests we want to run by providing a Test Suite Factory that returns a Test Suite Object containing only the desired tests.\n\nRunning Groups of Tests\n\nWe often want to run groups of tests (i.e., a test suite) but we don’t want this decision to constrain how we organize them. A popular convention is to create a special Test Suite Factory called AllTests for each package of tests. We don’t need to stop there, however: We can create Named Test Suites (page 592) for any collection of tests we want to run together. A good example is a Subset Suite (see Named Test Suite) that allows us to run just those tests that need software\n\nwww.it-ebooks.info\n\nOrganizing Test Suites\n\ndeployed to the Web server (or not deployed to the Web server!). We usually have at least a Subset Suite for all the unit tests and another Subset Suite for just the customer tests (they often take a long time to execute). Some variants of xUnit support Test Selection (page 403), which we can use instead of deﬁ ning Subset Suites.\n\nSuch runtime groupings of tests often reﬂ ect the environment in which they need to run. For example, we might have one Subset Suite that includes all tests that can be run without the database and another Subset Suite that includes all tests that depend on the database. Likewise, we might have separate Subset Suites for tests that do, and do not, rely on the Web server. If our test package includes these various kinds of test suites, we can deﬁ ne AllTests as a Suite of Suites (see Test Suite Object) composed of these Subset Suites. Then any test that is added to one of the Subset Suites will also be run in AllTests without incurring extra test maintenance effort.\n\nRunning a Single Test\n\nSuppose a Test Method fails in our Testcase Class. We decide to put a break- point on a particular method—but that method is called in every test. Our ﬁ rst reaction might be to just muddle through by clicking “Go” each time the breakpoint is hit until we are being called from the test of interest. One possibility is to disable (by commenting out) the other Test Methods so they are not run. Another option is to rename the other Test Methods so that the xUnit Test Discovery mechanism will not recognize them as tests. In variants of xUnit that use method attributes or annotations, we can add the “Ignore” attribute to a test method instead. Each of these approaches introduces the potential problem of a Lost Test (see Production Bugs on page 268), although the “Ignore” approach does remind us that some tests are being ignored. In members of the xUnit family that provide a Test Tree Explorer (see Test Run- ner), we can simply select a single test to be run from the hierarchy view of the test suite, as shown in Figure 12.7.\n\nWhen none of these options is available, we can use a Test Suite Factory to run a single test. Wait a minute! Aren’t test suites all about running groups of tests that live in different Testcase Classes? Well, yes, but that doesn’t mean we can’t use them for other purposes. We can deﬁ ne a Single Test Suite4 (see Named Test Suite) that runs a particular test. To do so, we call the constructor of the Testcase Class with the speciﬁ c Test Method’s name as an argument.\n\n4 I usually call it MyTest.\n\nwww.it-ebooks.info\n\n161\n\n162\n\nChapter 12 Organizing Our Tests\n\nFigure 12.7 A Test Tree Explorer showing the structure of the tests in our suite. We can use the Test Tree Explorer to drill down into the runtime structure of the test suite and run individual tests or subsuites.\n\nTest Code Reuse\n\nTest Code Duplication (page 213) can signiﬁ cantly increase the cost of writing and maintaining tests. Luckily, a number of techniques for reusing test logic are available to us. The most important consideration is that any reuse not compromise the value of the Tests as Documentation (see page 23). I don’t recommend reuse of the actual Test Method in different circumstances (e.g., with different ﬁ xtures), as this kind of reuse is typically a sign of a Flexible Test (see Conditional Test Logic on page 200) that tests different things in dif- ferent circumstances. Most test code reuse is achieved either through Implicit Setup or Test Utility Methods (page 599). The major exception is the reuse of Test Doubles (page 522) by many tests; we can treat these Test Double classes as a special kind of Test Helper (page 643) when thinking about where to put them.\n\nwww.it-ebooks.info\n\nTest Code Reuse\n\nTest Utility Method Locations\n\nTestcase Testcase Superclass Superclass\n\nTest Utility Test Utility Method Method\n\nFixture Fixture\n\nSUT SUT\n\ntestMethod_1 testMethod_1\n\nTest Helper Test Helper\n\ntestMethod_n testMethod_n\n\nTest Utility Test Utility Method Method\n\nTest Utility Test Utility Method Method\n\nTestcase Testcase Class Class\n\nFigure 12.8 The various places we can put Test Utility Methods. The primary decision-making criterion is the desired scope of reusability of the Test Methods.\n\nMany variants of xUnit provide a special Testcase Superclass (page 638)—typically called “TestCase”—from which all Testcase Classes should (and, in some cases, must) inherit either directly or indirectly (Figure 12.8). If we have useful utility methods on our Testcase Class that we want to reuse in other Testcase Classes, we may ﬁ nd it helpful to create one or more Testcase Superclasses from which to inherit instead of “TestCase.” If we take this step, we need to be careful if those methods need to see types or classes that reside in various packages within the SUT—our root Testcase Superclass should not depend on those types or classes directly, as that is likely to result in a cyclical dependency graph. We may be able to create a Testcase Superclass for each test package to keep our test class de- pendencies noncyclic. The alternative is to create a Test Helper for each domain package and put the various Test Helpers in the appropriate test packages. This way, a Testcase Class is not forced to choose a single Testcase Superclass; it can merely “use” the appropriate Test Helpers.\n\nTestCase Inheritance and Reuse\n\nThe most commonly used reason for inheriting methods from a Testcase Super- class is to access Test Utility Methods. Another use is when testing frameworks\n\nwww.it-ebooks.info\n\n163\n\n164\n\nChapter 12 Organizing Our Tests\n\nand their plug-ins; it can be useful to create a conformance test that speciﬁ es the general behavior of the plug-in via a Template Method [GOF] that calls meth- ods provided by a subclass speciﬁ c to the kind of plug-in being tested to check speciﬁ c details of the plug-in. This scenario is rare enough that I won’t describe it further here; please refer to [FaT] for a more complete description.\n\nTest File Organization\n\nNow we face a new question: Where should we put our Testcase Classes? Obviously, these classes should be stored in the source code repository [SCM] along with the production code. Beyond that criterion, we have quite a range of choices. The test packaging strategy we choose will very much depend on our environment—many IDEs include constraints that make certain strate- gies unworkable. The key issue is to Keep Test Logic Out of Production Code (see page 45) and yet to be able to ﬁ nd the corresponding test for each piece of code or functionality.\n\nBuilt-in Self-Test\n\nWith a built-in self-test, the tests are included with the production code and can be run at any time. No provision is made for keeping them separate. Many orga- nizations want to Keep Test Logic Out of Production Code so built-in self-tests may not be a good option for them. This consideration is particularly important in memory-constrained environments where we don’t want test code taking up valuable space.\n\nSome development environments encourage us to keep the tests and the pro- duction code together. For example, SAP’s ABAP Unit supports the keyword “For Testing,” which tells the system to disable the tests when the code is trans- ported into the production environment.\n\nTest Packages\n\nIf we decide to put the Testcase Classes into separate test packages, we can organize them in several ways. We can keep the tests separate by putting them into one or more test packages while keeping them in the same source tree, or we can put the tests into the same logical package but physically store them in a parallel source tree. The latter approach is frequently used in Java because it avoids the problem of tests not being able to see “package-protected” methods\n\nwww.it-ebooks.info\n\nWhat’s Next?\n\non the SUT.5 Some IDEs may reject using this approach by insisting that a pack- age be wholly contained within a single folder or project. When we use test packages under each production code package, we may need to use a build-time test stripper to exclude them from production builds.\n\nTest Dependencies\n\nHowever we decide to store and manage the source code, we need to ensure that we eliminate any Test Dependency in Production (see Test Logic in Production on page 217) because even a test stripper cannot remove the tests if production code needs them to be present to run. This requirement makes paying attention to our class dependencies important. We also don’t want to have any Test Logic in Production because it means we aren’t testing the same code that we will eventu- ally run in production. This issue is discussed in more detail in Chapter 6, Test Automation Strategy.\n\nWhat’s Next?\n\nNow that we’ve looked at how to organize our test code, we should become familiar with a few more testing patterns. These patterns are introduced in Chapter 13, Testing with Databases.\n\n5 Java offers another way to get around the visibility issue: We can deﬁ ne our own test Security Manager to allow tests to access all methods on the SUT, not just the “package- protected” ones. This approach solves the problem in a general way but requires a good understanding of Java class loaders. Other languages may not have the equivalent functionality (or problem!).\n\nwww.it-ebooks.info\n\n165\n\nThis page intentionally left blank\n\nwww.it-ebooks.info",
      "page_number": 216
    },
    {
      "number": 13,
      "title": "Testing with Databases",
      "start_page": 230,
      "end_page": 237,
      "detection_method": "regex_chapter",
      "content": "Chapter 13\n\nTesting with Databases\n\nAbout This Chapter\n\nIn Chapter 12, Organizing Our Tests, we looked at techniques for organizing our test code. In this chapter, we explore the issues that arise when our appli- cation includes a database. Applications with databases present some special challenges when writing automated tests. Databases are much slower than the processors used in modern computers. As a result, tests that interact with databases tend to run much, much more slowly than tests that can run entirely in memory.\n\nEven ignoring the potential for Slow Tests (page 253), databases are a ripe source for many test smells in our automated test suites. Some of these smells are a direct consequence of the persistent nature of the database, while others result from our choice to share the ﬁ xture instance between tests. These smells were introduced in Chapter 9, Persistent Fixture Management. This chapter expands on them and provides a more focused treatment of testing with databases.\n\nTesting with Databases\n\nHere is my ﬁ rst, and most critical, piece of advice on this subject:\n\nWhen there is any way to test without a database, test without the database!\n\nThis seems like pretty strong advice but it is phrased this way for a reason. Data- bases introduce all sorts of complications into our applications and especially into our tests. Tests that require a database run, on average, two orders of magnitude slower than the same tests that run without a database.\n\n167\n\nwww.it-ebooks.info\n\n168\n\nChapter 13 Testing with Databases\n\nWhy Test with Databases?\n\nMany applications include a database to persist objects or data into longer-term storage. The database is a necessary part of the application, so verifying that the database is used properly is a necessary part of building the application. Therefore, the use of a Database Sandbox (page 650) to isolate developers and testers from production (and each other) is a fundamental practice on almost every project (Figure 13.1).\n\nDeveloper 1 Developer 1\n\nDeveloper 2 Developer 2\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nSUT SUT\n\nSUT SUT\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nFixture Fixture\n\nFixture Fixture\n\nDatabase Database\n\nDatabase Database\n\nFigure 13.1 A Database Sandbox for each developer. Sharing a Database Sandbox among developers is false economy. Would you make a plumber and an electrician work in the same wall at the same time?\n\nIssues with Databases\n\nA database introduces a number of issues that complicate test automation. Many of these issues relate to the fact that the ﬁ xture is persistent. These issues were introduced in Chapter 9, Persistent Fixture Management, and are summarized brieﬂ y here.\n\nPersistent Fixtures\n\nApplications with databases present some special challenges when we are writing automated tests. Databases are much slower than the processors used in modern computers. As a consequence, tests that interact with a database tend to run much more slowly than tests that can run entirely in memory. But even ignoring the Slow Tests issue, databases are a prime source of test smells in our automated test suites. Commonly encountered smells include Erratic\n\nwww.it-ebooks.info\n\nTesting without Databases\n\nTests (page 228) and Obscure Tests (page 186). Because the data in a database may potentially persist long after we run our test, we must pay special atten- tion to this data to avoid creating tests that can be run only once or tests that interact with one another. These Unrepeatable Tests (see Erratic Test) and Interacting Tests (see Erratic Test) are a direct consequence of the persistence of the test ﬁ xture and can result in more expensive maintenance of our tests as the application evolves.\n\nShared Fixtures\n\nPersistence of the ﬁ xture is one thing; choosing to share it is another. Deliberate sharing of the ﬁ xture can result in Lonely Tests (see Erratic Test) if some tests depend on other tests to set up the ﬁ xture for them—a situation called Chained Tests (page 454). If we haven’t provided each developer with his or her own Database Sandbox, we might spark a Test Run War (see Erratic Test) between developers. This problem arises when the tests being run from two or more Test Runners (page 377) interact by virtue of their accessing the same ﬁ xture objects in the shared database instance. Each of these behavior smells is a direct conse- quence of the decision to share the test ﬁ xture. The degree of persistence and the scope of ﬁ xture sharing directly affect the presence or absence of these smells.\n\nGeneral Fixtures\n\nAnother problem with tests that rely on databases is that databases tend to evolve into a large General Fixture (see Obscure Test) that many tests use for different pur- poses. This outcome is particularly likely when we use a Prebuilt Fixture (page 429) to avoid setting up the ﬁ xture in each test. It can also result from the decision to use a Standard Fixture (page 305) when we employ a Fresh Fixture (page 311) strategy. This approach makes it difﬁ cult to determine exactly what each test is specifying. In effect, the database appears as a Mystery Guest (see Obscure Test) in all of the tests.\n\nTesting without Databases\n\nModern layered software architecture [DDD, PEAA, WWW] opens up the pos- sibility of testing the business logic without using the database at all. We can test the business logic layer in isolation from the other layers of the system by using Layer Tests (page 337) and replacing the data access layer with a Test Double (page 522); see Figure 13.2.\n\nwww.it-ebooks.info\n\n169\n\n170\n\nChapter 13 Testing with Databases\n\nLayer1TestcaseClass Layer1TestcaseClass testMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nLayernTestcaseClass LayernTestcaseClass testMethod_1 testMethod_1 testMethod_2 testMethod_2\n\nLayer n Layer n\n\nLayer 1 Layer 1\n\nTest Double Test Double\n\nTest Double Test Double\n\nDOC DOC\n\nFigure 13.2 A pair of Layer Tests, each of which tests a different layer of the system. Layer Tests allow us to build each layer independently of the other layers. They are especially useful when the persistence layer can be replaced by a Test Double that reduces the Context Sensitivity (see Fragile Test on page 239) of the tests.\n\nIf our architecture is not sufﬁ ciently layered to allow for Layer Tests, we may still be able to test without a real database by using either a Fake Database (see Fake Object on page 551) or an In-Memory Database (see Fake Object). An In-Memory Database is a database but stores its tables in memory; this structure makes it run much faster than a disk-based database. A Fake Database isn’t really a database at all; it is a data access layer that merely pretends to be one. As a rule, it is easier to ensure independence of tests by using a Fake Database because we typically cre- ate a new one as part of our ﬁ xture setup logic, thereby implementing a Transient Fresh Fixture (see Fresh Fixture) strategy. Nevertheless, both of these strategies allow our tests to run at in-memory speeds, thereby avoiding Slow Tests. We don’t introduce too much knowledge of the SUT’s structure as long as we continue to write our tests as round-trip tests.\n\nReplacing the database with a Test Double works well as long as we use the database only as a data repository. Things get more interesting if we use any vendor-speciﬁ c functionality, such as sequence number generation or stored pro- cedures. Replacing the database then becomes a bit more challenging because it requires more attention to creating a design for testability. The general strategy is to encapsulate all database interaction within the data access layer. Where the\n\nwww.it-ebooks.info\n\nTesting the Database\n\ndata access layer provides data access functionality, we can simply delegate these duties to the “database object.” We must provide test-speciﬁ c implementations for any parts of the data access layer interface that implement the vendor-speciﬁ c functionality—a task for which a Test Stub (page 529) ﬁ ts the bill nicely.\n\nIf we are taking advantage of vendor-speciﬁ c database features such as sequence number generation, we will need to provide this functionality when executing the tests in memory. Typically, we will not need to substitute a Test Double for any functionality-related object because the functionality happens behind the scenes within the database. We can add this functionality into the in-memory version of the application using a Strategy [GOF] object, which by default is initialized to a null object [PLOPD3]. When run in production, the null object does nothing; when run in memory, the strategy object provides the missing functionality. As an added beneﬁ t, we will ﬁ nd it easier to change to a different database vendor once we have taken this step because the hooks to provide this functionality al- ready exist.1\n\nReplacing the database (or the data access layer) via an automated test implies that we have a way to instruct the SUT to use the replacement object. This is com- monly done in one of two ways: through direct Dependency Injection (page 678) or by ensuring that the business logic layer uses Dependency Lookup (page 686) to ﬁ nd the data access layer.\n\nTesting the Database\n\nAssuming we have found ways to test most of our software without using a database, then what? Does the need to test the database disappear? Of course not! We should ensure that the database functions correctly, just like any other soft- ware we write. We can, however, focus our testing of the database logic so as to reduce the number and kinds of tests we need to write. Because tests that involve the database will run much more slowly than our in-memory tests, we want to keep the number of these tests to the bare minimum.\n\nWhat kinds of database tests will we require? The answer to this question depends on how our application uses the database. If we have stored proce- dures, we should write unit tests to verify their logic. If a data access layer hides the database from the business logic, we should write tests for the data access functionality.\n\n1 Just one more example of how design for testability improves the design of our applications.\n\nwww.it-ebooks.info\n\n171\n\n172\n\nChapter 13 Testing with Databases\n\nTesting Stored Procedures\n\nWe can write tests for stored procedures in one of two ways. A Remote Stored Procedure Test (see Stored Procedure Test on page 654) is written in the same programming language and framework as we write all of our other unit tests. It accesses the stored procedure via the same invocation mechanism as used within the application logic (i.e., by some sort of Remote Proxy [GOF], Facade [GOF], or Command object [GOF]). Alternatively, we can write In-Database Stored Procedure Tests (see Stored Procedure Test) in the same language as the stored procedure itself; these tests will run inside the database (Figure 13.3). xUnit fam- ily members are available for several of the most common stored procedure languages; utPLSQL is just one example.\n\nApplication Environment Application Environment\n\nTestcase Class Testcase Class\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nStored Stored Procedure Procedure Proxy Proxy\n\nTestcase Class Testcase Class\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nDatabase Database\n\nStored Stored Procedure Procedure\n\nFigure 13.3 Testing a stored procedure using Self-Checking Tests (see page 26). There is great value in having automated regression test for stored procedures, but we must take care to make them repeatable and robust.\n\nTesting the Data Access Layer\n\nWe also want to write some unit tests for the data access layer. For the most part, these data access layer tests can be round-trip tests. Nevertheless, it is useful to have a few layer-crossing tests to ensure that we are putting information into the correct columns. This can be done using xUnit framework extensions for\n\nwww.it-ebooks.info\n\nTesting with Databases (Again!)\n\ndatabase testing (e.g., DbUnit for Java) to insert data directly into the database (for “Read” tests) or to verify the post-test contents of the database (for “Cre- ate/Update/Delete” tests).\n\nA useful trick for keeping our ﬁ xture from becoming persistent during data access layer testing is to use Transaction Rollback Teardown (page 668). To do so, we rely on the Humble Transaction Controller (see Humble Object on page 695) DFT pattern when constructing our data access layer. That is, the code that reads or writes the database should never commit a transaction; this allows the code to be exercised by a test that rolls back the transaction to pre- vent any of the changes made by the SUT from being applied.\n\nAnother way to tear down any changes made to the database during the ﬁ xture setup and exercise SUT phases of the test is Table Truncation Tear- down (page 661). This “brute force” technique for deleting data works only when each developer has his or her own Database Sandbox and we want to clear out all the data in one or more tables.\n\nEnsuring Developer Independence\n\nTesting the database means we need to have the real database available for running these tests. During this testing process, every developer needs to have his or her ownDatabase Sandbox. Trying to share a single sandbox among several or all developers is a false economy; the developers will simply end up tripping over one another and wasting a lot of time.2 I have heard many different excuses for not giving each developer his or her own sandbox, but frankly none of them holds water. The most legitimate concern relates to the cost of a database license for each developer—but even this obstacle can be surmounted by choosing one of the “virtual sandbox” variations. If the database technology supports it, we can use a DB Schema per TestRunner (see Database Sandbox); otherwise, we have to use a Database Partitioning Scheme (see Database Sandbox).\n\nTesting with Databases (Again!)\n\nSuppose we have done a good job layering our system and achieved our goal of running most of our tests without accessing the real database. Now what kinds of tests should we run against the real database? The answer is simple: “As few as possible, but no fewer!” In practice, we want to run at least a representative sample of our customer tests against the database to ensure that the SUT behaves\n\n2 Can you image asking a team of carpenters to share a single hammer?\n\nwww.it-ebooks.info\n\n173\n\n174\n\nChapter 13 Testing with Databases\n\nthe same way with a database as without one. These tests need not access the busi- ness logic via the user interface unless some particular user interface functionality depends on the database; Subcutaneous Tests (see Layer Test) should be adequate in most circumstances.\n\nWhat’s Next?\n\nIn this chapter, we looked at special techniques for testing with databases. This discussion has merely scratched the surface of the interactions between agile software development and databases.3 Chapter 14, A Roadmap to Effective Test Automation, summarizes the material we have covered thus far and makes some suggestions about how a project team should come up to speed on developer test automation.\n\n3 For a more complete treatment of the topic, refer to [RDb].\n\nwww.it-ebooks.info",
      "page_number": 230
    },
    {
      "number": 14,
      "title": "A Roadmap to Effective Test Automation",
      "start_page": 238,
      "end_page": 248,
      "detection_method": "regex_chapter",
      "content": "Chapter 14\n\nA Roadmap to Effective Test Automation\n\nAbout This Chapter\n\nChapter 13, Testing with Databases, introduced a set of patterns speciﬁ c to testing applications that have a database. These patterns built on the techniques described in Chapter 6, Test Automation Strategy; Chapter 9, Persistent Fixture Manage- ment; and Chapter 11, Using Test Doubles. This was a lot of material to become familiar with before we could test effectively with and without databases!\n\nThis raises an important point: We don’t become experts in test automa- tion overnight—these skills take time to develop. It also takes time to learn the various tools and patterns at our disposal. This chapter provides something of a roadmap for how to learn the patterns and acquire the skills. It introduces the concept of “test automation maturity,” which is loosely based on the SEI’s Capability Maturity Model (CMM).\n\nTest Automation Difﬁ culty\n\nSome kinds of tests are harder to write than others. This difﬁ culty arises partly because the techniques are more involved and partly because they are less well known and the tools to do this kind of test automation are less readily avail- able. The following common kinds of tests are listed in approximate order of difﬁ culty, from easiest to most difﬁ cult:\n\n1. Simple entity objects (Domain Model [PEAA])\n\nSimple business classes with no dependencies\n\nComplex business classes with dependencies\n\n175\n\nwww.it-ebooks.info\n\n176\n\nChapter 14 A Roadmap to Effective Test Automation\n\n2. Stateless service objects\n\nIndividual components via component tests\n\nThe entire business logic layer via Layer Tests (page 337)\n\n3. Stateful service objects\n\nCustomer tests via a Service Facade [CJ2EEP] using Subcutaneous\n\nTests (see Layer Test)\n\nStateful components via component tests\n\n4. “Hard-to-test” code\n\nUser interface logic exposed via Humble Dialog (see Humble\n\nObject on page 695)\n\nDatabase logic\n\nMulti-threaded software\n\n5. Object-oriented legacy software (software built without any tests)\n\n6. Non-object-oriented legacy software\n\nAs we move down this list, the software becomes increasingly more challenging to test. The irony is that many teams “get their feet wet” by trying to retroﬁ t tests onto an existing application. This puts them in one of the last two categories in this list, which is precisely where the most experience is required. Unfortunately, many teams fail to test the legacy software successfully, which may then prejudice them against trying automated testing, with or without test-driven development. If you ﬁ nd your- self trying to learn test automation by retroﬁ tting tests onto legacy software, I have two pieces of advice for you: First, hire someone who has done it before to help you through this process. Second, read Michael Feathers’ excellent book [WEwLC]; he covers many techniques speciﬁ cally applicable to retroﬁ tting tests.\n\nRoadmap to Highly Maintainable Automated Tests\n\nGiven that some kinds of tests are much harder to write than others, it makes sense to focus on learning to write the easier tests ﬁ rst before we move on to the more difﬁ cult kinds of tests. When teaching automated testing to developers, I introduce the techniques in the following sequence. This roadmap is based on Maslow’s hierarchy of needs [HoN], which says that we strive to meet the higher- level needs only after we have satisﬁ ed the lower-level needs.\n\nwww.it-ebooks.info\n\nRoadmap to Highly Maintainable Automated Tests\n\n1. Exercise the happy path code\n\nSet up a simple pre-test state of the SUT\n\nExercise the SUT by calling the method being tested\n\n2. Verify direct outputs of the happy path\n\nCall Assertion Methods (page 362) on the SUT’s responses\n\nCall Assertion Methods on the post-test state\n\n3. Verify alternative paths\n\nVary the SUT method arguments\n\nVary the pre-test state of the SUT\n\nControl indirect inputs of the SUT via a Test Stub (page 529)\n\n4. Verify indirect output behavior\n\nUse Mock Objects (page 544) or Test Spies (page 538) to intercept\n\nand verify outgoing method calls\n\n5. Optimize test execution and maintainability\n\nMake the tests run faster\n\nMake the tests easy to understand and maintain\n\nDesign the SUT for testability\n\nReduce the risk of missed bugs\n\nThis ordering of needs isn’t meant to imply that this is the order in which we might think about implementing any speciﬁ c test.1 Rather, it is likely to be the order in which a project team might reasonably expect to learn about the tech- niques of test automation.\n\nLet’s look at each of these points in more detail.\n\nExercise the Happy Path Code\n\nTo run the happy path through the SUT, we must automate one Simple Success Test (see Test Method on page 348) as a simple round-trip test through the SUT’s API. To get this test to pass, we might simply hard-code some of the logic in the\n\n1 Although it can also be used that way, I ﬁ nd it better to write the assertions ﬁ rst and then work back from there.\n\nwww.it-ebooks.info\n\n177\n\n178\n\nChapter 14 A Roadmap to Effective Test Automation\n\nSUT, especially where it might call other components to retrieve information it needs to make decisions that would drive the test down the happy path. Before exercising the SUT, we need to set up the test ﬁ xture by initializing the SUT to the pre-test state. As long as the SUT executes without raising any errors, we consider the test as having passed; at this level of maturity we don’t check the actual results against the expected results.\n\nVerify Direct Outputs of the Happy Path\n\nOnce the happy path is executing successfully, we can add result veriﬁ cation logic to turn our test into a Self-Checking Test (see page 26). This involves adding calls to Assertion Methods to compare the expected results with what actually oc- curred. We can easily make this change for any objects or values returned to the test by the SUT (e.g., “return values,” “out parameters”). We can also call other methods on the SUT or use public ﬁ elds to access the post-test state of the SUT; we can then call Assertion Methods on these values as well.\n\nVerify Alternative Paths\n\nAt this point the happy path through the code is reasonably well tested. The alternative paths through the code are still Untested Code (see Production Bugs on page 268) so the next step is to write tests for these paths (whether we have already written the production code or we are striving to automate the tests that would drive us to implement them). The question to ask here is “What causes the alternative paths to be exercised?” The most common causes are as follows:\n\nDifferent values passed in by the client as arguments\n\nDifferent prior state of the SUT itself\n\nDifferent results of invoking methods on components on which the\n\nSUT depends\n\nThe ﬁ rst case can be tested by varying the logic in our tests that calls the SUT methods we are exercising and passing in different values as arguments. The second case involves initializing the SUT with a different starting state. Neither of these cases requires any “rocket science.” The third case, however, is where things get interesting.\n\nControlling Indirect Inputs\n\nBecause the responses from other components are supposed to cause the SUT to exercise the alternative paths through the code, we need to get control\n\nwww.it-ebooks.info\n\nRoadmap to Highly Maintainable Automated Tests\n\nover these indirect inputs. We can do so by using a Test Stub that returns the value that should drive the SUT into the desired code path. As part of ﬁ xture setup, we must force the SUT to use the stub instead of the real component. The Test Stub can be built two ways: as a Hard-Coded Test Stub (see Test Stub), which contains hand-written code that returns the speciﬁ c values, or as a Conﬁ gurable Test Stub (see Test Stub), which is conﬁ gured by the test to return the desired values. In both cases, the SUT must use the Test Stub instead of the real component.\n\nMany of these alternative paths result in “successful” outputs from the SUT; these tests are considered Simple Success Tests and use a style of Test Stub called a Responder (see Test Stub). Other paths are expected to raise errors or excep- tions; they are considered Expected Exception Tests (see Test Method) and use a style of stub called a Saboteur (see Test Stub).\n\nMaking Tests Repeatable and Robust\n\nThe act of replacing a real depended-on component (DOC) with a Test Stub has a very desirable side effect: It makes our tests both more robust and more repeat- able.2 By using a Test Stub, we replace a possibly nondeterministic component with one that is completely deterministic and under test control. This is a good example of the Isolate the SUT principle (see page 43).\n\nVerify Indirect Output Behavior\n\nThus far we have focused on getting control of the indirect inputs of the SUT and verifying readily visible direct outputs by inspecting the post-state test of the SUT. This kind of result veriﬁ cation is known as State Veriﬁ cation (page 462). Sometimes, however, we cannot conﬁ rm that the SUT has behaved correctly simply by looking at the post-test state. That is, we may still have some Untested Requirements (see Production Bugs) that can only be veriﬁ ed by doing Behavior Veriﬁ cation (page 468).\n\nWe can build on what we already know how to do by using one of the close relatives of the Test Stub to intercept the outgoing method calls from our SUT. A Test Spy “remembers” how it was called so that the test can later retrieve the usage information and use Assertion Method calls to compare it to the expected usage. A Mock Object can be loaded with expectations during ﬁ xture setup, which it subsequently compares with the actual calls as they occur while the SUT is being exercised.\n\n2 See Robust Test (see page 29) and Repeatable Test (see page 26) for a more detailed description.\n\nwww.it-ebooks.info\n\n179\n\n180\n\nChapter 14 A Roadmap to Effective Test Automation\n\nOptimize Test Execution and Maintenance\n\nAt this point we should have automated tests for all the paths through our code. We may, however, have less than optimal tests:\n\nWe may have Slow Tests (page 253).\n\nThe tests may contain Test Code Duplication (page 213) that makes\n\nthem hard to understand.\n\nWe may have Obscure Tests (page 186) that are hard to understand\n\nand maintain.\n\nWe may have Buggy Tests (page 260) that are caused by unreliable Test Utility Methods (page 599) or Conditional Test Logic (page 200).\n\nMake the Tests Run Faster\n\nSlow Tests is often the ﬁ rst behavior smell we need to address. To make tests run faster, we can reuse the test ﬁ xture across many tests—for example, by using some form of Shared Fixture (page 317). Unfortunately, this tactic typically produces its own share of problems. Replacing a DOC with a Fake Object (page 551) that is functionally equivalent but executes much faster is almost always a better solution. Use of a Fake Object builds on the techniques we learned for verifying indirect inputs and outputs.\n\nMake the Tests Easy to Understand and Maintain\n\nWe can make Obscure Tests easier to understand and remove a lot of Test Code Duplication by refactoring our Test Methods to call Test Utility Methods that contain any frequently used logic instead of doing everything on an in-line basis. Creation Methods (page 415), Custom Assertions (page 474), Finder Methods (see Test Utility Method), and Parameterized Tests (page 607) are all examples of this approach.\n\nIf our Testcase Classes (page 373) are getting too big to understand, we can reorganize these classes around ﬁ xtures or features. We can also better commu- nicate our intent by using a systematic way of naming Testcase Classes and Test Methods that exposes the test conditions we are verifying in them.\n\nReduce the Risk of Missed Bugs\n\nIf we are having problems with Buggy Tests or Production Bugs, we can reduce the risk of false negatives (tests that pass when they shouldn’t) by encapsulating complex test logic. When doing so, we should use intent-revealing names for our\n\nwww.it-ebooks.info\n\nWhat’s Next?\n\nTest Utility Methods. We should verify the behavior of nontrivial Test Utility Methods using Test Utility Tests (see Test Utility Method).\n\nWhat’s Next?\n\nThis chapter concludes Part I, The Narratives. Chapters 1–14 have provided an overview of the goals, principles, philosophies, patterns, smells, and coding idioms related to writing effective automated tests. Part II, The Test Smells, and Part III, The Patterns, contain detailed descriptions of each of the smells and patterns introduced in these narrative chapters, complete with code samples.\n\nwww.it-ebooks.info\n\n181\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nPART II\n\nThe Test Smells\n\nwww.it-ebooks.info\n\nThe Test Smells\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nChapter 15\n\nCode Smells\n\nSmells in This Chapter\n\nObscure Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\n\nConditional Test Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200\n\nHard-to-Test Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n\nTest Code Duplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n\nTest Logic in Production . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n\n185\n\nwww.it-ebooks.info\n\nCode Smells",
      "page_number": 238
    },
    {
      "number": 15,
      "title": "Code Smells",
      "start_page": 249,
      "end_page": 286,
      "detection_method": "regex_chapter",
      "content": "186\n\nObscure Test\n\nAlso known as: Long Test, Complex Test, Verbose Test\n\nChapter 15 Code Smells\n\nObscure Test\n\nIt is difﬁ cult to understand the test at a glance.\n\nAutomated tests should serve at least two purposes. First, they should act as documentation of how the system under test (SUT) should behave; we call this Tests as Documentation (see page 23). Second, they should be a self-verifying executable speciﬁ cation. These two goals are often contradictory because the level of detail needed for tests to be executable may make the test so verbose as to be difﬁ cult to understand.\n\nSymptoms\n\nWe are having trouble understanding what behavior a test is verifying.\n\nImpact\n\nThe ﬁ rst issue with an Obscure Test is that it makes the test harder to understand and therefore maintain. It will almost certainly preclude achieving Tests as Doc- umentation, which in turn can lead to High Test Maintenance Cost (page 265). The second issue with an Obscure Test is that it may allow bugs to slip through because of test coding errors hidden in the Obscure Test. This can re- sult in Buggy Tests (page 260). Furthermore, a failure of one assertion in an Eager Test may hide many more errors that simply aren’t run, leading to a loss of test debugging data.\n\nCauses\n\nParadoxically, an Obscure Test can be caused by either too much information in the Test Method (page 348) or too little information. Mystery Guest is an example of too little information; Eager Test and Irrelevant Information are examples of too much information.\n\nThe root cause of an Obscure Test is typically a lack of attention to keeping the test code clean and simple. Test code is just as important as the production code, and it needs to be refactored just as often. A major contributor to an Obscure Test is a “just do it in-line” mentality when writing tests. Putting code in-line results in large, complex Test Methods because some things just take a lot of code to do.\n\nThe ﬁ rst few causes of Obscure Test discussed here relate to having the\n\nwrong information in the test:\n\nwww.it-ebooks.info\n\nObscure Test\n\nEager Test: The test veriﬁ es too much functionality in a single Test Obscure Test\n\nEager Test: The test veriﬁ es too much functionality in a single Test Obscure Test\n\nMystery Guest: The test reader is not able to see the cause and effect between ﬁ xture and veriﬁ cation logic because part of it is done outside the Test Method.\n\nThe general problem of Verbose Tests—tests that use too much code to say what they need to say—can be further broken down into a number of root causes:\n\nGeneral Fixture: The test builds or references a larger ﬁ xture than is\n\nneeded to verify the functionality in question.\n\nIrrelevant Information: The test exposes a lot of irrelevant details about the ﬁ xture that distract the test reader from what really affects the be- havior of the SUT.\n\nHard-Coded Test Data: Data values in the ﬁ xture, assertions, or argu- ments of the SUT are hard-coded in the Test Method, obscuring cause– effect relationships between inputs and expected outputs.\n\nIndirect Testing: The Test Method interacts with the SUT indirectly via\n\nanother object, thereby making the interactions more complex.\n\nCause: Eager Test\n\nThe test veriﬁ es too much functionality in a single Test Method.\n\nSymptoms\n\nThe test goes on and on verifying this, that, and “everything but the kitchen sink.” It is hard to tell which part is ﬁ xture setup and which part is exercising the SUT.\n\npublic void testFlightMileage_asKm2() throws Exception { // set up ﬁxture // exercise constructor Flight newFlight = new Flight(validFlightNumber); // verify constructed object assertEquals(validFlightNumber, newFlight.number); assertEquals(\"\", newFlight.airlineCode); assertNull(newFlight.airline); // set up mileage newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); // verify results int expectedKilometres = 1810;\n\nwww.it-ebooks.info\n\n187\n\n188\n\nObscure Test\n\nChapter 15 Code Smells\n\nassertEquals( expectedKilometres, actualKilometres); // now try it with a canceled ﬂight newFlight.cancel(); try { newFlight.getMileageAsKm(); fail(\"Expected exception\"); } catch (InvalidRequestException e) { assertEquals( \"Cannot get cancelled ﬂight mileage\", e.getMessage()); } }\n\nRoot Cause\n\nWhen executing tests manually, it makes sense to chain a number of logically distinct test conditions into a single test case to reduce the setup overhead of each test. This works because we have liveware (an intelligent human being) executing the tests, and this person can decide at any point whether it makes sense to keep going or whether the failure of a step is severe enough to abandon the execution of the test.\n\nPossible Solution\n\nWhen the tests are automated, it is better to have a suite of independent Single- Condition Tests (see page 45) as these provide much better Defect Localization (see page 22).\n\nCause: Mystery Guest\n\nThe test reader is not able to see the cause and effect between ﬁ xture and veriﬁ - cation logic because part of it is done outside the Test Method.\n\nSymptoms\n\nTests invariably require passing data to the SUT. The data used in the ﬁ xture setup and exercise SUT phases of the Four-Phase Test (page 358) deﬁ ne the pre- conditions of the SUT and inﬂ uence how it should behave. The post-conditions (the expected outcomes) are reﬂ ected in the data passed as arguments to the Assertion Methods (page 362) in the verify outcome phase of the test.\n\nWhen either the ﬁ xture setup or the result veriﬁ cation part of a test depends on information that is not visible within the test and the test reader ﬁ nds it dif- ﬁ cult to understand the behavior that is being veriﬁ ed without ﬁ rst ﬁ nding and inspecting the external information, we have a Mystery Guest on our hands. Here’s an example where we cannot tell what the ﬁ xture looks like, making it difﬁ cult to relate the expected outcome to the pre-conditions of the test:\n\nwww.it-ebooks.info\n\nObscure Test\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight_mg() throws Exception { loadAirportsAndFlightsFromFile(\"test-ﬂights.csv\"); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirportCode( \"YYC\"); // Verify Outcome assertEquals( 1, ﬂightsAtOrigin.size()); FlightDto ﬁrstFlight = (FlightDto) ﬂightsAtOrigin.get(0); assertEquals( \"Calgary\", ﬁrstFlight.getOriginCity()); }\n\nImpact\n\nThe Mystery Guest makes it hard to see the cause–effect relationship between the test ﬁ xture (the pre-conditions of the test) and the expected outcome of the test. As a consequence, the tests don’t fulﬁ ll the role of Tests as Docu- mentation. Even worse, someone may modify or delete the external resource without realizing the impact this action will have when the tests are run. This behavior smell has its own name: Resource Optimism (see Erratic Test on page 228)!\n\nIf the Mystery Guest is a Shared Fixture (page 317), it may also lead to Erratic\n\nTests if other tests modify it.\n\nRoot Cause\n\nA test depends on mysterious external resources, making it difﬁ cult to under- stand the behavior that it is verifying. Mystery Guests may take many forms:\n\nA ﬁ lename of an existing external ﬁ le is passed to a method of the SUT; the contents of the ﬁ le should determine the behavior of the SUT.\n\nThe contents of a database record identiﬁ ed by a literal key are read\n\ninto an object that is then used by the test or passed to the SUT.\n\nThe contents of a ﬁ le are read and used in calls to Assertion Methods to\n\nverify the expected outcome.\n\nA Setup Decorator (page 447) is used to create a Shared Fixture, and objects in this ﬁ xture are then referenced via variables within the result veriﬁ cation logic.\n\nA General Fixture is set up using Implicit Setup (page 424), and the Test Methods then access them via instance variables or class variables.\n\nAll of these scenarios share a common outcome: It is hard to see the cause–effect relationship between the test ﬁ xture and the expected outcome of the test because\n\nwww.it-ebooks.info\n\n189\n\nObscure Test\n\n190\n\nObscure Test\n\nChapter 15 Code Smells\n\nthe relevant data are not visible in the tests. If the contents of the data are not clearly described by the names we give to the variables and ﬁ les that contain them, we have a Mystery Guest.\n\nPossible Solution\n\nUsing a Fresh Fixture (page 311) built using In-line Setup (page 408) is the obvious solution for a Mystery Guest. When applied to the ﬁ le example, this would involve creating the contents of the ﬁ le as a string within our test so that the contents are visible and then writing them out to the ﬁ le system [Setup External Resource (page 772) refactoring] or putting it into a ﬁ le sys- tem Test Stub (page 529) as part of the ﬁ xture setup.1 To avoid Irrelevant Information, we may want to hide the details of the construction behind one or more evocatively named Creation Methods (page 415) that append to the ﬁ le’s contents.\n\nIf we must use a Shared Fixture or Implicit Setup, we should consider using evocatively named Finder Methods (see Test Utility Method on page 599) to access the objects in the ﬁ xture. If we must use external resources such as ﬁ les, we should put them into a special folder or directory and give them names that make it obvious what kind of data they hold.\n\nCause: General Fixture\n\nThe test builds or references a larger ﬁ xture than is needed to verify the func- tionality in question.\n\nSymptoms\n\nThere seems to be a lot of test ﬁ xture being built—much more than would appear to be necessary for any particular test. It is hard to understand the cause–effect relationship between the ﬁ xture, the part of the SUT being exercised, and the expected outcome of a test.\n\nConsider the following set of tests:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { setupStandardAirportsAndFlights(); FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome\n\n1 See In-line Resource (page 736) refactoring for details.\n\nwww.it-ebooks.info\n\nObscure Test\n\nassertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { setupStandardAirportsAndFlights(); FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nFrom reading the exercise SUT and veriﬁ ng outcome parts of the tests, it would appear that they need very different ﬁ xtures. Even though these tests are using a Fresh Fixture setup strategy, they are using the same ﬁ xture setup logic by calling the setupStandardAirportsAndFlights method. The name of the method is a clue to this classic but easily recognized example of a General Fixture. A more difﬁ cult case to diagnose would be if each test created the Standard Fixture (page 305) in-line or if each test created a somewhat different ﬁ xture but each ﬁ xture con- tained much more than was needed by each individual test.\n\nWe may also be experiencing Slow Tests (page 253) or a Fragile Fixture (see\n\nFragile Test on page 239).\n\nRoot Cause\n\nThe most common cause of this problem is a test that uses a ﬁ xture that is designed to support many tests. Examples include the use of Implicit Setup or a Shared Fix- ture across many tests with different ﬁ xture requirements. This problem results in the ﬁ xture becoming large and difﬁ cult to understand. The ﬁ xture may also grow larger over time. The root cause is that both approaches rely on a Standard Fix- ture that must meet the requirements of all tests that use it. The more diverse the needs of those tests, the more likely we are to create a General Fixture.\n\nImpact\n\nWhen the test ﬁ xture is designed to support many different tests, it can be very difﬁ cult to understand how each test uses the ﬁ xture. This complexity reduces the likelihood of using Tests as Documentation and can result in a Fragile Fixture as people alter the ﬁ xture so that it can handle new tests. It can also result in Slow\n\nwww.it-ebooks.info\n\n191\n\nObscure Test\n\n192\n\nObscure Test\n\nChapter 15 Code Smells\n\nTests because a larger ﬁ xture takes more time to build, especially if a ﬁ le system or database is involved.\n\nPossible Solution\n\nWe need to move to a Minimal Fixture (page 302) to address this problem. To do so, we can use a Fresh Fixture for each test. If we must use a Shared Fixture, we should consider applying the Make Resource Unique (page 737) refactoring to create a virtual Database Sandbox (page 650) for each test.2\n\nCause: Irrelevant Information\n\nThe test exposes a lot of irrelevant details about the ﬁ xture that distract the test reader from what really affects the behavior of the SUT.\n\nSymptoms\n\nAs test readers, we ﬁ nd it hard to determine which of the values passed to objects actually affect the expected outcome:\n\npublic void testAddItemQuantity_severalQuantity_v10(){ // Set Up Fixture Address billingAddress = createAddress( \"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); Address shippingAddress = createAddress( \"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); Customer customer = createCustomer( 99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); Product product = createProduct( 88,\"SomeWidget\",new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, 5); // Verify Outcome LineItem expected = new LineItem(invoice, product,5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\n2 Switching to an Immutable Shared Fixture (see Shared Fixture) does not fully address the core of this problem because it does not help us determine which parts of the ﬁ xture are needed by each test; only the parts that are modiﬁ ed are so identiﬁ ed!\n\nwww.it-ebooks.info\n\nObscure Test\n\nFixture setup logic may seem very long and complicated as it weaves together many interrelated objects. This makes it hard to determine what the test is veri- fying because the reader doesn’t understand the pre-conditions of the test:\n\npublic void testGetFlightsByOriginAirport_TwoOutboundFlights() throws Exception { FlightDto expectedCalgaryToSanFran = new FlightDto(); expectedCalgaryToSanFran.setOriginAirportId(calgaryAirportId); expectedCalgaryToSanFran.setOriginCity(CALGARY_CITY); expectedCalgaryToSanFran.setDestinationAirportId(sanFranAirportId); expectedCalgaryToSanFran.setDestinationCity(SAN_FRAN_CITY); expectedCalgaryToSanFran.setFlightNumber( facade.createFlight(calgaryAirportId,sanFranAirportId)); FlightDto expectedCalgaryToVan = new FlightDto(); expectedCalgaryToVan.setOriginAirportId(calgaryAirportId); expectedCalgaryToVan.setOriginCity(CALGARY_CITY); expectedCalgaryToVan. setDestinationAirportId(vancouverAirportId); expectedCalgaryToVan.setDestinationCity(VANCOUVER_CITY); expectedCalgaryToVan.setFlightNumber(facade.createFlight( calgaryAirportId, vancouverAirportId));\n\nThe code that veriﬁ es the expected outcome of a test can also be too complicated to understand:\n\nList lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 2); // verify ﬁrst item LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem1.getInv(), actual.getInv()); assertEquals(expItem1.getProd(), actual.getProd()); assertEquals(expItem1.getQuantity(), actual.getQuantity()); // verify second item actual = (LineItem)lineItems.get(1); assertEquals(expItem2.getInv(), actual.getInv()); assertEquals(expItem2.getProd(), actual.getProd()); assertEquals(expItem2.getQuantity(), actual.getQuantity()); }\n\nRoot Cause\n\nA test contains a lot of data, either as Literal Values (page 714) or as variables. Irrelevant Information often occurs in conjunction with Hard-Coded Test Data or a General Fixture but can also arise because we make visible all data the test needs to execute rather than focusing on the data the test needs to be un- derstood. When writing tests, the path of least resistance is to use whatever methods are available (on the SUT and other objects) and to ﬁ ll in all parameters with values, whether or not they are relevant to the test.\n\nwww.it-ebooks.info\n\n193\n\nObscure Test\n\n194\n\nObscure Test\n\nChapter 15 Code Smells\n\nAnother possible cause is when we include all the code needed to verify the outcome using Procedural State Veriﬁ cation (see State Veriﬁ cation on page 462) rather than using a much more compact “declarative” style to spec- ify the expected outcome.\n\nImpact\n\nIt is hard to achieve Tests as Documentation if the tests contain many seemingly random bits of Obscure Test that don’t clearly link the pre-conditions with the post-conditions. Likewise, wading through many steps of ﬁ xture setup or result veriﬁ cation logic can result in High Test Maintenance Cost and can increase the likelihood of Production Bugs (page 268) or Buggy Tests.\n\nPossible Solution\n\nThe best way to get rid of Irrelevant Information in ﬁ xture setup logic is to replace direct calls to the constructor or Factory Methods [GOF] with calls to Parameterized Creation Methods (see Creation Method) that take only the rel- evant information as parameters. Fixture values that do not matter to the test (i.e., those that do not affect the expected outcome) should be defaulted within Creation Methods or replaced by Dummy Objects (page 728). In this way we say to the test reader, “The values you don’t see don’t affect the expected out- come.” We can replace ﬁ xture values that appear in both the ﬁ xture setup and outcome veriﬁ cation parts of the test with suitably initialized named constants as long as we are using a Fresh Fixture approach to ﬁ xture setup.\n\nTo hide Irrelevant Information in result veriﬁ cation logic, we can use asser- tions on entire Expected Objects (see State Veriﬁ cation), rather than asserting on individual ﬁ elds, and we can create Custom Assertions (page 474) that hide complex procedural veriﬁ cation logic.\n\nCause: Hard-Coded Test Data\n\nData values in the ﬁ xture, assertions, or arguments of the SUT are hard-coded in the Test Method, obscuring cause–effect relationships between inputs and expected outputs.\n\nSymptoms\n\nAs test readers, we ﬁ nd it difﬁ cult to determine how various hard-coded (i.e., literal) values in the test are related to one another and which values should affect the behavior of the SUT. We may also encounter behavior smells such as Erratic Tests.\n\nwww.it-ebooks.info\n\nObscure Test\n\npublic void testAddItemQuantity_severalQuantity_v12(){ // Set Up Fixture Customer cust = createACustomer(new BigDecimal(\"30\")); Product prod = createAProduct(new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(cust); // Exercise SUT invoice.addItemQuantity(prod, 5); // Verify Outcome LineItem expected = new LineItem(invoice, prod, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\nThis speciﬁ c example isn’t so bad because there aren’t very many literal values. If we aren’t good at doing math in our heads, however, we might miss the relation- ship between the unit price ($19.99), the item quantity (5), the discount (30%), and the total price ($69.96).\n\nRoot Cause\n\nHard-Coded Test Data occurs when a test contains a lot of seemingly unrelated Literal Values. Tests invariably require passing data to the SUT. The data used in the ﬁ xture setup and exercise SUT phases of the Four-Phase Test deﬁ ne the pre- conditions of the SUT and inﬂ uence how it should behave. The post-conditions (the expected outcomes) are reﬂ ected in the data passed as arguments to the Assertion Methods in the verify outcome phase of the test. When writing tests, the path of least resistance is to use whatever methods are available (on the SUT and other objects) and to ﬁ ll in all parameters with values, whether or not they are relevant to the test.\n\nWhen we use “cut-and-paste” reuse of test logic, we ﬁ nd ourselves replicat-\n\ning the literal values to the derivative tests.\n\nImpact\n\nIt is hard to achieve Tests as Documentation if the tests contain many seemingly random bits of Obscure Test that don’t clearly link the pre-conditions with the post-conditions. A few literal parameters might not seem like a bad thing—after all, they don’t require us to make that much more effort to understand a test. As the number of literal values grows, however, it can become much more difﬁ cult to understand a test. This is especially true when the signal-to-noise ratio drops dramatically because the majority of the values are irrelevant to the test.\n\nThe second major impact occurs when collisions between tests occur because the tests are using the same values. This situation happens only when we use a Shared Fixture because a Fresh Fixture strategy shouldn’t litter the scene with any objects with which a subsequent test can collide.\n\nwww.it-ebooks.info\n\n195\n\nObscure Test\n\n196\n\nObscure Test\n\nChapter 15 Code Smells\n\nPossible Solution\n\nThe best way to get rid of the Obscure Test smell is to replace the literal constants with something else. Fixture values that determine which scenario is being ex- ecuted (e.g., type codes) are probably the only ones that are reasonable to leave as literals—but even these values can be converted to named constants.\n\nFixture values that do not matter to the test (i.e., those that do not affect the expected outcome) should be defaulted within Creation Methods. In this way we say to the test reader, “The values you don’t see don’t affect the expected outcome.” We can replace ﬁ xture values that appear in both the ﬁ xture setup and outcome veriﬁ cation parts of the test with suitably initialized named con- stants as long as we are using a Fresh Fixture approach to ﬁ xture setup.\n\nValues in the result veriﬁ cation logic that are based on values used in the ﬁ x- ture or that are used as arguments of the SUT should be replaced with Derived Values (page 718) to make those calculations obvious to the test reader.\n\nIf we are using any variant of Shared Fixture, we should try to use Distinct Generated Values (see Generated Value on page 723) to ensure that each time a test is run, it uses a different value. This consideration is especially important for ﬁ elds that serve as unique keys in databases. A common way of encapsulating this logic is to use Anonymous Creation Methods (see Creation Method).\n\nCause: Indirect Testing\n\nThe Test Method interacts with the SUT indirectly via another object, thereby making the interactions more complex.\n\nSymptoms\n\nA test interacts primarily with objects other than the one whose behavior it purports to verify. The test must construct and interact with objects that contain references to the SUT rather than with the SUT itself. Testing business logic through the presentation layer is a common example of Indirect Testing.\n\nprivate ﬁnal int LEGAL_CONN_MINS_SAME = 30; public void testAnalyze_sameAirline_LessThanConnectionLimit() throws Exception { // setup FlightConnection illegalConn = createSameAirlineConn( LEGAL_CONN_MINS_SAME - 1); // exercise FlightConnectionAnalyzerImpl sut = new FlightConnectionAnalyzerImpl();\n\nwww.it-ebooks.info\n\nObscure Test\n\nString actualHtml = sut.getFlightConnectionAsHtmlFragment( illegalConn.getInboundFlightNumber(), illegalConn.getOutboundFlightNumber()); // veriﬁcation StringBuffer expected = new StringBuffer(); expected.append(\"<span class=\"boldRedText\">\"); expected.append(\"Connection time between ﬂight \"); expected.append(illegalConn.getInboundFlightNumber()); expected.append(\" and ﬂight \"); expected.append(illegalConn.getOutboundFlightNumber()); expected.append(\" is \"); expected.append(illegalConn.getActualConnectionTime()); expected.append(\" minutes.</span>\"); assertEquals(\"html\", expected.toString(), actualHtml); }\n\nImpact\n\nIt may not be possible to test “anything that could possibly break” in the SUT via the intermediate object. Indeed, such tests are unlikely to be very clear or understandable. They certainly will not result in Tests as Documentation.\n\nIndirect Testing may result in Fragile Tests because changes in the intermediate objects may require modiﬁ cation of the tests even when the SUT is not modiﬁ ed.\n\nRoot Cause\n\nThe SUT may be “private” to the class being used to access it from the test. It may not be possible to create the SUT directly because the constructors them- selves are private. This problem is just one sign that the software is not designed for testability.\n\nIt may be that the actual outcome of exercising the SUT cannot be observed directly. In such a case, the expected outcome of the test must be veriﬁ ed through an intermediate object.\n\nPossible Solution\n\nIt may be necessary to improve the design-for-testability of the SUT to remove this smell. We might be able to expose the SUT directly to the test by using an Extract Testable Component refactoring (a variant of the Sprout Class [WEwLC] refac- toring). This approach may result in an untestable Humble Object (page 695) and an easily tested object that contains most or all of the actual logic.\n\npublic void testAnalyze_sameAirline_EqualsConnectionLimit() throws Exception { // setup Mock ﬂightMgntStub = mock(FlightManagementFacade.class);\n\nwww.it-ebooks.info\n\n197\n\nObscure Test\n\n198\n\nObscure Test\n\nChapter 15 Code Smells\n\nFlight ﬁrstFlight = createFlight(); Flight secondFlight = createConnectingFlight( ﬁrstFlight, LEGAL_CONN_MINS_SAME); ﬂightMgntStub.expects(once()).method(\"getFlight\") .with(eq(ﬁrstFlight.getFlightNumber())) .will(returnValue(ﬁrstFlight)); ﬂightMgntStub.expects(once()).method(\"getFlight\") .with(eq(secondFlight.getFlightNumber())) .will(returnValue(secondFlight)); // exercise FlightConnAnalyzer theConnectionAnalyzer = new FlightConnAnalyzer(); theConnectionAnalyzer.facade = (FlightManagementFacade)ﬂightMgntStub.proxy(); FlightConnection actualConnection = theConnectionAnalyzer.getConn( ﬁrstFlight.getFlightNumber(), secondFlight.getFlightNumber()); // veriﬁcation assertNotNull(\"actual connection\", actualConnection); assertTrue(\"IsLegal\", actualConnection.isLegal()); }\n\nSometimes we may be forced to interact with the SUT indirectly because we cannot refactor the code to expose the logic we are trying to test. In these cases, we should encapsulate the complex logic forced by Indirect Testing behind suit- ably named Test Utility Methods. Similarly, ﬁ xture setup can be hidden behind Creation Methods and result veriﬁ cation can be hidden by Veriﬁ cation Methods (see Custom Assertion). Both are examples of SUT API Encapsulation (see Test Utility Method).\n\npublic void testAnalyze_sameAirline_LessThanConnLimit() throws Exception { // setup FlightConnection illegalConn = createSameAirlineConn( LEGAL_CONN_MINS_SAME - 1); FlightConnectionAnalyzerImpl sut = new FlightConnectionAnalyzerImpl(); // exercise SUT String actualHtml = sut.getFlightConnectionAsHtmlFragment( illegalConn.getInboundFlightNumber(), illegalConn.getOutboundFlightNumber()); // veriﬁcation assertConnectionIsIllegal(illegalConn, actualHtml); }\n\nThe following Custom Assertion hides the ugliness of extracting the business result from the presentation noise. It was created by doing a simple Extract Method [Fowler] refactoring on the test. Of course, this example would be more robust\n\nwww.it-ebooks.info\n\nObscure Test\n\nif it searched inside the HTML for key strings rather than building up the entire expected string and comparing it all at once. Other Presentation Layer Tests (see Layer Test on page 337) might then verify that the presentation logic is format- ting the HTML string properly.\n\nprivate void assertConnectionIsIllegal( FlightConnection conn, String actualHtml) { // set up expected value StringBuffer expected = new StringBuffer(); expected.append(\"<span class=\"boldRedText\">\"); expected.append(\"Connection time between ﬂight \"); expected.append(conn.getInboundFlightNumber()); expected.append(\" and ﬂight \"); expected.append(conn.getOutboundFlightNumber()); expected.append(\" is \"); expected.append(conn.getActualConnectionTime()); expected.append(\" minutes.</span>\"); // veriﬁcation assertEquals(\"html\", expected.toString(), actualHtml); }\n\nSolution Patterns\n\nA good test strategy helps keep the test code understandable. Nevertheless, just as “no battle plan survives the ﬁ rst contact with the enemy,” no test infrastruc- ture can anticipate all needs of all tests. We should expect the test infrastructure to evolve as the software matures and our test automation skills improve.\n\nWe can reuse test logic for several scenarios by having several tests call Test Utility Methods or by asking a common Parameterized Test (page 607) to pass in the already built test ﬁ xture or Expected Objects.\n\nWriting tests in an “outside-in” way can minimize the likelihood of produc- ing an Obscure Test that might then need to be refactored. This approach starts by outlining the Four-Phase Test using calls to nonexistent Test Utility Meth- ods. Once we are satisﬁ ed with these tests, we can write the utility methods needed to run them. By writing the tests ﬁ rst, we gain a better understanding of what the utility methods need to do for us to make writing the tests as simple as possible. The “test-infected” will, of course, write Test Utility Tests (see Test Utility Method) before writing the Test Utility Methods.\n\nwww.it-ebooks.info\n\n199\n\nObscure Test\n\n200\n\nConditional Test Logic\n\nAlso known as: Indented Test Code\n\nChapter 15 Code Smells\n\nConditional Test Logic\n\nA test contains code that may or may not be executed.\n\nA Fully Automated Test (see page 26) is just code that veriﬁ es the behavior of other code. But if this code is complicated, how do we verify that it works prop- erly? We could write tests for our tests—but when would this recursion stop? The simple answer is that Test Methods (page 348) must be simple enough to not need tests.\n\nConditional Test Logic is one factor that makes tests more complicated than\n\nthey really should be.\n\nSymptoms\n\nAs a code smell, Conditional Test Logic may not produce any behavioral symp- toms but its presence should be reasonably obvious to the test reader. View any control structures within a Test Method with extreme suspicion! The test reader may also wonder which code path is the one that is being executed. The following is an example of Conditional Test Logic that involves both looping and if statements:\n\n// verify Vancouver is in the list actual = null; i = ﬂightsFromCalgary.iterator(); while (i.hasNext()) { FlightDto ﬂightDto = (FlightDto) i.next(); if (ﬂightDto.getFlightNumber().equals( expectedCalgaryToVan.getFlightNumber())) { actual = ﬂightDto; assertEquals(\"Flight from Calgary to Vancouver\", expectedCalgaryToVan, ﬂightDto); break; } } }\n\nThis code begs the question, “What is this test code doing and how do we know that it is doing it correctly?” One behavioral symptom may be the pres- ence of the related project-level smell High Test Maintenance Cost (page 265), which may be caused by the complexity introduced by the Conditional Test Logic.\n\nwww.it-ebooks.info\n\nConditional Test Logic\n\nImpact\n\nConditional Test Logic makes it difﬁ cult to know exactly what a test is going to do when it really matters. Code that has only a single execution path al- ways executes in exactly the same way. Code that has multiple execution paths presents much greater challenges and does not inspire as much conﬁ dence about its outcome.\n\nTo increase our conﬁ dence in production code, we can write Self-Checking Tests (see page 26) that exercise the code. How can we increase our conﬁ dence in the test code if it executes differently each time we run it? It is hard to know (or prove) that the test is verifying the behavior we want it to verify. A test that has branches or loops, or that uses different values each time it is run, can be very difﬁ cult to debug simply because it isn’t completely deterministic.\n\nA related issue is that Conditional Test Logic makes writing the test correctly a more difﬁ cult task. Because the test itself cannot be tested easily, how do we know that it will actually detect the bugs it is intended to catch? [This is a gen- eral problem with Obscure Tests (page 186); they are more likely to result in Buggy Tests (page 260) than simple code.]\n\nCauses\n\nTest automaters may introduce Conditional Test Logic for several reasons:\n\nThey may use if statements to steer execution to a fail statement or to avoid executing certain pieces of test code when the SUT fails to return valid data.\n\nThey may use loops to verify the contents of collections of objects (Conditional Veriﬁ cation Logic). This may also result in an Obscure Test.\n\nThey may use Conditional Test Logic to verify complex objects or polymorphic data structures (another form of Conditional Veriﬁ cation Logic). This is just a Foreign Method [Fowler] implementation of the equals method.\n\nThey may use Conditional Test Logic to initialize the test ﬁ xture or Expected Object (see State Veriﬁ cation on page 462) so they can reuse a single test to verify several different cases (Flexible Test).\n\nThey may use if statements to avoid tearing down nonexistent ﬁ xture objects (Complex Teardown).\n\nwww.it-ebooks.info\n\n201\n\nConditional Test Logic\n\n202\n\nConditional Test Logic\n\nChapter 15 Code Smells\n\nSome of these causes are worth examining in more detail.\n\nCause: Flexible Test\n\nThe test code veriﬁ es different functionality depending on when or where it is run.\n\nSymptoms\n\nThe test contains conditional logic that does different things depending on the current environment. Most commonly this functionality takes the form of Con- ditional Test Logic to build different versions of the expected results based on some factor external to the test.\n\nConsider the following test, which gets the current time so that it can deter-\n\nmine what the output of the SUT should be:\n\npublic void testDisplayCurrentTime_whenever() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify outcome Calendar time = new DefaultTimeProvider().getTime(); StringBuffer expectedTime = new StringBuffer(); expectedTime.append(\"<span class=\\\"tinyBoldText\\\">\");\n\nif ((time.get(Calendar.HOUR_OF_DAY) == 0) && (time.get(Calendar.MINUTE) <= 1)) { expectedTime.append( \"Midnight\"); } else if ((time.get(Calendar.HOUR_OF_DAY) == 12) && (time.get(Calendar.MINUTE) == 0)) { // noon expectedTime.append(\"Noon\"); } else { SimpleDateFormat fr = new SimpleDateFormat(\"h:mm a\"); expectedTime.append(fr.format(time.getTime())); } expectedTime.append(\"</span>\");\n\nassertEquals( expectedTime, result); }\n\nRoot Cause\n\nA Flexible Test is caused by a lack of control of the environment. The test automater probably wasn’t able to decouple the SUT from its dependencies and decided to adapt the test logic based on the state of the environment.\n\nwww.it-ebooks.info\n\nConditional Test Logic\n\nImpact\n\nThe ﬁ rst issue is that using a Flexible Test makes the test harder to understand and therefore to maintain. The second issue is that we don’t know which test scenarios are actually being exercised and whether all scenarios are, in fact, exercised regularly. For example, in our sample test, is the midnight scenario ever exercised? How often? Probably rarely, if ever, because the test would have to be run at exactly midnight—an unlikely event, even if we timed the nightly build such that it ran over midnight.\n\nPossible Solution\n\nA Flexible Test is best addressed by decoupling the SUT from whatever depen- dencies prompted the test automater to make the test ﬂ exible. This involves refactoring the SUT to support substitutable dependency. We can then replace the dependency with a Test Double (page 522), such as a Test Stub (page 529) or Mock Object (page 544), and write separate tests for each circumstance previ- ously covered by the Flexible Test.\n\nCause: Conditional Veriﬁ cation Logic\n\nConditional Test Logic (page 200) may also create problems when it is used to verify the expected outcome. This issue usually arises when the tester tries to pre- vent the execution of assertions if the SUT fails to return the right objects or uses loops to verify the contents of collections returned by the SUT.\n\n// verify Vancouver is in the list actual = null; i = ﬂightsFromCalgary.iterator(); while (i.hasNext()) { FlightDto ﬂightDto = (FlightDto) i.next(); if (ﬂightDto.getFlightNumber().equals( expectedCalgaryToVan.getFlightNumber())) { actual = ﬂightDto; assertEquals(\"Flight from Calgary to Vancouver\", expectedCalgaryToVan, ﬂightDto); break; } } }\n\nPossible Solution\n\nWe can replace the if statements that steer execution to a call to fail with a Guard Assertion (page 490) that causes the test to fail before we reach the code we don’t\n\nwww.it-ebooks.info\n\n203\n\nConditional Test Logic\n\n204\n\nConditional Test Logic\n\nChapter 15 Code Smells\n\nwant to execute. This works well unless the test is an Expected Exception Test (see Test Method.) In the latter case, we should use the standard Expected Exception Test coding idiom for the xUnit family member and language.\n\nWe can replace Conditional Test Logic for veriﬁ cation of complex objects with an Equality Assertion (see Assertion Method on page 362) on an Expected Object. If the production code’s equals method is too strict, we can use a Custom Assertion (page 474) to deﬁ ne test-speciﬁ c equality.\n\nWe should move any loops in the veriﬁ cation logic to a Custom Assertion. We can then verify this assertion’s behavior by using Custom Assertion Tests (see Custom Assertion).\n\nWe can reuse test logic in several tests by calling a Test Utility Method (page 599) or a common Parameterized Test (page 607) that passes in the already built test ﬁ xture and Expected Objects.\n\nCause: Production Logic in Test\n\nSymptoms\n\nSome forms of Conditional Test Logic are found in the result veriﬁ cation section of our tests. Let us look more closely inside the loops of this test:\n\npublic void testCombinationsOfInputValues() { // Set up ﬁxture Calculator sut = new Calculator(); int expected; // TBD inside loops\n\nfor (int i = 0; i < 10; i++) { for (int j = 0; j < 10; j++) { // Exercise SUT int actual = sut.calculate( i, j );\n\n// Verify result if (i==3 & j==4) // special case expected = 8; else expected = i+j;\n\nassertEquals(message(i,j), expected, actual); } } }\n\nprivate String message(int i, int j) { return \"Cell( \" + String.valueOf(i)+ \",\" + String.valueOf(j) + \")\"; }\n\nwww.it-ebooks.info\n\nConditional Test Logic\n\nThe nested loops in this Loop-Driven Test (see Parameterized Test) exercise the SUT with various combinations of values of i and j as inputs. Here we will focus on the Conditional Test Logic inside the loop.\n\nRoot Cause\n\nThis Production Logic in Test is a direct result of wanting to verify multiple test conditions in a single Test Method. Given that multiple input values are passed to the SUT, we should also have multiple expected results. It is hard to enumerate the expected result for each set of inputs if we pass in many com- binations of several input arguments to the SUT in nested loops. A common solution to this problem is to use a Calculated Value (see Derived Value on page 718) based on the inputs. The potential downfall (as we see here) is that we ﬁ nd ourselves replicating the expected SUT logic inside our test to calculate the expected values for assertions.\n\nPossible Solution\n\nIf at all possible, it is better to enumerate the sets of precalculated values with which to test the SUT. The following example tests the same logic using a (smaller) set of enumerated values:\n\npublic void testMultipleValueSets() { // Set Up Fixture Calculator sut = new Calculator(); TestValues[] testValues = { new TestValues(1,2,3), new TestValues(2,3,5), new TestValues(3,4,8), // special case! new TestValues(4,5,9) };\n\nfor (int i = 0; i < testValues.length; i++) { TestValues values = testValues[i]; // Exercise SUT int actual = sut.calculate( values.a, values.b); // Verify Result assertEquals(message(i), values.expectedSum, actual); } }\n\nprivate String message(int i) { return \"Row \"+ String.valueOf(i); }\n\nwww.it-ebooks.info\n\n205\n\nConditional Test Logic\n\n206\n\nConditional Test Logic\n\nChapter 15 Code Smells\n\nCause: Complex Teardown\n\nSymptoms\n\nComplex ﬁ xture teardown code is more likely to leave the test environment cor- rupted if it does not clean up after itself correctly. It is hard to verify that tear- down code has been written correctly, and such code can easily result in “data leaks” that may later cause this or other tests to fail for no apparent reason. Consider this example:\n\npublic void testGetFlightsByOrigin_NoInboundFlight_SMRTD() throws Exception { // Set Up Fixture BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expFlightDto = createTestFlight(outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { try { facade.removeFlight(expFlightDto.getFlightNumber()); } ﬁnally { try { facade.removeAirport(inboundAirport); } ﬁnally { facade.removeAirport(outboundAirport); } } } }\n\nRoot Cause\n\nTeardown is typically required only when we use persistent resources that are beyond the reach of our garbage collection system. Complex Teardown occurs when many such resources are used in the same Test Method.\n\nPossible Solution\n\nTo avoid complex teardown logic, we should use Implicit Teardown (page 516), which will make the code both reusable and testable, or Automated Tear- down (page 503), which can be veriﬁ ed with automated unit tests. We can\n\nwww.it-ebooks.info\n\nConditional Test Logic\n\nalso eliminate the need to tear down any ﬁ xture objects by using a Fresh Fixture (page 311) strategy and by avoiding the use of any persistent objects in our tests by using some sort of Test Double.\n\nCause: Multiple Test Conditions\n\nSymptoms\n\nA test tries to apply the same test logic to many sets of input values, each with its own corresponding expected result. In the following example, the test iterates over a collection of test values and applies the test logic to each set:\n\npublic void testMultipleValueSets() { // Set Up Fixture Calculator sut = new Calculator(); TestValues[] testValues = { new TestValues(1,2,3), new TestValues(2,3,5), new TestValues(3,4,8), // special case! new TestValues(4,5,9) };\n\nfor (int i = 0; i < testValues.length; i++) { TestValues values = testValues[i]; // Exercise SUT int actual = sut.calculate( values.a, values.b); // Verify Outcome assertEquals(message(i), values.expectedSum, actual); } }\n\nprivate String message(int i) { return \"Row \"+ String.valueOf(i); }\n\nRoot Cause\n\nThe test automater is trying to test many test conditions using the same test logic in a single Test Method. In the preceding example, it is fairly simple Conditional Test Logic. Matters could be a lot worse if the code contained multiple nested loops and maybe even if statements to calculate different cases of the expected values.\n\nPossible Solution\n\nOf all sources of Conditional Test Logic, Multiple Test Conditions is prob- ably the most innocuous. Other than scaring the test reader, the main impact of such a test is that it stops executing at the ﬁ rst failure and doesn’t provide Defect Localization (see page 22) when a bug is introduced into the code. The\n\nwww.it-ebooks.info\n\n207\n\nConditional Test Logic\n\n208\n\nConditional Test Logic\n\nChapter 15 Code Smells\n\nreadability issue can easily be addressed by using an Extract Method [Fowler] refactoring to create a Parameterized Test call from within the loop. The lack of Defect Localization can be addressed by calling the Parameterized Test from a separate Test Method for each test condition. For large sets of values, a Data-Driven Test (page 288) might be a better solution.\n\nwww.it-ebooks.info\n\nHard-to-Test Code\n\nHard-to-Test Code\n\nCode is difﬁ cult to test.\n\nAutomated testing is a powerful tool that helps us develop software quickly even when we have a large code base to maintain. Of course, it provides these beneﬁ ts only if most of our code is protected by Fully Automated Tests (see page 26). The effort of writing these tests must be added to the effort of writing the product code they verify. Not surprisingly, we would prefer to make it easy to write the automated tests.3\n\nHard-to-Test Code is one factor that makes it difﬁ cult to write complete,\n\ncorrect automated tests in a cost-efﬁ cient manner.\n\nSymptoms\n\nSome kinds of code are inherently difﬁ cult to test—GUI components, multi- threaded code, and test code, for example. It may be difﬁ cult to get at the code to be tested because it is not visible to a test. It may be problematic to compile a test because the code is too highly coupled to other classes. It may be hard to create an instance of the object because the constructors don’t exist, are private, or take too many other objects as parameters.\n\nImpact\n\nWhenever we have Hard-to-Test Code, we cannot easily verify the quality of that code in an automated way. While manual quality assessment is often pos- sible, it doesn’t scale very well because the effort to perform this assessment after each code change usually means it doesn’t get done. Nor is this strategy readily repeated without a large test documentation cost.\n\nSolution Patterns\n\nA better solution is to make the code more amenable to testing. This topic is big enough that it warrants a whole chapter of its own, but this section covers a few of the highlights.\n\n3 We would also like to recoup this cost by reducing effort somewhere else. The best way to achieve this is to avoid Frequent Debugging (page 248) by writing the tests ﬁ rst and achieving Defect Localization (see page 22).\n\nwww.it-ebooks.info\n\n209\n\nHard-to-Test Code\n\n210\n\nHard-to-Test Code\n\nAlso known as: Hard-Coded Dependency\n\nChapter 15 Code Smells\n\nCauses\n\nThere are a number of reasons for Hard-to-Test Code; the most common causes are discussed here.\n\nCause: Highly Coupled Code\n\nSymptoms\n\nA class cannot be tested without also testing several other classes.\n\nImpact\n\nCode that is highly coupled to other code is very difﬁ cult to unit test because it won’t execute in isolation.\n\nRoot Cause\n\nHighly Coupled Code can be caused by many factors, including poor design, lack of object-oriented design experience, and lack of a reward structure that encourages decoupling.\n\nPossible Solution\n\nThe key to testing overly coupled code is to break the coupling. This happens naturally when we are doing test-driven development.\n\nA technique that we often use to decouple code for the purpose of testing is a Test Double (page 522) or, more speciﬁ cally, a Test Stub (page 529) or Mock Object (page 544). This topic is covered in much more detail in Chapter 11, Using Test Doubles.\n\nRetroﬁ tting tests onto existing code is a more challenging task, especially when we are dealing with a legacy code base. This is a big enough topic that Michael Feathers wrote a whole book on techniques for doing this, titled Work- ing Effectively with Legacy Code [WEwLC].\n\nCause: Asynchronous Code\n\nSymptoms\n\nA class cannot be tested via direct method calls. The test must start an execut- able (such as a thread, process, or application) and wait until its start-up has ﬁ nished before interacting with the executable.\n\nwww.it-ebooks.info\n\nHard-to-Test Code\n\nImpact\n\nCode that has an asynchronous interface is hard to test because the tests of these elements must coordinate their execution with that of the SUT. This requirement can add a lot of complexity to the tests and causes them to take much, much longer to run. The latter issue is a major concern with unit tests, which must run very quickly to ensure that developers will run them frequently.\n\nRoot Cause\n\nThe code that implements the algorithm we wish to test is highly coupled to the active object in which it normally executes.\n\nPossible Solution\n\nThe key to testing asynchronous code is to separate the logic from the asynchronous access mechanism. The design-for-testability pattern Humble Object (page 695; including Humble Dialog and Humble Executable) is a good example of a way to restructure otherwise asynchronous code so it can be tested in a synchronous manner.\n\nCause: Untestable Test Code\n\nSymptoms\n\nThe body of a Test Method (page 348) is obscure enough (Obscure Test; see page 186) or contains enough Conditional Test Logic (page 200) that we wonder whether the test is correct.\n\nImpact\n\nAny Conditional Test Logic within a Test Method has a higher probability of producing Buggy Tests (page 260) and will likely result in High Test Mainte- nance Cost (page 265). Too much code in the test method body can make the test hard to understand and hard to construct correctly.\n\nRoot Cause\n\nThe code within the body of the Test Method is inherently hard to test using a Self-Checking Test (see page 26). To do so, we would have to replace the SUT with a Test Double that injects the target error and then run the test method inside another Expected Exception Test (see Test Method) method—much too much trouble to bother with in all but the most unusual circumstances.\n\nwww.it-ebooks.info\n\n211\n\nHard-to-Test Code\n\n212\n\nHard-to-Test Code\n\nChapter 15 Code Smells\n\nPossible Solution\n\nWe can remove the need to test the body of a Test Method by making it extremely simple and relocating any Conditional Test Logic from it into Test Utility Methods (page 599), for which we can easily write Self-Checking Tests.\n\nwww.it-ebooks.info\n\nTest Code Duplication\n\nTest Code Duplication\n\nThe same test code is repeated many times.\n\nMany of the tests in a suite need to do similar things. For example, tests often exercise scenarios that are variations on the same theme. Tests may require simi- lar ﬁ xture setup or result veriﬁ cation logic. In some cases, even the exercise SUT phase of many tests involves repeating the same nontrivial logic.\n\nThe need for tests to do similar things often results in Test Code Duplication.\n\nSymptoms\n\nSeveral tests may contain a common subset of essentially the same statements, as in the following example:\n\npublic void testInvoice_addOneLineItem_quantity1_b() { // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); // Verify only item LineItem expItem = new LineItem(inv, product, QUANTITY); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\npublic void testRemoveLineItemsForProduct_oneOfTwo() { // Set up Invoice inv = createAnonInvoice(); inv.addItemQuantity(product, QUANTITY); inv.addItemQuantity(anotherProduct, QUANTITY); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.removeLineItemForProduct(anotherProduct); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\nwww.it-ebooks.info\n\n213\n\nTest Code Duplication\n\n214\n\nTest Code Duplication\n\nChapter 15 Code Smells\n\nA single test may also contain repeated groups of similar statements:\n\npublic void testInvoice_addTwoLineItems_sameProduct() { Invoice inv = createAnonInvoice(); LineItem expItem1 = new LineItem(inv, product, QUANTITY1); LineItem expItem2 = new LineItem(inv, product, QUANTITY2); // Exercise inv.addItemQuantity(product, QUANTITY1); inv.addItemQuantity(product, QUANTITY2); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 2); // Verify ﬁrst item LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem1.getInv(), actual.getInv()); assertEquals(expItem1.getProd(), actual.getProd()); assertEquals(expItem1.getQuantity(), actual.getQuantity()); // Verify second item actual = (LineItem)lineItems.get(1); assertEquals(expItem2.getInv(), actual.getInv()); assertEquals(expItem2.getProd(), actual.getProd()); assertEquals(expItem2.getQuantity(), actual.getQuantity()); }\n\nBoth of the preceding examples exhibit Test Code Duplication that is easily noticed. By comparison, it is more challenging to identify duplication when it occurs across Test Methods (page 348) that reside in different Testcase Classes (page 373).\n\nImpact\n\n“Cut and paste” often results in many copies of the same code. This code must be maintained every time the SUT is modiﬁ ed in a way that affects the seman- tics (e.g., number of arguments, argument attributes, returned object attributes, calling sequences) of its methods. This necessity can greatly increase the cost to introduce new functionality (High Test Maintenance Cost; see page 265) because of the effort involved in updating all tests that contain copies of the affected code.\n\nCauses\n\nCause: Cut-and-Paste Code Reuse\n\n“Cut and paste” is a powerful tool for writing code fast but it results in many copies of the same code, each of which must be maintained in parallel.\n\nwww.it-ebooks.info\n\nTest Code Duplication\n\nRoot Cause\n\nCut-and-Paste Code Reuse is often the default way to reuse logic. Developers who focus on details of “how” to do something will often repeat the same code many times because they cannot (or do not take the time to) focus on the big picture (the intent) of the test.\n\nA contributing factor may be a lack of refactoring skills or refactoring expe- rience that keeps developers from extracting the big picture from the detailed code they have written. Of course, time pressure may also be the culprit that keeps the refactoring from occurring. As a result, test code grows more compli- cated over time rather than becoming simpler.\n\nPossible Solution\n\nOnce Test Code Duplication has occurred, the best solution is to use an Extract Method [Fowler] refactoring to create a Test Utility Method (page 599) from one of the examples and then to generalize that method to handle each of the copies. When the Test Code Duplication consists of ﬁ xture setup logic, we end up with Creation Methods (page 415) or Finder Methods (see Test Utility Method). When the logic carries out result veriﬁ cation, we end up with Custom Assertions (page 474) or Veriﬁ cation Methods (see Custom Assertion).\n\nWe can use an Introduce Parameter [JBrains] refactoring to convert any lit- eral constants inside the extracted method into parameters that can be passed in to customize the method’s behavior for each test that calls it.\n\nMore simply, we can avoid most Test Code Duplication by writing the Test Methods in an “outside-in” manner, focusing on their intent. Whenever we need to do something that involves several lines of code, we simply call a nonexis- tent Test Utility Method to do it. We write all our tests this way and then ﬁ ll in implementations of the Test Utility Methods to get the tests to compile and run. (Modern IDEs facilitate this process by providing automatic method skeleton generation at a click of the mouse.)\n\nCause: Reinventing the Wheel\n\nWhile Cut-and-Paste Code Reuse deliberately makes copies of existing code to reduce the effort of writing tests, it is also possible to accidentally write the same sequence of statements in different tests.\n\nRoot Cause\n\nThis problem is primarily caused by a lack of awareness of which Test Utility Methods are available. It can also be caused by a predisposition to write one’s own code rather than reuse code written by others.\n\nwww.it-ebooks.info\n\n215\n\nTest Code Duplication\n\n216\n\nTest Code Duplication\n\nChapter 15 Code Smells\n\nPossible Solution\n\nThe technical solution is largely the same as for Cut-and-Paste Code Reuse but the process solution is somewhat different. The test automater must look around more places to discover which Test Utility Methods are available before reinventing the wheel (i.e., writing new code).\n\nFurther Reading\n\nTest Code Duplication was ﬁ rst described in a paper at XP2001 called “Refac- toring Test Code” [RTC].\n\nwww.it-ebooks.info\n\nTest Logic in Production\n\nTest Logic in Production\n\nThe code that is put into production contains logic that should be exercised only during tests.\n\nThe SUT may contain logic that cannot be run in a test environment. Tests may require the SUT to behave in speciﬁ c ways to allow full test coverage.\n\nSymptoms\n\nThe logic in the SUT is there solely to support testing. This logic may be “extra stuff” that the tests require to gain access to the SUT’s internal state for ﬁ xture setup or result veriﬁ cation purposes. It may also consist of changes that the logic of the system undergoes when it detects that it is being tested.\n\nImpact\n\nWe would prefer not to end up with Test Logic in Production, as it can make the SUT more complex and opens the door to additional kinds of bugs that we would like to avoid. A system that behaves one way in the test lab and an entirely different way in production is a recipe for disaster!\n\nCauses\n\nCause: Test Hook\n\nConditional logic within the SUT determines whether the “real” code or test- speciﬁ c logic is run.\n\nSymptoms\n\nWith this code smell, either there may be no behavioral symptoms or something may go wrong in production. We may see snippets of code in the SUT that look something like this:\n\nif (testing) { return hardCodedCannedData; } else { // the real logic ... return gatheredData; }\n\nwww.it-ebooks.info\n\n217\n\nTest Logic in Production\n\n218\n\nTest Logic in Production\n\nChapter 15 Code Smells\n\nAriane\n\nThe maiden ﬂ ight of the Ariane 5 rocket was a complete disaster: The rocket blew up only 37 seconds after takeoff. The culprit was a seem- ingly innocuous bit of code that was used only while the rocket was on the ground but unfortunately was left running for the ﬁ rst 40 seconds of ﬂ ight. When it tried to assign a 64-bit number representing the sideways velocity of the rocket to a 16-bit ﬁ eld, the navigation computer decided that the rocket was going the wrong way! It tried to correct the course, but the sudden change in direction tore the booster rocket apart. While this is not quite an example of Test Logic in Production (page 217), it certainly does illustrate the risks associated with this type of error.\n\nCould this disaster have been prevented by use of automated tests? While it is difﬁ cult to say with certainty, and one could certainly claim that any number of process changes could have detected this problem before it occurred, it is conceivable that automated tests could have averted this catastrophe.\n\nIn particular, a test should have addressed the boundary condition— namely, what happens when a number exceeds the maximum value stor- able. Such a test would have prevented an exception from occurring for the ﬁ rst time ever in production.\n\nIn addition, the presence of the tests from the Ariane 4 version of the rocket would have documented the maximum down-range velocity. It is quite possible that these tests would have been updated when the Ariane 5 software was being developed and that the new tests would have failed because of the new rocket’s higher speed.\n\nFor a slightly more detailed (and very interesting) description of “the little bug that could,” visit http://www.around.com/ariane.html.\n\nImpact\n\nCode that was not designed to work in production and that has not been veri- ﬁ ed to work properly in the production environment could accidentally be run in production and create serious problems.\n\nThe Ariane 5 rocket blew up 37 seconds after takeoff on its maiden ﬂ ight because a piece of code that was used only while the rocket was on the ground was left running for the ﬁ rst 40 seconds of ﬂ ight. This code tried to assign a 64-bit number representing the sideways velocity of the rocket to a 16-bit\n\nwww.it-ebooks.info\n\nTest Logic in Production\n\nﬁ eld—an operation that convinced the rocket’s navigation computer that it was going the wrong way. (See the sidebar on Ariane on page 218 for more details.) While we believe the Test Hook would never be exercised in produc- tion, do we really want to take this kind of chance?\n\nRoot Cause\n\nIn some cases, the Test Logic in Production is introduced to make the behavior of the SUT more deterministic by returning known (hard-coded) values. In other cases, the Test Logic in Production may have been introduced to avoid execut- ing code that cannot be run in a test environment. Unfortunately, this approach can result in failure to execute that code in the production environment if some- thing is misconﬁ gured.\n\nIn some cases, tests may require that the SUT execute additional code that would otherwise be executed by a depended-on component. For example, code run from a trigger in a database will not run if the database is replaced by a Fake Database (see Fake Object on page 551); thus the test needs to ensure that the equivalent logic is executed from somewhere within the SUT.\n\nPossible Solution\n\nInstead of adding test logic into the production code directly, we can move logic into a substitutable dependency. We can put code that should be run in only pro- duction into a Strategy [GOF] object that is installed by default and replaced by a Null Object [PLOPD3] when running our tests. In contrast, code that should be run only during tests can be put into a Strategy [GOF] object that is conﬁ gured as a Null Object by default. Then, when we want the SUT to execute extra code during testing, we can replace this Strategy object with a test-speciﬁ c version. To ensure this mechanism is conﬁ gured properly, we should have a Constructor Test (see Test Method on page 348) to verify that any variables holding references to Strategy objects are initialized correctly when they are not overridden by the test. It may also be possible to override speciﬁ c methods of the SUT in a Test- Speciﬁ c Subclass (page 579) if the production logic we want to circumvent is localized in overridable methods. This ability is enabled by Self-Calls [WWW].\n\nCause: For Tests Only\n\nCode exists in the SUT strictly for use by tests.\n\nSymptoms\n\nSome of the methods of the SUT are used only by tests. Some of the attributes are public when they really should be private.\n\nwww.it-ebooks.info\n\n219\n\nTest Logic in Production\n\n220\n\nTest Logic in Production\n\nChapter 15 Code Smells\n\nImpact\n\nSoftware that is added to the SUT For Tests Only makes the SUT more complex. It can confuse potential clients of the software’s interface by introducing addi- tional methods that should not be used by any code other than the tests. These methods may have been tested only in very speciﬁ c circumstances, so they might not work in the typical usage patterns used by real client software.\n\nRoot Cause\n\nThe test automater may need to add methods to a class that expose information needed by the test or methods that provide greater control over initialization (such as for the installation of a Test Double; see page 522). Test-driven devel- opment will lead to the creation of these additional methods even though they aren’t really needed by clients. When retroﬁ tting tests onto legacy code, the test automater may need access to information or functionality that is not already exposed.\n\nFor Tests Only can also result when a SUT is used asymmetrically in real life. Automated tests (especially round-trip tests) typically use software in a more symmetric fashion and hence may need methods that the real software clients do not need.\n\nPossible Solution\n\nWe can assure that tests have access to private information by creating a Test- Speciﬁ c Subclass of the SUT, which then provides methods to expose the needed attributes or initialization logic. A test needs to be able to create instances of the subclass instead of the SUT class for this approach to work.\n\nIf for some reason the extra methods cannot be moved to a Test-Speciﬁ c Sub- class, they should be clearly labeled For Tests Only. This can be done by adopt- ing a naming convention such as starting the names with “FTO_”.\n\nCause: Test Dependency in Production\n\nProduction executables depend on test executables.\n\nSymptoms\n\nWe cannot build only the production code; some test code must be included in the build to allow the production code to compile. Alternatively, we might notice that we cannot run the production code if the test executables are not present.\n\nwww.it-ebooks.info\n\nTest Logic in Production\n\nImpact\n\nEven if the production modules do not contain any test code, problems can arise if any of these modules depends on a test module. At minimum, this dependency increases the size of the executable even if none of the test code is actually used in production scenarios. It also opens the door to accidental execution of test code during production.\n\nRoot Cause\n\nTest Dependency in Production is usually caused by a lack of attention to inter-module dependencies. It may also arise when a built-in self-test requires access to parts of the test automation infrastructure, such as Test Utility Methods (page 599) or the Test Automation Framework (page 298), to report test results.\n\nPossible Solution\n\nWe must manage our dependencies carefully to ensure that no production code depends on test code even for innocuous things such as type deﬁ nitions.\n\nAnything required by both test and production code should live in a production\n\nmodule or class that is accessible to both.\n\nCause: Equality Pollution\n\nAnother cause of Test Logic in Production is the implementation of test-speciﬁ c equality in the equals method of the SUT.\n\nSymptoms\n\nEquality Pollution can be difﬁ cult to spot once it has occurred—what is notable is that the SUT doesn’t actually need the equals method to be implemented. In other cases, behavioral symptoms may appear, such as test failure when the equals method is modiﬁ ed to support the speciﬁ c needs of a test or when the deﬁ - nition of equals changes within the SUT as part of a new feature or user story.\n\nImpact\n\nWe may write unnecessary equals methods simply to satisfy tests. We may also change the deﬁ nition of equals so that it no longer satisﬁ es the business requirements.\n\nEquality Pollution may make it difﬁ cult to introduce the equals logic pre- scribed by some new requirement if it already exists to support test-speciﬁ c equality for another test.\n\nwww.it-ebooks.info\n\n221\n\nTest Logic in Production\n\n222\n\nTest Logic in Production\n\nChapter 15 Code Smells\n\nRoot Cause\n\nEquality Pollution is caused by a lack of awareness of the concept of test-speciﬁ c equality. Some early versions of dynamic Mock Object (page 544) generation tools forced us to use the SUT’s deﬁ nition of equals, which led to Equality Pollution.\n\nPossible Solution\n\nWhen a test requires test-speciﬁ c equality, we should use a Custom Asser- tion (page 474) instead of modifying the equals method just so that we can use a built-in Equality Assertion (see Assertion Method on page 362).\n\nWhen using dynamic Mock Object generation tools, we should use a Com- parator [WWW] rather than relying on the equals method supplied by the SUT. We can also implement the equals method on a Test-Speciﬁ c Subclass of an Expected Object (see State Veriﬁ cation on page 462) to avoid adding it to a production class directly.\n\nFurther Reading\n\nFor Tests Only and Equality Pollution were ﬁ rst introduced in a paper at XP2001 called “Refactoring Test Code” [RTC].\n\nwww.it-ebooks.info\n\nChapter 16\n\nBehavior Smells\n\nSmells in This Chapter\n\nAssertion Roulette . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\n\nErratic Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\n\nFragile Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n\nFrequent Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\n\nManual Intervention. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n\nSlow Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\n\n223\n\nwww.it-ebooks.info\n\nBehavior Smells",
      "page_number": 249
    },
    {
      "number": 16,
      "title": "Behavior Smells",
      "start_page": 287,
      "end_page": 322,
      "detection_method": "regex_chapter",
      "content": "224\n\nAssertion Roulette\n\nChapter 16 Behavior Smells\n\nAssertion Roulette\n\nIt is hard to tell which of several assertions within the same test method caused a test failure.\n\nSymptoms\n\nA test fails. Upon examining the output of the Test Runner (page 377), we cannot determine exactly which assertion failed.\n\nImpact\n\nWhen a test fails during an automated Integration Build [SCM], it may be hard to tell exactly which assertion failed. If the problem cannot be reproduced on a developer’s machine (as may be the case if the problem is caused by environ- mental issues or Resource Optimism; see Erratic Test on page 228) ﬁ xing the problem may be difﬁ cult and time-consuming.\n\nCauses\n\nCause: Eager Test\n\nA single test veriﬁ es too much functionality.\n\nSymptoms\n\nA test exercises several methods of the SUT or calls the same method several times interspersed with ﬁ xture setup logic and assertions.\n\npublic void testFlightMileage_asKm2() throws Exception { // set up ﬁxture // exercise constructor Flight newFlight = new Flight(validFlightNumber); // verify constructed object assertEquals(validFlightNumber, newFlight.number); assertEquals(\"\", newFlight.airlineCode); assertNull(newFlight.airline); // set up mileage newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); // verify results int expectedKilometres = 1810; assertEquals( expectedKilometres, actualKilometres);\n\nwww.it-ebooks.info\n\nAssertion Roulette\n\n// now try it with a canceled ﬂight newFlight.cancel(); try { newFlight.getMileageAsKm(); fail(\"Expected exception\"); } catch (InvalidRequestException e) { assertEquals( \"Cannot get cancelled ﬂight mileage\", e.getMessage()); } }\n\nAnother possible symptom is that the test automater wants to modify the Test Automation Framework (page 298) to keep going after an assertion has failed so that the rest of the assertions can be executed.\n\nRoot Cause\n\nAn Eager Test is often caused by trying to minimize the number of unit tests (whether consciously or unconsciously) by verifying many test conditions in a single Test Method (page 348). While this is a good practice for manu- ally executed tests that have “liveware” interpreting the results and adjusting the tests in real time, it just doesn’t work very well for Fully Automated Tests (see page 26).\n\nAnother common cause of Eager Tests is using xUnit to automate customer tests that require many steps, thereby verifying many aspects of the SUT in each test. These tests are necessarily longer than unit tests but care should be taken to keep them as short as possible (but no shorter!).\n\nPossible Solution\n\nFor unit tests, we break up the test into a suite of Single-Condition Tests (see page 45) by teasing apart the Eager Test. It may be possible to do so by using one or more Extract Method [Fowler] refactorings to pull out independent pieces into their own Test Methods. Sometimes it is easier to clone the test once for each test condition and then clean up each Test Method by removing any code that is not required for that particular test conditions. Any code required to set up the ﬁ xture or put the SUT into the correct starting state can be ex- tracted into a Creation Method (page 415). A good IDE or compiler will then help us determine which variables are no longer being used.\n\nIf we are automating customer tests using xUnit, and this effort has resulted in many steps in each test because the work ﬂ ows require complex ﬁ xture setup, we could consider using some other way to set up the ﬁ xture for the latter parts of the test. If we can use Back Door Setup (see Back Door Manipulation on page 327) to create the ﬁ xture for the last part of the test independently of the\n\nwww.it-ebooks.info\n\n225\n\nAssertion Roulette\n\n226\n\nAssertion Roulette\n\nChapter 16 Behavior Smells\n\nﬁ rst part, we can break one test into two, thereby improving our Defect Local- ization (see Goals of Test Automation). We should repeat this process as many times as it takes to make the tests short enough to be readable at a single glance and to Communicate Intent (see page 41) clearly.\n\nCause: Missing Assertion Message\n\nSymptoms\n\nA test fails. Upon examining the output of the Test Runner, we cannot deter- mine exactly which assertion failed.\n\nRoot Cause\n\nThis problem is caused by the use of Assertion Method (page 362) calls with identical or missing Assertion Messages (page 370). It is most commonly encountered when running tests using a Command-Line Test Runner (see Test Runner) or a Test Runner that is not integrated with the program text editor or development environment.\n\nIn the following test, we have a number of Equality Assertions (see Assertion\n\nMethod):\n\npublic void testInvoice_addLineItem7() { LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\nWhen an assertion fails, will we know which one it was? An Equality Assertion typically prints out both the expected and the actual values—but it may prove difﬁ cult to tell which assertion failed if the expected values are similar or print out cryptically. A good rule of thumb is to include at least a minimal Assertion Message whenever we have more than one call to the same kind of Assertion Method.\n\nPossible Solution\n\nIf the problem occurred while we were running a test using a Graphical Test Runner (see Test Runner) with IDE integration, we should be able to click on the appropriate line in the stack traceback to have the IDE highlight the failed\n\nwww.it-ebooks.info\n\nAssertion Roulette\n\nassertion. Failing this, we can turn on the debugger and single-step through the test to see which assertion statement fails.\n\nIf the problem occurred while we were running a test using a Command- Line Test Runner, we can try running the test from a Graphical Test Runner with IDE integration to determine the offending assertion. If that doesn’t work, we may have to resort to using line numbers (if available) or apply a process of elimination to deduce which of the assertions it couldn’t be to narrow down the possibilities. Of course, we could just bite the bullet and add a unique Assertion Message (even just a number!) to each call to an Assertion Method.\n\nFurther Reading\n\nAssertion Roulette and Eager Test were ﬁ rst described in a paper presented at XP2001 called “Refactoring Test Code” [RTC].\n\nwww.it-ebooks.info\n\n227\n\nAssertion Roulette\n\n228\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nErratic Test\n\nOne or more tests behave erratically; sometimes they pass and sometimes they fail.\n\nSymptoms\n\nWe have one or more tests that run but give different results depending on when they are run and who is running them. In some cases, the Erratic Test will con- sistently give the same results when run by one developer but fail when run by someone else or in a different environment. In other cases, the Erratic Test will give different results when run from the same Test Runner (page 377).\n\nImpact\n\nWe may be tempted to remove the failing test from the suite to “keep the bar green” but this would result in an (intentional) Lost Test (see Production Bugs on page 268). If we choose to keep the Erratic Test in the test suite despite the failures, the known failure may obscure other problems, such as another issue detected by the same tests. Just having a test fail can cause us to miss additional failures because it is much easier to see the change from a green bar to a red bar than to notice that two tests are failing instead of just the one we expected.\n\nTroubleshooting Advice\n\nErratic Tests can be challenging to troubleshoot because so many potential causes exist. If the cause cannot be easily determined, it may be necessary to collect data systematically over a period of time. Where (in which environments) did the tests pass, and where did they fail? Were all the tests being run or just a subset of them? Did any change in behavior occur when the test suite was run several times in a row? Did any change in behavior occur when it was run from several Test Runners at the same time?\n\nOnce we have some data, it should be easier to match up the observed symp- toms with those listed for each of the potential causes and to narrow the list of possibilities to a handful of candidates. Then we can collect some more data focusing on differences in symptoms between the possible causes. Figure 16.1 summarizes the process for determining which cause of an Erratic Test we are dealing with.\n\nwww.it-ebooks.info\n\nErratic Test\n\nDifferent Different Results Every Results Every Run? Run?\n\nYes Yes\n\nNo No\n\nOnly with Only with Multiple Test Multiple Test Runners? Runners?\n\nYes Yes\n\nResults Results Vary for Tests Vary for Tests vs. Suites? vs. Suites?\n\nNo No\n\nYes Yes\n\nProbably Test Probably Test Run War Run War\n\nNo No\n\nYes Yes\n\nProbably Probably Resource Resource Leakage Leakage\n\nGets Gets Worse with Worse with Time? Time?\n\nNo No\n\nProbably Non- Probably Non- Deterministic Deterministic Test Test\n\nHappens Happens When Test Run When Test Run Alone? Alone?\n\nYes Yes\n\nNo No\n\nDifferent Different Results for First Results for First Run? Run?\n\nYes Yes\n\nNo No\n\nProbably Probably Unrepeatable Unrepeatable Test Test\n\nYes Yes\n\nResults Results Vary by Vary by Location? Location?\n\nNo No\n\nProbably Probably Lonely Test Lonely Test\n\nProbably Probably Interacting Interacting Tests or Suites Tests or Suites\n\nProbably Probably Resource Resource Optimism Optimism\n\nHire an Hire an xUnit xUnit Expert! Expert!\n\nFigure 16.1 Troubleshooting an Erratic Test.\n\nCauses\n\nTests may behave erratically for a number of reasons. The underlying cause can usually be determined through some persistent sleuthing by paying attention to patterns regarding how and when the tests fail. Some of the causes are common enough to warrant giving them names and speciﬁ c advice for rectifying them.\n\nCause: Interacting Tests\n\nTests depend on other tests in some way. Note that Interacting Test Suites and Lonely Test are speciﬁ c variations of Interacting Tests.\n\nSymptoms\n\nA test that works by itself suddenly fails in the following circumstances:\n\nAnother test is added to (or removed from) the suite.\n\nAnother test in the suite fails (or starts to pass).\n\nThe test (or another test) is renamed or moved in the source ﬁ le.\n\nA new version of the Test Runner is installed.\n\nRoot Cause\n\nInteracting Tests usually arise when tests use a Shared Fixture (page 317), with one test depending in some way on the outcome of another test. The cause of Interacting Tests can be described from two perspectives:\n\nwww.it-ebooks.info\n\n229\n\nErratic Test\n\n230\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nThe mechanism of interaction\n\nThe reason for interaction\n\nThe mechanism for interaction could be something blatantly obvious—for example, testing an SUT that includes a database—or it could be more subtle. Anything that outlives the lifetime of the test can lead to interactions; static variables can be depended on to cause Interacting Tests and, therefore, should be avoided in both the SUT and the Test Automation Framework (page 298)! See the sidebar “There’s Always an Exception” on page 384 for an exam- ple of the latter problem. Singletons [GOF] and Registries [PEAA] are good examples of things to avoid in the SUT if at all possible. If we must use them, it is best to include a mechanism to reinitialize their static variables at the beginning of each test.\n\nTests may interact for a number of reasons, either by design or by accident:\n\nDepending on the ﬁ xture constructed by the ﬁ xture setup phase of\n\nanother test\n\nDepending on the changes made to the SUT during the exercise SUT\n\nphase of another test\n\nA collision caused by some mutually exclusive action (which may be either of the problems mentioned above) between two tests run in the same test run\n\nThe dependencies may suddenly cease to be satisﬁ ed if the depended-on test\n\nIs removed from the suite,\n\nIs modiﬁ ed to no longer change the state of the SUT,\n\nFails in its attempt to change the state of the SUT, or\n\nIs run after the test in question (because it was renamed or moved to a\n\ndifferent Testcase Class; see page 373).\n\nSimilarly, collisions may start occurring when the colliding test is\n\nAdded to the suite,\n\nPasses for the ﬁ rst time, or\n\nRuns before the dependent test.\n\nIn many of these cases, multiple tests will fail. Some of the tests may fail for a good reason—namely, the SUT is not doing what it is supposed to do. Depen- dent tests may fail for the wrong reason—because they were coded to depend\n\nwww.it-ebooks.info\n\nErratic Test\n\non other tests’ success. As a result, they may be giving a “false-positive” (false- failure) indication.\n\nIn general, depending on the order of test execution is not a wise approach because of the problems described above. Most variants of the xUnit frame- work do not make any guarantees about the order of test execution within a test suite. (TestNG, however, promotes interdependencies between tests by pro- viding features to manage the dependencies.)\n\nPossible Solution\n\nUsing a Fresh Fixture (page 311) is the preferred solution for Interacting Tests; it is almost guaranteed to solve the problem. If we must use a Shared Fixture, we should consider using an Immutable Shared Fixture (see Shared Fixture) to pre- vent the tests from interacting with one another through changes in the ﬁ xture by creating from scratch those parts of the ﬁ xture that they intend to modify.\n\nIf an unsatisﬁ ed dependency arises because another test does not create the expected objects or database data, we should consider using Lazy Setup (page 435) to create the objects or data in both tests. This approach ensures that the ﬁ rst test to execute creates the objects or data for both tests. We can put the ﬁ xture setup code into a Creation Method (page 415) to avoid Test Code Duplication (page 213). If the tests are on different Testcase Classes, we can move the ﬁ xture setup code to a Test Helper (page 643).\n\nSometimes the collision may be caused by objects or database data that are created in our test but not cleaned up afterward. In such a case, we should con- sider implementing Automated Fixture Teardown (see Automated Teardown on page 503) to remove them safely and efﬁ ciently.\n\nA quick way to ﬁ nd out whether any tests depend on one another is to run the tests in a different order than the normal order. Running the entire test suite in reverse order, for example, would do the trick nicely. Doing so regularly would help avoid accidental introduction of Interacting Tests.\n\nCause: Interacting Test Suites\n\nIn this special case of Interacting Tests, the tests are in different test suites.\n\nSymptoms\n\nA test passes when it is run in its own test suite but fails when it is run within a Suite of Suites (see Test Suite Object on page 387).\n\nSuite1.run()--> Green Suite2.run()--> Green Suite(Suite1,Suite2).run()--> Test C in Suite2 fails\n\nwww.it-ebooks.info\n\n231\n\nErratic Test\n\n232\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nRoot Cause\n\nInteracting Test Suites usually occur when tests in separate test suites try to cre- ate the same resource. When they are run in the same suite, the ﬁ rst one succeeds but the second one fails while trying to create the resource.\n\nThe nature of the problem may be obvious just by looking at the test failure or by reading the failed Test Method (page 348). If it is not, we can try remov- ing other tests from the (nonfailing) test suite, one by one. When the failure stops occurring, we simply examine the last test we removed for behaviors that might cause the interactions with the other (failing) test. In particular, we need to look at anything that might involve a Shared Fixture, including all places where class variables are initialized. These locations may be within the Test Method itself, within a setUp method, or in any Test Utility Methods (page 599) that are called.\n\nWarning: There may be more than one pair of tests interacting in the same test suite! The interaction may also be caused by the Suite Fixture Setup (page 441) or Setup Decorator (page 447) of several Testcase Classes clashing rather than by a conﬂ ict between the actual Test Methods!\n\nVariants of xUnit that use Testcase Class Discovery (see Test Discovery on page 393), such as NUnit, may appear to not use test suites. In reality, they do—they just don’t expect the test automaters to use a Test Suite Factory (see Test Enumeration on page 399) to identify the Test Suite Object to the Test Runner.\n\nPossible Solution\n\nWe could, of course, eliminate this problem entirely by using a Fresh Fixture. If this solution isn’t within our scope, we could try using an Immutable Shared Fixture to prevent the tests’ interaction.\n\nIf the problem is caused by leftover objects or database rows created by one test that conﬂ ict with the ﬁ xture being created by a later test, we should con- sider using Automated Teardown to eliminate the need to write error-prone cleanup code.\n\nCause: Lonely Test\n\nA Lonely Test is a special case of Interacting Tests. In this case, a test can be run as part of a suite but cannot be run by itself because it depends on something in a Shared Fixture that was created by another test (e.g., Chained Tests; see page 454) or by suite-level ﬁ xture setup logic (e.g., a Setup Decorator).\n\nWe can address this problem by converting the test to use a Fresh Fixture or\n\nby adding Lazy Setup logic to the Lonely Test to allow it to run by itself.\n\nwww.it-ebooks.info\n\nErratic Test\n\nCause: Resource Leakage\n\nTests or the SUT consume ﬁ nite resources.\n\nSymptoms\n\nTests run more and more slowly or start to fail suddenly. Reinitializing the Test Runner, SUT, or Database Sandbox (page 650) clears up the problem—only to have it reappear over time.\n\nRoot Cause\n\nTests or the SUT consume ﬁ nite resources by allocating those resources and failing to free them afterward. This practice may make the tests run more slowly. Over time, all the resources are used up and tests that depend on them start to fail.\n\nThis problem can be caused by one of two types of bugs:\n\nThe SUT fails to clean up the resources properly. The sooner we detect\n\nthis behavior, the sooner we can track it down and ﬁ x it.\n\nThe tests themselves cause the resource leakage by allocating resources as part of ﬁ xture setup and failing to clean them up during ﬁ xture teardown.\n\nPossible Solution\n\nIf the problem lies in the SUT, then the tests have done their job and we can ﬁ x the bug. If the tests are causing the resource leakage, then we must eliminate the source of the leaks. If the leaks are caused by failure to clean up properly when tests fail, we may need to ensure that all tests do Guaranteed In-line Teardown (see In-line Teardown on page 509) or convert them to use Automated Teardown.\n\nIn general, it is a good idea to set the size of all resource pools to 1. This choice will cause the tests to fail much sooner, allowing us to more quickly determine which tests are causing the leak(s).\n\nCause: Resource Optimism\n\nA test that depends on external resources has nondeterministic results depending on when or where it is run.\n\nSymptoms\n\nA test passes when it is run in one environment and fails when it is run in another environment.\n\nwww.it-ebooks.info\n\n233\n\nErratic Test\n\n234\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nRoot Cause\n\nA resource that is available in one environment is not available in another environment.\n\nPossible Solution\n\nIf possible, we should convert the test to use a Fresh Fixture by creating the resource as part of the test’s ﬁ xture setup phase. This approach ensures that the resource exists wherever it is run. It may necessitate the use of relative address- ing of ﬁ les to ensure that the speciﬁ c location in the ﬁ le system exists regardless of where the SUT is executed.\n\nIf an external resource must be used, the resources should be stored in the source code repository [SCM] so that all Test Runners run in the same en- vironment.\n\nCause: Unrepeatable Test\n\nA test behaves differently the ﬁ rst time it is run compared with how it behaves on subsequent test runs. In effect, it is interacting with itself across test runs.\n\nSymptoms\n\nEither a test passes the ﬁ rst time it is run and fails on all subsequent runs, or it fails the ﬁ rst time and passes on all subsequent runs. Here’s an example of what “Pass-Fail-Fail” might look like:\n\nSuite.run()--> Green Suite.run()--> Test C fails Suite.run()--> Test C fails User resets something Suite.run()--> Green Suite.run()--> Test C fails\n\nHere’s an example of what “Fail-Pass-Pass” might look like:\n\nSuite.run()--> Test C fails Suite.run()--> Green Suite.run()--> Green User resets something Suite.run()--> Test C fails Suite.run()--> Green\n\nBe forewarned that if our test suite contains several Unrepeatable Tests, we may see results that look more like this:\n\nSuite.run()--> Test C fails Suite.run()--> Test X fails Suite.run()--> Test X fails\n\nwww.it-ebooks.info\n\nErratic Test\n\nUser resets something Suite.run()--> Test C fails Suite.run()--> Test X fails\n\nTest C exhibits the “Fail-Pass-Pass” behavior, while test X exhibits the “Pass- Fail-Fail” behavior at the same time. It is easy to miss this problem because we see a red bar in each case; we notice the difference only if we look closely to see which tests fail each time we run them.\n\nRoot Cause\n\nThe most common cause of an Unrepeatable Test is the use—either deliberate or accidental—of a Shared Fixture. A test may be modifying the test ﬁ xture such that, during a subsequent run of the test suite, the ﬁ xture is in a different state. Although this problem most commonly occurs with a Prebuilt Fixture (see Shared Fixture), the only true prerequisite is that the ﬁ xture outlasts the test run.\n\nThe use of a Database Sandbox may isolate our tests from other developers’ tests but it won’t prevent the tests we run from colliding with themselves or with other tests we run from the same Test Runner.\n\nThe use of Lazy Setup to initialize a ﬁ xture holding class variable can result in the test ﬁ xture not being reinitialized on subsequent runs of the same test suite. In effect, we are sharing the test ﬁ xture between all runs started from the same Test Runner.\n\nPossible Solution\n\nBecause a persistent Shared Fixture is a prerequisite for an Unrepeatable Test, we can eliminate the problem by using a Fresh Fixture for each test. To fully isolate the tests, we must make sure that no shared resource, such as a Database Sandbox, outlasts the lifetimes of the individual tests. One option is to replace a database with a Fake Database (see Fake Object on page 551). If we must work with a persistent data store, we should use Distinct Generated Values (see Generated Value on page 723) for all database keys to ensure that we create different objects for each test and test run. The other alternative is to implement Automated Teardown to remove all newly created objects and rows safely and efﬁ ciently.\n\nCause: Test Run War\n\nTest failures occur at random when several people are running tests simultaneously.\n\nwww.it-ebooks.info\n\n235\n\nErratic Test\n\n236\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nSymptoms\n\nWe are running tests that depend on some shared external resource such as a database. From the perspective of a single person running tests, we might see something like this:\n\nSuite.run() --> Test 3 fails Suite.run() --> Test 2 fails Suite.run() --> All tests pass Suite.run() --> Test 1 fails\n\nUpon describing our problem to our teammates, we discover that they are having the same problem at the same time. When only one of us runs tests, all of the tests pass.\n\nImpact\n\nA Test Run War can be very frustrating because the probability of it occurring increases the closer we get to a code cutoff deadline. This isn’t just Murphy’s law kicking in: It really does happen more often at this point! We tend to commit smaller changes at more frequent intervals as the deadline approaches (think “last-minute bug ﬁ xing”!). This, in turn, increases the likelihood that someone else will be running the test suite at the same time, which itself increases the like- lihood of test collisions between test runs occurring at the same time.\n\nRoot Cause\n\nA Test Run War can happen only when we have a globally Shared Fixture that various tests access and sometimes modify. This shared ﬁ xture could be a ﬁ le that must be opened or read by either a test or the SUT, or it could consist of the records in a test database.\n\nDatabase contention can be caused by the following activities:\n\nTrying to update or delete a record while another test is also updating\n\nthe same record\n\nTrying to update or delete a record while another test has a read lock\n\n(pessimistic locking) on the same record\n\nFile contention can be caused by an attempt to access a ﬁ le that has already been opened by another instance of the test running from a different Test Runner.\n\nPossible Solution\n\nUsing a Fresh Fixture is the preferred solution for a Test Run War. An even sim- pler solution is to give each Test Runner his or her own Database Sandbox. This\n\nwww.it-ebooks.info\n\nErratic Test\n\nshould not involve making any changes to the tests but will completely eliminate the possibility of a Test Run War. It will not, however, eliminate other sources of Erratic Tests because the tests can still interact through the Shared Fixture (the Database Sandbox). Another option is to switch to an Immutable Shared Fixture by having each test create new objects whenever it plans to change those objects. This approach does require changes to the Test Methods.\n\nIf the problem is caused by leftover objects or database rows created by one test that pollutes the ﬁ xture of a later test, another solution is using Automated Teardown to clean up after each test safely and efﬁ ciently. This measure, by itself, is unlikely to completely eliminate a Test Run War but it might reduce its frequency.\n\nCause: Nondeterministic Test\n\nTest failures occur at random, even when only a single Test Runner is running tests.\n\nSymptoms\n\nWe are running tests and the results vary each time we run them, as shown here:\n\nSuite.run() --> Test 3 fails Suite.run() --> Test 3 crashes Suite.run() --> All tests pass Suite.run() --> Test 3 fails\n\nAfter comparing notes with our teammates, we rule out a Test Run War either because we are the only person running tests or because the test ﬁ xture is not shared between users or computers.\n\nAs with an Unrepeatable Test, having multiple Nondeterministic Tests in the same test suite can make it more difﬁ cult to detect the failure/error pat- tern: It looks like different tests are failing rather than a single test producing different results.\n\nImpact\n\nDebugging Nondeterministic Tests can be very time-consuming and frustrating because the code executes differently each time. Reproducing the failure can be problematic, and characterizing exactly what causes the failure may require many attempts. (Once the cause has been characterized, it is often a straight- forward process to replace the random value with a value known to cause the problem.)\n\nwww.it-ebooks.info\n\n237\n\nErratic Test\n\n238\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nRoot Cause\n\nNondeterministic Tests are caused by using different values each time a test is run. Sometimes, of course, it is a good idea to use different values each time the same test is run. For example, Distinct Generated Values may legitimately be used as unique keys for objects stored in a database. Use of generated values as input to an algorithm where the behavior of the SUT is expected to differ for different values can cause Nondeterministic Tests, however, as in the following examples:\n\nInteger values where negative (or even zero) values are treated differ- ently by the system, or where there is a maximum allowable value. If we generate a value at random, the test could fail in some test runs and pass on others.\n\nString values where the length of a string has minimum or maximum allowed values. This problem often occurs accidentally when we gener- ate a random or unique numeric value and then convert it to a string representation without using an explicit format that guarantees the length is constant.\n\nIt might seem like a good idea to use random values because they would improve our test coverage. Unfortunately, this tactic decreases our understanding of the test coverage and the repeatability of our tests (which violates the Repeatable Test principle; see page 26).\n\nAnother potential cause of Nondeterministic Tests is the use of Conditional Test Logic (page 200) in our tests. Its inclusion can result in different code paths being executed on different test runs, which in turn makes our tests non- deterministic. A common “reason” cited for doing so is the Flexible Test (see Conditional Test Logic). Anything that makes the tests less than completely deterministic is a bad idea!\n\nPossible Solution\n\nThe ﬁ rst step is to make our tests repeatable by ensuring that they execute in a completely linear fashion by removing any Conditional Test Logic. Then we can go about replacing any random values with deterministic values. If this results in poor test coverage, we can add more tests for the interesting cases we aren’t cov- ering. A good way to determine the best set of input values is to use the bound- ary values of the equivalence classes. If their use results in a lot of Test Code Duplication, we can extract a Parameterized Test (page 607) or put the input val- ues and the expected results into a ﬁ le read by a Data-Driven Test (page 288).\n\nwww.it-ebooks.info\n\nFragile Test\n\nFragile Test\n\nA test fails to compile or run when the SUT is changed in ways that do not affect the part the test is exercising.\n\nSymptoms\n\nWe have one or more tests that used to run and pass but now either fail to compile and run or fail when they are run. When we have changed the behavior of the SUT in question, such a change in test results is expected. When we don’t think the change should have affected the tests that are fail- ing or we haven’t changed any production code or tests, we have a case of Fragile Tests.\n\nPast efforts at automated testing have often run afoul of the “four sensitivities” of automated tests. These sensitivities are what cause Fully Automated Tests (see page 26) that previously passed to suddenly start failing. The root cause for tests failing can be loosely classiﬁ ed into one of these four sensitivities. Although each sensitivity may be caused by a variety of speciﬁ c test coding behaviors, it is useful to understand the sensitivities in their own right.\n\nImpact\n\nFragile Tests increase the cost of test maintenance by forcing us to visit many more tests each time we modify the functionality of the system or the ﬁ xture. They are particularly deadly when projects rely on highly incremental delivery, as in agile development (such as eXtreme Programming).\n\nTroubleshooting Advice\n\nWe need to look for patterns in how the tests fail. We ask ourselves, “What do all of the broken tests have in common?” The answer to this question should help us understand how the tests are coupled to the SUT. Then we look for ways to minimize this coupling.\n\nFigure 16.2 summarizes the process for determining which sensitivity we are\n\ndealing with.\n\nwww.it-ebooks.info\n\n239\n\nFragile Test\n\n240\n\nFragile Test\n\nChapter 16 Behavior Smells\n\nAre the Tests Are the Tests Compiling? Compiling?\n\nNo No\n\nProbably Interface Probably Interface Sensitivity Sensitivity\n\nYes Yes\n\nPossibly Interface Possibly Interface Sensitivity Sensitivity\n\nYes Yes\n\nAre the Tests Are the Tests Erroring? Erroring?\n\nNo No\n\nPossibly Not Possibly Not Fragile Test Fragile Test\n\nYes Yes\n\nHave the Failing Have the Failing Tests Changed? Tests Changed?\n\nNo No\n\nProbably Behavior Probably Behavior Sensitivity Sensitivity\n\nYes Yes\n\nHas Some Has Some Code Changed? Code Changed?\n\nNo No\n\nProbably Data Probably Data Sensitivity Sensitivity\n\nYes Yes\n\nHas the Test Has the Test Data Changed? Data Changed?\n\nNo No\n\nProbably Context Probably Context Sensitivity Sensitivity\n\nFigure 16.2 Troubleshooting a Fragile Test.\n\nThe general sequence is to ﬁ rst ask ourselves whether the tests are failing to compile; if so, Interface Sensitivity is likely to blame. With dynamic languages we may see type incompatibility test errors at runtime—another sign of Interface Sensitivity.\n\nIf the tests are running but the SUT is providing incorrect results, we must ask ourselves whether we have changed the code. If so, we can try backing out of the latest code changes to see if that ﬁ xes the problem. If that tactic stops the failing tests,1 then we had Behavior Sensitivity.\n\nIf the tests still fail with the latest code changes backed out, then something else must have changed and we must be dealing with either Data Sensitiv- ity or Context Sensitivity. The former occurs only when we use a Shared Fix- ture (page 317) or we have modiﬁ ed ﬁ xture setup code; otherwise, we must have a case of Context Sensitivity.\n\nWhile this sequence of asking questions isn’t foolproof, it will give the right\n\nanswer probably nine times out of ten. Caveat emptor!\n\nCauses\n\nFragile Tests may be the result of several different root causes. They may be a sign of Indirect Testing (see Obscure Test on page 186)—that is, using the objects we modiﬁ ed to access other objects—or they may be a sign that we have Eager Tests (see Assertion Roulette on page 224) that are verifying too much functionality. Fragile Tests may also be symptoms of overcoupled software that is hard to test in small pieces (Hard-to-Test Code; see page 209) or our lack of experience with unit testing using Test Doubles (page 522) to test pieces in isola- tion (Overspeciﬁ ed Software).\n\n1 Other tests may fail because we have removed the code that made them pass—but at least we have established which part of the code they depend on.\n\nwww.it-ebooks.info\n\nFragile Test\n\nRegardless of their root cause, Fragile Tests usually show up as one of the four sensitivities. Let’s start by looking at them in a bit more detail; we’ll then examine some more detailed examples of how speciﬁ c causes change test output.\n\nCause: Interface Sensitivity\n\nInterface Sensitivity occurs when a test fails to compile or run because some part of the interface of the SUT that the test uses has changed.\n\nSymptoms\n\nIn statically typed languages, Interface Sensitivity usually shows up as a failure to compile. In dynamically typed languages, it shows up only when we run the tests. A test written in a dynamically typed language may experience a test error when it invokes an application programming interface (API) that has been modi- ﬁ ed (via a method name change or method signature change). Alternatively, the test may fail to ﬁ nd a user interface element it needs to interact with the SUT via a user interface. Recorded Tests (page 278) that interact with the SUT through a user interface2 are particularly prone to this problem.\n\nPossible Solution\n\nThe cause of the failures is usually reasonably apparent. The point at which the test fails (to compile or execute) will usually point out the location of the prob- lem. It is rare for the test to continue to run beyond the point of change—after all, it is the change itself that causes the test error.\n\nWhen the interface is used only internally (within the organization or applica- tion) and by automated tests, SUT API Encapsulation (see Test Utility Method on page 599) is the best solution for Interface Sensitivity. It reduces the cost and impact of changes to the API and, therefore, does not discourage necessary changes from being made. A common way to implement SUT API Encapsula- tion is through the deﬁ nition of a Higher-Level Language (see page 41) that is used to express the tests. The verbs in the test language are translated into the appropriate method calls by the encapsulation layer, which is then the only soft- ware that needs to be modiﬁ ed when the interface is altered in somewhat back- ward-compatible ways. The “test language” can be implemented in the form of Test Utility Methods such as Creation Methods (page 415) and Veriﬁ cation Methods (see Custom Assertion on page 474) that hide the API of the SUT from the test.\n\n2 Often called “screen scraping.”\n\nwww.it-ebooks.info\n\n241\n\nFragile Test\n\n242\n\nFragile Test\n\nChapter 16 Behavior Smells\n\nThe only other way to avoid Interface Sensitivity is to put the interface under strict change control. When the clients of the interface are external and anonymous (such as the clients of Windows DLLs), this tactic may be the only viable alternative. In these cases, a protocol usually applies to mak- ing changes to interfaces. That is, all changes must be backward compatible; before older versions of methods can be removed, they must be deprecated, and deprecated methods must exist for a minimum number of releases or elapsed time.\n\nCause: Behavior Sensitivity\n\nBehavior Sensitivity occurs when changes to the SUT cause other tests to fail.\n\nSymptoms\n\nA test that once passed suddenly starts failing when a new feature is added to the SUT or a bug is ﬁ xed.\n\nRoot Cause\n\nTests may fail because the functionality they are verifying has been modiﬁ ed. This outcome does not necessarily signal a case of Behavior Sensitivity because it is the whole reason for having regression tests. It is a case of Behavior Sensitivity in any of the following circumstances:\n\nThe functionality the regression tests use to set up the pre-test state of\n\nthe SUT has been modiﬁ ed.\n\nThe functionality the regression tests use to verify the post-test state of\n\nthe SUT has been modiﬁ ed.\n\nThe code the regression tests use to tear down the ﬁ xture has been\n\nchanged.\n\nIf the code that changed is not part of the SUT we are verifying, then we are dealing with Context Sensitivity. That is, we may be testing too large a SUT. In such a case, what we really need to do is to separate the SUT into the part we are verifying and the components on which that part depends.\n\nPossible Solution\n\nAny newly incorrect assumptions about the behavior of the SUT used during ﬁ xture setup may be encapsulated behind Creation Methods. Similarly, assump- tions about the details of post-test state of the SUT can be encapsulated in Cus- tom Assertions or Veriﬁ cation Methods. While these measures won’t eliminate\n\nwww.it-ebooks.info\n\nFragile Test\n\nthe need to update test code when the assumptions change, they certainly do reduce the amount of test code that needs to be changed.\n\nCause: Data Sensitivity\n\nData Sensitivity occurs when a test fails because the data being used to test the SUT has been modiﬁ ed. This sensitivity most commonly arises when the con- tents of the test database change.\n\nSymptoms\n\nA test that once passed suddenly starts failing in any of the following circum- stances:\n\nData is added to the database that holds the pre-test state of the SUT.\n\nRecords in the database are modiﬁ ed or deleted.\n\nThe code that sets up a Standard Fixture (page 305) is modiﬁ ed.\n\nA Shared Fixture is modiﬁ ed before the ﬁ rst test that uses it.\n\nIn all of these cases, we must be using a Standard Fixture, which may be either a Fresh Fixture (page 311) or a Shared Fixture such as a Prebuilt Fixture (see Shared Fixture).\n\nRoot Cause\n\nTests may fail because the result veriﬁ cation logic in the test looks for data that no longer exists in the database or uses search criteria that accidentally include newly added records. Another potential cause of failure is that the SUT is being exercised with inputs that reference missing or modiﬁ ed data and, therefore, the SUT behaves differently.\n\nIn all cases, the tests make assumptions about which data exist in the data-\n\nbase—and those assumptions are violated.\n\nPossible Solution\n\nIn those cases where the failures occur during the exercise SUT phase of the test, we need to look at the pre-conditions of the logic we are exercising and make sure they have not been affected by recent changes to the database.\n\nIn most cases, the failures occur during result veriﬁ cation. We need to examine the result veriﬁ cation logic to ensure that it does not make any un- reasonable assumptions about which data exists. If it does, we can modify the veriﬁ cation logic.\n\nwww.it-ebooks.info\n\n243\n\nFragile Test\n\n244\n\nFragile Test\n\nChapter 16 Behavior Smells\n\nWhy Do We Need 100 Customers?\n\nA software development coworker of mine was working on a project as an analyst. One day, the manager she was working for came into her ofﬁ ce and asked, “Why have you requested 100 unique customers be cre- ated in the test database instance?”\n\nAs a systems analyst, my coworker was responsible for helping the busi- ness analysts deﬁ ne the requirements and the acceptance tests for a large, complex project. She wanted to automate the tests but had to overcome several hurdles. One of the biggest hurdles was the fact that the SUT got much of its data from an upstream system—it was too complex to try to generate this data manually.\n\nThe systems analyst came up with a way to generate XML from tests captured in spreadsheets. For the ﬁ xture setup part of the tests, she trans- formed the XML into QaRun (a Record and Playback Test tool—see Recorded Test on page 278) scripts that would load the data into the upstream system via the user interface. Because it took a while to run these scripts and for the data to make its way downstream to the SUT, the systems analyst had to run these scripts ahead of time. This meant that a Fresh Fixture (page 311) strategy was unachievable; a Prebuilt Fix- ture (page 429) was the best she could do. In an attempt to avoid the Interacting Tests (see Erratic Test on page 228) that were sure to result from a Shared Fixture (page 317), the systems analyst decided to imple- ment a virtual Database Sandbox (page 650) using a Database Partition- ing Scheme based on a unique customer number for each test. This way, any side effects of one test couldn’t affect any other tests.\n\nGiven that she had about 100 tests to automate, the systems analyst needed about 100 test customers deﬁ ned in the database. And that’s what she told her manager.\n\nThe failure can show up in the result veriﬁ cation logic even if the problem is that the inputs of the SUT refer to nonexistent or modiﬁ ed data. This may require ex- amining the “after” state of the SUT (which differs from the expected post-test state) and tracing it back to discover why it does not match our expectations. This should expose the mismatch between SUT inputs and the data that existed before the test started executing.\n\nThe best solution to Data Sensitivity is to make the tests independent of the existing contents of the database—that is, to use a Fresh Fixture. If this is not possible, we can try using some sort of Database Partitioning Scheme\n\nwww.it-ebooks.info\n\nFragile Test\n\n(see Database Sandbox on page 650) to ensure that the data modiﬁ ed for one test does not overlap with the data used by other tests. (See the sidebar “Why Do We Need 100 Customers?” on page 244 for an example.)\n\nAnother solution is to verify that the right changes have been made to the data. Delta Assertions (page 485) compare before and after “snapshots” of the data, thereby ignoring data that hasn’t changed. They eliminate the need to hard-code knowledge about the entire ﬁ xture into the result veriﬁ cation phase of the test.\n\nCause: Context Sensitivity\n\nContext Sensitivity occurs when a test fails because the state or behavior of the context in which the SUT executes has changed in some way.\n\nSymptoms\n\nA test that once passed suddenly starts failing for mysterious reasons. Unlike with an Erratic Test (page 228), the test produces consistent results when run repeatedly over a short period of time. What is different is that it consistently fails regardless of how it is run.\n\nRoot Cause\n\nTests may fail for two reasons:\n\nThe functionality they are verifying depends in some way on the time\n\nor date.\n\nThe behavior of some other code or system(s) on which the SUT\n\ndepends has changed.\n\nA major source of Context Sensitivity is confusion about which SUT we are intending to verify. Recall that the SUT is whatever piece of software we are intend- ing to verify. When unit testing, it should be a very small part of the overall system or application. Failure to isolate the speciﬁ c unit (e.g., class or method) is bound to lead to Context Sensitivity because we end up testing too much software all at once. Indirect inputs that should be controlled by the test are then left to chance. If someone then modiﬁ es a depended-on component (DOC), our tests fail.\n\nTo eliminate Context Sensitivity, we must track down which indirect input to the SUT has changed and why. If the system contains any date- or time-related logic, we should examine this logic to see whether the length of the month or other similar factors could be the cause of the problem.\n\nIf the SUT depends on input from any other systems, we should examine these inputs to see if anything has changed recently. Logs of previous interactions\n\nwww.it-ebooks.info\n\n245\n\nFragile Test\n\n246\n\nFragile Test\n\nAlso known as: Overcoupled Test\n\nChapter 16 Behavior Smells\n\nwith these other systems are very useful for comparison with logs of the failure scenarios.\n\nIf the problem comes and goes, we should look for patterns related to when it passes and when it fails. See Erratic Test for a more detailed discussion of possible causes of Context Sensitivity.\n\nPossible Solution\n\nWe need to control all the inputs of the SUT if our tests are to be deterministic. If we depend on inputs from other systems, we may need to control these inputs by using a Test Stub (page 529) that is conﬁ gured and installed by the test. If the system contains any time- or date-speciﬁ c logic, we need to be able to control the system clock as part of our testing. This may necessitate stubbing out the system clock with a Virtual Clock [VCTP] that gives the test a way to set the starting time or date and possibly to simulate the passage of time.\n\nCause: Overspeciﬁ ed Software\n\nA test says too much about how the software should be structured or behave. This form of Behavior Sensitivity (see Fragile Test on page 239) is associated with the style of testing called Behavior Veriﬁ cation (page 468). It is characterized by extensive use of Mock Objects (page 544) to build layer-crossing tests. The main issue is that the tests describe how the software should do something, not what it should achieve. That is, the tests will pass only if the software is implemented in a particular way. This problem can be avoided by applying the principle Use the Front Door First (see page 40) whenever possible to avoid encoding too much knowledge about the implementation of the SUT into the tests.\n\nCause: Sensitive Equality\n\nObjects to be veriﬁ ed are converted to strings and compared with an expected string. This is an example of Behavior Sensitivity in that the test is sensitive to behavior that it is not in the business of verifying. We could also think of it as a case of Interface Sensitivity where the semantics of the interface have changed. Either way, the problem arises from the way the test was coded; using the string representations of objects for verifying them against expected values is just asking for trouble.\n\nCause: Fragile Fixture\n\nWhen a Standard Fixture is modiﬁ ed to accommodate a new test, several other tests fail. This is an alias for either Data Sensitivity or Context Sensitivity depending on the nature of the ﬁ xture in question.\n\nwww.it-ebooks.info\n\nFragile Test\n\nFurther Reading\n\nSensitive Equality and Fragile Fixture were ﬁ rst described in [RTC], which was the ﬁ rst paper published on test smells and refactoring test code. The four sen- sitivities were ﬁ rst described in [ARTRP], which also described several ways to avoid Fragile Tests in Recorded Tests.\n\nwww.it-ebooks.info\n\n247\n\nFragile Test\n\n248\n\nFrequent Debugging\n\nAlso known as: Manual Debugging\n\nChapter 16 Behavior Smells\n\nFrequent Debugging\n\nManual debugging is required to determine the cause of most test failures.\n\nSymptoms\n\nA test run results in a test failure or a test error. The output of the Test Run- ner (page 377) is insufﬁ cient for us to determine the problem. Thus we have to use an interactive debugger (or sprinkle print statements throughout the code) to determine where things are going wrong.\n\nIf this case is an exception, we needn’t worry about it. If most test fail- ures require this kind of debugging, however, we have a case of Frequent Debugging.\n\nCauses\n\nFrequent Debugging is caused by a lack of Defect Localization (see page 22) in our suite of automated tests. The failed tests should tell us what went wrong either through their individual failure messages (see Assertion Message on page 370) or through the pattern of test failures. If they don’t:\n\nWe may be missing the detailed unit tests that would point out a logic\n\nerror inside an individual class.\n\nWe may be missing the component tests for a cluster of classes (i.e., a component) that would point out an integration error between the indi- vidual classes. This can happen when we use Mock Objects (page 544) extensively to replace depended-on objects but the unit tests of the depended-on objects don’t match the way the Mock Objects are pro- grammed to behave.\n\nI’ve encountered this problem most frequently when I wrote higher-level (func- tional or component) tests but failed to write all the unit tests for the individual methods. (Some people would call this approach storytest-driven development to distinguish it from unit test-driven development, in which every little bit of code is pulled into existence by a failing unit test.)\n\nFrequent Debugging can also be caused by Infrequently Run Tests (see Pro- duction Bugs on page 268). If we run our tests after every little change we make to the software, we can easily remember what we changed since the last time we ran the tests. Thus, when a test fails, we don’t have to spend a lot\n\nwww.it-ebooks.info\n\nFrequent Debugging\n\nof time troubleshooting the software to discover where the bug is—we know where it is because we remember putting it there!\n\nImpact\n\nManual debugging is a slow, tedious process. It is easy to overlook subtle indi- cations of a bug and spend many hours tracking down a single logic error. Fre- quent Debugging reduces productivity and makes development schedules much less predictable because a single manual debugging session could extend the time required to develop the software by half a day or more.\n\nSolution Patterns\n\nIf we are missing the customer tests for a piece of functionality and manual user testing has revealed a problem not exposed by any automated tests, we probably have a case of Untested Requirements (see Production Bugs). We can ask our- selves, “What kind of automated test would have prevented the manual debug- ging session?” Better yet, once we have identiﬁ ed the problem, we can write a test that exposes it. Then we can use the failing test to do test-driven bug ﬁ xing. If we suspect this to be a widespread problem, we can create a development task to identify and write any additional tests that would be required to ﬁ ll the gap we just exposed.\n\nDoing true test-driven development is the best way to avoid the circumstances that lead to Frequent Debugging. We should start as close as possible to the skin of the application and do storytest-driven development—that is, we should write unit tests for individual classes as well as component tests for the collec- tions of related classes to ensure we have good Defect Localization.\n\nwww.it-ebooks.info\n\n249\n\nFrequent Debugging\n\n250\n\nManual Intervention\n\nChapter 16 Behavior Smells\n\nManual Intervention\n\nA test requires a person to perform some manual action each time it is run.\n\nSymptoms\n\nThe person running the test must do something manually either before the test is run or partway through the test run; otherwise, the test fails. The Test Runner may need to verify the results of the test manually.\n\nImpact\n\nAutomated tests are all about getting early feedback on problems introduced into the software. If the cost of getting that feedback is too high—that is, if it takes the form of Manual Intervention—we likely won’t run the tests very often and we won’t get the feedback very often. If we don’t get that feedback very often, we’ll probably introduce lots of problems between test runs, which will ultimately lead to Frequent Debugging (page 248) and High Test Maintenance Cost (page 265).\n\nManual Intervention also makes it impractical to have a fully automated\n\nIntegration Build [SCM] and regression test process.\n\nCauses\n\nThe causes of Manual Intervention are as varied as the kinds of things our soft- ware does or encounters. The following are some general categories of the kinds of issues that require Manual Intervention. This list is by no means exhaustive, though.\n\nCause: Manual Fixture Setup\n\nSymptoms\n\nA person has to set up the test environment manually before the automated tests can be run. This activity may take the form of conﬁ guring servers, starting server processes, or running scripts to set up a Prebuilt Fixture (page 429).\n\nRoot Cause\n\nThis problem is typically caused by a lack of attention to automating the ﬁ xture setup phase of the test. It may also be caused by excessive coupling between\n\nwww.it-ebooks.info\n\nManual Intervention\n\ncomponents in the SUT that prevents us from testing a majority of the code in the system inside the development environment.\n\nPossible Solution\n\nWe need to make sure that we are writing Fully Automated Tests. This may require opening up test-speciﬁ c APIs to allow tests to set up the ﬁ xture. Where the issue is related to an inability to run the software in the development envi- ronment, we may need to refactor the software to decouple the SUT from the steps that would otherwise need to be done manually.\n\nCause: Manual Result Veriﬁ cation\n\nSymptoms\n\nWe can run the tests but they almost always pass—even when we know that the SUT is not returning the correct results.\n\nRoot Cause\n\nIf the tests we write are not Self-Checking Tests (see page 26), we can be given a false sense of security because tests will fail only if an error/exception is thrown.\n\nPossible Solution\n\nWe can ensure that our tests are all self-checking by including result veriﬁ ca- tion logic such as calls to Assertion Methods (page 362) within the Test Meth- ods (page 348).\n\nCause: Manual Event Injection\n\nSymptoms\n\nA person must intervene during test execution to perform some manual action before the test can proceed.\n\nRoot Cause\n\nMany events in a SUT are hard to generate under program control. Examples include unplugging network cables, bringing down database connections, and clicking buttons on a user interface.\n\nImpact\n\nIf a person needs to do something manually, it both increases the effort to run the test and ensures that the test cannot be run unattended. This torpedoes any attempt to do a fully automated build-and-test cycle.\n\nwww.it-ebooks.info\n\n251\n\nManual Intervention\n\n252\n\nManual Intervention\n\nChapter 16 Behavior Smells\n\nPossible Solution\n\nThe best solution is to ﬁ nd ways to test the software that do not require a real person to do the manual actions. If the events are reported to the SUT through asynchronous events, we can have the Test Method invoke the SUT directly, passing it a simulated event object. If the SUT experiences the situation as a syn- chronous response from some other part of the system, we can get control of the indirect inputs by replacing some part of the SUT with a Test Stub (page 529) that simulates the circumstances to which we want to expose the SUT.\n\nFurther Reading\n\nRefer to Chapter 11, Using Test Doubles, for a much more detailed description of how to get control of the indirect inputs of the SUT.\n\nwww.it-ebooks.info\n\nSlow Tests\n\nSlow Tests\n\nThe tests take too long to run.\n\nSymptoms\n\nThe tests take long enough to run that developers don’t run them every time they make a change to the SUT. Instead, the developers wait until the next coffee break or another interruption before running them. Or, whenever they run the tests, they walk around and chat with other team members (or play Doom or surf the Internet or . . .).\n\nImpact\n\nSlow Tests obviously have a direct cost: They reduce the productivity of the person running the test. When we are test driving the code, we’ll waste precious seconds every time we run our tests; when it is time to run all the tests before we commit our changes, we’ll have an even more signiﬁ cant wait time.\n\nSlow Tests also have many indirect costs:\n\nThe bottleneck created by holding the “integration token” longer because\n\nwe need to wait for the tests to run after merging all our changes.\n\nThe time during which other people are distracted by the person wait-\n\ning for his or her test run to ﬁ nish.\n\nThe time spent in debuggers ﬁ nding a problem that was inserted sometime after the last time we ran the test. The longer it has been since the test was run, the less likely we are to remember exactly what we did to break the test. This cost is a result of the breakdown of the rapid feedback that automated unit tests provide.\n\nA common reaction to Slow Tests is to immediately go for a Shared Fix- ture (page 317). Unfortunately, this approach almost always results in other problems, including Erratic Tests (page 228). A better solution is to use a Fake Object (page 551) to replace slow components (such as the database) with faster ones. However, if all else fails and we must use some kind of Shared Fixture, we should make it immutable if at all possible.\n\nTroubleshooting Advice\n\nSlow Tests can be caused either by the way the SUT is built and tested or by the way the tests are designed. Sometimes the problem is obvious—we can just\n\nwww.it-ebooks.info\n\n253\n\nSlow Tests\n\n254\n\nSlow Tests\n\nChapter 16 Behavior Smells\n\nwatch the green bar grow as we run the tests. There may be notable pauses in the execution; we may see explicit delays coded in a Test Method (page 348). If the cause is not obvious, however, we can run different subsets (or subsuites) of tests to see which ones run quickly and which ones take a long time to run.\n\nA proﬁ ling tool can come in handy to see where we are spending the extra time in test execution. Of course, xUnit gives us a simple means to build our own mini-proﬁ ler: We can edit the setUp and tearDown methods of our Testcase Superclass (page 638). We then write out the start/end times or test duration into a log ﬁ le, along with the name of the Testcase Class (page 373) and Test Method. Finally, we import this ﬁ le into a spreadsheet, sort by duration, and voila—we have found the culprits. The tests with the longest execution times are the ones on which it will be most worthwhile to focus our efforts.\n\nCauses\n\nThe speciﬁ c cause of the Slow Tests could lie either in how we built the SUT or in how we coded the tests themselves. Sometimes, the way the SUT was built forces us to write our tests in a way that makes them slow. This is particularly a problem with legacy code or code that was built with a “test last” perspective.\n\nCause: Slow Component Usage\n\nA component of the SUT has high latency.\n\nRoot Cause\n\nThe most common cause of Slow Tests is interacting with a database in many of the tests. Tests that have to write to a database to set up the ﬁ xture and read a database to verify the outcome (a form of Back Door Manipulation; see page 327) take about 50 times longer to run than the same tests that run against in-memory data structures. This is an example of the more general problem of using slow components.\n\nPossible Solution\n\nWe can make our tests run much faster by replacing the slow components with a Test Double (page 522) that provides near-instantaneous responses. When the slow component is the database, the use of a Fake Database (see Fake Object) can make the tests run on average 50 times faster! See the sidebar “Faster Tests Without Shared Fixtures” on page 319 for other ways to skin this cat.\n\nwww.it-ebooks.info\n\nSlow Tests\n\nCause: General Fixture\n\nSymptoms\n\nTests are consistently slow because each test builds the same over-engineered ﬁ xture.\n\nRoot Cause\n\nEach test constructs a large General Fixture each time a Fresh Fixture (page 311) is built. Because a General Fixture contains many more objects than a Mini- mal Fixture (page 302), it naturally takes longer to construct. Fresh Fixture involves setting up a brand-new instance of the ﬁ xture for each Testcase Object (page 382), so multiply “longer” by the number of tests to get an idea of the magnitude of the slowdown!\n\nPossible Solution\n\nOur ﬁ rst inclination is often to implement the General Fixture as a Shared Fix- ture to avoid rebuilding it for each test. Unless we can make this Shared Fixture immutable, however, this approach is likely to lead to Erratic Tests and should be avoided. A better solution is to reduce the amount of ﬁ xture setup performed by each test.\n\nCause: Asynchronous Test\n\nSymptoms\n\nA few tests take inordinately long to run; those tests contain explicit delays.\n\nRoot Cause\n\nDelays included within a Test Method slow down test execution considerably. This slow execution may be necessary when the software we are testing spawns threads or processes (Asynchronous Code; see Hard-to-Test Code on page 209) and the test needs to wait for them to launch, run, and verify whatever side ef- fects they were expected to have. Because of the variability in how long it takes for these threads or processes to be started, the test usually needs to include a long delay “just in case”—that is, to ensure it passes consistently. Here’s an example of a test with delays:\n\nwww.it-ebooks.info\n\n255\n\nSlow Tests\n\n256\n\nSlow Tests\n\nChapter 16 Behavior Smells\n\npublic class RequestHandlerThreadTest extends TestCase { private static ﬁnal int TWO_SECONDS = 3000;\n\npublic void testWasInitialized_Async() = throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); // Exercise sut.start(); // Verify Thread.sleep(TWO_SECONDS); assertTrue(sut.initializedSuccessfully()); }\n\npublic void testHandleOneRequest_Async() throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); sut.start(); // Exercise enqueRequest(makeSimpleRequest()); // Verify Thread.sleep(TWO_SECONDS); assertEquals(1, sut.getNumberOfRequestsCompleted()); assertResponseEquals(makeSimpleResponse(), getResponse()); } }\n\nImpact\n\nA two-second delay might not seem like a big deal. But consider what happens when we have a dozen such tests: It would take almost half a minute to run these tests. In contrast, we can run several hundred normal tests each second.\n\nPossible Solution\n\nThe best way to address this problem is to avoid asynchronicity in tests by test- ing the logic synchronously. This may require us to do an Extract Testable Com- ponent (page 767) refactoring to implement a Humble Executable (see Humble Object on page 695).\n\nCause: Too Many Tests\n\nSymptoms\n\nThere are so many tests that they are bound to take a long time to run regardless of how fast they execute.\n\nwww.it-ebooks.info\n\nSlow Tests\n\nRoot Cause\n\nThe obvious cause of this problem is having so many tests. Perhaps we have such a large system that the large number of tests really is necessary, or perhaps we have too much overlap between tests.\n\nThe less obvious cause is that we are running too many of the tests too fre-\n\nquently!\n\nPossible Solution\n\nWe don’t have to run all the tests all the time! The key is to ensure that all tests are run regularly. If the entire suite is taking too long to run, consider creating a Subset Suite (see Named Test Suite on page 592) with a suitable cross section of tests; run this subsuite before every commit operation. The rest of the tests can be run regularly, albeit less often, by scheduling them to run overnight or at some other convenient time. Some people call this technique a “build pipeline.” For more on this and other ideas, see the sidebar “Faster Tests Without Shared Fixtures” on page 319.\n\nIf the system is large in size, it is a good idea to break it into a number of fairly independent subsystems or components. This allows teams work- ing on each component to work independently and to run only those tests specific to their own component. Some of those tests should act as proxies for how the other components would use the component; they must be kept up-to-date if the interface contract changes. Hmmm, Tests as Documenta- tion (see page 23); I like it! Some end-to-end tests that exercise all the com- ponents together (likely a form of storytests) would be essential, but they don’t need to be included in the pre-commit suite.\n\nwww.it-ebooks.info\n\n257\n\nSlow Tests\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nChapter 17\n\nProject Smells\n\nSmells in This Chapter\n\nBuggy Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260\n\nDevelopers Not Writing Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n\nHigh Test Maintenance Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . 265\n\nProduction Bugs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\n\n259\n\nwww.it-ebooks.info\n\nProject Smells",
      "page_number": 287
    },
    {
      "number": 17,
      "title": "Project Smells",
      "start_page": 323,
      "end_page": 340,
      "detection_method": "regex_chapter",
      "content": "260\n\nBuggy Tests\n\nChapter 17 Project Smells\n\nBuggy Tests\n\nBugs are regularly found in the automated tests.\n\nFully Automated Tests (see page 26) are supposed to act as a “safety net” for teams doing iterative development. But how can we be sure the safety net actually works?\n\nBuggy Tests is a project-level indication that all is not well with our auto-\n\nmated tests.\n\nSymptoms\n\nA build fails, and a failed test is to blame. Upon closer inspection, we discover that the code being testing works correctly, but the test indicated it was broken. We encountered Production Bugs (page 268) despite having tests that verify the speciﬁ c scenario in which the bug was found. Root-cause analysis indicates the test contains a bug that precluded catching the error in the production code.\n\nImpact\n\nTests that give misleading results are dangerous! Tests that pass when they shouldn’t (a false negative, as in “nothing wrong here”) give a false sense of security. Tests that fail when they shouldn’t (a false positive) discredit the tests. They are like the little boy who cried, “Wolf!”; after a few occurrences, we tend to ignore them.\n\nCauses\n\nBuggy Tests can have many causes. Most of these problems also show up as code or behavior smells. As project managers, we are unlikely to see these un- derlying smells until we speciﬁ cally look for them.\n\nCause: Fragile Test\n\nBuggy Tests may just be project-level symptoms of a Fragile Test (page 239). For false-positive test failures, a good place to start is the “four sensitivities”: Interface Sensitivity (see Fragile Test), Behavior Sensitivity (see Fragile Test), Data Sensi- tivity (see Fragile Test), and Context Sensitivity (see Fragile Test). Each of these sensitivities could be the change that caused the test to fail. Removing the sensi- tivities by using Test Doubles (page 522) and refactoring can be challenging but ultimately it will make the tests much more dependable and cost-effective.\n\nwww.it-ebooks.info\n\nBuggy Tests\n\nCause: Obscure Test\n\nA common cause of false-negative test results (tests that pass when they shouldn’t) is an Obscure Test (page 186), which is difﬁ cult to get right—especially when we are modifying existing tests that were broken by a change we made. Because automated tests are hard to test, we don’t often verify that a modiﬁ ed test still catches all the bugs it was initially designed to trap. As long as we see a green bar, we think we are “good to go.” In reality, we may have created a test that never fails.\n\nObscure Tests are best addressed through refactoring of tests to focus on the reader of the tests. The real goal is Tests as Documentation (see page 23)— anything less will increase the likelihood of Buggy Tests.\n\nCause: Hard-to-Test Code\n\nAnother common cause of Buggy Tests, especially with “legacy software” (i.e., any software that doesn’t have a complete suite of automated tests), is that the design of the software is not conducive to automated testing. This Hard-to-Test Code (page 209) may force us to use Indirect Testing (see Obscure Test), which in turn may result in a Fragile Test.\n\nThe only way Hard-to-Test Code will become easy to test is if we refactor the code to improve its testability. (This transformation is described in Chapter 6, Test Automation Strategy, and Chapter 11, Using Test Doubles.) If this is not an option, we may be able to reduce the amount of test code affected by a change by applying SUT API Encapsulation (see Test Utility Method on page 599).\n\nTroubleshooting Advice\n\nWhen we have Buggy Tests, it is important to ask lots of questions. We must ask the “ﬁ ve why’s” [TPS] to get to the bottom of the problem—that is, we must determine exactly which code and/or behavior smells are causing the Buggy Tests and ﬁ nd the root cause of each smell.\n\nSolution Patterns\n\nThe solution depends very much on why the Buggy Tests occurred. Refer to the underlying behavior and code smells for possible solutions.\n\nAs with all “project smells,” we should look for project-level causes. These\n\ninclude not giving developers enough time to perform the following activities:\n\nwww.it-ebooks.info\n\n261\n\nBuggy Tests\n\n262\n\nBuggy Tests\n\nChapter 17 Project Smells\n\nLearn to write the tests properly\n\nRefactor the legacy code to make test automation easier and more robust\n\nWrite the tests ﬁ rst\n\nFailure to address these project-level causes guarantees that the problems will recur in the near future.\n\nwww.it-ebooks.info\n\nDevelopers Not Writing Tests\n\nDevelopers Not Writing Tests\n\nDevelopers aren’t writing automated tests.\n\nSymptoms\n\nWe hear that our developers aren’t writing tests. Or maybe we have observed Production Bugs (page 268) and asked, “Why are so many bugs getting through?”, only to be told, “Because we aren’t writing tests to cover that part of the software.”\n\nImpact\n\nIf the team isn’t writing automated tests for every piece of software “that could possibly break,” it is mortgaging its future. The current pace of software develop- ment will not be sustainable over the long haul because the system will be in test debt. It will take longer and longer to add new functionality, and refactoring the code to improve its design will be fraught with peril (so it will happen less and less frequently). This problem marks the beginning of a trip down the proverbial “slippery slope” to traditional paranoid, non-agile development. If that is where we aspire to be, we should stay the course. Otherwise, it is time to take action.\n\nCauses\n\nCause: Not Enough Time\n\nDevelopers may have trouble writing tests in the time they are given to do the development. This problem could be caused by an overly aggressive devel- opment schedule or supervisors/team leaders who instruct developers, “Don’t waste time writing tests.” Alternatively, developers may not have the skills needed to write tests efﬁ ciently and may not be allocated the time required to work their way up the learning curve.\n\nIf time is what the developers need, managers need to adjust the proj- ect schedule to give them that time. This extension need be only a temporary adjustment while the developers learn the skills and test automation infrastructure that will enable them to write the tests more quickly. In my experience, once developers have internalized the process, they can write the tests and the code in the same amount of time it once took them to write and debug just the code. The time spent writing the tests is more than compensated for by the time not spent in the debugger.\n\nwww.it-ebooks.info\n\n263\n\nDevelopers Not Writing Tests\n\n264\n\nDevelopers Not Writing Tests\n\nChapter 17 Project Smells\n\nCause: Hard-to-Test Code\n\nA common cause of Developers Not Writing Tests, especially with “legacy soft- ware” (i.e., any software that doesn’t have a complete suite of automated tests), is that the design of the software is not conducive to automated testing. This situa- tion is described in more detail in its own smell, Hard-to-Test Code (page 209).\n\nCause: Wrong Test Automation Strategy\n\nAnother cause of Developers Not Writing Tests may be a test environment or test automation strategy that leads to Fragile Tests (page 239) or Obscure Tests (page 186) that take too long to write. We need to ask the “ﬁ ve why’s” [TPS] to ﬁ nd the root causes. Then we can address those causes and get the ship back on course.\n\nTroubleshooting Advice\n\nProject-level smells such as Developers Not Writing Tests are more likely to be detected by a project manager, scrum master, or team leader than by a developer. As managers, we may not know how to ﬁ x the problem, but our awareness and recognition of it is what matters. This unique perspective allows managers to ask the development team questions about why they aren’t writing tests, in which circumstances, and how long it takes to write tests when they do so. Then managers can encourage and empower the developers to come up with ways of addressing the root causes so that they write all the necessary tests.\n\nOf course, managers must give the developers their full support in carrying out whatever improvement plan they come up with. That support must include enough time to learn the requisite skills and build or set up the necessary test infrastructure. And managers shouldn’t expect things to turn around overnight. They might set a process improvement goal for each iteration, such as “20% reduction in code not tested” or “20% improvement in code coverage.” These goals should be reasonable and at a high-enough level that they encourage the right behavior, as opposed to just making the numbers look good. (A goal of 205 more tests written, for example, could be achieved without increasing the test coverage one iota simply by splitting tests into smaller pieces or cloning tests.)\n\nwww.it-ebooks.info\n\nHigh Test Maintenance Cost\n\nHigh Test Maintenance Cost\n\nToo much effort is spent maintaining existing tests.\n\nTest code needs to be maintained along with the production code it veriﬁ es. As an application evolves, we will likely have to revisit our tests on a regular basis whenever we change the SUT classes to add new functionality or whenever we refactor the tests to simplify those classes. High Test Maintenance Cost occurs when the tests become overly difﬁ cult to understand and maintain.\n\nSymptoms\n\nDevelopment of new functionality slows down. Every time we add some new functionality, we need to make extensive changes to the existing tests. Develop- ers or test automaters may tell the project manager or coach that they need a “test refactoring/cleanup iteration.”\n\nIf we have been tracking the amount of time we spend writing the new tests and modifying existing tests separately from the time we spend implementing the code to make the tests pass, we notice that most of the time is spent modify- ing the existing tests.\n\nMost test maintainability issues are accompanied by other smells, such as the\n\nfollowing:\n\nA Fragile Test (page 239) indicates that tests are too closely coupled to\n\nthe SUT.\n\nA Fragile Fixture (see Fragile Test) signals that too many tests depend on the same ﬁ xture design (Standard Fixture on page 305), which leads to High Test Maintenance Cost.\n\nAn Erratic Test (page 228) may be a sign that a Shared Fixture (page 317)\n\nis causing our problem.\n\nImpact\n\nTeam productivity drops signiﬁ cantly because the tests take so much effort to main- tain. Developers may be agitating to “cut and run” (remove the affected tests from the test suites). While writing the production code is mandatory, maintaining the tests is completely optional (at least to the uninformed). If nothing is done about this problem, the entire test automation effort may be wasted when the team or manage- ment decides that test automation just “doesn’t work” and abandons the tests.\n\nwww.it-ebooks.info\n\n265\n\nHigh Test Maintenance Cost\n\n266\n\nHigh Test Maintenance Cost\n\nChapter 17 Project Smells\n\nCauses\n\nThe root cause of High Test Maintenance Cost is failing to pay attention to the principles described in Chapter 5, Principles of Test Automation. A more immediate cause is often too much Test Code Duplication (page 213) and tests that are too closely coupled to the API of the SUT.\n\nCause: Fragile Test\n\nTests that fail because minor changes were made to the SUT are called Fragile Tests. They result in High Test Maintenance Cost because they need to be revis- ited and “giggled” after all manner of minor changes that really shouldn’t affect them.\n\nThe root cause of this failure can be any of the “four sensitivities”: Inter- face Sensitivity (see Fragile Test), Behavior Sensitivity (see Fragile Test), Data Sensitivity (see Fragile Test), and Context Sensitivity (see Fragile Test). We can reduce the High Test Maintenance Cost by protecting the tests against as many of these sensitivities as possible through the use of Test Doubles (page 522) and by refactoring the system into smaller components and classes that can be tested individually.\n\nCause: Obscure Test\n\nObscure Tests (page 186) are a major contributor to High Test Maintenance Cost because they take longer to understand each time they are visited. When they need to be modiﬁ ed, they take more effort to adjust and are much less likely to “work the ﬁ rst time,” resulting in more debugging of tests. Obscure Tests are also more likely to end up not catching conditions they were intended to detect, which can lead to Buggy Tests (page 260).\n\nObscure Tests are best addressed by refactoring tests to focus on the reader of the tests. The real goal is Tests as Documentation (see page 23)—anything less will increase the likelihood of High Test Maintenance Cost.\n\nCause: Hard-to-Test Code\n\n“Legacy software” (i.e., any software that doesn’t have a complete suite of auto- mated tests) can be hard to test because we typically write the tests “last” (after the software already exists). If the design of the software is not conducive to automated testing, we may be forced to use Indirect Testing (see Obscure Test) via awkward interfaces that involve a lot of accidental complexity; that effort may result in Fragile Tests.\n\nwww.it-ebooks.info\n\nHigh Test Maintenance Cost\n\nIt will take both time and effort to refactor the code to improve its testability. Nevertheless, that time and effort are well spent if they eliminate the High Test Maintenance Cost. If refactoring is not an option, we may be able to reduce the amount of test code affected by a change by doing SUT API Encapsulation (see Test Utility Method on page 599) using Test Utility Methods. For example, Creation Methods (page 415) encapsulate the constructors, thereby rendering the tests less susceptible to changes in constructor signatures or semantics.\n\nTroubleshooting Advice\n\nAs a project-level smell, High Test Maintenance Cost is as likely to be detected by a project manager, scrum master, or team leader as by a developer. While managers may not have the technical depth needed to troubleshoot and ﬁ x the problem, the fact that they become aware of it is what is important. This aware- ness allows the manager to question the development team about how long it is taking to maintain tests, how often test maintenance occurs, and why it is neces- sary. Then the manager can challenge the developers to ﬁ nd a better way—one that won’t result in such High Test Maintenance Costs!\n\nOf course, the developers will need the manager’s support to carry out whatever improvement plan they come up with. That support must include time to conduct the investigations (spikes), learning/training time, and time to do the actual work. Managers can make time for this activity by having “test refactoring stories,” adjusting the velocity to reduce the new functionality com- mitted to the customer, or other means. Regardless of how managers carve out this time, they must remember that if they don’t give the development team the resources needed to ﬁ x the problem now, the problem will simply get worse and become even more challenging to ﬁ x in the future when the team has twice as many tests.\n\nwww.it-ebooks.info\n\n267\n\nHigh Test Maintenance Cost\n\n268\n\nProduction Bugs\n\nChapter 17 Project Smells\n\nProduction Bugs\n\nWe ﬁ nd too many bugs during formal tests or in production.\n\nSymptoms\n\nWe have put a lot of effort into writing automated tests, yet the number of bugs showing up in formal (i.e., system) testing or production remains too high.\n\nImpact\n\nIt takes longer to troubleshoot and ﬁ x bugs found in formal testing than those found in development, and even longer to troubleshoot and ﬁ x bugs found in production. We may be forced to delay shipping the product or putting the application into production to allow time for the bug ﬁ xes and retesting. This time and effort translate directly into monetary costs and consume resources that might otherwise be used to add more functionality to the product or to build other products. The delay may also damage the organization’s credibility in the eyes of its customers. Poor quality has an indirect cost as well, in that it lowers the value of the product or service we are supplying.\n\nCauses\n\nBugs may slip through to production for several reasons, including Infrequently Run Tests or Untested Code. The latter problem may result from Missing Unit Tests or Lost Tests.\n\nBy specifying that “enough tests” be run, we mean the test coverage should be adequate, rather than that some speciﬁ c number of tests must be carried out. Changes to Untested Code are more likely to result in Production Bugs because there are no automated tests to tell the developers when they have introduced problems. Untested Requirements aren’t being veriﬁ ed every time the tests are run, so we don’t know for sure what is working. Both of these problems are related to Developers Not Writing Tests (page 263).\n\nCause: Infrequently Run Tests\n\nSymptoms\n\nWe hear that our developers aren’t running the tests very often. When we ask some questions, we discover that running the tests takes too long(Slow Tests; see page 253) or produces too many extraneous failures (Buggy Tests; see page 260).\n\nwww.it-ebooks.info\n\nProduction Bugs\n\nWe see test failures in the daily Integration Build [SCM]. When we dig deeper, we ﬁ nd that developers often commit their code without running the tests on their own machines.\n\nRoot Cause\n\nOnce they’ve seen the beneﬁ ts of working with the safety net of automated tests, most developers will continue using these tests unless something gets in the way. The most common impediments are Slow Tests that slow down the pre-integration regression testing or Unrepeatable Tests (see Erratic Test on page 228) that force developers to restart their test environment or do Manual Intervention (page 250) before running the tests.\n\nPossible Solution\n\nIf the root cause is Unrepeatable Tests, we can try switching to a Fresh Fix- ture (page 311) strategy to make the tests more deterministic. If the cause is Slow Tests, we must put more effort into speeding up the test run.\n\nCause: Lost Test\n\nSymptoms\n\nThe number of tests being executed in a test suite has declined (or has not increased as much as expected). We may notice this directly if we are paying attention to test counts. Alternatively, we may ﬁ nd a bug that should have been caused by a test that we know exists but, upon poking around, we discover that the test has been disabled.\n\nRoot Cause\n\nLost Tests can be caused by either a Test Method (page 348) or a Testcase Class (page 373) that has been disabled or has never been added to the AllTests Suite (see Named Test Suite on page 592).\n\nTests can be accidentally left out (i.e., never run) of test suite in the following\n\ncircumstances:\n\nWe forget to add the [test] attribute to the Test Method, or we acci- dentally use a method name that doesn’t match the naming convention used by the Test Discovery (page 393) mechanism.\n\nWe forget to add a call to suite.addTest to add the Test Method to the Test Suite Object (page 387) when we are automating tests in a Test Automation Framework (page 298) that supports only Test Enumeration (page 399).\n\nwww.it-ebooks.info\n\n269\n\nProduction Bugs\n\n270\n\nProduction Bugs\n\nChapter 17 Project Smells\n\nWe forget to add a call to the Test Method explicitly in the Test Suite Procedure (see Test Suite Object) in procedural-language variations of xUnit.\n\nWe forget to add the test suite to the Suite of Suites (see Test Suite Object) or forget to add the [Test Fixture] attribute to the Testcase Class.\n\nTests that ran in the past may have been disabled in any of the following ways:\n\nWe renamed the Test Method to not match the pattern that causes Test Discovery to include the test in the test suite (e.g., the method name starts with “test . . .”).\n\nWe added an [Ignore] attribute in variants of xUnit that use method attributes to indicate Test Methods.\n\nWe commented out (or deleted) the code that adds the test (or suite) to\n\nthe suite explicitly.\n\nTypically, a Lost Test occurs when a test is failing and someone disables it to avoid having to wade through the failing tests when running other tests. It may also occur accidentally, of course.\n\nPossible Solution\n\nThere are a number of ways to avoid introducing Lost Tests.\n\nWe can use a Single Test Suite (see Named Test Suite) to run a single Test Method instead of disabling the failing or slow test. We can use the Test Tree Explorer (see Test Runner on page 377) to drill down and run a single test from within a test suite. Both of these techniques are made difﬁ cult by Chained Tests (page 454)—a deliberate form of Interacting Tests (see Erratic Test)—so this is just one more reason to avoid them.\n\nIf our variant of xUnit supports it, we can use the provided mechanism to ignore1 a test. It will typically remind us of the number of tests not being run so we don’t forget to re-enable them. We can also conﬁ gure our continuous integration tool to fail the build if the number of tests “ignored” exceeds a certain threshold.\n\nWe can compare the number of tests we have after check-in with the number of tests that existed in the code branch immediately before we started integra- tion. We simply verify that this count has increased by the number of tests we have added.\n\n1 For example, NUnit lets us put the attribute [Ignore] on a Test Method to keep it from being run.\n\nwww.it-ebooks.info\n\nProduction Bugs\n\nWe can implement or take advantage of Test Discovery if our programming\n\nlanguage supports reﬂ ection.\n\nWe can use a different strategy for ﬁ nding the tests to run in the Integration Build. Some build tools (such as Ant) let us ﬁ nd all ﬁ les that match a name pat- tern (e.g., those ending in “Test”). We won’t lose entire test suites if we use this capability to pick up all the tests.\n\nCause: Missing Unit Test\n\nSymptoms\n\nAll the unit tests pass but a customer test continues to fail. At some point, the customer test passed—but no unit tests were written to verify the behavior of the individual classes. Then, a subsequent code change modiﬁ ed the behavior of one of the classes, which broke its functionality.\n\nRoot Cause\n\nMissing Unit Tests often happen when a team focuses on writing the customer tests but fails to do test-driven development using unit tests. The team members may have built enough functionality to pass the customer tests, but a subsequent refactoring broke it. Unit tests would likely have prevented the code change from reaching the Integration Build.\n\nMissing Unit Tests can also arise during test-driven development when devel- opers get ahead of themselves and write some code without having a failing test to guide them.\n\nPossible Solution\n\nThe trite answer is to write more unit tests. Of course, this is easier said than done, and it isn’t always effective. Doing true test-driven development is the best way to avoid having Missing Unit Tests without writing unnecessary tests merely to get the test count up.\n\nCause: Untested Code\n\nSymptoms\n\nWe may just “know” that some piece of code in the SUT is not being exercised by any tests. Perhaps we have never seen that code execute, or perhaps we used code coverage tools to prove this fact beyond a doubt. In the following example, how can we test that when timeProvider throws an exception, this exception is handled correctly?\n\nwww.it-ebooks.info\n\n271\n\nProduction Bugs\n\n272\n\nProduction Bugs\n\nChapter 17 Project Smells\n\npublic String getCurrentTimeAsHtmlFragment() throws TimeProviderEx { Calendar currentTime; try { currentTime = getTimeProvider().getTime(); } catch (Exception e) { return e.getMessage(); } // etc.\n\nRoot Cause\n\nThe most common cause of Untested Code is that the SUT includes code paths that react to particular ways that a depended-on component (DOC) behaves and we haven’t found a way to exercise those paths. Typically, the DOC is being called synchronously and either returns certain values or throws excep- tions. During normal testing, only a subset of the possible equivalence classes of indirect inputs are actually encountered.\n\nAnother common cause of Untested Code is incompleteness of the test suite caused by incomplete characterization of the functionality exposed via the SUT’s interface.\n\nPossible Solution\n\nIf the Untested Code is caused by an inability to control the indirect inputs of the SUT, the most common solution is to use a Test Stub (page 529) to feed the various kinds of indirect inputs into the SUT to cover all the code paths. Other- wise, it may be sufﬁ cient to conﬁ gure the DOC to cause it to return the various indirect inputs required to fully test the SUT.\n\nCause: Untested Requirement\n\nSymptoms\n\nWe may just “know” that some piece of functionality is not being tested. Alter- natively, we may be trying to test a piece of software but cannot see any visible functionality that can be tested via the public interface of the software. All the tests we have written pass, however.\n\nWhen doing test-driven development, we know we need to add some code to handle a requirement. However, we cannot ﬁ nd a way to express the need for code to log the action in a Fully Automated Test (see page 26) such as this:\n\nwww.it-ebooks.info\n\nProduction Bugs\n\npublic void testRemoveFlight() throws Exception { // set up FlightDto expectedFlightDto = createARegisteredFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight should not exist after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); }\n\nNote that this test does not verify that the correct logging action has been done. It will pass regardless of whether the logging was implemented correctly—or even at all. Here’s the code that this test is verifying, complete with the indirect output of the SUT that has not been implemented correctly:\n\npublic void removeFlight(BigDecimal ﬂightNumber) throws FlightBookingException { System.out.println(\" removeFlight(\"+ﬂightNumber+\")\"); dataAccess.removeFlight(ﬂightNumber); logMessage(\"CreateFlight\", ﬂightNumber); // Bug! }\n\nIf we plan to depend on the information captured by logMessage when maintain- ing the application in production, how can we ensure that it is correct? Clearly, it is desirable to have automated tests verify this functionality.\n\nImpact\n\nPart of the required behavior of the SUT could be accidentally disabled without causing any tests to fail. Buggy software could be delivered to the customer. The fear of introducing bugs could discourage ruthless refactoring or deletion of code suspected to be unneeded (i.e., dead code).\n\nRoot Cause\n\nThe most common cause of Untested Requirements is that the SUT includes behavior that is not visible through its public interface. It may have expected “side effects” that cannot be observed directly by the test (such as writing out a ﬁ le or record or calling a method on another object or component)—in other words, it may have indirect outputs.\n\nWhen the SUT is an entire application, the Untested Requirement may be a result of not having a full suite of customer tests that verify all aspects of the visible behavior of the SUT.\n\nwww.it-ebooks.info\n\n273\n\nProduction Bugs\n\n274\n\nProduction Bugs\n\nChapter 17 Project Smells\n\nPossible Solution\n\nIf the problem is missing customer tests, we need to write at least enough cus- tomer tests to ensure that all components are integrated properly. This may require improving the design-for-testability of the application by separating the presentation layer from the business logic layer.\n\nWhen we have indirect outputs that we need to verify, we can do Behavior Veriﬁ cation (page 468) through the use of Mock Objects (page 544). Testing of indirect outputs is covered in Chapter 11, Using Test Doubles.\n\nCause: Neverfail Test\n\nSymptoms\n\nWe may just “know” that some piece of functionality is not working, even though the tests for that functionality pass. When doing test-driven develop- ment, we have added a test for functionality we have not yet written but we cannot get the test to fail.\n\nImpact\n\nIf a test won’t fail even when the code to implement the functionality doesn’t exist, how useful is it for Defect Localization (see page 22)? Not very!\n\nRoot Cause\n\nThis problem can be caused by improperly coded assertions such as assertTrue- (aVariable, true) instead of assertEquals(aVariable, true) or just assertTrue(aVariable). Another cause is more sinister: When we have asynchronous tests, failures thrown in the other thread or process may not be seen or reported by the Test Runner.\n\nPossible Solution\n\nWe can implement cross-thread failure detection mechanisms to ensure that asynchronous tests do, indeed, fail. An even better solution is to refactor the code to support a Humble Executable (see Humble Object on page 695).\n\nwww.it-ebooks.info\n\nPART III\n\nThe Patterns\n\nwww.it-ebooks.info\n\nThe Patterns\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nChapter 18\n\nTest Strategy Patterns\n\nPatterns in This Chapter\n\nTest Automation Strategy\n\nRecorded Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\n\nScripted Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285\n\nData-Driven Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\n\nTest Automation Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\n\nTest Fixture Strategy\n\nMinimal Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302\n\nStandard Fixture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305\n\nFresh Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311\n\nShared Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n\nSUT Interaction Strategy\n\nBack Door Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\n\nLayer Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\n\n277\n\nwww.it-ebooks.info\n\nTest Strategy Patterns",
      "page_number": 323
    },
    {
      "number": 18,
      "title": "Test Strategy Patterns",
      "start_page": 341,
      "end_page": 410,
      "detection_method": "regex_chapter",
      "content": "278\n\nRecorded Test\n\nAlso known as: Record and Playback Test, Robot User Test, Capture/ Playback Test\n\nChapter 18 Test Strategy Patterns\n\nRecorded Test\n\nHow do we prepare automated tests for our software?\n\nWe automate tests by recording interactions with the application and playing them back using a test tool.\n\nFixture Fixture\n\nTest Test Recorder Recorder\n\nInputs Inputs\n\nOutputs Outputs\n\nInputs Inputs\n\nOutputs Outputs\n\nSUT SUT\n\nInputs Inputs\n\nOutputs Outputs\n\nTest Script Repository Test Script Repository\n\nTest Test Script 1 Script 1\n\nTest Test Script 2 Script 2\n\nTest Test Script n Script n\n\nAutomated tests serve several purposes. They can be used for regression testing software after it has been changed. They can help document the behavior of the software. They can specify the behavior of the software before it has been writ- ten. How we prepare the automated test scripts affects which purposes they can be used for, how robust they are to changes in the SUT, and how much skill and effort it takes to prepare them.\n\nRecorded Tests allow us to rapidly create regression tests after the SUT has\n\nbeen built and before it is changed.\n\nHow It Works\n\nWe use a tool that monitors our interactions with the SUT as we work with it. This tool keeps track of most of what the SUT communicates to us and our responses to the SUT. When the recording session is done, we can save the ses- sion to a ﬁ le for later playback. When we are ready to run the test, we start up\n\nwww.it-ebooks.info\n\nRecorded Test\n\nthe “playback” part of the tool and point it at the recorded session. It starts up the SUT and feeds it our recorded inputs in response to the SUT’s outputs. It may also compare the SUT’s outputs with the SUT’s responses during the recording session. A mismatch may be cause for failing the test.\n\nSome Recorded Test tools allow us to adjust the sensitivity of the compari- sons that the tool makes between what the SUT said during the recording ses- sion and what it said during the playback. Most Recorded Test tools interact with the SUT through the user interface.\n\nWhen to Use It\n\nOnce an application is up and running and we don’t expect a lot of changes to it, we can use Recorded Tests to do regression testing. We could also use Recorded Tests when an existing application needs to be refactored (in anticipa- tion of modifying the functionality) and we do not have Scripted Tests (page 285) available to use as regression tests. It is typically much quicker to produce a set of Recorded Tests than to prepare Scripted Tests for the same functionality. In theory, the test recording can be done by anyone who knows how to operate the application; very little technical expertise should be required. In practice, many of the commercial tools have a steep learning curve. Also, some technical expertise may be required to add “checkpoints,” to adjust the sensitivity of the playback tool, or to adjust the test script if the recording tool became confused and recorded the wrong information.\n\nMost Recorded Test tools interact with the SUT through the user interface. This approach makes them particularly prone to fragility if the user interface of the SUT is evolving (Interface Sensitivity; see Fragile Test on page 239). Even small changes such as changing the internal name of a button or ﬁ eld may be enough to cause the playback tool to stumble. The tools also tend to record information at a very low and detailed level, making the tests hard to understand (Obscure Test; page 186); as a result, they are also difﬁ cult to repair by hand if they are broken by changes to the SUT. For these reasons, we should plan on rerecording the tests fairly regularly if the SUT will continue to evolve.\n\nIf we want to use the Tests as Documentation (see page 23) or if we want to use the tests to drive new development, we should consider using Scripted Tests. These goals are difﬁ cult to address with commercial Recorded Test tools because most do not let us deﬁ ne a Higher-Level Language (see page 41) for the test recording. This issue can be addressed by building the Recorded Test capability into the application itself or by using Refactored Recorded Test.\n\nwww.it-ebooks.info\n\n279\n\nRecorded Test\n\n280\n\nRecorded Test\n\nChapter 18 Test Strategy Patterns\n\nVariation: Refactored Recorded Test\n\nA hybrid of the two strategies is to use the “record, refactor, playback”1 sequence to extract a set of “action components” or “verbs” from the newly Recorded Tests and then rewire the test cases to call these “action components” instead of having detailed in-line code. Most commercial capture/replay tools provide the means to turn Literal Values (page 714) into parameters that can be passed into the “action component” by the main test case. When a screen changes, we simply rerecord the “action component”; all the test cases continue to function by automatically using the new “action component” deﬁ nition. This strategy is effectively the same as using Test Utility Methods (page 599) to interact with the SUT in unit tests. It opens the door to using the Refactored Recorded Test com- ponents as a Higher-Level Language in Scripted Tests. Tools such as Mercury Interactive’s BPT2 use this paradigm for scripting tests in a top-down manner; once the high-level scripts are developed and the components required for the test steps are speciﬁ ed, more technical people can either record or hand-code the individual components.\n\nImplementation Notes\n\nWe have two basic choices when using a Recorded Test strategy: We can either acquire third-party tools that record the communication that occurs while we interact with the application or we can build a “record and playback” mecha- nism right into our application.\n\nVariation: External Test Recording\n\nMany test recording tools are available commercially, each of which has its own strengths and weaknesses. The best choice will depend on the nature of the user interface of the application, our budget, the complexity of the functionality to be veriﬁ ed, and possibly other factors.\n\nIf we want to use the tests to drive development, we need to pick a tool that uses a test-recording ﬁ le format that is editable by hand and easily understood. We’ll need to handcraft the contents—this situation is really an example of a Scripted Test even if we are using a “record and playback” tool to execute the tests.\n\n1 The name “record, refactor, playback” was coined by Adam Geras. 2 BPT is short for “Business Process Testing.”\n\nwww.it-ebooks.info\n\nRecorded Test\n\nVariation: Built-In Test Recording\n\nIt is also possible to build a Recorded Test capability into the SUT. In such a case, the test scripting “language” can be deﬁ ned at a fairly high level—high enough to make it possible to hand-script the tests even before the system is built. In fact, it has been reported that the VBA macro capability of Microsoft’s Excel spreadsheet started out as a mechanism for automated testing of Excel.\n\nExample: Built-In Test Recording\n\nOn the surface, it doesn’t seem to make sense to provide a code sample for a Recorded Test because this pattern deals with how the test is produced, not how it is represented. When the test is played back, it is in effect a Data-Driven Test (page 288). Likewise, we don’t often refactor to a Recorded Test because it is often the ﬁ rst test automation strategy attempted on a project. Nevertheless, we might introduce a Recorded Test after attempting Scripted Tests if we discover that we have too many Missing Tests (page 268) because the cost of manual auto- mation is too high. In that case, we would not be trying to turn existing Scripted Tests into Recorded Tests; we would just record new tests.\n\nHere’s an example of a test recorded by the application itself. This test was used to regression-test a safety-critical application after it was ported from C on OS2 to C++ on Windows. Note how the recorded information forms a domain- speciﬁ c Higher-Level Language that is quite readable by a user.\n\n<interaction-log> <commands> <!-- more commands omitted --> <command seqno=\"2\" id=\"Supply Create\"> <ﬁeld name=\"engineno\" type=\"input\"> <used-value>5566</used-value> <expected></expected> <actual status=\"ok\"/> </ﬁeld> <ﬁeld name=\"direction\" type=\"selection\"> <used-value>SOUTH</used-value> <expected> <value>SOUTH</value> <value>NORTH</value> </expected> <actual> <value status=\"ok\">SOUTH</value> <value status=\"ok\">NORTH</value> </actual> </ﬁeld> </command> <!-- more commands omitted --> </commands> </interaction-log>\n\nwww.it-ebooks.info\n\n281\n\nRecorded Test\n\n282\n\nRecorded Test\n\nChapter 18 Test Strategy Patterns\n\nThis sample depicts the output of having played back the tests. The actual elements were inserted by the built-in playback mechanism. The status attributes indicate whether these elements match the expected values. We applied a style sheet to these ﬁ les to format them much like a Fit test with color-coded results. The business users on the project then handled the recording, replaying, and result analysis.\n\nThis recording was made by inserting hooks in the presentation layer of the software to record the lists of choices offered the user and the user’s responses. An example of one of these hooks follows:\n\nif (playback_is_on()) { choice = get_choice_for_playback(dialog_id, choices_list); } else { choice = display_dialog(choices_list, row, col, title, key); }\n\nif (recording_is_on()) { record_choice(dialog_id, choices_list, choice, key); }\n\nThe method get_choice_for_playback retrieves the contents of the used-value element instead of asking the user to pick from the list of choices. The method record_choice generates the actual element and makes the “assertions” against the expected elements, recording the result in the status attribute of each element. Note that recording_is_on() returns true whenever we are in playback mode so that the test results can be recorded.\n\nExample: Commercial Record and Playback Test Tool\n\nAlmost every commercial testing tool uses a “record and playback” metaphor. Each tool also deﬁ nes its own Recorded Test ﬁ le format, most of which are very verbose. The following is a “short” excerpt from a test recorded using Mercury Interactive’s QuickTest Professional [QTP] tool. It is shown in “Expert View,” which exposes what is really recorded: a VbScript program! The example includes comments (preceded by “@@”) that were inserted manually to clarify what this test is doing; these comments would be lost if the test were rerecorded after a change to the application caused the test to no longer run.\n\n@@ @@ GoToPageMaintainTaxonomy() @@ Browser(\"Inf\").Page(\"Inf\").WebButton(\"Login\").Click Browser(\"Inf\").Page(\"Inf_2\").Check CheckPoint(\"Inf_2\") Browser(\"Inf\").Page(\"Inf_2\"\").Link(\"TAXONOMY LINKING\").Click Browser(\"Inf\").Page(\"Inf_3\").Check CheckPoint(\"Inf_3\") Browser(\"Inf\").Page(\"Inf_3\").Link(\"MAINTAIN TAXONOMY\").Click Browser(\"Inf\").Page(\"Inf_4\").Check CheckPoint(\"Inf_4\") @@\n\nwww.it-ebooks.info\n\nRecorded Test\n\n@@ AddTerm(\"A\",\"Top Level\", \"Top Level Deﬁnition\") @@ Browser(\"Inf\").Page(\"Inf_4\").Link(\"Add\").Click wait 4 Browser(\"Inf_2\").Page(\"Inf\").Check CheckPoint(\"Inf_5\") Browser(\"Inf_2\").Page(\"Inf\").WebEdit(\"childCodeSufﬁx\").Set \"A\" Browser(\"Inf_2\").Page(\"Inf\"). WebEdit(\"taxonomyDto.descript\").Set \"Top Level\" Browser(\"Inf_2\").Page(\"Inf\"). WebEdit(\"taxonomyDto.deﬁniti\").Set \"Top Level Deﬁnition\" Browser(\"Inf_2\").Page(\"Inf\").WebButton(\"Save\").Click wait 4 Browser(\"Inf\").Page(\"Inf_5\").Check CheckPoint(\"Inf_5_2\") @@ @@ SelectTerm(\"[A]-Top Level\") @@ Browser(\"Inf\").Page(\"Inf_5\"). WebList(\"selectedTaxonomyCode\").Select \"[A]-Top Level\" @@ @@ AddTerm(\"B\",\"Second Top Level\", \"Second Top Level Deﬁnition\") @@ Browser(\"Inf\").Page(\"Inf_5\").Link(\"Add\").Click wait 4 Browser(\"Inf_2\").Page(\"Inf_2\").Check CheckPoint(\"Inf_2_2\") infoﬁle_;_Inform_Alberta_21.inf_;_hightlight id_; _Browser(\"Inf_2\").Page(\"Inf_2\")_;_ @@ @@ and it goes on, and on, and on ....\n\nNote how the test describes all inputs and outputs in terms of the user interface of the application. It suffers from two main issues: Obscure Tests (caused by the detailed nature of the recorded information) and Interface Sensitivity (resulting in Fragile Tests).\n\nRefactoring Notes\n\nWe can make this test more useful as documentation, reduce or avoid High Test Maintenance Cost (page 265), and support composition of other tests from a Higher-Level Language by using a series of Extract Method [Fowler] refactorings.\n\nExample: Refactored Commercial Recorded Test\n\nThe following example shows the same test refactored to Communicate Intent (see page 41):\n\nGoToPage_MaintainTaxonomy() AddTerm(\"A\",\"Top Level\", \"Top Level Deﬁnition\") SelectTerm(\"[A]-Top Level\") AddTerm(\"B\",\"Second Top Level\", \"Second Top Level Deﬁnition\")\n\nwww.it-ebooks.info\n\n283\n\nRecorded Test\n\n284\n\nRecorded Test\n\nChapter 18 Test Strategy Patterns\n\nNote how much more intent revealing this test has become. The Test Utility Methods we extracted look like this:\n\nMethod GoToPage_MaintainTaxonomy() Browser(\"Inf\").Page(\"Inf\").WebButton(\"Login\").Click Browser(\"Inf\").Page(\"Inf_2\").Check CheckPoint(\"Inf_2\") Browser(\"Inf\").Page(\"Inf_2\").Link(\"TAXONOMY LINKING\").Click Browser(\"Inf\").Page(\"Inf_3\").Check CheckPoint(\"Inf_3\") Browser(\"Inf\").Page(\"Inf_3\").Link(\"MAINTAIN TAXONOMY\").Click Browser(\"Inf\").Page(\"Inf_4\").Check CheckPoint(\"Inf_4\") End\n\nMethod AddTerm( code, name, description) Browser(\"Inf\").Page(\"Inf_4\").Link(\"Add\").Click wait 4 Browser(\"Inf_2\").Page(\"Inf\").Check CheckPoint(\"Inf_5\") Browser(\"Inf_2\").Page(\"Inf\"). WebEdit(\"childCodeSufﬁx\").Set code Browser(\"Inf_2\").Page(\"Inf\"). WebEdit(\"taxonomyDto.descript\").Set name Browser(\"Inf_2\").Page(\"Inf\"). WebEdit(\"taxonomyDto.deﬁniti\").Set description Browser(\"Inf_2\").Page(\"Inf\").WebButton(\"Save\").Click wait 4 Browser(\"Inf\").Page(\"Inf_5\").Check CheckPoint(\"Inf_5_2\") end\n\nMethod SelectTerm( path ) Browser(\"Inf\").Page(\"Inf_5\"). WebList(\"selectedTaxonomyCode\").Select path Browser(\"Inf\").Page(\"Inf_5\").Link(\"Add\").Click wait 4 end\n\nThis example is one I hacked together to illustrate the similarities to what we do in xUnit. Don’t try running this example at home—it is probably not syntactically correct.\n\nFurther Reading\n\nThe paper “Agile Regression Testing Using Record and Playback” [ARTRP] describes our experiences building a Recorded Test mechanism into an applica- tion to facilitate porting it to another platform.\n\nwww.it-ebooks.info\n\nScripted Test\n\nScripted Test\n\nHow do we prepare automated tests for our software?\n\nWe automate the tests by writing test programs by hand.\n\nFixture Fixture\n\nTest Test Development Development\n\nInputs Inputs\n\nOutputs Outputs\n\nSUT SUT\n\nInputs Inputs\n\nExpected Expected Outputs Outputs\n\nTest Script Repository Test Script Repository\n\nTest Test Script 1 Script 1\n\nTest Test Script 2 Script 2\n\nTest Test Script n Script n\n\nAutomated tests serve several purposes. They can be used for regression testing software after it has been changed. They can help document the behavior of the software. They can specify the behavior of the software before it has been written. How we prepare the automated test scripts affects which purpose they can be used for, how robust they are to changes in the SUT, and how much skill and effort it takes to prepare them.\n\nScripted Tests allow us to prepare our tests before the software is developed\n\nso they can help drive the design.\n\nHow It Works\n\nWe automate our tests by writing test programs that interact with the SUT for the purpose of exercising its functionality. Unlike Recorded Tests (page 278), these tests can be either customer tests or unit tests. These test programs are often called “test scripts” to distinguish them from the production code they test.\n\nwww.it-ebooks.info\n\n285\n\nScripted Test\n\nAlso known as: Hand-Written Test, Hand- Scripted Test, Programmatic Test, Automated Unit Test\n\n286\n\nScripted Test\n\nChapter 18 Test Strategy Patterns\n\nWhen to Use It\n\nWe almost always use Scripted Tests when preparing unit tests for our software. This is because it is easier to access the individual units directly from software written in the same programming language. It also allows us to exercise all the code paths, including the “pathological” cases.\n\nCustomer tests are a slightly more complicated picture; we should use a Scripted Test whenever we use automated storytests to drive the develop- ment of software. Recorded Tests don’t serve this need very well because it is difﬁ cult to record tests without having an application from which to record them. Preparing Scripted Tests takes programming experience as well as experience in testing techniques. It is unlikely that most business users on a project would be interested in learning how to prepare Scripted Tests. An alternative to scripting tests in a programming language is to deﬁ ne a Higher- Level Language (see page 41) for testing the SUT and then to implement the language as a Data-Driven Test (page 288) Interpreter [GOF]. An open- source framework for deﬁ ning Data-Driven Tests is Fit and its wiki-based cousin, FitNesse. Canoo WebTest is another tool that supports this style of testing.\n\nIn case of an existing legacy application,3 we can consider using Recorded Tests as a way of quickly creating a suite of regression tests that will protect us while we refactor the code to introduce testability. We can then prepare Scripted Tests for our now testable application.\n\nImplementation Notes\n\nTraditionally, Scripted Tests were written as “test programs,” often using a spe- cial test scripting language. Nowadays, we prefer to write Scripted Tests using a Test Automation Framework (page 298) such as xUnit in the same language as the SUT. In this case, each test program is typically captured in the form of a Test Method (page 348) on a Testcase Class (page 373). To minimize Manual Intervention (page 250), each test method should implement a Self-Checking Test (see page 26) that is also a Repeatable Test (see page 26).\n\n3 Among test drivers, a legacy application is any system that lacks a safety net of auto- mated tests.\n\nwww.it-ebooks.info\n\nScripted Test\n\nExample: Scripted Test\n\nThe following is an example of a Scripted Test written in JUnit:\n\npublic void testAddLineItem_quantityOne(){ ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE; ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE; // Set Up Fixture Customer customer = createACustomer(NO_CUST_DISCOUNT); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(PRODUCT, QUAN_ONE); // Verify Outcome LineItem expected = createLineItem( QUAN_ONE, NO_CUST_DISCOUNT, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\npublic void testChangeQuantity_severalQuantity(){ ﬁnal int ORIGINAL_QUANTITY = 3; ﬁnal int NEW_QUANTITY = 5; ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE.multiply( new BigDecimal(NEW_QUANTITY)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUST_DISCOUNT_PC.movePointLeft(2))); // Set Up Fixture Customer customer = createACustomer(CUST_DISCOUNT_PC); Invoice invoice = createInvoice(customer); Product product = createAProduct( UNIT_PRICE); invoice.addItemQuantity(product, ORIGINAL_QUANTITY); // Exercise SUT invoice.changeQuantityForProduct(product, NEW_QUANTITY); // Verify Outcome LineItem expected = createLineItem( NEW_QUANTITY, CUST_DISCOUNT_PC, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\nAbout the Name\n\nAutomated test programs are traditionally called “test scripts,” probably due to the heritage of such test programs—originally they were implemented in interpreted test scripting languages such as Tcl. The downside of calling them Scripted Tests is that this nomenclature opens the door to confusion with the kind of script a person would follow during manual testing as opposed to unscripted testing such as exploratory testing.\n\nFurther Reading\n\nMany books have been written about the process of writing Scripted Tests and using them to drive the design of the SUT. A good place to start would be [TDD-BE] or [TDD-APG].\n\nwww.it-ebooks.info\n\n287\n\nScripted Test\n\n288\n\nData-Driven Test\n\nChapter 18 Test Strategy Patterns\n\nData-Driven Test\n\nHow do we prepare automated tests for our software? How do we reduce Test Code Duplication?\n\nWe store all the information needed for each test in a data ﬁ le and write an interpreter that reads the ﬁ le and executes the tests.\n\nFixture Fixture\n\nTest 1 Test 1 Data Data\n\nSetup Setup\n\nTest 2 Test 2 Data Data data data\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nTeardown Teardown\n\nTest n Test n Data Data\n\nTesting can be very repetitious not only because we must run the same test over and over again, but also because many of the tests differ only slightly. For example, we might want to run essentially the same test with slightly different system inputs and verify that the actual output varies accordingly. Each of these tests would consist of exactly the same steps. While having so many tests is an excellent way to ensure good coverage of functionality, it is not so good for test maintainability because any change made to the algorithm of one of these tests must be propagated to all of the similar tests.\n\nA Data-Driven Test is one way to get excellent coverage while minimizing\n\nthe amount of test code we need to write and maintain.\n\nHow It Works\n\nWe write a Data-Driven Test interpreter that contains all the common logic from the tests. We put the data that varies from test to test into the Data-Driven Test ﬁ le that the interpreter reads to execute the tests. For each test it performs the same sequence of actions to implement the Four-Phase Test (page 358). First,\n\nwww.it-ebooks.info\n\nData-Driven Test\n\nthe interpreter retrieves the test data from the ﬁ le and sets up the test ﬁ xture us- ing the data from the ﬁ le. Second, it exercises the SUT with whatever arguments the ﬁ le speciﬁ es. Third, it compares the actual results produced by the SUT (e.g., returned values, post-test state) with the expected results from the ﬁ le. If the results don’t match, it marks the test as failed; if the SUT throws an exception, it catches the exception and marks the test accordingly and continues. Fourth, the interpreter does any ﬁ xture teardown that is necessary and then moves on to the next test in the ﬁ le.\n\nA test that might otherwise require a series of complex steps can be reduced to a single line of data in the Data-Driven Test ﬁ le. Fit is a popular example of a framework for writing Data-Driven Tests.\n\nWhen to Use It\n\nA Data-Driven Test is an alternative strategy to a Recorded Test (page 278) and a Scripted Test (page 285). It can also be used as part of a Scripted Test strategy, however, and Recorded Tests are, in fact, Data-Driven Tests when they are played back. A Data-Driven Test is an ideal strategy for getting business people involved in writing automated tests. By keeping the format of the data ﬁ le simple, we make it possible for the business person to populate the ﬁ le with data and execute the tests without having to ask a technical person to write test code for each test.\n\nWe can consider using a Data-Driven Test as part of a Scripted Test strategy whenever we have a lot of different data values with which we wish to exercise the SUT where the same sequence of steps must be executed for each data value. Usually, we discover this similarity over time and refactor ﬁ rst to a Parameterized Test (page 607) and then to a Data-Driven Test. We may also want to arrange a standard set of steps in different sequences with different data values much like in an Incremental Tabular Test (see Parameterized Test). This approach gives us the best coverage with the least amount of test code to maintain and makes it very easy to add more tests as they are needed.\n\nAnother consideration when deciding whether to use Data-Driven Tests is whether the behavior we are testing is hard-coded or driven by conﬁ guration data. If we automate tests for data-driven behavior using Scripted Tests, we must update the test programs whenever the conﬁ guration data changes. This behavior is just plain unnatural because it implies that we must commit changes to our source code repository [SCM] whenever we change the data in our conﬁ guration database.4 By making the tests data-driven, changes to the conﬁ guration data or\n\n4 Of course, we should be managing our test data in a version-controlled Repository, too—but that topic could ﬁ ll another book; see [RDb] for details.\n\nwww.it-ebooks.info\n\n289\n\nData-Driven Test\n\n290\n\nData-Driven Test\n\nChapter 18 Test Strategy Patterns\n\nmeta-objects are then driven by changes to the Data-Driven Tests—a much more natural relationship.\n\nImplementation Notes\n\nOur implementation options depend on whether we are using a Data-Driven Test as a distinct test strategy or as part of an xUnit-based strategy. Using a Data-Driven Test as a stand-alone test strategy typically involves using open- source tools such as Fit or commercial Recorded Test tools such as QTP. Using a Data-Driven Test as part of a Scripted Test strategy may involve implementing a Data-Driven Test interpreter within xUnit.\n\nRegardless of which strategy we elect to follow, we should use the appropri- ate Test Automation Framework (page 298) if one is available. By doing so, we effectively convert our tests into two parts: the Data-Driven Test interpreter and the Data-Driven Test ﬁ les. Both of these assets should be kept under ver- sion control so that we can see how they have evolved over time and to allow us to back out any misguided changes. It is particularly important to store the Data-Driven Test ﬁ les in some kind of Repository, even though this concept may be foreign to business users. We can make this operation transparent by provid- ing the users with a Data-Driven Test ﬁ le-authoring tool such as FitNesse, or we can set up a “user-friendly” Repository such as a document management system that just happens to support version control as well.\n\nIt is also important to run these tests as part of the continuous integration process to conﬁ rm that tests that once passed do not suddenly begin to fail. Failing to do so can result in bugs creeping into the software undetected and signiﬁ cant troubleshooting effort once the bugs are detected. Including the cus- tomer tests in the continuous integration process requires some way to keep track of which customer tests were already passing, because we don’t insist that all customer tests pass before any code is committed. One option is to keep two sets of input ﬁ les, migrating tests that pass from the “still red” ﬁ le into the “all green” ﬁ le that is used for regression testing as part of the automatic build process.\n\nVariation: Data-Driven Test Framework (Fit)\n\nWe should consider using a prebuilt Data-Driven Test framework when we are using Data-Driven Tests as a test strategy. Fit is a framework originally conceived by Ward Cunningham as a way of involving business users in the automation of tests. Although Fit is typically used to automate customer tests, it can also be used for unit tests if the number of tests warrants building the necessary ﬁ x- tures. Fit consists of two parts: the framework and a user-created ﬁ xture. The Fit\n\nwww.it-ebooks.info\n\nData-Driven Test\n\nFramework is a generic Data-Driven Test interpreter that reads the input ﬁ le and ﬁ nds all tables in it. It looks in the top-left cell of each table for a ﬁ xture classname and then searches our test executable for that class. When it ﬁ nds a class, it creates an instance of the class and passes control to that instance as it reads each row and column of the table. We can override methods deﬁ ned by the framework to specify what should happen for each cell in the table. A Fit ﬁ xture, then, is an adapter that Fit calls to interpret a table of data and invoke methods on the SUT.\n\nThe Fit table can also contain expected results from the SUT. Fit compares the speciﬁ ed values with the actual values returned by the SUT. Unlike Asser- tion Methods (page 362) in xUnit, however, Fit does not abandon a test at the ﬁ rst value that does not match the expected value. Instead, it colors in each cell in the table, with green cells indicating actual values that matched the expected values and red cells indicating wrong or unexpected values.\n\nUsing Fit offers several advantages:\n\nThere is much less code to write than when we build our own test\n\nInterpreter [GOF].\n\nThe output makes sense to a business person, not just a technical person.\n\nThe tests don’t stop at the ﬁ rst failed assertion. Fit has a way of com- municating multiple failures/errors in a way that allows us to see the failure patterns very easily.\n\nThere are a plethora of ﬁ xture types available to subclass or use as is.\n\nSo why wouldn’t we use Fit for all our unit testing instead of xUnit? The main disadvantages of using Fit are described here:\n\nThe test scenarios need to be very well understood before we can build the Fit ﬁ xture. We then need to translate each test’s logic into a tabular representation; this isn’t always a good ﬁ t, especially for developers who are used to thinking procedurally. While it may be appropriate to have testers who can write the Fit ﬁ xtures for customer tests, this approach wouldn’t be appropriate for true unit tests unless we had close to a 1:1 tester-to-developer ratio.\n\nThe tests need to employ the same SUT interaction logic in each test.5 To run several different styles of tests, we would probably have to build one or more different ﬁ xtures for each style of test. Building a new ﬁ xture is typically more complex than writing a few Test Methods (page 348).\n\n5 The tabular data must be injected into the SUT during the ﬁ xture setup or exercise SUT phases or retrieved from the SUT during the result veriﬁ cation phase.\n\nwww.it-ebooks.info\n\n291\n\nData-Driven Test\n\n292\n\nData-Driven Test\n\nChapter 18 Test Strategy Patterns\n\nAlthough many different ﬁ xture types are available to subclass or use as is, their use in this way is yet another thing that developers would be required to learn to do their jobs. Even then, not all unit tests are ame- nable to automation using Fit.\n\nFit tests aren’t normally integrated into developers’ regression tests that are run via xUnit. Instead, these tests must be run separately—which introduces the possibility that they will not be run at each check-in. Some teams include Fit tests as part of their continuous integration build process to partially mitigate this issue. Other teams have reported great success having a second “customer” build service or server that runs all the customer tests.\n\nEach of these issues is potentially surmountable, of course. In general, xUnit is a more appropriate framework for unit testing than Fit; the reverse is true for customer tests.\n\nVariation: Naive xUnit Test Interpreter\n\nWhen we have a small number of Data-Driven Tests that we wish to run as part of an xUnit-based Scripted Test strategy, the simplest implementation is to write a Test Method containing a loop that reads one set of input data values from the ﬁ le along with the expected results. This is the equivalent of converting a single Parameterized Test and all its callers into a Tabular Test (see Parameterized Test). As with a Tabular Test, this approach to building the Data-Driven Test interpreter will result in a single Testcase Object (page 382) with many asser- tions. This has several ramiﬁ cations:\n\nThe entire set of Data-Driven Tests will count as a single test. Hence, converting a set of Parameterized Tests into a single Data-Driven Test will reduce the count of tests executed.\n\nWe will stop executing the Data-Driven Test on the ﬁ rst failure or error. As a consequence, we will lose a lot of our Defect Localization (see page 22). Some variants of xUnit do allow us to specify that failed assertions shouldn’t abort execution of the Test Method.\n\nWe need to make sure our assertion failures tell us which subtest we\n\nwere executing when the failure occurred.\n\nWe could address the last two issues by including a try/catch statement inside the loop but surrounding the test logic and then continuing the code’s execution. Nevertheless, we still need to ﬁ nd a way to report the test results in a meaningful way (e.g., “Failed subtests 1, 3, and 6 with . . .”).\n\nwww.it-ebooks.info\n\nData-Driven Test\n\nTo make it easier to extend the Data-Driven Test interpreter to handle sev- eral different kinds of tests in the same data ﬁ le, we can include a “verb” or “action word” as part of each entry in the data ﬁ le. The interpreter can then dispatch to a different Parameterized Test based on the action word.\n\nVariation: Test Suite Object Generator\n\nWe can avoid the “stop on ﬁ rst failure” problem associated with a Naive xUnit Test Interpreter by having the suite method on the Test Suite Factory (see Test Enumeration on page 399) fabricate the same Test Suite Object (page 387) structure as the built-in mechanism for Test Discovery (page 393). To do so, we build a Testcase Object for each entry in the Data-Driven Test ﬁ le and ini- tialize each object with the test data for the particular test.6 That object knows how to execute the Parameterized Test with the data loaded into it when the test suite was built. This ensures that the Data-Driven Test continues execut- ing even after the ﬁ rst Testcase Object encounters an assertion failure. We can then let the Test Runner (page 377) count the tests, errors, and failures in the normal way.\n\nVariation: Test Suite Object Simulator\n\nAn alternative to building the Test Suite Object is to create a Testcase Object that behaves like one. This object reads the Data-Driven Test ﬁ le and iterates over all the tests when asked to run. It must catch any exceptions thrown by the Parameterized Test and continue executing the subsequent tests. When ﬁ nished, the Testcase Object must report the correct number of tests, failures, and errors back to the Test Runner. It also needs to implement any other meth- ods on the standard test interface on which the Test Runner depends, such as returning the number of tests in the “suite,” returning the name and status of each test in the suite (for the Graphical Test Tree Explorer, see Test Runner), and so forth.\n\nMotivating Example\n\nLet’s assume we have a set of tests as follows: def test_extref sourceXml = \"<extref id='abc' />\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<extref>\")\n\n6 This is very similar to how xUnit’s built-in Test Method Discovery (see Test Discovery) mechanism works, except that we are passing in the test data in addition to the Test Method name.\n\nwww.it-ebooks.info\n\n293\n\nData-Driven Test\n\n294\n\nData-Driven Test\n\nChapter 18 Test Strategy Patterns\n\nend\n\ndef test_testterm_normal sourceXml = \"<testterm id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<testterm>\") end\n\ndef test_testterm_plural sourceXml = \"<testterms id='abc'/>\" expectedHtml = \"<a href='abc.html'>abcs</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<plural>\") end\n\nThe succinctness of these tests is made possible by deﬁ ning the Parameterized Test as follows:\n\ndef generateAndVerifyHtml( sourceXml, expectedHtml, message, &block) mockFile = MockFile.new sourceXml.delete!(\"\\t\") @handler = setupHandler(sourceXml, mockFile ) block.call unless block == nil @handler.printBodyContents actual_html = mockFile.output assert_equal_html( expectedHtml, actual_html, message + \"html output\") actual_html end\n\nThe main problem with these tests is that they are still written in code when, in fact, the only difference between them is the data used as input.\n\nRefactoring Notes\n\nThe solution, of course, is to extract the common logic of the Parameterized Tests into a Data-Driven Test interpreter and to collect all sets of parameters into a single data ﬁ le that can be edited by anyone. We need to write a “main” test that knows which ﬁ le to read the test data from and a bit of logic to read and parse the test ﬁ le. This logic can call our existing Parameterized Test logic and let xUnit keep track of the test execution statistics for us.\n\nExample: xUnit Data-Driven Test with XML Data File\n\nIn this example, we will use XML as our ﬁ le representation. Each test consists of a test element with three main parts:\n\nwww.it-ebooks.info\n\nData-Driven Test\n\nAn action that tells the Data-Driven Test interpreter which test logic to run (e.g., crossref)\n\nThe input to be passed to the SUT—in this case, the sourceXml element\n\nThe HTML we expect the SUT to produce (in the expectedHtml element)\n\nThese three components are wrapped up in a testsuite element.\n\n<testsuite id=\"CrossRefHandlerTest\"> <test id=\"extref\"> <action>crossref</action> <sourceXml> <extref id='abc'/> </sourceXml> <expectedHtml> <a href='abc.html'>abc</a> </expectedHtml> </test> <test id=\"TestTerm\"> <action>crossref</action> <sourceXml> <testterm id='abc'/> </sourceXml> <expectedHtml> <a href='abc.html'>abc</a> </expectedHtml> </test> <test id=\"TestTerm Plural\"> <action>crossref</action> <sourceXml> <testterms id='abc'/> </sourceXml> <expectedHtml> <a href='abc.html'>abcs</a> </expectedHtml> </test> </testsuite>\n\nThis XML ﬁ le could be edited by anyone with an XML editor without any concern for introducing test logic errors. All the logic for verifying the expected outcome is encapsulated by the Data-Driven Test interpreter in much the same way as it would be by a Parameterized Test. For viewing purposes we could hide the XML structure from the user by deﬁ ning a style sheet. In addition, many XML editors will turn the XML into a form-based input to simplify editing.\n\nTo avoid dealing with the complexities of manipulating XML, the interpreter\n\ncan also use a CSV ﬁ le as input.\n\nwww.it-ebooks.info\n\n295\n\nData-Driven Test\n\n296\n\nData-Driven Test\n\nChapter 18 Test Strategy Patterns\n\nExample: xUnit Data-Driven Test with CSV Input File\n\nThe test in the previous example would look like this as a CSV ﬁ le:\n\nID, Action, SourceXml, ExpectedHtml Extref,crossref,<extref id='abc'/>,<a href='abc.html'>abc</a> TTerm,crossref,<testterm id='abc'/>,<a href='abc.html'>abc</a> TTerms,crossref,<testterms id='abc'/>,<a href='abc.html'>abcs</a>\n\nThe interpreter is relatively simple and is built on the logic we had already devel- oped for our Parameterized Test. This version reads the CSV ﬁ le and uses Ruby’s split function to parse each line.\n\ndef test_crossref executeDataDrivenTest \"CrossrefHandlerTest.txt\" end\n\ndef executeDataDrivenTest ﬁlename dataFile = File.open(ﬁlename) dataFile.each_line do | line | desc, action, part2 = line.split(\",\") sourceXml, expectedHtml, leftOver = part2.split(\",\") if \"crossref\"==action.strip generateAndVerifyHtml sourceXml, expectedHtml, desc else # new \"verbs\" go before here as elsif's report_error( \"unknown action\" + action.strip ) end end end\n\nUnless we changed the implementation of generateAndVerifyHtml to catch assertion failures and increment a failure counter, this Data-Driven Test will stop executing at the ﬁ rst failed assertion. While this behavior would be acceptable for regres- sion testing, it would not provide very good Defect Localization.\n\nExample: Data-Driven Test Using Fit Framework\n\nIf we wanted to have even more control over what the user can do, we could create a Fit “column ﬁ xture” with the columns “id,” “action,” “source XML,” and “expected Html()” and let the user edit an HTML Web page instead (Figure 18.1).\n\nwww.it-ebooks.info\n\nData-Driven Test\n\n[Figure 18.1: ‘CrossrefHandlerFitTest.vsd’]\n\nFigure 18.1 A Data-Driven test built using the Fit framework.\n\nWhen using Fit, the test interpreter is the Fit framework extended by the Fit ﬁ xture class speciﬁ c to the test:\n\npublic class CrossrefHandlerFixture extends ColumnFixture { // Input columns public String id; public String action; public String sourceXML;\n\n// Output columns public String expectedHtml() { return generateHtml(sourceXML); } }\n\nThe methods of this ﬁ xture class are called by the Fit framework for each cell in each line in the Fit table based on the column headers. Simple names are interpreted as the instance variable of the ﬁ xture (e.g., “id,” “source XML”). Column names ending in “()” signify a function that Fit calls and then compares its result with the contents of the cell.\n\nThe resulting output is shown in Figure 18.2. This colored-in table allows us\n\nto get an overview of the results of running one ﬁ le of tests at a single glance.\n\nFigure 18.2 The results of executing the Fit test.\n\nwww.it-ebooks.info\n\n297\n\nData-Driven Test\n\n298\n\nTest Automation Framework\n\nChapter 18 Test Strategy Patterns\n\nTest Automation Framework\n\nHow do we make it easy to write and run tests written by different people?\n\nWe use a framework that provides all the mechanisms needed to run the test logic so the test writer needs to provide only the test-speciﬁ c logic.\n\nTest Automation Framework Test Automation Framework\n\nFixture Fixture\n\nTest Runner Test Runner\n\nSetup Setup\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nInputs Inputs\n\nExpected Expected Outputs Outputs\n\nTest Test Script 1 Script 1\n\nTest Test Script 2 Script 2\n\nTest Test Script n Script n\n\nTest Automation Infrastructure Test Automation Infrastructure\n\nWriting and running automated tests involves several steps, but many of these steps are the same for every test. If every test had to include an implementation of these steps, writing automated tests would be very tedious, time-consuming, prone to errors, and expensive.\n\nUsing a Test Automation Framework is a way to minimize the effort of writing\n\nFully Automated Tests (see page 26).\n\nHow It Works\n\nWe build a framework that implements all the mechanisms required to run suites of tests and record the results. These mechanisms include the ability to ﬁ nd in- dividual tests, assemble them into a test suite, execute each test in turn, verify expected outcomes, collect and report any test failures or errors, and clean up when failures or errors do occur. The framework provides a way to plug in and run the test-speciﬁ c behavior that test automaters write.\n\nwww.it-ebooks.info\n\nTest Automation Framework\n\nWhy We Do This\n\nBuilding Fully Automated Tests that are repeatable and robust is a much more complicated process than just writing a test script that invokes the SUT. We need to handle success cases and error cases, both expected and unexpected. We need to set up and tear down test ﬁ xtures. We need to specify which test(s) to run. We also need to report on the results after we have run a suite of tests.\n\nThe amount of effort required to build Fully Automated Tests can act as a serious deterrent to automation of tests. We can reduce the cost of getting started signiﬁ cantly by providing a framework that implements the most com- mon functionality—the only entry cost is then incurred while learning to use the framework. This cost, in turn, can be reduced if the framework implements a common protocol such as xUnit that makes it easier for us to learn a second or third framework once we have experience with the ﬁ rst.\n\nUsing a framework also helps isolate the implementation of the logic re- quired to run the tests from the logic of the tests. This approach can help reduce Test Code Duplication (page 213) and minimize the occurrence of Obscure Tests (page 186). It also ensures that test written by different test automaters can be run easily in a single test run with a single report on the test results.\n\nImplementation Notes\n\nMany kinds of Test Automation Frameworks are available, from both com- mercial vendors and open-source resources. They can be classiﬁ ed into two main categories: “robot user” test tools and Scripted Tests (page 285). The latter category can be further subdivided into the xUnit and Data-Driven Tests (page 288) families of Test Automation Frameworks.\n\nVariation: Robot User Test Frameworks\n\nA large number of third-party test automation tools are designed to test applica- tions via the user interface. Most of them use the “record and playback” test metaphor. This metaphor leads to some very seductive marketing materials, because it makes test automation seem as simple as running some tests manu- ally while recording the test session. Such a robot user test tool consists of two major parts: the “test recorder,” which monitors and records the interactions between the user and the SUT, and the “test runner,” which executes the Recorded Tests (page 278). Most of these test automation tools are also frameworks that support a number of “widget recognizer” plug-ins. Most commercial tools come with a gaggle of built-in widget recognizers.\n\nwww.it-ebooks.info\n\n299\n\nTest Automation Framework\n\n300\n\nTest Automation Framework\n\nChapter 18 Test Strategy Patterns\n\nVariation: The xUnit Family of Test Automation Frameworks\n\nMost unit-testing tools belong to the xUnit family of testing frameworks designed for automating Hand-Scripted Tests (see Scripted Test). xUnit has been ported to (or developed from scratch for) most current programming languages. The xUnit family of unit-testing frameworks consists of several major components. The most visible is the Test Runner (page 377), which can be invoked either from the command line or as a Graphical Test Runner (see Test Runner). It builds the Testcase Objects (page 382), collects them into Test Suite Objects (page 387), and invokes each of the Test Methods (page 348). The other major component of the xUnit frameworks is the library of built-in Assertion Methods (page 362) that are used within the Test Methods to specify the expected outcome of each test.\n\nVariation: Data-Driven Test Frameworks\n\nA Data-Driven Test framework provides a way to plug in interpreters that know how to execute a speciﬁ c kind of test step. This ﬂ exibility, in effect, extends the format of the input ﬁ le with new “verbs” and objects. Such a framework also provides a test runner that reads in the ﬁ le, passes control to the plug-ins when their corresponding data formats are encountered, and keeps track of statistics for the test run. The most notable member of the Data-Driven Test Frameworks family is Fit, which enables test automaters to write tests in tabular form and to “plug in” ﬁ xture classes that know how to interpret speciﬁ c formats of tables.\n\nExample: Test Automation Framework\n\nThe Test Automation Framework looks somewhat different for each of the possible ways to automate tests. To see these variations, refer to Recorded Test, Scripted Test, and Data-Driven Test for examples of the respective Test Auto- mation Frameworks.\n\nFurther Reading\n\nSome of the more popular examples of Test Automation Frameworks for xUnit are JUnit (Java), SUnit (Smalltalk), CppUnit (C++), NUnit (all .NET languages), runit (Ruby), PyUnit (Python), and VbUnit (Visual Basic). A more complete and up-to-date list can be found at http://xprogramming.com, along with a list of the available extensions (e.g., HttpUnit, Cactus).\n\nwww.it-ebooks.info\n\nTest Automation Framework\n\nOther open-source Test Automation Frameworks include Fit, Canoo Web- Test, and Watir. Commercial Test Automation Frameworks include QTP, BPT, and eCATT, among many others.\n\nIn Test-Driven Development—By Example [TDD-BE], Kent Beck illustrates TDD by building a Test Automation Framework in Python. In an approach he likens to “doing brain surgery on yourself,” he uses the emerging Test Automa- tion Framework to run the tests he writes for each new capability. This applica- tion is a very good example of both TDD and bootstrapping.\n\nwww.it-ebooks.info\n\n301\n\nTest Automation Framework\n\n302\n\nMinimal Fixture\n\nAlso known as: Minimal Context\n\nChapter 18 Test Strategy Patterns\n\nMinimal Fixture\n\nWhich ﬁ xture strategy should we use?\n\nWe use the smallest and simplest ﬁ xture possible for each test.\n\nFixture Fixture\n\nsetUp setUp\n\ntest_1 test_1\n\nFixture Fixture\n\ntest_2 test_2\n\nSUT SUT\n\ntest_n test_n\n\nTestcase Class Testcase Class\n\nEvery test needs some kind of test ﬁ xture. A key part of understanding a test is understanding the test ﬁ xture and recognizing how it inﬂ uences the expected outcome of the test. Tests are much easier to understand if the ﬁ xture is small and simple.\n\nWhy We Do This\n\nA Minimal Fixture is important for achieving Tests as Documentation (see page 23) and for avoiding Slow Tests (page 253). A test that uses a Minimal Fixture will always be easier to understand than one that uses a ﬁ xture contain- ing unnecessary or irrelevant information. This is true whether we are using a Fresh Fixture (page 311) or a Shared Fixture (page 317), although the effort to build a Minimal Fixture is typically higher with a Shared Fixture because it must be designed to handle several tests. Deﬁ ning a Minimal Fixture is much easier for a Fresh Fixture because it need serve only a single test.\n\nwww.it-ebooks.info\n\nMinimal Fixture\n\nImplementation Notes\n\nWe design a ﬁ xture that includes only those objects that are absolutely necessary to express the behavior that the test veriﬁ es. Another way to phrase this is “If the object is not important to understand the test, it is important not to include it in the ﬁ xture.”\n\nTo build a Minimal Fixture, we ruthlessly remove anything from the ﬁ xture that does not help the test communicate how the SUT should behave. Two forms of “minimization” can be considered:\n\nWe can eliminate objects entirely. That is, we don’t even build the objects as part of the ﬁ xture. If the object isn’t necessary to prove something about how the SUT behaves, we don’t include it at all.\n\nWe can hide unnecessary attributes of the object when they don’t con-\n\ntribute to the understanding of the expected behavior.\n\nA simple way to ﬁ nd out whether an object is necessary as part of the ﬁ xture is to remove it. If the test fails as a result, the object was probably necessary in some way. Of course, it may have been necessary only as an argument to some method we are not interested in or as an attribute that is never used (even though the object to which the attribute belongs is required for some reason). Including these kinds of objects as part of ﬁ xture setup deﬁ nitely contributes to Obscure Tests (page 186). We can eliminate these unnecessary objects in one of two ways: (1) by hiding them or (2) by eliminating the need for them by passing in Dummy Objects (page 728) or using Entity Chain Snipping (see Test Stub on page 529). If the SUT actually accesses the object as it is executing the logic under test, however, we may be forced to include the object as part of the test ﬁ xture.\n\nHaving determined that the object is necessary for the execution of the test, we must now ask whether the object is helpful in understanding the test. If we were to initialize it “off-stage,” would that make it harder to understand the test? Would the object lead to an Obscure Test by acting as a Mystery Guest (see Obscure Test)? If so, we want to keep the object visible. Boundary values are a good example of a case in which we do want to keep the objects and at- tributes that take on the boundary values visible.\n\nIf we have established that the object or attribute isn’t necessary for understanding the test, we should make every effort to eliminate it from the Test Method (page 348), albeit not necessarily from the test ﬁ xture. Creation Methods (page 415) are a common way of achieving this goal. We can hide the attributes of objects that don’t affect the outcome of the test but that are needed for construction of the object by using Creation Methods to ﬁ ll in all\n\nwww.it-ebooks.info\n\n303\n\nMinimal Fixture\n\n304\n\nMinimal Fixture\n\nChapter 18 Test Strategy Patterns\n\nthe “don’t care” attributes with meaningful default values. We can also hide the creation of necessary depended-on objects within the Creation Methods. A good example of this occurs when we write tests that require badly formed objects as input (for testing the SUT with invalid inputs). In this case we don’t want to confuse the issue by showing all valid attributes of the object being passed to the SUT; there could be many of these extraneous attributes. Instead, we want to focus on the invalid attribute. To do so, we can use the One Bad Attribute pattern (see Derived Value on page 718) to build malformed objects with a minimum of code by calling a Creation Method to construct a valid object and then replacing a single attribute with the invalid value that we want to verify the SUT will handle correctly.\n\nwww.it-ebooks.info\n\nStandard Fixture\n\nStandard Fixture\n\nWhich ﬁ xture strategy should we use?\n\nWe reuse the design of the text ﬁ xture across the many tests.\n\nSetup Setup\n\nExercise Exercise\n\nStandard Fixture Standard Fixture Setup Logic Setup Logic\n\nFixture Fixture\n\nVerify Verify\n\nSUT SUT\n\nSetup Setup Teardown Teardown\n\nExercise Exercise\n\nFixture Fixture\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nTo execute an automated test, we require a text ﬁ xture that is well understood and completely deterministic. Designing a custom test ﬁ xture for each test requires extra effort. A Standard Fixture offers a way to reuse the same ﬁ xture design in several tests without necessarily sharing the same ﬁ xture instance.\n\nHow It Works\n\nA Standard Fixture is more about attitude than about technology. It requires us to decide early on in the testing process that we will design a Standard Fixture that can be used by several or many tests rather than mining a common ﬁ xture from tests that were designed independently. In a sense, a Standard Fixture is the result of “Big Design Upfront” of the test ﬁ xture for a whole suite of tests. We then deﬁ ne our speciﬁ c tests using this common test ﬁ xture design.\n\nThe choice of a Standard Fixture is independent of the choice between a Fresh Fixture (page 311) and a Shared Fixture (page 317). A Shared Fixture is, by deﬁ nition, a Standard Fixture. The reverse is not true, however, because a Standard Fixture focuses on reuse of the ﬁ xture’s design—not the time when the ﬁ xture is built or its visibility. Having chosen to use a Standard Fixture, we still need to decide whether each test will build its own instance of the Standard\n\nwww.it-ebooks.info\n\n305\n\nStandard Fixture\n\nAlso known as: Standard Context\n\n306\n\nStandard Fixture\n\nChapter 18 Test Strategy Patterns\n\nFixture (a Fresh Fixture) or whether we will build it once as a Shared Fixture and reuse it across many tests.\n\nWhen to Use It\n\nWhen I was reviewing an early draft of this book with Series Editor Martin Fowler, he asked me, “Do people actually do this?” This question exempliﬁ es the philosophical divide of ﬁ xture design. Coming from an agile background, Martin lets each test pull a ﬁ xture into existence. If several tests happen to need the same ﬁ xture, then it makes sense to factor it out into the setUp method and split the class into one Testcase Class per Fixture (page 631). It doesn’t even occur to Martin to design a Standard Fixture that all tests can use. So who uses them? Standard Fixtures are something of a tradition in the testing (quality assess- ment) community. It is very commonplace to deﬁ ne a large Standard Fixture that is then used as a test bed for testing activities. This approach makes a lot of sense in the context of manual execution of many customer tests because it eliminates the need for each tester to spend a lot of time setting up the test environment for each customer test and it allows several testers to work in the same test environ- ment at the same time. Some test automaters also use Standard Fixtures when deﬁ ning their automated customer tests. This strategy is especially prevalent when test automaters use a Shared Fixture, for obvious reasons.\n\nIn the xUnit community, use of a Standard Fixture simply to avoid designing a Minimal Fixture (page 302) for each test is considered undesirable and has been given the name General Fixture (see Obscure Test on page 186). A more accepted example is the use of Implicit Setup (page 424) in conjunction with Testcase Class per Fixture because only a few Test Methods (page 348) share the design of the ﬁ xture and they do so because they need the same design. As we make a Standard Fixture more reusable across many tests with disparate needs, it tends to grow larger and more complex. This trend can lead to a Fragile Fixture (see Fragile Test on page 239) as the needs of new tests introduce changes that break existing clients of the Standard Fixture. Depending on how we go about building the Standard Fixture, we may also ﬁ nd ourselves entertaining a Mystery Guest (see Obscure Test) if the cause–effect relationships between the ﬁ xture and out- come are not easy to discern either because the ﬁ xture setup is hidden from the test or because it is not clear which characteristics of the referenced part of the Standard Fixture serve as pre-conditions for the test.\n\nA Standard Fixture will also take longer to build than a Minimal Fixture because there is more ﬁ xture to construct. When we are building a Fresh Fixture for each Testcase Object (page 382), this effort can lead to Slow Tests (page 253), especially if the ﬁ xture setup involves a database. (See the sidebar “Unit Test Rulz”\n\nwww.it-ebooks.info\n\nStandard Fixture\n\nUnit Test Rulz\n\nMichael Feathers of Object Mentor writes:\n\nI’ve used these rules with a large number of teams. They encour- age good design and rapid feedback and they seem to help teams avoid a lot of trouble.\n\nA test is not a unit test if:\n\nIt talks to the database.\n\nIt communicates across the network.\n\nIt touches the ﬁ le system.\n\nIt can’t run correctly at the same time as any of your other unit\n\ntests.\n\nYou have to do special things to your environment (such as\n\nediting conﬁ g ﬁ les) to run it.\n\nTests that do these things aren’t bad. Often they are worth writ- ing, and they can be written in a unit test harness. However, it is important to be able to separate them from true unit tests so that we can keep a set of tests that we can run fast whenever we make our changes.\n\nhttp://www.objectmentor.com\n\nfor an opinion about what kinds of behavior are acceptable for a unit test.) For these reasons, we may be better off using a Minimal Fixture to avoid the extra ﬁ xture setup overhead associated with creating objects that are only needed in other tests.\n\nImplementation Notes\n\nAs mentioned earlier, we can use a Standard Fixture as either a Fresh Fixture or a Shared Fixture, and we can set it up using either Implicit Setup or Delegated Setup (page 411).7 When using it as a Fresh Fixture, we can deﬁ ne a Test Utility Method (page 599) (function or procedure) that builds the Standard Fixture; we can then call the Test Utility Method from each test that needs this particular design of ﬁ xture. Alternatively, we can take advantage of xUnit support for Implicit Setup by putting all of the ﬁ xture construction logic in the setUp method.\n\n7 Doing it with In-line Setup (page 408) would be silly—we would have to copy the code to construct the Standard Fixture to every Test Method.\n\nwww.it-ebooks.info\n\n307\n\nStandard Fixture\n\n308\n\nStandard Fixture\n\nChapter 18 Test Strategy Patterns\n\nWhen building a Standard Fixture for use as a Shared Fixture, we can employ any of the Shared Fixture setup patterns including Suite Fixture Setup (page 441), Lazy Setup (page 435), and Setup Decorator (page 447).\n\nMotivating Example\n\nAs mentioned earlier, we are most likely to end up using a Standard Fixture because we started that way—and we probably started that way as the result of the background of one of the project participants. We probably would not refactor our tests to use a Standard Fixture when those tests are already written to use a Minimal Fixture unless we were refactoring to create a Testcase Class per Fixture. For the sake of illustration, let’s assume that we did want to get to “here” from “there.” The following example uses Creation Methods (page 415) to build a custom Fresh Fixture for each test:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight_c() throws Exception { FlightDto outboundFlight = createOneOutboundFlightDto(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights_c() throws Exception { FlightDto[] outboundFlights = createTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nTo keep this test short, we have used Delegated Setup to populate the SUT with the Minimal Fixture needed for each test. We could have included the ﬁ xture setup code in-line in each method, but that choice would take us down the road toward an Obscure Test.\n\nwww.it-ebooks.info\n\nStandard Fixture\n\nRefactoring Notes\n\nTechnically speaking, converting a pile of tests to a Standard Fixture isn’t really a “refactoring” because we actually change the behavior of these tests. The biggest challenge is designing the reusable Standard Fixture in such a way that each Test Method can ﬁ nd some part of the ﬁ xture that serves its needs. This means synthe- sizing all of the individual purpose-built Minimal Fixtures into a single “jack of all trades” ﬁ xture. Not surprisingly, this reworking of the code can be a nontrivial exercise when we have a lot of tests.\n\nThe easy and mechanical part of the refactoring is to convert the logic in each test that constructs the ﬁ xture into calls to Finder Methods (see Test Utility Method) that retrieve the appropriate part of the Standard Fixture. This transfor- mation is most easily done as a series of steps. First, we extract the in-line ﬁ xture construction logic in each Test Method into one or more Creation Methods with Intent-Revealing Names [SBPP]. Next, we do a global replace on the “create” part of each call to “ﬁ nd.” Finally, we generate (either manually or using our IDE’s “quick ﬁ x” capability) the Finder Methods needed to get the calls to compile. Inside each Finder Methods we add in code to return the relevant part of the Standard Fixture.\n\nExample: Standard Fixture\n\nHere’s the example given earlier converted to use a Standard Fixture:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { setupStandardAirportsAndFlights(); FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { setupStandardAirportsAndFlights(); FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome\n\nwww.it-ebooks.info\n\n309\n\nStandard Fixture\n\n310\n\nStandard Fixture\n\nChapter 18 Test Strategy Patterns\n\nassertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nTo make the use of a Standard Fixture really obvious, this example shows a Fresh Fixture that is created explicitly in each test by calling the same Creation Method to set up the Standard Fixture (i.e., using Delegated Setup). We could have achieved the same effect by putting the ﬁ xture construction logic into the setUp method, thus using Implicit Setup. The resulting test would look identical to one that uses a Shared Fixture.\n\nwww.it-ebooks.info\n\nFresh Fixture\n\nFresh Fixture\n\nWhich ﬁ xture strategy should we use?\n\nEach test constructs its own brand-new test ﬁ xture for its own private use.\n\nSetup Setup\n\nFixture Fixture\n\nExercise Exercise\n\nSetup Setup\n\nVerify Verify\n\nFixture Fixture\n\nSUT SUT\n\nExercise Exercise\n\nTeardown Teardown\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nEvery test needs a test ﬁ xture. It deﬁ nes the state of the test environment before the test. The choice of whether to build the ﬁ xture from scratch each time the test is run or to reuse a ﬁ xture built earlier is a key test automation decision.\n\nWhen each test creates a Fresh Fixture, Erratic Tests (page 228) are less likely and the testing effort is more likely to result in Tests as Documentation (see page 23).\n\nHow It Works\n\nWe design and build the test ﬁ xture such that only a single run of a single test will use it. We construct the ﬁ xture as part of running the test and tear down the ﬁ xture when the test has ﬁ nished. We do not reuse any ﬁ xture left over by other tests or other test runs. This way, we start and end every test with a “clean slate.”\n\nwww.it-ebooks.info\n\n311\n\nFresh Fixture\n\nAlso known as: Fresh Context, Private Fixture\n\n312\n\nFresh Fixture\n\nChapter 18 Test Strategy Patterns\n\nWhen to Use It\n\nWe should use a Fresh Fixture whenever we want to avoid any interdependencies between tests that can result in Erratic Tests such as Lonely Tests (see Erratic Test) or Interacting Tests (see Erratic Test). If we cannot use a Fresh Fixture because it slows the tests down too much, we should consider using an Immutable Shared Fixture (see Shared Fixture on page 317) before resorting to a Shared Fixture. Note that using a Database Partitioning Scheme (see Database Sandbox on page 650) to create a private Database Sandbox for the test that no other tests will touch does not result in a Fresh Fixture because subsequent test runs could use the same ﬁ xture.\n\nImplementation Notes\n\nA ﬁ xture is considered a Fresh Fixture if we intend to use it a single time. Whether the Fresh Fixture is transient or persistent depends on the nature of the SUT and how the tests are written (Figure 18.3). While the intent is the same, the implemen- tation considerations are somewhat different when the Fresh Fixture is persistent. Fixture setup is largely unaffected, so it is discussed as a feature common to all such ﬁ xtures. Fixture teardown is speciﬁ c to the particular variation.\n\nTransient Transient\n\nFresh Fresh Fixture Fixture\n\nPersistent Persistent\n\nShared Shared Fixture Fixture\n\nImmutable Immutable Shared Shared Fixture Fixture\n\nFigure 18.3 Test ﬁ xture strategies. A ﬁ xture can be either Fresh, Shared, or a combination of the two (the immutable Shared Fixture) based on whether some, or all, of it persists between tests.\n\nwww.it-ebooks.info\n\nFresh Fixture\n\nWhy Does a Fixture Persist?\n\nThe ﬁ xture we construct may hang around after the Test Method (page 348) has ﬁ nished executing for one of two reasons. First, if the ﬁ xture primarily consists of the state of some other objects or components on which the SUT depends, its persistence is determined by whether those other objects are them- selves persistent. A database is one such beast. That’s because as soon as some code persists the ﬁ xture objects into a database, the objects “hang around” long after our test is done. Their existence in the database opens the door to collisions between multiple runs of our own test (Unrepeatable Test; see Erratic Test). Other tests may also be able to access those objects, which can result in other forms of Erratic Tests such as Interacting Tests and Test Run Wars. If we must use a database or other form of object persistence, we should take extra steps to keep the ﬁ xture private. In addition, we should tear down the ﬁ xture after each Test Method.\n\nThe second reason that a ﬁ xture might persist lies within the control of our tests—namely, which kind of variable we choose to hold the reference to the ﬁ xture. Local variables naturally go out of scope when the Test Method ﬁ nishes executing; therefore any ﬁ xture held in a local variable will be destroyed by garbage collection. Instance variables go out of scope when the Testcase Object is destroyed8 and require explicit teardown only if the xUnit framework doesn’t recreate the Testcase Objects during each test run. By contrast, class variables usually result in persistent ﬁ xtures that can outlive a single test method or even a test run and should therefore be avoided when using a Fresh Fixture.\n\nIn practice, our ﬁ xture will not normally be persistent in unit tests9 unless we have tightly coupled our application logic to the database. A ﬁ xture is more likely to be persistent when we are writing customer tests or possibly compo- nent tests.\n\nFresh Fixture Setup\n\nConstruction of the ﬁ xture is largely unaffected by whether it is persistent or tran- sient. The primary consideration is the location of the code to set up the ﬁ xture. We can use In-line Setup (page 408) if the ﬁ xture setup is relatively simple. For more complex ﬁ xtures, we generally prefer using Delegated Setup (page 411) when our\n\n8 Most members of the xUnit family create a separate Testcase Object (page 382) for each Test Method. A few do not, however. This difference can trip up unwary test automaters when they ﬁ rst start using these members of the family because instance variables may unexpectedly act like class variables. For a detailed description of this issue, see the sidebar “There’s Always an Exception” (page 384). 9 The sidebar “Unit Test Rulz” (page 307) explains what constitutes a unit test.\n\nwww.it-ebooks.info\n\n313\n\nFresh Fixture\n\n314\n\nFresh Fixture\n\nChapter 18 Test Strategy Patterns\n\nTest Methods are organized using Testcase Class per Class (page 617) or Testcase Class per Feature (page 624). We can use Implicit Setup (page 424) to build the ﬁ xture if we have used the Testcase Class per Fixture (page 631) organization.\n\nVariation: Transient Fresh Fixture\n\nIf we need to refer to the ﬁ xture from several places in the test, we should use only local variables or instance variables to refer to the ﬁ xture. In most cases we can depend on Garbage-Collected Teardown (page 500) to destroy the ﬁ xture without any effort on our part.\n\nNote that a Standard Fixture (page 305) can also be a Fresh Fixture if the ﬁ xture is built from scratch before each Test Method is run. This approach reuses the design of the ﬁ xture rather than the instance. It is commonly encountered when we use Implicit Setup but we are not using Testcase Class per Fixture to organize our Test Methods.\n\nVariation: Persistent Fresh Fixture\n\nIf we do end up using a Persistent Fresh Fixture, either we need to tear down the ﬁ xture or we need to take special measures to avoid the need for its teardown. We can tear down the ﬁ xture using In-line Teardown (page 509), Implicit Tear- down (page 516), Delegated Teardown (see In-line Teardown), or Automated Teardown (page 503) to leave the test environment in the same state as when we entered it.\n\nTo avoid ﬁ xture teardown, we can use a Distinct Generated Value (see Generated Value on page 723) for each ﬁ xture object that must be unique. This strategy can become the basis of a Database Partitioning Scheme that seeks to isolate the tests and test runners from one another. It would prevent Resource Leakage (see Erratic Test) in case our teardown process fails. We can also combine this approach with one of the teardown patterns to be doubly sure that no Unrepeatable Tests or Interacting Tests exist.\n\nNot surprisingly, this additional work has some drawbacks: It makes tests more complicated to write and it often leads to Slow Tests (page 253). A natural reaction is to take advantage of the persistence of the ﬁ xture by reusing it across many tests, thereby avoiding the overhead of setting it up and tearing it down. Unfortunately, this choice has many undesirable ramiﬁ cations because it violates one of our major principles: Keep Tests Independent (see page 42). The result- ing Shared Fixture invariably leads to Interacting Tests and Unrepeatable Tests, if not immediately, then at some point down the road. We should not venture down this road without fully understanding the consequences!\n\nwww.it-ebooks.info\n\nFresh Fixture\n\nMotivating Example\n\nHere’s an example of a Shared Fixture:\n\nstatic Flight ﬂight; public void setUp() { if (ﬂight == null) { // Lazy SetUp Airport departAirport = new Airport(\"Calgary\", \"YYC\"); Airport destAirport = new Airport(\"Toronto\", \"YYZ\"); ﬂight = new Flight( ﬂightNumber, departAirport, destAirport); } }\n\npublic void testGetStatus_inital_S() { // implicit setup // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // teardown } public void testGetStatus_cancelled() { // implicit setup partially overridden ﬂight.cancel(); // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // teardown }\n\nBased on the code that actually sets up the ﬁ xture as shown here, it is a normal Shared Fixture, but we could have just as easily used a Prebuilt Fixture (page 429) for this motivating example. Either way, these tests could start interacting at any time.\n\nRefactoring Notes\n\nSuppose we are using a Shared Fixture (same design, single copy) and decide to refactor it to use a Fresh Fixture. We can start by refactoring the test to use a fresh Standard Fixture (same design, many copies). Then we can decide whether we want to further evolve the test so that it builds a Minimal Fixture (page 302) by pruning the ﬁ xture setup logic to the bare minimum using a Minimize Data (page 738) refactoring. This point would also be good time to group Test Methods that need the same type of test ﬁ xture into a Testcase Class per Fixture and use Implicit Setup; this use of a Standard Fixture would reduce the number of Minimal Fixtures we need to design and build.\n\nwww.it-ebooks.info\n\n315\n\nFresh Fixture\n\n316\n\nFresh Fixture\n\nChapter 18 Test Strategy Patterns\n\nExample: Fresh Fixture\n\nHere’s the same test converted to a Fresh Fixture to avoid any possibility of Interacting Tests:\n\npublic void testGetStatus_inital() { // setup Flight ﬂight = createAnonymousFlight(); // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // teardown // garbage-collected }\n\npublic void testGetStatus_cancelled2() { // setup Flight ﬂight = createAnonymousCancelledFlight(); // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // teardown // garbage-collected }\n\nNote the use of Anonymous Creation Methods (see Creation Method on page 415) to construct the appropriate state Flight object in each test.\n\nwww.it-ebooks.info\n\nShared Fixture\n\nShared Fixture\n\nHow can we avoid Slow Tests? Which ﬁ xture strategy should we use?\n\nWe reuse the same instance of the test ﬁ xture across many tests.\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nExercise Exercise\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nVerify Verify\n\nTeardown Teardown\n\nTo execute an automated test, we require a text ﬁ xture that is well understood and completely deterministic. Setting up a Fresh Fixture (page 311) can be time- consuming, especially when we are dealing with complex system state stored in a test database.\n\nWe can make our tests run faster by reusing the same ﬁ xture for several or\n\nmany tests.\n\nHow It Works\n\nThe basic concept is pretty simple: We create a Standard Fixture (page 305) ﬁ xture that outlasts the lifetime of a single Testcase Object (page 382). This approach allows multiple tests to reuse the same test ﬁ xture without destroying that ﬁ xture and recreating it between tests. A Shared Fixture can be either a Prebuilt Fixture that is reused by one or more tests in many test runs or a ﬁ xture that is created by one test and reused by another test within the same test run. In either case, the key consideration is that many tests do not create their own ﬁ xtures but rather reuse a ﬁ xture “left over” from some other activity. The tests run faster because they have less ﬁ xture setup to perform, which may result in the test automater having to do less work to deﬁ ne the ﬁ xture for each test.\n\nwww.it-ebooks.info\n\n317\n\nShared Fixture\n\nAlso known as: Shared Context, Leftover Fixture, Reused Fixture, Stale Fixture\n\n318\n\nShared Fixture\n\nChapter 18 Test Strategy Patterns\n\nWhen to Use It\n\nRegardless of why we use them, Shared Fixtures come with some baggage that we should understand before we head down this path. The major issue with a Shared Fixture is that it can lead to interactions between tests, possibly resulting in Erratic Tests (page 228) if some tests depend on the outcomes of other tests. Another potential problem is that a ﬁ xture designed to serve many tests is bound to be much more complicated than the Minimal Fixture (page 302) needed for a single test. This greater complexity will typically take more effort to design and can lead to a Fragile Fixture (see Fragile Test on page 239) later on down the road when we need to modify the ﬁ xture.\n\nA Shared Fixture will often result in an Obscure Test (page 186) because the ﬁ xture is not constructed inside the test. This potential disadvantage can be mitigated by using Finder Methods (see Test Utility Method on page 599) with Intent-Revealing Names [SBPP] to access the relevant parts of the ﬁ xture.\n\nThere are some valid reasons for using a Shared Fixture and some misguided ones. Many of the variations have been devised primarily to mitigate the negative consequences of using a Shared Fixture. So, what are good reasons for using a Shared Fixture?\n\nVariation: Slow Tests\n\nWe can use a Shared Fixture when we cannot afford to build a new Fresh Fixture for each test. Typically, this scenario will occur when it takes too much processing to build a new ﬁ xture for each test, which often leads to Slow Tests (page 253). It most commonly occurs when we are testing with real test databases due to the high cost of creating each of the records. This growth in overhead tends to be exacerbated when we use the API of the SUT to create the reference data, because the SUT often does a lot of input validation, which may involve reading some of the just-written records.\n\nA better solution is to make the tests run faster by not interacting with the database at all. For a more complete list of options, see the solutions to Slow Tests and the sidebar “Faster Tests Without Shared Fixtures” (page 319).\n\nwww.it-ebooks.info\n\nShared Fixture\n\nFaster Tests Without Shared Fixtures\n\nThe ﬁ rst reaction to Slow Tests (page 253) is often to switch to a Shared Fixture (page 317) approach. Several other solutions are available, how- ever. This sidebar describes some experiences on several projects.\n\nFake Database On one of our early XP projects, we wrote a lot of tests that accessed the database. At ﬁ rst we used a Shared Fixture. When we encountered Interacting Tests (see Erratic Test on page 228) and later Test Run Wars (see Erratic Test), however, we changed to a Fresh Fixture (page 311) approach. Because these tests needed a fair bit of reference data, they were taking a long time to run. On average, for every read or write the SUT did to or from the database, each test did several more. It was tak- ing 15 minutes to run the full test suite of several hundred tests, which greatly impeded our ability to integrate our work quickly and often.\n\nAt the time, we were using a data access layer to keep the SQL out of our code. We soon discovered that it allowed us to replace the real data- base with a functionally equivalent Fake Database (see Fake Object on page 551). We started out by using simple HashTables to store the objects against a key. This approach allowed us to run many of our simpler tests “in memory” rather than against the database. And that bought us a sig- niﬁ cant drop in test execution time.\n\nOur persistence framework supported an object query interface. We were able to build an interpreter of the object queries that ran against our HashTable database implementation and that allowed the majority of our tests to work entirely in memory. On average, our tests ran about 50 times faster in memory than with the database. For example, a test suite that took 10 minutes to run with the database took 10 seconds to run in memory.\n\nThis approach was so successful that we have reused the same testing infrastructure on many of our subsequent projects. Using the faked-out persistence framework also means we don’t have to bother with building a “real database” until our object models stabilize, which can be several months into the project.\n\nIncremental Speedups Ted O’Grady and Joseph King are agile team leads on a large (50-plus developers, subject matter experts, and testers) eXtreme Programming project. Like many project teams building database-centric applications,\n\nContinued...\n\nwww.it-ebooks.info\n\n319\n\nShared Fixture\n\n320\n\nShared Fixture\n\nChapter 18 Test Strategy Patterns\n\nthey suffered from Slow Tests. But they found a way around this problem: As of late 2005, their check-in test suite ran in less than 8 minutes com- pared to 8 hours for a full test run against the database. That is a pretty impressive speed difference. Here is their story:\n\nCurrently we have about 6,700 tests that we run on a regular basis. We’ve actually tried a few things to speed up the tests and they’ve evolved over time.\n\nIn January 2004, we were running our tests directly against a database via Toplink.\n\nIn June 2004, we modiﬁ ed the application so we could run tests against an in-memory, in-process Java database (HSQL). This cut the time to run in half.\n\nIn August 2004, we created a test-only framework that allowed Toplink to work without a database at all. That cut the time to run all the tests by a factor of 10.\n\nIn July 2005, we built a shared “check-in” test execution server that allowed us to run tests remotely. This didn’t save any time at ﬁ rst but it has proven to be quite useful nonetheless.\n\nIn July 2005, we also started using a clustering framework that al- lowed us to run tests distributed across a network. This cut the time to run the tests in half.\n\nIn August 2005, we removed the GUI and Master Data (reference data crud) tests from the “check-in suite” and ran them only from Cruise Control. This cut the time to run by approximately 15% to 20%.\n\nSince May 2004, we have also had Cruise Control run all the tests against the database at regular intervals. The time it takes Cruise Control to complete [the build and run the tests] has grown with the number of tests from an hour to nearly 8 hours now.\n\nWhen a threshold has been met that prevents the developers from (a) running [the tests] frequently when developing and (b) creat- ing long check-in queues as people wait for the token to check in, we have adapted by experimenting with new techniques. As a rule we try to keep the running of the tests under 5 minutes, with any- thing over 8 minutes being a trigger to try something new.\n\nWe have resisted thus far the temptation to run only a subset of the tests and instead focused on ways to speed up running all the tests—although as you can see, we have begun removing the tests\n\nwww.it-ebooks.info\n\nShared Fixture\n\ndevelopers must run continuously (e.g., Master Data and GUI test suites are not required to check in, as they are run by Cruise Control and are areas that change infrequently).\n\nTwo of the most interesting solutions recently (aside from the in- memory framework) are the test server and the clustering frame- work.\n\nThe test server (named the “check-in” box here) is actually quite useful and has proven to be reliable and robust. We bought an Opteron box that is roughly twice as fast as the development boxes (really, the fastest box we could ﬁ nd). The server has an account set up for each development machine in the pit. Using the UNIX tool rsynch, the Eclipse workspace is synchronized with the user’s corresponding server account ﬁ le system. A series of shell scripts then recreates the database on the server for the remote account and runs all the development tests. When the tests have completed, a list of times to run each test is dumped to the console, along with a MyTestSuite.java class containing all the test failures, which the developer can use to run locally to ﬁ x any tests that have broken. The biggest advantage the remote server has provided is that it makes running a large number of tests feel fast again, because the developer can continue working while he or she waits for the results of the test server to come back.\n\nThe clustering framework (based on Condor) was quite fast but had the defect that it had to ship the entire workspace (11MB) to all the nodes on the network (×20), which had a signiﬁ cant cost, especially when a dozen pairs are using it. In comparison, the test server uses rsynch, which copies only the ﬁ les that are new or different in the developer’s workspace. The clustering framework also proved to be less reliable than the server solution, frequently not returning any status of the test run. There were also some tests that would not run reliably on the framework. Since it gave us roughly the same perfor- mance as the “check-in” test server, we have put this solution on the back burner.\n\nFurther Reading A more detailed description of the ﬁ rst experience can be found at http:// FasterTestsPaper.gerardmeszaros.com.\n\nwww.it-ebooks.info\n\n321\n\nShared Fixture\n\n322\n\nShared Fixture\n\nChapter 18 Test Strategy Patterns\n\nVariation: Incremental Tests\n\nWe may also use Shared Fixtures when we have a long, complex sequence of actions, each of which depends on the previous actions. In customer tests, this may show up as a work ﬂ ow; in unit tests, it may be a sequence of method calls on the same object. This case might be tested using a single Eager Test (see As- sertion Roulette on page 224). The alternative is to put each distinct action into a separate Test Method (page 348) that builds upon the actions of a previous test operating on a Shared Fixture. This approach, which is an example of Chained Tests (page 454), is how testers in the “testing” (i.e., QA) community often operate: They set up a ﬁ xture and then run a sequence of tests, each of which builds upon the ﬁ xture. The testers do have one signiﬁ cant advantage over our Fully Automated Tests (see page 26): When a test partway through the chain fails, they are available to make decisions about how to recover or whether it is worth proceeding at all. In contrast, our automated tests just keep running, and many of them will generate test failures or errors because they did not ﬁ nd the ﬁ xture as expected and, therefore, the SUT behaved (probably correctly) differ- ently. The resulting test results can obscure the real cause of the failure in a sea of red. With some experience it is often possible to recognize the failure pattern and deduce the root cause.10\n\nThis troubleshooting can be made simpler by starting each Test Method with one or more Guard Assertions (page 490) that document the assumptions the Test Method makes about the state of the ﬁ xture. When these assertions fail, they tell us to look elsewhere—either at tests that failed earlier in the test suite or at the order in which the tests were run.\n\nImplementation Notes\n\nA key implementation question with Shared Fixtures is, How do tests know about the objects in the Shared Fixture so they can (re)use them? Because the point of a Shared Fixture is to save execution time and effort by having multiple tests use the same instance of the test ﬁ xture, we’ll need to keep a reference to the ﬁ xture we create. That way, we can ﬁ nd the ﬁ xture if it already exists and we can inform other tests that it now exists once we have constructed it. We have more choices available to us with Per-Run Fixtures because we can “remember” the ﬁ xture we set up in code more easily than a Prebuilt Fixture (page 429) set up by a different program. Although we could just hard-code the identiﬁ ers (e.g., database keys) of the ﬁ xture objects into all our tests, that technique would result in a Fragile Fix- ture. To avoid this problem, we need to keep a reference to the ﬁ xture when we create it and we need to make it possible for all tests to access that reference.\n\n10 It may not be as simple as looking at the ﬁ rst test that failed.\n\nwww.it-ebooks.info\n\nShared Fixture\n\nVariation: Per-Run Fixture\n\nThe simplest form of Shared Fixture is the Per-Run Fixture, in which we set up the ﬁ xture at the beginning of a test run and allow it to be shared by the tests within the run. Ideally, the ﬁ xture won’t outlive the test run and we don’t have to worry about interactions between test runs such as Unrepeatable Tests (a cause of Erratic Tests). If the ﬁ xture is persistent, such as when it is stored in a database, we may need to do explicit ﬁ xture teardown.\n\nIf a Per-Run Fixture is shared only within a single Testcase Class (page 373), the simplest solution is to use a class variable for each ﬁ xture object we need to hold a reference to and then use either Lazy Setup (page 435) or Suite Fixture Setup (page 441) to initialize the objects just before we run the ﬁ rst test in the suite. If we want to share the test ﬁ xture between many Testcase Classes, we’ll need to use a Setup Decorator (page 447) to hold the setUp and tearDown methods and a Test Fixture Registry (see Test Helper on page 643) (which could just be the test database) to access the ﬁ xture.\n\nVariation: Immutable Shared Fixture\n\nThe problem with Shared Fixtures is that they lead to Erratic Tests if tests modify the Shared Fixture (page 317). Shared Fixtures violate the Independent Test prin- ciple (see page 42). We can avoid this problem by making the Shared Fixture immutable; that is, we partition the ﬁ xture needed by tests into two logical parts. The ﬁ rst part is the stuff every test needs to have present but is never modiﬁ ed by any tests—that is, the Immutable Shared Fixture. The second part is the objects that any test needs to modify or delete; these objects should be built by each test as Fresh Fixtures.\n\nThe most difﬁ cult part of applying an Immutable Shared Fixture is deciding what constitutes a change to an object. The key guideline is this: If any test per- ceives something done by another test as a change to an object in the Immutable Shared Fixture, then that change shouldn’t be allowed in any test with which it shares the ﬁ xture. Most commonly, the Immutable Shared Fixture consists of reference data that is needed by the actual per-test ﬁ xtures. The per-test ﬁ xtures can then be built as Fresh Fixtures on top of the Immutable Shared Fixture.\n\nMotivating Example\n\nThe following example shows a Testcase Class setting up the test ﬁ xture via Implicit Setup (page 424). Each Test Method uses an instance variable to access the contents of the ﬁ xture.\n\nwww.it-ebooks.info\n\n323\n\nShared Fixture\n\n324\n\nShared Fixture\n\nChapter 18 Test Strategy Patterns\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { setupStandardAirportsAndFlights(); FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { setupStandardAirportsAndFlights(); FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nNote that the setUp method is run once for each Test Method. If the ﬁ xture setup is fairly complex and involves accessing a database, this approach could result in Slow Tests.\n\nRefactoring Notes\n\nTo convert a Testcase Class from a Standard Fixture to a Shared Fixture, we simply convert the instance variables into class variables to make the ﬁ xture outlast the creating Testcase Object. We then need to initialize the class vari- ables just once to avoid recreating them for each Test Method; Lazy Setup is an easy way to accomplish this task. Of course, other ways to set up the Shared Fixture are also possible, such as Setup Decorator or Suite Fixture Setup.\n\nExample: Shared Fixture\n\nThis example shows the ﬁ xture converted to a Shared Fixture set up using Lazy Setup.\n\nprotected void setUp() throws Exception { if (sharedFixtureInitialized) { return;\n\nwww.it-ebooks.info\n\nShared Fixture\n\n} facade = new FlightMgmtFacadeImpl(); setupStandardAirportsAndFlights(); sharedFixtureInitialized = true; }\n\nprotected void tearDown() throws Exception { // We cannot delete any objects because we don't know // whether this is the last test }\n\nThe Lazy Initialization [SBPP] logic in the setUp method ensures that the Shared Fixture is created whenever the class variable is uninitialized. The Test Methods have also been modiﬁ ed to use a Finder Method to access the contents of the ﬁ xture:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nThe details of how the Test Utility Methods such as setupStandardAirportsAndFlights are implemented are not shown here, because they are not important for under- standing this example. It should be enough to understand that these methods create the airports and ﬂ ights and store references to them in static variables so that all Test Methods can access the same ﬁ xture either directly or via Test Utility Methods.\n\nwww.it-ebooks.info\n\n325\n\nShared Fixture\n\n326\n\nShared Fixture\n\nChapter 18 Test Strategy Patterns\n\nExample: Immutable Shared Fixture\n\nHere’s an example of Shared Fixture “pollution”:\n\npublic void testCancel_proposed_p()throws Exception { // shared ﬁxture BigDecimal proposedFlightId = ﬁndProposedFlight(); // exercise SUT facade.cancelFlight(proposedFlightId); // verify outcome try{ assertEquals(FlightState.CANCELLED, facade.ﬁndFlightById(proposedFlightId)); } ﬁnally { // teardown // try to undo the damage; hope this works! facade.overrideStatus( proposedFlightId, FlightState.PROPOSED); } }\n\nWe can avoid this problem by making the Shared Fixture immutable; that is, we partition the ﬁ xture needed by tests into two logical parts. The ﬁ rst part is the stuff every test needs to have present but is never modiﬁ ed by any tests—that is, the Immutable Shared Fixture. The second part is the objects that any test needs to modify or delete; these objects should be built by each test as Fresh Fixtures. Here’s the same test modiﬁ ed to use an Immutable Shared Fixture. We simply\n\ncreated our own mutableFlight within the test.\n\npublic void testCancel_proposed() throws Exception { // ﬁxture setup BigDecimal mutableFlightId = createFlightBetweenInsigiﬁcantAirports(); // exercise SUT facade.cancelFlight(mutableFlightId); // verify outcome assertEquals( FlightState.CANCELLED, facade.ﬁndFlightById(mutableFlightId)); // teardown // None required because we let the SUT create // new IDs for each ﬂight. We might need to clean out // the database eventually. }\n\nNote that we don’t need any ﬁ xture teardown logic in this version of the test because the SUT uses a Distinct Generated Value (see Generated Value on page 723)—that is, we do not supply a ﬂ ight number. We also use the predeﬁ ned dummyAirport1 and dummyAirport2 to avoid changing the number of ﬂ ights for airports used by other tests. Therefore, the mutable ﬂ ights can accumulate in the database trouble-free.\n\nwww.it-ebooks.info\n\nBack Door Manipulation\n\nBack Door Manipulation\n\nHow can we verify logic independently when we cannot use a round-trip test?\n\nWe set up the test ﬁ xture or verify the outcome by going through a back door (such as direct database access).\n\nSetup Setup\n\nExercise Exercise\n\nSUT SUT\n\nData Data Fixture Fixture\n\nVerify Verify\n\nTeardown Teardown\n\nEvery test requires a starting point (the test ﬁ xture) and an expected ﬁ nishing point (the expected results). The “normal” approach is to set up the ﬁ xture and verify the outcome by using the API of the SUT itself. In some circumstances this is either not possible or not desirable.\n\nIn some situations we can use Back Door Manipulation to set up the ﬁ xture\n\nand/or verify the SUT’s state.\n\nHow It Works\n\nThe state of the SUT comes in many ﬂ avors. It can be stored in memory, on disk as ﬁ les, in a database, or in other applications with which the SUT interacts. Whatever form it takes, the pre-conditions of a test typically require that the state of the SUT is not just known but is a speciﬁ c state. Likewise, at the end of the test we often want to do State Veriﬁ cation (page 462) of the SUT’s state.\n\nIf we have access to the state of the SUT from outside the SUT, the test can set up the pre-test state of the SUT by bypassing the normal API of the SUT and interacting directly with whatever is holding that state via a “back door.” When exercising of the SUT has been completed, the test can similarly access\n\nwww.it-ebooks.info\n\n327\n\nBack Door Manipulation\n\nAlso known as: Layer-Crossing Test\n\n328\n\nBack Door Manipulation\n\nChapter 18 Test Strategy Patterns\n\nthe post-test state of the SUT via a back door to compare it with expected outcome. For customer tests, the back door is most commonly a test database, but it could also be some other component on which the SUT depends, including a Registry [PEAA] object or even the ﬁ le system. For unit tests, the back door is some other class or object or an alternative interface of the SUT (or a Test-Speciﬁ c Subclass; page 579) that exposes the state in a way “normal” clients wouldn’t use. We can also replace a depended-on component (DOC) with a suitably conﬁ gured Test Double (page 522) instead of using the real thing if that makes the job easier.\n\nWhen to Use It\n\nWe might choose to use Back Door Manipulation for several reasons which we’ll examine in more detail shortly. A prerequisite for using this technique is that some sort of back door to the state of the system must exist. The main drawback of Back Door Manipulation is that our tests—or the Test Utility Methods (page 599) they call—become much more closely coupled to the design decisions we make about how to represent the state of the SUT. If we need to change those decisions, we may encounter Fragile Tests (page 239). We need to decide whether this price is acceptable on a case-by-case basis. We can greatly reduce the impact of the close coupling by encapsulating all Back Door Manipulation in Test Utility Methods.\n\nUsing Back Door Manipulation can also lead to Obscure Tests (page 186) by hiding the relationship of the test outcome to the test ﬁ xture. We can avoid this problem by including the test data being passed to the Back Door Manipulation mechanism within the Testcase Class (page 373), or at least mitigate it by using Finder Methods (see Test Utility Method) to refer to the objects in the ﬁ xture via intent-revealing names.\n\nA common application of Back Door Manipulation involves testing basic CRUD (Create, Read, Update, Delete) operations on the SUT’s state. In such a case, we want to verify that the information persisted and can be recovered in the same form. It is difﬁ cult to write round-trip tests for “Read” without also testing “Create”; likewise, it is difﬁ cult to test “Update” or “Delete” without testing both “Create” and “Read.” We can certainly test these operations by using round-trip tests, but this kind of testing won’t detect certain types of systemic problems, such as putting information into the wrong database column. One solution is to con- duct layer-crossing tests that use Back Door Manipulation to set up or verify the contents of the database directly. For a “Read” test, the test sets up the contents of the database using Back Door Setup and then asks the SUT to read the data. For a “Write” test, the test asks the system to write certain objects and then uses Back Door Veriﬁ cation on the contents of the database.\n\nwww.it-ebooks.info\n\nBack Door Manipulation\n\nVariation: Back Door Setup\n\nOne reason for doing Back Door Manipulation is to make tests run faster. If a system does a lot of processing before putting data into its data store, the time it takes for a test to set up the ﬁ xture via the SUT’s API could be quite signiﬁ cant. One way to make the tests run faster is to determine what those data stores should look like and then create a means to set them up via the back door rather than through the API. Unfortunately, this technique introduces its own problem: Because Back Door Setup bypasses enforcement of the object creation business rules, we may ﬁ nd ourselves creating ﬁ xtures that are not realistic and possibly even invalid. This problem may creep in over time as the business rules are modiﬁ ed in response to changing business needs. At the same time, this ap- proach may allow us to create test scenarios that the SUT will not let us set up through its API.\n\nWhen we share a database between our SUT and another application, we need to verify that we are using the database correctly and that we can handle all possible data conﬁ gurations the other applications might create. Back Door Setup is a good way to establish these conﬁ gurations—and it may be the only way if the SUT either doesn’t write those tables or writes only speciﬁ c (and valid) data conﬁ gurations. Back Door Setup lets us create those “impossible” conﬁ gurations easily so we can verify how the SUT behaves in these situations.\n\nVariation: Back Door Veriﬁ cation\n\nBack Door Veriﬁ cation involves sneaking in to do State Veriﬁ cation of the SUT’s post-exercise state via a back door; it is mostly applicable to customer tests (or functional tests, as they are sometimes called). The back door is typically an alternative way to examine the objects in the database, usually through a stan- dard API such as SQL or via data exports that can then be examined with a ﬁ le comparison utility program.\n\nAs mentioned earlier, Back Door Manipulation can make tests run faster. If the only way to get at the SUT’s state is to invoke an expensive operation (such as a complex report) or an operation that further modiﬁ es the SUT’s state, we may be better off using Back Door Manipulation.\n\nAnother reason for doing Back Door Manipulation is that other systems expect the SUT to store its state in a speciﬁ c way, which they can then access directly. This is a form of indirect output. In this situation, standard round-trip tests cannot prove that the SUT’s behavior is correct because they cannot detect a systematic problem if the “Write” and “Read” operations make the same mistake, such as putting information into the wrong database column. The solution is a layer-crossing test that looks at the contents of the database\n\nwww.it-ebooks.info\n\n329\n\nBack Door Manipulation\n\n330\n\nBack Door Manipulation\n\nChapter 18 Test Strategy Patterns\n\ndirectly to verify that the information is stored correctly. For a “Write” test, the test asks the system to write certain objects and then inspects the contents of the database via the back door.\n\nVariation: Back Door Teardown\n\nWe can also use Back Door Manipulation to tear down a Fresh Fixture (page 311) that is stored in a test database. This ability is especially beneﬁ cial if we can use bulk database commands to wipe clean whole tables, as in Table Truncation Teardown (page 661) or Transaction Rollback Teardown (page 668).\n\nImplementation Notes\n\nHow we implement Back Door Manipulation depends on where the ﬁ xture lives and how easily we can access the state of the SUT. It also depends on why we are doing Back Door Manipulation. This section lists the most common imple- mentations, but feel free to use your imagination and come up with other ways to use this pattern.\n\nVariation: Database Population Script\n\nWhen the SUT stores its state in a database that it accesses as it runs, the easiest way to do Back Door Manipulation is to load data directly into that database before invoking the SUT. This approach is most commonly required when we are writing customer tests, but it may also be required for unit tests if the classes we are testing interact directly with the database. We must ﬁ rst determine the pre-conditions of the test and, from that information, identify the data that the test requires for its ﬁ xture. We then deﬁ ne a database script that inserts the cor- responding records directly into the database bypassing the SUT logic. We use this Database Population Script whenever we want to set up the test ﬁ xture—a decision that depends on which test ﬁ xture strategy we have chosen. (See Chap- ter 6, Test Automation Strategy, for more on that topic.)\n\nWhen deciding to use a Database Population Script, we will need to maintain both the Database Population Script and the ﬁ les it takes as input whenever we modify either the structure of the SUT’s data stores or the semantics of the data in them. This requirement can increase the maintenance cost of the tests.\n\nVariation: Data Loader\n\nA Data Loader is a special program that loads data into the SUT’s data store. It differs from a Database Population Script in that the Data Loader is written in a programming language rather than a database language. This gives us a bit\n\nwww.it-ebooks.info\n\nBack Door Manipulation\n\nmore ﬂ exibility and allows us to use the Data Loader even when the system state is stored somewhere other than a relational database.\n\nIf the data store is external to the SUT, such as in a relational database, the Data Loader can be “just another application” that writes to that data store. It would use the database in much the same way as the SUT but would get its inputs from a ﬁ le rather than from wherever the SUT normally gets its inputs (e.g., other “upstream” programs). When we are using an object relational mapping (ORM) tool to access the database from our SUT, a simple way to build the Data Loader is to use the same domain objects and mappings in our Data Loader. We just create the desired objects in memory and commit the ORM’s unit of work to save them into the database.\n\nIf the SUT stores data in internal data structures (e.g., in memory), the Data Loader may need to be an interface provided by the SUT itself. The following characteristics differentiate it from the normal functionality provided by the SUT:\n\nIt is used only by the tests.\n\nIt reads the data from a ﬁ le rather than wherever the SUT normally gets\n\nthe data.\n\nIt bypasses a lot of the “edit checks” (input validation) normally done\n\nby the SUT.\n\nThe input ﬁ les may be simple ﬂ at ﬁ les containing comma- or tab-delimited text, or they could be structured using XML. DbUnit is an extension of JUnit that implements Data Loader for ﬁ xture setup.\n\nVariation: Database Extraction Script\n\nWhen the SUT stores its state in a database that it accesses as it runs, we can take advantage of this structure to do Back Door Veriﬁ cation. We simply use a database script to extract data from the test database and verify that it contains the right data either by comparing it to previously prepared “extract” ﬁ les or by ensuring that speciﬁ c queries return the right number of records.\n\nVariation: Data Retriever\n\nA Data Retriever is the analog of a Data Loader that retrieves the state from the SUT when doing Back Door Veriﬁ cation. Like a trusty dog, it “fetches” the data so that we can compare it with our expected results within our tests. DbUnit is an extension of JUnit that implements Data Retriever to support result veriﬁ cation.\n\nwww.it-ebooks.info\n\n331\n\nBack Door Manipulation\n\n332\n\nBack Door Manipulation\n\nChapter 18 Test Strategy Patterns\n\nVariation: Test Double as Back Door\n\nSo far, all of the implementation techniques described here have involved inter- acting with a DOC of the SUT to set up or tear down the ﬁ xture or to verify the expected outcome. Probably the most common form of Back Door Manipula- tion involves replacing the DOC with a Test Double. One option is to use a Fake Object (page 551) that we have preloaded with some data as though the SUT had already been interacting with it; this strategy allows us to avoid using the SUT to set up the SUT’s state. The other option is to use some kind of Con- ﬁ gurable Test Double (page 558), such as a Mock Object (page 544) or a Test Stub (page 529). Either way, we can completely avoid Obscure Tests by making the state of the Test Double visible within the Test Method (page 348).\n\nWhen we want to perform Behavior Veriﬁ cation (page 468) of the calls made by the SUT to one or more DOCs, we can use a layer-crossing test that replaces the DOC with a Test Spy (page 538) or a Mock Object. When we want to verify that the SUT behaves a speciﬁ c way when it receives indirect inputs from a DOC (or when in some speciﬁ c external state), we can replace the DOC with a Test Stub.\n\nMotivating Example\n\nThe following round-trip test veriﬁ es the basic functionality of removing a ﬂ ight by interacting with the SUT only via the front door. But it does not verify the indirect outputs of the SUT—namely, that the SUT is expected to call a logger to log each time a ﬂ ight is removed along with the day/time when the request was made and the user ID of the requester. In many systems, this would be an ex- ample of “layer-crossing behavior”: The logger is part of a generic infrastructure layer, while the SUT is an application-speciﬁ c behavior.\n\npublic void testRemoveFlight() throws Exception { // setup FlightDto expectedFlightDto = createARegisteredFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight should not exist after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); }\n\nwww.it-ebooks.info\n\nBack Door Manipulation\n\nRefactoring Notes\n\nWe can convert this test to use Back Door Veriﬁ cation by adding result veriﬁ cation code to access and verify the logger’s state. We can do so either by reading that state from the logger’s database or by replacing the logger with a Test Spy that saves the state for easy access by the tests.\n\nExample: Back Door Result Veriﬁ cation Using a Test Spy\n\nHere’s the same test converted to use a Test Spy to access the post-test state of the logger:\n\npublic void testRemoveFlightLogging_recordingTestStub() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // Test Double setup AuditLogSpy logSpy = new AuditLogSpy(); facade.setAuditLog(logSpy); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertEquals(\"number of calls\", 1, logSpy.getNumberOfCalls()); assertEquals(\"action code\", Helper.REMOVE_FLIGHT_ACTION_CODE, logSpy.getActionCode()); assertEquals(\"date\", helper.getTodaysDateWithoutTime(), logSpy.getDate()); assertEquals(\"user\", Helper.TEST_USER_NAME, logSpy.getUser()); assertEquals(\"detail\", expectedFlightDto.getFlightNumber(), logSpy.getDetail()); }\n\nThis approach would be the better way to verify the logging if the logger’s data- base contained so many entries that it wasn’t practical to verify the new entries using Delta Assertions (page 485).\n\nExample: Back Door Fixture Setup\n\nThe next example shows how we can set up a ﬁ xture using the database as a back door to the SUT. The test inserts a record into the EmailSubscription table and then asks the SUT to ﬁ nd it. It then makes assertions on various ﬁ elds of the object returned by the SUT to verify that the record was read correctly.\n\nwww.it-ebooks.info\n\n333\n\nBack Door Manipulation\n\n334\n\nBack Door Manipulation\n\nChapter 18 Test Strategy Patterns\n\nstatic ﬁnal String TABLE_NAME = \"EmailSubscription\"; static ﬁnal BigDecimal RECORD_ID = new BigDecimal(\"111\");\n\nstatic ﬁnal String LOGIN_ID = \"Bob\"; static ﬁnal String EMAIL_ID = \"bob@foo.com\";\n\npublic void setUp() throws Exception { String xmlString = \"<?xml version='1.0' encoding='UTF-8'?>\" + \"<dataset>\" + \" <\" + TABLE_NAME + \" EmailSubscriptionId='\" + RECORD_ID + \"'\" + \" UserLoginId='\" + LOGIN_ID + \"'\" + \" EmailAddress='\" + EMAIL_ID + \"'\" + \" RecordVersionNum='62' \" + \" CreateByUserId='MappingTest' \" + \" CreateDateTime='2004-03-01 00:00:00.0' \" + \" LastModByUserId='MappingTest' \" + \" LastModDateTime='2004-03-01 00:00:00.0'/>\" + \"</dataset>\"; insertRowsIntoDatabase(xmlString); }\n\npublic void testRead_Login() throws Exception { // exercise EmailSubscription subs = EmailSubscription.ﬁndInstanceWithId(RECORD_ID); // verify assertNotNull(\"Email Subscription\", subs); assertEquals(\"User Name\", LOGIN_ID, subs.getUserName()); }\n\npublic void testRead_Email() throws Exception { // exercise EmailSubscription subs = EmailSubscription.ﬁndInstanceWithId(RECORD_ID); // verify assertNotNull(\"Email Subscription\", subs); assertEquals(\"Email Address\", EMAIL_ID, subs.getEmailAddress()); }\n\nThe XML document used to populate the database is built within the Testcase Class so as to avoid the Mystery Guest (see Obscure Test) that would have been created if we had used an external ﬁ le for loading the database [the discussion of the In-line Resource (page 736) refactoring explains this approach]. To make the test clearer, we call intent-revealing methods that hide the details of how we use DbUnit to load the database and clean it out at the end of the test using Table Truncation Teardown. Here are the bodies of the Test Utility Methods used in this example:\n\nwww.it-ebooks.info\n\nBack Door Manipulation\n\nprivate void insertRowsIntoDatabase(String xmlString) throws Exception { IDataSet dataSet = new FlatXmlDataSet(new StringReader(xmlString)); DatabaseOperation.CLEAN_INSERT. execute( getDbConnection(), dataSet); }\n\npublic void tearDown() throws Exception{ emptyTable(TABLE_NAME); }\n\npublic void emptyTable(String tableName) throws Exception { IDataSet dataSet = new DefaultDataSet(new DefaultTable(tableName)); DatabaseOperation.DELETE_ALL. execute(getDbConnection(), dataSet); }\n\nOf course, the implementations of these methods are speciﬁ c to DbUnit; we must change them if we use some other member of the xUnit family.\n\nSome other observations on these tests: To avoid an Eager Test (see Assertion Roulette on page 224), the assertion on each ﬁ eld appears in a separate test. This structure could result in Slow Tests (page 253) because these tests interact with a database. We could use Lazy Setup (page 435) or Suite Fixture Setup (page 441) to avoid setting up the ﬁ xture more than once as long as the resulting Shared Fixture (page 317) was not modiﬁ ed by any of the tests. (I chose not to further complicate this example by taking this tack.)\n\nFurther Reading\n\nSee the sidebar “Database as SUT API?” on page 336 for an example of when the back door is really a front door.\n\nwww.it-ebooks.info\n\n335\n\nBack Door Manipulation\n\n336\n\nBack Door Manipulation\n\nChapter 18 Test Strategy Patterns\n\nDatabase as SUT API?\n\nA common technique for setting up test ﬁ xtures is Back Door Setup (see Back Door Manipulation on page 327); for verifying test outcomes, Back Door Veriﬁ cation (see Back Door Manipulation) is a popular option. But when is a test that interacts directly with the database behind a SUT not considered to be going through the back door?\n\nOn a recent project, some friends were struggling with this very question, though at ﬁ rst they didn’t realize it. One of their analysts (who was also a power user) seemed overly focused on the database schema. At ﬁ rst, they put this narrow focus down to the analyst’s Powerbuilder background and tried to break him of the habit. That didn’t work. The analyst just dug in his heels. The developers tried explaining that on agile projects it was important not to try to deﬁ ne the whole data schema at the begin- ning of the project; instead, the schema evolved as the requirements were implemented.\n\nOf course, the analyst complained every time they modiﬁ ed the database schema because the changes broke all his queries. As the project unfold- ed, the other team members slowly started to understand that the analyst really did need a stable database against which to run queries. It was his way to verify the correctness of the data generated by the system.\n\nOnce they recognized this requirement, the developers were able to treat the query schema as a formal interface provided by the system. Customer tests were written against this interface and developers had to ensure that those tests still passed whenever they changed the database. To minimize the impact of database refactorings, they deﬁ ned a set of query views that implemented this interface. This approach allowed them to refactor the database as needed.\n\nWhen might you ﬁ nd yourself in this situation? Any time your customer applies reporting tools (such as Crystal Reports) to your database, an argument can be made as to whether part of the requirements is a stable reporting interface. Similarly, if the customer uses scripts (such as DTS or SQL) to load data into the database, there may be a requirement for a stable data loading interface.\n\nwww.it-ebooks.info\n\nLayer Test\n\nLayer Test\n\nHow can we verify logic independently when it is part of a layered architecture?\n\nWe write separate tests for each layer of the layered architecture.\n\nLayer1TestcaseClass Layer1TestcaseClass testMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nLayernTestcaseClass LayernTestcaseClass testMethod_1 testMethod_1 testMethod_2 testMethod_2\n\nLayer n Layer n\n\nLayer 1 Layer 1\n\nTest Double Test Double\n\nTest Double Test Double\n\nDOC DOC\n\nIt is difﬁ cult to obtain good test coverage when testing an entire application in a top-to-bottom fashion; we are bound to end up doing Indirect Testing (see Obscure Test on page 186) on some parts of the application. Many applications use a Layered Architecture [DDD, PEAA, WWW] to separate the major technical concerns. Most applications have some kind of presentation (user interface) lay- er, a business logic layer or domain layer, and a persistence layer. Some layered architectures have even more layers.\n\nAn application with a layered architecture can be tested more effectively by\n\ntesting each layer in isolation.\n\nHow It Works\n\nWe design the SUT using a layered architecture that separates the presentation logic from the business logic and from any persistence mechanism or interfaces\n\nwww.it-ebooks.info\n\n337\n\nLayer Test\n\nAlso known as: Single Layer Test, Testing by Layers, Layered Test\n\n338\n\nLayer Test\n\nChapter 18 Test Strategy Patterns\n\nto other systems.11 We put all business logic into a Service Layer [PEAA] that exposes the application functionality to the presentation layer as an API. We treat each layer of the architecture as a separate SUT. We write component tests for each layer independent of the other layers of the architecture. That is, for layer n of the architecture, the tests will take the place of layer n+1; we may op- tionally replace layer n-1 with a Test Double (page 522).\n\nWhen to Use It\n\nWe can use a Layer Test whenever we have a layered architecture and we want to provide good test coverage of the logic in each layer. It can be much simpler to test each layer independently than it is to test all the layers at once. This is especially true when we want to do defensive coding for return values of calls across the layer boundary. In software that is working correctly, these errors “should never happen”; in real life, they do. To make sure our code handles these errors, we can inject these “never happen” scenarios as indirect inputs to our layer.\n\nLayer Tests are very useful when we want to divide up the project team into subteams based on the technology in which the team members specialize. Each layer of an architecture tends to require different knowledge and often uses different technologies; therefore, the layer boundaries serve as natural team boundaries. Layer Tests can be a good way to nail down and document the semantics of the layer interfaces.\n\nEven when we choose to use a Layer Test strategy, it is a good idea to include a few “top-to-bottom” tests just to verify that the various layers are integrated correctly. These tests need to cover only one or two basic scenarios; we don’t need to test every business test condition because all of them have already been tested in the Layer Tests for at least one of the layers.\n\nMost of the variations on this pattern reﬂ ect which layer is being tested inde-\n\npendently of the other layers.\n\nVariation: Presentation Layer Test\n\nOne could write a whole book just on patterns of presentation layer testing. The speciﬁ c patterns depend on the nature of the presentation layer technology (e.g., graphical user interface, traditional Web interface, “smart” Web interface, Web services). Regardless of the technology, the key is to test the presentation logic separately from the business logic so that we don’t have to worry about changes\n\n11 Not all presentation logic relates to the user interface; this logic can also appear in a messaging interface used by another application.\n\nwww.it-ebooks.info\n\nLayer Test\n\nin the underlying logic affecting our presentation layer tests. (They are hard enough to automate well as it is!)\n\nAnother consideration is to design the presentation layer so that its logic can be tested independently of the presentation framework. Humble Dialog (see Humble Object on page 695) is the key design-for-testability pattern to apply here. In effect, we are deﬁ ning sublayers within the presentation layer; the layer containing the Humble Dialogs is the “presentation graphic layer” and the layer we have made testable is the “presentation behavior layer.” This separation of layers allows us to verify that buttons are activated, menu items are grayed out, and so on, without instantiating any of the real graphical objects.\n\nVariation: Service Layer Test\n\nThe Service Layer is where most of our unit tests and component tests are traditionally concentrated. Testing the business logic using customer tests is a bit more challenging because testing the Service Layer via the presentation layer often involves Indirect Testing and Sensitive Equality (see Fragile Test on page 239), either of which can lead to Fragile Tests and High Test Maintenance Cost (page 265). Testing the Service Layer directly helps avoid these problems. To avoid Slow Tests (page 253), we usually replace the persistence layer with a Fake Database (see Fake Object on page 551) and then run the tests. In fact, most of the impetus behind a layered architecture is to isolate this code from the other, harder-to-test layers. Alistair Cockburn puts an interesting spin on this idea in his description of a Hexagonal Architecture at http://alistair.cockburn.us [WWW].\n\nThe Service Layer may come in handy for other uses. It can be used to run the application in “headless” mode (without a presentation layer attached), such as when using macros to automate frequently done tasks in Microsoft Excel.\n\nVariation: Persistence Layer Test\n\nThe persistence layer also needs to be tested. Round-trip tests will often sufﬁ ce if the application is the only one that uses the data store. But these tests won’t catch one kind of programming error: when we accidentally put information into the wrong columns. As long as the data type of the interchanged columns is compatible and we make the same error when reading the data, our round-trip tests will pass! This kind of bug won’t affect the operation of our application but it might make support more difﬁ cult and it will cause problems in interactions with other applications.\n\nWhen other applications also use the data store, it is highly advisable to imple- ment at least a few layer-crossing tests that verify information is put into the\n\nwww.it-ebooks.info\n\n339\n\nLayer Test\n\n340\n\nLayer Test\n\nChapter 18 Test Strategy Patterns\n\ncorrect columns of tables. We can use Back Door Manipulation (page 327) to either set up the database contents or to verify the post-test database contents.\n\nVariation: Subcutaneous Test\n\nA Subcutaneous Test is a degenerate form of Layer Test that bypasses the pre- sentation layer of the system to interact directly with the Service Layer. In most cases, the Service Layer is not isolated from the layer(s) below; therefore, we test everything except the presentation. Use of a Subcutaneous Test does not require as strict a separation of concerns as does a Service Layer Test, which makes Subcutaneous Test easier to use when we are retroﬁ tting tests onto an application that wasn’t designed for testability. We should use a Subcutaneous Test whenever we are writing customer tests for an application and we want to ensure our tests are robust. A Subcutaneous Test is much less likely to be broken by changes to the application12 because it does not interact with the application via the presentation layer; as a consequence, a whole category of changes won’t affect it.\n\nVariation: Component Test\n\nA Component Test is the most general form of Layer Test, in that we can think of the layers being made up of individual components that act as “micro-layers.” Component Tests are a good way to specify or document the behavior of indi- vidual components when we are doing component-based development and some of the components must be modiﬁ ed or built from scratch.\n\nImplementation Notes\n\nWe can write our Layer Tests as either round-trip tests or layer-crossing tests. Each has advantages. In practice, we typically mix both styles of tests. The round-trip tests are easier to write (assuming we already have a suitable Fake Object available to use for layer n-1). We need to use layer-crossing tests, how- ever, when we are verifying the error-handling logic in layer n.\n\nRound-Trip Tests\n\nA good starting point for Layer Tests is the round-trip test, as it should be sufﬁ cient for most Simple Success Tests (see Test Method on page 348). These tests can be written such that they do not care whether we have fully isolated the layer of interest from the layers below. We can either leave the real com- ponents in place so that they are exercised indirectly, or we can replace them\n\n12 Less likely than a test that exercises the logic via the presentation layer, that is.\n\nwww.it-ebooks.info\n\nLayer Test\n\nwith Fake Objects. The latter option is particularly useful when by a database or asynchronous mechanisms in the layer below lead to Slow Tests.\n\nControlling Indirect Inputs\n\nWe can replace a lower layer of the system with a Test Stub (page 529) that returns “canned” results based on what the client layer passes in a request (e.g., Customer 0001 is a valid customer, 0002 is a dormant customer, 0003 has three accounts). This technique allows us to test the client logic with well-understood indirect inputs from the layer below. It is particularly useful when we are auto- mating Expected Exception Tests (see Test Method) or when we are exercising behavior that depends on data that arrives from an upstream system.13 The alternative is to use Back Door Manipulation to set up the indirect inputs.\n\nVerifying Indirect Outputs\n\nWhen we want to verify the indirect outputs of the layer of interest, we can use a Mock Object (page 544) or Test Spy (page 538) to replace the components in the layer below the SUT. We can then compare the actual calls made to the DOC with the expected calls. The alternative is to use Back Door Manipulation to verify the indirect outputs of the SUT after they have occurred.\n\nMotivating Example\n\nWhen trying to test all layers of the application at the same time, we must verify the correctness of the business logic through the presentation layer. The following test is a very simple example of testing some trivial business logic through a trivial user interface:\n\nprivate ﬁnal int LEGAL_CONN_MINS_SAME = 30; public void testAnalyze_sameAirline_LessThanConnectionLimit() throws Exception { // setup FlightConnection illegalConn = createSameAirlineConn( LEGAL_CONN_MINS_SAME - 1); // exercise FlightConnectionAnalyzerImpl sut = new FlightConnectionAnalyzerImpl(); String actualHtml = sut.getFlightConnectionAsHtmlFragment( illegalConn.getInboundFlightNumber(), illegalConn.getOutboundFlightNumber());\n\n13 Typically this data goes directly into a shared database or is injected via a “data pump.”\n\nwww.it-ebooks.info\n\n341\n\nLayer Test\n\n342\n\nLayer Test\n\nChapter 18 Test Strategy Patterns\n\n// veriﬁcation StringBuffer expected = new StringBuffer(); expected.append(\"<span class=\"boldRedText\">\"); expected.append(\"Connection time between ﬂight \"); expected.append(illegalConn.getInboundFlightNumber()); expected.append(\" and ﬂight \"); expected.append(illegalConn.getOutboundFlightNumber()); expected.append(\" is \"); expected.append(illegalConn.getActualConnectionTime()); expected.append(\" minutes.</span>\"); assertEquals(\"html\", expected.toString(), actualHtml); }\n\nThis test contains knowledge about the business layer functionality (what makes a connection illegal) and presentation layer functionality (how an illegal connec- tion is presented). It also depends on the database because the FlightConnections are retrieved from another component. If any of these areas change, this test must be revisited as well.\n\nRefactoring Notes\n\nWe can split this test into two separate tests: one to test the business logic (What constitutes an illegal connection?) and one to test the presentation layer (Given an illegal connection, how should it be displayed to the user?). We would typically do so by duplicating the entire Testcase Class (page 373), stripping out the presentation layer logic veriﬁ cation from the business layer Test Methods, and stubbing out the business layer object(s) in the presentation layer Test Methods.\n\nAlong the way, we will probably ﬁ nd that we can reduce the number of tests in at least one of the Testcase Classes because few test conditions exist for that layer. In this example, we started out with four tests (the combinations of same/ different airlines and time periods), each of which tested both the business and presentation layers; we ended up with four tests in the business layer (the origi- nal combinations but tested directly) and two tests in the presentation layer (formatting of legal and illegal connections).14 Therefore, only the latter two tests need to be concerned with the details of the string formatting and, when a test fails, we know which layer holds the bug.\n\nWe can take our refactoring even further by using a Replace Dependency with Test Double (page 739) refactoring to turn this Subcutaneous Test into a true Service Layer Test.\n\n14 I’m glossing over the various error-handling tests to simplify this discussion, but note that a Layer Test also makes it easier to exercise the error-handling logic.\n\nwww.it-ebooks.info\n\nLayer Test\n\nExample: Presentation Layer Test\n\nThe following example shows the earlier test refactored to verify the behavior of the presentation layer when an illegal connection is requested. It stubs out the FlightConnAnalyzer and conﬁ gures it with the illegal connection to return to the HtmlFacade when it is called. This technique gives us complete control over the indirect input of the SUT.\n\npublic void testGetFlightConnAsHtml_illegalConnection() throws Exception { // setup FlightConnection illegalConn = createIllegalConnection(); Mock analyzerStub = mock(IFlightConnAnalyzer.class); analyzerStub.expects(once()).method(\"analyze\") .will(returnValue(illegalConn)); HTMLFacade htmlFacade = new HTMLFacade( (IFlightConnAnalyzer)analyzerStub.proxy()); // exercise String actualHtmlString = htmlFacade.getFlightConnectionAsHtmlFragment( illegalConn.getInboundFlightNumber(), illegalConn.getOutboundFlightNumber()); // verify StringBuffer expected = new StringBuffer(); expected.append(\"<span class=\"boldRedText\">\"); expected.append(\"Connection time between ﬂight \"); expected.append(illegalConn.getInboundFlightNumber()); expected.append(\" and ﬂight \"); expected.append(illegalConn.getOutboundFlightNumber()); expected.append(\" is \"); expected.append(illegalConn.getActualConnectionTime()); expected.append(\" minutes.</span>\"); assertEquals(\"returned HTML\", expected.toString(), actualHtmlString); }\n\nWe must compare the string representations of the HTML to determine whether the code has generated the correct response. Fortunately, we need only two such tests to verify the basic behavior of this component.\n\nExample: Subcutaneous Test\n\nHere’s the original test converted into a Subcutaneous Test that bypasses the presentation layer to verify that the connection information is calculated cor- rectly. Note the lack of any string manipulation in this test.\n\nwww.it-ebooks.info\n\n343\n\nLayer Test\n\n344\n\nLayer Test\n\nChapter 18 Test Strategy Patterns\n\nprivate ﬁnal int LEGAL_CONN_MINS_SAME = 30; public void testAnalyze_sameAirline_LessThanConnectionLimit() throws Exception { // setup FlightConnection expectedConnection = createSameAirlineConn( LEGAL_CONN_MINS_SAME -1); // exercise IFlightConnAnalyzer theConnectionAnalyzer = new FlightConnAnalyzer(); FlightConnection actualConnection = theConnectionAnalyzer.getConn( expectedConnection.getInboundFlightNumber(), expectedConnection.getOutboundFlightNumber()); // veriﬁcation assertNotNull(\"actual connection\", actualConnection); assertFalse(\"IsLegal\", actualConnection.isLegal()); }\n\nWhile we have bypassed the presentation layer, we have not attempted to isolate the Service Layer from the layers below. This omission could result in Slow Tests or Erratic Tests (page 228).\n\nExample: Business Layer Test\n\nThe next example shows the same test converted into a Service Layer Test that is fully isolated from the layers below it. We have used JMock to replace these components with Mock Objects that verify the correct ﬂ ights are being looked up and that inject the corresponding ﬂ ight constructed into the SUT.\n\npublic void testAnalyze_sameAirline_EqualsConnectionLimit() throws Exception { // setup Mock ﬂightMgntStub = mock(FlightManagementFacade.class); Flight ﬁrstFlight = createFlight(); Flight secondFlight = createConnectingFlight( ﬁrstFlight, LEGAL_CONN_MINS_SAME); ﬂightMgntStub.expects(once()).method(\"getFlight\") .with(eq(ﬁrstFlight.getFlightNumber())) .will(returnValue(ﬁrstFlight)); ﬂightMgntStub.expects(once()).method(\"getFlight\") .with(eq(secondFlight.getFlightNumber())) .will(returnValue(secondFlight)); // exercise FlightConnAnalyzer theConnectionAnalyzer = new FlightConnAnalyzer(); theConnectionAnalyzer.facade = (FlightManagementFacade)ﬂightMgntStub.proxy(); FlightConnection actualConnection = theConnectionAnalyzer.getConn( ﬁrstFlight.getFlightNumber(), secondFlight.getFlightNumber());\n\nwww.it-ebooks.info\n\nLayer Test\n\n// veriﬁcation assertNotNull(\"actual connection\", actualConnection); assertTrue(\"IsLegal\", actualConnection.isLegal()); }\n\nThis test runs very quickly because the Service Layer is fully isolated from any underlying layers. It is also likely to be much more robust because it tests much less code.\n\nwww.it-ebooks.info\n\n345\n\nLayer Test\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nChapter 19\n\nxUnit Basics Patterns\n\nPatterns in This Chapter\n\nTest Deﬁ nition\n\nTest Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\n\nFour-Phase Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358\n\nAssertion Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\n\nAssertion Message . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\n\nTestcase Class. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\n\nTest Execution\n\nTest Runner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\n\nTestcase Object. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n\nTest Suite Object. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387\n\nTest Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n\nTest Enumeration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399\n\nTest Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403\n\n347\n\nwww.it-ebooks.info\n\nxUnit Basics Patterns",
      "page_number": 341
    },
    {
      "number": 19,
      "title": "xUnit Basics Patterns",
      "start_page": 411,
      "end_page": 470,
      "detection_method": "regex_chapter",
      "content": "348\n\nTest Method\n\nChapter 19 xUnit Basics Patterns\n\nTest Method\n\nWhere do we put our test code?\n\nWe encode each test as a single Test Method on some class.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\nCreate Create\n\ntestMethod_n testMethod_n\n\nTestcase Testcase Class Class\n\nFully Automated Tests (see page 26) consist of test logic. That logic has to live somewhere before we can compile and execute it.\n\nHow It Works\n\nWe deﬁ ne each test as a method, procedure, or function that implements the four phases (see Four-Phase Test on page 358) necessary to realize a Fully Automated Test. Most notably, the Test Method must include assertions if it is to be a Self- Checking Test (see page 26).\n\nWe organize the test logic following one of the standard Test Method templates to make the type of test easily recognizable by test readers. In a Simple Success Test, we have a purely linear ﬂ ow of control from ﬁ xture setup through exercis- ing the SUT to result veriﬁ cation. In an Expected Exception Test, language-based structures direct us to error-handling code. If we reach that code, we pass the test; if we don’t, we fail it. In a Constructor Test, we simply instantiate an object and make assertions against its attributes.\n\nwww.it-ebooks.info\n\nTest Method\n\nWhy We Do This\n\nWe have to encode the test logic somewhere. In the procedural world, we would encode each test as a test case procedure located in a ﬁ le or module. In object - oriented programming languages, the preferred option is to encode them as methods on a suitable Testcase Class (page 373) and then to turn these Test Methods into Testcase Objects (page 382) at runtime using either Test Discovery (page 393) or Test Enumeration (page 399).\n\nWe follow the standard test templates to keep our Test Methods as simple as possible. This greatly increases their utility as system documentation (see page 23) by making it easier to ﬁ nd the description of the basic behavior of the SUT. It is a lot easier to recognize which tests describe this basic behavior if only Expected Exception Tests contain error-handling language constructs such as try/catch.\n\nImplementation Notes\n\nWe still need a way to run all the Test Methods tests on the Testcase Class. One solution is to deﬁ ne a static method on the Testcase Class that calls each of the test methods. Of course, we would also have to deal with counting the tests and determining how many passed and how many failed. Because this functionality is needed for a test suite anyway, a simple solution is to instantiate a Test Suite Object (page 387) to hold each Test Method.1 This approach is easy to imple- ment if we create an instance of the Testcase Class for each Test Method using either Test Discovery or Test Enumeration.\n\nIn statically typed languages such as Java and C#, we may have to include a throws clause as part of the Test Method declaration so the compiler won’t complain about the fact that we are not handling the checked exceptions that the SUT has declared it may throw. In effect, we tell the compiler that “The Test Runner (page 377) will deal with the exceptions.”\n\nOf course, different kinds of functionality need different kinds of Test Methods. Nevertheless, almost all tests can be boiled down to one of three basic types.\n\nVariation: Simple Success Test\n\nMost software has an obvious success scenario (or “happy path”). A Simple Success Test veriﬁ es the success scenario in a simple and easily recognized way.\n\n1 See the sidebar “There’s Always an Exception” (page 384) for an explanation of when this isn’t the case.\n\nwww.it-ebooks.info\n\n349\n\nTest Method\n\n350\n\nTest Method\n\nChapter 19 xUnit Basics Patterns\n\nWe create an instance of the SUT and call the method(s) that we want to test. We then assert that the expected outcome has occurred. In other words, we follow the normal steps of a Four-Phase Test. What we don’t do is catch any exceptions that could happen. Instead, we let the Test Automation Framework (page 298) catch and report them. Doing otherwise would result in Obscure Tests (page 186) and would mislead the test reader by making it appear as if exceptions were expected. See Tests as Documentation for the rationale behind this approach. Another beneﬁ t of avoiding try/catch-style code is that when errors do occur, it is a lot easier to track them down because the Test Automation Framework reports the location where the actual error occurred deep in the SUT rather than the place in our test where we called an Assertion Method (page 362) such as fail or assertTrue. These kinds of errors turn out to be much easier to trouble- shoot than assertion failures.\n\nVariation: Expected Exception Test\n\nWriting software that passes the Simple Success Test is pretty straightforward. Most of the defects in software appear in the various alternative paths—especially the ones that relate to error scenarios, because these scenarios are often Untested Requirements (see Production Bugs on page 268) or Untested Code (see Produc- tion Bugs). An Expected Exception Test helps us verify that the error scenarios have been coded correctly. We set up the test ﬁ xture and exercise the SUT in each way that should result in an error. We ensure that the expected error has occurred by using whatever language construct we have available to catch the error. If the error is raised, ﬂ ow will pass to the error-handling block. This diver- sion may be enough to let the test pass, but if the type or message contents of the exception or error is important (such as when the error message will be shown to a user), we can use an Equality Assertion (see Assertion Method) to verify it. If the error is not raised, we call fail to report that the SUT failed to raise an error as expected.\n\nWe should write an Expected Exception Test for each kind of exception that the SUT is expected to raise. It may raise the error because the client (i.e., our test) has asked it to do something invalid, or it may translate or pass through an error raised by some other component it uses. We should not write an Expected Exception Test for exceptions that the SUT might raise but that we cannot force to occur on cue, because these kinds of errors should show up as test failures in the Simple Success Tests. If we want to verify that these kinds of errors are handled properly, we must ﬁ nd a way to force them to occur. The most common way to do so is to use a Test Stub (page 529) to control the indirect input of the SUT and raise the appropriate errors in the Test Stub.\n\nwww.it-ebooks.info\n\nTest Method\n\nException tests are very interesting to write about because of the different ways the xUnit frameworks express them. JUnit 3.x provides a special Expected- Exception class to inherit from. This class forces us to create a Testcase Class for each Test Method (page 348), however, so it really doesn’t save any effort over coding a try/catch block and does result in a large number of very small Testcase Classes. Later versions of JUnit and NUnit (for .NET) provide a special ExpectedException method attribute (called an annotation in Java) to tell the Test Automation Framework to fail the test if that exception isn’t raised. This method attribute allows us to include message text if we want to specify exactly which text to expect in addition to the type of the exception.\n\nLanguages that support blocks, such as Smalltalk and Ruby, can provide special assertions to which we pass the block of code to be executed as well as the expected exception/error object. The Assertion Method implements the error-handling logic required to determine whether the error has, in fact, occurred. This makes our Test Methods much simpler, even though we may need to examine the names of the assertions more closely to see which type of test we have.\n\nVariation: Constructor Test\n\nWe would have a lot of Test Code Duplication (page 213) if every test we wrote had to verify that the objects it creates in its ﬁ xture setup phase are correctly instantiated. We avoid this step by testing the constructor(s) separately from other Test Methods whenever the constructor contains anything more com- plex than a simple ﬁ eld assignment from the constructor parameters. These Constructor Tests provide better Defect Localization (see page 22) than includ- ing constructor logic veriﬁ cation in other tests. We may need to write one or more tests for each constructor signature. Most Constructor Tests will follow a Simple Success Test template; however, we can use an Expected Exception Test to verify that the constructor correctly reports invalid arguments by raising an exception.\n\nWe should verify each attribute of the object or data structure regardless of whether we expect it to be initialized. For attributes that should be initialized, we can use an Equality Assertion to specify the correct value. For attributes that should not be initialized, we can use a Stated Outcome Assertion (see Assertion Method) appropriate to the type of the attribute [e.g., assertNull(anObjectReference) for object variables or pointers]. Note that if we are organizing our tests with one Testcase Class per Fixture (page 631), we can put each assertion into a sepa- rate Test Method to give optimal Defect Localization.\n\nwww.it-ebooks.info\n\n351\n\nTest Method\n\n352\n\nTest Method\n\nChapter 19 xUnit Basics Patterns\n\nVariation: Dependency Initialization Test\n\nWhen we have an object with a substitutable dependency, we need to make sure that the attribute that holds the reference to the depended-on component (DOC) is initialized to the real DOC when the software is run in production. A Depen- dency Initialization Test is a Constructor Test that asserts that this attribute is initialized correctly. It is often done in a different Test Method from the normal Constructor Tests to improve its visibility.\n\nExample: Simple Success Test\n\nThe following example illustrates a test where the novice test automater has included code to catch exceptions that he or she knows might occur (or that the test automater might have encountered while debugging the code).\n\npublic void testFlightMileage_asKm() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); try { // exercise SUT newFlight.setMileage(1122); // verify results int actualKilometres = newFlight.getMileageAsKm(); int expectedKilometres = 1810; // verify results assertEquals( expectedKilometres, actualKilometres); } catch (InvalidArgumentException e) { fail(e.getMessage()); } catch (ArrayStoreException e) { fail(e.getMessage()); } }\n\nThe majority of the code is unnecessary and just obscures the intent of the test. Luckily for us, all of this exception handling can be avoided. xUnit has built-in support for catching unexpected exceptions. We can rip out all the exception- handling code and let the Test Automation Framework catch any unexpected exception that might be thrown. Unexpected exceptions are counted as test errors because the test terminates in a way we didn’t anticipate. This is useful information and is not considered to be any more severe than a test failure.\n\npublic void testFlightMileage_asKm() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm();\n\nwww.it-ebooks.info\n\nTest Method\n\n// verify results int expectedKilometres = 1810; assertEquals( expectedKilometres, actualKilometres); }\n\nThis example is in Java (a statically typed language), so we had to declare that the SUT may throw an exception as part of the Test Method signature.\n\nExample: Expected Exception Test Using try/catch\n\nThe following example is a partially complete test to verify an exception case. The novice test automater has set up the right test condition to cause the SUT to raise an error.\n\npublic void testSetMileage_invalidInput() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); // exercise SUT newFlight.setMileage(-1122); // invalid // how do we verify an exception was thrown? }\n\nBecause the Test Automation Framework will catch the exception and fail the test, the Test Runner will not exhibit the green bar even though the SUT’s behavior is correct. We can introduce an error-handling block around the exercise phase of the test and use it to invert the pass/fail criteria (pass when the exception is thrown; fail when it is not). Here’s how to verify that the SUT fails as expected in JUnit 3.x:\n\npublic void testSetMileage_invalidInput() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); try { // exercise SUT newFlight.setMileage(-1122); fail(\"Should have thrown InvalidInputException\"); } catch( InvalidArgumentException e) { // verify results assertEquals( \"Flight mileage must be positive\", e.getMessage()); } }\n\nThis style of try/catch can be used only in languages that allow us to specify exactly which exception to catch. It won’t work if we want to catch a generic exception or the same exception that the Assertion Method fail throws, because these excep- tions will send us into the catch clause. In these cases we need to use the same style of Expected Exception Test as used in tests of Custom Assertions (page 474).\n\nwww.it-ebooks.info\n\n353\n\nTest Method\n\n354\n\nTest Method\n\nChapter 19 xUnit Basics Patterns\n\npublic void testSetMileage_invalidInput2() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); try { // exercise SUT newFlight.setMileage(-1122); // cannot fail() here if SUT throws same kind of exception } catch( AssertionFailedError e) { // verify results assertEquals( \"Flight mileage must be positive\", e.getMessage()); return; } fail(\"Should have thrown InvalidInputException\"); }\n\nExample: Expected Exception Test Using Method Attributes\n\nNUnit provides a method attribute that lets us write an Expected Exception Test without forcing us to code a try/catch block explicitly.\n\n[Test] [ExpectedException(typeof( InvalidArgumentException), \"Flight mileage must be > zero\")] public void testSetMileage_invalidInput_AttributeWithMessage() { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); // exercise SUT newFlight.setMileage(-1122); }\n\nThis approach does make the test much more compact but doesn’t provide a way to specify anything but the type of the exception or the message it contains. If we want to make any assertions on other contents of the exception (to avoid Sensitive Equality; see Fragile Test on page 239), we’ll need to use try/catch.\n\nExample: Expected Exception Test Using Block Closure\n\nSmalltalk’s SUnit provides another mechanism to achieve the same thing:\n\ntestSetMileageWithInvalidInput self should: [Flight new mileage: -1122] raise: RuntimeError new 'Should have raised error'\n\nBecause Smalltalk supports block closures, we pass the block of code to be executed to the method should:raise: along with the expected Exception object. Ruby’s Test::Unit uses the same approach:\n\nwww.it-ebooks.info\n\nTest Method\n\ndef testSetMileage_invalidInput ﬂight = Flight.new(); assert_raises( RuntimeError, \"Should have raised error\") do ﬂight.setMileage(-1122) end end\n\nThe code between the do/end pair is a closure that is executed by the assert_raises method. If it doesn’t raise an instance of the ﬁ rst argument (the class RuntimeError), the test fails and presents the error message supplied.\n\nExample: Constructor Test\n\nIn this example, we need to build a ﬂ ight to test the conversion of the ﬂ ight distance from miles to kilometers. First, we’ll make sure the ﬂ ight is constructed properly.\n\npublic void testFlightMileage_asKm2() throws Exception { // set up ﬁxture // exercise constructor Flight newFlight = new Flight(validFlightNumber); // verify constructed object assertEquals(validFlightNumber, newFlight.number); assertEquals(\"\", newFlight.airlineCode); assertNull(newFlight.airline); // set up mileage newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); // verify results int expectedKilometres = 1810; assertEquals( expectedKilometres, actualKilometres); // now try it with a canceled ﬂight newFlight.cancel(); try { newFlight.getMileageAsKm(); fail(\"Expected exception\"); } catch (InvalidRequestException e) { assertEquals( \"Cannot get cancelled ﬂight mileage\", e.getMessage()); } }\n\nThis test is not a Single-Condition Test (see page 45) because it examines both object construction and distance conversion behavior. If object construction fails, we won’t know which issue was the cause of the failure until we start debugging the test.\n\nwww.it-ebooks.info\n\n355\n\nTest Method\n\n356\n\nTest Method\n\nChapter 19 xUnit Basics Patterns\n\nIt would be better to separate this Eager Test (see Assertion Roulette on page 224) into two tests, each of which is a Single-Condition Test. This is most easily done by cloning the Test Method, renaming each copy to reﬂ ect what it would do if it were a Single-Condition Test, and then removing any code that doesn’t satisfy that goal.\n\nHere’s an example of a simple Constructor Test:\n\npublic void testFlightConstructor_OK() throws Exception { // set up ﬁxture // exercise SUT Flight newFlight = new Flight(validFlightNumber); // verify results assertEquals( validFlightNumber, newFlight.number ); assertEquals( \"\", newFlight.airlineCode ); assertNull( newFlight.airline ); }\n\nWhile we are at it, we might as well specify what should occur if an invalid argument is passed to the constructor by using the Expected Exception Test template for our Constructor Test:\n\npublic void testFlightConstructor_badInput() { // set up ﬁxture BigDecimal invalidFlightNumber = new BigDecimal(-1023); // exercise SUT try { Flight newFlight = new Flight(invalidFlightNumber); fail(\"Didn't catch negative ﬂight number!\"); } catch (InvalidArgumentException e) { // verify results assertEquals( \"Flight numbers must be positive\", e.getMessage()); } }\n\nNow that we know that our constructor logic is well tested, we are ready to write our Simple Success Test for our mileage translation functionality. Note how much simpler it has become because we can focus on verifying the business logic:\n\npublic void testFlightMileage_asKm() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); // verify results int expectedKilometres = 1810; assertEquals( expectedKilometres, actualKilometres); }\n\nwww.it-ebooks.info\n\nTest Method\n\nSo what happens if the constructor logic is defective? This test will likely fail because its output depends on the value passed to the constructor. The con- structor test will also fail. That failure will tell us to look at the constructor logic ﬁ rst. Once that problem is ﬁ xed, this test will likely pass. If it doesn’t, then we can focus on ﬁ xing the getMileageAsKm method logic. This is a good example of Defect Localization.\n\nwww.it-ebooks.info\n\n357\n\nTest Method\n\n358\n\nFour-Phase Test\n\nChapter 19 xUnit Basics Patterns\n\nFour-Phase Test\n\nHow do we structure our test logic to make what we are testing obvious?\n\nWe structure each test with four distinct parts executed in sequence.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test\n\nSuite Suite Object Object\n\nSetup Setup\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTeardown Teardown\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nHow It Works\n\nWe design each test to have four distinct phases that are executed in sequence: ﬁ xture setup, exercise SUT, result veriﬁ cation, and ﬁ xture teardown.\n\nIn the ﬁ rst phase, we set up the test ﬁ xture (the “before” picture) that is required for the SUT to exhibit the expected behavior as well as any- thing you need to put in place to be able to observe the actual outcome (such as using a Test Double; see page 522).\n\nIn the second phase, we interact with the SUT.\n\nIn the third phase, we do whatever is necessary to determine whether\n\nthe expected outcome has been obtained.\n\nIn the fourth phase, we tear down the test ﬁ xture to put the world back\n\ninto the state in which we found it.\n\nWhy We Do This\n\nThe test reader must be able to quickly determine what behavior the test is verifying. It can be very confusing when various behaviors of the SUT are being invoked—some to set up the pre-test state (ﬁ xture) of the SUT, others to exercise\n\nwww.it-ebooks.info\n\nFour-Phase Test\n\nthe SUT, and yet others to verify the post-test state of the SUT. Clearly identifying the four phases makes the intent of the test much easier to see.\n\nThe ﬁ xture setup phase of the test establishes the SUT’s state prior to the test, which is an important input to the test. The exercise SUT phase is where we actu- ally run the software we are testing. When reading the test, we need to see which software is being run. The result veriﬁ cation phase of the test is where we specify the expected outcome. The ﬁ nal phase, ﬁ xture teardown, is all about housekeeping. We wouldn’t want to obscure the important test logic with it because it is com- pletely irrelevant from the perspective of Tests as Documentation (see page 23). We should avoid the temptation to test as much functionality as pos- sible in a single Test Method (page 348) because that can result in Obscure Tests (page 186). In fact, it is preferable to have many small Single-Condition Tests (see page 45). Using comments to mark the phases of a Four-Phase Test is a good source of self-discipline, in that it makes it very obvious when our tests are not Single-Condition Tests. It will be self-evident if we have multiple exercise SUT phases separated by result veriﬁ cation phases or if we have inter- spersed ﬁ xture setup and exercise SUT phases. Sure, the tests may work—but they will provide less Defect Localization (see page 22) than if we have a bunch of independent Single-Condition Tests.\n\nImplementation Notes\n\nWe have several options for implementing the Four-Phase Test. In the simplest case, each test is completely free-standing. All four phases of the test are con- tained within the body of the Test Method. This structure implies we are using In-line Setup (page 408) and either Garbage-Collected Teardown (page 500) or In-line Teardown (page 509). It is the most appropriate choice when we are using Testcase Class per Class (page 617) or Testcase Class per Feature (page 624) to organize our Test Methods.\n\nThe other choice is to take advantage of the Test Automation Framework’s (page 298) support for Implicit Setup (page 424) and Implicit Teardown (page 516). We factor out the common ﬁ xture setup and ﬁ xture teardown logic into setUp and tearDown methods on the Testcase Class (page 373). This leaves only the exercise SUT and result veriﬁ cation phases in the Test Method. This approach is an appropriate choice when we are using Testcase Class per Fixture (page 631). We can also use this approach to set up common parts of the ﬁ xture when using Testcase Class per Class (page 617) or Testcase Class per Feature or to tear down the ﬁ xture when using Automated Teardown (page 503).\n\nwww.it-ebooks.info\n\n359\n\nFour-Phase Test\n\n360\n\nFour-Phase Test\n\nChapter 19 xUnit Basics Patterns\n\nExample: Four-Phase Test (In-line)\n\nHere is an example of a test that is clearly a Four-Phase Test:\n\npublic void testGetFlightsByOriginAirport_NoFlights_inline() throws Exception { // Fixture setup NonTxFlightMngtFacade facade =new NonTxFlightMngtFacade(); BigDecimal airportId = facade.createTestAirport(\"1OF\"); try { // Exercise system List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(airportId); // Verify outcome assertEquals( 0, ﬂightsAtDestination1.size() ); } ﬁnally { // Fixture teardown facade.removeAirport( airportId ); } }\n\nAll four phases of the Four-Phase Test are included as in-line code. Because the calls to Assertion Methods (page 362) raise exceptions, we need to surround the ﬁ xture teardown part of the Test Method with a try/ﬁ nally construct to ensure that it is run in all cases.\n\nExample: Four-Phase Test (Implicit Setup/Teardown)\n\nHere is the same Four-Phase Test with the ﬁ xture setup and ﬁ xture teardown logic moved out of the Test Method:\n\nNonTxFlightMngtFacade facade = new NonTxFlightMngtFacade(); private BigDecimal airportId;\n\nprotected void setUp() throws Exception { // Fixture setup super.setUp(); airportId = facade.createTestAirport(\"1OF\"); }\n\npublic void testGetFlightsByOriginAirport_NoFlights_implicit() throws Exception { // Exercise SUT List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(airportId); // Verify outcome assertEquals( 0, ﬂightsAtDestination1.size() ); }\n\nprotected void tearDown() throws Exception {\n\nwww.it-ebooks.info\n\nFour-Phase Test\n\n// Fixture teardown facade.removeAirport(airportId); super.tearDown(); }\n\nBecause the tearDown method is called automatically even after test failures, we don’t need the try/ﬁ nally construct inside the Test Method. The downside, however, is that references to our ﬁ xture must be held in instance variables rather than local variables.\n\nwww.it-ebooks.info\n\n361\n\nFour-Phase Test\n\n362\n\nAssertion Method\n\nChapter 19 xUnit Basics Patterns\n\nAssertion Method\n\nHow do we make tests self-checking?\n\nWe call a utility method to evaluate whether an expected outcome has been achieved.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nTest failed Test failed\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nSetup Setup\n\nExercise Exercise\n\nTest Test Suite Suite Object Object\n\nVerify Verify\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTeardown Teardown\n\nTestcase Testcase Object Object\n\nAssertion Assertion Method Method\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nA key part of writing Fully Automated Tests (see page 26) is to make them Self- Checking Tests (see page 26) to avoid having to inspect the outcome of each test for correctness each time it is run. This strategy involves ﬁ nding a way to express the expected outcome so that it can be veriﬁ ed automatically by the test itself.\n\nAssertion Methods give us a way to express the expected outcome in a way that is both executable by the computer and useful to the human reader, who can then use the Tests as Documentation (see page 23).\n\nHow It Works\n\nWe encode the expected outcome of the test as a series of assertions that state what should be true for the test to pass. The assertions are realized as calls to Assertion Methods that encapsulate the mechanism that causes the test to fail. The Assertion Methods may be provided by the Test Automation Framework (page 298) or by the test automater as Custom Assertions (page 474).\n\nwww.it-ebooks.info\n\nAssertion Method\n\nWhy We Do This\n\nEncoding the expected outcome using Conditional Test Logic (page 200) is very verbose and makes tests hard to read and understand. It is also much more likely to lead to Test Code Duplication (page 213) and Buggy Tests (page 260). Asser- tion Methods help us avoid these issues by moving that complexity into reusable Test Utility Methods (page 599); these methods can then be veriﬁ ed as working correctly using Test Utility Tests (see Test Utility Method).\n\nImplementation Notes\n\nAlthough all members of the xUnit family provide Assertion Methods, they do so with a fair degree of variability. The key implementation considerations are as follows:\n\nHow to call the Assertion Methods\n\nHow to choose the best Assertion Method to call\n\nWhat information to include in the Assertion Message (page 370)\n\nCalling Built-in Assertion Methods\n\nThe way the Assertion Methods are called from within the Test Method (page 348) varies from language to language and from framework to framework. The lan- guage features determine what is possible and preferable, while the framework builders chose which options to use. The names these developers chose for the Assertion Methods were inﬂ uenced by how they chose to access them. Here are the most common options for accessing the Assertion Methods:\n\nTheAssertion Methods are inherited from a Testcase Superclass (page 638) provided by the framework. Such methods may be invoked as though they were provided locally on the Testcase Class (page 373). The original version of Java’s JUnit, for example, used this approach by providing a Testcase Superclass that inherits from the class Assert, which contains the actual Assertion Methods.\n\nThe Assertion Methods are provided via a globally accessible class or module. They are invoked using the class or module name to fully qualify the Assertion Method name. NUnit, for example, uses this approach [e.g., Assert.isTrue(x);]. JUnit does allow assertions to be invoked as static\n\nwww.it-ebooks.info\n\n363\n\nAssertion Method\n\n364\n\nAssertion Method\n\nChapter 19 xUnit Basics Patterns\n\nmethods on the Assert class [e.g., Assert.assertTrue(x)] but this is not usually necessary because they are inherited via the Testcase Superclass.\n\nThe Assertion Methods are provided as “mix-ins” or macros. Ruby’s Test:: Unit, for example, provides the Assertion Methods in a module called Assert that can be included in any class,2 thereby allowing the Assertion Methods to be used as though deﬁ ned within the Testcase Class [e.g., assert_equal(a,b)]. CppUnit, by contrast, deﬁ nes the Assertion Methods as macros, which are expanded before the compiler sees the code.\n\nAssertion Messages\n\nAssertion Methods typically take an optional Assertion Message as a text param- eter that is included in the output when the assertion fails. This structure allows the test automater to explain to the test maintainer exactly which Assertion Method failed and to better explain what should have occurred. The error detected by the test will be much easier to debug if the Assertion Method provides more informa- tion about why it failed. Choosing the right Assertion Method goes a long way toward achieving this goal because many of the built-in Assertion Methods provide useful diagnostic information about the values of the arguments. This is especially true for Equality Assertions.\n\nOne of the biggest differences between members of the xUnit family is where the optional Assertion Message appears in the argument list. Most members tack it on to the end as an optional argument. JUnit, however, makes the Asser- tion Message the ﬁ rst argument when it is present.\n\nChoosing the Right Assertion\n\nWe have two goals for the calls to Assertion Methods in our Test Methods:\n\nFail the test when something other than the expected outcome occurs\n\nDocument how the SUT is supposed to behave (i.e., Tests as Documen-\n\ntation)\n\nTo achieve these goals we must strive to use the most appropriate Assertion Method. While the syntax and naming conventions vary from one member of the xUnit family to the next, most provide a basic set of assertions that fall into the following categories:\n\n2 This approach is particularly useful when we are building Mock Objects (page 544) because these objects are outside the Testcase Class but need to invoke Assertion Methods.\n\nwww.it-ebooks.info\n\nAssertion Method\n\nSingle-Outcome Assertions such as fail; these take no arguments because they always behave the same way.\n\nStated Outcome Assertions such as assertNotNull(anObjectReference) and assertTrue(aBooleanExpression); these compare a single argument to an outcome implied by the method name.\n\nExpected Exception Assertions such as assert_raises(expectedError) { codeToExecute }; these evaluate a block of code and a single expected exception argument.\n\nEquality Assertions such as assertEqual(expected, actual); these compare two objects or values for equality.\n\nFuzzy Equality Assertions such as assertEqual(expected, actual, tolerance); these determine whether two values are “close enough” to each other by using a “tolerance” or “comparison mask.”\n\nVariation: Equality Assertion\n\nEquality Assertions are the most common examples of Assertion Methods. They are used to compare the actual outcome with an expected outcome that is expressed in the form of a constant Literal Value (page 714) or an Expected Object (see State Veriﬁ cation on page 462). By convention, the expected value is speci- ﬁ ed ﬁ rst and the actual value follows it. The diagnostic message that is generated by the Test Automation Framework makes sense only when they are provided in this order. The equality of the two objects is usually determined by invoking the equals method on the expected object. If the SUT’s deﬁ nition of equals is not what we want to use in our tests, either we can make Equality Assertions on individual ﬁ elds of the object or we can implement our test-speciﬁ c equality on a Test-Speciﬁ c Subclass (page 579) of the Expected Object.\n\nVariation: Fuzzy Equality Assertion\n\nWhen we cannot guarantee an exact match due to variations in precision or expected variations in value, it may be appropriate to use a Fuzzy Equality Assertion. Typically, these assertions look just like Equality Assertions with the addition of an extra “tolerance” or “comparison map” parameter that speciﬁ es how close the actual argument must be to the expected one. The most common example of a Fuzzy Equality Assertion is the comparison of ﬂ oating-point num- bers where the limitations of arithmetic precision need to be accounted for by providing a tolerance (the maximum acceptable distance between the two values).\n\nwww.it-ebooks.info\n\n365\n\nAssertion Method\n\n366\n\nAssertion Method\n\nChapter 19 xUnit Basics Patterns\n\nWe use the same approach when comparing XML documents where direct string comparisons may result in failure owing to certain ﬁ elds having unpre- dictable content. In this case, the “fuzz” speciﬁ cation is a “comparison schema” that speciﬁ es which ﬁ elds need to match or which ﬁ elds should be ignored. This kind of Equality Assertion is very similar to asserting that a string conforms to a regular expression or other form of pattern matching.\n\nVariation: Stated Outcome Assertion\n\nStated Outcome Assertions are a way of saying exactly what the outcome should be without passing an expected value as an argument. The outcome must be common enough to warrant a special Assertion Method. The most common examples are as follows:\n\nassertTrue(aBooleanExpression), which fails if the expression evaluates to FALSE\n\nassertNotNull(anObjectReference), which fails if the objectReference doesn’t refer to a valid object\n\nStated Outcome Assertions are often used as Guard Assertions (page 490) to avoid Conditional Test Logic.\n\nVariation: Expected Exception Assertion\n\nIn languages that support block closures, we can use a variation of a Stated Outcome Assertion that takes an additional parameter specifying the kind of exception we expect. We can use this Expected Exception Assertion to say, “Run this block and verify that the following exception is thrown.” This format is more compact than using a try/catch construct. Some typical examples follow:\n\nshould: [aBlockToExecute] raise: expectedException in Smalltalk’s SUnit\n\nassert_raises( expectedError) { codeToExecute } in Ruby’s Test::Unit\n\nVariation: Single-Outcome Assertion\n\nA Single-Outcome Assertion always behaves the same way. The most commonly used Single-Outcome Assertion is fail, which causes a test to be treated as a failure. It is typically used in two circumstances:\n\nAs an Unﬁ nished Test Assertion (page 494) when a test is ﬁ rst identiﬁ ed and implemented as a nearly empty Test Method. By including a call to fail, we can have the Test Runner (page 377) remind us that we still have a test to ﬁ nish writing.\n\nwww.it-ebooks.info\n\nAssertion Method\n\nAs part of a try/catch (or equivalent) block in an Expected Exception Test (see Test Method) by including a call to fail in the try block immediately after the call that is expected to throw an exception. If we don’t want to assert something about the exception that was caught, we can avoid an empty catch block by using the Single-Outcome Assertion success to document that this is the expected outcome. Assertion Method\n\nOne circumstance in which we really should not use Single-Outcome Assertions is in Conditional Test Logic. There is almost never a good reason to include conditional logic in a Test Method, as there is usually a more declarative way to handle this situation using other styles of Assertion Methods. For example, use of Guard Assertions results in tests that are more easily understood and less likely to yield incorrect results.\n\nMotivating Example\n\nThe following example illustrates the kind of code that would be required for each item we wanted to verify if we did not have Assertion Methods. All we really want to do is this:\n\nif (x.equals(y)) { throw new AssertionFailedError( \"expected: <\" + x.toString() + \"> but found: <\" + y.toString() + \">\"); } else { // Okay, continue // ... }\n\nUnfortunately, this code will cause a NullPointerException if x is null, and it would be hard to distinguish this exception from an error in the SUT. Thus we need to put some guard clauses around this functionality so that we always throw an AssertionFailedException:\n\nif (x == null) { // cannot do null.equals(null) if (y == null ) { // they are both null so equal return; } else { throw new AssertionFailedError( \"expected null but found: <\" + y.toString() +\">\"); } } else if (!x.equals(y)) { // comparable but not equal! throw new AssertionFailedError( \"expected: <\" + x.toString() + \"> but found: <\" + y.toString() + \">\");\n\n} // equal\n\nwww.it-ebooks.info\n\n367\n\n368\n\nAssertion Method\n\nChapter 19 xUnit Basics Patterns\n\nYikes! That got pretty messy. And we’ll have to do the same thing for every attribute we want to verify? This is not good. There must be a better way.\n\nRefactoring Notes\n\nLuckily for us, the inventors of xUnit recognized this problem and did the requisite Extract Method [Fowler] refactoring to create a library of Assertion Methods that we can call instead. We simply replace the mess of in-line if statements and thrown exceptions with a call to the appropriate Assertion Method. The next example is the code for the JUnit assertEquals method. Although the intent of this example is the same as the code we wrote earlier, it has been rewritten in terms of guard clauses that identify when things are equal.\n\n/** * Asserts that two objects are equal. If they are not, * an AssertionFailedError is thrown with the given message. */ static public void assertEquals(String message, Object expected, Object actual) { if (expected == null && actual == null) return; if (expected != null && expected.equals(actual)) return; failNotEquals(message, expected, actual); }\n\nThe method failNotEquals is a Test Utility Method that fails the test and provides a diagnostic assertion message.\n\nExample: Equality Assertion\n\nHere is the same assertion logic recoded to take advantage of JUnit’s Equality Assertion:\n\nassertEquals( x, y );\n\nHere is the same assertion coded in C#. Note the classname qualiﬁ er and the resulting difference in the method name:\n\nAssert.AreEqual( x, y );\n\nExample: Fuzzy Equality Assertion\n\nTo compare two ﬂ oating-point numbers (which are rarely ever really equal), we specify the acceptable differences using a Fuzzy Equality Assertion:\n\nwww.it-ebooks.info\n\nAssertion Method\n\nassertEquals( 3.1415, diameter/2/radius, 0.001); assertEquals( expectedXml, actualXml, elementsToCompare );\n\nExample: Stated Outcome Assertion\n\nTo insist that a particular outcome has occurred, we use a Stated Outcome Assertion:\n\nassertNotNull( a ); assertTrue( b > c ); assertNonZero( b );\n\nExample: Expected Exception Assertion\n\nHere is an example of how we verify that the correct exception was raised when we have blocks. In Smalltalk’s SUnit, it looks like this:\n\nself should: [Flight new mileage: -1122] raise: RuntimeError new 'Should have raised error'\n\nThe should: indicates the block of code to run (surrounded by square brackets), while the raise: speciﬁ es the expected exception object. In Ruby, it looks like this:\n\nassert_raises( RuntimeError, \"Should have raised error\") {ﬂight.setMileage(-1122) }\n\nThe Ruby language syntax also lets us use this “control structure”-style syntax by delimiting the block using do/end instead of curly braces:\n\nassert_raises( RuntimeError, \"Should have raised error\") do ﬂight.setMileage(-1122) end\n\nExample: Single-Outcome Assertion\n\nTo fail the test, use the Single Outcome Assertion:\n\nfail( \"Expected an exception\" ); unﬁnishedTest();\n\nwww.it-ebooks.info\n\n369\n\nAssertion Method\n\n370\n\nAssertion Message\n\nChapter 19 xUnit Basics Patterns\n\nAssertion Message\n\nHow do we structure our test logic to know which assertion failed?\n\nWe include a descriptive string argument in each call to an Assertion Method.\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nTest Failed: Test Failed: message message\n\nSuite Suite\n\ntestMethod_1 testMethod_1\n\nrun run\n\nCreate Create\n\nSetup Setup Exercise Exercise Test Test Verify Verify Suite Suite Teardown Teardown Object Object\n\ntestMethod_n testMethod_n\n\nTestcase Testcase Class Class\n\nmessage message\n\nmessage message\n\nAssertion Assertion Method Method\n\nTestcase Testcase Object Object testMethod_n testMethod_n\n\nFixture Fixture\n\nSUT SUT\n\nWe make tests Self-Checking (see page 26) by including calls to Assertion Meth- ods (page 362) that specify the expected outcome. When a test fails, the Test Runner (page 377) writes an entry to the test result log.\n\nA well-crafted Assertion Message makes it very easy to determine which asser-\n\ntion failed and exactly what the symptoms were when the failure happened.\n\nHow It Works\n\nEvery Assertion Method takes an optional string parameter that is included in the failure log. When the condition being asserted is not true, the Assertion Message is output to the Test Runner’s log along with whatever output the assertion method normally generates.\n\nWhen to Use It\n\nThere are two schools of thought on this subject. Test drivers who belong to the “single assertion per Test Method” school believe that they don’t need to include Assertion Messages because only one assertion can possibly fail and, therefore, they always know exactly which assertion happened. They count on the Assertion Method to include the arguments (e.g., expected “x” but was “y”) but they don’t need to include a message.\n\nwww.it-ebooks.info\n\nAssertion Message\n\nConversely, people who ﬁ nd themselves coding several or many assertion method calls in their tests should strongly consider including a message that at least distinguishes which assertion failed. This information is especially impor- tant if the tests are frequently run using a Command-Line Test Runner (see Test Runner), which rarely provides failure location information.\n\nImplementation Notes\n\nIt is easy to state that we need a message for each assertion method call—but what should we say in the message? It is useful to take a moment as we write each assertion and ask ourselves what the person reading the failure log would hope to get out of it.\n\nVariation: Assertion-Identifying Message\n\nWhen we include several assertions of the same type in the same Test Method (page 348), we make it more difﬁ cult to determine exactly which one failed the test. By including some unique text in each Assertion Message, we can make it very easy to determine which assertion method call failed. A common practice is to use the name of the variable or attribute being asserted on as the message. This technique is very simple and requires very little thought. Another option is to number the assertions. This information would certainly be unique but understanding it may be less intuitive as we would have to look at the code to determine which assertion was failing.\n\nVariation: Expectation-Describing Message\n\nWhen a test fails, we know what has actually happened. The big question is, “What should have happened?” There are several ways of documenting the expected behavior for the test reader. For example, we could place comments in the test code. A better solution is to include a description of the expectation in the Assertion Message. While this is done automatically for an Equality Asser- tion (see Assertion Method), we need to provide this information ourselves for any Stated Outcome Assertions (see Assertion Method).\n\nVariation: Argument-Describing Message\n\nSome types of Assertion Methods provide less helpful failure messages than others. Among the worst are Stated Outcome Assertions such as assertTrue (aBooleanExpression). When they fail, all we know is that the stated outcome did not occur. In these cases we can include the expression that was being evalu- ated (including the actual values) as part of the Assertion Message text. The\n\nwww.it-ebooks.info\n\n371\n\nAssertion Message\n\n372\n\nAssertion Message\n\nChapter 19 xUnit Basics Patterns\n\ntest maintainer can then examine the failure log and determine what was being evaluated and why it caused the test to fail.\n\nMotivating Example\n\nassertTrue( a > b ); assertTrue( b > c );\n\nThis code emits a failure message—something like “Assertion Failed.” From this output, we cannot even tell which of the two Assertion Messages failed. Not very useful, is it?\n\nRefactoring Notes\n\nFixing this problem is a simple matter of adding one more parameter to each Assertion Method call. In this case, we want to communicate that we are expecting “a” to be greater than “b.” Of course, it would also be useful to be able to see what the values of “a” and “b” actually were. We can add both pieces of information into the Assertion Message through some judicious string concatenation.\n\nExample: Expectation-Describing Message\n\nHere is the same test with the Argument-Describing Message added:\n\nassertTrue( \"Expected a > b but a was '\" + a.toString() + \"' and b was '\" + b.toString() + \"'\", a.gt(b) ); assertTrue( \"Expected b > c but b was '\" + b.toString() + \"' and c was '\" + c.toString + \"'\", b > c );\n\nThis will now result in a useful failure message:\n\nAssertion Failed. Expected a > b but a was '17' and b was '19'.\n\nOf course, this output would be even more meaningful if the variables had Intent-Revealing Names [SBPP]!\n\nwww.it-ebooks.info\n\nTestcase Class\n\nTestcase Class\n\nWhere do we put our test code?\n\nWe group a set of related Test Methods on a single Testcase Class.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nWe put our test logic into Test Methods (page 348) but those Test Methods need to be associated with a class. A Testcase Class gives us a place to host those methods that we can later turn into Testcase Objects (page 382).\n\nHow It Works\n\nWe collect all Test Methods that are related in some way onto a special kind of class, the Testcase Class. At runtime, the Testcase Class acts as a Test Suite Factory (see Test Enumeration on page 399) that creates a Testcase Object for each Test Method. It adds all of these objects to a Test Suite Object (page 387) that the Test Runner (page 377) will use to run them all.\n\nWhy We Do This\n\nIn object-oriented languages, we prefer to put our Test Methods onto a class rather than having them as global functions or procedures (even if that is allowed). By making them instance methods of a Testcase Class, we can create a Testcase Object for each test by instantiating the Testcase Class once for each Test Method. This strategy allows us to manipulate the Test Methods at runtime.\n\nwww.it-ebooks.info\n\n373\n\nTestcase Class\n\nAlso known as: Test Fixture\n\n374\n\nTestcase Class\n\nChapter 19 xUnit Basics Patterns\n\nClass–Instance Duality\n\nBack in high school physics, we learned about the “wave–particle duality” of light. Sometimes light acts like a particle (e.g., going through a small aperture), and sometimes it acts like a wave (e.g., rainbows). The behavior of Testcase Classes (page 373) sometimes reminds me of this concept. Let me explain why.\n\nDevelopers new to xUnit often ask, “Why is the class we subclass called TestCase when we have several Test Methods on it? Shouldn’t it be called TestSuite?” These questions make a lot of sense when we are focused primarily on the view of the class when we are writing the test code as opposed to when we are running the code.\n\nWhen we are writing test code, we concentrate on the Test Methods. The Testcase Class is primarily just a place to put the methods. About the only time we think of objects is when we use Implicit Setup (page 424) and need to create ﬁ elds (instance variables) to hold them between the invo- cation of the setUp method and when they are used in the Test Method. When developers new to xUnit test automation are writing their ﬁ rst tests, they tend to code by example. Following an existing example is a good way to get something working quickly but it doesn’t necessarily help the developer understand what is really going on.\n\nAt runtime, the xUnit framework typically creates one instance of the Testcase Class for each Test Method. The Testcase Class acts as a Test Suite Factory (see Test Enumeration on page 399) that builds a Test Suite Object (page 387) containing all the instances of itself, one instance for each Test Method. Now, it’s not very often that a static method on a class returns an instance of another class containing many instances of itself. If this behavior wasn’t odd enough, the fact that xUnit reports the test failures using the Test Method name can be enough to obscure from many test automaters the existence of “objects inside.”\n\nWhen we examine the object relationships at runtime, things become a bit clearer. The Test Suite Object returned by the Test Suite Factory contains one or more Testcase Objects (page 382). So far, so good. Each of these objects is an instance of our Testcase Class. Each instance is conﬁ gured to run one of the Test Methods. More importantly, each will run a differ- ent Test Method. (How this happens is described in more detail in Test Discovery on page 393.) So each instance of our Testcase Class is, indeed, a test case. The Test Methods are just how we tell each instance what it should test.\n\nwww.it-ebooks.info\n\nTestcase Class\n\nFurther Reading Martin Fowler has a great piece on his blog about this issue called “JUnit New Instance” [JNI].\n\nWe could, of course, implement each Test Method on a separate class—but that creates additional overhead and clutters the class namespace. It also makes it harder (although not impossible) to reuse functionality between tests.\n\nImplementation Notes\n\nMost of the complexity of writing tests involves how to write the Test Methods: what to include in-line and what to factor out into Test Utility Methods (page 599), how to Isolate the SUT (see page 43), and so on.\n\nThe real magic associated with the Testcase Class occurs at runtime and is described in Testcase Object and Test Runner. As far as we are concerned, all we have to do is write some Test Methods that contain our test logic and let the Test Runner work its magic. We can avoid Test Code Duplica- tion (page 213) by using an Extract Method [Fowler] refactoring to factor out common code into Test Utility Methods. These methods can be left on the Testcase Class or they can be moved to an Abstract Testcase superclass (see Test- case Superclass on page 638), a Test Helper class (page 643), or a Test Helper Mixin (see Testcase Superclass).\n\nExample: Testcase Class\n\nHere is an example of a simple Testcase Class:\n\npublic class TestScheduleFlight extends TestCase {\n\npublic void testUnscheduled_shouldEndUpInScheduled() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.schedule(); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); }\n\npublic void testScheduledState_shouldThrowInvalidRequestEx() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.schedule(); fail(\"not allowed in scheduled state\");\n\nwww.it-ebooks.info\n\n375\n\nTestcase Class\n\n376\n\nTestcase Class\n\nChapter 19 xUnit Basics Patterns\n\n} catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testAwaitingApproval_shouldThrowInvalidRequestEx() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.schedule(); fail(\"not allowed in schedule state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } } }\n\nFurther Reading\n\nIn some variants of xUnit (most notably VbUnit and NUnit), the Testcase Class is called a test ﬁ xture. This usage should not be confused with the test ﬁ xture (or test context) that consists of everything we need to have in place before we can start exercising the SUT.3 Neither should it be confused with the ﬁ xture term as used by the Fit framework, which is the Adapter [GOF] that interacts with the Fit table and thereby implements a Data-Driven Test (page 288) Interpreter [GOF].\n\n3 These are the pre-conditions of the test.\n\nwww.it-ebooks.info\n\nTest Runner\n\nTest Runner\n\nHow do we run the tests?\n\nWe deﬁ ne an application that instantiates a Test Suite Object and executes all the Testcase Objects it contains.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nFixture Fixture\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nAssuming we have deﬁ ned our Test Methods (page 348) on one or more Testcase Classes (page 373), how do we actually cause the Test Automation Frameworks (page 298) to run our tests?\n\nHow It Works\n\nEach member of the xUnit family of Test Automation Frameworks provides some form of command-line or graphical application that can be used to run our automated tests and report on the results. The Test Runner uses Test Enu- meration (page 399), Test Discovery (page 393), or Test Selection (page 403) to obtain a Composite [GOF] test object. The latter may either be a single Testcase Object (page 382), a Test Suite Object (page 387), or a Composite test suite (a Suite of Suites; see Test Suite Object). Because all of these objects implement the same interface, the Test Runner need not care whether it is dealing with a single test or a multilevel suite. The Test Runner keeps track of, and reports on, how many tests it has run, how many tests had failed assertions, and how many tests raised errors or exceptions.\n\nwww.it-ebooks.info\n\n377\n\nTest Runner\n\n378\n\nTest Runner\n\nChapter 19 xUnit Basics Patterns\n\nWhy We Do This\n\nWe wouldn’t want each test automater to have to provide a special means of running his or her own test suites. That requirement would just get in the way of our ability to simultaneously run all the tests automated by different people. By providing a standard Test Runner, we encourage developers to make it easy to run tests written by different people. We can also provide different ways of running the same tests.\n\nImplementation Notes\n\nSeveral styles of Test Runners are available. The most common variations are running tests from within an IDE and running tests from the command line. All of these schemes depend on the fact that all Testcase Objects implement a standard interface.\n\nStandard Test Interface\n\nStatically typed languages (such as Java and C#) typically include an interface type (fully abstract class) that deﬁ nes the interface that all Testcase Objects and Test Suite Objects must implement. Some languages (such as C# and Java 5.0) “mix” in the implementation by using class attributes or annotations on the Testcase Class. In dynamically typed languages, this interface may not exist explicitly. Instead, each implementation class simply implements the standard interface methods. Typically, the standard test interface includes methods on it to count the available tests and to run the tests. Where the framework supports Test Enumeration, each Testcase Class and test suite class must also implement the Test Suite Factory method (see Test Enumeration on page 399), which is typically called suite.\n\nVariation: Graphical Test Runner\n\nA Graphical Test Runner is typically a desktop application or part of an IDE (either built-in or a plug-in) for running tests. At least one, IeUnit, runs inside a Web browser rather than an IDE. The most common feature of the Graphical Test Runner is some sort of real-time progress indicator. This monitor typically includes a running count of test failures and errors and often includes a colored progress bar that starts off green and turns red as soon as an error or failure is encountered. Some members of the xUnit family include a graphical Test Tree Explorer as a means to drill down and run a single test from within a Suite of Suites.\n\nwww.it-ebooks.info\n\nTest Runner\n\nHere is the Graphical Test Runner from the JUnit plug-in for Eclipse:\n\nThe red bar near the top indicates that at least one test has failed. The upper text pane shows a list of test failures and test errors. The lower pane shows the traceback from the failed test selected in the upper pane.\n\nVariation: Command-Line Test Runner\n\nCommand-Line Test Runners are designed to be used from an operating system command line or from batch ﬁ les or shell scripts. They are very handy when working remotely via remote shells or when running the tests from a build script such as “make,” Ant, or a continuous integration tool such as “Cruise Control.” The following example shows how to run an runit (one of the xUnit imple- mentations for the Ruby programming language) test from the command line:\n\nwww.it-ebooks.info\n\n379\n\nTest Runner\n\n380\n\nTest Runner\n\nChapter 19 xUnit Basics Patterns\n\n>ruby testrunner.rb c:/examples/tests/SmellHandlerTest.rb Loaded suite SmellHandlerTest Started ..... Finished in 0.016 seconds. 5 tests, 6 assertions, 0 failures, 0 errors >Exit code: 0\n\nThe ﬁ rst line is the invocation at the command prompt. In this example we are running the tests deﬁ ned in a single Testcase Class, SmellHandlerTest. The next two lines are the initial feedback as the tests begin. The series of dots indicates the tests’ progress, one per test completed. This particular Command-Line Test Runner replaces the dot with an “E” or an “F” if the test produces an error or fails. The last three lines are summary statistics that provide an overview of what happened. Typically, the exit code is set to the total number of failed/error tests so that a non-zero exit code can be interpreted easily as a build failure when run from an automated build tool.\n\nVariation: File System Test Runner\n\nSome Command-Line Test Runners provide the option of searching a speciﬁ ed directory for all ﬁ les that are tests and running them all at once. This automated Testcase Class Discovery (see Test Discovery) avoids the need to build the Suite of Suites in code (Test Enumeration) and helps avoid Lost Tests (see Production Bugs on page 268).\n\nIn addition, some external tools will search the ﬁ le system for ﬁ les matching speciﬁ c patterns and then invoke an arbitrary command against the matched ﬁ les. These ﬁ les can be passed to the Test Runner from a build tool.\n\nVariation: Test Tree Explorer\n\nMembers of the xUnit family that turn each Test Method into a Testcase Object can manipulate the tests easily. Many of them provide a graphical representation of the Suite of Suites and allow the user to select an entire Test Suite Object or a single Testcase Object to run. This eliminates the need to create a Single Test Suite (see Named Test Suite on page 592) class to run a single test.\n\nHere is the Test Tree Explorer of JUnit plug-in for Eclipse shown “popped\n\nout” over other Eclipse views:\n\nwww.it-ebooks.info\n\nTest Runner\n\nThe left pane of the IDE is the JUnit view within Eclipse. The progress bar ap- pears at the top of the view, the upper pane is the Test Tree Explorer, and the lower pane is the traceback for the currently selected test failure. Note that some Test Suite Objects in the Test Tree Explorer are “open,” revealing their contents; others are closed down. The colored annotation next to each Testcase Object shows its status; the annotations for each Test Suite Object indicate whether any contained Testcase Objects failed or produced an error. The Test Suite Object called “Test for com.clrstream.ex8.test” is a Suite of Suites for the package “com. clrstream.ex8.test”; “Test for allJUnitTests” is the topmost Suite of Suites for run- ning all the tests.\n\nwww.it-ebooks.info\n\n381\n\nTest Runner\n\n382\n\nTestcase Object\n\nChapter 19 xUnit Basics Patterns\n\nTestcase Object\n\nHow do we run the tests?\n\nWe create a Command object for each test and call the run method when we wish to execute it.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nThe Test Runner (page 377) needs a way to ﬁ nd and invoke the appropriate Test Methods (page 348) and to present the results to the user. Many Graphical Test Runners (see Test Runner) let the user drill down into the tree of tests and pick individual tests to run. This capability requires that the Test Runner be able to inspect and manipulate the tests at runtime.\n\nHow It Works\n\nWe instantiate a Command [GOF] object to represent each Test Method that should execute. We use the Testcase Class (page 373) as a Test Suite Factory to create a Test Suite Object (page 387) to hold all the Testcase Objects for a particular Testcase Class. We can use either Test Discovery (page 393) or Test Enumeration to create the Testcase Objects.\n\nwww.it-ebooks.info\n\nTestcase Object\n\nWhy We Do This\n\nTreating tests as ﬁ rst-class objects opens up many new possibilities that are not available to us if we treat the tests as simple procedures. It is a lot easier for the Test Runner of the Test Automation Framework (page 298) to manipulate tests when they are objects. We can hold them in collections (Test Suite Objects), iterate over them, invoke them, and so on.\n\nMost members of the xUnit family create a separate Testcase Object for each test to isolate the tests from one another as prescribed by Independent Test (see page 42). Unfortunately, there is always an exception (see the sidebar “There’s Always an Exception” on page 384), and users of the affected Test Automation Frameworks need to be a bit more cautious.\n\nImplementation Notes\n\nEach Testcase Object implements a standard test interface so that the Test Runner does not need to know the speciﬁ c interface for each test. This scheme allows each Testcase Object to act as a Command object [GOF]. This allows us to build collections of these Testcase Objects, which we can then iterate across to do counting, running, displaying, and other operations.\n\nIn most programming languages, we need to create a class to deﬁ ne the behavior of the Testcase Objects. We could create a separate Testcase Class for each Testcase Object. It is more convenient to host many Test Methods on a single Testcase Class, however, as this strategy results in fewer classes to manage and facilitates reuse of Test Utility Methods (page 599). This approach requires that each Testcase Object of the Testcase Class have a way to deter- mine which Test Method it should invoke. Pluggable Behavior [SBPP] is the most common way to do this. The constructor of the Testcase Class takes the name of the method to be invoked as a parameter and stores this name in an instance variable. When the Test Runner invokes the run method on the Test- case Object, it uses reﬂ ection to ﬁ nd and invoke the method whose name is in the instance variable.\n\nwww.it-ebooks.info\n\n383\n\nTestcase Object\n\n384\n\nTestcase Object\n\nChapter 19 xUnit Basics Patterns\n\nThere’s Always an Exception\n\nWhether we are learning to conjugate verbs in a new language or looking for patterns in how software is built, there’s always an exception!\n\nOne of the most notable exceptions in the xUnit family relates to the use of a Testcase Object (page 382) to represent each Test Meth- od (page 348) at runtime. This key design feature of xUnit offers a way to achieve an Independent Test (see page 42). The only members of the xUnit family that don’t follow this scheme are TestNG and NUnit (version 2.x). For the reasons described below, the builders of NUnit 2.0 chose to stray from the well-worn path of one Testcase Object per Test Method and create only a single instance of the Test- case Class (page 373). This instance, which they call the test ﬁ xture, is then reused for each Test Method. One of the authors of NUnit 2.0, James Newkirk, writes:\n\nI think one of the biggest screw-ups that was made when we wrote NUnit V2.0 was to not create a new instance of the test ﬁ xture class for each contained test method. I say “we” but I think this one was my fault. I did not quite understand the reasoning in JUnit for cre- ating a new instance of the test ﬁ xture for each test method. I look back now and see that reusing the instance for each test method allows someone to store a member variable from one test and use it in another. This can introduce execution-order dependencies, which for this type of testing is an anti-pattern. It is much better to fully isolate each test method from the others. This requires that a new object be created for each test method.\n\nUnfortunately, this has some very interesting—and undesirable— consequences when one is familiar with the “JUnit New Instance Behav- ior” of a separate Testcase Object per method. Because the object is reused, any objects it refers to via an instance variable are available to all subse- quent tests. This results in an implicit Shared Fixture (page 317) along with all the forms of Erratic Tests (page 228) that go with it. James goes on to say:\n\nSince it would be difﬁ cult to change the way that NUnit works now, and too many people would complain, I now make all of the mem- ber variables in test ﬁ xture classes static. It’s almost like truth in advertising. The result is that there is only one instance of this variable, no matter how many test ﬁ xture objects are created. If the variable is static, then someone who may not be familiar with\n\nwww.it-ebooks.info\n\nTestcase Object\n\nhow NUnit executes would not assume that a new one is created before each test is executed. This is the closest I can get to how JUnit works without changing the way that NUnit executes test methods.\n\nMartin Fowler felt this exception was important enough that he wrote an article about why JUnit’s approach is correct. See http://martinfowler. com/bliki/JunitNewInstance.html.\n\nExample: Testcase Object\n\nThe main evidence of the existence of Testcase Objects appears in the Test Tree Explorer (see Test Runner) when we “drill down” into the Test Suite Object to expose the Testcase Objects it contains. Let’s look at an example from the JUnit Graphical Test Runner that is built into Eclipse. Here’s the list of objects created from the sample code from the write-up of Testcase Class:\n\nTestSuite(\"...ﬂightstate.featuretests.AllTests\") TestSuite(\"...ﬂightstate.featuretests.TestApproveFlight\") TestApproveFlight(\"testScheduledState_shouldThrowIn..ReEx\") TestApproveFlight(\"testUnsheduled_shouldEndUpInAwai..oval\") TestApproveFlight(\"testAwaitingApproval_shouldThrow..stEx\") TestApproveFlight(\"testWithNullArgument_shouldThrow..ntEx\") TestApproveFlight(\"testWithInvalidApprover_shouldTh..ntEx\") TestSuite(\"...ﬂightstate.featuretests.TestDescheduleFlight\") TestDescheduleFlight(\"testScheduled_shouldEndUpInSc..tate\") TestDescheduleFlight(\"testUnscheduled_shouldThrowIn..stEx\") TestDescheduleFlight(\"testAwaitingApproval_shouldTh..stEx\") TestSuite(\"...ﬂightstate.featuretests.TestRequestApproval\") TestRequestApproval(\"testScheduledState_shouldThrow..stEx\") TestRequestApproval(\"testUnsheduledState_shouldEndU..oval\") TestRequestApproval(\"testAwaitingApprovalState_shou..stEx\") TestSuite(\"...ﬂightstate.featuretests.TestScheduleFlight\") TestScheduleFlight(\"testUnscheduled_shouldEndUpInSc..uled\") TestScheduleFlight(\"testScheduledState_shouldThrowI..stEx\") TestScheduleFlight(\"testAwaitingApproval_shouldThro..stEx\")\n\nThe name outside the parentheses is the name of the class; the string inside the parentheses is the name of the object created from that class. By convention, the name of the Test Method4 to be run is used as the name of the Testcase Object, and the name of a Test Suite Object is whatever string was passed to the Test Suite Object constructor. In this example we’ve used the full package and class- name of the Testcase Class.\n\n4 I replaced part of the name with “..” to keep each line within the page width limit.\n\nwww.it-ebooks.info\n\n385\n\nTestcase Object\n\n386\n\nTestcase Object\n\nChapter 19 xUnit Basics Patterns\n\nThis is what this scheme might look like when viewed in a Test Tree Explorer:\n\nwww.it-ebooks.info\n\nTest Suite Object\n\nTest Suite Object\n\nHow do we run the tests when we have many tests to run?\n\nWe deﬁ ne a collection class that implements the standard test interface and use it to run a set of related Testcase Objects.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nRun Run\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nRun Run\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nTestcase Testcase Class Class\n\nCreate Create\n\nGiven that we have created Test Methods (page 348) containing our test logic and placed them on a Testcase Class (page 373) so we can construct a Testcase Object (page 382) for each test, it would be nice to be able to run these tests as a single user operation.\n\nHow It Works\n\nWe deﬁ ne a Composite [GOF] Testcase Object called a Test Suite Object to hold the collection of individual Testcase Objects to execute. When we want to run all tests in the test suite at once, the Test Runner (page 377) asks the Test Suite Object to run all its tests.\n\nWhy We Do This\n\nTreating test suites as ﬁ rst-class objects makes it easier for the Test Runner of the Test Automation Framework (page 298) to manipulate tests in the test suite. With or without a Test Suite Object, the Test Runner would have to hold some kind of collection of Testcase Objects (so that we could iterate over them, count them, and so on). When we make the collection “smart,” it becomes a simple matter to add other uses such as the Suite of Suites.\n\nwww.it-ebooks.info\n\n387\n\nTest Suite Object\n\n388\n\nTest Suite Object\n\nChapter 19 xUnit Basics Patterns\n\nVariation: Testcase Class Suite\n\nTo run all the Test Methods in a single Testcase Class, we simply build a Test Suite Object for the Testcase Class and add one Testcase Object for each Test Method. This allows us to run all the Test Methods in the Testcase Class simply by passing the name of the Testcase Class to the Test Runner.\n\nVariation: Suite of Suites\n\nWe can build up larger Named Test Suites (page 592) by organizing smaller test suites into a tree structure. The Composite pattern makes this organization invisible to the Test Runner, allowing it to treat a Suite of Suites exactly the same way it treats a simple Testcase Class Suite or a single Testcase Object.\n\nImplementation Notes\n\nAs a Composite object, each Test Suite Object implements the same interface as a simple Testcase Object. Thus neither the Test Runner nor the Test Suite Object needs to be aware of whether it is holding a reference to a single test or an entire suite. This makes it easier to implement any operations that involve iterating across all the tests such as counting, running, and displaying.\n\nBefore we can do anything with our Test Suite Object, we must construct it.\n\nWe can choose from several options to do so:\n\nTest Discovery (page 393): We can let the Test Automation Framework\n\ndiscover our Testcase Classes and Test Methods for us.\n\nTest Enumeration (page 399): We can write code that enumerates which Test Methods we want to include in a Test Suite Object. This usually involves creating a Test Suite Factory (see Test Enumeration).\n\nTest Selection (page 403): We can specify which subset of the Testcase\n\nObjects we want to include from an existing Test Suite Object.\n\nVariation: Test Suite Procedure\n\nSometimes we have to write code in programming or scripting languages that do not support objects. Given that we have written a number of Test Methods, we need to give the Test Runner some way to ﬁ nd the tests. A Test Suite Procedure allows us to enumerate all the tests we want to run by invoking each test in turn. The calls to each test are hard-coded within the body of the Test Suite Object. Of course, a Test Suite Procedure may call several other Test Suite Procedures to realize a Suite of Suites.\n\nwww.it-ebooks.info\n\nTest Suite Object\n\nThe major disadvantage of this approach is that it forces us into Test Enumeration, which increases both the effort required to write tests and the likelihood of Lost Tests (see Production Bugs on page 268). Because we do not treat our code as “data,” we lose the ability to manipulate the code at runtime. As a consequence, it is more difﬁ cult to build a Graphical Test Runner (see Test Runner) with a hierarchy (tree) view of our Suite of Suites.\n\nExample: Test Suite Object\n\nMost members of the xUnit family implement Test Discovery, so there isn’t much of an example of Test Suite Object to see. The main evidence of the existence of Test Suite Objects appears in the Test Tree Explorer (see Test Runner) when we “drill down” into the Test Suite Object to expose the Testcase Objects it contains. Here’s an example from the JUnit Graphical Test Runner built into Eclipse:\n\nExample: Suite of Suites Built Using Test Enumeration\n\nHere is an example of using Test Enumeration to construct a Suite of Suites:\n\npublic class AllTests {\n\npublic static Test suite() {\n\nwww.it-ebooks.info\n\n389\n\nTest Suite Object\n\n390\n\nTest Suite Object\n\nChapter 19 xUnit Basics Patterns\n\nTestSuite suite = new TestSuite(\"Test for allJunitTests\"); suite.addTestSuite( com.clrstream.camug.example.test.InvoiceTest.class); suite.addTest(com.clrstream.ex7.test.AllTests.suite()); suite.addTest(com.clrstream.ex8.test.AllTests.suite()); suite.addTestSuite(com.xunitpatterns.guardassertion.Example.class); return suite; } }\n\nThe ﬁ rst and last lines add the Test Suite Objects created from a single Testcase Class. Each of the middle two lines calls the Test Suite Factory for another Suite of Suites. The Test Suite Object we return is likely at least three levels deep:\n\n1. The Test Suite Object we instantiated and populated before returning\n\n2. The AllTests Test Suite Objects returned by the two calls to factory methods\n\n3. The Test Suite Objects for each of the Testcase Classes aggregated into\n\nthose Test Suite Objects\n\nThis is illustrated in the following tree of objects:\n\nTestSuite(\"Test for allJunitTests\"); TestSuite(\"com.clrstream.camug.example.test.InvoiceTest\") TestCase(\"testInvoice_addLineItem\") ... TestCase(\"testRemoveLineItemsForProduct_oneOfTwo\") TestSuite(\"com.clrstream.ex7.test.AllTests\") TestSuite(\"com.clrstream.ex7.test.TimeDisplayTest\") TestCase(\"testDisplayCurrentTime_AtMidnight\") TestCase(\"testDisplayCurrentTime_AtOneMinAfterMidnight\") TestCase(\"testDisplayCurrentTime_AtOneMinuteBeforeNoon\") TestCase(\"testDisplayCurrentTime_AtNoon\") ... TestSuite(\"com.clrstream.ex7.test.TimeDisplaySolutionTest\") TestCase(\"testDisplayCurrentTime_AtMidnight\") TestCase(\"testDisplayCurrentTime_AtOneMinAfterMidnight\") TestCase(\"testDisplayCurrentTime_AtOneMinuteBeforeNoon\") TestCase(\"testDisplayCurrentTime_AtNoon\") ... TestSuite(\"com.clrstream.ex8.test.AllTests\") TestSuite(\"com.clrstream.ex8.FlightMgntFacadeTest\") TestCase(\"testAddFlight\") TestCase(\"testAddFlightLogging\") TestCase(\"testRemoveFlight\") TestCase(\"testRemoveFlightLogging\") ... TestSuite(\"com.xunitpatterns.guardassertion.Example\") TestCase(\"testWithConditionals\") TestCase(\"testWithoutConditionals\") ...\n\nwww.it-ebooks.info\n\nTest Suite Object\n\nNote that this class doesn’t subclass any other class. It does need to import TestSuite and the classes it is using as Test Suite Factories.\n\nExample: Test Suite Procedure\n\nIn the early days of agile software development, before any agile project manage- ment tools were available, I built a set of Excel spreadsheets for managing tasks and user stories. To make life simpler, I automated frequently performed tasks such as sorting all stories by release and iteration, sorting tasks by iteration and status, and so on. Eventually, I got bold enough to write a macro (a program, really) that would sum up the estimated and actual effort of all tasks for each story. At this point, the code was becoming somewhat complex and was more challenging to maintain. In particular, if one of the named ranges used by the sorting macros was accidentally deleted, the macro would produce an error.\n\nUnfortunately, there was no xUnit framework for VBA at the time, so all of this work was done without Tests as Safety Net (see page 24). Here is the main program of the reporting macro. All output was written to a new sheet in the workbook.\n\n'Main Macro\n\nSub summarizeActivities() Call VerifyVersionCompatability Call initialize Call SortByActivity\n\nFor row = ﬁrstTaskDataRow To lastTaskDataRow If numberOfNumberlessTasks < MaxNumberlessTasks Then thisActivity = ActiveSheet.Cells(row, TaskActivityColumn).Value\n\nIf thisActivity <> currentActivity Then Call ﬁnalizeCurrentActivityTotals currentActivity = thisActivity Call initializeCurrentActivityTotals End If\n\nCall accumulateActivityTotals(row) Else lastTaskDataRow = row ' end the For loop right away End If Next row Call cleanUp End Sub\n\nwww.it-ebooks.info\n\n391\n\nTest Suite Object\n\n392\n\nTest Suite Object\n\nChapter 19 xUnit Basics Patterns\n\nWithout any tests or Test Automation Framework, I had to do what I could to introduce some kind of regression testing. In this case, it was enough of a challenge (and a win) just to be able to exercise all the macros. If they ran to completion, it was a much better indication that I hadn’t broken anything major than not running the macros at all. Because VBA is based on Visual Basic 5, it has no classes. Thus we have no Testcase Class and no runtime Testcase Objects. The following is an example of the various Test Suite Procedures and the Test Methods my tests called:\n\nSub TestAll() Call TestAllStoryMacros Call TestAllTaskMacros Call TestReportingMacros Call TestToolbarMenus 'All The Same End Sub\n\nSub TestAllStoryMacros() Call TestActivitySorting Call TestStoryHiding Call ReportSuccess(\"All Story Macros\") End Sub\n\nSub TestActivitySorting() Call SortStoriesbyAreaAndNumber Call SortActivitiesByIteration Call SortActivitiesByIterationAndOrder Call SortActivitiesByNumber Call SortActivitiesByPercentDone End Sub\n\nSub TestReportingMacros() Call summarizeActivities End Sub\n\nThe ﬁ rst Test Suite Procedure is a Suite of Suites; the second Test Suite Procedure is the equivalent of a single Test Suite Object. The third Sub is the Test Method for exercising all of the sorting macros. The last Sub exercises the summarizeActivities macro using a Prebuilt Fixture (page 429). 5\n\n5 For those who might be wondering what happened to the verify outcome phase of the test, there isn’t one in this test. It is neither a Self-Checking Test nor a Single-Condition Test. Shame on me!\n\nwww.it-ebooks.info\n\nTest Discovery\n\nTest Discovery\n\nHow does the Test Runner know which tests to run?\n\nThe Test Automation Framework discovers all tests that belong to the test suite automatically.\n\nTest Test Discovery Discovery Mechanism Mechanism\n\nCreate Create\n\ntestMethod_1 testMethod_1\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nCreate Create\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nGiven that we have written a number of Test Methods (page 348) on one or more Testcase Classes (page 373), we need to give the Test Runner (page 377) some way to ﬁ nd the tests. Test Discovery eliminates most of the hassles associated with Test Enumeration (page 399).\n\nHow It Works\n\nThe Test Automation Framework (page 298) uses runtime reﬂ ection (or com- pile-time knowledge) to discover all Test Methods that belong to the test suite and/or all Test Suite Objects (page 387) that belong to a Suite of Suites (see Test Suite Object). It then builds up the Test Suite Objects containing the corresponding Testcase Objects (page 382) and other Test Suite Objects in preparation for running all the tests.\n\nWhen to Use It\n\nWe should use Test Discovery whenever our Test Automation Framework supports it. This pattern reduces the effort required to automate tests and greatly reduces the possibility of Lost Tests (see Production Bugs on page 268). The\n\nwww.it-ebooks.info\n\n393\n\nTest Discovery\n\n394\n\nTest Discovery\n\nChapter 19 xUnit Basics Patterns\n\nonly times to consider using Test Enumeration are (1) when our framework does not support Test Discovery and (2) when we wish to deﬁ ne a Named Test Suite (page 592) that consists of a subset of tests6 chosen from other test suites and the Test Automation Framework does not support Test Selection (page 403). It is not uncommon to combine Test Suite Enumeration (see Test Enumeration) with Test Method Discovery; the reverse is less common.\n\nImplementation Notes\n\nBuilding the Suite of Suites to be executed by the Test Runner involves two steps. First, we must ﬁ nd all Test Methods to be included in each Test Suite Object. Second, we must ﬁ nd all Test Suite Objects to be included in the test run, albeit not necessarily in this order. Each of these steps may be done manually via Test Method Enumeration (see Test Enumeration) and Test Suite Enumeration or automatically via Test Method Discovery and Testcase Class Discovery.\n\nVariation: Testcase Class Discovery\n\nTestcase Class Discovery is the process by which the Test Automation Frame- work discovers the Testcase Classes on which it should do Test Method Dis- covery. One solution involves tagging each Testcase Class by subclassing a Testcase Superclass (page 638) or implementing a Marker Interface [PJV1]. Another alternative, used in the .NET languages and newer versions of JUnit, is to use a class attribute (e.g., \"[Test Fixture]\") or annotation (e.g., \"@Testcase\") to identify each Testcase Class. Yet another solution is to put all Testcase Classes into a common directory and point the Test Runner or some other program at this directory. A fourth solution is to follow a Testcase Class naming convention and use an external program to ﬁ nd all ﬁ les matching this naming pattern. Whichever way we choose to perform this task, once a Testcase Class has been discovered we can proceed to either Test Method Discovery or Test Method Enumeration.\n\nVariation: Test Method Discovery\n\nTest Method Discovery involves providing a way for the Test Automation Frame- work to discover the Test Methods in our Testcase Classes. There are two basic ways to indicate that a method of a Testcase Class is a Test Method. The more traditional approach is to use a Test Method naming convention such as “starts with ‘test’.” The Test Automation Framework then iterates over all methods of the Testcase Class, selects those that start with the string “test” (e.g., testCounters),\n\n6 A Smoke Test [SCM] suite is a good example.\n\nwww.it-ebooks.info\n\nTest Discovery\n\nand calls the one-argument constructor to create the Testcase Object for that Test Method. The other alternative, which is used in the .NET languages and newer versions of JUnit, is to use a method attribute (e.g., “[Test]”) or annotation (e.g., “@Test”) to identify each Test Method.\n\nMotivating Example\n\nThe following example illustrates the kind of code that would be required for each Test Method to do Test Method Enumeration if we did not have Test Discovery available:\n\npublic: static CppUnit::Test *suite() { CppUnit::TestSuite *suite = new CppUnit::TestSuite( \"ComplexNumberTest\" ); suite>addTest( new CppUnit::TestCaller<ComplexNumberTest>( \"testEquality\", &ComplexNumberTest::testEquality ) ); suite>addTest( new CppUnit::TestCaller<ComplexNumberTest>( \"testAddition\", &ComplexNumberTest::testAddition ) ); return suite; }\n\nThis example is from the tutorial for an earlier version of CppUnit. Newer versions no longer require this approach.\n\nRefactoring Notes\n\nLuckily for the users of existing xUnit family members, the inventors of xUnit realized the importance of Test Discovery. Therefore all we have to do is follow their advice on how to identify our test methods. If the developers of our xUnit version used a naming convention, we may have to do a Rename Method [Fowler] refactoring to get xUnit to discover our Test Method. If they implemented method attributes, we just add the appropriate attribute to our Test Methods.\n\nExample: Test Method Discovery (Using Method Naming and Compiler Macro)\n\nWhen the programming language is capable of managing the tests as objects and invoking the methods but cannot easily ﬁ nd all methods to use as tests, we\n\nwww.it-ebooks.info\n\n395\n\nTest Discovery\n\n396\n\nTest Discovery\n\nChapter 19 xUnit Basics Patterns\n\nmay need to give it a small push as encouragement to do so. Newer versions of CppUnit provide a macro that ﬁ nds all Test Methods at compile time and generates the code to build the test suite as illustrated in the previous example. The following code snippet triggers the Test Method Discovery:\n\nCPPUNIT_TEST_SUITE_REGISTRATION( FlightManagementFacadeTest );\n\nThis macro uses a method naming convention to determine which methods (“member functions”) it should turn into Testcase Objects by wrapping each with a TestCaller, much like in the manual example we saw earlier.\n\nExample: Test Method Discovery (Using Method Naming)\n\nThe following examples are more notable for the code that is missing than for the code that is present. Note that there is no code to add the Test Methods to the Test Suite Object.\n\nIn this Java example, the framework automatically runs all test methods that\n\nstart with “test” and have no arguments (a total of two):\n\npublic class TimeDisplayTest extends TestCase { public void testDisplayCurrentTime_AtMidnight() throws Exception { // Set up SUT TimeDisplay theTimeDisplay = new TimeDisplay(); // Exercise SUT String actualTimeString = theTimeDisplay.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( \"Midnight\", expectedTimeString, actualTimeString); }\n\npublic void testDisplayCurrentTime_AtOneMinuteAfterMidnight() throws Exception { // Set up SUT TimeDisplay actualTimeDisplay = new TimeDisplay(); // Exercise SUT String actualTimeString = actualTimeDisplay.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">12:01 AM</span>\"; assertEquals( \"12:01 AM\", expectedTimeString, actualTimeString); } }\n\nwww.it-ebooks.info\n\nTest Discovery\n\nExample: Test Method Discovery (Using Method Attributes)\n\nIn this C# example, the tests are labeled with the method attribute [Test]. Both CsUnit and NUnit use this way of identifying Test Methods.\n\n[Test] public void testFlightMileage_asKm() { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); int expectedKilometres = 1810; // verify results Assert.AreEqual( expectedKilometres, actualKilometres); }\n\n[Test] [ExpectedException(typeof(InvalidArgumentException))] public void testSetMileage_invalidInput_attribute() { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); // exercise SUT newFlight.setMileage(-1122); }\n\nExample: Testcase Class Discovery (Using Class Attributes)\n\nHere is an example of using a class attribute to identify a Testcase Class (called a “Test Fixture” in NUnit) to the Test Runner:\n\n[TestFixture] public class SampleTestcase {\n\n}\n\nExample: Testcase Class Discovery (Using Common Location and Testcase Superclass)\n\nThe following Ruby example ﬁ nds all ﬁ les with the .rb extension in the “tests” directory and requires them from this ﬁ le. This causes Test::Unit to look for all tests in each ﬁ le because the Testcase Class in each ﬁ le extends Test::Unit::TestCase.\n\nDir['tests/*.rb'].each do |each| require each end\n\nwww.it-ebooks.info\n\n397\n\nTest Discovery\n\n398\n\nTest Discovery\n\nChapter 19 xUnit Basics Patterns\n\nThe Dir['tests/*.rb'] returns a collection of ﬁ les over which the each method iter- ates with the block containing “require each” to implement Testcase Class Dis- covery. The Ruby interpreter and Test::Unit ﬁ nish the job by doing Test Method Discovery on each required class.\n\nwww.it-ebooks.info\n\nTest Enumeration\n\nTest Enumeration\n\nHow does the Test Runner know which tests to run?\n\nThe test automater manually writes the code that enumerates all tests that belong to the test suite.\n\nTest Test Suite Suite Factory Factory\n\nCreate Create\n\ntestMethod_1 testMethod_1\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\nSuite Suite\n\nTest Runner Test Runner\n\nCreate Create\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nGiven that we have written a number of Test Methods (page 348) on one or more Testcase Classes (page 373), we need to give the Test Runner (page 377) some way to ﬁ nd the tests. Test Enumeration is the way we do so when we lack support for Test Discovery (page 393).\n\nHow It Works\n\nThe test automater manually writes the code that enumerates all Test Methods that belong to the test suite and/or all Test Suite Objects (page 387) that belong to a Suite of Suites (see Test Suite Object). This is typically done by implement- ing the method suite either on a Testcase Class for Test Method Enumeration or on a Test Suite Factory for Test Suite Enumeration.\n\nWhen to Use It\n\nWe need to use Test Enumeration if our Test Automation Framework (page 298) does not support Test Discovery. We can also choose to use Test Enumeration\n\nwww.it-ebooks.info\n\n399\n\nTest Enumeration\n\nAlso known as: Test Suite Factory\n\n400\n\nTest Enumeration\n\nChapter 19 xUnit Basics Patterns\n\nwhen we wish to deﬁ ne a Named Test Suite (page 592) that consists of a subset of tests7 chosen from other test suites and the framework does not support Test Selection (page 403).\n\nMany members of the xUnit family support Test Discovery at the Test Method\n\nlevel but force us to use Test Enumeration at the Testcase Class level.\n\nImplementation Notes\n\nBuilding the Suite of Suites to be executed by the Test Runner involves two steps. First, we must ﬁ nd all Test Methods to be included in each Test Suite Object. Second, we must ﬁ nd all Test Suite Objects to be included in the test run, albeit not necessarily in this order. Each of these steps may be done manually via Test Method Enumeration and Test Suite Enumeration or automatically via Test Method Discovery (see Test Discovery) and Testcase Class Discovery (see Test Discovery). When done manually, we typically use a “Test Suite Factory” that returns the Test Suite Object.\n\nVariation: Test Suite Enumeration\n\nMany members of the xUnit family require that we provide a Test Suite Factory that builds the top-level Suite of Suites (often called “AllTests”) as means to specify which Test Suite Objects we would like to include in a test run. We do so by providing a class method on a factory class; this Factory Method [GOF] is called suite in most members of the xUnit family. Inside the suite method we use calls to methods such as addTest to add each nested Test Suite Object to the suite we are building.\n\nAlthough this approach is fairly ﬂ exible, it can result in Lost Tests (see Production Bugs on page 268). The alternative is to let the development tools build the AllTests Suite (see Named Test Suite) automatically or to use a Test Runner that ﬁ nds all test suites in a ﬁ le system directory automatically. For example, NUnit provides a built-in mechanism that implements Testcase Class Discovery at the assembly level. We can also use third-party tools such as Ant to ﬁ nd all Testcase Class ﬁ les in a directory structure.\n\nEven in statically typed languages such as Java, the Test Suite Factory (see Test Enumeration on page 399) does not need to subclass a speciﬁ c class or implement a speciﬁ c interface. Instead, the only dependencies are on the generic Test Suite Object class it returns and the Testcase Classes or Test Suite Factories it asks for the nested suites.\n\n7 A Smoke Test [SCM] suite is a good example.\n\nwww.it-ebooks.info\n\nTest Enumeration\n\nVariation: Test Method Enumeration\n\nMany members of the xUnit family now support Test Method Discovery. If we happen to be using a version that does not, we need to ﬁ nd all Test Methods in a Testcase Class, turn them into Testcase Objects (page 382), and put them into a Test Suite Object. We implement Test Method Enumeration by providing a class method, typically called suite, on the Testcase Class itself.\n\nThe capability to construct an object that calls an arbitrary method is often in- herited from the Test Automation Framework via a Testcase Superclass (page 638) or mixed in via a class attribute or Include directive. In some members of the xUnit family, this Pluggable Behavior [SBPP] capability is provided by a separate class (see the CppUnit example below).\n\nVariation: Direct Test Method Invocation\n\nIn the pure procedural world where we cannot treat a Test Method as an object or data item, we have no choice but to hand-code a Test Suite Procedure (see Test Suite Object) for each test suite. This procedure then calls each Test Method (or other Test Suite Procedures) one by one.\n\nExample: Test Method Enumeration in CppUnit\n\nEarly versions of most xUnit family members required that the test automater add each Test Method manually. Those versions that cannot use reﬂ ection still have this requirement. Here is an example from an older version of CppUnit that uses this approach:\n\npublic: static CppUnit::Test *suite() { CppUnit::TestSuite *suite = new CppUnit::TestSuite( \"ComplexNumberTest\" ); suite>addTest( new CppUnit::TestCaller<ComplexNumberTest>( \"testEquality\", &ComplexNumberTest::testEquality ) ); suite>addTest( new CppUnit::TestCaller<ComplexNumberTest>( \"testAddition\", &ComplexNumberTest::testAddition ) ); return suite; }\n\nThis example also illustrates how CppUnit wraps each Test Method with an instance of a class (TestCaller) to turn it into a Testcase Object.\n\nwww.it-ebooks.info\n\n401\n\nTest Enumeration\n\n402\n\nTest Enumeration\n\nChapter 19 xUnit Basics Patterns\n\nExample: Test Method Invocation (Hard-Coded)\n\nThe following example is from a test suite for a program written in VBA (Visual Basic for Applications, the macro language used in Microsoft Ofﬁ ce products), which lacks support for objects:\n\nSub TestAllStoryMacros() Call TestActivitySorting Call TestStoryHiding Call ReportSuccess(\"All Story Macros\") End Sub\n\nExample: Test Suite Enumeration\n\nWe can use Test Suite Enumeration when the Test Automation Framework does not support Test Discovery or when we want to deﬁ ne a Named Test Suite that includes only a subset of the tests.\n\nThe main drawback of using Test Suite Enumeration for running all tests is the potential for Lost Tests if we forget to include a new test suite in the AllTests Suite. This risk can be reduced by paying attention to the number of tests that were run when we ﬁ rst checked out the code and ensuring that the number run just before check-in goes up by the number of new tests we added.\n\npublic class AllTests {\n\npublic static Test suite() { TestSuite suite = new TestSuite(\"Test for allJunitTests\"); //$JUnit-BEGIN$ suite.addTestSuite( com.clrstream.camug.example.test.InvoiceTest.class); suite.addTest(com.clrstream.ex7.test.AllTests.suite()); suite.addTest(com.clrstream.ex8.test.AllTests.suite()); suite.addTestSuite( com.xunitpatterns.guardassertion.Example.class); //$JUnit-END$ return suite; } }\n\nIn this example, we take advantage of the IDE’s ability to (re)generate the AllTests suite for us. (Eclipse will regenerate the code between the two marker comments whenever we request it to do so.) We still need to remember to regenerate the suite occasionally, but this approach goes a long way toward avoiding Lost Tests in the absence of Test Discovery.\n\nwww.it-ebooks.info\n\nTest Selection\n\nTest Selection\n\nHow does the Test Runner know which tests to run?\n\nThe Test Automation Framework selects the Test Methods to be run at runtime based on attributes of the tests.\n\nTest Test Selection Selection Mechanism Mechanism\n\nTest Runner Test Runner\n\nGet Get\n\nCreate Create\n\nAdd Add\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nRun Run\n\nSubset Subset Suite Suite Object Object\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nGiven that we have written a number of Test Methods (page 348) on one or more Testcase Classes (page 373), we need to give the Test Runner (page 377) some way to ﬁ nd those tests. Test Selection is a way to pick subsets of tests dynamically.\n\nHow It Works\n\nThe test automater speciﬁ es the subset of tests to be run when invoking the Test Runner by providing test selection criteria. These selection criteria may be based on implicit or explicit attributes of the Testcase Classes or Test Methods.\n\nWhen to Use It\n\nWe should use Test Selection when we wish to run a subset of tests chosen from other test suites and we do not want to maintain a separate structure built using Test Enumeration (page 399). A Smoke Test [SCM] suite is a common usage; see Named Test Suite (page 592) for other uses.\n\nwww.it-ebooks.info\n\n403\n\nTest Selection\n\n404\n\nTest Selection\n\nChapter 19 xUnit Basics Patterns\n\nImplementation Notes\n\nTest Selection can be implemented either by creating a Subset Suite (see Named Test Suite) from an existing Test Suite Object (page 387) or by skip- ping some of the tests within the Test Suite Object as we execute the Testcase Objects (page 382) it contains.\n\nAs with Test Discovery (page 393) and Test Enumeration, Test Selection can be applied at two different levels: selecting Testcase Classes or selecting Test Methods. Test Selection can be built into the Test Automation Frame- work (page 298) or it can be implemented more crudely as part of the build task.\n\nVariation: Testcase Class Selection\n\nWe can select the Testcase Classes to be examined for Test Methods in several ways. The crudest way to do Testcase Class Selection is simply to place the Test- case Classes into test packages based on some criteria. Unfortunately, this strategy works only for a single test classiﬁ cation scheme and is likely to reduce the value of Tests as Documentation (see page 23). A somewhat more ﬂ exible approach is to use a naming convention such as “contains ‘WebServer’” to select only those classes that verify the behavior of certain parts of the system. This, too, is some- what constrained in its utility.\n\nThe most ﬂ exible way to implement Test Selection is within the Test Auto- mation Framework. We can use class attributes (.NET) or annotations (Java) to indicate characteristics of the Testcase Class. The same technique can also be applied at the Test Method level.\n\nVariation: Test Method Selection\n\nWhen implemented as part of the Test Automation Framework, Test Method Selection can be done by specifying the “category” (or categories) to which a Test Method belongs. This usually requires language support for method attributes (.NET) or annotations (Java). It could also be based on a method name scheme, although this approach is not as ﬂ exible and would require tighter coupling to the Test Runner.\n\nExample: Testcase Class Selection Using Class Attributes\n\nThe following example of Testcase Class Selection is from NUnit. The class attribute Category(“FastSuite”) indicates that all tests in this Testcase Class should be included (or excluded) when the category “FastSuite” is speciﬁ ed in the Test Runner.\n\nwww.it-ebooks.info\n\nTest Selection\n\n[TestFixture] [Category(\"FastSuite\")] public class CategorizedTests { [Test] public void testFlightConstructor_OK() // Methods omitted }\n\nExample: Test Method Selection Using Method Attributes\n\nThis example of Test Method Selection is from NUnit. The method attribute Category(“SmokeTest”) indicates that this Test Method should be included (or excluded) when the category “SmokeTest” is speciﬁ ed in the Test Runner.\n\n[Test] [Category(\"SmokeTests\")] public void testFlightMileage_asKm() { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); int expectedKilometres = 1810; // verify results Assert.AreEqual( expectedKilometres, actualKilometres); }\n\nwww.it-ebooks.info\n\n405\n\nTest Selection\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nChapter 20\n\nFixture Setup Patterns\n\nPatterns in This Chapter\n\nFresh Fixture Setup\n\nIn-line Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n\nDelegated Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\n\nCreation Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n\nImplicit Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424\n\nShared Fixture Construction\n\nPrebuilt Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\n\nLazy Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435\n\nSuite Fixture Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\n\nSetup Decorator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447\n\nChained Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454\n\n407\n\nwww.it-ebooks.info\n\nFixture Setup Patterns",
      "page_number": 411
    },
    {
      "number": 20,
      "title": "Fixture Setup Patterns",
      "start_page": 471,
      "end_page": 524,
      "detection_method": "regex_chapter",
      "content": "408\n\nIn-line Setup\n\nChapter 20 Fixture Setup Patterns\n\nIn-line Setup\n\nHow do we construct the Fresh Fixture?\n\nEach Test Method creates its own Fresh Fixture by calling the appropriate constructor methods to build exactly the test ﬁ xture it requires.\n\nsetUp setUp\n\nSetup Setup\n\nFixture Fixture\n\ntest_1 test_1\n\ntest_2 test_2\n\nSUT SUT\n\ntest_n test_n\n\nTestcase Class Testcase Class\n\nTo execute an automated test, we require a text ﬁ xture that is well understood and completely deterministic. We can use the Fresh Fixture (page 311) approach to build a Minimal Fixture (page 302) for the use of this one test. Setting up the test ﬁ xture on an in-line basis in each test is the most obvious way to build it.\n\nHow It Works\n\nEach Test Method (page 348) sets up its own test ﬁ xture by directly calling what- ever SUT code is required to construct exactly the test ﬁ xture it requires. We put the code that creates the ﬁ xture, the ﬁ rst phase of the Four-Phase Test (page 358), at the top of each Test Method.\n\nwww.it-ebooks.info\n\nIn-line Setup\n\nWhen to Use It\n\nWe can use In-line Setup when the ﬁ xture setup logic is very simple and straightforward. As soon as the ﬁ xture setup gets at all complex, we should consider using Delegated Setup (page 411) or Implicit Setup (page 424) for part or all of the ﬁ xture setup.\n\nWe can also use In-line Setup when we are writing a ﬁ rst draft of tests and haven’t yet ﬁ gured out which part of the ﬁ xture setup will be repeated from test to test. This is an example of applying the “Red–Green–Refactor” process pattern to the tests themselves. Nevertheless, we need to be careful when we refactor the tests to ensure that we don’t break the tests in ways that are undetectable.\n\nA third occasion to use In-line Setup is when refactoring obtuse ﬁ xture setup code. A ﬁ rst step may be to use In-line Method [Fowler] refactorings on all Creation Methods (page 415) and the setUp method. Then we can try using a series of Extract Method [Fowler] refactorings to deﬁ ne a new set of Creation Methods that are more intent-revealing and reusable.\n\nImplementation Notes\n\nIn practice, most ﬁ xture setup logic will include a mix of styles, such as In-line Setup building on top of Implicit Setup or Delegated Setup interspersed with In-line Setup.\n\nExample: In-line Setup\n\nHere’s an example of simple in-line setup. Everything each Test Method needs for exercising the SUT is included in-line.\n\npublic void testStatus_initial() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // tearDown: // garbage-collected }\n\npublic void testStatus_cancelled() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\");\n\nwww.it-ebooks.info\n\n409\n\nIn-line Setup\n\n410\n\nIn-line Setup\n\nChapter 20 Fixture Setup Patterns\n\nFlight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); ﬂight.cancel(); // still part of setup // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // tearDown: // garbage-collected }\n\nRefactoring Notes\n\nIn-line Setup is normally the starting point for refactoring, not the end goal. Sometimes, however, we ﬁ nd ourselves with tests that are too hard to under- stand because of all the stuff happening behind the scenes, which is a form of Mystery Guest (see Obscure Test on page 186). At other times, we may ﬁ nd ourselves modifying the previously setup ﬁ xture in many of the tests.\n\nBoth of these situations are indications it may be time to refactor our test class into multiple classes based on the ﬁ xture they build. First, we use an In- line Method refactoring on the code to produce an In-line Setup. Next, we reorganize the tests using an Extract Class [Fowler] refactoring. Finally, we use a series of Extract Method refactorings to deﬁ ne a more understandable set of ﬁ xture setup methods.\n\nwww.it-ebooks.info\n\nDelegated Setup\n\nDelegated Setup\n\nHow do we construct the Fresh Fixture?\n\nEach Test Method creates its own Fresh Fixture by calling Creation Methods from within the Test Methods.\n\nsetUp setUp\n\nUtility Utility Method Method\n\nSetup Setup\n\nFixture Fixture\n\ntest_1 test_1\n\nUtility Utility Method Method\n\ntest_2 test_2\n\nSUT SUT\n\ntest_n test_n\n\nTestcase Class Testcase Class\n\nTo execute an automated test, we require a text ﬁ xture that is well understood and completely deterministic. We are using a Fresh Fixture (page 311) approach to build a Minimal Fixture (page 302) for the use of this one test and we’d like to avoid Test Code Duplication (page 213).\n\nDelegated Setup lets us reuse the code to set up the ﬁ xture without compromis-\n\ning our goal of Tests as Documentation (see page 23).\n\nHow It Works\n\nEach Test Method (page 348) sets up its own test ﬁ xture by calling one or more Creation Methods (page 415) to construct exactly the test ﬁ xture it requires. To ensure Tests as Documentation, we build a Minimal Fixture using Creation Methods that build fully formed objects that are ready for use by the test. We strive to ensure that the method calls will convey the “big picture” to the test reader by passing in only those values that affect the behavior of the SUT.\n\nwww.it-ebooks.info\n\n411\n\nDelegated Setup\n\n412\n\nDelegated Setup\n\nChapter 20 Fixture Setup Patterns\n\nWhen to Use It\n\nWe can use a Delegated Setup when we want to avoid the Test Code Duplication caused by having to set up similar ﬁ xtures for several tests and we want to keep the nature of the ﬁ xture visible within the Test Methods. A reasonable goal is to encapsulate the essential but irrelevant steps of setting up the ﬁ xture and leave only the steps and values essential to understanding the test within the Test Meth- od. This scheme helps us achieve Tests as Documentation by ensuring that excess In-line Setup (page 408) code does not obscure the intent of the test. It also avoids the Mystery Guest problem (see Obscure Test on page 186) by leaving the Intent- Revealing Name [SBPP] of the Creation Method call within the Test Method.\n\nFurthermore, Delegated Setup allows us to use whatever organization scheme we want for our Test Methods. In particular, we are not forced to put Test Methods that require the same test ﬁ xture into the same Testcase Class (page 373) just to reuse the setUp method as we would have to when using Implicit Setup (page 424). Furthermore, Delegated Setup helps prevent Fragile Tests (page 239) by moving much of the nonessential interaction with the SUT out of the very numerous Test Methods and into a much smaller number of Creation Method bodies, where it is easier to maintain.\n\nImplementation Notes\n\nWith modern refactoring tools, we can often create the ﬁ rst cut of a Creation Method by performing a simple Extract Method [Fowler] refactoring. As we are writing a set of tests using “clone and twiddle,” we must watch for any Test Code Duplication in the ﬁ xture setup logic within our tests. For each object that needs to be veriﬁ ed in the veriﬁ cation logic, we extract a Creation Method that takes only those attributes as parameters that affect the outcome of the test.\n\nInitially, we can leave the Creation Method on our Testcase Class. If we need to share them with another class, however, we can move the Creation Methods to an Abstract Testcase class (see Testcase Superclass on page 638) or a Test Helper (page 643) class.\n\nMotivating Example\n\nSuppose we are testing the state model of the Flight class. In each test, we need to have a ﬂ ight in the right state. Because a ﬂ ight needs to connect at least two airports, we need to create airports before we can create a ﬂ ight. Of course, air- ports are typically associated with cities or states/provinces. To keep the example manageable, let’s assume that our airports require only a city name and an air- port code.\n\nwww.it-ebooks.info\n\nDelegated Setup\n\npublic void testStatus_initial() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight(ﬂightNumber, departureAirport, destinationAirport); // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // teardown // garbage-collected }\n\npublic void testStatus_cancelled() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); ﬂight.cancel(); // still part of setup // Exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // teardown // garbage-collected }\n\nThese tests contain a fair amount of Test Code Duplication.\n\nRefactoring Notes\n\nWe can refactor the ﬁ xture setup logic by using an Extract Method refactoring to remove any frequently repeated code sequences into utility methods with Intent-Revealing Names. We leave the calls to the methods in the test, however, so that the reader can see what is being done. The method calls that remain within the test will convey the “big picture” to the test reader. The utility meth- od bodies contain the irrelevant mechanics of carrying out the intent. If we need to share the Delegated Setups with another Testcase Class, we can use either a Pull Up Method [Fowler] refactoring to move them to a Testcase Superclass or a Move Method [Fowler] refactoring to move them to a Test Helper class.\n\nExample: Delegated Setup\n\nIn this version of the test, we use a method that hides the fact that we need two airports instead of creating the two airports needed by the ﬂ ight within each Test Method. We could produce this version of the tests either through refactoring or by writing the test in this intent-revealing style right off the bat.\n\nwww.it-ebooks.info\n\n413\n\nDelegated Setup\n\n414\n\nDelegated Setup\n\nChapter 20 Fixture Setup Patterns\n\npublic void testGetStatus_initial() { // setup Flight ﬂight = createAnonymousFlight(); // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // teardown // garbage-collected }\n\npublic void testGetStatus_cancelled2() { // setup Flight ﬂight = createAnonymousCancelledFlight(); // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // teardown // garbage-collected }\n\nThe simplicity of these tests was made possible by the following Creation Methods, which hide the “necessary but irrelevant” steps from the test reader:\n\nprivate int uniqueFlightNumber = 2000;\n\npublic Flight createAnonymousFlight(){ Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( new BigDecimal(uniqueFlightNumber++), departureAirport, destinationAirport); return ﬂight; } public Flight createAnonymousCancelledFlight(){ Flight ﬂight = createAnonymousFlight(); ﬂight.cancel(); return ﬂight; }\n\nwww.it-ebooks.info\n\nCreation Method\n\nCreation Method\n\nHow do we construct the Fresh Fixture?\n\nWe set up the test ﬁ xture by calling methods that hide the mechanics of building ready-to-use objects behind Intent-Revealing Names.\n\nSetup Setup\n\nCreation Creation Method Method\n\nFixture Fixture\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nTeardown Teardown\n\nFixture setup usually involves the creation of a number of objects. In many cases, the details of those objects (i.e., the attribute values) are unimportant but must be speciﬁ ed to satisfy each object’s constructor method. Including all of this unnecessary complexity within the ﬁ xture setup part of the test can lead to Obscure Tests (page 186) and certainly doesn’t help us achieve Tests as Docu- mentation (see page 23)!\n\nHow can a properly initialized object be created without having to clutter the test with In-line Setup (page 408)? The answer, of course, is to encapsulate this complexity. Delegated Setup (page 411) moves the mechanics of the ﬁ xture setup into other methods but leaves overall control and coordination within the test itself. But what to delegate to? A Creation Method is one way we can encapsulate the mechanics of object creation so that irrelevant details do not distract the reader.\n\nwww.it-ebooks.info\n\n415\n\nCreation Method\n\n416\n\nCreation Method\n\nChapter 20 Fixture Setup Patterns\n\nHow It Works\n\nAs we write tests, we don’t bother asking whether a desired utility function exists; we just use it! (It helps to pretend that we have a loyal helper sitting next to us who will quickly ﬁ ll in the bodies of any functions we call that do not exist as yet.) We write our tests in terms of these magic functions with Intent-Revealing Names [SBPP], passing as parameters only those things that will be veriﬁ ed in the assertions or that should affect the outcome of the test. Once we’ve written the test in this very intent-revealing style, we must implement all of the magic functions that we’ve been calling. The functions that create objects are our Creation Methods; they encapsulate the complexity of object creation. The simple ones call the appropriate constructor, passing it suitable default values for anything needed but not supplied as a parameter. If any of the constructor argu- ments are other objects, the Creation Method will ﬁ rst create those depended-on objects before calling the constructor.\n\nThe Creation Method may be placed in all the same places where we put Test Utility Methods (page 599). As usual, the decision is based on the expected scope of reuse and the Creation Method’s dependencies on the API of the SUT. A related pattern is Object Mother (see Test Helper on page 643), which is a combination of Creation Method, Test Helper, and optionally Automated Tear- down (page 503).\n\nWhen to Use It\n\nWe should use a Creation Method whenever constructing a Fresh Fixture (page 311) requires signiﬁ cant complexity and we value Tests as Documentation. Another key indicator for using Creation Method is that we are building the system in a highly incremental way and we expect the API of the system (and especially the object constructors) to change frequently. Encapsulating knowl- edge of how to create a ﬁ xture object is a special case of SUT API Encapsulation (see Test Utility Method), and it helps us avoid both Fragile Tests (page 239) and Obscure Tests.\n\nThe main drawback of a Creation Method is that it creates another API for test automaters to learn. This isn’t much of a problem for the initial test devel- opers because they are typically involved in building this API but it can create “one more thing” for new additions to the team to learn. Even so, this API should be pretty easy to understand because it is just a set of Factory Methods [GOF] organized in some way.\n\nIf we are using a Prebuilt Fixture (page 429), we should use Finder Methods (see Test Utility Method) to locate the prebuilt objects. At the same time, we\n\nwww.it-ebooks.info\n\nCreation Method\n\nmay still use Creation Methods to lay mutable objects that we plan to modify on top of an Immutable Shared Fixture (see Shared Fixture on page 317).\n\nSeveral variations of Creation Method are worth exploring.\n\nVariation: Parameterized Creation Method\n\nWhile it is possible (and often very desirable) for Creation Methods to take no parameters whatsoever, many tests will require some customization of the cre- ated object. A Parameterized Creation Method allows the test to pass in some attributes to be used in the creation of the object. In such a case, we should pass only those attributes that are expected to affect (or those we want to demon- strate do not affect) the test’s outcome; otherwise, we could be headed down the slippery slope to Obscure Tests.\n\nVariation: Anonymous Creation Method\n\nAn Anonymous Creation Method automatically creates a Distinct Generated Value (see Generated Value on page 723) as the unique identiﬁ er for the object it is creating even though the arguments it receives may not be unique. This behav- ior is invaluable for avoiding Unrepeatable Tests (see Erratic Test on page 228) because it ensures that every object we create is unique, even across multiple test runs. If the test cares about some attributes of the object to be created, it can pass them as parameters of the Creation Method; this behavior turns the Anony- mous Creation Method into a Parameterized Anonymous Creation Method.\n\nVariation: Parameterized Anonymous Creation Method\n\nA Parameterized Anonymous Creation Method is a combination of several other variations of Creation Method in that we pass in some attributes to be used in the creation of the object but let the Creation Method create the unique identi- ﬁ er for it. A Creation Method could also take zero parameters if the test doesn’t care about any of the attributes.\n\nVariation: Named State Reaching Method\n\nSome SUTs are essentially stateless, meaning we can call any method at any time. By contrast, when the SUT is state-rich and the validity or behavior of methods is affected by the state of the SUT, it is important to test each method from each possible starting state. We could chain a bunch of such tests together in a single Test Method (page 348), but that approach would create an Eager Test (see Assertion Roulette on page 224). It is better to use a series of Single- Condition Tests (see page 45) for this purpose. Unfortunately, that leaves us\n\nwww.it-ebooks.info\n\n417\n\nCreation Method\n\n418\n\nCreation Method\n\nChapter 20 Fixture Setup Patterns\n\nwith the problem of how to set up the starting state in each test without a lot of Test Code Duplication (page 213).\n\nOne obvious solution is to put all tests that depend on the same starting state into the same Testcase Class (page 373) and to create the SUT in the appropri- ate state in the setUp method using Implicit Setup (page 424) (called Testcase Class per Fixture; see page 631). The alternative is to use Delegated Setup by calling a Named State Reaching Method; this approach allows us to choose some other way to organize our Testcase Classes.\n\nEither way, the code that sets up the SUT will be easier to understand if it is short and sweet. That’s where a Named State Reaching Method comes in handy. By encapsulating the logic required to create the test objects in the cor- rect state in a single place (whether on the Testcase Class or a Test Helper), we reduce the amount of code we must update if we need to change how we put the test object into that state.\n\nVariation: Attachment Method\n\nSuppose we already have a test object and we want to modify it in some way. We ﬁ nd ourselves performing this task in enough tests to want to code this modiﬁ ca- tion once and only once. The solution in this case is an Attachment Method. The main difference between this variation and the original Creation Method pattern is that we pass in the object to be modiﬁ ed (one that was probably returned by another Creation Method) and the object we want to set one of its attributes to; the Attachment Method does the rest of the work for us.\n\nImplementation Notes\n\nMost Creation Methods are created by doing an Extract Method [Fowler] refac- toring on parts of an existing test. When we write tests in an “outside-in” man- ner, we assume that the Creation Methods already exist and ﬁ ll in the method bodies later. In effect, we deﬁ ne a Higher-Level Language (see page 41) for deﬁ n- ing our ﬁ xtures. Nevertheless, there is another, completely different way to deﬁ ne Creation Methods.\n\nVariation: Reuse Test for Fixture Setup\n\nWe can set up the ﬁ xture by calling another Test Method to do the ﬁ xture setup for us. This assumes that we have some way of accessing the ﬁ xture that the other test created, either through a Registry [PEAA] object or through instance variables of the Testcase Object (page 382).\n\nIt may be appropriate to implement a Creation Method in this way when we already have tests that depend on other tests to set up their test ﬁ xture but\n\nwww.it-ebooks.info\n\nCreation Method\n\nwe want to reduce the likelihood that a change in the test execution order of Chained Tests (page 454) will cause tests to fail. Mind you, the tests will run more slowly because each test will call all the preceding tests it depends on each time each test is run rather than each test being run only once per test run. Of course, each test needs to call only the speciﬁ c tests it actually depends on, not all tests in the test suite. This slowdown won’t be very noticeable if we have replaced any slow components, such as a database, with a Fake Object (page 551).\n\nWrapping the Test Method in a Creation Method is a better option than calling the Test Method directly from the client Test Method because most Test Methods are named based on which test condition(s) they verify, not what (ﬁ x- ture) they leave behind. The Creation Method lets us put a nice Intent-Revealing Name between the client Test Method and the implementing Test Method. It also solves the Lonely Test (see Erratic Test) problem because the other test is run explicitly from within the calling test rather than just assuming that it was already run. This scheme makes the test less fragile and easier to understand but it won’t solve the Interacting Tests (see Erratic Test) problem: If the test we call fails and leaves the test ﬁ xture in a different state than we expected, our test will likely fail as well, even if the functionality we are testing is still working.\n\nMotivating Example\n\nIn the following example, the testPurchase test requires a Customer to ﬁ ll the role of the buyer. The ﬁ rst and last names of the buyer have no bearing on the act of pur- chasing, but are required parameters of the Customer constructor; we do care that the Customer’s credit rating is good (“G”) and that he or she is currently active.\n\npublic void testPurchase_ﬁrstPurchase_ICC() { Customer buyer = new Customer(17, \"FirstName\", \"LastName\", \"G\",\"ACTIVE\"); // ... } public void testPurchase_subsequentPurchase_ICC() { Customer buyer = new Customer(18, \"FirstName\", \"LastName\", \"G\",\"ACTIVE\"); // ... }\n\nThe use of constructors in tests can be problematic, especially when we are building an application incrementally. Every change to the parameters of the constructor will force us to revisit a lot of tests or jump through hoops to keep the constructor signatures backward compatible for the sake of the tests.\n\nwww.it-ebooks.info\n\n419\n\nCreation Method\n\n420\n\nCreation Method\n\nChapter 20 Fixture Setup Patterns\n\nRefactoring Notes\n\nWe can use an Extract Method refactoring to remove the direct call to the construc- tor. We can give the new Creation Method an appropriate Intent-Revealing Name such as createCustomer based on the style of Creation Method we have created.\n\nExample: Anonymous Creation Method\n\nIn the following example, instead of making that direct call to the Customer constructor, we now use the Customer Creation Method. Notice that the coupling between the ﬁ xture setup code and the constructor has been removed. If another parameter such as phone number is added to the Customer constructor, only the Customer Creation Method must be updated to provide a default value; the ﬁ xture setup code remains insulated from the change thanks to encapsulation.\n\npublic void testPurchase_ﬁrstPurchase_ACM() { Customer buyer = createAnonymousCustomer(); // ... } public void testPurchase_subsequentPurchase_ACM() { Customer buyer = createAnonymousCustomer(); // ... }\n\nWe call this pattern an Anonymous Creation Method because the identity of the customer does not matter. The Anonymous Creation Method might look something like this:\n\npublic Customer createAnonymousCustomer() { int uniqueid = getUniqueCustomerId(); return new Customer(uniqueid, \"FirstName\" + uniqueid, \"LastName\" + uniqueid, \"G\", \"ACTIVE\"); }\n\nNote the use of a Distinct Generated Value to ensure that each anonymous Customer is slightly different to avoid accidentally creating an identical Customer.\n\nExample: Parameterized Creation Method\n\nIf we wanted to supply some of the Customer’s attributes as parameters, we could deﬁ ne a Parameterized Creation Method:\n\npublic void testPurchase_ﬁrstPurchase_PCM() { Customer buyer = createCreditworthyCustomer(\"FirstName\", \"LastName\");\n\nwww.it-ebooks.info\n\nCreation Method\n\n// ... } public void testPurchase_subsequentPurchase_PCM() { Customer buyer = createCreditworthyCustomer(\"FirstName\", \"LastName\"); // ... }\n\nHere’s the corresponding Parameterized Creation Method deﬁ nition:\n\npublic Customer createCreditworthyCustomer( String ﬁrstName, String lastName) { int uniqueid = getUniqueCustomerId(); Customer customer = new Customer(uniqueid,ﬁrstName,lastName,\"G\",\"ACTIVE\"); customer.setCredit(CreditRating.EXCELLENT); customer.approveCredit(); return customer; }\n\nExample: Attachment Method\n\nHere’s an example of a test that uses an Attachment Method to associate two customers to verify that both get the best discount either of them has earned or negotiated:\n\npublic void testPurchase_relatedCustomerDiscount_AM() { Customer buyer = createCreditworthyCustomer(\"Related\", \"Buyer\"); Customer discountHolder = createCreditworthyCustomer(\"Discount\", \"Holder\"); createRelationshipBetweenCustomers( buyer, discountHolder); // ... }\n\nBehind the scenes, the Attachment Method does whatever it takes to establish the relationship:\n\nprivate void createRelationshipBetweenCustomers( Customer buyer, Customer discountHolder) { buyer.addToRelatedCustomersList( discountHolder ); discountHolder.addToRelatedCustomersList( buyer ); }\n\nAlthough this example is relatively simple, the call to this method is still easier to understand than reading both the method calls of which it consists.\n\nwww.it-ebooks.info\n\n421\n\nCreation Method\n\n422\n\nCreation Method\n\nChapter 20 Fixture Setup Patterns\n\nExample: Test Reused for Fixture Setup\n\nWe can reuse other tests to set up the ﬁ xture for our test. Here is an example of how not to do it:\n\nprivate Customer buyer; private AccountManager sut = new AccountManager(); private Account account;\n\npublic void testCustomerConstructor_SRT() { // Exercise buyer = new Customer(17, \"First\", \"Last\", \"G\", \"ACTIVE\"); // Verify assertEquals( \"First\", buyer.ﬁrstName(), \"ﬁrst\"); // ... } public void testPurchase_SRT() { testCustomerConstructor_SRT(); // Leaves in ﬁeld \"buyer\" account = sut.createAccountForCustomer( buyer ); assertEquals( buyer.name, account.customerName, \"cust\"); // ... }\n\nThe problem here is twofold. First, the name of the Test Method we are calling describes what it veriﬁ es (e.g., a name) and not what it leaves behind (i.e., a Customer in the buyer ﬁ eld. Second, the test does not return a Customer; it leaves the Customer in an instance variable. This scheme works only because the Test Method we want to reuse is on the same Testcase Class; if it were on an unrelated class, we would have to do a few backﬂ ips to access the buyer. A better way to accomplish this goal is to encapsulate this call behind a Creation Method:\n\nprivate Customer buyer; private AccountManager sut = new AccountManager(); private Account account;\n\npublic void testCustomerConstructor_RTCM() { // Exercise buyer = new Customer(17, \"First\", \"Last\", \"G\", \"ACTIVE\"); // Verify assertEquals( \"First\", buyer.ﬁrstName(), \"ﬁrst\"); // ... } public void testPurchase_RTCM() { buyer = createCreditworthyCustomer(); account = sut.createAccountForCustomer( buyer ); assertEquals( buyer.name, account.customerName, \"cust\"); // ... } public Customer createCreditworthyCustomer() { testCustomerConstructor_RTCM();\n\nwww.it-ebooks.info\n\nCreation Method\n\nreturn buyer; // ... }\n\nNotice how much more readable this test has become? We can see where the buyer came from! This was easy to do because both Test Methods were on the same class. If they were on different classes, our Creation Method would have to create an instance of the other Testcase Class before it could run the test. Then it would have to ﬁ nd a way to access the buyer instance variable so that it could return it to the calling Test Method.\n\nwww.it-ebooks.info\n\n423\n\nCreation Method\n\n424\n\nImplicit Setup\n\nAlso known as: Hooked Setup, Framework- Invoked Setup, Shared Setup Method\n\nChapter 20 Fixture Setup Patterns\n\nImplicit Setup\n\nHow do we construct the Fresh Fixture?\n\nWe build the test ﬁ xture common to several tests in the setUp method.\n\nsetup setup\n\nSetup Setup\n\nFixture Fixture\n\ntest_1 test_1\n\ntest_2 test_2\n\nSUT SUT\n\ntest_n test_n\n\nTestcase Class Testcase Class\n\nTo execute an automated test, we require a text ﬁ xture that is well understood and completely deterministic. We are using a Fresh Fixture (page 311) approach to build the Minimal Fixture (page 302) for the use of this one test.\n\nImplicit Setup is a way to reuse the ﬁ xture setup code for all Test Meth-\n\nods (page 348) in a Testcase Class (page 373).\n\nHow It Works\n\nAll tests in a Testcase Class create identical Fresh Fixtures by doing test ﬁ xture setup in a special setUp method on the Testcase Class. The setUp method is called automatically by the Test Automation Framework (page 298) before it calls each Test Method. This allows the ﬁ xture setup code placed in the setUp method to be reused without reusing the same instance of the test ﬁ xture. This approach is called “implicit” setup because the calls to the ﬁ xture setup logic are not explicit within the Test Method, unlike with In-line Setup (page 408) and Delegated Set- up (page 411).\n\nwww.it-ebooks.info\n\nImplicit Setup\n\nWhen to Use It\n\nWe can use Implicit Setup when several Test Methods on the same Testcase Class need an identical Fresh Fixture. If all Test Methods need the exact same ﬁ xture, then the entire Minimal Fixture needed by each test can be set up in the setUp method. This form of Test Method organization is known as Testcase Class per Fixture (page 631).\n\nWhen the Test Methods need different ﬁ xtures because we are using a Testcase Class per Feature (page 624) or Testcase Class per Class (page 617) scheme, it is more difﬁ cult to use Implicit Setup and still build a Minimal Fixture. We can use the setUp method only to set up the part of the ﬁ xture that does not cause any problems for the other tests. A reasonable compromise is to use Implicit Setup to set up the parts of the ﬁ xture that are essential but irrelevant and leave the setup of critical (and different from test to test) parts of the ﬁ xture to the individual Test Methods. Examples of “essential but irrelevant” ﬁ xture setup include ini- tializing variables with “don’t care” values and initializing hidden “plumbing” such as database connections. Fixture setup logic that directly affects the state of the SUT should be left to the individual Test Methods unless every Test Method requires the same starting state.\n\nThe obvious alternatives for creating a Fresh Fixture are In-line Setup, in which we include all setup logic within each Test Method without factoring out any common code, and Delegated Setup, in which we move all common ﬁ xture setup code into a set of Creation Methods (page 415) that we can call from within the setup part of each Test Method.\n\nImplicit Setup removes a lot of Test Code Duplication (page 213) and helps prevent Fragile Tests (page 239) by moving much of the nonessential interaction with the SUT out of the very numerous tests and into a much smaller num- ber of places where it is easier to maintain. It can, however, lead to Obscure Tests (page 186) when a Mystery Guest makes the test ﬁ xture used by each test less obvious. It can also lead to a Fragile Fixture (see Fragile Test) if all tests in the class do not really need identical test ﬁ xtures.\n\nImplementation Notes\n\nThe main implementation considerations for Implicit Setup are as follows:\n\nHow do we cause the ﬁ xture setUp method to be called?\n\nHow do we tear the ﬁ xture down?\n\nHow do the Test Methods access the ﬁ xture?\n\nwww.it-ebooks.info\n\n425\n\nImplicit Setup\n\n426\n\nImplicit Setup\n\nChapter 20 Fixture Setup Patterns\n\nCalling the Setup Code\n\nA setUp method is the most common way to handle Implicit Setup; it consists of having the Test Automation Framework call the setUp method before each Test Method. Strictly speaking, the setUp method is not the only form of implicit ﬁ x- ture setup. Suite Fixture Setup (page 441), for example, is used to set up and tear down a Shared Fixture (page 317) that is reused by the Test Methods on a single Testcase Class. In addition, Setup Decorator (page 447) moves the setUp method to a Decorator [GOF] object installed between the Test Suite Object (page 387) and the Test Runner (page 377). Both are forms of Implicit Setup because the setUp logic is not explicit within the Test Method.\n\nTearing Down the Fixture\n\nThe ﬁ xture teardown counterpart of Implicit Setup is Implicit Teardown (page 516). Anything that we set up in the setUp method that is not automatically cleaned up by Automated Teardown (page 503) or garbage collection should be torn down in the corresponding tearDown method.\n\nAccessing the Fixture\n\nThe Test Methods need to be able to access the test ﬁ xture built in the setUp method. When they were used in the same method, local variables were sufﬁ cient. To communicate between the setUp method and the Test Method, how- ever, the local variables must be changed into instance variables. We must be careful not to make them class variables as this will result in the potential for a Shared Fixture. (See the sidebar “There’s Always an Exception” on page 384 for a description of when instance variations do not provide this level of isolation.)\n\nMotivating Example\n\nIn the following example, each test needs to create a ﬂ ight between a pair of airports.\n\npublic void testStatus_initial() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // teardown // garbage-collected\n\nwww.it-ebooks.info\n\nImplicit Setup\n\n}\n\npublic void testStatus_cancelled() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); ﬂight.cancel(); // still part of setup // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // teardown // garbage-collected }\n\nRefactoring Notes\n\nThese tests contain a fair amount of Test Code Duplication. We can remove this duplication by refactoring this Testcase Class to use Implicit Setup. There are two refactoring cases to consider.\n\nFirst, when we discover that all tests are doing similar work to set up their test ﬁ xtures but are not sharing a setUp method, we can do an Extract Meth- od [Fowler] refactoring of the ﬁ xture setup logic in one of the tests to create our setUp method. We will also need to convert any local variables to instance variables (ﬁ elds) that hold the references to the resulting ﬁ xture until the Test Method can access it.\n\nSecond, when we discover that a Testcase Class already uses the setUp method to build the ﬁ xture and has tests that need a different ﬁ xture, we can use an Extract Class [Fowler] refactoring to move all Test Methods that need a different setup method to a different class. We need to ensure any instance variables that are used to convey knowledge of the ﬁ xture from the setup method to the Test Methods are transferred along with the setUp method. Sometimes it is simpler to clone the Testcase Class and delete each test from one or the other copy of the class; we can then delete from each class any instance variables that are no longer being used.\n\nExample: Implicit Setup\n\nIn this modiﬁ ed example, we have moved all common ﬁ xture setup code to the setUp method of our Testcase Class. This avoids the need to repeat this code in each test and makes each test much shorter—which is a good thing.\n\nAirport departureAirport; Airport destinationAirport;\n\nwww.it-ebooks.info\n\n427\n\nImplicit Setup\n\n428\n\nImplicit Setup\n\nChapter 20 Fixture Setup Patterns\n\nFlight ﬂight;\n\npublic void setUp() throws Exception{ super.setUp(); departureAirport = new Airport(\"Calgary\", \"YYC\"); destinationAirport = new Airport(\"Toronto\", \"YYZ\"); BigDecimal ﬂightNumber = new BigDecimal(\"999\"); ﬂight = new Flight( ﬂightNumber , departureAirport, destinationAirport); }\n\npublic void testGetStatus_initial() { // implicit setup // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); }\n\npublic void testGetStatus_cancelled() { // implicit setup partially overridden ﬂight.cancel(); // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); }\n\nThis approach has several disadvantages, which arise because we are not organizing our Test Methods around a Testcase Class per Fixture. (We are using Testcase Class per Feature here.) All the Test Methods on the Testcase Class must be able to make do with the same ﬁ xture (at least as a starting point), as evidenced by the partially overridden ﬁ xture setup in the second test in the exam- ple. The ﬁ xture is also not very obvious in these tests. Where does the ﬂ ight come from? Is there anything special about it? We cannot even rename the instance variable to communicate the nature of the ﬂ ight better because we are using it to hold ﬂ ights with different characteristics in each test.\n\nwww.it-ebooks.info\n\nPrebuilt Fixture\n\nPrebuilt Fixture\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nWe build the Shared Fixture separately from running the tests.\n\nSetup Setup\n\nSetup Setup\n\nFixture Fixture\n\nTest Runner Test Runner\n\nExercise Exercise\n\nSUT SUT\n\nExercise Exercise\n\nVerify Verify\n\nTeardown Teardown\n\nVerify Verify\n\nTeardown Teardown\n\nWhen we choose to use a Shared Fixture (page 317), whether it be for reasons of convenience or out of necessity, we need to create the Shared Fixture before we use it.\n\nHow It Works\n\nWe create the ﬁ xture sometime before running the test suite. We can create the ﬁ xture a number of different ways that we’ll discuss later. The most important point is that we don’t need to build the ﬁ xture each time the test suite is run because the ﬁ xture outlives both the mechanism used to build it and any one test run that uses it.\n\nWhen to Use It\n\nWe can reduce the overhead of creating a Shared Fixture each time a test suite is run by creating the ﬁ xture only occasionally. This pattern is especially appropri- ate when the cost of constructing the Shared Fixture is extremely high or cannot be automated easily.\n\nwww.it-ebooks.info\n\n429\n\nPrebuilt Fixture\n\nAlso known as: Prebuilt Context, Test Bed\n\n430\n\nPrebuilt Fixture\n\nChapter 20 Fixture Setup Patterns\n\nBecause of the Manual Intervention (page 250) required to (re)build the ﬁ xture before the tests are run, we’ll probably end up using the same ﬁ xture several times, which can lead to Erratic Tests (page 228) caused by shared ﬁ xture pollution. We may be able to avoid these problems by treating the Prebuilt Fixture as an Immu- table Shared Fixture (see Shared Fixture) and building a Fresh Fixture (page 311) for anything we plan to modify.\n\nThe alternatives to a Prebuilt Fixture are a Shared Fixture that is built once per test run and a Fresh Fixture. Shared Fixtures can be constructed using Suite Fixture Setup (page 441), Lazy Setup (page 435), or Setup Decorator (page 447). Fresh Fixtures can be constructed using In-line Setup (page 408), Implicit Setup (page 424), or Delegated Setup (page 411).\n\nVariation: Global Fixture\n\nA Global Fixture is a special case of Prebuilt Fixture where we shared the ﬁ xture between multiple test automaters. The key difference is that the ﬁ xture is globally visible and not “private” to a particular user. This pattern is most commonly em- ployed when we are using a single shared Database Sandbox (page 650) without using some form of Database Partitioning Scheme (see Database Sandbox).\n\nThe tests themselves can be the same as those used for a basic Prebuilt Fix- ture; likewise, the ﬁ xture setup is the same as that for a Prebuilt Fixture. What’s different here are the kinds of problems we can encounter. Because the ﬁ xture is now shared among multiple users, each of whom is running a separate Test Runner (page 377) on a different CPU, we may experience all sorts of multipro- cessing-related issues. The most common problem is a Test Run War (see Erratic Test) where we see seemingly random results. We can avoid this possibility by adopting some kind of Database Partitioning Scheme or by using Distinct Generated Values (see Generated Value on page 723) for any ﬁ elds with unique key constraints.\n\nImplementation Notes\n\nThe tests themselves look identical to a basic Shared Fixture. What’s different is how the ﬁ xture is set up. The test reader won’t be able to ﬁ nd any sign of it either within the Testcase Class (page 373) or in a Setup Decorator or Suite Fixture Setup method. Instead, the ﬁ xture setup is most probably performed manually via some kind of database copy operation, by using a Data Loader (see Back Door Manipulation on page 327) or by running a database population script. In these examples of Back Door Setup (see Back Door Manipulation), we bypass the SUT and interact with its database directly. (See the sidebar “Database as\n\nwww.it-ebooks.info\n\nPrebuilt Fixture\n\nSUT API?” on page 336 for an example of when the back door really is a front door.) Another option is to use a Fixture Setup Testcase (see Chained Tests on page 454) run from a Test Runner either manually or on a regular schedule.\n\nAnother difference is how the Finder Methods (see Test Utility Method on page 599) are implemented. We cannot just store the results of creating the objects in a class variable or an in-memory Test Fixture Registry (see Test Helper on page 643) because we aren’t setting the ﬁ xture up in code within the test run. Two of the more commonly used options available to us are (1) to store the unique identiﬁ ers generated during ﬁ xture construction in a persistent Test Fixture Registry (such as a ﬁ le) as we build the ﬁ xture so that the Finder Meth- ods can retrieve them later and (2) to hard-code the identiﬁ ers in the Finder Methods. We could search for objects/records that meet the Finder Methods’ criteria at runtime, but that approach might result in Nondeterministic Tests (see Erratic Test) because each test run could end up using a different object/re- cord from the Prebuilt Fixture. This strategy may be a good idea if each test run modiﬁ es the objects such that they no longer satisfy the criteria. Nevertheless, it may make debugging a failing test rather difﬁ cult, especially if the failures occur intermittently because some other attribute of the selected object is different.\n\nMotivating Example\n\nThe following example shows the construction of a Shared Fixture using Lazy Setup:1\n\nprotected void setUp() throws Exception { if (sharedFixtureInitialized) { return; } facade = new FlightMgmtFacadeImpl(); setupStandardAirportsAndFlights(); sharedFixtureInitialized = true; }\n\nprotected void tearDown() throws Exception { // Cannot delete any objects because we don't know // whether this is the last test }\n\nNote the call to setupStandardAirports in the setUp method. The tests use this ﬁ xture by calling Finder Methods that return objects from the ﬁ xture that match certain criteria:\n\n1 Of course, there are other ways to set up the Shared Fixture, such as Setup Decorator and Suite Fixture Setup.\n\nwww.it-ebooks.info\n\n431\n\nPrebuilt Fixture\n\n432\n\nPrebuilt Fixture\n\nChapter 20 Fixture Setup Patterns\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nRefactoring Notes\n\nOne way to convert a Testcase Class from a Standard Fixture (page 305) to a Prebuilt Fixture is to do an Extract Class [Fowler] refactoring so that the ﬁ xture is set up in one class and the Test Methods (page 348) are located in another class. Of course, we need to provide a way for the Finder Methods to deter- mine which objects or records exist in the structure because we won’t be able to guarantee that any instance or class variables will bridge the time gap between ﬁ xture construction and ﬁ xture usage.\n\nExample: Prebuilt Fixture Test\n\nHere is the resulting Testcase Class that contains the Test Methods. Note that it looks almost identical to the basic Shared Fixture tests.\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome\n\nwww.it-ebooks.info\n\nPrebuilt Fixture\n\nassertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nWhat’s different is how the ﬁ xture is set up and how the Finder Methods are implemented.\n\nExample: Fixture Setup Testcase\n\nWe may ﬁ nd it to be convenient to set up our Prebuilt Fixture using xUnit. This is simple to do if we already have the appropriate Creation Methods (page 415) or constructors already deﬁ ned and we have a way to easily persist the objects into the Database Sandbox. In the following example, we call the same method as in the previous example from the setUp method, except that now the method lives in the setUp method of a Fixture Setup Testcase that can be run whenever we want to regenerate the Prebuilt Fixture:\n\npublic class FlightManagementFacadeSetupTestcase extends AbstractFlightManagementFacadeTestCase { public FlightManagementFacadeSetupTestcase(String name) { super(name); }\n\nprotected void setUp() throws Exception { facade = new FlightMgmtFacadeImpl(); helper = new FlightManagementTestHelper(); setupStandardAirportsAndFlights(); saveFixtureInformation(); }\n\nprotected void tearDown() throws Exception { // Leave the Prebuilt Fixture for later use }\n\n}\n\nwww.it-ebooks.info\n\n433\n\nPrebuilt Fixture\n\n434\n\nPrebuilt Fixture\n\nChapter 20 Fixture Setup Patterns\n\nNote that there are no Test Methods on this Testcase Class and the tearDown method is empty. Here we want to do only the setup—nothing else.\n\nOnce we created the objects, we saved the information to the database using the call to saveFixtureInformation; this method persists the objects and saves the various keys in a ﬁ le so that we can reload them for use from the subsequent real test runs. This approach avoids the need to hard-code knowledge of the ﬁ xture into Test Methods or Test Utility Methods. In the interest of space, I’ll spare you the details of how we ﬁ nd the “dirty” objects and save the key infor- mation; there is more than one way to handle this task and any of these tactics will sufﬁ ce.\n\nExample: Prebuilt Fixture Setup Using a Data Population Script\n\nThere are as many ways to build a Prebuilt Fixture in a Database Sandbox as there are programming languages—everything from SQL scripts to Pearl and Ruby programs. These scripts can contain the data or they can read the data from a collection of ﬂ at ﬁ les. We can even copy the contents of a “golden” data- base into our Database Sandbox. I’ll leave it as an exercise for you to ﬁ gure out what’s most appropriate in your particular circumstance.\n\nwww.it-ebooks.info\n\nLazy Setup\n\nLazy Setup\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nWe use Lazy Initialization of the ﬁ xture to create it in the ﬁ rst test that needs it.\n\nTestcase Testcase Object Object Implicit setUp Implicit setUp\n\nsetUp setUp\n\nTest Test Suite Suite Object Object\n\ntestMethod_1 testMethod_1\n\nIs Fixture Set Is Fixture Set Up Yet? Up Yet?\n\nNo No\n\nCreate Create\n\nFixture Fixture\n\nTestcase Testcase Object Object Implicit setUp Implicit setUp\n\nExercise Exercise\n\nSUT SUT\n\ntestMethod_n testMethod_n\n\nShared Fixtures (page 317) are often used to speed up test execution by reducing the number of times a complex ﬁ xture needs to be created. Unfortunately, a test that depends on other tests to set up the ﬁ xture cannot be run by itself; it is a Lonely Test (see Erratic Test on page 228)\n\nWe can avoid this problem by having each test use Lazy Setup to set up the\n\nﬁ xture if it is not already set up.\n\nHow It Works\n\nWe use Lazy Initialization [SBPP] to construct the ﬁ xture in the ﬁ rst test that needs it and then store a reference to the ﬁ xture in a class variable that every test can access. All subsequently run tests will discover that the ﬁ xture is already created and that they can reuse it, thereby avoiding the effort of constructing the ﬁ xture anew.\n\nwww.it-ebooks.info\n\n435\n\nLazy Setup\n\n436\n\nLazy Setup\n\nChapter 20 Fixture Setup Patterns\n\nWhen to Use It\n\nWe can use Lazy Setup whenever we need to create a Shared Fixture yet still want to be able to run each test by itself. We can also use Lazy Setup instead of other techniques such as Setup Decorator (page 447) and Suite Fixture Set- up (page 441) if it is not crucial that the ﬁ xture be torn down. For example, we could use Lazy Setup when we are using a ﬁ xture that can be torn down by Garbage-Collected Teardown (page 500). We might also use Lazy Setup when we are using Distinct Generated Values (see Generated Value on page 723) for all database keys and aren’t worried about leaving extra records lying around after each test; Delta Assertions (page 485) make this approach possible.\n\nThe major disadvantage of Lazy Setup is the fact that while it is easy to discover that we are running the ﬁ rst test and need to construct the ﬁ xture, it is difﬁ cult to determine that we are running the last test and the ﬁ xture should be destroyed. Most members of the xUnit family of Test Automation Frameworks (page 298) do not provide any way to determine this fact other than by using a Setup Decorator for the entire test suite. A few members of the xUnit family support Suite Fixture Setup (NUnit, VbUnit, and JUnit 4.0 and newer, to name a few), which provides setUp/tearDown “bookends” for a Testcase Class (page 373). Unfortunately, this ability won’t help us if we are writing our tests in Ruby, Python, or PLSQL!\n\nSome IDEs and Test Runners (page 377) automatically reload our classes every time the test suite is run. This causes the original class variable to go out of scope, and the ﬁ xture will be garbage-collected before the new version of the class is run. In these cases there may be no negative consequence of using Lazy Setup.\n\nA Prebuilt Fixture (page 429) is another alternative to setting up the Shared Fixture for each test run. Its use can lead to Unrepeatable Tests (see Erratic Test) if the ﬁ xture is corrupted by some of the tests.\n\nImplementation Notes\n\nBecause Lazy Setup makes sense only with Shared Fixtures, Lazy Setup carries all the same baggage that comes with Shared Fixtures.\n\nNormally, Lazy Setup is used to build a Shared Fixture to be used by a single Testcase Class. The reference to the ﬁ xture is held in a class variable. Things get a bit trickier if we want to share the ﬁ xture across several Testcase Classes. We could move both the Lazy Initialization logic and the class variable to a Testcase Superclass (page 638) but only if our language supports inheritance of class variables. The other alternative is to move the logic and variables to a Test Helper (page 643).\n\nwww.it-ebooks.info\n\nLazy Setup\n\nOf course, we could use an approach such as reference counting as a way to know whether all Test Methods (page 348) have run. The challenge would be to know how many Testcase Objects (page 382) are in the Test Suite Object (page 387) so that we can compare this number with the number of times the tearDown method has been called. I have never seen anyone do this so I won’t call it a pattern! Adding logic to the Test Runner to invoke a tearDown method at the Test Suite Object level would amount to implementing Suite Fixture Setup.\n\nMotivating Example\n\nIn this example, we have been building a new ﬁ xture for each Testcase Object:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { setupStandardAirportsAndFlights(); FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { setupStandardAirportsAndFlights(); FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nNot surprisingly, these tests are slow because creating the airports and ﬂ ights involves a database. We can try refactoring these tests to set up the ﬁ xture in the setUp method (Implicit Setup; see page 424):\n\nprotected void setUp() throws Exception { facade = new FlightMgmtFacadeImpl(); helper = new FlightManagementTestHelper(); setupStandardAirportsAndFlights(); oneOutboundFlight = ﬁndOneOutboundFlight(); }\n\nwww.it-ebooks.info\n\n437\n\nLazy Setup\n\n438\n\nLazy Setup\n\nChapter 20 Fixture Setup Patterns\n\nprotected void tearDown() throws Exception { removeStandardAirportsAndFlights(); }\n\npublic void testGetFlightsByOriginAirport_NoFlights_td() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); try { // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { facade.removeAirport(outboundAirport); } }\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( oneOutboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", oneOutboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nThis doesn’t speed up our tests one bit because the Test Automation Framework calls the setUp and tearDown methods for each Testcase Object. All we have done is moved the code. We need to ﬁ nd a way to set up the ﬁ xture only once per test run.\n\nwww.it-ebooks.info\n\nLazy Setup\n\nRefactoring Notes\n\nWe can reduce the number of times we set up the ﬁ xture by converting this test to Lazy Setup. Because the ﬁ xture setup is already handled by the setUp method, we need simply insert the Lazy Initialization logic into the setUp method so that only the ﬁ rst test will cause it to be run. We must not forget to remove the tearDown logic, because it will render the Lazy Initialization logic useless if it removes the ﬁ xture after each Test Method has run! Sorry, but there is nowhere that we can move this logic to so that it will be run after the last Test Method has completed if our xUnit family member doesn’t support Suite Fixture Setup.\n\nExample: Lazy Setup\n\nHere is the same test refactored to use Lazy Setup:\n\nprotected void setUp() throws Exception { if (sharedFixtureInitialized) { return; } facade = new FlightMgmtFacadeImpl(); setupStandardAirportsAndFlights(); sharedFixtureInitialized = true; }\n\nprotected void tearDown() throws Exception { // Cannot delete any objects because we don't know // whether this is the last test }\n\nWhile there is a tearDown method on AirportFixture, there is no way to know when to call it! That’s the main consequence of using Lazy Setup. Because the variables are static, they will not go out of scope; hence the ﬁ xture will not be garbage col- lected until the class is unloaded or reloaded.\n\nThe tests are unchanged from the Implicit Setup version:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\nwww.it-ebooks.info\n\n439\n\nLazy Setup\n\n440\n\nLazy Setup\n\nChapter 20 Fixture Setup Patterns\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nwww.it-ebooks.info\n\nSuite Fixture Setup\n\nSuite Fixture Setup\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nWe build/destroy the shared ﬁ xture in special methods called by the Test Automation Framework before/after the ﬁ rst/last Test Method is called.\n\nSuiteFixture setUp SuiteFixture setUp\n\nSetup Setup\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nFixture Fixture\n\nInline Setup Inline Setup Exercise Exercise Verify Verify Inline Teardown Inline Teardown\n\nTestSuite Object TestSuite Object for for Testcase Class Testcase Class\n\nImplicit tearDown Implicit tearDown\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nSUT SUT\n\nInline Setup Inline Setup Exercise Exercise Verify Verify Inline Teardown Inline Teardown\n\nImplicit tearDown Implicit tearDown\n\nSuiteFixture tearDown SuiteFixture tearDown\n\nShared Fixtures (page 317) are commonly used to reduce the amount of per-test overhead required to set up the ﬁ xture. Sharing a ﬁ xture involves extra test program- ming effort because we must create the ﬁ xture and have a way of discovering the ﬁ xture in each test. Regardless of how the ﬁ xture is accessed, it must be initialized (constructed) before it is used.\n\nSuite Fixture Setup is one way to initialize the ﬁ xture if all the Test Meth- ods (page 348) that need it are deﬁ ned on the same Testcase Class (page 373).\n\nHow It Works\n\nWe implement or override a pair of methods that the Test Automation Frame- work (page 298) calls automatically. The name or annotation of these methods varies between members of the xUnit family but all work the same way: The framework calls the Suite Fixture Setup method before it calls the setUp method for the ﬁ rst Test Method; it calls the Suite Fixture Teardown method after it calls the tearDown method for the ﬁ nal Test Method. (I would have preferred to\n\nwww.it-ebooks.info\n\n441\n\nSuite Fixture Setup\n\n442\n\nSuite Fixture Setup\n\nChapter 20 Fixture Setup Patterns\n\nsay, “method on the ﬁ rst/ﬁ nal Testcase Object” but that isn’t true: NUnit, unlike other members of the xUnit family, creates only a single Testcase Object. See the sidebar “There’s Always an Exception” on page 384 for details.)\n\nWhen to Use It\n\nWe can use Suite Fixture Setup when we have a test ﬁ xture we wish to share between all Test Methods of a single Testcase Class and our variant of xUnit sup- ports this feature. This pattern is particularly useful if we need to tear down the ﬁ xture after the last test is run. At the time of writing this book, only VbUnit, NUnit, and JUnit 4.0 supported Suite Fixture Setup “out of the box.” Nevertheless, it is not difﬁ cult to add this capability in most variants of xUnit.\n\nIf we need to share the ﬁ xture more widely, we must use either a Prebuilt Fixture (page 429), a Setup Decorator (page 447), or Lazy Setup (page 435). If we don’t want to share the actual instance of the ﬁ xture but we do want to share the code to set up the ﬁ xture, we can use Implicit Setup (page 424) or Delegated Setup (page 411).\n\nThe main reason for using a Shared Fixture, and hence Suite Fixture Setup, is to overcome the problem of Slow Tests (page 253) caused by too many test ﬁ xture objects being created each time every test is run. Of course, a Shared Fixture can lead to Interacting Tests (see Erratic Test on page 228) or even a Test Run War (see Erratic Test); the sidebar “Faster Tests Without Shared Fixtures” (page 319) describes other ways to solve this problem.\n\nImplementation Notes\n\nFor Suite Fixture Setup to work properly, we must ensure that the ﬁ xture is remembered between calls to the Test Methods. This criterion implies we need to use a class variable, Registry [PEAA], or Singleton [GOF] to hold the references to the ﬁ xture (except in NUnit; see the sidebar “There’s Always an Exception” on page 384). The exact implementation varies from one member of the xUnit family to the next. Here are a few highlights:\n\nIn VbUnit, we implement the interface IFixtureFrame in the Testcase Class, thereby causing the Test Automation Framework (1) to call the IFixture Frame_Create method before the ﬁ rst Test Method is called and (2) to call the IFixtureFrame_Destroy method after the last Test Method is called.\n\nIn NUnit, the attributes [TestFixtureSetUp] and [TestFixtureTearDown] are used inside a test ﬁ xture to designate the methods to be called (1) once\n\nwww.it-ebooks.info\n\nSuite Fixture Setup\n\nprior to executing any of the tests in the ﬁ xture and (2) once after all tests are completed.\n\nIn JUnit 4.0 and later, the attribute @BeforeClass is used to indicate that a method should be run once before the ﬁ rst Test Method is executed. The method with the attribute @AfterClass is run after the last Test Method is run. JUnit allows these methods to be inherited and overridden; the subclass’s methods are run between the superclass’s methods. Suite Fixture Setup\n\nIn JUnit 4.0 and later, the attribute @BeforeClass is used to indicate that a method should be run once before the ﬁ rst Test Method is executed. The method with the attribute @AfterClass is run after the last Test Method is run. JUnit allows these methods to be inherited and overridden; the subclass’s methods are run between the superclass’s methods. Suite Fixture Setup\n\nMotivating Example\n\nSuppose we have the following test:\n\n[SetUp] protected void setUp() { helper.setupStandardAirportsAndFlights(); }\n\n[TearDown] protected void tearDown() { helper.removeStandardAirportsAndFlights(); }\n\n[Test] public void testGetFlightsByOriginAirport_2OutboundFlights(){ FlightDto[] expectedFlights = helper.ﬁndTwoOutboundFlightsFromOneAirport(); long originAirportId = expectedFlights[0].OriginAirportId; // Exercise System IList ﬂightsAtOrigin = facade.GetFlightsByOriginAirport(originAirportId); // Verify Outcome AssertExactly2FlightsInDtoList( expectedFlights[0], expectedFlights[1], ﬂightsAtOrigin, \"Flights at origin\"); }\n\n[Test] public void testGetFlightsByOriginAirport_OneOutboundFlight(){ FlightDto expectedFlight = helper.ﬁndOneOutboundFlight(); // Exercise System\n\nwww.it-ebooks.info\n\n443\n\n444\n\nSuite Fixture Setup\n\nChapter 20 Fixture Setup Patterns\n\nIList ﬂightsAtOrigin = facade.GetFlightsByOriginAirport( expectedFlight.OriginAirportId); // Verify Outcome AssertOnly1FlightInDtoList( expectedFlight, ﬂightsAtOrigin, \"Outbound ﬂight at origin\"); }\n\nFigure 20.1 is the console generated by an instrumented version of these tests.\n\n-------------------- setUp setupStandardAirportsAndFlights testGetFlightsByOriginAirport_OneOutboundFlight tearDown removeStandardAirportsAndFlights -------------------- setUp setupStandardAirportsAndFlights testGetFlightsByOriginAirport_TwoOutboundFlights tearDown removeStandardAirportsAndFlights -------------------- Figure 20.1 The calling sequence of Implicit Setup and Test Methods. The setupStandardAirportsAndFlights method is called before each Test Method. The hori- zontal lines delineate the Test Method boundaries.\n\nRefactoring Notes\n\nSuppose we want to refactor this example to a Shared Fixture. If we don’t care about destroying the ﬁ xture when the test run is ﬁ nished, we could use Lazy Setup. Otherwise, we can convert this example to a Suite Fixture Setup strategy by simply moving our code from the setUp and tearDown methods to the suiteFixtureSetUp and suiteFixtureTearDown methods, respectively.\n\nIn NUnit, we use the attributes [TestFixtureSetUp] and [TestFixtureTearDown] to indicate these methods to the Test Automation Framework. If we don’t want to leave anything in our setUp/tearDown methods, we can simply change the attributes from [Setup] and TearDown to [TestFixtureSetUp] and [TestFixtureTearDown], respectively.\n\nExample: Suite Fixture Setup\n\nHere’s the result of our refactoring to Suite Fixture Setup:\n\n[TestFixtureSetUp] protected void suiteFixtureSetUp() {\n\nwww.it-ebooks.info\n\nSuite Fixture Setup\n\nhelper.setupStandardAirportsAndFlights(); }\n\n[TestFixtureTearDown] protected void suiteFixtureTearDown() { helper.removeStandardAirportsAndFlights(); }\n\n[SetUp] protected void setUp() { }\n\n[TearDown] protected void tearDown() { }\n\n[Test] public void testGetFlightsByOrigin_TwoOutboundFlights(){ FlightDto[] expectedFlights = helper.ﬁndTwoOutboundFlightsFromOneAirport(); long originAirportId = expectedFlights[0].OriginAirportId; // Exercise System IList ﬂightsAtOrigin = facade.GetFlightsByOriginAirport(originAirportId); // Verify Outcome AssertExactly2FlightsInDtoList( expectedFlights[0], expectedFlights[1], ﬂightsAtOrigin, \"Flights at origin\"); }\n\n[Test] public void testGetFlightsByOrigin_OneOutboundFlight() { FlightDto expectedFlight = helper.ﬁndOneOutboundFlight(); // Exercise System IList ﬂightsAtOrigin = facade.GetFlightsByOriginAirport( expectedFlight.OriginAirportId); // Verify Outcome AssertOnly1FlightInDtoList( expectedFlight, ﬂightsAtOrigin, \"Outbound ﬂight at origin\"); }\n\nNow when various methods of the Testcase Class are called, the console looks like Figure 20.2.\n\nwww.it-ebooks.info\n\n445\n\nSuite Fixture Setup\n\n446\n\nSuite Fixture Setup\n\nChapter 20 Fixture Setup Patterns\n\nsuiteFixtureSetUp setupStandardAirportsAndFlights -------------------- setUp testGetFlightsByOriginAirport_OneOutboundFlight tearDown -------------------- setUp testGetFlightsByOriginAirport_TwoOutboundFlights tearDown -------------------- suiteFixtureTearDown removeStandardAirportsAndFlights\n\nFigure 20.2 The calling sequence of Suite Fixture Setup and Test Methods. The setupStandardAndAirportsAndFlights method is called once only for the Testcase Class rather than before each Test Method. The horizontal lines delineate the Test Method boundaries.\n\nThe setUp method is still called before each Test Method, along with the suite FixtureSetUp method where we are now calling setupStandardAirportsAndFlights to set up our ﬁ xture. So far, this is no different than Lazy Setup; the difference arises in that removeStandardAirportsAndFlights is called after the last of our Test Methods.\n\nAbout the Name\n\nNaming this pattern was tough because each variant of xUnit that implements it has a different name for it. Complicating matters is the fact that the Microsoft camp uses “test ﬁ xture” to mean more than what the Java/Pearl/Ruby/etc. camp means. I landed on Suite Fixture Setup by focusing on the scope of the Shared Fixture; it is shared across the test suite for one Testcase Class that spawns a single Test Suite Object (page 387). The ﬁ xture that is built for the Test Suite Object could be called a “SuiteFixture.”\n\nFurther Reading\n\nSee http://www.vbunit.com/doc/Advanced.htm for more information on Suite Fixture Setup as implemented in VbUnit. See http://nunit.org for more informa- tion on Suite Fixture Setup as implemented in NUnit.\n\nwww.it-ebooks.info\n\nSetup Decorator\n\nSetup Decorator\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nWe wrap the test suite with a Decorator that sets up the shared test ﬁ xture before running the tests and tears it down after all tests are done.\n\nsetUp setUp\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nFixture Fixture\n\nFixture Fixture Setup Setup Decorator Decorator\n\nTestSuite TestSuite Object Object\n\nInline Setup Inline Setup Exercise Exercise Verify Verify Inline Teardown Inline Teardown\n\nImplicit tearDown Implicit tearDown\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nSUT SUT\n\nInline Setup Inline Setup Exercise Exercise Verify Verify Inline Teardown Inline Teardown\n\nImplicit tearDown Implicit tearDown\n\ntearDown tearDown\n\nIf we have chosen to use a Shared Fixture (page 317), whether for reasons of convenience or out of necessity, and we have chosen not to use a Prebuilt Fix- ture (page 429), we will need to ensure that the ﬁ xture is built before each test run. Lazy Setup (page 435) is one strategy we could employ to create the test ﬁ xture “just in time” for the ﬁ rst test. But if it is critical to tear down the ﬁ xture after the last test, how do we know that all tests have been completed?\n\nHow It Works\n\nA Setup Decorator works by “bracketing” the execution of the entire test suite with a set of matching setUp and tearDown “bookends.” The pattern Decorator [GOF] is just what we need to make this happen. We construct a Setup Decora- tor that holds a reference to the Test Suite Object (page 387) we wish to decorate and then pass our Decorator to the Test Runner (page 377). When it is time to\n\nwww.it-ebooks.info\n\n447\n\nSetup Decorator\n\n448\n\nSetup Decorator\n\nChapter 20 Fixture Setup Patterns\n\nrun the test, the Test Runner calls the run method on our Setup Decorator rather than the run method on the actual Test Suite Object. The Setup Decorator performs the ﬁ xture setup before calling the run method on the Test Suite Object and tears down the ﬁ xture after it returns.\n\nWhen to Use It\n\nWe can use a Setup Decorator when it is critical that a Shared Fixture be set up before every test run and that the ﬁ xture is torn down after the run is complete. This behavior may be critical because tests are using Hard-Coded Values (see Literal Value on page 714) that would cause the tests to fail if they are run again without cleaning up after each run (Unrepeatable Tests; see Erratic Test on page 228). Alternatively, this behavior may be necessary to avoid the incremental consumption of some limited resource, such as our database slowly ﬁ lling up with data from repeated test runs.\n\nWe might also use a Setup Decorator when the tests need to change some global parameter before exercising the SUT and then need to change this parameter back when they are ﬁ nished. Replacing the database with a Fake Database (see Fake Object on page 551) in an effort to avoid Slow Tests (page 253) is one common reason for taking this approach; setting global switches to a particular conﬁ gura- tion is another. Setup Decorators are installed at runtime, so nothing stops us from using several different decorators on the same test suite at different times (or even the same time).\n\nAs an alternative to a Setup Decorator, we can use Suite Fixture Setup (page 441) if we only want to share the ﬁ xture across the tests in a single Testcase Class (page 373) and our member of the xUnit family supports this behavior. If it is not essential that the ﬁ xture be torn down after every test run, we could use Lazy Setup instead.\n\nImplementation Notes\n\nA Setup Decorator consists of an object that sets up the ﬁ xture, delegates test execution to the test suite to be run, and then executes the code to tear down the ﬁ xture. To better line up with the normal xUnit calling conventions, we typically put the code that constructs the test ﬁ xture into a method called setUp and the code that tears down the ﬁ xture into a method called tearDown. Then our Setup Decorator’s run logic consists of three lines of code:\n\nwww.it-ebooks.info\n\nSetup Decorator\n\nvoid run() { setup(); decoratedSuite.run(); teardown(); }\n\nThere are several ways to build the Setup Decorator.\n\nVariation: Abstract Setup Decorator\n\nMany members of the xUnit family of Test Automation Frameworks (page 298) provide a reusable superclass that implements a Setup Decorator. This class usually implements the setUp/run/tearDown sequence as a Template Method [GOF]. All we have to do is to subclass this class and implement the setUp and tearDown methods as we would in a normal Testcase Class. When instantiating our Setup Decorator class, we pass the Test Suite Object we are decorating as the construc- tor argument.\n\nVariation: Hard-Coded Setup Decorator\n\nIf we need to build our Setup Decorator from scratch, the “simplest thing that could possibly work” is to hard-code the name of the decorated class in the suite method of the Setup Decorator. This allows the Setup Decorator class to act as the Test Suite Factory (see Test Enumeration on page 399) for the decorated suite.\n\nVariation: Parameterized Setup Decorator\n\nIf we want to reuse the Setup Decorator for different test suites, we can param- eterize its constructor method with the Test Suite Object to be run. This means that the setup and teardown logic can be coded within the Setup Decorator, thereby eliminating the need for a separate Test Helper (page 643) class just to reuse the setup logic across tests.\n\nVariation: Decorated Lazy Setup\n\nOne of the main drawbacks of using a Setup Decorator is that tests cannot be run by themselves because they depend on the Setup Decorator to set up the ﬁ xture. We can work around this requirement by augmenting the Setup Decorator with Lazy Setup in the setUp method so that an undecorated Testcase Object (page 382) can construct its own ﬁ xture. The Testcase Object can also remember that it built its own ﬁ xture and destroy it in the tearDown method. This functionality could be implemented on a generic Testcase Superclass (page 638) so that it has to be built and tested just once.\n\nwww.it-ebooks.info\n\n449\n\nSetup Decorator\n\n450\n\nSetup Decorator\n\nChapter 20 Fixture Setup Patterns\n\nThe only other alternative is to use a Pushdown Decorator. That would negate any test speedup the Shared Fixture bought us, however, so this approach can be used only in those cases when we use the Setup Decorator for reasons other than setting up a Shared Fixture.\n\nVariation: Pushdown Decorator\n\nOne of the main drawbacks of using a Setup Decorator is that tests cannot be run by themselves because they depend on the Setup Decorator to set up the ﬁ xture. One way we can circumvent this obstacle is to provide a means to push the decorator down to the level of the individual tests rather than the whole test suite. This step requires a few modiﬁ cations to the TestSuite class to allow the Setup Decorator to be passed down to where the individual Testcase Objects are constructed during the Test Discovery (page 393) process. As each object is created from the Test Method (page 348), it is wrapped in the Setup Decorator before it is added to the Test Suite Object’s collection of tests.\n\nOf course, this negates one of the major sources of the speed advantage created by using a Setup Decorator by forcing a new test ﬁ xture to be built for each test. See the sidebar “Faster Tests Without Shared Fixtures” on page 319 for other ways to address the test execution speed issue.\n\nMotivating Example\n\nIn this example, we have a set of tests that use Lazy Setup to build the Shared Fixture and Finder Methods (see Test Utility Method on page 599) to ﬁ nd the objects in the ﬁ xture. We have discovered that the leftover ﬁ xture is causing Unrepeatable Tests, so we want to clean up properly after the last test has ﬁ n- ished running.\n\nprotected void setUp() throws Exception { if (sharedFixtureInitialized) { return; } facade = new FlightMgmtFacadeImpl(); setupStandardAirportsAndFlights(); sharedFixtureInitialized = true; }\n\nprotected void tearDown() throws Exception { // Cannot delete any objects because we don't know // whether this is the last test }\n\nwww.it-ebooks.info\n\nSetup Decorator\n\nBecause there is no easy way to accomplish this goal with Lazy Setup, we must change our ﬁ xture setup strategy. One option is to use a Setup Decorator instead.\n\nRefactoring Notes\n\nWhen creating a Setup Decorator, we can reuse the exact same ﬁ xture setup logic; we just need to call it at a different time. Thus this refactoring consists mostly of moving the call to the ﬁ xture setup logic from the setUp method on the Testcase Class to the setUp method of a Setup Decorator class. Assuming we have an Abstract Setup Decorator available to subclass, we can create our new sub- class and provide concrete implementations of the setUp and tearDown methods.\n\nIf our instance of xUnit does not support Setup Decorator directly, we can create our own Setup Decorator superclass by building a single-purpose Setup Decorator and then introducing a constructor parameter and instance variable to hold the test suite to be run. Finally, we do an Extract Superclass [Fowler] refactoring to create our reusable superclass.\n\nExample: Hard-Coded Setup Decorator\n\nIn this example, we have moved all of the setup logic to the setUp method of a Setup Decorator that inherits its basic functionality from an Abstract Setup Decorator. We have also written some ﬁ xture teardown logic in the tearDown method so that we clean up the ﬁ xture after the entire test suite has been run.\n\npublic class FlightManagementTestSetup extends TestSetup { private FlightManagementTestHelper helper;\n\npublic FlightManagementTestSetup() { // Construct the Test Suite Object to be decorated and // pass it to our Abstract Setup Decorator superclass super( SafeFlightManagementFacadeTest.suite() ); helper = new FlightManagementTestHelper(); }\n\npublic void setUp() throws Exception { helper.setupStandardAirportsAndFlights(); }\n\npublic void tearDown() throws Exception { helper.removeStandardAirportsAndFlights(); }\n\nwww.it-ebooks.info\n\n451\n\nSetup Decorator\n\n452\n\nSetup Decorator\n\nChapter 20 Fixture Setup Patterns\n\npublic static Test suite() { // Return an instance of this decorator class return new FlightManagementTestSetup(); } }\n\nBecause this is a Hard-Coded Setup Decorator, the call to the Test Suite Factory that builds the actual Test Suite Object is hard-coded inside the constructor. The suite method just calls the constructor.\n\nExample: Parameterized Setup Decorator\n\nTo make our Setup Decorator reusable with several different test suites, we need to do an Introduce Parameter [JBrains] refactoring on the name of the Test Suite Factory inside the constructor:\n\npublic class ParameterizedFlightManagementTestSetup extends TestSetup {\n\nprivate FlightManagementTestHelper helper = new FlightManagementTestHelper();\n\npublic ParameterizedFlightManagementTestSetup( Test testSuiteToDecorate) { super(testSuiteToDecorate); }\n\npublic void setUp() throws Exception { helper.setupStandardAirportsAndFlights(); }\n\npublic void tearDown() throws Exception { helper.removeStandardAirportsAndFlights(); } }\n\nTo make it easy for the Test Runner to create our test suite, we also need to cre- ate a Test Suite Factory that calls the Setup Decorator’s constructor with the Test Suite Object to be decorated:\n\npublic class DecoratedFlightManagementFacadeTestFactory { public static Test suite() { // Return a new Test Suite Object suitably decorated return new ParameterizedFlightManagementTestSetup( SafeFlightManagementFacadeTest.suite()); } }\n\nwww.it-ebooks.info\n\nSetup Decorator\n\nWe will need one of these Test Suite Factories for each test suite we want to be able to run by itself. Even so, this is a small price to pay for reusing the actual Setup Decorator.\n\nExample: Abstract Decorator Class\n\nHere’s what the Abstract Decorator Class looks like:\n\npublic class TestSetup extends TestCase { Test decoratedSuite;\n\nAbstractSetupDecorator(Test testSuiteToDecorate) { decoratedSuite = testSuiteToDecorate; }\n\npublic void setUp() throws Exception { // subclass responsibility } public void tearDown() throws Exception { // subclass responsibility }\n\nvoid run() { setup(); decoratedSuite.run(); teardown(); } }\n\nwww.it-ebooks.info\n\n453\n\nSetup Decorator\n\n454\n\nChained Tests\n\nChapter 20 Fixture Setup Patterns\n\nChained Tests\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nWe let the other tests in a test suite set up the test ﬁ xture.\n\nFixture Fixture Fixture Fixture\n\nExercise Exercise\n\nVerify Verify\n\nTest Runner Test Runner\n\nExercise Exercise\n\nVerify Verify\n\nSUT SUT\n\nExercise Exercise\n\nVerify Verify\n\nShared Fixtures (page 317) are commonly used to reduce the amount of per- test overhead required to set up the ﬁ xture. Sharing a ﬁ xture involves extra test programming effort because we need to create the ﬁ xture and have a way of discovering the ﬁ xture in each test. Regardless of how the ﬁ xture is accessed, it must be initialized (constructed) before it is used.\n\nChained Tests offer a way to reuse the test ﬁ xture left over from one test and\n\nthe Shared Fixture of a subsequent test.\n\nHow It Works\n\nChained Tests take advantage of the objects created by the tests that run before our current test in the test suite. This approach is very similar to how a human tester tests a large number of test conditions in a single test—by building up a complex test ﬁ xture through a series of actions, with the outcome of each action ﬁ rst being veriﬁ ed. We can achieve a similar result with automated tests by building a set of Self-Checking Tests (see page 26) that do not perform any ﬁ xture setup but instead\n\nwww.it-ebooks.info\n\nChained Tests\n\nrely on the “leftovers” of the test(s) that run before them. Unlike with the Reuse Test for Fixture Setup pattern (see Creation Method on page 415), we don’t actu- ally call another Test Method (page 348) from within out test; we just assume that it has been run and has left behind something we can use as a test ﬁ xture.\n\nWhen to Use It\n\nChained Tests is a ﬁ xture strategy that people either love or hate. Those who hate it do so because this approach is simply the test smell Interacting Tests (see Erratic Test on page 228) recast as a pattern. Those who love it typically do so because it solves a nasty problem introduced by using Shared Fixtures to deal with Slow Tests (page 253). Either way, it is a valid strategy for refactoring exist- ing tests that are overly long and contain many steps that build on one another. Such tests will stop executing when the ﬁ rst assertion fails. We can refactor such tests into a set of Chained Tests fairly quickly because this strategy doesn’t require determining exactly which test ﬁ xture we need to build for each test. This may be the ﬁ rst step in evolving the tests into a set of Independent Tests (see page 42).\n\nChained Tests help prevent Fragile Tests (page 239) because they are a crude form of SUT API Encapsulation (see Test Utility Method on page 599). Our test doesn’t need to interact with the SUT to set up the ﬁ xture because we let another test that was already using the same API set up the ﬁ xture for us. Fragile Fixtures (see Fragile Test) may be a problem, however; if one of the preceding tests is modiﬁ ed to create a different ﬁ xture, the depending test will probably fail. This is also true if some of the earlier tests fail or have errors; they may leave the Shared Fixture in a different state from what the current test expects.\n\nOne of the key problems with Chained Tests is the nondeterminism of the order in which xUnit executes tests in a test suite. Most members of the family make no guarantees about this order (TestNG is an exception). Thus tests could start to fail when a new version of xUnit is installed or even when one of the Test Methods is renamed [if the xUnit implementation happens to sort the Test- case Objects (page 382) by method name].\n\nAnother problem is that Chained Tests are Lonely Tests (see Erratic Test) because the current test depends on the tests that precede it to set up the test ﬁ xture. If we run the test by itself, it will likely fail because the test ﬁ xture it assumes is not set up for it. As a consequence, we cannot run just the one test when we are debugging failures it exposes.\n\nDepending on other tests to set up the test ﬁ xture invariably results in tests that are more difﬁ cult to understand because the test ﬁ xture is invisible to the\n\nwww.it-ebooks.info\n\n455\n\nChained Tests\n\n456\n\nChained Tests\n\nChapter 20 Fixture Setup Patterns\n\ntest reader—a classic case of a Mystery Guest (see Obscure Test on page 186). This problem can be at least partially mitigated through the use of appropri- ately named Finder Methods (see Test Utility Method) to access the objects in the Shared Fixture. It is less of an issue if all the Test Methods are on the same Testcase Class (page 373) and are listed in the same order as they are executed.\n\nVariation: Fixture Setup Testcase\n\nIf we need to set up a Shared Fixture and we cannot use any of the other tech- niques to set it up [e.g., Lazy Setup (page 435), Suite Fixture Setup (page 441), or Setup Decorator (page 447)], we can arrange to have a Fixture Setup Testcase run as the ﬁ rst test in the test suite. This is simple to do if we are using Test Enu- meration (page 399); we just include the appropriate addTest method call in our Test Suite Factory (see Test Enumeration). This variation is a degenerate form of the Chained Tests pattern in that we are chaining a test suite behind a single Fixture Setup Testcase.\n\nImplementation Notes\n\nThere are two key challenges in implementing Chained Tests:\n\nGetting tests in the test suite to run in the desired order\n\nAccessing the ﬁ xture leftover by the previous test(s)\n\nWhile a few members of the xUnit family provide an explicit mechanism for deﬁ ning the order of tests, most members make no such guarantees about this order. We can probably ﬁ gure out what order the xUnit member uses by per- forming a few experiments. Most commonly, we will discover that it is either the order in which the Test Methods appear in the ﬁ le or alphabetical order by Test Method name (in which case, the easiest solution is to include a test sequence number in the test name). In the worst-case scenario, we could always revert to Test Method Enumeration (see Test Enumeration) to ensure that Testcase Objects are added to the test suite in the correct order.\n\nTo refer to the objects created by the previous tests, we need to use one of the ﬁ xture object access patterns. If the preceding tests are Test Methods on the same Testcase Class, it is sufﬁ cient for each test to store any object references that subse- quent tests will use to access the ﬁ xture in a ﬁ xture holding class variable. (Fixture holding instance variables typically won’t work here because each test runs on a separate Testcase Object and, therefore, the tests don’t share instance variables. See the sidebar “There’s Always an Exception” on page 384 for a description of when instance variations do not behave this way.)\n\nwww.it-ebooks.info\n\nChained Tests\n\nIf our test depends on a Test Method on a different Testcase Class being run as a part of a Suite of Suites (see Test Suite Object on page 387), neither of these solutions will work. Our best bet will be to use a Test Fixture Registry (see Test Helper on page 643) as the means to store references to the objects used by the tests. A test database is a good example.\n\nObviously, we don’t want the test we are depending on to clean up after itself—that would leave nothing for us to reuse as our test ﬁ xture. That re- quirement makes Chained Tests incompatible with the Fresh Fixture (page 311) approach.\n\nMotivating Example\n\nHere’s an example of an incremental Tabular Test (see Parameterized Test on page 607) provided by Clint Shank on his blog:\n\npublic class TabularTest extends TestCase { private Order order = new Order(); private static ﬁnal double tolerance = 0.001;\n\npublic void testGetTotal() { assertEquals(\"initial\", 0.00, order.getTotal(), tolerance); testAddItemAndGetTotal(\"ﬁrst\", 1, 3.00, 3.00); testAddItemAndGetTotal(\"second\",3, 5.00, 18.00); // etc. }\n\nprivate void testAddItemAndGetTotal( String msg, int lineItemQuantity, double lineItemPrice, double expectedTotal) { // setup LineItem item = new LineItem( lineItemQuantity, lineItemPrice); // exercise SUT order.addItem(item); // verify total assertEquals(msg,expectedTotal,order.getTotal(),tolerance); } }\n\nThis test begins by building an empty order, veriﬁ es the total is zero, and then proceeds to add several items verifying the total after each item (Figure 20.3). The main issue with this test is that if one of the subtests fails, all subsequent subtests don’t get run. For example, suppose a rounding error makes the total after the second item incorrect: Wouldn’t we like to see whether the fourth, ﬁ fth, and six items are still correct?\n\nwww.it-ebooks.info\n\n457\n\nChained Tests\n\n458\n\nChained Tests\n\nChapter 20 Fixture Setup Patterns\n\nFigure 20.3 Tabular Test results. The lower pane shows the details of the ﬁ rst failure inside the single Tabular Test method listed in the upper pane. Because of the failure, the rest of the test method is not executed.\n\nRefactoring Notes\n\nWe can convert this Tabular Test to a set of Chained Tests simply by breaking up the single Test Method into one Test Method per subtest. One way to do so is to use a series of Extract Method [Fowler] refactorings to create the Test Methods. This will force us to use an Introduce Field [JetBrains] refactoring for any local variables before the ﬁ rst Extract Method refactoring operation. Once we have de- ﬁ ned all of the new Test Methods, we simply delete the original Test Method and let the Test Automation Framework (page 298) call our new methods directly.2\n\nWe need to ensure the tests run in the same order. Because JUnit seems to sort the Testcase Objects by method name, we can force them into the right order by including a sequence number in the Test Method name.\n\nFinally, we need to convert our Fresh Fixture into a Shared Fixture. We do so by changing our order ﬁ eld (instance variable) into a class variable (a static variable in Java) so that all of the Testcase Objects use the same Order.\n\n2 If we don’t have a refactoring tool handy, no worries. Just end the Test Method after each subtest and type in the signature of the next Test Method before the next subtest. We then move any Shared Fixture variables out of the ﬁ rst Test Method.\n\nwww.it-ebooks.info\n\nChained Tests\n\nExample: Chained Tests\n\nHere’s the simple example turned into three separate tests:\n\nprivate static Order order = new Order(); private static ﬁnal double tolerance = 0.001;\n\npublic void test_01_initialTotalShouldBeZero() { assertEquals(\"initial\", 0.00, order.getTotal(), tolerance); }\n\npublic void test_02_totalAfter1stItemShouldBeOnlyItemAmount(){ testAddItemAndGetTotal( \"ﬁrst\", 1, 3.00, 3.00); }\n\npublic void test_03_totalAfter2ndItemShouldBeSumOfAmounts() { testAddItemAndGetTotal( \"second\",3, 5.00, 18.00); }\n\nprivate void testAddItemAndGetTotal( String msg, int lineItemQuantity, double lineItemPrice, double expectedTotal) { // create a line item LineItem item = new LineItem(lineItemQuantity, lineItemPrice); // add line item to order order.addItem(item); // verify total assertEquals(msg,expectedTotal,order.getTotal(),tolerance); }\n\nThe Test Runner (page 377) gives us a better overview of what is wrong and what is working (Figure 20.4).\n\nUnfortunately, we will not be able to run any of the tests by themselves while we debug this problem (except for the very ﬁ rst test) because of the interdepen- dencies between the tests; they are Lonely Tests.\n\nwww.it-ebooks.info\n\n459\n\nChained Tests\n\n460\n\nChained Tests\n\nChapter 20 Fixture Setup Patterns\n\nFigure 20.4 Chained Tests result. The upper pane shows the three test methods with two tests passing. The lower pane shows the details of the one failing Test Method.\n\nwww.it-ebooks.info\n\nChapter 21\n\nResult Veriﬁ cation Patterns\n\nPatterns in This Chapter\n\nVeriﬁ cation Strategy\n\nState Veriﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462\n\nBehavior Veriﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468\n\nAssertion Method Styles\n\nCustom Assertion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474\n\nDelta Assertion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485\n\nGuard Assertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490\n\nUnﬁ nished Test Assertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494\n\n461\n\nwww.it-ebooks.info\n\nResult Veriﬁ cation Patterns",
      "page_number": 471
    },
    {
      "number": 21,
      "title": "Result Verification Patterns",
      "start_page": 525,
      "end_page": 562,
      "detection_method": "regex_chapter",
      "content": "462\n\nAlso known as: State-Based Testing\n\nState Veriﬁ cation\n\nChapter 21 Result Verification Patterns\n\nState Veriﬁ cation\n\nHow do we make tests self-checking when there is state to be veriﬁ ed?\n\nWe inspect the state of the system under test after it has been exercised and compare it to the expected state.\n\nFixture Fixture\n\nSetup Setup\n\nDOC DOC\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nGet State Get State\n\nSUT SUT\n\nB B\n\nBehavior Behavior (Indirect (Indirect Outputs) Outputs)\n\nA A\n\nC C\n\nTeardown Teardown\n\nA Self-Checking Test (see page 26) must verify that the expected outcome has occurred without manual intervention by whoever is running the test. But what do we mean by “expected outcome”? The SUT may or may not be “stateful”; if it is stateful, it may or may not have a different state after it has been exercised. As test automaters, it is our job to determine whether our expected outcome is a change of ﬁ nal state or whether we need to be more speciﬁ c about what occurs while the SUT is being exercised.\n\nState Veriﬁ cation involves inspecting the state of the SUT after it has been\n\nexercised.\n\nHow It Works\n\nWe exercise the SUT by invoking the methods of interest. Then, as a separate step, we interact with the SUT to retrieve its post-exercise state and compare it with the expected end state by calling Assertion Methods (page 362).\n\nwww.it-ebooks.info\n\nState Verification\n\nNormally, we can access the state of the SUT simply by calling methods or functions that return its state. This is especially true when we are doing test-driven development because the tests will have ensured that the state is easily accessible. When we are retroﬁ tting tests, however, we may ﬁ nd it more challenging to access the relevant state information. In these cases, we may need to use a Test-Speciﬁ c Subclass (page 579) or some other technique to expose the state without introduc- ing Test Logic in Production (page 217).\n\nA related question is “Where is the state of the SUT stored?” Sometimes, the state is stored within the actual SUT; in other cases, the state may be stored in another component such as a database. In the latter case, State Veriﬁ cation may involve accessing the state within the other component (essentially a layer-crossing test). By contrast, Behavior Veriﬁ cation (page 468) would involve verifying the interactions between the SUT and the other component.\n\nWhen to Use It\n\nWe should use State Veriﬁ cation when we care about only the end state of the SUT—not how the SUT got there. Taking such a limited view helps us maintain encapsulation of the implementation of the SUT.\n\nState Veriﬁ cation comes naturally when we are building the software inside out. That is, we build the innermost objects ﬁ rst and then build the next layer of objects on top of them. Of course, we may need to use Test Stubs (page 529) to control the indirect inputs of the SUT to avoid Production Bugs (page 268) caused by untested code paths. Even then, we are choosing not to verify the indirect outputs of the SUT.\n\nWhen we do care about the side effects of exercising the SUT that are not visible in its end state (its indirect outputs), we can use Behavior Veriﬁ cation to observe the behavior directly. We must be careful, however, not to create Fragile Tests (page 239) by overspecifying the software.\n\nImplementation Notes\n\nThere are two basic styles of implementing State Veriﬁ cation.\n\nVariation: Procedural State Veriﬁ cation\n\nWhen doing Procedural State Veriﬁ cation, we simply write a series of calls to Assertion Methods that pick apart the state information into pieces and com- pare those bits of information to individual expected values. Most people who are new to automating tests take such a “path of least resistance.” The major disadvantage of this approach is that it can result in Obscure Tests (page 186)\n\nwww.it-ebooks.info\n\n463\n\nState Veriﬁ cation\n\n464\n\nAlso known as: Expected Object\n\nState Veriﬁ cation\n\nChapter 21 Result Verification Patterns\n\nowing to the number of assertions it may take to specify the expected outcome. When the same sequence of assertions must be carried out in many tests or many times within a single Test Method (page 348), we also have Test Code Duplica- tion (page 213).\n\nVariation: Expected State Speciﬁ cation\n\nWhen doing Expected State Speciﬁ cation, we construct a speciﬁ cation for the post-exercise state of the SUT in the form of one or more objects populated with the expected attributes. We then compare the actual state directly with these objects using a single call to an Equality Assertion (see Assertion Method). This tends to result in more concise and readable tests. We can use an Expected State Speciﬁ cation whenever we need to verify several attributes and it is possible to construct an object that looks like the object we expect the SUT to return. The more attributes we have that need to be compared and the more tests that need to compare them, the more compelling the argument for using an Expected State Speciﬁ cation. In the most extreme cases, when we have a lot of data to verify, we can construct an “expected table” and verify that the SUT contains it. Fit’s “row ﬁ xtures” offer a good way to do this in customer tests; tools such as DbUnit are a good way to use Back Door Manipulation (page 327) for this purpose.\n\nWhen constructing the Expected State Speciﬁ cation, we may prefer to use a Parameterized Creation Method (see Creation Method on page 415) so that the reader is not distracted by all the necessary but unimportant attributes of the Expected State Speciﬁ cation. The Expected State Speciﬁ cation is most often an instance of the same class that we expect to get back from the SUT. We may have difﬁ culty using an Expected State Speciﬁ cation if the object doesn’t imple- ment equality in a way that involves comparing the values of attributes (e.g., by comparing the object references with each other) or if our test-speciﬁ c deﬁ nition of equality differs from that implemented by the equals method.\n\nIn these cases, we can still use an Expected State Speciﬁ cation if we create a Custom Assertion (page 474) that implements test-speciﬁ c equality. Alterna- tively, we can build the Expected State Speciﬁ cation from a class that imple- ments our test-speciﬁ c equality. This class can either be a Test-Speciﬁ c Subclass that overrides the equals method or a simple Data Transfer Object [CJ2EEP] that implements equals(TheRealObjectClass other). Both of these measures are preferable to modifying (or introducing) the equals method on the production class, as that would be a form of Equality Pollution (see Test Logic in Production). When the class is difﬁ cult to instantiate, we can deﬁ ne a Fake Object (page 551) that has the necessary attributes plus an equals method that implements test-speciﬁ c equality. These last few “tricks” are made possible by the fact that Equality\n\nwww.it-ebooks.info\n\nState Verification\n\nAssertions usually ask the Expected State Speciﬁ cation to compare itself to the actual result, rather than the reverse.\n\nWe can build the Expected State Speciﬁ cation either during the result veriﬁ - cation phase of the test immediately before it is used in the Equality Assertion or during the ﬁ xture setup phase of the test. The latter strategy allows us to use attributes of the Expected State Speciﬁ cation as parameters passed to the SUT or as the base for Derived Values (page 718) when building other objects in the test ﬁ xture. This makes it easier to see the cause–effect relationship between the ﬁ xture and the Expected State Speciﬁ cation, which in turn helps us achieve Tests as Documentation (see page 23). It is particularly useful when the Ex- pected State Speciﬁ cation is created out of sight of the test reader such as when using Creation Methods to do the construction.\n\nMotivating Example\n\nThis simple1 example features a test that exercises the code that adds a line item to an invoice. Because it contains no assertions, it is not a Self-Checking Test.\n\npublic void testInvoice_addOneLineItem_quantity1() { // Exercise inv.addItemQuantity(product, QUANTITY); }\n\nWe have chosen to create the invoice and product in the setUp method, an approach called Implicit Setup (page 424).\n\npublic void setUp() { product = createAnonProduct(); anotherProduct = createAnonProduct(); inv = createAnonInvoice(); }\n\nRefactoring Notes\n\nThe ﬁ rst refactoring we can do is not really a refactoring at all, because we are changing the behavior of the tests (for the better): We introduce some assertions that specify the expected outcome. This results in an example of Procedural State Veriﬁ cation because we make this change within the Test Method as a series of calls to built-in Assertion Methods.\n\n1 The natural example for this pattern is not very good at illustrating the difference between State Veriﬁ cation and Behavior Veriﬁ cation. For this purpose, refer to Behavior Veriﬁ cation, which provides a second example of State Veriﬁ cation that is more directly comparable.\n\nwww.it-ebooks.info\n\n465\n\nState Veriﬁ cation\n\n466\n\nState Veriﬁ cation\n\nChapter 21 Result Verification Patterns\n\nWe can further simplify the Test Method by refactoring it to use an Expected Object. First, we build an Expected Object by constructing an object of the expected class, or a suitable Test Double (page 522), and initializing it with the values that were previously speciﬁ ed in the assertions. Then we replace the series of assertions with a single Equality Assertion that compares the actual result with an Expected Object. We may have to use a Custom Assertion if we need test-speciﬁ c equality.\n\nExample: Procedural State Veriﬁ cation\n\nHere we have added the assertions to the Test Method to turn it into a Self- Checking Test. Because several steps must be carried out to verify the expected outcome, this test suffers from a mild case of Obscure Test.\n\npublic void testInvoice_addOneLineItem_quantity1() { // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); // Verify only item LineItem actual = (LineItem) lineItems.get(0); assertEquals(inv, actual.getInv()); assertEquals(product, actual.getProd()); assertEquals(QUANTITY, actual.getQuantity()); }\n\nExample: Expected Object\n\nIn this simpliﬁ ed version of the test, we use the Expected Object with a single Equality Assertion instead of a series of assertions on individual attributes:\n\npublic void testInvoice_addLineItem1() { LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity( expItem.getProd(), expItem.getQuantity()); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem) lineItems.get(0); assertEquals(\"Item\", expItem, actual); }\n\nwww.it-ebooks.info\n\nState Verification\n\nBecause we are also using some of the attributes as arguments of the SUT, we have chosen to build the Expected Object during the ﬁ xture setup phase of the test and to use the attributes of the Expected Object as the SUT arguments.\n\nwww.it-ebooks.info\n\n467\n\nState Veriﬁ cation\n\n468\n\nAlso known as: Interaction Testing\n\nBehavior Veriﬁ cation\n\nChapter 21 Result Verification Patterns\n\nBehavior Veriﬁ cation\n\nHow do we make tests self-checking when there is no state to verify?\n\nWe capture the indirect outputs of the SUT as they occur and compare them to the expected behavior.\n\nFixture Fixture\n\nSetup Setup\n\nDOC DOC\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nSUT SUT\n\nB B\n\nBehavior Behavior (Indirect (Indirect Outputs) Outputs)\n\nA A\n\nC C\n\nTeardown Teardown\n\nA Self-Checking Test (see page 26) must verify that the expected outcome has occurred without manual intervention by whoever is running the test. But what do we mean by “expected outcome”? The SUT may or may not be “stateful”; if it is stateful, it may or may not be expected to end up in a different state after it has been exercised. The SUT may also be expected to invoke methods on other objects or components.\n\nBehavior Veriﬁ cation involves verifying the indirect outputs of the SUT as it\n\nis being exercised.\n\nHow It Works\n\nEach test speciﬁ es not only how the client of the SUT interacts with it during the exercise SUT phase of the test, but also how the SUT interacts with the compo- nents on which it should depend. This ensures that the SUT really is behaving as speciﬁ ed rather than just ending up in the correct post-exercise state.\n\nwww.it-ebooks.info\n\ny y f f i i r r e e V V\n\nBehavior Verification\n\nBehavior Veriﬁ cation almost always involves interacting with or replacing a depended-on component (DOC) with which the SUT interacts at runtime. The line between Behavior Veriﬁ cation and State Veriﬁ cation (page 462) can get a bit blurry when the SUT stores its state in the DOC because both forms of veriﬁ cation involve layer-crossing tests. We can distinguish between the two cases based on whether we are verifying the post-test state in the DOC (State Veriﬁ cation) or whether we are verifying the method calls made by the SUT on the DOC (Behavior Veriﬁ cation).\n\nWhen to Use It\n\nBehavior Veriﬁ cation is primarily a technique for unit tests and component tests. We can use Behavior Veriﬁ cation whenever the SUT calls methods on other objects or components. We must use Behavior Veriﬁ cation whenever the expected outputs of the SUT are transient and cannot be determined simply by looking at the post-exercise state of the SUT or the DOC. This forces us to monitor these indirect outputs as they occur.\n\nA common application of Behavior Veriﬁ cation is when we are writing our code in an “outside-in” manner. This approach, which is often called need-driven development, involves writing the client code before we write the DOC. It is a good way to ﬁ nd out exactly what the interface provided by the DOC needs to be based on real, concrete examples rather than on speculation. The main objection to this approach is that we need to use a lot of Test Doubles (page 522) to write these tests. That could result in Fragile Tests (page 239) because each test knows so much about how the SUT is implemented. Because the tests specify the behavior of the SUT in terms of its interactions with the DOC, a change in the implementation of the SUT could break a lot of tests. This kind of Overspeciﬁ ed Software (see Fragile Test) could lead to High Test Maintenance Cost (page 265).\n\nThe jury is still out on whether Behavior Veriﬁ cation is a better approach than State Veriﬁ cation. In most cases, State Veriﬁ cation is clearly necessary; in some cases, Behavior Veriﬁ cation is clearly necessary. What has yet to be deter- mined is whether Behavior Veriﬁ cation should be used in all cases or whether we should use State Veriﬁ cation most of the time and resort to Behavior Veriﬁ - cation only when State Veriﬁ cation falls short of full test coverage.\n\nImplementation Notes\n\nBefore we exercise the SUT by invoking the methods of interest, we must ensure that we have a way of observing its behavior. Sometimes the mechanisms that the\n\nwww.it-ebooks.info\n\n469\n\nBehavior Veriﬁ cation\n\n470\n\nBehavior Veriﬁ cation\n\nAlso known as: Expected Behavior\n\nChapter 21 Result Verification Patterns\n\nSUT uses to interact with the components surrounding it make such observation possible; when this is not the case, we must install some sort of Test Double to monitor the SUT’s indirect outputs. We can use a Test Double as long as we have a way to replace the DOC with the Test Double. This could be via Dependency Injection (page 678) or by Dependency Lookup (page 686).\n\nThere are two fundamentally different ways to implement Behavior Veriﬁ ca- tion, each with its own proponents. The Mock Object (page 544) community has been very vocal about the use of “mocks” as an Expected Behavior Speciﬁ cation, so it is the more commonly used approach. Nevertheless, Mock Objects are not the only way of doing Behavior Veriﬁ cation.\n\nVariation: Procedural Behavior Veriﬁ cation\n\nIn Procedural Behavior Veriﬁ cation, we capture the method calls made by the SUT as it executes and later get access to them from within the Test Method (page 348). Then we use Equality Assertions (see Assertion Method on page 362) to compare them with the expected results.\n\nThe most common way of trapping the indirect outputs of the SUT is to install a Test Spy (page 538) in place of the DOC during the ﬁ xture setup phase of the Four-Phase Test (page 358). During the result veriﬁ cation phase of the test, we ask the Test Spy how it was used by the SUT during the exercise SUT phase. Use of a Test Spy does not require any advance knowledge of how the methods of the DOC will be called.\n\nThe alternative is to ask the real DOC how it was used. Although this scheme is not always feasible, when it is, it avoids the need to use a Test Double and minimizes the degree to which we have Overspeciﬁ ed Software.\n\nWe can reduce the amount of code in the Test Method (and avoid Test Code Duplication; see page 213) by deﬁ ning Expected Objects (see State Veriﬁ cation) for the arguments of method calls or by delegating the veriﬁ cation of them to Custom Assertions (page 474).\n\nVariation: Expected Behavior Speciﬁ cation\n\nExpected Behavior Speciﬁ cation is a different way of doing Behavior Veriﬁ cation. Instead of waiting until after the fact to verify the indirect outputs of the SUT by using a sequence of assertions, we load the Expected Behavior Speciﬁ cation into a Mock Object and let it verify that the method calls are correct as they are received.\n\nWe can use an Expected Behavior Speciﬁ cation when we know exactly what should happen ahead of time and we want to remove all Procedural Behavior Veriﬁ cation from the Test Method. This pattern variation tends to make the\n\nwww.it-ebooks.info\n\nBehavior Verification\n\ntest shorter (assuming we are using a compact representation of the expected behavior) and can be used to cause the test to fail on the ﬁ rst deviation from the expected behavior if we so choose.\n\nOne distinct advantage of using Mock Objects is that Test Double generation tools are available for many members of the xUnit family. They make imple- menting Expected Behavior Speciﬁ cation very easy because we don’t need to manually build a Test Double for each set of tests. One drawback of using a Mock Object is that it requires that we can predict how the methods of the DOC will be called and what arguments will be passed to it in the method calls.\n\nMotivating Example\n\nThe following test is not a Self-Checking Test because it does not verify that the expected outcome has actually occurred; it contains no calls to Assertion Methods, nor does it set up any expectations on a Mock Object. Because we are testing the logging functionality of the SUT, the state that interests us is actu- ally stored in the logger rather than within the SUT itself. The writer of this test hasn’t found a way to access the state we are trying to verify.\n\npublic void testRemoveFlightLogging_NSC() throws Exception { // setup FlightDto expectedFlightDto = createARegisteredFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify // have not found a way to verify the outcome yet // Log contains record of Flight removal }\n\nTo verify the outcome, whoever is running the tests must access the database and the log console and compare what was actually output to what should have been output.\n\nOne way to make the test Self-Checking is to enhance the test with Expected\n\nState Speciﬁ cation (see State Veriﬁ cation) of the SUT as follows:\n\npublic void testRemoveFlightLogging_ESS() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacadeImplTI facade = new FlightManagementFacadeImplTI(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify\n\nwww.it-ebooks.info\n\n471\n\nBehavior Veriﬁ cation\n\n472\n\nBehavior Veriﬁ cation\n\nChapter 21 Result Verification Patterns\n\nassertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); }\n\nUnfortunately, this test does not verify the logging function of the SUT in any way. It also illustrates one reason why Behavior Veriﬁ cation came about: Some functionality of the SUT is not visible within the end state of the SUT itself, but can be seen only if we intercept the behavior at an internal observation point between the SUT and the DOC or if we express the behavior in terms of state changes for the objects with which the SUT interacts.\n\nRefactoring Notes\n\nWhen we made the changes in the second code sample in the “Motivating Example,” we weren’t really refactoring; instead, we added veriﬁ cation logic to make the tests behave differently. There are, however, several refactoring cases that are worth discussing.\n\nTo refactor from State Veriﬁ cation to Behavior Veriﬁ cation, we must do a Replace Dependency with Test Double (page 522) refactoring to gain visibility of the indirect outputs of the SUT via a Test Spy or Mock Object.\n\nTo refactor from an Expected Behavior Speciﬁ cation to Procedural Behavior Veriﬁ cation, we install a Test Spy instead of the Mock Object. After exercising the SUT, we make assertions on values returned by the Test Spy and compare them with the expected values that were originally used as arguments when we initially conﬁ gured the Mock Object (the one that we just converted into a Test Spy).\n\nTo refactor from Procedural Behavior Veriﬁ cation to an Expected Behavior Speciﬁ cation, we conﬁ gure a Mock Object with the expected values from the assertions made on values returned by the Test Spy and install the Mock Object instead of the Test Spy.\n\nExample: Procedural Behavior Veriﬁ cation\n\nThe following test veriﬁ es the basic functionality of creating a ﬂ ight but uses Procedural Behavior Veriﬁ cation to verify the indirect outputs of the SUT. That is, it uses a Test Spy to capture the indirect outputs and then veriﬁ es those out- puts are correct by making in-line calls to the Assertion Methods.\n\npublic void testRemoveFlightLogging_recordingTestStub() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl();\n\nwww.it-ebooks.info\n\nBehavior Verification\n\n// Test Double setup AuditLogSpy logSpy = new AuditLogSpy(); facade.setAuditLog(logSpy); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertEquals(\"number of calls\", 1, logSpy.getNumberOfCalls()); assertEquals(\"action code\", Helper.REMOVE_FLIGHT_ACTION_CODE, logSpy.getActionCode()); assertEquals(\"date\", helper.getTodaysDateWithoutTime(), logSpy.getDate()); assertEquals(\"user\", Helper.TEST_USER_NAME, logSpy.getUser()); assertEquals(\"detail\", expectedFlightDto.getFlightNumber(), logSpy.getDetail()); }\n\nExample: Expected Behavior Speciﬁ cation\n\nIn this version of the test, we use the JMock framework to deﬁ ne the expected behavior of the SUT. The method expects on mockLog conﬁ gures the Mock Object with the Expected Behavior Speciﬁ cation (speciﬁ cally, the expected log message).\n\npublic void testRemoveFlight_JMock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // mock conﬁguration Mock mockLog = mock(AuditLog.class); mockLog.expects(once()).method(\"logMessage\") .with(eq(helper.getTodaysDateWithoutTime()), eq(Helper.TEST_USER_NAME), eq(Helper.REMOVE_FLIGHT_ACTION_CODE), eq(expectedFlightDto.getFlightNumber())); // mock installation facade.setAuditLog((AuditLog) mockLog.proxy()); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify // verify() method called automatically by JMock }\n\nwww.it-ebooks.info\n\n473\n\nBehavior Veriﬁ cation\n\n474\n\nAlso known as: Bespoke Assertion\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\nCustom Assertion\n\nHow do we make tests self-checking when we have test-speciﬁ c equality logic? How do we reduce Test Code Duplication when the same assertion logic appears in many tests? How do we avoid Conditional Test Logic?\n\nWe create a purpose-built Assertion Method that compares only those attributes of the object that deﬁ ne test-speciﬁ c equality.\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nTeardown Teardown\n\nCustom Custom Assertion Assertion\n\nAssertion Assertion Method Method\n\nAssertion Assertion Method Method\n\nMost members of the xUnit family provide a reasonably rich set of Assertion Methods (page 362). But sooner or later, we inevitably ﬁ nd ourselves saying, “This test would be so much easier to write if I just had an assertion that did . . . .” So why not write it ourselves?\n\nThe reasons for writing a Custom Assertion are many, but the technique is pretty much the same regardless of our goal. We hide the complexity of whatever it takes to prove the system is behaving correctly behind an Assertion Method with an Intent-Revealing Name [SBPP].\n\nHow It Works\n\nWe encapsulate the mechanics of verifying that something is true (an assertion) behind an Intent-Revealing Name. To do so, we factor out all the common assertion code within the tests into a Custom Assertion that implements the\n\nwww.it-ebooks.info\n\nCUSTOM ASSERTION\n\nveriﬁ cation logic. A Custom Equality Assertion takes two parameters: an Expected Object (see State Veriﬁ cation on page 462) and the actual object.\n\nA key characteristic of Custom Assertions is that they receive everything they need to pass or fail the test as parameters. Other than causing the test to fail, they have no side effects.\n\nTypically, we create Custom Assertions through refactoring by identifying common patterns of assertions in our tests. When test driving, we might just go ahead and call a nonexistent Custom Assertion because it makes writing our test easier; this tactic lets us focus on the part of the SUT that needs to be tested rather than the mechanics of how the test would be carried out.\n\nWhen to Use It\n\nWe should consider creating a Custom Assertion whenever any of the following statements are true:\n\nWe ﬁ nd ourselves writing (or cloning) the same assertion logic in test\n\nafter test (Test Code Duplication; see page 213).\n\nWe ﬁ nd ourselves writing Conditional Test Logic (page 200) in the result veriﬁ cation part of our tests. That is, our calls to Assertion Methods are embedded in if statements or loops.\n\nThe result veriﬁ cation parts of our tests suffer from Obscure Test (page 186) because we use procedural rather than declarative result veriﬁ cation in the tests.\n\nWe ﬁ nd ourselves doing Frequent Debugging (page 248) whenever\n\nassertions fail because they do not provide enough information.\n\nA key reason for moving the assertion logic out of the tests and into Custom Assertions is to Minimize Untestable Code (see page 44). Once the veriﬁ cation logic has been moved into a Custom Assertion, we can write Custom Assertion Tests (see Custom Assertion on page 474) to prove the veriﬁ cation logic is work- ing properly. Another important beneﬁ t of using Custom Assertions is that they help avoid Obscure Tests and make tests Communicate Intent (see page 41). That, in turn, will help produce robust, easily maintained tests.\n\nIf the veriﬁ cation logic must interact with the SUT to determine the actual outcome, we can use a Veriﬁ cation Method (see Custom Assertion) instead of a Custom Assertion. If the setup and exercise parts of the tests are also the same except for the values of the actual/expected objects, we should consider using a Parameterized Test (page 607). The primary advantage of Custom Assertions over both of these techniques is reusability; the same Custom Assertion can be\n\nwww.it-ebooks.info\n\n475\n\nCustom Assertion\n\n476\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\nreused in many different circumstances because it is independent of its context (its only contact with the outside world occurs through its parameter list).\n\nWe most commonly write Custom Assertions that are Equality Assertions (see Assertion Method), but there is no reason why we cannot write other kinds as well.\n\nVariation: Custom Equality Assertion\n\nFor custom equality assertions, the Custom Assertion must be passed an Expected Object and the actual object to be veriﬁ ed. It should also take an Assertion Mes- sage (page 370) to avoid playing Assertion Roulette (page 224). Such an assertion is essentially an equals method implemented as a Foreign Method [Fowler].\n\nVariation: Object Attribute Equality Assertion\n\nWe often run across Custom Assertions that take one actual object and several different Expected Objects that need to be compared with speciﬁ c attributes of the actual object. (The set of attributes to be compared is implied by the name of the Custom Assertion.) The key difference between these Custom Assertions and a Veriﬁ cation Method is that the latter interacts with the SUT while the Object Attribute Equality Assertion looks only at the objects passed in as parameters.\n\nVariation: Domain Assertion\n\nAll of the built-in Assertion Methods are domain independent. Custom Equal- ity Assertions implement test-speciﬁ c equality but still compare only two objects. Another style of Custom Assertion helps contribute to the deﬁ nition of a “domain-speciﬁ c” Higher-Level Language (see page 41)—namely, the Domain Assertion.\n\nA Domain Assertion is a Stated Outcome Assertion (see Assertion Method) that states something that should be true in domain-speciﬁ c terms. It helps elevate the test into “business-speak.”\n\nVariation: Diagnostic Assertion\n\nSometimes we ﬁ nd ourselves doing Frequent Debugging whenever a test fails because the assertions tell us only that something is wrong but do not identify the speciﬁ c problem (e.g., the assertions indicate these two objects are not equal but it isn’t clear what isn’t equal about the object). We can write a special kind of Custom Assertion that may look just like one of the built-in assertions but pro- vide more information about what is different between the expected and actual values than a built-in assertion because it is speciﬁ c to our types. (For example, it might tell us which attributes are different or where long strings differ.)\n\nwww.it-ebooks.info\n\nCustom Assertion\n\nOn one project, we were comparing string variables containing XML. Whenever a test failed, we had to bring up two string inspectors and scroll through them looking for the difference. Finally, we got smart and included the logic in a Custom Assertion that told us where the ﬁ rst difference between the two XML strings occurred. The small amount of time we spent writing the diagnostic custom assertion was paid back many times over as we ran our tests.\n\nVariation: Veriﬁ cation Method\n\nIn customer tests, a lot of the complexity of verifying the outcome is related to interacting with the SUT. Veriﬁ cation Methods are a form of Custom Asser- tions that interact directly with the SUT, thereby relieving their callers from this task. This simpliﬁ es the tests signiﬁ cantly and leads to a more “declarative” style of outcome speciﬁ cation. After the Custom Assertion has been written, we can write subsequent tests that result in the same outcome much more quickly. In some cases, it may be advantageous to incorporate even the exercise SUT phase of the test into the Veriﬁ cation Method. This is one step short of a full Parameterized Test that incorporates all the test logic in a reusable Test Utility Method (page 599).\n\nImplementation Notes\n\nThe Custom Assertion is typically implemented as a set of calls to the various built-in Assertion Methods. Depending on how we plan to use it in our tests, we may also want to include the standard Equality Assertion template to ensure correct behavior with null parameters. Because the Custom Assertion is itself an Assertion Method, it should not have any side effects, nor should it call the SUT. (If it needs to do so, it would be a Veriﬁ cation Method.)\n\nVariation: Custom Assertion Test\n\nTesting zealots would also write a Custom Assertion Test (a Self-Checking Test— see page 26—for Custom Assertions) to verify the Custom Assertion. The beneﬁ t from doing so is obvious: increased conﬁ dence in our tests. In most cases, writing Custom Assertion Tests isn’t particularly difﬁ cult because Assertion Methods take all their arguments as parameters.\n\nWe can treat the Custom Assertion as the SUT simply by calling it with various arguments and verifying that it fails in the right cases. Single-Outcome Assertions (see Assertion Method) need only a single test because they don’t take any parameters (other than possibly an Assertion Message). Stated Outcome Assertions need one\n\nwww.it-ebooks.info\n\n477\n\nCustom Assertion\n\n478\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\ntest for each possible value (or boundary value). Equality Assertions need one test that compares two objects deemed to be equivalent, one test that compares an object with itself, and one test for each attribute whose inequality should cause the assertion to fail. Attributes that don’t affect equality can be veriﬁ ed in one additional test because the Equality Assertion should not raise an error for any of them.\n\nThe Custom Assertions follow the normal Simple Success Test (see Test Method on page 348) and Expected Exception Test (see Test Method) tem- plates with one minor difference: Because the Assertion Method is the SUT, the exercise SUT and verify outcome phases of the Four-Phase Test (page 358) are combined into a single phase.\n\nEach test consists of setting up the Expected Object and the actual object and then calling the Custom Assertion. If the objects should be equivalent, that’s all there is to it. (The Test Automation Framework described on page 298 would catch any assertion failures and fail the test.) For the tests where we expect the Custom Assertion to fail, we can write the test as an Expected Exception Test (except that the exercise SUT and verify outcome phases of the Four-Phase Test are combined into the single call to the Custom Assertion).\n\nThe simplest way to build the objects to be compared for a speciﬁ c test is to do something similar to One Bad Attribute (see Derived Value on page 718)— that is, build the ﬁ rst object and make a deep copy of it. For successful tests, modify any of the attributes that should not be compared. For each test failure, modify one attribute that should be grounds for failing the assertion.\n\nA brief warning about a possible complication in a few members of the xUnit family: If all of the test failure handling does not occur in the Test Runner (page 377), calls to fail or built-in assertions may add messages to the failure log even if we catch the error or exception in our Custom Assertion Test. The only way to circumvent this behavior is to use an “Encapsulated Test Runner” to run each test by itself and verify that the one test failed with the expected error message.\n\nMotivating Example\n\nIn the following example, several test methods repeat the same series of assertions:\n\npublic void testInvoice_addOneLineItem_quantity1_b() { // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1);\n\nwww.it-ebooks.info\n\nCustom Assertion\n\n// Verify only item LineItem expItem = new LineItem(inv, product, QUANTITY); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\npublic void testRemoveLineItemsForProduct_oneOfTwo() { // Setup Invoice inv = createAnonInvoice(); inv.addItemQuantity(product, QUANTITY); inv.addItemQuantity(anotherProduct, QUANTITY); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.removeLineItemForProduct(anotherProduct); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\n// // Adding TWO line items //\n\npublic void testInvoice_addTwoLineItems_sameProduct() { Invoice inv = createAnonInvoice(); LineItem expItem1 = new LineItem(inv, product, QUANTITY1); LineItem expItem2 = new LineItem(inv, product, QUANTITY2); // Exercise inv.addItemQuantity(product, QUANTITY1); inv.addItemQuantity(product, QUANTITY2); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 2); // Verify ﬁrst item LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem1.getInv(), actual.getInv()); assertEquals(expItem1.getProd(), actual.getProd()); assertEquals(expItem1.getQuantity(), actual.getQuantity()); // Verify second item actual = (LineItem)lineItems.get(1); assertEquals(expItem2.getInv(), actual.getInv()); assertEquals(expItem2.getProd(), actual.getProd()); assertEquals(expItem2.getQuantity(), actual.getQuantity()); }\n\nwww.it-ebooks.info\n\n479\n\nCustom Assertion\n\n480\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\nNote that the ﬁ rst test ends with a series of three assertions and the second test repeats the series of three assertions twice, once for each line item. This is clearly a bad case of Test Code Duplication.\n\nRefactoring Notes\n\nRefactoring zealots can probably see that the solution is to do an Extract Meth- od [Fowler] refactoring on these tests. If we pull out all the common calls to Assertion Methods, we will be left with only the differences in each test. The extracted method is our Custom Assertion. We may also need to introduce an Expected Object to hold all the values that were being passed to the individual Assertion Methods on a single object to be passed to the Custom Assertion.\n\nExample: Custom Assertion\n\nIn this test, we use a Custom Assertion to verify that LineItem matches the expected LineItem(s). For one reason or another, we have chosen to implement a test-speciﬁ c equality rather than using a standard Equality Assertion.\n\npublic void testInvoice_addOneLineItem_quantity1_() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); // Verify only item LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"LineItem\", expItem, actual); }\n\npublic void testAddItemQuantity_sameProduct_() { Invoice inv = createAnonInvoice(); LineItem expItem1 = new LineItem(inv, product, QUANTITY1); LineItem expItem2 = new LineItem(inv, product, QUANTITY2); // Exercise inv.addItemQuantity(product, QUANTITY1); inv.addItemQuantity(product, QUANTITY2); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 2); // Verify ﬁrst item LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"Item 1\",expItem1,actual); // Verify second item\n\nwww.it-ebooks.info\n\nCustom Assertion\n\nactual = (LineItem)lineItems.get(1); assertLineItemsEqual(\"Item 2\",expItem2, actual); }\n\nThe tests have become signiﬁ cantly smaller and more intent-revealing. We have also chosen to pass a string indicating which item we are examining as an argu- ment to the Custom Assertion to avoid playing Assertion Roulette when a test fails.\n\nThis simpliﬁ ed test was made possible by having the following Custom\n\nAssertion available to us:\n\nstatic void assertLineItemsEqual( String msg, LineItem exp, LineItem act) { assertEquals(msg+\" Inv\", exp.getInv(),act.getInv()); assertEquals(msg+\" Prod\", exp.getProd(), act.getProd()); assertEquals(msg+\" Quan\", exp.getQuantity(), act.getQuantity()); }\n\nThis Custom Assertion compares the same attributes of the object as we were comparing on an in-line basis in the previous version of the test; thus the seman- tics of the test haven’t changed. We also concatenate the name of the attribute being compared with the message parameter to get a unique failure message, which allows us to avoid playing Assertion Roulette when a test fails.\n\nExample: Domain Assertion\n\nIn this next version of the test, we have further elevated the level of the asser- tions to better communicate the expected outcome of the test scenarios:\n\npublic void testAddOneLineItem_quantity1() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity( product, QUANTITY); // Verify assertInvoiceContainsOnlyThisLineItem( inv, expItem); }\n\npublic void testRemoveLineItemsForProduct_oneOfTwo_() { Invoice inv = createAnonInvoice(); inv.addItemQuantity( product, QUANTITY); inv.addItemQuantity( anotherProduct, QUANTITY); LineItem expItem = new LineItem( inv, product, QUANTITY); // Exercise inv.removeLineItemForProduct( anotherProduct ); // Verify assertInvoiceContainsOnlyThisLineItem( inv, expItem); }\n\nwww.it-ebooks.info\n\n481\n\nCustom Assertion\n\n482\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\nThis simpliﬁ ed version of the test was made possible by extracting the following Domain Assertion method:\n\nvoid assertInvoiceContainsOnlyThisLineItem( Invoice inv, LineItem expItem) { List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"item\",expItem, actual); }\n\nThis example chose to forgo passing a message to the Domain Assertion to save a bit of space. In real life, we would typically include a message string in the parameter list and concatenate the messages of the individual assertions to one passed in. See Assertion Message (page 370) for more details.\n\nExample: Veriﬁ cation Method\n\nIf the exercise SUT and result veriﬁ cation phases of several tests are pretty much identical, we can incorporate both phases into our reusable Custom Assertion. Because this approach changes the semantics of the Custom Assertion from being just a function free of side effects to an operation that changes the state of the SUT, we usually give it a more distinctive name starting with “verify”.\n\nThis version of the test merely sets up the test ﬁ xture before calling a Veriﬁ ca- tion Method that incorporates both the exercise SUT and verify outcome phases of the test. It is most easily recognized by the lack of a distinct “exercise” phase in the calling test and the presence of calls to methods that modify the state of one of the objects passed as a parameter of the Veriﬁ cation Method.\n\npublic void testAddOneLineItem_quantity2() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise & Verify verifyOneLineItemCanBeAdded(inv, product, QUANTITY, expItem); }\n\nThe Veriﬁ cation Method for this example looks like this:\n\npublic void verifyOneLineItemCanBeAdded( Invoice inv, Product product, int QUANTITY, LineItem expItem) { // Exercise inv.addItemQuantity(product, QUANTITY); // Verify assertInvoiceContainsOnlyThisLineItem(inv, expItem); }\n\nwww.it-ebooks.info\n\nCustom Assertion\n\nThis Veriﬁ cation Method calls the “pure” Custom Assertion, although it could just as easily have included all the assertion logic if we didn’t have the other Cus- tom Assertion to call. Note the call to addItemQuantity on the parameter inv; this is what changes if from a Custom Assertion to a Veriﬁ cation Method.\n\nExample: Custom Assertion Test\n\nThis Custom Assertion isn’t particularly complicated, so we may feel comfort- able without having any automated tests for it. If there is anything complex about it, however, we may ﬁ nd it worthwhile to write tests like these:\n\npublic void testassertLineItemsEqual_equivalent() { Invoice inv = createAnonInvoice(); LineItem item1 = new LineItem(inv, product, QUANTITY1); LineItem item2 = new LineItem(inv, product, QUANTITY1); // exercise/verify assertLineItemsEqual(\"This should not fail\",item1, item2); }\n\npublic void testassertLineItemsEqual_differentInvoice() { Invoice inv1 = createAnonInvoice(); Invoice inv2 = createAnonInvoice(); LineItem item1 = new LineItem(inv1, product, QUANTITY1); LineItem item2 = new LineItem(inv2, product, QUANTITY1); // exercise/verify try { assertLineItemsEqual(\"Msg\",item1, item2); } catch (AssertionFailedError e) { assertEquals(\"e.getMsg\", \"Invoice-expected: <123> but was <124>\", e.getMessage()); return; } fail(\"Should have thrown exception\"); }\n\npublic void testassertLineItemsEqual_differentQuantity() { Invoice inv = createAnonInvoice(); LineItem item1 = new LineItem(inv, product, QUANTITY1); LineItem item2 = new LineItem(inv, product, QUANTITY2); // exercise/verify try { assertLineItemsEqual(\"Msg\",item1, item2); } catch (AssertionFailedError e) { pass(); // to indicate that no assertion is needed return; } fail(\"Should have thrown exception\"); }\n\nwww.it-ebooks.info\n\n483\n\nCustom Assertion\n\n484\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\nThis example includes a few of the Custom Assertion Tests needed for this Custom Assertion. Note that the code includes one “equivalent” and several “different” tests (one for each attribute whose difference should cause the test to fail). We have to use the second form of the Expected Exception Test template in those cases where the assertion was expected to fail, because fail throws the same exception as our assertion method. In one of the “different” tests, we have included sample logic for asserting on the exception message. (Although I’ve abridged it to save space, the example here should give you an idea of where to assert on the message.)\n\nwww.it-ebooks.info\n\nDelta Assertion\n\nDelta Assertion\n\nHow do we make tests self-checking when we cannot control the initial contents of the ﬁ xture?\n\nWe specify assertions based on differences between the pre- and post-exercise state of the SUT.\n\n1. Get Pre-test State 1. Get Pre-test State\n\nSetup Setup\n\nBefore Before\n\nExercise Exercise\n\nSUT SUT\n\nFixture Fixture Data Data\n\nVerify Verify\n\nAfter After\n\n2. Get Post-test State 2. Get Post-test State\n\nTeardown Teardown\n\n3. Compare with 3. Compare with Expected Difference Expected Difference\n\nWhen we are using a Shared Fixture (page 317) such as a test database, it can be challenging to code the assertions that state what the content of the ﬁ xture should be after the SUT has been exercised. This is because other tests may have created objects in the ﬁ xture that our assertions may detect and that may cause our assertions to fail. One solution is to isolate the current test from all other tests by using a Database Partitioning Scheme (see Database Sandbox on page 650). But what can we do if this option is not available to us?\n\nUsing Delta Assertions allows us to be less dependent on which data already\n\nexist in the Shared Fixture.\n\nHow It Works\n\nBefore exercising the SUT, we take a snapshot of relevant parts of the Shared Fixture. After exercising the SUT, we specify our assertions relative to the saved snapshot. The Delta Assertions typically verify that the number of objects has changed by the right number and that the contents of collections of objects returned by the SUT in response to our queries have been augmented by the expected objects.\n\nwww.it-ebooks.info\n\n485\n\nDelta Assertion\n\n486\n\nDelta Assertion\n\nChapter 21 Result Verification Patterns\n\nWhen to Use It\n\nWe can use a Delta Assertion whenever we don’t have full control over the test ﬁ xture and we want to avoid Interacting Tests (see Erratic Test on page 228). Using Delta Assertions will help make our tests more resilient to changes in the ﬁ xture. We can also use Delta Assertions in concert with Implicit Teardown (page 516) to detect memory or data leaks in the code that we are testing. See the sidebar “Using Delta Assertions to Detect Data Leakage” on page 487 for a more detailed description.\n\nDelta Assertions work well when tests are run one after another from the same Test Runner (page 377). Unfortunately, they cannot prevent a Test Run War (see Erratic Test) because such a problem arises when tests are run at the same time from different processes. Delta Assertions work whenever the state of the SUT and the ﬁ xture are modiﬁ ed only by our own test. If other tests are running in parallel (not before or after the current test, but at the same time), a Delta Assertion won’t be sufﬁ cient to avoid the Test Run War problem.\n\nImplementation Notes\n\nWhen saving the pre-test state of the Shared Fixture or SUT, we must make sure that the SUT cannot change our snapshot. For example, if our snapshot consists of a collection of objects returned by the SUT in response to a query, we must perform a deep copy; a shallow copy copies only the Collection object and not the objects to which it refers. Shallow copying would allow the SUT to modify the very objects it returned to us as we exercise it; as a consequence, we would lose the reference snapshot with which we are comparing the post-test state.\n\nWe can ensure that we have the correct post-test state in several different ways. Assuming that our test adds any new objects it plans to modify, one approach is to ﬁ rst check that the result collection (1) has the right number of items, (2) con- tains all the pre-test items, and (3) contains the new Expected Objects (see State Veriﬁ cation on page 462). Another approach is to remove all the saved items from the result collection and then compare what remains with the collection of new expected objects. Both of these approaches can be hidden behind a Custom Assertion (page 474) or a Veriﬁ cation Method (see Custom Assertion).\n\nwww.it-ebooks.info\n\nDelta Assertion\n\nUsing Delta Assertions to Detect Data Leakage\n\nA long time ago, on a project far away, we were experimenting with different ways to clean up our test ﬁ xtures after the customer tests. Our tests were accessing a database and leaving objects behind. This behavior caused all sorts of problems with Unrepeatable Tests (see Erratic Test on page 228) and Interacting Tests (see Erratic Test). We were also suffering from Slow Tests (page 253).\n\nEventually we hit upon the idea of keeping track of all the objects we were creating in our tests by registering them with an Automated Tear- down (page 503) mechanism. Then we found a way to stub out the data- base with a Fake Database (see Fake Object on page 551). Next we made it possible to run the same test against either the fake database or the real one. This solved many of the interaction problems when running against the fake database, although those problems still occurred when we ran the tests against the real database—tests still left objects behind, and we wanted to know why. But ﬁ rst we had to determine precisely which tests were at fault.\n\nThe solution turned out to be pretty simple. In our Fake Database—which was implemented using simple hash tables—we added a method to count the total number of objects. We simply saved this value in an instance variable in the setUp method and used it as the expected value passed to an Equality Assertion (see Assertion Method on page 362) called in the tear- Down method to verify that we had cleaned up all objects properly. [This is an example of using Delta Assertions (page 485).] Once we implemented this little trick, we quickly found out which tests were suffering from the Data Leak (see Erratic Test). We could then focus our efforts on a much smaller number of tests.\n\nEven today, we ﬁ nd it useful to be able to run the same test against the database and in memory. Similarly, we still occasionally see a test fail when the tearDown method inherited from our company-speciﬁ c Testcase Superclass (page 638) has a Delta Assertion failure. Perhaps the same idea could be applied to checking for memory leaks in programming lan- guages with manual memory management (such as C++).\n\nwww.it-ebooks.info\n\n487\n\nDelta Assertion\n\n488\n\nDelta Assertion\n\nChapter 21 Result Verification Patterns\n\nMotivating Example\n\nThe following test retrieves some objects from the SUT. It then compares the objects it actually found with the objects it expected to ﬁ nd.\n\npublic void testGetFlightsByOriginAirport_OneOutboundFlight() throws Exception { FlightDto expectedFlightDto = createNewFlightBetweenExistingAirports(); // Exercise System facade.createFlight( expectedFlightDto.getOriginAirportId(), expectedFlightDto.getDestinationAirportId()); // Verify Outcome List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( expectedFlightDto.getOriginAirportId()); assertOnly1FlightInDtoList( \"Outbound ﬂight at origin\", expectedFlightDto, ﬂightsAtOrigin); }\n\nUnfortunately, because this test used a Shared Fixture, other tests that ran before it may have added objects as well. That behavior could cause the current test to fail if we encounter additional, unexpected objects.\n\nRefactoring Notes\n\nTo convert the test to use a Delta Assertion, we must ﬁ rst take a snapshot of the data (or collection of objects) we will later be asserting on. Next, we need to modify our assertions to focus on the difference between the pre-test data/ objects and the post-test data/objects. To avoid introducing Conditional Test Logic (page 200) into the Test Method (page 348), we may want to introduce a new Custom Assertion. Although we may be able to use existing assertions (custom or otherwise) as a starting point, we’ll probably have to modify them to take the pre-test data into account.\n\nExample: Delta Assertion\n\nIn this version of the test, we use a Delta Assertion to verify the objects added when we exercised the SUT. Here we are verifying that we have one more object than before and that the collection of objects returned by the SUT includes the new Expected Object and all objects that it previously contained.\n\npublic void testCreateFlight_Delta() throws Exception { FlightDto expectedFlightDto = createNewFlightBetweenExistingAirports();\n\nwww.it-ebooks.info\n\nDelta Assertion\n\n// Remember prior state List ﬂightsBeforeCreate = facade.getFlightsByOriginAirport( expectedFlightDto.getOriginAirportId()); // Exercise system facade.createFlight( expectedFlightDto.getOriginAirportId(), expectedFlightDto.getDestinationAirportId()); // Verify outcome relative to prior state List ﬂightsAfterCreate = facade.getFlightsByOriginAirport( expectedFlightDto.getOriginAirportId()); assertFlightIncludedInDtoList( \"new ﬂight \", expectedFlightDto, ﬂightsAfterCreate); assertAllFlightsIncludedInDtoList( \"previous ﬂights\", ﬂightsBeforeCreate, ﬂightsAfterCreate); assertEquals( \"Number of ﬂights after create\", ﬂightsBeforeCreate.size()+1, ﬂightsAfterCreate.size()); }\n\nBecause the SUT returns Data Transfer Objects [CJ2EEP], we can be assured that the objects we saved before exercising the SUT cannot possibly change. We have modiﬁ ed our Custom Assertions to ignore the pre-test objects (by not insisting that the Expected Object is the only one) and have written a new Cus- tom Assertion that ensures all pre-test objects are also present. Another way to accomplish this task is to remove the pre-test objects from the result collection and then verify that only the new Expected Objects are left.\n\nI’ve omitted the implementation of the Custom Assertions, as it is purely an exercise in comparing objects and is not salient to understanding the Delta Assertion pattern. The “test infected” among us would, of course, write the Custom Assertions driven by some Custom Assertion Tests (see Custom Assertion).\n\nwww.it-ebooks.info\n\n489\n\nDelta Assertion\n\n490\n\nGuard Assertion\n\nChapter 21 Result Verification Patterns\n\nGuard Assertion\n\nHow do we avoid Conditional Test Logic?\n\nWe replace an if statement in a test with an assertion that fails the test if not satisﬁ ed.\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nobj obj\n\nAssertNotNull AssertNotNull\n\nTeardown Teardown\n\nobj.attr obj.attr\n\nAssert Assert\n\nSome veriﬁ cation logic may fail because information returned by the SUT is not initialized as expected. When a test encounters an unexpected problem, it may produce a test error rather than a test failure. While the Test Runner (page 377) does its best to provide useful diagnostic information, the test automater can of- ten do better by checking for the particular condition and reporting it explicitly. A Guard Assertion is a good way to do so without introducing Conditional\n\nTest Logic (page 200).\n\nHow It Works\n\nTests either pass or fail. We fail tests by calling Assertion Methods (page 362) that stop the test from executing further if the assertion’s condition is not met. Alternatively, we can replace Conditional Test Logic that is used to avoid ex- ecuting assertions when they would cause test errors with assertions that fail the test instead. This pattern also documents the fact that we expect the condition of the Guard Assertion to be true. A failure of the Guard Assertion makes it very\n\nwww.it-ebooks.info\n\nGuard Assertion\n\nclear that some condition we expected to be true was not; it eliminates the effort needed to infer the test result from the conditional logic.\n\nWhen to Use It\n\nWe should use a Guard Assertion whenever we want to avoid executing state- ments in our Test Method (page 348) because they would cause an error if they were executed when some condition related to values returned by the SUT is not true. We take this step instead of putting an if then else fail code construct around the sensitive statements. Normally, a Guard Assertion is placed between the exer- cise SUT and the verify outcome phases of a Four-Phase Test (page 358).\n\nVariation: Shared Fixture State Assertion\n\nWhen the test uses a Shared Fixture (page 317), a Guard Assertion can also be useful at the beginning of the test (before the exercise SUT phase) to verify that the Shared Fixture satisﬁ es the test’s needs. It also makes it clearer to the test reader which parts of the Shared Fixture this test actually uses; the greater clar- ity improves the likelihood of achieving Tests as Documentation (see page 23).\n\nImplementation Notes\n\nWe can use Stated Outcome Assertions (see Assertion Method) like assertNotNil and Equality Assertions (see Assertion Method) like assertEquals as Guard Assertions that fail the test and prevent execution of other statements that would cause test errors.\n\nMotivating Example\n\nConsider the following example:\n\npublic void testWithConditionals() throws Exception { String expectedLastname = \"smith\"; List foundPeople = PeopleFinder. ﬁndPeopleWithLastname(expectedLastname); if (foundPeople != null) { if (foundPeople.size() == 1) { Person solePerson = (Person) foundPeople.get(0); assertEquals( expectedLastname,solePerson.getName()); } else { fail(\"list should have exactly one element\"); } } else { fail(\"list is null\"); } }\n\nwww.it-ebooks.info\n\n491\n\nGuard Assertion\n\n492\n\nGuard Assertion\n\nChapter 21 Result Verification Patterns\n\nThis example includes plenty of conditional statements that the author might get wrong—things like writing (foundPeople == null) instead of (foundPeople != null). In C-based languages, one might mistakenly use = instead of ==, which would result in the test always passing!\n\nRefactoring Notes\n\nWe can use a Replace Nested Conditional with Guard Clauses [Fowler] refactoring to transform this spider web of Conditional Test Logic into a nice linear sequence of statements. (In a test, even a single conditional statement is considered too much and hence “nested”!) We can use Stated Outcome Assertions to check for null object references and Equality Assertions to verify the number of objects in the collection. If each assertion is satisﬁ ed, the test proceeds. If they are not satisﬁ ed, the test ends in failure before it reaches the next statement.\n\nExample: Simple Guard Assertion\n\nThis simpliﬁ ed version of the test replaces all conditional statements with asser- tions. It is shorter than the original test and much easier to read.\n\npublic void testWithoutConditionals() throws Exception { String expectedLastname = \"smith\"; List foundPeople = PeopleFinder. ﬁndPeopleWithLastname(expectedLastname); assertNotNull(\"found people list\", foundPeople); assertEquals( \"number of people\", 1, foundPeople.size() ); Person solePerson = (Person) foundPeople.get(0); assertEquals( \"last name\", expectedLastname, solePerson.getName() ); }\n\nWe now have a single linear execution path through this Test Method (page 348); it should improve our conﬁ dence in the correctness of this test immensely!\n\nExample: Shared Fixture Guard Assertion\n\nHere’s an example of a test that depends on a Shared Fixture. If a previous test (or even this test in a previous test run) modiﬁ es the state of the ﬁ xture, our SUT could return unexpected results. It might take a fair bit of effort to determine that the problem lies in the test’s pre-conditions rather than being a bug in the SUT. We can avoid all of this trouble by making the assumptions of this test explicit through the use of a Guard Assertion during the ﬁ xture lookup phase of the test.\n\nwww.it-ebooks.info\n\nGuard Assertion\n\npublic void testAddFlightsByFromAirport_OneOutboundFlight_GA() throws Exception { // Fixture Lookup List ﬂights = facade.getFlightsByOriginAirport( ONE_OUTBOUND_FLIGHT_AIRPORT_ID ); // Guard Assertion on Fixture Contents assertEquals( \"# ﬂights precondition\", 1, ﬂights.size()); FlightDto ﬁrstFlight = (FlightDto) ﬂights.get(0); // Exercise System BigDecimal ﬂightNum = facade.createFlight( ﬁrstFlight.getOriginAirportId(), ﬁrstFlight.getDestAirportId()); // Verify Outcome FlightDto expFlight = (FlightDto) ﬁrstFlight.clone(); expFlight.setFlightNumber( ﬂightNum ); List actual = facade.getFlightsByOriginAirport( ﬁrstFlight.getOriginAirportId()); assertExactly2FlightsInDtoList( \"Flights at origin\", ﬁrstFlight, expFlight, actual); }\n\nWe now have a way to determine that the assumptions were violated without extensive debugging! This is another way we achieve Defect Localization (see page 22). This time the defect is in the tests’ assumptions on the previously run tests’ behavior.\n\nwww.it-ebooks.info\n\n493\n\nGuard Assertion\n\n494\n\nUnﬁ nished Test Assertion\n\nChapter 21 Result Verification Patterns\n\nUnﬁ nished Test Assertion\n\nHow do we structure our test logic to avoid leaving tests unﬁ nished?\n\nWe ensure that incomplete tests fail by executing an assertion that is guaranteed to fail.\n\nvoid testSomething() { // Outline: // create a ﬂight in ... state // call the ... method // verify ﬂight is in ... state fail(\"Unﬁnished Test!\"); }\n\nWhen we start deﬁ ning the tests for a particular piece of code, it is useful to “rough in” the tests by deﬁ ning Test Methods (page 348) on the appropriate Testcase Class (page 373) as we think of the test conditions. We do, however, want to ensure that we don’t accidentally forget to ﬁ ll in the bodies of these tests if we get distracted. We want the tests to fail until we ﬁ nish coding them.\n\nIncluding an Unﬁ nished Test Assertion is a good way to make sure we don’t\n\nforget.\n\nHow It Works\n\nWe put a single call to fail in each Test Method as we deﬁ ne it. The fail method is a Single-Outcome Assertion (see Assertion Method on page 362) that always fails. We include the Assertion Message (page 370) “Unﬁ nished Test” as a reminder of why the test fails when we do run the tests.\n\nWhen to Use It\n\nWe should not deliberately write any tests that might accidentally pass. A failing test makes a good reminder that we still have work to do. We can remind our- selves of this work by putting an Unﬁ nished Test Assertion at the end of every test we write and by removing it only when we are satisﬁ ed that the test is coded properly. There is no real cost to doing so, but a lot of beneﬁ t. It is just a matter of getting into the habit. Some IDEs even help us out by letting us put the Unﬁ n- ished Test Assertion into the code generation template for a Test Method\n\nIf we need to check in the tests before all code is working, we shouldn’t remove the tests or the Unﬁ nished Test Assertions just to get a green bar, as this could\n\nwww.it-ebooks.info\n\nUnfinished Test Assertion\n\nresult in Lost Tests (see Production Bugs on page 268). Instead, we can add an [Ignore] attribute to the test if our member of the xUnit family supports it, rename the test method if xUnit uses name-based Test Discovery (page 393), or exclude the entire Testcase Class from the AllTests Suite (see Named Test Suite on page 592) if we are using Test Enumeration (page 399) at the suite level.\n\nImplementation Notes\n\nMost members of the xUnit family have a fail method already deﬁ ned. If the member that we are using doesn’t include it, we should avoid the temptation to sprinkle assertTrue(false) throughout our code. This kind of code is obtuse and easy to get wrong because it is counter-intuitive. Instead, we should take a minute to write this method ourselves as a Custom Assertion (page 474) and write the Custom Assertion Test (see Custom Assertion) for it ﬁ rst to make sure we got it right.\n\nSome IDEs include the ability to customize code generation templates. Some even include a template for a Test Method that includes an Unﬁ nished Test Assertion.\n\nMotivating Example\n\nConsider the following Testcase Class that we are roughing in:\n\npublic void testPull_emptyStack() {\n\n}\n\npublic void testPull_oneItemOnStack () {\n\n}\n\npublic void testPull_twoItemsOnStack () { //To do: Write this test }\n\npublic void testPull_oneItemsRemainingOnStack () { //To do: Write this test }\n\nIncluding the // To do: ... comments may remind us that the test still needs work if our IDE supports that feature—but it won’t remind us of the unﬁ nished work when we run the tests. Running this Testcase Class will result in a green bar even though we may not have implemented our stack at all!\n\nwww.it-ebooks.info\n\n495\n\nUnﬁ nished Test Assertion\n\n496\n\nUnﬁ nished Test Assertion\n\nChapter 21 Result Verification Patterns\n\nRefactoring Notes\n\nTo implement Unﬁ nished Test Assertion all we need to do is add the following line to each test as we rough it in:\n\nfail(\"Unﬁnished Test!\");\n\nThe exclamation mark is optional. It might be even better to create a Custom Assertion such as this one:\n\nprivate void unﬁnishedTest() { fail(\"Test not implemented!\"); }\n\nThis would allow us to ﬁ nd all the Unﬁ nished Test Assertions easily by using the “search for references” feature of our IDE.\n\nExample: Unﬁ nished Test Assertion\n\nHere are the tests with an Unﬁ nished Test Assertion added to each one:\n\npublic void testPull_emptyStack() { unﬁnishedTest(); }\n\npublic void testPull_oneItemOnStack () { unﬁnishedTest(); }\n\npublic void testPull_twoItemsOnStack() { unﬁnishedTest(); }\n\npublic void testPull_oneItemsRemainingOnStack () { unﬁnishedTest(); }\n\nNow we have a Testcase Class that is guaranteed to fail until we ﬁ nish writing the code. The failing tests act as a “to do” list for writing the tests.\n\nExample: Unﬁ nished Test Method Generation from Template\n\nEclipse (version 3.0) is an example of an IDE that includes the ability to custom- ize templates. Its testmethod template inserts the following code into our Testcase Class:\n\npublic void testname() throws Exception { fail(\"ClassName::testname not implemented\"); }\n\nwww.it-ebooks.info\n\nUnfinished Test Assertion\n\nThe strings “ClassName” and “testname” are placeholders for the names of the Testcase Class and Test Method, respectively; they are ﬁ lled in automatically by the IDE. As we modify the test name in the signature, the test name in the fail statement is adjusted automatically. All we have to do to insert a new Test Method into a class is to type “testmethod” and then press CTRL-SPACEBAR.\n\nwww.it-ebooks.info\n\n497\n\nUnﬁ nished Test Assertion\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nChapter 22\n\nFixture Teardown Patterns\n\nPatterns in This Chapter\n\nTeardown Strategy\n\nGarbage-Collected Teardown. . . . . . . . . . . . . . . . . . . . . . . . . . . 500\n\nAutomated Teardown. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503\n\nCode Organization\n\nIn-line Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n\nImplicit Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516\n\n499\n\nwww.it-ebooks.info\n\nFixture Teardown Patterns",
      "page_number": 525
    },
    {
      "number": 22,
      "title": "Fixture Teardown Patterns",
      "start_page": 563,
      "end_page": 584,
      "detection_method": "regex_chapter",
      "content": "500\n\nGarbage- Collected Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nGarbage-Collected Teardown\n\nHow do we tear down the Test Fixture?\n\nWe let the garbage collection mechanism provided by the programming language clean up after our test.\n\nTestcase Class Testcase Class\n\nSetup Setup\n\nSetup Setup\n\nFixture Fixture\n\ntest_method_1 test_method_1\n\ntest_method_2 test_method_2\n\nSUT SUT\n\nSetup Setup\n\ntest_method_n test_method_n\n\ntearDown tearDown\n\nGarbage Garbage Collection Collection\n\nTeardown Teardown\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ x- ture is torn down after each test and a new one created for the next test run. This strategy is known as a Fresh Fixture (page 311). In languages that provide garbage collection, much of the teardown can happen automatically if we refer to resources via local and instance variables.\n\nHow It Works\n\nMany of the objects created during the course of our test (including both ﬁ xture setup and exercising the SUT) are transient objects that are kept alive only as long as there is a reference to them somewhere in the program that created them. The garbage collection mechanisms of modern languages use various algorithms to detect “garbage.” What is most important, though, is how they determine that something is not garbage: Any object that is reachable from any other live object or from global (i.e., static) variables will not be garbage collected.\n\nWhen running our tests, the Test Automation Framework (page 298) creates a Testcase Object (page 382) for each Test Method (page 348) in our Testcase\n\nwww.it-ebooks.info\n\nGarbage-Collected Teardown\n\nClass (page 373) and adds those objects to a Test Suite Object (page 387). When- ever a new test run is started, the framework typically throws away the existing test suite and builds a new one (to be sure everything is fresh). When the old test suite is discarded, any objects referenced only by instance variables in those tests become candidates for garbage collection.\n\nWhen to Use It\n\nWe should use Garbage-Collected Teardown whenever we possibly can because it will save us a lot of effort!\n\nIf our programming takes place in an environment that doesn’t support garbage collection, or if we have resources that aren’t garbage collected automatically (e.g., ﬁ les, sockets, records in a database), we’ll need to destroy or free those resources explicitly. If we are using a Shared Fixture (page 317), we won’t be able to use Garbage-Collected Teardown unless we do something fancy to hold the reference to the ﬁ xture in such a way that it will go out of scope when our test suite has ﬁ nished running.\n\nWe can use In-line Teardown (page 509), Implicit Teardown (page 516), or\n\nAutomated Teardown (page 503) to ensure that they are released properly.\n\nImplementation Notes\n\nSome members of the xUnit family and some IDEs go so far as to replace the classes each time the test suite is run. We may see this behavior show up as an option called “Reload Classes” or it may be forced upon us. We must be care- ful if we decide to take advantage of this feature to perform Garbage-Collected Teardown with ﬁ xture holding class variables, as our tests may stop running if we change IDEs or try running our tests from the command line (e.g., from “Cruise Control” or a build script.)\n\nMotivating Example\n\nThe following test creates some in-memory objects during ﬁ xture setup and explicitly destroys them using In-line Teardown. (We could have used Implicit Teardown in this example but that just makes it harder for readers to see what is going on.)\n\npublic void testCancel_proposed_UIT() { // ﬁxture setup Flight proposedFlight = createAnonymousProposedFlight(); // exercise SUT proposedFlight.cancel();\n\nwww.it-ebooks.info\n\n501\n\nGarbage- Collected Teardown\n\n502\n\nGarbage- Collected Teardown\n\nChapter 22 Fixture Teardown Patterns\n\n// verify outcome try{ assertEquals( FlightState.CANCELLED, proposedFlight.getStatus()); } ﬁnally { // teardown proposedFlight.delete(); proposedFlight = null; } }\n\nBecause these objects are not persistent, the code to delete the proposedFlight is unnecessary and just makes the test more complicated and harder to understand.\n\nRefactoring Notes\n\nTo convert to Garbage-Collected Teardown, we need only remove the unneces- sary cleanup code. If we had been using a class variable to hold the reference to the object, we would have had to convert it to either an instance variable or a local variable, both of which would have moved us from a Shared Fixture to a Fresh Fixture.\n\nExample: Garbage-Collected Teardown\n\nIn this reworked test, we let Garbage-Collected Teardown do the job for us.\n\npublic void testCancel_proposed_GCT() { // ﬁxture setup Flight proposedFlight = createAnonymousProposedFlight(); // exercise SUT proposedFlight.cancel(); // verify outcome assertEquals( FlightState.CANCELLED, proposedFlight.getStatus()); // teardown // Garbage collected when proposedFlight goes out of scope }\n\nNote how much simpler the test has become!\n\nwww.it-ebooks.info\n\nAutomated Teardown\n\nAutomated Teardown\n\nHow do we tear down the Test Fixture?\n\nWe keep track of all resources that are created in a test and automatically destroy/free them during teardown.\n\nTestcase Class Testcase Class\n\nCreation Creation\n\nCreation Creation\n\nFixture Fixture\n\ntest_method_1 test_method_1\n\ntest_method_2 test_method_2\n\nSUT SUT\n\nRegister Register Test Object Test Object\n\ntest_method_n test_method_n\n\nTeardown Teardown\n\ntearDown tearDown\n\nDestroy All Destroy All\n\nTest Object Test Object Registry Registry\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ xture is torn down after each test and a new one created for the next test run. This strategy is known as a Fresh Fixture (page 311). Leftover objects and database records, as well as open ﬁ les and connections, can at best cause performance degradations and at worst cause tests to fail or systems to crash. While some of these resources may be cleaned up automatically by garbage collection, others may be left hanging if they are not torn down explicitly.\n\nWriting teardown code that can be relied upon to clean up properly in all possible circumstances is challenging and time-consuming. It involves understanding what could be left over for each possible outcome of the test and writing code to deal with that scenario. This Complex Teardown (see Obscure Test on page 186) introduces a fair bit of Conditional Test Logic (page 200) and—worst of all—Untestable Test Code (see Hard-to-Test Code on page 209).\n\nA better solution is to let the test infrastructure track the objects created and\n\nclean them up auto-magically when the test is complete.\n\nwww.it-ebooks.info\n\n503\n\nAlso known as: Test Object Registry\n\nAutomated Teardown\n\n504\n\nAutomated Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nHow It Works\n\nThe core of the solution is a mechanism to register each persistent item (i.e., object, record, connection, and so on) we create in the test. We maintain a list (or lists) of registered objects that need some action to be taken to destroy them. This can be as simple as tossing each object into a collection. At the end of the test, we traverse the collection and destroy each object. We will want to catch any errors that we encounter so that one object’s cleanup error will not cause the rest of the cleanup to be aborted.\n\nWhen to Use It\n\nWe can use Automated Teardown whenever we have persistent resources that need to be cleaned up to keep the test environment functioning. (This happens more often in customer tests than in unit tests.) This pattern addresses both Unrepeatable Tests (see Erratic Test on page 228) and Interacting Tests (see Erratic Test) by keeping the objects created in one test from lingering into the execution of a subsequent test.\n\nAutomated Teardown isn’t very difﬁ cult to build, and it will save us a large amount of grief and effort. Once we have built it for one project, we should be able to reuse the teardown logic on subsequent projects for very little effort.\n\nImplementation Notes\n\nAutomated Teardown comes in two ﬂ avors. The basic ﬂ avor tears down only objects that were created as part of ﬁ xture setup. The more advanced version also destroys any objects that were created by the SUT while it was being exercised.\n\nVariation: Automated Fixture Teardown\n\nThe simplest solution is to register the objects we create in our Creation Methods (page 415). Although this pattern will not tear down the objects created by the SUT, by dealing with our ﬁ xture it reduces the effort and likelihood of errors signiﬁ cantly.\n\nThere are two key challenges with this variation:\n\nFinding a generic way to clean up the registered objects\n\nEnsuring that our Automated Teardown code is run for each registered\n\nobject\n\nGiven that the latter challenge is the easier problem, let us deal with it ﬁ rst. When we are tearing down a Persistent Fresh Fixture (see Fresh Fixture), the\n\nwww.it-ebooks.info\n\nAutomated Teardown\n\nsimplest solution is to put the call to the Automated Teardown mechanism into the tearDown method on our Testcase Class (page 373). This method is called regardless of whether the test passes or fails as long as the setUp method succeeds. When we are tearing down a Shared Fixture (page 317), we want the tearDown method to run only after all the Test Methods (page 348) have been run. In this case, we can use either Suite Fixture Setup (page 441), if our member of the xUnit family supports it, or a Setup Decorator (page 447).\n\nNow let’s go back to the harder problem: the generic mechanism for cleaning up the resources. We have at least two options here. First, we can ensure that all persistent (non-garbage-collected) objects implement a generic cleanup mechanism that we can call from within the Automated Teardown mechanism. Alternatively, we can wrap each object in another object that knows how to clean up the object in question. The latter strategy is an example of the Command [GOF] pattern.\n\nIf we build our Automated Teardown mechanism in a completely generic way, we can include it in the Testcase Superclass (page 638) on which we can base all our Testcase Classes. Otherwise, we may need to put it onto a Test Helper (page 643) that is visible from all Testcase Classes that need it. A Test Helper that both creates ﬁ xture objects and tears them down automatically is sometimes called an Object Mother (see Test Helper).\n\nBeing a nontrivial (and very critical) piece of code, the Automated Teardown mechanism deserves its own unit tests. Because it is now outside the Test Method, we can write Self-Checking Tests (see page 26) for it! If we want to be really careful (some might say paranoid), we can use Delta Assertions (page 485) to verify that any objects that persist after the teardown operation really existed before the test was performed.\n\nVariation: Automated Exercise Teardown\n\nWe can make the tests even more “self-cleaning” by also cleaning up the objects created by the SUT. This effort involves designing the SUT using an observable Object Factory (see Dependency Lookup on page 686) so that we can automati- cally register any objects created by the SUT while it is being exercised. During the teardown phase we can delete these objects, too.\n\nMotivating Example\n\nIn this example, we create several objects using Creation Methods and need to tear them down when the test in complete. To do so, we introduce a try/ﬁ nally block to ensure that our In-line Teardown (page 509) code executes even when the assertions fail.\n\nwww.it-ebooks.info\n\n505\n\nAutomated Teardown\n\n506\n\nAutomated Teardown\n\nChapter 22 Fixture Teardown Patterns\n\npublic void testGetFlightsByOrigin_NoInboundFlight_SMRTD() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expFlightDto = createTestFlight(outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { try { facade.removeFlight(expFlightDto.getFlightNumber()); } ﬁnally { try { facade.removeAirport(inboundAirport); } ﬁnally { facade.removeAirport(outboundAirport); } } } }\n\nNote that we must use nested try/ﬁ nally constructs within the ﬁ nally block to ensure that any errors in the teardown don’t keep us from ﬁ nishing the job.\n\nRefactoring Notes\n\nIntroducing Automated Teardown involves two steps. First, we add the Automated Teardown mechanism to our Testcase Class. Second, we remove any In-line Teardown code from our tests.\n\nAutomated Teardown can be implemented on a speciﬁ c Testcase Class or it can be inherited (or mixed in) via a generic class. Either way, we need to make sure we register all of our newly created objects so that the mechanism knows to delete them when the test is ﬁ nished. This is most easily done inside Creation Methods that already exist. Alternatively, we can use an Extract Method [Fowler] refactoring to move the direct constructor calls into newly created Creation Methods and add the registration.\n\nThe generic Automated Teardown mechanism should be invoked from the tearDown method. Although this can be done on our own Testcase Class, it is almost always better to put this method on a Testcase Superclass that all our Testcase Classes inherit from. If we don’t already have a Testcase Superclass,\n\nwww.it-ebooks.info\n\nAutomated Teardown\n\nwe can easily create one by doing an Extract Class [Fowler] refactoring and then doing a Pull Up Method [Fowler] refactoring on any methods (and ﬁ elds) associated with the Automated Teardown mechanism.\n\nExample: Automated Teardown\n\nThere is not much to see in this refactored test because all of the teardown code has been removed.\n\npublic void testGetFlightsByOriginAirport_OneOutboundFlight() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = createTestAirport(\"1IF\"); FlightDto expectedFlightDto = createTestFlight( outboundAirport, inboundAirport); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", expectedFlightDto, ﬂightsAtOrigin); }\n\nHere is where all the work gets done! The Creation Method has been modiﬁ ed to register the object it just created.\n\nprivate List allAirportIds; private List allFlights;\n\nprotected void setUp() throws Exception { allAirportIds = new ArrayList(); allFlights = new ArrayList(); }\n\nprivate BigDecimal createTestAirport(String airportName) throws FlightBookingException { BigDecimal newAirportId = facade.createAirport( airportName, \" Airport\" + airportName, \"City\" + airportName); allAirportIds.add(newAirportId); return newAirportId; }\n\nNext comes the actual Automated Teardown logic. In this example, it lives on our Testcase Class and is called from the tearDown method. To keep this example very simple, this logic has been written speciﬁ cally to handle airports and ﬂ ights. More typically, it would live in the Testcase Superclass, where it could be used\n\nwww.it-ebooks.info\n\n507\n\nAutomated Teardown\n\n508\n\nAutomated Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nby all Testcase Classes, and would use a generic object destruction mechanism so that it would not have to care what types of objects it was deleting.\n\nprotected void tearDown() throws Exception { removeObjects(allAirportIds, \"Airport\"); removeObjects(allFlights, \"Flight\"); }\n\npublic void removeObjects(List objectsToDelete, String type) { Iterator i = objectsToDelete.iterator(); while (i.hasNext()) { try { BigDecimal id = (BigDecimal) i.next(); if (\"Airport\"==type) { facade.removeAirport(id); } else { facade.removeFlight(id); } } catch (Exception e) { // do nothing if the remove failed } } }\n\nIf we were tearing down a Shared Fixture, we would annotate our tearDown method with the suitable annotation or attribute (e.g., @afterClass or [TestFixtureTearDown]) or move it to a Setup Decorator.\n\nExample: Automated Exercise Teardown\n\nIf we wanted to take the next step and automatically tear down any objects created within the SUT, we could modify the SUT to use an observable Object Factory. In our test, we would add the following code:\n\nResourceTracker tracker;\n\npublic void setUp() { tracker = new ResourceTracker(); ObjectFactory.addObserver(tracker); }\n\npublic void tearDown() { tracker.cleanup(); ObjectFactory.removeObserver(tracker); }\n\nThis last example assumes that the Automated Teardown logic has been moved into the cleanup method on the ResourceTracker.\n\nwww.it-ebooks.info\n\nIn-line Teardown\n\nIn-line Teardown\n\nHow do we tear down the Test Fixture?\n\nWe include teardown logic at the end of the Test Method immediately after the result veriﬁ cation.\n\nTestcase Class Testcase Class\n\nFixture Fixture\n\ntest_method_1 test_method_1\n\ntest_method_2 test_method_2\n\nTeardown Teardown\n\nSUT SUT\n\ntest_method_n test_method_n\n\nTeardown Teardown\n\ntearDown tearDown\n\nTeardown Teardown\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ xture is torn down after each test and a new one created for the next test run. This strategy is known as a Fresh Fixture (page 311). Leftover objects and database records, as well as open ﬁ les and connections, can at best cause performance degradations and at worst cause tests to fail or systems to crash. While some of these resources may be cleaned up automatically by garbage collection, others may be left hanging if they are not torn down explicitly.\n\nAt a minimum, we should write In-line Teardown code that cleans up\n\nresources left over after our test.\n\nHow It Works\n\nAs we write a test, we mentally keep track of all objects the test creates that will not be cleaned up automatically. After writing the code to exercise the SUT and verify the outcome, we add logic to the end of the Test Method (page 348) to destroy any objects that will not be cleaned up automatically by the garbage collector. We use the relevant language feature to ensure that the teardown code is run regardless of the outcome of the test.\n\nwww.it-ebooks.info\n\n509\n\nIn-line Teardown\n\n510\n\nIn-line Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nWhen to Use It\n\nWe should use some form of teardown logic whenever we have resources that will not be freed automatically after the Test Method is run; we can use In-line Teardown when each test has different objects to clean up. We may discover that objects need to be cleaned up because we have Unrepeatable Tests (see Erratic Test on page 228) or Slow Tests (page 253) caused by the accumulation of detritus from many test runs.\n\nUnlike ﬁ xture setup, the teardown logic is not important from the perspective of Tests as Documentation (see page 23). Use of any form of teardown logic may potentially contribute to High Test Maintenance Cost (page 265) and should be avoided if at all possible. Thus the only real beneﬁ t of including the teardown logic on an in-line basis is that it may make it easier to maintain the teardown logic—a pretty slim beneﬁ t, indeed. It is almost always better to strive for Automated Tear- down (page 503) or to use Implicit Teardown (page 516) if we are using Testcase Class per Fixture (page 631), where all tests in a Testcase Class (page 373) have the same test ﬁ xture.\n\nWe can also use In-line Teardown as a steppingstone to Implicit Teardown, thereby following the principle of “the simplest thing that could possibly work.” First, we learn how to do In-line Teardown for each Test Method; next, we extract the common logic from those tests into the tearDown method. We should not use In-line Teardown if the objects created by the test are subject to automated memory management. In such a case, we should use Garbage- Collected Teardown (page 500) instead because it is much less error-prone and keeps the tests easier to understand and maintain.\n\nImplementation Notes\n\nThe primary consideration in In-line Teardown is ensuring that the teardown code actually runs even when the test is failed by an Assertion Method (page 362) or ends in an error in either the SUT or the Test Method. A secondary consider- ation is ensuring that the teardown code does not introduce additional errors.\n\nThe key to doing In-line Teardown correctly is to use language-level constructs to ensure that the teardown code is run. Most modern languages include some sort of error/exception-handling construct that will attempt the execution of a block of code with the guarantee that a second block of code will be run regard- less of how the ﬁ rst block terminates. In Java, this construct takes the form of a try block with an associated ﬁ nally block.\n\nwww.it-ebooks.info\n\nIn-line Teardown\n\nVariation: Teardown Guard Clause\n\nTo protect against a failure caused by trying to tear down a resource that doesn’t exist, we can put a “guard clause” around the logic. Its inclusion reduces the likelihood of a test error caused by the teardown logic.\n\nVariation: Delegated Teardown\n\nWe can move much of the teardown logic out of the Test Method by calling a Test Utility Method (page 599). Although this strategy reduces the amount of teardown logic cluttering the test, we still need to place an error-handling con- struct around at least the assertions and the exercising of the SUT to ensure that it gets called. Using Implicit Teardown is almost always a better solution.\n\nVariation: Naive In-line Teardown\n\nNaive In-line Teardown is what we have when we forget to put the equivalent of a try/ﬁ nally block around our test logic to ensure that our teardown logic always executes. It leads to Resource Leakage (see Erratic Test), which in turn may lead to Erratic Tests.\n\nMotivating Example\n\nThe following test creates a persistent object (airport) as part of the ﬁ xture. Because the object is stored in a database, it is not subject to Garbage-Collected Teardown and must be explicitly destroyed. If we do not include teardown logic in the test, each time the test is run it will create another object in the database. This may lead to Unrepeatable Tests unless the test uses Distinct Generated Values (see Generated Value on page 723) to ensure that the created objects do not violate any unique key constraints.\n\npublic void testGetFlightsByOriginAirport_NoFlights_ntd() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); }\n\nwww.it-ebooks.info\n\n511\n\nIn-line Teardown\n\n512\n\nIn-line Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nExample: Naive In-line Teardown\n\nIn this naive solution to this problem, we added a line after the assertion to destroy the airport created in the ﬁ xture setup.\n\npublic void testGetFlightsByOriginAirport_NoFlights() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); facade.removeAirport(outboundAirport); }\n\nUnfortunately, this solution isn’t really adequate because the teardown logic won’t be exercised if the SUT encounters an error or if the assertions fail. We could try moving the ﬁ xture cleanup before the assertions but this still wouldn’t address the issue with errors occurring inside the SUT. Clearly, we need a more general solution.\n\nRefactoring Notes\n\nWe need either to place an error-handling construct around the exercising of the SUT and the assertions or to move the teardown code into the tearDown method. Either way, we need to ensure that all the teardown code runs, even if some parts of it fail. This usually involves the judicious use of try/ﬁ nally control structures around each step of the teardown process.\n\nExample: In-line Teardown\n\nIn this Java example, we have introduced a try/ﬁ nally block around the exercise SUT and result veriﬁ cation phases of the test to ensure that our teardown code is run.\n\npublic void testGetFlightsByOriginAirport_NoFlights_td() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); try { // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size());\n\nwww.it-ebooks.info\n\nIn-line Teardown\n\n} ﬁnally { facade.removeAirport(outboundAirport); } }\n\nNow the exercising of the SUT and the assertions both appear in the try block and the teardown logic is found in the ﬁ nally block. This separation is crucial to making In-line Teardown work properly. We should not include a catch block unless we are writing an Expected Exception Test (see Test Method).\n\nExample: Teardown Guard Clause\n\nHere, we’ve added a Teardown Guard Clause to the teardown code to ensure it isn’t run if the airport doesn’t exist:\n\npublic void testGetFlightsByOriginAirport_NoFlights_TDGC() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); try { // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { if (outboundAirport!=null) { facade.removeAirport(outboundAirport); } } }\n\nExample: Multiresource In-line Teardown (Java)\n\nIf multiple resources need to be cleaned up in the same test, we must ensure that all the teardown code runs even if some of the teardown statements contain errors. This goal can be accomplished by nesting each subsequent teardown step inside another block of guaranteed code, as in this Java example:\n\npublic void testGetFlightsByOrigin_NoInboundFlight_SMRTD() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expFlightDto = createTestFlight(outboundAirport, inboundAirport);\n\nwww.it-ebooks.info\n\n513\n\nIn-line Teardown\n\n514\n\nIn-line Teardown\n\nChapter 22 Fixture Teardown Patterns\n\n// Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { try { facade.removeFlight(expFlightDto.getFlightNumber()); } ﬁnally { try { facade.removeAirport(inboundAirport); } ﬁnally { facade.removeAirport(outboundAirport); } } } }\n\nThis scheme gets very messy in a hurry if we must clean up more than a few resources. In such a situation, it makes more sense to organize the resources into an array or list and then to iterate over that array or list. At that point we are halfway to implementing Automated Teardown.\n\nExample: Delegated Teardown\n\nWe can also delegate the teardown from within the Test Method if we don’t believe we can come up with a completely generic way cleanup strategy that will work for all tests.\n\npublic void testGetFlightsByOrigin_NoInboundFlight_DTD() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expectedFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expectedFlightDto = createTestFlight( outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { teardownFlightAndAirports( outboundAirport, inboundAirport, expectedFlightDto); } }\n\nwww.it-ebooks.info\n\nIn-line Teardown\n\nprivate void teardownFlightAndAirports( BigDecimal ﬁrstAirport, BigDecimal secondAirport, FlightDto ﬂightDto) throws FlightBookingException { try { facade.removeFlight( ﬂightDto.getFlightNumber() ); } ﬁnally { try { facade.removeAirport(secondAirport); } ﬁnally { facade.removeAirport(ﬁrstAirport); } } }\n\nThe optimizers among us will notice that the two ﬂ ight numbers are actually available as attributes of the ﬂ ightDto. The paranoid will counter that because the teardownFlightAndAirports method could be called before the ﬂ ightDto is constructed, we cannot count on using it to access the Airports. Hence we must pass the Airports in individually. The need to think this way is why a generic Automated Teardown is so attractive; it avoids having to think at all!\n\nwww.it-ebooks.info\n\n515\n\nIn-line Teardown\n\n516\n\nAlso known as: Hooked Teardown, Framework- Invoked Teardown, Teardown Method\n\nImplicit Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nImplicit Teardown\n\nHow do we tear down the Test Fixture?\n\nThe Test Automation Framework calls our cleanup logic in the tearDown method after every Test Method.\n\nTestcase Class\n\nFixture\n\ntest_1\n\ntest_2\n\nSUT\n\ntest_n\n\nTeardown\n\nTeardown\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ xture is torn down after each test and a new one created for the next test run. This strategy is known as a Fresh Fixture (page 311). Leftover objects and database records, as well as open ﬁ les and connections, can at best cause performance degradations and at worst cause tests to fail or systems to crash. When we can’t take advantage of Garbage-Collected Teardown (page 500) and we have several tests with the same objects to tear down, we can put the teardown logic into a special tearDown method that the Test Automation Framework (page 298) calls after each Test Method (page 348) is run.\n\nHow It Works\n\nAnything that needs to be cleaned up can be freed or destroyed in the ﬁ nal phase of the Four-Phase Test (page 358)—namely, the ﬁ xture teardown phase. Most members of the xUnit family of Test Automation Frameworks support\n\nwww.it-ebooks.info\n\nImplicit Teardown\n\nthe concept of Implicit Teardown, wherein they call the tearDown method of each Testcase Object (page 382) after the Test Method has been run.\n\nThe tearDown method is called regardless of whether the test passes or fails. This scheme ensures that we have the opportunity to clean up, undisturbed by any failed assertions. Be aware, however, that many members of the xUnit family do not call tearDown if the setUp method raises an error.\n\nWhen to Use It\n\nWe can use Implicit Teardown whenever several tests with the same resources need to be destroyed or freed explicitly after the test has been completed and those resources will not be destroyed or freed automatically. We may discover this require- ment because we have Unrepeatable Tests (see Erratic Test on page 228) or Slow Tests (page 253) caused by the accumulation of detritus from many test runs.\n\nIf the objects created by the test are internal resources and subject to automated memory management, then Garbage-Collected Teardown may eliminate a lot of work for us. If each test has a completely different set of objects to tear down, then In-line Teardown (page 509) may be more appropriate. In many cases, we can completely avoid manually written teardown logic by using Automated Tear- down (page 503).\n\nImplementation Notes\n\nThe teardown logic in the tearDown method is most often created by refactoring from tests that had In-line Teardown. The tearDown method may need to be “ﬂ exible” or “accommodating” for several reasons:\n\nWhen a test fails or when a test error occurs, the Test Method may not\n\nhave created all the ﬁ xture objects.\n\nIf all the Test Methods in the Testcase Class (page 373) don’t use identical ﬁ xtures,1 there may be different sets of objects to clean up for different tests.\n\nVariation: Teardown Guard Clause\n\nWe can avoid arbitrarily Conditional Test Logic (page 200) if we deal with the case where only a subset of the objects to be torn down are actually present by putting a guard clause (a simple if statement) around each teardown operation\n\n1 That is, they augment the Implicit Teardown with some additional In-line Setup (page 408) or Delegated Setup (page 411).\n\nwww.it-ebooks.info\n\n517\n\nImplicit Teardown\n\n518\n\nImplicit Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nto guard against the resource not being present. With this technique, a suitably coded tearDown method can tear down various ﬁ xture conﬁ gurations. Contrast this with the setUp method, which can set up only the lowest common denominator ﬁ xture for the Test Methods that share it.\n\nMotivating Example\n\nThe following test creates several standard objects during ﬁ xture setup. Because the objects are persisted in a database, they must be cleaned up explicitly after every test. Each test (only one of several is shown here) contains the same in-line teardown logic to delete the objects.\n\npublic void testGetFlightsByOrigin_NoInboundFlight_SMRTD() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expFlightDto = createTestFlight(outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { try { facade.removeFlight(expFlightDto.getFlightNumber()); } ﬁnally { try { facade.removeAirport(inboundAirport); } ﬁnally { facade.removeAirport(outboundAirport); } } } }\n\nThere is enough Test Code Duplication (page 213) here to warrant converting these tests to Implicit Teardown.\n\nRefactoring Notes\n\nFirst, we ﬁ nd the most representative example of teardown in all the tests. Next, we do an Extract Method [Fowler] refactoring on that code and call the resulting method tearDown. Finally, we delete the teardown logic in each of the other tests.\n\nwww.it-ebooks.info\n\nImplicit Teardown\n\nWe may need to introduce Teardown Guard Clauses around any teardown logic that may not be needed in every test. We should also surround each teardown attempt with a try/ﬁ nally block to ensure that the remaining teardown logic executes even if an earlier attempt fails.\n\nExample: Implicit Teardown\n\nThis example shows the same tests with the teardown logic removed to the tearDown method. Note how much smaller the tests have become.\n\nBigDecimal outboundAirport; BigDecimal inboundAirport; FlightDto expFlightDto;\n\npublic void testGetFlightsByAirport_NoInboundFlights_NIT() throws Exception { // Fixture Setup outboundAirport = createTestAirport(\"1OF\"); inboundAirport = createTestAirport(\"1IF\"); expFlightDto = createTestFlight( outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); }\n\nprotected void tearDown() throws Exception { try { facade.removeFlight( expFlightDto.getFlightNumber() ); } ﬁnally { try { facade.removeAirport(inboundAirport); } ﬁnally { facade.removeAirport(outboundAirport); } } }\n\nNote that there is no try/ﬁ nally block around the exercising of the SUT and the assertions. This structure helps the test reader understand that this is not an Expected Exception Test (see Test Method). Also, we didn’t need to put a Guard Clause [SBPP] in front of each operation because the try/ﬁ nally block ensures that a failure is noncatastrophic; thus there is no real harm in trying to perform the operation. We did have to convert our ﬁ xture holding local variables into instance variables to allow the tearDown method to access the ﬁ xture.\n\nwww.it-ebooks.info\n\n519\n\nImplicit Teardown\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nChapter 23\n\nTest Double Patterns\n\nPatterns in This Chapter\n\nTest Double. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522\n\nTest Double Usage\n\nTest Stub. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529\n\nTest Spy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538\n\nMock Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544\n\nFake Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551\n\nTest Double Construction\n\nConﬁ gurable Test Double. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558\n\nHard-Coded Test Double . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568\n\nTest-Speciﬁ c Subclass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n\n521\n\nwww.it-ebooks.info\n\nTest Double Patterns",
      "page_number": 563
    },
    {
      "number": 23,
      "title": "Test Double Patterns",
      "start_page": 585,
      "end_page": 653,
      "detection_method": "regex_chapter",
      "content": "522\n\nAlso known as: Imposter\n\nTest Double\n\nChapter 23 Test Double Patterns\n\nTest Double\n\nHow can we verify logic independently when code it depends on is unusable? How can we avoid Slow Tests?\n\nWe replace a component on which the SUT depends with a “test-speciﬁ c equivalent.”\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nTest Test Double Double\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nTeardown Teardown\n\nSometimes it is just plain hard to test the SUT because it depends on other components that cannot be used in the test environment. Such a situation may arise because those components aren’t available, because they will not return the results needed for the test, or because executing them would have unde- sirable side effects. In other cases, our test strategy requires us to have more control over or visibility of the internal behavior of the SUT.\n\nWhen we are writing a test in which we cannot (or choose not to) use a real depended-on component (DOC), we can replace it with a Test Double. The Test Double doesn’t have to behave exactly like the real DOC; it merely has to provide the same API as the real DOC so that the SUT thinks it is the real one!\n\nwww.it-ebooks.info\n\nTest Double\n\nHow It Works\n\nWhen the producers of a movie want to ﬁ lm something that is potentially risky or dangerous for the leading actor to carry out, they hire a “stunt double” to take the place of the actor in the scene. The stunt double is a highly trained individual who is capable of meeting the speciﬁ c requirements of the scene. The stunt double may not be able to act, but he or she knows how to fall from great heights, crash a car, or do whatever the scene calls for. How closely the stunt double needs to resemble the actor depends on the nature of the scene. Usually, things can be arranged such that someone who vaguely resembles the actor in stature can take the actor’s place.\n\nFor testing purposes, we can replace the real DOC (not the SUT!) with our equivalent of the “stunt double”: the Test Double. During the ﬁ xture setup phase of our Four-Phase Test (page 358), we replace the real DOC with our Test Double. Depending on the kind of test we are executing, we may hard-code the behavior of the Test Double or we may conﬁ gure it during the setup phase. When the SUT interacts with the Test Double, it won’t be aware that it isn’t talking to the real McCoy, but we will have achieved our goal of making impossible tests possible.\n\nRegardless of which variation of Test Double we choose to use, we must keep in mind that we don’t need to implement the entire interface of the DOC. Instead, we provide only the functionality needed for our particular test. We can even build different Test Doubles for different tests that involve the same DOC.\n\nWhen to Use It\n\nWe might want to use some sort of Test Double during our tests in the following circumstances:\n\nIf we have an Untested Requirement (see Production Bugs on page 268) because neither the SUT nor its DOCs provide an observation point for the SUT’s indirect output that we need to verify using Behavior Veriﬁ - cation (page 468)\n\nIf we have Untested Code (see Production Bugs) and a DOC does not provide the control point to allow us to exercise the SUT with the nec- essary indirect inputs\n\nIf we have Slow Tests (page 253) and we want to be able to run our\n\ntests more quickly and hence more often\n\nEach of these scenarios can be addressed in some way by using a Test Double. Of course, we have to be careful when using Test Doubles because we are testing\n\nwww.it-ebooks.info\n\n523\n\nTest Double\n\n524\n\nTest Double\n\nChapter 23 Test Double Patterns\n\nthe SUT in a different conﬁ guration from the one that will be used in production. For this reason, we really should have at least one test that veriﬁ es the SUT works without a Test Double. We need to be careful that we don’t replace the parts of the SUT that we are trying to verify because that practice can result in tests that test the wrong software! Also, excessive use of Test Doubles can result in Fragile Tests (page 239) as a result of Overspeciﬁ ed Software.\n\nTest Doubles come in several major ﬂ avors, as summarized in Figure 23.1. The implementation variations of these patterns are described in more detail in the corresponding pattern write-ups.\n\nTest Test Double Double\n\nDummy Dummy Object Object\n\nTest Test Stub Stub\n\nTest Test Spy Spy\n\nMock Mock Object Object\n\nFake Fake Object Object\n\nFigure 23.1 Types of Test Doubles. Dummy Objects are really an alternative to the value patterns. Test Stubs are used to verify indirect inputs; Test Spies and Mock Objects are used to verify indirect outputs. Fake objects provide an alternative implementation.\n\nThese variations are classiﬁ ed based on how/why we use the Test Double. We will deal with variations around how we build the Test Doubles in the “Imple- mentation” section.\n\nVariation: Test Stub\n\nWe use a Test Stub (page 529) to replace a real component on which the SUT depends so that the test has a control point for the indirect inputs of the SUT. Its inclusion allows the test to force the SUT down paths it might not otherwise execute. We can further classify Test Stubs by the kind of indirect inputs they are used to inject into the SUT. A Responder (see Test Stub) injects valid values, while a Saboteur (see Test Stub) injects errors or exceptions.\n\nSome people use the term “test stub” to mean a temporary implementation that is used only until the real object or procedure becomes available. I prefer to call this usage a Temporary Test Stub (see Test Stub) to avoid confusion.\n\nwww.it-ebooks.info\n\nTest Double\n\nVariation: Test Spy\n\nWe can use a more capable version of a Test Stub, the Test Spy (page 538), as an observation point for the indirect outputs of the SUT. Like a Test Stub, a Test Spy may need to provide values to the SUT in response to method calls. The Test Spy, however, also captures the indirect outputs of the SUT as it is exercised and saves them for later veriﬁ cation by the test. Thus, in many ways, the Test Spy is “just a” Test Stub with some recording capability. While a Test Spy is used for the same fundamental purpose as a Mock Object (page 544), the style of test we write using a Test Spy looks much more like a test written with a Test Stub.\n\nVariation: Mock Object\n\nWe can use a Mock Object as an observation point to verify the indirect outputs of the SUT as it is exercised. Typically, the Mock Object also includes the func- tionality of a Test Stub in that it must return values to the SUT if it hasn’t already failed the tests but the emphasis1 is on the veriﬁ cation of the indirect outputs. Therefore, a Mock Object is a lot more than just a Test Stub plus assertions: It is used in a fundamentally different way.\n\nVariation: Fake Object\n\nWe use a Fake Object (page 551) to replace the functionality of a real DOC in a test for reasons other than veriﬁ cation of indirect inputs and outputs of the SUT. Typically, a Fake Object implements the same functionality as the real DOC but in a much simpler way. While a Fake Object is typically built speciﬁ cally for testing, the test does not use it as either a control point or an observation point.\n\nThe most common reason for using a Fake Object is that the real DOC is not available yet, is too slow, or cannot be used in the test environment because of deleterious side effects. The sidebar “Faster Tests Without Shared Fixtures” (page 319) describes how we encapsulated all database access behind a persistence layer interface and then replaced the database with in-memory hash tables and made our tests run 50 times faster. Chapter 6, Test Automation Strategy, and Chapter 11, Using Test Doubles, provide an overview of the vari- ous techniques available for making our SUT easier to test.\n\n1 My mother grew up in Hungary and has retained a part of her Hungarian accent—think Zsa Zsa Gabor—all her life. She says, “It is important to put the emphasis on the right syllable.”\n\nwww.it-ebooks.info\n\n525\n\nTest Double\n\n526\n\nTest Double\n\nChapter 23 Test Double Patterns\n\nVariation: Dummy Object\n\nSome method signatures of the SUT may require objects as parameters. If neither the test nor the SUT cares about these objects, we may choose to pass in a Dummy Object (page 728), which may be as simple as a null object ref- erence, an instance of the Object class, or an instance of a Pseudo-Object (see Hard-Coded Test Double on page 568). In this sense, a Dummy Object isn’t really a Test Double per se but rather an alternative to the value patterns Literal Value (page 714), Derived Value (page 718), and Generated Value (page 723).\n\nVariation: Procedural Test Stub\n\nA Test Double implemented in a procedural programming language is often called a “test stub,” but I prefer to call it a Procedural Test Stub (see Test Stub) to distinguish this usage from the modern Test Stub variation of Test Doubles. Typically, we use a Procedural Test Stub to allow testing/debugging to proceed while waiting for other code to become available. It is rare for these objects to be “swapped in” at runtime but sometimes we make the code conditional on a “Debugging” ﬂ ag—a form of Test Logic in Production (page 217).\n\nImplementation Notes\n\nSeveral considerations must be taken into account when we are building the Test Double (Figure 23.2):\n\nWhether the Test Double should be speciﬁ c to a single test or reusable\n\nacross many tests\n\nWhether the Test Double should exist in code or be generated on-the-ﬂ y\n\nHow we tell the SUT to use the Test Double (installation)\n\nThe ﬁ rst and last points are addressed here. The discussion of Test Double gen- eration is left to the section on Conﬁ gurable Test Doubles.\n\nBecause the techniques for building Test Doubles are pretty much independent of their behavior (e.g., they apply to both Test Stubs and Mock Objects), I’ve chosen to split out the descriptions of the various ways we can build Hard-Coded Test Doubles and Conﬁ gurable Test Doubles (page 558) into separate patterns.\n\nwww.it-ebooks.info\n\nTest Double\n\nTest Test Double Double\n\nDummy Dummy Object Object\n\nTest Test Stub Stub\n\nTest Test Spy Spy\n\nMock Mock Object Object\n\nFake Fake Object Object\n\nConfigurable Configurable Test Double Test Double\n\nHard-Coded Hard-Coded Test Double Test Double\n\nFigure 23.2 Types of Test Doubles with implementation choices. Only Test Stubs, Test Spies, and Mock Objects need to be hard-coded or conﬁ gured by the test. Dummy Objects have no implementation; Fake Objects are installed but not controlled by the test.\n\nVariation: Unconﬁ gurable Test Doubles\n\nNeither Dummy Objects nor Fake Objects need to be conﬁ gured, each for their own reason. Dummies should never be used by the receiver so they need no “real” implementation. Fake Objects, by contrast, need a “real” implementa- tion but one that is much simpler or “lighter” than the object that they replace. Therefore, neither the test nor the test automater will need to conﬁ gure “canned” responses or expectations; we just install the Test Double and let the SUT use it as if it were real.\n\nVariation: Hard-Coded Test Double\n\nWhen we plan to use a speciﬁ c Test Double in only a single test, it is often sim- plest to just hard-code the Test Double to return speciﬁ c values (for Test Stubs) or expect speciﬁ c method calls (Mock Objects). Hard-Coded Test Doubles are typically hand-built by the test automater. They come in several forms, including the Self Shunt (see Hard-Coded Test Double), where the Testcase Class (page 373) acts as the Test Double; the Anonymous Inner Test Double (see Hard-Coded Test Double), where language features are used to create the Test Double inside the Test Method (page 348); and the Test Double implemented as separate Test Double Class (see Hard-Coded Test Double). Each of these options is discussed in more detail in Hard-Coded Test Double.\n\nwww.it-ebooks.info\n\n527\n\nTest Double\n\n528\n\nTest Double\n\nChapter 23 Test Double Patterns\n\nVariation: Conﬁ gurable Test Double\n\nWhen we want to use the same Test Double implementation in many tests, we will typically prefer to use a Conﬁ gurable Test Double. Although the test auto- mater can manually build these objects, many members of the xUnit family have reusable toolkits available for generating Conﬁ gurable Test Doubles.\n\nInstalling the Test Double\n\nBefore we can exercise the SUT, we must tell it to use the Test Double instead of the object that the Test Double replaces. We can use any of the substitutable dependency patterns to install the Test Double during the ﬁ xture setup phase of our Four-Phase Test. Conﬁ gurable Test Doubles need to be conﬁ gured before we exercise the SUT, and we typically perform this conﬁ guration before we install them.\n\nExample: Test Double\n\nBecause there are a wide variety of reasons for using the variations of Test Dou- bles, it is difﬁ cult to provide a single example that characterizes the motivation behind each style. Please refer to the examples in each of the more detailed pat- terns referenced earlier.\n\nwww.it-ebooks.info\n\nTest Stub\n\nTest Stub\n\nHow can we verify logic independently when it depends on indirect inputs from other software components?\n\nWe replace a real object with a test-speciﬁ c object that feeds the desired indirect inputs into the system under test.\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nCreation Creation\n\nTest Test Stub Stub\n\nInstallation Installation\n\nExercise Exercise\n\nSUT SUT\n\nReturn Return Values Values\n\nVerify Verify\n\nIndirect Indirect Input Input\n\nTeardown Teardown\n\nIn many circumstances, the environment or context in which the SUT operates very much inﬂ uences the behavior of the SUT. To get adequate control over the indirect inputs of the SUT, we may have to replace some of the context with something we can control—namely, a Test Stub.\n\nHow It Works\n\nFirst, we deﬁ ne a test-speciﬁ c implementation of an interface on which the SUT depends. This implementation is conﬁ gured to respond to calls from the SUT with the values (or exceptions) that will exercise the Untested Code (see Production Bugs on page 268) within the SUT. Before exercising the SUT, we install the Test Stub so that the SUT uses it instead of the real implementation. When called by\n\nwww.it-ebooks.info\n\n529\n\nAlso known as: Stub\n\nTest Stub\n\n530\n\nTest Stub\n\nChapter 23 Test Double Patterns\n\nthe SUT during test execution, the Test Stub returns the previously deﬁ ned values. The test can then verify the expected outcome in the normal way.\n\nWhen to Use It\n\nA key indication for using a Test Stub is having Untested Code caused by our inability to control the indirect inputs of the SUT. We can use a Test Stub as a control point that allows us to control the behavior of the SUT with vari- ous indirect inputs and we have no need to verify the indirect outputs. We can also use a Test Stub to inject values that allow us to get past a particular point in the software where the SUT calls software that is unavailable in our test environment.\n\nIf we do need an observation point that allows us to verify the indirect out- puts of the SUT, we should consider using a Mock Object (page 544) or a Test Spy (page 538). Of course, we must have a way of installing a Test Double (page 522) into the SUT to be able to use any form of Test Double.\n\nVariation: Responder\n\nA Test Stub that is used to inject valid indirect inputs into the SUT so that it can go about its business is called a Responder. Responders are commonly used in “happy path” testing when the real component is uncontrollable, is not yet available, or is unusable in the development environment. The tests will invari- ably be Simple Success Tests (see Test Method on page 348).\n\nVariation: Saboteur\n\nA Test Stub that is used to inject invalid indirect inputs into the SUT is often called a Saboteur because its purpose is to derail whatever the SUT is trying to do so that we can see how the SUT copes under these circumstances. The “derailment” might be caused by returning unexpected values or objects, or it might result from raising an exception or causing a runtime error. Each test may be either a Simple Success Test or an Expected Exception Test (see Test Method), depending on how the SUT is expected to behave in response to the indirect input.\n\nVariation: Temporary Test Stub\n\nA Temporary Test Stub stands in for a DOC that is not yet available. This kind of Test Stub typically consists of an empty shell of a real class with hard-coded return statements. As soon as the real DOC is available, it replaces the Tempo- rary Test Stub. Test-driven development often requires us to create Temporary\n\nwww.it-ebooks.info\n\nTest Stub\n\nTest Stubs as we write code from the outside in; these shells evolve into the real classes as we add code to them. In need-driven development, we tend to use Mock Objects because we want to verify that the SUT calls the right methods on the Temporary Test Stub; in addition, we typically continue using the Mock Object even after the real DOC becomes available.\n\nVariation: Procedural Test Stub\n\nA Procedural Test Stub is a Test Stub written in a procedural programming lan- guage. It is particularly challenging to create in procedural programming languages that do not support procedure variables (also known as function pointers). In most cases, we must put if testing then hooks into the production code (a form of Test Logic in Production; see page 217).\n\nVariation: Entity Chain Snipping\n\nEntity Chain Snipping (see Test Stub on page 529) is a special case of a Responder that is used to replace a complex network of objects with a single Test Stub that pretends to be the network of objects. Its inclusion can make ﬁ x- ture setup go much more quickly (especially when the objects would normally have to be persisted into a database) and can make the tests much easier to understand.\n\nImplementation Notes\n\nWe must be careful when using Test Stubs because we are testing the SUT in a different conﬁ guration from the one that will be used in production. We really should have at least one test that veriﬁ es the SUT works without a Test Stub. A common mistake made by test automaters who are new to stubs is to replace a part of the SUT that they are trying to test. For this reason, it is important to be really clear about what is playing the role of SUT and what is playing the role of test ﬁ xture. Also, note that excessive use of Test Stubs can result in Overspeci- ﬁ ed Software (see Fragile Test on page 239).\n\nTest Stubs may be built in several different ways depending on our speciﬁ c\n\nneeds and the tools we have on hand.\n\nVariation: Hard-Coded Test Stub\n\nA Hard-Coded Test Stub has its responses hard-coded within its program logic. These Test Stubs tend to be purpose-built for a single test or a very small number of tests. See Hard-Coded Test Double (page 568) for more information.\n\nwww.it-ebooks.info\n\n531\n\nTest Stub\n\n532\n\nTest Stub\n\nChapter 23 Test Double Patterns\n\nVariation: Conﬁ gurable Test Stub\n\nWhen we want to avoid building a different Hard-Coded Test Stub for each test, we can use a Conﬁ gurable Test Stub (see Conﬁ gurable Test Double on page 558). A test conﬁ gures the Conﬁ gurable Test Stub as part of its ﬁ xture setup phase. Many members of the xUnit family offer tools with which to generate Conﬁ gurable Test Doubles (page 558), including Conﬁ gurable Test Stubs.\n\nMotivating Example\n\nThe following test veriﬁ es the basic functionality of a component that formats an HTML string containing the current time. Unfortunately, it depends on the real system clock so it rarely ever passes!\n\npublic void testDisplayCurrentTime_AtMidnight() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result); }\n\nWe could try to address this problem by making the test calculate the expected results based on the current system time as follows:\n\npublic void testDisplayCurrentTime_whenever() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify outcome Calendar time = new DefaultTimeProvider().getTime(); StringBuffer expectedTime = new StringBuffer(); expectedTime.append(\"<span class=\\\"tinyBoldText\\\">\");\n\nif ((time.get(Calendar.HOUR_OF_DAY) == 0) && (time.get(Calendar.MINUTE) <= 1)) { expectedTime.append( \"Midnight\"); } else if ((time.get(Calendar.HOUR_OF_DAY) == 12) && (time.get(Calendar.MINUTE) == 0)) { // noon expectedTime.append(\"N3oon\"); } else { SimpleDateFormat fr = new SimpleDateFormat(\"h:mm a\"); expectedTime.append(fr.format(time.getTime())); }\n\nwww.it-ebooks.info\n\nTest Stub\n\nexpectedTime.append(\"</span>\");\n\nassertEquals( expectedTime, result); }\n\nThis Flexible Test (see Conditional Test Logic on page 200) introduces two prob- lems. First, some test conditions are never exercised. (Do you want to come in to work to run the tests at midnight to prove the software works at midnight?) Second, the test needs to duplicate much of the logic in the SUT to calculate the expected results. How do we prove the logic is actually correct?\n\nRefactoring Notes\n\nWe can achieve proper veriﬁ cation of the indirect inputs by getting control of the time. To do so, we use the Replace Dependency with Test Double (page 522) refactoring to replace the real system clock (represented here by TimeProvider) with a Virtual Clock [VCTP]. We then implement it as a Test Stub that is conﬁ g- ured by the test with the time we want to use as the indirect input to the SUT.\n\nExample: Responder (as Hand-Coded Test Stub)\n\nThe following test veriﬁ es one of the happy path test conditions using a Responder to get control over the indirect inputs of the SUT. Based on the time injected into the SUT, the expected result can be hard-coded safely.\n\npublic void testDisplayCurrentTime_AtMidnight() throws Exception { // Fixture setup // Test Double conﬁguration TimeProviderTestStub tpStub = new TimeProviderTestStub(); tpStub.setHours(0); tpStub.setMinutes(0); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Test Double installation sut.setTimeProvider(tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nwww.it-ebooks.info\n\n533\n\nTest Stub\n\n534\n\nTest Stub\n\nChapter 23 Test Double Patterns\n\nThis test makes use of the following hand-coded configurable Test Stub implementation:\n\nprivate Calendar myTime = new GregorianCalendar(); /** * The complete constructor for the TimeProviderTestStub * @param hours speciﬁes the hours using a 24-hour clock * (e.g., 10 = 10 AM, 12 = noon, 22 = 10 PM, 0 = midnight) * @param minutes speciﬁes the minutes after the hour * (e.g., 0 = exactly on the hour, 1 = 1 min after the hour) */ public TimeProviderTestStub(int hours, int minutes) { setTime(hours, minutes); }\n\npublic void setTime(int hours, int minutes) { setHours(hours); setMinutes(minutes); }\n\n// Conﬁguration interface public void setHours(int hours) { // 0 is midnight; 12 is noon myTime.set(Calendar.HOUR_OF_DAY, hours); }\n\npublic void setMinutes(int minutes) { myTime.set(Calendar.MINUTE, minutes); } // Interface used by SUT public Calendar getTime() { // @return the last time that was set return myTime; }\n\nExample: Responder (Dynamically Generated)\n\nHere’s the same test coded using the JMock Conﬁ gurable Test Double frame- work:\n\npublic void testDisplayCurrentTime_AtMidnight_JM() throws Exception { // Fixture setup TimeDisplay sut = new TimeDisplay(); // Test Double conﬁguration Mock tpStub = mock(TimeProvider.class); Calendar midnight = makeTime(0,0); tpStub.stubs().method(\"getTime\"). withNoArguments(). will(returnValue(midnight));\n\nwww.it-ebooks.info\n\nTest Stub\n\n// Test Double installation sut.setTimeProvider((TimeProvider) tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nThere is no Test Stub implementation to examine for this test because the JMock framework implements the Test Stub using reﬂ ection. Thus we had to write a Test Utility Method (page 599) called makeTime that contains the logic to construct the Calendar object to be returned. In the hand-coded Test Stub, this logic appeared inside the getTime method.\n\nExample: Saboteur (as Anonymous Inner Class)\n\nThe following test uses a Saboteur to inject invalid indirect inputs into the SUT so we can see how the SUT copes under these circumstances.\n\npublic void testDisplayCurrentTime_exception() throws Exception { // Fixture setup // Deﬁne and instantiate Test Stub TimeProvider testStub = new TimeProvider() { // Anonymous inner Test Stub public Calendar getTime() throws TimeProviderEx { throw new TimeProviderEx(\"Sample\"); } }; // Instantiate SUT TimeDisplay sut = new TimeDisplay(); sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"error\\\">Invalid Time</span>\"; assertEquals(\"Exception\", expectedTimeString, result); }\n\nIn this case, we used an Inner Test Double (see Hard-Coded Test Double) to throw an exception that we expect the SUT to handle gracefully. One interest- ing thing about this test is that it uses the Simple Success Test method template rather than the Expected Exception Test template, even though we are injecting an exception as the indirect input. The rationale behind this choice is that we are expecting the SUT to catch the exception and change the string formatting; we are not expecting the SUT to throw an exception.\n\nwww.it-ebooks.info\n\n535\n\nTest Stub\n\n536\n\nTest Stub\n\nChapter 23 Test Double Patterns\n\nExample: Entity Chain Snipping\n\nIn this example, we are testing the Invoice but require a Customer to instantiate the Invoice. The Customer requires an Address, which in turn requires a City. Thus we ﬁ nd ourselves creating numerous additional objects just to set up the ﬁ xture. Suppose the behavior of the invoice depends on some attribute of the Customer that is calculated from the Address by calling the method get_zone on the Customer.\n\npublic void testInvoice_addLineItem_noECS() { ﬁnal int QUANTITY = 1; Product product = new Product(getUniqueNumberAsString(), getUniqueNumber()); State state = new State(\"West Dakota\", \"WD\"); City city = new City(\"Centreville\", state); Address address = new Address(\"123 Blake St.\", city, \"12345\"); Customer customer= new Customer(getUniqueNumberAsString(), getUniqueNumberAsString(), address); Invoice inv = new Invoice(customer); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); LineItem expItem = new LineItem(inv, product, QUANTITY); assertLineItemsEqual(\"\",expItem, actual); }\n\nIn this test, we want to verify only the behavior of the invoice logic that depends on this zone attribute—not the way this attribute is calculated from the Customer’s address. (There are separate Customer unit tests to verify the zone is calculated correctly.) All of the setup of the address, city, and other information merely distracts the reader.\n\nHere’s the same test using a Test Stub instead of the Customer. Note how much\n\nsimpler the ﬁ xture setup has become as a result of Entity Chain Snipping!\n\npublic void testInvoice_addLineItem_ECS() { ﬁnal int QUANTITY = 1; Product product = new Product(getUniqueNumberAsString(), getUniqueNumber()); Mock customerStub = mock(ICustomer.class); customerStub.stubs().method(\"getZone\").will(returnValue(ZONE_3)); Invoice inv = new Invoice((ICustomer)customerStub.proxy()); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify\n\nwww.it-ebooks.info\n\nTest Stub\n\nList lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); LineItem expItem = new LineItem(inv, product, QUANTITY); assertLineItemsEqual(\"\", expItem, actual); }\n\nWe have used JMock to stub out the Customer with a customerStub that returns ZONE_3 when getZone is called. This is all we need to verify the Invoice behavior, and we have managed to get rid of all that distracting extra object construction. It is also much clearer from reading this test that invoicing behavior depends only on the value returned by get_zone and not any other attributes of the Customer or Address.\n\nFurther Reading\n\nAlmost every book on automated testing using xUnit has something to say about Test Stubs, so I won’t list those resources here. As you are reading other books, however, keep in mind that the term Test Stub is often used to refer to a Mock Object. Mocks, Fakes, Stubs, and Dummies (in Appendix B) contains a more thorough comparison of the terminology used in various books and articles.\n\nSven Gorts describes a number of different ways we can use a Test Stub [UTwHCM]. I have adopted many of his names and adapted a few to better ﬁ t into this pattern language. Paolo Perrotta wrote a pattern describing a com- mon example of a Responder called Virtual Clock. He uses a Test Stub as a Decorator [GOF] for the real system clock that allows the time to be “frozen” or resumed. Of course, we could use a Hard-Coded Test Stub or a Conﬁ gu- rable Test Stub just as easily for most tests.\n\nwww.it-ebooks.info\n\n537\n\nTest Stub\n\n538\n\nAlso known as: Spy, Recording Test Stub\n\nTest Spy\n\nChapter 23 Test Double Patterns\n\nTest Spy\n\nHow do we implement Behavior Veriﬁ cation? How can we verify logic independently when it has indirect outputs to other software components?\n\nWe use a Test Double to capture the indirect output calls made to another component by the SUT for later veriﬁ cation by the test.\n\nFixture Fixture\n\nDOC DOC\n\nCreation Creation\n\nTest Spy Test Spy\n\nSetup Setup\n\nExercise Exercise\n\nInstallation Installation\n\nExercise Exercise\n\nSUT SUT\n\nIndirect Indirect Output Output\n\nIndirect Indirect Outputs Outputs\n\nVerify Verify\n\nTeardown Teardown\n\nIn many circumstances, the environment or context in which the SUT operates very much inﬂ uences the behavior of the SUT. To get adequate visibility of the indirect outputs of the SUT, we may have to replace some of the context with something we can use to capture these outputs of the SUT.\n\nUse of a Test Spy is a simple and intuitive way to implement Behavior Veriﬁ - cation (page 468) via an observation point that exposes the indirect outputs of the SUT so they can be veriﬁ ed.\n\nHow It Works\n\nBefore we exercise the SUT, we install a Test Spy as a stand-in for a DOC used by the SUT. The Test Spy is designed to act as an observation point by recording the method calls made to it by the SUT as it is exercised. During the\n\nwww.it-ebooks.info\n\nTest Spy\n\nresult veriﬁ cation phase, the test compares the actual values passed to the Test Spy by the SUT with the values expected by the test.\n\nWhen to Use It\n\nA key indication for using a Test Spy is having an Untested Requirement (see Production Bugs on page 268) caused by an inability to observe the side effects of invoking methods on the SUT. Test Spies are a natural and intuitive way to extend the existing tests to cover these indirect outputs because the calls to the Assertion Methods (page 362) are invoked by the test after the SUT has been exercised just like in “normal” tests. The Test Spy merely acts as the observation point that gives the Test Method (page 348) access to the values recorded during the SUT execution.\n\nWe should use a Test Spy in the following circumstances:\n\nWe are verifying the indirect outputs of the SUT and we cannot predict the values of all attributes of the interactions with the SUT ahead of time.\n\nWe want the assertions to be visible in the test and we don’t think the way in which the Mock Object (page 544) expectations are established is sufﬁ ciently intent-revealing. Test Spy\n\nOur test requires test-speciﬁ c equality (so we cannot use the standard deﬁ nition of equality as implemented in the SUT) and we are using tools that generate the Mock Object but do not give us control over the Assertion Methods being called.\n\nA failed assertion cannot be reported effectively back to the Test Run- ner (page 377). This might occur if the SUT is running inside a contain- er that catches all exceptions and makes it difﬁ cult to report the results or if the logic of the SUT runs in a different thread or process from the test that invokes it. (Both of these cases really beg refactoring to allow us to test the SUT logic directly, but that is the subject of another chapter.)\n\nWe would like to have access to all the outgoing calls of the SUT before\n\nmaking any assertions on them.\n\nIf none of these criteria apply, we may want to consider using a Mock Object. If we are trying to address Untested Code (see Production Bugs) by controlling the indirect inputs of the SUT, a simple Test Stub (page 529) may be all we need.\n\nwww.it-ebooks.info\n\n539\n\n540\n\nTest Spy\n\nAlso known as: Loopback\n\nChapter 23 Test Double Patterns\n\nUnlike a Mock Object, a Test Spy does not fail the test at the ﬁ rst deviation from the expected behavior. Thus our tests will be able to include more detailed diagnostic information in the Assertion Message (page 370) based on informa- tion gathered after a Mock Object would have failed the test. At the point of test failure, however, only the information within the Test Method itself is avail- able to be used in the calls to the Assertion Methods. If we need to include information that is accessible only while the SUT is being exercised, either we must explicitly capture it within our Test Spy or we must use a Mock Object.\n\nOf course, we won’t be able to use any Test Doubles (page 522) unless the\n\nSUT implements some form of substitutable dependency.\n\nImplementation Notes\n\nThe Test Spy itself can be built as a Hard-Coded Test Double (page 568) or as a Conﬁ gurable Test Double (page 558). Because detailed examples appear in the discussion of those patterns, only a quick summary is provided here. Likewise, we can use any of the substitutable dependency patterns to install the Test Spy before we exercise the SUT.\n\nThe key characteristic in how a test uses a Test Spy relates to the fact that as- sertions are made from within the Test Method. Therefore, the test must recover the indirect outputs captured by the Test Spy before it can make its assertions, which can be done in several ways.\n\nVariation: Retrieval Interface\n\nWe can deﬁ ne the Test Spy as a separate class with a Retrieval Interface that exposes the recorded information. The Test Method installs the Test Spy instead of the normal DOC as part of the ﬁ xture setup phase of the test. After the test has exercised the SUT, it uses the Retrieval Interface to retrieve the actual indi- rect outputs of the SUT from the Test Spy and then calls Assertion Methods with those outputs as arguments.\n\nVariation: Self Shunt\n\nWe can collapse the Test Spy and the Testcase Class (page 373) into a single object called a Self Shunt. The Test Method installs itself, the Testcase Object (page 382), as the DOC into the SUT. Whenever the SUT delegates to the DOC, it is actually calling methods on the Testcase Object, which implements the methods by saving the actual values into instance variables that can be accessed by the Test Method. The methods could also make assertions in the Test Spy methods, in which case the Self Shunt is a variation on a Mock Object rather than a Test Spy. In stati- cally typed languages, the Testcase Class must implement the outgoing interface\n\nwww.it-ebooks.info\n\nTest Spy\n\n(the observation point) on which the SUT depends so that the Testcase Class is type-compatible with the variables that are used to hold the DOC.\n\nVariation: Inner Test Double\n\nA popular way to implement the Test Spy as a Hard-Coded Test Double is to code it as an anonymous inner class or block closure within the Test Method and to have this class or block save the actual values into instance or local variables that are accessible by the Test Method. This variation is really another way to implement a Self Shunt (see Hard-Coded Test Double).\n\nVariation: Indirect Output Registry\n\nYet another possibility is to have the Test Spy store the actual parameters in a well-known place where the Test Method can access them. For example, the Test Spy could save those values in a ﬁ le or in a Registry [PEAA] object.\n\nMotivating Example\n\nThe following test veriﬁ es the basic functionality of removing a ﬂ ight but does not verify the indirect outputs of the SUT—namely, the fact that the SUT is expected to log each time a ﬂ ight is removed along with the date/time and user- name of the requester.\n\npublic void testRemoveFlight() throws Exception { // setup FlightDto expectedFlightDto = createARegisteredFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight should not exist after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); }\n\nRefactoring Notes\n\nWe can add veriﬁ cation of indirect outputs to existing tests using a Replace Dependency with Test Double (page 522) refactoring. It involves adding code to the ﬁ xture setup logic of the tests to create the Test Spy, conﬁ guring the Test Spy with any values it needs to return, and installing it. At the end of the test, we add assertions comparing the expected method names and arguments of the\n\nwww.it-ebooks.info\n\n541\n\nTest Spy\n\n542\n\nTest Spy\n\nChapter 23 Test Double Patterns\n\nindirect outputs with the actual values retrieved from the Test Spy using the Retrieval Interface.\n\nExample: Test Spy\n\nIn this improved version of the test, logSpy is our Test Spy. The statement facade. setAuditLog(logSpy) installs the Test Spy using the Setter Injection pattern (see Dependency Injection on page 678). The methods getDate, getActionCode, and so on are the Retrieval Interface used to access the actual arguments of the call to the logger.\n\npublic void testRemoveFlightLogging_recordingTestStub() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // Test Double setup AuditLogSpy logSpy = new AuditLogSpy(); facade.setAuditLog(logSpy); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); assertEquals(\"number of calls\", 1, logSpy.getNumberOfCalls()); assertEquals(\"action code\", Helper.REMOVE_FLIGHT_ACTION_CODE, logSpy.getActionCode()); assertEquals(\"date\", helper.getTodaysDateWithoutTime(), logSpy.getDate()); assertEquals(\"user\", Helper.TEST_USER_NAME, logSpy.getUser()); assertEquals(\"detail\", expectedFlightDto.getFlightNumber(), logSpy.getDetail()); }\n\nThis test depends on the following deﬁ nition of the Test Spy:\n\npublic class AuditLogSpy implements AuditLog { // Fields into which we record actual usage information private Date date; private String user; private String actionCode; private Object detail; private int numberOfCalls = 0;\n\nwww.it-ebooks.info\n\nTest Spy\n\n// Recording implementation of real AuditLog interface public void logMessage(Date date, String user, String actionCode, Object detail) { this.date = date; this.user = user; this.actionCode = actionCode; this.detail = detail;\n\nnumberOfCalls++; }\n\n// Retrieval Interface public int getNumberOfCalls() { return numberOfCalls; } public Date getDate() { return date; } public String getUser() { return user; } public String getActionCode() { return actionCode; } public Object getDetail() { return detail; } }\n\nOf course, we could have implemented the Retrieval Interface by making the various ﬁ elds of our spy public and thereby avoided the need for accessor methods. Please refer to the examples in Hard-Coded Test Double for other implementation options.\n\nwww.it-ebooks.info\n\n543\n\nTest Spy\n\n544\n\nMock Object\n\nChapter 23 Test Double Patterns\n\nMock Object\n\nHow do we implement Behavior Veriﬁ cation for indirect outputs of the SUT? How can we verify logic independently when it depends on indirect inputs from other software components?\n\nWe replace an object on which the SUT depends on with a test-speciﬁ c object that veriﬁ es it is being used correctly by the SUT.\n\nSetup Setup\n\nCreation Creation\n\nFixture Fixture\n\nMock Mock Object Object\n\nDOC DOC\n\nInstallation Installation\n\nExpectations Expectations\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nSUT SUT\n\nFinal Verification Final Verification\n\nIndirect Indirect Output Output\n\ny y f f i i r r e e V V\n\nTeardown Teardown\n\nIn many circumstances, the environment or context in which the SUT operates very much inﬂ uences the behavior of the SUT. In other cases, we must peer “inside”2 the SUT to determine whether the expected behavior has occurred. A Mock Object is a powerful way to implement Behavior Veriﬁ cation (page 468) while avoiding Test Code Duplication (page 213) between similar tests. It works by delegating the job of verifying the indirect outputs of the SUT entirely to a Test Double (page 522).\n\n2 Technically, the SUT is whatever software we are testing and doesn’t include anything it depends on; thus “inside” is somewhat of a misnomer. It is better to think of the DOC that is the destination of the indirect outputs as being “behind” the SUT and part of the ﬁ xture.\n\nwww.it-ebooks.info\n\nMock Object\n\nHow It Works\n\nFirst, we deﬁ ne a Mock Object that implements the same interface as an object on which the SUT depends. Then, during the test, we conﬁ gure the Mock Object with the values with which it should respond to the SUT and the method calls (complete with expected arguments) it should expect from the SUT. Before exer- cising the SUT, we install the Mock Object so that the SUT uses it instead of the real implementation. When called during SUT execution, the Mock Object com- pares the actual arguments received with the expected arguments using Equality Assertions (see Assertion Method on page 362) and fails the test if they don’t match. The test need not make any assertions at all!\n\nWhen to Use It\n\nWe can use a Mock Object as an observation point when we need to do Behavior Veriﬁ cation to avoid having an Untested Requirement (see Production Bugs on page 268) caused by our inability to observe the side effects of invoking meth- ods on the SUT. This pattern is commonly used during endoscopic testing [ET] or need-driven development [MRNO]. Although we don’t need to use a Mock Object when we are doing State Veriﬁ cation (page 462), we might use a Test Stub (page 529) or Fake Object (page 551). Note that test drivers have found other uses for the Mock Object toolkits, but many of these are actually examples of using a Test Stub rather than a Mock Object.\n\nTo use a Mock Object, we must be able to predict the values of most or all arguments of the method calls before we exercise the SUT. We should not use a Mock Object if a failed assertion cannot be reported back to the Test Runner (page 377) effectively. This may be the case if the SUT runs inside a container that catches and eats all exceptions. In these circumstances, we may be better off using a Test Spy (page 538) instead.\n\nMock Objects (especially those created using dynamic mocking tools) often use the equals methods of the various objects being compared. If our test-speciﬁ c equality differs from how the SUT would interpret equals, we may not be able to use a Mock Object or we may be forced to add an equals method where we didn’t need one. This smell is called Equality Pollution (see Test Logic in Production on page 217). Some implementations of Mock Objects avoid this problem by allow- ing us to specify the “comparator” to be used in the Equality Assertions.\n\nMock Objects can be either “strict” or “lenient” (sometimes called “nice”). A “strict” Mock Object fails the test if the calls are received in a different order than was speciﬁ ed when the Mock Object was programmed. A “lenient” Mock Object tolerates out-of-order calls.\n\nwww.it-ebooks.info\n\n545\n\nMock Object\n\n546\n\nMock Object\n\nChapter 23 Test Double Patterns\n\nImplementation Notes\n\nTests written using Mock Objects look different from more traditional tests be- cause all the expected behavior must be speciﬁ ed before the SUT is exercised. This makes the tests harder to write and to understand for test automation neophytes. This factor may be enough to cause us to prefer writing our tests using Test Spies. The standard Four-Phase Test (page 358) is altered somewhat when we use Mock Objects. In particular, the ﬁ xture setup phase of the test is broken down into three speciﬁ c activities and the result veriﬁ cation phase more or less dis- appears, except for the possible presence of a call to the “ﬁ nal veriﬁ cation” method at the end of the test.\n\nFixture setup:\n\nTest constructs Mock Object.\n\nTest conﬁ gures Mock Object. This step is omitted for Hard-Coded Test\n\nDoubles (page 568).\n\nTest installs Mock Object into SUT.\n\nExercise SUT:\n\nSUT calls Mock Object; Mock Object does assertions.\n\nResult veriﬁ cation:\n\nTest calls “ﬁ nal veriﬁ cation” method.\n\nFixture teardown:\n\nNo impact.\n\nLet’s examine these differences a bit more closely:\n\nConstruction\n\nAs part of the ﬁ xture setup phase of our Four-Phase Test, we must construct the Mock Object that we will use to replace the substitutable dependency. Depend- ing on which tools are available in our programming language, we can either build the Mock Object class manually, use a code generator to create a Mock Object class, or use a dynamically generated Mock Object.\n\nConﬁ guration with Expected Values\n\nBecause the Mock Object toolkits available in many members of the xUnit family typically create Conﬁ gurable Mock Objects (page 544), we need\n\nwww.it-ebooks.info\n\nMock Object\n\nto conﬁ gure the Mock Object with the expected method calls (and their parameters) as well as the values to be returned by any functions. (Some Mock Object frameworks allow us to disable veriﬁ cation of the method calls or just their parameters.) We typically perform this conﬁ guration before we install the Test Double.\n\nThis step is not needed when we are using a Hard-Coded Test Double such\n\nas an Inner Test Double (see Hard-Coded Test Double).\n\nInstallation\n\nOf course, we must have a way of installing a Test Double into the SUT to be able to use a Mock Object. We can use whichever substitutable dependency pattern the SUT supports. A common approach in the test-driven development community is Dependency Injection (page 678); more traditional developers may favor Dependency Lookup (page 686).\n\nUsage\n\nWhen the SUT calls the methods of the Mock Object, these methods compare the method call (method name plus arguments) with the expectations. If the method call is unexpected or the arguments are incorrect, the assertion fails the test im- mediately. If the call is expected but came out of sequence, a strict Mock Object fails the test immediately; by contrast, a lenient Mock Object notes that the call was received and carries on. Missed calls are detected when the ﬁ nal veriﬁ cation method is called.\n\nIf the method call has any outgoing parameters or return values, the Mock Object needs to return or update something to allow the SUT to continue executing the test scenario. This behavior may be either hard-coded or conﬁ gured at the same time as the expectations. This behavior is the same as for Test Stubs, except that we typically return happy path values.\n\nFinal Veriﬁ cation\n\nMost of the result veriﬁ cation occurs inside the Mock Object as it is called by the SUT. The Mock Object will fail the test if the methods are called with the wrong arguments or if methods are called unexpectedly. But what happens if the expected method calls are never received by the Mock Object? The Mock Object may have trouble detecting that the test is over and it is time to check for unfulﬁ lled expectations. Therefore, we need to ensure that the ﬁ nal veriﬁ cation method is called. Some Mock Object toolkits have found a way to invoke this\n\nwww.it-ebooks.info\n\n547\n\nMock Object\n\n548\n\nMock Object\n\nChapter 23 Test Double Patterns\n\nmethod automatically by including the call in the tearDown method.3 Many other toolkits require us to remember to call the ﬁ nal veriﬁ cation method ourselves.\n\nMotivating Example\n\nThe following test veriﬁ es the basic functionality of creating a ﬂ ight. But it does not verify the indirect outputs of the SUT—namely, the SUT is expected to log each time a ﬂ ight is created along with the date/time and username of the requester.\n\npublic void testRemoveFlight() throws Exception { // setup FlightDto expectedFlightDto = createARegisteredFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight should not exist after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); }\n\nRefactoring Notes\n\nVeriﬁ cation of indirect outputs can be added to existing tests by using a Replace Dependency with Test Double (page 522) refactoring. This involves adding code to the ﬁ xture setup logic of our test to create the Mock Object; conﬁ guring the Mock Object with the expected method calls, arguments, and values to be returned; and installing it using whatever substitutable dependency mechanism is provided by the SUT. At the end of the test, we add a call to the ﬁ nal veriﬁ cation method if our Mock Object framework requires one.\n\nExample: Mock Object (Hand-Coded)\n\nIn this improved version of the test, mockLog is our Mock Object. The method setExpectedLogMessage is used to program it with the expected log message. The statement facade.setAuditLog(mockLog) installs the Mock Object using the Setter Injection (see Dependency Injection) test double-installation pattern. Finally, the verify() method ensures that the call to logMessage() was actually made.\n\n3 This usually requires that we subclass our testcase from a special MockObjectTestCase class.\n\nwww.it-ebooks.info\n\nMock Object\n\npublic void testRemoveFlight_Mock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); // mock conﬁguration ConﬁgurableMockAuditLog mockLog = new ConﬁgurableMockAuditLog(); mockLog.setExpectedLogMessage( helper.getTodaysDateWithoutTime(), Helper.TEST_USER_NAME, Helper.REMOVE_FLIGHT_ACTION_CODE, expectedFlightDto.getFlightNumber()); mockLog.setExpectedNumberCalls(1); // mock installation FlightManagementFacade facade = new FlightManagementFacadeImpl(); facade.setAuditLog(mockLog); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); mockLog.verify(); }\n\nThis approach was made possible by use of the following Mock Object. Here we have chosen to use a hand-built Mock Object. In the interest of space, just the logMessage method is shown:\n\npublic void logMessage( Date actualDate, String actualUser, String actualActionCode, Object actualDetail) { actualNumberCalls++;\n\nAssert.assertEquals(\"date\", expectedDate, actualDate); Assert.assertEquals(\"user\", expectedUser, actualUser); Assert.assertEquals(\"action code\", expectedActionCode, actualActionCode); Assert.assertEquals(\"detail\", expectedDetail,actualDetail); }\n\nThe Assertion Methods are called as static methods. In JUnit, this approach is required because the Mock Object is not a subclass of TestCase; thus it does not inherit the assertion methods from Assert. Other members of the xUnit family may provide different mechanisms to access the Assertion Methods. For exam- ple, NUnit provides them only as static methods on the Assert class, so even Test Methods (page 348) need to access the Assertion Methods this way. Test::Unit,\n\nwww.it-ebooks.info\n\n549\n\nMock Object\n\n550\n\nMock Object\n\nChapter 23 Test Double Patterns\n\nthe xUnit family member for the Ruby programming language, provides them as mixins; as a consequence, they can be called in the normal fashion.\n\nExample: Mock Object (Dynamically Generated)\n\nThe last example used a hand-coded Mock Object. Most members of the xUnit family, however, have dynamic Mock Object frameworks available. Here’s the same test rewritten using JMock:\n\npublic void testRemoveFlight_JMock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // mock conﬁguration Mock mockLog = mock(AuditLog.class); mockLog.expects(once()).method(\"logMessage\") .with(eq(helper.getTodaysDateWithoutTime()), eq(Helper.TEST_USER_NAME), eq(Helper.REMOVE_FLIGHT_ACTION_CODE), eq(expectedFlightDto.getFlightNumber())); // mock installation facade.setAuditLog((AuditLog) mockLog.proxy()); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); // verify() method called automatically by JMock }\n\nNote how JMock provides a “ﬂ uent” Conﬁ guration Interface (see Conﬁ gurable Test Double) that allows us to specify the expected method calls in a fairly readable fashion. JMock also allows us to specify the comparator to be used by the asser- tions; in this case, the calls to eq cause the default equals method to be called.\n\nFurther Reading\n\nAlmost every book on automated testing using xUnit has something to say about Mock Objects, so I won’t list those resources here. As you are reading other books, keep in mind that the term Mock Object is often used to refer to a Test Stub and sometimes even to Fake Objects. Mocks, Fakes, Stubs, and Dummies (in Appendix B) contains a more thorough comparison of the terminology used in various books and articles.\n\nwww.it-ebooks.info\n\nFake Object\n\nFake Object\n\nHow can we verify logic independently when depended-on objects cannot be used? How can we avoid Slow Tests?\n\nWe replace a component that the SUT depends on with a much lighter-weight implementation.\n\nSetup Setup\n\nInstallation Installation\n\nCreation Creation\n\nFixture Fixture\n\nDOC DOC\n\nFake Fake Object Object\n\nSetup Setup\n\nExercise Exercise\n\nExercise Exercise\n\nSUT SUT\n\nData Data\n\nVerify Verify\n\nTeardown Teardown\n\nThe SUT often depends on other components or systems. Although the inter- actions with these other components may be necessary, the side effects of these interactions as implemented by the real DOC may be unnecessary or even detrimental.\n\nA Fake Object is a much simpler and lighter-weight implementation of the functionality provided by the DOC without the side effects we choose to do without.\n\nHow It Works\n\nWe acquire or build a very lightweight implementation of the same functionality as provided by a component on which the SUT depends and instruct the SUT to use it instead of the real DOC. This implementation need not have any of the\n\nwww.it-ebooks.info\n\n551\n\nAlso known as: Dummy\n\nFake Object\n\n552\n\nFake Object\n\nChapter 23 Test Double Patterns\n\n“-ilities” that the real DOC needs to have (such as scalability); it need provide only the equivalent services to the SUT so that the SUT remains unaware it isn’t using the real DOC.\n\nA Fake Object is a kind of Test Double (page 522) that is similar to a Test Stub (page 529) in many ways, including the need to install into the SUT a substitutable dependency. Whereas a Test Stub acts as a control point to inject indirect inputs into the SUT, however, the Fake Object does not: It merely provides a way for the interactions to occur in a self-consistent manner. These interactions (i.e., between the SUT and the Fake Object) will typically be many, and the values passed in as arguments of earlier method calls will often be returned as results of later method calls. Contrast this behavior with that of Test Stubs and Mock Objects (page 544), where the responses are either hard-coded or conﬁ gured by the test.\n\nWhile the test does not normally conﬁ gure a Fake Object, complex ﬁ xture setup that would typically involve initializing the state of the DOC may also be done with the Fake Object directly using Back Door Manipulation (page 327). Techniques such as Data Loader (see Back Door Manipulation) and Back Door Setup (see Back Door Manipulation) can be used quite successfully with less fear of Overspeciﬁ ed Software (see Fragile Test on page 239) because they sim- ply bind us to the interface between the SUT and the Fake Object; the interface used to conﬁ gure the Fake Object is a test-only concern.\n\nWhen to Use It\n\nWe should use a Fake Object whenever the SUT depends on other components that are unavailable or that make testing difﬁ cult or slow (e.g., Slow Tests; see page 253) and the tests need more complex sequences of behavior than are worth implement- ing in a Test Stub or Mock Object. It must also be easier to create a lightweight implementation than to build and program suitable Mock Objects, at least in the long run, if building a Fake Object is to be worthwhile.\n\nUsing a Fake Object helps us avoid Overspeciﬁ ed Software because we do not encode the exact calling sequences expected of the DOC within the test. The SUT can vary how many times the methods of the DOC are called without causing tests to fail.\n\nIf we need to control the indirect inputs or verify the indirect outputs of the\n\nSUT, we should probably use a Mock Object or Test Stub instead.\n\nSome speciﬁ c situations where we replace the real component with a Fake\n\nObject are described next.\n\nwww.it-ebooks.info\n\nFake Object\n\nVariation: Fake Database\n\nWith the Fake Database pattern, the real database or persistence layer is replaced by a Fake Object that is functionally equivalent but that has much better perfor- mance characteristics. An approach we have often used involves replacing the database with a set of in-memory HashTables that act as a very lightweight way of retrieving objects that have been “persisted” earlier in the test.\n\nVariation: In-Memory Database\n\nAnother example of a Fake Object is the use of a small-footprint, diskless database instead of a full-featured disk-based database. This kind of In-Memory Database will improve the speed of tests by at least an order of magnitude while giving up less functionality than a Fake Database.\n\nVariation: Fake Web Service\n\nWhen testing software that depends on other components that are accessed as Web services, we can build a small hard-coded or data-driven implementation that can be used instead of the real Web service to make our tests more robust and to avoid having to create a test instance of the real Web service in our development environment.\n\nVariation: Fake Service Layer\n\nWhen testing user interfaces, we can avoid Data Sensitivity (see Fragile Test) and Behavior Sensitivity (see Fragile Test) of the tests by replacing the component that implements the Service Layer [PEAA] (including the domain layer) of our application with a Fake Object that returns remembered or data-driven results. This approach allows us to focus on testing the user interface without having to worry about the data being returned changing over time.\n\nImplementation Notes\n\nIntroducing a Fake Object involves two basic concerns:\n\nBuilding the Fake Object implementation\n\nInstalling the Fake Object\n\nBuilding the Fake Object\n\nMost Fake Objects are hand-built. Often, the Fake Object is used to replace a real implementation that suffers from latency issues owing to real messaging\n\nwww.it-ebooks.info\n\n553\n\nFake Object\n\n554\n\nFake Object\n\nChapter 23 Test Double Patterns\n\nor disk I/O with a much lighter in-memory implementation. With the rich class libraries available in most object-oriented programming languages, it is usually possible to build a fake implementation that is sufﬁ cient to satisfy the needs of the SUT, at least for the purposes of speciﬁ c tests, with relatively little effort.\n\nA popular strategy is to start by building a Fake Object to support a speciﬁ c set of tests where the SUT requires only a subset of the DOC’s services. If this proves successful, we may consider expanding the Fake Object to handle addi- tional tests. Over time, we may ﬁ nd that we can run all of our tests using the Fake Object. (See the sidebar “Faster Tests Without Shared Fixtures” on page 319 for a description of how we faked out the entire database with hash tables and made our tests run 50 times faster.)\n\nInstalling the Fake Object\n\nOf course, we must have a way of installing the Fake Object into the SUT to be able to take advantage of it. We can use whichever substitutable dependency pattern the SUT supports. A common approach in the test-driven development community is Dependency Injection (page 678); more traditional developers may favor Dependency Lookup (page 686). The latter technique is also more appropriate when we introduce a Fake Database (see Fake Object on page 551) in an effort to speed up execution of the customer tests; Dependency Injection doesn’t work so well with these kinds of tests.\n\nMotivating Example\n\nIn this example, the SUT needs to read and write records from a database. The test must set up the ﬁ xture in the database (several writes), the SUT interacts (reads and writes) with the database several more times, and then the test removes the records from the database (several deletes). All of this work takes time—several seconds per test. This very quickly adds up to minutes, and soon we ﬁ nd that our developers aren’t running the tests quite so frequently. Here is an example of one of these tests:\n\npublic void testReadWrite() throws Exception{ // Setup FlightMngtFacade facade = new FlightMgmtFacadeImpl(); BigDecimal yyc = facade.createAirport(\"YYC\", \"Calgary\", \"Calgary\"); BigDecimal lax = facade.createAirport(\"LAX\", \"LAX Intl\", \"LA\"); facade.createFlight(yyc, lax); // Exercise List ﬂights = facade.getFlightsByOriginAirport(yyc);\n\nwww.it-ebooks.info\n\nFake Object\n\n// Verify assertEquals( \"# of ﬂights\", 1, ﬂights.size()); Flight ﬂight = (Flight) ﬂights.get(0); assertEquals( \"origin\", yyc, ﬂight.getOrigin().getCode()); }\n\nThe test calls createAirport on our Service Facade [CJ2EEP], which calls, among other things, our data access layer. Here is the actual implementation of several of the methods we are calling:\n\npublic BigDecimal createAirport( String airportCode, String name, String nearbyCity) throws FlightBookingException{ TransactionManager.beginTransaction(); Airport airport = dataAccess. createAirport(airportCode, name, nearbyCity); logMessage(\"Wrong Action Code\", airport.getCode());//bug TransactionManager.commitTransaction(); return airport.getId(); }\n\npublic List getFlightsByOriginAirport( BigDecimal originAirportId) throws FlightBookingException {\n\nif (originAirportId == null) throw new InvalidArgumentException( \"Origin Airport Id has not been provided\", \"originAirportId\", null); Airport origin = dataAccess.getAirportByPrimaryKey(originAirportId); List ﬂights = dataAccess.getFlightsByOriginAirport(origin);\n\nreturn ﬂights; }\n\nThe calls to dataAccess.createAirport, dataAccess.createFlight, and TransactionManager. commitTransaction cause our test to slow down the most. The calls to dataAccess. getAirportByPrimaryKey and dataAccess.getFlightsByOriginAirport are a lesser factor but still contribute to the slow test.\n\nRefactoring Notes\n\nThe steps for introducing a Fake Object are very similar to those for adding a Mock Object. If one doesn’t already exist, we use a Replace Dependency with Test Double (page 522) refactoring to introduce a way to substitute the Fake Object for the DOC—usually a ﬁ eld (attribute) to hold the reference to it. In statically typed languages, we may have to do an Extract Interface [Fowler] refactoring before we\n\nwww.it-ebooks.info\n\n555\n\nFake Object\n\n556\n\nFake Object\n\nChapter 23 Test Double Patterns\n\ncan introduce the fake implementation. Then, we use this interface as the type of variable that holds the reference to the substitutable dependency.\n\nOne notable difference is that we do not need to conﬁ gure the Fake Object with\n\nexpectations or return values; we merely set up the ﬁ xture in the normal way.\n\nExample: Fake Database\n\nIn this example, we’ve created a Fake Object that replaces the database—that is, a Fake Database implemented entirely in memory using hash tables. The test doesn’t change a lot, but the test execution occurs much, much faster.\n\npublic void testReadWrite_inMemory() throws Exception{ // Setup FlightMgmtFacadeImpl facade = new FlightMgmtFacadeImpl(); facade.setDao(new InMemoryDatabase()); BigDecimal yyc = facade.createAirport(\"YYC\", \"Calgary\", \"Calgary\"); BigDecimal lax = facade.createAirport(\"LAX\", \"LAX Intl\", \"LA\"); facade.createFlight(yyc, lax); // Exercise List ﬂights = facade.getFlightsByOriginAirport(yyc); // Verify assertEquals( \"# of ﬂights\", 1, ﬂights.size()); Flight ﬂight = (Flight) ﬂights.get(0); assertEquals( \"origin\", yyc, ﬂight.getOrigin().getCode()); }\n\nHere’s the implementation of the Fake Database:\n\npublic class InMemoryDatabase implements FlightDao{ private List airports = new Vector(); public Airport createAirport(String airportCode, String name, String nearbyCity) throws DataException, InvalidArgumentException { assertParamtersAreValid( airportCode, name, nearbyCity); assertAirportDoesntExist( airportCode); Airport result = new Airport(getNextAirportId(), airportCode, name, createCity(nearbyCity)); airports.add(result); return result; } public Airport getAirportByPrimaryKey(BigDecimal airportId) throws DataException, InvalidArgumentException { assertAirportNotNull(airportId);\n\nAirport result = null; Iterator i = airports.iterator(); while (i.hasNext()) {\n\nwww.it-ebooks.info\n\nFake Object\n\nAirport airport = (Airport) i.next(); if (airport.getId().equals(airportId)) { return airport; } } throw new DataException(\"Airport not found:\"+airportId); }\n\nNow all we need is the implementation of the method that installs the Fake Database into the facade to make our developers more than happy to run all the tests after every code change.\n\npublic void setDao(FlightDao) { dataAccess = dao; }\n\nFurther Reading\n\nThe sidebar “Faster Tests Without Shared Fixtures” on page 319 provides a more in-depth description of how we faked out the entire database with hash tables and made our tests run 50 times faster. Mocks, Fakes, Stubs, and Dum- mies (in Appendix B) contains a more thorough comparison of the terminology used in various books and articles.\n\nwww.it-ebooks.info\n\n557\n\nFake Object\n\n558\n\nAlso known as: Conﬁ gurable Mock Object, Conﬁ gurable Test Spy, Conﬁ gurable Test Stub\n\nConﬁ gurable Test Double\n\nChapter 23 Test Double Patterns\n\nConﬁ gurable Test Double\n\nHow do we tell a Test Double what to return or expect?\n\nWe conﬁ gure a reusable Test Double with the values to be returned or veriﬁ ed during the ﬁ xture setup phase of a test.\n\nExpectations, Expectations, Return Values Return Values\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nConfiguration Configuration\n\nInstallation Installation\n\nExpectations Expectations\n\nExercise Exercise\n\nSUT SUT\n\nReturn Return Values Values\n\nVerify Verify\n\nTeardown Teardown\n\nTest Test Double Double\n\nSome tests require unique values to be fed into the SUT as indirect inputs or to be veriﬁ ed as indirect outputs of the SUT. This approach typically requires the use of Test Doubles (page 522) as the conduit between the test and the SUT; at the same time, the Test Double somehow needs to be told which values to return or verify. AConﬁ gurable Test Double is a way to reduce Test Code Duplication (page 213) by reusing a Test Double in many tests. The key to its use is to conﬁ gure the Test Double’s values to be returned or expected at runtime.\n\nHow It Works\n\nThe Test Double is built with instance variables that hold the values to be returned to the SUT or to serve as the expected values of arguments to method calls. The test initializes these variables during the setup phase of the test by calling the appropri- ate methods on the Test Double’s interface. When the SUT calls the methods on the Test Double, the Test Double uses the contents of the appropriate variable as the value to return or as the expected value in assertions.\n\nwww.it-ebooks.info\n\nConfigurable Test Double\n\nWhen to Use It\n\nWe can use a Conﬁ gurable Test Double whenever we need similar but slightly different behavior in several tests that depend on Test Doubles and we want to avoid Test Code Duplication or Obscure Tests (page 186)—in the latter case, we need to see what values the Test Double is using as we read the test. If we expect only a single usage of a Test Double, we can consider using a Hard-Coded Test Double (page 568) if the extra effort and complexity of building a Conﬁ gurable Test Double are not warranted.\n\nImplementation Notes\n\nA Test Double is a Conﬁ gurable Test Double because it needs to provide a way for the tests to conﬁ gure it with values to return and/or method arguments to expect. Conﬁ gurable Test Stubs (page 529) and Test Spies (page 538) simply require a way to conﬁ gure the responses to calls on their methods; conﬁ gurable Mock Objects (page 544) also require a way to conﬁ gure their expectations (which methods should be called and with which arguments).\n\nConﬁ gurable Test Doubles may be built in many ways. Deciding on a par- ticular implementation involves making two relatively independent decisions: (1) how the Conﬁ gurable Test Double will be conﬁ gured and (2) how the Conﬁ gurable Test Double will be coded.\n\nThere are two common ways to conﬁ gure a Conﬁ gurable Test Double. The most popular approach is to provide a Conﬁ guration Interface that is used only by the test to conﬁ gure the values to be returned as indirect inputs and the expected values of the indirect outputs. Alternatively, we may build the Conﬁ gurable Test Double with two modes. The Conﬁ guration Mode is used during ﬁ xture setup to install the indirect inputs and expected indirect out- puts by calling the methods of the Conﬁ gurable Test Double with the expected arguments. Before the Conﬁ gurable Test Double is installed, it is put into the normal (“usage” or “playback”) mode.\n\nThe obvious way to build a Conﬁ gurable Test Double is to create a Hand- Built Test Double. If we are lucky, however, someone will have already built a tool to generate a Conﬁ gurable Test Double for us. Test Double genera- tors come in two ﬂ avors: code generators and tools that fabricate the object at runtime. Developers have built several generations of “mocking” tools, and several of these have been ported to other programming languages; check out http://xprogramming.com to see what is available in your programming language of choice. If the answer is “nothing,” you can hand-code the Test Double your- self, although this does take somewhat more effort.\n\nwww.it-ebooks.info\n\n559\n\nConﬁ gurable Test Double\n\n560\n\nConﬁ gurable Test Double\n\nChapter 23 Test Double Patterns\n\nVariation: Conﬁ guration Interface\n\nA Conﬁ guration Interface comprises a separate set of methods that the Conﬁ gurable Test Double provides speciﬁ cally for use by the test to set each value that the Conﬁ gurable Test Double returns or expects to receive. The test simply calls these methods during the ﬁ xture setup phase of the Four-Phase Test (page 358). The SUT uses the “other” methods on the Conﬁ gurable Test Double (the “normal” interface). It isn’t aware that the Conﬁ guration Interface exists on the object to which it is delegating.\n\nConﬁ guration Interfaces come in two ﬂ avors. Early toolkits, such as Mock- Maker, generated a distinct method for each value we needed to conﬁ gure. The collection of these setter methods made up the Conﬁ guration Interface. More recently introduced toolkits, such as JMock, provide a generic interface that is used to build an Expected Behavior Speciﬁ cation (see Behavior Veriﬁ cation on page 468) that the Conﬁ gurable Test Double interprets at runtime. A well-designed ﬂ uent interface can make the test much easier to read and understand.\n\nVariation: Conﬁ guration Mode\n\nWe can avoid deﬁ ning a separate set of methods to conﬁ gure the Test Double by providing a Conﬁ guration Mode that the test uses to “teach” the Conﬁ gurable Test Double what to expect. At ﬁ rst glance, this means of conﬁ guring the Test Double can be confusing: Why does the Test Method (page 348) call the methods of this other object before it calls the methods it is exercising on the SUT? When we come to grips with the fact that we are doing a form of “record and play- back,” this technique makes a bit more sense.\n\nThe main advantage of using a Conﬁ guration Mode is that it avoids creating a separate set of methods for conﬁ guring the Conﬁ gurable Test Double because we reuse the same methods that the SUT will be calling. (We do have to pro- vide a way to set the values to be returned by the methods, so we have at least one additional method to add.) On the ﬂ ip side, each method that the SUT is expected to call now has two code paths through it: one for the Conﬁ guration Mode and another for the “usage mode.”\n\nVariation: Hand-Built Test Double\n\nA Hand-Built Test Double is one that was deﬁ ned by the test automater for one or more speciﬁ c tests. A Hard-Coded Test Double is inherently a Hand-Built Test Double, while a Conﬁ gurable Test Double can be either hand-built or gener- ated. This book uses Hand-Built Test Doubles in a lot of the examples because it is easier to see what is going on when we have actual, simple, concrete code to look at. This is the main advantage of using a Hand-Built Test Double; indeed,\n\nwww.it-ebooks.info\n\nConfigurable Test Double\n\nsome people consider this beneﬁ t to be so important that they use Hand-Built Test Doubles exclusively. We may also use a Hand-Built Test Double when no third-party toolkits are available or if we are prevented from using those tools by project or corporate policy.\n\nVariation: Statically Generated Test Double\n\nThe early third-party toolkits used code generators to create the code for Stati- cally Generated Test Doubles. The code is then compiled and linked with our handwritten test code. Typically, we will store the code in a source code repository [SCM]. Whenever the interface of the target class changes, of course, we must regenerate the code for our Statically Generated Test Doubles. It may be advan- tageous to include this step as part of the automated build script to ensure that it really does happen whenever the interface changes.\n\nInstantiating a Statically Generated Test Double is the same as instantiating a Hand-Built Test Double. That is, we use the name of the generated class to construct the Conﬁ gurable Test Double.\n\nAn interesting problem arises during refactoring. Suppose we change the interface of the class we are replacing by adding an argument to one of the methods. Should we then refactor the generated code? Or should we regener- ate the Statically Generated Test Double after the code it replaces has been refactored? With modern refactoring tools, it may seem easier to refactor the generated code and the tests that use it in a single step; this strategy, however, may leave the Statically Generated Test Double without argument veriﬁ cation logic or variables for the new parameter. Therefore, we should regenerate the Statically Generated Test Double after the refactoring is ﬁ nished to ensure that the refactored Statically Generated Test Double works properly and can be recreated by the code generator.\n\nVariation: Dynamically Generated Test Double\n\nNewer third-party toolkits generate Conﬁ gurable Test Doubles at runtime by using the reﬂ ection capabilities of the programming language to examine a class or interface and build an object that is capable of understanding all calls to its methods. These Conﬁ gurable Test Doubles may interpret the behavior speciﬁ cation at runtime or they may generate executable code; nevertheless, there is no source code for us to generate and maintain or regenerate. The down side is simply that there is no code to look at—but that really isn’t a disadvantage unless we are particularly suspicious or paranoid.\n\nMost of today’s tools generate Mock Objects because they are the most fashionable and widely used options. We can still use these objects as Test Stubs,\n\nwww.it-ebooks.info\n\n561\n\nConﬁ gurable Test Double\n\n562\n\nConﬁ gurable Test Double\n\nChapter 23 Test Double Patterns\n\nhowever, because they do provide a way of setting the value to be returned when a particular method is called. If we aren’t particularly interested in verifying the methods being called or the arguments passed to them, most toolkits provide a way to specify “don’t care” arguments. Given that most toolkits generate Mock Objects, they typically don’t provide a Retrieval Interface (see Test Spy).\n\nMotivating Example\n\nHere’s a test that uses a Hard-Coded Test Double to give it control over the time:\n\npublic void testDisplayCurrentTime_AtMidnight_HCM() throws Exception { // Fixture Setup // Instantiate hard-code Test Stub: TimeProvider testStub = new MidnightTimeProvider(); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Inject Stub into SUT sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify Direct Output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nThis test is hard to understand without seeing the deﬁ nition of the Hard-Coded Test Double. It is easy to see how this lack of clarity can lead to a Mystery Guest (see Obscure Test) if the deﬁ nition is not close at hand.\n\nclass MidnightTimeProvider implements TimeProvider { public Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.HOUR_OF_DAY, 0); myTime.set(Calendar.MINUTE, 0); return myTime; } }\n\nWe can solve the Obscure Test problem by using a Self Shunt (see Hard-Coded Test Double) to make the Hard-Coded Test Double visible within the test:\n\npublic class SelfShuntExample extends TestCase implements TimeProvider { public void testDisplayCurrentTime_AtMidnight() throws Exception { // Fixture Setup\n\nwww.it-ebooks.info\n\nConfigurable Test Double\n\nTimeDisplay sut = new TimeDisplay(); // Mock Setup sut.setTimeProvider(this); // self shunt installation // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify Direct Output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\npublic Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.MINUTE, 0); myTime.set(Calendar.HOUR_OF_DAY, 0); return myTime; } }\n\nUnfortunately, we will need to build the Test Double behavior into each Testcase Class (page 373) that requires it, which results in Test Code Duplication.\n\nRefactoring Notes\n\nRefactoring a test that uses a Hard-Coded Test Double to become a test that uses a third-party Conﬁ gurable Test Double is relatively straightforward. We simply follow the directions provided with the toolkit to instantiate the Conﬁ gurable Test Double and conﬁ gure it with the same values as we used in the Hard-Coded Test Double. We may also have to move some of the logic that was originally hard-coded within the Test Double into the Test Method and pass it in to the Test Double as part of the conﬁ guration step.\n\nConverting the actual Hard-Coded Test Double into a Conﬁ gurable Test Double is a bit more complicated, but not overly so if we need to capture only simple behavior. (For more complex behavior, we’re probably better off examining one of the existing toolkits and porting it to our environment if it is not yet available.) First we need to introduce a way to set the values to be returned or expected. The best choice is to start by modifying the test to see how we want to interact with the Conﬁ gurable Test Double. After instantiating it during the ﬁ xture setup part of the test, we then pass the test-speciﬁ c values to the Conﬁ gurable Test Double using the emerging Conﬁ guration Interface or Conﬁ guration Mode. Once we’ve seen how we want to use the Conﬁ gurable Test Double, we can use an Introduce Field [JetBrains] refactoring to create the instance variables of the Conﬁ gurable Test Double to hold each of the previ- ously hard-coded values.\n\nwww.it-ebooks.info\n\n563\n\nConﬁ gurable Test Double\n\n564\n\nConﬁ gurable Test Double\n\nChapter 23 Test Double Patterns\n\nExample: Conﬁ guration Interface Using Setters\n\nThe following example shows how a test would use a simple hand-built Conﬁ guration Interface using Setter Injection:\n\npublic void testDisplayCurrentTime_AtMidnight() throws Exception { // Fixture setup // Test Double conﬁguration TimeProviderTestStub tpStub = new TimeProviderTestStub(); tpStub.setHours(0); tpStub.setMinutes(0); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Test Double installation sut.setTimeProvider(tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify Outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nThe Conﬁ gurable Test Double is implemented as follows:\n\nclass TimeProviderTestStub implements TimeProvider { // Conﬁguration Interface public void setHours(int hours) { // 0 is midnight; 12 is noon myTime.set(Calendar.HOUR_OF_DAY, hours); }\n\npublic void setMinutes(int minutes) { myTime.set(Calendar.MINUTE, minutes); } // Interface Used by SUT public Calendar getTime() { // @return the last time that was set return myTime; } }\n\nExample: Conﬁ guration Interface Using Expression Builder\n\nNow let’s contrast the Conﬁ guration Interface we deﬁ ned in the previous example with the one provided by the JMock framework. JMock generates Mock Objects dynamically and provides a generic ﬂ uent interface for conﬁ guring the Mock Object in an intent-revealing style. Here’s the same test converted to use JMock:\n\nwww.it-ebooks.info\n\nConfigurable Test Double\n\npublic void testDisplayCurrentTime_AtMidnight_JM() throws Exception { // Fixture setup TimeDisplay sut = new TimeDisplay(); // Test Double conﬁguration Mock tpStub = mock(TimeProvider.class); Calendar midnight = makeTime(0,0); tpStub.stubs().method(\"getTime\"). withNoArguments(). will(returnValue(midnight)); // Test Double installation sut.setTimeProvider((TimeProvider) tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify Outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nHere we have moved some of the logic to construct the time to be returned into the Testcase Class because there is no way to do it in the generic mocking frame- work; we’ve used a Test Utility Method (page 599) to construct the time to be returned. This next example shows a conﬁ gurable Mock Object complete with multiple expected parameters:\n\npublic void testRemoveFlight_JMock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // mock conﬁguration Mock mockLog = mock(AuditLog.class); mockLog.expects(once()).method(\"logMessage\") .with(eq(helper.getTodaysDateWithoutTime()), eq(Helper.TEST_USER_NAME), eq(Helper.REMOVE_FLIGHT_ACTION_CODE), eq(expectedFlightDto.getFlightNumber())); // mock installation facade.setAuditLog((AuditLog) mockLog.proxy()); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); // verify() method called automatically by JMock }\n\nThe Expected Behavior Speciﬁ cation is built by calling expression-building methods such as expects, once, and method to describe how the Conﬁ gurable\n\nwww.it-ebooks.info\n\n565\n\nConﬁ gurable Test Double\n\n566\n\nConﬁ gurable Test Double\n\nChapter 23 Test Double Patterns\n\nTest Double should be used and what it should return. JMock supports the speciﬁ cation of much more sophisticated behavior (such as multiple calls to the same method with different arguments and return values) than does our hand-built Conﬁ gurable Test Double.\n\nExample: Conﬁ guration Mode\n\nIn the next example, the test has been converted to use a Mock Object with a Conﬁ guration Mode:\n\npublic void testRemoveFlight_ModalMock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); // mock conﬁguration (in Conﬁguration Mode) ModalMockAuditLog mockLog = new ModalMockAuditLog(); mockLog.logMessage(Helper.getTodaysDateWithoutTime(), Helper.TEST_USER_NAME, Helper.REMOVE_FLIGHT_ACTION_CODE, expectedFlightDto.getFlightNumber()); mockLog.enterPlaybackMode(); // mock installation FlightManagementFacade facade = new FlightManagementFacadeImpl(); facade.setAuditLog(mockLog); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); mockLog.verify(); }\n\nHere the test calls the methods on the Conﬁ gurable Test Double during the ﬁ xture setup phase. If we weren’t aware that this test uses a Conﬁ gurable Test Double mock, we might ﬁ nd this structure confusing at ﬁ rst glance. The most obvious clue to its intent is the call to the method enterPlaybackMode, which tells the Conﬁ gurable Test Double to stop saving expected values and to start asserting on them.\n\nThe Conﬁ gurable Test Double used by this test is implemented like this:\n\nprivate int mode = record;\n\npublic void enterPlaybackMode() { mode = playback; }\n\npublic void logMessage( Date date, String user, String action, Object detail) {\n\nwww.it-ebooks.info\n\nConfigurable Test Double\n\nif (mode == record) { Assert.assertEquals(\"Only supports 1 expected call\", 0, expectedNumberCalls); expectedNumberCalls = 1; expectedDate = date; expectedUser = user; expectedCode = action; expectedDetail = detail; } else { Assert.assertEquals(\"Date\", expectedDate, date); Assert.assertEquals(\"User\", expectedUser, user); Assert.assertEquals(\"Action\", expectedCode, action); Assert.assertEquals(\"Detail\", expectedDetail, detail); } }\n\nThe if statement checks whether we are in record or playback mode. Because this simple hand-built Conﬁ gurable Test Double allows only a single value to be stored, a Guard Assertion (page 490) fails the test if it tries to record more than one call to this method. The rest of the then clause saves the parameters into variables that it uses as the expected values of the Equality Assertions (see Assertion Method on page 362) in the else clause.\n\nwww.it-ebooks.info\n\n567\n\nConﬁ gurable Test Double\n\n568\n\nAlso known as: Hard-Coded Mock Object, Hard-Coded Test Stub, Hard-Coded Test Spy\n\nHard-Coded Test Double\n\nChapter 23 Test Double Patterns\n\nHard-Coded Test Double\n\nHow do we tell a Test Double what to return or expect?\n\nWe build the Test Double by hard-coding the return values and/or expected calls.\n\nFixture Fixture\n\nDOC DOC\n\nCreation Creation\n\nSetup Setup\n\nExpectations Expectations\n\nInstallation Installation\n\nExercise Exercise\n\nSUT SUT\n\nReturn Return Values Values\n\nVerify Verify\n\nTeardown Teardown\n\nTest Test Double Double\n\nTest Doubles (page 522) are used for many reasons during the development of Fully Automated Tests (see page 26). The behavior of the Test Double may vary from test to test, and there are many ways to deﬁ ne this behavior.\n\nWhen the Test Double is very simple or very speciﬁ c to a single test, the sim-\n\nplest solution is often to hard-code the behavior into the Test Double.\n\nHow It Works\n\nThe test automater hard-codes all of the Test Double’s behavior into the Test Double. For example, if the Test Double needs to return a value for a method call, the value is hard-coded into the return statement. If it needs to verify that a certain parameter had a speciﬁ c value, the assertion is hard-coded with the value that is expected.\n\nwww.it-ebooks.info\n\nHard-Coded Test Double\n\nWhen to Use It\n\nWe typically use a Hard-Coded Test Double when the behavior of the Test Double is very simple or is very speciﬁ c to a single test or Testcase Class (page 373). The Hard-Coded Test Double can be either a Test Stub (page 529), a Test Spy (page 538), or a Mock Object (page 544), depending on what we encode in the method(s) called by the SUT.\n\nBecause each Hard-Coded Test Double is purpose-built by hand, its construction may take more effort than using a third-party Conﬁ gurable Test Double (page 558). It can also result in more test code to maintain and refactor as the SUT changes. If different tests require that the Test Double behave in different ways and the use of Hard-Coded Test Doubles results in too much Test Code Duplication (page 213), we should consider using a Conﬁ gurable Test Double instead.\n\nImplementation Notes\n\nHard-Coded Test Doubles are inherently Hand-Built Test Doubles (see Conﬁ gurable Test Double) because there tends to be no point in generating Hard-Coded Test Doubles automatically. Hard-Coded Test Doubles can be implemented with dedicated classes, but they are most commonly used when the programming language supports blocks, closures, or inner classes. All of these language features help to avoid the ﬁ le/class overhead associated with creating a Hard-Coded Test Double; they also keep the Hard-Coded Test Double’s behavior visible within the test that uses it. In some languages, this can make the tests a bit more difﬁ cult to read. This is especially true when we use anonymous inner classes, which require a lot of syntactic overhead to deﬁ ne the class in-line. In languages that support blocks directly, and in which developers are very familiar with their usage idioms, using Hard-Coded Test Doubles can actually make the tests easier to read.\n\nThere are many different ways to implement a Hard-Coded Test Double,\n\neach of which has its own advantages and disadvantages.\n\nVariation: Test Double Class\n\nWe can implement the Hard-Coded Test Double as a class distinct from either the Testcase Class or the SUT. This allows the Hard-Coded Test Double to be reused by several Testcase Classes but may result in an Obscure Test (page 186; caused by a Mystery Guest) because it moves important indirect inputs or indi- rect outputs of the SUT out of the test to somewhere else, possibly out of sight of the test reader. Depending on how we implement the Test Double Class, it may also result in code proliferation and additional Test Double classes to maintain.\n\nwww.it-ebooks.info\n\n569\n\nHard-Coded Test Double\n\n570\n\nHard-Coded Test Double\n\nAlso known as: Loopback, Testcase Class as Test Double\n\nChapter 23 Test Double Patterns\n\nOne way to ensure that the Test Double Class is type-compatible with the component it will replace is to make the Test Double Class a subclass of that component. We then override any methods whose behavior we want to change.\n\nVariation: Test Double Subclass\n\nWe can also implement the Hard-Coded Test Double by subclassing the real DOC and overriding the behavior of the methods we expect the SUT to call as we exercise it. Unfortunately, this approach can have unpredictable consequences if the SUT calls other DOC methods that we have not overridden. It also ties our test code very closely to the implementation of the DOC and can result in Over- speciﬁ ed Software (see Fragile Test on page 239). Using a Test Double Subclass may be a reasonable option in very speciﬁ c circumstances (e.g., while doing a spike or when it is the only option available to us), but this strategy isn’t recom- mended on a routine basis.\n\nVariation: Self Shunt\n\nWe can implement the methods that we want the SUT to call on the Testcase Class and install the Testcase Object (page 382) into the SUT as the Test Double to be used. This approach is called a Self Shunt.\n\nThe Self Shunt can be either a Test Stub, a Test Spy, or a Mock Object, depending on what the method called by the SUT does. In each case, it will need to access instance variables of the Testcase Class to know what to do or expect. In statically typed languages, the Testcase Class must also implement the interface on which the SUT depends.\n\nWe typically use a Self Shunt when we need a Hard-Coded Test Double that is very speciﬁ c to a single Testcase Class. If only a single Test Method (page 348) requires the Hard-Coded Test Double, using an Inner Test Double may result in greater clarity if our language supports it.\n\nVariation: Inner Test Double\n\nA popular way to implement a Hard-Coded Test Double is to code it as an anonymous inner class or block closure within the Test Method. This strategy gives the Test Double access to instance variables and constants of the Testcase Class and even the local variables of the Test Method, which can eliminate the need to conﬁ gure the Test Double.\n\nWhile the name of this variation is based on the name of the Java language construct of which it takes advantage, many programming languages have an equivalent mechanism for deﬁ ning code to be run later using blocks or closures.\n\nwww.it-ebooks.info\n\nHard-Coded Test Double\n\nWe typically use an Inner Test Double when we are building a Hard-Coded Test Double that is relatively simple and is used only within a single Test Method. Many people ﬁ nd the use of a Hard-Coded Test Double more intuitive than using a Self Shunt because they can see exactly what is going on within the Test Method. Readers who are unfamiliar with the syntax of anonymous inner classes or blocks may ﬁ nd the test difﬁ cult to understand, however.\n\nVariation: Pseudo-Object\n\nOne challenge facing writers of Hard-Coded Test Doubles is that we must implement all the methods in the interface that the SUT might call. In statically typed languages such as Java and C#, we must at least implement all methods declared in the interface implied by the class or type associated with however we access the DOC. This often “forces” us to subclass from the real DOC to avoid providing dummy implementations for these methods.\n\nOne way of reducing the programming effort is to provide a default class that implements all the interface methods and throws a unique error. We can then implement a Hard-Coded Test Double by subclassing this concrete class and overriding just the one method we expect the SUT to call while we are exercising it. If the SUT calls any other methods, the Pseudo-Object throws an error, thereby failing the test.\n\nMotivating Example\n\nThe following test veriﬁ es the basic functionality of the component that formats an HTML string containing the current time. Unfortunately, it depends on the real system clock, so it rarely passes!\n\npublic void testDisplayCurrentTime_AtMidnight() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result); }\n\nwww.it-ebooks.info\n\n571\n\nHard-Coded Test Double\n\n572\n\nHard-Coded Test Double\n\nChapter 23 Test Double Patterns\n\nRefactoring Notes\n\nThe most common transition is from using the real component to using a Hard-Coded Test Double.4 To make this transition, we need to build the Test Double itself and install it from within our Test Method. We may also need to introduce a way to install the Test Double using one of the Dependency Injection patterns (page 678) if the SUT does not already support this installation. The process for doing so is described in the Replace Dependency with Test Double (page 522) refactoring.\n\nExample: Test Double Class\n\nHere’s the same test modiﬁ ed to use a Hard-Coded Test Double class to allow control over the time:\n\npublic void testDisplayCurrentTime_AtMidnight_HCM() throws Exception { // Fixture setup // Instantiate hard-coded Test Stub TimeProvider testStub = new MidnightTimeProvider(); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Inject Test Stub into SUT sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nThis test is hard to understand without seeing the deﬁ nition of the Hard-Coded Test Double. We can readily see how this approach might lead to an Obscure Test caused by a Mystery Guest if the Hard-Coded Test Double is not close at hand.\n\nclass MidnightTimeProvider implements TimeProvider { public Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.HOUR_OF_DAY, 0); myTime.set(Calendar.MINUTE, 0); return myTime; } }\n\n4 We rarely move from a Conﬁ gurable Test Double to a Hard-Coded Test Double because we generally seek to make the Test Double more—not less—reusable.\n\nwww.it-ebooks.info\n\nHard-Coded Test Double\n\nDepending on the programming language, this Test Double Class can be deﬁ ned in a number of different places, including within the body of the Testcase Class (an inner class) and as a separate free-standing class either in the same ﬁ le as the test or in its own ﬁ le. Of course, the farther away the Test Double Class resides from the Test Method, the more of a Mystery Guest it becomes.\n\nExample: Self Shunt/Loopback\n\nHere’s a test that uses a Self Shunt to allow control over the time:\n\npublic class SelfShuntExample extends TestCase implements TimeProvider { public void testDisplayCurrentTime_AtMidnight() throws Exception { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // mock setup sut.setTimeProvider(this); // self shunt installation // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\npublic Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.MINUTE, 0); myTime.set(Calendar.HOUR_OF_DAY, 0); return myTime; } }\n\nNote how both the Test Method that installs the Hard-Coded Test Double and the implementation of the getTime method called by the SUT are members of the same class. We used the Setter Injection pattern (see Dependency Injection) to install the Hard-Coded Test Double. Because this example is written in a statically typed language, we had to add the clause implements TimeProvider to the Testcase Class declaration so that the sut.setTimeProvider(this) statement will compile. In a dynamically typed language, this step is unnecessary.\n\nExample: Subclassed Inner Test Double\n\nHere’s a JUnit test that uses a Subclassed Inner Test Double using Java’s “Anon- ymous Inner Class” syntax:\n\nwww.it-ebooks.info\n\n573\n\nHard-Coded Test Double\n\n574\n\nHard-Coded Test Double\n\nChapter 23 Test Double Patterns\n\npublic void testDisplayCurrentTime_AtMidnight_AIM() throws Exception { // Fixture setup // Deﬁne and instantiate Test Stub TimeProvider testStub = new TimeProvider() { // Anonymous inner stub public Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.MINUTE, 0); myTime.set(Calendar.HOUR_OF_DAY, 0); return myTime; } }; // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Inject Test Stub into SUT sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nHere we used the name of the real depended-on class (TimeProvider) in the call to new for the deﬁ nition of the Hard-Coded Test Double. By including a deﬁ nition of the method getTime within curly braces after the classname, we are actually creating an anonymous Subclassed Test Double inside the Test Method.\n\nExample: Inner Test Double Subclassed from Pseudo-Class\n\nSuppose we have replaced one implementation of a method with another imple- mentation that we need to leave around for backward-compatibility purposes, but we want to write tests to ensure that the old method is no longer called. This is easy to do if we already have the following Pseudo-Object deﬁ nition:\n\n/** * Base class for hand-coded Test Stubs and Mock Objects */ public class PseudoTimeProvider implements ComplexTimeProvider {\n\npublic Calendar getTime() throws TimeProviderEx { throw new PseudoClassException(); }\n\npublic Calendar getTimeDifference(Calendar baseTime, Calendar otherTime) throws TimeProviderEx { throw new PseudoClassException();\n\nwww.it-ebooks.info\n\nHard-Coded Test Double\n\n}\n\npublic Calendar getTime( String timeZone ) throws TimeProviderEx { throw new PseudoClassException(); } }\n\nWe can now write a test that ensures the old version of the getTime method is not called by subclassing and overriding the newer version of the method (the one we expect to be called by the SUT):\n\npublic void testDisplayCurrentTime_AtMidnight_PS() throws Exception { // Fixture setup // Deﬁne and instantiate Test Stub TimeProvider testStub = new PseudoTimeProvider() { // Anonymous inner stub public Calendar getTime(String timeZone) { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.MINUTE, 0); myTime.set(Calendar.HOUR_OF_DAY, 0); return myTime; } }; // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Inject Test Stub into SUT: sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nIf any of the other methods are called, the base class methods are invoked and throw an exception. Therefore, if we run this test and one of the methods we didn’t override is called, we will see the following output as the ﬁ rst line of the JUnit stack trace for this test error:\n\ncom..PseudoClassEx: Unexpected call to unsupported method. at com..PseudoTimeProvider.getTime(PseudoTimeProvider.java:22) at com..TimeDisplay.getCurrentTimeAsHtmlFragment(TimeDisplay.java:64) at com..TimeDisplayTestSolution. testDisplayCurrentTime_AtMidnight_PS( TimeDisplayTestSolution.java:247)\n\nwww.it-ebooks.info\n\n575\n\nHard-Coded Test Double\n\n576\n\nHard-Coded Test Double\n\nChapter 23 Test Double Patterns\n\nWhat’s in a (Pattern) Name?\n\nThe Importance of Good Names Names are important because they are a key part of how we communicate. Names are labels we attach to concepts. Good names help us communi- cate those concepts. This is true when we are communicating with people who already know the names, but especially when we are communicating with people who don’t. Consider the following example.\n\nEarly in my pattern-writing days, I attended the very ﬁ rst Pattern Languages of Programs (PLoP) conference (http://www.hillside.net/ conferences/plop). At the conference, the well-known author Jim Co- plien (“Cope,” to his friends) had a pattern language of organizational patterns being workshopped. One of the patterns was called “Buffalo Mountain”; another was called “Architect Also Implements.” These two pattern names are at opposite ends of the spectrum as far as pat- tern names are concerned.\n\nThe gist of “Architect Also Implements” can be gleaned from the pattern name even if a person has not read the actual pattern. The name is both a placeholder for the pattern and meaningful in its own right.\n\nThe name “Buffalo Mountain,” by contrast, does not readily communi- cate its underlying meaning. To this day I can still remember the story behind the name—but I cannot remember the actual focus of the pattern. The name was based on a graph that plotted some data related to the pattern. An early reviewer thought it resembled the proﬁ le of a nearby mountain called Buffalo Mountain. Thus, while the pattern name is mem- orable, it is not very evocative.\n\nCloser to home, Self Shunt (see Hard-Coded Test Double on page 568) is an example of a name that is less than evocative because the term “shunt” is not widely used except in a few specialized ﬁ elds. Michael Feathers does a good job explaining the background of the name in his description of the pattern. Unless you’ve read that description, however, the name is “just a name.” A more evocative name might be something like “Testcase Class as Test Double” or “Loopback” but even the latter suffers from ambiguity because it isn’t clear what is being looped back. So the name Self Shunt survives because it is in common use.\n\nwww.it-ebooks.info\n\nHard-Coded Test Double\n\nOther Naming Considerations People might ask why I sometimes propose alternative names for some patterns. The preceding story highlights one of the reasons. Another reason is that in a larger collection of patterns (such as this book), it is important that there exists a “system of names.”\n\nLet me illustrate this second reason with an example. Many people advocate the use of a setUp method to create the test ﬁ xture. This approach moves the ﬁ xture setup logic out of each individual Test Method (page 348) and into a single place where it can be reused. Many people might refer to this pattern as “Shared Setup Method.” But in this pattern language, I’ve chosen to call it Implicit Setup (page 424). Why?\n\nIt comes down to the names of other patterns in the language. On the one hand, “Shared Setup Method” could easily be confused with the existing pattern Shared Fixture (page 317). (The former pattern deals with sharing code, whereas the latter pattern focuses on sharing the runtime objects in the ﬁ xture.) On the other hand, the two major alternatives to Implicit Setup are called In-line Setup (page 408) and Delegated Setup (page 411). Wouldn’t you agree that “In-line Setup, Delegated Setup, Implicit Setup” forms a better “system of names” than “In-line Setup, Delegated Setup, Shared Setup Method”? The connection between the pattern names is much more obvious when we consider all the major alternative patterns when choosing the system of names.\n\nWhy Standardize Testing Patterns? The last part of this soapbox highlights why I think it is important for us to standardize the names of the test automation patterns, especially those related to Test Stubs (page 529) and Mock Objects (page 544). The key issue here relates to succinctness of communication.\n\nWhen someone tells you, “Put a mock in it” (pun intended!), what advice is that person giving you? Depending on what the person means by a “mock,” he or she could be suggesting that you control the indirect inputs of your SUT using a Test Stub or that you replace your database with a Fake Database (see Fake Object on page 551) that will reduce test inter- actions and speed up your tests by a factor of 50. (Yes, 50! See the sidebar “Faster Tests Without Shared Fixtures” on page 319.) Or perhaps the person is suggesting that you verify that your SUT calls the correct meth- ods by installing an Eager Mock Object (see Mock Object) preconﬁ gured\n\nContinued...\n\nwww.it-ebooks.info\n\n577\n\nHard-Coded Test Double\n\n578\n\nHard-Coded Test Double\n\nChapter 23 Test Double Patterns\n\nwith the Expected Behavior (see Behavior Veriﬁ cation on page 468). If everyone used “mock” to mean a Mock Object—no more or less—then the advice would be pretty clear. As I write this, the advice is very murky because we have taken to calling just about any Test Double (page 522) a “mock object” (despite the objections of the authors of the original paper on Mock Objects [ET]).\n\nFurther Reading If you want to ﬁ nd out what “Buffalo Mountain” is really about, go to http://www1.bell-labs.com/user/cope/Patterns/Process/section29.html.\n\nYou can ﬁ nd “Architect Also Implements” at http://www1.bell-labs.com/ user/cope/Patterns/Process/section16.html.\n\nInterestingly, Alistair Cockburn wrote a similar comparison of pattern names in an article on his Web site (http://alistair.cockburn.us) and chose exactly the same two pattern names in his comparison. Coincidence or pattern?\n\nIn addition to failing the test, this scheme makes it very easy to see exactly which method was called. The bonus is that it works for calls to all unexpected methods with no additional effort.\n\nFurther Reading\n\nMany of the “how to” books on test-driven development provide examples of Self Shunt, including [TDD-APG], [TDD-BE], [UTwJ], [PUT], and [JuPG]. The original write-up was by Michael Feathers and is accessible at http://www.objectmentor. com/resources/articles/SelfShunPtrn.pdf\n\nThe original “Shunt” pattern is written up at http://http://c2.com/cgi/wiki? ShuntPattern, along with a list of alternative names including “Loopback.” See the sidebar “What’s in a (Pattern) Name?” on page 576 for a discussion of how to select meaningful and evocative pattern names.\n\nThe Pseudo-Object pattern is described in the paper “Pseudo-Classes: Very Simple and Lightweight Mock Object-like Classes for Unit-Testing” available at http://www.devx.com/Java/Article/22599/1954?pf=true.\n\nwww.it-ebooks.info\n\nTest-Specific Subclass\n\nTest-Speciﬁ c Subclass\n\nHow can we make code testable when we need to access private state of the SUT?\n\nWe add methods that expose the state or behavior needed by the test to a subclass of the SUT.\n\nSUT SUT\n\nExercise Exercise\n\nMethod Under Test Method Under Test\n\nSetup Setup\n\nCreate Create\n\nInternal Method Internal Method\n\nExercise Exercise\n\nVerify Verify\n\nSet State Set State\n\nGet State Get State\n\nTest- Test- Specific Specific Subclass Subclass\n\nInternal Method Internal Method\n\nOverridden Overridden Self Call Self Call\n\nTeardown Teardown\n\nIf the SUT was not designed speciﬁ cally to be testable, we may ﬁ nd that the test cannot gain access to a state that it must initialize or verify at some point in the test.\n\nA Test-Speciﬁ c Subclass is a simple yet very powerful way to open up the\n\nSUT for testing purposes without modifying the code of the SUT itself.\n\nHow It Works\n\nWe deﬁ ne a subclass of the SUT and add methods that modify the behavior of the SUT just enough to make it testable by implementing control points and observation points. This effort typically involves exposing instance variables using setters and getters or perhaps adding a method to put the SUT into a speciﬁ c state without moving through its entire life cycle.\n\nwww.it-ebooks.info\n\n579\n\nAlso known as: Test-Speciﬁ c Extension\n\nTest-Speciﬁ c Subclass\n\n580\n\nTest-Speciﬁ c Subclass\n\nAlso known as: Subclassed Test Double\n\nChapter 23 Test Double Patterns\n\nBecause the Test-Speciﬁ c Subclass would be packaged together with the tests that use it, the use of a Test-Speciﬁ c Subclass does not change how the SUT is seen by the rest of the application.\n\nWhen to Use It\n\nWe should use a Test-Speciﬁ c Subclass whenever we need to modify the SUT to improve its testability but doing so directly would result in Test Logic in Produc- tion (page 217). Although we can use a Test-Speciﬁ c Subclass for a number of purposes, all of those scenarios share a common goal: They improve testability by letting us get at the insides of the SUT more easily. A Test-Speciﬁ c Subclass can be a double-edged sword, however. By breaking encapsulation, it allows us to tie our tests even more closely to the implementation, which can in turn result in Fragile Tests (page 239).\n\nVariation: State-Exposing Subclass\n\nIf we are doing State Veriﬁ cation (page 462), we can subclass the SUT (or some component of it) so that we can see the internal state of the SUT for use in Assertion Methods (page 362). Usually, this effort involves adding accessor methods for pri- vate instance variables. We may also allow the test to set the state as a way to avoid Obscure Tests (page 186) caused by Obscure Setup (see Obscure Test) logic.\n\nVariation: Behavior-Exposing Subclass\n\nIf we want to test the individual steps of a complex algorithm individually, we can subclass the SUT to expose the private methods that implement the Self- Calls [WWW]. Because most languages do not allow for relaxing the visibility of a method, we often have to use a different name in the Test-Speciﬁ c Subclass and make a call to the superclass’s method.\n\nVariation: Behavior-Modifying Subclass\n\nIf the SUT contains some behavior that we do not want to occur when testing, we can override whatever method implements the behavior with an empty method body. This technique works best when the SUT uses Self-Calls (or a Template Method [GOF]) to delegate the steps of an algorithm to methods on itself or subclasses.\n\nVariation: Test Double Subclass\n\nTo ensure that a Test Double (page 522) is type-compatible with a DOC we wish to replace, we can make the Test Double a subclass of that component. This may\n\nwww.it-ebooks.info\n\nTest-Specific Subclass\n\nbe the only way we can build a Test Double that the compiler will accept when variables are statically typed using concrete classes.5 (We should not have to take this step with dynamically typed languages such as Ruby, Python, Perl, and JavaScript.) We then override any methods whose behavior we want to change and add any methods we require to transform the Test Double into a Conﬁ gu- rable Test Double (page 558) if we so desire.\n\nUnlike the Behavior-Modifying Subclass, the Test Double Subclass does not just “tweak” the behavior of the SUT (or a part thereof) but replaces it entirely with canned behavior.\n\nVariation: Substituted Singleton\n\nThe Substituted Singleton is a special case of Test Double Subclass. We use it when we want to replace a DOC with a Test Double and the SUT does not sup- port Dependency Injection (page 678) or Dependency Lookup (page 686).\n\nImplementation Notes\n\nThe use of a Test-Speciﬁ c Subclass brings some challenges:\n\nFeature granularity: ensuring that any behavior we want to override or expose is in its own single-purpose method. It is enabled through copi- ous use of small methods and Self-Calls.\n\nFeature visibility: ensuring that subclasses can access attributes and be- havior of the SUT class. It is primarily an issue in statically typed lan- guages such as Java, C#, and C++; dynamically typed languages typically do not enforce visibility.\n\nAs with Test Doubles, we must be careful to ensure that we do not replace any of the behavior we are actually trying to test.\n\nIn languages that support class extensions without the need for subclassing (e.g., Smalltalk, Ruby, JavaScript, and other dynamic languages), a Test-Speciﬁ c Subclass can be implemented as a class extension in the test package. We need to be aware, however, whether the extensions will make it into production; doing so would introduce Test Logic in Production.\n\n5 That is, by using a concrete class as the type of the variable rather than an abstract class or interface.\n\nwww.it-ebooks.info\n\n581\n\nAlso known as: Subclassed Singleton, Substitutable Singleton\n\nTest-Speciﬁ c Subclass\n\n582\n\nTest-Speciﬁ c Subclass\n\nChapter 23 Test Double Patterns\n\nVisibility of Features\n\nIn languages that enforce scope (visibility) of variables and methods, we may need to change the visibility of the variables to allow subclasses to access them. While such a change affects the actual SUT code, it would typically be con- sidered much less intrusive or misleading than changing the visibility to public (thereby allowing any code in the application to access the variables) or adding the test-speciﬁ c methods directly to the SUT.\n\nFor example, in Java, we might change the visibility of instance variables from private to protected to allow the Test-Speciﬁ c Subclass to access them. Similarly, we might change the visibility of methods to allow the Test-Speciﬁ c Subclass to call them.\n\nGranularity of Features\n\nLong methods are difﬁ cult to test because they often bring too many dependen- cies into play. By comparison, short methods tend to be much simpler to test because they do only one thing. Self-Call offers an easy way to reduce the size of methods. We delegate parts of an algorithm to other methods implemented on the same class. This strategy allows us to test these methods independently. We can also conﬁ rm that the calling method calls these methods in the right sequence by overriding them in a Test Double Subclass (see Test-Speciﬁ c Subclass on page 579).\n\nSelf-Call is a part of good object-oriented code design in that it keeps methods small and focused on implementing a single responsibility of the SUT. We can use this pattern whenever we are doing test-driven development and have control over the design of the SUT. We may ﬁ nd that we need to introduce Self-Call when we encounter long methods where some parts of the algorithm depend on things we do not want to exercise (e.g., database calls). This likelihood is especially high, for example, when the SUT is built using a Transaction Script [PEAA] architecture. Self-Call can be retroﬁ tted easily using the Extract Method [Fowler] refactoring supported by most modern IDEs.\n\nMotivating Example\n\nThe test in the following example is nondeterministic because it depends on the time. Our SUT is an object that formats the time for display as part of a Web page. It gets the time by asking a Singleton called TimeProvider to retrieve the time from a calendar object that it gets from the container.\n\npublic void testDisplayCurrentTime_AtMidnight() throws Exception { // Set up SUT\n\nwww.it-ebooks.info\n\nTest-Specific Subclass\n\nTimeDisplay theTimeDisplay = new TimeDisplay(); // Exercise SUT String actualTimeString = theTimeDisplay.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( \"Midnight\", expectedTimeString, actualTimeString); }\n\npublic void testDisplayCurrentTime_AtOneMinuteAfterMidnight() throws Exception { // Set up SUT TimeDisplay actualTimeDisplay = new TimeDisplay(); // Exercise SUT String actualTimeString = actualTimeDisplay.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">12:01 AM</span>\"; assertEquals( \"12:01 AM\", expectedTimeString, actualTimeString); }\n\nThese tests rarely pass, and they never pass in the same test run! The code within the SUT looks like this:\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar timeProvider; try { timeProvider = getTime(); } catch (Exception e) { return e.getMessage(); } // etc. }\n\nprotected Calendar getTime() { return TimeProvider.getInstance().getTime(); }\n\nThe code for the Singleton follows:\n\npublic class TimeProvider { protected static TimeProvider soleInstance = null;\n\nprotected TimeProvider() {};\n\npublic static TimeProvider getInstance() {\n\nwww.it-ebooks.info\n\n583\n\nTest-Speciﬁ c Subclass\n\n584\n\nTest-Speciﬁ c Subclass\n\nChapter 23 Test Double Patterns\n\nif (soleInstance==null) soleInstance = new TimeProvider(); return soleInstance; }\n\npublic Calendar getTime() { return Calendar.getInstance(); } }\n\nRefactoring Notes\n\nThe precise nature of the refactoring employed to introduce a Test-Speciﬁ c Subclass depends on why we are using one. When we are using a Test-Speciﬁ c Subclass to expose “private parts” of the SUT or override undesirable parts of its behavior, we merely deﬁ ne the Test-Speciﬁ c Subclass as a subclass of the SUT and create an instance of the Test-Speciﬁ c Subclass to exercise in the setup ﬁ xture phase of our Four-Phase Test (page 358).\n\nWhen we are using the Test-Speciﬁ c Subclass to replace a DOC of the SUT, however, we need to use a Replace Dependency with Test Double (page 522) refactoring to tell the SUT to use our Test-Speciﬁ c Subclass instead of the real DOC.\n\nIn either case, we either override existing methods or add new methods to the Test-Speciﬁ c Subclass using our language-speciﬁ c capabilities (e.g., subclass- ing or mixins) as required by our tests.\n\nExample: Behavior-Modifying Subclass (Test Stub)\n\nBecause the SUT uses a Self-Call to the getTime method to ask the TimeProvider for the time, we have an opportunity to use a Subclassed Test Double to control the time.6 Based on this idea we can take a stab at writing our tests as follows (I have shown only one test here):\n\npublic void testDisplayCurrentTime_AtMidnight() { // Fixture setup TimeDisplayTestStubSubclass tss = new TimeDisplayTestStubSubclass(); TimeDisplay sut = tss; // Test Double conﬁguration tss.setHours(0); tss.setMinutes(0); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment();\n\n6 This decision is enabled by the fact that getTime was deﬁ ned to be protected; we would not be able to do this if it was private.\n\nwww.it-ebooks.info\n\nTest-Specific Subclass\n\n// Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result ); }\n\nNote that we have used the Test-Speciﬁ c Subclass class for the variable that receives the instance of the SUT; this approach ensures that the methods of the Conﬁ gura- tion Interface (see Conﬁ gurable Test Double) deﬁ ned on the Test-Speciﬁ c Subclass are visible to the test.7 For documentation purposes, we have then assigned the Test-Speciﬁ c Subclass to the variable sut; this is a safe cast because the Test-Speciﬁ c Subclass class is a subclass of the SUT class. This technique also helps us avoid the Mystery Guest (see Obscure Test) problem caused by hard-coding an important indirect input of our SUT inside the Test Stub (page 529).\n\nNow that we have seen how it will be used, it is a simple matter to imple-\n\nment the Test-Speciﬁ c Subclass:\n\npublic class TimeDisplayTestStubSubclass extends TimeDisplay {\n\nprivate int hours; private int minutes;\n\n// Overridden method protected Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.HOUR_OF_DAY, this.hours); myTime.set(Calendar.MINUTE, this.minutes); return myTime; } /* * Conﬁguration Interface */ public void setHours(int hours) { this.hours = hours; }\n\npublic void setMinutes(int minutes) { this.minutes = minutes; } }\n\nThere’s no rocket science here—we just had to implement the methods used by the test.\n\n7 We could have used a Hard-Coded Test Double (page 568) subclass instead, but that tactic would have required a different Test-Speciﬁ c Subclass for each time we want to test with. Each subclass would simply hard-code the return value of the getTime method.\n\nwww.it-ebooks.info\n\n585\n\nTest-Speciﬁ c Subclass\n\n586\n\nTest-Speciﬁ c Subclass\n\nChapter 23 Test Double Patterns\n\nExample: Behavior-Modifying Subclass (Substituted Singleton)\n\nSuppose our getTime method was declared to beprivate8 or static, ﬁ nal or sealed, and so on.9 Such a declaration would prevent us from overriding the method’s behavior in our Test-Speciﬁ c Subclass. What could we do to address our Nondeterministic Tests (see Erratic Test on page 228)?\n\nBecause the design uses a Singleton [GOF] to provide the time, a simple solution is to replace the Singleton during test execution with a Test Double Subclass. We can do so as long as it is possible for a subclass to access its soleInstance variable. We use the Introduce Local Extension [Fowler] refactoring (speciﬁ cally, the subclass variant of it) to create the Test-Speciﬁ c Subclass. Writ- ing the tests ﬁ rst helps us understand the interface we want to implement.\n\npublic void testDisplayCurrentTime_AtMidnight() { TimeDisplay sut = new TimeDisplay(); // Install test Singleton TimeProviderTestSingleton timeProvideSingleton = TimeProviderTestSingleton.overrideSoleInstance(); timeProvideSingleton.setTime(0,0); // Exercise SUT String actualTimeString = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, actualTimeString ); }\n\nNow that we have a test that uses the Substituted Singleton, we can proceed to implement it by subclassing the Singleton and deﬁ ning the methods the tests will use.\n\npublic class TimeProviderTestSingleton extends TimeProvider { private Calendar myTime = new GregorianCalendar(); private TimeProviderTestSingleton() {};\n\n// Installation Interface static TimeProviderTestSingleton overrideSoleInstance() { // We could save the real instance ﬁrst, but we won't! soleInstance = new TimeProviderTestSingleton(); return (TimeProviderTestSingleton) soleInstance; }\n\n// Conﬁguration Interface used by the test\n\n8 A private method cannot be seen or overridden by a subclass. 9 This choice prevents a subclass from overriding the method’s behavior.\n\nwww.it-ebooks.info\n\nTest-Specific Subclass\n\npublic void setTime(int hours, int minutes) { myTime.set(Calendar.HOUR_OF_DAY, hours); myTime.set(Calendar.MINUTE, minutes); }\n\n// Usage Interface used by the client public Calendar getTime() { return myTime; } }\n\nHere the Test Double is a subclass of the real component and has overridden the instance method called by the clients of the Singleton.\n\nExample: Behavior-Exposing Subclass\n\nSuppose we wanted to test the getTime method directly. Because getTime is protected and our test is in a different package from the TimeDisplay class, our test cannot call this method. We could try making our test a subclass of TimeDisplay or we could put it into the same package as TimeDisplay. Unfortunately, both of these solutions come with baggage and may not always be possible.\n\nA more general solution is to expose the behavior using a Behavior-Exposing Subclass. We can do so by deﬁ ning a Test-Speciﬁ c Subclass and adding a public method that calls this method.\n\npublic class TimeDisplayBehaviorExposingTss extends TimeDisplay {\n\npublic Calendar callGetTime() { return super.getTime(); } }\n\nWe can now write the test using the Behavior-Exposing Subclass as follows:\n\npublic void testGetTime_default() { // create SUT TimeDisplayBehaviorExposingTss tsSut = new TimeDisplayBehaviorExposingTss(); // exercise SUT // want to do // Calendar time = sut.getTime(); // have to do Calendar time = tsSut.callGetTime(); // verify outcome assertEquals( defaultTime, time ); }\n\nwww.it-ebooks.info\n\n587\n\nTest-Speciﬁ c Subclass\n\n588\n\nTest-Speciﬁ c Subclass\n\nChapter 23 Test Double Patterns\n\nExample: Deﬁ ning Test-Speciﬁ c Equality (Behavior-Modifying Subclass)\n\nHere is an example of a very simple test that fails because the object we pass to assertEquals does not implement test-speciﬁ c equality. That is, the default equals method returns false even though our test considers the two objects to be equals.\n\nprotected void setUp() throws Exception { oneOutboundFlight = ﬁndOneOutboundFlightDto(); }\n\npublic void testGetFlights_OneFlight() throws Exception { // Exercise System List ﬂights = facade.getFlightsByOriginAirport( oneOutboundFlight.getOriginAirportId()); // Verify Outcome assertEquals(\"Flights at origin - number of ﬂights: \", 1, ﬂights.size()); FlightDto actualFlightDto = (FlightDto)ﬂights.get(0); assertEquals(\"Flight DTOs at origin\", oneOutboundFlight, actualFlightDto); }\n\nOne option is to write a Custom Assertion (page 474). Another option is to use a Test-Speciﬁ c Subclass to add a more appropriate deﬁ nition of equality for our test purposes alone. We can change our ﬁ xture setup code slightly to create the Test-Speciﬁ c Subclass as our Expected Object (see State Veriﬁ cation).\n\nprivate FlightDtoTss oneOutboundFlight;\n\nprivate FlightDtoTss ﬁndOneOutboundFlightDto() { FlightDto realDto = helper.ﬁndOneOutboundFlightDto(); return new FlightDtoTss(realDto) ; }\n\nFinally, we implement the Test-Speciﬁ c Subclass by copying and comparing only those ﬁ elds that we want to use for our test-speciﬁ c equality.\n\npublic class FlightDtoTss extends FlightDto { public FlightDtoTss(FlightDto realDto) { this.destAirportId = realDto.getDestinationAirportId(); this.equipmentType = realDto.getEquipmentType(); this.ﬂightNumber = realDto.getFlightNumber(); this.originAirportId = realDto.getOriginAirportId(); }\n\nwww.it-ebooks.info\n\nTest-Specific Subclass\n\npublic boolean equals(Object obj) { FlightDto otherDto = (FlightDto) obj; if (otherDto == null) return false; if (otherDto.getDestAirportId()!= this.destAirportId) return false; if (otherDto.getOriginAirportId()!= this.originAirportId) return false; if (otherDto.getFlightNumber()!= this.ﬂightNumber) return false; if (otherDto.getEquipmentType() != this.equipmentType ) return false; return true; } }\n\nIn this case we copied the ﬁ elds from the real DTO into our Test-Speciﬁ c Subclass, but we could just as easily have used the Test-Speciﬁ c Subclass as a wrapper for the real DTO. There are other ways we could have created the Test-Speciﬁ c Subclass; the only real limit is our imagination.\n\nThis example also assumes that we have a reasonable toString implementa- tion on our base class that prints out the values of the ﬁ elds being compared. It is needed because assertEquals will use that implementation when the equals method returns false. Otherwise, we will have no idea of why the objects are considered unequal.\n\nExample: State-Exposing Subclass\n\nSuppose we have the following test, which requires a Flight to be in a particular state:\n\nprotected void setUp() throws Exception { super.setUp(); scheduledFlight = createScheduledFlight(); }\n\nFlight createScheduledFlight() throws InvalidRequestException{ Flight newFlight = new Flight(); newFlight.schedule(); return newFlight; }\n\npublic void testDeschedule_shouldEndUpInUnscheduleState() throws Exception { scheduledFlight.deschedule(); assertTrue(\"isUnsched\", scheduledFlight.isUnscheduled()); }\n\nwww.it-ebooks.info\n\n589\n\nTest-Speciﬁ c Subclass\n\n590\n\nTest-Speciﬁ c Subclass\n\nChapter 23 Test Double Patterns\n\nSetting up the ﬁ xture for this test requires us to call the method schedule on the ﬂ ight:\n\npublic class Flight{ protected FlightState currentState = new UnscheduledState();\n\n/** * Transitions the Flight from the <code>unscheduled</code> * state to the <code>scheduled</code> state. * @throws InvalidRequestException when an invalid state * transition is requested */ public void schedule() throws InvalidRequestException{ currentState.schedule(); } }\n\nThe Flight class uses the State [GOF] pattern and delegates handling of the schedule method to whatever State object is currently referenced by currentState. This test will fail during ﬁ xture setup if schedule does not work yet on the default content of currentState. We can avoid this problem by using a State-Exposing Subclass that provides a method to move directly into the state, thereby making this an Inde- pendent Test (see page 42).\n\npublic class FlightTss extends Flight {\n\npublic void becomeScheduled() { currentState = new ScheduledState(); } }\n\nBy introducing a new method becomeScheduled on the Test-Speciﬁ c Subclass, we ensure that we will not accidentally override any existing behavior of the SUT. Now all we have to do is instantiate the Test-Speciﬁ c Subclass in our test instead of the base class by modifying our Creation Method (page 415).\n\nFlight createScheduledFlight() throws InvalidRequestException{ FlightTss newFlight = new FlightTss(); newFlight.becomeScheduled(); return newFlight; }\n\nNote how we still declare that we are returning an instance of the Flight class when we are, in fact, returning an instance of the Test-Speciﬁ c Subclass that has the additional method.\n\nwww.it-ebooks.info",
      "page_number": 585
    },
    {
      "number": 24,
      "title": "Test Organization Patterns",
      "start_page": 654,
      "end_page": 714,
      "detection_method": "regex_chapter",
      "content": "Chapter 24\n\nTest Organization Patterns\n\nPatterns in This Chapter\n\nNamed Test Suite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592\n\nTest Code Reuse\n\nTest Utility Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599\n\nParameterized Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607\n\nTestcase Class Structure\n\nTestcase Class per Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617\n\nTestcase Class per Feature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n\nTestcase Class per Fixture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631\n\nUtility Method Location\n\nTestcase Superclass. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638\n\nTest Helper. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643\n\n591\n\nwww.it-ebooks.info\n\nTest Organization Patterns\n\n592\n\nNamed Test Suite\n\nChapter 24 Test Organization Patterns\n\nNamed Test Suite\n\nHow do we run the tests when we have arbitrary groups of tests to run?\n\nWe deﬁ ne a test suite, suitably named, that contains a set of tests that we wish to be able to run as a group.\n\nTestcase Class Testcase Class\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nFixture Fixture\n\ntestMethod_1 testMethod_1\n\nImplicit tearDown Implicit tearDown\n\nTest Test Suite Suite Factory Factory\n\nCreation Creation\n\nTest Test Suite Suite Object Object\n\nSUT SUT\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\ntestMethod_n testMethod_n\n\nImplicit tearDown Implicit tearDown\n\nWhen we have a large number of tests, we need to organize them in a systematic way. A test suite allows us to group tests that have related functionality close to each other. Although we want to be able to run all the tests for the entire applica- tion or component easily, we also want to be able to run only those tests applicable to speciﬁ c subsets of the functionality or subcomponents of the system. In other situations, we want to run only a subset of all the tests we have deﬁ ned.\n\nNamed Test Suites give us a way to choose which predeﬁ ned subset of the\n\ntests we want to run.\n\nHow It Works\n\nFor each group of related tests that we would like to be able to run as a group, we can deﬁ ne a special Test Suite Factory (see Test Enumeration on page 399) with an Intent-Revealing Name. The Factory Method [GOF] can use any of several\n\nwww.it-ebooks.info\n\nNamed Test Suite\n\ntest suite construction techniques to return a Test Suite Object (page 387) containing only the speciﬁ c Testcase Objects (page 382) we wish to execute.\n\nWhen to Use It\n\nAlthough we often want to run all the tests with a single command, sometimes we want to run only a subset of the tests. The most common reason for doing so is time; for this purpose, running the AllTests Suite for a speciﬁ c context is prob- ably our best bet. When our member of xUnit doesn’t support Test Selection and the tests we want to run are scattered across multiple contexts and some contexts contain tests we deﬁ nitely don’t want run, we can use a Subset Suite.\n\nVariation: AllTests Suite\n\nWe often want to run all the tests we have available. With smaller systems, it may be standard practice to run the AllTests Suite after checking out a new code base (to ensure we start at a known point) and before every check-in (to ensure all our code works). We typically have an AllTests Suite for each package or namespace of software so that we can run subsets of the tests after each code change as part of the “red–green–refactor” cycle.\n\nVariation: Subset Suite\n\nDevelopers often do not want to run tests because they are Slow Tests (page 253). Tests that exercise components that access a database will inevitably run much more slowly than tests that run entirely in memory. By deﬁ ning one Named Test Suite for the database tests and another Named Test Suite for the in-memory tests, we can choose not to run the database tests simply by choosing to run the in-memory Subset Suite.\n\nAnother common reason given for not running tests is because the context they need to run is not available. For example, if we don’t have a Web server running on our development desktop, or if deploying our software to the Web server takes too long, we won’t want to run the tests of components that require the Web server to be running (they would just take extra time to run, and we know they will fail and spoil our chances of achieving a green bar).\n\nVariation: Single Test Suite\n\nThe degenerate form of a Subset Suite is the Single Test Suite, in which we instanti- ate a single Testcase Object so that we can run a single Test Method (page 348). This variation is particularly useful when we don’t have a Test Tree Explorer (see Test Runner on page 377) available or when the Test Method requires some\n\nwww.it-ebooks.info\n\n593\n\nNamed Test Suite\n\n594\n\nNamed Test Suite\n\nChapter 24 Test Organization Patterns\n\nform of Setup Decorator (page 447) to run properly. Some test automaters keep a “MyTest” Testcase Class (page 373) open in their workspace at all times speciﬁ - cally for this purpose.\n\nImplementation Notes\n\nThe concept of running named sets of tests is independent of how we build the Named Test Suites. For example, we can use Test Enumeration to build up our suites of tests explicitly or we can use Test Discovery (page 393) to ﬁ nd all tests in a particular place (e.g., a namespace or assembly). We can also do Test Selec- tion (page 403) from within a suite of tests to create a smaller suite dynamically. Some members of the xUnit family require us to deﬁ ne the AllTests Suites for each test package or subsystem manually; others, such as NUnit, automatically create a Test Suite Object for each namespace.\n\nWhen we are using Test Enumeration and have Named Test Suites for various subsets of the tests, it is better to deﬁ ne our AllTests Suite in terms of these subsets. When we implement the AllTests Suite as a Suite of Suites (see Test Suite Object), we need to add a new Testcase Class to only a single Named Test Suite; this collection of tests is then rolled up into the AllTests Suite for the local context as well as the Named Test Suite and the next higher context.\n\nRefactoring Notes\n\nThe steps to refactor existing code to a Named Test Suite are highly dependent on the variant of Named Test Suite we are using. For this reason, I’ll dispense with the motivating example and skip directly to examples of Named Test Suites.\n\nExample: AllTests Suite\n\nAn AllTests Suite helps us run all the tests for different subsets of the functional- ity of our choosing. For each subcomponent or context (e.g., a Java package), we deﬁ ne a special test suite (and its corresponding Test Suite Factory) called AllTests. In the suite Factory Method on the Test Suite Factory, we add all the tests in the current context and all the Named Test Suites from any nested con- texts (such as nested Java packages). That way, when the top-level Named Test Suite is run, all Named Test Suites for the nested contexts will be run as well.\n\nThe following example illustrates the kind of code that would be required to\n\nrun all the tests in most members of the xUnit family:\n\npublic class AllTests {\n\npublic static Test suite() {\n\nwww.it-ebooks.info\n\nNamed Test Suite\n\nTestSuite suite = new TestSuite(\"Test for allJunitTests\"); //$JUnit-BEGIN$ suite.addTestSuite( com.clrstream.camug.example.test.InvoiceTest.class); suite.addTest(com.clrstream.ex7.test.AllTests.suite()); suite.addTest(com.clrstream.ex8.test.AllTests.suite()); suite.addTestSuite( com.xunitpatterns.guardassertion.Example.class); //$JUnit-END$ return suite; } }\n\nWe had to use a mix of methods in this case because we are adding other Named Test Suites as well as Test Suite Objects representing a single Testcase Class. In JUnit, we use different methods to do this. Other members of the xUnit family, however, may use the same method signature.\n\nThe other notable aspect of this example is the JUnit-start and JUnit-end com- ments. The IDE (in this case, Eclipse) helps us out by automatically regener- ating the list between these two comments—a semi-automated form of Test Discovery.\n\nExample: Special-Purpose Suite\n\nSuppose we have three major packages (A, B, and C) containing business logic. Each package contains both in-memory objects and database access classes. We would then have corresponding test packages for each of the three packages. Some tests in each package would require the database, while others could run purely in memory.\n\nWe want to be able to run the following sets of tests for the entire system,\n\nand for each package (A, B, and C):\n\nAll tests\n\nAll database tests\n\nAll in-memory tests\n\nThis implies a total of 12 named sets of tests (three named sets for each of four contexts).\n\nIn each of the three packages (A, B, and C), we should deﬁ ne the following\n\nNamed Test Suites:\n\nwww.it-ebooks.info\n\n595\n\nNamed Test Suite\n\n596\n\nNamed Test Suite\n\nChapter 24 Test Organization Patterns\n\n\n\nAllDbTests, by adding all the Testcase Classes containing database tests\n\n\n\nAllInMemoryTests, by adding all the Testcase Classes containing in-memory tests\n\n\n\nAllTests, by combining AllDbTests and AllInMemoryTests\n\nThen, at the top-level testing context, we deﬁ ne Named Test Suites by the same names as follows:\n\n\n\nAllDbTests, by composing all the AllDbTests Testcase Classes from pack- ages A, B, and C\n\n\n\nAllInMemoryTests, by composing all the AllInMemoryTests Testcase Classes from packages A, B, and C\n\n\n\nAllTests, by composing all the AllTests Testcase Classes from packages A, B, and C (This is just the normal AllTests Suite.)\n\nIf we ﬁ nd ourselves needing to include some tests from a single Testcase Class in both Named Test Suites, we should split the class into one class for each context (e.g., database tests and in-memory tests).\n\nExample: Single Test Suite\n\nIn some circumstances—especially when we are using a debugger—it is highly desirable to not run all the tests in a Testcase Class. One way to run only a subset of these tests is to use the Test Tree Explorer provided by some Graphical Test Run- ners (see Test Runner). When this capability isn’t available, a common practice is to disable the tests we don’t want run by either commenting them out, copying the entire Testcase Class and deleting most of the tests, or changing the names or attri- butes of the test that cause them to be included by the Test Discovery algorithm.\n\npublic class LostTests extends TestCase { public LostTests(String name) { super(name); }\n\npublic void xtestOne() throws Exception { fail(\"test not implemented\"); }\n\n/* public void testTwo() throws Exception { fail(\"test not implemented\"); } */\n\nwww.it-ebooks.info\n\nNamed Test Suite\n\npublic void testSeventeen() throws Exception { assertTrue(true); } }\n\nAll of these approaches suffer from the potential for Lost Tests (see Produc- tion Bugs on page 268) if the means of running a single test is not reversed properly when the situation requiring this testing strategy has passed. A Sin- gle Test Suite makes it possible to run the speciﬁ c test(s) without making any changes to the Testcase Class in question. This technique takes advantage of the fact that most implementations of xUnit require a one-argument constructor on our Testcase Class; this argument consists of the name of the method that this instance of the class will invoke using reﬂ ection. The one-argument construc- tor is called once for each Test Method on the class, and the resulting Testcase Object is added to the Test Suite Object. (This is an example of the Pluggable Behavior [SBPP] pattern.)\n\nWe can run a single test by implementing a Test Suite Factory class with a single method suite that creates an instance of the desired Testcase Class by call- ing the one-argument constructor with the name of the one Test Method to be run. By returning a Test Suite Object containing only this one Testcase Object from suite, we achieve the desired result (running a single test) without touching the target Testcase Class.\n\npublic class MyTest extends TestCase {\n\npublic static Test suite() { return new LostTests(\"testSeventeen\"); } }\n\nI like to keep a Single Test Suite class around all the time and just plug in what- ever test I want to run by changing the import statements and the suite method. Often, I maintain several Single Test Suite classes so I can ﬂ ip back and forth between different tests very quickly. I ﬁ nd this technique easier to do than drill- ing down in the Test Tree Explorer and picking the speciﬁ c test to run manually. (Your mileage may vary!)\n\nExample: Smoke Test Suite\n\nWe can take the idea of a Special-Purpose Suite and combine it with the imple- mentation technique of a Single Test Suite to create a Smoke Test [SCM] suite. This strategy involves picking a representative test or two from each of the major areas of the system and including those tests in a single Test Suite Object.\n\nwww.it-ebooks.info\n\n597\n\nNamed Test Suite\n\n598\n\nNamed Test Suite\n\nChapter 24 Test Organization Patterns\n\npublic class SmokeTestSuite extends TestCase { public static Test suite() { TestSuite mySuite = new TestSuite(\"Smoke Tests\");\n\nmySuite.addTest( new LostTests(\"testSeventeen\") ); mySuite.addTest( new SampleTests(\"testOne\") ); mySuite.addTest( new FlightManagementFacadeTest( \"testGetFlightsByOriginAirports_TwoOutboundFlights\")); // add additional tests here as needed... return mySuite; } }\n\nThis scheme won’t test our system thoroughly, but it is a quick way to ﬁ nd out whether some part of the core functionality is broken.\n\nwww.it-ebooks.info\n\nTest Utility Method\n\nTest Utility Method\n\nHow do we reduce Test Code Duplication?\n\nWe encapsulate the test logic we want to reuse behind a suitably named utility method.\n\nSetup Setup\n\nTest Utility Test Utility Methods Methods Creation Creation Method Method Finder Finder Method Method\n\nFixture Fixture\n\nExercise Exercise\n\nEncapsulation Encapsulation Method Method\n\nSUT SUT SUT SUT\n\nVerify Verify\n\nVerification Verification Method Method\n\nTeardown Teardown\n\nCustom Custom Assertion Assertion\n\nCleanup Cleanup Method Method\n\nAs we write tests, we will invariably ﬁ nd ourselves needing to repeat the same logic in many, many tests. Initially, we will just “clone and twiddle” as we write additional tests that need the same logic. Sooner or later, however, we will come to the realization that this Test Code Duplication (page 213) is starting to cause prob- lems. This point is a good time to think about introducing a Test Utility Method.\n\nHow It Works\n\nThe subroutine and the function were two of the earliest ways devised to reuse logic in several places within a program. A Test Utility Method is just the same principle applied to object-oriented test code. We move any logic that appears in more than one test into a Test Utility Method; we can then call this method from various tests or even several times from within a single test. Of course, we will want to pass in anything that varies from usage to usage as arguments to the Test Utility Method.\n\nwww.it-ebooks.info\n\n599\n\nTest Utility Method\n\n600\n\nTest Utility Method\n\nChapter 24 Test Organization Patterns\n\nWhen to Use It\n\nWe should use a Test Utility Method whenever test logic appears in several tests and we want to be able to reuse that logic. We might also use a Test Utility Method because we want to be very sure that the logic works as expected. The best way to achieve that kind of certainty is to write Self-Checking Tests (unit tests—see page 26) for the reusable test logic. Because the Test Methods (page 348) cannot easily be tested, it is best to do this by moving the logic out of the test methods and into Test Utility Methods, where it can be more easily tested.\n\nThe main drawback of using the Test Utility Method pattern is that it creates another API that the test automaters must build and understand. This extra ef- fort can be largely mitigated through the use of Intent-Revealing Names [SBPP] for the Test Utility Methods and through the use of refactoring as the means for deﬁ ning the Test Utility Methods.\n\nThere are as many different kinds of Test Utility Methods as there are kinds of logic in a Test Method. Next, we brieﬂ y summarize some of the most popu- lar kinds. Some of these variations are important enough to warrant their own pattern write-ups in the corresponding section of this book.\n\nVariation: Creation Method\n\nCreation Methods (page 415) are used to create ready-to-use objects as part of ﬁ xture setup. They hide the complexity of object creation and interdependencies from the test. Creation Method has enough variants to warrant addressing this pattern in its own section.\n\nVariation: Attachment Method\n\nAn Attachment Method (see Creation Method) is a special form of Creation Method used to amend already-created objects as part of ﬁ xture setup.\n\nVariation: Finder Method\n\nWe can encapsulate any logic required to retrieve objects from a Shared Fix- ture (page 317) within a function that returns the object(s). We then give this function an Intent-Revealing Name so that anyone reading the test can easily understand the ﬁ xture we are using in this test.\n\nWe should use a Finder Method whenever we need to ﬁ nd an existing Shared Fixture object that meets some criteria and we want to avoid a Fragile Fixture (see Fragile Test on page 239) and High Test Maintenance Cost (page 265). Finder Methods can be used in either a pure Shared Fixture strategy or a hybrid strategy such as Immutable Shared Fixture (see Shared Fixture). Finder\n\nwww.it-ebooks.info\n\nTest Utility Method\n\nMethods also help prevent Obscure Tests (page 186) by encapsulating the mechanism of how the required objects are found and exactly which objects to use, thereby enabling the reader to focus on understanding why a particular object is being used and how it relates to the expected outcome described in the assertions. This helps us move toward Tests as Documentation (see page 23).\n\nAlthough most Finder Methods return a single object reference, that object may be the root of a tree of objects (e.g., an invoice might refer to the customer and various addresses as well as containing a list of line items). In some circum- stances, we may choose to deﬁ ne a Finder Method that returns a collection (Array or Hash) of objects, but the use of this type of Finder Method is less common. Finder Methods may also update parameters to pass additional objects back to the test that called them, although this approach is not as intent-revealing as use of a function. I do not recommend initialization of instance variables as a way of passing back objects because it is obscure and keeps us from moving the Finder Method to a Test Helper (page 643) later.\n\nThe Finder Method can ﬁ nd objects in the Shared Fixture in several ways: by using direct references (instance variables or class variables initialized in the ﬁ xture setup logic), by looking the objects up using known keys, or by search- ing for the objects using speciﬁ c criteria. Using direct references or known keys has the advantage of always returning exactly the same object each time the test is run. The main drawback is that some other test may have modiﬁ ed the object such that it may no longer match the criteria implied by the Finder Method’s name. Searching by criteria can avoid this problem, though the resulting tests may take longer to run and might be less deterministic if they use different objects each time they are run. Either way, we must modify the code in fewer places whenever the Shared Fixture is modiﬁ ed (compared to when the objects are used directly within the Test Method).\n\nVariation: SUT Encapsulation Method\n\nAnother reason for using a Test Utility Method is to encapsulate unnecessary knowl- edge of the API of the SUT. What constitutes unnecessary? Any method we call on the SUT that is not the method being tested creates additional coupling between the test and the SUT. Creation Methods and Custom Assertions (page 474) are common enough examples of SUT Encapsulation Methods to warrant their own write-ups as separate patterns. This section focuses on the less common uses of SUT Encapsulation Methods. For example, if the method that we are exercising (or that we use for verifying the outcome) has a complicated signature, we increase the amount of work involved to write and maintain the test code and may\n\nwww.it-ebooks.info\n\n601\n\nTest Utility Method\n\nAlso known as: SUT API Encapsulation\n\n602\n\nTest Utility Method\n\nChapter 24 Test Organization Patterns\n\nmake it harder to understand the tests (Obscure Test). We can avoid this problem by wrapping these calls in SUT Encapsulation Methods that are intent-revealing and may have simpler signatures.\n\nVariation: Custom Assertion\n\nCustom Assertions are used to specify test-speciﬁ c equality in a way that is reusable across many tests. They hide the complexity of comparing the expected outcome with the actual outcome. Custom Assertions are typically free of side effects in that they do not interact with the SUT to retrieve the outcome; that task is left to the caller.\n\nVariation: Veriﬁ cation Method\n\nVeriﬁ cation Methods (see Custom Assertion) are used to verify that the expected outcome has occurred. They hide the complexity of verifying the outcome from the test. Unlike Custom Assertions, Veriﬁ cation Methods interact with the SUT.\n\nVariation: Parameterized Test\n\nThe most complete form of the Test Utility Method pattern is the Parameterized Test (page 607). It is, in essence, an almost complete test that can be reused in many circumstances. We simply provide the data that varies from test to test as a parameter and let the Parameterized Test execute all the stages of the Four- Phase Test (page 358) for us.\n\nVariation: Cleanup Method\n\nCleanup Methods1 are used during the ﬁ xture teardown phase of the test to clean up any resources that might still be allocated after the test ends. Refer to the pattern Automated Teardown (page 503) for a more detailed discussion and examples.\n\nImplementation Notes\n\nThe main objection some people have to using Test Utility Methods is that this pattern removes some of the logic from the test, which may make the test harder to read. One way we can avoid this problem when using Test Utility Methods is to give Intent-Revealing Names to the Test Utility Methods. In fact, well-chosen names can make the tests even easier to understand because they\n\n1 One could call this pattern a “Teardown Method,” but that name might be confused with the method used in Implicit Teardown (page 516).\n\nwww.it-ebooks.info\n\nTest Utility Method\n\nhelp prevent Obscure Tests by deﬁ ning a Higher Level Language (see page 41) for deﬁ ning tests. It is also helpful to keep the Test Utility Methods relatively small and self-contained. We can achieve this goal by passing all arguments to these methods explicitly as parameters (rather than using instance variables) and by returning any objects that the tests will require as explicit return values or updated parameters.\n\nTo ensure that the Test Utility Methods have Intent-Revealing Names, we should let the tests pull the Test Utility Methods into existence rather than just inventing Test Utility Methods that we think may be needed later. This “out- side-in” approach to writing code avoids “borrowing tomorrow’s trouble” and helps us ﬁ nd the minimal solution.\n\nWriting the reusable Test Utility Method is relatively straightforward. The trickier question is where we would put this method. If the Test Utility Method is needed only in Test Methods in a single Testcase Class (page 373), then we can put it onto that class. If we need the Test Utility Method in several classes, however, the solution becomes a bit more complicated. The key issue relates to type visibility. The client classes need to be able to see the Test Utility Method, and the Test Utility Method needs to be able to see all the types and classes on which it depends. When it doesn’t depend on many types/classes or when everything it depends on is visible from a single place, we can put the Test Utility Method into a common Testcase Superclass (page 638) that we deﬁ ne for our project or company. If it depends on types/classes that cannot be seen from a single place that all the clients can see, then we may need to put the Test Utility Method on a Test Helper in the appropriate test package or subsystem. In larger systems with many groups of domain objects, it is common practice to have one Test Helper for each group (package) of related domain objects.\n\nVariation: Test Utility Test\n\nOne major advantage of using Test Utility Methods is that otherwise Untestable Test Code (see Hard-to-Test Code on page 209) can now be tested with Self- Checking Tests. The exact nature of such tests varies based on the kind of Test Utility Method being tested but a good example is a Custom Assertion Test (see Custom Assertion).\n\nMotivating Example\n\nThe following example shows a test as many novice test automaters would ﬁ rst write it:\n\npublic void testAddItemQuantity_severalQuantity_v1(){ Address billingAddress = null;\n\nwww.it-ebooks.info\n\n603\n\nTest Utility Method\n\n604\n\nTest Utility Method\n\nChapter 24 Test Organization Patterns\n\nAddress shippingAddress = null; Customer customer = null; Product product = null; Invoice invoice = null; try { // Fixture Setup billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); customer = new Customer( 99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); product = new Product( 88, \"SomeWidget\", new BigDecimal(\"19.99\")); invoice = new Invoice( customer ); // Exercise SUT invoice.addItemQuantity( product, 5 ); // Verify Outcome List lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"inv\", invoice, actItem.getInv()); assertEquals(\"prod\", product, actItem.getProd()); assertEquals(\"quant\", 5, actItem.getQuantity()); assertEquals(\"discount\", new BigDecimal(\"30\"), actItem.getPercentDiscount()); assertEquals(\"unit price\", new BigDecimal(\"19.99\"), actItem.getUnitPrice()); assertEquals(\"extended\", new BigDecimal(\"69.96\"), actItem.getExtendedPrice()); } else { assertTrue(\"Invoice should have 1 item\", false); } } ﬁnally { // Teardown deleteObject(invoice); deleteObject(product); deleteObject(customer); deleteObject(billingAddress); deleteObject(shippingAddress); } }\n\nThis test is difﬁ cult to understand because it exhibits many code smells, includ- ing Obscure Test and Hard-Coded Test Data (see Obscure Test).\n\nwww.it-ebooks.info\n\nTest Utility Method\n\nRefactoring Notes\n\nWe often create Test Utility Methods by mining existing tests for reusable logic when we are writing new tests. We can use an Extract Method [Fowler] refac- toring to pull the code for the Test Utility Method out of one Test Method and put it onto the Testcase Class as a Test Utility Method. From there, we may choose to move the Test Utility Method to a superclass by using a Pull Up Method [Fowler] refactoring or to another class by using a Move Method [Fowler] refactoring.\n\nExample: Test Utility Method\n\nHere’s the refactored version of the earlier test. Note how much simpler this test is to understand than the original version. And this is just one example of what we can achieve by using Test Utility Methods!\n\npublic void testAddItemQuantity_severalQuantity_v13(){ ﬁnal int QUANTITY = 5; ﬁnal BigDecimal CUSTOMER_DISCOUNT = new BigDecimal(\"30\"); // Fixture Setup Customer customer = ﬁndActiveCustomerWithDiscount(CUSTOMER_DISCOUNT); Product product = ﬁndCurrentProductWith3DigitPrice( ); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, QUANTITY); // Verify Outcome ﬁnal BigDecimal BASE_PRICE = product.getUnitPrice(). multiply(new BigDecimal(QUANTITY)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUSTOMER_DISCOUNT.movePointLeft(2))); LineItem expected = createLineItem( QUANTITY, CUSTOMER_DISCOUNT, EXTENDED_PRICE, product, invoice); assertContainsExactlyOneLineItem(invoice, expected); }\n\nLet’s go through the changes step by step. First, we replaced the code to create the Customer and the Product with calls to Finder Methods that retrieve those objects from an Immutable Shared Fixture. We altered the code in this way because we don’t plan to change these objects.\n\nprotected Customer ﬁndActiveCustomerWithDiscount( BigDecimal percentDiscount) { return CustomerHome.ﬁndCustomerById( ACTIVE_CUSTOMER_WITH_30PC_DISCOUNT_ID); }\n\nwww.it-ebooks.info\n\n605\n\nTest Utility Method\n\n606\n\nTest Utility Method\n\nChapter 24 Test Organization Patterns\n\nNext, we introduced a Creation Method for the Invoice to which we plan to add the LineItem.\n\nprotected Invoice createInvoice(Customer customer) { Invoice newInvoice = new Invoice(customer); registerTestObject(newInvoice); return newInvoice; }\n\nList testObjects; protected void registerTestObject(Object testObject) { testObjects.add(testObject); }\n\nTo avoid the need for In-line Teardown (page 509), we registered each of the objects we created with our Automated Teardown mechanism, which we call from the tearDown method.\n\nprivate void deleteTestObjects() { Iterator i = testObjects.iterator(); while (i.hasNext()) { try { deleteObject(i.next()); } catch (RuntimeException e) { // Nothing to do; we just want to make sure // we continue on to the next object in the list. } } }\n\npublic void tearDown() { deleteTestObjects(); }\n\nFinally, we extracted a Custom Assertion to verify that the correct LineItem has been added to the Invoice.\n\nvoid assertContainsExactlyOneLineItem( Invoice invoice, LineItem expected) { List lineItems = invoice.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actItem = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\",expected, actItem); }\n\nwww.it-ebooks.info\n\nParameterized Test\n\nParameterized Test\n\nHow do we reduce Test Code Duplication when the same test logic appears in many tests?\n\nWe pass the information needed to do ﬁ xture setup and result veriﬁ cation to a utility method that implements the entire test life cycle.\n\nData Data\n\nTest Test Method1 Method1\n\nData Data\n\nFixture Fixture\n\nTest Test Method2 Method2\n\nSetup Setup\n\nExercise Exercise\n\nSUT SUT\n\nData Data\n\nVerify Verify\n\nTest Test Method n Method n\n\nTeardown Teardown\n\nTesting can be very repetitious not only because we must run the same test over and over again, but also because many of the tests differ only slightly from one another. For example, we might want to run essentially the same test with slightly different system inputs and verify that the actual output varies accord- ingly. Each of these tests would consist of the exact same steps. While having a large number of tests is an excellent way to ensure good code coverage, it is not so attractive from a test maintainability standpoint because any change made to the algorithm of one of the tests must be propagated to all similar tests.\n\nA Parameterized Test offers a way to reuse the same test logic in many Test\n\nMethods (page 348).\n\nwww.it-ebooks.info\n\n607\n\nParame- terized Test\n\n608\n\nParame- terized Test\n\nChapter 24 Test Organization Patterns\n\nHow It Works\n\nThe solution, of course, is to factor out the common logic into a utility method. When this logic includes all four parts of the entire Four-Phase Test (page 358) life cycle—that is, ﬁ xture setup, exercise SUT, result veriﬁ cation, and ﬁ xture teardown—we call the resulting utility method a Parameterized Test. This kind of test gives us the best coverage with the least code to maintain and makes it very easy to add more tests as they are needed.\n\nIf the right utility method is available to us, we can reduce a test that would otherwise require a series of complex steps to a single line of code. As we detect similarities between our tests, we can factor out the commonalities into a Test Utility Method (page 599) that takes only the information that differs from test to test as its arguments. The Test Methods pass in as parameters any information that the Parameterized Test requires to run and that varies from test to test.\n\nWhen to Use It\n\nWe can use a Parameterized Test whenever Test Code Duplication (page 213) results from several tests implementing the same test algorithm but with slightly different data. The data that differs becomes the arguments passed to the Param- eterized Test, and the logic is encapsulated by the utility method. A Parameterized Test also helps us avoid Obscure Tests (page 186); by reducing the number of times the same logic is repeated, it can make the Testcase Class (page 373) much more compact. A Parameterized Test is also a good steppingstone to a Data- Driven Test (page 288); the name of the Parameterized Test maps to the verb or “action word” of the Data-Driven Test, and the parameters are the attributes.\n\nIf our extracted utility method doesn’t do any ﬁ xture setup, it is called a Veriﬁ cation Method (see Custom Assertion on page 474). If it also doesn’t exercise the SUT, it is called a Custom Assertion.\n\nImplementation Notes\n\nWe need to ensure that the Parameterized Test has an Intent-Revealing Name [SBPP] so that readers of the test will understand what it is doing. This name should imply that the test encompasses the whole life cycle to avoid any con- fusion. One convention is to start or end the name in “test”; the presence of parameters conveys the fact that the test is parameterized. Most members of the xUnit family that implement Test Discovery (page 393) will create only Testcase Objects (page 382) for “no arg” methods that start with “test,” so this restriction shouldn’t prevent us from starting our Parameterized Test names with “test.” At least one member of the xUnit family—MbUnit—implements\n\nwww.it-ebooks.info\n\nParameterized Test\n\nParameterized Tests at the Test Automation Framework (page 298) level. Extensions are becoming available for other members of the xUnit family, with DDSteps for JUnit being one of the ﬁ rst to appear.\n\nTesting zealots would advocate writing a Self-Checking Test (see page 26) to verify the Parameterized Test. The beneﬁ ts of doing so are obvious—including increased conﬁ dence in our tests—and in most cases it isn’t that hard to do. It is a bit harder than writing unit tests for a Custom Assertion because of the inter- action with the SUT. We will likely need to replace the SUT2 with a Test Double so that we can observe how it is called and control what it returns.\n\nVariation: Tabular Test\n\nSeveral early reviewers of this book wrote to me about a variation of Param- eterized Test that they use regularly: the Tabular Test. The essence of this test is the same as that for a Parameterized Test, except that the entire table of values resides in a single Test Method. Unfortunately, this approach makes the test an Eager Test (see Assertion Roulette on page 224) because it veriﬁ es many test conditions. This issue isn’t a problem when all of the tests pass, but it does lead to a lack of Defect Localization (see page 22) when one of the “rows” fails.\n\nAnother potential problem is that “row tests” may depend on one another either on purpose or by accident because they are running on the same Testcase Object; see Incremental Tabular Test for an example of this behavior.\n\nDespite these potential issues, Tabular Tests can be a very effective way to test. At least one member of the xUnit family implements Tabular Tests at the framework level: MbUnit provides an attribute [RowTest] to indicate that a test is a Parameterized Test and another attribute [Row(x,y,...)] to specify the parameters to be passed to it. Perhaps it will be ported to other members of the xUnit family? (Hint, hint!)\n\nVariation: Incremental Tabular Test\n\nAn Incremental Tabular Test is a variant of the Tabular Test pattern in which we deliberately build on the ﬁ xture left over by the previous rows of the test. It is identical to a deliberate form of Interacting Tests (see Erratic Test on page 228)\n\n2 The terminology of SUT becomes very confusing in this case because we cannot replace the SUT with a Test Double if it truly is the SUT. Strictly speaking, we are replacing the object that would normally be the SUT with respect to this test. Because we are actually verifying the behavior of the Parameterized Test, whatever normally plays the role of SUT for this test now becomes a DOC. (My head is starting to hurt just describing this; fortunately, it really isn’t very complicated and will make a lot more sense when you actually try it out.)\n\nwww.it-ebooks.info\n\n609\n\nAlso known as: Row Test\n\nParame- terized Test\n\n610\n\nParame- terized Test\n\nChapter 24 Test Organization Patterns\n\ncalled Chained Tests (page 454), except that all the tests reside within the same Test Method. The steps within the Test Method act somewhat like the steps of a “DoFixture” in Fit but without individual reporting of failed steps.3\n\nVariation: Loop-Driven Test\n\nWhen we want to test the SUT with all the values in a particular list or range, we can call the Parameterized Test from within a loop that iterates over the values in the list or range. By nesting loops within loops, we can verify the behavior of the SUT with combinations of input values. The main requirement for doing this type of testing is that we must either enumerate the expected result for each input value (or combination) or use a Calculated Value (see Derived Value on page 718) without introducing Production Logic in Test (see Conditional Test Logic on page 200). A Loop-Driven Test suffers from many of the same issues associated with a Tabular Test, however, because we are hiding many tests inside a single Test Method (and, therefore, Testcase Object).\n\nMotivating Example\n\nThe following example includes some of the runit (Ruby Unit) tests from the Web site publishing infrastructure I built in Ruby while writing this book. All of the Simple Success Tests (see Test Method) for my cross-referencing tags went through the same sequence of steps: deﬁ ning the input XML, deﬁ ning the expected HTML, stubbing out the output ﬁ le, setting up the handler for the XML, extracting the resulting HTML, and comparing it with the expected HTML.\n\ndef test_extref # setup sourceXml = \"<extref id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\" mockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) # execute @handler.printBodyContents # verify assert_equals_html( expectedHtml, mockFile.output, \"extref: html output\") end\n\ndef testTestterm_normal sourceXml = \"<testterm id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\"\n\n3 This is because most members of the xUnit terminate the Test Method on the ﬁ rst failed assertion.\n\nwww.it-ebooks.info\n\nParameterized Test\n\nmockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) @handler.printBodyContents assert_equals_html( expectedHtml, mockFile.output, \"testterm: html output\") end\n\ndef testTestterm_plural sourceXml =\"<testterms id='abc'/>\" expectedHtml = \"<a href='abc.html'>abcs</a>\" mockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) @handler.printBodyContents assert_equals_html( expectedHtml, mockFile.output, \"testterms: html output\") end\n\nEven though we have already factored out much of the common logic into the setupHandler method, some Test Code Duplication remains. In my case, I had at least 20 tests that followed this same pattern (with lots more on the way), so I felt it was worthwhile to make these tests really easy to write.\n\nRefactoring Notes\n\nRefactoring to a Parameterized Test is a lot like refactoring to a Custom Asser- tion. The main difference is that we include the calls to the SUT made as part of the exercise SUT phase of the test within the code to which we apply the Extract Method [Fowler] refactoring. Because these tests are virtually identical once we have deﬁ ned our ﬁ xture and expected results, the rest can be extracted into the Parameterized Test.\n\nExample: Parameterized Test\n\nIn the following tests, we have reduced each test to two steps: initializing two variables and calling a utility method that does all the real work. This utility method is a Parameterized Test.\n\ndef test_extref sourceXml = \"<extref id='abc' />\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<extref>\") end\n\ndef test_testterm_normal sourceXml = \"<testterm id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<testterm>\")\n\nwww.it-ebooks.info\n\n611\n\nParame- terized Test\n\n612\n\nParame- terized Test\n\nChapter 24 Test Organization Patterns\n\nend\n\ndef test_testterm_plural sourceXml = \"<testterms id='abc'/>\" expectedHtml = \"<a href='abc.html'>abcs</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<plural>\") end\n\nThe succinctness of these tests is made possible by deﬁ ning the Parameterized Test as follows:\n\ndef generateAndVerifyHtml( sourceXml, expectedHtml, message, &block) mockFile = MockFile.new sourceXml.delete!(\"\\t\") @handler = setupHandler(sourceXml, mockFile ) block.call unless block == nil @handler.printBodyContents actual_html = mockFile.output assert_equal_html( expectedHtml, actual_html, message + \"html output\") actual_html end\n\nWhat distinguishes this Parameterized Test from a Veriﬁ cation Method is that it contains the ﬁ rst three phases of the Four-Phase Test (from setup to verify), whereas the Veriﬁ cation Method performs only the exercise SUT and verify re- sult phases. Note that our tests did not need the teardown phase because we are using Garbage-Collected Teardown (page 500).\n\nExample: Independent Tabular Test\n\nHere’s an example of the same tests coded as a single Independent Tabular Test:\n\ndef test_a_href_Generation row( \"extref\" ,\"abc\",\"abc.html\",\"abc\" ) row( \"testterm\" ,'abc',\"abc.html\",\"abc\" ) row( \"testterms\",'abc',\"abc.html\",\"abcs\") end\n\ndef row( tag, id, expected_href_id, expected_a_contents) sourceXml = \"<\" + tag + \" id='\" + id + \"'/>\" expectedHtml = \"<a href='\" + expected_href_id + \"'>\" + expected_a_contents + \"</a>\" msg = \"<\" + tag + \"> \" generateAndVerifyHtml( sourceXml, expectedHtml, msg) end\n\nwww.it-ebooks.info\n\nParameterized Test\n\nIsn’t this a nice, compact representation of the various test conditions? I simply did an In-line Temp [Fowler] refactoring on the local variables sourceXml and expectedHtml in the argument list of generateAndVerify and “munged” the various Test Methods together into one. Most of the work involved something we won’t have to do in real life: squeeze the table down to ﬁ t within the page-width limit for this book. That constraint forced me to abridge the text in each row and rebuild the HTML and the expected XML within the row method. I chose the name row to better align this example with the MbUnit example provided later in this section but I could have called it something else like test_element.\n\nUnfortunately, from the Test Runner’s (page 377) perspective, this is a single test, unlike the earlier examples. Because the tests all reside within the same Test Method, a failure in any row other than the last will cause a loss of infor- mation. In this example, we need not worry about Interacting Tests because generateAndVerify builds a new test ﬁ xture each time it is called. In the real world, however, we have to be aware of that possibility.\n\nExample: Incremental Tabular Test\n\nBecause a Tabular Test is deﬁ ned in a single Test Method, it will run on a single Testcase Object. This opens up the possibility of building up series of actions. Here’s an example provided by Clint Shank on his blog:\n\npublic class TabularTest extends TestCase { private Order order = new Order(); private static ﬁnal double tolerance = 0.001;\n\npublic void testGetTotal() { assertEquals(\"initial\", 0.00, order.getTotal(), tolerance); testAddItemAndGetTotal(\"ﬁrst\", 1, 3.00, 3.00); testAddItemAndGetTotal(\"second\",3, 5.00, 18.00); // etc. }\n\nprivate void testAddItemAndGetTotal( String msg, int lineItemQuantity, double lineItemPrice, double expectedTotal) { // setup LineItem item = new LineItem( lineItemQuantity, lineItemPrice); // exercise SUT order.addItem(item); // verify total assertEquals(msg,expectedTotal,order.getTotal(),tolerance); } }\n\nwww.it-ebooks.info\n\n613\n\nParame- terized Test\n\n614\n\nParame- terized Test\n\nChapter 24 Test Organization Patterns\n\nNote how each row of the Incremental Tabular Test builds on what was already done by the previous row.\n\nExample: Tabular Test with Framework Support (MbUnit)\n\nHere’s an example from the MbUnit documentation that shows how to use the [RowTest] attribute to indicate that a test is a Parameterized Test and another attribute [Row(x,y,...)] to specify the parameters to be passed to it.\n\n[RowTest()] [Row(1,2,3)] [Row(2,3,5)] [Row(3,4,8)] [Row(4,5,9)] public void tAdd(Int32 x, Int32 y, Int32 expectedSum) { Int32 Sum; Sum = this.Subject.Add(x,y); Assert.AreEqual(expectedSum, Sum); }\n\nExcept for the syntactic sugar of the [Row(x,y,...)] attributes, this code sure looks similar to the previous example. It doesn’t suffer from the loss of Defect Local- ization, however, because each row is considered a separate test. It would be a simple matter to convert the previous example to this format using the “ﬁ nd and replace” feature in a text editor.\n\nExample: Loop-Driven Test (Enumerated Values)\n\nThe following test uses a loop to exercise the SUT with various sets of input values:\n\npublic void testMultipleValueSets() { // Set up ﬁxture Calculator sut = new Calculator(); TestValues[] testValues = { new TestValues(1,2,3), new TestValues(2,3,5), new TestValues(3,4,8), // special case! new TestValues(4,5,9) };\n\nfor (int i = 0; i < testValues.length; i++) { TestValues values = testValues[i]; // Exercise SUT int actual = sut.calculate( values.a, values.b); // Verify result\n\nwww.it-ebooks.info\n\nParameterized Test\n\nassertEquals(message(i), values.expectedSum, actual); } }\n\nprivate String message(int i) { return \"Row \"+ String.valueOf(i); }\n\nIn this case we enumerated the expected value for each set of test inputs. This strategy avoids Production Logic in Test.\n\nExample: Loop-Driven Test (Calculated Values)\n\nThis next example is a bit more complex:\n\npublic void testCombinationsOfInputValues() { // Set up ﬁxture Calculator sut = new Calculator(); int expected; // TBD inside loops\n\nfor (int i = 0; i < 10; i++) { for (int j = 0; j < 10; j++) { // Exercise SUT int actual = sut.calculate( i, j );\n\n// Verify result if (i==3 & j==4) // Special case expected = 8; else expected = i+j;\n\nassertEquals(message(i,j), expected, actual); } } }\n\nprivate String message(int i, int j) { return \"Cell( \" + String.valueOf(i)+ \",\" + String.valueOf(j) + \")\"; }\n\nUnfortunately, it suffers from Production Logic in Test because of the need to deal with the special case.\n\nFurther Reading\n\nSee the documentation for MbUnit for more information on the [RowTest] and [Row()] attributes. Likewise, see http://www.ddsteps.org for a description of the DDSteps extension for JUnit; while its name suggests a tool that supports\n\nwww.it-ebooks.info\n\n615\n\nParame- terized Test\n\n616\n\nParame- terized Test\n\nChapter 24 Test Organization Patterns\n\nData-Driven Testing, the examples given are Parameterized Tests. More argu- ments for Tabular Test can be found on Clint Shank’s blog at http://clintshank. javadevelopersjournal.com/tabulartests.htm.\n\nwww.it-ebooks.info\n\nTestcase Class per Class\n\nTestcase Class per Class\n\nHow do we organize our Test Methods onto Testcase Classes?\n\nWe put all the Test Methods for one SUT class onto a single Testcase Class.\n\nTestcaseClass TestcaseClass\n\nCreation Creation\n\ntestMethod_A_1 testMethod_A_1\n\nFixture A Fixture A\n\ntestMethod_A_2 testMethod_A_2\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nExercise Exercise\n\nfeature_2 feature_2\n\ntestMethod_B_1 testMethod_B_1\n\ntestMethod_B_2 testMethod_B_2\n\nCreation Creation\n\nFixture B Fixture B\n\nAs the number of Test Methods (page 348) grows, we need to decide on which Testcase Class (page 373) to put each Test Method. Our choice of a test organi- zation strategy affects how easily we can get a “big picture” view of our tests. It also affects our choice of a ﬁ xture setup strategy.\n\nUsing a Testcase Class per Class is a simple way to start off organizing our\n\ntests.\n\nHow It Works\n\nWe create a separate Testcase Class for each class we wish to test. Each Testcase Class acts as a home to all the Test Methods that are used to verify the behavior of the SUT class.\n\nwww.it-ebooks.info\n\n617\n\nTestcase Class per Class\n\n618\n\nTestcase Class per Class\n\nChapter 24 Test Organization Patterns\n\nWhen to Use It\n\nUsing a Testcase Class per Class is a good starting point when we don’t have very many Test Methods or we are just starting to write tests for our SUT. As the number of tests increases and we gain a better understanding of our test ﬁ xture require- ments, we may want to split the Testcase Class into multiple classes. This choice will result in either Testcase Class per Fixture (page 631; if we have a small number of frequently used starting points for our tests) or Testcase Class per Feature (page 624; if we have several distinct features to test). As Kent Beck would say, “Let the code tell you what to do!”\n\nImplementation Notes\n\nChoosing a name for the Testcase Class is pretty simple: Just use the SUT class- name, possibly preﬁ xed or sufﬁ xed with “Test.” The method names should try to capture at least the starting state (ﬁ xture) and the feature (method) being exercised, along with a summary of the parameters to be passed to the SUT. Given these requirements, we likely won’t have “room” for the expected outcome in the method name, so the test reader must look at the Test Method body to determine the expected outcome.\n\nThe creation of the ﬁ xture is the primary implementation concern when using a Testcase Class per Class. Conﬂ icting ﬁ xture requirements will inevitably arise among the various Test Methods, which makes use of Implicit Setup (page 424) difﬁ cult and forces us to use either In-line Setup (page 408) or Delegated Set- up (page 411). A second consideration is how to make the nature of the ﬁ x- ture visible within each test method so as to avoid Obscure Tests (page 186). Delegated Setup (using Creation Methods; see page 415) tends to lead to more readable tests unless the In-line Setup is very simple.\n\nExample: Testcase Class per Class\n\nHere’s an example of using the Testcase Class per Class pattern to structure the Test Methods for a Flight class that has three states (Unscheduled, Scheduled, and AwaitingApproval) and four methods (schedule, requestApproval, deSchedule, and approve. Because the class is stateful, we need at least one test for each state for each method.\n\npublic class FlightStateTest extends TestCase {\n\npublic void testRequestApproval_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper.getAnonymousFlightInScheduledState();\n\nwww.it-ebooks.info\n\nTestcase Class per Class\n\ntry { ﬂight.requestApproval(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testRequestApproval_FromUnsheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.requestApproval(); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); }\n\npublic void testRequestApproval_FromAwaitingApprovalState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.requestApproval(); fail(\"not allowed in awaitingApproval state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testSchedule_FromUnscheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.schedule(); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); }\n\npublic void testSchedule_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.schedule(); fail(\"not allowed in scheduled state\");\n\nwww.it-ebooks.info\n\n619\n\nTestcase Class per Class\n\n620\n\nTestcase Class per Class\n\nChapter 24 Test Organization Patterns\n\n} catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testSchedule_FromAwaitingApprovalState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.schedule(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testDeschedule_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); ﬂight.deschedule(); assertTrue(\"isUnscheduled()\", ﬂight.isUnscheduled()); }\n\npublic void testDeschedule_FromUnscheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); try { ﬂight.deschedule(); fail(\"not allowed in unscheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"deschedule\", e.getRequest()); assertTrue(\"isUnscheduled()\", ﬂight.isUnscheduled()); } }\n\npublic void testDeschedule_FromAwaitingApprovalState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try {\n\nwww.it-ebooks.info\n\nTestcase Class per Class\n\nﬂight.deschedule(); fail(\"not allowed in awaitingApproval state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"deschedule\", e.getRequest()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testApprove_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.approve(\"Fred\"); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"approve\", e.getRequest()); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testApprove_FromUnsheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); try { ﬂight.approve(\"Fred\"); fail(\"not allowed in unscheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"approve\", e.getRequest()); assertTrue( \"isUnscheduled()\", ﬂight.isUnscheduled()); } }\n\npublic void testApprove_FromAwaitingApprovalState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); ﬂight.approve(\"Fred\"); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); }\n\npublic void testApprove_NullArgument() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState();\n\nwww.it-ebooks.info\n\n621\n\nTestcase Class per Class\n\n622\n\nTestcase Class per Class\n\nChapter 24 Test Organization Patterns\n\ntry { ﬂight.approve(null); fail(\"Failed to catch no approver\"); } catch (InvalidArgumentException e) { assertEquals(\"e.getArgumentName()\", \"approverName\", e.getArgumentName()); assertNull( \"e.getArgumentValue()\", e.getArgumentValue()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testApprove_InvalidApprover() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.approve(\"John\"); fail(\"Failed to validate approver\"); } catch (InvalidArgumentException e) { assertEquals(\"e.getArgumentName()\", \"approverName\", e.getArgumentName()); assertEquals(\"e.getArgumentValue()\", \"John\", e.getArgumentValue()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } } }\n\nThis example uses Delegated Setup of a Fresh Fixture (page 311) to achieve a more declarative style of ﬁ xture construction. Even so, this class is getting rather large and keeping track of the Test Methods is becoming a bit of a chore. Even the “big picture” provided by our IDE is not that illuminating; we can see the test conditions being exercised but cannot tell what the expected outcome should be without looking at the method bodies (Figure 24.1).\n\nwww.it-ebooks.info\n\nTestcase Class per Class\n\nFigure 24.1 Testcase Class per Class example as seen in the Package Explorer of the Eclipse IDE. Note how both the starting state and event are included in the Test Method names.\n\nwww.it-ebooks.info\n\n623\n\nTestcase Class per Class\n\n624\n\nTestcase Class per Feature\n\nChapter 24 Test Organization Patterns\n\nTestcase Class per Feature\n\nHow do we organize our Test Methods onto Testcase Classes?\n\nWe group the Test Methods onto Testcase Classes based on which testable feature of the SUT they exercise.\n\nFeature1TestcaseClass Feature1TestcaseClass\n\nCreation Creation\n\ntestMethod_A testMethod_A\n\nFixture A Fixture A\n\ntestMethod_B testMethod_B\n\nOutputs) Outputs)\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nFeature2TestcaseClass Feature2TestcaseClass\n\nExercise Exercise\n\nfeature_2 feature_2\n\ntestMethod_A testMethod_A\n\ntestMethod_B testMethod_B\n\nCreation Creation\n\nFixture B Fixture B\n\nAs the number of Test Methods (page 348) grows, we need to decide on which Testcase Class (page 373) to put each Test Method. Our choice of a test organi- zation strategy affects how easily we can get a “big picture” view of our tests. It also affects our choice of a ﬁ xture setup strategy.\n\nUsing a Testcase Class per Feature gives us a systematic way to break up a large Testcase Class into several smaller ones without having to change our Test Methods.\n\nHow It Works\n\nWe group our Test Methods onto Testcase Classes based on which feature of the Testcase Class they verify. This organizational scheme allows us to have smaller Testcase Classes and to see at a glance all the test conditions for a particular feature of the class.\n\nwww.it-ebooks.info\n\nTestcase Class per Feature\n\nWhen to Use It\n\nWe can use a Testcase Class per Feature when we have a signiﬁ cant number of Test Methods and we want to make the speciﬁ cation of each feature of the SUT more obvious. Unfortunately, Testcase Class per Feature does not make each individual Test Method any simpler or easier to understand; only Testcase Class per Fixture (page 631) helps on that front. Likewise, it doesn’t make much sense to use Testcase Class per Feature when each feature of the SUT requires only one or two tests; in that case, we can stick with a single Testcase Class per Class (page 617).\n\nNote that having a large number of features on a class is a “smell” indicating the possibility that the class might have too many responsibilities. We typically use Testcase Class per Feature when we are writing customer tests for methods on a service Facade [GOF].\n\nVariation: Testcase Class per Method\n\nWhen a class has methods that take a lot of different parameters, we may have many tests for the one method. We can group all of these Test Methods onto a single Testcase Class per Method and put the rest of the Test Methods onto one or more other Testcase Classes.\n\nVariation: Testcase Class per Feature\n\nAlthough a “feature” of a class is typically a single operation or function, it may also be a set of related methods that operate on the same instance variable of the object. For example, the set and get methods of a Java Bean would be con- sidered a single (and trivial) “feature” of the class that contains those methods. Similarly, a Data Access Object [CJ2EEP] would provide methods to both read and write objects. It is difﬁ cult to test these methods in isolation, so we can treat the reading and writing of one kind of object as a feature.\n\nVariation: Testcase Class per User Story\n\nIf we are doing highly incremental development (such as we might do with eXtreme Programming), it can be useful to put the new Test Methods for each story into a different Testcase Class. This practice prevents commit-related conﬂ icts when dif- ferent people are working on different stories that affect the same SUT class. The Testcase Class per User Story pattern may or may not end up being the same as Testcase Class per Feature or Testcase Class per Method, depending on how we partition our user stories.\n\nwww.it-ebooks.info\n\n625\n\nTestcase Class per Feature\n\n626\n\nTestcase Class per Feature\n\nChapter 24 Test Organization Patterns\n\nImplementation Notes\n\nBecause each Testcase Class represents the requirements for a single feature of the SUT, it makes sense to name the Testcase Class based on the feature it veri- ﬁ es. Similarly, we can name each test method based on which test condition of the SUT is being veriﬁ ed. This nomenclature allows us to see all the test condi- tions at a glance by merely looking at the names of the Test Methods of the Testcase Class.\n\nOne consequence of using Testcase Class per Feature is that we end up with a larger number of Testcase Classes for a single production class. Because we still want to run all the tests for this class, we should put these Testcase Classes into a single nested folder, package, or namespace. We can use an AllTests Suite (see Named Test Suite on page 592) to aggregate all of the Testcase Classes into a single test suite if we are using Test Enumeration (page 399).\n\nMotivating Example\n\nThis example uses the Testcase Class per Class pattern to structure the Test Methods for a Flight class that has three states (Unscheduled, Scheduled, and Await- ingApproval) and four methods (schedule, requestApproval, deSchedule, and approve. Because the class is stateful, we need at least one test for each state for each method. (In the interest of saving trees, I’ve omitted many of the method bodies; please refer to Testcase Class per Class for the full listing.)\n\npublic class FlightStateTest extends TestCase {\n\npublic void testRequestApproval_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.requestApproval(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testRequestApproval_FromUnsheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState();\n\nwww.it-ebooks.info\n\nTestcase Class per Feature\n\nﬂight.requestApproval(); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); }\n\npublic void testRequestApproval_FromAwaitingApprovalState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.requestApproval(); fail(\"not allowed in awaitingApproval state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testSchedule_FromUnscheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.schedule(); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); }\n\npublic void testSchedule_FromScheduledState() throws Exception { // I've omitted the bodies of the rest of the tests to // save a few trees } }\n\nThis example uses Delegated Setup (page 411) of a Fresh Fixture (page 311) to achieve a more declarative style of ﬁ xture construction. Even so, this class is getting rather large and keeping track of the Test Methods is becoming a bit of a chore. Because the Test Methods on this Testcase Class require four distinct methods, it is a good example of a test that can be improved through refactoring to Testcase Class per Feature.\n\nRefactoring Notes\n\nWe can reduce the size of each Testcase Class and make the names of the Test Methods more meaningful by converting them to follow the Testcase Class per Feature pattern. First, we determine how many classes we want to create and\n\nwww.it-ebooks.info\n\n627\n\nTestcase Class per Feature\n\n628\n\nTestcase Class per Feature\n\nChapter 24 Test Organization Patterns\n\nwhich Test Methods should go into each one. If some Testcase Classes will end up being smaller than others, it makes the job easier if we start by building the smaller classes. Next, we do an Extract Class [Fowler] refactoring to create one of the new Testcase Classes and give it a name that describes the feature it exercises. Then, we do a Move Method [Fowler] refactoring (or a simple “cut and paste”) on each Test Method that belongs in this new class along with any instance variables it uses.\n\nWe repeat this process until we are down to just one feature in the original Testcase Class; we then rename that class based on the feature it exercises. At this point, each of the Testcase Classes should compile and run—but we still aren’t completely done. To get the full beneﬁ t of the Testcase Class per Feature pattern, we have one ﬁ nal step to carry out. We should do a Rename Method [Fowler] refactoring on each of the Test Methods to better reﬂ ect what the Test Method is verifying. As part of this refactoring, we can remove any mention of the feature being exercised from each Test Method name—that informa- tion should be captured in the name of the Testcase Class. This leaves us with “room” to include both the starting state (the ﬁ xture) and the expected result in the method name. If we have multiple tests for each feature with different method arguments, we’ll need to ﬁ nd a way to include those aspects of the test conditions in the method name, too.\n\nAnother way to perform this refactoring is simply to make copies of the orig- inal Testcase Class and rename them as described above. Then we simply delete the Test Methods that aren’t relevant for each class. We do need to be careful that we don’t delete all copies of a Test Method; a less critical oversight is to leave a copy of the same method in several Testcase Classes. We can avoid both of the potential errors by making one copy of the original Testcase Class for each of the features and rename them as described above. Then we simply de- lete the Test Methods that aren’t relevant for each class. When we are done, we simply delete the original Testcase Class.\n\nExample: Testcase Class per Feature\n\nIn this example, we have converted the previously mentioned set of tests to use Testcase Class per Feature.\n\npublic class TestScheduleFlight extends TestCase {\n\npublic void testUnscheduled_shouldEndUpInScheduled() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.schedule(); assertTrue( \"isScheduled()\", ﬂight.isScheduled());\n\nwww.it-ebooks.info\n\nTestcase Class per Feature\n\n}\n\npublic void testScheduledState_shouldThrowInvalidRequestEx() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.schedule(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testAwaitingApproval_shouldThrowInvalidRequestEx() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.schedule(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } } }\n\nExcept for their names, the Test Methods really haven’t changed here. Because the names include the pre-conditions (ﬁ xture), the feature being exercised, and the expected outcome, they help us see the big picture when we look at the list of tests in our IDE’s “outline view” (see Figure 24.2). This satisﬁ es our need for Tests as Documentation (see page 23).\n\nwww.it-ebooks.info\n\n629\n\nTestcase Class per Feature\n\n630\n\nTestcase Class per Feature\n\nChapter 24 Test Organization Patterns\n\nFigure 24.2 Testcase Class per Feature example as seen in the Package Explorer of the Eclipse IDE. Note how we do not need to include the starting state in the Test Method names, leaving room for the name of the method being called and the expected end state.\n\nwww.it-ebooks.info\n\nTestcase Class per Fixture\n\nTestcase Class per Fixture\n\nHow do we organize our Test Methods onto Testcase Classes?\n\nWe organize Test Methods into Testcase Classes based on commonality of the test ﬁ xture.\n\nFixtureATestcaseClass FixtureATestcaseClass FixtureATestcaseClass FixtureATestcaseClass\n\nsetUp setUp\n\ntestMethod_1 testMethod_1\n\nCreation Creation\n\nFixture A Fixture A\n\ntestMethod_2 testMethod_2\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nFixtureBTestcaseClass FixtureBTestcaseClass\n\nExercise Exercise\n\nfeature_2 feature_2\n\nsetUp setUp\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nCreation Creation\n\nFixture B Fixture B\n\nAs the number of Test Methods (page 348) grows, we need to decide on which Testcase Class (page 373) to put each Test Method. Our choice of a test organi- zation strategy affects how easily we can get a “big picture” view of our tests. It also affects our choice of a ﬁ xture setup strategy.\n\nUsing a Testcase Class per Fixture lets us take advantage of the Implicit Setup (page 424) mechanism provided by the Test Automation Framework (page 298).\n\nHow It Works\n\nWe group our Test Methods onto Testcase Classes based on which test ﬁ xture they require as a starting point. This organization allows us to use Implicit Setup to move the entire ﬁ xture setup logic into the setUp method, thereby allow- ing each test method to focus on the exercise SUT and verify outcome phases of the Four-Phase Test (page 358).\n\nwww.it-ebooks.info\n\n631\n\nTestcase Class per Fixture\n\n632\n\nTestcase Class per Fixture\n\nChapter 24 Test Organization Patterns\n\nWhen to Use It\n\nWe can use the Testcase Class per Fixture pattern whenever we have a group of Test Methods that need an identical ﬁ xture and we want to make each test method as simple as possible. If each test needs a unique ﬁ xture, using Testcase Class per Fixture doesn’t make a lot of sense because we will end up with a large number of single-test classes; in such a case, it would be better to use either Testcase Class per Feature (page 624) or simply Testcase Class per Class (page 617).\n\nOne beneﬁ t of Testcase Class per Fixture is that we can easily see whether we are testing all the operations from each starting state. We should end up with the same lineup of test methods on each Testcase Class, which is very easy to see in an “outline view” or “method browser” of an IDE. This attribute makes the Testcase Class per Fixture pattern particularly useful for discovering Missing Unit Tests (see Production Bugs on page 268) long before we go into production.\n\nTestcase Class per Fixture is a key part of the behavior-driven development style of testing/speciﬁ cation. It leads to very short test methods, often featuring only a single assertion per test method. When combined with a test method naming con- vention that summarizes the expected outcome of the test, this pattern leads to Tests as Documentation (see page 23).\n\nImplementation Notes\n\nBecause we set up the ﬁ xture in a method called by the Test Automation Frame- work (the setUp method), we must use an instance variable to hold a reference to the ﬁ xture we created. In such a case, we must be careful not to use a class vari- able, as it can lead to a Shared Fixture (page 317) and the Erratic Tests (page 228) that often accompany this kind of ﬁ xture. [The sidebar “There’s Always an Exception” on page 384 lists xUnit members that don’t guarantee Independent Tests (see page 42) when we use instance variables.]\n\nBecause each Testcase Class represents a single test ﬁ xture conﬁ guration, it makes sense to name the Testcase Class based on the ﬁ xture it creates. Similarly, we can name each test method based on the method of the SUT being exercised, the characteristics of any arguments passed to the SUT method, and the expected outcome of that method call.\n\nOne side effect of using Testcase Class per Fixture is that we end up with a larger number of Testcase Classes. We may want to ﬁ nd a way to group the various Testcase Classes that verify a single SUT class. One way to do so is to create a nested folder, package, or namespace to hold just these test classes. If we are using Test Enumeration (page 399), we’ll also want to create an AllTests\n\nwww.it-ebooks.info\n\nTestcase Class per Fixture\n\nSuite (see Named Test Suite on page 592) to aggregate all the Testcase Class per Fixtures into a single suite.\n\nAnother side effect is that the tests for a single feature of the SUT are spread across several Testcase Classes. This distribution may be a good thing if the features are closely related to one another because it highlights their interdepen- dency. Conversely, if the features are somewhat unrelated, their dispersal may be disconcerting. In such a case, we can either refactor to use Testcase Class per Feature or apply an Extract Class [Fowler] refactoring on the SUT if we decide that this symptom indicates that the class has too many responsibilities.\n\nMotivating Example\n\nThe following example uses Testcase Class per Class to structure the Test Methods for a Flight class that has three states (Unscheduled, Scheduled, and AwaitingApproval) and four methods (schedule, requestApproval, deSchedule, and approve). Because the class is stateful, we need at least one test for each state for each method. (In the interest of saving trees, I’ve omitted many of the method bodies; please refer to Testcase Class per Class for the full listing.)\n\npublic class FlightStateTest extends TestCase {\n\npublic void testRequestApproval_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.requestApproval(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testRequestApproval_FromUnsheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.requestApproval(); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); }\n\npublic void testRequestApproval_FromAwaitingApprovalState() throws Exception {\n\nwww.it-ebooks.info\n\n633\n\nTestcase Class per Fixture\n\n634\n\nTestcase Class per Fixture\n\nChapter 24 Test Organization Patterns\n\nFlight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.requestApproval(); fail(\"not allowed in awaitingApproval state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testSchedule_FromUnscheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.schedule(); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); }\n\npublic void testSchedule_FromScheduledState() throws Exception { // I've omitted the bodies of the rest of the tests to // save a few trees } }\n\nThis example uses Delegated Setup (page 411) of a Fresh Fixture (page 311) to achieve a more declarative style of ﬁ xture construction. Even so, this class is getting rather large and keeping track of the Test Methods is becoming a bit of a chore. Because the Test Methods on this Testcase Class require three distinct test ﬁ xtures (one for each state the ﬂ ight can be in), it is a good example of a test that can be improved through refactoring to Testcase Class per Fixture.\n\nRefactoring Notes\n\nWe can remove Test Code Duplication (page 213) in the ﬁ xture setup and make the Test Methods easier to understand by converting them to use the Testcase Class per Fixture pattern. First, we determine how many classes we want to cre- ate and which Test Methods should go into each one. If some Testcase Classes will end up being smaller than others, it will reduce our work if we start with the smaller ones. Next, we do an Extract Class refactoring to create one of the Testcase Classes and give it a name that describes the ﬁ xture it requires. Then, we do a Move Method [Fowler] refactoring on each Test Method that belongs in this new class, along with any instance variables it uses.\n\nwww.it-ebooks.info\n\nTestcase Class per Fixture\n\nWe repeat this process until we are down to just one ﬁ xture in the original class; we can then rename that class based on the ﬁ xture it creates. At this point, each of the Testcase Classes should compile and run—but we still aren’t com- pletely done. To get the full beneﬁ t of the Testcase Class per Fixture pattern, we have two more steps to complete. First, we should factor out any common ﬁ xture setup logic from each of the Test Methods into the setUp method, result- ing in an Implicit Setup. This type of setup is made possible because the Test Methods on each class have the same ﬁ xture requirements. Second, we should do a Rename Method [Fowler] refactoring on each of the Test Methods to bet- ter reﬂ ect what the Test Method is verifying. We can remove any mention of the starting state from each Test Method name, because that information should be captured in the name of the Testcase Class. This refactoring leaves us with “room” to include both the action (the method being called plus the nature of the arguments) and the expected result in the method name.\n\nAs described in Testcase Class per Fixture, we can also refactor to this pat- tern by making one copy of the Testcase Class (suitably named) for each ﬁ xture, deleting the unnecessary Test Methods from each one, and ﬁ nally deleting the old Testcase Class.\n\nExample: Testcase Class per Fixture\n\nIn this example, the earlier set of tests has been converted to use the Testcase Class per Fixture pattern. (In the interest of saving trees, I’ve shown only one of the resulting Testcase Classes; the others look pretty similar.)\n\npublic class TestScheduledFlight extends TestCase {\n\nFlight scheduledFlight;\n\nprotected void setUp() throws Exception { super.setUp(); scheduledFlight = createScheduledFlight(); }\n\nFlight createScheduledFlight() throws InvalidRequestException{ Flight newFlight = new Flight(); newFlight.schedule(); return newFlight; }\n\npublic void testDeschedule_shouldEndUpInUnscheduleState() throws Exception { scheduledFlight.deschedule(); assertTrue(\"isUnsched\", scheduledFlight.isUnscheduled()); }\n\nwww.it-ebooks.info\n\n635\n\nTestcase Class per Fixture\n\n636\n\nTestcase Class per Fixture\n\nChapter 24 Test Organization Patterns\n\npublic void testRequestApproval_shouldThrowInvalidRequestEx(){ try { scheduledFlight.requestApproval(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isScheduled()\", scheduledFlight.isScheduled()); } }\n\npublic void testSchedule_shouldThrowInvalidRequestEx() { try { scheduledFlight.schedule(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue(\"isScheduled()\", scheduledFlight.isScheduled()); } }\n\npublic void testApprove_shouldThrowInvalidRequestEx() throws Exception { try { scheduledFlight.approve(\"Fred\"); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"approve\", e.getRequest()); assertTrue(\"isScheduled()\", scheduledFlight.isScheduled()); } } }\n\nNote how much simpler each Test Method has become! Because we have used Intent-Revealing Names [SBPP] for each of the Test Methods, we can use the Tests as Documentation. By looking at the list of methods in the “outline view” of our IDE, we can see the starting state (ﬁ xture), the action (method being called), and the expected outcome (what it returns or the post-test state)—all without even opening up the method body (Figure 24.3).\n\nwww.it-ebooks.info\n\nTestcase Class per Fixture\n\nFigure 24.3 The tests for our Testcase Class per Fixture as seen in the Package Explorer of the Eclipse IDE. Note how we do not need to include the name of the method being called in the Test Method names, leaving room for the starting state and the expected end state.\n\nThis “big picture” view of our tests makes it clear that we are only testing the approve method arguments when the Flight is in the awaitingApproval state. We can now decide whether that limitation is a shortcoming of the tests or part of the speciﬁ cation (i.e., the result of calling approve is “undeﬁ ned” for some states of the Flight).\n\nwww.it-ebooks.info\n\n637\n\nTestcase Class per Fixture\n\n638\n\nAlso known as: Abstract Testcase, Abstract Test Fixture, Testcase Baseclass\n\nTestcase Superclass\n\nChapter 24 Test Organization Patterns\n\nTestcase Superclass\n\nWhere do we put our test code when it is in reusable Test Utility Methods?\n\nWe inherit reusable test-speciﬁ c logic from an abstract Testcase Super class.\n\nTestcase Testcase Superclass Superclass\n\nTest Utility Test Utility Method_1 Method_1\n\nTest Utility Test Utility Method_2 Method_2\n\nFixture Fixture\n\nSUT SUT\n\ntestMethod_1 testMethod_1\n\ntestMethod_n testMethod_n\n\nTestcase Testcase Class Class\n\nAs we write tests, we will invariably ﬁ nd ourselves needing to repeat the same logic in many, many tests. Initially, we may just “clone and twiddle” as we write addi- tional tests that need the same logic. Ultimately, we may introduce Test Utility Meth- ods (page 599) to hold this logic—but where do we put the Test Utility Methods? A Testcase Superclass is one option as a home for our Test Utility Methods.\n\nHow It Works\n\nWe deﬁ ne an abstract superclass to hold the reusable Test Utility Method that should be available to several Testcase Classes (page 373). We make the methods that will be reused visible to subclasses (e.g., protected in Java). We then use this abstract class as the superclass (base class) for any tests that wish to reuse the logic. The logic can be accessed simply by calling the method as though it were deﬁ ned on the Testcase Class itself.\n\nwww.it-ebooks.info\n\nTestcase Superclass\n\nWhen to Use It\n\nWe can use a Testcase Superclass if we wish to reuse Test Utility Methods between several Testcase Classes and can ﬁ nd or deﬁ ne a Testcase Superclass from which we can subclass all tests that require the logic.\n\nThis pattern assumes that our programming language supports inheritance, we are not already using inheritance for some other conﬂ icting purpose, and the Test Utility Method doesn’t need access to speciﬁ c types that are not visible from the Testcase Superclass.\n\nThe decision between a Testcase Superclass and a Test Helper (page 643) comes down to type visibility. The client classes need to see the Test Utility Method, and the Test Utility Method needs to see the types and classes it depends on. When it doesn’t depend on many types/classes or when everything it depends on is visible from a single place, we can put the Test Utility Method into a common Testcase Superclass we deﬁ ne for our project or company. If the Test Utility Method depends on types/classes that cannot be seen from a single place that all clients can access, it may be necessary to put it on a Test Helper in the appropriate test package or subsystem.\n\nVariation: Test Helper Mixin\n\nIn languages that support mixins, Test Helper Mixins give us the best of both worlds. As with a Test Helper, we can choose which Test Helper Mixins to in- clude without being constrained by a single-inheritance hierarchy. As with a Test Helper Object (see Test Helper), we can hold a test-speciﬁ c state in the mixin but we don’t have to instantiate and delegate that task to a separate object. As with a Testcase Superclass, we can access everything as methods and attributes on self.\n\nImplementation Notes\n\nIn variants of xUnit that require all Testcase Classes to be subclasses of a Test- case Superclass provided by the Test Automation Framework (page 298), we deﬁ ne that class as the superclass of our Testcase Superclass. In variants that use annotations or method attributes to identify the Test Method (page 348), we can subclass any class that we ﬁ nd useful.\n\nWe can implement the methods on the Testcase Superclass either as class methods or as instance methods. For any stateless Test Utility Methods, it is perfectly reasonable to use class methods. If it isn’t possible to use class meth- ods for some reason, we can work with instance methods. Either way, because the methods are inherited, we can access them as though they were deﬁ ned on the Testcase Class itself. If our language supports managing the visibility\n\nwww.it-ebooks.info\n\n639\n\nTestcase Superclass\n\n640\n\nTestcase Superclass\n\nChapter 24 Test Organization Patterns\n\nof methods, we must ensure that we make the methods visible enough (e.g., protected in Java).\n\nMotivating Example\n\nThe following example shows a Test Utility Method that is on the Testcase Class:\n\npublic class TestRefactoringExample extends TestCase { public void testAddOneLineItem_quantity1() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify assertInvoiceContainsOnlyThisLineItem(inv, expItem); }\n\nvoid assertInvoiceContainsOnlyThisLineItem( Invoice inv, LineItem expItem) { List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\",expItem, actual); } }\n\nThis Test Utility Method is not reusable outside this particular class or its subclasses.\n\nRefactoring Notes\n\nWe can make the Test Utility Method more reusable by moving it to a Testcase Superclass by using a Pull Up Method [Fowler] refactoring. Because the method is inherited by our Testcase Class, we can access it as if the method were deﬁ ned locally. If the Test Utility Method accesses any instance variables, we must perform a Pull Up Field [Fowler] refactoring to move those variables to a place where the Test Utility Method can see them. In languages that have visibility restrictions, we may need to make the ﬁ elds visible to subclasses (e.g., default or protected in Java) if Test Methods on the Testcase Class need to access the ﬁ elds as well.\n\nwww.it-ebooks.info\n\nTestcase Superclass\n\nExample: Testcase Superclass\n\nBecause the method is inherited by our Testcase Class, we can access it as if it were deﬁ ned locally. Thus the usage looks identical.\n\npublic class TestRefactoringExample extends OurTestCase { public void testAddItemQuantity_severalQuantity_v12(){ // Fixture Setup Customer cust = createACustomer(new BigDecimal(\"30\")); Product prod = createAProduct(new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(cust); // Exercise SUT invoice.addItemQuantity(prod, 5); // Verify Outcome LineItem expected = new LineItem(invoice, prod, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); } }\n\nThe only difference is the class in which the method is deﬁ ned and its visibility:\n\npublic class OurTestCase extends TestCase { void assertContainsExactlyOneLineItem(Invoice invoice, LineItem expected) { List lineItems = invoice.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actItem = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\",expected, actItem); } }\n\nExample: Test Helper Mixin\n\nHere are some tests written in Ruby using Test::Unit:\n\ndef test_extref # setup sourceXml = \"<extref id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\" mockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) # execute @handler.printBodyContents # verify assert_equals_html( expectedHtml, mockFile.output, \"extref: html output\") end\n\ndef testTestterm_normal sourceXml = \"<testterm id='abc'/>\"\n\nwww.it-ebooks.info\n\n641\n\nTestcase Superclass\n\n642\n\nTestcase Superclass\n\nChapter 24 Test Organization Patterns\n\nexpectedHtml = \"<a href='abc.html'>abc</a>\" mockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) @handler.printBodyContents assert_equals_html( expectedHtml, mockFile.output, \"testterm: html output\") end\n\ndef testTestterm_plural sourceXml =\"<testterms id='abc'/>\" expectedHtml = \"<a href='abc.html'>abcs</a>\" mockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) @handler.printBodyContents assert_equals_html( expectedHtml, mockFile.output, \"testterms: html output\") end\n\nThese tests contain a fair bit of Test Code Duplication (page 213). We can address this issue by using an Extract Method [Fowler] refactoring to create a Test Utility Method. We can then make the Test Utility Method more reusable by moving it to a Test Helper Mixin using a Pull Up Method refactoring. Because the mixed-in functionality is considered part of our Testcase Class, we can access it as if it were deﬁ ned locally. Thus the usage looks identical.\n\nclass CrossrefHandlerTest < Test::Unit::TestCase include HandlerTest\n\ndef test_extref sourceXml = \"<extref id='abc' />\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<extref>\") end\n\nThe only difference is the location where the method is deﬁ ned and its visibility. In particular, Ruby requires mixins to be deﬁ ned in a module rather than a class.\n\nmodule HandlerTest def generateAndVerifyHtml( sourceXml, expectedHtml, message, &block) mockFile = MockFile.new sourceXml.delete!(\"\\t\") @handler = setupHandler(sourceXml, mockFile ) block.call unless block == nil @handler.printBodyContents actual_html = mockFile.output assert_equal_html( expectedHtml, actual_html, message + \"html output\") actual_html end\n\nwww.it-ebooks.info\n\nTest Helper\n\nTest Helper\n\nWhere do we put our test code when it is in reusable Test Utility Methods?\n\nWe deﬁ ne a helper class to hold any Test Utility Methods we want to reuse in several tests.\n\nTest Helper Test Helper\n\nFixture Fixture\n\ntestMethod_1 testMethod_1\n\nTest Utility Test Utility Method_1 Method_1\n\ntestMethod_n testMethod_n\n\nTest Utility Test Utility Method_2 Method_2\n\nSUT SUT\n\nTestcase Testcase Class Class\n\nAs we write tests, we will invariably ﬁ nd ourselves needing to repeat the same logic in many, many tests. Initially, we may just “clone and twiddle” as we write additional tests that need the same logic. Ultimately, we may introduce Test Utility Methods (page 599) to hold this logic—but where should we put such reusable logic?\n\nA Test Helper is one possible choice of home for reusable test logic.\n\nHow It Works\n\nWe deﬁ ne a separate class to hold the reusable Test Utility Methods that should be available to several Testcase Classes (page 373). In each test that wishes to use this logic, we access the logic either using static method calls or via an instance created speciﬁ cally for the purpose.\n\nWhen to Use It\n\nWe can use a Test Helper if we wish to share logic or variables between several Testcase Classes and cannot (or choose not to) ﬁ nd or deﬁ ne a Testcase Super- class (page 638) from which we might otherwise subclass all tests that require this logic. We might pursue this course in several circumstances: Perhaps our\n\nwww.it-ebooks.info\n\n643\n\nTest Helper\n\n644\n\nTest Helper\n\nChapter 24 Test Organization Patterns\n\nprogramming language doesn’t support inheritance (e.g., Visual Basic 5 or 6), perhaps we are already using inheritance for some other conﬂ icting purpose, or perhaps the Test Utility Method needs access to speciﬁ c types that are not visible from the Testcase Superclass.\n\nThe decision between a Test Helper and a Testcase Superclass comes down to type visibility. The client classes need to see the Test Utility Method, and the Test Utility Method needs to see all the types and classes it depends on. When it doesn’t depend on many types/classes or when everything it depends on is visible from a single place, we can put the Test Utility Method into a common Testcase Superclass we deﬁ ne for our project or company. If the Test Utility Method depends on types/classes that cannot be seen from a single place that all clients can access, it may be necessary to put it on a Test Helper in the appropri- ate test package or subsystem. In larger systems with many groups of domain objects, it is common practice to have one Test Helper for each group (package) of related domain objects.\n\nVariation: Test Fixture Registry\n\nA Registry [PEAA] is a well-known object that can be accessed from anywhere in a program. We can use the Registry to store and retrieve objects from dif- ferent parts of our program or tests. (Registry objects are often confused with Singletons [GOF], which are also well known but have only a single instance. With a Registry object, there may be one or more instances—we don’t really care.) A Test Fixture Registry gives the tests the ability to access the same ﬁ xture as other tests in the same test run. Depending on how we implement our Test Helper, we may choose to provide a different instance of the Test Fixture Regis- try for each Test Runner (page 377) in an effort to prevent a Test Run War (see Erratic Test on page 228). A common example of a Test Fixture Registry is the Database Sandbox (page 650).\n\nA Test Fixture Registry is typically used with a Setup Decorator (page 447) or with Lazy Setup (page 435); it isn’t needed with Suite Fixture Setup (page 441) because only tests on the same Testcase Class need to share the ﬁ xture. In such a case, using a ﬁ xture holding class variable works well for this purpose.\n\nVariation: Object Mother\n\nThe Object Mother pattern is simply an aggregate of several other patterns, each of which makes a small but signiﬁ cant contribution to making the test ﬁ xture easier to manage. The Object Mother consists of one or more Test Helpers that provide Creation Methods (page 415) and Attachment Methods (see Creation Method), which our tests then use to create ready-to-use test ﬁ xture objects. Object Mothers\n\nwww.it-ebooks.info\n\nTest Helper\n\noften provide several Creation Methods that create instances of the same class, where each method results in a test object in a different starting state (a Named State Reaching Method; see Creation Method). The Object Mother may also have the ability to delete the objects it creates automatically—an example of Automated Teardown (page 503).\n\nBecause there is no single, crisp deﬁ nition of what someone means by “Object Mother,” it is advisable to refer to the individual patterns (such as Automated Teardown) when referring to speciﬁ c capabilities of the Object Mother.\n\nImplementation Notes\n\nThe methods on the Test Helper can be implemented as either class methods or instance methods depending on the degree to which we want to keep the tests from interacting.\n\nVariation: Test Helper Class\n\nIf all of the Test Utility Methods are stateless, the simplest approach is to imple- ment the functionality of the Test Helper as class methods and then to have the tests access those methods using the ClassName.methodName (or equivalent) notation. If we need to hold references to ﬁ xture objects, we could place them in class variables. We need to be careful to avoid inadvertently creating a Shared Fixture (page 317), however—unless, of course, that is exactly what we are trying to do. In such a case, we are actually building a Test Fixture Registry.\n\nVariation: Test Helper Object\n\nIf we can’t use class methods for some reason, we can work with instance meth- ods instead. In this case, the client test will need to create an instance of the Test Helper class and store it in an instance variable; the methods can then be accessed via this variable. This pattern is a good approach when the Test Helper holds references to ﬁ xture or SUT objects and we want to make sure that we don’t creep into a Shared Fixture situation. It is also useful when the Test Helper stores expec- tations for a set of Mock Objects (page 544), because this pattern ensures that we can verify the calls are interleaved between the Mock Objects correctly.\n\nMotivating Example\n\nThe following example shows a Test Utility Method that is on the Testcase Class:\n\npublic class TestUtilityExample extends TestCase {\n\npublic void testAddOneLineItem_quantity1() {\n\nwww.it-ebooks.info\n\n645\n\nTest Helper\n\n646\n\nTest Helper\n\nChapter 24 Test Organization Patterns\n\nInvoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify assertInvoiceContainsOnlyThisLineItem(inv, expItem); }\n\nvoid assertInvoiceContainsOnlyThisLineItem( Invoice inv, LineItem expItem) { List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\",expItem, actual); } }\n\nThis Test Utility Method is not reusable outside this particular class.\n\nRefactoring Notes\n\nWe can make a Test Utility Method more reusable by moving it to a Test Helper class. This transformation is often as simple as doing a Move Method [Fowler] refactoring to our Test Helper class. One potential problem arises when we have used instance variables to pass arguments to or return data from the Test Utility Method. These “global data” need to be converted to explicit arguments and return values before we can perform the Move Method refactoring.\n\nExample: Test Helper with Class Methods\n\nIn this modiﬁ ed version of the preceding test, we have turned the Test Utility Method into a class method on a Test Helper Class so we can access it via the classname without creating an instance:\n\npublic class TestUtilityExample extends TestCase { public void testAddOneLineItem_quantity1_staticHelper() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify TestHelper.assertContainsExactlyOneLineItem(inv, expItem); } }\n\nwww.it-ebooks.info\n\nTest Helper\n\nExample: Test Helper with Instance Methods\n\nIn this example, we have moved the Test Utility Method to a Test Helper as an instance method. Note that we must now access the method via an object refer- ence (a variable that holds an instance of the Test Helper).\n\npublic class TestUtilityExample extends TestCase { public void testAddOneLineItem_quantity1_instanceHelper() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify TestHelper helper = new TestHelper(); helper.assertInvContainsExactlyOneLineItem(inv, expItem); } }\n\nwww.it-ebooks.info\n\n647\n\nTest Helper\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nCHAPTER 25\n\nDatabase Patterns\n\nPatterns in This Chapter\n\nDatabase Sandbox. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650\n\nStored Procedure Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654\n\nTable Truncation Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661\n\nTransaction Rollback Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . 668\n\n649\n\nwww.it-ebooks.info\n\nDatabase Patterns\n\n650\n\nDatabase Sandbox\n\nChapter 25 Database Patterns\n\nDatabase Sandbox\n\nHow do we develop and test software that depends on a database?\n\nWe provide a separate test database for each developer or tester.\n\nDeveloper 1 Developer 1\n\nDeveloper 2 Developer 2\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nSUT SUT\n\nSUT SUT\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nFixture Fixture\n\nFixture Fixture\n\nDatabase Database\n\nDatabase Database\n\nMany applications use a database to store the persistent state of the application. At least some of the tests for such an application will require accessing the data- base. Unfortunately, a database is a primary cause of Erratic Tests (page 228) due to the fact that data may persist between tests. A major goal in keeping tests from interacting is ensuring that the test ﬁ xtures used by each test do not overlap. This is especially difﬁ cult when the development environment contains only a single test database and all tests run by all developers run against the same database.\n\nA Database Sandbox is one way to keep the tests from interacting by acciden-\n\ntally accessing the same records in the database.\n\nHow It Works\n\nWe provide each user with a separate, self-consistent sandbox in which to work. This sandbox includes the user’s own copy of any code plus—most importantly—the user’s own copy of the database. Such an arrangement allows each user to modify the database in any way he or she sees ﬁ t and to exercise the application with tests without worrying about any interactions between the user’s own tests and the tests conducted by other users.\n\nWhen to Use It\n\nWe should use a Database Sandbox whenever we are building or modifying an application that depends on a database for a signiﬁ cant portion of its functionality.\n\nwww.it-ebooks.info\n\nDatabase Sandbox\n\nThis need is especially evident if we have chosen to use a Shared Fixture (page 317). Using a Database Sandbox will help us avoid Test Run Wars (see Erratic Test) between different users of the database. Depending on how we have chosen to implement the Database Sandbox, it may or may not allow different users to modify the structure of the database. A Database Sandbox will not prevent Un- repeatable Tests (see Erratic Test) or Interacting Tests (see Erratic Test), however, because it merely separates different users (and their test runs) from one another; tests within a single test run may continue to share a test ﬁ xture.\n\nImplementation Notes\n\nThe application needs to be made conﬁ gurable so that the database to be used in testing can be changed without modifying the code. Typically, this goal is accomplished by reading the database conﬁ guration information from a proper- ties ﬁ le that is customized in each user’s environment.\n\nADatabase Sandbox can be implemented in many different ways. Fundamentally, the choice comes down to whether we give each user a separate database instance or just simulate one. In general, giving each user a real separate database instance is the preferred choice. This scheme may not always be feasible, however—especially if the database vendor’s licensing structure makes it cost prohibitive.\n\nVariation: Dedicated Database Sandbox\n\nWe give each developer, tester, or test user a separate database instance. This is typically accomplished by installing a lightweight database technology in each user’s test environment. Examples of lightweight database technologies include MySql and Personal Oracle. The database instance can be installed on the user’s own machine, on a shared test server, or on a dedicated “virtual server” running on shared server hardware.\n\nA Dedicated Database Sandbox is the preferred solution because it provides the greatest ﬂ exibility. It allows a developer to modify the database schema, load his or her own test data, and so on.\n\nVariation: DB Schema per Test Runner\n\nWith DB Schema per Test Runner, we give each developer, tester, or test user what appears to be a separate database instance by using built-in database sup- port for multiple schemas.\n\nOne considerable advantage that the DB Schema per Test Runner pattern enjoys relative to the Dedicated Database Sandbox pattern is that we can share an Immutable Shared Fixture (see Shared Fixture) deﬁ ned in a common schema and put each user’s mutable ﬁ xture in his or her own private schema. Note that\n\nwww.it-ebooks.info\n\n651\n\nDatabase Sandbox",
      "page_number": 654
    },
    {
      "number": 25,
      "title": "Database Patterns",
      "start_page": 715,
      "end_page": 742,
      "detection_method": "regex_chapter",
      "content": "652\n\nDatabase Sandbox\n\nChapter 25 Database Patterns\n\nthis scheme does not allow the user to modify the structure of the database (at least not to the same degree as is possible with a Dedicated Database Sandbox). It also forces all users, including both developers and testers, to use the same database structure. This can create logistical issues when database structure upgrades need to be rolled out.\n\nVariation: Database Partitioning Scheme\n\nWe give each developer, tester, or test user a separate set of data within a single Database Sandbox. Each user can modify that data as he or she sees ﬁ t but is not allowed to modify the data assigned to other users.\n\nThis approach requires less database administration overhead but more data administration overhead than with the other ways to implement a Database Sandbox. Because it does not allow developers to modify the database schema, a Database Partitioning Scheme is not appropriate for evolutionary database development. It is, however, appropriate for preventing Interacting Tests when applied to different tests run from the same Test Runner. That is, we give each test a unique key such as a CustomerNumber that it uses for all data. As a conse- quence, other tests within the same test run use different data. This pattern can be combined with many of the other variations of Database Sandbox to prevent Interacting Tests when using a Shared Fixture. Note that this pattern does not prevent Unrepeatable Tests unless we also use Distinct Generated Values (see Generated Value on page 723).\n\nMotivating Example\n\nThe following test uses Literal Values for the arguments to a constructor of a Product that is persisted into a database instance shared among several developers. The name of the Product must be unique:\n\npublic void testProductPrice_HCV() { // Setup Product product = new Product( 88, // ID \"Widget\", // Name new BigDecimal(\"19.99\")); // Price // Exercise SUT // ... }\n\nUnfortunately, we may end up with a Test Run War when we run this test against a shared database instance regardless of how effectively we tear down the Product after each test. This is because we are trying to create the same Product that the same test run from another Test Runner might be in the process of using at the same time.\n\nwww.it-ebooks.info\n\nDatabase Sandbox\n\nRefactoring Notes\n\nThere are no code changes required of our test when we create a Dedicated Database Sandbox for each developer and tester. Therefore, tests should not have to do anything special to run completely independently of tests being run from other Test Runners (page 377). There is a small change required of the SUT, however, to allow the SUT to connect to different database instances based on conﬁ guration data. How we make this change varies with the technology we use and is beyond the scope of this book.\n\nWe can convert the test to use a Database Partitioning Scheme by replacing the Literal Values with calls to the appropriate getUnique method passing an ID speciﬁ c to the Test Runner as a seed.\n\nExample: Database Partitioning Scheme\n\nHere is the same test using a Database Partitioning Scheme to ensure that each test uses a different set of products. For the getUniqueString method, we pass a string based on the MAC address of our computer.\n\npublic void testProductPrice_DPS() { // Setup Product product = new Product( getUniqueInt(), // ID getUniqueString(getMacAddress()), // Name new BigDecimal(\"19.99\")); // Price // Exercise SUT // ... }\n\nstatic int counter = 0;\n\nint getUniqueInt() { counter++; return counter; }\n\nBigDecimal getUniqueBigDecimal() { return new BigDecimal(getUniqueInt()); }\n\nString getUniqueString(String baseName) { return baseName.concat(String.valueOf( getUniqueInt())); }\n\nThis test can now be run from several different computers against the same shared database instance without fear of a Test Run War.\n\nwww.it-ebooks.info\n\n653\n\nDatabase Sandbox\n\n654\n\nStored Procedure Test\n\nChapter 25 Database Patterns\n\nStored Procedure Test\n\nHow can we verify logic independently when we have stored procedures?\n\nWe write Fully Automated Tests for each stored procedure.\n\nApplication Environment Application Environment\n\nTestcase Class Testcase Class\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nStored Stored Procedure Procedure Proxy Proxy\n\nTestcase Class Testcase Class\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nDatabase Database\n\nStored Stored Procedure Procedure\n\nMany applications that use a database to store the persistent state of the appli- cation also use stored procedures and triggers to improve performance and do common processing on updates.\n\nA Stored Procedure Test is a way to apply automated testing practices to this\n\ncode that lives inside the database.\n\nHow It Works\n\nWe write unit tests for the stored procedures independent of the client application software. These tests may be layer-crossing tests or round-trip tests, depending on the nature of the store procedure(s) being tested.\n\nWhen to Use It\n\nWe should write Stored Procedure Tests whenever we have nontrivial logic in stored procedures. This pattern will help us verify that the stored procedures—our\n\nwww.it-ebooks.info\n\nStored Procedure Test\n\nSUT for the purposes of these tests—are working properly independently of the client application. This consideration is particularly important when more than one application will use the stored procedures or when the stored procedures are being developed by a different development team. Stored Procedure Tests are particularly important when we cannot ensure the procedures are tested ade- quately simply by exercising the application software (a form of Indirect Testing; see Obscure Test on page 186). Using Stored Procedure Tests also helps us to enumerate all the conditions under which the stored procedure could be called and what should happen in each circumstance. The very act of thinking about these circumstances is likely to improve the design—a common result of doing test-ﬁ rst development.\n\nImplementation Notes\n\nThere are two fundamentally different ways to implement Stored Procedure Tests: (1) We can write the tests in the same programming language as the stored proce- dure and run them in the database or (2) we can write the tests in our application programming language and access the stored procedure via a Remote Proxy [GOF]. We might even write tests both ways. For example, the stored-procedure developers might write unit tests in the database programming language, whereas the applica- tion developers might prepare some acceptance tests in the application programming language to run as part of the application build.\n\nEither way, we need to decide how the test will set up the ﬁ xture (the “before” state of the database) and verify the expected outcome (the “after” state of the database as well as any expected actions such as cascading deletes). The test may interact directly with the database to insert/verify the data (a form of Back Door Manipulation; see page 327) or it could use another stored procedure (a form of round-trip test).\n\nVariation: In-Database Stored Procedure Test\n\nOne advantage of the xUnit approach to automated testing is that the tests are written in the same language as the code we are testing. This makes it easier for the developers to learn how to automate the tests without learning a new program- ming language, debugger, and so on. Extending this idea to its logical conclusion, it makes sense to test stored procedures using tests that are written in the stored- procedure programming language. Naturally, we will need to run these tests inside the database. Unfortunately, that requirement may make it hard to run them as part of the Integration Build [SCM].\n\nThis variation on the Stored Procedure Test pattern is appropriate when we have more experience writing code in the stored-procedure language and/or\n\nwww.it-ebooks.info\n\n655\n\nStored Procedure Test\n\n656\n\nStored Procedure Test\n\nChapter 25 Database Patterns\n\nenvironment than in the application environment and it is not essential that all tests be run from a single place. For example, a database or data services team that is writing stored procedures for use by other teams would ﬁ nd this approach attractive. Another circumstance in which it would be appropriate to use In-Database Stored Procedure Tests arises when the procedures are stored in a different source code repository than the application logic. Using In-Database Stored Procedure Test allows us to store the tests in the same repository as the SUT (in this case, the stored procedures).\n\nIn-Database Stored Procedure Tests may allow somewhat more thorough unit testing (and test-driven development) of the stored procedures because we may have better access to implementation details of the stored procedure from our tests. Of course, this violation of encapsulation could result in Overspeciﬁ ed Software (see Fragile Test on page 239). If the client code uses a data access layer, we must still write unit tests for that software in the application programming language to ensure that we handle errors correctly (e.g., failure to connect).\n\nSome databases support several programming languages. In such a case, we can choose to use the more test-friendly programming language for our tests but write the stored procedures in the more traditional stored-procedure programming language. For example, Oracle databases support both PLSQL and Java, so we could use JUnit tests to verify our PLSQL stored procedures. Likewise, Microsoft’s SQL Server supports C#, so we could use NUnit tests written in C# to verify the stored procedures written in Transact-SQL.\n\nVariation: Remoted Stored Procedure Test\n\nThe purpose of Remoted Stored Procedure Tests is to allow us to write the tests in the same language as the unit tests for the client application logic. We must access the stored procedure via a Remote Proxy [GOF] that hides the mechanics of inter- acting with that procedure. This proxy can be structured as either a Service Facade [CJ2EEP] or a Command [GOF] (such as Java’s JdbcOdbcCallableStatement).\n\nRemoted Stored Procedure Tests are, in effect, component tests in that they treat the stored procedure as a “black box” component. Because Remoted Stored Procedure Tests do not run inside the database, we are more likely to write them as round-trip tests (calling other stored procedures to set up the ﬁ xture, verify the outcome, and perform other necessary tasks) unless we have an easy way to insert or verify data. Some members of the xUnit family have extensions that are speciﬁ cally intended to facilitate this behavior (e.g., DbUnit for Java and NDbUnit for .NET languages).\n\nThis solution is more appropriate if we want to keep all our tests in a single programming language. The Remoted Stored Procedure Test pattern makes it easier to run all the tests every time we check in changes to the application code.\n\nwww.it-ebooks.info\n\nStored Procedure Test\n\nTesting Stored Procedures with JUnit\n\nOn an early XP project, our application was mandated to use stored procedures being developed by another group. It seemed that every time we integrated our Java with those developers’ PLSQL code, we found serious bugs in the fundamental behavior of their stored procedures. We were writing automated tests using JUnit for our code. Although we were sure that writing unit tests for the stored procedures would clarify the interface contract and improve the quality of the other group’s code, we couldn’t force the other team to write unit tests. Nor had utPLSQL even been invented at that point.\n\nWe decided to try writing unit tests for the stored procedures in the xUnit family member we were comfortable with: JUnit. Because we had to write JDBC code to access the stored procedures anyway, we deﬁ ned JUnit tests for each stored procedure via the JDBC PreparedStatement classes that we had built. The tests exercised the basic behavior of the stored behaviors and a few of the more obvious failure cases. Whenever we received a new version of the stored procedures, we would run the JUnit tests before we even tried to exercise the procedures from our application code. Needless to say, many of the tests failed.\n\nWe sat down with the developers who were building the stored proce- dures and showed them our tests—including how they were failing left, right, and center. Needless to say, the developers were a bit embarrassed but they agreed that our tests were correct. They went off to ﬁ x the stored procedures and gave us a new version to test. The revision fared somewhat better but still produced some failures. Then a very important thing hap- pened: The members of the other group asked for a copy of the tests we had written and instructions on how to run them for themselves. Before long, these developers were writing their own PLSQL unit tests in JUnit!\n\nThis capability is particularly useful if the stored procedures are being writ- ten and/or modiﬁ ed by the same team that is developing the client code. We can also use Remoted Stored Procedure Tests when another team is provid- ing the stored procedures and we are not conﬁ dent in those developers’ ability to write defect-free code (probably because they are not writing In-Database Stored Procedure Tests for their code). In this situation, we can use the Remot- ed Stored Procedure Tests as a form of acceptance test for their code. See the sidebar “Testing Stored Procedures with JUnit” for an illustration of how this setup worked on one project.\n\nwww.it-ebooks.info\n\n657\n\nStored Procedure Test\n\n658\n\nStored Procedure Test\n\nChapter 25 Database Patterns\n\nOne disadvantage of using Remoted Stored Procedure Tests is that they will likely cause the test suite to run more slowly because the tests require the database to be available and populated with data. The tests for the stored procedures can be put into a separate Subset Suite (see Named Test Suite on page 592) so that they need not be run with all the in-memory tests. This can signiﬁ cantly speed up test execution, thereby avoiding Slow Tests (page 253).\n\nRemoted Stored Procedure Tests also come in handy when logic written in our programming language of choice already has unit tests and we need to move that logic into the database. By using a Remoted Stored Procedure Test, we can avoid rewriting the tests in a different programming language and Test Automation Framework (page 298), which can in turn save time and money. This pattern also enables us to avoid any translation errors when recoding the logic, so we can be sure the recoded logic really does produce the same results.\n\nMotivating Example\n\nHere is an example of a stored procedure written in PLSQL:\n\nCREATE OR REPLACE PROCEDURE calc_secs_between ( date1 IN DATE, date2 IN DATE, secs OUT NUMBER ) IS BEGIN secs := (date2 - date1) * 24 * 60 * 60; END; /\n\nThis sample was taken from the examples that come with the utPLSQL tool. In real life we might not bother testing this code because it is so simple (but then again, maybe not?) but it will work just ﬁ ne to illustrate how we could go about testing it.\n\nRefactoring Notes\n\nThis example doesn’t deal so much with refactoring as with adding a missing test. Let’s ﬁ nd a way to write one. We will see what is involved by using the two main variants: In-Database Stored Procedure Test and Remote Stored Procedure Test.\n\nExample: In-Database Stored Procedure Test\n\nThis example uses utPLSQL, the xUnit family member for PLSQL, to automate tests that run inside the database:\n\nwww.it-ebooks.info\n\nStored Procedure Test\n\nCREATE OR REPLACE PACKAGE BODY ut_calc_secs_between IS PROCEDURE ut_setup IS BEGIN NULL; END;\n\nPROCEDURE ut_teardown IS BEGIN NULL; END;\n\n-- For each program to test... PROCEDURE ut_CALC_SECS_BETWEEN IS secs PLS_INTEGER; BEGIN CALC_SECS_BETWEEN ( DATE1 => SYSDATE ' DATE2 => SYSDATE ' SECS => secs );\n\nutAssert.eq ( 'Same dates', secs, 0 ); END ut_CALC_SECS_BETWEEN;\n\nEND ut_calc_secs_between; /\n\nThis test uses many of the familiar xUnit patterns. It is one of several tests we would normally write for this stored procedure—one test for each possible scenario. (This sample was taken from the examples that come with the utPLSQL tool. Not being a PLSQL programmer, I did not want to mess with the formatting in case it mattered!)\n\nExample: Remoted Stored Procedure Test\n\nTo test this stored procedure in our normal programming and test execution environment, we must ﬁ rst ﬁ nd or create a Remote Proxy for it in our unit-testing environment of choice. Then we can write our unit tests in the usual manner.\n\nwww.it-ebooks.info\n\n659\n\nStored Procedure Test\n\n660\n\nStored Procedure Test\n\nChapter 25 Database Patterns\n\nThe following test uses JUnit to automate tests that run outside the database and call our PLSQL stored procedure remotely:\n\npublic class StoredProcedureTest extends TestCase { public void testCalcSecsBetween_SameTime() { // Setup TimeCalculatorProxy SUT = new TimeCalculatorProxy(); Calendar cal = new GregorianCalendar(); long now = cal.getTimeInMillis(); // Exercise long timeDifference = SUT.calc_secs_between(now,now); // Verify assertEquals( 0, timeDifference ); } }\n\nWe have reduced the complexity of the original test to a simple test of a function by hiding the JdbcOdbcCallableStatement behind a Service Facade. Looking at this example, it is difﬁ cult to tell that we are not testing a Java method. We would prob- ably have additional Expected Exception Tests (see Test Method on page 348) to verify failed connections and other problems.\n\nwww.it-ebooks.info\n\nTable Truncation Teardown\n\nTable Truncation Teardown\n\nHow do we tear down the Test Fixture when it is in a relational database?\n\nWe truncate the tables modiﬁ ed during the test to tear down the ﬁ xture.\n\nSetup Setup\n\nInsert Insert\n\nFixture Fixture\n\nExercise Exercise\n\nData Data\n\nVerify Verify\n\nExercise Exercise\n\nSUT SUT\n\nUpdate Update\n\nTeardown Teardown\n\nTruncate Truncate\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ xture is torn down after each test. Leftover objects and database records, as well as open ﬁ les and connections, can at best cause performance degradation and at worst cause tests to fail or systems to crash. While some of these resources may be cleaned up automatically by garbage collection, others may be left hanging if they are not torn down explicitly.\n\nWriting teardown code that can be relied upon to clean up properly in all pos- sible circumstances is challenging and time-consuming. It involves understand- ing what could be left over for each possible outcome of the test and writing code to deal with that possibility. This Complex Teardown (see Obscure Test on page 186) introduces a fair bit of Conditional Test Logic (page 200) and—worst of all—Untestable Test Code (see Hard-to-Test Code on page 209).\n\nWhen testing a system that uses a relational database, we can take advantage of the database’s capabilities by using the TRUNCATE command to remove all data from a table we have modiﬁ ed.\n\nHow It Works\n\nWhen we no longer need a persistent ﬁ xture, we issue a TRUNCATE command for each table in the ﬁ xture. It blasts all data out of the tables very efﬁ ciently with no side effects (e.g., triggers).\n\nwww.it-ebooks.info\n\n661\n\nTable Truncation Teardown\n\n662\n\nTable Truncation Teardown\n\nChapter 25 Database Patterns\n\nWhen to Use It\n\nWe often turn to Table Truncation Teardown when we are using a Persistent Fresh Fixture (see Fresh Fixture on page 311) strategy with an SUT that includes a database. It is rarely our ﬁ rst choice, however. That distinction goes to Transac- tion Rollback Teardown (page 668). Nevertheless, Table Truncation Teardown is a better choice for use with a Shared Fixture (page 317), as this type of ﬁ xture, by deﬁ nition, outlives any one test. By contrast, using Transaction Rollback Teardown with a Shared Fixture would require a very long-running transaction. While not impossible, such a long-lived transaction is troublesome.\n\nBefore we can use Table Truncation Teardown, we must satisfy a couple of criteria. The ﬁ rst requirement is that we really want all data in the affected tables removed. The second requirement is that each Test Runner (page 377) has its own Database Sandbox (page 650). Table Truncation Teardown will not work if we are using a Database Partitioning Scheme (see Database Sandbox) to isolate users or tests from one another. It is ideally suited for use with a DB Schema per Test Runner (see Database Sandbox), especially when we are implementing an Immutable Shared Fixture (see Shared Fixture) as a separate shared schema in the database. This allows us to blast away all the Fresh Fixture data in our own Database Sandbox without affecting the Immutable Shared Fixture.\n\nIf we are not using a transactional database, the closest approximation is Automated Teardown (page 503), which deletes only those records that were created by the test. Automated Teardown does not depend on the database transactions to do the work for it, but it does involve more development work on our part. We can also avoid the need to do teardown entirely by using Delta Assertions (page 485).\n\nImplementation Notes\n\nBesides the usual “Where do we put the teardown code?” decision, implementa- tion of Table Truncation Teardown needs to deal with the following questions:\n\nHow do we actually delete the data—that is, which database commands\n\ndo we use?\n\nHow do we deal with foreign key constraints and triggers?\n\nHow do we ensure consistency when we are using an object-relational\n\nmapping (ORM)?\n\nSome databases support the TRUNCATE command directly. Where this is the case, the obvious choice is to use this command. Oracle, for example, supports TRUNCATE.\n\nwww.it-ebooks.info\n\nTable Truncation Teardown\n\nOtherwise, we may have to use a DELETE * FROM table-name command instead. The TRUN- CATE or DELETE commands can be issued using In-line Teardown (page 509—called from within each Test Method; see page 348) or Implicit Teardown (page 516— called from the tearDown method). Some people prefer to use this command with Lazy Teardown because it ensures that the tables are empty at the beginning of the test in cases where those tables would be affected by extraneous data.\n\nDatabase foreign key constraints can be a problem for Table Truncation Teardown if our database does not offer something similar to Oracle’s ON DELETE CASCADE option. In Oracle, if the command to truncate a table includes the ON DELETE CASCADE option, then rows dependent on the truncated table rows are deleted as well. If our database does not cascade deletes, we must ensure that the tables are truncated in the order required by the schema. Schema changes can invalidate this order, resulting in failures in the teardown code. Fortunately, such failures are easy to detect: A test error tells us that our teardown needs adjusting. Correction is fairly straightforward—typically, we just need to reor- der the TRUNCATE commands. We could, of course, come up with a way to issue the TRUNCATE commands in the correct order dynamically based on the dependen- cies between the tables. Usually, however, it is enough to encapsulate this trun- cation logic behind a Test Utility Method (page 599).\n\nIf we want to avoid the side effects of triggers and other complications for databases where TRUNCATE is not supported, we can disable the constraints and/or triggers for the duration of the test. We should take this step only if other tests exercise the SUT with the constraints and triggers in place.\n\nIf we are using an ORM layer such as Toplink, (N)Hibernate, or EJB 3.0, we may need to force the ORM to clear its cache of objects already read from the database so that subsequent object lookups do not ﬁ nd the recently deleted objects. For example, NHibernate provides the ClearAllCaches method on the TransactionManager for this purpose.\n\nVariation: Lazy Teardown\n\nA teardown technique that works with only a few styles of Shared Fixtures is Lazy Teardown. With this pattern, the ﬁ xture must be destroyable at an arbitrary point in time. Thus we cannot depend on “remembering” what needs to be torn down; it must be obvious without any “memory.” Table Truncation Teardown ﬁ ts the bill because how we perform teardown is exactly the same whenever we choose to do it. We simply issue the table truncation commands during ﬁ xture setup before setting up the new ﬁ xture.\n\nwww.it-ebooks.info\n\n663\n\nTable Truncation Teardown\n\n664\n\nTable Truncation Teardown\n\nChapter 25 Database Patterns\n\nMotivating Example\n\nThe following test attempts to use Guaranteed In-line Teardown (see In-line Teardown) to remove all the records it created:\n\n[Test] public void TestGetFlightsByOrigin_NoInboundFlights() { // Fixture Setup long OutboundAirport = CreateTestAirport(\"1OF\"); long InboundAirport = CreateTestAirport(\"1IF\"); FlightDto ExpFlightDto = null; try { ExpFlightDto = CreateTestFlight(OutboundAirport, InboundAirport); // Exercise System IList FlightsAtDestination1 = Facade.GetFlightsByOriginAirport( InboundAirport); // Verify Outcome Assert.AreEqual( 0, FlightsAtDestination1.Count ); } ﬁnally { Facade.RemoveFlight( ExpFlightDto.FlightNumber ); Facade.RemoveAirport( OutboundAirport ); Facade.RemoveAirport( InboundAirport ); } }\n\nThis code is neither easy to write nor correct!1 Trying to keep track of the many objects the SUT has created and then tear them down one by one in a safe man- ner is very tricky.\n\nRefactoring Notes\n\nWe can avoid most of the issues with coordinating In-line Teardown of mul- tiple resources in a safe way by using Table Truncation Teardown and blasting away all the airports in one fell swoop.2 Most of the refactoring work involves deleting the existing teardown code from the ﬁ nally clause and inserting a call to cleanDatabase. We then implement this method using the truncation commands.\n\n1 See In-line Teardown for an explanation of what is wrong here. 2 This assumes that we start with no airports and want to end with no airports. If we want to delete just these speciﬁ c airports, we cannot use Table Truncation Teardown.\n\nwww.it-ebooks.info\n\nTable Truncation Teardown\n\nExample: Table Truncation (Delegated) Teardown Test\n\nThis is what the test looks like when we are done:\n\npublic void TestGetFlightsByOrigin_NoInboundFlight_TTTD() { // Fixture Setup long OutboundAirport = CreateTestAirport(\"1OF\"); long InboundAirport = 0; FlightDto ExpectedFlightDto = null; try { InboundAirport = CreateTestAirport(\"1IF\"); ExpectedFlightDto = CreateTestFlight( OutboundAirport,InboundAirport); // Exercise System IList FlightsAtDestination1 = Facade.GetFlightsByOriginAirport(InboundAirport); // Verify Outcome Assert.AreEqual(0,FlightsAtDestination1.Count); } ﬁnally { CleanDatabase(); } }\n\nThis example uses Delegated Teardown (see In-line Teardown) to keep the teardown code visible. Normally, however, we would use Implicit Teardown by putting this logic into the tearDown method. The try/catch ensures that clean- Database is run but it does not ensure that a failure inside cleanDatabase will not prevent the teardown from completing.\n\nExample: Lazy Teardown Test\n\nHere is the same example converted to use Lazy Teardown:\n\n[Test] public void TestGetFlightsByOrigin_NoInboundFlight_LTD() { // Lazy Teardown CleanDatabase(); // Fixture Setup long OutboundAirport = CreateTestAirport(\"1OF\"); long InboundAirport = 0; FlightDto ExpectedFlightDto = null; InboundAirport = CreateTestAirport(\"1IF\"); ExpectedFlightDto = CreateTestFlight( OutboundAirport, InboundAirport); // Exercise System\n\nwww.it-ebooks.info\n\n665\n\nTable Truncation Teardown\n\n666\n\nTable Truncation Teardown\n\nChapter 25 Database Patterns\n\nIList FlightsAtDestination1 = Facade.GetFlightsByOriginAirport(InboundAirport); // Verify Outcome Assert.AreEqual(0,FlightsAtDestination1.Count); }\n\nBy moving the call to cleanDatabase to the front of the Test Method, we ensure that the database is in the state we expect it. This code cleans up whatever the last test did, regardless of whether that test provided proper teardown. It also takes care of anything added to the relevant tables since the last test was run. It has the added beneﬁ t of eliminating the need for the try/ﬁ nally construct, thereby making the test simpler and easier to understand.\n\nExample: Table Truncation Teardown Using SQL\n\nThis implementation of the cleanDatabase method uses SQL statements constructed within the code:\n\npublic static void CleanDatabase() { string[] tablesToTruncate = new string[] {\"Airport\",\"City\",\"Airline_Cd\",\"Flight\"}; IDbConnection conn = getCurrentConnection(); IDbTransaction txn = conn.BeginTransaction(); try { foreach (string eachTableToTruncate in tablesToTruncate) { TruncateTable(txn, eachTableToTruncate); } txn.Commit(); conn.Close(); } catch (Exception e) { txn.Rollback(); } ﬁnally { conn.Close(); } }\n\nprivate static void TruncateTable( IDbTransaction txn, string tableName) { const string C_DELETE_SQL = \"DELETE FROM {0}\";\n\nIDbCommand cmd = txn.Connection.CreateCommand(); cmd.Transaction = txn; cmd.CommandText = string.Format(C_DELETE_SQL, tableName);\n\ncmd.ExecuteNonQuery(); }\n\nwww.it-ebooks.info\n\nTable Truncation Teardown\n\nBecause we are using SQL Server as the database, we had to implement our own TruncateTable method that issues a Delete * from ... SQL command. We would not have to take this step if our database implemented TRUNCATE directly.\n\nExample: Table Truncation Teardown Using ORM\n\nHere is the implementation of the cleanDatabase method using NHibernate, an ORM layer:\n\npublic static void CleanDatabase() { ISession session = TransactionManager.Instance.CurrentSession; TransactionManager.Instance.BeginTransaction(); try { // We need to delete only the root classes because // cascade rules will delete all related child entities session.Delete(\"from Airport\"); session.Delete(\"from City\"); session.Flush(); TransactionManager.Instance.Commit(); } catch (Exception e) { Console.Write(e); throw e; } ﬁnally { TransactionManager.Instance.CloseSession(); } }\n\nWhen using an ORM, we read, write, and delete domain objects; the tool deter- mines which underlying tables they map to and takes the appropriate actions. Because we have chosen to make City and Airport “root” (parent) objects, any subordinate (child) objects such as the Flights are deleted automatically when the root is deleted. This approach further decouples us from the details of the table implementations.\n\nwww.it-ebooks.info\n\n667\n\nTable Truncation Teardown\n\n668\n\nTransaction Rollback Teardown\n\nChapter 25 Database Patterns\n\nTransaction Rollback Teardown\n\nHow do we tear down the Test Fixture when it is in a relational database?\n\nWe roll back the uncommitted test transaction as part of the teardown.\n\nSetup Setup\n\nStart Transaction Start Transaction\n\nFixture Fixture\n\nInsert Insert\n\nExercise Exercise\n\nData Data\n\nVerify Verify\n\nExercise Exercise\n\nSUT SUT\n\nUpdate Update\n\nTeardown Teardown\n\nRollback Rollback\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ x- ture is torn down after each test. Leftover objects and database records, as well as open ﬁ les and connections, can at best cause performance degradation and at worst cause tests to fail or systems to crash. While some of these resources may be cleaned up automatically by garbage collection, others may be left hanging if they are not torn down explicitly.\n\nWriting teardown code that can be relied upon to clean up properly in all possible circumstances is challenging and time-consuming. It involves under- standing what could be left over for each possible outcome of the test and writing code to deal with this case. This Complex Teardown (see Obscure Test on page 186) introduces a fair bit of Conditional Test Logic (page 200) and— worst of all—Untestable Test Code (see Hard-to-Test Code on page 209).\n\nWe can avoid making any lasting changes to the database contents by not committing the transaction and taking advantage of the rollback capabilities of the database.\n\nHow It Works\n\nOur test starts a new test transaction, sets up the ﬁ xture, exercises the SUT, and veriﬁ es the outcome of the test. Each of these steps may involve interacting with\n\nwww.it-ebooks.info\n\nTransaction Rollback Teardown\n\nthe database. At the end of the test, the test rolls back the test transaction, which prevents any of the changes from becoming persistent.\n\nWhen to Use It\n\nWe can use Transaction Rollback Teardown when we are using a Fresh Fix- ture (page 311) approach with an SUT that includes a database that supports rolling back a transaction. There are, however, some prerequisites for using Transaction Rollback Teardown.\n\nIn particular, the SUT must expose methods that are normally called in the context of an existing transaction by a Humble Transaction Controller (see Humble Object on page 695). That is, the methods should not start their own transaction and must never commit a transaction. If we are doing test-driven development, this design will come about as a result of applying the Transac- tion Rollback Teardown pattern as we write our code. If we are retroﬁ tting the tests to existing software, we may need to refactor the code to use a Humble Transaction Controller before we can use Transaction Rollback Teardown.\n\nThe nice thing about Transaction Rollback Teardown is that it leaves the database in exactly the same state as it was when we started the test, regard- less of what changes we made to the database contents during the test. As a result, we do not need to determine what needs to be cleaned up and what does not. Changes to the database schema or contents do not affect our teardown logic. Clearly, this pattern is much simpler to apply than Table Truncation Tear- down (page 661).\n\nThe usual caveats apply to any tests that run against a real database; such tests will take approximately 50 (yes, 50!) times as long to run as tests that do not access the database. This testing approach will almost surely result in Slow Tests (page 253) unless we replace the real database with an In-Memory Database (see Fake Object on page 551) for most of our tests. Because we are depending on the transactional properties of the database, a simple Fake Data- base (see Fake Object) will probably not be sufﬁ cient unless it supports ACID.\n\nAnother prerequisite with Transaction Rollback Teardown is that we cannot do anything that results in a commit anywhere in the tests or the code they exer- cise. The sidebar “Transaction Rollback Pain” on page 670 describes examples of where commits can sneak in and cause havoc.\n\nwww.it-ebooks.info\n\n669\n\nTransaction Rollback Teardown\n\n670\n\nTransaction Rollback Teardown\n\nChapter 25 Database Patterns\n\nTransaction Rollback Pain\n\nJohn Hurst sent me an e-mail in which he described some of the issues his team had encountered using Transaction Rollback Teardown. He writes:\n\nWe used Transaction Rollback Teardown for our database integra- tion tests for a while, after a discussion on TheServerSide during which Rod Johnson advocated the approach. I gathered his main motivation for using it was for performance; a rollback is usually a lot faster than repriming the database in a new transaction for the next test. Indeed, we did ﬁ nd it somewhat faster than our pre- vious approach. We used Spring’s excellent AbstractTransactionalData- SourceSpringContextTests base class, which supports most of what you need to do for this pattern out of the box.\n\nHowever, I chose to abandon this pattern after a few months. Here are the drawbacks I came across with this approach:\n\n1. You lose some test isolation. In the way we implemented this pattern, anyway, each test assumed the database was in a cer- tain base starting condition, and the rollback would revert it to that condition. In our current model, each test is respon- sible—usually via a base class’s setUp()—for priming the data- base into a known state.\n\n2. You can’t see what’s in the database when something goes wrong. If your test fails, you usually want to examine the database to see what happened. If you’ve rolled back all the changes, it makes it harder to ﬁ nd the bug.\n\n3. You have to be very careful not to inadvertently commit during your test. Yes, the code under test has declarative transaction management, and does nothing surprising. But we occasionally would need to do things in the test setup like drop and recreate a sequence to reset its value. This, being DDL, commits any outstanding transaction—and confused programmers.\n\n4. You can’t easily mix in tests that do need to commit changes. Lately I have added some PLSQL stored procedures and tests. Some of the stored procedures do explicit commits. I cannot mix these in the same JUnit suite with tests that assume the database always remains in a certain state.\n\nI apologize if my terminology isn’t consistent with what’s in your book. Also, my experience is probably a little limited; I’ve only\n\nwww.it-ebooks.info\n\nTransaction Rollback Teardown\n\ntried this approach in a Spring environment and I prefer to do most things in a “Spring” way. Finally, I am sure these limitations can be and are worked around in various ways. It’s just that, for our team, this pattern turned out to be more trouble than it was worth.\n\nDon’t get me wrong—I DO think the pattern should be included. I just think the consequences should be noted, and maybe it isn’t for everyone.\n\nImplementation Notes\n\nA few members of the xUnit family support Transaction Rollback Teardown directly; open-source extensions may be available for other members. If nothing is available, coding this teardown logic is not very complicated. The more signiﬁ - cant implementation consideration is giving the tests access to nontransactional methods on the SUT. Most domain model objects are nontransactional, so this requirement should not be a problem for unit tests of domain objects. We are more likely to experience a problem when we are writing Subcutaneous Tests (see Layer Test on page 337) against a Service Facade [CJ2EEP] because these methods often perform transaction control. If this is the case, we will need to expose a nontransactional version of the methods by refactoring to the Humble Transaction Controller pattern. We could either use a transactional Decorator [GOF] as a separate object or simply have the transactional methods delegate to the nontransactional versions of the methods on self. This approach is called a Poor Man’s Humble Object (see Humble Object).\n\nIf the methods exist but are not visible to the client, we will need to expose them to the test. We can do so either by making the methods to be tested pub- lic or by exposing them indirectly via a Test-Speciﬁ c Subclass (page 579). We could also do an Extract Testable Component (page 735) refactoring to move the nontransactional versions of the methods to a different class and make them visible to the test from there.\n\nAny reading of the updated data in the database must occur within the context of the same transaction. This normally is not a problem except when we are trying to simulate or test concurrency. If we are using an ORM layer such as Toplink, (N)Hibernate, or EJB 3.0, we may need to force the ORM to write the changes made to the objects to the database so that methods that read the database directly (from within the same transactional context) can see them. For example, EJB 3.0 provides the EntityManager.ﬂ ush static method for exactly this purpose.\n\nwww.it-ebooks.info\n\n671\n\nTransaction Rollback Teardown\n\n672\n\nTransaction Rollback Teardown\n\nChapter 25 Database Patterns\n\nMotivating Example\n\nThe following test attempts to use Guaranteed In-line Teardown (see In-line Teardown on page 509) to remove all the records it created:\n\npublic void testGetFlightsByOriginAirport_NoInboundFlights() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = createTestAirport(\"1IF\"); FlightDto expFlightDto = null; try { expFlightDto = createTestFlight(outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport( inboundAirport); // Verify Outcome assertEquals( 0, ﬂightsAtDestination1.size() ); } ﬁnally { facade.removeFlight( expFlightDto.getFlightNumber() ); facade.removeAirport( outboundAirport ); facade.removeAirport( inboundAirport ); } }\n\nThis code is neither easy to write nor correct!3 Trying to keep track of all objects the SUT has created and then tear them down one by one in a safe manner is very tricky.\n\nRefactoring Notes\n\nWe can avoid most of the issues related to coordinating In-line Teardown of multiple resources in a safe way by using Transaction Rollback Teardown and blasting away all changes to the objects in one fell swoop. Most of the refactor- ing work consists of deleting the existing teardown code from the ﬁ nally clause and inserting a call to the abortTransaction method. We also need to make the call to beginTransaction before we do any ﬁ xture setup, and we have to modify the Creation Methods (page 415) to ensure that they do not commit a transaction. To do so, we have them call a nontransactional version of the methods on the Service Facade.\n\n3 See In-line Teardown for an explanation of what is wrong here.\n\nwww.it-ebooks.info\n\nTransaction Rollback Teardown\n\nExample: Object Transaction Rollback Teardown\n\nHere is what the test looks like when we are done:\n\npublic void testGetFlightsByOrigin_NoInboundFlight_TRBTD() throws Exception { // Fixture Setup TransactionManager.beginTransaction(); BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expectedFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expectedFlightDto = createTestFlight( outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { TransactionManager.abortTransaction(); } }\n\nIn this refactored test, we have replaced the multiple lines of teardown code in the ﬁ nally clause with a single call to abortTransaction. We still need the ﬁ nally clause because this example is using In-line Teardown; we could easily move this call to the TransactionManager to the tearDown method because it is so generic.\n\nIn this example, Transaction Rollback Teardown undoes the ﬁ xture setup performed by the various Creation Methods we called earlier in the test. The ﬁ xture objects have not yet been committed to the database. Because getFlights- FromAirport is being called within the context of the transaction, however, it returns the newly added but not yet committed ﬂ ights. (That is the “C” for “consistent” in ACID working on our behalf!)\n\nprivate BigDecimal createTestAirport(String airportName) throws FlightBookingException { BigDecimal newAirportId = facade._createAirport( airportName, \" Airport\" + airportName, \"City\" + airportName); return newAirportId; }\n\nwww.it-ebooks.info\n\n673\n\nTransaction Rollback Teardown\n\n674\n\nTransaction Rollback Teardown\n\nChapter 25 Database Patterns\n\nThe creation method calls the nontransactional version of the facade method (an example of a Poor Man’s Humble Object):\n\npublic BigDecimal createAirport( String airportCode, String name, String nearbyCity) throws FlightBookingException{ TransactionManager.beginTransaction(); BigDecimal airportId = _createAirport(airportCode, name, nearbyCity); TransactionManager.commitTransaction(); return airportId; }\n\n// private, nontransactional version for use by tests BigDecimal _createAirport( String airportCode, String name, String nearbyCity) throws DataException, InvalidArgumentException { Airport airport = dataAccess.createAirport(airportCode,name,nearbyCity); logMessage(\"CreateFlight\", airport.getCode()); return airport.getId(); }\n\nIf the method we were exercising (e.g., getFlightsFromAirport) did modify the state of the SUT and did begin and end its own transaction, we would have to do a similar refactoring on it as well.\n\nExample: Database Transaction Rollback Teardown\n\nThe ﬁ rst example hid the database from the code behind a data access layer that returned or accepted objects. This is common practice when using the Domain Model [PEAA] pattern for organizing the business logic. Transaction Rollback Teardown is typically used when manipulating the database directly in our ap- plication logic (a style known as a Transaction Script [PEAA]). The following example illustrates this approach using .NET row sets (or something similar):\n\n[TestFixture] public class TransactionRollbackTearDownTest { private SqlConnection _Connection; private SqlTransaction _Transaction;\n\npublic TransactionRollbackTearDownTest() { }\n\n[SetUp]\n\nwww.it-ebooks.info\n\nTransaction Rollback Teardown\n\npublic void Setup() { string dbConnectionString = ConﬁgurationSettings. AppSettings.Get(\"DbConnectionString\"); _Connection = new SqlConnection(dbConnectionString); _Connection.Open(); _Transaction = _Connection.BeginTransaction(); }\n\n[TearDown] public void TearDown() { _Transaction.Rollback(); _Connection.Close(); // Avoid NUnit \"instance behavior\" bug _Transaction = null; _Connection = null; }\n\n[Test] public void AnNUnitTest() { const string C_INSERT_SQL = \"INSERT INTO Invoice(Amount, Tax, CustomerId)\" + \" VALUES({0}, {1}, {2})\"; SqlCommand cmd = _Connection.CreateCommand(); cmd.Transaction = _Transaction; cmd.CommandText = string.Format( C_INSERT_SQL, new object[] {\"100.00\", \"7.00\", 2001}); // Exercise SUT cmd.ExecuteNonQuery(); // Verify result // etc. } } }\n\nThis example uses Implicit Setup (page 424) to establish the connection and start the transaction. After the Test Method (page 348) has run, it uses Implicit Teardown (page 516) to roll back the transaction and close the connection. We assign null to the instance variables because NUnit does not create a separate Testcase Object (page 382) for each Test Method, unlike most other members of xUnit. See the sidebar “There’s Always an Exception” on page 384 for details.\n\nwww.it-ebooks.info\n\n675\n\nTransaction Rollback Teardown\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nChapter 26\n\nDesign-for-Testability Patterns\n\nPatterns in This Chapter\n\nDependency Injection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 678\n\nDependency Lookup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 686\n\nHumble Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 695\n\nTest Hook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 709\n\n677\n\nwww.it-ebooks.info\n\nDesign-for- Testability Patterns\n\n678\n\nDependency Injection\n\nChapter 26 Design-for-Testability Patterns\n\nDependency Injection\n\nHow do we design the SUT so that we can replace its dependencies at runtime?\n\nThe client provides the depended-on object to the SUT.\n\nClient Client\n\nCreation Creation\n\nDOC DOC\n\nUsage Usage\n\nUsage Usage\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nExercise Exercise\n\nCreation Creation\n\nSUT SUT\n\nUsage Usage\n\nTest Test Double Double\n\nAlmost every piece of code depends on some other classes, objects, modules, or procedures. To unit-test a piece of code properly, we would like to isolate the code from its dependencies. This isolation is difﬁ cult to achieve if those depen- dencies are hard-coded in the form of literal classnames.\n\nDependency Injection is a way to allow the normal coupling between a SUT\n\nand its dependencies to be broken during automated testing.\n\nHow It Works\n\nWe avoid hard-coding the names of classes on which we depend into our code by providing some other means for the client or system conﬁ guration to tell the SUT which objects to use for each dependency as it is executed. As part of the design of the SUT, we arrange to pass the dependency in to the SUT through the “front door.” That is, the means to specify the dependency becomes part of the API of the SUT. We can include it as an argument with each method call, include it on the constructor, or make it a settable attribute (property).\n\nWhen to Use It\n\nWe need to provide a means to substitute a depended-on component (DOC) to make it easy to use a Test Double (page 522) while testing our code. Static\n\nwww.it-ebooks.info\n\nDependency Injection\n\nbinding—that is, specifying exact types or classes at compile time—severely limits our options regarding how the software is conﬁ gured as it runs. Dynamic binding creates much more ﬂ exible software by deferring the decision of exactly which type or class to use until runtime. Dependency Injection is a good choice for com- municating which class to use when we are designing the software from scratch. It offers a natural way to design the code when we are doing test-driven development (TDD) because many of the tests we write for dependent objects seek to replace a DOC with a Test Double.\n\nWhen we don’t have complete control over the code we are testing, such as when we are retroﬁ tting tests to existing code,1 we may need to use some other means to introduce the Test Doubles. If the SUT uses Dependency Lookup (page 686) to ﬁ nd the DOC, we can override the lookup mechanism to return the Test Double. We can also use a Test-Speciﬁ c Subclass (page 579) of the SUT to return a Test Double as long as access to the DOC remains encapsulated behind a method call.\n\nImplementation Notes\n\nIntroducing Dependency Injection requires solving two problems. First, we must be able to use a Test Double wherever the real DOC is used. This constraint is primarily an issue in statically typed languages because we must convince the compiler to allow us to pass off a Test Double as the real thing. Second, we must provide a way to tell the SUT to use the Test Double.\n\nType Compatibility\n\nWhichever way we choose to install the dependency into the SUT, we must also ensure that the Test Double we want to replace it with is “type compatible” with the code that uses the Test Double. This is most easily accomplished if both the real component and the Test Double implement the same interface (in statically typed languages) or have the same signature (in dynamically typed languages). A quick way to introduce a Test Double into existing code is to do an Extract Interface [Fowler] refactoring on the real DOC and then have the Test Double implement the new interface.\n\nInstalling the Test Double\n\nThere are a number of different ways to tell the SUT to use the Test Double, but they all involve replacing a hard-coded name with a mechanism that determines the type of object to use at execution time. The three basic options are as follows:\n\n1 “If it ain’t broke, don’t change it (even to improve the testability)” is a common, albeit somewhat misguided, constraint in these circumstances.\n\nwww.it-ebooks.info\n\n679\n\nDependency Injection",
      "page_number": 715
    },
    {
      "number": 26,
      "title": "Design-for-Testability Patterns",
      "start_page": 743,
      "end_page": 776,
      "detection_method": "regex_chapter",
      "content": "680\n\nDependency Injection\n\nChapter 26 Design-for-Testability Patterns\n\nParameter Injection: We pass the dependency directly to the SUT as we\n\ninvoke it.\n\nConstructor Injection: We tell the SUT which DOC to use when we\n\nconstruct it.\n\nSetter Injection: We tell the SUT about the DOC sometime between\n\nwhen we construct it and when we exercise it.\n\nEach of these three variations of Dependency Injection can be hand-coded. Another option is to use an “Inversion of Control” (IoC) framework to link the various components together at runtime. This scheme avoids superﬂ uous diversity in how Dependency Injection is implemented across the application and can simplify the process of reconﬁ guring the application for different deployment models.\n\nVariation: Parameter Injection\n\nParameter Injection is a form of Dependency Injection in which the SUT does not keep or initialize a reference to the DOC; instead, it is passed in as an argument of the method being called on the SUT. All clients of the SUT—whether they are tests or production code—supply the DOC. As a consequence, the SUT is more indepen- dent of the context because it makes no assumptions about the dependency other than its usage interface. The main drawback is that Parameter Injection forces the client to know about the dependency, which is more appropriate in some circum- stances than in others. Most of the other variants of Dependency Injection move this knowledge somewhere other than the client or at least make it optional.\n\nParameter Injection is advocated by the original paper on Mock Objects (page 544) [ET]. It is especially effective when we are doing true TDD because that’s when we have the greatest control over the design. It is possible to introduce Parameter In- jection in an optional fashion by providing an alternative signature for the method in question with the extra parameter; we can then have the more traditional style method create the instance of the dependency and call the method that takes the de- pendency as a parameter.\n\nVariation: Constructor Injection\n\nBoth Constructor Injection and Setter Injection involve storing a reference to the DOC as an attribute (ﬁ eld or instance variable) of the SUT. With Dependency Injection, the ﬁ eld is initialized from a constructor argument. The SUT may optionally provide a simpler constructor that calls this constructor with the value normally used in production. When a test wants to replace the real DOC with a Test Double, it passes in the Test Double to the constructor when it builds the SUT.\n\nwww.it-ebooks.info\n\nDependency Injection\n\nThis approach to introducing Dependency Injection works well when the code includes only one or two constructors and they have small argument lists. Constructor Injection is the only approach that works if the DOC is an active object that creates its own thread of execution during construction; such behavior would make for Hard-to-Test Code (page 209), and we should probably consider turning it into a Humble Executable (see Humble Object on page 695). If we have a large number of dependencies as constructor argu- ments, we probably need to refactor the code to remove this code smell.\n\nVariation: Setter Injection\n\nAs with Constructor Injection, the SUT holds a reference to the DOC as an attri- bute (ﬁ eld) that is initialized in the constructor. Where it differs is that the attribute is exposed to the client either as a public attribute or via a “setter” method. When a test wants to replace the real DOC with a Test Double, it assigns to the exposed attribute (or calls the setter with) an instance of the Test Double. This approach works well when constructing the real DOC has no unpleasant side effects and assuming that nothing can happen automatically between the constructor call and the point at which the test calls the setter for the property. Setter Injection cannot be used if the SUT performs any signiﬁ cant processing in the constructor that relies on the dependency. In that case, we must use Constructor Injection. If constructing the real DOC has deleterious side effects, we can avoid creating it via the construc- tor by modifying the SUT to use Lazy Initialization [SBPP] to instantiate the DOC the ﬁ rst time the SUT needs to use it.\n\nRetroﬁ tting Dependency Injection\n\nWhen the SUT does not support any of these options “out of the box,” we may be able to retroﬁ t this capability via a Test-Speciﬁ c Subclass. If the actual class to be used is normally retrieved from conﬁ guration data, this retrieval should be done by some component other than the SUT and the class then passed to the SUT using Dependency Injection. Such a use of the Humble Object pattern for the client or conﬁ guration decouples the SUT from the environment and ensures that tests do not need to set up some external dependency (the conﬁ guration ﬁ le) to introduce the Test Double.\n\nAnother possibility is to use aspect-oriented programming (AOP) to insert the Dependency Injection mechanism into the development environment. For example, we might inject the decision to use the Test Double or inject the test- speciﬁ c logic—the Test Double—directly into the SUT. I don’t think we have enough experience with using AOP to call this a pattern just yet.\n\nwww.it-ebooks.info\n\n681\n\nDependency Injection\n\n682\n\nDependency Injection\n\nChapter 26 Design-for-Testability Patterns\n\nMotivating Example\n\nThe following test cannot be made to pass “as is”:\n\npublic void testDisplayCurrentTime_AtMidnight() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result); }\n\nThis test almost always fails because it depends on the current time being returned to the SUT by a DOC. The test cannot control the values being returned by that component, the DefaultTimeProvider. Therefore, this test will pass only when the system time is exactly midnight.\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar currentTime; try { currentTime = new DefaultTimeProvider().getTime(); } catch (Exception e) { return e.getMessage(); } // etc. }\n\nBecause the SUT is hard-coded to use a particular class to retrieve the time, we cannot replace the DOC with a Test Double. That constraint makes this test nondeterministic and pretty much useless. We need to ﬁ nd a way to gain control over the indirect inputs of the SUT.\n\nRefactoring Notes\n\nWe can use a Replace Dependency with Test Double (page 522) refactoring to gain control over the time. Setter Injection can be introduced into existing code if we have control over the code and the method in question is not widely used or if we have refactoring tools that support the Introduce Parameter [JBrains] refactor- ing. Failing that, we can use an Extract Method [Fowler] refactoring to create the new method signature that takes the Dependency Injection as an argument and leave the old method as an Adapter [GOF] that calls the new method.\n\nwww.it-ebooks.info\n\nDependency Injection\n\nExample: Parameter Injection\n\nHere’s the test rewritten to use Parameter Injection:\n\npublic void testDisplayCurrentTime_AtMidnight_PI() { // Fixture setup // Test Double instantiation TimeProvider tpStub = new MidnightTimeProvider(); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Exercise SUT using Test Double String result = sut.getCurrentTimeAsHtmlFragment(tpStub); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nIn this case, only the test will use the new signature. The existing code can use the old signature and the method adapter instantiates the real dependency object before passing it in.\n\npublic String getCurrentTimeAsHtmlFragment( TimeProvider timeProviderArg) { Calendar currentTime; try { currentTime = timeProviderArg.getTime(); } catch (Exception e) { return e.getMessage(); } // etc. }\n\nExample: Constructor Injection\n\nHere’s the same test rewritten to use Constructor Injection:\n\npublic void testDisplayCurrentTime_AtMidnight_CI() throws Exception { // Fixture setup // Test Double instantiation TimeProvider tpStub = new MidnightTimeProvider(); // Instantiate SUT injecting Test Double TimeDisplay sut = new TimeDisplay(tpStub); // Exercise SUT String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">12:01 AM</span>\"; // Verify outcome\n\nwww.it-ebooks.info\n\n683\n\nDependency Injection\n\n684\n\nDependency Injection\n\nChapter 26 Design-for-Testability Patterns\n\nassertEquals(\"12:01 AM\", expectedTimeString, sut.getCurrentTimeAsHtmlFragment()); }\n\nTo convert the SUT to use Constructor Injection, we can do an Introduce Field [JetBrains] refactoring to hold the DOC in a ﬁ eld that is initialized in the existing constructor. We can then do an Introduce Parameter refactoring to modify all callers of the existing constructor so that they pass the real DOC as a parameter of the constructor. If we cannot or do not want to modify all existing callers of the constructor, we can deﬁ ne a new constructor that takes the DOC as a parameter and modify the existing constructor to instantiate the real DOC and pass it in to our new constructor.\n\npublic class TimeDisplay {\n\nprivate TimeProvider timeProvider;\n\npublic TimeDisplay() { // backwards compatible constructor timeProvider = new DefaultTimeProvider(); } public TimeDisplay(TimeProvider timeProvider) { // new constructor this.timeProvider = timeProvider; }\n\nAnother approach is to do an Extract Method refactoring on the call to the con- structor and then use Move Method [Fowler] refactoring to move it to an Object Factory (see Dependency Lookup). That would result in Dependency Lookup.\n\nExample: Setter Injection\n\nHere is the same test refactored to use Setter Injection:\n\npublic void testDisplayCurrentTime_AtMidnight_SI() throws Exception { // Fixture setup // Test Double instantiation TimeProvider tpStub = new MidnightTimeProvider(); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Test Double installation sut.setTimeProvider(tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nwww.it-ebooks.info\n\nDependency Injection\n\nNote the call to setTimeProvider to install the Hard-Coded Test Double (page 568). If we had used a Conﬁ gurable Test Double (page 558), its conﬁ guration would occur immediately before the call to setTimeProvider.\n\nTo refactor the SUT to support Setter Injection, we can do an Introduce Field refactoring to hold the DOC in a variable that is initialized in the exist- ing constructor and call the DOC via this ﬁ eld. We can then expose the ﬁ eld either directly or via a setter so that the test can override its value. Here is the refactored version of the SUT:\n\npublic class TimeDisplay {\n\nprivate TimeProvider timeProvider;\n\npublic TimeDisplay() { timeProvider = new DefaultTimeProvider(); } public void setTimeProvider(TimeProvider provider) { this.timeProvider = provider; } public String getCurrentTimeAsHtmlFragment() throws TimeProviderEx { Calendar currentTime; try { currentTime = getTimeProvider().getTime(); } catch (Exception e) { return e.getMessage(); } // etc.\n\nHere we chose to use a getter to retrieve the DOC. We could just as easily have used the timeProvider ﬁ eld directly.\n\nwww.it-ebooks.info\n\n685\n\nDependency Injection\n\n686\n\nAlso known as: Service Locator, Object Factory, Component Broker, Component Registry\n\nDependency Lookup\n\nChapter 26 Design-for-Testability Patterns\n\nDependency Lookup\n\nHow do we design the SUT so that we can replace its dependencies at runtime?\n\nThe SUT asks another object to return the depended-on object before it uses it.\n\nConfiguration Configuration with Test Double with Test Double\n\nSetup Setup\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nFind or Create Find or Create\n\nCreation Creation\n\nor or\n\nDOC DOC\n\nTeardown Teardown\n\nCreation Creation\n\nClient Client\n\nUsage Usage\n\nSUT SUT\n\nUsage Usage\n\nUsage Usage\n\nTest Test Double Double\n\nAlmost every piece of code depends on some other classes, objects, modules, or procedures. To unit-test a piece of code properly, we would like to isolate it from its dependencies. Such isolation is difﬁ cult to achieve, however, if those depen- dencies are hard-coded within the code in the form of literal classnames.\n\nDependency Lookup is a way to allow the normal coupling between a SUT\n\nand its dependencies to be broken during automated testing.\n\nHow It Works\n\nWe avoid hard-coding the names of classes on which the SUT depends into our code because static binding severely limits our options regarding how the software is conﬁ gured as it runs. Instead, we hard-code that name of a “compo- nent broker” that returns a ready-to-use object. The component broker provides some means for the client software or perhaps a system conﬁ guration manager to tell the SUT in question which objects to use for each component request.\n\nwww.it-ebooks.info\n\nDependency Lookup\n\nWhen to Use It\n\nDependency Lookup is most appropriate when we need to retrieve DOCs from deep inside the system and it would be too messy to pass the Test Double (page 522) in from the client. A good example of such a situation is when we want to replace the data access layer of the system with a Fake Database (see Fake Object on page 551) or In-Memory Database (see Fake Object) to speed up execution of the automated customer tests. It would be too complex for each Subcutaneous Test (see Layer Test on page 337) to pass the Fake Database in through the Service Facade [CJ2EEP] and all the way down to the data access layer. Using Dependency Lookup allows the test or even a Setup Decorator (page 447) to use a “conﬁ guration facade” to install the Fake Database, which the SUT can magically use without any further ado. Jeremy Miller writes:\n\nYou cannot understate the value of using a Service Locator for automated testing. We routinely use alternative dependencies in testing, both to deal with difﬁ cult dependencies and for test performance. For example, in a functional test we’ll collapse a Web site and a backing application server into a single process for better performance.\n\nDependency Lookup tends to be a lot simpler to retroﬁ t onto existing legacy software because it affects only those places where object construction actually occurs; we do not need to modify every intermediate object or method, as we might have to do with Dependency Injection (page 678). It is also much simpler to retroﬁ t existing round-trip tests so that they use a Fake Object to speed them up by wrapping them in a Setup Decorator. With this scheme, we do not have to change each test; instead, we can create new instances of the SUT in each test and still have the test use the same Fake Object because the Service Locator remembers it across tests.2\n\nThe main alternative to Dependency Lookup is to provide a substitution mechanism within the SUT using Dependency Injection. This approach is gen- erally preferable for unit tests because it makes the replacement of the DOC more obvious and directly connected to exercising the SUT. Another option is to use AOP to install test-speciﬁ c logic using the development tools rather than modifying the design of the software. The least preferred solution is to use a Test Hook (page 709) within the SUT to avoid calling the DOC or within the DOC so that it behaves in a test-speciﬁ c way.\n\n2 We call these tests “bimodal” or “multimodal” because they can be run with both real and fake DOCs.\n\nwww.it-ebooks.info\n\n687\n\nDependency Lookup\n\n688\n\nDependency Lookup\n\nChapter 26 Design-for-Testability Patterns\n\nThe well-known intermediary may be called a “Service Locator,” “Object Factory,” “Component Broker,” or “Component Registry.” While these names imply different semantics (new versus existing objects), this need not be the case. For performance reasons, we may choose to return new objects from a “Service Locator” or “previously enjoyed” objects from an Object Factory. To simplify this discussion, the term “Component Broker” is used here.\n\nImplementation Notes\n\nA desire to use a Test Double when testing our code implies a need to make DOCs substitutable. This constraint rules out hard-coding the names of classes on which we depend into our code because static binding severely limits our options regard- ing how the software is conﬁ gured as it runs. One way to avoid this issue is to have the SUT delegate DOC fabrication to another object. Of course, this scheme implies we need a way to get a reference to that object. We solve this recursive problem by having a well-known object act as an intermediary between the test and the DOC. This well-known object is referenced by a hard-coded classname. To be useful for installing Test Doubles, this well-known object must supply a mechanism by which the test can specify the object to be returned.\n\nDependency Lookup has the following characteristics:\n\nEither a Singleton [GOF], a Registry [PEAA], or some kind of Thread-\n\nSpeciﬁ c Storage [POSA2]\n\nAn interface that fully encapsulates which implementation we are using\n\nA built-in substitution mechanism for replacing the returned object\n\nwith a Test Double\n\nAccess via well-known global name\n\nThe Dependency Lookup mechanism returns an object that can be used directly by the client. The nature of the actual object returned determines whether it is more appropriate to call it a “Service Locator” or an “Object Factory.” Once the object is retrieved, the SUT uses it directly. During testing, the test arranges for the Dependency Lookup mechanism to return a test-speciﬁ c object.\n\nEncapsulated Implementation\n\nA major requirement of Dependency Lookup is the existence of a well-known object to which we can delegate our requests for DOCs. This well-known\n\nwww.it-ebooks.info\n\nDependency Lookup\n\nobject could be a Singleton, a Registry, or some kind of Thread-Speciﬁ c Storage mechanism.3\n\nThe “Component Broker” should encapsulate its implementation from the client (our SUT). That is, the interface provided by the “Component Broker” should not expose whether it is a Singleton or a Registry or whether some type of Thread-Speciﬁ c Storage mechanism is in use under the covers. In fact, the test environment may want to provide a different implementation speciﬁ cally to avoid issues caused by Singletons in tests, such as a Substitutable Singleton (see Test-Speciﬁ c Subclass on page 579).\n\nSubstitution Mechanism\n\nWhen a test wants to replace the real DOC with a Test Double, it needs a way to tell the “Component Broker” that a Test Double should be returned instead of the real component. The “Component Broker” may provide a conﬁ guration interface to conﬁ gure it with the object to be returned or the test can replace the component Registry with a suitable Test-Speciﬁ c Subclass. It may also need to provide a way to restore the original or default conﬁ guration of the broker so that the conﬁ guration used in one test does not “leak” into another test, effec- tively changing the “Component Broker” into a Shared Fixture (page 317).\n\nA less desirable conﬁ guration alternative is to have the “Component Broker” read the classnames to be constructed for each request from a conﬁ guration ﬁ le. This approach poses several problems, however. First, the test must write the ﬁ le as part of ﬁ xture setup unless the test offers a way to replace the ﬁ le access mechanism. This is sure to result in Slow Tests (page 253). Second, this scheme will not work with Conﬁ gurable Test Doubles (page 558) unless the conﬁ gura- tion ﬁ le can also provide initialization data for the object. Finally, the need to write a ﬁ le opens the door to Interacting Tests (see Erratic Test on page 228) because different tests may need different conﬁ guration information.\n\nIf the “Component Broker” must return objects based on conﬁ guration data, a better solution is to have a separate Humble Object (page 695) read the ﬁ le and call a conﬁ guration interface on the “Component Broker.” The test can then use this same interface to conﬁ gure the broker on a per-test basis.\n\n3 The main difference is that a Singleton has only a single instance, whereas a Registry makes no such promise. Thread-Speciﬁ c Storage allows objects to access “global” data via a well-known object, where the data accessed is speciﬁ c to a particular thread; the same object might retrieve different data depending on which thread is being run.\n\nwww.it-ebooks.info\n\n689\n\nDependency Lookup\n\n690\n\nDependency Lookup\n\nChapter 26 Design-for-Testability Patterns\n\nMotivating Example\n\nThe following test cannot be made to pass “as is”:\n\npublic void testDisplayCurrentTime_AtMidnight() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result); }\n\nThis test almost always fails because it assumes that the current time will be returned to the SUT by a DOC. The test cannot control which values are returned by that component (the DefaultTimeProvider), however, so this test will pass only when the system time is exactly midnight.\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar currentTime; try { currentTime = new DefaultTimeProvider().getTime(); } catch (Exception e) { return e.getMessage(); } // etc. }\n\nBecause the SUT is hard-coded to use a particular class to retrieve the time, we cannot replace the DOC with a Test Double. That makes this test nondeter- ministic and pretty much useless. We need to ﬁ nd a way to gain control over the indirect inputs of the SUT.\n\nRefactoring Notes\n\nThe ﬁ rst step to making this behavior testable is to replace the hard-coded classname with a call to a “Service Locator”:\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar currentTime; try { TimeProvider timeProvider = (TimeProvider) ServiceLocator.getInstance(). ﬁndService(\"Time\"); currentTime = timeProvider.getTime(); } catch (Exception e) { return e.getMessage(); } // etc.\n\nwww.it-ebooks.info\n\nDependency Lookup\n\nAlthough we could have provided a class method to avoid the chained method calls, that step would just move the getInstance into the class method. The next refactoring step depends on whether we have a conﬁ guration interface on our “Service Locator.” If it makes sense to conﬁ gure the production version of the “Service Locator,” we can introduce the conﬁ guration mechanism directly into it (as illustrated in the next example). Otherwise, we can simply override what the Service Locator returns in a Test-Speciﬁ c Subclass (as illustrated in the sec- ond example).\n\nExample: Conﬁ gurable Registry\n\nThis version of the test has been modiﬁ ed to use the conﬁ guration interface on the “Service Locator” to install a Test Double:\n\npublic void testDisplayCurrentTime_AtMidnight_CSL() { // Fixture setup // Test Double conﬁguration MidnightTimeProvider tpStub = new MidnightTimeProvider(); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Test Double installation ServiceLocator.getInstance().registerServiceForName(tpStub, \"Time\"); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nThe code in the SUT was described previously. The code for the Conﬁ guration Interface (see Conﬁ gurable Test Double) of the Conﬁ gurable Registry follows:\n\npublic class ServiceLocator { protected ServiceLocator() {};\n\nprotected static ServiceLocator soleInstance = null;\n\npublic static ServiceLocator getInstance() { if (soleInstance==null) soleInstance = new ServiceLocator(); return soleInstance; }\n\nprivate HashMap providers = new HashMap();\n\npublic ServiceProvider ﬁndService(String serviceName) { return (ServiceProvider) providers.get(serviceName);\n\nwww.it-ebooks.info\n\n691\n\nDependency Lookup\n\n692\n\nDependency Lookup\n\nChapter 26 Design-for-Testability Patterns\n\n}\n\n// conﬁguration interface public void registerServiceForName( ServiceProvider provider, String serviceName) { providers.put( serviceName, provider); } }\n\nThe interesting thing about this example is our use of a Conﬁ guration Interface on a production class rather than a Test Double. In fact, the Conﬁ gurable Registry avoids the need to use a Test Double by providing the test with a mechanism to alter the service component the Conﬁ gurable Registry returns.\n\nExample: Substituted Singleton\n\nThis version of the test deals with a nonconﬁ gurable Dependency Lookup mechanism by replacing the soleInstance of the “Service Locator” with a Sub- stituted Singleton (see Test-Speciﬁ c Subclass). To ensure the reusability of the conﬁ guration interface of the Substituted Singleton, we pass the TimeProvider Test Stub (page 529) as an argument to overrideSoleInstance.\n\npublic void testDisplayCurrentTime_AtMidnight_TSS() { // Fixture setup // Test Double conﬁguration MidnightTimeProvider tpStub = new MidnightTimeProvider();\n\n// Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Test Double installation // Replaces the entire Service Locator with one that // always returns our Test Stub ServiceLocatorTestSingleton.overrideSoleInstance(tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nNote how the test overrides the object normally returned by getInstance with an instance of a Test-Speciﬁ c Subclass. The code for the Singleton follows:\n\npublic class ServiceLocator { protected ServiceLocator() {};\n\nprotected static ServiceLocator soleInstance = null;\n\nwww.it-ebooks.info\n\nDependency Lookup\n\npublic static ServiceLocator getInstance() { if (soleInstance==null) soleInstance = new ServiceLocator(); return soleInstance; }\n\nprivate HashMap providers = new HashMap();\n\npublic ServiceProvider ﬁndService(String serviceName) { return (ServiceProvider) providers.get(serviceName); } }\n\nNote that we had to make the constructor and soleInstance protected rather than private to allow them to be overridden by the subclass. Finally, here is the code for the Substituted Singleton:\n\npublic class ServiceLocatorTestSingleton extends ServiceLocator { private ServiceProvider tpStub;\n\nprivate ServiceLocatorTestSingleton(TimeProvider newTpStub) { this.tpStub = newTpStub; };\n\n// Installation interface static ServiceLocatorTestSingleton overrideSoleInstance(TimeProvider tpStub) { // We could save the real instance before reassigning // soleInstance so we could restore it later, but we'll // forego that complexity for this example soleInstance = new ServiceLocatorTestSingleton( tpStub); return (ServiceLocatorTestSingleton) soleInstance; }\n\n// Overridden superclass method public ServiceProvider ﬁndService(String serviceName) { return tpStub; // Hard-coded; ignores serviceName } }\n\nBecause it cannot see the private HashMap of providers, this code simply returns the contents of the tpStub ﬁ eld that it initialized in the constructor.\n\nAbout the Name\n\nChoosing a name for this pattern was tough. Service Locator and Component Broker were already in widespread use. Both are good names for use in their particular circumstance. Unfortunately, neither name can encompass the other, so I had to come up with another name that uniﬁ ed the two major variants.\n\nwww.it-ebooks.info\n\n693\n\nDependency Lookup\n\n694\n\nDependency Lookup\n\nChapter 26 Design-for-Testability Patterns\n\nThe name Dependency Injection was already in widespread use for the alter- nate pattern; a desire for consistency with that name led to using Dependency Lookup. See the sidebar “What’s in a (Pattern) Name?” on page 576 for more on this decision-making process.\n\nwww.it-ebooks.info\n\nHumble Object\n\nHumble Object\n\nHow can we make code testable when it is too closely coupled to its environment?\n\nWe extract the logic into a separate, easy-to-test component that is decoupled from its environment.\n\nFixture Fixture\n\nHumble Humble Object Object\n\nImpossible Impossible Dependency Dependency\n\nSetup Setup\n\nExercise Exercise\n\nTestable Testable\n\nVerify Verify\n\nComponent Component\n\nTeardown Teardown\n\nWe are often faced with trying to test software that is closely coupled to some kind of framework. Examples include visual components (e.g., widgets, dialogs) and transactional component plug-ins. Testing these objects is difﬁ cult because constructing all the objects with which our SUT needs to interact may be expensive—or even impossible. In other cases, objects may be hard to test because they run asynchronously; examples include active objects (e.g., threads, processes, Web servers) and user interfaces. These objects’ asynchronicity intro- duces uncertainty, a requirement for interprocess coordination, and the need for delays into tests. Faced with these thorny issues, developers often just give up on testing this kind of code. The result: Production Bugs (page 268) caused by Untested Code and Untested Requirements.\n\nHumble Object is a way to bring the logic of these hard-to-instantiate objects\n\nunder test in a cost-effective manner.\n\nwww.it-ebooks.info\n\n695\n\nHumble Object\n\n696\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nHow It Works\n\nWe extract all the logic from the hard-to-test component into a component that is testable via synchronous tests. This component implements a service interface consisting of methods that expose the logic of the untestable component; the only difference is that these methods are accessible via synchronous method calls. As a result, the Humble Object component becomes a very thin adapter layer that contains very little code. Each time the framework calls the Humble Object, this object delegates its responsibilities to the testable component. If the testable component needs any information from the context, the Humble Object is responsible for retrieving it and passing it to the testable component. The Humble Object code is typically so simple that we often don’t bother writing tests for it because it can be quite difﬁ cult to set up the environment needed to run those tests.\n\nWhen to Use It\n\nWe can and should introduce a Humble Object whenever we have nontrivial logic in a component that is problematic to instantiate because it depends on a framework or can be accessed only asynchronously. There are lots of reasons for objects being hard to test; consequently, there are lots of variations in how we break the dependencies that are required. The following variations are the most common examples of Humble Object—but we shouldn’t be surprised if we sometimes need to invent our own variation.\n\nVariation: Humble Dialog\n\nGraphical user interface (GUI) frameworks require us to provide objects to represent our pages and controls. These objects provide logic to translate user actions into the underlying system actions and to translate the system responses back into user recognizable behavior. This logic may involve invoking the application behind the user interface and/or modifying the state of this or other visual objects.\n\nVisual objects are very difﬁ cult to test efﬁ ciently because they are tightly coupled to the presentation framework that invokes them. To be effective, a test would need to simulate that environment to provide the visual object with all the information and facilities it requires. Further complicating the issue is the fact that these frameworks often run in their own thread of control, which means that we must use asynchronous tests. These tests are challenging to write, and they often result in Slow Tests (page 253) and Nondeterministic Tests (see Erratic Test on page 228). Under these circumstances, we may beneﬁ t by using\n\nwww.it-ebooks.info\n\nHumble Object\n\na Humble Object to move all of the controller and view-updating logic out of the framework-dependent object and into a testable object.\n\nVariation: Humble Executable\n\nMany programs contain active objects. Active objects have their own thread of execution so they can do things in parallel with other activities of the system. Examples of active objects include anything that runs in a separate process (e.g., Windows applications in .exe ﬁ les) or thread (in Java, any object that imple- ments Runnable). These objects may be launched directly by the client, or they may be started automatically, process requests from a queue, and send replies via a return message. Either way, we must write asynchronous tests (complete with interprocess coordination and/or explicit delays and Neverfail Tests; see Production Bugs) to verify their behavior.\n\nThe Humble Executable pattern provides a way to bring the logic of the exe- cutable under test without incurring the delays that might otherwise lead to Slow Tests and Nondeterministic Tests. We extract all the logic from the executable into a component that is testable via synchronous tests. This component implements a service interface consisting of methods that expose all logic of the executable; the only difference is that these methods are accessible via synchronous method calls. The testable component may be a Windows DLL, a Java JAR containing a Service Facade [CJ2EEP] class, or some other language component or class that exposes the services of the executable in a testable way.\n\nThe Humble Executable component itself contains very little code. All it does in its thread of control is to load the testable component (if a True Hum- ble Object) and delegate to it. As a result, the Humble Executable requires only one or two tests to verify that it performs this load/delegate function correctly. Although these tests still take seconds to execute, they have a much smaller impact on the overall test suite execution time because so few of them exist. Given that this code will not change very often, these tests can even be omitted from the suite of tests that developers execute before check-in to speed up test suite execution times. Of course, we would still prefer to run the Humble Executable tests as part of the automated build process.\n\nVariation: Humble Transaction Controller\n\nMany applications use databases to persist their state. Fixture setup with databases can be slow and complex, and leftover ﬁ xtures can wreak havoc with subsequent tests and test runs. If we are using a Shared Fixture (page 317), the ﬁ xture’s persis- tence may lead to Erratic Tests. Humble Transaction Controller facilitates testing of the logic that runs within the transaction by making it possible for the test to control\n\nwww.it-ebooks.info\n\n697\n\nHumble Object\n\n698\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nthe transaction. As a consequence, we can exercise the logic, verify the outcome, and then abort the transaction, leaving no trace of our activity in the database.\n\nTo implement Humble Transaction Controller, we use an Extract Method [Fowler] refactoring to move all the logic we want to test out of the code that controls the transaction and into a separate method that knows nothing about transaction control and that can be called by the test. Because the caller con- trols the transaction, the test can start, commit (if it so chooses), and (most commonly) roll back the transaction. In this case, the behavior—not the dependencies—causes us to bypass the Humble Object when we are testing the business logic. As a result, we are more likely to be able to get away with a Poor Man’s Humble Object.\n\nAs for the Humble Object, it contains no business logic. Thus the only behavior that needs to be tested is whether the Humble Object commits and rolls back the transaction properly based on the outcome of the methods it calls. We can write a test that replaces the testable component with a Test Stub (page 529) that throws an exception and then verify that this activity results in a rollback of the transac- tion. If we are using a Poor Man’s Humble Object, the stub would be implemented as a Subclassed Test Double (see Test-Speciﬁ c Subclass on page 579) that overrides the “real” methods with methods that throw exceptions.\n\nMany of the major application server technologies support this pattern either directly or indirectly by taking transaction control away from the business objects that we write. If we are building our software without using a transaction control framework, we may need to implement our own Humble Transaction Controller. See the “Implementation Notes” section for some ideas on how we can enforce the separation.\n\nVariation: Humble Container Adapter\n\nSpeaking of “containers,” we often have to implement speciﬁ c interfaces to allow our objects to run inside an application server (e.g., the “EJB session bean” interface). Another variation on the Humble Object pattern is to design our objects to be container-independent and then have a Humble Container Adapter adapt them to the interface required by container. This strategy makes our logic components easy to test outside the container, which dramatically reduces the time required for an “edit–compile–test” cycle.\n\nImplementation Notes\n\nWe can make the logic that normally runs inside the Humble Object testable in several different ways. All of these techniques share one commonality: They in- volve exposing the logic so that it can be veriﬁ ed using synchronous tests. They\n\nwww.it-ebooks.info\n\nHumble Object\n\nvary, however, in terms of how the logic is exposed. Regardless of how logic ex- posure occurs, test-driven purists would prefer that tests verify that the Humble Object is calling the extracted logic properly. This can be done by replacing the real logic methods with some kind of Test Double (page 522) implementation.\n\nVariation: Poor Man’s Humble Object\n\nThe simplest way to isolate and expose each piece of logic we want to verify is to place it into a separate method. We can do so by using an Extract Method refactoring on in-line logic and then making the resulting method visible from the test. Of course, this method cannot require anything from the context. Ideally everything the method needs to do its work will be passed in as arguments but this information could also be placed in ﬁ elds. Problems may arise if the testable com- ponent needs to call methods to access information it needs and those methods are dependent on the (nonexistent/faked) context, as this dependency makes writing the tests more complex.\n\nThis approach, which constitutes the “poor man’s” Humble Object, works well if no obstacles prevent the instantiation of the Humble Object (e.g., automatically starting its thread, no public constructor, unsatisﬁ able dependencies). Use of a Test- Speciﬁ c Subclass can also help break these dependencies by providing a test-friendly constructor and exposing private methods to the test.\n\nWhen testing a Subclassed Humble Object or a Poor Man’s Humble Object, we can build the Test Spy (page 538) as a Subclassed Test Double of the Humble Object to record when the methods in question were called. We can then use assertions within the Test Method (page 348) to verify that the values recorded match the values expected.\n\nVariation: True Humble Object\n\nAt the other extreme, we can put the logic we want to test into a separate class and have the Humble Object delegate to an instance of it. This approach, which was implied in the introduction to this pattern, will work in almost any circum- stance where we have complete control over the code.\n\nSometimes the host framework requires that its objects hold certain responsi- bilities that we cannot move elsewhere. For example, a GUI framework expects its view objects to contain data for the controls of the GUI and the data that those controls display on the screen. In these cases we must either give the test- able object a reference to the Humble Object and have it manipulate the data for that object or put some minimal update logic in the Humble Object and accept that it won’t be covered by automated tests. The former approach is almost always possible and is always preferable.\n\nwww.it-ebooks.info\n\n699\n\nHumble Object\n\n700\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nTo refactor to a True Humble Object, we normally do a series of Extract Method refactorings to decouple the public interface of the Humble Object from the implementation logic we plan to delegate. Then we do an Extract Class [Fowler] refactoring to move all the methods—except the ones that deﬁ ne the public interface of the Humble Object—to the new “testable” class. We introduce an attribute (a ﬁ eld) to hold a reference to an instance of the new class and initial- ize it to an instance of the new class either as part of the constructor or using Lazy Initialization [SBPP] in each interface method.\n\nWhen testing a True Humble Object (where the Humble Object delegates to a separate class), we typically use a Lazy Mock Object (see Mock Object on page 544) or Test Spy to verify that the extracted class is called correctly. By contrast, using the more common Active Mock Object (see Mock Object) is problematic in this situation because the assertions are made on a different thread from the Testcase Object (page 382) and failures won’t be detected unless we ﬁ nd a way to channel them back to the test thread.\n\nTo ensure that the extracted testable component is instantiated properly, we can use an observable Object Factory (see Dependency Lookup on page 686) to construct the extracted component. The test can register as a listener to verify the correct method is called on the factory. We can also use a regular factory object and replace it during the test with a Mock Object or Test Stub to monitor which factory method was called.\n\nVariation: Subclassed Humble Object\n\nIn between the extremes of the Poor Man’s Humble Object and the True Humble Object are approaches that involve clever use of subclassing to put the logic into separate classes while still allowing them to be on a single object. A number of different ways to do this are possible, depending on whether the Humble Object class needs to subclass a speciﬁ c framework class. I won’t go into a lot of detail here as this technique is very speciﬁ c to the language and runtime environment. Nevertheless, you should recognize that the basic options are either having the framework-dependent class inherit the logic to be tested from a superclass or having the class delegate to an abstract method that is implemented by a subclass.\n\nMotivating Example (Humble Executable)\n\nIn this example, we are testing some logic that runs in its own thread and processes each request as it arrives. In each test, we start up the thread, send it some messages, and wait long enough so that our assertions pass. Unfortu- nately, it takes several seconds for the thread to start up, become initialized,\n\nwww.it-ebooks.info\n\nHumble Object\n\nand process the ﬁ rst request. Thus the test fails sporadically unless we include a two-second delay after starting the thread.\n\npublic class RequestHandlerThreadTest extends TestCase { private static ﬁnal int TWO_SECONDS = 3000;\n\npublic void testWasInitialized_Async() throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); // Exercise sut.start(); // Verify Thread.sleep(TWO_SECONDS); assertTrue(sut.initializedSuccessfully()); }\n\npublic void testHandleOneRequest_Async() throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); sut.start(); // Exercise enqueRequest(makeSimpleRequest()); // Verify Thread.sleep(TWO_SECONDS); assertEquals(1, sut.getNumberOfRequestsCompleted()); assertResponseEquals(makeSimpleResponse(), getResponse()); } }\n\nIdeally, we would like to test the thread with each kind of transaction individu- ally to achieve better Defect Localization (see page 22). Unfortunately, if we did so our test suite would take many minutes to run because each test includes a delay of several seconds. Another problem is that the tests won’t result in an error if our active object has an exception in its own thread.\n\nA two-second delay may not seem like a big deal, but consider what happens when we have a dozen such tests. It would take us almost half a minute to run these tests. Contrast this performance with that of normal tests—we can run several hundred of those tests each second. Testing via the executable is affecting our productivity negatively. For the record, here’s the code for the executable:\n\npublic class RequestHandlerThread extends Thread { private boolean _initializationCompleted = false; private int _numberOfRequests = 0;\n\npublic void run() { initializeThread(); processRequestsForever(); }\n\nwww.it-ebooks.info\n\n701\n\nHumble Object\n\n702\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\npublic boolean initializedSuccessfully() { return _initializationCompleted; }\n\nvoid processRequestsForever() { Request request = nextMessage(); do { Response response = processOneRequest(request); if (response != null) { putMsgOntoOutputQueue(response); } request = nextMessage(); } while (request != null); } }\n\nTo avoid the distraction of the business logic, I have already used an Extract Method refactoring to move the real logic into the method processOneRequest. Likewise, the actual initialization logic is not shown here; sufﬁ ce it to say that this logic sets the variable _initializationCompleted when it ﬁ nishes successfully.\n\nRefactoring Notes\n\nTo create a Poor Man’s Humble Object, we expose the methods to make them visible from the test. (If the code used in-line logic, we would do an Extract Method refactoring ﬁ rst.) If there were any dependencies on the context, we would need to do an Introduce Parameter [JBrains] refactoring or an Introduce Field [JetBrains] refactoring so that the processOneRequest method need not access anything from the context.\n\nTo create a true Humble Object, we can do an Extract Class refactoring on the executable to create the testable component, leaving behind just the Humble Object as an empty shell. This step typically involves doing the Extract Method refactoring described above to separate the logic we want to test (e.g., the initializeThread method and the processOneRequest method) from the logic that interacts with the context of the executable. We then do an Extract Class refactoring to introduce the testable component class (essentially a single Strategy [GOF] object) and move all methods except the public interface methods over to it. The Extract Class refac- toring includes introducing a ﬁ eld to hold a reference to the new object and creating an instance. It also includes ﬁ xing all of the public methods so that they call the methods that were moved to the new testable class.\n\nwww.it-ebooks.info\n\nHumble Object\n\nExample: Poor Man’s Humble Executable\n\nHere is the same set of tests rewritten as a Poor Man’s Humble Object:\n\npublic void testWasInitialized_Sync() throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); // Exercise sut.initializeThread(); // Verify assertTrue(sut.initializedSuccessfully()); }\n\npublic void testHandleOneRequest_Sync() throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); // Exercise Response response = sut.processOneRequest(makeSimpleRequest()); // Verify assertEquals(1, sut.getNumberOfRequestsCompleted()); assertResponseEquals(makeSimpleResponse(), response); }\n\nHere, we have made the methods initializeThread and processOneRequest public so that we can call them synchronously from the test. Note the absence of a delay in this test. This approach works well as long as we can instantiate the executable component easily.\n\nExample: True Humble Executable\n\nHere is the code for our SUT refactored to use a True Humble Executable:\n\npublic class HumbleRequestHandlerThread extends Thread implements Runnable { public RequestHandler requestHandler;\n\npublic HumbleRequestHandlerThread() { super(); requestHandler = new RequestHandlerImpl(); }\n\npublic void run() { requestHandler.initializeThread(); processRequestsForever(); }\n\npublic boolean initializedSuccessfully() {\n\nwww.it-ebooks.info\n\n703\n\nHumble Object\n\n704\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nreturn requestHandler.initializedSuccessfully(); }\n\npublic void processRequestsForever() { Request request = nextMessage(); do { Response response = requestHandler.processOneRequest(request); if (response != null) { putMsgOntoOutputQueue(response); } request = nextMessage(); } while (request != null); }\n\nHere, we have moved the method processOneRequest to a separate class that we can instantiate easily. Below is the same test rewritten to take advantage of the extracted component. Note the absence of a delay in this test.\n\npublic void testNotInitialized_Sync() throws InterruptedException { // Setup/Exercise RequestHandler sut = new RequestHandlerImpl(); // Verify assertFalse(\"init\", sut.initializedSuccessfully()); }\n\npublic void testWasInitialized_Sync() throws InterruptedException { // Setup RequestHandler sut = new RequestHandlerImpl(); // Exercise sut.initializeThread(); // Verify assertTrue(\"init\", sut.initializedSuccessfully()); }\n\npublic void testHandleOneRequest_Sync() throws InterruptedException { // Setup RequestHandler sut = new RequestHandlerImpl(); // Exercise Response response = sut.processOneRequest( makeSimpleRequest() ); // Verify assertEquals( 1, sut.getNumberOfRequestsDone()); assertResponseEquals( makeSimpleResponse(), response); }\n\nBecause we have introduced delegation to another object, we should probably verify that the delegation occurs properly. The next test veriﬁ es that the Humble\n\nwww.it-ebooks.info\n\nHumble Object\n\nObject calls the initializeThread method and the processOneRequest method on the newly created testable component:\n\npublic void testLogicCalled_Sync() throws InterruptedException { // Setup RequestHandlerRecordingStub mockHandler = new RequestHandlerRecordingStub(); HumbleRequestHandlerThread sut = new HumbleRequestHandlerThread(); // Mock Installation sut.setHandler( mockHandler ); sut.start(); // Exercise enqueRequest(makeSimpleRequest()); // Verify Thread.sleep(TWO_SECONDS); assertTrue(\"init\", mockHandler.initializedSuccessfully() ); assertEquals( 1, mockHandler.getNumberOfRequestsDone() ); }\n\nNote that this test does require at least a small delay to allow the thread to start up. The delay is shorter, however, because we have replaced the real logic component with a Test Double that responds instantly and only one test now requires the delay. We could even move this test to a separate test suite that is run less frequently (e.g., only during the automated build process) to ensure that all tests performed before each check-in run quickly.\n\nThe other signiﬁ cant thing to note is that we are using a Test Spy rather than a Mock Object. Because the assertions done by the Mock Object would be raised in a different thread from the Test Method, the Test Automation Frame- work (page 298)—in this example, JUnit—won’t catch them. As a consequence, the test might indicate “pass” even though assertions in the Mock Object are failing. By making the assertions in the Test Method, we avoid having to do something special to relay the exceptions thrown by the Mock Object back to the thread in which the Test Method is executing.\n\nThe preceding test veriﬁ ed that our Humble Object actually delegates to the Test Spy that we have installed. It would also be a good idea to verify that our Humble Object actually initializes the variable holding the delegate to the appropriate class. Here’s a simple way to do so:\n\npublic void testConstructor() { // Exercise HumbleRequestHandlerThread sut = new HumbleRequestHandlerThread(); // Verify String actualDelegateClass = sut.requestHandler.getClass().getName(); assertEquals( RequestHandlerImpl.class.getName(), actualDelegateClass); }\n\nwww.it-ebooks.info\n\n705\n\nHumble Object\n\n706\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nThis Constructor Test (see Test Method) veriﬁ es that a speciﬁ c attribute has been initialized.\n\nExample: Humble Dialog\n\nMany development environments let us build the user interface visually by dragging and dropping various objects (“widgets”) onto a canvas. They let us add behavior to these visual objects by selecting one of several possible actions or events speciﬁ c to that visual object and typing logic into the code window presented by the IDE. This logic may involve invoking the application behind the user interface or it may involve modifying the state of this or some other visual object.\n\nVisual objects are very difﬁ cult to test efﬁ ciently because they are tightly coupled to the presentation framework that invokes them. To provide the visual object with all the information and facilities it requires, the test would need to simulate that environment—quite a challenge. This makes testing very complicated, so much so that many development teams don’t bother testing the presentation logic at all. This lack of testing, not surprisingly, often leads to Production Bugs caused by untested code and Untested Requirements.\n\nTo create the Humble Dialog, we extract all the logic from the view com- ponent into a nonvisual component that is testable via synchronous tests. If this component needs to update the view object’s (Humble Dialog’s) state, the Humble Dialog is passed in as an argument. When testing the nonvisual com- ponent, we typically replace the Humble Dialog with a Mock Object that is conﬁ gured with the indirect input values and the expected behavior (indirect outputs). In GUI frameworks that require the Humble Dialog to register itself with the framework for each event it wishes to see, the nonvisual component can register itself instead of the Humble Dialog (as long as that doesn’t introduce unmanageable dependencies on the context). This ﬂ exibility makes the Humble Dialog even simpler because the events go directly to the nonvisual component and require no delegation logic.\n\nThe following code sample is taken from a VB view component (.ctl) that includes some nontrivial logic. It is part of a custom plug-in we built for Mercury Interactive’s TestDirector tool.\n\n' Interface method, TestDirector will call this method ' to display the results. Public Sub ShowResultEx(TestSetKey As TdTestSetKey, _ TSTestKey As TdTestKey, _ ResultKey As TdResultKey)\n\nwww.it-ebooks.info\n\nHumble Object\n\nDim RpbFiles As OcsRpbFiles Set RpbFiles = getTestResultFileNames(ResultKey) ResultsFileName = RpbFiles.ActualResultFileName ShowFileInBrowser ResultsFileName End Sub\n\nFunction getTestResultFileNames(ResultKey As Variant) As OcsRpbFiles On Error GoTo Error Dim Attachments As Collection Dim thisTest As Run Dim RpbFiles As New OcsRpbFiles\n\nCall EnsureConnectedToTd\n\nSet Attachments = testManager.GetAllAttachmentsOfRunTest(ResultKey) Call RpbFiles.LoadFromCollection(Attachments, \"RunTest\") Set getTestResultFileNames = RpbFiles Exit Function Error: ' do something ... End Function\n\nIdeally, we would like to test the logic. Unfortunately, we cannot construct the objects passed in as parameters because they don’t have public constructors. Passing in objects of some other type isn’t possible either, because the types of the function parameters are hard-coded to be speciﬁ c concrete classes.\n\nWe can do an Extract Testable Component (page 735) refactoring on the ex- ecutable to create the testable component, leaving behind just the Humble Dialog as an empty shell. This approach typically involves doing several Extract Method refactorings (already done in the original example to make the refactoring easier to understand), one for each chunk of logic that we want to move. We then do an Extract Class refactoring to create our new testable component class. The Extract Class refactoring may include both Move Method [Fowler] and Move Field [Fowler] refactorings to move the logic and the data it requires out of the Humble Dialog and into the new testable component. Here’s the same view converted to a Humble Dialog:\n\n' Interface method, TestDirector will call this method ' to display the results. Public Sub ShowResultEx(TestSetKey As TdTestSetKey, _ TSTestKey As TdTestKey, _ ResultKey As TdResultKey) Dim RpbFiles As OcsRpbFiles Call EnsureImplExists Set RpbFiles = Implementation.getTestResultFileNames(ResultKey)\n\nwww.it-ebooks.info\n\n707\n\nHumble Object\n\n708\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nResultsFileName = RpbFiles.ActualResultFileName ShowFileInBrowser ResultsFileName End Sub\n\nPrivate Sub EnsureImplExists() If Implementation Is Nothing Then Set Implementation = New OcsScriptViewerImpl End If End Sub\n\nHere’s the testable component OcsScriptViewerImpl that the Humble Object calls:\n\n' ResultViewer Implementation: Public Function getTestResultFileNames(ResultKey As Variant) As OcsRpbFiles On Error GoTo Error\n\nDim Attachments As Collection Dim thisTest As Run Dim RpbFiles As New OcsRpbFiles\n\nCall EnsureConnectedToTd\n\nSet Attachments = testManager.GetAllAttachmentsOfRunTest(ResultKey) Call RpbFiles.LoadFromCollection(Attachments, \"RunTest\") Set getTestResultFileNames = RpbFiles Exit Function Error: ' do something ... End Function\n\nWe could now instantiate this OcsScriptViewerImpl class easily and write VbUnit tests for it. I’ve omitted the tests for space reasons because they don’t really show anything particularly interesting.\n\nExample: Humble Transaction Controller\n\nTransaction Rollback Teardown (page 668) contains an example of writing tests that bypass the Humble Transaction Controller.\n\nFurther Reading\n\nSee http://www.objectmentor.com/resources/articles/TheHumbleDialogBox.pdf for Michael Feathers’ original write-up of the Humble Dialog pattern.\n\nwww.it-ebooks.info\n\nTest Hook\n\nTest Hook\n\nHow do we design the SUT so that we can replace its dependencies at runtime?\n\nWe modify the SUT to behave differently during the test.\n\nSUT SUT\n\nDOC DOC\n\nSetup Setup\n\nExercise Exercise\n\nExercise Exercise\n\nNo No\n\nIf If Testing? Testing?\n\nYes Yes\n\nUsage Usage\n\nNo No\n\nIf If Testing? Testing?\n\nYes Yes\n\nVerify Verify\n\nProduction Production Logic Logic\n\nTest- Test- Specific Specific Logic Logic\n\nProduction Production Logic Logic\n\nTest- Test- Specific Specific Logic Logic\n\nTeardown Teardown\n\nAlmost every piece of code depends on some other classes, objects, modules, or procedures. To unit-test a piece of code properly, we would like to isolate it from its dependencies. Such isolation is difﬁ cult to achieve if those dependencies are hard-coded within the code in the form of literal classnames.\n\nTest Hook is a “method of last resort” for introducing test-speciﬁ c behavior\n\nduring automated testing.\n\nHow It Works\n\nWe modify the behavior of the SUT to support testing by putting a hook directly into the SUT or into a DOC. This approach implies that we use some kind of testing ﬂ ag that can be checked in the appropriate place.\n\nWhen to Use It\n\nSometimes it is appropriate to use this “pattern of last resort” when we cannot use either Dependency Injection (page 678) or Dependency Lookup (page 686). In this situation, we use a Test Hook because we have no other way to address the Untested Code (see Production Bugs on page 268) caused by a Hard-Coded Dependency (see Hard-to-Test Code on page 209).\n\nwww.it-ebooks.info\n\n709\n\nTest Hook\n\n710\n\nTest Hook\n\nChapter 26 Design-for-Testability Patterns\n\nA Test Hook may be the only way to introduce Test Double (page 522) behavior when we are programming in a procedural language that does not support objects, function pointers, or any other form of dynamic binding.\n\nTest Hooks can be used as a transition strategy to bring legacy code under the testing umbrella. We can introduce testability using the Test Hooks and then use those Tests as Safety Net (see page 24) while we refactor for even more test- ability. At some point we should be able to discard the initial round of tests that required the Test Hooks because we have enough “modern” tests to protect us.\n\nImplementation Notes\n\nThe essence of the Test Hook pattern is that we insert some code into the SUT that lets us test it. Regardless of how we insert this code into the SUT, the code itself can either\n\nDivert control to a Test Double instead of the real object, or\n\nBe the Test Double within the real object, or\n\nBe a test-speciﬁ c Decorator [GOF] that delegates to the real object\n\nwhen in production.\n\nThe ﬂ ag that indicates testing is in progress can be a compile-time constant, which may, for example, cause the compiler to optimize out all the testing logic. In languages that support preprocessors or compiler macros, such constructs may also be used to remove the Test Hook before the code enters the production phase. The value of the ﬂ ag can also be read in from conﬁ guration data or stored in a global variable that the test sets directly.\n\nMotivating Example\n\nThe following test cannot be made to pass “as is”:\n\npublic void testDisplayCurrentTime_AtMidnight() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result); }\n\nThis test almost always fails because it depends on a DOC to return the current time to the SUT. The test cannot control the values returned by that component,\n\nwww.it-ebooks.info\n\nTest Hook\n\nthe DefaultTimeProvider. As a consequence, this test will pass only when the system time is exactly midnight.\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar currentTime; try { currentTime = new DefaultTimeProvider().getTime(); } catch (Exception e) { return e.getMessage(); } // etc. }\n\nBecause the SUT is hard-coded to use a particular class to retrieve the time, we cannot replace the DOC with a Test Double. As a result, this test is nondeter- ministic and pretty much useless. We need to ﬁ nd a way to gain control over the indirect inputs of the SUT.\n\nRefactoring Notes\n\nWe can introduce a Test Hook by creating a ﬂ ag that can be checked into the SUT. We then wrap the production code with an if/then/else control structure and put the test-speciﬁ c logic into the then clause.\n\nExample: Test Hook in System Under Test\n\nHere’s the production code modiﬁ ed to accommodate testing via a Test Hook:\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar theTime; try { if (TESTING) { theTime = new GregorianCalendar(); theTime.set(Calendar.HOUR_OF_DAY, 0); theTime.set(Calendar.MINUTE, 0);} else { theTime = new DefaultTimeProvider().getTime(); } } catch (Exception e) { return e.getMessage(); } // etc.\n\nHere we have implemented the testing ﬂ ag as global constant, which we can edit as necessary. This ﬂ exibility implies a separate build step is necessary for versions of the system to be tested. Such a strategy is somewhat safer than using a dynamic conﬁ guration parameter or member variable because many compilers will optimize this hook right out of the object code.\n\nwww.it-ebooks.info\n\n711\n\nTest Hook\n\n712\n\nTest Hook\n\nChapter 26 Design-for-Testability Patterns\n\nExample: Test Hook in Depended-on Component\n\nWe can also introduce a Test Hook by putting the hook into a DOC rather than into the SUT:\n\npublic Calendar getTime() throws TimeProviderEx { Calendar theTime = new GregorianCalendar(); if (TESTING) { theTime.set(Calendar.HOUR_OF_DAY, 0); theTime.set(Calendar.MINUTE, 0);} else { // just return the calendar } return theTime; };\n\nThis approach is somewhat better because we are not modifying the SUT as we test it.\n\nwww.it-ebooks.info\n\nChapter 27\n\nValue Patterns\n\nPatterns in This Chapter\n\nLiteral Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 714\n\nDerived Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718\n\nGenerated Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723\n\nDummy Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 728\n\n713\n\nwww.it-ebooks.info\n\nValue Patterns",
      "page_number": 743
    },
    {
      "number": 27,
      "title": "Value Patterns",
      "start_page": 777,
      "end_page": 948,
      "detection_method": "regex_chapter",
      "content": "714\n\nAlso known as: Hard-Coded Value, Constant Value\n\nLiteral Value\n\nChapter 27 Value Patterns\n\nLiteral Value\n\nHow do we specify the values to be used in tests?\n\nWe use literal constants for object attributes and assertions.\n\nBigDecimal expectedTotal = new BigDecimal(\"99.95\");\n\nThe values we use for the attributes of objects in our test ﬁ xture and the expect- ed outcome of our test are often related to one another in a way that is deﬁ ned in the requirements. Getting these values—and, in particular, the relationship between the pre-conditions and the post-conditions—right is crucial because it drives the correct behavior into the SUT.\n\nLiteral Values are a popular way to specify the values of attributes of objects\n\nin a test.\n\nHow It Works\n\nWe use a literal constant of the appropriate type for each attribute of an object or for use as an argument of a method call to the SUT or an Assertion Method (page 362). The expected values are calculated by hand, calculator, or spreadsheet and hard-coded within the test as Literal Values.\n\nWhen to Use It\n\nUsing a Literal Value in-line makes it very clear which value is being used; there is no doubt about the value’s identity because it is right in front of our face. Unfortunately, using Literal Values can make it difﬁ cult to see the relationships between the values used in various places in the test, which may in turn lead to Obscure Tests (page 186). It certainly makes sense to use Literal Values if the testing requirements specify which values are to be used and we want to make it clear that we are, in fact, using those values. [We might sometimes consider us- ing a Data-Driven Test (page 288) instead to avoid the effort and transcription errors associated with copying the data into test methods.]\n\nOne downside of using a Literal Value is that we might use the same value for two unrelated attributes; if the SUT happens to use the wrong one, tests may pass even though they should not. If the Literal Value is a ﬁ lename or a key used to access a database, the meaning of the value is lost—the content of the ﬁ le or record actually drives the behavior of the SUT. Using a Literal Value as the key does nothing to help the reader understand the test in such a case, and we are likely to suffer from Obscure Tests.\n\nwww.it-ebooks.info\n\nLiteral Value\n\nIf the values in the expected outcome can be derived from the values in the ﬁ xture setup logic, we will be more likely to use the Tests as Documentation (see page 23) if we use Derived Values (page 718). Conversely, if the values are not important to the speciﬁ cation of the logic being tested, we should consider using Generated Values (page 723).\n\nImplementation Notes\n\nThe most common way to use a Literal Value is with literal constants within the code. When the same value needs to be used in several places in the test (typically during ﬁ xture setup and result veriﬁ cation), this approach can obscure the relationship between the test pre-conditions and post-conditions. Introducing an evocatively named symbolic constant can make this relationship much clearer. Likewise, if we cannot use a self-describing value, we can still make the code easier to use by deﬁ ning a suitably named symbolic constant and using it wherever we would have used the Literal Value.\n\nVariation: Symbolic Constant\n\nWhen we need to use the same Literal Value in several places in a single Test Method (page 348) or within several distinct tests, it is a good practice to use a Symbolic Constant instead of a Literal Value. A Symbolic Constant is function- ally equivalent to a Literal Value but reduces the likelihood of High Test Mainte- nance Cost (page 265).\n\nVariation: Self-Describing Value\n\nWhen several attributes of an object need the same kind of value, using different values provides advantages by helping us to prove that the SUT is working with the correct attribute. When an attribute or argument is an unconstrained string, it can be useful to choose a value that describes the role of the value in the test (a Self-Describing Value). For example, using “Not an existing customer” for the name of a customer might be more helpful to the reader than using “Joe Blow,” especially when we are debugging or when the attributes are included in the test failure output.\n\nExample: Literal Value\n\nBecause Literal Value is usually the starting point when writing tests, I’ll dis- pense with a motivating example and cut straight to the chase. Here’s an example of the Literal Value pattern in action. Note the use of Literal Values in both the ﬁ xture setup logic and the assertion.\n\nwww.it-ebooks.info\n\n715\n\nLiteral Value\n\n716\n\nLiteral Value\n\nChapter 27 Value Patterns\n\npublic void testAddItemQuantity_1() throws Exception { Product product = new Product(\"Widget\", 19.95); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, 1); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); assertEquals(new BigDecimal(\"19.95\"), actualItem.getExtendedPrice()); }\n\nThe Product constructor requires both a name and a cost. The assertion on the extendedCost of the lineItem requires a value for the total cost of the product for that line item. In this example, we included these values as hard-coded literal constants. In the next example, we’ll use symbolic constants instead.\n\nRefactoring Notes\n\nWe can reduce the Test Code Duplication (page 213) in the form of the hard- coded Literal Value of 19.95 by doing a Replace Magic Number with Symbolic Constant [Fowler] refactoring.\n\nExample: Symbolic Constant\n\nThis refactored version of the original test replaces the duplicated Literal Value of the widget’s price (19.95) with a suitably named Symbolic Constant that is used during ﬁ xture setup as well as result veriﬁ cation:\n\npublic void testAddItemQuantity_1s() throws Exception { BigDecimal widgetPrice = new BigDecimal(\"19.95\"); Product product = new Product(\"Widget\", widgetPrice); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, 1); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); assertEquals(widgetPrice, actualItem.getExtendedPrice()); }\n\nwww.it-ebooks.info\n\nLiteral Value\n\nExample: Self-Describing Value\n\nThis refactored version of the test provides a Self-Describing Value for the mandatory name argument passed to the Product constructor. This value is not used by the method we are testing; it is merely stored for later access by another method we are not testing here.\n\npublic void testAddItemQuantity_1b() throws Exception { BigDecimal widgetPrice = new BigDecimal(\"19.95\"); Product product = new Product(\"Irrelevant product name\", widgetPrice); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, 1); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); assertEquals(widgetPrice, actualItem.getExtendedPrice()); }\n\nExample: Distinct Value\n\nThis test needs to verify that the item’s name is taken from the product’s name. We’ll use a Distinct Value for the name and the SKU so we can tell them apart.\n\npublic void testAddItemQuantity_1c() throws Exception { BigDecimal widgetPrice = new BigDecimal(\"19.95\"); String name = \"Product name\"; String sku = \"Product SKU\"; Product product = new Product(name, sku, widgetPrice); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, 1); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); assertEquals(name, actualItem.getName()); }\n\nThis also happens to be an example of a self-describing value.\n\nwww.it-ebooks.info\n\n717\n\nLiteral Value\n\n718\n\nAlso known as: Calculated Value\n\nDerived Value\n\nChapter 27 Value Patterns\n\nDerived Value\n\nHow do we specify the values to be used in tests?\n\nWe use expressions to calculate values that can be derived from other values.\n\nBigDecimal expectedTotal = itemPrice.multiply(QUANTITY);\n\nThe values we use for the attributes of objects in our test ﬁ xtures and the result veriﬁ cation parts of our tests are often related to one another in a way that is deﬁ ned in the requirements. Getting these values—and, in particular, the rela- tionship between the pre-conditions and the post-conditions—right is crucial because it drives the correct behavior into the SUT and helps the tests act as documentation of our software.\n\nOften, some of these values can be derived from other values in the same test. In these cases the beneﬁ ts from using our Tests as Documentation (see page 23) are improved if we show the derivation by calculating the values using the appro- priate expression.\n\nHow It Works\n\nComputers are really good at math and string concatenation. We can avoid doing the math in our head (or with a calculator) by coding the math for expected results as arguments of the Assertion Method (page 362) calls directly into the tests. We can also use Derived Values as arguments for ﬁ xture object creation and as method arguments when exercising the SUT.\n\nDerived Values, by their very nature, encourage us to use variables or symbolic constants to hold the values. These variables/constants can be initialized at com- pile time (constants), during class or Testcase Object (page 382) initialization, during ﬁ xture setup, or within the body of the Test Method (page 348).\n\nWhen to Use It\n\nWe should use a Derived Value whenever we have values that can be derived in some deterministic way from other values in our tests. The main drawback of using Derived Values is that the same math error (e.g., rounding errors) could appear in both the SUT and the tests. To be safe, we might want to code a few of the patho- logical test cases using Literal Values (page 714) just in case such a problem might be present. If the values we are using must be unique or don’t affect the logic in the SUT, we may be better off using Generated Values (page 723) instead.\n\nwww.it-ebooks.info\n\nDerived Value\n\nWe can use a Derived Value either as part of ﬁ xture setup (Derived Input or One Bad Attribute) or when determining the expected values to be com- pared with those generated by the SUT (Derived Expectation). These uses are described in a bit more detail later in this section.\n\nVariation: Derived Input\n\nSometimes our test ﬁ xture contains similar values that the SUT might compare or use to base its logic on the difference between them. For example, a Derived Input might be calculated in the ﬁ xture setup portion of the test by adding the difference to a base value. This operation makes the relationship between the two values explicit. We can even put the value to be added in a symbolic constant with an Intent-Revealing Name [SBPP] such as MAXIMUM_ALLOWABLE_TIME_DIFFERENCE.\n\nVariation: One Bad Attribute\n\nA Derived Input is often employed when we need to test a method that takes a complex object as an argument. For example, thorough “input validation” testing requires that we exercise the method with each of the attributes of the object set to one or more possible invalid values to ensure that it handles all of these cases cor- rectly. Because the ﬁ rst rejected value could cause termination of the method, we must verify each bad attribute in a separate call to the SUT; each of these calls, in turn, should be done in a separate test method (each should be a Single-Condition Test; see page 45). We can instantiate the invalid object easily by ﬁ rst creating a valid object and then replacing one of its attributes with an invalid value. It is best to create the valid object using a Creation Method (page 415) so as to avoid Test Code Duplication (page 213).\n\nVariation: Derived Expectation\n\nWhen some value produced by the SUT should be related to one or more of the values we passed in to the SUT as arguments or as values in the ﬁ xture, we can often derive the expected value from the input values as the test executes rather than using precalculated Literal Values. We then use the result as the expected value in an Equality Assertion (see Assertion Method).\n\nMotivating Example\n\nThe following test doesn’t use Derived Values. Note the use of Literal Values in both the ﬁ xture setup logic and the assertion.\n\npublic void testAddItemQuantity_2a() throws Exception { BigDecimal widgetPrice = new BigDecimal(\"19.99\");\n\nwww.it-ebooks.info\n\n719\n\nDerived Value\n\n720\n\nDerived Value\n\nChapter 27 Value Patterns\n\nProduct product = new Product(\"Widget\", widgetPrice); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, 5); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); assertEquals(new BigDecimal(\"99.95\"), actualItem.getExtendedPrice()); }\n\nTest readers may have to do some math in their heads to fully appreciate the relationship between the values in the ﬁ xture setup and the value in the result veriﬁ cation part of the test.\n\nRefactoring Notes\n\nTo make this test more readable, we can replace any Literal Values that are actu- ally derived from other values with formulas that calculate these values.\n\nExample: Derived Expectation\n\nThe original example contained only one line item for ﬁ ve instances of the prod- uct. We therefore calculated the expected value of the extended price attribute by multiplying the unit price by the quantity, which makes the relationship between the values explicit.\n\npublic void testAddItemQuantity_2b() throws Exception { BigDecimal widgetPrice = new BigDecimal(\"19.99\"); BigDecimal numberOfUnits = new BigDecimal(\"5\"); Product product = new Product(\"Widget\", widgetPrice); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, numberOfUnits); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); BigDecimal totalPrice = widgetPrice.multiply(numberOfUnits); assertEquals(totalPrice, actualItem.getExtendedPrice()); }\n\nNote that we have also introduced symbolic constants for the unit price and quantity to make the expression even more obvious and to reduce the effort of changing the values later.\n\nwww.it-ebooks.info\n\nDerived Value\n\nExample: One Bad Attribute\n\nSuppose we have the following Customer Factory Method [GOF], which takes a CustomerDto object as an argument. We want to write tests to verify what occurs when we pass in invalid values for each of the attributes in the CustomerDto. We could create the CustomerDto in-line in each Test Method with the appropriate attribute initialized to some invalid value.\n\npublic void testCreateCustomerFromDto_BadCredit() { // ﬁxture setup CustomerDto customerDto = new CustomerDto(); customerDto.ﬁrstName = \"xxx\"; customerDto.lastName = \"yyy\"; // etc. customerDto.address = createValidAddress(); customerDto.creditRating = CreditRating.JUNK; // exercise the SUT try { sut.createCustomerFromDto(customerDto); fail(\"Expected an exception\"); } catch (InvalidInputException e) { assertEquals( \"Field\", \"Credit\", e.ﬁeld ); } }\n\npublic void testCreateCustomerFromDto_NullAddress() { // ﬁxture setup CustomerDto customerDto = new CustomerDto(); customerDto.ﬁrstName = \"xxx\"; customerDto.lastName = \"yyy\"; // etc. customerDto.address = null; customerDto.creditRating = CreditRating.AAA; // exercise the SUT try { sut.createCustomerFromDto(customerDto); fail(\"Expected an exception\"); } catch (InvalidInputException e) { assertEquals( \"Field\", \"Address\", e.ﬁeld ); } }\n\nThe obvious problem with this code is that we end up with a lot of Test Code Duplication because we need at least one test per attribute. The problem becomes even worse if we are doing incremental development: We will require more tests for each newly added attribute, and we will have to revisit all existing tests to add the new attribute to the Factory Method signature.\n\nwww.it-ebooks.info\n\n721\n\nDerived Value\n\n722\n\nDerived Value\n\nChapter 27 Value Patterns\n\nThe solution is to deﬁ ne a Creation Method that produces a valid instance of the CustomerDto (by doing an Extract Method [Fowler] refactoring on one of the tests) and uses it in each test to create a valid DTO. Then we simply replace one of the attributes with an invalid value in each of the tests. Each test now has an object with One Bad Attribute, with each one invalid in a slightly different way.\n\npublic void testCreateCustomerFromDto_BadCredit_OBA() { CustomerDto customerDto = createValidCustomerDto(); customerDto.creditRating = CreditRating.JUNK; try { sut.createCustomerFromDto(customerDto); fail(\"Expected an exception\"); } catch (InvalidInputException e) { assertEquals( \"Field\", \"Credit\", e.ﬁeld ); } }\n\npublic void testCreateCustomerFromDto_NullAddress_OBA() { CustomerDto customerDto = createValidCustomerDto(); customerDto.address = null; try { sut.createCustomerFromDto(customerDto); fail(\"Expected an exception\"); } catch (InvalidInputException e) { assertEquals( \"Field\", \"Address\", e.ﬁeld ); } }\n\nwww.it-ebooks.info\n\nGenerated Value\n\nGenerated Value\n\nHow do we specify the values to be used in tests?\n\nWe generate a suitable value each time the test is run.\n\nBigDecimal uniqueCustomerNumber = getUniqueNumber();\n\nWhen initializing the objects in the test ﬁ xture, one issue that must be dealt with is the fact that most objects have various attributes (ﬁ elds) that need to be supplied as arguments to the constructor. Sometimes the exact values to be used affect the outcome of the test. More often than not, however, it is important only that each object use a different value. When the precise values of these attributes are not important to the test, it is important not to have them visible within the test!\n\nGenerated Values are used in conjunction with Creation Methods (page 415)\n\nto help us remove this potentially distracting information from the test.\n\nHow It Works\n\nInstead of deciding which values to use in our tests while we are coding the tests, we generate the values when we actually execute the tests. We can then pick values to satisfy speciﬁ c criteria such as “must be unique in the database” that can be determined only as the test run unfolds.\n\nWhen to Use It\n\nWe use a Generated Value whenever we cannot or do not want to specify the test values until the test is executing. Perhaps the value of an attribute is not expected to affect the outcome of the test and we don’t want to be bothered to deﬁ ne Literal Values (page 714), or perhaps we need to ensure some quality of the attribute that can be determined only at runtime. In some cases, the SUT requires the value of an attribute to be unique; using a Generated Value can ensure that this criterion is satisﬁ ed and thereby prevent Unrepeatable Tests (see Erratic Test on page 228) and Test Run Wars (see Erratic Test) by reducing the likelihood of a test conﬂ icting with its parallel incarnation in another test run. Optionally, we can use this distinct value for all attributes of the object; object recognition then becomes very easy when we inspect the object in a debugger.\n\nOne thing to be wary of is that different values could expose different bugs. For example, a single-digit number may be formatted correctly, whereas a multidigit number might not (or vice versa). Generated Values can result\n\nwww.it-ebooks.info\n\n723\n\nGenerated Value\n\n724\n\nGenerated Value\n\nChapter 27 Value Patterns\n\nin Nondeterministic Tests (see Erratic Test); if we encounter nondeterminism (sometimes the test passes and then fails during the very next run), we must check the SUT code to see whether differences in value could be the root cause. In general, we shouldn’t use a Generated Value unless the value must be unique because of the nondeterminism such a value may introduce. The obvi- ous alternative is to use a Literal Value. A less obvious alternative is to use a Derived Value (page 718), especially when we must determine the expected results of a test.\n\nImplementation Notes\n\nWe can generate values in a number of ways. The appropriateness of each tech- nique depends on the circumstance.\n\nVariation: Distinct Generated Value\n\nWhen we need to ensure that each test or object uses a different value, we can take advantage of Distinct Generated Values. In such a case, we can create a set of util- ity functions that will return unique values of various types (e.g., integers, strings, ﬂ oating-point numbers). The various getUnique methods can all be built upon an integer sequence number generator. For numbers that must be unique within the scope of a shared database, we can use database sequences or a sequence table. For numbers that must be unique within the scope of a particular test run, we can use an in-memory sequence number generator (e.g., use a Java static variable that is incremented before usage). In-memory sequence numbers that start from the number 1 each time a test suite is run offer a useful quality: The values generated in each test are the same for each run and can simplify debugging.\n\nVariation: Random Generated Value\n\nOne way to obtain good test coverage without spending a lot of time analyzing the behavior and generating test conditions is to use different values each time we run the tests. Using a Random Generated Value is one way to accomplish this goal. While use of such values may seem like a good idea, it makes the tests nondeterministic (Nondeterministic Tests) and can make debugging failed tests very difﬁ cult. Ideally, when a test fails, we want to be able to repeat that test failure on demand. To do so, we can log the Random Generated Value as the test is run and show it as part of the test failure. We then need to ﬁ nd a way to force the test to use that value again while we are troubleshooting the failed test. In most cases, the effort required outweighs the potential beneﬁ t. Of course, when we need this technique, we really need it.\n\nwww.it-ebooks.info\n\nGenerated Value\n\nVariation: Related Generated Value\n\nAn optional enhancement is to combine a Generated Value with a Derived Value by using the same generated integer as the root for all attributes of a single object. This result can be accomplished by calling getUniqueInt once and then using that value to build unique strings, ﬂ oating-point numbers, and other values. With a Related Generated Value, all ﬁ elds of the object contain “related” data, which makes the object easier to recognize when debugging. Another option is to sepa- rate the generation of the root from the generation of the values by calling gener- ateNewUniqueRoot explicitly before calling getUniqueInt, getUniqueString, and so on.\n\nAnother nice touch for strings is to pass a role-describing argument to the function that is combined with the unique integer key to make the code more intent-revealing. Although we could also pass such arguments to the other functions, of course we wouldn’t be able to build them into an integer value.\n\nMotivating Example\n\nThe following test uses Literal Values for the arguments to a constructor:\n\npublic void testProductPrice_HCV() { // Setup Product product = new Product( 88, // ID \"Widget\", // Name new BigDecimal(\"19.99\")); // Price // Exercise // ... }\n\nRefactoring Notes\n\nWe can convert the test to use Distinct Generated Values by replacing the Literal Values with calls to the appropriate getUnique method. These methods simply increment a counter each time they are called and use that counter value as the root for construction of an appropriately typed value.\n\nExample: Distinct Generated Value\n\nHere is the same test using a Distinct Generated Value. For the getUniqueString method, we’ll pass a string describing the role (“Widget Name”).\n\npublic void testProductPrice_DVG() { // Setup Product product =\n\nwww.it-ebooks.info\n\n725\n\nGenerated Value\n\n726\n\nGenerated Value\n\nChapter 27 Value Patterns\n\nnew Product( getUniqueInt(), // ID getUniqueString(\"Widget\"), // Name getUniqueBigDecimal()); // Price // Exercise // ... }\n\nstatic int counter = 0;\n\nint getUniqueInt() { counter++; return counter; }\n\nBigDecimal getUniqueBigDecimal() { return new BigDecimal(getUniqueInt()); }\n\nString getUniqueString(String baseName) { return baseName.concat(String.valueOf( getUniqueInt())); }\n\nThis test uses a different generated value for each argument of the constructor call. The numbers generated in this way are consecutive but the test reader still needs to look at a speciﬁ c attribute when debugging to get a consistent view. We probably should not generate the price value if the logic we were testing was related to price calculation because that would force our veriﬁ cation logic to accommodate different total costs.\n\nExample: Related Generated Value\n\nWe can ensure that all values used by the test are obviously related by separating the generation of the root value from the construction of the individual values. In the following example, we’ve moved the generation of the root to the setUp method so each test method gets a new value only once. The methods that retrieve the various values (e.g., getUniqueString) simply use the previously gener- ated root when deriving the Generated Values.\n\npublic void testProductPrice_DRVG() { // Setup Product product = new Product( getUniqueInt(), // ID getUniqueString(\"Widget\"), // Name getUniqueBigDecimal()); // Price // Exercise // ... }\n\nwww.it-ebooks.info\n\nGenerated Value\n\nstatic int counter = 0;\n\npublic void setUp() { counter++; }\n\nint getUniqueInt() { return counter; }\n\nString getUniqueString(String baseName) { return baseName.concat(String.valueOf( getUniqueInt())); }\n\nBigDecimal getUniqueBigDecimal() { return new BigDecimal(getUniqueInt()); }\n\nIf we looked at this object in an object inspector or database or if we dumped part of it to a log, we could readily tell which object we were looking at regard- less of which ﬁ eld we happened to see.\n\nwww.it-ebooks.info\n\n727\n\nGenerated Value\n\n728\n\nAlso known as: Dummy, Dummy Parameter, Dummy Value, Placeholder, Stub\n\nDummy Object\n\nChapter 27 Value Patterns\n\nDummy Object\n\nHow do we specify the values to be used in tests when the only usage is as irrelevant arguments of SUT method calls?\n\nWe pass an object that has no implementation as an argument of a method called on the SUT.\n\nInvoice inv = new Invoice( new DummyCustomer() );\n\nGetting the SUT into the right state to start a test often requires calling other methods of the SUT. These methods commonly take as arguments objects that are stored in instance variables for later use. Often, these objects (or at least some attributes of these objects) are never used in the code that we are actu- ally testing. Instead, we create them solely to conform to the signature of some method we must call to get the SUT into the right state. Constructing these objects can be nontrivial and adds unnecessary complexity to the test.\n\nIn these cases, a Dummy Object can be passed as an argument, eliminating\n\nthe need to build a real object.\n\nHow It Works\n\nWe create an instance of some object that can be instantiated easily and with no dependencies; we then pass that instance as the argument of the method of the SUT. Because it won’t actually be used within the SUT, we don’t need any implementation for this object. If any of the methods of the Dummy Object are invoked, the test really should throw an error. Trying to invoke a nonexistent method will typically produce that result.\n\nWhen to Use It\n\nWe can use Dummy Objects whenever we need to use objects as attributes of other objects or arguments of methods on the SUT or other ﬁ xture objects. Using Dummy Objects helps us avoid Obscure Tests (page 186) by leaving out the irrelevant code that would be necessary to build real objects and by making it clear which objects and values are not used by the SUT.\n\nIf we need to control the indirect inputs or verify the indirect outputs of the SUT, we should probably use a Test Stub (page 529) or a Mock Object (page 544) instead. If the object will be used by the SUT but we cannot provide the real object, we should consider providing a Fake Object (page 551) that provides just enough behavior for the test to execute.\n\nwww.it-ebooks.info\n\nDummy Object\n\nWe can use one of the value patterns when the SUT really does need to use the object in some way. Either a Literal Value (page 714), a Generated Value (page 723), or a Derived Value (page 718) may be appropriate, depend- ing on the circumstance.\n\nVariation: Dummy Argument\n\nWe can use a Dummy Argument whenever methods of the SUT take objects as arguments1 and those objects are not relevant to the test.\n\nVariation: Dummy Attribute\n\nWe can use a Dummy Attribute whenever we are creating objects that will be used as part of the ﬁ xture or as arguments of SUT methods, and some of the attributes of those objects are not relevant to the test.\n\nImplementation Notes\n\nThe simplest implementation of a Dummy Object is to pass a null value as the argument. This approach works even in a statically typed language such as Java, albeit only if the method being called doesn’t check for null arguments. If the method complains when we pass it null, we’ll need to employ a slightly more sophisticated implementation. The biggest disadvantage to using null is that it is not very descriptive.\n\nIn dynamically typed languages such as Ruby, Perl, and Python, the actual type of the object will never be checked (because it will never be used), so we can use any class such as String or Object. In such a case, it is useful to give the object a Self-Describing Value (see Literal Value) such as “Dummy Customer.” In statically typed languages (such as Java, C#, and C++), we must ensure that the Dummy Object is type compatible with the parameter it is to match. Type compatibility is much easier to achieve if the parameter has an abstract type (e.g., an Interface in Java) because we can create our own trivial implementation of the type or pass a suitable Pseudo-Object (see Hard-Coded Test Double on page 568). If the parameter type is a concrete class, we may be able to create\n\n1 From Wikipedia: Parameters are also commonly referred to as arguments, although ar- guments are more properly thought of as the actual values or references assigned to the parameter variables when the subroutine is called at runtime. When discussing code that is calling into a subroutine, any values or references passed into the subroutine are the arguments, and the place in the code where these values or references are given is the parameter list. When discussing the code inside the subroutine deﬁ nition, the variables in the subroutine’s parameter list are the parameters, while the values of the parameters at runtime are the arguments.\n\nwww.it-ebooks.info\n\n729\n\nDummy Object\n\n730\n\nDummy Object\n\nChapter 27 Value Patterns\n\na trivial instance of it or we may need to create an instance of a Test-Speciﬁ c Subclass (page 579) within our test.\n\nSome Mock Object frameworks have Test Utility Methods (page 599) that will generate a Dummy Object for a speciﬁ ed class that takes a String argument for a Self-Describing Value.\n\nWhile the Dummy Object may, in fact, be null, it is not the same as a Null Object [PLOPD3]. A Dummy Object is not used by the SUT, so its behavior is either irrelevant or it should throw an exception when executed. In contrast, a Null Object is used by the SUT but is designed to do nothing. That’s a small but very important distinction!\n\nMotivating Example\n\nIn this example, we are testing the Invoice but we require a Customer to instantiate the invoice. The Customer requires an Address, which in turn requires a City. Thus we ﬁ nd ourselves creating several additional objects just to set up the ﬁ xture. But if we know that the behavior we are testing should not access the Customer at all, why do we need to create it and all the objects on which it depends?\n\npublic void testInvoice_addLineItem_noECS() { ﬁnal int QUANTITY = 1; Product product = new Product(getUniqueNumberAsString(), getUniqueNumber()); State state = new State(\"West Dakota\", \"WD\"); City city = new City(\"Centreville\", state); Address address = new Address(\"123 Blake St.\", city, \"12345\"); Customer customer= new Customer(getUniqueNumberAsString(), getUniqueNumberAsString(), address); Invoice inv = new Invoice(customer); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); LineItem expItem = new LineItem(inv, product, QUANTITY); assertLineItemsEqual(\"\",expItem, actual); }\n\nThis test is quite cluttered as a result of the extra object creation. How is the behavior we are testing related to the Address and City? From this test, we can only assume that there is some relation. But this misleads the test reader!\n\nwww.it-ebooks.info\n\nDummy Object\n\nRefactoring Notes\n\nIf the objects in the ﬁ xture are not relevant to the test, they should not be visible in the test. Therefore, we should try to eliminate the need to create all these objects. We could try passing in null for the Customer. In this case, the constructor checks for null and rejects it, so we have to ﬁ nd another way.\n\nThe solution is to replace the object that is not important to our test with a Dummy Object. In dynamically typed languages, we could just pass in a string. In statically typed languages such as Java and C#, however, we must pass in a type-compatible object. In this case, we have chosen to do an Extract Interface [Fowler] refactoring on Customer to create a new interface and then create a new implementation class called DummyCustomer. Of course, as part of the Extract Inter- face refactoring, we must replace all references to Customer with the new interface name so that the DummyCustomer will be acceptable. A less intrusive option would be to use a Test-Speciﬁ c Subclass of Customer that adds a test-friendly constructor.\n\nExample: Dummy Values and Dummy Objects\n\nHere’s the same test using a Dummy Object instead of the Product name and the Customer. Note how much simpler the ﬁ xture setup has become!\n\npublic void testInvoice_addLineItem_DO() { ﬁnal int QUANTITY = 1; Product product = new Product(\"Dummy Product Name\", getUniqueNumber()); Invoice inv = new Invoice( new DummyCustomer() ); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\", expItem, actual); }\n\nUsing a Dummy Object for the name of the Product was simple because it is a string and has no uniqueness requirement. Thus we were able to use a Self- Describing Value. We were not able to use a Dummy Object for the Product number because it must be unique, so we left it as a Generated Value. The Customer was a bit trickier because the LineItem’s constructor expected a non- null object. Because this example is written in Java, the method parameter is strongly typed; for this reason, we needed to create an alternative implemen- tation of the ICustomer interface with a no-argument constructor to simplify in-line construction. Because the DummyCustomer is never used, we have created\n\nwww.it-ebooks.info\n\n731\n\nDummy Object\n\n732\n\nDummy Object\n\nChapter 27 Value Patterns\n\nit in-line rather than declaring a variable to hold it. This choice reduces the ﬁ xture setup code by one line, and the presence of the in-line constructor call within the call to the Invoice constructor reinforces the message that we need the Dummy Object only for the constructor call and not for the rest of the test. Here is the code for the DummyCustomer:\n\npublic class DummyCustomer implements ICustomer {\n\npublic DummyCustomer() { // Real simple; nothing to initialize! }\n\npublic int getZone() { throw new RuntimeException(\"This should never be called!\"); } }\n\nWe have implemented the DummyCustomer class with just those methods declared in the interface; because each method throws an exception, we know when it is hit. We could also have used a Pseudo-Object for the DummyCustomer. In other circum- stances we might have been able to simply pass in null or construct a dummy instance of the real class. The major problem with the latter technique is that we won’t know for sure if the Dummy Object is actually used.\n\nFurther Reading\n\nWhen [UTwJ] refers to a “dummy object,” these authors are referring to what this book terms a Test Stub. See Mocks, Fakes, Stubs, and Dummies in Appen- dix B for a more thorough comparison of the terminology used in various books and articles. The JMock and NMock frameworks for testing with Mock Objects support auto-generation of Dummy Objects.\n\nwww.it-ebooks.info\n\nPART IV\n\nAppendixes\n\n733\n\nwww.it-ebooks.info\n\nAppendixes\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nAppendix A\n\nTest Refactorings\n\nExtract Testable Component\n\nYou want to be able to test the logic easily but the component is too closely tied to its context to allow such testing.\n\nExtract the logic you want to test into a separate component that is designed for testability and is independent of the context in which it is run.\n\nImplementation Notes\n\nWe extract the logic from the untestable component into a component that is testable via synchronous tests, leaving behind all the ties to the context. This usually means that anything required by the testable component logic from the context is retrieved by the untestable component and passed in to the testable component as arguments of the methods under test or constructor methods. The untestable component then contains very little code and is considered to be a Humble Object (page 695). It simply retrieves the information the testable component requires from the context, instantiates the testable component, and delegates to it. All interactions with the new testable component consist of synchronous method calls.\n\nThe testable component may be a Windows DLL, a Java JAR containing a Service Facade [CJ2EEP] class, or some other language component or class that exposes the services of the executable in a testable way. The untestable code may be an executable, a dialog box or some other presentation compo- nent, logic that is executed inside a transaction, or even a complex test method. Extraction of the testable component should leave behind a Humble Object that requires very little, if any, testing.\n\n735\n\nwww.it-ebooks.info\n\nExtract Testable Component\n\nAlso known as: Sprout Class [WEwLC]\n\n736\n\nIn-line Resource\n\nAppendix A Test Refactorings\n\nDepending on the nature of the untestable component, we may choose to write tests for the delegation logic or we may be unable to do so because the logic is so closely tied to the context. If we do write tests for it, we require only one or two tests to verify that the instantiation and delegation occur correctly. Because this code will not change very often, these tests are much less critical than other tests and can even be omitted from the suite of tests that developers execute before check-in if we want to speed up test suite execution times. Of course, we would still prefer to run them from the automated build process.\n\nFurther Reading\n\nThis refactoring is similar to an Extract Interface [Fowler] refactoring and an Extract Implementer [Fowler] refactoring, except that Extract Testable Component does not require keeping the same interface. It can also be viewed as a special case of the Extract Class [Fowler] refactoring.\n\nIn-line Resource\n\nTests that depend on an unseen external resource create a Mystery Guest problem.\n\nMove the contents of an external resource into the ﬁ xture setup logic of the test.\n\nFrom [RTC]:\n\nTo remove the dependency between a test method and some external resource, we incorporate that resource in the test code. This is done by setting up a ﬁ xture in the test code that holds the same contents as the resource. This ﬁ xture is then used instead of the resource to run the test. A simple example of this refactoring is putting the contents of a ﬁ le that is used into some string in the test code.\n\nIf the contents of the resource are large, chances are high that you are also suffering from Eager Tests (see Assertion Roulette on page 224). Consider applying an Extract Method [Fowler] refactoring or a Minimize Data (page 738) refactoring.\n\nwww.it-ebooks.info\n\nMake Resource Unique\n\nImplementation Notes\n\nThe problem with tests that depend on an external resource is that we cannot see the pre-conditions of the test. The resource may be a ﬁ le sitting in the ﬁ le system, the contents of a database, or some other object created outside the test. None of these Prebuilt Fixtures (page 429) is visible to the test reader. The solution is to make them visible by including the resource in-line within the test. The simplest way to do so is to create the resource from within the test itself. For example, we could build the contents of a text ﬁ le by writing to the ﬁ le rather than just referring to a preexisting ﬁ le. If we delete the ﬁ le at the end of the test, this step also moves us from a Prebuilt Fixture approach to a Persistent Fresh Fixture (see Fresh Fixture on page 311) approach. As a result, our tests may execute somewhat more slowly.\n\nA more innovative way to in-line the external resource is to replace the actual resource with a Test Stub (page 529) that is initialized within the test. The contents of the resource then become visible to the test reader. When the system under test (SUT) executes, it uses the Test Stub instead of the real resource.\n\nAnother option is to refactor the design of the SUT so as to improve its test- ability. We can apply the Extract Testable Component (page 735) refactoring to the part of the SUT that uses the contents of the resource so that it can be tested directly without actually accessing an external resource. That is, the test passes the contents of the resource to the logic that uses it. We can also test the Humble Object (page 695) that reads the resource independently by replacing the extracted component with a Test Stub or Mock Object (page 544).\n\nMake Resource Unique\n\nSeveral tests are accidentally creating or using the same resource in a Shared Fixture.\n\nMake the name of any resources used by a test unique.\n\nFrom [RTC]:\n\nA lot of problems originate from the use of overlapping resource names, either between different tests run by the same user or between simultaneous test runs done by different users.\n\nSuch problems can easily be prevented (or repaired) by using unique identiﬁ ers for all resources that are allocated—for example, by including a time stamp. When you also include the name of the test responsible for\n\nwww.it-ebooks.info\n\n737\n\nMake Resource Unique\n\n738\n\nMinimize Data\n\nAlso known as: Reduce Data\n\nAppendix A Test Refactorings\n\nallocating the resource in this identiﬁ er, you will have fewer problems ﬁ nding tests that do not properly release their resources.\n\nImplementation Notes\n\nWe make the name of any resources used by a test unique across all tests by using a Distinct Generated Value (see Generated Value on page 723) as part of the name. Ideally, the name should include the name of the test that “owns” the resource. To avoid Interacting Tests (see Erratic Test on page 228), we include a time stamp in the name of any resources created by the tests and use Automated Teardown (page 503) to delete those resources at the end of the test.\n\nMinimize Data\n\nThe test ﬁ xture is too large, making the test hard to understand.\n\nWe remove things from the ﬁ xture until we have a Minimal Fixture.\n\nFrom [RTC]:\n\nMinimize the data that is set up in ﬁ xtures to the bare essentials. This will have two advantages: (1) It makes them more suitable as documentation, and (2) your tests will be less sensitive to changes.\n\nImplementation Notes\n\nReducing the data in our test ﬁ xture to the bare minimum results in a Minimal Fix- ture (page 302) that helps the tests achieve Tests as Documentation (see page 23). How we do this depends on how our Test Methods (page 348) are organized into Testcase Classes (page 373).\n\nWhen our Test Methods are organized via the Testcase Class per Fixture pat- tern (page 631) and we believe we have a General Fixture (see Obscure Test on page 186), we can remove the ﬁ xture setup logic for any parts of the ﬁ xture that we suspect are not used by the tests. It is best to remove this logic incrementally so that if a test fails, we can undo our most recent change and try again.\n\nWhen our Test Methods are organized as a Testcase Class per Feature (page 624) or a Testcase Class per Class (page 617), Minimize Data may also involve copying ﬁ xture setup logic from the setUp method of a Testcase Class or Setup Decora- tor (page 447) into each test that needs the ﬁ xture. Assuming the collection of\n\nwww.it-ebooks.info\n\nReplace Dependency with Test Double\n\nobjects in the Shared Fixture (page 317) is overkill for any one test, we can use a series of Extract Method [Fowler] refactorings to create a set of Creation Methods (page 415), which we then call from the tests. Next, we remove the calls to the Creation Methods from the setUp method and put them into only those Test Methods that require that part of the original ﬁ xture. The ﬁ nal step would be to convert any ﬁ xture-holding instance variables into local variables.\n\nReplace Dependency with Test Double\n\nThe dependencies of an object being tested get in the way of running tests.\n\nBreak the dependency by replacing a depended-on component with a Test Double.\n\nImplementation Notes\n\nThe ﬁ rst step is to choose the form of dependency substitution. Dependency Injection (page 678) is the best option for unit tests, whereas Dependency Look- up (page 686) often works better for customer tests. We then refactor the SUT to support this choice or design the capability into the SUT as we do test-driven development. The next decision is whether to use a Fake Object (page 551), a Test Stub (page 529), a Test Spy (page 538), or a Mock Object (page 544) based on how the Test Double will be used by the test. This decision is described in Chapter 11, Using Test Doubles.\n\nIf we are using a Test Stub or Mock Object, we must decide whether we want to use a Hard-Coded Test Double (page 568) or a Conﬁ gurable Test Double (page 558). The trade-offs are discussed in Chapter 11 and in the detailed descriptions of the patterns. That decision then dictates the shape of our test—for example, Tests that use Mock Objects are more “front-loaded” by the construction of the Mock Object.\n\nFinally, we modify our test to construct, optionally conﬁ gure, and then install the Mock Object. We may also have to add a call to the veriﬁ cation method for some kinds of Mock Objects. In statically typed languages, we may have to do an Extract Interface [Fowler] refactoring before we can introduce the fake imple- mentation. We then use this interface as the type of the variable that holds the reference to the substitutable dependency.\n\nwww.it-ebooks.info\n\n739\n\nReplace Dependency with Test Double\n\n740\n\nSetup External Resource\n\nAppendix A Test Refactorings\n\nSetup External Resource\n\nThe SUT depends on the contents of an external resource that is acting as a Mystery Guest in our test.\n\nCreate an external resource within the ﬁ xture setup logic of the test rather than using a predeﬁ ned resource.\n\nFrom [RTC]:\n\nIf it is necessary for a test to rely on external resources, such as directories, databases, or ﬁ les, make sure the test that uses them explicitly creates or allocates these resources before testing, and releases them when done (take precautions to ensure the resource is also released when tests fail).\n\nImplementation Notes\n\nWhen our SUT must use an external resource such as a ﬁ le and we absolutely, positively cannot replace the access mechanism with a Test Stub (page 529) or Fake Object (page 551), we may need to live with the fact that we have to use an external resource. The problems with external resources are obvious: The test reader can- not tell what they contain; those resources may disappear unexpectedly, causing tests to fail because of Resource Optimism (see Erratic Test on page 228); and the resources may result in Interacting Tests (see Erratic Test) and Test Run Wars (see Erratic Test). Setup External Resource does not help us with the last problem but it does avoid the problems of a Mystery Guest (see Obscure Test on page 186) and Resource Optimism.\n\nTo implement the Setup External Resource refactoring, we simply pull the contents of the external resource into our Test Method (page 348), setUp method, or a Test Utility Method (page 599) called by them. Using the contents we con- struct the external resource within our test code, thereby making it evident to the test reader exactly what the test depends on. This approach also guarantees that the resource exists because we create it in every test run.\n\nwww.it-ebooks.info\n\nAppendix B\n\nxUnit Terminology\n\nMocks, Fakes, Stubs, and Dummies\n\nAre you confused about what someone means when that individual says “test stub” or “mock object”? Do you sometimes feel that the person you are talking to is using a very different deﬁ nition? Well, you are not alone!\n\nThe terminology for the various kinds of Test Doubles (page 522) is confusing and inconsistent. Different authors use different terms to mean the same thing. And sometimes they mean different things even when they use the same term! Ouch! (See the sidebar “What’s in a (Pattern) Name?” on page 576 for why I think names are important.)\n\nPart of my reason for writing this book was to try to establish some consistency in the terminology, thereby giving people a set of names with clear deﬁ nitions of what they mean. In this appendix, I provide a list of the current sources and cross- reference the terminology they use with the terminology used in this book.\n\nRole Descriptions\n\nThe table on page 742 is a summary of what I mean by each of the major Test Double pattern names.\n\n741\n\nwww.it-ebooks.info\n\nMocks, Fakes, Stubs, and Dummies\n\nPattern\n\nPurpose\n\nTest Double (page 522)\n\nGeneric name for family\n\nDummy Object Attribute or (page 728)\n\nmethod parameter\n\nTest Stub (page 529)\n\nVerify indirect inputs of SUT\n\nTest Spy (page 538)\n\nVerify indirect outputs of SUT\n\nMock Object (page 544)\n\nVerify indirect outputs of SUT\n\nFake Object (page 551)\n\nRun (unrunnable) tests (faster)\n\nTemporary Test Stub (see Test Stub)\n\nStand in for procedural code not yet written\n\nHas Behavior\n\nNo\n\nYes\n\nYes\n\nYes\n\nYes\n\nYes\n\nD e s c r i p t i o n s\n\nR o l e\n\nInjects Indirect Handles Indirect Values Provided Examples Inputs into SUT Outputs of SUT\n\nby Test(er)\n\nNo, never called\n\nNo, never called\n\nNo\n\nNull, “Ignored String,” new Object()\n\nYes\n\nIgnores them\n\nInputs\n\nOptional\n\nCaptures them for later veriﬁ cation\n\nInputs (optional)\n\nOptional\n\nVeriﬁ es correctness against expectations\n\nInputs (optional) and expected outputs.\n\nNo\n\nUses them\n\nNone\n\nIn-memory database emulator\n\nNo\n\nUses them\n\nNone\n\nIn-memory database emulator\n\nwww.it-ebooks.info\n\n7 4 2\n\nA p p e n d\n\ni x B\n\nx U n i t T e r m i n o l o g y\n\nTerminology Cross-Reference\n\nTerminology Cross-Reference\n\nThe following table lists some sources of conﬂ icting deﬁ nitions just to make it clear what the mapping is to the pattern names used in this book.\n\nSources and Names Used in Them\n\nPattern Astels Beck Feathers Fowler jMock UTWJ OMG Pragmatic Recipes\n\nTest Double\n\nDouble or stand-in\n\nDummy Stub Object\n\nDummy\n\nStub\n\nTest Stub\n\nFake\n\nFake\n\nStub\n\nStub\n\nDummy\n\nMock\n\nFake\n\nTest Spy\n\nDummy\n\nSpy\n\nMock Mock Object\n\nMock Mock Mock Mock\n\nMock\n\nMock\n\nFake Object\n\nDummy\n\nTempo- rary Test Stub\n\nStub\n\nOMG’s CORBA Stub\n\nStub\n\nUnit Testing with Java [UTwJ] uses the term “Dummy Object” to refer\n\nto what this book calls a “Fake Object.”\n\nPragmatic Unit Testing [PUT] describes a “Stub” as an empty imple- mentation of a method. This is a common interpretation in the proce- dural world; in the object world, however, it is typically called a Null Object [PLOPD3].\n\nSome of the early Mock Objects literature could be interpreted to equate a “Stub” with a “Mock Object.” The distinction between the two has since been clariﬁ ed in [MRNO] and [MAS].\n\nwww.it-ebooks.info\n\n743\n\nTerminology Cross- Reference\n\n744\n\nxUnit Terminology Cross- Reference\n\nAppendix B xUnit Terminology\n\nThe CORBA standard1 and other remote-procedure call speciﬁ cations use the terms “stubs” and “skeletons” to refer to the automatically generated code for the near- and far-end implementations of a remote interface deﬁ ned in IDL. (I’ve included this information here because it is another use of a term that is commonly used in the TDD and auto- mated developer testing community.)\n\nThe sources quoted in the preceding table are provided here:\n\nSource\n\nDescription\n\nCitation\n\nPublisher\n\nAstels\n\nBook: Test-Driven Development\n\n[TDD-APG] Prentice Hall\n\nBeck\n\nBook: Test-Driven Development\n\n[TDD-BE]\n\nAddison-Wesley\n\nFeathers\n\nBook: Working Effectively with Legacy Code\n\n[WEwLC]\n\nPrentice Hall\n\nFowler\n\nBlog: Mocks Aren’t Stubs\n\n[MAS]\n\nmartinfowler.com\n\njMock\n\nPaper: Mock Roles, Not Objects\n\n[MRNO]\n\nACM (OOPSLA)\n\nUTWJ\n\nBook: Unit Testing in Java\n\n[UTwJ]\n\nMorgan Kaufmann\n\nOMG\n\nObject Management Group’s CORBA speciﬁ cations\n\nOMG\n\nPragmatic\n\nBook: Pragmatic Unit Testing with NUnit\n\n[PUT]\n\nPragmatic Pro- grammers\n\nRecipes\n\nBook: JUnit Recipes\n\nManning\n\nxUnit Terminology Cross-Reference\n\nThe following table maps the terminology used in this book to the terminology used by speciﬁ c members of the xUnit family. This list is not intended to be exhaustive but rather is meant to illustrate the adaptations of the standard xUnit terminology to the idioms and culture of each language and community.\n\n1 CORBA is an acronym for Common Object Request Broker Architecture. This standard is defined by the Object Management Group.\n\nwww.it-ebooks.info\n\nTool\n\nLanguage\n\nJava 1.4\n\nJava 5\n\n.NET\n\n.NET\n\n.NET\n\n.NET\n\n.NET\n\nPHP\n\nBook Term\n\nxUnit Member\n\nTestcase Class\n\nJUnit 3.8.2 Subclass of\n\nTestCase\n\nJUnit 4.0+\n\nimport org. junit.Test\n\nCsUnit\n\n[TestFixture]\n\nNUnit 2.0\n\n[TestFixture]\n\nNUnit 2.1+\n\n[TestFixture]\n\nMbUnit 2.0 [TestFixture]\n\nMSTest\n\n[TestClass]\n\nPHPUnit\n\nSubclass of TestCase\n\nTest Suite Factory\n\nTest Method\n\nFixture setup\n\nFixture teardown\n\nSuite Fixture Setup\n\nSuite Fixture Teardown Test\n\nExpected Exception\n\nstatic suite() testXxx() setUp()\n\ntearDown() Not\n\nNot\n\nSubclass of\n\napplicable applicable Expected\n\nException Test\n\nstatic suite() @Test\n\n@Before @After\n\n@Before @After Class\n\nClass\n\n@Exception\n\n[Suite]\n\n[Test]\n\n[SetUp]\n\n[TearDown] Not\n\nNot\n\n[Expected\n\napplicable applicable Exception()]\n\n[Suite]\n\n[Test]\n\n[SetUp]\n\n[TearDown] Not\n\nNot\n\n[Expected\n\napplicable applicable Exception()]\n\n[Suite]\n\n[Test]\n\n[SetUp]\n\n[TearDown]\n\n[Test Fixture SetUp]\n\n[TestFixture [Expected TearDown] Exception()]\n\n[Suite]\n\n[Test]\n\n[SetUp]\n\n[TearDown]\n\n[Fixture Setup]\n\n[Fixture Teardown] Exception()]\n\n[Expected\n\nNot applicable Method]\n\n[Test\n\n[Test Initialize] Cleanup]\n\n[Test\n\n[Class Initialize] Cleanup]\n\n[Class\n\n[Expected Exception()]\n\nstatic suite() testXxx() setUp()\n\ntearDown() Not\n\nNot\n\nSubclass of\n\napplicable applicable Expected\n\nException Test\n\nContinued...\n\nwww.it-ebooks.info\n\nR e f e r e n c e\n\nC r o s s -\n\nT e r m n o o g y\n\ni\n\nl\n\nx U n i t\n\nx U n i t T e r m i n o l o g y C r o s s - R e f e r e n c e\n\n7 4 5\n\nTool\n\nBook Term\n\nLanguage\n\nxUnit Member\n\nTestcase Class\n\nPython\n\nPyUnit\n\nSubclass of unittest. TestCase\n\nRuby\n\nTest::Unit\n\nSubclass of Test::Unit:: TestCase\n\nSmalltalk\n\nSUnit\n\nSuperclass: TestCase\n\nVB 6\n\nVbUnit\n\nImplements IFixture\n\nSAP ABAP ABAP Unit FOR\n\nTESTING\n\nTest Suite Factory\n\nTest Method\n\nFixture Fixture Setup\n\nTeardown Teardown\n\nTest Loader()\n\ntestXxx\n\nsetUp\n\ntearDown\n\nClassname. suite()\n\ntestXxx() setup()\n\nteardown\n\nTestSuite named:\n\ntestXxx\n\nsetUp\n\ntearDown\n\nImplements TestXxx() IFixture_ IFixture_ ISuite\n\nSetup()\n\nTearDown\n\nAutomatic Any\n\nsetup\n\nteardown\n\nwww.it-ebooks.info\n\nR e f e r e n c e\n\nC r o s s -\n\nT e r m n o o g y\n\ni\n\nl\n\nSuite Fixture Test\n\nSuite Fixture\n\nNot applicable applicable\n\nNot\n\nNot applicable applicable\n\nNot\n\nTo be determined determined\n\nTo be\n\nIFixture Frame_ Create()\n\nIFixture Frame_ Destroy\n\nclass_setup class_\n\nteardown\n\nx U n i t\n\nExpected Exception\n\nassert raise\n\nassert_raise\n\nshould:raise:\n\non error...\n\nTo be determined\n\n7 4 6\n\nA p p e n d\n\ni x B\n\nx U n i t T e r m i n o l o g y\n\nAppendix C\n\nxUnit Family Members\n\nThis (incomplete) list of members of the xUnit family of test automation frameworks is included here to illustrate the diversity of the family and the extent to which automated unit testing is supported in various programming languages. This appendix also includes comments about speciﬁ c capabilities of some members of the family. A much more complete and up-to-date list can be found at http://xprogramming.com/software.htm.\n\nABAP Object Unit\n\nThe member of the xUnit family for SAP’s ABAP programming language. ABAP Object Unit is more or less a direct port of JUnit to ABAP except for the fact that it cannot catch exceptions encountered within the system under test (SUT).\n\nABAP Object Unit is available for download at http://www.abapunittests. com, along with articles about unit testing in ABAP. See ABAP Unit for versions of SAP/ABAP starting with 6.40.\n\nABAP Unit\n\nThe member of the xUnit family for versions of SAP’s ABAP programming lan- guage starting with Basis version 6.40 (NetWeaver 2004s). The most notable aspect of ABAP Unit is its special support that allows tests to be stripped from the code as the code is “transported” from the acceptance test environment to the production environment.\n\nABAP Unit is available directly from SAP AG as part of the NetWeaver 2004s development tools. More information on unit testing in ABAP is available in the SAP documentation and from http://www.abapunittests.com. See ABAP Object Unit for versions of SAP/ABAP prior to Basis version 6.40 (NetWeaver 2004s).\n\n747\n\nwww.it-ebooks.info\n\nxUnit Family Members\n\n748\n\nxUnit Family Members\n\nAppendix C xUnit Family Members\n\nCppUnit\n\nThe member of the xUnit family for the C++ programming language. It is available for download from http://cppunit.sourceforge.net. Another option for some .NET programmers is NUnit.\n\nCsUnit\n\nThe member of the xUnit family for the C# programming language. It is available from http://www.csunit.org. Another option for .NET programmers is NUnit.\n\nCUnit\n\nThe member of the xUnit family for the C programming language. Details can be found at http://cunit.sourceforge.net/doc/index.html.\n\nDbUnit\n\nAn extension of the JUnit framework intended to simplify testing of databases. It can be downloaded from http://www.dbunit.org/.\n\nIeUnit\n\nThe member of the xUnit family for testing Web pages rendered in Microsoft’s Internet Explorer browser using JavaScript and DHTML. It can be downloaded from http://ieunit.sourceforge.net/.\n\nJBehave\n\nOne of the ﬁ rst of a new generation of xUnit members designed to make tests written as part of TDD more useful Tests as Speciﬁ cation. The main difference between JBehave and more traditional members of the xUnit family is that JBehave eschews the “test” terminology and replaces it with terms more appro- priate for speciﬁ cation—that is, “ﬁ xture” becomes “context,” “assert” becomes “should,” and so on. JBehave is available at http://jbehave.codehaus.org. RSpec is the Ruby equivalent.\n\nJUnit\n\nThe member of the xUnit family for the Java programming language. JUnit was rewritten in late 2005 to take advantage of the annotations introduced in Java 1.5. It can be downloaded from http://www.junit.org.\n\nwww.it-ebooks.info\n\nxUnit Family Members\n\nMbUnit\n\nThe xUnit family member for the C# programming language. MbUnit’s main claim to fame is its direct support for Parameterized Tests. It is available from http://www.nunit.orgmbunit.com. Other options for .NET programmers include NUnit, CsUnit, and MSTest.\n\nMSTest\n\nMicrosoft’s member of xUnit family does not seem to have a formal name other than its namespace Microsoft.VisualStudio.TestTools.UnitTesting but most people refer to it as MSTest. Technically, it is just the name of the Command- Line Test Runner mstest.exe. MSTest’s main claim to fame is that it ships with Visual Studio 2005 Team System. It does not appear to be available in the less expensive versions of Visual Studio or for free download. MSTest includes a number of innovative features, such as direct support for Data-Driven Tests. Information is available on MSDN at http://msdn.microsoft.com/en-us/library/ ms182516.aspx. Other (and cheaper) options for .NET programmers include NUnit, CsUnit, and MbUnit.\n\nNUnit\n\nThe member of the xUnit family for the .NET programming languages. It is available from http://www.nunit.org. Other options for C# programmers in- clude CsUnit, MbUnit, and MSTest.\n\nPHPUnit\n\nThe member of the xUnit family for the PHP programming language. Accord- ing to Sebastian Bergmann, “PHPUnit is a complete port of JUnit 3.8. On top of this original feature set it adds out-of-the-box support for Mock Objects, Code Coverage, Agile Documentation, and Incomplete and Skipped Tests.” More information about PHPUnit can be found at http://www.phpunit.de, including the free book on PHPUnit.\n\nPyUnit\n\nThe member of the xUnit family written to support Python programmers. It is a full port of JUnit. More information can be found at http://pyunit.sourceforge.net/.\n\nwww.it-ebooks.info\n\n749\n\nxUnit Family Members\n\n750\n\nxUnit Family Members\n\nAppendix C xUnit Family Members\n\nRSpec\n\nOne of the ﬁ rst of a new generation of xUnit members designed to make tests written as part of TDD more useful Tests as Speciﬁ cation. The main differ- ence between RSpec and more traditional members of the xUnit family is that RSpec eschews the “test” terminology and replaces it with terms more appropri- ate for speciﬁ cation—for example, “ﬁ xture” becomes “context,” Test Methods becomes “specify,” “assert” becomes “should,” and so on. RSpec is available at http://rspec.rubyforge.org. JBehave is the Java equivalent.\n\nrunit\n\nOne member of the xUnit family for the Ruby programming language. It is a wrapper on Test::Unit that adds additional functionality. It is available at www.rubypeople.org.\n\nSUnit\n\nThe self-proclaimed “mother of all unit-testing frameworks.” SUnit is the mem- ber of the xUnit family for the Smalltalk programming language. It is available for download at http://sunit.sourceforge.net.\n\nTest::Unit\n\nThe member of the xUnit family for the Ruby programming language. It is available for download from http://www.rubypeople.org and comes as part of the “Ruby Development Tools” feature for the Eclipse IDE framework.\n\nTestNG\n\nA member of the xUnit family for Java that behaves a bit differently from JUnit. TestNG speciﬁ cally supports dependencies between tests and the shar- ing of the test ﬁ xture between Test Methods. More information is available at http://testng.org.\n\nutPLSQL\n\nThe member of the xUnit family for the PLSQL database programming lan- guage. You can get more information and download the source for this tool at http://utplsql.sourceforge.net/. A plug-in that integrates utPLSQL into the Oracle toolset is available at http://www.ounit.com.\n\nwww.it-ebooks.info\n\nxUnit Family Members\n\nVB Lite Unit\n\nAnother member of the xUnit family written to support Visual Basic and VBA (Visual Basic for Applications). “VB Lite Unit is a reliable, lightweight unit-testing tool for Visual Basic and VBA written by Steve Jorgensen. The driving principle behind VB Lite Unit was to create the simplest, most reliable unit-testing tool possible that would still do everything that usually matters for doing test-driven development in VB 6 or VBA. Things that don’t work or don’t work reliably in VB and VBA are avoided, such as attempts at introspection to identify the test methods.” Another option for VB and VBA programmers is VbUnit. For VB.NET programmers, options include NUnit, CsUnit, and MbUnit.\n\nVbUnit\n\nThe member of the xUnit family written to support Visual Basic 6.0. It was the ﬁ rst member of the xUnit family to support Suite Fixture Setup and introduced the concept of calling a Testcase Class “test ﬁ xture.”\n\nOne major quirk of VbUnit is that when an Assertion Method fails the test, it writes the messages into the failure log immediately rather than just raising an error that is then caught by the Test Runner. The practical implication of this behavior is that it becomes difﬁ cult to test Custom Assertions because the messages in the logs are not prevented by the normal Expected Exception Test construct. The work-around is to run the Custom Assertion Tests inside an “Encapsulated Test Runner.”\n\nAnother quirk is that VbUnit is one of the few members of the xUnit family that is not free (as in beer). It is available from http://www.vbunit.org. There used to be a free version available—who knows, it may reappear some day. Another option for VB and VBA programmers is VB Lite Unit. For VB.NET program- mers, options include NUnit, CsUnit, and MbUnit.\n\nxUnit\n\nThe generic name for any Test Automation Framework for unit testing that is patterned on JUnit or SUnit. The xUnit test framework for most languages can be found at http://xprogramming.com or http://en.wikipedia.org/wiki/XUnit. Another place to look for both unit test and customer test tools is http://www. opensourcetesting.org.\n\nwww.it-ebooks.info\n\n751\n\nxUnit Family Members\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nAppendix D\n\nTools\n\nThe following tools are mentioned at some point within this book. This section describes their purpose and how they relate to xUnit test automation in just a wee bit more detail.\n\nAnt\n\nA build automation tool used in the Java community. NAnt is the equivalent for .NET projects.\n\nAntHill\n\nA continuous integration tool used in the Java community.\n\nBPT\n\nA commercial Scripted Test tool that allows less technically advanced users to compose tests from reusable test components that are the result of Refactored Recorded Tests. It can also be used to specify reusable test components to be built by more technically oriented test automaters. More information can be found on Mercury Interactive’s Web site. As this book went to press, Mercury Interactive was in the process of being acquired by Hewlett-Packard, so the URL may have changed.\n\nCanoo WebTest\n\nA framework for preparing Scripted Tests written in XML. Conceptually, Canoo WebTest is similar to Fit in that it allows us to deﬁ ne our own domain-speciﬁ c testing language for deﬁ ning customer tests. More information can be found at http://webtest.canoo.com and http://webtest-community.canoo.com.\n\n753\n\nwww.it-ebooks.info\n\nTools\n\n754\n\nTools\n\nAppendix D Tools\n\nCruise Control\n\nA continuous integration tool used in the Java community. Cruise Control.net is the equivalent for .NET projects.\n\nDDSteps\n\nA Data-Driven Test extension for JUnit. “DDSteps is a JUnit extension for building data driven test cases. In a nutshell, DDSteps lets you parameterize your test cases, and run them more than once using different data.” See http:// www.ddsteps.org for more information.\n\nEasyMock\n\nA static Mock Object generation toolkit for Java tests. Because EasyMock uses a Conﬁ guration Mode for specifying the expectations, the tests look a bit strange and may take a bit of getting used to. More information can be found at http:// www.easymock.org.\n\neCATT\n\nThe Recorded Test tool that comes with SAP’s development tools. More infor- mation can be found at http://www.sap.com and at http://www.sdn.sap.com.\n\nEclipse\n\nA Java integrated development environment (IDE) and platform for rich client applications. Eclipse was originally created by IBM and is now managed by the Eclipse Foundation. Several of the language-speciﬁ c plug-ins are integrated with the corresponding xUnit family member. For example, the Java IDE includes JUnit and the Ruby Development Tools IDE includes Test::Unit. Eclipse is available for download from http://www.eclipse.org.\n\nFit\n\nThe framework conceived by Ward Cunningham that made it possible for cus- tomers to write automated tests. Fit separates the work of deﬁ ning the tests using tables in Web pages or spreadsheets from the programming work of exercising the SUT. While Fit was once a particular tool, it is now a speciﬁ cation for a fam- ily of tools implemented in a variety of languages, including Java, .NET, Ruby,\n\nwww.it-ebooks.info\n\nand Python. Some members of the family are simply test execution frameworks; others, such as Fitnesse, include test authoring and versioning capabilities. All should implement the same set of standard ﬁ xtures. More information can be found at Ward’s Web site (http://ﬁ t.c2.com) or in the book [FitB] he co-wrote with Rick Mudgridge.\n\nFitNesse\n\nA Fit test authoring tool conceived by (Uncle) Bob Martin of Object Mentor. FitNesse provides a wiki-like test authoring system with a set of predeﬁ ned Fit ﬁ xtures that makes it possible for customers to write and run automated tests. More information can be found at http://www.ﬁ tnesse.org.\n\nHttpUnit\n\nA front end that layers on top of JUnit to allow tests to exercise a Web applica- tion via the HTTP protocol. HttpUnit bypasses the browser, so it is not suitable for use with applications that make extensive use of on-page scripting (e.g., AJAX). See http://httpunit.sourceforge.net for more information.\n\nIdea\n\nA Java IDE that offers rich support for refactoring. The Idea Web site [JBrains] contains fairly detailed descriptions of many of the refactorings. The same group also offers a very popular refactoring plug-in for Visual Studio, called ReSharper.\n\nJFCUnit\n\nA JUnit front end that layers on top of HttpUnit to allow tests to exercise a Web application via the HTTP protocol. JFCUnit provides a number of Test Utility Methods that form a Higher-Level Language for expressing tests of Web applications. Because it is a layer on top of HttpUnit, it bypasses the browser. Thus JFCUnit is not suitable for use with applications that make extensive use of on-page scripting (e.g., AJAX). See http://jfcunit.sourceforge.net for more information.\n\nJMock\n\nA widely used dynamic Mock Object framework for Java tests. The ﬂ uent Conﬁ guration Interface used for specifying the expectations makes the tests highly readable. More information can be found at http://www.jmock.org.\n\nwww.it-ebooks.info\n\nTools\n\n755\n\nTools\n\n756\n\nTools\n\nAppendix D Tools\n\nNMock\n\nA widely used dynamic Mock Object framework for .NET tests. The ﬂ uent Conﬁ guration Interface used for specifying the expectations makes the tests highly readable. More information can be found at http://nmock.org.\n\nQTP (QuickTest Professional)\n\nA commercial Recorded Test tool that allows less technically advanced users to record tests as they use an application. In conjunction with the “Expert View” of the Recorded Tests, QTP can also be used to refactor the tests into reusable test components that are appropriate for use by less technically adept test automa- ters. More information can be found on Mercury Interactive’s Web site. As this book went to press, Mercury Interactive was in the process of being acquired by Hewlett-Packard, so the URL has probably changed.\n\nReSharper\n\nA refactoring plug-in for Visual Studio by JetBrains, the makers of the Idea IDE. Their Web site [JBrains] contains fairly detailed descriptions of many of the refactorings.\n\nVisual Studio\n\nMicrosoft’s integrated development environment intended for developing .NET applications software. Visual Studio comes in several versions (at various price points), some of which include MSTest and code/test refactoring support. Third- party plug-ins are also available for both refactoring (see [JBrains]) and xUnit (see CsUnit, MbUnit, and NUnit).\n\nWatir\n\n“Web Application Testing in Ruby.” This set of components allows us to drive Internet Explorer from Scripted Tests written in the Ruby programming language. More information can be found at http://wtr.rubyforge.org/.\n\nwww.it-ebooks.info\n\nAppendix E\n\nGoals and Principles\n\nName\n\nPage\n\nRelation\n\nBase Name\n\nChapter\n\nBug Repellent\n\n22\n\nBug Repellent\n\nChapter 3, Goals of Test Automation\n\nCommunicate Intent\n\n41\n\nCommunicate Intent\n\nChapter 5, Principles of Test Automation\n\nDefect Localization\n\n22\n\nDefect Localization\n\nChapter 3, Goals of Test Automation\n\nDesign for Testability\n\n40\n\nDesign for Testability\n\nChapter 5, Principles of Test Automation\n\nDo No Harm\n\n24\n\nDo No Harm\n\nChapter 3, Goals of Test Automation\n\nDon’t Modify the SUT\n\n41\n\nDon’t Modify the SUT\n\nChapter 5, Principles of Test Automation\n\nEnsure Commen- surate Effort and Responsibility\n\n47\n\nEnsure Commen- Chapter 5, Principles surate Effort and Responsibility\n\nof Test Automation\n\nExecutable Speciﬁ cation\n\n22\n\nAlias\n\nTests as Speciﬁ cation\n\nChapter 3, Goals of Test Automation\n\nExpressive Tests\n\n28\n\nExpressive Tests\n\nChapter 3, Goals of Test Automation\n\nFront Door First\n\n40\n\nAlias\n\nUse the Front Door First\n\nChapter 5, Principles of Test Automation\n\nFully Automated Test\n\n26\n\nFully Automated Chapter 3, Goals of Test\n\nTest Automation\n\nHigher Level Language\n\n41\n\nAlias\n\nCommunicate Intent\n\nChapter 5, Principles of Test Automation\n\nContinued...\n\n757\n\nwww.it-ebooks.info\n\nGoals and Principles\n\n758\n\nGoals and Principles\n\nAppendix E Goals and Principles\n\nName\n\nPage\n\nRelation\n\nBase Name\n\nChapter\n\nIndependent Test 42\n\nAlias\n\nKeep Tests Independent\n\nChapter 5, Principles of Test Automation\n\nIsolate the SUT\n\n43\n\nIsolate the SUT\n\nChapter 5, Principles of Test Automation\n\nKeep Test Logic Out of Production Code\n\n45\n\nKeep Test Logic Out of Production Code\n\nChapter 5, Principles of Test Automation\n\nKeep Tests Independent\n\n42\n\nKeep Tests Independent\n\nChapter 5, Principles of Test Automation\n\nMinimize Test Overlap\n\n44\n\nMinimize Test Overlap\n\nChapter 5, Principles of Test Automation\n\nMinimize Untestable Code\n\n44\n\nMinimize Untestable Code\n\nChapter 5, Principles of Test Automation\n\nNo Test Logic in 45 Production Code\n\nKeep Test Logic Out Chapter 5, Principles of of Production Code\n\nTest Automation\n\nNo Test Risk\n\n21\n\nAlias\n\nDo No Harm\n\nChapter 5, Principles of Test Automation\n\nRepeatable Test\n\n26\n\nRepeatable Test\n\nChapter 3, Goals of Test Automation\n\nRobust Test\n\n29\n\nRobust Test\n\nChapter 3, Goals of Test Automation\n\nSafety Net\n\n24\n\nAlias\n\nTests as Safety Net Chapter 3, Goals of\n\nTest Automation\n\nSelf-Checking Test\n\n26\n\nSelf-Checking Test\n\nChapter 3, Goals of Test Automation\n\nSeparation of Concerns\n\n28\n\nSeparation of Concerns\n\nChapter 3, Goals of Test Automation\n\nSimple Tests\n\n28\n\nSimple Tests\n\nChapter 3, Goals of Test Automation\n\nSingle Condition 45 Test\n\nAlias\n\nVerify One Condition per Test\n\nChapter 5, Principles of of Test Automation\n\nSingle Glance Readable\n\n41\n\nAlias\n\nCommunicate Intent\n\nChapter 5, Principles of Test Automation\n\nTest Concerns Separately\n\n47\n\nTest Concerns Separately\n\nChapter 5, Principles of Test Automation\n\nTest-Driven Development\n\n40\n\nAlias\n\nWrite the Tests First\n\nChapter 5, Principles of Test Automation\n\nwww.it-ebooks.info\n\nName\n\nTest First Development\n\nTests as Documentation\n\nTests as Safety Net\n\nTests as Speciﬁ cation\n\nUse the Front Door First\n\nVerify One Condition per Test\n\nWrite the Tests First\n\nPage\n\n40\n\n23\n\n24\n\n22\n\n40\n\n45\n\n40\n\nRelation\n\nBase Name\n\nAlias\n\nWrite the Tests First\n\nTests as Documentation\n\nTests as Safety Net\n\nTests as Speciﬁ cation\n\nUse the Front Door First\n\nVerify One Condition per Test\n\nWrite the Tests First\n\nwww.it-ebooks.info\n\nGoals and Principles\n\nChapter\n\nChapter 5, Principles of Test Automation\n\nChapter 3, Goals of Test Automation\n\nChapter 3, Goals of Test Automation\n\nChapter 3, Goals of Test Automation\n\nChapter 5, Principles of Test Automation\n\nChapter 5, Principles of Test Automation\n\nChapter 5, Principles of Test Automation\n\n759\n\nGoals and Principles\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nAppendix F\n\nSmells, Aliases, and Causes\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nAssertion Roulette\n\n224\n\nAssertion Roulette\n\nChapter 16, Behavior Smells\n\nAsynchronous Code\n\n210\n\nCause of\n\nHard-to-Test Code\n\nChapter 15, Code Smells\n\nAsynchronous Test\n\n255\n\nCause of\n\nSlow Tests\n\nChapter 16, Behavior Smells\n\nBehavior Sensitivity\n\n242\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nBuggy Tests\n\n260\n\nBuggy Tests\n\nChapter 17, Project Smells\n\nComplex Teardown\n\n206\n\nCause of\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nComplex Test\n\n186\n\nAlias\n\nObscure Test\n\nChapter 15, Code Smells\n\nConditional Test 200 Logic\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nConditional Veriﬁ cation Logic\n\n203\n\nCause of\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nContext Sensitivity\n\n245\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nCut-and-Paste Code Reuse\n\n214\n\nCause of\n\nTest Code Duplication\n\nChapter 15, Code Smells\n\nData Sensitivity\n\n243\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nDevelopers Not Writing Tests\n\n263\n\nDevelopers Not Chapter 17, Project Writing Tests\n\nSmells\n\nContinued...\n\n761\n\nwww.it-ebooks.info\n\nSmells, Aliases, and Causes\n\n762\n\nSmells, Aliases, and Causes\n\nAppendix F Smells, Aliases, and Causes\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nEager Test\n\n224\n\nCause of\n\nAssertion Roulette Chapter 16, Behavior\n\nSmells\n\nEquality Pollution\n\n221\n\nCause of\n\nTest Logic in Production\n\nChapter 15, Code Smells\n\nErratic Test\n\n228\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nFlexible Test\n\n202\n\nCause of\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nFor Tests Only\n\n219\n\nCause of\n\nTest Logic in Production\n\nChapter 15, Code Smells\n\nFragile Fixture\n\n246\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nFragile Test\n\n239\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nFrequent Debugging\n\n248\n\nFrequent Debugging\n\nChapter 16, Behavior Smells\n\nGeneral Fixture\n\n190\n\nCause of\n\nObscure Test\n\nChapter 15, Code Smells\n\nHard-to-Test Code\n\n209\n\nHard-to-Test Code\n\nChapter 15, Code Smells\n\nHard-Coded Dependency\n\n210\n\nAlias\n\nHard-to-Test Code\n\nChapter 15, Code Smells\n\nHard-Coded Test Data\n\n194\n\nCause of\n\nObscure Test\n\nChapter 15, Code Smells\n\nHigh Test Maintenance Cost\n\n265\n\nHigh Test Maintenance Cost\n\nChapter 17, Project Smells\n\nHighly Coupled 210 Code\n\nCause of\n\nHard-to-Test Code\n\nChapter 15, Code Smells\n\nIndented Test Code\n\n200\n\nAlias\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nIndirect Testing\n\n196\n\nCause of\n\nObscure Test\n\nChapter 15, Code Smells\n\nInfrequently Run Tests\n\n268\n\nCause of\n\nProduction Bugs\n\nChapter 17, Project Smells\n\nwww.it-ebooks.info\n\nName\n\nPage\n\nInteracting Test Suites\n\n231\n\nInteracting Tests\n\n229\n\nInterface Sensitivity\n\n241\n\nIrrelevant Information\n\n192\n\nLonely Test\n\n232\n\nLong Test\n\n186\n\nLost Test\n\n269\n\nManual Debugging\n\n248\n\nManual Event Injection\n\n281\n\nManual Fixture Setup\n\n250\n\nManual Intervention\n\n250\n\nManual Result Veriﬁ cation\n\n251\n\nMissing Assertion 226 Message\n\nMissing Unit Test\n\n271\n\nMultiple Test Conditions\n\n207\n\nMystery Guest\n\n188\n\nNeverfail Test\n\n274\n\nNondeterministic 237 Test\n\nSmells, Aliases, and Causes\n\nRelationship Base Name\n\nChapter\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nCause of\n\nObscure Test\n\nChapter 15, Code Smells\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nAlias\n\nObscure Test\n\nChapter 15, Code Smells\n\nCause of\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nAlias\n\nFrequent Debugging\n\nChapter 16, Behavior Smells\n\nCause of\n\nManual Intervention\n\nChapter 16, Behavior Smells\n\nCause of\n\nManual Intervention\n\nChapter 16, Behavior Smells\n\nManual Intervention\n\nChapter 16, Behavior Smells\n\nCause of\n\nManual Intervention\n\nChapter 16, Behavior Smells\n\nCause of\n\nAssertion Roulette\n\nChapter 16, Behavior Smells\n\nCause of\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nCause of\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nCause of\n\nObscure Test\n\nChapter 15, Code Smells\n\nCause of\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nContinued...\n\nwww.it-ebooks.info\n\n763\n\nSmells, Aliases, and Causes\n\n764\n\nSmells, Aliases, and Causes\n\nAppendix F Smells, Aliases, and Causes\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nNot Enough Time\n\n263\n\nCause of\n\nDevelopers Not Chapter 17, Project Writing Tests\n\nSmells\n\nObscure Test\n\n186\n\nObscure Test Smells\n\nChapter 15, Code\n\nOvercoupled Test\n\n246\n\nAlias\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nOverspeciﬁ ed Software\n\n246\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nProduction Bugs\n\n268\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nProduction Logic in Test\n\n204\n\nCause of\n\nConditional Test Logic\n\nChapter 15, Code Smells\n\nReinventing the Wheel\n\n215\n\nCause of\n\nTest Code Duplication\n\nChapter 15, Code Smells\n\nResource Leakage\n\n233\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nResource Optimism\n\n233\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nSensitive Equality\n\n246\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nSlow Component 254 Usage\n\nCause of\n\nSlow Tests\n\nChapter 16, Behavior Smells\n\nSlow Tests\n\n253\n\nSlow Tests\n\nChapter 16, Behavior Smells\n\nTest Code Duplication\n\n213\n\nTest Code Duplication\n\nChapter 15, Code Smells\n\nTest Dependency 220 in Production\n\nCause of\n\nTest Logic in Production\n\nChapter 15, Code Smells\n\nTest Logic in Production\n\n217\n\nTest Logic in Production\n\nChapter 15, Code Smells\n\nTest Run War\n\n235\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nToo Many Tests\n\n256\n\nCause of\n\nSlow Tests\n\nChapter 16, Behavior Smells\n\nUnrepeatable Test\n\n234\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nUntestable Test Code\n\n211\n\nCause of\n\nHard-to-Test Code\n\nChapter 15, Code Smells\n\nwww.it-ebooks.info\n\nName\n\nUntested Code\n\nUntested Requirement\n\nVerbose Test\n\nWrong Test Automation Strategy\n\nPage\n\n271\n\n272\n\n186\n\n264\n\nSmells, Aliases, and Causes\n\nRelationship Base Name\n\nChapter\n\nCause of\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nCause of\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nAlias\n\nObscure Test\n\nChapter 15, Code Smells\n\nCause of\n\nDevelopers Not Chapter 17, Project Writing Tests\n\nSmells\n\nwww.it-ebooks.info\n\n765\n\nSmells, Aliases, and Causes\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nAppendix G\n\nPatterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nAbstract Setup Decorator\n\n449\n\nVariation\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nAbstract Test Fixture\n\n638\n\nAlias\n\nTestcase Superclass\n\nChapter 24, Test Organization Patterns\n\nAbstract Testcase\n\n638\n\nAlias\n\nTestcase Superclass\n\nChapter 24, Test Organization Patterns\n\nAllTests Suite\n\n593\n\nVariation\n\nNamed Test Suite Chapter 24, Test\n\nOrganization Patterns\n\nAnonymous Creation Method\n\n417\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nArgument- Describing Message\n\n371\n\nVariation\n\nAssertion Message Chapter 19, xUnit\n\nBasics Patterns\n\nAssertion- Identifying Message\n\n371\n\nVariation\n\nAssertion Message Chapter 19, xUnit\n\nBasics Patterns\n\nAssertion Message\n\n370\n\nAssertion Message Chapter 19, xUnit\n\nBasics Patterns\n\nAssertion Method\n\n362\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nAttachment Method\n\n418\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nContinued...\n\n767\n\nwww.it-ebooks.info\n\nPatterns, Aliases, and Variations\n\n768\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nAutomated Exercise Teardown\n\n505\n\nVariation\n\nAutomated Teardown\n\nChapter 22, Fixture Teardown Patterns\n\nAutomated Fixture Teardown\n\n504\n\nVariation\n\nAutomated Teardown\n\nChapter 22, Fixture Teardown Patterns\n\nAutomated Teardown\n\n503\n\nAutomated Teardown\n\nChapter 22, Fixture Teardown Patterns\n\nAutomated Unit Test\n\n285\n\nAlias\n\nScripted Test\n\nChapter 18, Test Strategy Patterns\n\nBack Door Manipulation\n\n327\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nBack Door Setup\n\n329\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nBack Door Teardown\n\n330\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nBack Door Veriﬁ cation\n\n329\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nBehavior- Exposing Subclass\n\n580\n\nVariation\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nBehavior- Modifying Subclass\n\n580\n\nVariation\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nBehavior Veriﬁ cation\n\n468\n\nBehavior Veriﬁ cation\n\nChapter 21, Result Veriﬁ cation Patterns\n\nBespoke Assertion\n\n474\n\nAlias\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nBuilt-in Test Recording\n\n281\n\nVariation\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nCalculated Values\n\n718\n\nAlias\n\nDerived Value\n\nChapter 27, Value Patterns\n\nCapture/ Playback Test\n\n278\n\nAlias\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nChained Tests\n\n454\n\nChained Tests\n\nChapter 20, Fixture Setup Patterns\n\nCleanup Method 602\n\nVariation\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nwww.it-ebooks.info\n\nName\n\nPage\n\nCommand-Line Test Runner\n\n379\n\nComponent Broker\n\n686\n\nComponent Registry\n\n686\n\nComponent Test\n\n340\n\nConﬁ gurable Mock Object\n\n558\n\nConﬁ gurable Test Double\n\n558\n\nConﬁ gurable Test Spy\n\n558\n\nConﬁ gurable Test Stub\n\n558\n\nConﬁ guration Interface\n\n560\n\nConﬁ guration Mode\n\n560\n\nConstant Value\n\n714\n\nConstructor Injection\n\n680\n\nConstructor Test 351\n\nCreation Method 415\n\nCustom Assertion\n\n474\n\nCustom Assertion Test\n\n477\n\nCustom Equality 476 Assertion\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nVariation\n\nTest Runner\n\nChapter 19, xUnit Basics Patterns\n\nAlias\n\nDependency Lookup\n\nChapter 26, Design- for-Testability Patterns\n\nAlias\n\nDependency Lookup\n\nChapter 26, Design- for-Testability Patterns\n\nVariation\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nLiteral Value\n\nChapter 27, Value Patterns\n\nVariation\n\nDependency Injection\n\nChapter 26, Design- for-Testability Patterns\n\nVariation\n\nTest Method\n\nChapter 19, xUnit Basics Patterns\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n769\n\nPatterns, Aliases, and Variations\n\n770\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nDB Schema per Test-Runner\n\n651\n\nVariation\n\nDatabase Sandbox Chapter 25, Database\n\nPatterns\n\nData Loader\n\n330\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nData Retriever\n\n331\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nData-Driven Test 288\n\nData-Driven Test Chapter 18, Test\n\nStrategy Patterns\n\nData-Driven Test 290 Framework (Fit)\n\nVariation\n\nData-Driven Test Chapter 18, Test Strategy Patterns\n\nData-Driven Test 300 Frameworks\n\nVariation\n\nTest Automation Chapter 18, Test Strategy Patterns Framework\n\nDatabase Extraction Script\n\n331\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nDatabase Partitioning Scheme\n\n652\n\nVariation\n\nDatabase Sandbox Chapter 25, Database\n\nPatterns\n\nDatabase Population Script\n\n330\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nDatabase Sandbox\n\n650\n\nDatabase Sandbox Chapter 25, Database\n\nPatterns\n\nDecorated Lazy Setup\n\n449\n\nVariation\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nDedicated Database Sandbox\n\n651\n\nVariation\n\nDatabase Sandbox Chapter 25, Database\n\nPatterns\n\nDelegated Setup\n\n411\n\nDelegated Setup\n\nChapter 20, Fixture Setup Patterns\n\nDelegated Teardown\n\n511\n\nVariation\n\nIn-line Teardown Chapter 22, Fixture Teardown Patterns\n\nDelta Assertion\n\n485\n\nDelta Assertion\n\nChapter 21, Result Veriﬁ cation Patterns\n\nDependency Initialization Test\n\n352\n\nVariation\n\nTest Method\n\nChapter 19, xUnit Basics Patterns\n\nDependency Injection\n\n678\n\nDependency Injection\n\nChapter 26, Design- for-Testability Patterns\n\nwww.it-ebooks.info\n\nName\n\nDependency Lookup\n\nDerived Expectation\n\nDerived Input\n\nDerived Value\n\nDiagnostic Assertion\n\nDirect Test Method Invocation\n\nDistinct Generated Value\n\nDomain Assertion\n\nDummy\n\nDummy Argument\n\nDummy Attribute\n\nDummy Object\n\nDummy Parameter\n\nDummy Value\n\nDynamically Generated Test Double\n\nEntity Chain Snipping\n\nEquality Assertion\n\nPage\n\n686\n\n719\n\n719\n\n718\n\n476\n\n401\n\n724\n\n476\n\n728\n\n729\n\n729\n\n728\n\n728\n\n728\n\n561\n\n531\n\n365\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nDependency Lookup\n\nChapter 26, Design- for-Testability Patterns\n\nVariation\n\nDerived Value\n\nChapter 27, Value Patterns\n\nVariation\n\nDerived Value\n\nChapter 27, Value Patterns\n\nDerived Value\n\nChapter 27, Value Patterns\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nVariation\n\nTest Enumeration Chapter 19, xUnit\n\nBasics Patterns\n\nVariation\n\nGenerated Value\n\nChapter 27, Value Patterns\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nAlias\n\nDummy Object\n\nChapter 27, Value Patterns\n\nVariation\n\nDummy Object\n\nChapter 27, Value Patterns\n\nVariation\n\nDummy Object\n\nChapter 27, Value Patterns\n\nDummy Object\n\nChapter 27, Value Patterns\n\nAlias\n\nDummy Object\n\nChapter 27, Value Patterns\n\nAlias\n\nDummy Object\n\nChapter 27, Value Patterns\n\nVariation\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nVariation\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n771\n\nPatterns, Aliases, and Variations\n\n772\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nExpectation- Describing Message\n\n371\n\nVariation\n\nAssertion Message Chapter 19, xUnit\n\nBasics Patterns\n\nExpected Behavior\n\n470\n\nAlias\n\nBehavior Veriﬁ cation\n\nChapter 21, Result Veriﬁ cation Patterns\n\nExpected Behavior Speciﬁ cation\n\n470\n\nVariation\n\nBehavior Veriﬁ cation\n\nChapter 21, Result Veriﬁ cation Patterns\n\nExpected Exception Assertion\n\n366\n\nVariation\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nExpected Exception Test\n\n350\n\nVariation\n\nTest Method\n\nChapter 19, xUnit Basics Patterns\n\nExpected Object 464\n\nAlias\n\nState Veriﬁ cation Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nExpected State Speciﬁ cation\n\n464\n\nVariation\n\nState Veriﬁ cation Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nExternal Test Recording\n\n280\n\nVariation\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nFake Database\n\n553\n\nVariation\n\nFake Object\n\nChapter 23, Test Double Patterns\n\nFake Object\n\n551\n\nFake Object\n\nChapter 23, Test Double Patterns\n\nFake Service Layer\n\n553\n\nVariation\n\nFake Object\n\nChapter 23, Test Double Patterns\n\nFake Web Service\n\n553\n\nVariation\n\nFake Object\n\nChapter 23, Test Double Patterns\n\nFile System Test Runner\n\n380\n\nVariation\n\nTest Runner\n\nChapter 19, xUnit Basics Patterns\n\nFinder Method\n\n600\n\nVariation\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nFixture Setup Testcase\n\n456\n\nVariation\n\nChained Tests\n\nChapter 20, Fixture Setup Patterns\n\nFour-Phase Test\n\n358\n\nFour-Phase Test\n\nChapter 19, xUnit Basics Patterns\n\nFramework- Invoked Setup\n\n424\n\nAlias\n\nImplicit Setup\n\nChapter 20, Fixture Setup Patterns\n\nwww.it-ebooks.info\n\nName\n\nFramework- Invoked Teardown\n\nFresh Context\n\nFresh Fixture\n\nFuzzy Equality Assertion\n\nGarbage- Collected Teardown\n\nGenerated Value\n\nGlobal Fixture\n\nGraphical Test Runner\n\nGuard Assertion\n\nHand-Built Test Double\n\nHand-Scripted Test\n\nHand-Written Test\n\nHard-Coded Mock Object\n\nHard-Coded Setup Decorator\n\nHard-Coded Test Double\n\nHard-Coded Test Stub\n\nHard-Coded Value\n\nPage\n\n516\n\n311\n\n311\n\n365\n\n500\n\n723\n\n430\n\n378\n\n490\n\n560\n\n285\n\n285\n\n568\n\n449\n\n568\n\n568\n\n714\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nAlias\n\nImplicit Teardown Chapter 22, Fixture Teardown Patterns\n\nAlias\n\nFresh Fixture\n\nChapter 18, Test Strategy Patterns\n\nFresh Fixture\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nGarbage- Collected Teardown\n\nChapter 22, Fixture Teardown Patterns\n\nGenerated Value\n\nChapter 27, Value Patterns\n\nVariation\n\nPrebuilt Fixture\n\nChapter 20, Fixture Setup Patterns\n\nVariation\n\nTest Runner\n\nChapter 19, xUnit Basics Patterns\n\nGuard Assertion\n\nChapter 21, Result Veriﬁ cation Patterns\n\nVariation\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nScripted Test\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nScripted Test\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nLiteral Value\n\nChapter 27, Value Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n773\n\nPatterns, Aliases, and Variations\n\n774\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nHooked Setup\n\n424\n\nAlias\n\nImplicit Setup\n\nChapter 20, Fixture Setup Patterns\n\nHooked Teardown\n\n516\n\nAlias\n\nImplicit Teardown Chapter 22, Fixture Teardown Patterns\n\nHumble Container Adapter\n\n698\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nHumble Dialog\n\n696\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nHumble Executable\n\n697\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nHumble Object\n\n695\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nHumble Transaction Controller\n\n697\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nImmutable Shared Fixture\n\n323\n\nVariation\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nImplicit Setup\n\n424\n\nImplicit Setup\n\nChapter 20, Fixture Setup Patterns\n\nImplicit Teardown\n\n516\n\nImplicit Teardown Chapter 22, Fixture Teardown Patterns\n\nImposter\n\n522\n\nAlias\n\nTest Double\n\nChapter 23, Test Double Patterns\n\nIn-Database Stored Procedure Test\n\n655\n\nVariation\n\nStored Procedure Chapter 25, Database Test\n\nPatterns\n\nIn-Memory Database\n\n553\n\nVariation\n\nFake Object\n\nChapter 23, Test Double Patterns\n\nIncremental Tabular Test\n\n609\n\nVariation\n\nParameterized Test Chapter 24, Result\n\nVeriﬁ cation Patterns\n\nIncremental Tests 322\n\nVariation\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nIndirect Output Registry\n\n541\n\nVariation\n\nTest Spy\n\nChapter 23, Test Double Patterns\n\nwww.it-ebooks.info\n\nName\n\nPage\n\nIn-line Setup\n\n408\n\nIn-line Teardown 509\n\nInner Test Double\n\n570\n\nInteraction Testing\n\n468\n\nLayer Test\n\n337\n\nLayer-Crossing Test\n\n327\n\nLayered Test\n\n337\n\nLazy Setup\n\n435\n\nLazy Teardown\n\n663\n\nLeftover Fixture\n\n317\n\nLiteral Value\n\n714\n\nLoop-Driven Test\n\n610\n\nMinimal Fixture\n\n302\n\nMinimal Context\n\n302\n\nMock Object\n\n544\n\nNaive In-line Teardown\n\n511\n\nNaive xUnit Test Interpreter\n\n292\n\nNamed State Reaching Method\n\n417\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nIn-line Setup\n\nChapter 20, Fixture Setup Patterns\n\nIn-line Teardown Chapter 22, Fixture Teardown Patterns\n\nVariation\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nBehavior Veriﬁ cation\n\nChapter 21, Result Veriﬁ cation Patterns\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nLazy Setup\n\nChapter 20, Fixture Setup Patterns\n\nVariation\n\nTable Truncation Chapter 25, Database Teardown\n\nPatterns\n\nAlias\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nLiteral Value\n\nChapter 27, Value Patterns\n\nVariation\n\nParameterized Test\n\nChapter 24, Result Veriﬁ cation Patterns\n\nMinimal Fixture\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nMinimal Fixture\n\nChapter 18, Test Strategy Patterns\n\nMock Object\n\nChapter 23, Test Double Patterns\n\nVariation\n\nIn-line Teardown Chapter 22, Fixture Teardown Patterns\n\nVariation\n\nData-Driven Test Chapter 18, Test Strategy Patterns\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n775\n\nPatterns, Aliases, and Variations\n\n776\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nNamed Test Suite 592\n\nNamed Test Suite Chapter 24, Test\n\nOrganization Patterns\n\nObject Attribute 476 Equality Assertion\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nObject Factory\n\n686\n\nAlias\n\nDependency Lookup\n\nChapter 26, Design- for-Testability Patterns\n\nObject Mother\n\n644\n\nVariation\n\nTest Helper\n\nChapter 24, Test Organization Patterns\n\nOne Bad Attribute\n\n719\n\nVariation\n\nDerived Value\n\nChapter 27, Value Patterns\n\nParameter Injection\n\n680\n\nVariation\n\nDependency Injection\n\nChapter 26, Design- for-Testability Patterns\n\nParameterized Anonymous Creation Method\n\n417\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nParameterized Creation Method\n\n417\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nParameterized Setup Decorator\n\n449\n\nVariation\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nParameterized Test\n\n607\n\nParameterized Test Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nPer-Run Fixture\n\n323\n\nVariation\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nPersistence Layer 339 Test\n\nVariation\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nPersistent Fresh Fixture\n\n314\n\nVariation\n\nFresh Fixture\n\nChapter 18, Test Strategy Patterns\n\nPlaceholder\n\n728\n\nAlias\n\nDummy Object\n\nChapter 27, Value Patterns\n\nPoor Man’s Humble Object\n\n699\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nwww.it-ebooks.info\n\nName\n\nPage\n\nPrebuilt Context 429\n\nPrebuilt Fixture\n\n429\n\nPresentation Layer Test\n\n338\n\nPrivate Fixture\n\n311\n\nProcedural Behavior Veriﬁ cation\n\n470\n\nProcedural State 463 Veriﬁ cation\n\nProcedural Test Stub\n\n526\n\nProgrammatic Test\n\n285\n\nPseudo-Object\n\n571\n\nPushdown Decorator\n\n450\n\nRandom Generated Value\n\n724\n\nRecord and Playback Test\n\n278\n\nRecorded Test\n\n278\n\nRefactored Recorded Test\n\n280\n\nRelated Generated Value\n\n725\n\nRemoted Stored 656 Procedure Test\n\nResponder\n\n530\n\nRetrieval Interface\n\n540\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nAlias\n\nPrebuilt Fixture\n\nChapter 20, Fixture Setup Patterns\n\nPrebuilt Fixture\n\nChapter 20, Fixture Setup Patterns\n\nVariation\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nFresh Fixture\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nBehavior Veriﬁ cation\n\nChapter 21, Result Veriﬁ cation Patterns\n\nVariation\n\nState Veriﬁ cation Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nVariation\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nAlias\n\nScripted Test\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nVariation\n\nGenerated Value\n\nChapter 27, Value Patterns\n\nAlias\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nGenerated Value\n\nChapter 27, Value Patterns\n\nVariation\n\nStored Procedure Chapter 25, Database Test\n\nPatterns\n\nVariation\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nVariation\n\nTest Spy\n\nChapter 23, Test Double Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n777\n\nPatterns, Aliases, and Variations\n\n778\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nReuse Test for Fixture Setup\n\n418\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nReused Fixture\n\n317\n\nAlias\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nRobot User Test\n\n278\n\nAlias\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nRobot User Test Framework\n\n299\n\nVariation\n\nTest Automation Chapter 18, Test Strategy Patterns Framework\n\nRow Test\n\n609\n\nAlias\n\nParameterized Test\n\nChapter 24, Test Organization Patterns\n\nSUT API Encapsulation\n\n601\n\nAlias\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nSUT Encapsulation Method\n\n601\n\nVariation\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nSaboteur\n\n530\n\nVariation\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nScripted Test\n\n285\n\nScripted Test\n\nChapter 18, Test Strategy Patterns\n\nSelf Shunt\n\n540\n\nVariation\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nSelf-Describing Value\n\n715\n\nVariation\n\nLiteral Value\n\nChapter 27, Value Patterns\n\nService Layer Test\n\n339\n\nVariation\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nService Locator\n\n686\n\nAlias\n\nDependency Lookup\n\nChapter 26, Design- for-Testability Patterns\n\nSetter Injection\n\n681\n\nVariation\n\nDependency Injection\n\nChapter 26, Design- for-Testability Patterns\n\nSetup Decorator\n\n447\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nShared Context\n\n317\n\nAlias\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nShared Fixture\n\n317\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nwww.it-ebooks.info\n\nName\n\nPage\n\nShared Fixture State Assertion\n\n491\n\nShared Setup Method\n\n424\n\nSimple Success Test\n\n349\n\nSingle-Layer Test 337\n\nSingle-Outcome Assertion\n\n366\n\nSingle Test Suite\n\n593\n\nSlow Tests\n\n318\n\nSpy\n\n538\n\nStale Fixture\n\n317\n\nStandard Context 305\n\nStandard Fixture 305\n\nState-Exposing Subclass\n\n580\n\nState Veriﬁ cation 462\n\nState-Based Testing\n\n462\n\nStated Outcome 366 Assertion\n\nStatically Generated Test Double\n\n561\n\nStored Procedure 654 Test\n\nStub\n\n529\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nVariation\n\nGuard Assertion\n\nChapter 21, Result Veriﬁ cation Patterns\n\nAlias\n\nImplicit Setup\n\nChapter 20, Fixture Setup Patterns\n\nVariation\n\nTest Method\n\nChapter 19, xUnit Basics Patterns\n\nAlias\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nVariation\n\nNamed Test Suite Chapter 24, Test\n\nOrganization Patterns\n\nVariation\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nTest Spy\n\nChapter 23, Test Double Patterns\n\nAlias\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nStandard Fixture Chapter 18, Test Strategy Patterns\n\nStandard Fixture Chapter 18, Test Strategy Patterns\n\nVariation\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nState Veriﬁ cation Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nAlias\n\nState Veriﬁ cation Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nVariation\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nVariation\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nStored Procedure Chapter 25, Database Test\n\nPatterns\n\nAlias\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n779\n\nPatterns, Aliases, and Variations\n\n780\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nStub\n\n728\n\nAlias\n\nDummy Object\n\nChapter 27, Value Patterns\n\nSubclassed Humble Object\n\n700\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nSubclassed Singleton\n\n581\n\nAlias\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nSubclassed Test Double\n\n581\n\nAlias\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nSubcutaneous Test\n\n340\n\nVariation\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nSubset Suite\n\n593\n\nVariation\n\nNamed Test Suite Chapter 24, Test\n\nOrganization Patterns\n\nSubstitutable Singleton\n\n581\n\nAlias\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nSubstituted Singleton\n\n581\n\nVariation\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nSuite of Suites\n\n388\n\nVariation\n\nTest Suite Object Chapter 19, xUnit\n\nBasics Patterns\n\nSuite Fixture Setup\n\n441\n\nSuite Fixture Setup\n\nChapter 20, Fixture Setup Patterns\n\nSymbolic Constant\n\n715\n\nVariation\n\nLiteral Value\n\nChapter 27, Value Patterns\n\nTable Truncation 661 Teardown\n\nTable Truncation Chapter 25, Database Teardown\n\nPatterns\n\nTabular Test\n\n609\n\nVariation\n\nParameterized Test\n\nChapter 24, Test Organization Patterns\n\nTeardown Guard Clause\n\n511\n\nVariation\n\nIn-line Teardown Chapter 22, Fixture Teardown Patterns\n\nTemporary Test Stub\n\n530\n\nVariation\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nTest Automation 298 Framework\n\nTest Automation Chapter 18, Test Strategy Patterns Framework\n\nTest Bed\n\n429\n\nAlias\n\nPrebuilt Fixture\n\nChapter 20, Fixture Setup Patterns\n\nwww.it-ebooks.info\n\nName\n\nPage\n\nTest Discovery\n\n393\n\nTest Double\n\n522\n\nTest Double Class\n\n569\n\nTest Double Subclass\n\n580\n\nTest Double as Back Door\n\n332\n\nTest Enumeration 399\n\nTest Fixture\n\n373\n\nTest Fixture Registry\n\n644\n\nTest Helper\n\n643\n\nTest Helper Class\n\n645\n\nTest Helper Mixin\n\n639\n\nTest Helper Object\n\n645\n\nTest Hook\n\n709\n\nTest Method\n\n348\n\nTest Method Discovery\n\n394\n\nTest Method Enumeration\n\n401\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nTest Discovery\n\nChapter 19, xUnit Basics Patterns\n\nTest Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nTest Enumeration Chapter 19, xUnit\n\nBasics Patterns\n\nAlias\n\nTestcase Class\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTest Helper\n\nChapter 24, Test Organization Patterns\n\nTest Helper\n\nChapter 24, Test Organization Patterns\n\nVariation\n\nTest Helper\n\nChapter 24, Test Organization Patterns\n\nVariation\n\nTestcase Superclass\n\nChapter 24, Test Organization Patterns\n\nVariation\n\nTest Helper\n\nChapter 24, Test Organization Patterns\n\nTest Hook\n\nChapter 26, Design- for-Testability Patterns\n\nTest Method\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTest Discovery\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTest Enumeration Chapter 19, xUnit\n\nBasics Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n781\n\nPatterns, Aliases, and Variations\n\n782\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nTest Method Selection\n\n404\n\nVariation\n\nTest Selection\n\nChapter 19, xUnit Basics Patterns\n\nTest Object Registry\n\n503\n\nAlias\n\nAutomated Teardown\n\nChapter 22, Fixture Teardown Patterns\n\nTest Runner\n\n377\n\nTest Runner\n\nChapter 19, xUnit Basics Patterns\n\nTest Selection\n\n403\n\nTest Selection\n\nChapter 19, xUnit Basics Patterns\n\nTest Spy\n\n538\n\nTest Spy\n\nChapter 23, Test Double Patterns\n\nTest Spy\n\n568\n\nAlias\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nTest Stub\n\n529\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nTest Suite Enumeration\n\n400\n\nVariation\n\nTest Enumeration Chapter 19, xUnit\n\nBasics Patterns\n\nTest Suite Factory\n\n399\n\nAlias\n\nTest Enumeration Chapter 19, xUnit\n\nBasics Patterns\n\nTest Suite Object 387\n\nTest Suite Object Chapter 19, xUnit\n\nBasics Patterns\n\nTest Suite Object 293 Generator\n\nVariation\n\nData-Driven Test Chapter 18, Test Strategy Patterns\n\nTest Suite Object 293 Simulator\n\nVariation\n\nData-Driven Test Chapter 18, Test Strategy Patterns\n\nTest Suite Procedure\n\n388\n\nVariation\n\nTest Suite Object Chapter 19, xUnit\n\nBasics Patterns\n\nTest Tree Explorer\n\n380\n\nVariation\n\nTest Runner\n\nChapter 19, xUnit Basics Patterns\n\nTest Utility Method\n\n599\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nTest Utility Test\n\n603\n\nVariation\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nTest-Speciﬁ c Extension\n\n579\n\nAlias\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nTest-Speciﬁ c Subclass\n\n579\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nwww.it-ebooks.info\n\nName\n\nTestcase Class\n\nTestcase Class Discovery\n\nTestcase Class per Method\n\nTestcase Class per User Story\n\nTestcase Class Selection\n\nTestcase Class Suite\n\nTestcase Class per Class\n\nTestcase Class per Feature\n\nTestcase Class per Fixture\n\nTestcase Object\n\nTestcase Superclass\n\nTesting by Layers\n\nThe xUnit Family\n\nTransaction Rollback Teardown\n\nTransient Fresh Fixture\n\nPage\n\n373\n\n394\n\n625\n\n625\n\n404\n\n388\n\n617\n\n624\n\n631\n\n382\n\n638\n\n337\n\n300\n\n668\n\n314\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nTestcase Class\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTest Discovery\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTestcase Class per Feature\n\nChapter 24, Test Organization Patterns\n\nVariation\n\nTestcase Class per Feature\n\nChapter 24, Test Organization Patterns\n\nVariation\n\nTest Selection\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTest Suite Object Chapter 19, xUnit\n\nBasics Patterns\n\nTestcase Class per Class\n\nChapter 24, Test Organization Patterns\n\nTestcase Class per Feature\n\nChapter 24, Test Organization Patterns\n\nTestcase Class per Fixture\n\nChapter 24, Test Organization Patterns\n\nTestcase Object\n\nChapter 19, xUnit Basics Patterns\n\nTestcase Superclass\n\nChapter 24, Test Organization Patterns\n\nAlias\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nTest Automation Chapter 18, Test Strategy Patterns Framework\n\nTransaction Rollback Teardown\n\nChapter 25, Database Patterns\n\nVariation\n\nFresh Fixture\n\nChapter 18, Test Strategy Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n783\n\nPatterns, Aliases, and Variations\n\n784\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nTrue Humble Object\n\n699\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nUnﬁ nished Test Assertion\n\n494\n\nUnﬁ nished Test Assertion\n\nChapter 21, Result Veriﬁ cation Patterns\n\nVeriﬁ cation Method\n\n477\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nwww.it-ebooks.info\n\nGlossary\n\nThis glossary contains the author’s deﬁ nitions of the terms used throughout this book.\n\nacceptance test\n\nA customer test that the customer of the software plans to run to help the customer decide whether he or she will accept the software system. Acceptance tests are usually run manually after all automated customer tests have passed. They exercise all layers of the system—from the user interface back to the database—and should include any integration with other systems on which the application depends.\n\naccessor\n\nA method that provides access to an instance variable of an object either by returning its value or by providing a way to set its value.\n\nACID\n\nThe four qualities of transactions that modern databases ensure:\n\nAtomic: A transaction is all or nothing.\n\nConsistent: All operations within a transaction see the same view of the\n\nworld.\n\nIndependent: Transactions are independent of one another (no cross-\n\ntransaction leakage of changes).\n\nDurable: Once committed, the changes made within a transaction are\n\npermanent (they don’t just vanish for no reason!).\n\nagile method\n\nA method of executing projects (typically, but not always, restricted to software) that reduces the cost of change and allows customers of the software to have more control over how much they spend and what they get for their money. Agile\n\n785\n\nwww.it-ebooks.info\n\nAlso known as: user acceptance test (UAT)\n\n786\n\nGlossary\n\nmethods include eXtreme Programming, SCRUM, Feature-Driven Development (FDD), and Dynamic Systems Development Method (DSDM), among many others. A core practice of most agile methods is the use of automated unit tests.\n\nannotation\n\nA way of indicating something about something. JUnit version 4.0 uses annota- tions to indicate which classes are Testcase Classes and which methods are Test Methods; NUnit uses .NET attributes for this purpose.\n\nanonymous inner class\n\nAn inner class in Java that is deﬁ ned without a unique name. Anonymous inner classes are often used when deﬁ ning Hard-Coded Test Doubles.\n\nanti-pattern\n\nA pattern that shouldn’t be used because it is known to produce less than optimal results. Code smells, or their underlying causes, are a kind of anti-pattern.\n\napplication programming interface (API)\n\nThe means by which other software can invoke some piece of functionality. In object-oriented software, an API consists of the classes and their publicly acces- sible methods. In procedural software, it consists of the module or package name plus the publicly accessible procedures.\n\naspect-oriented programming\n\nAn advanced software modularization technique that allows improved separa- tion of concerns by “weaving” cross-cutting concerns into code after the affected software has been built but before it is executed.\n\nassertion\n\nA statement that something should be true. In xUnit-style Test Automation Frameworks, an assertion takes the form of an Assertion Method that fails when the actual outcome passed to it does not match the expected outcome.\n\nwww.it-ebooks.info\n\nGlossary\n\nasynchronous test\n\nA test that runs in a separate thread of control from the system under test (SUT) and interacts with it using asynchronous (i.e., “real”) messages. An asynchro- nous test must coordinate its steps with those of the SUT because this interac- tion is not managed automatically by the runtime system. An asynchronous test may have to include delays to give the SUT enough time to ﬁ nish execution before inspecting the outcome. Contrast this with a synchronous test, which interacts with the SUT via simple method calls.\n\nattribute\n\nA characteristic of something. The members of the xUnit family for the .NET languages use class and method attributes to indicate which classes are Testcase Classes and which methods are Test Methods. The term attribute is also a syn- onym for “instance variable” in some circles.\n\nback door\n\nAn alternative interface to a system under test (SUT) that test software can use to inject indirect inputs into the SUT. A database is a common example of a back door, but it could also be any component that can be either manipulated to return test-speciﬁ c values or replaced by a Test Double. Contrast this with the front door: the application programming interface (API).\n\nBDUF\n\n“Big design up front” is the classic “waterfall” approach to software design. In BDUF, all requirements must be understood early in the project, and the software is designed to support those requirements in a single design “phase.” Contrast this with the emergent design favored by agile projects.\n\nbehavior-driven development\n\nA variation on the test-driven development process wherein the focus of the tests is to clearly describe the expected behavior of the system under test (SUT). The emphasis is on Tests as Documentation rather than merely using tests for veriﬁ cation.\n\nBehavior-driven development can be done using traditional members of the xUnit family. New “members” of the family, however, have been built speciﬁ - cally to emphasize the change in focus. They include changes in terminology\n\nwww.it-ebooks.info\n\n787\n\n788\n\nGlossary\n\n(e.g., “test” becomes “spec”; “ﬁ xture” becomes “context”) and more explicit framework support for clarity of the speciﬁ cation.\n\nbehavior smell\n\nA test smell we encounter while compiling or running tests. We don’t have to be particularly observant to notice behavior smells, as they will present themselves to us via compile errors or test failures. See also: code smell, project smell.\n\nblack box\n\nA piece of software that we treat as an opaque object whose internal workings cannot be seen. Tests written for the black box can verify only externally visible behavior and are independent of the implementation inside the system under test (SUT).\n\nblock\n\nA block of code that can be run. Many programming languages (most notably, Smalltalk and Ruby) use blocks (also known as “block closures”) as a way of passing a chunk of code to a method, which can then run the code in its own context. Java’s anonymous inner classes are a way to achieve the same thing without direct support for blocks. C# uses delegates for the same purpose.\n\nblock closure\n\nSee block.\n\nboundary value\n\nAn input value for a system under test (SUT) that is immediately adjacent to the boundary between two equivalence classes. Tests using two adjacent boundary values help us verify that the behavior changes with exactly the right input and that we don’t have “off by one” problems.\n\nbuilt-in self-test\n\nA means of organizing test code in which the tests live inside the same module or class as the production code and are run automatically when the system is initialized.\n\nwww.it-ebooks.info\n\nGlossary\n\nbusiness logic\n\nThe core logic related to the domain model of a business system. Because busi- ness logic usually reﬂ ects the results of many independent business decisions, it often seems anything but logical!\n\nclass attribute\n\nAn attribute that is placed on a class in the source code to tell the compiler or runtime system that this class is “special.” In some variants of xUnit, class at- tributes are used to indicate that a class is a Testcase Class.\n\nclass method\n\nA method that is associated with a class rather than an object. Class methods can be invoked using a classname.methodname notation [e.g., Assert.assertEquals(message, expected, actual);] and do not require an instance of the class to be invoked. Class methods cannot access instance methods or instance variables of objects; that is, they do not have access to self or this. In Java, a class method is called a static method. Other languages may use different names or keywords.\n\nclass variable\n\nA variable that is associated with a class rather than an instance of the class and is typically used to access information that all instances need to share. In some languages, class variables can be accessed using the syntax classname.variable- name (e.g., TestHelper.lineFeedCharacter;). That is, they do not need to be accessed via self or this. In Java, a class variable is called a static variable. Other lan- guages may use different names or keywords.\n\nclosure\n\nSee block.\n\ncode smell\n\nThe “classic” bad smell, as ﬁ rst described by Martin Fowler in [Ref]. Test au- tomaters must recognize code smells that arise as they maintain test code. Code smells typically affect maintenance cost of tests but may also be early warning signs of behavior smells to follow.\n\nSee also: test smell, behavior smell, project smell.\n\nwww.it-ebooks.info\n\n789\n\n790\n\nGlossary\n\ncomponent\n\nA larger part of the overall system that is often separately deployable. Component- based development involves decomposing the overall functionality into a series of individual components that can be built and deployed separately. This allows sharing of the components between applications that need the same functionality. Each component is a consequence of one or more design decisions, although its behavior may also be traced back to some aspect of the requirements.\n\nComponents can take many forms, depending on the technology being employed. The Windows platform uses dynamic linked libraries (DLLs) or assemblies as components. The Java platform uses Java Archives (JARs). A service-oriented architecture (SOA) uses Web Services as its large-grained components. The components may implement front-end logic (e.g., a “File Open Dialog”) or back-end logic (e.g., a “Customer Persistence” component). A component can and should be veriﬁ ed using component tests before the overall application is tested using customer tests.\n\ncomponent test\n\nA test that veriﬁ es the behavior of some component of the overall system. The component is a consequence of one or more design decisions, although its be- havior may also be traced back to some aspect of the requirements. There is no need for component tests to be readable, recognizable, or veriﬁ able by the customer or business domain expert. Contrast this with a customer test, which is derived almost entirely from the requirements and should be veriﬁ able by the customer, and with a unit test, which veriﬁ es a much smaller component. A component test lies somewhere in between these two extremes.\n\nDuring test-driven development, component tests are written after the cus- tomer tests are written and the overall design is solidiﬁ ed. They are written as the architectural decisions are made but before the individual units are designed or coded. They are usually automated using a member of the xUnit family.\n\nconstructor\n\nA special method used in some object-oriented programming languages to con- struct a brand-new object. It often has the same name as the class and is typically called automatically by the runtime system whenever the special operation new is invoked. A Complete Constructor Method [SBPP] returns a ready-to-use object that requires no additional tweaking; this usually implies arguments must be passed to the constructor.\n\nwww.it-ebooks.info\n\nGlossary\n\ncontinuous integration\n\nThe agile software development practice of integrating software changes continu- ously. In practice, developers typically integrate their changes every few hours to days. Continuous integration often includes the practice of an automated build that is triggered by each check-in. The build process typically runs all automated tests and may even run tests that aren’t run before check-in because they take too long. The build is considered to have “failed” if any tests fail. When the build fails, teams typically consider getting the build working again to be the top priority; only code changes aimed at ﬁ xing the build are allowed until a successful build has occurred.\n\ncontrol point\n\nHow the test asks the system under test (SUT) to do something for it. A control point could be created for the purpose of setting up or tearing down the ﬁ xture or it could be used during the exercise SUT phase of the test. It is a kind of in- teraction point. Some control points are provided strictly for testing purposes; they should not be used by the production code because they bypass input validation or short-circuit the normal life cycle of the SUT or some object on which it depends.\n\ncustomer test\n\nA test that veriﬁ es the behavior of a slice of the visible functionality of the over- all system. The system under test (SUT) may consist of the entire system or a fully functional top-to-bottom slice (“module”) of the system. A customer test should be independent of the design decisions made while building the SUT. That is, we should require the same set of customer tests regardless of how we choose to build the SUT. (Of course, how the customer tests interact with the SUT may be affected by high-level software architecture decisions.)\n\ndata access layer\n\nA way of keeping data access logic from permeating the application code by put- ting it into a separate component that encapsulates the database.\n\ndepended-on component (DOC)\n\nAn individual class or a large-grained component on which the system under test (SUT) depends. The dependency is usually one of delegation via method\n\nwww.it-ebooks.info\n\n791\n\nAlso known as: data access object (DAO), data abstraction layer (DAL)\n\n792\n\nAlso known as: DfT\n\nGlossary\n\ncalls. In test automation, the DOC is primarily of interest in that we need to be able to observe and control its interactions with the SUT to get complete test coverage.\n\ndesign pattern\n\nA pattern that we can use to solve a particular software design problem. Most design patterns are programming language independent; the language-speciﬁ c ones are typically called “coding idioms.” Design patterns were ﬁ rst popularized by the book Design Patterns [GOF].\n\ndesign for testability\n\nA way of ensuring that code is easily tested by making sure that testing require- ments are considered as the code is designed. When doing test-driven develop- ment, design for testability occurs as a natural side effect of development\n\ndeveloper test\n\nAnother name for an automated unit test that is prepared by someone playing the developer role on an eXtreme Programming project.\n\nDfT\n\nSee design for testability.\n\ndirect input\n\nA test may interact with the system under test (SUT) directly via its “front door” or public application programming interface (API) or indirectly via its “back door.” The stimuli injected by the test into the SUT via its front door are direct inputs of the SUT. Direct inputs may consist of method or function calls to an- other component or messages sent on a message channel (e.g., MQ or JMS) and the arguments or contents thereof.\n\ndirect output\n\nA test may interact with the system under test (SUT) directly via its “front door” or public application programming interface (API) or indirectly via its “back door.” The responses received by the test from the SUT via its front door are\n\nwww.it-ebooks.info\n\nGlossary\n\ndirect outputs of the SUT. Direct outputs may consist of the return values of method or function calls, updated arguments passed by reference, exceptions raised by the SUT, or messages received on a message channel (e.g., MQ or JMS) from the SUT.\n\ndocument-driven development\n\nA development process that focuses on producing documents that describe how the code will be structured and then coding from those documents. Docu- ment-driven development is normally associated with “big design up front” (BDUF, also known as “waterfall”) software development. Contrast this with test-driven development, which focuses on producing working code one test at a time.\n\ndomain layer\n\nThe layer of a Layered Architecture [DDD, PEAA, WWW] that corresponds to the domain model. See Eric Evans’ book, Domain-Driven Design [DDD].\n\ndomain model\n\nA model of the problem domain that may form the basis of the object model in the business domain layer of a software application. See Eric Evans’ book, Domain-Driven Design [DDD].\n\nDTO\n\nShort for the Data Transfer Object [CJ2EEP] design pattern.\n\ndynamic binding\n\nDeferring the decision about which piece of software to transfer control to until execution time. The same method name can be used to invoke different behavior (method bodies) based on the class of the object on which it is invoked; the latter class is determined only at execution time. Dynamic binding is the opposite of static binding; it is also called polymorphism (from the Latin, meaning “taking on many shapes”).\n\nEDD\n\nSee example-driven development.\n\nwww.it-ebooks.info\n\n793\n\n794\n\nAlso known as: domain object\n\nGlossary\n\nemergent design\n\nThe opposite of BDUF (big design up front). Emergent design involves letting the right design be discovered as the software slowly evolves to pass one test at a time during test-driven development.\n\nendoscopic testing\n\nA testing technique pioneered by the authors of the original Mock Object paper [ET], which involves testing software from the inside.\n\nentity object\n\nAn object that represents an entity concept from a domain. Entity objects typi- cally have a life cycle that is represented as their state. Contrast this with a service object, which has no single state. EJB Entity Beans are one example of an entity object.\n\nequivalence class\n\nA test condition identiﬁ cation technique that reduces the number of tests required by grouping together inputs that should result in the same output or that should exercise the same logic in the system. This organization allows us to focus our tests on key boundary values at which the expected output changes.\n\nexample-driven development (EDD)\n\nA reframing of the test-driven development process to focus on the “executable speciﬁ cation” aspect of the tests. The act of providing examples is more intuitive to many people; it doesn’t carry the baggage of “testing” software that doesn’t yet exist.\n\nexercise SUT\n\nAfter the ﬁ xture setup phase of testing, the test stimulates the system under test (SUT) logic that is to be tested. This phase of the testing process is called exercise SUT.\n\nwww.it-ebooks.info\n\nGlossary\n\nexpectation\n\nWhat a test expects the system under test (SUT) to have done. When we are using Mock Objects to verify the indirect outputs of the SUT, we load each Mock Object with the expected method calls (including the expected argu- ments); these are called the expectations.\n\nexpected outcome\n\nThe outcome that we verify after exercising the system under test (SUT). A Self-Checking Test veriﬁ es the expected outcome using calls to Assertion Methods.\n\nexploratory testing\n\nInteractive testing of an application without a speciﬁ c script in hand. The tester “explores” the system, making up theories about how it should behave based on what the application has already done and then testing those theories to see if they hold up. While there is no rigid plan, exploratory testing is a disciplined activity that is more likely to ﬁ nd real bugs than rigidly scripted tests.\n\neXtreme Programming\n\nAn agile software development methodology that showcases pair programming, automated unit testing, and short iterations.\n\nfactory\n\nA method, object, or class that exists to build other objects.\n\nfalse negative\n\nA situation in which a test passes even though the system under test (SUT) is not working properly. Such a test is said to give a false-negative indication or a “false pass.”\n\nSee also: false positive.\n\nfalse positive\n\nA situation in which a test fails even though the system under test (SUT) is working properly. Such a test is said to give a false-positive indication or a\n\nwww.it-ebooks.info\n\n795\n\n796\n\nGlossary\n\n“false failure.” The terminology comes from statistical science and relates to our attempt to calculate the probability of some observation error occurring. For example, in medicine we run tests to ﬁ nd out if a medical condition is pres- ent; if it is, the test is “positive.” It is useful to know the probability that a test might indicate that a condition (such as diabetes) is present when it is not—that is, a false “positive.” If we think of software tests as a way of determining whether a condition (a particular defect or bug) is present, a test that reports a defect (a test failure or error) when it is not, in fact, present is giving us a false positive.\n\nSee also: false negative. Wikipedia [Wp] has an extensive description under\n\nthe topic “Type I and type II errors.”\n\nfault insertion test\n\nA kind of test in which a deliberate fault is introduced in one part of the sys- tem to verify that another part reacts to the error appropriately. Initially, the faults were related to hardware but the same concept is now applied to software faults as well. Replacing a depended-on component (DOC) with a Saboteur that throws an exception is an example of a software fault insertion test.\n\nfeature\n\nA testable unit of functionality that can be built onto the evolving software sys- tem. In eXtreme Programming, a user story corresponds roughly to a feature.\n\nFit test\n\nA test that uses the Fit testing framework; most commonly a customer test.\n\nﬁ xture\n\nSee test ﬁ xture (disambiguation).\n\nﬁ xture (Fit)\n\nIn Fit, the Adapter [GOF] that interprets the Fit table and invokes methods on the system under test (SUT), thereby implementing a Data-Driven Test. For meanings in other contexts, see test ﬁ xture (disambiguation), test ﬁ xture (in xUnit), and test ﬁ xture (in NUnit).\n\nwww.it-ebooks.info\n\nGlossary\n\nﬁ xture holding class variable\n\nA class variable of a Testcase Class that is used to hold a reference to the test ﬁ xture. It typically holds a reference to a Shared Fixture.\n\nﬁ xture holding instance variable\n\nAn instance variable of a Testcase Object that is used to hold a reference to the test ﬁ xture. It typically holds a reference to a Fresh Fixture that is set up using Implicit Setup.\n\nﬁ xture holding local variable\n\nA local variable of a Test Method that is used to hold a reference to the test ﬁ x- ture. It typically holds a reference to a Fresh Fixture that is set up within the test method using In-line Setup or returned from Delegated Setup.\n\nﬁ xture setup\n\nBefore the desired logic of the system under test (SUT) can be exercised, the pre- conditions of the test need to be set up. Collectively, all objects (and their states) are called the test ﬁ xture (or test context), and the phase of the test that sets up the test ﬁ xture is called ﬁ xture setup.\n\nﬁ xture teardown\n\nAfter a test is run, the test ﬁ xture that was built by the test should be destroyed. This phase of the test is called ﬁ xture teardown.\n\nﬂ uent interface\n\nA style of object constructor API that results in easy-to-understand statements. The Conﬁ guration Interface provided by the Mock Object toolkit JMock is an example of a ﬂ uent interface.\n\nfront door\n\nThe public application programming interface (API) of a piece of software. Con- trast this with the back door.\n\nwww.it-ebooks.info\n\n797\n\n798\n\nAlso known as: procedure variable, delegate in .NET languages\n\nGlossary\n\nfunction pointer\n\nFrom Wikipedia [Wp]: “A function pointer is a type of pointer in C, C++, D, and other C-like programming languages. When dereferenced, a function pointer in- vokes a function, passing it zero or more arguments like a normal function.”\n\nfunctional test (common usage)\n\nA black-box test of the end-user functionality of an application. The agile com- munity is trying to avoid this usage of “functional test” because of the potential for confusion when talking about verifying functional (as opposed to nonfunctional or extra-functional properties) properties of a unit or component. This book uses the terms “customer test” and “acceptance test” for a functional test of the entire appli- cation and “unit test” for a functional test of an individual unit of the application.\n\nfunctional test (contrast with extra-functional test)\n\nA test that veriﬁ es the functionality implemented by a piece of software. De- pending on the scope of the software, a functional test may be a customer test, a unit test, or a component test.\n\nIn some circles a functional test is a customer test. This usage becomes con- fusing, however, when we talk about testing nonfunctional or extra-functional properties of the system under test (SUT). This book uses the terms “customer test” and “acceptance test” for a functional test of the entire application and “unit test” for a functional test of an individual unit of the application.\n\ngarbage collection\n\nA mechanism that automatically recovers the memory used by any objects that are no longer accessible. Many modern object-oriented programming environ- ments provide garbage collection.\n\nglobal variable\n\nA variable that is global to a whole program. A global variable is accessible from anywhere within the program and never goes out of scope, although the memory to which it refers can be deallocated explicitly.\n\ngreen bar\n\nMany Graphical Test Runners portray the progress of the test run using a prog- ress bar. As long as all tests have passed, the bar stays green. When any tests fail, the indicator changes to a red bar.\n\nwww.it-ebooks.info\n\nGlossary\n\nGUI\n\nGraphical user interface.\n\nhappy path\n\nThe “normal” path of execution through a use case or through the software that implements it; also known as the “sunny day” scenario. Nothing goes wrong, nothing out of the ordinary happens, and we swiftly and directly achieve the user’s or caller’s goal.\n\nHollywood principle\n\nWhat directors in Hollywood tell aspiring actors at mass-casting calls: “Don’t call us; we’ll call you (if we want you).” In software, this concept is often called inversion of control (IOC).\n\nIDE\n\nIntegrated development environment. An environment that provides tools to edit, compile, execute, and (typically) test code within a single development tool.\n\nincremental delivery\n\nA method of building and deploying a software system in stages and releasing the software as each stage, called an “increment,” is completed. This approach results in earlier delivery to the user of a working system, where the capabilities of the system increase over time. In agile methods, the increment of functionality is the feature or user story. Incremental delivery goes beyond iterative develop- ment and incremental development, however, by actually putting the functional- ity into production on a regular basis. This idea is summarized by the following mantra: “Deliver early, deliver often.”\n\nincremental development\n\nA method of building a software system in stages such that the functionality built to date can be tested before the next stage is started. This approach allows for the earlier delivery to the user of a working system, where the capabilities of the system increase over time (see incremental delivery). In agile methods, the incre- ment of functionality is the feature or user story. Incremental development goes beyond iterative development, however, in that it promises to produce working,\n\nwww.it-ebooks.info\n\n799\n\n800\n\nAlso known as: outgoing interface\n\nAlso known as: member variable\n\nGlossary\n\ntestable, and potentially deployable software with every iteration. With incre- mental delivery, we also promise to “Deliver early, deliver often.”\n\nindirect input\n\nWhen the behavior of the system under test (SUT) is affected by the values returned by another component whose services it uses, we call those values the indirect in- puts of the SUT. Indirect inputs may consist of actual return values of functions, updated (out) parameters of procedures or subroutines, and any errors or excep- tions raised by the depended-on component (DOC). Testing of the SUT behavior with indirect inputs requires the appropriate control point on the “back side” of the SUT. We often use a Test Stub to inject the indirect inputs into the SUT.\n\nindirect output\n\nWhen the behavior of the system under test (SUT) includes actions that cannot be observed through the public application programming interface (API) of the SUT but that are seen or experienced by other systems or application compo- nents, we call those actions the indirect outputs of the SUT. Indirect outputs may consist of method or function calls to another component, messages sent on a message channel (e.g., MQ or JMS), and records inserted into a database or written to a ﬁ le. Veriﬁ cation of the indirect output behaviors of the SUT requires the use of appropriate observation points on the “back side” of the SUT. Mock Objects are often used to implement the observation point by intercepting the indirect outputs of the SUT and comparing them to the expected values.\n\nSee also: outgoing interface.\n\ninner class\n\nA class in Java that is deﬁ ned inside another class. Anonymous inner classes are deﬁ ned inside a method, whereas inner classes are deﬁ ned outside a method. In- ner classes are often used when deﬁ ning Hard-Coded Test Doubles.\n\ninstance method\n\nA method that is associated with an object rather than the class of the object. An instance method is accessible only from within or via an instance of the class. It is typically used to access information that is expected to differ from one instance to another.\n\nwww.it-ebooks.info\n\nGlossary\n\nThe exact syntax used to access an instance method varies from language to language. The most common syntax is objectReference.methodName(). When referenced from within other methods on the object, some languages require an explicit reference to the object (e.g., this.methodName() or self methodName); other languages simply assume that any unqualiﬁ ed references to methods are references to instance methods.\n\ninstance variable\n\nA variable that is associated with an object rather than the class of object. An instance variable is accessible only from within or via an instance of the class. It is typically used to access information that is expected to differ from one instance to another.\n\ninteraction point\n\nA point at which a test interacts with the system under test (SUT). An interac- tion point can be either a control point or an observation point.\n\ninterface\n\nIn general, a fully abstract class that deﬁ nes only the public methods that all im- plementers of the interface must provide. In Java, an interface is a type deﬁ nition that does not provide any implementation. In most single-inheritance languages, a class may implement any number of interfaces, even though it can extend (subclass) only one other class.\n\ninversion of control (IOC)\n\nA control paradigm that distinguishes software frameworks from “toolkits” or components. The framework calls the software plug-in (rather than the reverse). In the real world, inversion of control is often called the Hollywood principle. With the advent of automated unit testing, a class of framework known as an inversion of control framework has sprung up speciﬁ cally to simplify the re- placement of depended-on components (DOCs) with Test Doubles.\n\nIOC\n\nSee inversion of control.\n\nwww.it-ebooks.info\n\n801\n\nAlso known as: member function\n\n802\n\nAlso known as: wetware, mushware\n\nGlossary\n\niterative development\n\nA method of building a software system using time-boxed “iterations.” Each iteration is planned and then executed. At the end of the “time box,” the status of all the work is reviewed and the next iteration is planned. The strict time- boxing prevents “runaway development,” where the state of the system is never assessed because nothing is ever ﬁ nished. Unlike incremental development, itera- tive development does not require working software to be delivered at the end of each iteration.\n\nlayer-crossing test\n\nA test that either sets up the ﬁ xture or veriﬁ es the outcome using Back Door Manipulation, which involves using a “back door” of the system under test (SUT) such as a database. Contrast this with a round-trip test.\n\nlegacy software\n\nIn the test-driven development community, any software that does not have a Safety Net of Fully Automated Tests.\n\nliveware\n\nThe people who use our software. They are usually assumed to be much more intelligent than either the software or the hardware but they can also be rather unpredictable.\n\nlocal variable\n\nA variable that is associated with a block of code rather than an object or class. A local variable is accessible only from within the code block; it goes out of scope when the block of code returns to its caller.\n\nmanual test\n\nA test that is executed by a person interacting with the system under test (SUT). The user may be following some sort of “test script” (not to be confused with a Scripted Test) or doing ad hoc or exploratory testing.\n\nwww.it-ebooks.info\n\nGlossary\n\nmeta object\n\nAn object that holds data that controls the behavior of another object. A meta object protocol is the interface by which the meta object is constructed or con- ﬁ gured.\n\nmetatest\n\nA test that veriﬁ es the behavior of one or more tests. Such a test is mostly used during test-driven development, when we are writing tests as examples or course material and we want to ensure that tests are, indeed, failing to illustrate a par- ticular problem.\n\nmethod attribute\n\nAn attribute that is placed on a method in the source code to tell the compiler or runtime system that this method is “special.” In some xUnit family members, method attributes are used to indicate that a method is a Test Method.\n\nmixin\n\nFunctionality intended to be inherited by another class as part of that class’s implementation without implying specialization (“kind of” relationship) of the providing class.\n\n“The term mixin comes from an ice cream store in Somerville, Massachu- setts, where candies and cakes were mixed into the basic ice cream ﬂ avors. This seemed like a good metaphor to some of the object-oriented programmers who used to take a summer break there, especially while working with the object- oriented programming language SCOOPS” (SAMS Teach Yourself C++ in 21 Days, 4th ed., p. 458).\n\nmodule\n\nIn legacy programming environments (and probably a few current ones, too): An independently compilable unit of source code (e.g., the “ﬁ le I/O module”) that is later linked into the ﬁ nal executable. Unlike a component, this kind of module is typically not independently deployable. It may or may not have a cor- responding set of unit tests or component tests.\n\nWhen describing the functionality of a software system or application: A complete vertical chunk of the application that provides a particular piece of functionality (e.g., the “Customer Management Module”) that can be used\n\nwww.it-ebooks.info\n\n803\n\n804\n\nGlossary\n\nsomewhat independently of the other modules. It would have a corresponding set of acceptance tests and may be the unit of incremental delivery.\n\nneed-driven development\n\nA variation on the test-driven development process where code is written from the outside in and all depended-on code is replaced by Mock Objects that verify the expected indirect outputs of the code being written. This approach ensures that the responsibilities of each software unit are well understood before they are coded, by virtue of having unit tests inspired by examples of real usage. The outermost layer of software is written using storytest-driven development. It should have examples of usage by real clients (e.g., a user interface driving the Service Facade [CJ2EEP]) in addition to the customer tests.\n\nobject-relational mapping (ORM)\n\nA middleware component that translates between the object-oriented domain model of an application and the table-oriented view presented by a relational database management system.\n\nobservation point\n\nThe means by which the test observes the behavior of the system under test (SUT). This kind of interaction point can be used to inspect the post-exercise state of the SUT or to monitor interactions between the SUT and its depended- on components. Some observation points are provided strictly for the tests; they should not be used by the production code because they may expose private implementation details of the SUT that cannot be depended on not to change.\n\nORM\n\nSee object-relational mapping.\n\noutgoing interface\n\nA component (e.g., a class or a collection of classes) often depends on other components to implement its behavior. The interfaces it uses to access these components are known as outgoing interfaces, and the inputs and outputs trans- mitted via test interfaces are called indirect inputs and indirect outputs. Outgoing interfaces may consist of method or function calls to another component, mes- sages sent on a message channel (e.g., MQ or JMS), or records inserted into a\n\nwww.it-ebooks.info\n\nGlossary\n\ndatabase or written to a ﬁ le. Testing the behavior of the system under test (SUT) with outgoing interfaces requires special techniques such as Mock Objects to intercept and verify the usage of outgoing interfaces.\n\npattern\n\nA solution to a recurring problem. A pattern has a context in which it is typically applied and forces that help you choose one pattern over another based on that context. Design patterns are a particular kind of pattern. Organizational pat- terns are not discussed in this book.\n\npattern language\n\nA collection of patterns that work together to lead the reader from a very high- level problem to a very detailed solution customized for his or her particular context. When a pattern language achieves this goal, it is said to be “genera- tive”; this characteristic differentiates a pattern language from a simple collec- tion of patterns. Refer to “A Pattern Language for Pattern Writing” [APLfPW] to learn more about how to write a pattern language.\n\npolymorphism\n\nDynamic binding. The word is derived from the Latin, meaning “taking on many shapes.”\n\npresentation layer\n\nThe part of a Layered Architecture [DDD, PEAA, WWW] that contains the presentation logic.\n\npresentation logic\n\nThe logic embedded in the presentation layer of a business system. It decides which screen to show, which items to put on menus, which items or buttons to enable or disable, and so on.\n\nprocedure variable\n\nA variable that refers to a procedure or function rather than a piece of data. It allows the code to be called to be determined at runtime (dynamic binding) rather than at compile time. The actual procedure to be invoked is assigned to\n\nwww.it-ebooks.info\n\n805\n\nAlso known as: function pointer, delegate (in .NET languages)\n\n806\n\nAlso known as: pull system\n\nGlossary\n\nthe variable either during program initialization or during execution. Procedure variables were a precursor to true object-oriented programming languages (OOPLs). Early OOPLs such as C++ were built by using tables (arrays) of data structures containing procedure variables to implement the method (member function) dispatch tables for classes.\n\nproduction\n\nIn IT shops, the environment in which applications being used by real users run. This environment is distinguished from the various testing environments, such as “acceptance,” “integration,” “development,” and “qual” (short for “quality assessment or assurance”).\n\nproduction code\n\nIn IT shops, the environment in which applications run is often called produc- tion. Production code is the code that we are writing for eventual deployment to this environment, whether the code is to be shipped in a product or deployed into “production.” Compare to “test code.”\n\nprogrammer test\n\nA developer test.\n\nproject smell\n\nA symptom that something has gone wrong on the project. Its underlying root cause is likely to be one or more code smells or behavior smells. Because project managers rarely run or write tests, project smells are likely the ﬁ rst hint they have that something may be less than perfect in test automation land.\n\npull\n\nA concept from lean manufacturing that states that things should be produced only once a real demand for them exists. In a “pull system,” upstream (i.e., subcomponent) assembly lines produce only enough products to replace the items withdrawn from the pool that buffers them from the downstream assem- bly lines. In software development, this idea can be translated as follows: “We should only write methods that have already been called by other software and only handle those cases that the other software actually needs.” This approach avoids speculation and the writing of unnecessary software, which is one of\n\nwww.it-ebooks.info\n\nGlossary\n\nsoftware development’s key forms of inventory (which is considered waste in lean systems).\n\nred bar\n\nMany Graphical Test Runners portray the progress of the test run using a prog- ress bar that starts off green in color. When any tests fail, this indicator changes to a red bar.\n\nrefactoring\n\nChanging the structure of existing code without changing its behavior. Refactor- ing is used to improve the design of existing code, often as a ﬁ rst step before add- ing new functionality. The authoritative source for information on refactoring is Martin Fowler’s book [Ref].\n\nreﬂ ection\n\nThe ability of a software program to examine its own structure as it is executing. Reﬂ ection is often used in software development tools to facilitate adding new capabilities.\n\nregression test\n\nA test that veriﬁ es that the behavior of a system under test (SUT) has not changed. Most regression tests are originally written as either unit tests or ac- ceptance tests, but are subsequently included in the regression test suite to keep that functionality from being accidentally changed.\n\nresult veriﬁ cation\n\nAfter the exercise SUT phase of the Four-Phase Test, the test veriﬁ es that the expected (correct) outcome has actually occurred. This phase of the test is called result veriﬁ cation.\n\nretrospective\n\nA process whereby a team reviews its processes and performance for the pur- pose of identifying better ways of working. Retrospectives are often conducted at the end of a project (called a project retrospective) to collect data and make recommendations for future projects. They have more impact if they are done\n\nwww.it-ebooks.info\n\n807\n\nAlso known as: postmortem, postpartum\n\n808\n\nAlso known as: service component\n\nGlossary\n\nregularly during a project. Agile projects tend to do retrospectives after at least every release (called a release retrospective) and often after every iteration (called an iteration retrospective.)\n\nroot cause analysis\n\nA process wherein the cause of a failure or bug is traced back to all possible contributing factors. A root cause analysis helps us avoid treating symptoms by identifying the true sources of our problems. A number of techniques for doing root cause analysis exist, including Toyota’s “ﬁ ve why’s” [TPS].\n\nround-trip test\n\nA test that interacts only via the “front door” (public interface) of the system under test (SUT). Compare with layer-crossing test.\n\nservice object\n\nAn object that provides a service to other objects. Service objects typically do not have a life cycle of their own; any state they do contain tends to be an aggre- gate of the states of the entity objects that they vend. The interface of a service object is often deﬁ ned via a Service Facade [CJ2EEP] class. EJB Session Beans are one example of a service object.\n\nsetter\n\nA method provided by an object speciﬁ cally to set the value of one of its attri- butes. By convention, it either has the same name as the attribute or its name includes the preﬁ x “set” (e.g., setName).\n\nsmell\n\nA symptom of a problem. A smell doesn’t necessarily tell us what is wrong, be- cause it may have several possible causes. A smell must pass the “sniffability test”—that is, it must grab us by the nose and say, “Something is wrong here.” To ﬁ gure out exactly what the smell means, we must perform root cause analysis.\n\nWe classify smells based on where we ﬁ nd them. The most common kinds are (production) code smells, test smells, and project smells. Test smells may be either (test) code smells or behavior smells.\n\nwww.it-ebooks.info\n\nGlossary\n\nspike\n\nIn agile methods such as eXtreme Programming, a time-boxed experiment used to obtain enough information to estimate the effort required to implement a new kind of functionality.\n\nstateless\n\nAn object that does not maintain any state between invocations of its opera- tions. That is, each request is self-contained and does not require that the same server object be used for a series of requests.\n\nstatic binding\n\nResolving exactly which piece of software we will transfer control to at compile time. Static binding is the opposite of dynamic binding.\n\nstatic method\n\nIn Java, a method that the compiler resolves at compile time (rather than at run- time using dynamic binding). This behavior is the opposite of dynamic (or virtual in C++). A static method is also a class method because only class methods can be resolved at compile time in Java. A static method is not necessarily a class method in all languages, however. For example:\n\nAssert.assertEquals(message, expected, actual);\n\nstatic variable\n\nIn Java, a variable (ﬁ eld) that the compiler resolves at compile time rather than at runtime using dynamic binding. A static variable is also a class variable be- cause only class variables can be resolved at compile time in Java. Being static (i.e., not dynamic) does not necessarily imply that something is associated with a class (rather than an instance) in all languages.\n\nSTDD\n\nSee storytest-driven development.\n\nstory\n\nSee user story.\n\nwww.it-ebooks.info\n\n809\n\n810\n\nAlso known as: AUT, CUT, MUT, OUT\n\nGlossary\n\nstorytest\n\nA customer test that is the “conﬁ rmation” part of the user story “trilogy”: card, conversation, conﬁ rmation [XPC]. When the storytests are written before any software is developed, we call the process storytest-driven development.\n\nstorytest-driven development (STDD)\n\nA variation of the test-driven development process that entails writing (and usually automating) customer tests before the development of the correspond- ing functionality begins. This approach ensures that integration of the various software units veriﬁ ed by the unit tests results in a usable whole. The term “storytest-driven development” was ﬁ rst coined by Joshua Kerievsky as part of his methodology “Industrial XP” [IXP].\n\nSTTCPW\n\n“The simplest thing that could possibly work.” This approach is commonly used on XP projects when someone is over-engineering the software by trying to anticipate future requirements.\n\nsubstitutable dependency\n\nA software component may depend on any number of other components. If we are to test this component by itself, we must be able to replace the other components with Test Doubles—that is, each component must be a substitutable dependency. We can turn something into a substitutable dependency in several ways, including Dependency Injection, Dependency Lookup, and Test-Speciﬁ c Subclass.\n\nsynchronous test\n\nA test that interacts with the system under test (SUT) using normal (synchronous) method calls that return the results that the test will make assertions against. A synchronous test does not need to coordinate its steps with those of the SUT; this activity is managed automatically by the runtime system. Contrast this with an asynchronous test, which runs in a separate thread of control from the SUT.\n\nsystem under test (SUT)\n\nWhatever thing we are testing. The SUT is always deﬁ ned from the perspective of the test. When we are writing unit tests, the SUT is whatever class (also known\n\nwww.it-ebooks.info\n\nGlossary\n\nas CUT), object (also known as OUT), or method (also known as MUT) we are testing; when we are writing customer tests, the SUT is probably the entire appli- cation (also known as AUT) or at least a major subsystem of it. The parts of the application that we are not verifying in this particular test may still be involved as a depended-on component (DOC).\n\ntask\n\nThe unit of work assignment (or volunteering) in eXtreme Programming. One or more tasks may be involved in delivering a user story (a feature).\n\nTDD\n\nSee test-driven development.\n\ntest\n\nA procedure, whether manually executed or automated, that can be used to verify that the system under test (SUT) is behaving as expected. Often called a test case.\n\ntest automater\n\nThe person or project role that is responsible for building the tests. Sometimes a “subject matter expert” may be responsible for coming up with the tests to be automated by the test automater.\n\ntest case\n\nUsually a synonym for “test.” In xUnit, it may also refer to a Testcase Class, which is actually a Test Suite Factory as well as a place to put a set of related Test Methods.\n\ntest code\n\nCode written speciﬁ cally to test other code (either production or other test code).\n\ntest condition\n\nA particular behavior of the system under test (SUT) that we need to verify. It can be described as the following collection of points:\n\nwww.it-ebooks.info\n\n811\n\n812\n\nGlossary\n\nIf the SUT is in some state S1, and\n\nWe exercise the SUT in some way X, then\n\nThe SUT should respond with R and\n\nThe SUT should be in state S2.\n\ntest context\n\nEverything a system under test (SUT) needs to have in place so that we can ex- ercise the SUT for the purpose of verifying its behavior. For this reason, RSpec calls the test ﬁ xture (as used in xUnit) a “context.”\n\nContext: a set fruits with contents = {apple, orange, pear} Exercise: remove orange from the fruits set Verify: fruits set contents = {apple, pear}\n\nIn this example, the ﬁ xture consists of a single set and is created directly in the test. How we choose to construct the ﬁ xture has very far-reaching ramiﬁ cations for all aspects of test writing and maintenance.\n\ntest database\n\nA database instance that is used primarily for the execution of tests. It should not be the same database as is used in production!\n\ntest debt\n\nI ﬁ rst became aware of the concept of various kinds of debts via the Industrial XP mailing list on the Internet. The concept of “debt” is a metaphor for “not doing enough of” something. To get out of debt, we must put extra effort into the something we were not doing enough of. Test debt arises when we do not write all of the necessary tests. As a result, we have “unprotected code” in that the code could break without causing any tests to fail.\n\ntest-driven bug ﬁ xing\n\nA way of ﬁ xing bugs that entails writing and automating unit tests that reproduce each bug before we begin debugging the code and ﬁ xing the bug; the bug-ﬁ xing extension of test-driven development.\n\nwww.it-ebooks.info\n\nGlossary\n\ntest-driven development (TDD)\n\nA development process that entails writing and automating unit tests before the development of the corresponding units begins. TDD ensures that the responsi- bilities of each software unit are well understood before they are coded. Unlike test-ﬁ rst development, test-driven development is typically meant to imply that the production code is made to work one test at a time (a characteristic called emergent design).\n\nSee also: storytest-driven development.\n\ntest driver\n\nA person doing test-driven development.\n\ntest driving\n\nThe act of doing test-driven development.\n\ntest error\n\nWhen a test is run, an error that keeps the test from running to completion. The error may be explicitly raised or thrown by the system under test (SUT) or by the test itself, or it may be thrown by the runtime system (e.g., operating system, virtual machine). In general, it is much easier to debug a test error than a test failure because the cause of the problem tends to be much more local to where the test error occurs. Compare with test failure and test success.\n\ntest failure\n\nWhen a test is run and the actual outcome does not match the expected out- come. Compare with test error and test success.\n\ntest-ﬁ rst development\n\nA development process that entails writing and automating unit tests before the development of the corresponding units begins. Test-ﬁ rst development ensures that the responsibilities of each software unit are well understood before that unit is coded. Unlike test-driven development, test-ﬁ rst development merely says that the tests are written before the production code; it does not imply that the production code is made to work one test at a time (emergent design). Test-ﬁ rst\n\nwww.it-ebooks.info\n\n813\n\n814\n\nAlso known as: Testcase Class\n\nAlso known as: test context\n\nGlossary\n\ndevelopment may be applied at the unit test or customer test level, depending on which tests we have chosen to automate.\n\ntest ﬁ xture (disambiguation)\n\nIn generic xUnit: All the things we need to have in place to run a test and expect a particular outcome. The test ﬁ xture comprises the pre-conditions of the test; that is, it is the “before” picture of the SUT and its context. See also: test ﬁ xture (in xUnit) and test context.\n\nIn NUnit and VbUnit: The Testcase Class. See also: test ﬁ xture (in NUnit). In Fit: The adapter that interprets the Fit table and invokes methods on the\n\nsystem under test (SUT), thereby implementing a Data-Driven Test.\n\nSee also: ﬁ xture (Fit).\n\ntest ﬁ xture (in NUnit)\n\nIn NUnit (and in VbUnit and most .NET implementations of xUnit): The Test- case Class on which the Test Methods are implemented. We add the attribute [TestFixture] to the class that hosts the Test Methods.\n\nSome members of the xUnit family assume that an instance of the Testcase Class “is a” test context; NUnit is a good example. This interpretation assumes we are using the Testcase Class per Fixture approach to organizing the tests. When we choose to use a different way of organizing the tests, such as Testcase Class per Class or Testcase Class per Feature, this merging of the concepts of test context and Testcase Class can be confusing. This book uses “test ﬁ xture” to mean “the pre-conditions of the test” (also known as the test context) and Testcase Class to mean “the class that contains the Test Methods and any code needed to set up the test context.”\n\ntest ﬁ xture (in xUnit)\n\nIn xUnit: All the things we need to have in place to run a test and expect a par- ticular outcome (i.e., the test context). Some variants of xUnit keep the concept of the test context separate from the Testcase Class that creates it; JUnit and its direct ports fall into this camp. Setting up the test ﬁ xture is the ﬁ rst phase of the Four-Phase Test. For meanings of the term “test ﬁ xture” in other contexts, see test ﬁ xture (disambiguation).\n\nwww.it-ebooks.info\n\nGlossary\n\ntest-last development\n\nA development process that entails executing unit tests after the development of the corresponding units is ﬁ nished. Unlike test-ﬁ rst development, test-last devel- opment merely says that testing should be done before the code goes into pro- duction; it does not imply that the tests are automated. Traditional QA (quality assurance) testing is inherently test-last development unless the tests are pre- pared as part of the requirements phase of the project and are shared with the development team.\n\ntest maintainer\n\nThe person or project role responsible for maintaining the tests as the system or application evolves. Most commonly, this person is enhancing the system with new functionality or ﬁ xing bugs. The test maintainer could also be who- ever is called in when the automated tests fail for whatever reason. If the test maintainer is doing the enhancements by writing tests ﬁ rst, he or she is also a test driver.\n\ntest package\n\nIn languages that provide packages or namespaces, a package or name that exists for the purpose of hosting Testcase Classes.\n\ntest reader\n\nAnyone who has reason to read tests, including a test maintainer or test driver. This individual may be reading the tests primarily for the purpose of under- standing what the system under test (SUT) is supposed to do (Tests as Docu- mentation) or as part of a test maintenance or software development activity.\n\ntest result\n\nA test or test suite can be run many times, each time yielding a different test result.\n\ntest run\n\nA test or test suite can be run many times, each time yielding a different test result. Some commercial test automation tools record the results of each test run for prosperity.\n\nwww.it-ebooks.info\n\n815\n\n816\n\nGlossary\n\ntest smell\n\nA symptom of a problem in test code. A smell doesn’t necessarily tell us what is wrong because it may have several possible causes. Like all smells, a test smell must pass the “sniffability test”—that is, it must grab us by the nose and say, “Something is wrong here.”\n\ntest-speciﬁ c equality\n\nTests and the system under test (SUT) may have different ideas about what con- stitutes equality of two objects. In fact, this understanding may differ from one test to another. It is not advisable to modify the deﬁ nition of equality within the SUT to match the tests’ expectations, as this practice leads to Equality Pollution. Making individual Equality Assertions on many attributes of an object is not the answer either, as it can result in Obscure Tests and Test Code Duplication. In- stead, build one or more Custom Assertions that meets your tests’ needs.\n\ntest stripper\n\nA step or program in the build process that removes all the test code from the compiled and linked executable.\n\ntest success\n\nA situation in which a test is run and all actual outcomes match the expected outcomes. Compare with test failure and test error.\n\ntest suite\n\nA way to name a collection of tests that we want to run together.\n\nUniﬁ ed Modeling Language (UML)\n\nFrom Wikipedia [Wp]: “[A] nonproprietary speciﬁ cation language for object modeling. UML is a general-purpose modeling language that includes a stan- dardized graphical notation used to create an abstract model of a system, referred to as a UML model.”\n\nwww.it-ebooks.info\n\nGlossary\n\nunit test\n\nA test that veriﬁ es the behavior of some small part of the overall system. What turns a test into a unit test is that the system under test (SUT) is a very small subset of the overall system and may be unrecognizable to someone who is not involved in building the software. The actual SUT may be as small as a single object or method that is a consequence of one or more design decisions, although its behav- ior may also be traced back to some aspect of the functional requirements. Unit tests need not be readable, recognizable, or veriﬁ able by the customer or business domain expert. Contrast this with a customer test, which is derived almost entirely from the requirements and which should be veriﬁ able by the customer. In eXtreme Programming, unit tests are also called developer tests or programmer tests.\n\nuse case\n\nA way of describing the functionality of a system in terms of what its users are trying to achieve and what the system needs to do to achieve their goals. Unlike user stories, use cases may cover many different scenarios yet are often not test- able independently.\n\nuser acceptance test (UAT)\n\nSee acceptance test.\n\nuser story\n\nThe unit of incremental development in eXtreme Programming. We must INVEST in good user stories—that is, each user story must be Independent, Negotiable, Valuable, Estimatable, Small, and Testable [XP123]. A user story corresponds roughly to a “feature” in non-eXtreme Programming terminology and is typically decomposed into one or more tasks to be carried out by project team members.\n\nverify outcome\n\nAfter the exercise SUT phase of the test, the test compares the actual outcome— including returned values, indirect outputs, and the post-test state of the system under test (SUT)—with the expected outcome. This phase of the test is called the verify outcome phase.\n\nwww.it-ebooks.info\n\n817\n\nAlso known as: story, feature\n\nThis page intentionally left blank\n\nwww.it-ebooks.info\n\nReferences\n\n[AP] AntiPatterns: Refactoring Software, Architectures, and Projects in Crisis\n\nPublished by: John Wiley (1998) ISBN: 0-471-19713-0 By: William J. Brown et al.\n\nThis book describes common problems on software projects and suggests how to eliminate them by changing the architecture or project organization.\n\n[APLfPW] A Pattern Language for Pattern Writing\n\nIn: Pattern Languages of Program Design 3 [PLoPD3], pp. 529–574. Published by: Addison-Wesley (1998) By: Gerard Meszaros and James Doble\n\nAs the patterns community has accumulated experience in writing and reviewing patterns and pattern languages, we have begun to develop insight into pattern-writing techniques and approaches that have been observed to be particularly effective at addressing certain recurring problems. This pat- tern language attempts to capture some of these “best practices” of pattern writing, both by describing them in pattern form and by demonstrating them in action. As such, this pattern language is its own running example.\n\nFurther Reading\n\nFull text of this paper is available online in PDF form at http://Pattern- WritingPatterns.gerardmeszaros.com and in HTML form, complete with a hyperlinked table of contents, at http://hillside.net/patterns/writing/pattern- writingpaper.htm.\n\n819\n\nwww.it-ebooks.info\n\n820\n\nReferences\n\n[ARTRP] Agile Regression Testing Using Record and Playback\n\nhttp://AgileRegressionTestPaper.gerardmeszaros.com By: Gerard Meszaros and Ralph Bohnet\n\nThis paper was presented at XP/Agile Universe 2003. It describes how we built a “record and playback” test mechanism into a safety-critical appli- cation to make it easier to regression test it as it was ported from OS2 to Windows.\n\n[CJ2EEP] Core J2EE™ Patterns, Second Edition: Best Practices and Design Strategies\n\nPublished by: Prentice Hall (2003) ISBN: 0-131-42246-4 By: Deepak Alur, Dan Malks, and John Crupi\n\nThis book catalogs the core patterns of usage of Enterprise Java Beans (EJB), which are a key part of the Java 2 Enterprise Edition. Examples include Session Facade [CJ2EEP].\n\n[DDD] Domain-Driven Design: Tackling Complexity in the Heart of Software\n\nPublished by: Addison-Wesley (2004) ISBN: 0-321-12521-5 By: Eric Evans\n\nThis book is a good introduction to the process of using a domain model as the heart of a software system.\n\nReaders learn how to use a domain model to make complex development effort more focused and dynamic. A core of best practices and standard patterns provides a common language for the development team.\n\nwww.it-ebooks.info\n\nReferences\n\n[ET] Endo-Testing\n\nhttp://www.connextra.com/aboutUs/mockobjects.pdf By: Tim Mackinnon, Steve Freeman, and Philip Craig\n\nThis paper, which was presented at XP 2000 in Sardinia, describes the use of Mock Objects (page 544) to facilitate testing of the behavior of an object by monitoring its behavior while it is executing.\n\nUnit testing is a fundamental practice in eXtreme Programming, but most nontrivial code is difﬁ cult to test in isolation. It is hard to avoid writing test suites that are complex, incomplete, and difﬁ cult to maintain and interpret. Using Mock Objects for unit testing improves both domain code and test suites. These objects allow unit tests to be written for everything, simplify test structure, and avoid polluting domain code with testing infrastructure.\n\n[FaT] Frameworks and Testing\n\nIn: Proceedings of XP2002 http://www.agilealliance.org/articles/roockstefanframeworks/ﬁ le By: Stefan Roock\n\nThis paper is mandatory reading for framework builders. It describes four kinds of automated testing that should accompany a framework, including the ability to test a plug-in’s compliance with the framework’s protocol and a testing framework that makes it easier to test applications built on the framework.\n\nwww.it-ebooks.info\n\n821\n\n822\n\nReferences\n\n[FitB] Fit for Developing Software\n\nPublished by: Addison-Wesley (2005) ISBN: 0-321-26934-9 By: Rick Mugridge and Ward Cunningham\n\nThis book is a great introduction to the use of Data-Driven Tests (page 288) for preparing customer tests, whether as part of agile or traditional projects. This is what I wrote for inclusion as “advance praise”:\n\nWow! This is the book I wish I had on my desk when I did my ﬁ rst storytest-driven development project. It explains the philosophy behind the Fit framework and a process for using it to interact with the customers to help deﬁ ne the requirements of the project. It makes Fit so easy and approachable that I wrote my ﬁ rst FitNesse tests before I even I ﬁ nished the book.\n\nFurther Reading\n\nMore information on Fit can be found at Ward’s Web site, http://ﬁ t.c2.com.\n\n[GOF] Design Patterns: Elements of Reusable Object-Oriented Software\n\nPublished by: Addison-Wesley (1995) ISBN: 0-201-63361-2 By: Erich Gamma, Richard Helm, Ralph Johnson, and John M.Vlissides\n\nThis book started the patterns movement. In it, the “Gang of Four” describe 23 recurring patterns in object-oriented software systems. Examples include Composite [GOF], Factory Method [GOF], and Facade [GOF].\n\nwww.it-ebooks.info\n\nReferences\n\n[HoN] Hierarchy of Needs\n\nFrom Wikipedia [Wp]:\n\nMaslow’s hierarchy of needs is a theory in psychology that Abraham Maslow proposed in his 1943 paper “A Theory of Human Motivation,” which he subsequently extended. His theory contends that as humans meet “basic needs,” they seek to satisfy successively “higher needs” that occupy a set hierarchy. . . .\n\nMaslow’s hierarchy of needs is often depicted as a pyramid consisting of ﬁ ve levels: The four lower levels are grouped together as deﬁ ciency needs associated with physiological needs, while the top level is termed growth needs associated with psychological needs. While our deﬁ ciency needs must be met, our being needs are continually shaping our behavior. The basic concept is that the higher needs in this hierarchy only come into focus once all the needs that are lower down in the pyramid are mainly or entirely satisﬁ ed. Growth forces create upward movement in the hierarchy, whereas regressive forces push prepotent needs farther down the hierarchy.\n\n[IEAT] Improving the Effectiveness of Automated Tests\n\nhttp://FasterTestsPaper.gerardmeszaros.com. By: Gerard Meszaros, Shaun Smith, and Jennitta Andrea\n\nThis paper was presented at XP2001 in Sardinia, Italy. It describes a number of issues that reduce the speed and effectiveness of automated unit tests and suggests ways to address them.\n\n[IXP] Industrial XP\n\nhttp://ixp.industriallogic.com.\n\nIndustrial XP is a “branded” variant of eXtreme Programming created by Joshua Kerievsky of Industrial Logic. It includes a number of practices required to scale eXtreme Programming to work in larger enterprises, such as “Project Chartering.”\n\nwww.it-ebooks.info\n\n823\n\n824\n\nReferences\n\n[JBrains] JetBrains\n\nhttp://www.jetbrains.com.\n\nJetBrains builds software development tools that automate (among other things) refactoring. Its Web site contains a list of all refactorings that the company’s various tools support, including some that are not described in [Ref].\n\n[JNI] JUnit New Instance\n\nhttp://www.martinfowler.com/bliki/JunitNewInstance.html\n\nThis article by Martin Fowler provides the background for why it makes sense for JUnit and many of its ports to create a new instance of the Testcase Class (page 373) for each Test Method (page 348).\n\n[JuPG] JUnit Pocket Guide\n\nPublished by: O’Reilly ISBN: 0-596-00743-4 By: Kent Beck\n\nThis 80-page, small-format book is an excellent summary of key features of JUnit and best practices for writing tests. Being small enough to ﬁ t in a pocket, it doesn’t go into much detail, but it does give us an idea of what is possible and where to look for details.\n\n[LSD] Lean Software Development : An Agile Toolkit\n\nPublished by: Addison-Wesley (2003) ISBN: 0-321-15078-3 By: Mary Poppendieck and Tom Poppendieck\n\nThis excellent book describes 22 “thinking tools” that are used to work quickly and effectively in many domains. The authors describe how to apply these tools to software development. If you want to understand why agile development methods work, this book is a must read!\n\nwww.it-ebooks.info\n\nReferences\n\n[MAS] Mocks Aren’t Stubs\n\nhttp://www.martinfowler.com/articles/mocksArentStubs.html By: Martin Fowler\n\nThis article clariﬁ es the difference between Mock Objects (page 544) and Test Stubs (page 529). It goes on to describe the two fundamentally differ- ent approaches to test-driven development engendered by these differences: “classical TDD” versus “mockist TDD.”\n\n[MRNO] Mock Roles, Not Objects\n\nPaper presented at OOPSLA 2004 in Vancouver, British Columbia, Canada. By: Steve Freeman, Tim Mackinnon, Nat Pryce, and Joe Walnes\n\nThis paper describes the use of Mock Objects (page 544) to help the developer discover the signatures of the objects on which the class being designed and tested depends. This approach allows the design of the supporting classes to be deferred until after the client classes have been coded and tested. Members can obtain this paper at the ACM portal http://portal.acm.org/ft_gateway. cfm?id=1028765&type=pdf; nonmembers of the ACM can ﬁ nd it at http:// joe.truemesh.com/MockRoles.pdf.\n\n[PEAA] Patterns of Enterprise Application Architecture\n\nPublished by: Addison-Wesley (2003) ISBN: 0-321-12742-0 By: Martin Fowler\n\nThis book is an indispensable handbook of architectural patterns that are applicable to any enterprise application platform. It is a great way to understand how the various approaches to developing large business systems differ.\n\nwww.it-ebooks.info\n\n825\n\n826\n\nReferences\n\n[PiJV1] Patterns in Java, Volume 1: A Catalog of Reusable Design Patterns Illustrated with UML\n\nPublished by: Wiley Publishing (2002) ISBN: 0-471-22729-3 By: Mark Grand\n\nA catalog of design patterns commonly used in Java.\n\nFurther Reading http://www.markgrand.com/id1.html\n\n[PLoPD3] Pattern Languages of Program Design 3\n\nPublished by: Addison-Wesley (1998) ISBN: 0-201-31011-2 Edited by: Robert C. Martin, Dirk Riehle, and Frank Buschmann\n\nA collection of patterns originally workshopped at the Pattern Languages of Programs (PLoP) conferences.\n\n[POSA2] Pattern-Oriented Software Architecture, Volume 2: Patterns for Concurrent and Networked Objects\n\nPublished by: Wiley & Sons (2000) ISBN: 0-471-60695-2 By: Douglas Schmidt, Michael Stal, Hans Robert, and Frank Buschmann\n\nThis book is the second volume in the highly acclaimed Pattern-Oriented Software Architecture (POSA) series. POSA1 was published in 1996; hence this book is referred to as POSA2. It presents 17 interrelated pat- terns that cover core elements of building concurrent and networked sys- tems: service access and conﬁ guration, event handling, synchronization, and concurrency.\n\nwww.it-ebooks.info\n\nReferences\n\n[PUT] Pragmatic Unit Testing\n\nPublished by: Pragmatic Bookshelf ISBN: 0-9745140-2-0 (In C# with NUnit) ISBN: 0-9745140-1-2 (In Java with JUnit) By: Andy Hunt and Dave Thomas\n\nThis book by the “pragmatic programmers” introduces the concept of automated unit testing in a very approachable way. Both versions lower the entry barriers by focusing on the essentials without belaboring the ﬁ ner points. They also include a very good section on how to determine which tests you need to write for a particular class or method.\n\n[RDb] Refactoring Databases: Evolutionary Database Design\n\nPublished by: Addison-Wesley (2006) ISBN: 0-321-29353-3 By: Pramodkumar J. Sadalage and Scott W. Ambler\n\nThis book is a good introduction to techniques for applying agile principles to development of database-dependent software. It describes techniques for eliminating the need to do “big design up front” on the database. It deserves to be on the bookshelf of every agile developer who needs to work with a database. A summary of the contents can be found at http://www. ambysoft.com/books/refactoringDatabases.html.\n\n[Ref] Refactoring: Improving the Design of Existing Code\n\nPublished by: Addison-Wesley (1999) ISBN: 0-201-48567-2 By: Martin Fowler et al.\n\nThis book offers a good introduction to the process of refactoring software. It introduces a number of “code smells” and suggests ways to refactor the code to eliminate those smells.\n\nwww.it-ebooks.info\n\n827\n\n828\n\nReferences\n\n[RTC] Refactoring Test Code\n\nPaper presented at XP2001 in Sardinia, Italy By: Arie van Deursen, Leon Moonen, Alex van den Bergh, and Gerard Kok\n\nThis paper was the ﬁ rst to apply the concept of “code smells” to test code. It described a collection of 12 “test smells” and proposed a set of refac- torings that could be used to improve the code. The original paper can be found at http://homepages.cwi.nl/~leon/papers/xp2001/xp2001.pdf.\n\n[RtP] Refactoring to Patterns\n\nPublished by: Addison-Wesley (2005) ISBN: 0-321-21335-1 By: Joshua Kerievsky\n\nThis book deals with the marriage of refactoring (the process of improving the design of existing code) with patterns (the classic solutions to recurring design problems). Refactoring to Patterns suggests that using patterns to improve an existing design is a better approach than using patterns early in a new design, whether the code is years old or minutes old. We can improve designs with patterns by applying sequences of low-level design transfor- mations, known as refactorings.\n\n[SBPP] Smalltalk Best Practice Patterns\n\nPublished by: Prentice Hall (1997) ISBN: 0-13-476904-X By: Kent Beck\n\nThis book describes low-level programming patterns that are used in good object-oriented software. On the back cover, Martin Fowler wrote:\n\nKent’s Smalltalk style is the standard I aim to emulate in my work. This book does not just set that standard, but also explains why it is the standard. Every Smalltalk developer should have it close at hand.\n\nWhile Smalltalk is no longer the dominant object-oriented development language, many of the patterns established by Smalltalk programmers have been adopted as the standard way of doing things in the mainstream object- oriented development languages. The patterns in this book remain highly relevant even if the examples are in Smalltalk.\n\nwww.it-ebooks.info\n\nReferences\n\n[SCMP] Software Conﬁ guration Management Patterns: Effective Teamwork, Practical Integration\n\nPublished by: Addison-Wesley (2003) ISBN: 0-201-74117-1 By: Steve Berczuk (with Brad Appleton)\n\nThis book describes, in pattern form, the how’s and why’s of using a source code conﬁ guration management system to synchronize the activities of multiple developers on a project. The practices described here are equally applicable to agile and traditional projects.\n\nFurther Reading\n\nhttp://www.scmpatterns.com\n\nhttp://www.scmpatterns.com/book/pattern-summary.html\n\n[SoC] Secrets of Consulting: A Guide to Giving and Getting Advice Successfully\n\nPublished by: Dorset House (1985) ISBN: 0-932633-01-3 By: Gerald M. Weinberg\n\nFull of Gerry’s laws and rules, such as “The Law of Raspberry Jam: The farther you spread it, the thinner it gets.”\n\n[TAM] Test Automation Manifesto\n\nhttp://TestAutomationManifesto.gerardmeszaros.com By: Shaun Smith and Gerard Meszaros\n\nThis paper was presented at the August 2003 XP/Agile Universe meeting in New Orleans, Louisiana. It describes a number of principles that should be followed to make automated testing using xUnit cost-effective.\n\nwww.it-ebooks.info\n\n829\n\n830\n\nReferences\n\n[TDD-APG] Test-Driven Development: A Practical Guide\n\nPublished by: Prentice Hall (2004) ISBN: 0-13-101649-0 By: David Astels\n\nThis book provides a good introduction to the process of driving software development with unit tests. Part III of the book is an end-to-end example of using tests to drive a small Java project.\n\n[TDD-BE] Test-Driven Development: By Example\n\nPublished by: Addison-Wesley (2003) ISBN: 0-321-14653-0 By: Kent Beck\n\nThis book provides a good introduction to the process of driving software development with unit tests. In the second part of the book, Kent illustrates TDD by building a Test Automation Framework (page 298) in Python. In an approach he likens to “doing brain surgery on yourself,” he uses the emerg- ing framework to run the tests he writes for each new capability. It is a very good example of both TDD and bootstrapping.\n\n[TDD.Net] Test-Driven Development in Microsoft .NET\n\nPublished by: Microsoft Press (2004) ISBN: 0-735-61948-4 By: James W. Newkirk and Alexei A. Vorontsov\n\nThis book is a good introduction to the test-driven development process and the tools used to do it in Microsoft’s. Net development environment.\n\n[TI] Test Infected\n\nhttp://junit.sourceforge.net/doc/testinfected/testing.htm By: Eric Gamma and Kent Beck\n\nThis article was ﬁ rst published in the Java Report issue called “Test Infected— Programmers Love Writing Tests.” It has been credited by some as being what led to the meteoric rise in JUnit’s popularity. This article is an excellent intro- duction to the how’s and why’s of test automation using xUnit.\n\nwww.it-ebooks.info\n\nReferences\n\n[TPS] Toyota Production System: Beyond Large-Scale Production\n\nPublished by: Productivity Press (1995) ISBN: 0-915-2991-4-3 By: Taiichi Ohno\n\nThis book, which was written by the father of just-in-time manufacturing, describes how Toyota came up with the system driven by its need to pro- duce a small number of cars while realizing economies of scale. Among the techniques described here are “kanban” and the “ﬁ ve why’s.”\n\n[UTF] Unit Test Frameworks: Tools for High-Quality Software Development\n\nPublished by: O’Reilly (2004) ISBN: 0-596-00689-6 By: Paul Hamill\n\nThis book is a brief introduction to the most popular implementations of xUnit.\n\n[UTwHCM] Unit Testing with Hand-Crafted Mocks\n\nhttp://refactoring.be/articles/mocks/mocks.html By: Sven Gorts\n\nThis paper summarizes and names a number of idioms related to Hand-Built Test Doubles (see Conﬁ gurable Test Double on page 522)—speciﬁ cally, Test Stubs (page 529) and Mock Objects (page 544). Sven Gorts writes:\n\nMany of the unit tests I wrote over the last couple of years use mock objects in order to test the behavior of a component in isolation of the rest of the system. So far, despite the availability of various mocking frameworks, each of the mock classes I’ve used has been handwritten. In this article I do some retrospection and try to wrap up the mocking idioms I’ve found most useful.\n\nwww.it-ebooks.info\n\n831\n\n832\n\nReferences\n\n[UTwJ] Unit Testing in Java: How Tests Drive the Code\n\nPublished by: Morgan Kaufmann ISBN: 1-55860-868-0 By: Johannes Link, with contributions by Peter Fröhlich\n\nThis book does a very nice job of introducing many of the concepts and techniques of unit testing. It uses intertwined narratives and examples to introduce a wide range of techniques. Unfortunately, due to the format, it can be difﬁ cult to ﬁ nd something at a later time.\n\n[VCTP] The Virtual Clock Test Pattern\n\nhttp://www.nusco.org/docs/virtual_clock.pdf By: Paolo Perrotta\n\nThis paper describes a common example of a Responder called Virtual Clock [VCTP]. The author uses the Virtual Clock Test Pattern as a Decorator [GOF] for the real system clock, which allows the time to be “frozen” or resumed. One could use a Hard-Coded Test Stub or a Conﬁ gurable Test Stub just as easily for most tests. Paolo Perrotta summarizes the thrust of his article:\n\nWe can have a hard time unit-testing code that depends on the system clock. This paper describes both the problem and a common, reusable solution.\n\n[WEwLC] Working Effectively with Legacy Code\n\nPublished by: Prentice Hall (2005) ISBN: 0-13-117705-2 By: Michael Feathers\n\nThis book describes how to get your legacy software system back under control by retroﬁ tting automated unit tests. A key contribution is a set of “dependency-breaking techniques”—mostly refactorings—that can help you isolate the software for the purpose of automated testing.\n\nwww.it-ebooks.info\n\nDatabase Refactoring\n\n[Wp] Wikipedia\n\nFrom Wikipedia [Wp]: “Wikipedia is a multilingual, Web-based free con- tent encyclopedia project. The name Wikipedia is a blend of the words ‘wiki’ and ‘encyclopedia.’ Wikipedia is written collaboratively by volun- teers, allowing most articles to be changed by almost anyone with access to the Web site.”\n\n[WWW] World Wide Web\n\nA reference annotation of [WWW] indicates that the information was found on the World Wide Web. You can use your favorite search engine to ﬁ nd a copy by searching for it by the title.\n\n[XP123] XP123\n\nhttp://xp123.com Web site hosted by: William Wake\n\nA Web site hosting various resources for teams doing eXtreme Program- ming.\n\n[XPC] XProgramming.com\n\nhttp://xprogramming.com Web site hosted by: Ron Jeffries\n\nA Web site hosting various resources for teams doing eXtreme Program- ming. One of the better places to look for links to software downloads for unit test automation tools including members of the xUnit family.\n\nwww.it-ebooks.info\n\n833\n\n834\n\nReferences\n\n[XPE] eXtreme Programming Explained, Second Edition: Embrace Change\n\nPublished by: Addison-Wesley (2005) ISBN: 0-321-27865-8 By: Kent Beck\n\nThis book kick-started the eXtreme Programming movement. The ﬁ rst edi- tion (0-201-61641-6) described a recipe consisting of 12 practices backed by principles and values. The second edition focuses more on the values and principles. It breaks the practices into a primary set and a corollary set; the latter set should be attempted only after the primary practices are mas- tered. Among the practices both editions describe are pair programming and test-driven development.\n\nwww.it-ebooks.info\n\nIndex\n\nA\n\nABAP Object Unit, 747 ABAP Unit, 747 Abstract Setup Decorator\n\ndeﬁ ned, 449 example, 453\n\nacceptance tests. See also\n\ncustomer tests deﬁ ned, 785 why test?, 19 accessor methods, 785 ACID, 785 acknowledgements, xxvii–xxviii action components, 280 agile method\n\ndeﬁ ned, 785–786 property tests, 52\n\nAllTests Suite\n\nexample, 594–595 introduction, 13 when to use, 593\n\nannotation\n\ndeﬁ ned, 786 Test Methods, 351\n\nAnonymous Creation Method\n\ndeﬁ ned, 417 example, 420\n\nHard-Coded Test Data\n\nsolution, 196\n\npreface, xxi\n\nanonymous inner class deﬁ ned, 786 Test Stub examples, 535–536\n\nAnt, 753 AntHill, 753 anti-pattern (AP) deﬁ ned, 786 test smells, xxxv\n\nAOP (aspect-oriented programming)\n\ndeﬁ ned, 786 Dependency Injection, 681 retroﬁ tting testability, 148 API (application programming inter-\n\nface)\n\nCreation Methods, 416 database as SUT, 336 deﬁ ned, 786 Test Utility Method, 600 architecture, design for testability.\n\nSee design-for-testability\n\narguments\n\nmessages describing, 371–372 as parameters (Dummy\n\nArguments), 729 role-describing, 725\n\n835\n\nwww.it-ebooks.info\n\n836\n\nIndex\n\nArguments, Dummy, 729 Ariane 5 rocket, 218 aspect-oriented programming (AOP)\n\ndeﬁ ned, 786 Dependency Injection, 681 retroﬁ tting testability, 148\n\nimproperly coded in Neverfail\n\nTests, 274\n\nintroduction, 77 Missing Assertion Messages,\n\n226–227\n\nreducing Test Code Duplication,\n\nAssertion Message\n\n114–119\n\nof Assertion Method, 364 pattern description, 370–372\n\nAssertion Method\n\nAssertion Messages, 364 calling built-in, 363–364 choosing right, 364–365 Equality Assertions, 365 examples, 368–369 Expected Exception Assertions, 366\n\nFuzzy Equality Assertions,\n\n365–366\n\nimplementation, 363 as macros, 364 motivating example, 367–368 overview, 362–363 refactoring, 368 Single-Outcome Assertions,\n\n366–367\n\nStated Outcome Assertions, 366\n\nAssertion Roulette\n\nrefactoring, xlvi–xlix Self-Checking Tests, 107–108 unit testing, 6 Verify One Condition per Test,\n\n46–47\n\nassumptions, xxxix–xl Astels, Dave, 110 asynchronous tests deﬁ ned, 787 Hard-To-Test Code, 210–211 Humble Object, 696–697 Slow Tests, 255–256 testability, 70–71 Attachment Method deﬁ ned, 418 example, 421\n\nattributes\n\ndeﬁ ned, 787 dummy, 729 hiding unnecessary, 303–304 One Bad Attribute. See One\n\nEager Tests, 224–226 impact, 224 introduction, 14 Missing Assertion Message,\n\n226–227\n\nsymptoms, 224\n\nBad Attribute\n\nparameters as, 608 Suite Fixture Setup, 442–443 Test Discovery using, 397 Test Selection, 403–405 Automated Exercise Teardown\n\nassertions\n\nBuilt-in, 110–111 custom. See Custom Assertion deﬁ ned, 786 diagramming notation, xlii Domain Assertions, 476,\n\n481–482\n\ndeﬁ ned, 505 example, 508\n\nAutomated Fixture Teardown,\n\n504–505\n\nAutomated Teardown\n\nensuring Repeatable Tests, 27 examples, 507–508\n\nwww.it-ebooks.info\n\nIndex\n\nimplementation, 504–505 Interacting Test Suites, 232 Interacting Tests solution, 231 motivating example, 505–506 overview, 503–504 of persistent ﬁ xtures, 99–100 refactoring, 506–507 resource leakage solution, 233 when to use, 504 automated unit testing\n\nBeck, Kent, xxii\n\nsniff test, xxxviii Test Automation Frameworks,\n\n301\n\ntest smells, 9 Testcase Class per Class, 618 xUnit, 57 Behavior Sensitivity\n\ncause of Fragile Tests, 242–243 caused by Overspeciﬁ ed\n\nauthor’s motivation, xxiv–xxv fragile test problem, xxxi–xxxii introduction, xxx–xxxii\n\nSoftware, 246\n\ndeﬁ ned, xxxi smells, 14\n\nbehavior smells, 223–247\n\nB\n\nback door, deﬁ ned, 787 Back Door Manipulation\n\ncontrol/observation points, 66–67 database as SUT API, 336 Expected State Speciﬁ cation, 464 ﬁ xture setup, 333–335 implementation, 330–332 motivating example, 332 overview, 327–328 refactoring, 333 setup, 329 teardown, 330 veriﬁ cation, 329–330 veriﬁ cation using Test Spy, 333 when to use, 328\n\nBack Door Setup\n\ncontrolling indirect inputs, 128 ﬁ xture design, 59 Prebuilt Fixtures, 430–431 transient ﬁ xtures, 86\n\nAssertion Roulette. See Assertion Roulette deﬁ ned, 10–11, 788 Erratic Tests. See Erratic Test Fragile Tests. See Fragile Test Frequent Debugging. See Frequent Debugging Manual Intervention. See Manual Intervention\n\noverview, 13–15 Slow Tests. See Slow Tests\n\nBehavior Veriﬁ cation\n\napproach to Self-Checking\n\nTests, 108\n\nexamples, 472–473 implementation, 469–471 indirect outputs, 179–180 motivating example, 471–472 overview, 468–469 refactoring, 472 vs. state, 36 test results, 112–114 using Mock Objects. See\n\nBack Door Veriﬁ cation, 130–133 BDUF (big design upfront)\n\nMock Object\n\ndeﬁ ned, 787 design for testability, 65 test automation strategy, 49\n\nusing Test Spies. See Test Spy using Use the Front Door\n\nFirst, 40\n\nwww.it-ebooks.info\n\n837\n\n838\n\nIndex\n\nverifying indirect outputs,\n\nBPT (Business Process Testing)\n\n130–133\n\nwhen to use, 469\n\nbehavior-driven development\n\ndeﬁ ned, 753 Recorded Tests, 280 Test Automation\n\ndeﬁ ned, 787–788 Testcase Class per Fixture\n\nusage, 632\n\nFrameworks, 301\n\nBug Repellent, 22 Buggy Test\n\nBehavior-Exposing Subclass\n\nTest-Speciﬁ c Subclass\n\nexample, 587 when to use, 580\n\nintroduction, 12–13 reducing risk, 181 symptoms, 260–262\n\nBuilt-in Assertion\n\nBehavior-Modifying Subclass\n\nDeﬁ ning Test-Speciﬁ c Equality,\n\ncalling, 363–364 introduction, 110–111\n\n588–589\n\nSubstituted Singleton,\n\n586–587\n\nbuilt-in self-tests deﬁ ned, 788 test ﬁ le organization, 164\n\nTest Stub, 584–585 when to use, 580\n\nBespoke Assertion. See Custom\n\nbuilt-in test recording deﬁ ned, 281 example, 281–282\n\nAssertion\n\nbusiness logic\n\nbimodal tests, 687 binding, static\n\ndeﬁ ned, 809 Dependency Injection, 678–679\n\nblack box\n\ndeﬁ ned, 789 developer testing, xxx development process, 4–5 Layer Tests example, 344–345 testing without databases,\n\ndeﬁ ned, 788 Remoted Stored Procedure\n\n169–171\n\nBusiness Process Testing (BPT).\n\nTests, 656\n\nSee BPT (Business Process Testing)\n\nblock closures\n\ndeﬁ ned, 788 Expected Exception Tests,\n\nC\n\n354–355\n\nCalculated Value. See also Derived\n\nblocks\n\nValue\n\ncleaning up ﬁ xture teardown\n\nlogic, l–liv deﬁ ned, 788 try/ﬁ nally. See try/ﬁ nally block\n\nboundary values deﬁ ned, 788 erratic tests, 238 Minimal Fixtures, 303 result veriﬁ cation patterns, 478\n\nLoop-Driven Tests, 615 Production Logic in Test\n\nsolution, 205\n\nCanoo WebTest deﬁ ned, 753 Scripted Tests, 286 Test Automation\n\nFrameworks, 301\n\ntest automation tools, 53\n\nwww.it-ebooks.info\n\nIndex\n\ncapacity tests, 52 Capture/Playback Test.\n\nsamples, xli–xlii writing tests, 27–29\n\nSee Recorded Test\n\ncode smells\n\nChained Test\n\ncustomer testing, 6 examples, 459–460 implementation, 456–457 motivating example, 457–458 overview, 454–455 refactoring, 458 Shared Fixture strategies, 64–65 Shared Fixtures, 104–105, 322 when to use, 455–456 xUnit introduction, 57\n\nConditional Test Logic. See Conditional Test Logic\n\ndeﬁ ned, 10–11, 789 Hard-To-Test Code. See Hard-To-Test Code\n\nobscure tests. See Obscure Test Test Code Duplication. See Test\n\nCode Duplication\n\nTest Logic in Production. See Test Logic in Production\n\ntypes of, 16–17\n\nclass attributes\n\ncoding idioms\n\ndeﬁ ned, 789 Test Discovery using, 397 Testcase Class Selection using,\n\ndeﬁ ned, xxxv design patterns, 792\n\ncollisions\n\n404–405 class methods\n\nInteracting Tests, 229–231 Shared Fixtures, 318\n\ndeﬁ ned, 789 with Test Helper, 645, 646\n\nclass variables\n\ndeﬁ ned, 789 Suite Fixture Setup, 442\n\nclasses\n\ndiagramming notation, xlii as ﬁ xtures, 59 Test Double, 569–570, 572–573 Testcase. See Testcase Class\n\nCommand object\n\nintroduction, 82 Testcase Object as, 382 Command-Line Test Runner Assertion Message, 371 deﬁ ned, 379–380 introduction, 79 Missing Assertion Message,\n\n226–227\n\ncommercial recorded tests\n\nclass-instance duality, 374 Cleanup Method, 602 closure, block\n\nrefactored, 283–284 tools, 282–283\n\ncommon location, Test Discovery,\n\ndeﬁ ned, 788 Expected Exception Tests,\n\n354–355 Cockburn, Alistair\n\n397–398\n\nCommunicate Intent deﬁ ned, 41 refactoring Recorded Tests to,\n\npattern naming, 578 service layer tests, 339\n\n283–284\n\ncompiler macro, Test Method\n\ncode\n\nDiscovery, 395–396\n\ninside-out development, 34–36 organization. See test\n\nComplex Teardown, 206–207 Complex Test. See Dependency\n\norganization\n\nLookup\n\nwww.it-ebooks.info\n\n839\n\n840\n\nIndex\n\nComponent Broker. See Dependency\n\nLookup\n\nComponent Registry, 688 component tests deﬁ ned, 790 layer-crossing tests, 69 per-functionality, 52 test automation philosophies,\n\nConﬁ gurable Test Double examples, 564–567 implementation, 559–562 installing, 141–142 as kind of Test Double, 528 motivating example, 562–563 overview, 558 refactoring, 563 when to use, 559\n\n34–36\n\ntest strategy patterns, 340\n\nConﬁ gurable Test Stub. See also\n\ncomponents\n\ndeﬁ ned, 790 depended-on component. See\n\nConﬁ gurable Test Double implementation, 532 indirect input control, 179\n\nDOC (depended-on component)\n\nComposite object, deﬁ ned, 82 Concerns, Separation of, 28–29 concrete classes, 581 Condition Veriﬁ cation Logic, 203–204 Conditional Test Logic\n\nvs. Assertion Method, 363 avoidance, 119–121 avoiding via Custom\n\nAssertion, 475\n\nConﬁ guration Interface examples, 564–566 implementation, 560\n\nConﬁ guration Mode\n\nexample, 566–567 implementation, 560\n\nConstant Value. See Literal Value constants in Derived Value,\n\n718–722\n\nconstructing Mock Object, 546 Constructor Injection\n\navoiding via Guard Assertion,\n\n490–493\n\ncauses, 201–202 Complex Teardown, 206–207 Condition Veriﬁ cation Logic,\n\n203–204\n\nFlexible Tests, 202–203 impact, 201 introduction, 16 Multiple Test Conditions,\n\nexample, 683–684 implementation, 680–681 installing Test Doubles, 144\n\nConstructor Test deﬁ ned, 351 example, 355–357 introduction, 77\n\nconstructors\n\ndeﬁ ned, 790 problems with, 419\n\n207–208\n\ncontainers, Humble Container\n\nProduction Logic in Test,\n\nAdapter, 698\n\n204–205\n\nContext Sensitivity\n\nsymptoms, 200 Test Methods, 155\n\navoiding via Isolate the SUT,\n\n43–44\n\nConﬁ gurable Mock Object, 546–547. See also Conﬁ gurable Test Double\n\ndeﬁ ned, 245–246 introduction, xxxii, 14\n\nConﬁ gurable Registry, 691–692\n\ncontinuous design, xxxiii\n\nwww.it-ebooks.info\n\nIndex\n\ncontinuous integration\n\nCSV ﬁ les, xUnit Data-Driven\n\navoiding Lost Tests, 270 deﬁ ned, 791 impact of Data-Driven Tests, 290 steps, 14 control points\n\nTest, 296 CUnit, 748 Cunningham, Ward, xxv, 290 Custom Assertion\n\nas Conditional Veriﬁ cation\n\ndeﬁ ned, 791 testability, 66–67\n\nCoplien, Jim, 576 CORBA standards, 744 cost effectiveness, Self-Checking\n\nLogic solution, 204\n\nexamples, 480–484 implementation, 477–478 Indirect Testing solution,\n\n198–199\n\nTests, 107–108\n\nIrrelevant Information\n\ncosts, test automation, 20–21 Covey, Stephen, 121 CppUnit\n\ndeﬁ ned, 748 Test Automation Frameworks,\n\nsolution, 193\n\nmotivating example, 478–480 overview, 474–475 reducing Test Code Duplication,\n\n116–117\n\n300\n\nTest Method enumeration, 401\n\nCreation Method\n\nDelegated Setup, 89–91,\n\nrefactoring, 480 Test Utility Methods, 602 when to use, 475–477 writing simple tests, 28\n\n411–414\n\neliminating unnecessary\n\nobjects/attributes, 303–304\n\nCustom Assertion test example, 483–484 implementation, 477–478\n\nexamples, 420–423 as Hard-Coded Test Data\n\nCustom Equality Assertion, 476 customer tests\n\nsolution, 196 hybrid setup, 93 implementation, 418–419 motivating example, 419 overview, 415–416 persistent ﬁ xtures teardown, 100\n\ndeﬁ ned, 791 Eager Tests cause, 225 Missing Unit Test, 271 overview, 5–6 per-functionality, 51 as Scripted Test, 285–287\n\nCut and Paste code reuse,\n\npreface, xxiii refactoring, 420 as Test Utility Method, 600 when to use, 416–418 writing simple tests, 28 cross-functional tests, 52–53 cross-thread failure assertion, 274 Cruise Control, 754 CsUnit, 748\n\n214–215\n\nD\n\ndata access layer\n\ndatabase testing, 172–173 deﬁ ned, 791 Slow Tests with Shared\n\nFixtures, 319\n\nwww.it-ebooks.info\n\n841\n\n842\n\nIndex\n\ndata leaks\n\navoiding with Delta Assertions,\n\n486–487\n\nComplex Teardown, 206\n\ndatabase testing, 167–174 overview, 167–169 persistent ﬁ xtures, 313 testing without databases,\n\nData Loader, Back Door Manipulation, 330–331 data minimization, 738–739 data population script, 434 Data Retriever, 331 Data Sensitivity\n\ndeﬁ ned, 243–245 introduction, xxxii, 14\n\n169–171\n\ntypes of, 171–174\n\nDatabase Transaction Rollback Tear-\n\ndown, 674–675\n\ndatabases\n\nfake. See Fake Database as SUT API, 336 teardown, 100\n\nData Transfer Object (DTO)\n\nData-Driven Test\n\ndeﬁ ned, 793 result veriﬁ cation, 116 Database Extraction Script, 331 Database Partitioning Scheme Data Sensitivity solution,\n\n244–245\n\ncustomer testing, 5 Fit framework example,\n\n296–297\n\nframeworks, 300 implementation, 290 implemented as Recorded\n\ndeveloper independence, 173 example, 653 Global Fixtures, 430 implementation, 652 database patterns, 649–675 Database Sandbox,\n\nTest, 281\n\nintroduction, 83 motivating example, 293–294 overview, 288–289 principles, 48 reducing Test Code Duplication,\n\n650–653\n\n118–119\n\nStored Procedure Test,\n\n654–660\n\nTable Truncation Teardown,\n\nrefactoring notes, 294 Test Suite Object Simulator, 293 using Fit framework,\n\n661–667\n\n290–292\n\nTransaction Rollback Teardown, 668–675 Database Population Script, 330 Database Sandbox\n\nvia Naive xUnit Test Interpreter,\n\n292–293\n\nvia Test Suite Object\n\nGenerator, 293\n\ndatabase testing, 168 design for testability, 7 pattern description, 650–653 as Test Run Wars solution,\n\nwhen to use, 289–290 xUnit with CSV input ﬁ le, 296 xUnit with XML data ﬁ le,\n\n294–295\n\n236–237\n\nDB Schema per Test Runner\n\nUnrepeatable Tests cause, 235 when to use, 650\n\ndeveloper independence, 173 implementation, 651–652\n\nwww.it-ebooks.info\n\nIndex\n\nDbUnit\n\nDelta Assertion\n\nBack Door Manipulation, 335 deﬁ ned, 748 Expected State Speciﬁ cation, 464\n\nDDSteps, 754 Decorated Lazy Setup, 449–450 Decorator\n\nAbstract Setup Decorator,\n\n449, 453\n\navoiding ﬁ xture collisions, 101 as Data Sensitivity solution, 245 detecting data leakage with, 487 examples, 488–489 introduction, 111 pattern description, 485–486 depended-on component (DOC). See DOC (depended-on component)\n\nParameterized Setup Decorator,\n\ndependencies\n\n452–453\n\nPushdown Decorator, 450 Setup. See Setup Decorator Test Hook as, 710\n\nInteracting Tests, 230–231 replacement with Test\n\nDoubles, 739\n\nreplacing using Test Hooks,\n\nDedicated Database Sandbox, 651 Defect Localization\n\ncustomer testing, 5 deﬁ ned, 22–23 Frequent Debugging, 248 Keep Tests Independent Tests, 43 right-sizing Test Methods, 154 test automation philosophies, 34 unit testing, 6 Verify One Condition per Test, 45\n\ndeﬁ ning tests\n\nintroduction, 76–78 suites of, 78–79\n\n709–712\n\nretroﬁ tting testability, 148 test automation philosophies, 34 Test Dependency in Production,\n\n220–221\n\ntest ﬁ le organization, 165 Dependency Initialization Test, 352 Dependency Injection\n\ndesign for testability, 7 examples, 683–685 implementation, 679–681 installing Test Doubles via,\n\n143–144\n\ndelays. See Slow Tests Delegated Setup\n\nexample, 413–414 introduction, 77 matching with teardown code,\n\nIsolate the SUT, 44 motivating example, 682 overview, 678 Persistent Fresh Fixtures\n\navoidance, 62–63\n\n98–99\n\noverview, 411–414 of transient ﬁ xtures, 89–91 when to use, 412 Delegated Teardown\n\nrefactoring, 682 testability improvement, 70 when database testing, 171 when to use, 678–679\n\nDependency Lookup\n\nexample, 514–515 overview, 511 of persistent ﬁ xtures, 98–99 Table Truncation Teardown, 665\n\ndesign for testability, 7 examples, 691–693 implementation, 688–689 installing Test Doubles, 144–145\n\nwww.it-ebooks.info\n\n843\n\n844\n\nIndex\n\nIsolate the SUT, 44 motivating example, 690 names, 693–694 overview, 686 Persistent Fresh Fixtures,\n\ndeterministic values, 238 developer independence, 173 developer testing deﬁ ned, 792 introduction, xxx\n\n62–63\n\nrefactoring, 690–691 when database testing, 171 when to use, 687–688\n\nDerived Expectation example, 720 when to use, 719\n\nDevelopers Not Writing Tests, 13 development\n\nagile, 239 behavior driven, 632, 787–788 document-driven, 793 EDD. See EDD (example-driven\n\ndevelopment)\n\nDerived Input, 719 Derived Value\n\nexamples, 719–722 overview, 718 when to use, 718–719 design patterns, xxxv, 792 design-for-testability\n\ncontrol points and observation\n\nincremental, 33–34, 799–800 inside-out, 463 inside-out vs. outside in, 34–36 need-driven. See need-driven\n\ndevelopment outside-in, 469 process, 4–5 TDD. See TDD (test-driven\n\npoints, 66–67\n\ndevelopment)\n\ndeﬁ ned, 792 divide and test, 71–72 ensuring testability, 65 interaction styles and testability\n\npatterns, 67–71\n\noverview, 7 Separation of Concerns, 28–29 test automation philosophies.\n\nSee test automation philosophies\n\ntest automation principles, 40 test-driven testability, 66\n\ndesign-for-testability patterns,\n\n677–712\n\ntest-ﬁ rst. See test-ﬁ rst\n\ndevelopment\n\ntest-last. See test-last development\n\nDiagnostic Assertion, 476–477 diagramming notation, xlii Dialog, Humble. See Humble Dialog direct output\n\ndeﬁ ned, 792–793 veriﬁ cation, 178\n\nDirect Test Method Invocation, 401 disambiguation, test ﬁ xtures, 814 Discovery, Test. See Test Discovery Distinct Generated Values Anonymous Creation\n\nDependency Injection. See Dependency Injection Dependency Lookup. See Dependency Lookup\n\nMethods, 417\n\nDelegated Setup, 90 example, 725–726 Hard-Coded Test Data\n\nHumble Object. See Humble\n\nsolution, 196\n\nObject\n\nTest Hooks, 709–712\n\nimplementation, 724 Unrepeatable Tests solution, 235\n\nwww.it-ebooks.info\n\nIndex\n\nDistinct Values, 717 Do No Harm, 24–25 DOC (depended-on component)\n\ndynamic binding deﬁ ned, 793 use in Dependency Injection, 679\n\nBehavior Veriﬁ cation, 469 control points and observation\n\nDynamically Generated Mock\n\nObject, 550\n\npoints, 66–67 deﬁ ned, 791–792 outside-in development, 35 replacing with Test Double.\n\nDynamically Generated Test Double implementation, 561–562 providing, 140–141\n\nDynamically Generated Test Stub,\n\nSee Test Double\n\n534–535\n\nretrieving. See Dependency\n\nLookup\n\nterminology, xl–xli Test Hook in, 712 Documentation, Tests as.\n\nSee Tests as Documentation document-driven development,\n\n793\n\nE\n\nEager Test\n\nAssertion Roulette, 224–226 Fragile Tests, 240 Obscure Tests, 187–188 right-sizing Test Methods, 154\n\nDomain Assertion\n\nEasyMock\n\ndeﬁ ned, 476 example, 481–482\n\ndeﬁ ned, 754 Test Doubles, 140\n\ndomain layer\n\neCATT\n\ndeﬁ ned, 793 test strategy patterns, 337\n\ndeﬁ ned, 754 Test Automation Frameworks,\n\ndomain model, 793 Don’t Modify the SUT, 41–42 drivers, test\n\ndeﬁ ned, 813 lack of Assertion Messages,\n\n370\n\n301\n\nEclipse\n\nDebugger, 110 deﬁ ned, 754\n\neconomics of test automation, 20–21 EDD (example-driven development)\n\nDRY (don’t repeat yourself), 28 DTO (Data Transfer Object)\n\ndeﬁ ned, 794 tests as examples, 33\n\ndeﬁ ned, 793 result veriﬁ cation, 116\n\nefﬁ ciency, 11 emergent design\n\nDummy Argument, 729 Dummy Attribute, 729 Dummy Object\n\nvs. BDUF, 65 deﬁ ned, xxxiii, 794\n\nencapsulation\n\nconﬁ guring, 141–142 deﬁ ned, 133 as Test Double, 134–135, 526 as value pattern, 728–732 xUnit terminology, 741–744\n\nCreation Method. See Creation\n\nMethod\n\nDependency Lookup\n\nimplementation, 688–689\n\nindirect outputs and, 126\n\nwww.it-ebooks.info\n\n845\n\n846\n\nIndex\n\nIndirect Testing solution, 198 SUT API. See SUT API\n\nEncapsulation\n\nusing Test Utility Methods. See Test Utility Method\n\nequivalence class\n\nBehavior Smells, 238 deﬁ ned, 794 Untested Code, 272\n\nErratic Test\n\nendoscopic testing (ET)\n\ndeﬁ ned, 794 Mock Objects, 545 Test Doubles, 149\n\nEnsure Commensurate Effort and\n\nResponsibility, 47–48 Entity Chain Snipping example, 536–537 testing with doubles, 149 when to use, 531\n\nAutomated Teardown and, 27 customer testing, 5 database testing, 168–169 impact, 228 Interacting Test Suites, 231–232 Interacting Tests, 229–231 introduction, 14–16 Lonely Tests, 232 Nondeterministic Tests,\n\n237–238\n\nentity object, 794 enumeration\n\ncustomer testing, 5 Suite of Suites built using, 389–391 test conditions in Loop-Driven\n\nTests, 614–615\n\nResource Leakage, 233 Resource Optimism, 233–234 symptoms, 228 Test Run Wars, 235–237 troubleshooting, 228–229 Unrepeatable Tests, 234–235\n\nTest Enumeration, 399–402 Test Suite Object built using, 388 xUnit organization mechanisms, 153\n\nEquality, Sensitivity\n\nFragile Tests, 246 test-ﬁ rst development, 32\n\nEquality Assertion\n\nessential but irrelevant ﬁ xture\n\nsetup, 425\n\nET (endoscopic testing)\n\ndeﬁ ned, 794 Mock Object use for, 149, 545 example-driven development (EDD)\n\ndeﬁ ned, 794 tests as examples, 33\n\nAssertion Methods, 365 Custom, 476 example, 368 Guard Assertion as, 491 introduction, 110 reducing Test Code Duplication, 115\n\nexamples, tests as, 33 exclamation marks, xlii Executable, Humble. See Humble\n\nExecutable\n\nExecutable Speciﬁ cation, 51 execution optimization, 180–181 exercise SUT\n\nunit testing, 6\n\nEquality Pollution, 221–222 equals method\n\ndeﬁ ned, 794 test phases, 359\n\nexpectations\n\nEquality Pollution, 221–222 Expected State Speciﬁ cation, 464 reducing Test Code Duplication,\n\n115–116\n\ndeﬁ ned, 795 Derived Expectations, 719, 720 messages describing, 371–372 naming conventions, 159\n\nwww.it-ebooks.info\n\nIndex\n\nExpected Behavior Speciﬁ cation\n\ndeﬁ ned, 470–471 example, 473\n\nin persistent ﬁ xture teardown, 98 refactoring Recorded Tests, 283 Extract Testable Component, 197,\n\nExpected Behavior Veriﬁ cation\n\n735–736\n\ndeﬁ ned, 112 indirect outputs, 131–132 Expected Exception Assertion\n\neXtreme Programming deﬁ ned, 795 projects affected by Slow Tests,\n\ndeﬁ ned as Assertion Method,\n\n319–321\n\n365–366 example, 369\n\neXtreme Programming Explained\n\n(Beck), xxii\n\nExpected Exception Test\n\nConditional Veriﬁ cation Logic\n\nsolution, 204 introduction, 77 as Test Method, 350–351 using block closure, 354–355 using method attributes, 354 using try/catch, 353–354\n\nF\n\nfactories\n\ndeﬁ ned, 795 Factory Method, 592–593 Object Factories, 145, 688\n\nfailed tests\n\nExpected Object\n\nreducing Test Code Duplication,\n\ndue to Unﬁ nished Test Assertions, 494–497\n\n115–116\n\nrefactoring tests, xlv–xlviii State Veriﬁ cations, 109, 466–467 unit testing, 6 expected outcome, 795 Expected State Speciﬁ cation,\n\n464–465\n\nimplementation, 80 “Fail-Pass-Pass”, 234–235 failure messages\n\nAssertion Messages, 370–372 Built-in Assertions, 110–111 removing “if” statements, 120 Single-Outcome Assertions,\n\nexpected values, 546–547 exploratory testing\n\n366–367 Fake Database\n\ncross-functionality, 53 deﬁ ned, 795 Scripted Tests, 287\n\nExpression Builders, 564–566 expressiveness gaps, 27–28 external resource setup, 740 external result veriﬁ cation, 111–112 external test recording, 280 Extract Method\n\navoiding persistence, 101 database testing, 170 example, 556–557 Slow Component Usage\n\nsolution, 254\n\nSlow Tests with Shared\n\nFixtures, 319 when to use, 553\n\nFake Object\n\nCreation Methods, 418 Custom Assertions, 117 Delegated Setup, 89 as Eager Tests solution, 225 example, xlvii\n\nconﬁ guring, 141–142 customer testing, 6 deﬁ ned, 134 examples, 556–557 implementation, 553–554\n\nwww.it-ebooks.info\n\n847\n\n848\n\nIndex\n\nmotivating example, 554–555 optimizing test execution, 180 overview, 551–552 refactoring, 555–556 as Test Double, 139, 525 when to use, 552–553 xUnit terminology, 741–744\n\nFake Service Layer, 553 Fake Web Services, 553 false negative, 795 false positive, 795–796 fault insertion tests deﬁ ned, 796 per-functionality, 52\n\nFeathers, Michael, 40\n\nHighly Coupled Code\n\nFit\n\nData-Driven Test example,\n\n296–297\n\nData-Driven Test\n\nimplementation, 290–292\n\ndeﬁ ned, 754–755, 796 Expected State Speciﬁ cation, 464 ﬁ xture deﬁ nition, 59, 86 ﬁ xture vs. Testcase Class, 376 Scripted Tests\n\nimplementation, 286\n\nTest Automation Framework, 301\n\ntest automation tools, 54 tests as examples, 33 vs. xUnit, 57\n\nsolution, 210\n\nFitnesse\n\nHumble Object, 708 pattern naming, 576 retroﬁ tting testability, 148 Self Shunt, 578 test automation roadmap, 176 Unit Test Rulz, 307\n\nfeatures\n\nData-Driven Test\n\nimplementation, 290\n\ndeﬁ ned, 755 Scripted Test\n\nimplementation, 286\n\n“Five Whys”, 11 ﬁ xture design\n\ndeﬁ ned, 796 right-sizing Test Methods,\n\nupfront or test-by-test, 36 Verify One Condition per\n\n156–157\n\nTest, 46\n\nTestcase Class per. See Testcase\n\nxUnit sweet spot, 58\n\nClass per Feature\n\nvisibility/granularity in Test-Speciﬁ c Subclass, 581–582\n\nﬁ xture holding class variables, 797 ﬁ xture holding instance\n\nvariables, 797\n\nﬁ xture setup\n\nfeedback in test automation, xxix ﬁ le contention. See Test Run War File System Test Runner, 380 Finder Method\n\naccessing Shared Fixtures,\n\n103–104\n\nMystery Guests solution, 190 when to use, 600–601 ﬁ ne-grained testing, 33–34\n\nBack Door Manipulation, 329,\n\n333–335\n\ncleaning up, liv–lvii deﬁ ned, 797 Delegated Setup, 89–91 external resources, 740 Four-Phase Test, 358–361 Fresh Fixtures, 313–314 hybrid setup, 93\n\nwww.it-ebooks.info\n\nIndex\n\nImplicit Setup, 91–93 In-Line Setup, 88–89 introduction, 77 matching with teardown code,\n\n98–99\n\nShared Fixtures, 104–105 speeding up with doubles,\n\n149–150 strategies, 60\n\nintroduction, 77 Lazy Setup problems, 439 persistent ﬁ xtures, 97–100 Persistent Fresh Fixtures, 314 refactoring, l–liv Shared Fixtures, 105 transient ﬁ xtures, 93–94 Verify One Condition per\n\nTest, 46\n\nﬁ xture setup patterns, 407–459\n\nﬁ xture teardown patterns, 499–519\n\nChained Test. See Chained Test Creation Method. See Creation\n\nAutomated Teardown,\n\n503–508\n\nMethod\n\nGarbage-Collected Teardown,\n\nDelegated Setup, 411–414 Implicit Setup, 424–428. See also\n\nImplicit Setup\n\nIn-line Setup, 408–410. See also\n\nIn-line Setup\n\nLazy Setup. See Lazy Setup Prebuilt Fixture. See Prebuilt\n\n500–502\n\nImplicit Teardown, 516–519. See also Implicit Teardown In-line Teardown, 509–515. See also In-line Teardown Table Truncation Teardown,\n\n661–667\n\nFixture\n\nTransaction Rollback\n\nSetup Decorator. See Setup\n\nDecorator\n\nTeardown. See Transaction Rollback Teardown\n\nSuite Fixture Setup. See Suite\n\nﬁ xtures\n\nFixture Setup\n\nFixture Setup Testcase, 456 ﬁ xture strategies\n\noverview, 58–61 persistent fresh ﬁ xtures, 62–63 shared ﬁ xture strategies, 63–65\n\nﬁ xture teardown\n\navoiding in persistent ﬁ xtures,\n\ncollisions, 100–101 database testing, 168–169 deﬁ ned, 796, 814 Four-Phase Test, 358–361 fresh. See Fresh Fixture introduction, 78 Minimal. See Minimal Fixture right-sizing Test Methods,\n\n100–101\n\n156–157\n\nBack Door Manipulation, 330 cleaning up, l–liv Complex Teardown, 206–207 data access layer testing, 173 deﬁ ned, 797 ﬁ xture strategies, 60 Four-Phase Test, 358–361 Implicit Setup, 426\n\nShared. See Shared Fixture speeding up setup with doubles,\n\n149–150\n\nStandard. See Standard Fixture Testcase Class as, 376 Testcase Class per Fixture.\n\nSee Testcase Class per Fixture transient. See transient ﬁ xtures\n\nwww.it-ebooks.info\n\n849\n\n850\n\nIndex\n\nFlexible Test, 202–203 ﬂ uent interface, 797 For Tests Only, 219–220 foreign-key constraints, 663 forms, pattern, xxxiv–xxxv Four-Phase Test\n\nCustom Assertions, 478 ﬁ xture design, 59 introduction, 76–78 Mock Object patterns, 546 pattern description, 358–361 unit testing, 6 Verify One Condition per Test, 46\n\nData Sensitivity, 243–245 Fragile Fixture, 246–247 High Test Maintenance\n\nCost, 266 impact, 239 Interface Sensitivity, 241–242 introduction, xxiii, xxxi–xxxii,\n\n13–14\n\nOverspeciﬁ ed Software, 246 Sensitivity Equality, 246 symptoms, 239 troubleshooting, 239–240\n\nframeworks\n\nFowler, Martin, xxvi\n\ncode smells, 16 Creation Methods, 418 Custom Assertions, 117 Cut and Paste code reuse, 215 Delegated Setup, 89, 413 Eager Tests solution, 225 Multiple Test Conditions\n\nsolution, 208\n\npattern forms, xxxvi refactoring, xxxix refactoring Recorded Tests, 283 reusable test logic, 123 self-testing code, xxi Standard Fixtures, 306 state vs. behavior veriﬁ cation, 36\n\nFit. See Fit Test Automation Framework, 75,\n\n298–301 Frequent Debugging\n\navoidance with Custom\n\nAssertion, 475 causes, 248–249 impact, 249 introduction, 15 solution patterns, 249 symptoms, 248\n\nFresh Fixture\n\nCreation Method. See Creation\n\nMethod\n\nData Sensitivity solution,\n\n244–245\n\ntest smells, 9 Testcase Object exception, 385\n\nFragile Fixture\n\ndeﬁ ned, 246–247 introduction, 14, 16 setUp method misuse, 93\n\nFragile Test\n\nBehavior Sensitivity, 242–243 Buggy Tests, 260 causes, 240–241 Context Sensitivity, 245–246\n\nDelegated Setup, 411–414 example, 316 ﬁ xture strategies, 60–61 implementation, 312 Implicit Setup, 424–428 Interacting Tests solution, 231 motivating example, 315 Mystery Guests solution, 190 overview, 311 persistent, 62–63, 313–314. See also persistent ﬁ xtures\n\nrefactoring, 315\n\nwww.it-ebooks.info\n\nIndex\n\nsetup, 313–314 test automation philosophies, 36 Test Run Wars solution, 236–237 transient, 61–62. See also\n\nmisuse of setUp method,\n\n92–93\n\nObscure Tests, 190–192 Slow Tests, 255\n\ntransient ﬁ xtures\n\nTransient Fresh Fixture, 314 when to use, 312\n\nfront door, 797 Front Door First\n\ndeﬁ ned, 40–41 Overspeciﬁ ed Software\n\nGenerated Value, 723–727 Geras, Adam, 280 Global Fixture, 430 global variables deﬁ ned, 798 instance variables as, 92\n\ngoals, test automation.\n\navoidance, 246 Fully Automated Test\n\nbehavior smells and, 15 Communicate Intent and, 41 Manual Fixture Setup\n\nSee test automation goals\n\nGorts, Sven, 537 granularity\n\ntest automation tools and,\n\n53–54\n\nsolution, 251\n\nTest-Speciﬁ c Subclass,\n\nminimizing untested code, 44–45 running, 25–26 unit testing, 6\n\n581–582\n\nGraphical Test Runner\n\nclicking through to test code,\n\nfunctional tests\n\n226–227\n\ndeﬁ ned, 798 per-functionality, 50–52\n\nFuzzy Equality Assertion deﬁ ned, 365–366 example, 368–369 external result veriﬁ cation,\n\n111–112\n\ndeﬁ ned, 378–379 green bar, 26 introduction, 79, 300 graphical user interface (GUI).\n\nSee GUI (graphical user interface)\n\ngreen bar, deﬁ ned, 798 Guaranteed In-Line Teardown,\n\nintroduction, 110\n\n233\n\nGuard Assertion\n\nG\n\nConditional Veriﬁ cation Logic\n\nsolution, 203–204\n\nGamma, Erich, 57 garbage collection, 798 Garbage-Collected Teardown design-for-testability, 7 pattern description, 500–502 persistent ﬁ xtures, 97 transient ﬁ xtures, 87–88\n\nGeneral Fixture\n\ndatabase testing, 169 deﬁ ned, 187\n\nintroduction, 80 pattern description, 490–493 removing “if” statements in\n\nTest Method, 120 GUI (graphical user interface)\n\ndeﬁ ned, 799 design for testability, 7 Interface Sensitivity, xxxii testing with Humble\n\nDialogs, 696\n\nwww.it-ebooks.info\n\n851\n\n852\n\nIndex\n\nH\n\nHand-Built Test Double. See also\n\nHard-Coded Test Double\n\ntesting with, 140–142 when to use, 569 Hard-Coded Test Spy. See Hard-Coded Test Double\n\nConﬁ gurable Test Double,\n\n560–561\n\nproviding, 140–141\n\nHand-Coded Mock Object, 548–550 hand-coded teardown, 97–98 Hand-Coded Test Stub, 533–534 Hand-Scripted Test. See also\n\nScripted Test\n\nintroduction, 75 tools for automating, 53–54 Hand-Written Test. See Scripted Test happy path\n\ndeﬁ ned, 799 Responder use, 530 Simple Success Tests, 349–350 test automation roadmap,\n\n177–178\n\nHard-Coded Mock Object. See Hard-\n\nCoded Test Double\n\nHard-Coded Setup Decorator\n\nHard-Coded Test Stub. See also\n\nHard-Coded Test Double\n\nimplementation, 531–532 indirect input control, 179\n\nHard-Coded Value, 103 Hard-To-Test Code\n\nAsynchronous Code, 210–211 Buggy Tests, 261 code smells, 16 Developers Not Writing\n\nTests, 264\n\ndivide and test, 71–72 High Test Maintenance Cost,\n\n266–267\n\nHighly Coupled Code, 210 impact, 209 solution patterns, 209 symptoms, 209 Untestable Test Code, 211–212 hierarchy of test automation needs,\n\ndeﬁ ned, 449 example, 451–452 Hard-Coded Test Data\n\ncausing Obscure Tests, 194–196 deﬁ ned, 187 introduction, lv–lvii, 16\n\n176–177\n\nHigh Test Maintenance Cost\n\nConditional Test Logic, 200 In-Line Setup, 89 introduction, 12–13 smell description, 265–267\n\nHard-Coded Test Double conﬁ guring, 141–142 implementation, 527, 569–571 motivating example, 571 naming patterns, 576–578 overview, 568 refactoring, 572 Self Shunt/Loopback, 573 Subclassed Inner Test Double,\n\nHigher Level Language\n\nCustom Assertion, 117 Interface Sensitivity solution, 241 xUnit sweet spot, 58 Highly Coupled Code, 210 historical patterns and smells, xxxviii Hollywood principle deﬁ ned, 56, 799 test results, 79\n\n573–575, 578\n\nTest Double Class, 572–573\n\nHook, Test. See Test Hook HTML user interface sensitivity, xxxii\n\nwww.it-ebooks.info\n\nIndex\n\nHttpUnit, 755 Humble Container Adapter, 698 Humble Dialog\n\ndesign-for-testability, 7 example, 706–708 Hard-To-Test Code, 72 minimizing untested code, 45 when to use, 696–697\n\nHumble Executable\n\nasynchronous tests, 70–71 minimizing untested code, 44 motivating example, 700–702 Neverfail Test solution, 274 when to use, 697\n\nHumble Object\n\nAsynchronous Code solution, 211 Humble Dialog, 706–708 Humble Transaction\n\nIdea, 755 IeUnit\n\ndeﬁ ned, 748 Graphical Test Runner, 378\n\n“if” statements\n\nConditional Test Logic, 201 Guard Assertions, 490–491 removing, 120 IFixtureFrame, 442 ignoring tests, 270 Immutable Shared Fixture\n\ndeﬁ ned, 323 example, 326 Interacting Tests solution, 231 introduction, 61, 65 vs. Irrelevant Information, 192 Test Run Wars solution, 237\n\nimpact\n\nController, 708\n\nimplementation, 698–700 motivating example, 700–702 overview, 695–696 Poor Manís Humble Executable, 703\n\nAssertion Roulette, 224 Asynchronous Code, 211 Buggy Tests, 260 Conditional Test Logic, 201 Developers Not Writing\n\nTests, 263\n\nrefactoring, 702 True Humble Executable,\n\n703–706\n\nwhen to use, 696–698 Humble Transaction Controller\n\ndata access layer testing, 173 example, 708 when to use, 697–698\n\nEquality Pollution, 221 Erratic Tests, 228 Flexible Tests, 203 Fragile Tests, 239 Frequent Debugging, 249 General Fixtures, 191–192 Hard-Coded Test Data, 195 Hard-To-Test Code, 209 High Test Maintenance\n\nHurst, John, 670–671 hybrid setup, 93\n\nCost, 265\n\nI\n\nIDE (integrated development\n\nenvironment)\n\ndeﬁ ned, 799 introduction, 78 refactoring, xxxix\n\nHighly Coupled Code, 210 Indirect Testing, 197 Irrelevant Information, 193 Manual Intervention, 250 Mystery Guests, 189 Neverfail Tests, 274 Nondeterministic Tests, 237\n\nwww.it-ebooks.info\n\n853\n\n854\n\nIndex\n\nObscure Tests, 186 Production Bugs, 268 Slow Tests, 253 Test Code Duplication, 214 Test Dependency in Production, 221\n\nTest Hooks, 218–219 Test Logic in Production, 217 Test Run Wars, 236 For Tests Only, 220 Untestable Test Code, 211 Untested Requirements, 273\n\nImplicit Setup\n\nvs. Four-Phase Test, 360–361 introduction, 7, 77 matching with teardown code,\n\nincremental tests, 322 In-Database Stored Procedure Test\n\ndatabase testing, 172 example, 658–659 implementation, 655–656 Independent Tabular Test, 612–613 independent testing. See Keep Tests\n\nIndependent indirect input\n\nalternative path veriﬁ cation, 179 controlling, 128–129 controlling in Layer Tests, 341 deﬁ ned, 800 importance of, 126 Test Doubles, 125–126\n\nindirect output\n\n98–99\n\nBehavior Veriﬁ cation.\n\npattern description, 424–428 pattern naming, 577 reusing test code with, 162 transient ﬁ xtures, 91–93\n\nImplicit Teardown\n\nComplex Teardown solution,\n\n206–207 database, 100 vs. Four-Phase Test, 360–361 pattern description, 516–519 persistent ﬁ xtures, 98–99 Self-Checking Tests with, 108\n\nImposter. See Test Double incremental delivery\n\nSee Behavior Veriﬁ cation\n\ndeﬁ ned, 800 importance of, 126–127 registries, 541 Test Doubles, 125–126 veriﬁ cation, 130–133, 178–180 verifying in Layer Tests, 341\n\nIndirect Testing deﬁ ned, 187 Fragile Tests cause, 240 Obscure Tests cause, 196–199 testability, 70–71 Infrequently Run Test\n\nFrequent Debugging cause,\n\nagile development, 239 deﬁ ned, 799\n\n248–249\n\nProduction Bugs cause, 268–269\n\nincremental development deﬁ ned, 799–800 test automation philosophies,\n\n33–34\n\ninheritance\n\nreusing test code, 164 reusing test ﬁ xtures, 62 injected values, Test Stub.\n\nIncremental Tabular Test\n\nSee Test Stub\n\nimplementation, 609–610 Parameterized Test patterns,\n\nInjection, Parameter. See Parameter\n\nInjection\n\n613–614\n\nin-line Four Phase Test, 360\n\nwww.it-ebooks.info\n\nIndex\n\nin-line resources, 736–737 In-line Setup\n\nMock Object, 547 retroﬁ tting testability,\n\nintroduction, 77 matching with teardown code,\n\n146–148\n\ninstance methods\n\n98–99\n\nMystery Guest solution, 190 pattern description, 408–410 transient ﬁ xtures, 88–89\n\nIn-line Teardown\n\ndeﬁ ned, 800–801 with Test Helper, 645, 647\n\ninstance variables\n\nconverting for Implicit Setup, 427 Data-Driven Tests using Fit\n\nexamples, 512–515 implementation, 510–511 motivating example, 511 Naive In-Line Teardown, 512 overview, 509 of persistent ﬁ xtures, 98–99 refactoring, 512 when to use, 510 In-Memory Database, 553 inner class\n\nFramework, 297\n\ndeﬁ ned, 801 Fresh Fixtures, 313 as global variables, 92 Reuse Tests for Fixture Setup,\n\n418–419\n\nwith Test Speciﬁ c Subclass, 558 Testcase Class per Fixture, 632\n\ninstances\n\nanonymous, 535–536, 786 deﬁ ned, 800\n\nreusing, 63 Testcase Object exception,\n\n384–385\n\nInner Test Double\n\nintegrated development environment\n\nexample, 573–574 Hard-Coded Test Double\n\n(IDE). See IDE (integrated development environment)\n\nimplementation, 570–571\n\nSubclassed from Pseudo-Class,\n\nIntegration Build, 4 Intent-Revealing Name\n\n574–575, 578\n\nTest Spy implementation, 541\n\ninput\n\nderived, 719 indirect. See indirect input naming conventions, 158–159\n\nCustom Assertion, 474–475 Implicit Setup, 92 Parameterized Test, 608 Test Utility Method, 602–603\n\nInteracting Test Suites,\n\n231–232\n\ninside-out development\n\nInteracting Tests\n\nvs. outside-in development, 34–36 State Veriﬁ cation, 463\n\navoiding with Database\n\nSandbox, 650–653\n\ninstalling Test Doubles, 528\n\navoiding with Delta Assertion,\n\nDependency Injection, 143–144,\n\n111, 486\n\n679–680\n\nDependency Lookup, 144–145 Fake Object, 554 introduction, 143\n\ncaused by Shared Fixture, 63 Chained Tests, 455 customer testing, 5–6 database testing, 169\n\nwww.it-ebooks.info\n\n855\n\n856\n\nIndex\n\nErratic Test cause, 229–231 introduction, 15 Keep Tests Independent, 43\n\nJBehave\n\ndeﬁ ned, 748 tests as examples, 33\n\ninteraction point, 801 interaction styles, 67–71 Interaction Testing. See Behavior\n\nVeriﬁ cation\n\nInterface Sensitivity\n\nJFCUnit, 755 JMock\n\nConﬁ guration Interface, 560 deﬁ ned, 755 Test Double implementation,\n\ndeﬁ ned, 241–242 introduction, xxxii, 13\n\ninterfaces\n\n140\n\nJohnson, Rod, 670 JUnit\n\nConﬁ guration Interface, 560 deﬁ ned, 801 GUI. See GUI (graphical user\n\ninterface)\n\noutgoing interface, 804–805 standard test, 378 Test Runner. See Test Runner Use the Front Door First, 40–41\n\ninternal recording tools, 56 interpreters in Data-Driven Tests.\n\nSee Data-Driven Test\n\ndeﬁ ned, 748 Expected Exception Test\n\nexpression, 351 ﬁ xture design, 59 language-speciﬁ c terminology, xl Suite Fixture Setup support,\n\n442–443\n\nTest Automation Framework, 300\n\ntest automation tools, 55 Testcase Object exception,\n\nIntervention, Manual. See Manual\n\n384–385\n\nIntervention\n\ntesting stored procedures, 657\n\nIntroduce Explaining Variable\n\nrefactoring, lvii–lviii\n\nK\n\nIoC (inversion of control) framework\n\ndeﬁ ned, 801 for Dependency Injection, 680\n\nKeep Test Logic Out of Production\n\nCode\n\nirrelevant information deﬁ ned, 187 Obscure Test, 192–194\n\nminimizing risk, 24 principle, 45 test code organization, 164–165\n\nIsolate the SUT, 43–44 iterative development, 802\n\nKeep Tests Independent\n\nrunning, 26 test automation principles,\n\nJ\n\n42–43\n\nusing Fake Object. See Fake\n\nJava\n\nObject\n\nlanguage-speciﬁ c xUnit\n\nterminology, xl\n\ntest code packaging, 165\n\nKerievsky, Joshua, xxxix keys, Literal Values as, 714 King, Joseph, 319–321\n\nwww.it-ebooks.info\n\nIndex\n\nL\n\nleakage, resource\n\nlanguages\n\nErratic Tests, 233 persistent ﬁ xtures, 99\n\nterminology, xl–xli variations in Built-in Assertions,\n\nlearning styles, xxxix–xl legacy software\n\n110–111\n\nxUnit implementations, 76 language-speciﬁ c xUnit terminology,\n\nBuggy Tests, 261–262 deﬁ ned, 802 tests as safety net, 24\n\nxl–xli\n\n“Law of Raspberry Jam”, xxv Layer Test\n\nlenient Mock Object deﬁ ned, 138 when to use, 545\n\nBusiness Layer Tests, 344–345 database testing, 169–171 implementation, 340–341 motivating example, 341–342 overview, 337–338 Presentation Layer Tests, 343 refactoring, 342 Subcutaneous Tests, 343–344 when to use, 338–340\n\nlayer-crossing tests deﬁ ned, 802 testability, 67–69 Layered Architecture\n\ndesign-for-testability, 7 layer-crossing tests, 67–69\n\nlightweight implementation using Fake Object. See Fake Object\n\nLiteral Value\n\nHard-Coded Test Data, 195 pattern description, 714–717\n\nlocal variables\n\nconverting in Implicit\n\nSetup, 427 deﬁ ned, 802 Fresh Fixtures, 313\n\nLonely Test\n\ncaused by Chained Test. See\n\nChained Test Erratic Tests, 232 Interacting Tests.\n\nLazy Initialization, 435 Lazy Setup\n\nDecorated, 449–450 examples, 439–440 implementation, 436–437 Interacting Tests solution, 231 motivating example, 437–438 overview, 435 vs. Prebuilt Fixtures, 431–432 refactoring, 439 Shared Fixture, 64, 105 when to use, 436\n\nSee Interacting Tests\n\nLong Tests. See Obscure Test Loopback. See Self Shunt Loop-Driven Test\n\nimplementation, 610 Parameterized Test, 614–615\n\nloops\n\nas Conditional Test Logic, 201 eliminating, 121 Production Logic in Test cause,\n\n204–205\n\nLazy Teardown\n\nexample, 665–666 implementation, 663–664\n\nLost Tests\n\navoiding, 597 Production Bugs cause,\n\n269–271\n\nwww.it-ebooks.info\n\n857\n\n858\n\nIndex\n\nM\n\nMackinnon, Tim, 149 macros, Assertion Methods as, 364 maintenance\n\nHigh Test Maintenance Cost. See High Test Maintenance Cost\n\noptimizing, 180–181 test automation goals, 27–29 Manual Event Injection, 251–252 Manual Fixture Setup, 250–251 Manual Intervention impact, 250 introduction, 15 Manual Event Injection,\n\n251–252\n\nManual Fixture Setup, 250–251 Manual Result Veriﬁ cation, 251 symptoms, 250\n\nManual Result Veriﬁ cation, 251 manual testing\n\nmetatests, 803 method attributes deﬁ ned, 803 Expected Exception Tests, 354 Test Discovery using, 397 Test Method Selection\n\nusing, 405\n\nmethod names\n\nlanguage-speciﬁ c xUnit terminology, xl–xli\n\nTest Method Discovery, 395–396\n\nmethods\n\ndiagramming notation, xlii instance. See instance methods setUp. See setUp method static, 809 suite, 399 tearDown. See tearDown method Template Method, 164 test commands, 82 veriﬁ cation. See result\n\ndeﬁ ned, 802 right-sizing Test Methods, 154\n\nMarrick, Brian\n\nveriﬁ cation\n\nMiller, Jeremy, 687 Minimal Fixture\n\npurpose of tests, 51 right-sizing Test Methods, 155 tests as examples, 33\n\nMaslow, 176 MbUnit\n\ndeﬁ ned, 749 Parameterized Test\n\nexternal result veriﬁ cation, 112 General Fixtures solution, 192 minimizing data, 738–739 misuse of setUp method, 93 pattern description, 302–304 strategy, 62–63 test automation philosophies, 36\n\nimplementation, 608–609 Tabular Test with framework\n\nsupport, 614\n\nMessage, Assertion. See Assertion\n\nMinimize Test Overlap, 44 Minimize Untestable Code, 44–45 Missing Assertion Message, 226–227 Missing Unit Test\n\nMessage\n\nmessages, failure. See failure\n\nDefect Localization, 23 Production Bugs, 271\n\nmessages meta objects\n\nData-Driven Tests, 290 deﬁ ned, 803\n\nmixins\n\ndeﬁ ned, 803 Test Helper Mixins, 639,\n\n641–642\n\nwww.it-ebooks.info\n\nIndex\n\nMock Object\n\nN\n\nConﬁ gurable. See Conﬁ gurable\n\nTest Double\n\nconﬁ guring, 141–142 deﬁ ned, 133 examples, 548–550 Expected Behavior Speciﬁ cation,\n\nNaive In-line Teardown\n\ndeﬁ ned, 511 example, 512 of persistent ﬁ xtures, 97 Naive xUnit Test Interpreter,\n\n470–471\n\nimplementation, 546–548 motivating example, 548 Overspeciﬁ ed Software\n\n292–293\n\nNamed State Reaching Method,\n\n417–418\n\nNamed Test Suite\n\ncause, 246\n\noverview, 544–545 refactoring, 548 Test Double patterns, 525 Test Doubles, 137–139 unit testing, 6 vs. Use the Front Door First, 40 verifying indirect output,\n\nexamples, 594–598 implementation, 594 introduction, 160–161 overview, 592–593 refactoring, 594 Test Enumeration, 400 when to use, 593–594\n\nnames\n\n131–133\n\nwhen to use, 545 xUnit terminology, 741–744\n\nDependency Lookup, 693–694 intent-revealing. See\n\nIntent-Revealing Name\n\nMockMaker, 560 modules, 803–804 Move Method, 413 MSTest, 749 Mugridge, Rick, xxiv multimodal tests, 687 multiple-condition tests\n\nreferring to patterns and smells,\n\nxxxviii\n\nScripted Test, 287 Suite Fixture Setup, 446\n\nnaming conventions\n\nassertion-identifying\n\nmessages, 371\n\nConditional Test Logic,\n\n207–208\n\ndeﬁ ned, 45–47\n\nMultiresource In-line Teardown,\n\n513–514 MySql, 651 Mystery Guest\n\nmaking resources unique,\n\n737–738\n\npatterns, 576–578 vs. test code organization,\n\n158–159\n\nTest Method Discovery,\n\n395–396\n\ndeﬁ ned, 187 Obscure Test cause, 188–190\n\nTestcase Class per Class, 618 Testcase Class per Feature, 626 Testcase Class per Fixture, 632 For Tests Only solution, 220\n\nwww.it-ebooks.info\n\n859\n\n860\n\nIndex\n\nneed-driven development\n\nObject Mother\n\nBehavior Veriﬁ cation, 469 deﬁ ned, 804 testing with doubles, 149 using Mock Objects, 545\n\nin Delegated Setup, 90–91 when to use, 644–645\n\nobject technology, xxxix–xl Object Transaction Rollback\n\nNeverfail Test, 274 New River Gorge bridge, xxvi Newkirk, James, 384–385 NMock, 756 No Test Risk, 24–25 Nondeterministic Test dangers of, 26–27 Erratic Test, 237–238 Generated Values cause, 723–724\n\nTeardown, 673–674\n\nobject-oriented programming\n\nlanguage (OOPL), 76\n\nobject-relational mapping (ORM).\n\nSee ORM (object-relational mapping)\n\nobjects\n\nCreation Method. See Creation\n\nMethod\n\nnotation, diagramming, xlii Null Object vs. Dummy Object, 730 null values in Dummy Objects,\n\n729–732\n\nNUnit\n\ndetermining necessary,\n\n303–304\n\ndiagramming notation, xlii fake. See Fake Object Test Suite Objects. See Test Suite\n\ndeﬁ ned, 749 Expected Exception Test\n\nObject\n\nTestcase. See Testcase Object\n\nexpression, 351 ﬁ xture design, 59 Interacting Test Suites, 232 Suite Fixture Setup support,\n\nObscure Test\n\navoiding with Custom Assertion,\n\n475\n\navoiding with Separation of Con-\n\n442–443\n\nTest Automation Frameworks,\n\n300\n\ntest automation ways and\n\nmeans, 55\n\ntest ﬁ xtures, 814 Testcase Classes, 376 Testcase Object exception,\n\ncerns, 28–29 Buggy Test, 261 causes, 186–187 vs. Communicate Intent, 41 customer testing, 5 database testing, 169 Eager Test, 187–188 General Fixture, 190–192 Hard-Coded Test Data,\n\n384–385\n\n194–196\n\nO\n\nHigh Test Maintenance Cost,\n\n266\n\nObject Attribute Equality Assertion,\n\n476\n\nObject Factory\n\nDependency Lookup, 688 installing Test Double, 145\n\nimpact, 186 Indirect Testing, 196–199 introduction, xlvi, 12–13, 16 Irrelevant Information, 192–194 Mystery Guests, 188–190\n\nwww.it-ebooks.info\n\nIndex\n\noptimizing test execution/\n\noverlapping tests\n\nmaintenance, 180\n\nsmells, 10 solution patterns, 199 symptoms, 186 observation points\n\nminimizing, 44 Too Many Tests, 256–257\n\nOverspeciﬁ ed Software\n\navoiding with Fake Objects,\n\n552\n\ndeﬁ ned, 804 test automation strategy, 66–67\n\nO’Grady, Ted, 319–321 One Bad Attribute\n\nFragile Tests, 246 testing with doubles, 150 Use the Front Door First, 40\n\nexample, 721–722 introduction, xxiii, 90 Minimal Fixtures, 304 when to use, 719 OOPL (object-oriented\n\nprogramming language), 76\n\noptimism, resource, 189, 233–234 order of tests, 456 organization, test. See test\n\nP\n\nParameter Injection\n\nexample, 683 implementation, 680 installing Test Doubles, 144 Parameterized Anonymous Creation\n\nMethod, 417\n\nParameterized Creation Method\n\norganization; test organization patterns\n\nORM (object-relational mapping)\n\ndeﬁ ned, 804 Table Truncation Teardown, 663 Table Truncation Teardown\n\ndeﬁ ned, 417 Delegated Setup, 90 example, xxiii, 420–421 Irrelevant Information\n\nsolution, 193\n\nParameterized Setup Decorator\n\nusing, 667\n\nTransaction Rollback\n\ndeﬁ ned, 449 example, 452–453\n\nTeardown, 671\n\nParameterized Test\n\nOutcome Assertions, Stated. See\n\nStated Outcome Assertion\n\noutcome veriﬁ cation patterns. See\n\nresult veriﬁ cation patterns\n\noutcome-describing Veriﬁ cation\n\nMethod, 117\n\nexample, 611–612 extracting. See Data-Driven Test further reading, 615–616 implementation, 608–610 Incremental Tabular Test,\n\n613–614\n\noutgoing interface, 804–805 out-of-order calls, 138 output, indirect. See indirect output outside-in development\n\nBehavior Veriﬁ cation, 469 vs. inside-out development,\n\nIndependent Tabular Test,\n\n612–613\n\nLoop-Driven Tests, 614–615 motivating example, 610–611 overview, 607–608 reducing Test Code Duplication,\n\n34–36\n\nOvercoupled Software, 40\n\n118–119\n\nrefactoring, 611\n\nwww.it-ebooks.info\n\n861\n\n862\n\nIndex\n\nTabular Test with framework\n\nsupport, 614\n\nTest Utility Method, 602 when to use, 608\n\nparameters, arguments as, 729 “Pass-Fail-Fail”, 234–235 pattern language\n\ndeﬁ ned, xxxv–xxxvi, 805 pattern naming, 577\n\nmanaging, 103–105 overview, 95–96 Slow Tests cause, 102 Table Truncation Teardown. See Table Truncation Teardown teardown avoidance, 100–101 tearing down, 97–100 test strategy patterns, 313–314 what’s next, 106 Persistent Fresh Fixture\n\nPattern Languages of Programming\n\n(PLoP), 576\n\npatterns\n\naliases and variations, 767–784 database. See database patterns deﬁ ned, 805 design-for-testability. See\n\nbuilding, 88 deﬁ ned, 60–61 strategies, 62–63 Personal Oracle, 651 philosophy, test automation. See test\n\nautomation philosophies\n\ndesign-for-testability patterns ﬁ xture setup. See ﬁ xture setup\n\nPHPUnit, 749 PLoP (Pattern Languages of\n\npatterns\n\nProgramming), 576\n\nresult veriﬁ cation. See result\n\nPluggable Behavior\n\nveriﬁ cation patterns\n\ntest automation introduction,\n\nin Named Test Suites, 597 Testcase Object\n\nxxxiv–xxxviii\n\nimplementation, 383\n\nTest Double. See Test Double test organization. See test organization patterns\n\npollution\n\nEquality Pollution, 221–222 Shared Fixture, 326\n\ntest strategy. See test strategy\n\npatterns\n\ntestability, 67–71 value. See value patterns xUnit basics. See xUnit basics\n\npatterns peeling the onion, 11 per-functionality test, 50–52 Perrotta, Paolo, 537 Per-Run Fixtures, 323 persistence layer, 339–340 persistence resources, 504 persistent ﬁ xtures, 95–106\n\npolymorphism, 805 Poor Manís Humble Executable, 703\n\nPoor Man’s Humble Object implementation, 699 Transaction Rollback\n\nTeardown, 671\n\nPoppendieck, Mary, 51 Pragmatic Unit Testing, 743 Prebuilt Fixture\n\nexamples, 432–434 implementation, 430–431 motivating example,\n\ndatabase testing, 168–169 issues caused by, 96\n\n431–432\n\noverview, 429–430\n\nwww.it-ebooks.info\n\nIndex\n\nrefactoring, 432 Shared Fixture strategies, 64 Shared Fixtures, 104–105 Unrepeatable Tests cause, 235\n\npresentation layer\n\ndeﬁ ned, 805 Layer Tests example, 343 testing, 338–339\n\npresentation logic, 805 Preserve Whole Object refactoring,\n\nxlviii–xlix\n\nprinciples\n\nlist of, 757–759 patterns vs., xxxv–xxxvi test automation. See test automation principles Private Fixture. See Fresh Fixture private methods, 586 problem statements, xxxvi–xxxvii Procedural Behavior Veriﬁ cation\n\nMissing Unit Tests, 271 Neverfail Tests, 274 overview, 268 reducing risk, 181 Untested Code, 271–272 Untested Requirements, 272–274\n\nproduction code deﬁ ned, 806 keeping test logic out of, 45 Production Logic in Test, 204–205 proﬁ ling tools, 254 Programmatic Test. See Scripted Test programmer tests, 806 project smells, 259–274\n\nBuggy Tests, 260–262 deﬁ ned, 806 Developers Not Writing Tests,\n\n263–264\n\nHigh Test Maintenance Cost,\n\n265–267\n\ndeﬁ ned, 470 example, 472–473 indirect outputs, 131 introduction, 112–113 Test Spy usage, 137 Procedural State Veriﬁ cation\n\noverview, 12–13 Production Bugs. See Production\n\nBugs property tests, 52 Pseudo-Object\n\nHard-Coded Test Double\n\ndeﬁ ned, 463–464 example, 466 introduction, 109 Procedural Test Stub deﬁ ned, 526 introduction, 135–136 when to use, 531\n\nProcedure Test, Stored. See Stored\n\nProcedure Test\n\nprocedure variables, 805–806 production, 806 Production Bugs\n\nimplementation, 570–571 Inner Test Double Subclassed\n\nfrom Pseudo-Class, 574–575, 578\n\ntesting with doubles, 140–141\n\npull system, 806–807 Pull-Up Method refactoring Delegated Setup, 413 moving reusable test logic, 123 Testcase Superclass, 640\n\nPushdown Decorator, 450 PyUnit\n\nInfrequently Run Tests, 268–269 introduction, 12–13 Lost Tests, 269–271\n\ndeﬁ ned, 749 Test Automation Framework,\n\n300\n\nwww.it-ebooks.info\n\n863\n\n864\n\nIndex\n\nQ\n\nQA (quality assurance), 22–23 QaRun, 244 QTP (QuickTest Professional) Data-Driven Tests, 290 deﬁ ned, 756 record and playback tools, 282 Test Automation Framework, 301\n\nred bar, 807 Refactored Recorded Tests commercial, 283–284 overview, 280\n\nrefactoring. See also test refactorings\n\nAssertion Message, 372 Assertion Method, 368 Automated Teardown,\n\n506–507\n\nquality assurance (QA), 22–23 QuickTest Professional (QTP).\n\nSee QTP (QuickTest Professional)\n\nR\n\nrandom values\n\nNondeterministic Tests, 238 Random Generated Values, 724\n\nRecord and Playback Test, 13 record and playback tools introduction, xxxi Recorded Tests, 282–283 xUnit sweet spot, 58\n\nRecorded Test\n\nbuilt-in test recording,\n\nBack Door Manipulation, 333 Chained Test, 458 Conﬁ gurable Test Double, 463 Creation Method, 420 Custom Assertion, 480 Database Sandbox, 653 Data-Driven Test, 294 deﬁ ned, 807 Delegated Setup, 413 Delta Assertion, 488 Dependency Injection, 682 Dependency Lookup, 690–691 Derived Value, 720 Dummy Object, 731 Fake Object, 555–556 Fresh Fixture, 315–316 Garbage-Collected Teardown, 502\n\n281–282\n\ncommercial record and\n\nplayback tool, 282–283\n\ncustomer testing, 5 Data-Driven Tests and, 289 implementation, 280–281 Interface Sensitivity, 241 overview, 278–279 refactored commercial recorded\n\ntests, 283–284\n\nvs. Scripted Tests, 286 smells, 10 tools, 56 tools for automating, 53–54 when to use, 279–280\n\nGenerated Value, 725 Guard Assertion, 492 Hard-Coded Test Double, 572 Humble Object, 702 Implicit Setup, 427 Implicit Teardown, 518–519 In-line Setup, 410 In-line Teardown, 512 Layer Test, 342 Lazy Setup, 439 Literal Value, 716 Mock Object, 548 Named Test Suite, 594 Parameterized Test, 611\n\nRecording Test Stub. See Test Spy\n\nwww.it-ebooks.info\n\nIndex\n\nPrebuilt Fixture, 432 Setup Decorator, 451 Shared Fixture, 324 Standard Fixture, 309–310 State Veriﬁ cation, 465–466 Stored Procedure Test, 658 Suite Fixture Setup, 444 Table Truncation Teardown,\n\nRelated Generated Values example, 726–727 implementation, 725 Remoted Stored Procedure Test\n\nexample, 659–660 implementation, 656–658 introduction, 172\n\nRepeatable Test\n\n664–665\n\nTest Discovery, 395 Test Helper, 646 Test Spy, 541–542 Test Stub, 533 Test Utility Method, 605 Testcase Class per Feature,\n\ndeﬁ ned, 26–27 indirect inputs control, 179 Replace Dependency with Test\n\nDouble refactoring\n\nBehavior Veriﬁ cation, 472 deﬁ ned, 739\n\nRepository\n\n627–628\n\nTestcase Class per Fixture,\n\n634–635\n\nData-Driven Test ﬁ les, 290 persistent objects, 90 source code, 24, 79, 234,\n\nTestcase Superclass, 640 Test-Speciﬁ c Subclass, 584 Transaction Rollback\n\n561, 656\n\ntest code, 164, 561 Requirement, Untested.\n\nTeardown, 672\n\nSee Untested Requirement\n\nUnﬁ nished Test Assertion, 496\n\nRefactoring: Improving the\n\nReSharper, 756 Resource Leakage\n\nDesign of Existing Code (Fowler), 9, 16\n\nErratic Tests, 233 persistent ﬁ xtures, 99\n\nreferences, 819–832 reﬂ ection\n\nResource Optimism, 189, 233–234 resources\n\ndeﬁ ned, 807 Test Discovery, 393 Testcase Object\n\nexternal, 740 in-line, 736–737 unique, 737–738\n\nimplementation, 383\n\nResponder\n\nRegistry\n\nconﬁ gurable, 691–692 in Dependency Lookup, 688–689 Interacting Tests, 230 Test Fixture, 644\n\nregression tests\n\ndeﬁ ned, 807 Recorded Tests. See Recorded\n\nTest\n\nScripted Tests, 285–287\n\ndeﬁ ned, 524 examples, 533–535 indirect input control, 179 introduction, 135 when to use, 530 response time tests, 52 result veriﬁ cation, 107–123\n\nBehavior Veriﬁ cation, 112–114 Conditional Test Logic avoidance, 119–121\n\nwww.it-ebooks.info\n\n865\n\n866\n\nIndex\n\nData Sensitivity, 243–245 deﬁ ned, 807 Four-Phase Test, 358–361 Mock Object, 547–548 other techniques, 121–122 reducing Test Code Duplication,\n\n114–119\n\nRobust Tests\n\ndeﬁ ned, 29 indirect inputs control, 179 role-describing arguments, 725 root cause analysis deﬁ ned, 808 smells, 11\n\nreusable test logic, 123 Self-Checking Tests, 107–108 State Veriﬁ cation, 109–112 result veriﬁ cation patterns, 461–497 Behavior Veriﬁ cation. See Behavior Veriﬁ cation\n\nround-trip tests\n\ndeﬁ ned, 808 introduction, 67–69 Layer Tests, 340–341\n\nrow tests. See Tabular Test RSpec\n\nCustom Assertion. See Custom\n\nAssertion\n\nDelta Assertion, 485–489 Guard Assertion, 490–493 State Veriﬁ cation. See State Veri-\n\nﬁ cation\n\ndeﬁ ned, 750 ﬁ xture design, 59 tests as examples, 33\n\nrunit\n\ndeﬁ ned, 750 Test Automation\n\nUnﬁ nished Test Assertion,\n\nFrameworks, 300\n\n494–497\n\nrunning tests\n\nresults, test\n\ndeﬁ ned, 815 introduction, 79–80 Retrieval Interface, 137, 540 retrospective, 807–808 reusable test logic\n\nintroduction, 79 structure, 81 test automation goals, 25–27\n\nruntime reﬂ ection, 393\n\nS\n\nCreation Method, 418–419 ﬁ xture setup patterns, 422–423 organization, 162–164 result veriﬁ cation, 123 Test Code Duplication, 214–215 Test Utility Method. See Test\n\nSaboteur\n\ndeﬁ ned, 135 example, 535–536 inside-out development, 35 Test Double patterns, 524 when to use, 530\n\nUtility Method\n\nReuse Tests for Fixture Setup, 90 Robot User Test. See Recorded Test robot user tools\n\ndeﬁ ned, 55–56 introduction, xxxi Test Automation Framework,\n\n299\n\nSafety Net\n\nBuggy Tests, 260 tests as, 24 sample code, xli–xlii screen scraping, 241 Scripted Test\n\nCommunicate Intent, 41 customer testing, 5\n\nwww.it-ebooks.info\n\nIndex\n\nData-Driven Tests and, 289 introduction, 75 pattern description, 285–287 vs. Recorded Tests, 279 smells, 10 UI, 55 Verify One Condition per\n\nTest, 46\n\nSelf Shunt\n\ndata. See Data Sensitivity interface. See Interface\n\nSensitivity\n\nSeparation of Concerns, 28–29 Service Facade, 71–72 service layers fake, 553 tests, 7, 339 Service Locator\n\nBehavior Veriﬁ cations, 113 example, 573 Hard-Coded Test Double implementation, 570\n\npattern naming, 576 Test Spy implementation,\n\nin Dependency Lookup.\n\nSee Dependency Lookup installing Test Doubles, 145\n\nservice objects, 808 Setter Injection\n\nConﬁ guration Interface\n\n540–541 Self-Call, 582 Self-Checking Test\n\nAssertion Method usage, 362 Conditional Test Logic\n\nsolution, 201\n\ndeﬁ ned, 80 happy path code, 178 introduction, 107–108 running, 26 Self-Describing Value example, 717 Literal Value patterns, 715\n\nself-testing code, xxi self-tests, built-in deﬁ ned, 788 test ﬁ le organization, 164\n\nusing, 564\n\nexample, 684–685 implementation, 681 installing Test Doubles, 143\n\nsetters, 808 setup, ﬁ xtures. See ﬁ xture setup Setup Decorator\n\nexamples, 451–453 implementation, 448–450 Implicit Setup, 426 motivating example, 450–451 overview, 447–448 refactoring, 451 Shared Fixture strategies, 64,\n\n104–105\n\nwhen to use, 448\n\nsetUp method\n\nSensitive Equality\n\nFragile Tests, 246 test-ﬁ rst development, 32\n\nsensitivities\n\nImplicit Setup, 91–92, 424–428 misuse of, 92–93 pattern naming, 577 Setup Decorator. See Setup\n\nautomated unit testing,\n\nDecorator\n\nxxxi–xxxii\n\nSuite Fixture Setup. See Suite\n\nbehavior. See Behavior Sensitivity Buggy Tests cause, 260 context. See Context Sensitivity\n\nFixture Setup\n\nshadows, diagramming notation, xlii Shank, Clint, 457–458, 613, 616\n\nwww.it-ebooks.info\n\n867\n\n868\n\nIndex\n\nShared Fixture. See also Standard\n\nFixture\n\nintroduction, 77 pattern description, 349–350\n\nBehavior Veriﬁ cation, 108 Chained Test. See Chained Test customer testing, 5 Data Sensitivity cause, 243 database testing, 169 deﬁ ned, 60–61 Delta Assertions, 111 example, 324–325 Immutable. See Immutable\n\nThe simplest thing that could\n\npossibly work (STTCPW), 810\n\nSingle Glance Readable.\n\nSee Communicate Intent\n\nSingle Layer Test. See Layer Test Single Test Suite\n\nexample, 596–597 Lost Tests solution, 270 when to use, 593–594\n\nShared Fixture\n\nImmutable Shared Fixtures, 326 implementation, 322–323 incremental tests, 322 Interacting Tests cause, 229–231 introduction, 15, 63–65 Lazy Setup. See Lazy Setup managing, 103–105 motivating example, 323–324 in Nondeterministic Tests, 27 overview, 317 Prebuilt Fixture. See Prebuilt\n\nsingle tests, 161–162 Single-Condition Test\n\nEager Tests solution, 225–226 Obscure Tests solution, 188 principles. See Verify One\n\nCondition per Test\n\nunit testing, 6\n\nSingle-Outcome Assertion\n\nAssertion Method, 366–367 deﬁ ned, 365 example, 369\n\nSingleton\n\nFixture\n\nin Dependency Lookup,\n\nrefactoring, 324 Setup Decorator. See Setup\n\nDecorator\n\n688–689\n\nInteracting Tests, 230 retroﬁ tting testability, 146–147\n\nSlow Tests cause, 318–321 Suite Fixture Setup. See Suite\n\nFixture Setup\n\nSingleton, Substituted example, 586–587 when to use, 581\n\nTable Truncation Teardown.\n\nSee Table Truncation Teardown\n\nTest Run Wars cause, 236 Unrepeatable Tests cause, 235 using Finder Methods, 600–601 when to use, 318\n\nskeletons, 744 Slow Component Usage, 254 Slow Tests\n\nAsynchronous Tests, 255–256 avoiding with Shared Fixture,\n\n318–321\n\nShared Fixture Guard Assertion,\n\n492–493\n\nShared Fixture State Assertion, 491 Simple Success Test\n\ndatabase testing, 168 design for testability, 7 due to Transaction Rollback\n\nTeardown, 669\n\nexample, 352–353 happy path code, 177\n\nGeneral Fixtures, 255 impact, 253\n\nwww.it-ebooks.info\n\nIndex\n\nintroduction, 15 optimizing execution, 180 persistent ﬁ xtures, 102 preventing with Fake Object.\n\nsolution patterns, code smells Asynchronous Code, 211 Conditional Veriﬁ cation Logic,\n\n203–204\n\nSee Fake Object preventing with Test\n\nDouble, 523\n\nSlow Component Usage, 254 symptoms, 253 Too Many Tests, 256–257 troubleshooting, 253–254\n\nsmells, test. See test smells Smith, Shaun, 39 Smoke Test\n\nCut and Paste code reuse, 215 Eager Test, 188 Equality Pollution, 222 Flexible Test, 203 General Fixture, 192 Hard-Coded Test Data, 196 Hard-To-Test Code, 209 Highly Coupled Code, 210 Indirect Testing, 197–199 Irrelevant Information, 193 Multiple Test Conditions,\n\ndevelopment process, 4 suites, 597–598 Test Discovery, 394\n\nsniff test\n\ndeﬁ ned, xxxviii test smells, 10\n\nsolution patterns, behavior smells Asynchronous Tests, 256 Behavior Sensitivity, 242–243 Context Sensitivity, 246 Data Sensitivity, 243–245 Eager Tests, 225–226 Frequent Debugging, 249 General Fixture, 255 Interacting Test Suites, 232 Interacting Tests, 231 Interface Sensitivity, 241–242 Manual Intervention,\n\n250–252\n\n207–208\n\nMystery Guests, 190 Obscure Tests, 199 Production Logic in Test, 205 Test Code Duplication, 115–216 Test Dependency in Production, 221\n\nTest Hook, 219 For Tests Only, 220 Untestable Test Code, 212\n\nsolution patterns, project smells Buggy Test, 261–262 Infrequently Run Test, 269 Lost Test, 270–271 Missing Unit Test, 271 Neverfail Test, 274 Untested Code, 272 Untested Requirements, 274\n\nMissing Assertion Messages,\n\n226–227\n\nSpecial-Purpose Suite, 595–596 speciﬁ cation\n\nResource Leakage, 233 Resource Optimism, 234 Slow Component Usage, 254 Test Run War, 236–237 Too Many Tests, 257 Unrepeatable Tests, 235\n\nExpected Behavior, 470–471 Expected Behavior\n\nexample, 473\n\nExpected Object example, 466 Expected State, 464–465 tests as, xxxiii, 22\n\nwww.it-ebooks.info\n\n869\n\n870\n\nIndex\n\nspikes, 809 Spy, Test. See Test Spy SQL, Table Truncation Teardown\n\nstatic binding\n\ndeﬁ ned, 809 Dependency Injection,\n\nusing, 666–667 Standard Fixture\n\nimplementation, 307–308 motivating example, 308 overview, 305–306 refactoring, 309–310 when to use, 306–307 standard test interface, 378 starbursts, diagramming\n\n678–679 static methods, 809 static variables, 809 Statically Generated Test\n\nDoubles, 561\n\nSTDD (storytest-driven development), 4, 810\n\nstop on ﬁ rst failure\n\nNaive xUnit Test Interpreter,\n\nnotation, xlii\n\n292–293\n\nstate, initializing via\n\nxUnit introduction, 57\n\nBack Door Manipulation.\n\nStored Procedure Test\n\nSee Back Door Manipulation Named State Reaching Method,\n\n417–418\n\nState Veriﬁ cation\n\nvs. behavior, 36 examples, 466–467 implementation, 463–465 indirect outputs, 179–180 introduction, 109–112 motivating example, 465 overview, 462–463 refactoring, 465–466 Self-Checking Tests, 108 Use the Front Door First, 41 when to use, 463 Stated Outcome Assertion\n\ndatabase testing, 172 examples, 658–660 implementation, 655–658 motivating example, 658 overview, 654 refactoring, 658 when to use, 654–655\n\nstorytest, 810 storytest-driven development\n\n(STDD), 4, 810\n\nstrategies, test automation. See test\n\nautomation strategies\n\nstress tests, cross-functionality, 52 strict Mock Object deﬁ ned, 138 when to use, 545\n\nAssertion Methods, 366 deﬁ ned, 365 example, 369 Guard Assertions as, 491 introduction, 110–111\n\nSTTCPW (The simplest thing that\n\ncould possibly work), 810\n\nStub, Test. See Test Stub Subclass, Test-Speciﬁ c. See\n\nTest-Speciﬁ c Subclass\n\nState-Exposing Subclass\n\nTest-Speciﬁ c Subclass, 289–590 when to use, 580\n\nSubclassed Humble Object, 700 Subclassed Inner Test Double,\n\n573–574\n\nstateless, 809 statements, “if”. See “if” statements\n\nSubclassed Singleton, 7 Subclassed Test Double, 146–147\n\nwww.it-ebooks.info\n\nIndex\n\nSubcutaneous Test\n\nSuites of Suites\n\ncustomer testing, 5 database testing, 174 design for testability, 7 Layer Tests, 343–344\n\nSubset Suite\n\nexample, 594–598 implementation, 594 introduction, 160–161 overview, 592 Too Many Tests solution, 257 when to use, 593 substitutable dependencies\n\nbuilding with Test enumeration,\n\n400\n\ndeﬁ ned, 388 example, 389–391 Interacting Test Suites, 231–232 introduction, 7, 15, 78\n\nSUnit\n\ndeﬁ ned, 750 Test Automation\n\nFrameworks, 300\n\nSuperclass, Testcase. See Testcase\n\nSuperclass\n\ndeﬁ ned, 810 Dependency Initialization\n\nSUT (system under test)\n\ncontrol points and observation\n\nTest, 352\n\npoints, 66–67\n\nusing Test Spy, 540 Substitutable Singleton\n\nin Dependency Lookup, 689 example, 586–587, 692–693 retroﬁ tting testability, 146–147 when to use, 581 substitution mechanisms,\n\n688–689\n\ndangers of modifying, 41–42 deﬁ ned, 810–811 Four-Phase Test, 358–361 interface sensitivity, xxxii isolation principle, 43–44 minimizing risk, 24–25 preface, xxii–xxiii replacing in Parameterized\n\nSuite Fixture Setup\n\nTest, 609\n\nexample, 444–446 implementation, 442–443 implicit, 426 motivating example, 443–444 overview, 441–442 refactoring, 444 Shared Fixture strategies, 64 Shared Fixtures, 104–105 when to use, 442\n\nresult veriﬁ cation. See result\n\nveriﬁ cation\n\nstate vs. behavior veriﬁ cation, 36 terminology, xl–xli test automation tools, 53–54 Test Hook in, 711–712 understanding with test\n\nautomation, 23\n\nsuite method, 399 suites\n\nNamed Test Suite. See Named\n\nTest Suite\n\nSUT API Encapsulation\n\nChained Tests as, 455 Indirect Testing solution, 198 Interface Sensitivity\n\ntest organization, 160–162 Test Suite Object. See Test Suite\n\nsolution, 241\n\nSUT Encapsulation Method,\n\nObject\n\n601–602\n\nwww.it-ebooks.info\n\n871\n\n872\n\nIndex\n\nSymbolic Constants example, 716 Literal Value, 715 symptoms, behavior smells Assertion Roulette, 224 Asynchronous Tests, 255 Behavior Sensitivity, 242 Context Sensitivity, 245 Data Sensitivity, 243 Eager Tests, 224–225 Erratic Tests, 228 Fragile Tests, 239 Frequent Debugging, 248 General Fixtures, 255 Interacting Test Suites, 231 Interacting Tests, 229 Interface Sensitivity, 241 Manual Intervention, 250–252 Missing Assertion Messages, 226 Nondeterministic Tests, 237 Resource Leakage, 233 Resource Optimism, 233 Slow Tests, 253 Test Run Wars, 236 Too Many Tests, 256 Unrepeatable Tests, 234–235\n\nMystery Guests, 188–189 Obscure Tests, 186 Production Logic in Test,\n\n204–205\n\nTest Code Duplication, 213–214 Test Dependency in Production, 220\n\nTest Logic in Production, 217 test smells, 10 For Tests Only, 219 Untestable Test Code, 211\n\nsymptoms, project smells Buggy Tests, 260 Developers Not Writing Tests,\n\n263\n\nHigh Test Maintenance\n\nCost, 265\n\nInfrequently Run Tests, 268–269 Lost Tests, 269 Missing Unit Tests, 271 Neverfail Tests, 274 Production Bugs, 268 Untested Code, 271–272 Untested Requirements, 272–273\n\nsymptoms, test smells, 10 synchronous tests\n\nsymptoms, code smells\n\navoiding with Humble Object,\n\nAsynchronous Code, 210 Complex Teardown, 206 Conditional Test Logic, 200 Eager Tests, 187–188 Equality Pollution, 221 Flexible Tests, 202 General Fixtures, 190–191 Hard-Coded Test Data,\n\n696–697 deﬁ ned, 810\n\nsystem under test (SUT). See SUT\n\n(system under test)\n\nT\n\nTable Truncation Teardown\n\n194–195\n\nHard-To-Test Code, 209 Highly Coupled Code, 210 Indirect Testing, 196–197 Irrelevant Information, 192–193 Multiple Test Conditions, 207\n\ndata access layer testing, 173 deﬁ ned, 100 examples, 665–667 implementation, 662–664 motivating example, 664 overview, 661–662\n\nwww.it-ebooks.info\n\nIndex\n\nrefactoring, 664–665 when to use, 662\n\ntabular data, 291 Tabular Test\n\nChained Tests, 457–458 with framework support, 614 implementation, 609–610 Incremental, 613–614 Independent, 612–613\n\ntasks, 811 TDD (test-driven development)\n\ndeﬁ ned, 813 implementing utility methods,\n\n122\n\nintroduction, xxxiii–xxxiv Missing Unit Tests, 271 need-driven development, 149 process, 4–5 Test Automation\n\nFrameworks, 301\n\ntest automater, 811 test automation, xxix–xliii assumptions, xxxix–xl automated unit testing, xxx–xxxii brief tour, 3–8 code samples, xli–xlii developer testing, xxx diagramming notation, xlii feedback, xxix fragile test problem, xxxi–xxxii limitations, xliii overview, xxix patterns, xxxiv–xxxviii refactoring, xxxviii–xxxix terminology, xl–xli testing, xxx uses of, xxxiii–xxxiv Test Automation Framework\n\nintroduction, 75 pattern description, 298–301\n\ntest automation principles, 40\n\ntest automation goals, 19–29\n\nteardown, ﬁ xture. See ﬁ xture\n\nteardown\n\nTeardown Guard Clause\n\nexample, 513 Implicit Teardown, 517–518 In-line Teardown, 511\n\ntearDown method\n\nImplicit Teardown, 516–519 persistent ﬁ xtures, 98 Setup Decorator. See Setup\n\nDecorator Template Method, 164 Temporary Test Stub\n\nwhen to use, 530–531 xUnit terminology, 741–744\n\nterminology\n\ntest automation introduction,\n\nxl–xli\n\ntransient ﬁ xtures, 86–88 xUnit. See xUnit basics\n\nease of running, 25–27 improving quality, 22–23 list of, 757–759 objectives, 21–22 reducing risk, 23–25 system evolution, 29 understanding SUT, 23 why test?, 19–21 writing and maintaining, 27–29\n\nTest Automation Manifesto, 39 test automation philosophies, 31–37\n\nauthor’s, 37 differences, 32–36 importance of, 31–32\n\ntest automation principles, 39–48 Communicate Intent, 41 Design for Testability, 40 Don’t Modify the SUT, 41–42 Ensure Commensurate Effort and Responsibility, 47–48\n\nwww.it-ebooks.info\n\n873\n\n874\n\nIndex\n\nIsolate the SUT, 43–44 Keep Test Logic Out of Production Code, 45 Keep Tests Independent,\n\ntools for, 53–58 transient fresh ﬁ xtures, 61–62 what’s next, 73 wrong, 264\n\n42–43\n\nMinimize Test Overlap, 44 Minimize Untestable Code,\n\n44–45\n\nTest Bed. See Prebuilt Fixture test cases, 811 test code, 811 Test Code Duplication\n\noverview, 39–40 Test Concerns Separately, 47 Use the Front Door First,\n\n40–41\n\ncauses, 214–215 Custom Assertions, 475 Delegated Setup, 412 High Test Maintenance\n\nVerify One Condition per Test,\n\n45–47\n\nWrite the Tests First, 40\n\ntest automation roadmap, 175–181 alternative path veriﬁ cation,\n\n178–179\n\ndifﬁ culties, 175–176 direct output veriﬁ cation, 178 execution and maintenance optimization, 180–181 happy path code, 177–178 indirect outputs veriﬁ cation,\n\n178–180\n\nmaintainability, 176–177 test automation strategies, 49–73\n\nbrief tour, 3–8 control points and observation\n\nCost, 266 impact, 214 In-Line Setup, 89 introduction, 16 possible solution, 216 reducing, 114–119 reducing with Conﬁ gurable\n\nTest Doubles. See Conﬁ gurable Test Double\n\nreducing with Parameterized Tests. See Parameterized Test\n\nreducing with Test Utility Methods. See Test Utility Method\n\nremoving with Testcase Class per Fixture. See Testcase Class per Fixture\n\npoints, 66–67\n\ncross-functional tests, 52–53 divide and test, 71–72 ensuring testability, 65 ﬁ xture strategies overview, 58–61 interaction styles and testability\n\npatterns, 67–71 overview, 49–50 per-functionality tests, 50–52 persistent fresh ﬁ xtures, 62–63 shared ﬁ xture strategies, 63–65 test-driven testability, 66\n\nreusing test code, 162 symptoms, 213–214\n\nTest Commands, 82 Test Concerns Separately, 47 test conditions, 154, 811–812 test database, 812 test debt, 812 Test Dependency in Production,\n\n220–221\n\nTest Discovery\n\nintroduction, 78 Lost Tests solution, 271\n\nwww.it-ebooks.info\n\nIndex\n\npattern description, 393–398 Test Suite Object Generator, 293 Test Suite Objects, 388 Test Double, 125–151, 521–590\n\nBack Door Manipulation, 332 Behavior Veriﬁ cation, 112 Conﬁ gurable Test Double.\n\nSee Conﬁ gurable Test Double\n\nconﬁ guring, 141–142 considerations, 150 customer testing, 5 database testing, 169–171 Dependency Injection.\n\nproviding, 140–141 retroﬁ tting testability,\n\n146–148\n\nreusing test code, 162 terminology, 741–744 vs. Test Hook, 709–712 Test Spy, 137, 538–543 Test Stub. See Test Stub Test-Speciﬁ c Subclass.\n\nSee Test-Speciﬁ c Subclass\n\ntypes of, 133–134 when to use, 523–526\n\nTest Double Class\n\nSee Dependency Injection\n\nDependency Lookup, 144–145 dependency replacement, 739 design for testability, 7 Don’t Modify the SUT, 41–42 Dummy Object, 134–135 example, 526–528 Fake Object. See Fake Object Fragile Test, 240 Hard-Coded Test Double.\n\nexample, 572–573 implementation, 569–570\n\nTest Double Subclass\n\nimplementation, 570 when to use, 580–581\n\ntest drivers\n\nAssertion Messages, 370 deﬁ ned, 813 test driving, 813 Test Enumeration\n\nSee Hard-Coded Test Double\n\nHighly Coupled Code\n\nintroduction, 153 pattern description, 399–402\n\nsolution, 210\n\nindirect input and output,\n\n125–126\n\nindirect input control, 128–129 indirect input, importance\n\nof, 126\n\ntest errors, 80, 813 test failure, 80, 813 test ﬁ rst development deﬁ ned, 813–814 process, 4–5 test automation philosophy,\n\nindirect output, importance of,\n\n32–33\n\n126–127\n\nvs. test-last development, xxxiv\n\nindirect output veriﬁ cation,\n\nTest Fixture Registry\n\n130–133\n\ninstalling, 143 minimizing risk, 25 Mock Object. See Mock Object other uses, 148–150 outside-in development, 35–36 overview, 522–523\n\naccessing Shared Fixtures, 104 Test Helper use, 644 test ﬁ xtures. See ﬁ xtures Test Helper\n\nAutomated Teardown, 505 introduction, xxiii pattern description, 643–647\n\nwww.it-ebooks.info\n\n875\n\n876\n\nIndex\n\nTest Helper Mixin\n\nexample, 641–642 vs. Testcase Superclass, 639\n\nTest Hook\n\nimplementation, 349 invocation, 402 Lost Tests, 269–270 minimizing untested code,\n\npattern description, 709–712 in Procedural Test Stub,\n\n44–45\n\norganization, 7, 155–158. See\n\n135–136\n\nalso test organization patterns\n\nretroﬁ tting testability, 148 Test Logic in Production,\n\noverview, 348–349 persistent ﬁ xtures. See persistent\n\n217–219 testability, 70\n\nTest Logic, Conditional.\n\nSee Conditional Test Logic\n\nTest Logic in Production\n\nEquality Pollution, 221–222 impact, 217 introduction, 17 symptoms, 217 Test Dependency in Production,\n\n220–221\n\nTest Hooks, 148, 217–219 For Tests Only, 219–220\n\nﬁ xtures\n\nright-sizing, 154–155 running, 81 selection, 404–405 Simple Success Test, 349–350 Simple Success Test example,\n\n352–353\n\ntest automation philosophies, 34 Test Commands, 82 Test Concerns Separately, 47 Test Suite Objects, 82 Testcase Object implementation,\n\n384–385\n\ntest maintainer, 815 Test Method\n\ntransient ﬁ xture management.\n\nSee transient ﬁ xtures\n\ncalling Assertion. See Assertion\n\nMethod\n\nunit testing, 6 Verify One Condition per Test,\n\nConstructor Test example,\n\n46–47\n\n355–357\n\nwriting simple tests, 28\n\nConstructor Tests, 351 Dependency Initialization\n\nTests, 352\n\nTest Method Discovery\n\ndeﬁ ned, 394–395 examples, 395–397\n\nenumeration, 401 Expected Exception Test,\n\nTest Object Registry. See Automated\n\nTeardown\n\n350–351\n\nExpected Exception Test using\n\nblock closure, 354–355\n\nExpected Exception Test using\n\nmethod attributes, 354\n\nExpected Exception Test using\n\ntest organization, 153–165 code reuse, 162–164 introduction, 153 naming conventions, 158–159 overview, 7 right-sizing Test Methods,\n\ntry/catch, 353–354\n\n154–155\n\nﬁ xture design, 59\n\ntest ﬁ les, 164–165\n\nwww.it-ebooks.info\n\nIndex\n\nTest Methods and Testcase\n\nTest Runner\n\nClasses, 155–158 test suites, 160–162\n\nGraphical. See Graphical Test\n\nRunner\n\ntest organization patterns, 591–647 Named Test Suite. See Named\n\nTest Suite\n\nimplementation, 378–381 introduction, 79 Missing Assertion Messages,\n\nParameterized Test.\n\n226–227\n\nSee Parameterized Test\n\nTest Helper, 643–647 Test Utility Method. See Test\n\nUtility Method\n\nTestcase Class per Class.\n\noverview, 377–378 Test Automation Frameworks,\n\n300 test runs, 815 Test Selection\n\nSee Testcase Class per Class\n\nTestcase Class per Feature.\n\npattern description, 403–405 Test Suite Object, 388\n\nSee Testcase Class per Feature\n\ntest smells, 9–17\n\nTestcase Class per Fixture.\n\nSee Testcase Class per Fixture\n\nTestcase Superclass, 638–642\n\ntest packages\n\ndeﬁ ned, 815 test ﬁ le organization, 164–165\n\naliases and causes, 761–765 behavior. See behavior smells catalog of, 12–17 code smells. See code smells database testing. See database\n\ntesting\n\ntest readers, 815 test refactorings. See also refactoring Extractable Test Component,\n\n735–736\n\ndeﬁ ned, 808, 816 introduction, xxxvi overview, 9–11 patterns and principles vs.,\n\nIn-line Resource, 736–737 Make Resources Unique,\n\n737–738\n\nxxxv–xxxvi\n\nproject smells. See project smells reducing Test Code Duplication,\n\nMinimize Data, 738–739 Replace Dependency with Test\n\n114–119\n\nTest Spy\n\nDouble, 739\n\nSet Up External Resource, 740\n\ntest results\n\nBack Door Veriﬁ cation, 333 Behavior Veriﬁ cation, 113 Conﬁ gurable. See Conﬁ gurable\n\ndeﬁ ned, 815 introduction, 79–80 veriﬁ cation. See result veriﬁ cation\n\nTest Run War\n\nTest Double\n\nexamples, 542–543 implementation, 540–541 indirect outputs veriﬁ cation,\n\ndatabase testing, 169 Erratic Tests cause, 235–237 introduction, 15 vs. Shared Fixture strategy, 64\n\n179–180\n\nintroduction, 131–133,\n\n137, 525\n\nmotivating example, 541\n\nwww.it-ebooks.info\n\n877\n\n878\n\nIndex\n\noverview, 538–539 Procedural Behavior Veriﬁ cation, 470 refactoring, 541–542 when to use, 539–540 xUnit terminology, 741–744\n\ntest strategy patterns, 277–345\n\nData-Driven Test. See Data-\n\nwhen to use, 530–531 xUnit terminology, 741–744\n\ntest success, 816 Test Suite Enumeration\n\ndeﬁ ned, 400 example, 402 Test Suite Factory, 232 Test Suite Object\n\nDriven Test\n\nFresh Fixture. See Fresh Fixture Layer Test. See Layer Test Minimal Fixture, 302–304 Recorded Test. See Recorded\n\nTest\n\nScripted Test, 285–287 Shared Fixture. See Shared\n\nFixture\n\nenumeration, 400 Interacting Test Suites, 231–232 introduction, 7, 82 pattern description, 387–392 Test Suite Object Generator, 293 Test Suite Object Simulator, 293 Test Suite Procedure\n\ndeﬁ ned, 388–389 example, 391–392\n\nStandard Fixture. See Standard\n\ntest suites\n\nFixture\n\nTest Automation Framework,\n\n298–301\n\ndeﬁ ned, 816 Lost Tests, 269–270 Named Test Suites. See Named\n\ntest strippers, 816 Test Stub\n\nTest Suite\n\nTest Tree Explorer, 161–162,\n\nBehavior-Modifying Subclass,\n\n380–381\n\n584–585\n\nTest Utility Method\n\nConﬁ gurable. See Conﬁ gurable\n\nTest Double\n\nconﬁ guring, 141–142 Context Sensitivity solution, 246 controlling indirect inputs, 129 creating in-line resources, 737 examples, 533–537 implementation, 531–532 indirect inputs control, 179 inside-out development, 34–35 introduction, 133, 135–136, 524 motivating example, 532–533 overview, 529–530 refactoring, 533 unit testing, 6\n\nCommunicate Intent, 41 eliminating loops, 121 example, 605–606 implementation, 602–603 introduction, xxiii, 16–17, 23,\n\n162–163\n\nmotivating example, 603–604 Obscure Tests solution, 199 overview, 599 reducing risk of bugs, 181 refactoring, 605 reusing, lviii–lix reusing via Test Helper, 643–647 reusing via Testcase Superclass,\n\n638–642\n\nwww.it-ebooks.info\n\nIndex\n\nusing TDD to write, 122 when to use, 600–602\n\nTest Utility Test, 603 testability, design for. See design-\n\nTestcase Superclass\n\npattern description, 638–642 reusing test code, 163–164 Test Discovery using, 397–398\n\nfor-testability Testcase Class\n\ntest-driven bug ﬁ xing, 812 test-driven development (TDD).\n\nintroduction, 78 organization, 7, 155–158 pattern description, 373–376 reusable test logic, 123 selection, 404–405\n\nTestcase Class Discovery\n\ndeﬁ ned, 394 example, 397–398\n\nTestcase Class per Class\n\nexample, 618–623 implementation, 618 overview, 617 when to use, 618 Testcase Class per Feature example, 628–630 implementation, 626 motivating example, 626–627 overview, 624 refactoring, 627–628 when to use, 625 Testcase Class per Fixture example, 635–637 implementation, 632–633 motivating example,\n\nSee TDD (test-driven development)\n\nTest-Driven Development: By\n\nExample (Beck), 301 test-driven testability, 66 Testing by Layers. See Layer Test testing terminology. See terminology test-last development deﬁ ned, 815 strategy, 65 test automation philosophy,\n\n32–33\n\nvs. test-ﬁ rst development, xxxiv\n\nTestNG\n\ndeﬁ ned, 750 Interacting Tests, 231 Testcase Object exception,\n\n384–385 vs. xUnit, 57\n\nTests as Documentation\n\nCommunicate Intent, 41 customer testing, 5 deﬁ ned, 23 reusing test code, 162 unit testing, 6\n\n633–634 overview, 631 refactoring, 634–635 Verify One Condition per Test,\n\nTests as Safety Net, 24, 260 Tests as Speciﬁ cation, xxxiii, 22 test-speciﬁ c equality, 588–589, 816 Test-Speciﬁ c Extension.\n\n46–47\n\nSee Test-Speciﬁ c Subclass\n\nwhen to use, 632\n\nTest-Speciﬁ c Subclass\n\nTestcase Class per Method, 625 Testcase Class per User Story, 625 Testcase Object\n\nBehavior-Exposing Subclass,\n\n587\n\nBehavior-Modifying Subclass\n\nintroduction, 81 pattern description, 382–386\n\n(Substituted Singleton), 586–587\n\nwww.it-ebooks.info\n\n879\n\n880\n\nIndex\n\nBehavior-Modifying Subclass\n\ntransient ﬁ xtures, 85–94\n\n(Test Stub), 584–585\n\ndeﬁ ning Test-Speciﬁ c Equality,\n\n588–589\n\nDon’t Modify the SUT, 42 implementation, 581–582 Isolate the SUT, 44 motivating example, 582–584 overview, 579–580 refactoring, 584 retroﬁ tting testability, 146–147 State-Exposing Subclass,\n\n289–590\n\nFor Tests Only solution, 220 when to use, 580–581\n\nDelegated Setup, 89–91 hybrid setup, 93 Implicit Setup, 91–93 In-Line Setup, 88–89 overview, 85–86 vs. persistent ﬁ xtures, 96 tearing down, 93–94 terminology, 86–88 what’s next, 94 Transient Fresh Fixture\n\ndatabase testing, 170 deﬁ ned, 60–61, 314 vs. Shared Fixture, 61–62\n\ntroubleshooting\n\nTest::Unit, 750 Thread-Speciﬁ c Storage, 688–689 Too Many Tests, 256–257 tools\n\nautomated unit testing,\n\nxxx–xxxi\n\nBuggy Tests, 261 Developers Not Writing Tests,\n\n264\n\nErratic Tests, 228–229 Fragile Tests, 239–240 High Test Maintenance Cost,\n\ncommercial record and playback,\n\n267\n\n282–283\n\nSlow Tests, 253–254\n\nQTP. See QTP (QuickTest\n\nProfessional)\n\nrobot user. See robot user tools for test automation strategy,\n\nTrue Humble Executable, 703–706 True Humble Objects, 699–700 TRUNCATE command. See Table\n\nTruncation Teardown\n\n53–58\n\ntry/catch\n\ntypes of, 753–756\n\nExpected Exception Tests,\n\nTransaction Controller, Humble.\n\n353–354\n\nSee Humble Transaction Controller\n\nSingle-Outcome Assertions, 367\n\ntry/ﬁ nally block\n\nTransaction Rollback Teardown data access layer testing, 173 deﬁ ned, 100 examples, 673–675 implementation, 671 motivating example, 672 overview, 668–669 refactoring, 672 when to use, 669–671\n\ncleaning up ﬁ xture teardown\n\nlogic, l–liv\n\nImplicit Teardown, 519 In-line Teardown, 512–513\n\ntype compatibility, 679 type visibility\n\nTest Helper use, 644 Test Utility Methods, 603 Testcase Superclass use, 639\n\nwww.it-ebooks.info\n\nIndex\n\nU\n\nUntested Code\n\nUAT (user acceptance tests)\n\ndeﬁ ned, 817 principles, 42 UI (User Interface) tests\n\nasynchronous tests, 70–71 Hard-To-Test Code, 71–72 tools, 55\n\nUML (Uniﬁ ed Modeling\n\nLanguage), 816\n\nalternative path veriﬁ cation,\n\n178–179\n\nindirect inputs and, 126 Isolate the SUT, 43 minimizing, 44–45 preventing with Test Doubles,\n\n523\n\nProduction Bugs, 271–272 unit testing, 6 Untested Requirement\n\nUnconﬁ gurable Test Doubles, 527 unexpected exceptions, 352 Unﬁ nished Test Assertion, 494–497 Unﬁ nished Test Method from\n\nFrequent Debugging cause,\n\n249\n\nindirect output testing, 127 preventing with Test\n\nTemplate, 496–497\n\nUniﬁ ed Modeling Language\n\n(UML), 816\n\nunique resources, 737–738 Unit Testing with Java (Link), 743 unit tests\n\ndeﬁ ned, 817 introduction, 6 per-functionality, 51 rules, 307 Scripted Tests, 285–287 xUnit vs. Fit, 290–292 unnecessary object elimination,\n\nDoubles, 523\n\nProduction Bugs cause,\n\n272–274\n\nreducing via Isolate the\n\nSUT, 43 usability tests, 53 use cases, 817 Use the Front Door First deﬁ ned, 40–41 Overspeciﬁ ed Software\n\navoidance, 246 user acceptance tests (UAT)\n\n303–304\n\nUnrepeatable Test\n\ndeﬁ ned, 817 principles, 42 User Interface (UI) tests\n\ndatabase testing, 169 Erratic Test cause, 234–235 introduction, 15, 64 persistent fresh ﬁ xtures, 96 vs. Repeatable Test, 26–27\n\nasynchronous tests, 70–71 Hard-To-Test Code, 71–72 tools, 55\n\nuser story\n\nUntestable Test Code\n\navoiding Conditional Logic,\n\ndeﬁ ned, 817 Testcase Class per, 625 utility methods. See Test Utility\n\n119–121\n\nHard-To-Test Code, 211–212\n\nMethod\n\nutPLSQL, 750\n\nwww.it-ebooks.info\n\n881\n\n882\n\nIndex\n\nV\n\nvisibility\n\nvalue patterns, 713–732\n\nDerived Values, 718–722 Dummy Objects, 728–732 Generated Values, 723–727 Literal Values, 714–717\n\nvariables\n\nin Derived Values, 718–722 global, 92, 798 instance. See instance variables local. See local variables procedure variables, 805–806 static, 809 VB Lite Unit, 751 VbUnit\n\nof SUT features from Test-\n\nSpeciﬁ c Subclass, 581–582\n\ntest ﬁ le organization, 165 type. See type visibility visual objects, Humble Dialog\n\nuse, 706\n\nVisual Studio, 756\n\nW\n\nwaterfall design, 65 Watir\n\ndeﬁ ned, 756 Test Automation Frameworks,\n\n301\n\ndeﬁ ned, 751 Suite Fixture Setup support, 442 Testcase Class terminology, 376 xUnit terminology, 300 Verbose Tests. See Obscure Test veriﬁ cation\n\nalternative path, 178–179 Back Door Manipulation,\n\n329–330\n\nBack Door using Test Spy, 333 cleaning up logic, xlvi–l direct output, 178 indirect outputs, 130–133,\n\n178–180\n\nstate vs. behavior, 36 test results. See result veriﬁ cation Verify One Condition per Test,\n\ntest automation tools, 53 Weinberg, Gerry, xxiv–xxv, 61–62 widgets\n\nHumble Dialog use, 706 recognizers, 299\n\nWikipedia, 729 Working Effectively with Legacy\n\nCode (Feathers), 210 Write the Tests First, 40 writing tests\n\nDevelopers Not Writing Tests\n\nproject smells, 263–264 development process, 4–5 goals, 27–29 philosophies. See test automation\n\nphilosophies\n\nprinciples. See test automation\n\n45–47\n\nprinciples\n\nVeriﬁ cation Method\n\ndeﬁ ned, 477, 602 example, 482–483\n\nX\n\nXML data ﬁ les, Data-Driven Tests,\n\nVerify One Condition per Test\n\n294–295\n\ndeﬁ ned, 40, 45–47 right-sizing Test Methods,\n\nxUnit\n\nData-Driven Tests with CSV\n\n154–155 verify outcome, 817 Virtual Clock, 246\n\ninput ﬁ le, 296\n\nData-Driven Tests with XML\n\ndata ﬁ le, 294–295\n\nwww.it-ebooks.info\n\ndeﬁ ned, 751 family members, 747–751 vs. Fit, 291–292 ﬁ xture deﬁ nitions, 86 Interacting Test Suites, 232 introduction, 56–57 language-speciﬁ c terminology,\n\nxl–xli\n\nﬁ xtures, 78 overview, 75–76 procedural world, 82–83 running Test Methods, 81 running tests, 79 Test Commands, 82 test results, 79–80 Test Suite Object, 82 xUnit basics patterns, 347–405\n\nmodern, 55 Naive xUnit Test Interpreter,\n\n292–293\n\nAssertion Message, 370–372 Assertion Method.\n\nproﬁ ling tools, 254 Suite Fixture Setup support,\n\n442–443\n\nsweet spot, 58 terminology, 741–746 Test Automation Frameworks,\n\nSee Assertion Method Four-Phase Test, 358–361 Test Discovery, 393–398 Test Enumeration, 399–402 Test Method.\n\nSee Test Method\n\n300\n\nTest Runner.\n\ntest ﬁ xtures, 814 test organization mechanisms,\n\n153\n\nSee Test Runner\n\nTest Selection, 403–405 Test Suite Object, 82,\n\nxUnit basics, 75–83\n\n387–392\n\ndeﬁ ning suites of tests, 78–79 deﬁ ning tests, 76–78\n\nTestcase Class, 373–376 Testcase Object, 382–386\n\nwww.it-ebooks.info\n\nIndex\n\n883\n\nList of Smells\n\nAssertion Roulette (224): It is hard to tell which of several assertions within the same test method caused a test failure. Includes Eager Test, Missing Assertion Message.\n\nBuggy Tests (260): Bugs are regularly found in the automated tests. Includes Fragile Test, Hard-to-Test Code, Obscure Test.\n\nConditional Test Logic (200): A test contains code that may or may not be executed. Includes Complex Teardown, Condi- tional Veriﬁ cation Logic, Flexible Test, Multiple Test Conditions, Production Logic in Test.\n\nDevelopers Not Writing Tests (263): Developers aren’t writing automated tests. Includes Hard-to-Test Code, Not Enough Time, Wrong Test Automation Strategy.\n\nErratic Test (228): One or more tests are behaving erratically; sometimes they pass and sometimes they fail. Includes Inter- acting Test Suites, Interacting Tests, Lonely Test, Nondeterministic Test, Resource Leakage, Resource Optimism, Test Run War, Unrepeatable Test.\n\nFragile Test (239): A test fails to compile or run when the SUT is changed in ways that do not affect the part the test is exer- cising. Includes Behavior Sensitivity, Context Sensitivity, Data Sensitivity, Fragile Fixture, Interface Sensitivity, Overspeciﬁ ed Software, Sensitive Equality.\n\nFrequent Debugging (248): Manual debugging is required to determine the cause of most test failures.\n\nHard-to-Test Code (209): Code is difﬁ cult to test. Includes Asynchronous Code, Hard-Coded Dependency, Highly Coupled Code, Untestable Test Code.\n\nHigh Test Maintenance Cost (265): Too much effort is spent maintaining existing tests. Includes Fragile Test, Hard-to-Test Code, Obscure Test.\n\nManual Intervention (250): A test requires a person to perform some manual action each time it is run. Includes Manual Event Injection, Manual Fixture Setup, Manual Result Veriﬁ cation.\n\nObscure Test (186): It is difﬁ cult to understand the test at a glance. Includes Eager Test, General Fixture, Hard-Coded Test Data, Indirect Testing, Irrelevant Information, Mystery Guest.\n\nProduction Bugs (268): We ﬁ nd too many bugs during formal test or in production. Includes Infrequently Run Tests, Lost Test, Missing Unit Test, Neverfail Test, Untested Code, Untested Requirement.\n\nSlow Tests (253): The tests take too long to run. Includes Asynchronous Test, General Fixture, Slow Component Usage, Too Many Tests.\n\nTest Code Duplication (213): The same test code is repeated many times. Includes Cut-and-Paste Code Reuse, Reinventing the Wheel.\n\nTest Logic in Production (217): The code that is put into production contains logic that should be exercised only during tests. Includes Equality Pollution, For Tests Only, Test Dependency in Production, Test Hook.\n\nwww.it-ebooks.info\n\nAll Patterns Listed by the Problem They Solve\n\nHow do we prepare automated tests for our software?\n\nRecorded Test (278); Scripted Test (285); Data-Driven Test (288)\n\nHow do we make it easy to write and run tests?\n\nTest Automation Framework (298)\n\nWhere do we put our test code?\n\nTest Method (348); Testcase Class (373); Test Helper (643); Testcase Superclass (638)\n\nHow do we organize our Test Methods onto Testcase Classes?\n\nTestcase Class per Feature (624); Testcase Class per Fixture (631); Testcase Class per Class (617)\n\nHow do we make tests self-checking?\n\nState Veriﬁ cation (462); Behavior Veriﬁ cation (468); Assertion Method (362); Custom Assertion (474); Delta\n\nAssertion (485)\n\nHow do we structure our test logic?\n\nFour-Phase Test (358); Assertion Message (370); Unﬁ nished Test Assertion (494)\n\nHow do we reduce Test Code Duplication?\n\nData-Driven Test (288); Custom Assertion (474); Test Utility Method (599); Parameterized Test (607)\n\nHow do we run the tests?\n\nTest Runner (377); Testcase Object (382); Test Suite Object (387); Named Test Suite (592)\n\nHow does the Test Runner know which tests to run?\n\nTest Discovery (393); Test Enumeration (399); Test Selection (403)\n\nWhich ﬁ xture strategy should we use?\n\nMinimal Fixture (302); Standard Fixture (305); Fresh Fixture (311); Shared Fixture (317)\n\nHow do we construct the ﬁ xture?\n\nIn-line Setup (408); Delegated Setup (411); Creation Method (415); Implicit Setup (424)\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nPrebuilt Fixture (429); Lazy Setup (435); Suite Fixture Setup (441); Setup Decorator (447); Chained Tests (454)\n\nHow do we specify the values to be used in tests?\n\nDummy Object (728); Literal Value (714); Derived Value (718); Generated Value (723)\n\nHow do we tear down the Test Fixture?\n\nGarbage-Collected Teardown (500); In-line Teardown (509); Implicit Teardown (516); Automated Teardown (503);\n\nTable Truncation Teardown (661); Transaction Rollback Teardown (668)\n\nHow can we avoid Slow Tests?\n\nShared Fixture (317); Test Double (522); Fake Object (551)\n\nHow do we avoid Conditional Test Logic?\n\nCustom Assertion (474); Guard Assertion (490)\n\nHow can we verify logic independently?\n\nBack Door Manipulation (327); Layer Test (337); Test Double (522); Test Stub (529); Test Spy (538); Mock\n\nObject (544); Fake Object (551); Stored Procedure Test (654)\n\nHow do we implement Behavior Veriﬁ cation?\n\nTest Spy (538); Mock Object (544)\n\nHow do we tell a Test Double what to return or expect?\n\nConﬁ gurable Test Double (558); Hard-Coded Test Double (568)\n\nHow can we make code testable?\n\nHumble Object (695); Test-Speciﬁ c Subclass (579)\n\nHow do we design the SUT so that we can replace its dependencies at runtime? Dependency Injection (678); Dependency Lookup (686); Test Hook (709)\n\nwww.it-ebooks.info",
      "page_number": 777
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "www.it-ebooks.info",
      "content_length": 18,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "List of Patterns\n\nAssertion Message (370): We include a descriptive string argument in each call to an Assertion Method.\n\nAssertion Method (362): We call a utility method to evaluate whether an expected outcome has been achieved.\n\nAutomated Teardown (503): We keep track of all resources that are created in a test and automatically destroy/free them during teardown.\n\nBack Door Manipulation (327): We set up the test ﬁ xture or verify the outcome by going through a back door (such as direct database access).\n\nBehavior Veriﬁ cation (468): We capture the indirect outputs of the system under test (SUT) as they occur and compare them to the expected behavior.\n\nChained Tests (454): We let the other tests in a test suite set up the test ﬁ xture.\n\nConﬁ gurable Test Double (558): We conﬁ gure a reusable Test Double with the values to be returned or veriﬁ ed during the ﬁ xture setup phase of a test.\n\nCreation Method (415): We set up the test ﬁ xture by calling methods that hide the mechanics of building ready-to-use objects behind Intent-Revealing Names.\n\nCustom Assertion (474): We create a purpose-built Assertion Method that compares only those attributes of the object that deﬁ ne test-speciﬁ c equality.\n\nData-Driven Test (288): We store all the information needed for each test in a data ﬁ le and write an interpreter that reads the ﬁ le and executes the tests.\n\nDatabase Sandbox (650): We provide a separate test database for each developer or tester.\n\nDelegated Setup (411): Each test creates its own Fresh Fixture by calling Creation Methods from within the Test Methods.\n\nDelta Assertion (485): We specify assertions based on differences between the pre- and post-exercise state of the SUT.\n\nDependency Injection (678): The client provides the depended-on object to the SUT.\n\nDependency Lookup (686): The SUT asks another object to return the depended-on object before it uses it.\n\nDerived Value (718): We use expressions to calculate values that can be derived from other values.\n\nDummy Object (728): We pass an object that has no implementation as an argument of a method called on the SUT.\n\nFake Object (551): We replace a component that the SUT depends on with a much lighter-weight implementation.\n\nFour-Phase Test (358): We structure each test with four distinct parts executed in sequence.\n\nFresh Fixture (311): Each test constructs its own brand-new test ﬁ xture for its own private use.\n\nGarbage-Collected Teardown (500): We let the garbage collection mechanism provided by the programming language clean up after our test.\n\nGenerated Value (723): We generate a suitable value each time the test is run. Guard Assertion (490): We replace an if statement in a test with an assertion that fails the test if not satisﬁ ed.\n\nHard-Coded Test Double (568): We build the Test Double by hard-coding the return values and/or expected calls.\n\nHumble Object (695): We extract the logic into a separate, easy-to-test component that is decoupled from its environment. Implicit Setup (424): We build the test ﬁ xture common to several tests in the setUp method. Implicit Teardown (516): The Test Automation Framework calls our clean up logic in the tearDown method after every Test Method.\n\nIn-line Setup (408): Each Test Method creates its own Fresh Fixture by calling the appropriate constructor methods to build exactly the test ﬁ xture it requires.\n\nIn-line Teardown (509): We include teardown logic at the end of the Test Method immediately after the result veriﬁ cation.\n\nLayer Test (337): We can write separate tests for each layer of the layered architecture.\n\nLazy Setup (435): We use Lazy Initialization of the ﬁ xture to create it in the ﬁ rst test that needs it.\n\nLiteral Value (714): We use literal constants for object attributes and assertions.\n\nMinimal Fixture (302): We use the smallest and simplest ﬁ xture possible for each test.\n\nMock Object (544): We replace an object the SUT depends on with a test-speciﬁ c object that veriﬁ es it is being used correctly by the SUT.\n\nwww.it-ebooks.info",
      "content_length": 4018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Named Test Suite (592): We deﬁ ne a test suite, suitably named, that contains a set of tests that we wish to be able to run as a group.\n\nParameterized Test (607): We pass the information needed to do ﬁ xture setup and result veriﬁ cation to a utility method that implements the entire test life cycle.\n\nPrebuilt Fixture (429): We build the Shared Fixture separately from running the tests.\n\nRecorded Test (278): We automate tests by recording interactions with the application and playing them back using a test tool.\n\nScripted Test (285): We automate the tests by writing test programs by hand.\n\nSetup Decorator (447): We wrap the test suite with a Decorator that sets up the shared test ﬁ xture before running the tests and tears it down after all the tests are done.\n\nShared Fixture (317): We reuse the same instance of the test ﬁ xture across many tests.\n\nStandard Fixture (305): We reuse the same design of the test ﬁ xture across many tests.\n\nState Veriﬁ cation (462): We inspect the state of the SUT after it has been exercised and compare it to the expected state.\n\nStored Procedure Test (654): We write Fully Automated Tests for each stored procedure.\n\nSuite Fixture Setup (441): We build/destroy the shared ﬁ xture in special methods called by the Test Automation Framework before/after the ﬁ rst/last Test Method is called.\n\nTable Truncation Teardown (661): We truncate the tables modiﬁ ed during the test to tear down the ﬁ xture.\n\nTest Automation Framework (298): We use a framework that provides all the mechanisms needed to run the test logic so the test writer needs to provide only the test-speciﬁ c logic.\n\nTest Discovery (393): The Test Automation Framework discovers all the tests that belong to the test suite automatically.\n\nTest Double (522): We replace a component on which the SUT depends with a “test-speciﬁ c equivalent.”\n\nTest Enumeration (399): The test automater manually writes the code that enumerates all tests that belong to the test suite.\n\nTest Helper (643): We deﬁ ne a helper class to hold any Test Utility Methods we want to reuse in several tests.\n\nTest Hook (709): We modify the SUT to behave differently during the test.\n\nTest Method (348): We encode each test as a single Test Method on some class.\n\nTest Runner (377): We deﬁ ne an application that instantiates a Test Suite Object and executes all the Testcase Objects it contains.\n\nTest Selection (403): The Test Automation Framework selects the Test Methods to be run at runtime based on attributes of the tests.\n\nTest Spy (538): We use a Test Double to capture the indirect output calls made to another component by the SUT for later veriﬁ cation by the test.\n\nTest Stub (529): We replace a real object with a test-speciﬁ c object that feeds the desired indirect inputs into the SUT.\n\nTest Suite Object (387): We deﬁ ne a collection class that implements the standard test interface and use it to run a set of related Testcase Objects.\n\nTest Utility Method (599): We encapsulate the test logic we want to reuse behind a suitably named utility method.\n\nTest-Speciﬁ c Subclass (579): We add methods that expose the state or behavior needed by the test to a subclass of the SUT.\n\nTestcase Class (373): We group a set of related Test Methods on a single Testcase Class.\n\nTestcase Class per Class (617): We put all the Test Methods for one SUT class onto a single Testcase Class.\n\nTestcase Class per Feature (624): We group the Test Methods onto Testcase Classes based on which testable feature of the SUT they exercise.\n\nTestcase Class per Fixture (631): We organize Test Methods into Testcase Classes based on commonality of the test ﬁ xture.\n\nTestcase Object (382): We create a Command object for each test and call the run method when we wish to execute it.\n\nTestcase Superclass (638): We inherit reusable test-speciﬁ c logic from an abstract Testcase Superclass.\n\nTransaction Rollback Teardown (668): We roll back the uncommitted test transaction as part of the teardown.\n\nUnﬁ nished Test Assertion (494): We ensure that incomplete tests fail by executing an assertion that is guaranteed to fail.\n\nwww.it-ebooks.info",
      "content_length": 4113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "xUnit Test Patterns\n\nwww.it-ebooks.info",
      "content_length": 39,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "www.it-ebooks.info",
      "content_length": 18,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "xUnit Test Patterns\n\nRefactoring Test Code\n\nGerard Meszaros\n\nUpper Saddle River, NJ • Boston • Indianapolis • San Francisco New York • Toronto • Montreal • London • Munich • Paris • Madrid Capetown • Sydney • Tokyo • Singapore • Mexico City\n\nwww.it-ebooks.info",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trade- marks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed with initial capital letters or in all capitals.\n\nThe author and publisher have taken care in the preparation of this book, but make no expressed or implied war- ranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or con- sequential damages in connection with or arising out of the use of the information or programs contained herein.\n\nThe publisher offers excellent discounts on this book when ordered in quantity for bulk purchases or special sales, which may include electronic versions and/or custom covers and content particular to your business, training goals, marketing focus, and branding interests. For more information, please contact:\n\nU.S. Corporate and Government Sales (800) 382-3419 corpsales@pearsontechgroup.com\n\nFor sales outside the United States please contact:\n\nInternational Sales international@pearsoned.com\n\nLibrary of Congress Cataloging-in-Publication Data\n\nMeszaros, Gerard.\n\nXUnit test patterns : refactoring test code / Gerard Meszaros. p. cm. Includes bibliographical references and index. ISBN-13: 978-0-13-149505-0 (hardback : alk. paper) ISBN-10: 0-13-149505-4 1. Software patterns. 2. Computer software—Testing. I. Title. QA76.76.P37M49 2007 005.1—dc22 2006103488\n\nCopyright © 2007 Pearson Education, Inc.\n\nAll rights reserved. Printed in the United States of America. This publication is protected by copyright, and per- mission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, write to:\n\nPearson Education, Inc. Rights and Contracts Department 75 Arlington Street, Suite 300 Boston, MA 02116 Fax: (617) 848-7047\n\nISBN 13: 978-0-13-149505-0 ISBN 10: 0-13-149505-4 Text printed in the United States on recycled paper at Courier in Westford, Massachusetts. First printing, May 2007\n\nwww.it-ebooks.info",
      "content_length": 2245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "This book is dedicated to the memory of Denis Clelland, who recruited me away from Nortel in 1995 to work at ClearStream Consulting and thereby gave me the opportunity to have the experiences that led to this book. Sadly, Denis passed away on April 27, 2006, while I was ﬁ nalizing the second draft.\n\nwww.it-ebooks.info",
      "content_length": 319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Contents\n\nVisual Summary of the Pattern Language . . . . . . . . . . . . . . . . . . . xvii\n\nForeword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix\n\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxi\n\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxvii\n\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxix\n\nRefactoring a Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xlv\n\nPART I. The Narratives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n\nChapter 1. A Brief Tour . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 The Simplest Test Automation Strategy That Could Possibly Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Development Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Customer Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Unit Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Design for Testability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Test Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n\nChapter 2. Test Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 An Introduction to Test Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 What’s a Test Smell? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Kinds of Test Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 What to Do about Smells? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 A Catalog of Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 The Project Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 The Behavior Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 The Code Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\nvii\n\nwww.it-ebooks.info",
      "content_length": 2663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "viii\n\nContents\n\nChapter 3. Goals of Test Automation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Why Test? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Economics of Test Automation . . . . . . . . . . . . . . . . . . . . . . . . . 20 Goals of Test Automation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Tests Should Help Us Improve Quality . . . . . . . . . . . . . . . . . . . 22 Tests Should Help Us Understand the SUT . . . . . . . . . . . . . . . . . 23 Tests Should Reduce (and Not Introduce) Risk . . . . . . . . . . . . . 23 Tests Should Be Easy to Run . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Tests Should Be Easy to Write and Maintain . . . . . . . . . . . . . . . 27 Tests Should Require Minimal Maintenance as the System Evolves Around Them . . . . . . . . . . . . . . . . . . . . . . . 29 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n\nChapter 4. Philosophy of Test Automation . . . . . . . . . . . . . . . . . . . . . . . . . 31\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Why Is Philosophy Important? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Some Philosophical Differences . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 Test First or Last? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 Tests or Examples? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 Test-by-Test or Test All-at-Once? . . . . . . . . . . . . . . . . . . . . . . . . 33 Outside-In or Inside-Out? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 State or Behavior Veriﬁ cation? . . . . . . . . . . . . . . . . . . . . . . . . . . 36 Fixture Design Upfront or Test-by-Test? . . . . . . . . . . . . . . . . . . 36 When Philosophies Differ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 My Philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\nChapter 5. Principles of Test Automation . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 The Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n\nChapter 6. Test Automation Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 What’s Strategic? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 Which Kinds of Tests Should We Automate? . . . . . . . . . . . . . . . . . 50 Per-Functionality Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 Cross-Functional Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n\nwww.it-ebooks.info",
      "content_length": 3227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Contents\n\nWhich Tools Do We Use to Automate Which Tests? . . . . . . . . . . . . 53 Test Automation Ways and Means . . . . . . . . . . . . . . . . . . . . . . . 54 Introducing xUnit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 The xUnit Sweet Spot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 Which Test Fixture Strategy Do We Use? . . . . . . . . . . . . . . . . . . . . 58 What Is a Fixture? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 Major Fixture Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 Transient Fresh Fixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 Persistent Fresh Fixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 Shared Fixture Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 How Do We Ensure Testability? . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 Test Last—at Your Peril . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 Design for Testability—Upfront . . . . . . . . . . . . . . . . . . . . . . . . . 65 Test-Driven Testability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 Control Points and Observation Points . . . . . . . . . . . . . . . . . . . 66 Interaction Styles and Testability Patterns . . . . . . . . . . . . . . . . . 67 Divide and Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n\nChapter 7. xUnit Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 An Introduction to xUnit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 Common Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 The Bare Minimum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 Deﬁ ning Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 What’s a Fixture? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 Deﬁ ning Suites of Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 Running Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 Test Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 Under the xUnit Covers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 Test Commands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 Test Suite Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 xUnit in the Procedural World . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n\nwww.it-ebooks.info\n\nix",
      "content_length": 3006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "x\n\nContents\n\nChapter 8. Transient Fixture Management . . . . . . . . . . . . . . . . . . . . . . . . . 85\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 Test Fixture Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 What Is a Fixture? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 What Is a Fresh Fixture? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 What Is a Transient Fresh Fixture? . . . . . . . . . . . . . . . . . . . . . . . 87 Building Fresh Fixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 In-line Fixture Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 Delegated Fixture Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 Implicit Fixture Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 Hybrid Fixture Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 Tearing Down Transient Fresh Fixtures . . . . . . . . . . . . . . . . . . . . . 93 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n\nChapter 9. Persistent Fixture Management . . . . . . . . . . . . . . . . . . . . . . . . . .95\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 Managing Persistent Fresh Fixtures . . . . . . . . . . . . . . . . . . . . . . . . 95 What Makes Fixtures Persistent? . . . . . . . . . . . . . . . . . . . . . . . . 95 Issues Caused by Persistent Fresh Fixtures . . . . . . . . . . . . . . . . . 96 Tearing Down Persistent Fresh Fixtures . . . . . . . . . . . . . . . . . . . 97 Avoiding the Need for Teardown . . . . . . . . . . . . . . . . . . . . . . . 100 Dealing with Slow Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 Managing Shared Fixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 Accessing Shared Fixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 Triggering Shared Fixture Construction . . . . . . . . . . . . . . . . . . 104 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n\nChapter 10. Result Veriﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 Making Tests Self-Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 Verify State or Behavior? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 State Veriﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 Using Built-in Assertions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 Delta Assertions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 External Result Veriﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 Verifying Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 Procedural Behavior Veriﬁ cation . . . . . . . . . . . . . . . . . . . . . . . 113 Expected Behavior Speciﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . 113\n\nwww.it-ebooks.info",
      "content_length": 3253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Contents\n\nReducing Test Code Duplication . . . . . . . . . . . . . . . . . . . . . . . . . 114 Expected Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 Custom Assertions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 Outcome-Describing Veriﬁ cation Method . . . . . . . . . . . . . . . . 117 Parameterized and Data-Driven Tests . . . . . . . . . . . . . . . . . . . 118 Avoiding Conditional Test Logic . . . . . . . . . . . . . . . . . . . . . . . . . 119 Eliminating “if” Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 Eliminating Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 Other Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 Working Backward, Outside-In . . . . . . . . . . . . . . . . . . . . . . . . 121 Using Test-Driven Development to Write Test Utility Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 Where to Put Reusable Veriﬁ cation Logic? . . . . . . . . . . . . . . . . . . 122 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n\nChapter 11. Using Test Doubles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 What Are Indirect Inputs and Outputs? . . . . . . . . . . . . . . . . . . . . 125 Why Do We Care about Indirect Inputs? . . . . . . . . . . . . . . . . . 126 Why Do We Care about Indirect Outputs? . . . . . . . . . . . . . . . 126 How Do We Control Indirect Inputs? . . . . . . . . . . . . . . . . . . . 128 How Do We Verify Indirect Outputs? . . . . . . . . . . . . . . . . . . . 130 Testing with Doubles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 Types of Test Doubles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 Providing the Test Double . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 Conﬁ guring the Test Double . . . . . . . . . . . . . . . . . . . . . . . . . . 141 Installing the Test Double . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 Other Uses of Test Doubles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 Endoscopic Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 Need-Driven Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 Speeding Up Fixture Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 Speeding Up Test Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 Other Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n\nChapter 12. Organizing Our Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 Basic xUnit Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n\nwww.it-ebooks.info\n\nxi",
      "content_length": 3110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "xii\n\nContents\n\nRight-Sizing Test Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 Test Methods and Testcase Classes . . . . . . . . . . . . . . . . . . . . . . . . 155 Testcase Class per Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 Testcase Class per Feature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 Testcase Class per Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 Choosing a Test Method Organization Strategy . . . . . . . . . . . . 158 Test Naming Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 Organizing Test Suites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 Running Groups of Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 Running a Single Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 Test Code Reuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 Test Utility Method Locations . . . . . . . . . . . . . . . . . . . . . . . . . 163 TestCase Inheritance and Reuse . . . . . . . . . . . . . . . . . . . . . . . . 163 Test File Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 Built-in Self-Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 Test Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 Test Dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n\nChapter 13. Testing with Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 Testing with Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 Why Test with Databases? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 Issues with Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 Testing without Databases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 Testing the Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171 Testing Stored Procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 Testing the Data Access Layer . . . . . . . . . . . . . . . . . . . . . . . . . . 172 Ensuring Developer Independence. . . . . . . . . . . . . . . . . . . . . . . 173 Testing with Databases (Again!) . . . . . . . . . . . . . . . . . . . . . . . . . . 173 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\n\nChapter 14. A Roadmap to Effective Test Automation . . . . . . . . . . . . . . . 175\n\nAbout This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175 Test Automation Difﬁ culty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175 Roadmap to Highly Maintainable Automated Tests . . . . . . . . . . . 176 Exercise the Happy Path Code . . . . . . . . . . . . . . . . . . . . . . . . . 177 Verify Direct Outputs of the Happy Path . . . . . . . . . . . . . . . . . 178\n\nwww.it-ebooks.info",
      "content_length": 3226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Contents\n\nVerify Alternative Paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178 Verify Indirect Output Behavior . . . . . . . . . . . . . . . . . . . . . . . . 179 Optimize Test Execution and Maintenance . . . . . . . . . . . . . . . 180 What’s Next? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n\nPART II. The Test Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n\nChapter 15. Code Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n\nObscure Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 Conditional Test Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200 Hard-to-Test Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 Test Code Duplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 Test Logic in Production . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n\nChapter 16. Behavior Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n\nAssertion Roulette . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 Erratic Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 Fragile Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239 Frequent Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248 Manual Intervention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250 Slow Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\n\nChapter 17. Project Smells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\n\nBuggy Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260 Developers Not Writing Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 High Test Maintenance Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 Production Bugs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\n\nPART III. The Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\n\nChapter 18. Test Strategy Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n\nRecorded Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278 Scripted Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 Data-Driven Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288 Test Automation Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298 Minimal Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302 Standard Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305 Fresh Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311\n\nwww.it-ebooks.info\n\nxiii",
      "content_length": 3115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "xiv\n\nContents\n\nShared Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 Back Door Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327 Layer Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\n\nChapter 19. xUnit Basics Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347\n\nTest Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348 Four-Phase Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358 Assertion Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362 Assertion Message . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370 Testcase Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373 Test Runner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377 Testcase Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382 Test Suite Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387 Test Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393 Test Enumeration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399 Test Selection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403\n\nChapter 20. Fixture Setup Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407\n\nIn-line Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408 Delegated Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411 Creation Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 Implicit Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424 Prebuilt Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429 Lazy Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435 Suite Fixture Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441 Setup Decorator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447 Chained Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454\n\nChapter 21. Result Veriﬁ cation Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . 461\n\nState Veriﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462 Behavior Veriﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468 Custom Assertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474 Delta Assertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485 Guard Assertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490 Unﬁ nished Test Assertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494\n\nChapter 22. Fixture Teardown Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . 499\n\nGarbage-Collected Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500\n\nwww.it-ebooks.info",
      "content_length": 3343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Contents\n\nAutomated Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503 In-line Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509 Implicit Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516\n\nChapter 23. Test Double Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521\n\nTest Double . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522 Test Stub . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529 Test Spy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538 Mock Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544 Fake Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551 Conﬁ gurable Test Double . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558 Hard-Coded Test Double . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568 Test-Speciﬁ c Subclass. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n\nChapter 24. Test Organization Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . 591\n\nNamed Test Suite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592 Test Utility Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599 Parameterized Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607 Testcase Class per Class. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617 Testcase Class per Feature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624 Testcase Class per Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631 Testcase Superclass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638 Test Helper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643\n\nChapter 25. Database Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 649\n\nDatabase Sandbox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650 Stored Procedure Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654 Table Truncation Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661 Transaction Rollback Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . 668\n\nChapter 26. Design-for-Testability Patterns . . . . . . . . . . . . . . . . . . . . . . . . 677\n\nDependency Injection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 678 Dependency Lookup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 686 Humble Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 695 Test Hook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 709\n\nChapter 27. Value Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713\n\nLiteral Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 714\n\nwww.it-ebooks.info\n\nxv",
      "content_length": 3205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "xvi\n\nVisual Summary of the Pattern Language\n\nDerived Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718 Generated Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723 Dummy Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 728\n\nPART IV. Appendixes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 733\n\nAppendix A. Test Refactorings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 735\n\nAppendix B. xUnit Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 741\n\nAppendix C. xUnit Family Members . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 747\n\nAppendix D. Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753\n\nAppendix E. Goals and Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757\n\nAppendix F. Smells, Aliases, and Causes . . . . . . . . . . . . . . . . . . . . . . . . . . 761\n\nAppendix G. Patterns, Aliases, and Variations . . . . . . . . . . . . . . . . . . . . . . 767\n\nGlossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 785\n\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 819\n\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 835\n\nwww.it-ebooks.info",
      "content_length": 1462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Visual Summary of the Pattern Language\n\nGoals, Principles, and Smells\n\nGoals of Test Automation Goals of Test Automation\n\nCode Smells Code Smells\n\nProject Goals Project Goals Tests as Specification Tests as Specification\n\nTests as Documentation Tests as Documentation\n\nTests as Safety Net Tests as Safety Net\n\nDefect Localization Defect Localization\n\nTest Writing Goals Test Writing Goals Fully Automated Fully Automated\n\nSelf-Checking Self-Checking\n\nRepeatable Test Repeatable Test\n\nRobust Test Robust Test\n\nConditional Test Logic Conditional Test Logic\n\nHard-to-Test Code Hard-to-Test Code\n\nTest Code Duplication Test Code Duplication\n\nTest Logic In Production Test Logic In Production\n\nObscure Test Obscure Test Eager Test Eager Test General Fixture General Fixture Indirect Testing Indirect Testing Mystery Guest Mystery Guest And more! And more!\n\nEasy to Write/Maintain Easy to Write/Maintain\n\nSimple Test Simple Test\n\nImprove Quality Improve Quality\n\nExpressive Tests Expressive Tests\n\nBehavior Smells Behavior Smells\n\nReduce Risk Reduce Risk\n\nSeparation of Concerns Separation of Concerns\n\nBug Repellent Bug Repellent\n\nDo No Harm Do No Harm\n\nPrinciples of Test Automation Principles of Test Automation\n\nWrite the Tests First Write the Tests First\n\nUse the Front Door First Use the Front Door First\n\nErratic Test Erratic Test Unrepeatable Test Unrepeatable Test Interacting Tests Interacting Tests Test Run War Test Run War Resource Optimism Resource Optimism And more! And more!\n\nAssertion Roulette Assertion Roulette Eager Test Eager Test\n\nFrequent Debugging Frequent Debugging\n\nManual Intervention Manual Intervention\n\nIsolate the SUT Isolate the SUT\n\nVerify One Condition per Test Verify One Condition per Test\n\nSlow Tests Slow Tests\n\nFragile Test Fragile Test Fragile Fixture Fragile Fixture\n\nDon’t Modify the SUT Don’t Modify the SUT\n\nTest Concerns Separately Test Concerns Separately\n\nMinimize Test Overlap Minimize Test Overlap\n\nKeep Tests Independent Keep Tests Independent\n\nProject Smells Project Smells\n\nCommunicate Intent Communicate Intent\n\nMinimize Untestable Code Minimize Untestable Code\n\nBuggy Tests Buggy Tests\n\nProduction Bugs Production Bugs\n\nKeep Test Logic Out of Production Keep Test Logic Out of Production\n\nDevelopers Not Writing Tests Developers Not Writing Tests\n\nEnsure Commensurate Effort and Responsibility Ensure Commensurate Effort and Responsibility\n\nHigh Test Maintenance Cost High Test Maintenance Cost\n\nKey to Visual Summary of the Pattern Language\n\nChapter Name Chapter Name\n\nChapter Name Chapter Name\n\nSub-Category Sub-Category\n\nPattern 1 Pattern 1\n\nPattern 2 Pattern 2\n\nAlternative Pattern Alternative Pattern from Other Chapter from Other Chapter\n\nSmell Smell Cause of Smell Cause of Smell\n\nPattern Pattern Variation of Pattern Variation of Pattern\n\nVariation described Variation described separately separately\n\nvariation of variation of\n\nused with used with each other each other\n\nPattern Pattern\n\nleads to leads to\n\nSub-Category Sub-Category Alternative Alternative Pattern 1 Pattern 1\n\nAlternative Alternative Pattern 2 Pattern 2\n\nSmell Smell\n\nSmell Smell\n\nxvii\n\nwww.it-ebooks.info",
      "content_length": 3131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "xviii\n\nVisual Summary of the Pattern Language\n\nThe Patterns\n\nTest Automation Strategy Patterns Test Automation Strategy Patterns\n\nxUnit Basics Patterns xUnit Basics Patterns\n\nRecorded Recorded Test Test\n\nTest Automation Strategy Test Automation Strategy Scripted Scripted Test Test\n\nData-Driven Data-Driven Test Test\n\nTest Test Automation Automation Framework Framework\n\nTest Execution Test Execution\n\nTest Test Runner Runner\n\nTest Discovery Test Discovery\n\nTest Enumeration Test Enumeration\n\nTest Automation Framework Test Automation Framework\n\nTest Selection Test Selection\n\nTest Case Object Test Case Object\n\nTest Suite Object Test Suite Object\n\nSUT Interaction SUT Interaction Strategy Strategy\n\nLayer Test Layer Test\n\nTest Fixture Strategy Test Fixture Strategy\n\nMinimal Fixture Minimal Fixture\n\nStandard Fixture Standard Fixture\n\nTest Definition Test Definition\n\nTest Method Test Method\n\nAssertion Assertion Method Method\n\nBack Door Back Door Manipulation Manipulation\n\nFresh Fixture Fresh Fixture Persistent Persistent Transient Transient\n\nShared Fixture Shared Fixture Immutable Fixture Immutable Fixture\n\nTestcase Testcase Class Class\n\nFour Phase Four Phase Test Test\n\nAssertion Assertion Message Message\n\nFixture Setup Patterns Fixture Setup Patterns\n\nFresh Fixture Setup Fresh Fixture Setup\n\nShared Fixture Construction Shared Fixture Construction\n\nInline Inline Setup Setup\n\nDelegated Delegated Setup Setup\n\nImplicit Implicit Setup Setup\n\nPrebuilt Prebuilt Fixture Fixture\n\nLazy Lazy Setup Setup\n\nSuiteFixture SuiteFixture Setup Setup\n\nSetup Setup Decorator Decorator\n\nChained Chained Tests Tests\n\nTest Helper Test Helper Object Mother Object Mother\n\nCreation Creation Method Method\n\nShared Fixture Access Shared Fixture Access Test Utility Method Test Utility Method Finder Method Finder Method\n\nResult Verification Result Verification\n\nDelta Assertion Delta Assertion\n\nResult Verification Patterns Result Verification Patterns\n\nFixture TearDown Patterns Fixture TearDown Patterns\n\nVerificationStrategy VerificationStrategy\n\nState State Verification Verification\n\nBehavior Behavior Verification Verification\n\nBack Door Back Door Verification Verification\n\nAssertion Method Assertion Method\n\nApplicability Applicability\n\nPersistent Fresh Fixture Persistent Fresh Fixture\n\nShared Fixture Shared Fixture\n\nCode Organization Code Organization\n\nInline Teardown Inline Teardown\n\nImplicit Teardown Implicit Teardown\n\nAssertion Method Styles Assertion Method Styles\n\nStrategy Strategy\n\nGuard Guard Assertion Assertion\n\nCustom Assertion Custom Assertion Verification Method Verification Method\n\nTest Organization Patterns Test Organization Patterns\n\nDelta Delta Assertion Assertion\n\nGarbage- Garbage- Collected Collected Teardown Teardown\n\nTable Table Truncation Truncation Teardown Teardown\n\nTransaction Transaction Rollback Rollback Teardown Teardown\n\nAutomated Automated Teardown Teardown\n\nTestcase Class Structure Testcase Class Structure\n\nNamed Named Test Test Suite Suite\n\nTestcase Class per Class Testcase Class per Class\n\nTestcase Class per Fixture Testcase Class per Fixture\n\nTestcase Class per Feature Testcase Class per Feature\n\nDummy Dummy Object Object\n\nTest Double Patterns Test Double Patterns\n\nFake Fake Object Object\n\nTest Test Stub Stub\n\nTest Test Spy Spy\n\nMock Mock Object Object\n\nTest Code Reuse Test Code Reuse\n\nTest Utility Method Test Utility Method Finder Method Finder Method\n\nCreation Method Creation Method\n\nCustom Assertion Custom Assertion Verification Method Verification Method\n\nUtility Method Location Utility Method Location\n\nTestcase Testcase Class Class\n\nTestcase Testcase Superclass Superclass\n\nTest-Specific Test-Specific Subclass Subclass Subclassed Test Subclassed Test Double Double\n\nTest Test Double Double\n\nTest Double Test Double Construction Construction Confiugrable Confiugrable Test Double Test Double\n\nHard -Coded Hard -Coded Test Double Test Double\n\nParameterized Test Parameterized Test\n\nTest Helper Test Helper Object Mother Object Mother\n\nDesign- for-Testability Patterns Design- for-Testability Patterns\n\nLiteral Literal Value Value\n\nValue Patterns Value Patterns\n\nGenerated Generated Value Value\n\nDerived Derived Value Value\n\nDummy Dummy Object Object\n\nDependency Dependency Injection Injection Setter Injection Setter Injection Parameter Injection Parameter Injection Constructor Injection Constructor Injection\n\nDependency Dependency Lookup Lookup\n\nObject Factory Object Factory Service Locator Service Locator\n\nDatabase Patterns Database Patterns\n\nFake Fake Database Database\n\nTable Table Truncation Truncation Teardown Teardown Lazy Teardown Lazy Teardown\n\nDatabase Database Sandbox Sandbox\n\nTransaction Transaction Rollback Rollback Teardown Teardown\n\nDelta Delta Assertion Assertion\n\nStored Stored Procedure Procedure Test Test\n\nHumble Object Humble Object Humble Container Adapter Humble Container Adapter Humble Transaction Controller Humble Transaction Controller Humble Executable Humble Executable Humble Dialog Humble Dialog\n\nTest Hook Test Hook\n\nTest-Specific Test-Specific Subclass Subclass Substituted Singleton Substituted Singleton\n\nwww.it-ebooks.info",
      "content_length": 5125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Foreword\n\nIf you go to junit.org, you’ll see a quote from me: “Never in the ﬁ eld of software development have so many owed so much to so few lines of code.” JUnit has been criticized as a minor thing, something any reasonable programmer could produce in a weekend. This is true, but utterly misses the point. The reason JUnit is important, and deserves the Churchillian knock-off, is that the presence of this tiny tool has been essential to a fundamental shift for many programmers: Testing has moved to a front and central part of programming. People have advocated it before, but JUnit made it happen more than anything else.\n\nIt’s more than just JUnit, of course. Ports of JUnit have been written for lots of programming languages. This loose family of tools, often referred to as xUnit tools, has spread far beyond its java roots. (And of course the roots weren’t really in Java—Kent Beck wrote this code for Smalltalk years before.)\n\nxUnit tools, and more importantly their philosophy, offer up huge opportu- nities to programming teams—the opportunity to write powerful regression test suites that enable teams to make drastic changes to a code-base with far less risk; the opportunity to re-think the design process with Test Driven Development.\n\nBut with these opportunities come new problems and new techniques. Like any tool, the xUnit family can be used well or badly. Thoughtful people have ﬁ gured out various ways to use xUnit, to organize the tests and data effectively. Like the early days of objects, much of the knowledge to really use the tools is hidden in the heads of its skilled users. Without this hidden knowledge you really can’t reap the full beneﬁ ts.\n\nIt was nearly twenty years ago when people in the object-oriented commu- nity realized this problem for objects and began to formulate an answer. The answer was to describe their hidden knowledge in the form of patterns. Gerard Meszaros was one of the pioneers in doing this. When I ﬁ rst started exploring patterns, Gerard was one of the leaders that I learned from. Like many in the patterns world, Gerard also was an early adopter of eXtreme Programming, and thus worked with xUnit tools from the earliest days. So it’s entirely logical that he should have taken on the task of capturing that expert knowledge in the form of patterns.\n\nI’ve been excited by this project since I ﬁ rst heard about it. (I had to launch a commando raid to steal this book from Bob Martin because I wanted it to\n\nxix\n\nwww.it-ebooks.info",
      "content_length": 2501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "xx\n\nForeword\n\ngrace my series instead.) Like any good patterns book it provides knowledge to new people in the ﬁ eld, and just as important, provides the vocabulary and foundations for experienced practitioners to pass their knowledge on to their colleagues. For many people, the famous Gang of Four book Design Patterns unlocked the hidden gems of object-oriented design. This book does the same for xUnit.\n\nMartin Fowler Series Editor Chief Scientist, ThoughtWorks\n\nwww.it-ebooks.info",
      "content_length": 486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Preface\n\nThe Value of Self-Testing Code\n\nIn Chapter 4 of Refactoring [Ref], Martin Fowler writes:\n\nIf you look at how most programmers spend their time, you’ll ﬁ nd that writing code is actually a small fraction. Some time is spent ﬁ guring out what ought to be going on, some time is spent designing, but most time is spent debugging. I’m sure every reader can remember long hours of debugging, often long into the night. Every programmer can tell a story of a bug that took a whole day (or more) to ﬁ nd. Fixing the bug is usually pretty quick, but ﬁ nding it is a nightmare. And then when you do ﬁ x a bug, there’s always a chance that anther one will appear and that you might not even notice it until much later. Then you spend ages ﬁ nding that bug.\n\nSome software is very difﬁ cult to test manually. In these cases, we are often forced into writing test programs.\n\nI recall a project I was working on in 1996. My task was to build an event framework that would let client software register for an event and be notiﬁ ed when some other software raised that event (the Observer [GOF] pattern). I could not think of a way to test this framework without writing some sample client software. I had about 20 different scenarios I needed to test, so I coded up each scenario with the requisite number of observers, events, and event raisers. At ﬁ rst, I logged what was occurring in the console and scanned it manually. This scanning became very tedious very quickly.\n\nBeing quite lazy, I naturally looked for an easier way to perform this test- ing. For each test I populated a Dictionary indexed by the expected event and the expected receiver of it with the name of the receiver as the value. When a particular receiver was notiﬁ ed of the event, it looked in the Dictionary for the entry indexed by itself and the event it had just received. If this entry existed, the receiver removed the entry. If it didn’t, the receiver added the entry with an error message saying it was an unexpected event notiﬁ cation.\n\nAfter running all the tests, the test program merely looked in the Dictionary and printed out its contents if it was not empty. As a result, running all of my tests had a nearly zero cost. The tests either passed quietly or spewed a list of test failures. I had unwittingly discovered the concept of a Mock Object (page 544) and a Test Automation Framework (page 298) out of necessity!\n\nxxi\n\nwww.it-ebooks.info",
      "content_length": 2425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "xxii\n\nPreface\n\nMy First XP Project\n\nIn late 1999, I attended the OOPSLA conference, where I picked up a copy of Kent Beck’s new book, eXtreme Programming Explained [XPE]. I was used to doing iterative and incremental development and already believed in the value of automated unit testing, although I had not tried to apply it universally. I had a lot of respect for Kent, whom I had known since the ﬁ rst PLoP1 conference in 1994. For all these reasons, I decided that it was worth trying to apply eXtreme Programming on a ClearStream Consulting project. Shortly after OOPSLA, I was fortunate to come across a suitable project for trying out this develop- ment approach—namely, an add-on application that interacted with an existing database but had no user interface. The client was open to developing software in a different way.\n\nWe started doing eXtreme Programming “by the book” using pretty much all of the practices it recommended, including pair programming, collective owner- ship, and test-driven development. Of course, we encountered a few challenges in ﬁ guring out how to test some aspects of the behavior of the application, but we still managed to write tests for most of the code. Then, as the project pro- gressed, I started to notice a disturbing trend: It was taking longer and longer to implement seemingly similar tasks.\n\nI explained the problem to the developers and asked them to record on each task card how much time had been spent writing new tests, modifying existing tests, and writing the production code. Very quickly, a trend emerged. While the time spent writing new tests and writing the production code seemed to be staying more or less constant, the amount of time spent modifying existing tests was increasing and the developers’ estimates were going up as a result. When a developer asked me to pair on a task and we spent 90% of the time modify- ing existing tests to accommodate a relatively minor change, I knew we had to change something, and soon!\n\nWhen we analyzed the kinds of compile errors and test failures we were experiencing as we introduced the new functionality, we discovered that many of the tests were affected by changes to methods of the system under test (SUT). This came as no surprise, of course. What was surprising was that most of the impact was felt during the ﬁ xture setup part of the test and that the changes were not affecting the core logic of the tests.\n\nThis revelation was an important discovery because it showed us that we had the knowledge about how to create the objects of the SUT scattered across most of the tests. In other words, the tests knew too much about nonessential\n\n1 The Pattern Languages of Programs conference.\n\nwww.it-ebooks.info",
      "content_length": 2724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Preface\n\nparts of the behavior of the SUT. I say “nonessential” because most of the af- fected tests did not care about how the objects in the ﬁ xture were created; they were interested in ensuring that those objects were in the correct state. Upon further examination, we found that many of the tests were creating identical or nearly identical objects in their test ﬁ xtures.\n\nThe obvious solution to this problem was to factor out this logic into a small\n\nset of Test Utility Methods (page 599). There were several variations:\n\nWhen we had a bunch of tests that needed identical objects, we simply created a method that returned that kind of object ready to use. We now call these Creation Methods (page 415).\n\nSome tests needed to specify different values for some attribute of the object. In these cases, we passed that attribute as a parameter to the Parameterized Creation Method (see Creation Method).\n\nSome tests wanted to create a malformed object to ensure that the SUT would reject it. Writing a separate Parameterized Creation Method for each attribute cluttered the signature of our Test Helper (page 643), so we created a valid object and then replaced the value of the One Bad Attribute (see Derived Value on page 718).\n\nWe had discovered what would become2 our ﬁ rst test automation patterns.\n\nLater, when tests started failing because the database did not like the fact that we were trying to insert another object with the same key that had a unique constraint, we added code to generate the unique key programmatically. We called this variant an Anonymous Creation Method (see Creation Method) to indicate the presence of this added behavior.\n\nIdentifying the problem that we now call a Fragile Test (page 239) was an im- portant event on this project, and the subsequent deﬁ nition of its solution pat- terns saved this project from possible failure. Without this discovery we would, at best, have abandoned the automated unit tests that we had already built. At worst, the tests would have reduced our productivity so much that we would have been unable to deliver on our commitments to the client. As it turned out, we were able to deliver what we had promised and with very good quality. Yes, the testers3 still found bugs in our code because we were deﬁ nitely missing some tests. Introducing the changes needed to ﬁ x those bugs, once we had ﬁ gured\n\n2 Technically, they are not truly patterns until they have been discovered by three inde- pendent project teams. 3 The testing function is sometimes referred to as “Quality Assurance.” This usage is, strictly speaking, incorrect.\n\nwww.it-ebooks.info\n\nxxiii",
      "content_length": 2633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "xxiv\n\nPreface\n\nout what the missing tests needed to look like, was a relatively straightforward process, however.\n\nWe were hooked. Automated unit testing and test-driven development really\n\ndid work, and we have been using them consistently ever since.\n\nAs we applied the practices and patterns on subsequent projects, we have run into new problems and challenges. In each case, we have “peeled the on- ion” to ﬁ nd the root cause and come up with ways to address it. As these tech- niques have matured, we have added them to our repertoire of techniques for automated unit testing.\n\nWe ﬁ rst described some of these patterns in a paper presented at XP2001. In discussions with other participants at that and subsequent conferences, we discovered that many of our peers were using the same or similar techniques. That elevated our methods from “practice” to “pattern” (a recurring solution to a recurring problem in a context). The ﬁ rst paper on test smells [RTC] was presented at the same conference, building on the concept of code smells ﬁ rst described in [Ref].\n\nMy Motivation\n\nI am a great believer in the value of automated unit testing. I practiced software development without it for the better part of two decades, and I know that my professional life is much better with it than without it. I believe that the xUnit framework and the automated tests it enables are among the truly great ad- vances in software development. I ﬁ nd it very frustrating when I see companies trying to adopt automated unit testing but being unsuccessful because of a lack of key information and skills.\n\nAs a software development consultant with ClearStream Consulting, I see a lot of projects. Sometimes I am called in early on a project to help clients make sure they “do things right.” More often than not, however, I am called in when things are already off the rails. As a result, I see a lot of “worst practices” that result in test smells. If I am lucky and I am called early enough, I can help the client recover from the mistakes. If not, the client will likely muddle through less than satisﬁ ed with how TDD and automated unit testing worked—and the word goes out that automated unit testing is a waste of time.\n\nIn hindsight, most of these mistakes and best practices are easily avoid- able given the right knowledge at the right time. But how do you obtain that knowledge without making the mistakes for yourself? At the risk of sounding self-serving, hiring someone who has the knowledge is the most time-efﬁ cient way of learning any new practice or technology. According to Gerry Weinberg’s\n\nwww.it-ebooks.info",
      "content_length": 2617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Preface\n\n“Law of Raspberry Jam” [SoC],4 taking a course or reading a book is a much less effective (though less expensive) alternative. I hope that by writing down a lot of these mistakes and suggesting ways to avoid them, I can save you a lot of grief on your project, whether it is fully agile or just more agile than it has been in the past—the “Law of Raspberry Jam” not withstanding.\n\nWho This Book Is For\n\nI have written this book primarily for software developers (programmers, designers, and architects) who want to write better tests and for the managers and coaches who need to understand what the developers are doing and why the developers need to be cut enough slack so they can learn to do it even bet- ter! The focus here is on developer tests and customer tests that are automated using xUnit. In addition, some of the higher-level patterns apply to tests that are automated using technologies other than xUnit. Rick Mugridge and Ward Cun- ningham have written an excellent book on Fit [FitB], and they advocate many of the same practices.\n\nDevelopers will likely want to read the book from cover to cover, but they should focus on skimming the reference chapters rather than trying to read them word for word. The emphasis should be on getting an overall idea of which pat- terns exist and how they work. Developers can then return to a particular pat- tern when the need for it arises. The ﬁ rst few elements (up to and include the “When to Use It” section) of each pattern should provide this overview.\n\nManagers and coaches might prefer to focus on reading Part I, The Nar- ratives, and perhaps Part II, The Test Smells. They might also need to read Chapter 18, Test Strategy Patterns, as these are decisions they need to under- stand and provide support to the developers as they work their way through these patterns. At a minimum, managers should read Chapter 3, Goals of Test Automation.\n\nAbout the Cover Photo\n\nEvery book in the Martin Fowler Signature Series features a picture of a bridge on the cover. One of the thoughts I had when Martin Fowler asked if he could “steal me for his series” was “Which bridge should I put on the cover?” I thought about the ability of testing to avoid catastrophic failures of software\n\n4 The Law of Raspberry Jam: “The wider you spread it, the thinner it gets.”\n\nwww.it-ebooks.info\n\nxxv",
      "content_length": 2348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "xxvi\n\nPreface\n\nand how that related to bridges. Several famous bridge failures immediately came to mind, including “Galloping Gertie” (the Tacoma Narrows bridge) and the Iron Workers Memorial Bridge in Vancouver (named for the iron workers who died when a part of it collapsed during construction).\n\nAfter further reﬂ ection, it just did not seem right to claim that testing might have prevented these failures, so I chose a bridge with a more personal con- nection. The picture on the cover shows the New River Gorge bridge in West Virginia. I ﬁ rst passed over and subsequently paddled under this bridge on a whitewater kayaking trip in the late 1980s. The style of the bridge is also rel- evant to this book’s content: The complex arch structure underneath the bridge is largely hidden from those who use it to get to the other side of the gorge. The road deck is completely level and four lanes wide, resulting in a very smooth passage. In fact, at night it is quite possible to remain completely oblivious to the fact that one is thousands of feet above the valley ﬂ oor. A good test automa- tion infrastructure has the same effect: Writing tests is easy because most of the complexity lies hidden beneath the road bed.\n\nColophon\n\nThis book’s manuscript was written using XML, which I published to HTML for previewing on my Web site. I edited the XML using Eclipse and the XML Buddy plug-in. The HTML was generated using a Ruby program that I ﬁ rst obtained from Martin Fowler and which I then evolved quite extensively as I evolved my custom markup language. Code samples were written, compiled, and executed in (mostly) Eclipse and were inserted into the HTML automati- cally by XML tag handlers (one of the main reasons for using Ruby instead of XSLT). This gave me the ability to “publish early, publish often” to the Web site. I could also generate a single Word or PDF document for reviewers from the source, although this required some manual steps.\n\nwww.it-ebooks.info",
      "content_length": 1981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Acknowledgments\n\nWhile this book is largely a solo writing effort, many people have contributed to it in their own ways. Apologies in advance to anyone whom I may have missed.\n\nPeople who know me well may wonder how I found enough time to write a book like this. When I am not working, I am usually off doing various (some would say “extreme”) outdoor sports, such as back-country (extreme) skiing, whitewater (extreme) kayaking, and mountain (extreme) biking. Personally, I do not agree with this application of the “extreme” adjective to my activities any more than I agree with its use for highly iterative and incremental (extreme) programming. Nevertheless, the question of where I found the time to write this book is a valid one. I must give special thanks to my friend Heather Armitage, with whom I engage in most of the above activities. She has driven many long hours on the way to or from these adventures with me hunched over my laptop computer in the passenger seat working on this book. Also, thanks go to Alf Skrastins, who loves to drive all his friends to back-country skiing venues west of Calgary in his Previa. Also, thanks to the operators of the various back-country ski lodges who let me recharge my laptop from their generators so I could work on the book while on vacation—Grania Devine at Selkirk Lodge, Tannis Dakin at Sorcerer Lodge, and Dave Flear and Aaron Cooperman at Sol Mountain Touring. Without their help, this book would have taken much longer to write!\n\nAs usual, I’d like to thank all my reviewers, both ofﬁ cial and unofﬁ cial. Rob- ert C. (“Uncle Bob”) Martin reviewed an early draft. The ofﬁ cial reviewers of the ﬁ rst “ofﬁ cial” draft were Lisa Crispin and Rick Mugridge. Lisa Crispin, Jer- emy Miller, Alistair Duguid, Michael Hedgpeth, and Andrew Stopford reviewed the second draft.\n\nThanks to my “shepherds” from the various PLoP conferences who provided feedback on drafts of these patterns—Michael Stahl, Danny Dig, and especially Joe Yoder; they provided expert comments on my experiments with the pattern form. I would also like to thank the members of the PLoP workshop group on Pattern Languages at PLoP 2004 and especially Eugene Wallingford, Ralph Johnson, and Joseph Bergin. Brian Foote and the SAG group at UIUC posted several gigabytes of MP3’s of the review sessions in which they discussed the early drafts of the book. Their comments caused me to rewrite from scratch at least one of the narrative chapters.\n\nMany people e-mailed me comments about the material posted on my Web site at http://xunitpatterns.com or posted comments on the Yahoo! group; they\n\nxxvii\n\nwww.it-ebooks.info",
      "content_length": 2643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "xxviii\n\nAcknowledgments\n\nprovided very timely feedback on the sometimes very draft-like material I had posted there. These folks included Javid Jamae, Philip Nelson, Tomasz Gajewski, John Hurst, Sven Gorts, Bradley T. Landis, Cédric Beust, Joseph Pelrine, Sebas- tian Bergmann, Kevin Rutherford, Scott W. Ambler, J. B. Rainsberger, Oli Bye, Dale Emery, David Nunn, Alex Chaffee, Burkhardt Hufnagel, Johannes Brod- wall, Bret Pettichord, Clint Shank, Sunil Joglekar, Rachel Davies, Nat Pryce, Paul Hodgetts, Owen Rogers, Amir Kolsky, Kevin Lawrence, Alistair Cockburn, Michael Feathers, and Joe Schmetzer. Special thanks go to Neal Norwitz, Markus Gaelli, Stephane Ducasse, and Stefan Reichhart, who provided copious feedback as unofﬁ cial reviewers.\n\nQuite a few people sent me e-mails describing their favorite pattern or special feature from their member of the xUnit family. Most of these were variations on patterns I had already documented; I’ve included them in this book as aliases or implementation variations as appropriate. A few were more esoteric patterns that I had to leave out for space reasons—for that, I apologize.\n\nMany of the ideas described in this book came from projects I worked on with my colleagues from ClearStream Consulting. We all pushed one another to ﬁ nd better ways of doing things back in the early days of eXtreme Program- ming when few—if any—resources were available. It was this single-minded determination that led to many of the more useful techniques described here. Those colleagues are Jennitta Andrea, Ralph Bohnet, Dave Braat, Russel Bryant, Greg Cook, Geoff Hardy, Shaun Smith, and Thomas (T2) Tannahill. Many of them also provided early reviews of various chapters. Greg also provided many of the code samples in Chapter 25, Database Patterns, while Ralph set up my CVS repository and automated build process for the Web site. I would also like to thank my bosses at ClearStream, who let me take time off from consulting engagements to work on the book and for permission to use the code-based exercises from our two-day “Testing for Developers” course as the basis for many of the code samples. Thanks, Denis Clelland and Luke McFarlane!\n\nSeveral people encouraged me to keep working on the book when the going got tough. They were always willing to take a phone call to discuss some sticky issue I was grappling with. Foremost among these individuals were Joshua Kerievsky and Martin Fowler.\n\nI’d like to especially thank Shaun Smith for helping me get started on this book and for the technical support he provided throughout the early part of writing it. He hosted my Web site, created the ﬁ rst CSS style sheets, taught me Ruby, set up a wiki for discussing the patterns, and even provided some of the early content before personal and work demands forced him to pull out of the writing side of the project. Whenever I say “we” when I talk about experi- ences, I am probably referring to Shaun and myself, although other coworkers may also share the same opinion.\n\nwww.it-ebooks.info",
      "content_length": 3036,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Introduction\n\nIt has been said before but it bears repeating: Writing defect-free software is exceedingly difﬁ cult. Proof of correctness of real systems is still well beyond our abilities, and speciﬁ cation of behavior is equally challenging. Predicting future needs is a hit or miss affair—we’d all be getting rich on the stock market instead of building software systems if we were any good at it!\n\nAutomated veriﬁ cation of software behavior is one of the biggest advances in development methods in the last few decades. This very developer-friendly prac- tice has huge beneﬁ ts in terms of increasing productivity, improving quality, and keeping software from becoming brittle. The very fact that so many developers are now doing it of their own free will speaks for its effectiveness.\n\nThis chapter introduces the concept of test automation using a variety of tools (including xUnit), describes why you would do it, and explains what makes it difﬁ cult to do test automation well.\n\nFeedback\n\nFeedback is a very important element in many activities. Feedback tells us whether our actions are having the right effect. The sooner we get feedback, the more quickly we can react. A good example of this kind of feedback is the rumble strips now being ground into many highways between the main driving surface and the shoulders. Yes, driving off the shoulder gives us feedback that we have left the road. But getting feedback earlier (when our wheels ﬁ rst enter the shoulder) gives us more time to correct our course and reduces the likeli- hood that we will drive off the road at all.\n\nTesting is all about getting feedback on software. For this reason, feedback is one of the essential elements of “agile” or “lean” software development. Hav- ing feedback loops in the development process is what gives us conﬁ dence in the software that we write. It lets us work more quickly and with less paranoia. It lets us focus on the new functionality we are adding by having the tests tell us whenever we break old functionality.\n\nxxix\n\nwww.it-ebooks.info",
      "content_length": 2051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "xxx\n\nIntroduction\n\nTesting\n\nThe traditional deﬁ nition of “testing” comes from the world of quality assurance. We test software because we are sure it has bugs in it! So we test and we test and we test some more, until we cannot prove there are still bugs in the software. Traditionally, this testing occurs after the software is complete. As a result, it is a way of measuring quality—not a way of building quality into the product. In many organizations, testing is done by someone other than the software developers. The feedback provided by this kind of testing is very valuable, but it comes so late in the development cycle that its value is greatly diminished. It also has the nasty effect of extending the schedule as the problems found are sent back to development for rework, to be followed by another round of testing. So what kind of testing should software developers do to get feedback earlier?\n\nDeveloper Testing\n\nRare is the software developer who believes he or she can write code that works “ﬁ rst time, every time.” In fact, most of us are pleasantly surprised when some- thing does work the ﬁ rst time. (I hope I am not shattering any illusions for the nondeveloper readers out there!)\n\nSo developers do testing, too. We want to prove to ourselves that the soft- ware works as we intended it to. Some developers might do their testing the same way as testers do it: by testing the whole system as a single entity. Most developers, however, prefer to test their software unit by unit. The “units” may be larger-grained components or they may be individual classes, methods, or functions. The key thing that distinguishes these tests from the ones that the testers write is that the units being tested are a consequence of the design of the software, rather than being a direct translation of the requirements.1\n\nAutomated Testing\n\nAutomated testing has been around for several decades. When I worked on telephone switching systems at Nortel’s R&D subsidiary Bell-Northern Research in the early 1980s, we did automated regression and load testing of\n\n1 A small percentage of the unit tests may correspond directly to the business logic described in the requirements and the customer tests, but a large majority tests the code that surrounds the business logic.\n\nwww.it-ebooks.info",
      "content_length": 2298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Introduction\n\nthe software/hardware that we were building. This testing was done primarily in the context of the “System Test” organization using specialized hardware and software that were programmed with test scripts. The test machines con- nected to the switch being tested as though it were a bunch of telephones and other telephone switches; it made telephone calls and exercised the myriad of telephone features. Of course, this automated testing infrastructure was not suitable for unit testing, nor was it generally available to the developers because of the huge amounts of hardware involved.\n\nIn the last decade, more general-purpose test automation tools have become available for testing applications through their user interfaces. Some of these tools use scripting languages to deﬁ ne the tests; the sexier tools rely on the “robot user” or “record and playback” metaphor for test automation. Unfor- tunately, many of the early experiences with these latter tools left the testers and test managers less than satisﬁ ed. The cause was high test maintenance costs caused by the “fragile test” problem.\n\nThe “Fragile Test” Problem\n\nTest automation using commercial “record and playback” or “robot user” tools has gained a bad reputation among early users of these tools. Tests automated using this approach often fail for seemingly trivial reasons. It is important to understand the limitations of this style of test automation to avoid falling vic- tim to the pitfalls commonly associated with it—namely, behavior sensitivity, interface sensitivity, data sensitivity, and context sensitivity.\n\nBehavior Sensitivity\n\nIf the behavior of the system is changed (e.g., if the requirements are changed and the system is modiﬁ ed to meet the new requirements), any tests that exer- cise the modiﬁ ed functionality will most likely fail when replayed.2 This is a basic reality of testing regardless of the test automation approach used. The real problem is that we often need to use that functionality to maneuver the system into the right state to start a test. As a consequence, behavioral changes have a much larger impact on the testing process than one might expect.\n\n2 A change in behavior could occur because the system is doing something different or because it is doing the same thing with different timing or sequencing.\n\nwww.it-ebooks.info\n\nxxxi",
      "content_length": 2359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "xxxii\n\nIntroduction\n\nInterface Sensitivity\n\nTesting the business logic inside the system under test (SUT) via the user inter- face is a bad idea. Even minor changes to the interface can cause tests to fail, even though a human user might say the test should still pass. Such unintended interface sensitivity is partly what gave test automation tools such a bad name in the past decade. Although the problem occurs regardless of which user inter- face technology is being used, it seems to be worse with some types of interfaces than with others. Graphical user interfaces (GUIs) are a particularly challeng- ing way to interact with the business logic inside the system. The recent shift to Web-based (HTML) user interfaces has made some aspects of test automation easier but has introduced yet another problem because of the executable code needed within the HTML to provide a rich user experience.\n\nData Sensitivity\n\nAll tests assume some starting point, called the test ﬁ xture; this test context is sometimes called the “pre-conditions” or “before picture” of the test. Most commonly, this test ﬁ xture is deﬁ ned in terms of data that is already in the sys- tem. If the data changes, the tests may fail unless great effort has been expended to make them insensitive to the data being used.\n\nContext Sensitivity\n\nThe behavior of the system may be affected by the state of things outside the system. These external factors could include the states of devices (e.g., printers, servers), other applications, or even the system clock (e.g., the time and/or date of execution of the test). Any tests that are affected by this context will be dif- ﬁ cult to repeat deterministically without getting control over the context.\n\nOvercoming the Four Sensitivities\n\nThe four sensitivities exist regardless of which technology we use to automate the tests. Of course, some technologies give us ways to work around these sen- sitivities, while others force us down a particular path. The xUnit family of test automation frameworks gives us a large degree of control; we just have to learn how to use it effectively.\n\nwww.it-ebooks.info",
      "content_length": 2126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Introduction\n\nUses of Automated Tests\n\nThus far, most of the discussion here has centered on regression testing of applications. This is a very valuable form of feedback when modifying existing applications because it helps us catch defects that we have introduced inadver- tently.\n\nTests as Speciﬁ cation\n\nA completely different use of automated testing is seen in test-driven devel- opment (TDD), which is one of the core practices of agile methods such as eXtreme Programming. This use of automated testing is more about speciﬁ cation of the behavior of the software yet to be written than it is about regression testing. The effectiveness of TDD comes from the way it lets us separate our thinking about software into two separate phases: what it should do, and how it should do it.\n\nHold on a minute! Don’t the proponents of agile software development eschew waterfall-style development? Yes, indeed. Agilists prefer to design and build a system feature by feature, with working software being available at every step to prove that each feature works before they move on to develop the next feature. That does not mean we do not do design; it simply means we do “continuous design”! Taking this to the extreme results in “emergent design,” where very little design is done upfront. But development does not have to be done that way. We can combine high-level design (or architecture) upfront with detailed design on a feature-by-feature basis. Either way, it can be useful to delay thinking about how to achieve the behavior of a speciﬁ c class or method for a few minutes while we capture what that behavior should be in the form of an executable speciﬁ cation. After all, most of us have trouble concentrating on one thing at a time, let alone several things simultaneously.\n\nOnce we have ﬁ nished writing the tests and verifying that they fail as expected, we can switch our perspective and focus on making them pass. The tests are now acting as a progress measurement. If we implement the functionality incremen- tally, we can see each test pass one by one as we write more code. As we work, we keep running all of the previously written tests as regression tests to make sure our changes have not had any unexpected side effects. This is where the true value of automated unit testing lies: in its ability to “pin down” the functionality of the SUT so that the functionality is not changed accidentally. That is what al- lows us to sleep well at night!\n\nwww.it-ebooks.info\n\nxxxiii",
      "content_length": 2490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "xxxiv\n\nIntroduction\n\nTest-Driven Development\n\nMany books have been written recently on the topic of test-driven develop- ment, so this one will not devote a lot of space to that topic. This book focuses on what the code in the tests looks like, rather than when we wrote the tests. The closest we will get to talking about how the tests come into being is when we investigate refactoring of tests and learn how to refactor tests written using one pattern into tests that use a pattern with different characteristics.\n\nI am trying to stay “development process agnostic” in this book because au- tomated testing can help any team regardless of whether its members are doing TDD, test-ﬁ rst development, or test-last development. Also, once people learn how to automate tests in a “test last” environment, they are likely to be more inclined to experiment with a “test ﬁ rst” approach. Nevertheless, we do ex- plore some parts of the development process because they affect how easily we can do test automation. There are two key aspects of this investigation: (1) the interplay between Fully Automated Tests (see page 26) and our development in- tegration process and tools, and (2) the way in which the development process affects the testability of our designs.\n\nPatterns\n\nIn preparing to write this book, I read a lot of conference papers and books on xUnit-based test automation. Not surprisingly, each author seems to have a particular area of interest and favorite techniques. While I do not always agree with their practices, I am always trying to understand why these authors do things a particular way and when it would be more appropriate to use their techniques than the ones I already use.\n\nThis level of understanding is one of the major differences between examples and prose that merely explain the “how to” of a technique and a pattern. A pat- tern helps readers understand the why behind the practice, allowing them to make intelligent choices between the alternative patterns and thereby avoid any unexpected nasty consequences in the future.\n\nSoftware patterns have been around for a decade, so most readers should at least be aware of the concept. A pattern is a “solution to a recurring problem.” Some problems are bigger than others and, therefore, too big to solve with a single pattern. That is where the pattern language comes into play; this collec- tion (or grammar) of patterns leads the reader from an overall problem step by step to a detailed solution. In a pattern language, some of the patterns will nec- essarily be of higher levels of abstraction, while others will focus on lower-level details. To be useful, there must be linkages between the patterns so that we\n\nwww.it-ebooks.info",
      "content_length": 2717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Introduction\n\ncan work our way down from the higher-level “strategy” patterns to the more detailed “design patterns” and the most detailed “coding idioms.”\n\nPatterns versus Principles versus Smells\n\nThis book includes three kinds of patterns. The most traditional kind of pat- tern is the “recurring solution to a common problem”; most of the patterns in this book fall into this general category. I do distinguish between three different levels:\n\n“Strategy”-level patterns have far-reaching consequences. The decision to use a Shared Fixture (page 317) rather than a Fresh Fixture (page 311) takes us down a very different path and leads to a different set of test design patterns. Each of the strategy patterns has its own write-up in the “Strategy Patterns” chapter in the reference section of the book.\n\nTest “design”-level patterns are used when developing tests for speciﬁ c functionality. They focus on how we organize our test logic. An exam- ple that should be familiar to most readers is the Mock Object pattern (page 544). Each test design pattern has its own write-up and the pat- terns are grouped into chapters in the reference section of the book based on topics such as Test Double patterns.\n\nTest “coding idioms” describe different ways to code a speciﬁ c test. Many of these are language speciﬁ c; examples include using block closures for Expected Exception Tests (see Test Method on page 348) in Smalltalk and anonymous inner classes for Mock Objects in Java. Some, such as Simple Success Test (see Test Method), are fairly generic in that they have analogs in each language. These idioms are typically listed as implementation variations or examples within the write-up of a “test design pattern.”\n\nOften, several alternative patterns could be used at each level. Of course, I almost always have a preference for which patterns to use, but one person’s “anti- pattern” may be another person’s “best practice pattern.” As a result, this book includes patterns that I do not necessarily advocate. It describes the advantages and disadvantages of each of those patterns, allowing readers to make informed decisions about their use. I have tried to provide linkages to those alternatives in each of the pattern descriptions as well as in the introductory narratives.\n\nThe nice thing about patterns is that they provide enough information to make an intelligent decision between several alternatives. The pattern we choose may be affected by the goals we have for test automation. The goals describe\n\nwww.it-ebooks.info\n\nxxxv",
      "content_length": 2541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "xxxvi\n\nIntroduction\n\ndesired outcomes of the test automation efforts. These goals are supported by a number of principles that codify a belief system about what makes automated tests “good.” In this book, the goals of test automation are described in Chapter 3, Goals of Test Automation, and the principles are described in Chapter 5, Prin- ciples of Test Automation.\n\nThe ﬁ nal kind of pattern is more of an anti-pattern [AP]. These test smells describe recurring problems that our patterns help us address in terms of the symptoms we might observe and the root causes of those symptoms. Code smells were ﬁ rst popularized in Martin Fowler’s book [Ref] and applied to xUnit-based testing as test smells in a paper presented at XP2001 [RTC]. The test smells are cross-referenced with the patterns that can be used to banish them as well as the patterns3 that are more likely to lead to them.4 In addition, the test smells are covered in depth in their own section: Part II, The Test Smells.\n\nPattern Form\n\nThis book includes my descriptions of patterns. The patterns themselves existed before I started cataloging them, by virtue of having been invented indepen- dently by at least three different test automaters. I took it upon myself to write them down as a way of making the knowledge more easily distributable. But to do so, I had to choose a pattern description form.\n\nPattern descriptions come in many shapes and sizes. Some have a very rigid structure deﬁ ned by many headings that help the reader ﬁ nd the various sec- tions. Others read more like literature but may be more difﬁ cult to use as a ref- erence. Nevertheless, all patterns have a common core of information, however it is presented.\n\nMy Pattern Form\n\nI have really enjoyed reading the works of Martin Fowler, and I attribute much of that enjoyment to the pattern form that he uses. As the saying goes, “Imita- tion is the sincerest form of ﬂ attery”: I have copied his format shamelessly with only a few minor modiﬁ cations.\n\nThe template begins with the problem statement, the summary statement, and a sketch. The italicized problem statement summarizes the core of the problem\n\n3 Some might want to call these patterns “anti-patterns.” Just because a pattern often has negative consequences, it does not imply that the pattern is always bad. For this reason, I prefer not to call these anti-patterns; I just do not use them very often. 4 In a few cases, there are even a pattern and a smell with similar names.\n\nwww.it-ebooks.info",
      "content_length": 2505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Introduction\n\nthat the pattern addresses. It is often stated as a question: “How do we . . . ?” The boldface summary statement captures the essence of the pattern in one or two sentences, while the sketch provides a visual representation of the pattern. The untitled section of text immediately after the sketch summarizes why we might want to use the pattern in just a few sentences. It elaborates on the problem statement and includes both the “Problem” and “Context” sections from the tra- ditional pattern template. A reader should be able to get a sense of whether he or she wants to read any further by skimming this section.\n\nThe next three sections provide the meat of the pattern. The “How It Works” section describes the essence of how the pattern is structured and what it is about. It also includes information about the “resulting context” when there are several ways to implement some important aspect of the pattern. This sec- tion corresponds to the “Solution” or “Therefore” sections of more traditional pattern forms. The “When to Use It” section describes the circumstances in which you should consider using the pattern. This section corresponds to the “Problem,” “Forces,” “Context,” and “Related Patterns” sections of traditional pattern templates. It also includes information about the “Resulting Context,” when this information might affect whether you would want to use this pattern. I also include any “test smells” that might suggest that you should use this pat- tern. The “Implementation Notes” section describes the nuts and bolts of how to implement the pattern. Subheadings within this section indicate key compo- nents of the pattern or variations in how the pattern can be implemented.\n\nMost of the concrete patterns include three additional sections. The “Moti- vating Example” section provides examples of what the test code might have looked like before this pattern was applied. The section titled “Example: {Pat- tern Name}” shows what the test would look like after the pattern was applied. The “Refactoring Notes” section provides more detailed instructions on how to get from the “Motivating Example” to the “Example: {Pattern Name}.”\n\nIf the pattern is written up elsewhere, the description may include a section titled “Further Reading.” A “Known Uses” section appears when there is some- thing particularly interesting about those applications. Most of these patterns have been seen in many systems, of course, so picking three uses to substantiate them would be arbitrary and meaningless.\n\nWhere a number of related techniques exist, they are often presented here as a single pattern with several variations. If the variations are different ways to implement the same fundamental pattern (namely, solving the same prob- lem the same general way), the variations and the differences between them are listed in the “Implementation Notes” section. If the variations are primarily a different reason for using the pattern, the variations are listed in the “When to Use It” section.\n\nwww.it-ebooks.info\n\nxxxvii",
      "content_length": 3052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "xxxviii\n\nIntroduction\n\nHistorical Patterns and Smells\n\nI struggled mightily when trying to come up with a concise enough list of pat- terns and smells while still keeping historical names whenever possible. I often list the historical name as an alias for the pattern or smell. In some cases, it made more sense to consider the historical version of the pattern as a speciﬁ c variation of a larger pattern. In such a case, I usually include the historical pat- tern as a named variation in the “Implementation Notes” section.\n\nMany of the historical smells did not pass the “sniff test”—that is, the smell described a root cause rather than a symptom.5 Where an historical test smell describes a cause and not a symptom, I have chosen to move it into the cor- responding symptom-based smell as a special kind of variation titled “Cause.” Mystery Guest (see Obscure Test on page 186) is a good example.\n\nReferring to Patterns and Smells\n\nI also struggled to come up with a good way to refer to patterns and smells, espe- cially the historical ones. I wanted to be able to use both the historical names when appropriate and the new aggregate names, whichever was more appropriate. I also wanted the reader to be able to see which was which. In the online version of this book, hyperlinks were used for this purpose. For the printed version, however, I needed a way to represent this linkage as a page number annotation of the refer- ence without cluttering up the entire text with references. The solution I landed on after several tries includes the page number where the pattern or smell can be found the ﬁ rst time it is referenced in a chapter, pattern, or smell. If the reference is to a pattern variation or the cause of a smell, I include the aggregate pattern or smell name the ﬁ rst time. Note how this second reference to the Mystery Guest cause of Obscure Test shows up without the smell name, whereas references to other causes of Obscure Test such as Irrelevant Information (see Obscure Test) include the aggregate smell name but not the page number.\n\nRefactoring\n\nRefactoring is a relatively new concept in software development. While people have always had a need to modify existing code, refactoring is a highly\n\n5 The “sniff test” is based on the diaper story in [Ref] wherein Kent Beck asks Grandma Beck, “How do I know that it is time to change the diaper?” “If it stinks, change it!” was her response. Smells are named based on the “stink,” not the cause of the stink.\n\nwww.it-ebooks.info",
      "content_length": 2506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Introduction\n\ndisciplined approach to changing the design without changing the behavior of the code. It goes hand-in-hand with automated testing because it is very difﬁ cult to do refactoring without having the safety net of automated tests to prove that you have not broken anything during your redesign.\n\nMany of the modern integrated development environments (IDEs) have built-in support for refactoring. Most of them automate the refactoring steps of at least a few of the refactorings described in Martin Fowler’s book [Ref]. Unfortunately, the tools do not tell us when or why we should use refactoring. We will have to get a copy of Martin’s book for that! Another piece of mandatory reading on this topic is Joshua Kerievsky’s book [RtP].\n\nRefactoring tests differs a bit from refactoring production code because we do not have automated tests for our automated tests! If a test fails after a refac- toring of the test, did the failure occur because we made a mistake during the refactoring? Just because a test passes after a test refactoring, can we be sure it will still fail when appropriate? To address this issue, many test refactorings are very conservative, “safe refactorings” that minimize the chance of introducing a change of behavior into the test. We also try to avoid having to do major refac- torings of tests by adopting an appropriate test strategy, as described in Chapter 6, Test Automation Strategy.\n\nThis book focuses more on the target of the refactoring than on the mechanics of this endeavor. A short summary of the refactorings does appear in Appendix A, but the process of refactoring is not the primary focus of this book. The patterns themselves are new enough that we have not yet had time to agree on their names, content, or applicability, let alone reach consensus on the best way to refactor to them. A further complication is that there are potentially many starting points for each refactoring target (pattern), and attempting to provide detailed refactoring instructions would make this already large book much larger.\n\nAssumptions\n\nIn writing this book, I assumed that the reader is somewhat familiar with object technology (also known as “object-oriented programming”); object technology seemed to be a prerequisite for automated unit testing to become popular. That does not mean we cannot perform testing in procedural or functional languages, but use of these languages may make it more challenging (or at least different). Different people have different learning styles. Some need to start with the “big picture” abstractions and work down to “just enough” detail. Others can understand only the details and have no need for the “big picture.” Some learn best by hearing or reading words; others need pictures to help them visualize\n\nwww.it-ebooks.info\n\nxxxix",
      "content_length": 2811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "xl\n\nIntroduction\n\na concept. Still others learn programming concepts best by reading code. I’ve tried to accommodate all of these learning styles by providing a summary, a detailed description, code samples, and a picture wherever possible. These items should be Skippable Sections [PLOPD3] for those readers who won’t beneﬁ t from that style of learning.\n\nTerminology\n\nThis book brings together terminology from two different domains: software development and software testing. As a consequence, some terminology will inevitably be unfamiliar to some readers. Readers should refer to the glossary when they encounter any terms that they do not understand. I will, however, point out one or two terms here, because becoming familiar with these terms is essential to understanding most of the material in this book.\n\nTesting Terminology\n\nSoftware developers will probably ﬁ nd the term “system under test” (abbrevi- ated throughout this book as SUT) unfamiliar. It is short for “whatever thing we are testing.” When we are writing unit tests, the SUT is whatever class or method(s) we are testing; when we are writing customer tests, the SUT is prob- ably the entire application (or at least a major subsystem of it).\n\nAny part of the application or system we are building that is not included in the SUT may still be required to run our test because it is called by the SUT or because it sets up prerequisite data that the SUT will use as we exercise it. The former type of element is called a depended-on component (DOC), and both types are part of the test ﬁ xture. This is illustrated in Figure I.1.\n\nLanguage-Speciﬁ c xUnit Terminology\n\nAlthough this book includes examples in a variety of languages and xUnit fam- ily members, JUnit ﬁ gures prominently in this coverage. JUnit is the language and xUnit framework that most people are at least somewhat familiar with. Many of the translations of JUnit to other languages are relatively faithful ports, with only minor changes in class and method names needed to accom- modate the differences in the underlying language. Where this isn’t the case, Appendix B, xUnit Terminology Cross-Reference, often includes the appropri- ate mapping.\n\nwww.it-ebooks.info",
      "content_length": 2209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Introduction\n\nUnit1 Unit1 Test Test\n\nExercise Exercise\n\nUnit1 Unit1 SUT SUT\n\nUnit2 Unit2 Test Test\n\nExercise Exercise\n\nUnit2 Unit2 SUT SUT\n\nUses Uses\n\nComp1 Comp1 Test Test\n\nExercise Exercise\n\nComp1 Comp1 SUT SUT\n\nComp2 Comp2 Test Test\n\nExercise Exercise\n\nComp2 Comp2 SUT SUT\n\nUses Uses\n\nApp1 App1 Test Test\n\nExercise Exercise\n\nApp1 App1 SUT SUT\n\nFigure I.1. A range of tests each with its own SUT. An application, component, or unit is only the SUT with respect to a speciﬁ c set of tests. The “Unit1 SUT” plays the role of DOC (part of the ﬁ xture) to “Unit2 Test” and is part of the “Comp1 SUT” and the “App1 SUT.”\n\nUsing Java as the main sample language also means that in some discussions we will refer to the JUnit name of a method and will not list the corresponding method names in each of the xUnit frameworks. For example, a discussion may refer to JUnit’s assertTrue method without mentioning that the NUnit equiva- lent is Assert.IsTrue, the SUnit equivalent is should:, and the VbUnit equivalent is verify. Readers are expected to do the mental swap of method names to the SUnit, VbUnit, Test::Unit, and other equivalents with which they may be most familiar. The Intent-Revealing Names [SBPP] of the JUnit methods should be clear enough for the purposes of our discussion.\n\nCode Samples\n\nSample code is always a problem. Samples of code from real projects are typi- cally much too large to include and are usually covered by nondisclosure agree- ments that preclude their publication. “Toy programs” do not get much respect because “they aren’t real.” A book such as this one has little choice except to use “toy programs,” but I have tried to make them as representative as possible of real projects.\n\nwww.it-ebooks.info\n\nxli",
      "content_length": 1740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "xlii\n\nIntroduction\n\nAlmost all of the code samples presented here came from “real” compilable and executable code, so they should not (knock on wood) contain any compile errors unless they were introduced during the editing process. Most of the Ruby examples come from the XML-based publishing system I used to prepare this book, while many of the Java and C# samples came from courseware that we use at ClearStream to teach these concepts to ClearStream’s clients.\n\nI have tried to use a variety of languages to illustrate the nearly universal application of the patterns across the members of the xUnit family. In some cases, the speciﬁ c pattern dictated the use of language because of speciﬁ c features of either the language or the xUnit family member. In other cases, the language was dictated by the availability of third-party extensions for a speciﬁ c member of the xUnit family. Otherwise, the default language for examples is Java with some C# because most people have at least reading-level familiarity with them. Formatting code for a book is a particular challenge due to the recommended line length of just 65 characters. I have taken some liberties in shortening vari- able and class names simply to reduce the number of lines that wrap. I’ve also invented some line-wrapping conventions to minimize the vertical size of these samples. You can take solace in the fact that your test code should look a lot “shorter” than mine because you have to wrap many fewer lines!\n\nDiagramming Notation\n\n“A picture is worth a thousand words.” Wherever possible, I have tried to include a sketch of each pattern or smell. I’ve based the sketches loosely on the Uniﬁ ed Modeling Language (UML) but took a few liberties to make them more expres- sive. For example, I use the aggregation symbol (diamond) and the inheritance symbol (a triangle) of UML class diagrams, but I mix classes and objects on the same diagram along with associations and object interactions. Most of the nota- tion is introduced in the patterns in Chapter 19, xUnit Basics Patterns, so you may ﬁ nd it worthwhile to skim this chapter just to look at the pictures.\n\nAlthough I have tried to make this notation “discoverable” simply through comparing sketches, a few conventions are worth pointing out. Objects have shadows; classes and methods do not. Classes have square corners, in keep- ing with UML; methods have round corners. Large exclamation marks are as- sertions (potential test failures), and a starburst is an error or exception being raised. The ﬁ xture is a cloud, reﬂ ecting its nebulous nature, and any compo- nents the SUT depends on are superimposed on the cloud. Whatever the sketch is trying to illustrate is highlighted with heavier lines and darker shading. As a result, you should be able to compare two sketches of related concepts and quickly determine what is emphasized in each.\n\nwww.it-ebooks.info",
      "content_length": 2899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Introduction\n\nLimitations\n\nAs you use these patterns, please keep in mind that I could not have seen every test automation problem and every solution to every problem; there may well be other, possibly better, ways to solve some of these problems. These solutions are just the ones that have worked for me and for the people I have been com- municating with. Accept everyone’s advice with a grain of salt!\n\nMy hope is that these patterns will give you a starting point for writing good, robust automated tests. With luck, you will avoid many of the mistakes we made on our ﬁ rst attempts and will go on to invent even better ways of auto- mating tests. I’d love to hear about them!\n\nwww.it-ebooks.info\n\nxliii",
      "content_length": 708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Refactoring a Test\n\nWhy Refactor Tests?\n\nTests can quickly become a bottleneck in an agile development process. This may not be immediately obvious to those who have never experienced the difference between simple, easily understood tests and complex, obtuse, hard-to-maintain tests. The productivity difference can be staggering!\n\nThis section of the book acts as a “motivating example” for the entire book by showing you how much of a difference refactoring tests can make. It walks you through an example starting with a complex test and, step by step, refac- tors it to a simple, easily understood test. Along the way, I will point out some key smells and the patterns that we can use to remove them. Ideally, this exer- cise will whet your appetite for more.\n\nA Complex Test\n\nHere is a test that is not atypical of some of the tests I have seen on various projects:\n\npublic void testAddItemQuantity_severalQuantity_v1(){ Address billingAddress = null; Address shippingAddress = null; Customer customer = null; Product product = null; Invoice invoice = null; try { // Set up ﬁxture billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\",\"Canada\"); shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); customer = new Customer(99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); product = new Product(88, \"SomeWidget\", new BigDecimal(\"19.99\")); invoice = new Invoice(customer);\n\nxlv\n\nwww.it-ebooks.info",
      "content_length": 1490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "xlvi\n\nRefactoring a Test\n\n// Exercise SUT invoice.addItemQuantity(product, 5); // Verify outcome List lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"inv\", invoice, actItem.getInv()); assertEquals(\"prod\", product, actItem.getProd()); assertEquals(\"quant\", 5, actItem.getQuantity()); assertEquals(\"discount\", new BigDecimal(\"30\"), actItem.getPercentDiscount()); assertEquals(\"unit price\",new BigDecimal(\"19.99\"), actItem.getUnitPrice()); assertEquals(\"extended\", new BigDecimal(\"69.96\"), actItem.getExtendedPrice()); } else { assertTrue(\"Invoice should have 1 item\", false); } } ﬁnally { // Teardown deleteObject(invoice); deleteObject(product); deleteObject(customer); deleteObject(billingAddress); deleteObject(shippingAddress); } }\n\nThis test is quite long1 and is much more complicated than it needs to be. This Obscure Test (page 186) is difﬁ cult to understand because the sheer number of lines in the test makes it hard to see the big picture. It also suffers from a num- ber of other problems that we will address individually.\n\nCleaning Up the Test\n\nLet’s look at each of the various parts of the test.\n\nCleaning Up the Veriﬁ cation Logic\n\nFirst, let’s focus on the part that veriﬁ es the expected outcome. Maybe we can infer from the assertions which test conditions this test is trying to verify.\n\n1 While the need to wrap lines to keep them at 65 characters makes this code look even longer than it really is, it is still unnecessarily long. It contains 25 executable statements including initialized declarations, 6 lines of control statements, 4 in-line comments, and 2lines to declare the test method—giving a total of 37 lines of unwrapped source code.\n\nwww.it-ebooks.info",
      "content_length": 1766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Cleaning Up the Test\n\nList lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"inv\", invoice, actItem.getInv()); assertEquals(\"prod\", product, actItem.getProd()); assertEquals(\"quant\", 5, actItem.getQuantity()); assertEquals(\"discount\", new BigDecimal(\"30\"), actItem.getPercentDiscount()); assertEquals(\"unit price\",new BigDecimal(\"19.99\"), actItem.getUnitPrice()); assertEquals(\"extended\", new BigDecimal(\"69.96\"), actItem.getExtendedPrice()); } else { assertTrue(\"Invoice should have 1 item\", false); }\n\nA simple problem to ﬁ x is the obtuse assertion on the very last line. Calling assertTrue with an argument of false should always result in a test failure, so why don’t we say so directly? Let’s change this to a call to fail:\n\nList lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"inv\", invoice, actItem.getInv()); assertEquals(\"prod\", product, actItem.getProd()); assertEquals(\"quant\", 5, actItem.getQuantity()); assertEquals(\"discount\", new BigDecimal(\"30\"), actItem.getPercentDiscount()); assertEquals(\"unit price\",new BigDecimal(\"19.99\"), actItem.getUnitPrice()); assertEquals(\"extended\", new BigDecimal(\"69.96\"), actItem.getExtendedPrice()); } else { fail(\"Invoice should have exactly one line item\"); }\n\nWe can think of this move as an Extract Method [Fowler] refactoring, because we are replacing the Stated Outcome Assertion (see Assertion Method on page 362) with a hard-coded parameter with a more intent-revealing call to a Single Out- come Assertion (see Assertion Method) method that encapsulates the call.\n\nOf course, this set of assertions suffers from several more problems. For exam- ple, why do we need so many of them? It turns out that many of these assertions are testing ﬁ elds set by the constructor for the LineItem, which is itself covered by another unit test. So why repeat these assertions here? It will just create more test code to maintain when the logic changes.\n\nOne solution is to use a single assertion on an Expected Object (see State Veri- ﬁ cation on page 462) instead of one assertion per object ﬁ eld. First, we deﬁ ne an object that looks exactly how we expect the result to look. In this case, we create\n\nwww.it-ebooks.info\n\nxlvii",
      "content_length": 2331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "xlviii\n\nRefactoring a Test\n\nan expected LineItem with the ﬁ elds ﬁ lled in with the expected values, including the unitPrice and extendedPrice initialized from the product.\n\nList lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"invoice\", expected.getInv(), actItem.getInv()); assertEquals(\"product\", expected.getProd(), actItem.getProd()); assertEquals(\"quantity\",expected.getQuantity(), actItem.getQuantity()); assertEquals(\"discount\", expected.getPercentDiscount(), actItem.getPercentDiscount()); assertEquals(\"unit pr\", new BigDecimal(\"19.99\"), actItem.getUnitPrice()); assertEquals(\"extend pr\",new BigDecimal(\"69.96\"), actItem.getExtendedPrice()); } else { fail(\"Invoice should have exactly one line item\"); }\n\nOnce we have created our Expected Object, we can then assert on it using assertEquals:\n\nList lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem expected = new LineItem(invoice, product,5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"invoice\", expected, actItem); } else { fail(\"Invoice should have exactly one line item\"); }\n\nClearly, the Preserve Whole Object [Fowler] refactoring makes the code a lot simpler and more obvious. But wait! Why do we have an if statement in a test? If there are several paths through a test, how do we know which one is actually being executed? It would be a lot better if we could eliminate this Conditional Test Logic (page 200). Luckily for us, the pattern Guard Assertion (page 490) is designed to handle exactly this case. We simply use a Replace Conditional with Guard Clause [Fowler] refactoring to replace the if ... else fail() ... sequence with an assertion on the same condition. This Guard Assertion halts execution if the condition is not met without introducing Conditional Test Logic.\n\nwww.it-ebooks.info",
      "content_length": 2034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Cleaning Up the Test\n\nList lineItems = invoice.getLineItems(); assertEquals(\"number of items\", 1,lineItems.size()); LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"invoice\", expected, actItem);\n\nSo far, we have reduced 11 lines of veriﬁ cation code to just 4, and those 4 lines are a lot simpler code to boot.2 Some people might suggest that this refactor- ing is good enough. But can’t we make this assertion even more obvious? What are we really trying to verify? We are trying to say that there should be only one line item and it should look exactly like our expectedLineItem. We can say this explicitly by using an Extract Method refactoring to deﬁ ne a Custom Asser- tion (page 474).\n\nLineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected);\n\nThat is better! Now we have the veriﬁ cation part of the test down to just two lines. Let’s review what the whole test looks like:\n\npublic void testAddItemQuantity_severalQuantity_v6(){ Address billingAddress = null; Address shippingAddress = null; Customer customer = null; Product product = null; Invoice invoice = null; try { // Set up ﬁxture billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); customer = new Customer(99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); product = new Product(88, \"SomeWidget\", new BigDecimal(\"19.99\")); invoice = new Invoice(customer); // Exercise SUT invoice.addItemQuantity(product, 5);\n\n2 It’s a good thing we are not being rewarded for the number of lines of code we write! This is yet another example of why KLOC is such a poor measure of productivity.\n\nwww.it-ebooks.info\n\nxlix",
      "content_length": 1937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "l\n\nRefactoring a Test\n\n// Verify outcome LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); } ﬁnally { // Teardown deleteObject(invoice); deleteObject(product); deleteObject(customer); deleteObject(billingAddress); deleteObject(shippingAddress); } }\n\nCleaning Up the Fixture Teardown Logic\n\nNow that we have cleaned up the result veriﬁ cation logic, let’s turn our atten- tion to the ﬁ nally block at the end of the test. What is this code doing?\n\n} ﬁnally { // Teardown deleteObject(invoice); deleteObject(product); deleteObject(customer); deleteObject(billingAddress); deleteObject(shippingAddress); }\n\nMost modern languages have an equivalent construct to the try/ﬁ nally block that can be used to ensure that code gets run even when an error or exception occurs. In a Test Method (page 348), the ﬁ nally block ensures that any cleanup code gets run regardless of whether the test passed or failed. A failed assertion throws an exception, which would transfer control back to the Test Automation Framework’s (page 298) exception-handling code, so we use the ﬁ nally block to clean up ﬁ rst. This approach means that we avoid having to catch the exception and then rethrow it.\n\nIn this test, the ﬁ nally block calls the deleteObject method on each of the objects created by the test. Unfortunately, this code suffers from a fatal ﬂ aw. Have you noticed it yet?\n\nThings could go wrong during the teardown itself. What happens if the ﬁ rst call to deleteObject throws an exception? As coded here, none of the other calls to deleteObject would be executed. The solution is to use a nested try/ﬁ nally block around this ﬁ rst call, thereby ensuring that the second call to deleteObject always executes. But what if the second call fails? In this case, we would need a total\n\nwww.it-ebooks.info",
      "content_length": 1895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Cleaning Up the Test\n\nof six nested try/ﬁ nally blocks to make this maneuver work. That would almost double the length of the test, and we cannot afford to write and maintain so much code in each test.\n\n} ﬁnally { // Teardown try { deleteObject(invoice); } ﬁnally { try { deleteObject(product); } ﬁnally { try { deleteObject(customer); } ﬁnally { try { deleteObject(billingAddress); } ﬁnally { deleteObject(shippingAddress); } } } }\n\nThe problem is that we now have a Complex Teardown (see Obscure Test). What are the chances of getting this code right? And how do we test the test code? Clearly, our current approach is not going to be very effective.\n\nOf course, we could move this code into the tearDown method. That would have the advantage of removing it from the Test Method. Also, because the tearDown method acts as a ﬁ nally block, we would get rid of the outermost try/ ﬁ nally. Unfortunately, this strategy doesn’t address the root of the problem: the need to write detailed teardown code in each test.\n\nWe could try to avoid creating the objects in the ﬁ rst place by using a Shared Fixture (page 317) that is not torn down between tests. Unfortunately, this approach is likely to lead to a number of test smells, including Unrepeatable Test (see Erratic Test on page 228) and Interacting Tests (see Erratic Test), caused by interactions via the shared ﬁ xture. Another issue is that the references to objects used from the shared ﬁ xture are often Mystery Guests (see Obscure Test).3\n\nThe best solution is to use a Fresh Fixture (page 311) but to avoid writ- ing teardown code for every test. To do so, we can use an in-memory ﬁ xture that is automatically garbage collected. This approach won’t work, however, if the objects we create are persistent (e.g., if they are saved in a database). While it is best to construct the system architecture so that most of our tests can\n\n3 The test reader cannot see the objects being used by the test.\n\nwww.it-ebooks.info\n\nli",
      "content_length": 1978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "lii\n\nRefactoring a Test\n\nbe executed without the database, we almost always have some tests that need it. In these cases, we can extend the Test Automation Framework to do most of the work for us. We can add a means to register each object we create with the framework so that it can do the deleting for us.\n\nFirst, we need to register each object as we create it:\n\n// Set up ﬁxture billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); registerTestObject(billingAddress); shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\",\"T2N 2V2\", \"Canada\"); registerTestObject(shippingAddress); customer = new Customer(99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); registerTestObject(shippingAddress); product = new Product(88, \"SomeWidget\", new BigDecimal(\"19.99\")); registerTestObject(shippingAddress); invoice = new Invoice(customer); registerTestObject(shippingAddress);\n\nRegistration consists of adding the object to a collection of test objects:\n\nList testObjects;\n\nprotected void setUp() throws Exception { super.setUp(); testObjects = new ArrayList(); }\n\nprotected void registerTestObject(Object testObject) { testObjects.add(testObject); }\n\nIn the tearDown method, we iterate through the list of test objects and delete each one:\n\npublic void tearDown() { Iterator i = testObjects.iterator(); while (i.hasNext()) { try { deleteObject(i.next()); } catch (RuntimeException e) { // Nothing to do; we just want to make sure // we continue on to the next object in the list } } }\n\nwww.it-ebooks.info",
      "content_length": 1575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Cleaning Up the Test\n\nNow our test looks like this:\n\npublic void testAddItemQuantity_severalQuantity_v8(){ Address billingAddress = null; Address shippingAddress = null; Customer customer = null; Product product = null; Invoice invoice = null; // Set up ﬁxture billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); registerTestObject(billingAddress); shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\",\"T2N 2V2\", \"Canada\"); registerTestObject(shippingAddress); customer = new Customer(99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); registerTestObject(shippingAddress); product = new Product(88, \"SomeWidget\", new BigDecimal(\"19.99\")); registerTestObject(shippingAddress); invoice = new Invoice(customer); registerTestObject(shippingAddress); // Exercise SUT invoice.addItemQuantity(product, 5); // Verify outcome LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\nWe have been able to remove the try/ﬁ nally block and, except for the additional calls to registerTestObject, our code is much simpler. But we can still clean this code up a bit more. Why, for example, do we need to declare the variables and initialize them to null, only to reinitialize them later? This action was needed with the original test because they had to be accessible in the ﬁ nally block; now that we have removed this block, we can combine the declaration with the initialization:\n\npublic void testAddItemQuantity_severalQuantity_v9(){ // Set up ﬁxture Address billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); registerTestObject(billingAddress); Address shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\");\n\nwww.it-ebooks.info\n\nliii",
      "content_length": 1881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "liv\n\nRefactoring a Test\n\nregisterTestObject(shippingAddress); Customer customer = new Customer(99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); registerTestObject(shippingAddress); Product product = new Product(88, \"SomeWidget\", new BigDecimal(\"19.99\")); registerTestObject(shippingAddress); Invoice invoice = new Invoice(customer); registerTestObject(shippingAddress); // Exercise SUT invoice.addItemQuantity(product, 5); // Verify outcome LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.95\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\nCleaning Up the Fixture Setup\n\nNow that we have cleaned up the assertions and the ﬁ xture teardown, let’s turn our attention to the ﬁ xture setup. One obvious “quick ﬁ x” would be to take each of the calls to a constructor, take the subsequent call to registerTestObject, and use an Extract Method refactoring to deﬁ ne a Creation Method (page 415). This will make the test a bit simpler to read and write. The use of Creation Methods has another advantage: They encapsulate the API of the SUT and reduce the test maintenance effort when the various object constructors change by allowing us to modify only a single place rather than having to change each test.\n\npublic void testAddItemQuantity_severalQuantity_v10(){ // Set up ﬁxture Address billingAddress = createAddress( \"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); Address shippingAddress = createAddress( \"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); Customer customer = createCustomer( 99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); Product product = createProduct( 88,\"SomeWidget\",new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, 5);\n\nwww.it-ebooks.info",
      "content_length": 1866,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Cleaning Up the Test\n\n// Verify outcome LineItem expected = new LineItem(invoice, product,5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\nThis ﬁ xture setup logic still suffers from several problems. The ﬁ rst problem is that it is difﬁ cult to tell how the ﬁ xture is related to the expected outcome of the test. Do the customer’s particulars affect the outcome in some way? Does the customer’s address affect the outcome? What is this test really verifying?\n\nThe other problem is that this test exhibits Hard-Coded Test Data (see Obscure Test). Given that our SUT persists all objects we create in a database, the use of Hard-Coded Test Data may result in an Unrepeatable Test, an Interacting Test, or a Test Run War (see Erratic Test) if any of the ﬁ elds of the customer, product, or invoice must be unique.\n\nWe can solve this problem by generating a unique value for each test and then using that value to seed the attributes of the objects we create for the test. This approach will ensure that the test creates different objects each time the test is run. Because we have already moved the object creation logic into Creation Meth- ods, this step is relatively easy; we just put this logic into the Creation Method and remove the corresponding parameters. This is another application of the Extract Method refactoring, in which we create a new, parameterless version of the Cre- ation Method.\n\npublic void testAddItemQuantity_severalQuantity_v11(){ ﬁnal int QUANTITY = 5; // Set up ﬁxture Address billingAddress = createAnAddress(); Address shippingAddress = createAnAddress(); Customer customer = createACustomer(new BigDecimal(\"30\"), billingAddress, shippingAddress); Product product = createAProduct(new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, QUANTITY); // Verify outcome LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); } private Product createAProduct(BigDecimal unitPrice) { BigDecimal uniqueId = getUniqueNumber(); String uniqueString = uniqueId.toString(); return new Product(uniqueId.toBigInteger().intValue(), uniqueString, unitPrice); }\n\nwww.it-ebooks.info\n\nlv",
      "content_length": 2318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "lvi\n\nRefactoring a Test\n\nWe call this pattern an Anonymous Creation Method (see Creation Method) because we are declaring that we don’t care about the particulars of the object. If the expected behavior of the SUT depends on a particular value, we can either pass the value as a parameter or imply it in the name of the creation method.\n\nThis test looks a lot better now, but we are not done yet. Does the expected outcome depend in any way on the addresses of the customer? If not, we can hide their construction completely by using an Extract Method refactoring (again!) to create a version of the createACustomer method that fabricates them for us.\n\npublic void testAddItemQuantity_severalQuantity_v12(){ // Set up ﬁxture Customer cust = createACustomer(new BigDecimal(\"30\")); Product prod = createAProduct(new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(cust); // Exercise SUT invoice.addItemQuantity(prod, 5); // Verify outcome LineItem expected = new LineItem(invoice, prod, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\nBy moving the calls that create the addresses into the method that creates the customer, we have made it clear that the addresses do not affect the logic that we are verifying in this test. The outcome does depend on the customer’s dis- count, however, so we pass the discount percentage to the customer creation method.\n\nWe still have one or two things to clean up. For example, the Hard-Coded Test Data for the unit price, quantity, and customer’s discount is repeated twice in the test. We can clarify the meaning of these numbers by using a Replace Magic Number with Symbolic Constant [Fowler] refactoring to give them role- describing names. Also, the constructor we are using to create the LineItem is not used anywhere in the SUT itself because the LineItem normally calculates the extendedCost when it is constructed. We should turn this test-speciﬁ c code into a Foreign Method [Fowler] implemented within the test harness. We have already seen examples of how to do so with the Customer and Product: We use a Param- eterized Creation Method (see Creation Method) to return the expected LineItem based on only those values of interest.\n\npublic void testAddItemQuantity_severalQuantity_v13(){ ﬁnal int QUANTITY = 5; ﬁnal BigDecimal UNIT_PRICE = new BigDecimal(\"19.99\"); ﬁnal BigDecimal CUST_DISCOUNT_PC = new BigDecimal(\"30\");\n\nwww.it-ebooks.info",
      "content_length": 2452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "The Cleaned-Up Test\n\n// Set up ﬁxture Customer customer = createACustomer(CUST_DISCOUNT_PC); Product product = createAProduct( UNIT_PRICE); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, QUANTITY); // Verify outcome ﬁnal BigDecimal EXTENDED_PRICE = new BigDecimal(\"69.96\"); LineItem expected = new LineItem(invoice, product, QUANTITY, CUST_DISCOUNT_PC, EXTENDED_PRICE); assertContainsExactlyOneLineItem(invoice, expected); }\n\nOne ﬁ nal point: Where did the value “69.96” come from? If this value comes from the output of some reference system, we should say so. Because it was just manually calculated and typed into the test, we can show the calculation in the test for the test reader’s beneﬁ t.\n\nThe Cleaned-Up Test\n\nHere is the ﬁ nal cleaned-up version of the test:\n\npublic void testAddItemQuantity_severalQuantity_v14(){ ﬁnal int QUANTITY = 5; ﬁnal BigDecimal UNIT_PRICE = new BigDecimal(\"19.99\"); ﬁnal BigDecimal CUST_DISCOUNT_PC = new BigDecimal(\"30\"); // Set up ﬁxture Customer customer = createACustomer(CUST_DISCOUNT_PC); Product product = createAProduct( UNIT_PRICE); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, QUANTITY); // Verify outcome ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE.multiply(new BigDecimal(QUANTITY)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUST_DISCOUNT_PC.movePointLeft(2))); LineItem expected = createLineItem(QUANTITY, CUST_DISCOUNT_PC, EXTENDED_PRICE, product, invoice); assertContainsExactlyOneLineItem(invoice, expected); }\n\nWe have used an Introduce Explaining Variable [Fowler] refactoring to better document the calculation of the BASE_PRICE (price*quantity) and EXTENDED_PRICE (the price with discount). The revised test is now much smaller and clearer than\n\nwww.it-ebooks.info\n\nlvii",
      "content_length": 1848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "lviii\n\nRefactoring a Test\n\nthe bulky code we started with. It fulﬁ lls the role of Tests as Documentation (see page 23) very well. So what did we discover that this test veriﬁ es? It con- ﬁ rms that the line items added to an invoice are, indeed, added to the invoice and that the extended cost is based on the product price, the customer’s dis- count, and the quantity ordered.\n\nWriting More Tests\n\nIt seemed like we went to a lot of effort to refactor this test to make it clearer. Will we have to spend so much effort on every test?\n\nI should hope not! Much of the effort here related to the discovery of which Test Utility Methods (page 599) were required for writing the test. We deﬁ ned a Higher-Level Language (see page 41) for testing our application. Once we have those methods in place, writing other tests becomes much simpler. For example, if we want to write a test that veriﬁ es that the extended cost is recalculated when we change the quantity of a LineItem, we can reuse most of the Test Utility Methods.\n\npublic void testAddLineItem_quantityOne(){ ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE; ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE; // Set up ﬁxture Customer customer = createACustomer(NO_CUST_DISCOUNT); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(PRODUCT, QUAN_ONE); // Verify outcome LineItem expected = createLineItem( QUAN_ONE, NO_CUST_DISCOUNT, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\npublic void testChangeQuantity_severalQuantity(){ ﬁnal int ORIGINAL_QUANTITY = 3; ﬁnal int NEW_QUANTITY = 5; ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE.multiply( new BigDecimal(NEW_QUANTITY)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUST_DISCOUNT_PC.movePointLeft(2))); // Set up ﬁxture Customer customer = createACustomer(CUST_DISCOUNT_PC); Invoice invoice = createInvoice(customer); Product product = createAProduct( UNIT_PRICE); invoice.addItemQuantity(product, ORIGINAL_QUANTITY);\n\nwww.it-ebooks.info",
      "content_length": 2037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Further Compaction\n\n// Exercise SUT invoice.changeQuantityForProduct(product, NEW_QUANTITY); // Verify outcome LineItem expected = createLineItem( NEW_QUANTITY, CUST_DISCOUNT_PC, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\nThis test was written in about two minutes and did not require adding any new Test Utility Methods. Contrast that with how long it would have taken to write a completely new test in the original style. And the effort saved in writing the tests is just part of the equation—we also need to consider the effort we saved understanding existing tests each time we need to revisit them. Over the course of a development project and the subsequent maintenance activity, this cost sav- ings will really add up.\n\nFurther Compaction\n\nWriting these additional tests revealed a few more sources of Test Code Duplication (page 213). For example, it seems that we always create both a Customer and an Invoice. Why not combine these two lines? Similarly, we continually deﬁ ne and initialize the QUANTITY and CUSTOMER_DISCOUNT_PC constants inside our test methods. Why can’t we do these tasks just once? The Product does not seem to play any roles in these tests; we always create it exactly the same way. Can we factor this responsibility out, too? Certainly! We just apply an Extract Method refactoring to each set of duplicated code to create more powerful Creation Methods.\n\npublic void testAddItemQuantity_severalQuantity_v15(){ // Set up ﬁxture Invoice invoice = createCustomerInvoice(CUST_DISCOUNT_PC); // Exercise SUT invoice.addItemQuantity(PRODUCT, SEVERAL); // Verify outcome ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE.multiply(new BigDecimal(SEVERAL)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUST_DISCOUNT_PC.movePointLeft(2))); LineItem expected = createLineItem( SEVERAL, CUST_DISCOUNT_PC, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem(invoice, expected); }\n\npublic void testAddLineItem_quantityOne_v2(){ ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE; ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE;\n\nwww.it-ebooks.info\n\nlix",
      "content_length": 2141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "lx\n\nRefactoring a Test\n\n// Set up ﬁxture Invoice invoice = createCustomerInvoice(NO_CUST_DISCOUNT); // Exercise SUT invoice.addItemQuantity(PRODUCT, QUAN_ONE); // Verify outcome LineItem expected = createLineItem( SEVERAL, CUST_DISCOUNT_PC, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\npublic void testChangeQuantity_severalQuantity_V2(){ ﬁnal int NEW_QUANTITY = SEVERAL + 2; ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE.multiply( new BigDecimal(NEW_QUANTITY)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUST_DISCOUNT_PC.movePointLeft(2))); // Set up ﬁxture Invoice invoice = createCustomerInvoice(CUST_DISCOUNT_PC); invoice.addItemQuantity(PRODUCT, SEVERAL); // Exercise SUT invoice.changeQuantityForProduct(PRODUCT, NEW_QUANTITY); // Verify outcome LineItem expected = createLineItem( NEW_QUANTITY, CUST_DISCOUNT_PC, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\nWe have now reduced the number of lines of code we need to understand from 35 statements in the original test to just 6 statements.4 We are left with just a bit more than one sixth of the original code to maintain! We could go further by factoring out the ﬁ xture setup into a setUp method, but that effort would be worthwhile only if a lot of tests needed the same Customer/Discount/Invoice conﬁ guration. If we wanted to reuse these Test Utility Methods from other Testcase Classes (page 373), we could use an Extract Superclass [Fowler] refactoring to create a Testcase Super- class (page 638), and then use a Pull Up Method [Fowler] refactoring to move the Test Utility Methods to it so they can be reused.\n\n4 Ignoring wrapped lines, we have 6 executable statements surrounded by the two lines of method declarations/end.\n\nwww.it-ebooks.info",
      "content_length": 1832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "PART I\n\nThe Narratives\n\nwww.it-ebooks.info",
      "content_length": 42,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Chapter 1\n\nA Brief Tour\n\nAbout This Chapter\n\nThere are a lot of principles, patterns, and smells in this book—and even more pat- terns that couldn’t ﬁ t into the book. Do you need to learn them all? Do you need to use them all? Probably not! This chapter provides an abbreviated introduction to the bulk of the material in the entire book. You can use it as a quick tour of the material before diving into particular patterns or smells of interest. You can also use it as a warm-up before exploring the more detailed narrative chapters.\n\nThe Simplest Test Automation Strategy That Could Possibly Work\n\nThere is a simple test automation strategy that will work for many, many projects. This section describes this minimal test strategy. The principles, pat- terns, and smells referenced here are the core patterns that will serve us well in the long run. If we learn to apply them effectively, we will probably be success- ful in our test automation endeavors. If we ﬁ nd that we really cannot make the minimal test strategy work on our project by using these patterns, we can fall back to the alternative patterns listed in the full descriptions of these patterns and in the other narratives.\n\nI have laid out this simple strategy in ﬁ ve parts:\n\nDevelopment Process: How the process we use to develop the code\n\naffects our tests.\n\nCustomer Tests: The ﬁ rst tests we should write as the ultimate deﬁ ni-\n\ntion of “what done looks like.”\n\n3\n\nwww.it-ebooks.info",
      "content_length": 1459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "4\n\nChapter 1 A Brief Tour\n\nUnit Tests: The tests that help our design emerge incrementally and\n\nensure that all our code is tested.\n\nDesign for Testability: The patterns that make our design easier to test,\n\nthereby reducing the cost of test automation.\n\nTest Organization: How we can organize our Test Methods (page 348)\n\nand Testcase Classes (page 373).\n\nDevelopment Process\n\nFirst things ﬁ rst: When do we write our tests? Writing tests before we write our software has several beneﬁ ts. In particular, it gives us an agreed-upon deﬁ nition of what success looks like.1\n\nWhen doing new software development, we strive to do storytest-driven development by ﬁ rst automating a suite of customer tests that verify the func- tionality provided by the application. To ensure that all of our software is tested, we augment these tests with a suite of unit tests that verify all code paths or, at a minimum, all the code paths that are not covered by the customer tests. We can use code coverage tools to discover which code is not being exercised and then retroﬁ t unit tests to accommodate the untested code.2\n\nBy organizing the unit tests and customer tests into separate test suites, we ensure that we can run just the unit tests or just the customer tests if neces- sary. The unit tests should always pass before we check them in; this is what we mean by the phrase “keep the bar green.” To ensure that the unit tests are run frequently, we can include them in the Smoke Tests [SCM] that are run as part of the Integration Build [SCM]. Although many of the customer tests will fail until the corresponding functionality is built, it is nevertheless useful to run all the passing customer tests as part of the integration build phase—but only if this step does not slow the build down too much. In that case, we can leave them out of the check-in build and simply run them every night.\n\nWe can ensure that our software is testable by doing test-driven development (TDD). That is, we write the unit tests before we write the code, and we use the tests to help us deﬁ ne the software’s design. This strategy helps concentrate all the business logic that needs veriﬁ cation in well-deﬁ ned objects that can be tested independently of the database. Although we should also have unit tests\n\n1 If our customer cannot deﬁ ne the tests before we have built the software, we have every reason to be worried! 2 We will likely ﬁ nd fewer Missing Unit Tests (see Production Bugs on page 268) when we practice test-driven development than if we adopt a “test last” policy. Even so, there is still value in running the code coverage tools with TDD.\n\nwww.it-ebooks.info",
      "content_length": 2654,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "The Simplest Test Automation Strategy That Could Possibly Work\n\nfor the data access layer and the database, we try to keep the dependency on the database to a minimum in the unit tests for the business logic.\n\nCustomer Tests\n\nThe customer tests should capture the essence of what the customer wants the system to do. Enumerating the tests before we begin their development is an important step whether or not we actually automate the tests, because it helps the development team understand what the customer really wants; these tests deﬁ ne what success looks like. We can automate the tests using Scripted Tests (page 285) or Data-Driven Tests (page 288) depending on who is pre- paring the tests; customers can take part in test automation if we use Data- Driven Tests. On rare occasions, we might even use Recorded Tests (page 278) for regression testing an existing application while we refactor the application to improve its testability. Of course, we usually discard these tests once we have developed other tests that cover the functionality, because Recorded Tests tend to be Fragile Tests (page 239).\n\nDuring their development, we strive to make our customer tests represen- tative of how the system is really used. Unfortunately, this goal often conﬂ icts with attempts to keep the tests from becoming too long, because long tests are often Obscure Tests (page 186) and tend not to provide very good Defect Localization (see page 22) when they fail partway through the test. We can also use well-written Tests as Documentation (see page 23) to identify how the system is supposed to work. To keep the tests simple and easy to understand, we can bypass the user interface by performing Subcutaneous Testing (see Layer Test on page 337) against one or more Service Facades [CJ2EEP]. Service Facades encap- sulate all of the business logic behind a simple interface that is also used by the presentation layer.\n\nEvery test needs a starting point. As part of our testing plan, we take care that each test sets up this starting point, known as the test ﬁ xture, each time the test is run. This Fresh Fixture (page 311) helps us avoid Interacting Tests (see Erratic Test on page 228) by ensuring that tests do not depend on anything they did not set up themselves. We avoid using a Shared Fixture (page 317), unless it is an Immutable Shared Fixture, to avoid starting down the slippery slope to Erratic Tests.\n\nIf our application normally interacts with other applications, we may need to isolate it from any applications that we do not have in our development en- vironment by using some form of Test Double (page 522) for the objects that act as interfaces to the other applications. If the tests run too slowly because of database access or other slow components, we can replace them with functionally\n\nwww.it-ebooks.info\n\n5",
      "content_length": 2833,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "6\n\nChapter 1 A Brief Tour\n\nequivalent Fake Objects (page 551) to speed up our tests, thereby encouraging developers to run them more regularly. If at all possible, we avoid using Chained Tests (page 454)—they are just the test smell Interacting Tests in disguise.\n\nUnit Tests\n\nFor our unit tests to be effective, each one should be a Fully Automated Test (page 26) that does a round-trip test against a class through its public interface. We can strive for Defect Localization by ensuring that each test is a Single- Condition Test (see page 45) that exercises a single method or object in a single scenario. We should also write our tests so that each part of the Four-Phase Test (page 358) is easily recognizable, which enables us to use the Tests as Docu- mentation.\n\nWe use a Fresh Fixture strategy so that we do not have to worry about In- teracting Tests or ﬁ xture teardown. We begin by creating a Testcase Class for each class we are testing (see Testcase Class per Class on page 617), with each test being a separate Test Method on that class. Each Test Method can use Del- egated Setup (page 411) to build a Minimal Fixture (page 302) that makes the tests easily understood by calling well-named Creation Methods (page 415) to build the objects required for each test ﬁ xture.\n\nTo make the tests self-checking (Self-Checking Test; see page 26), we express the expected outcome of each test as one or more Expected Objects (see State Veriﬁ cation on page 462) and compare them with the actual objects returned by the system under test (SUT) using the built-in Equality Assertions (see Assertion Method on page 362) or Custom Assertions (page 474) that implement our own test-speciﬁ c equality. If several tests are expected to result in the same outcome, we can factor out the veriﬁ cation logic into an outcome- describing Veriﬁ cation Method (see Custom Assertion) that the test reader can more easily recognize.\n\nIf we have Untested Code (see Production Bugs on page 268) because we cannot ﬁ nd a way to execute the path through the code, we can use a Test Stub (page 529) to gain control of the indirect inputs of the SUT. If there are Untested Requirements (see Production Bugs) because not all of the system’s behavior is observable via its public interface, we can use a Mock Object (page 544) to intercept and verify the indirect outputs of the SUT.\n\nwww.it-ebooks.info",
      "content_length": 2386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "The Simplest Test Automation Strategy That Could Possibly Work\n\nDesign for Testability\n\nAutomated testing is much simpler if we adopt a Layered Architecture [DDD, PEAA, WWW]. At a minimum, we should separate our business logic from the database and the user interface, thereby enabling us to test it easily using either Subcutaneous Tests or Service Layer Tests (see Layer Test). We can minimize any dependence on a Database Sandbox (page 650) by doing most—if not all—of our testing using in-memory objects only. This scheme lets the runtime environ- ment implement Garbage-Collected Teardown (page 500) for us automatically, meaning that we can avoid writing potentially complex, error-prone teardown logic (a sure source of Resource Leakage; see Erratic Test). It also helps us avoid Slow Tests (page 253) by reducing disk I/O, which is much slower than memory manipulation.\n\nIf we are building a GUI, we should try to keep the complex GUI logic out of the visual classes. Using a Humble Dialog (see Humble Object on page 695) that delegates all decision making to nonvisual classes allows us to write unit tests for the GUI logic (e.g., enabling/disabling buttons) without having to instantiate the graphical objects or the framework on which they depend.\n\nIf the application is complex enough or if we are expected to build compo- nents that will be reused by other projects, we can augment the unit tests with component tests that verify the behavior of each component in isolation. We will probably need to use Test Doubles to replace any components on which our component depends. To install the Test Doubles at runtime, we can use either Dependency Injection (page 678), Dependency Lookup (page 686), or a Subclassed Singleton (see Test-Speciﬁ c Subclass on page 579).\n\nTest Organization\n\nIf we end up with too many Test Methods on our Testcase Class, we can con- sider splitting the class based on either the methods (or features) veriﬁ ed by the tests or their ﬁ xture needs. These patterns are called Testcase Class per Fea- ture (page 624) and Testcase Class per Fixture (page 631), respectively. Testcase Class per Fixture allows us to move all of the ﬁ xture setup code into the setUp method, an approach called Implicit Setup (page 424). We can then aggregate the Test Suite Objects (page 387) for the resulting Testcase Classes into a single Test Suite Object, resulting in a Suite of Suites (see Test Suite Object) containing all the tests from the original Testcase Class. This Test Suite Object can, in turn, be added to the Test Suite Object for the containing package or namespace. We can then run all of the tests or just a subset that is relevant to the area of the software in which we are working.\n\nwww.it-ebooks.info\n\n7",
      "content_length": 2746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "8\n\nChapter 1 A Brief Tour\n\nWhat’s Next?\n\nThis whirlwind tour of the most important goals, principles, patterns, and smells is just a brief introduction to test automation. Chapters 2 through 14 give a more detailed overview of each area touched upon here. If you have already spotted some patterns or smells you want to learn more about, you can certainly proceed directly to the detailed descriptions in Parts II and III. Other- wise, your next step is to delve into the subsequent narratives, which provide a somewhat more in-depth examination of these patterns and the alternatives to them. First up is Chapter 2, Test Smells, which describes some common “test smells” that motivate much of the refactoring we do on our tests.\n\nwww.it-ebooks.info",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Chapter 2\n\nTest Smells\n\nAbout This Chapter\n\nChapter 1, A Brief Tour, provided a very quick introduction to the core patterns and smells covered in this book. This chapter provides a more detailed examina- tion of the “test smells” we are likely to encounter on our projects. We explore the basic concept of test smells ﬁ rst, and then move on to investigate the smells in three broad categories: test code smells, automated test behavior smells, and project smells related to automated testing.\n\nAn Introduction to Test Smells\n\nIn his book Refactoring: Improving the Design of Existing Code, Martin Fowler documented a number of ways that the design of code can be changed without actually changing what the code does. The motivation for this refactoring was the identiﬁ cation of “bad smells” that frequently occur in object-oriented code. These code smells were described in a chapter coauthored by Kent Beck that started with the famous quote from Grandma Beck: “If it stinks, change it.” The context of this quote was the question, “How do you know you need to change a baby’s diaper?” And so a new term was added to the programmer’s lexicon.\n\nThe code smells described in Refactoring focused on problems commonly found in production code. Many of us had long suspected that there were smells unique to automated test scripts. At XP2001, the paper “Refactoring Test Code” [RTC] conﬁ rmed these suspicions by identifying a number of “bad smells” that occur speciﬁ cally in test code. The authors also recommended a set of refactorings that can be applied to the tests to remove the noxious smells.\n\nThis chapter provides an overview of these test smells. More detailed ex-\n\namples of each test smell can be found in the reference section.\n\n9\n\nwww.it-ebooks.info",
      "content_length": 1764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "10\n\nChapter 2 Test Smells\n\nWhat’s a Test Smell?\n\nA smell is a symptom of a problem. A smell doesn’t necessarily tell us what is wrong, because a particular smell may originate from any of several sources. Most of the smells in this book have several different named causes; some causes even appear under several smells. That’s because a root cause may reveal itself through several different symptoms (i.e., smells).\n\nNot all problems are considered smells, and some problems may even be the root cause of several smells. The “Occam’s razor” test for deciding whether something really is a smell (versus just a problem) is the “sniffability test.” That is, the smell must grab us by the nose and say, “Something is wrong here.” As discussed in the next section, I have classiﬁ ed the smells based on the kinds of symptoms they exhibit (how they “grab us by the nose”).\n\nBased on the “sniffability” criteria, I have demoted some of the test smells listed in prior papers and articles to “cause” status. I have mostly left their names unchanged so that we can still refer to them when talking about a par- ticular side effect of applying a pattern. In this case, it is more appropriate to refer directly to the cause rather than to the more general but sniffable smell.\n\nKinds of Test Smells\n\nOver the years we have discovered that there are at least two different kinds of smells: code smells, which must be recognized when looking at code, and behav- ior smells, which affect the outcome of tests as they execute.\n\nCode smells are coding-level anti-patterns that a developer, tester, or coach may notice while reading or writing test code. That is, the code just doesn’t look quite right or doesn’t communicate its intent very clearly. Code smells must ﬁ rst be recognized before we can take any action, and the need for action may not be equally obvious to everyone. Code smells apply to all kinds of tests, including both Scripted Tests (page 285) and Recorded Tests (page 278). They become particu- larly relevant for Recorded Tests when we must maintain the recorded code. Un- fortunately, most Recorded Tests suffer from Obscure Tests (page 186), because they are recorded by a tool that doesn’t know what is relevant to the human reader. Behavior smells, by contrast, are much more difﬁ cult to ignore because they cause tests to fail (or not compile at all) at the most inopportune times, such as when we are trying to integrate our code into a crucial build; we are forced to unearth the problems before we can “make the bar green.” Like code smells, behavior smells are relevant to both Scripted Tests and Recorded Tests.\n\nwww.it-ebooks.info",
      "content_length": 2650,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "An Introduction To Test Smells\n\nDevelopers typically notice code and behavior smells when they automate, maintain, and run tests. More recently, we have identiﬁ ed a third kind of smell—a smell that is usually noticed by the project manager or the customer, who does not look at the test code or run the tests. These project smells are in- dicators of the overall health of a project.\n\nWhat to Do about Smells?\n\nSome smells are inevitable simply because they take too much effort to elimi- nate. The important thing is that we are aware of the smells and know what causes them. We can then make a conscious decision about which ones we must address to keep the project running efﬁ ciently.\n\nThe decision of which smells must be eliminated comes down to the balance between cost and beneﬁ t. Some smells are harder to stamp out than others; some smells cause more grief than others. We need to eradicate those smells that cause us the most grief because they will keep us from being suc- cessful. That being said, many smells can be avoided by selecting a sound test automation strategy and by following good test automation coding standards.\n\nWhile we carefully delineated the various types of smells, it is important to note that very often we will observe symptoms of each kind of smell at the same time. Project smells, for example, are the project-level symptoms of some underlying cause. That cause may show up as a behavior smell but ultimately there is probably an underlying code smell that is the root cause of the problem. The good news: We have three different ways to identify a problem. The bad news: It is easy to focus on the symptom at one level and to try to solve that problem directly without understanding the root cause.\n\nA very effective technique for identifying the root cause is the “Five Why’s” [TPS]. First, we ask why something is occurring. Once we have identiﬁ ed the factors that led to it, we next ask why each of those factors occurred. We repeat this process until no new information is forthcoming. In practice, asking why ﬁ ve times is usually enough—hence the name “Five Why’s.”1\n\nIn the rest of this chapter, we will look at the test-related smells that we are most likely to encounter on our projects. We will begin with the project smells, and then work our way down to the behavior smells and code smells that cause them.\n\n1 This practice is also called “root cause analysis” or “peeling the onion” in some circles.\n\nwww.it-ebooks.info\n\n11",
      "content_length": 2480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "12\n\nChapter 2 Test Smells\n\nA Catalog of Smells\n\nNow that we have a better understanding of test smells and their role in projects that use automated testing, let’s look at some smells. Based on the “sniffability” criteria outlined earlier, this section focuses on introducing the smells. Discus- sions of their causes and the individual smell descriptions appear in Part II of this book.\n\nThe Project Smells\n\nProject smells are symptoms that something has gone wrong on the project. Their root cause is likely to be one or more of the code or behavior smells. Because proj- ect managers rarely run or write tests, however, project smells are likely to be the ﬁ rst hint they get that something may be less than perfect in test automation land. Project managers focus most on functionality, quality, resources, and cost. For this reason, the project-level smells tend to cluster around these issues. The most obvious metric a project manager is likely to encounter as a smell is the quality of the software as measured in defects found in formal testing or by users/customers. If the number of Production Bugs (page 268) is higher than expected, the project manager must ask, “Why are all of these bugs getting through our safety net of automated tests?”\n\nThe project manager may be monitoring the number of times the daily in- tegration build fails as a way of getting an early indication of software quality and adherence to the team’s development process. The manager may become worried if the build fails too frequently, and especially if it takes more than a few minutes to ﬁ x the build. Root cause analysis of the failures may indicate that many of the test failures are not the result of buggy software but rather derive from Buggy Tests (page 260). This is an example in which the tests cry “Wolf!” and consume a lot of resources as part of their correction, but do not actually increase the quality of the production code.\n\nBuggy Tests are just one contributor to the more general problem of High Test Maintenance Cost (page 265), which can severely affect the productivity of the team if not addressed quickly. If the tests need to be modiﬁ ed too often (e.g., every time the SUT is modiﬁ ed) or if the cost of modifying tests is too high due to Obscure Tests, the project manager may decide that the effort and ex- pense being directed toward writing the automated tests would be better spent\n\nwww.it-ebooks.info",
      "content_length": 2423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "A Catalog of Smells\n\non writing more production code or doing manual testing. At this point, the manager is likely to tell the developers to stop writing tests.2\n\nAlternatively, the project manager may decide that the Production Bugs are caused by Developers Not Writing Tests (page 263). This pronouncement is likely to come during a process retrospective or as part of a root cause analysis session. Developers Not Writing Tests may be caused by an overly aggressive development schedule, supervisors who tell developers not to “waste time writ- ing tests,” or developers who do not have the skills to write tests. Other poten- tial causes might include an imposed design that is not conducive to testing or a test environment that leads to Fragile Tests (page 239). Finally, this problem could result from Lost Tests (see Production Bugs)—tests that exist but are not included in the AllTests Suite (see Named Test Suite on page 592) used by devel- opers during check-in or by the automated build tool.\n\nThe Behavior Smells\n\nBehavior smells are encountered when we compile or run tests. We don’t have to be particularly observant to notice them, as these smells will take the form of compile errors or test failures.\n\nThe most common behavior smell is Fragile Tests. It arises when tests that once passed begin failing for some reason. The Fragile Test problem has given test automation a bad name in many circles, especially when commercial “record and playback” test tools fail to deliver on their promise of easy test automation. Once recorded, these tests are very susceptible to breakage. Often the only remedy is to rerecord them because the test recordings are difﬁ cult to understand or modify by hand.\n\nThe root causes of Fragile Tests can be classiﬁ ed into four broad categories:\n\nInterface Sensitivity (see Fragile Test) occurs when tests are broken by changes to the test programming API or the user interface used to au- tomate the tests. Commercial Record and Playback Test (see Recorded Test) tools typically interact with the system via the user interface. Even minor changes to the interface can cause tests to fail, even in circum- stances in which a human user would say that the test should still pass.\n\n2 It can be hard enough to get project managers to buy into letting developers write automated tests. It is crucial that we don’t squander this opportunity by being sloppy or inefﬁ cient. The need for this balancing act is, in a nutshell, why I started writing this book: to help developers succeed and avoid giving the pessimistic project manager an excuse for calling a halt to automated unit testing.\n\nwww.it-ebooks.info\n\n13",
      "content_length": 2655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "14\n\nChapter 2 Test Smells\n\nBehavior Sensitivity (see Fragile Test) occurs when tests are broken by changes to the behavior of the SUT. This may seem like a “no-brainer” (of course, the tests should break if we change the SUT!) but the issue is that only a few tests should be broken by any one change. If many or most of the tests break, we have a problem.\n\nData Sensitivity (see Fragile Test) occurs when tests are broken by changes to the data already in the SUT. This issue is particularly a problem for applications that use databases. Data Sensitivity is a spe- cial case of Context Sensitivity (see Fragile Test) where the context in question is the database.\n\nContext Sensitivity occurs when tests are broken by differences in the environment surrounding the SUT. The most common example is when tests depend on the time or date, but this problem can also arise when tests rely on the state of devices such as servers, printers, or monitors.\n\nData Sensitivity and Context Sensitivity are examples of a special kind of Frag- ile Test, known as a Fragile Fixture, in which changes to a commonly used test ﬁ xture cause multiple existing tests to fail. This scenario increases the cost of extending the Standard Fixture (page 305) to support new tests and, in turn, discourages good test coverage. Although Fragile Fixture’s root cause is poor test design, the problem actually appears when the ﬁ xture is changed rather than when the SUT is changed.\n\nMost agile projects use some form of daily or continuous integration that includes two steps: compiling the latest version of the code and running all of the automated tests against the newly compiled build. Assertion Rou- lette (page 224) can make it difﬁ cult to determine how and why tests failed during the integration build because the failure log does not include sufﬁ - cient information to clearly identify which assertion failed. Troubleshooting of the build failures may proceed slowly, because the failure must be repro- duced in the development environment before we can speculate on the cause of the failure.\n\nA common cause of grief is tests that fail for no apparent reason. That is, neither the tests nor the production code has been modiﬁ ed, yet the tests sud- denly begin failing. When we try to reproduce these results in the development environment, the tests may or may not fail. These Erratic Tests (page 228) are both very annoying and time-consuming to ﬁ x, because they have numerous possible causes. A few are listed here:\n\nwww.it-ebooks.info",
      "content_length": 2525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "A Catalog of Smells\n\nInteracting Tests arise when several tests use a Shared Fixture (page 317). They make it hard to run tests individually or to run several test suites as part of a larger Suite of Suites (see Test Suite Object on page 387). They can also cause cascading failures (where a single test failure leaves the Shared Fixture in a state that causes many other tests to fail).\n\nTest Run Wars occur when several Test Runners (page 377) run tests against a Shared Fixture at the same time. They invariably happen at the worst possible time, such as when you are trying to ﬁ x the last few bugs before a release.\n\nUnrepeatable Tests provide a different result between the ﬁ rst and subsequent test runs. They may force the test automater to perform a Manual Intervention (page 250) between test runs.\n\nAnother productivity-sapping smell is Frequent Debugging (page 248). Auto- mated unit tests should obviate the need to use a debugger in all but rare cases, because the set of tests that are failing should make it obvious why the failure is occurring. Frequent Debugging is a sign that the unit tests are lacking in cover- age or are trying to test too much functionality at once.\n\nThe real value of having Fully Automated Tests (page 26) is being able to run them frequently. Agile developers who are doing test-driven development often run (at least a subset of) the tests every few minutes. This behavior should be encouraged because it shortens the feedback loop, thereby reducing the cost of any defects introduced into the code. When tests require Manual Intervention each time they are run, developers tend to run the tests less frequently. This practice increases the cost of ﬁ nding all defects introduced since the tests were last run, because more changes will have been made to the software since it was last tested.\n\nAnother smell that has the same net impact on productivity is Slow Tests (page 253). When tests take more than approximately 30 seconds to run, developers stop running them after every individual code change, instead wait- ing for a “logical time” to run them—for example, before a coffee break, lunch, or a meeting. This delayed feedback results in a loss of “ﬂ ow” and increases the time between when a defect is introduced and when it is identiﬁ ed by a test. The most frequently used solution to Slow Tests is also the most problematic; a Shared Fixture can result in many behavior smells and should be the solution of last resort.\n\nwww.it-ebooks.info\n\n15",
      "content_length": 2499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "16\n\nChapter 2 Test Smells\n\nThe Code Smells\n\nCode smells are the “classic” bad smells that were ﬁ rst described by Martin Fowler in Refactoring [Ref]. Indeed, most of the smells identiﬁ ed by Fowler are code smells. These smells must be recognized by test automaters as they main- tain test code. Although code smells typically affect maintenance cost of tests, they may also be early warning signs of behavior smells to follow.\n\nWhen reading tests, a fairly obvious—albeit often overlooked—smell is Obscure Test. It can take many forms, but all versions have the same impact: It becomes difﬁ cult to tell what the test is trying to do, because the test does not Communicate Intent (page 41). This ambiguity increases the cost of test main- tenance and can lead to Buggy Tests when a test maintainer makes the wrong change to the test.\n\nA related smell is Conditional Test Logic (page 200). Tests should be simple, linear sequences of statements. When tests have multiple execution paths, we cannot be sure exactly how the test will execute in a speciﬁ c case.\n\nHard-Coded Test Data (see Obscure Test) can be insidious for several rea- sons. First, it makes tests more problematic to understand: We need to look at each value and guess whether it is related to any of the other values to under- stand how the SUT is supposed to behave. Second, it creates challenges when we are testing a SUT that includes a database. Hard-Coded Test Data can lead to Erratic Tests (if tests happen to use the same database key) or Fragile Fix- tures (if the values refer to records in the database that have been changed).\n\nHard-to-Test Code (page 209) may be a contributing factor to a number of other code and behavior smells. This problem is most obvious to the person who is writing a test and cannot ﬁ nd a way to set up the ﬁ xture, exercise the SUT, or verify the expected outcome. The test automater may then be forced to test more software (a larger SUT consisting of many classes) than he or she would like. When reading a test, Hard-to-Test Code tends to show up as an Obscure Test because of the hoops the test automater had to jump through to interact with the SUT.\n\nTest Code Duplication (page 213) is a poor practice because it increases the cost of maintaining tests. We have more test code to maintain and that code is more challenging to maintain because it often coincides with an Obscure Test. Duplication often arises when the automated tester clones tests and does not put enough thought into how to reuse test logic intelligently.3 As testing needs emerge, it is important that the test automater factor out commonly used sequences of statements into Test Utility Methods (page 599) that can be reused\n\n3 Note that I said “reuse test logic” and not “reuse Test Methods.”\n\nwww.it-ebooks.info",
      "content_length": 2796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "What’s Next\n\nby various Test Methods (page 348).4 This practice reduces the maintenance cost of tests in several ways.\n\nTest Logic in Production (page 217) is undesirable because there is no way to ensure that it will not run accidentally.5 It also makes the production code larger and more complicated. Finally, this error may cause other software com- ponents or libraries to be included in the executable.\n\nWhat’s Next?\n\nIn this chapter, we saw a plethora of things that can go wrong when automating tests. Chapter 3, Goals of Test Automation, describes the goals we need to keep in mind while automating tests so that we can have an effective test automation experience. That understanding will prepare us to look at the principles that will help us steer clear of many of the problems described in this chapter.\n\n4 It is equally important that we do not reuse Test Methods, as that practice results in Flexible Tests (see Conditional Test Logic). 5 See the sidebar on Ariane (page 218) for a cautionary tale.\n\nwww.it-ebooks.info\n\n17",
      "content_length": 1037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Chapter 3\n\nGoals of Test Automation\n\nAbout This Chapter\n\nChapter 2, Test Smells, introduced the various “test smells” that can act as symptoms of problems with automated testing. This chapter describes the goals we should be striving to reach to ensure successful automated unit tests and customer tests. It begins with a general discussion of why we automate tests, then turns to a description of the overall goals of test automation, including re- ducing costs, improving quality, and improving the understanding of code. Each of these areas has more detailed named goals that are discussed brieﬂ y here as well. This chapter doesn’t describe how to achieve these goals; that explanation will come in subsequent chapters where these goals are used as the rationale for many of the principles and patterns.\n\nWhy Test?\n\nMuch has been written about the need for automated unit and acceptance tests as part of agile software development. Writing good test code is hard, and main- taining obtuse test code is even harder. Because test code is optional (i.e., it is not what the customer is paying for), there is a strong temptation to abandon testing when the tests become difﬁ cult or expensive to maintain. Once we have given up on the principle of “keep the bar green to keep the code clean,” much of the value of the automated tests is lost.\n\nOver a series of projects, the teams I have worked with have faced a number of challenges to automated testing. The cost of writing and maintaining test suites has been a particular challenge, especially on projects with thousands of tests. Fortunately, as the cliché says, “Necessity is the mother of invention.” My teams, and others, have developed a number of solutions to address these challenges. I have since spent a lot of time reﬂ ecting on these solutions to ask why they are good\n\n19\n\nwww.it-ebooks.info",
      "content_length": 1857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "20\n\nChapter 3 Goals of Test Automation\n\nsolutions. Along the way, I have divided the components of successful solutions into goals (things to achieve) and principles (ways to achieve them). Adherence to these goals and principles will result in automated tests that are easier to write, read, and maintain.\n\nEconomics of Test Automation\n\nOf course, there is always a cost incurred in building and maintaining an auto- mated test suite. Ardent test automation advocates will argue that it is worth spending more to have the ability to change the software later. Unfortunately, this “pay me now so you don’t have to pay me later” argument doesn’t go very far in a tough economic climate.1\n\nOur goal should be to make the decision to do test automation a “no-brainer” by ensuring that it does not increase the cost of software development. Thus the additional cost of building and maintaining automated tests must be offset by savings through reduced manual unit testing and debugging/troubleshooting as well as the remediation cost of the defects that would have gone undetected until the formal test phase of the project or early production usage of the application. Figure 3.1 shows how the cost of automation is offset by the savings received from automation.\n\nEffort Effort Spent on Spent on Automating Automating Tests Tests Development Development Effort on Effort on Production Production Code Code\n\nInitial Initial Effort Effort\n\nIncreased Increased Effort Effort (Hump) (Hump)\n\nReduced Reduced Effort Effort\n\nSaved Effort Saved Effort\n\nTime Time\n\nFigure 3.1 An automated unit test project with a good return on investment. The cost-beneﬁ t trade-off when the total cost is reduced by good test practices.\n\nInitially, the cost of learning the new technology and practices takes additional effort. Once we get over this “hump,” however, we should settle down to a steady state where the added cost (the part above the line) is fully offset by the\n\n1 The argument that the quality improvement is worth the extra cost also doesn’t go very far in these days of “just good enough” software quality.\n\nwww.it-ebooks.info",
      "content_length": 2119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Goals of Test Automation\n\nsavings (the part below the line). If tests are difﬁ cult to write, are difﬁ cult to understand, and require frequent, expensive maintenance, the total cost of soft- ware development (the heights of the vertical arrows) goes up as illustrated in Figure 3.2.\n\nEffort Effort Spent on Spent on Automating Automating Tests Tests\n\nDevelopment Development Effort on Effort on Production Production Code Code\n\nInitial Initial Effort Effort\n\nIncreased Increased Effort Effort (Hump) (Hump)\n\nOngoing Ongoing Effort Effort\n\nSaved Effort Saved Effort\n\nTime Time\n\nFigure 3.2 An automated unit test project with a poor return on investment. The cost-beneﬁ t trade-off when the total cost is increased by poor test practices.\n\nNote how the added work above the line in Figure 3.2 is more than that seen in Figure 3.1 and continues to increase over time. Also, the saved effort below the line is reduced. This reﬂ ects the increase in overall effort, which exceeds the original effort without test automation.\n\nGoals of Test Automation\n\nWe all come to test automation with some notion of why having automated tests would be a “good thing.” Here are some high-level objectives that might apply:\n\nTests should help us improve quality.\n\nTests should help us understand the SUT.\n\nTests should reduce (and not introduce) risk.\n\nTests should be easy to run.\n\nTests should be easy to write and maintain.\n\nTests should require minimal maintenance as the system evolves\n\naround them.\n\nwww.it-ebooks.info\n\n21",
      "content_length": 1509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "22\n\nAlso known as: Executable Speciﬁ cation\n\nChapter 3 Goals of Test Automation\n\nThe ﬁ rst three objectives demonstrate the value provided by the tests, whereas the last three objectives focus on the characteristics of the tests themselves. Most of these objectives can be decomposed into more concrete (and measurable) goals. I have given these short catchy names so that I can refer to them as moti- vators of speciﬁ c principles or patterns.\n\nTests Should Help Us Improve Quality\n\nThe traditional reason given for doing testing is for quality assurance (QA). What, precisely, do we mean by this? What is quality? Traditional deﬁ nitions distinguish two main categories of quality based on the following questions: (1) Is the software built correctly? and (2) Have we built the right software?\n\nGoal: Tests as Speciﬁ cation\n\nIf we are doing test-driven development or test-ﬁ rst development, the tests give us a way to capture what the SUT should be doing before we start building it. They enable us to specify the behavior in various scenarios captured in a form that we can then execute (essentially an “executable speciﬁ cation”). To ensure that we are “building the right software,” we must ensure that our tests reﬂ ect how the SUT will actually be used. This effort can be facilitated by developing user interface mockups that capture just enough detail about how the applica- tion appears and behaves so that we can write our tests.\n\nThe very act of thinking through various scenarios in enough detail to turn them into tests helps us identify those areas where the requirements are ambigu- ous or self-contradictory. Such analysis improves the quality of the speciﬁ ca- tion, which improves the quality of the software so speciﬁ ed.\n\nGoal: Bug Repellent\n\nYes, tests ﬁ nd bugs—but that really isn’t what automated testing is about. Auto- mated testing tries to prevent bugs from being introduced. Think of automated tests as “bug repellent” that keeps nasty little bugs from crawling back into our software after we have made sure it doesn’t contain any bugs. Wherever we have regression tests, we won’t have bugs because the tests will point the bugs out before we even check in our code. (We are running all the tests before every check-in, aren’t we?)\n\nGoal: Defect Localization\n\nMistakes happen! Of course, some mistakes are much more expensive to pre- vent than to ﬁ x. Suppose a bug does slip through somehow and shows up in\n\nwww.it-ebooks.info",
      "content_length": 2459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Goals of Test Automation\n\nthe Integration Build [SCM]. If our unit tests are fairly small (i.e., we test only a single behavior in each one), we should be able to pinpoint the bug quickly based on which test fails. This speciﬁ city is one of the major advantages that unit tests enjoy over customer tests. The customer tests tell us that some behavior expected by the customer isn’t working; the unit tests tell us why. We call this phenomenon Defect Localization. If a customer test fails but no unit tests fail, it indicates a Missing Unit Test (see Production Bugs on page 268).\n\nAll of these beneﬁ ts are wonderful—but we cannot achieve them if we don’t write tests for all possible scenarios that each unit of software needs to cover. Nor will we realize these beneﬁ ts if the tests themselves contain bugs. Clearly, it is crucial that we keep the tests as simple as possible so that they can be easily seen to be correct. While writing unit tests for our unit tests is not a practical solution, we can—and should—write unit tests for any Test Utility Method (page 599) to which we delegate complex algorithms needed by the test methods.\n\nTests Should Help Us Understand the SUT\n\nRepelling bugs isn’t the only thing the tests can do for us. They can also show the test reader how the code is supposed to work. Black box component tests are—in effect—describing the requirements of that of software component.\n\nGoal: Tests as Documentation\n\nWithout automated tests, we would need to pore over the SUT code trying to answer the question, “What should be the result if . . . ?” With automated tests, we simply use the corresponding Tests as Documentation; they tell us what the result should be (recall that a Self-Checking Test states the expected outcome in one or more assertions). If we want to know how the system does something, we can turn on the debugger, run the test, and single-step through the code to see how it works. In this sense, the automated tests act as a form of documentation for the SUT.\n\nTests Should Reduce (and Not Introduce) Risk\n\nAs mentioned earlier, tests should improve the quality of our software by help- ing us better document the requirements and prevent bugs from creeping in dur- ing incremental development. This is certainly one form of risk reduction. Other forms of risk reduction involve verifying the software’s behavior in the “impos- sible” circumstances that cannot be induced when doing traditional customer testing of the entire application as a black box. It is a very useful exercise to\n\nwww.it-ebooks.info\n\n23",
      "content_length": 2562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "24\n\nAlso known as: Safety Net\n\nAlso known as: No Test Risk\n\nChapter 3 Goals of Test Automation\n\nreview all of the project’s risks and brainstorm about which kinds of risks could be at least partially mitigated through the use of Fully Automated Tests.\n\nGoal: Tests as Safety Net\n\nWhen working on legacy code, I always feel nervous. By deﬁ nition, legacy code doesn’t have a suite of automated regression tests. Changing this kind of code is risky because we never know what we might break, and we have no way of know- ing whether we have broken something! As a consequence, we must work very slowly and carefully, doing a lot of manual analysis before making any changes.\n\nWhen working with code that has a regression test suite, by contrast, we can work much more quickly. We can adopt a more experimental style of changing the software: “I wonder what would happen if I changed this? Which tests fail? Interesting! So that’s what this parameter is for.” In this way, the automated tests act as a safety net that allows us to take chances.2\n\nThe effectiveness of the safety net is determined by how completely our tests verify the behavior of the system. Missing tests are like holes in the safety net. Incomplete assertions are like broken strands. Each gap in the safety net can let bugs of various sizes through.\n\nThe effectiveness of the safety net is ampliﬁ ed by the version-control capabil- ities of modern software development environments. A source code repository [SCM] such as CVS, Subversion, or SourceSafe lets us roll back our changes to a known point if our tests suggest that the current set of changes is affecting the code too extensively. The built-in “undo” or “local history” features of the IDE let us turn the clock back 5 seconds, 5 minutes, or even 5 hours.\n\nGoal: Do No Harm\n\nNaturally, there is a ﬂ ip side to this discussion: How might automated tests in- troduce risk? We must be careful not to introduce new kinds of problems into the SUT as a result of doing automated testing. The Keep Test Logic Out of Production Code principle directs us to avoid putting test-speciﬁ c hooks into the SUT. It is certainly desirable to design the system for testability, but any test- speciﬁ c code should be plugged in by the test and only in the test environment; it should not exist in the SUT when it is in production.\n\nAnother form of risk is believing that some code is reliable because it has been thoroughly tested when, in fact, it has not. A common mistake made by developers new to the use of Test Doubles (page 522) is replacing too much of\n\n2 Imagine trying to learn to be a trapeze artist in the circus without having that big net that allows you to make mistakes. You would never progress beyond swinging back and forth!\n\nwww.it-ebooks.info",
      "content_length": 2773,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Goals of Test Automation\n\nthe SUT with a Test Double. This leads to another important principle: Don’t Modify the SUT. That is, we must be clear about which SUT we are testing and avoid replacing the parts we are testing with test-speciﬁ c logic (Figure 3.3).\n\nUnit1 Unit1 Test Test\n\nExercise Exercise\n\nUnit1 Unit1 SUT SUT\n\nUnit2 Unit2 Test Test\n\nExercise Exercise\n\nUnit2 Unit2 SUT SUT\n\nuses uses\n\nComp1 Comp1 Test Test\n\nExercise Exercise\n\nComp1 Comp1 SUT SUT\n\nComp2 Comp2 Test Test\n\nExercise Exercise\n\nComp2 Comp2 SUT SUT\n\nuses uses\n\nApp1 App1 Test Test\n\nExercise Exercise\n\nApp1 App1 SUT SUT\n\nFigure 3.3 A range of tests, each with its own SUT. An application, component, or unit is only the SUT with respect to a speciﬁ c set of tests. The “Unit1 SUT” plays the role of DOC (part of the ﬁ xture) to the “Unit2 Test” and is part of the “Comp1 SUT.”\n\nTests Should Be Easy to Run\n\nMost software developers just want to write code; testing is simply a necessary evil in our line of work. Automated tests provide a nice safety net so that we can write code more quickly,3 but we will run the automated tests frequently only if they are really easy to run.\n\nWhat makes tests easy to run? Four speciﬁ c goals answer this question:\n\nThey must be Fully Automated Tests so they can be run without any\n\neffort.\n\n3 “With less paranoia” is probably more accurate!\n\nwww.it-ebooks.info\n\n25",
      "content_length": 1376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "26\n\nChapter 3 Goals of Test Automation\n\nThey must be Self-Checking Tests so they can detect and report any\n\nerrors without manual inspection.\n\nThey must be Repeatable Tests so they can be run multiple times with\n\nthe same result.\n\nIdeally, each test should be an Independent Test that can be run by itself.\n\nWith these four goals satisﬁ ed, one click of a button (or keyboard shortcut) is all it should take to get the valuable feedback the tests provide. Let’s look at these goals in a bit more detail.\n\nGoal: Fully Automated Test\n\nA test that can be run without any Manual Intervention (page 250) is a Fully Automated Test. Satisfying this criterion is a prerequisite to meeting many of the other goals. Yes, it is possible to write Fully Automated Tests that don’t check the results and that can be run only once. The main() program that runs the code and directs print statements to the console is a good example of such a test. I consider these two aspects of test automation to be so important in making tests easy to run that I have made them separate goals: Self-Checking Test and Repeatable Test.\n\nGoal: Self-Checking Test\n\nA Self-Checking Test has encoded within it everything that the test needs to verify that the expected outcome is correct. Self-Checking Tests apply the Holly- wood principle (“Don’t call us; we’ll call you”) to running tests. That is, the Test Runner (page 377) “calls us” only when a test did not pass; as a consequence, a clean test run requires zero manual effort. Many members of the xUnit fam- ily provide a Graphical Test Runner (see Test Runner) that uses a green bar to signal that everything is “A-okay”; a red bar indicates that a test has failed and warrants further investigation.\n\nGoal: Repeatable Test\n\nA Repeatable Test can be run many times in a row and will produce exactly the same results without any human intervention between runs. Unrepeatable Tests (see Erratic Test on page 228) increase the overhead of running tests signiﬁ cantly. This outcome is very undesirable because we want all developers to be able to run the tests very frequently—as often as after every “save.” Unrepeatable Tests can be run only once before whoever is running the tests must perform a Manual Interven- tion. Just as bad are Nondeterministic Tests (see Erratic Test) that produce different results at different times; they force us to spend lots of time chasing down failing tests. The power of the red bar diminishes signiﬁ cantly when we see it regularly\n\nwww.it-ebooks.info",
      "content_length": 2511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Goals of Test Automation\n\nwithout good reason. All too soon, we begin ignoring the red bar, assuming that it will go away if we wait long enough. Once this happens, we have lost a lot of the value of our automated tests, because the feedback indicating that we have intro- duced a bug and should ﬁ x it right away disappears. The longer we wait, the more effort it takes to ﬁ nd the source of the failing test.\n\nTests that run only in memory and that use only local variables or ﬁ elds are usually repeatable without us expending any additional effort. Unrepeatable Tests usually come about because we are using a Shared Fixture (page 317) of some sort (this deﬁ nition includes any persistence of data implemented within the SUT). In such a case, we must ensure that our tests are “self-cleaning” as well. When cleaning is necessary, the most consistent and foolproof strategy is to use a generic Automated Teardown (page 503) mechanism. Although it is possible to write teardown code for each test, this approach can result in Erratic Tests when it is not implemented correctly in every test.\n\nTests Should Be Easy to Write and Maintain\n\nCoding is a fundamentally difﬁ cult activity because we must keep a lot of in- formation in our heads as we work. When we are writing tests, we should stay focused on testing rather than coding of the tests. This means that tests must be simple—simple to read and simple to write. They need to be simple to read and understand because testing the automated tests themselves is a complicated endeavor. They can be tested properly only by introducing the very bugs that they are intended to detect into the SUT; this is hard to do in an automated way so it is usually done only once (if at all), when the test is ﬁ rst written. For these reasons, we need to rely on our eyes to catch any problems that creep into the tests, and that means we must keep the tests simple enough to read quickly.\n\nOf course, if we are changing the behavior of part of the system, we should expect a small number of tests to be affected by our modiﬁ cations. We want to Minimize Test Overlap so that only a few tests are affected by any one change. Contrary to popular opinion, having more tests pass through the same code doesn’t improve the quality of the code if most of the tests do exactly the same thing.\n\nTests become complicated for two reasons:\n\nWe try to verify too much functionality in a single test.\n\nToo large an “expressiveness gap” separates the test scripting language (e.g., Java) and the before/after relationships between domain concepts that we are trying to express in the test.\n\nwww.it-ebooks.info\n\n27",
      "content_length": 2641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "28\n\nChapter 3 Goals of Test Automation\n\nGoal: Simple Tests\n\nTo avoid “biting off more than they can chew,” our tests should be small and test one thing at a time. Keeping tests simple is particularly important during test- driven development because code is written to pass one test at a time and we want each test to introduce only one new bit of behavior into the SUT. We should strive to Verify One Condition per Test by creating a separate Test Method (page 348) for each unique combination of pre-test state and input. Each Test Method should drive the SUT through a single code path.4\n\nThe major exception to the mandate to keep Test Methods short occurs with customer tests that express real usage scenarios of the application. Such extend- ed tests offer a useful way to document how a potential user of the software would go about using it; if these interactions involve long sequences of steps, the Test Methods should reﬂ ect this reality.\n\nGoal: Expressive Tests\n\nThe “expressiveness gap” can be addressed by building up a library of Test Utility Methods that constitute a domain-speciﬁ c testing language. Such a col- lection of methods allows test automaters to express the concepts that they wish to test without having to translate their thoughts into much more detailed code. Creation Methods (page 415) and Custom Assertion (page 474) are good examples of the building blocks that make up such a Higher-Level Language.\n\nThe key to solving this dilemma is avoiding duplication within tests. The DRY principle—“Don’t repeat yourself”—of the Pragmatic Programmers (http://www. pragmaticprogrammer.com) should be applied to test code in the same way it is applied to production code. There is, however, a counterforce at play. Because the tests should Communicate Intent, it is best to keep the core test logic in each Test Method so it can be seen in one place. Nevertheless, this idea doesn’t pre- clude moving a lot of supporting code into Test Utility Methods, where it needs to be modiﬁ ed in only one place if it is affected by a change in the SUT.\n\nGoal: Separation of Concerns\n\nSeparation of Concerns applies in two dimensions: (1) We want to keep test code separate from our production code (Keep Test Logic Out of Production Code) and (2) we want each test to focus on a single concern (Test Concerns Separately) to avoid Obscure Tests (page 186). A good example of what not to do is testing the business logic in the same tests as the user interface, because it involves testing\n\n4 There should be at least one Test Method for each unique path through the code; often there will be several, one for each boundary value of the equivalence class.\n\nwww.it-ebooks.info",
      "content_length": 2689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "What’s Next?\n\ntwo concerns at the same time. If either concern is modiﬁ ed (e.g., the user inter- face changes), all the tests would need to be modiﬁ ed as well. Testing one concern at a time may require separating the logic into different components. This is a key aspect of design for testability, a consideration that is explored further in Chapter 11, Using Test Doubles.\n\nTests Should Require Minimal Maintenance as the System Evolves Around Them\n\nChange is a fact of life. Indeed, we write automated tests mostly to make change easier, so we should strive to ensure that our tests don’t inadvertently make change more difﬁ cult.\n\nSuppose we want to change the signature of some method on a class. When we add a new parameter, suddenly 50 tests no longer compile. Does that result en- courage us to make the change? Probably not. To counter this problem, we intro- duce a new method with the parameter and arrange to have the old method call the new method, defaulting the missing parameter to some value. Now all of the tests compile but 30 of them still fail! Are the tests helping us make the change?\n\nGoal: Robust Test\n\nInevitably, we will want to make many kinds of changes to the code as a project unfolds and its requirements evolve. For this reason, we want to write our tests in such a way that the number of tests affected by any one change is quite small. That means we need to minimize overlap between tests. We also need to ensure that changes to the test environment don’t affect our tests; we do this by isolat- ing the SUT from the environment as much as possible. This results in much more Robust Tests.\n\nWe should strive to Verify One Condition per Test. Ideally, only one kind of change should cause a test to require maintenance. System changes that affect ﬁ xture setup or teardown code can be encapsulated behind Test Utility Methods to further reduce the number of tests directly affected by the change.\n\nWhat’s Next?\n\nThis chapter discussed why we have automated tests and speciﬁ c goals we should try to achieve when writing Fully Automated Tests. Before moving on to Chapter 5, Principles of Test Automation, we need to take a short side-trip to Chapter 4, Philosophy of Test Automation, to understand the different mindsets of various kinds of test automaters.\n\nwww.it-ebooks.info\n\n29",
      "content_length": 2316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Chapter 4\n\nPhilosophy of Test Automation\n\nAbout This Chapter\n\nChapter 3, Goals of Test Automation, described many of the goals and beneﬁ ts of having an effective test automation program in place. This chapter introduces some differences in the way people think about design, construction, and testing that change the way they might naturally apply these patterns. The “big picture” questions include whether we write tests ﬁ rst or last, whether we think of them as tests or examples, whether we build the software from the inside-out or from the outside-in, whether we verify state or behavior, and whether we design the ﬁ xture upfront or test by test.\n\nWhy Is Philosophy Important?\n\nWhat’s philosophy got to do with test automation? A lot! Our outlook on life (and testing) strongly affects how we go about automating tests. When I was discussing an early draft of this book with Martin Fowler (the series editor), we came to the conclusion that there were philosophical differences between how different people approached xUnit-based test automation. These differences lie at the heart of why, for example, some people use Mock Objects (page 544) sparingly and others use them everywhere.\n\nSince that eye-opening discussion, I have been on the lookout for other phil- osophical differences among test automaters. These alternative viewpoints tend to come up as a result of someone saying, “I never (ﬁ nd a need to) use that pat- tern” or “I never run into that smell.” By questioning these statements, I can learn a lot about the testing philosophy of the speaker. Out of these discussions have come the following philosophical differences:\n\n31\n\nwww.it-ebooks.info",
      "content_length": 1669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "32\n\nChapter 4 Philosophy of Test Automation\n\n“Test after” versus “test ﬁ rst”\n\nTest-by-test versus test all-at-once\n\n“Outside-in” versus “inside-out” (applies independently to design and\n\ncoding)\n\nBehavior veriﬁ cation versus state veriﬁ cation\n\n“Fixture designed test-by-test” versus “big ﬁ xture design upfront”\n\nSome Philosophical Differences\n\nTest First or Last?\n\nTraditional software development prepares and executes tests after all software is designed and coded. This order of steps holds true for both customer tests and unit tests. In contrast, the agile community has made writing the tests ﬁ rst the standard way of doing things. Why is the order in which testing and develop- ment take place important? Anyone who has tried to retroﬁ t Fully Automated Tests (page 22) onto a legacy system will tell you how much more difﬁ cult it is to write automated tests after the fact. Just having the discipline to write auto- mated unit tests after the software is “already ﬁ nished” is challenging, whether or not the tests themselves are easy to construct. Even if we design for testability, the likelihood that we can write the tests easily and naturally without modifying the production code is low. When tests are written ﬁ rst, however, the design of the system is inherently testable.\n\nWriting the tests ﬁ rst has some other advantages. When tests are written ﬁ rst and we write only enough code to make the tests pass, the production code tends to be more minimalist. Functionality that is optional tends not to be written; no extra effort goes into fancy error-handling code that doesn’t work. The tests tend to be more robust because only the necessary methods are provided on each object based on the tests’ needs.\n\nAccess to the state of the object for the purposes of ﬁ xture setup and result veriﬁ cation comes much more naturally if the software is written “test ﬁ rst.” For example, we may avoid the test smell Sensitive Equality (see Fragile Test on page 239) entirely because the correct attributes of objects are used in assertions rather than comparing the string representations of those objects. We may even ﬁ nd that we don’t need to implement a String representation at all because we\n\nwww.it-ebooks.info",
      "content_length": 2231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Some Philosophical Differences\n\nhave no real need for it. The ability to substitute dependencies with Test Doubles (page 522) for the purpose of verifying the outcome is also greatly enhanced be- cause substitutable dependency is designed into the software from the start.\n\nTests or Examples?\n\nWhenever I mention the concept of writing automated tests for software before the software has been written, some listeners get strange looks on their faces. They ask, “How can you possibly write tests for software that doesn’t exist?” In these cases, I follow Brian Marrick’s lead by reframing the discussion to talk about “examples” and example-driven development (EDD). It seems that examples are much easier for some people to envision writing before code than are “tests.” The fact that the examples are executable and reveal whether the requirements have been satisﬁ ed can be left for a later discussion or a discussion with people who have a bit more imagination.\n\nBy the time this book is in your hands, a family of EDD frameworks is likely to have emerged. The Ruby-based RSpec kicked off the reframing of TDD to EDD, and the Java-based JBehave followed shortly thereafter. The basic design of these “unit test frameworks” is the same as xUnit but the terminology has changed to reﬂ ect the Executable Speciﬁ cation (see Goals of Test Automation on page 21) mindset.\n\nAnother popular alternative for specifying components that contain business logic is to use Fit tests. These will invariably be more readable by nontechnical people than something written in a programming language regardless of how “business friendly” we make the programming language syntax!\n\nTest-by-Test or Test All-at-Once?\n\nThe test-driven development process encourages us to “write a test” and then “write some code” to pass that test. This process isn’t a case of all tests being written before any code, but rather the writing of tests and code being inter- leaved in a very ﬁ ne-grained way. “Test a bit, code a bit, test a bit more”—this is incremental development at its ﬁ nest. Is this approach the only way to do things? Not at all! Some developers prefer to identify all tests needed by the current feature before starting any coding. This strategy enables them to “think like a client” or “think like a tester” and lets developers avoid being sucked into “solution mode” too early.\n\nTest-driven purists argue that we can design more incrementally if we build the software one test at a time. “It’s easier to stay focused if only a single test is failing,” they say. Many test drivers report not using the debugger very much\n\nwww.it-ebooks.info\n\n33",
      "content_length": 2635,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "34\n\nChapter 4 Philosophy of Test Automation\n\nbecause the ﬁ ne-grained testing and incremental development leave little doubt about why tests are failing; the tests provide Defect Localization (see Goals of Test Automation on page 22) while the last change we made (which caused the problem) is still fresh in our minds.\n\nThis consideration is especially relevant when we are talking about unit tests because we can choose when to enumerate the detailed requirements (tests) of each object or method. A reasonable compromise is to identify all unit tests at the beginning of a task—possibly roughing in empty Test Method (page 348) skeletons, but coding only a single Test Method body at a time. We could also code all Test Method bodies and then disable all but one of the tests so that we can focus on building the production code one test at a time.\n\nWith customer tests, we probably don’t want to feed the tests to the devel- oper one by one within a user story. Therefore, it makes sense to prepare all the tests for a single story before we begin development of that story. Some teams prefer to have the customer tests for the story identiﬁ ed—although not neces- sarily ﬂ eshed out—before they are asked to estimate the effort needed to build the story, because the tests help frame the story.\n\nOutside-In or Inside-Out?\n\nDesigning the software from the outside inward implies that we think ﬁ rst about black-box customer tests (also known as storytests) for the entire system and then think about unit tests for each piece of software we design. Along the way, we may also implement component tests for the large-grained components we decide to build.\n\nEach of these sets of tests inspires us to “think like the client” well before we start thinking like a software developer. We focus ﬁ rst on the interface provided to the user of the software, whether that user is a person or another piece of software. The tests capture these usage patterns and help us enumerate the vari- ous scenarios we need to support. Only when we have identiﬁ ed all the tests are we “ﬁ nished” with the speciﬁ cation. Some people prefer to design outside-in but then code inside-out to avoid dealing with the “dependency problem.” This tactic requires anticipating the needs of the outer software when writing the tests for the inner software. It also means that we don’t actually test the outer software in isolation from the inner software. Figure 4.1 illustrates this concept. The top-to-bottom progression in the diagram implies the order in which we write the software. Tests for the middle and lower classes can take advantage of the already-built classes above them—a strategy that avoids the need for Test Stubs (page 529) or Mock Objects in many of the tests. We may still need to use Test Stubs in those tests where the inner components could potentially return\n\nwww.it-ebooks.info",
      "content_length": 2877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Some Philosophical Differences\n\nspeciﬁ c values or throw exceptions, but cannot be made to do so on cue. In such a case, a Saboteur (see Test Stub) comes in very handy.\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nUses Uses\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nUses Uses\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nFigure 4.1 “Inside-out” development of functionality. Development starts with the innermost components and proceeds toward the user interface, building on the previously constructed components.\n\nOther test drivers prefer to design and code from the outside-in. Writing the code outside-in forces us to deal with the “dependency problem.” We can use Test Stubs to stand in for the software we haven’t yet written, so that the outer layer of software can be executed and tested. We can also use Test Stubs to inject “impossible” indirect inputs (return values, out parameters, or exceptions) into the SUT to verify that it handles these cases correctly.\n\nIn Figure 4.2, we have reversed the order in which we build our classes. Be- cause the subordinate classes don’t exist yet, we used Test Doubles to stand in for them.\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nTest Test Double Double\n\nUses Uses\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nTest Test Double Double\n\nUses Uses\n\nTest Test\n\nExercise Exercise\n\nSUT SUT\n\nFigure 4.2 “Outside-in” development of functionality supported by Test Doubles. Development starts at the outside using Test Doubles in place of the depended-on components (DOCs) and proceeds inward as requirements for each DOC are identiﬁ ed.\n\nwww.it-ebooks.info\n\n35",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "36\n\nChapter 4 Philosophy of Test Automation\n\nOnce the subordinate classes have been built, we could remove the Test Doubles from many of the tests. Keeping them provides better Defect Localization at the cost of potentially higher test maintenance cost.\n\nState or Behavior Veriﬁ cation?\n\nFrom writing code outside-in, it is but a small step to verifying behavior rather than just state. The “statist” view suggests that it is sufﬁ cient to put the SUT into a speciﬁ c state, exercise it, and verify that the SUT is in the expected state at the end of the test. The “behaviorist” view says that we should specify not only the start and end states of the SUT, but also the calls the SUT makes to its dependencies. That is, we should specify the details of the calls to the “outgoing interfaces” of the SUT. These indirect outputs of the SUT are outputs just like the values returned by functions, except that we must use special measures to trap them because they do not come directly back to the client or test.\n\nThe behaviorist school of thought is sometimes called behavior-driven development. It is evidenced by the copious use of Mock Objects or Test Spies (page 538) throughout the tests. Behavior verification does a better job of testing each unit of software in isolation, albeit at a possible cost of more difficult refactoring. Martin Fowler provides a detailed discussion of the statist and behaviorist approaches in [MAS].\n\nFixture Design Upfront or Test-by-Test?\n\nIn the traditional test community, a popular approach is to deﬁ ne a “test bed” consisting of the application and a database already populated with a variety of test data. The content of the database is carefully designed to allow many differ- ent test scenarios to be exercised.\n\nWhen the ﬁ xture for xUnit tests is approached in a similar manner, the test automater may deﬁ ne a Standard Fixture (page 305) that is then used for all the Test Methods of one or more Testcase Classes (page 373). This ﬁ xture may be set up as a Fresh Fixture (page 311) in each Test Method using Delegated Setup (page 411) or in the setUp method using Implicit Setup (page 424). Alter- natively, it can be set up as a Shared Fixture (page 317) that is reused by many tests. Either way, the test reader may ﬁ nd it difﬁ cult to determine which parts of the ﬁ xture are truly pre-conditions for a particular Test Method.\n\nThe more agile approach is to custom design a Minimal Fixture (page 302) for each Test Method. With this perspective, there is no “big ﬁ xture design up- front” activity. This approach is most consistent with using a Fresh Fixture.\n\nwww.it-ebooks.info",
      "content_length": 2630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "What’s Next?\n\nWhen Philosophies Differ\n\nWe cannot always persuade the people we work with to adopt our philosophy, of course. Even so, understanding that others subscribe to a different philosophy helps us appreciate why they do things differently. It’s not that these individuals don’t share the same goals as ours;1 it’s just that they make the decisions about how to achieve those goals using a different philosophy. Understanding that dif- ferent philosophies exist and recognizing which ones we subscribe to are good ﬁ rst steps toward ﬁ nding some common ground between us.\n\nMy Philosophy\n\nIn case you were wondering what my personal philosophy is, here it is:\n\nWrite the tests ﬁ rst!\n\nTests are examples!\n\nI usually write tests one at a time, but sometimes I list all the tests I can\n\nthink of as skeletons upfront.\n\nOutside-in development helps clarify which tests are needed for the\n\nnext layer inward.\n\nI use primarily State Veriﬁ cation (page 462) but will resort to Behavior\n\nVeriﬁ cation (page 468) when needed to get good code coverage.\n\nI perform ﬁ xture design on a test-by-test basis.\n\nThere! Now you know where I’m coming from.\n\nWhat’s Next?\n\nThis chapter introduced the philosophies that anchor software design, construc- tion, testing, and test automation. Chapter 5, Principles of Test Automation, describes key principles that will help us achieve the goals described in Chapter 3, Goals of Test Automation. We will then be ready to start looking at the over- all test automation strategy and the individual patterns.\n\n1 For example, high-quality software, ﬁ t for purpose, on time, under budget.\n\nwww.it-ebooks.info\n\n37",
      "content_length": 1642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Chapter 5\n\nPrinciples of Test Automation\n\nAbout This Chapter\n\nChapter 3, Goals of Test Automation, described the goals we should strive to achieve to help us be successful at automating our unit tests and customer tests. Chapter 4, Philosophy of Test Automation, discussed some of the differences in the way people approach software design, construction, and testing. This pro- vides the background for the principles that experienced test automaters follow while automating their tests. I call them “principles” for two reasons: They are too high level to be patterns and they represent a value system that not everyone will share. A different value system may cause you to choose different patterns than the ones presented in this book. Making this value system explicit will, I hope, accelerate the process of understanding where we disagree and why.\n\nThe Principles\n\nWhen Shaun Smith and I came up with the list in the original Test Automation Manifesto [TAM], we considered what was driving us to write tests the way we did. The Manifesto is a list of the qualities we would like to see in a test—not a set of patterns that can be directly applied. However, those principles have led us to identify a number of somewhat more concrete principles, some of which are described in this chapter. What makes these principles different from the goals is that there is more debate about them.\n\nPrinciples are more “prescriptive” than patterns and higher level in nature. Un- like patterns, they don’t have alternatives, but rather are presented in a “do this because” fashion. To distinguish them from patterns, I have given them imperative names rather than the noun-phrase names I use for goals, patterns, and smells.\n\n39\n\nwww.it-ebooks.info",
      "content_length": 1740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "40\n\nAlso known as: Test-Driven Development, Test-First Development\n\nAlso known as: Front Door First\n\nChapter 5 Principles of Test Automation\n\nFor the most part, these principles apply equally well to unit tests and story- tests. A possible exception is the principle Verify One Condition per Test, which may not be practical for customer tests that exercise more involved chunks of functionality. It is, however, still worth striving to follow these principles and to deviate from them only when you are fully cognizant of the consequences.\n\nPrinciple: Write the Tests First\n\nTest-driven development is very much an acquired habit. Once one has “gotten the hang of it,” writing code in any other way can seem just as strange as TDD seems to those who have never done it. There are two major arguments in favor of doing TDD:\n\n1. The unit tests save us a lot of debugging effort—effort that often fully\n\noffsets the cost of automating the tests.\n\n2. Writing the tests before we write the code forces the code to be designed for testability. We don’t need to think about testability as a separate design condition; it just happens because we have written tests.\n\nPrinciple: Design for Testability\n\nGiven the last principle, this principle may seem redundant. For developers who choose to ignore Write the Tests First, Design for Testability becomes an even more important principle because they won’t be able to write automated tests after the fact if the testability wasn’t designed in. Anyone who has tried to retroﬁ t automated unit tests onto legacy software can testify to the difﬁ culty this raises. Mike Feathers talks about special techniques for introducing tests in this case in [WEwLC].\n\nPrinciple: Use the Front Door First\n\nObjects have several kinds of interfaces. There is the “public” interface that clients are expected to use. There may also be a “private” interface that only close friends should use. Many objects also have an “outgoing interface” consisting of the used part of the interfaces of any objects on which they depend.\n\nThe types of interfaces we use inﬂ uence the robustness of our tests. The use of Back Door Manipulation (page 327) to set up the ﬁ xture or verify the expected outcome or a test can result in Overcoupled Software (see Fragile Test on page 239) that needs more frequent test maintenance. Overuse of Behavior Veriﬁ ca- tion (page 468) and Mock Objects (page 544) can result in Overspeciﬁ ed Software (see Fragile Test) and tests that are more brittle and may discourage developers from doing desirable refactorings.\n\nwww.it-ebooks.info",
      "content_length": 2581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "The Principles\n\nWhen all choices are equally effective, we should use round-trip tests to test our SUT. To do so, we test an object through its public interface and use State Veriﬁ cation (page 462) to determine whether it behaved correctly. If this is not suf- ﬁ cient to accurately describe the expected behavior, we can make our tests layer- crossing tests and use Behavior Veriﬁ cation to verify the calls the SUT makes to depended-on components (DOCs). If we must replace a slow or unavailable DOC with a faster Test Double (page 522), using a Fake Object (page 551) is preferable because it encodes fewer assumptions into the test (the only assumption is that the component that the Fake Object replaces is actually needed).\n\nPrinciple: Communicate Intent\n\nFully Automated Tests, especially Scripted Tests (page 285), are programs. They need to be syntactically correct to compile and semantically correct to run success- fully. They need to implement whatever detailed logic is required to put the SUT into the appropriate starting state and to verify that the expected outcome has occurred. While these characteristics are necessary, they are not sufﬁ cient because they neglect the single most important interpreter of the tests: the test maintainer.\n\nTests that contain a lot of code1 or Conditional Test Logic (page 200) are usually Obscure Tests (page 186). They are much harder to understand because we need to infer the “big picture” from all the details. This reverse engineering of meaning takes extra time whenever we need to revisit the test either to main- tain it or to use the Tests as Documentation. It also increases the cost of owner- ship of the tests and reduces their return on investment.\n\nTests can be made easier to understand and maintain if we Communi- cate Intent. We can do so by calling Test Utility Methods (page 599) with Intent-Revealing Names [SBPP] to set up our test ﬁ xture and to verify that the expected outcome has been realized. It should be readily apparent within the Test Method (page 348) how the test ﬁ xture inﬂ uences the expected outcome of each test—that is, which inputs result in which outputs. A rich library of Test Utility Methods also makes tests easier to write because we don’t have to code the details into every test.\n\nPrinciple: Don’t Modify the SUT\n\nEffective testing often requires us to replace a part of the application with a Test Double or override part of its behavior using a Test-Speciﬁ c Subclass (page 579). This may be because we need to gain control over its indirect inputs or because we need to perform Behavior Veriﬁ cation by intercepting its indirect outputs. It may\n\n1 Anything more than about ten lines is getting to be too much.\n\nwww.it-ebooks.info\n\n41\n\nAlso known as: Higher-Level Language, Single-Glance Readable",
      "content_length": 2801,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "42\n\nAlso known as: Independent Test\n\nChapter 5 Principles of Test Automation\n\nalso be because parts of the application’s behavior have unacceptable side effects or dependencies that are impossible to satisfy in our development or test environment. Modifying the SUT is a dangerous thing whether we are putting in Test Hooks (page 709), overriding behavior in a Test-Speciﬁ c Subclass, or replacing a DOC with a Test Double. In any of these circumstances, we may no longer actually be testing the code we plan to put into production.\n\nWe need to ensure that we are testing the software in a conﬁ guration that is truly representative of how it will be used in production. If we do need to replace something the SUT depends on to get better control of the context surrounding the SUT, we must make sure that we are doing so in a representative way. Otherwise, we may end up replacing part of the SUT that we think we are testing. Suppose, for example, that we are writing tests for objects X, Y, and Z, where object X depends on object Y, which in turn depends on object Z. When writing tests for X, it is reasonable to replace Y and Z with a Test Double. When testing Y, we can replace Z with a Test Double. When testing Z, however, we cannot replace it with a Test Double because Z is what we are testing! This consideration is particularly salient when we have to refactor the code to improve its testability.\n\nWhen we use a Test-Speciﬁ c Subclass to override part of the behavior of an object to allow testing, we have to be careful that we override only those meth- ods that the test speciﬁ cally needs to null out or use to inject indirect inputs. If we choose to reuse a Test-Speciﬁ c Subclass created for another test, we must ensure that it does not override any of the behavior that this test is verifying.\n\nAnother way of looking at this principle is as follows: The term SUT is rela- tive to the tests we are writing. In our “X uses Y uses Z” example, the SUT for some component tests might be the aggregate of X, Y, and Z; for unit testing purposes, it might be just X for some tests, just Y for other tests, and just Z for yet other tests. Just about the only time we consider the entire application to be the SUT is when we are doing user acceptance testing using the user interface and going all the way back to the database. Even here, we might be testing only one module of the entire application (e.g., the “Customer Management Mod- ule”). Thus “SUT” rarely equals “application.”\n\nPrinciple: Keep Tests Independent\n\nWhen doing manual testing, it is common practice to have long test procedures that verify many aspects of the SUT’s behavior in a single test. This aggregation of tasks is necessary because the steps involved in setting up the starting state of the system for one test may simply repeat the steps used to verify other parts of its behavior. When tests are executed manually, this repetition is not cost-effective. In addition, human testers have the ability to recognize when a test failure should preclude continuing\n\nwww.it-ebooks.info",
      "content_length": 3070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "The Principles\n\nexecution of the test, when it should cause certain tests to be skipped, or when the failure is immaterial to subsequent tests (though it may still count as a failed test.)\n\nIf tests are interdependent and (even worse) order dependent, we will deprive ourselves of the useful feedback that individual test failures provide. Interacting Tests (see Erratic Test on page 228) tend to fail in a group. The failure of a test that moved the SUT into the state required by the dependent test will lead to the failure of the dependent test, too. With both tests failing, how can we tell whether the failure reﬂ ects a problem in code that both tests rely on in some way or whether it signals a problem in code that only the ﬁ rst test relies on? When both tests fail, we can’t tell. And we are talking about only two tests in this case—imagine how much worse matters would be with tens or even hun- dreds of Interacting Tests.\n\nAn Independent Test can be run by itself. It sets up its own Fresh Fix- ture (page 311) to put the SUT into a state that lets it verify the behavior it is testing. Tests that build a Fresh Fixture are much more likely to be independent than tests that use a Shared Fixture (page 317). The latter can lead to various kinds of Erratic Tests, including Lonely Tests, Interacting Tests, and Test Run Wars. With independent tests, unit test failures give us Defect Localization to help us pinpoint the source of the failure.\n\nPrinciple: Isolate the SUT\n\nSome pieces of software depend on nothing but the (presumably correct) run- time system or operating system. Most pieces of software build on other pieces of software developed by us or by others. When our software depends on other software that may change over time, our tests may suddenly start failing because the behavior of the other software has changed. This problem, which is called Context Sensitivity (see Fragile Test), is a form of Fragile Test.\n\nWhen our software depends on other software whose behavior we cannot control, we may ﬁ nd it difﬁ cult to verify that our software behaves properly with all possible return values. This is likely to lead to Untested Code (see Pro- duction Bugs on page 268) or Untested Requirements (see Production Bugs). To avoid this problem, we need to be able to inject all possible reactions of the DOC into our software under the complete control of our tests.\n\nWhatever application, component, class, or method we are testing, we should strive to isolate it as much as possible from all other parts of the software that we choose not to test. This isolation of elements allows us to Test Concerns Separately and allows us to Keep Tests Independent of one another. It also helps us create a Robust Test by reducing the likelihood of Context Sensitivity caused by too much coupling between our SUT and the software that surrounds it.\n\nwww.it-ebooks.info\n\n43",
      "content_length": 2889,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "44\n\nChapter 5 Principles of Test Automation\n\nWe can satisfy this principle by designing our software such that each piece of depended-on software can be replaced with a Test Double using Dependency Injection (page 678) or Dependency Lookup (page 686) or overridden with a Test-Speciﬁ c Subclass that gives us control of the indirect inputs of the SUT. This design for testability makes our tests more repeatable and robust.\n\nPrinciple: Minimize Test Overlap\n\nMost applications have lots of functionality to verify. Proving that all of the functionality works correctly in all possible combinations and interaction sce- narios is nearly impossible. Therefore, picking the tests to write is an exercise in risk management.\n\nWe should structure our tests so that as few tests as possible depend on a particular piece of functionality. This may seem counter-intuitive at ﬁ rst be- cause one would think that we would want to improve test coverage by testing the software as often as possible. Unfortunately, tests that verify the same func- tionality typically fail at the same time. They also tend to need the same mainte- nance when the functionality of the SUT is modiﬁ ed. Having several tests verify the same functionality is likely to increase test maintenance costs and probably won’t improve quality very much.\n\nWe do want to ensure that all test conditions are covered by the tests that we do use. Each test condition should be covered by exactly one test—no more, no less. If it seems to provide value to test the code in several different ways, we may have identiﬁ ed several different test conditions.\n\nPrinciple: Minimize Untestable Code\n\nSome kinds of code are difﬁ cult to test using Fully Automated Tests. GUI com- ponents, multithreaded code, and Test Methods immediately spring to mind as “untestable” code. All of these kinds of code share the same problem: They are embedded in a context that makes it hard to instantiate or interact with them from automated tests.\n\nUntestable code simply won’t have any Fully Automated Tests to protect it from those nefarious little bugs that can creep into code when we aren’t look- ing. That makes it more difﬁ cult to refactor this code safely and more danger- ous to modify existing functionality or introduce new functionality.\n\nIt is highly desirable to minimize the amount of untestable code that we have to maintain. We can refactor the untestable code to improve its testability by moving the logic we want to test out of the class that is causing the lack of test- ability. For active objects and multithreaded code, we can refactor to Humble Executable (see Humble Object on page 695). For user interface objects, we\n\nwww.it-ebooks.info",
      "content_length": 2699,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "The Principles\n\ncan refactor to Humble Dialog (see Humble Object). Even Test Methods can have much of their untestable code extracted into Test Utility Methods, which can then be tested.\n\nWhen we Minimize Untestable Code, we improve the overall test coverage of our code. In so doing, we also improve our conﬁ dence in the code and extend our ability to refactor at will. The fact that this technique improves the quality of the code is yet another beneﬁ t.\n\nPrinciple: Keep Test Logic Out of Production Code\n\nWhen the production code hasn’t been designed for testability (whether as a result of test-driven development or otherwise), we may be tempted to put “hooks” into the production code to make it easier to test. These hooks typi- cally take the form of if testing then ... and may either run alternative logic or prevent certain logic from running.\n\nTesting is about verifying the behavior of a system. If the system behaves dif- ferently when under test, then how can we be certain that the production code actually works? Even worse, the test hooks could cause the software to fail in production!\n\nThe production code should not contain any conditional statements of the if testing then sort. Likewise, it should not contain any test logic. A well-designed system (from a testing perspective) is one that allows for the isolation of func- tionality. Object-oriented systems are particularly amenable to testing because they are composed of discrete objects. Unfortunately, even object-oriented sys- tems can be built in such a way as to be difﬁ cult to test, and we may still en- counter code with embedded test logic.\n\nPrinciple: Verify One Condition per Test\n\nMany tests require a starting state other than the default state of the SUT, and many operations of the SUT leave it in a different state from its original state. There is a strong temptation to reuse the end state of one test condition as the starting state of the next test condition by combining the veriﬁ cation of the two test conditions into a single Test Method because this makes testing more efﬁ - cient. This approach is not recommended, however, because when one assertion fails, the rest of the test will not be executed. As a consequence, it becomes more difﬁ cult to achieve Defect Localization.\n\nVerifying multiple conditions in a single test makes sense when we execute tests manually because of the high overhead of test setup and because the live- ware can adapt to test failures. It is too much work to set up the ﬁ xture for a large number of manual tests, so human testers naturally tend to write long\n\nwww.it-ebooks.info\n\n45\n\nAlso known as: No Test Logic in Production Code\n\nAlso known as: Single-Condition Test",
      "content_length": 2705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "46\n\nChapter 5 Principles of Test Automation\n\nmultiple-condition tests.2 They also have the intelligence to work around any issues they encounter so that all is not lost if a single step fails. In contrast, with automated tests, a single failed assertion will cause the test to stop running and the rest of the test will provide no data on what works and what doesn’t.\n\nEach Scripted Test should verify a single test condition. This single-mindedness is possible because the test ﬁ xture is set up programmatically rather than by a human. Programs can set up ﬁ xtures very quickly and they don’t have trouble ex- ecuting exactly the same sequence of steps hundreds of times! If several tests need the same test ﬁ xture, either we can move the Test Methods into a single Testcase Class per Fixture (page 631) so we can use Implicit Setup (page 424) or we can call Test Utility Methods to set up the ﬁ xture using Delegated Setup (page 411). We design each test to have four distinct phases (see Four-Phase Test on page 358) that are executed in sequence: ﬁ xture setup, exercise SUT, result veriﬁ cation, and ﬁ xture teardown.\n\nIn the ﬁ rst phase, we set up the test ﬁ xture (the “before” picture) that is required for the SUT to exhibit the expected behavior as well as any- thing we need to put in place to observe the actual outcome (such as using a Test Double).\n\nIn the second phase, we interact with the SUT to exercise whatever behavior we are trying to verify. This should be a single, distinct behav- ior; if we try to exercise several parts of the SUT, we are not writing a Single-Condition Test.\n\nIn the third phase, we do whatever is necessary to determine whether the expected outcome has been obtained and fail the test if it has not.\n\nIn the fourth phase, we tear down the test ﬁ xture and put the world\n\nback into the state in which we found it.\n\nNote that there is a single exercise SUT phase and a single result veriﬁ cation phase. We avoid having a series of such alternating calls (exercise, verify, exercise, verify) because that approach would be trying to verify several distinct condi- tions—something that is better handled via distinct Test Methods.\n\nOne possibly contentious aspect of Verify One Condition per Test is what we mean by “one condition.” Some test drivers insist on one assertion per test. This insistence may be based on using a Testcase Class per Fixture organization of the Test Methods and naming each test based on what the one assertion is\n\n2 Clever testers often use automated test scripts to put the SUT into the correct starting state for their manual tests, thereby avoiding long manual test scripts.\n\nwww.it-ebooks.info",
      "content_length": 2668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "The Principles\n\nverifying.3 Having one assertion per test makes such naming very easy but also leads to many more test methods if we have to assert on many output ﬁ elds. Of course, we can often comply with this interpretation by extracting a Custom Assertion (page 474) or Veriﬁ cation Method (see Custom Assertion) that allows us to reduce the multiple assertion method calls to a single call. Sometimes that approach makes the test more readable. When it doesn’t, I wouldn’t be too dog- matic about insisting on a single assertion.\n\nPrinciple: Test Concerns Separately\n\nThe behavior of a complex application consists of the aggregate of a large num- ber of smaller behaviors. Sometimes several of these behaviors are provided by the same component. Each of these behaviors is a different concern and may have a signiﬁ cant number of scenarios in which it needs to be veriﬁ ed.\n\nThe problem with testing several concerns in a single Test Method is that this method will be broken whenever any of the tested concerns is modiﬁ ed. Even worse, it won’t be obvious which concern is the one at fault. Identify- ing the real culprit typically requires Manual Debugging (see Frequent Debug- ging on page 248) because of the lack of Defect Localization. The net effect is that more tests will fail and each test will take longer to troubleshoot and ﬁ x. Refactoring is also made more difﬁ cult by testing several concerns in the same test; it will be harder to “tease apart” the eager class into several independent classes, each of which implements a single concern, because the tests will need extensive redesign.\n\nTesting our concerns separately allows a failure to tell us that we have a problem in a speciﬁ c part of our system rather than simply saying that we have a problem somewhere. This approach to testing also makes it easier to understand the behavior now and to separate the concerns in subsequent refactorings. That is, we should just be able to move a subset of the tests to a different Testcase Class (page 373) that veriﬁ es the newly created class; it shouldn’t be necessary to modify the test much more than changing the class name of the SUT.\n\nPrinciple: Ensure Commensurate Effort and Responsibility\n\nThe amount of effort it takes to write or modify tests should not exceed the effort it takes to implement the corresponding functionality. Likewise, the tools required to write or maintain the test should require no more expertise than the tools used to implement the functionality. For example, if we can conﬁ gure the\n\n3 For example, AwaitingApprovalFlight.validApproverRequestShouldBeApproved.\n\nwww.it-ebooks.info\n\n47",
      "content_length": 2638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "48\n\nChapter 5 Principles of Test Automation\n\nbehavior of a SUT using metadata and we want to write tests that verify that the metadata is set up correctly, we should not have to write code to do so. A Data-Driven Test (page 288) would be much more appropriate in these circum- stances.\n\nWhat’s Next?\n\nPrevious chapters covered the common pitfalls (in the form of test smells) and goals of test automation. This chapter made the value system we use while choosing patterns explicit. In Chapter 6, Test Automation Strategy, we will examine the “hard to change” decisions that we should try to get right early in the project.\n\nwww.it-ebooks.info",
      "content_length": 642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Chapter 6\n\nTest Automation Strategy\n\nAbout This Chapter\n\nIn previous chapters, we saw some of the problems we might encounter with test automation. In Chapter 5, Principles of Test Automation, we learned about some of the principles we can apply to help address those problems. This chapter gets a bit more concrete but still focuses at the 30,000-foot level. In the logical sequence of things, test strategy comes before ﬁ xture setup but is a somewhat more advanced topic. If you are new to test automation using xUnit, you may want to skip this chapter and come back after reading more about the basics of xUnit in Chapter 7, xUnit Basics, and about ﬁ xture setup and teardown in Chapter 8, Transient Fixture Management, and subsequent chapters.\n\nWhat’s Strategic?\n\nAs the story in the preface amply demonstrates, it is easy to get off on the wrong foot. This is especially true when you lack experience in test automation and when this testing strategy is adopted “bottom up.” If we catch the problems early enough, the cost of refactoring the tests to eliminate the problems can be manage- able. If, however, the problems are left to fester for too long or the wrong approach is taken to address them, a very large amount of effort can be wasted. This is not to suggest that we should follow a “big design upfront” (BDUF) approach to test automation. BDUF is almost always the wrong answer. Rather, it is helpful to be aware of the strategic decisions necessary and to make them “just in time” rather than “much too late.” This chapter gives a “head’s up” about some of the strategic issues we want to keep in mind so that we don’t get blindsided by them later.\n\nWhat makes a decision “strategic”? A decision is strategic if it is “hard to change.” That is, a strategic decision affects a large number of tests, especially such that many or all the tests would need to be converted to a different approach\n\n49\n\nwww.it-ebooks.info",
      "content_length": 1934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "50\n\nChapter 6 Test Automation Strategy\n\nat the same time. Put another way, any decision that could cost a large amount of effort to change is strategic.\n\nCommon strategic decisions include the following considerations:\n\nWhich kinds of tests to automate?\n\nWhich tools to use to automate them?\n\nHow to manage the test ﬁ xture?\n\nHow to ensure that the system is easily tested and how the tests interact\n\nwith the SUT?\n\nEach of these decisions can have far-reaching consequences, so they are best made consciously, at the right time, and based on the best available information.\n\nThe strategies and more detailed patterns described in this book are equally applicable regardless of the kind of Test Automation Framework (page 298) we choose to use. Most of my experience is with xUnit, so it is the focus of this book. But “don’t throw out the baby with the bath water”: If you ﬁ nd yourself using a different kind of Test Automation Framework, remember that most of what you learn in regard to xUnit may still be applicable.\n\nWhich Kinds of Tests Should We Automate?\n\nRoughly speaking, we can divide tests into the following two categories:\n\nPer-functionality tests (also known as functional tests) verify the behavior\n\nof the SUT in response to a particular stimulus.\n\nCross-functional tests verify various aspects of the system’s behavior\n\nthat cut across speciﬁ c functionality.\n\nFigure 6.1 shows these two basic kinds of tests as two columns, each of which is further subdivided into more speciﬁ c kinds of tests.\n\nPer-Functionality Tests\n\nPer-functionality tests verify the directly observable behavior of a piece of soft- ware. The functionality can be business related (e.g., the principal use cases of the system) or related to operational requirements (e.g., system maintenance and speciﬁ c fault-tolerance scenarios). Most of these requirements can also be expressed as use cases, features, user stories, or test scenarios.\n\nPer-functionality tests can be characterized by whether the functionality is\n\nbusiness (or user) facing and by the size of the SUT on which they operate.\n\nwww.it-ebooks.info",
      "content_length": 2105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Which Kinds of Tests Should We Automate?\n\nAutomated Automated Automated Automated various various\n\nBusiness Business Facing Facing\n\nAutomated Automated Automated Automated xUnit xUnit\n\nTechnology Technology Facing Facing\n\nAutomated Automated Automated Automated xUnit xUnit\n\nKind of Behavior Kind of Behavior\n\nPer Functionality Per Functionality Customer Customer Tests Tests Business Intent Business Intent (Executable Specification) (Executable Specification) Component Component Tests Tests Architect Intent Architect Intent (Design of the System) (Design of the System) Unit Unit Tests Tests Developer Intent Developer Intent (Design of the Code) (Design of the Code)\n\nSupport Support Development Development\n\nCross-Functional Cross-Functional Usability Usability Testing Testing Is it pleasurable? Is it pleasurable?\n\nExploratory Exploratory Testing Testing Is it self-consistent? Is it self-consistent?\n\nProperty Property Testing Testing Is it responsive, Is it responsive, secure, scalable? secure, scalable? Critique Critique Product Product\n\nManual Manual Manual Manual\n\nManual Manual Manual Manual\n\nDiagram adapted Diagram adapted from Mary from Mary Poppendieck and Poppendieck and Brian Marick Brian Marick\n\nSpecial-Purpose Special-Purpose Tool - Tool - Based Based Tool-Based Tool-Based\n\nPurpose of Tests Purpose of Tests\n\nFigure 6.1 A summary of the kinds of tests we write and why. The left column contains the tests we write that describe the functionality of the product at various levels of granularity; we perform these tests to support development. The right column contains tests that span speciﬁ c chunks of functionality; we execute these tests to critique the product. The bottom of each cell describes what we are trying to communicate or verify.\n\nCustomer Tests\n\nCustomer tests verify the behavior of the entire system or application. They typi- cally correspond to scenarios of one or more use cases, features, or user stories. These tests often go by other names such as functional tests, acceptance tests, or end-user tests. Although they may be automated by developers, their key char- acteristic is that an end user should be able to recognize the behavior speciﬁ ed by the test even if the user cannot read the test representation.\n\nUnit Tests\n\nUnit tests verify the behavior of a single class or method that is a consequence of a design decision. This behavior is typically not directly related to the require- ments except when a key chunk of business logic is encapsulated within the class or method in question. These tests are written by developers for their own use; they help developers describe what “done looks like” by summarizing the behavior of the unit in the form of tests.\n\nwww.it-ebooks.info\n\n51",
      "content_length": 2743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "52\n\nChapter 6 Test Automation Strategy\n\nComponent Tests\n\nComponent tests verify components consisting of groups of classes that collec- tively provide some service. They ﬁ t somewhere between unit tests and customer tests in terms of the size of the SUT being veriﬁ ed. Although some people call these “integration tests” or “subsystem tests,” those terms can mean something entirely different from “tests of a speciﬁ c larger-grained subcomponent of the overall system.”\n\nFault Insertion Tests\n\nFault insertion tests typically show up at all three levels of granularity within these functional tests, with different kinds of faults being inserted at each level. From a test automation strategy point of view, fault insertion is just another set of tests at the unit and component test levels. Things get more interesting at the whole-application level, however. Inserting faults here can be hard to automate because it is challenging to automate insertion of the faults without replacing parts of the application.\n\nCross-Functional Tests\n\nProperty Tests\n\nPerformance tests verify various “nonfunctional” (also known as “extra-functional” or “cross-functional”) requirements of the system. These requirements are different in that they span the various kinds of functionality. They often correspond to the architectural “-ilities.” These kinds of tests include\n\nResponse time tests\n\nCapacity tests\n\nStress tests\n\nFrom a test automation perspective, many of these tests must be automated (at least partially) because human testers would have a hard time creating enough load to verify the behavior under stress. While we can run the same test many times in a row in xUnit, the xUnit framework is not particularly well suited to automating performance tests.\n\nOne advantage of agile methods is that we can start running these kinds of tests quite early in the project—as soon as the key components of the architecture have been roughed in and the skeleton of the functionality is executable. The same tests can then be run continuously throughout the project as new features are added to the system skeleton.\n\nwww.it-ebooks.info",
      "content_length": 2126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Which Tools Do We Use to Automate Which Tests?\n\nUsability Tests\n\nUsability tests verify “ﬁ tness for purpose” by conﬁ rming that real users can use the software application to achieve the stated goals. These tests are very difﬁ cult to automate because they require subjective assessment by people regarding how easy it is to use the SUT. For this reason, usability tests are rarely automated and will not be discussed further in this book.\n\nExploratory Testing\n\nExploratory testing is a way to determine whether the product is self-consistent. The testers use the product, observe how it behaves, form hypotheses, design tests to verify those hypotheses, and exercise the product with them. By its very nature, exploratory testing cannot be automated, although automated tests can be used to set up the SUT in preparation for doing exploratory testing.\n\nWhich Tools Do We Use to Automate Which Tests?\n\nChoosing the right tool for the job is as important as having good skills with the tools selected for use. A wide array of tools are available in the marketplace, and it is easy to be seduced by the features of a particular tool. The choice of tool is a strategic decision: Once we have invested a lot of time and effort in learning a tool and automating many tests using that tool, it becomes much more difﬁ cult to change to a different tool.\n\nThere are two fundamentally different approaches to automating tests (Figure 6.2). The Recorded Test (page 278) approach involves the use of tools that monitor our interactions with the SUT while we test it manually. This information is then saved to a ﬁ le or database and becomes the script for re- playing this test against another (or even the same) version of the SUT. The main problem with Recorded Tests is the level of granularity they record. Most commercial tools record actions at the user interface (UI) element level, which results in Fragile Tests (page 239).\n\nThe second approach to automating tests, Hand-Scripted Tests (see Scripted Test on page 285), involves the hand-coding of test programs (“scripts”) that ex- ercise the system. While xUnit is probably the most commonly used Test Automation Framework for preparing Hand-Scripted Tests, they may be pre- pared in other ways, including “batch” ﬁ les, macro languages, and commercial or open-source test tools. Some of the better-known open-source tools for preparing Scripted Tests are Watir (test scripts coded in Ruby and run inside Internet Ex- plorer), Canoo WebTest (tests scripted in XML and run using the WebTest tool),\n\nwww.it-ebooks.info\n\n53",
      "content_length": 2570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "54\n\nChapter 6 Test Automation Strategy\n\nand the ever-popular Fit (and its wiki-based sibling FitNesse). Some of these tools even provide a test capture capability, thereby blurring the lines between Scripted Tests and Recorded Tests.\n\nGranularity Granularity SUT SUT\n\nunit unit Unit Unit component component Component Component\n\nsystem system System System\n\nMeans of Means of Test – SUT Test – SUT Interaction Interaction\n\nAPI API\n\nUI UI\n\nRecorded Recorded Way of Capturing Tests Way of Capturing Tests\n\nScripted Scripted\n\nFigure 6.2 A summary of the three dimensions of test automation choices. The left side shows the two ways of interacting with the SUT. The bottom edge enumerates how we create the test scripts. The front-to-back dimension categorizes the different sizes of SUT we may choose to test.\n\nChoosing which test automation tools to use is a large part of the test strategy decision. A full survey of the different kinds of tools available is beyond the scope of this book, but a somewhat more detailed treatment of the topic is avail- able in [ARTRP]. The following sections summarize the information here to provide an overview of the strengths and weaknesses of each approach.\n\nTest Automation Ways and Means\n\nFigure 6.3 depicts the decision-making possibilities as a matrix. In theory, there are 2 × 2 × 3 possible combinations in this matrix, but it is possible to under- stand the primary differences between the approaches by looking at the front face of the cube. Some of the four quadrants are applicable to all levels of granu- larity; others are primarily used for automating customer tests.\n\nwww.it-ebooks.info",
      "content_length": 1637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Which Tools Do We Use to Automate Which Tests?\n\nBuilt-in Built-in Built-in Built-in R&PB R&PB R&PB R&PB\n\nMeans of Means of Test – SUT Test – SUT Interaction Interaction\n\nRobot Robot Robot Robot User User User User\n\nAPI API\n\nUI UI\n\n+ Somewhat robust + Somewhat robust - Less maintainable? - Less maintainable? - Cannot be prebuilt - Cannot be prebuilt + Fewer skills required + Fewer skills required - API required - API required - Few COTS tools - Few COTS tools - Very fragile - Very fragile - Not maintainable - Not maintainable - Cannot be pre-built - Cannot be pre-built + No special skills + No special skills + API not required + API not required - Mostly complex, flaky, - Mostly complex, flaky,\n\nexpensive tools expensive tools Recorded Tests Recorded Tests\n\n+ Robust + Robust + More maintainable + More maintainable + Can be prebuilt + Can be prebuilt - - - More skills required - More skills required - - - API required - API required + Simple, cheap tools + Simple, cheap tools - - - Somewhat fragile - Somewhat fragile - - - High maintenance - High maintenance + Can be prebuilt + Can be prebuilt - More skills required - More skills required + UI is the API + UI is the API + Mostly open- + Mostly open- source tools source tools Scripted Tests Scripted Tests\n\nModern Modern Modern Modern xUnit xUnit XUnit XUnit\n\nScripted Scripted Scripted Scripted UI Tests UI Tests UI Tests UI Tests\n\nWay of Capturing Tests Way of Capturing Tests\n\nFigure 6.3 The choices on the front face of the cube. A more detailed look at the front face of the cube in Figure 6.2 along with the advantages (+) and disadvantages of each (–).\n\nUpper-Right Quadrant: Modern xUnit\n\nThe upper-right quadrant of the front face of the cube is dominated by the xUnit family of testing frameworks. These frameworks involve hand-scripting tests that exercise the system at all three levels of granularity (system, component, and unit) via internal interfaces. A good example is unit tests automated using JUnit or NUnit.\n\nLower-Right Quadrant: Scripted UI Tests\n\nThis quadrant represents a variation on the “modern xUnit” approach, with the most common examples being the use of HttpUnit, JFCUnit, Watir, or similar tools to hand-script tests using the UI. It is also possible to hand-script tests using commercial Recorded Test tools such as QTP. These approaches all reside within the lower-right quadrant at various levels of SUT granularity. For example, when used for customer tests, these tools would perform at the system test level of granularity. They could also be used to test just the UI component of the system (or possibly even some UI units such as custom widgets), although this effort would require stubbing out the actual system behind the UI.\n\nLower-Left Quadrant: Robot User\n\nThe “robot user” quadrant focuses on recording tests that interact with the system via the UI. Most commercial test automation tools follow this approach. It applies primarily at the “whole system” granularity but, like scripted UI Tests,\n\nwww.it-ebooks.info\n\n55",
      "content_length": 3034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "56\n\nChapter 6 Test Automation Strategy\n\ncould be applied to the UI components or units if the rest of the system can be stubbed out.\n\nUpper-Left Quadrant: Internal Recording\n\nFor completeness, the upper-left quadrant involves creating Recorded Tests via an API somewhere behind the UI by recording all inputs and responses as the SUT is exercised. It may even involve inserting observation points between the SUT (at whatever granularity we are testing) and any DOCs. During test play- back, the test APIs inject the inputs recorded earlier and compare the results with what was recorded\n\nThis quadrant is not well populated with commercial tools1 but is a feasible\n\noption when building a Recorded Test mechanism into the application itself.\n\nIntroducing xUnit\n\nThe xUnit family of Test Automation Frameworks is designed for use in auto- mating programmer tests. Its design is intended to meet the following goals:\n\nMake it easy for developers to write tests without needing to learn a new programming language. xUnit is available in most languages in use today.\n\nMake it easy to test individual classes and objects without needing to have the rest of the application available. xUnit is designed to allow us to test the software from the inside; we just have to design for testability to take advantage of this capability.\n\nMake it easy to run one test or many tests with a single action. xUnit includes the concept of a test suite and Suite of Suites (see Test Suite Object on page 387) to support this kind of test execution.\n\nMinimize the cost of running the tests so programmers aren’t discour- aged from running the existing tests. For this reason, each test should be a Self-Checking Test (page 26) that implements the Hollywood principle.2\n\n1 Most of the tools in this quadrant focus on recording regression tests by inserting obser- vation points into a component-based application and recording the (remote) method calls and responses between the components. This approach is becoming more popular with the advent of service-oriented architecture (SOA). 2 The name is derived from what directors in Hollywood tell aspiring applicants at mass casting calls: “Don’t call us; we’ll call you (if we want you).”\n\nwww.it-ebooks.info",
      "content_length": 2237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Which Tools Do We Use to Automate Which Tests?\n\nThe xUnit family has been extraordinarily successful at meeting its goals. I cannot imagine that Erich Gamma and Kent Beck could have possibly antici- pated just how big an impact that ﬁ rst version of JUnit would have on software development!3 The same characteristics that make xUnit particularly well suited to automating programmer tests, however, may make it less suitable for writing some other kinds of tests. In particular, the “stop on ﬁ rst failure” behavior of as- sertions in xUnit has often been criticized (or overridden) by people who want to use xUnit for automating multistep customer tests so that they can see the whole score (what worked and what didn’t) rather than merely the ﬁ rst deviation from the expected results. This disagreement points out several things:\n\n“Stop on ﬁ rst failure” is a tool philosophy, not a characteristic of unit tests. It so happens that most test automaters prefer to have their unit tests stop on ﬁ rst failure, and most recognize that customer tests must necessarily be longer than unit tests.\n\nIt is possible to change the fundamental behavior of xUnit to satisfy speciﬁ c needs; this ﬂ exibility is just one advantage of open-source tools.\n\nSeeing a need to change the fundamental behavior of xUnit should probably be interpreted as a trigger for considering whether some other tool might possibly be a better ﬁ t.\n\nFor example, the Fit framework has been designed speciﬁ cally for the purpose of running customer tests. It overcomes the limitations of xUnit that lead to the “stop on ﬁ rst failure” behavior by communicating the pass/fail status of each step of a test using color coding. Another option for Java developers is TestNG, which provides capabilities for explicitly sequencing Chained Tests (page 454).\n\nHaving said this, choosing a different tool doesn’t eliminate the need to make many of the strategic decisions unless the tool constrains that decision making in some way. For example, we still need to set up the test ﬁ xture for a Fit test. Some patterns—such as Chained Tests, where one test sets up the ﬁ xture for a subsequent test—are difﬁ cult to automate and may therefore be less attractive in Fit than in xUnit. And isn’t it ironic that the very ﬂ exibility of xUnit is what al- lows test automaters to get themselves into so much trouble by creating Obscure Tests (page 186) that result in High Test Maintenance Cost (page 265)?\n\n3 Technically, SUnit came ﬁ rst but it took JUnit and the “Test Infected” article [TI] to really get things rolling.\n\nwww.it-ebooks.info\n\n57",
      "content_length": 2600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "58\n\nChapter 6 Test Automation Strategy\n\nThe xUnit Sweet Spot\n\nThe xUnit family works best when we can organize our tests as a large set of small tests, each of which requires a small test ﬁ xture that is relatively easy to set up. This allows us to create a separate test for each test scenario of each object. The test ﬁ xture should be managed using a Fresh Fixture (page 311) strategy by setting up a new Minimal Fixture (page 302) for each test.\n\nxUnit works best when we write tests against software APIs and then test single classes or small groups of classes in isolation. This approach allows us to build small test ﬁ xtures that can be instantiated quickly.\n\nWhen doing customer tests, xUnit works best if we deﬁ ne a Higher-Level Language (page 41) with which to describe our tests. This choice moves the level of abstraction higher, away from the nitty-gritty of the technology and closer to the business concepts that customers understand. From here, it is a very small step to convert these tests to Data-Driven Tests (page 288) imple- mented in xUnit or Fit.\n\nNote that many of the higher-level patterns and principles described in this book apply equally well to both Fit tests and xUnit tests. I have also found them to be useful when working with commercial GUI-based testing tools, which typically use a “record and playback” metaphor. The ﬁ xture management patterns are particularly salient in this arena, as are reusable “test components” that may be strung together to form a variety of test scripts. This is entirely analogous to the xUnit practice of single-purpose Test Methods (page 348) calling reusable Test Utility Methods (page 599) to reduce their coupling to the SUT’s API.\n\nWhich Test Fixture Strategy Do We Use?\n\nThe test ﬁ xture management strategy is strategic because it has a large impact on the execution time and robustness of the tests. The effects of picking the wrong strategy won’t be felt immediately because it takes at least a few hundred tests before the Slow Tests (page 253) smell becomes evident and probably several months of development before the High Test Maintenance Cost smell starts to emerge. Once these smells appear, however, the need to change the test automa- tion strategy will become apparent—and its cost will be signiﬁ cant because of the number of tests affected.\n\nwww.it-ebooks.info",
      "content_length": 2351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Which Test Fixture Strategy Do We Use?\n\nWhat Is a Fixture?\n\nEvery test consists of four parts, as described in Four-Phase Test (page 358). In the ﬁ rst phase, we create the SUT and everything it depends on and put them into the state required to exercise the SUT. In xUnit, we call everything we need in place to exercise the SUT the test ﬁ xture, and we call the part of the test logic that we execute to set it up the ﬁ xture setup phase of the test.\n\nAt this point, a word of caution is in order. The term “ﬁ xture” means many\n\nthings to many people:\n\nSome variants of xUnit keep the concept of the ﬁ xture separate from the Testcase Class (page 373) that creates it. JUnit and its direct ports fall into this category.\n\nOther members of the xUnit family assume that an instance of the Test-\n\ncase Class “is a” ﬁ xture. NUnit is a good example.\n\nA third camp uses an entirely different name for the ﬁ xture. For example, RSpec captures the pre-conditions of the test in a test con- text class that holds the Test Methods (same idea as NUnit but with different terminology).\n\nThe term “ﬁ xture” is used to mean entirely different things in other kinds of test automation. In Fit, for example, it means the custom-built parts of the Data-Driven Test Interpreter [GOF] that we use to deﬁ ne our Higher-Level Language.\n\nThe “class ‘is a’ ﬁ xture” approach assumes the Testcase Class per Fixture (page 631) approach to organizing the tests. When we choose a different way of organizing the tests, such as Testcase Class per Class (page 617) or Testcase Class per Fea- ture (page 624), this merging of the concepts of test ﬁ xture and Testcase Class can be confusing. Throughout this book, I use “test ﬁ xture”—or just “ﬁ xture”—to mean “the pre-conditions of the test” and Testcase Class to mean “the class that contains the Test Methods and any code needed to set up the test ﬁ xture.”\n\nThe most common way to set up the ﬁ xture is to use front door ﬁ xture setup by calling the appropriate methods on the SUT to construct the objects. When the state of the SUT is stored in other objects or components, we can do Back Door Setup (see Back Door Manipulation on page 327) by inserting the neces- sary records directly into the other component on which the behavior of the SUT depends. We use Back Door Setup most often with databases or when we need to use a Mock Object (page 544) or Test Double (page 522); these concepts are covered in more detail in Chapter 13, Testing with Databases, and Chapter 11, Using Test Doubles.\n\nwww.it-ebooks.info\n\n59",
      "content_length": 2547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "60\n\nChapter 6 Test Automation Strategy\n\nMajor Fixture Strategies\n\nThere are probably many ways to classify just about anything. For the purposes of this discussion, we will classify our test ﬁ xture strategies based on what kinds of test development work we need to do for each one.\n\nThe ﬁ rst and simplest ﬁ xture management strategy requires us to worry only how we will organize the code to build the ﬁ xture for each test. That is, do we put this code in our Test Methods, factor it into Test Utility Methods that we call from our Test Methods, or put it into a setUp method in our Testcase Class? This strategy involves the use of Transient Fresh Fixtures (see Fresh Fixture). These ﬁ xtures live only in memory and very conveniently disappear as soon as we are done with them.\n\nA second strategy involves the use of Fresh Fixtures that, for one reason or another, persist beyond the single Test Method that uses it. To keep them from turning into Shared Fixtures (page 317), these Persistent Fresh Fixtures (see Fresh Fixture) require explicit code to tear them down at the end of each test. This requirement brings into play the ﬁ xture teardown patterns.\n\nA third strategy involves persistent ﬁ xtures that are deliberately reused across many tests. This Shared Fixture strategy is often used to improve the execu- tion speed of tests that use a Persistent Fresh Fixture but comes with a fair amount of baggage. These tests require the use of one of the ﬁ xture construc- tion and teardown triggering patterns. They also involve tests that interact with one another, whether by design or by consequence, which often leads to Erratic Tests (page 228) and High Test Maintenance Costs.\n\nTable 6.1 summarizes the ﬁ xture management overhead associated with each\n\nof the three styles of ﬁ xtures.\n\nTable 6.1 A Summary of the Fixture Setup and Teardown Requirements of the Various Test Fixture Strategies\n\nSet Up Code\n\nTear Down Code\n\nSetup/Teardown Triggering\n\nTransient Fresh Fixture\n\nYes\n\nPersistent Fresh Fixture\n\nYes\n\nYes\n\nShared Fixture\n\nYes\n\nYes\n\nYes\n\nNote: The Shared Fixture row assumes we are building a new Shared Fixture each test\n\nrun rather than using a Prebuilt Fixture (page 429).\n\nwww.it-ebooks.info",
      "content_length": 2218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Which Test Fixture Strategy Do We Use?\n\nFigure 6.4 illustrates the interaction between our goals, freshness of ﬁ xtures or ﬁ xture reuse, and ﬁ xture persistence. It also illustrates a few variations of the Shared Fixture.\n\nTransient Transient\n\nFresh Fresh Fixture Fixture\n\nPersistent Persistent\n\nShared Shared Fixture Fixture\n\nImmutable Immutable Shared Shared Fixture Fixture\n\nFigure 6.4 A summary of the main test ﬁ xture strategies. Fresh Fixtures can be either transient or persistent; Shared Fixtures must be persistent. An Immutable Shared Fixture (see Shared Fixture) must not be modiﬁ ed by any test. As a consequence, most tests augment the Shared Fixture with a Fresh Fixture that they can modify.\n\nThe relationship between persistence and freshness is reasonably obvious for two of these combinations. The persistent Fresh Fixture is discussed in more detail later in this chapter. The transient Shared Fixture is inherently transient— how we hold references to these ﬁ xtures is what makes them persist. Other than this distinction, transient Shared Fixtures can be treated exactly like persistent Shared Fixtures.\n\nTransient Fresh Fixtures\n\nIn this approach, each test creates a temporary Fresh Fixture as it runs. Any objects or records it requires are created by the test itself (though not necessarily inside the Test Method). Because the test ﬁ xture visibility is restricted to the one test alone, we ensure that each test is completely independent because it cannot depend, either accidentally or on purpose, on the output of any other tests that use the same ﬁ xture.\n\nwww.it-ebooks.info\n\n61",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "62\n\nChapter 6 Test Automation Strategy\n\nWe call this approach Fresh Fixture because each test starts with a clean slate and builds from there. It does not “inherit” or “reuse” any part of the ﬁ xture from other tests or from a Prebuilt Fixture (page 429). Every object or record used by the SUT is “fresh,” “brand new,” and not “previously enjoyed.”\n\nThe main disadvantage of using the Fresh Fixture approach is the additional CPU cycles it takes to create all the objects for each test. As a consequence, the tests may run more slowly than under a Shared Fixture approach, especially if we use a Persistent Fresh Fixture.\n\nPersistent Fresh Fixtures\n\nA Persistent Fresh Fixture sounds a bit oxymoronic. We want the ﬁ xture to be fresh, yet it persists beyond the lifetime of a single test! What kind of strategy is that? Some might say “stupid,” but sometimes one has to do this.\n\nWe are “forced” into this strategy when we are testing components that are tightly coupled to a database or other persistence mechanism. The obvious so- lution is that we should not let the coupling be so tight, but rather make the database a substitutable dependency of the component we are testing. This step may not be practical when testing legacy software, however—yet we may still want to partake of the beneﬁ ts of a Fresh Fixture. Hence the existence of the Persistent Fresh Fixture strategy. The key difference between this strategy and the Transient Fresh Fixture is the need for code to tear down the ﬁ xture after each test. Persistent Fresh Fixtures can result in Slow Tests if the persistence of the ﬁ xture is caused by the use of a database, ﬁ le system, or other high-latency dependency.\n\nWe can at least partially address the resulting Slow Tests by applying one or\n\nmore of the following patterns:\n\n1. Construct a Minimal Fixture (the smallest ﬁ xture possible).\n\n2. Speed up the construction by using a Test Double to replace the pro-\n\nvider of any data that takes too long to set up.\n\n3.\n\nIf the tests still are not fast enough, minimize the size of the part of the ﬁ xture we need to destroy and reconstruct each time by using an Immutable Shared Fixture for any objects that are referenced but not modiﬁ ed.\n\nThe project teams with which I have worked have found that, on average, our tests run 50 times faster (yes, they take 2% as long) when we use Dependency Injection (page 678) or Dependency Lookup (page 686) to replace the entire database with a Fake Database (see Fake Object on page 551) that uses a set of\n\nwww.it-ebooks.info",
      "content_length": 2539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Which Test Fixture Strategy Do We Use?\n\nhash tables instead of tables. Each test may require many, many database opera- tions to set up and tear down the ﬁ xture required by a single query in the SUT. There is a lot to be said for minimizing the size and complexity of the test ﬁ xture. A Minimal Fixture (see Minimal Fixture) is much easier to understand and helps highlight the cause–effect relationship between the ﬁ xture and the ex- pected outcome. In this regard, it is a major enabler of Tests as Documentation (page 23). In some cases, we can make the test ﬁ xture much smaller by using Entity Chain Snipping (see Test Stub on page 529) to eliminate the need to in- stantiate those objects on which our test depends only indirectly. This tactic will certainly speed up the instantiation of our test ﬁ xture.\n\nShared Fixture Strategies\n\nSometimes we cannot—or choose not to—use a Fresh Fixture strategy. In these cases, we can use a Shared Fixture. In this approach, many tests reuse the same instance of a test ﬁ xture.\n\nThe major advantage of Shared Fixtures is that we save a lot of execution time in setting up and tearing down the ﬁ xture. The main disadvantage is con- veyed by one of its aliases, Stale Fixture, and by the test smell that describes its most common side effects, Interacting Tests (see Erratic Test). Although Shared Fixtures do have other beneﬁ ts, most can be realized by applying other patterns to Fresh Fixtures; Standard Fixture (page 305) avoids the ﬁ xture design and coding effort for every test without actually sharing the ﬁ xture.\n\nNow, if Shared Fixtures are so bad, why even discuss them? Because every- one seems to go down this road at least once in his or her career—so we might as well share the best available information about them should you venture down that path. Mind you, this discussion isn’t meant to encourage anyone to go down this path unnecessarily because it is paved with broken glass, infested with poisonous snakes, and . . . well, you get my drift.\n\nGiven that we have decided to use a Shared Fixture (we did investigate every possible alternative, didn’t we?), what are our options? We can make the fol- lowing adjustments (Figure 6.5):\n\nHow far and wide we share a ﬁ xture (e.g., a Testcase Class, all tests in\n\na test suite, all test run by a particular user)\n\nHow often we recreate the ﬁ xture\n\nwww.it-ebooks.info\n\n63",
      "content_length": 2386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "64\n\nChapter 6 Test Automation Strategy\n\nShared Shared Fixture Fixture\n\nPrebuilt Prebuilt Fixture Fixture\n\nLazy Lazy Setup Setup\n\nSetup Setup Decorator Decorator\n\nSuite Suite Fixture Fixture Setup Setup\n\nChained Chained Tests Tests\n\nShared Fixture Setup Shared Fixture Setup\n\nFigure 6.5 The various ways we can manage a Shared Fixture. The strategies are ordered by the length of the ﬁ xture’s lifetime, with the longestlasting ﬁ xture appearing on the left.\n\nThe more tests that share a ﬁ xture, the more likely that one of them will make a mess of things and spoil everything for all the tests that follow it. The less often we reconstruct the ﬁ xture, the longer the effects of a messed-up ﬁ xture will per- sist. For example, a Prebuilt Fixture can be set up outside the test run, thereby avoiding the entire cost of setting up the ﬁ xture as part of the test run; unfor- tunately, it can also result in Unrepeatable Tests (see Erratic Test) if tests don’t clean up after themselves properly. This strategy is most commonly used with a Database Sandbox (page 650) that is initialized using a database script; once the ﬁ xture is corrupted, it must be reinitialized by rerunning the script. If the Shared Fixture is accessible to more than one Test Runner (page 377), we may end up in a Test Run War (see Erratic Test), in which tests fail randomly as they try to use the same ﬁ xture resource at the same time as some other test.\n\nWe can avoid both Unrepeatable Tests and Test Run Wars by setting up the ﬁ xture each time the test suite is run. xUnit provides several ways to do so, including Lazy Setup (page 435), Suite Fixture Setup (page 441), and Setup Decorator (page 447). The concept of “lazy initialization” should be familiar to most object-oriented developers; here we just apply the concept to the construc- tion of the test ﬁ xture. The latter two choices provide a way to tear down the test ﬁ xture when the test run is ﬁ nished because they call a setUp method and a corresponding tearDown at the appropriate times; Lazy Setup does not give us a way to do this.\n\nChained Tests represent another option for setting up a Shared Fixture, one that involves running the tests in a predeﬁ ned order and letting each test use the previ ous test’s results as its test ﬁ xture. Unfortunately, once one test fails, many of the tests that follow will provide erratic results because their pre-conditions have not\n\nwww.it-ebooks.info",
      "content_length": 2439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "How Do We Ensure Testability?\n\nbeen satisﬁ ed. This problem can be made easier to diagnose by having each test use Guard Assertions (page 490) to verify that its pre-conditions have been met.4\n\nAs mentioned earlier, an Immutable Shared Fixture is a strategy for speeding up tests that use a Fresh Fixture. We can also use an Immutable Shared Fixture to make tests based on a Shared Fixture less erratic by restricting changes to a smaller, mutable part of a Shared Fixture.\n\nHow Do We Ensure Testability?\n\nThe last strategic concern touched on in this chapter is ensuring testability. The discussion here isn’t intended to be a complete treatment of the topic—it is too large to cover in a single chapter on test strategy. Nevertheless, we shouldn’t sweep this issue under the carpet either, because it deﬁ nitely has a major impact on test automation. But ﬁ rst, I must climb onto my soapbox for a short digres- sion into the development process.\n\nTest Last—at Your Peril\n\nAnyone who has tried to retroﬁ t unit tests onto an existing application has prob- ably experienced a lot of pain! This is the hardest kind of test automation we can do as well as the least productive. A lot of the beneﬁ t of automated tests is derived during the “debugging phase” of software development, when such tests can reduce the amount of time spent working with debugging tools. Tackling a test retroﬁ t on legacy software as your ﬁ rst attempt at automated unit testing is the last thing you want to try, as it is sure to discourage even the most deter- mined developers and project managers.\n\nDesign for Testability—Upfront\n\nBDUF5 design for testability is hard because it is difﬁ cult to know what the tests will need in the way of control points and observation points on the SUT. We can easily build software that is difﬁ cult to test. We can also spend a lot of time designing in testability mechanisms that are either insufﬁ cient or unnecessary. Either way, we will have spent a lot of effort with nothing to show for it.\n\n4 Unfortunately, this may result in slower tests when the ﬁ xture is in a database. Never- theless, it will still be many times faster than if each test had to insert all the records it needed. 5 “Big Design Upfront” (also known as “waterfall design”) is the opposite of emergent design (“just-in-time design”).\n\nwww.it-ebooks.info\n\n65",
      "content_length": 2350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "66\n\nChapter 6 Test Automation Strategy\n\nTest-Driven Testability\n\nThe nice thing about building our software driven by tests is that we don’t have to think very much about design for testability; we just write the tests and that forces us to build for testability. The act of writing the test deﬁ nes the control points and observation points that the SUT needs to provide. Once we have passed the tests, we know we have a testable design.\n\nNow that I’ve done my bit promoting TDD as a “design for testability” pro- cess, let’s get on with our discussion of the mechanics of actually making our software testable.\n\nControl Points and Observation Points\n\nA test interacts with the software6 through one or more interfaces or interaction points. From the test’s point of view, these interfaces can act as either control points or observation points (Figure 6.6).\n\nFixture Fixture\n\nSetup Setup\n\nInitialize Initialize\n\nDirect Inputs Direct Inputs (Control Points) (Control Points)\n\nSUT SUT\n\nIndirect Output Indirect Output (Observation Point) (Observation Point)\n\nGet Something Get Something (with return value) (with return value)\n\nDOC DOC\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise (with return value) (with return value)\n\nDirect Outputs Direct Outputs (Observation Points) (Observation Points)\n\nGet State Get State\n\nA A\n\nB B\n\nC C\n\nIndirect Input Indirect Input (Control Point) (Control Point)\n\nDo Something Do Something (no return value) (no return value)\n\nIndirect Output Indirect Output (Observation Point) (Observation Point)\n\nTeardown Teardown\n\nFigure 6.6 Control points and observation points. The test interacts with the SUT through interaction points. Direct interaction points are synchronous method calls made by the test; indirect interaction points require some form of Back Door Manipulation. Control points have arrows pointing toward the SUT; observation points have arrows pointing away from the SUT.\n\n6 I am deliberately not saying “SUT” here because it interacts with more than just the SUT.\n\nwww.it-ebooks.info",
      "content_length": 2036,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "How Do We Ensure Testability?\n\nA control point is how the test asks the software to do something for it. This could be for the purpose of putting the software into a speciﬁ c state as part of setting up or tearing down the test ﬁ xture, or it could be to exercise the SUT. Some control points are provided strictly for the tests; they should not be used by the production code because they bypass input validation or short-circuit the normal life cycle of the SUT or some object on which it depends.\n\nAn observation point is how the test ﬁ nds out about the SUT’s behavior dur- ing the result veriﬁ cation phase of the test. Observation points can be used to retrieve the post-test state of the SUT or a DOC. They can also be used to spy on the interactions between the SUT and any components with which it is expected to interact while it is being exercised. Verifying these indirect outputs is an example of Back Door Veriﬁ cation (see Back Door Manipulation).\n\nBoth control points and observation points can be provided by the SUT as synchronous method calls; we call this “going in the front door.” Some inter- action points may be via a “back door” to the SUT; we call this Back Door Manipulation. In the diagrams that follow, control points are represented by the arrowheads that point to the SUT, whether from the test or from a DOC. Observation points are represented by the arrows whose heads point back to the test itself. These arrows typically start at the SUT or DOC7 or start at the test and interact with either the SUT or DOC before returning to the test.8\n\nInteraction Styles and Testability Patterns\n\nWhen testing a particular piece of software, our tests can take one of two basic forms.\n\nA round-trip test interacts with the SUT in question only through its public interface—that is, its “front door” (Figure 6.7). Both the control points and the observation points in a typical round-trip test are simple method calls. The nice thing about this approach is that it does not violate encapsulation. The test needs to know only the public interface of the software; it doesn’t need to know anything about how it is built.\n\nThe main alternative is the layer-crossing test (Figure 6.8), in which we exer- cise the SUT through the API and keep an eye on what comes out the back door using some form of Test Double such as a Test Spy (page 538) or Mock Object. This can be a very powerful testing technique for verifying certain kinds of mostly architectural requirements. Unfortunately, this approach can also result in Overspeciﬁ ed Software (see Fragile Test) if it is overused because changes in how the software implements its responsibilities can cause tests to fail.\n\n7 An asynchronous observation point. 8 A synchronous observation point.\n\nwww.it-ebooks.info\n\n67",
      "content_length": 2784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "68\n\nChapter 6 Test Automation Strategy\n\nRealComponentTest RealComponentTest\n\nFakeComponentTest FakeComponentTest\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\ntestMethod_2 testMethod_2\n\nInstallation Installation\n\nSUT SUT\n\nCreation Creation\n\nSUT SUT\n\nDOC DOC\n\nFake Fake Object Object\n\nDOC DOC\n\nFigure 6.7 A round-trip test interacts with the SUT only via the front door. The test on the right replaces a DOC with a Fake Object to improve its repeatability or performance.\n\nIndirectInputTest IndirectInputTest\n\nIndirectOutputTest IndirectOutputTest\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\ntestMethod_2 testMethod_2\n\nInstallation Installation\n\nInstallation Installation\n\nCreation Creation and and Configuration Configuration\n\nSUT SUT\n\nIndirect Indirect Inputs Inputs\n\nCreation Creation and and Configuration Configuration\n\nSUT SUT\n\nIndirect Indirect Outputs Outputs\n\nTest Stub Test Stub\n\nMock Object Mock Object\n\nDOC DOC\n\nDOC DOC\n\nFigure 6.8 A layer-crossing test can interact with the SUT via a “back door.” The test on the left controls the SUT’s indirect inputs using a Test Stub. The test on the right veriﬁ es its indirect outputs using a Mock Object.\n\nwww.it-ebooks.info",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "How Do We Ensure Testability?\n\nIn Figure 6.8, the test on the right uses a Mock Object that stands in for the DOC as the observation point. The test on the left uses a Test Stub that stands in for the DOC as a control point. Testing in this style implies a Layered Architecture [DDD, PEAA, WWW], which in turn opens the door to using Layer Tests (page 337) to test each layer of the architecture independently (Figure 6.9). An even more general concept is the use of Component Tests (see Layer Test) to test each com- ponent within a layer independently.\n\nLayer1TestcaseClass Layer1TestcaseClass testMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nLayernTestcaseClass LayernTestcaseClass testMethod_1 testMethod_1 testMethod_2 testMethod_2\n\nLayer n Layer n\n\nLayer 1 Layer 1\n\nTest Double Test Double\n\nTest Double Test Double\n\nDOC DOC\n\nFigure 6.9 A pair of Layer Tests each testing a different layer of the system. Each layer of a layered architecture can be tested independently using a distinct set of Layer Tests. This ensures good separation of concerns, and the tests reinforce the layered architecture.\n\nWhenever we want to write layer-crossing tests, we need to ensure that we have built in a substitutable dependency mechanism for any components on which the SUT depends but that we want to test independently. The leading contend- ers include any of the variations of Dependency Injection (Figure 6.10) or some form of Dependency Lookup such as Object Factory or Service Locator. These dependency substitution mechanisms can be hand-coded or we can use an in- version of control (IOC) framework if one is available in our programming environment. The fallback plan is to use a Test-Speciﬁ c Subclass (page 579) of the SUT or the DOC in question. This subclass can be used to override the dependency access or construction mechanism within the SUT or to replace the behavior of the DOC with test-speciﬁ c behavior.\n\nwww.it-ebooks.info\n\n69",
      "content_length": 1946,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "70\n\nChapter 6 Test Automation Strategy\n\nThe “solution of last resort” is the Test Hook (page 709).9 These constructs do have utility as temporary measures that allow us to automate tests to act as a Safety Net (page 24) while refactoring to retroﬁ t testability. We deﬁ nitely shouldn’t make a habit of using them, however, as continued use of Test Hooks will result in Test Logic in Production (page 217).\n\nClient Client\n\nCreation Creation\n\nDOC DOC\n\nUsage Usage\n\nUsage Usage\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nExercise Exercise\n\nCreation Creation\n\nSUT SUT\n\nUsage Usage\n\nTest Test Double Double\n\nFigure 6.10 A Test Double being “injected” into a SUT by a test. A test can use Dependency Injection to replace a DOC with an appropriate Test Double. The DOC is passed to the SUT by the test as or after it has been created.\n\nA third kind of test worth mentioning is the asynchronous test, in which the test interacts with the SUT through real messaging. Because the responses to these requests also come asynchronously, these tests must include some kind of inter- process synchronization such as calls to wait. Unfortunately, the need to wait for message responses that might never arrive can cause these tests to take much, much longer to execute. This style of testing should be avoided at all costs in unit and component tests.\n\nFortunately, the Humble Executable pattern (see Humble Object on page 695) can remove the need to conduct unit tests this way (Figure 6.11). It involves putting the logic that handles the incoming message into a separate class or component, which can then be tested synchronously using either a round-trip or layer-crossing style.\n\nA related issue is the testing of business logic through a UI. In general, such Indirect Testing (see Obscure Test) is a bad idea because changes to the UI code will break tests that are trying to verify the business logic behind it. Because the UI tends to change frequently, especially on agile projects, this strategy will greatly increase test maintenance costs. Another reason this is a bad idea is that\n\n9 These typically take the form of if (testing) then ... else ... endif.\n\nwww.it-ebooks.info",
      "content_length": 2193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "How Do We Ensure Testability?\n\nUIs are inherently asynchronous. Tests that exercise the system through the UI have to be asynchronous tests along with all the issues that come with them.\n\nAsynchronous Asynchronous Interface Interface\n\nHumble Humble Executable Executable\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nSynchronous Synchronous Interface Interface\n\nTestable Testable\n\nVerify Verify\n\nComponent Component\n\nTeardown Teardown\n\nFigure 6.11 A Humble Executable making testing easier. The Humble Executable pattern can improve the repeatability and speed of verifying logic that would otherwise have to be veriﬁ ed via asynchronous tests.\n\nDivide and Test\n\nWe can turn almost any Hard-to-Test Code (page 209) into easily tested code through refactoring as long as we have enough tests in place to ensure that we do not introduce bugs during this refactoring.\n\nWe can avoid using the UI for customer tests by writing those tests as Subcu- taneous Tests (see Layer Test). These tests bypass the UI layer of the system and exercise the business logic via a Service Facade [CJ2EEP] that exposes the neces- sary synchronous interaction points to the test. The UI relies on the same facade, enabling us to verify that the business logic works correctly even before we hook up the UI logic. The layered architecture also enables us to test the UI logic before the business logic is ﬁ nished; we can replace the Service Facade with a Test Double that provides completely deterministic behavior that our tests can depend on.10\n\n10 This Test Double can be either hard-coded or ﬁ le driven. Either way, it should be inde- pendent of the real implementation so that the UI tests need to know only which data to use to evoke speciﬁ c behaviors from the Service Facade, not the logic behind it.\n\nwww.it-ebooks.info\n\n71",
      "content_length": 1814,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "72\n\nChapter 6 Test Automation Strategy\n\nWhen conducting unit testing of nontrivial UIs,11 we can use a Humble Dialog (see Humble Object) to move the logic that makes decisions about the UI out of the visual layer, which is difﬁ cult to test synchronously, and into a layer of supporting objects, which can be veriﬁ ed with standard unit-testing techniques (Figure 6.12). This approach allows the presentation logic behavior to be tested as thoroughly as the business logic behavior.\n\nMock Mock Dialog Dialog\n\nHumble Dialog Humble Dialog Abc Abc def def ghi ghi\n\nOK OK\n\nCancel Cancel\n\nGUI GUI Frame- Frame- work work\n\nSetup Setup\n\nExercise Exercise\n\nTestable Testable\n\nVerify Verify\n\nGUI Logic GUI Logic\n\nComponent Component\n\nTeardown Teardown\n\nFigure 6.12 A Humble Dialog reducing the dependency of the test on the UI framework. The logic that controls the state of UI components can be very difﬁ cult to test. Extracting it into a testable component leaves behind a Humble Dialog that requires very little testing.\n\nFrom a test automation strategy perspective, the key thing is to make the decision about which test–SUT interaction styles should be used and which ones should be avoided, and to ensure that the software is designed to support that decision.\n\n11 Any UI that contains state information or supports conditional display or enabling of elements should be considered nontrivial.\n\nwww.it-ebooks.info",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "What’s Next?\n\nWhat’s Next?\n\nThis concludes our introduction to the hard-to-change decisions we must make as we settle upon our test automation strategy. Given that you are still reading, I will assume that you have decided xUnit is an appropriate tool for doing your test automation. The following chapters introduce the detailed patterns for im- plementing our chosen ﬁ xture strategy, whether it involves a Fresh Fixture or a Shared Fixture. First, we will explore the simplest case, a Transient Fresh Fixture, in Chapter 8, Transient Fixture Management. We will then investigate the use of persistent ﬁ xtures in Chapter 9, Persistent Fixture Management. But ﬁ rst, we must establish the basic xUnit terminology and notation that is used throughout this book in Chapter 7, xUnit Basics.\n\nwww.it-ebooks.info\n\n73",
      "content_length": 813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Chapter 7\n\nxUnit Basics\n\nAbout This Chapter\n\nChapter 6, Test Automation Strategy, introduced the “hard to change” decisions that we need to get right early in the project. The current chapter serves two purposes. First, it introduces the xUnit terminology and diagramming notation used throughout this book. Second, it explains how the xUnit framework oper- ates beneath the covers and why it was built that way. This knowledge can help the builder of a new Test Automation Framework (page 298) understand how to port xUnit. It can also help test automaters understand how to use certain features of xUnit.\n\nAn Introduction to xUnit\n\nThe term xUnit is how we refer to any member of the family of Test Automa- tion Frameworks used for automating Hand-Scripted Tests (see Scripted Test on page 285) that share the common set of features described here. Most pro- gramming languages in widespread use today have at least one implementation of xUnit; Hand-Scripted Tests are usually automated using the same program- ming language as is used for building the SUT. Although this is not necessarily the case, this strategy is usually much easier because our tests have easy access to the SUT API. By using a programming language with which the developers are familiar, less effort is required to learn how to automate Fully Automated Tests (page 26).1\n\n1 See the sidebar “Testing Stored Procs with JUnit” (page 657) for an example of using a testing framework in one language to test an SUT in another language.\n\n75\n\nwww.it-ebooks.info",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "76\n\nChapter 7 xUnit Basics\n\nCommon Features\n\nGiven that most members of the xUnit family are implemented using an object- oriented programming language (OOPL), they are described here ﬁ rst and then places where the non-OOPL members of the family differ are noted.\n\nAll members of the xUnit family implement a basic set of features. They all\n\nprovide a way to perform the following tasks:\n\nSpecify a test as a Test Method (page 348)\n\nSpecify the expected results within the test method in the form of calls\n\nto Assertion Methods (page 362)\n\nAggregate the tests into test suites that can be run as a single operation\n\nRun one or more tests to get a report on the results of the test run\n\nBecause many members of the xUnit family support Test Method Discovery (see Test Discovery on page 393), we do not have to use Test Enumeration (page 399) in these members to manually add each Test Method we want to run to a test suite. Some members also support some form of Test Selection (page 403) to run subsets of test methods based on some criteria.\n\nThe Bare Minimum\n\nHere is the bare minimum we need to understand about how xUnit operates (Figure 7.1):\n\nHow we deﬁ ne tests using Test Methods on Testcase Classes (page 373)\n\nHow we can build up arbitrary Suites of Suites (see Test Suite Object on page 387)2\n\nHow we run the tests\n\nHow we interpret the test results\n\nDeﬁ ning Tests\n\nEach test is represented by a Test Method that implements a single Four-Phase Test (page 358) by following these steps:\n\n2 Even those xUnit variants that don’t have an explicit Suite class or method still build Test Suite Objects behind the scene.\n\nwww.it-ebooks.info",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "The Bare Minimum\n\nSetting up the test ﬁ xture using either In-line Setup (page 408), Delegated\n\nSetup (page 411), or Implicit Setup (page 424)\n\nExercising the SUT by interacting with methods in its public or private\n\ninterface\n\nVerifying that the expected outcome has occurred using calls to Assertion\n\nMethods\n\nTearing down the test ﬁ xture using either Garbage-Collected Tear- down (page 500), In-line Teardown (page 509), Implicit Teardown (page 516), or Automated Teardown (page 503)\n\nTest Test Suite Suite Factory Factory\n\ntestMethod_1 testMethod_1\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\nSuite Suite\n\nTest Runner Test Runner\n\nCreate Create\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite\n\nSetup Setup\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nObject Object\n\nTeardown Teardown\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nFigure 7.1 The static test structure as seen by a test automater. The test automater sees only the static structure as he or she reads or writes tests. The test automater writes one Test Method with four distinct phases for each test in the Testcase Class. The Test Suite Factory (see Test Enumeration) is used only for Test Enumeration. The runtime structure (shown grayed out) is left to the test automater’s imagination.\n\nThe most common types of tests are the Simple Success Test (see Test Method), which veriﬁ es that the SUT has behaved correctly with valid inputs, and the Expected Exception Test (see Test Method), which veriﬁ es that the SUT raises an exception when used incorrectly. A special type of test, the Constructor Test (see Test Method), veriﬁ es that the object constructor logic builds new objects cor- rectly. Both “simple success” and “expected exception” forms of the Constructor Test may be needed. The Test Methods that contain our test logic need to live\n\nwww.it-ebooks.info\n\n77",
      "content_length": 2014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "78\n\nChapter 7 xUnit Basics\n\nsomewhere, so we deﬁ ne them as methods of a Testcase Class.3 We then pass the name of this Testcase Class (or the module or assembly in which it resides) to the Test Runner (page 377) to run our tests. This may be done explicitly—such as when invoking the Test Runner on a command line—or implicitly by the integrated development environment (IDE) that we are using.\n\nWhat’s a Fixture?\n\nThe test ﬁ xture is everything we need to have in place to exercise the SUT. Typi- cally, it includes at least an instance of the class whose method we are testing. It may also include other objects on which the SUT depends. Note that some mem- bers of the xUnit family call the Testcase Class the test ﬁ xture—a preference that likely reﬂ ects an assumption that all Test Methods on the Testcase Class should use the same ﬁ xture. This unfortunate name collision makes discussing test ﬁ xtures particularly problematic. In this book, I have used different names for the Testcase Class and the test ﬁ xture it creates. I trust that the reader will translate this termi- nology to the terminology of his or her particular member of the xUnit family.\n\nDeﬁ ning Suites of Tests\n\nMost Test Runners “auto-magically” construct a test suite containing all of the Test Methods in the Testcase Class. Often, this is all we need. Sometimes we want to run all the tests for an entire application; at other times we want to run just those tests that focus on a speciﬁ c subset of the functionality. Some mem- bers of the xUnit family and some third-party tools implement Testcase Class Discovery (see Test Discovery) in which the Test Runner ﬁ nds the test suites by searching either the ﬁ le system or an executable for test suites.\n\nIf we do not have this capability, we need to use Test Suite Enumeration (see Test Enumeration), in which we deﬁ ne the overall test suite for the entire system or application as an aggregate of several smaller test suites. To do so, we must deﬁ ne a special Test Suite Factory class whose suite method returns a Test Suite Object containing the collection of Test Methods and other Test Suite Objects to run.\n\nThis collection of test suites into increasingly larger Suites of Suites is com- monly used as a way to include the unit test suite for a class into the test suite for the package or module, which is in turn included in the test suite for the entire system. Such a hierarchical organization supports the running of test suites with varying degrees of completeness and provides a practical way for developers to run that subset of the tests that is most relevant to the software of\n\n3 This scheme is called a test ﬁ xture in some variants of xUnit, probably because the creators assumed we would have a single Testcase Class per Fixture (page 631).\n\nwww.it-ebooks.info",
      "content_length": 2817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "The Bare Minimum\n\ninterest. It also allows them to run all the tests that exist with a single command before they commit their changes into the source code repository [SCM].\n\nRunning Tests\n\nTests are run by using a Test Runner. Several different kinds of Test Runners are available for most members of the xUnit family.\n\nA Graphical Test Runner (see Test Runner) provides a visual way for the user to specify, invoke, and observe the results of running a test suite. Some Graphi- cal Test Runners allow the user to specify a test by typing in the name of a Test Suite Factory; others provide a graphical Test Tree Explorer (see Test Runner) that can be used to select a speciﬁ c Test Method to execute from within a tree of test suites, where the Test Methods serve as the tree’s leaves. Many Graphical Test Runners are integrated into an IDE to make running tests as easy as select- ing the Run As Test command from a context menu.\n\nA Command-Line Test Runner (see Test Runner) can be used to execute tests when running the test suite from the command line, as in Figure 7.2. The name of the Test Suite Factory that should be used to create the test suite is included as a command-line parameter. Command-Line Test Runners are most common- ly used when invoking the Test Runner from Integration Build [SCM] scripts or sometimes from within an IDE.\n\n>ruby testrunner.rb c:/examples/tests/SmellHandlerTest.rb Loaded suite SmellHandlerTest Started ..... Finished in 0.016 seconds. 5 tests, 6 assertions, 0 failures, 0 errors >Exit code: 0\n\nFigure 7.2 Using a Command-Line Test Runner to run tests from the command line.\n\nTest Results\n\nNaturally, the main reason for running automated tests is to determine the re- sults. For the results to be meaningful, we need a standard way to describe them. In general, members of the xUnit family follow the Hollywood principle (“Don’t call us; we’ll call you”). In other words, “No news is good news”; the tests will “call you” when a problem occurs. Thus we can focus on the test failures rather than inspecting a bunch of passing tests as they roll by.\n\nTest results are classiﬁ ed into one of three categories, each of which is trea- ted slightly differently. When a test runs without any errors or failures, it is\n\nwww.it-ebooks.info\n\n79",
      "content_length": 2279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "80\n\nChapter 7 xUnit Basics\n\nconsidered to be successful. In general, xUnit does not do anything special for successful tests—there should be no need to examine any output when a Self- Checking Test (page 26) passes.\n\nA test is considered to have failed when an assertion fails. That is, the test asserts that something should be true by calling an Assertion Method, but that assertion turns out not to be the case. When it fails, an Assertion Method throws an assertion failure exception (or whatever facsimile the programming language supports). The Test Automation Framework increments a counter for each failure and adds the failure details to a list of failures; this list can be ex- amined more closely later, after the test run is complete. The failure of a single test, while signiﬁ cant, does not prevent the remaining tests from being run; this is in keeping with the principle Keep Tests Independent (see page 42).\n\nA test is considered to have an error when either the SUT or the test itself fails in an unexpected way. Depending on the language being used, this problem could consist of an uncaught exception, a raised error, or something else. As with assertion failures, the Test Automation Framework increments a counter for each error and adds the error details to a list of errors, which can then be examined after the test run is complete.\n\nFor each test error or test failure, xUnit records information that can be ex- amined to help understand exactly what went wrong. As a minimum, the name of the Test Method and Testcase Class are recorded, along with the nature of the problem (whether it was a failed assertion or a software error). In most Graphical Test Runners that are integrated with an IDE, one merely has to (double-) click on the appropriate line in the traceback to see the source code that emitted the failure or caused the error.\n\nBecause the name test error sounds more drastic than a test failure, some test automaters try to catch all errors raised by the SUT and turn them into test failures. This is simply unnecessary. Ironically, in most cases it is easier to deter- mine the cause of a test error than the cause of a test failure: The stack trace for a test error will typically pinpoint the problem code within the SUT, whereas the stack track for a test failure merely shows the location in the test where the failed assertion was made. It is, however, worthwhile using Guard Asser- tions (page 490) to avoid executing code within the Test Method that would result in a test error being raised from within the Test Method4 itself; this is just a normal part of verifying the expected outcome of exercising the SUT and does not remove useful diagnostic tracebacks.\n\n4 For example, before executing an assertion on the contents of a ﬁ eld of an object returned by the SUT, it is worthwhile to assertNotNull on the object reference so as to avoid a “null reference” error.\n\nwww.it-ebooks.info",
      "content_length": 2935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Under the xUnit Covers\n\nUnder the xUnit Covers\n\nThe description thus far has focused on Test Methods and Testcase Classes with the odd mention of test suites. This simpliﬁ ed “compile time” view is enough for most people to get started writing automated unit tests in xUnit. It is pos- sible to use xUnit without any further understanding of how the Test Automa- tion Framework operates—but the lack of more extensive knowledge is likely to lead to confusion when building and reusing test ﬁ xtures. Thus it is better to understand how xUnit actually runs the Test Methods. In most5 members of the xUnit family, each Test Method is represented at runtime by a Testcase Object (page 382) because it is a lot easier to manipulate tests if they are “ﬁ rst- class” objects (Figure 7.3). The Testcase Objects are aggregated into Test Suite Objects, which can then be used to run many tests with a single user action.\n\nTest Test Suite Suite Factory Factory\n\nCreate Create\n\ntestMethod_1 testMethod_1\n\nTestcase Testcase Object Object testMethod_1 testMethod_1\n\nSuite Suite\n\nTest Runner Test Runner\n\nCreate Create\n\nRun Run\n\nTest Test Suite Suite Object Object\n\nRun Run\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nRun Testcase Run Testcase Object Object testMethod_n testMethod_n\n\nExercise Exercise\n\nTestcase Testcase Class Class\n\nFigure 7.3 The runtime test structure as seen by the Test Automation Framework. At runtime, the Test Runner asks the Testcase Class or a Test Suite Factory to instantiate one Testcase Object for each Test Method, with the objects being wrapped up in a single Test Suite Object. The Test Runner tells this Composite [GOF] object to run its tests and collect the results. Each Testcase Object runs one Test Method.\n\n5 NUnit is a known exception and others may exist. See the sidebar “There’s Always an Exception” (page 384) for more information.\n\nwww.it-ebooks.info\n\n81",
      "content_length": 1932,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "82\n\nChapter 7 xUnit Basics\n\nTest Commands\n\nThe Test Runner cannot possibly know how to call each Test Method individu- ally. To avoid the need for this, most members of the xUnit family convert each Test Method into a Command [GOF] object with a run method. To create these Testcase Objects, the Test Runner calls the suite method of the Testcase Class to get a Test Suite Object. It then calls the run method via the standard test inter- face. The run method of a Testcase Object executes the speciﬁ c Test Method for which it was instantiated and reports whether it passed or failed. The run method of a Test Suite Object iterates over all the members of the collection of tests, keeping track of how many tests were run and which ones failed.\n\nTest Suite Objects\n\nA Test Suite Object is a Composite object that implements the same standard test interface that all Testcase Objects implement. That interface (implicit in lan- guages lacking a type or interface construct) requires provision of a run method. The expectation is that when run is invoked, all of the tests contained in the receiver will be run. In the case of a Testcase Object, it is itself a “test” and will run the corresponding Test Method. In the case of a Test Suite Object, that means invoking run on all of the Testcase Objects it contains. The value of using a Composite Command is that it turns the processes of running one test and running many tests into exactly the same process.\n\nTo this point, we have assumed that we already have the Test Suite Object instantiated. But where did it come from? By convention, each Testcase Class acts as a Test Suite Factory. The Test Suite Factory provides a class method called suite that returns a Test Suite Object containing one Testcase Object for each Test Method in the class. In languages that support some form of reﬂ ec- tion, xUnit may use Test Method Discovery to discover the test methods and automatically construct the Test Suite Object containing them. Other mem- bers of the xUnit family require test automaters to implement the suite method themselves; this kind of Test Enumeration takes more effort and is more likely to lead to Lost Tests (see Production Bugs on page 268).\n\nxUnit in the Procedural World\n\nTest Automation Frameworks and test-driven development became popular only after object-oriented programming became commonplace. Most members of the xUnit family are implemented in object-oriented programming languages\n\nwww.it-ebooks.info",
      "content_length": 2481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "What’s Next?\n\nthat support the concept of a Testcase Object. Although the lack of objects should not keep us from testing procedural code, it does make writing Self- Checking Tests more labor-intensive and building generic, reusable Test Runners more difﬁ cult.\n\nIn the absence of objects or classes, we must treat Test Methods as global (public static) procedures. These methods are typically stored in ﬁ les or mod- ules (or whatever modularity mechanism the language supports). If the language supports the concept of procedure variables (also known as function pointers), we can deﬁ ne a generic Test Suite Procedure (see Test Suite Object) that takes an array of Test Methods (commonly called “test procedures”) as an argument. Typically, the Test Methods must be aggregated into the arrays using Test Enu- meration because very few non-object-oriented programming languages sup- port reﬂ ection.\n\nIf the language does not support any way of treating Test Methods as data, we must deﬁ ne the test suites by writing Test Suite Procedures that make explicit calls to Test Methods and/or other Test Suite Procedures. Test runs may be initi- ated by deﬁ ning a main method on the module.\n\nA ﬁ nal option is to encode the tests as data in a ﬁ le and use a single Data- Driven Test (page 288) interpreter to execute them. The main disadvantage of this approach is that it restricts the kinds of tests that can be run to those imple- mented by the Data-Driven Test interpreter, which must itself be written anew for each SUT. This strategy does have the advantage of moving the coding of the actual tests out of the developer arena and into the end-user or tester arena, which makes it particularly appropriate for customer tests.\n\nWhat’s Next?\n\nIn this chapter we established the basic terminology for talking about how xUnit tests are put together. Now we turn our attention to a new task—constructing our ﬁ rst test ﬁ xture in Chapter 8, Transient Fixture Management.\n\nwww.it-ebooks.info\n\n83",
      "content_length": 1992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Chapter 8\n\nTransient Fixture Management\n\nAbout This Chapter\n\nChapter 6, Test Automation Strategy, looked at the strategic decisions that we need to make. That included the deﬁ nition of the term “ﬁ xture” and the selection of a test ﬁ xture strategy. Chapter 7, xUnit Basics, established our basic xUnit terminol- ogy and diagramming notation. This chapter builds on both of these earlier chap- ters by focusing on the mechanics of implementing the chosen ﬁ xture strategy.\n\nThere are several different ways to set up a Fresh Fixture (page 311), and our decision will affect how much effort it takes to write the tests, how much effort\n\nTransient\n\nFresh Fixture\n\nPersistent\n\nShared Fixture\n\nImmutable Shared Fixture\n\nFigure 8.1 Transient Fresh Fixture. Fresh Fixtures come in two ﬂ avors: Transient and Persistent. Both require ﬁ xture setup; the latter also requires ﬁ xture teardown.\n\n85\n\nwww.it-ebooks.info",
      "content_length": 909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "86\n\nChapter 8 Transient Fixture Management\n\nit takes to maintain our tests, and whether we achieve Tests as Documentation (see page 23). Persistent Fresh Fixtures (see Fresh Fixture) are set up the same way as Transient Fresh Fixtures (see Fresh Fixture), albeit with some additional factors to consider related to ﬁ xture teardown (Figure 8.1). Shared Fixtures (page 317) introduce another set of considerations. Persistent Fresh Fixtures and Shared Fixtures are discussed in detail in Chapter 9.\n\nTest Fixture Terminology\n\nBefore we can talk about setting up a ﬁ xture, we need to agree what a ﬁ xture is.\n\nWhat Is a Fixture?\n\nEvery test consists of four parts, as described in Four-Phase Test (page 358). The ﬁ rst part is where we create the SUT and everything it depends on and where we put those elements into the state required to exercise the SUT. In xUnit, we call everything we need in place to exercise the SUT the test ﬁ xture and the part of the test logic that we execute to set it up the ﬁ xture setup.\n\nThe most common way to set up the ﬁ xture is using front door ﬁ xture set- up—that is, to call the appropriate methods on the SUT to put it into the start- ing state. This may require constructing other objects and passing them to the SUT as arguments of method calls. When the state of the SUT is stored in other objects or components, we can do Back Door Setup (see Back Door Manipulation on page 327)—that is, we can insert the necessary records directly into the other component on which the behavior of the SUT depends. We use Back Door Setup most often with databases or when we need to use a Mock Object (page 544) or Test Double (page 522). These possibilities are covered in Chapter 13, Testing with Databases, and Chapter 11, Using Test Doubles, respectively.\n\nIt is worth noting that the term “ﬁ xture” is used to mean different things in different kinds of test automation. The xUnit variants for the Microsoft lan- guages call the Testcase Class (page 373) the test ﬁ xture. Most other variants of xUnit distinguish between the Testcase Class and the test ﬁ xture (or test con- text) it sets up. In Fit [FitB], the term “ﬁ xture” is used to mean the custom-built parts of the Data-Driven Test (page 288) interpreter that we use to deﬁ ne our Higher-Level Language (see page 41). Whenever this book says “test ﬁ xture” without further qualifying this term, it refers to the stuff we set up before ex- ercising the SUT. To refer to the class that hosts the Test Methods (page 348), whether it be in Java or C#, Ruby or VB, this book uses Testcase Class.\n\nwww.it-ebooks.info",
      "content_length": 2603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Test Fixture Terminology\n\nWhat Is a Fresh Fixture?\n\nIn a Fresh Fixture strategy, we set up a brand-new ﬁ xture for every test we run (Figure 8.2). That is, each Testcase Object (page 382) builds its own ﬁ xture be- fore exercising the SUT and does so every time it is rerun. That is what makes the ﬁ xture “fresh.” As a result, we completely avoid the problems associated with Interacting Tests (see Erratic Test on page 228).\n\nSetup Setup\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nVerify Verify\n\nFixture Fixture\n\nSUT SUT\n\nExercise Exercise\n\nTeardown Teardown\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nFigure 8.2 A pair of Fresh Fixtures, each with its creator. A Fresh Fixture is built speciﬁ cally for a single test, used once, and then retired.\n\nWhat Is a Transient Fresh Fixture?\n\nWhen our ﬁ xture is an in-memory ﬁ xture referenced only by local variables or instance variables,1 the ﬁ xture just “disappears” after every test courtesy of Garbage-Collected Teardown (page 500). When ﬁ xtures are persistent, this is not the case. Thus we have some decisions to make about how we implement the Fresh Fixture strategy. In particular, we have two different ways to keep them “fresh.” The obvious option is tear down the ﬁ xture after each test. The less obvious option is to leave the old ﬁ xture around and then build a new ﬁ xture in such a way that it does not collide with the old ﬁ xture.\n\n1 See the sidebar “There’s Always an Exception” (page 384).\n\nwww.it-ebooks.info\n\n87",
      "content_length": 1490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "88\n\nChapter 8 Transient Fixture Management\n\nMost Fresh Fixtures we build are transient, so we will cover that case ﬁ rst.\n\nWe will then come back to managing Persistent Fresh Fixtures in Chapter 9.\n\nBuilding Fresh Fixtures\n\nWhether we are building a Transient Fresh Fixture or a Persistent Fresh Fixture, the choices we have for how to construct it are pretty much the same. The ﬁ xture setup logic includes the code needed to instantiate the SUT,2 the code to put the SUT into the appropriate starting state, and the code to create and initialize the state of anything the SUT depends on or that will be passed to it as an argument. The most obvious way to set up a Fresh Fixture is through In-line Setup (page 408), in which all ﬁ xture setup logic is contained within the Test Method. This type of ﬁ xture can also be constructed by using Delegated Setup (page 411), which involves calling Test Utility Methods (page 599). Finally, we can use Implicit Setup (page 424), in which the Test Automation Framework (page 298) calls a special setUp method we provide on our Testcase Class. We can also use a combination of these three approaches. Let’s look at each possibility individually.\n\nIn-line Fixture Setup\n\nIn In-line Setup, the test handles all of the fixture setup within the body of the Test Method. We construct objects, call methods on them, construct the SUT, and call methods on it to put into a specific state. We perform all of these tasks from within our Test Method. Think of In-line Setup as the do- it-yourself approach to fixture creation.\n\npublic void testStatus_initial() { // In-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); // Exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // tearDown: // Garbage-collected }\n\n2 This discussion assumes that the SUT is an object and not just static methods on a class.\n\nwww.it-ebooks.info",
      "content_length": 2047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Building Fresh Fixtures\n\nThe main drawback of In-line Setup is that it tends to lead to Test Code Dupli- cation (page 213) because each Test Method needs to construct the SUT. Many of the Test Methods also need to perform similar ﬁ xture setup. This Test Code Duplication leads, in turn, to High Test Maintenance Cost (page 265) caused by Fragile Tests (page 239). If the work to create the ﬁ xture is complex, it can also lead to Obscure Tests (page 186). A related problem is that In-line Setup tends to encourage Hard-Coded Test Data (see Obscure Test) within each Test Method because creating a local variable with an Intent-Revealing Name [SBPP] may seem like too much work for the beneﬁ t yielded.\n\nWe can prevent these test smells by moving the code that sets up the ﬁ xture out of the Test Method. The location where we move it determines which of the alternative ﬁ xture setup strategies we have used.\n\nDelegated Fixture Setup\n\nA quick and easy way to reduce Test Code Duplication and the resulting Obscure Tests is to refactor our Test Methods to use Delegated Setup. We can use an Extract Method [Fowler] refactoring to move a sequence of statements used in several Test Methods into a Test Utility Method that we then call from those Test Methods. This is a very simple and safe refactoring, especially when we let the IDE do all the heavy lifting for us. When the extracted method contains logic to create an object on which our test depends, we call it a Creation Method (page 415). Creation Methods3 with Intent-Revealing Names make the test’s pre-conditions readily apparent to the reader while avoiding unnecessary Test Code Duplication. They allow both the test reader and the test automater to focus on what is being created without being distracted by how it is created. The Creation Methods act as reusable building blocks for test ﬁ xture construction.\n\npublic void testGetStatus_inital() { // Setup Flight ﬂight = createAnonymousFlight(); // Exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // Teardown // Garbage-collected }\n\nOne goal of these Creation Methods is to eliminate the need for every test to know the details of how the objects it requires are created. This stream- lining goes a long way toward preventing Fragile Tests caused by changes to\n\n3 When referenced via a Test Helper (page 643) class, they are often called the Object Mother pattern (see Test Helper on page 643).\n\nwww.it-ebooks.info\n\n89",
      "content_length": 2473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "90\n\nChapter 8 Transient Fixture Management\n\nconstructor method signatures or semantics. When a test does not care about the speciﬁ c identity of the objects it is creating, we can use Anonymous Cre- ation Methods (see Creation Method). These methods generate any unique keys required by the object being created. By using a Distinct Generated Value (see Generated Value on page 723), we can guarantee that no other test instance that requires a similar object will accidentally use the same object as this test. This safeguard prevents many forms of the behavior smell Erratic Test, includ- ing Unrepeatable Tests, Interacting Tests, and Test Run Wars, even if we hap- pen to be using a persistent object repository that supports Shared Fixtures.\n\nWhen a test does care about the attributes of the object being created, we use a Parameterized Anonymous Creation Method (see Creation Method). This method is passed any attributes that the test cares about (i.e., attributes that are important to the test outcome), leaving all other attributes to be defaulted by the implementation of the Creation Method. My motto is this:\n\nWhen it is not important for something to be seen in the test method, it is important that it not be seen in the test method!\n\nDelegated Setup is often used when we write input validation tests for SUT methods that are expected to validate the attributes of an object argu- ment. In such a case, we need to write a separate test for each invalid at- tribute that should be detected. Building all of these slightly invalid objects would be a lot of work using In-line Setup. We can reduce the effort and the amount of Test Code Duplication dramatically by using the pattern One Bad Attribute (see Derived Value on page 718). That is, we first call a Creation Method to create a valid object, and then we replace one attri- bute with an invalid value that should be rejected by the SUT. Similarly, we might create an object in the correct state by using a Named State Reaching Method (see Creation Method).\n\nSome people prefer to Reuse Tests for Fixture Setup (see Creation Method) as an alternative to using Chained Tests (page 454). That is, they call other tests directly within the setup portion of their test. This approach is not an unreasonable one as long as the test reader can readily identify what the other test is setting up for the current test. Unfortunately, very few tests are named in such a way as to convey this intention. For this reason, if we value Tests as Documentation, we will want to con- sider wrapping the called test with a Creation Method that has an Intent-Revealing Name so that test reader can get a sense of what the ﬁ xture looks like.\n\nThe Creation Methods can be kept as private methods on the Testcase Class, pulled up to a Testcase Superclass (page 638), or moved to a Test Help- er (page 643). The “mother of all creation methods” is Object Mother (see Test\n\nwww.it-ebooks.info",
      "content_length": 2941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Building Fresh Fixtures\n\nHelper). This strategy-level pattern describes a family of approaches that center on the use of Creation Methods on one or more Test Helpers and may include Automated Teardown (page 503) as well.\n\nImplicit Fixture Setup\n\nMost members of the xUnit family provide a convenient hook for calling code that needs to be run before every Test Method. Some members call a method with a speciﬁ c name (e.g., setUp). Others call a method that has a speciﬁ c annota- tion (e.g., “@before” in JUnit) or method attribute (e.g., “[Setup]” in NUnit). To avoid repeating these alternative ways every time we need to refer to this mecha- nism, this book simply calls it the setUp method regardless of how we indicate this fact to the Test Automation Framework. The setUp method is optional or an empty default implementation is provided by the framework, so we do not have to provide one in each Testcase Class.\n\nIn Implicit Setup, we take advantage of this framework “hook” by putting all of the ﬁ xture creation logic into the setUp method. Because every Test Method on the Testcase Class shares this ﬁ xture setup logic, all Test Methods need to be able to use the ﬁ xture it creates. This tactic certainly addresses the Test Code Duplica- tion problem but it does have several consequences. What does the following test actually verify?\n\nAirport departureAirport; Airport destinationAirport; Flight ﬂight;\n\npublic void testGetStatus_inital() { // Implicit setup // Exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); }\n\nThe ﬁ rst consequence is that this approach can make the tests more difﬁ cult to understand because we cannot see how the pre-conditions of the test (the test ﬁ xture) correlate with the expected outcome within the Test Method; we have to look in the setUp method to see this relationship.\n\npublic void setUp() throws Exception{ super.setUp(); departureAirport = new Airport(\"Calgary\", \"YYC\"); destinationAirport = new Airport(\"Toronto\", \"YYZ\"); BigDecimal ﬂightNumber = new BigDecimal(\"999\"); ﬂight = new Flight( ﬂightNumber , departureAirport, destinationAirport); }\n\nwww.it-ebooks.info\n\n91",
      "content_length": 2159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "92\n\nChapter 8 Transient Fixture Management\n\nWe can mitigate this problem by naming our Testcase Class based on the test ﬁ xture created in the setUp method. Of course, this makes sense only if all of the Test Methods really need the same ﬁ xture—it is an example of Testcase Class per Fixture (page 631). As mentioned earlier, several members of the xUnit family (VbUnit and NUnit, to name two) use the term “test ﬁ xture” to describe what this book calls the Testcase Class. This nomenclature is probably based on the assumption that we are using a Testcase Class per Fixture strategy.\n\nAnother consequence of using Implicit Setup is that we cannot use local vari- ables to hold references to the objects in our ﬁ xture. Instead, we are forced to use instance variables to refer to any objects that are constructed in the setUp method and that are needed either when exercising the SUT, when verifying the expected outcome, or when tearing down the ﬁ xture. These instance vari- ables act as global variables between the parts of the test. As long as we stick to instance variables rather than class variables, however, the test ﬁ xture will be newly constructed for each test case in the Testcase Class. Most members of xUnit provide isolation between the ﬁ xture created for each Test Method but at least one (NUnit) does not; see the sidebar “There’s Always an Excep- tion” (page 384) for more information. In any event, we should deﬁ nitely give the variables Intent-Revealing Names so that we do not need to keep referring back to the setUp method to understand what they hold.\n\nMisuse of the SetUp Method\n\nWhen you have a new hammer, everything looks like a nail!\n\nLike any feature of any system, the setUp method can be abused. We should not feel obligated to use it just because it is provided. It is one of several code reuse mechanisms that are available for our application. When object-oriented languages were ﬁ rst introduced, programmers were enamored with inheritance and tried to apply it in all possible reuse scenarios. Over time, we learned when inheritance was appropriate and when we should resort to other mechanisms such as delegation. The setUp method is xUnit’s inheritance.\n\nThe setUp method is most prone to misuse when it is applied to build a Gen- eral Fixture (see Obscure Test) with multiple distinct parts, each of which is dedicated to a different Test Method. This can lead to Slow Tests (page 253) if we are building a Persistent Fresh Fixture. More importantly, it can lead to Obscure Tests by hiding the cause–effect relationship between the ﬁ xture and the expected outcome of exercising the SUT.\n\nIf we do not adopt the practice of grouping the Test Methods into Testcase Classes based on identical ﬁ xtures but we do use the setUp method, we should build only the lowest common denominator part of the ﬁ xture in the setUp\n\nwww.it-ebooks.info",
      "content_length": 2883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Tearing Down Transient Fresh Fixtures\n\nmethod. That is, only the setup logic that will not cause problems in any of the tests should be placed in the setUp method. Even the ﬁ xture setup code that does not cause problems for any of the Test Methods can still cause other problems if we use the setUp method to build a General Fixture instead of a Minimal Fixture (page 302). A General Fixture is a common cause of Slow Tests because each test spends much more time than necessary building the test ﬁ xture. It also tends to produce Obscure Tests because the test reader cannot easily see which part of the ﬁ xture a particular Test Method depends on. A General Fixture often evolves into a Fragile Fixture (see Fragile Test) as the relationship between its various elements and the tests that use them is forgotten over time. Changes made to the ﬁ xture to support a newly added test may then cause existing tests to fail.\n\nNote that if we use a class variable to hold the object, we may have crossed the line into the world of Persistent Fresh Fixtures. Use of Lazy Setup (page 435) to populate the variable, by contrast, carries us into the world of Shared Fix- tures because later tests within the test suite may reuse the object(s) created in earlier tests and thus may become dependent on the changes the other test (should have) made to it.\n\nHybrid Fixture Setup\n\nThis chapter has presented the three styles of ﬁ xture construction as strict alter- natives to one another. In practice, there is value in combining them. Test auto- maters often call some Creation Methods from within the Test Method but then do some additional setup on an in-line basis. The readability of the setUp method can also be improved if it calls Creation Methods to construct the ﬁ xture. An additional beneﬁ t is that the Creation Methods can be unit-tested much more easily than either in-line ﬁ xture construction logic or the setUp method. These methods can also be located on a class outside the Testcase Class hierarchy such as a Test Helper.\n\nTearing Down Transient Fresh Fixtures\n\nOne really nice thing about Transient Fresh Fixtures is that ﬁ xture teardown requires very little effort. Most members of the xUnit family are implemented in languages that support garbage collection. As long as our references to the ﬁ xture are held in variables that go out of scope, we can count on Garbage-Collected Teardown to do all the work for us. See the sidebar “There’s Always an Exception” on page 384 for a description of why the same is not true in NUnit.\n\nwww.it-ebooks.info\n\n93",
      "content_length": 2566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "94\n\nChapter 8 Transient Fixture Management\n\nIf we are using one of the few members of the xUnit family that does not sup-\n\nport garbage collection, we may have to treat all Fresh Fixtures as persistent.\n\nWhat’s Next?\n\nThis chapter introduced techniques for setting up and tearing down an in-memory Fresh Fixture. With some planning and a bit of luck, they are all you should need for the majority of your tests. Managing Fresh Fixtures is more complicated when the ﬁ xture is persisted either by the SUT or by the test itself. Chapter 9, Persistent Fixture Management, introduces additional techniques needed for managing persistent ﬁ x- tures, including Persistent Fresh Fixtures and Shared Fixtures.\n\nwww.it-ebooks.info",
      "content_length": 721,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "Chapter 9\n\nPersistent Fixture Management\n\nAbout This Chapter\n\nIn Chapter 8, Transient Fixture Management, we saw how we can go about building in-memory Fresh Fixtures (page 311). We noted in that chapter that managing Fresh Fixtures is more complicated when the ﬁ xture is persisted either by the SUT or by the test itself. This chapter introduces the additional patterns required to manage persistent ﬁ xtures, including both Persistent Fresh Fixtures (see Fresh Fixture) and Shared Fixtures (page 317).\n\nManaging Persistent Fresh Fixtures\n\nThe term Persistent Fresh Fixture might sound like an oxymoron but it is actually not as large a contradiction as it might ﬁ rst seem. The Fresh Fixture strat- egy means that each run of each Test Method (page 348) uses a newly created ﬁ x- ture. The name speaks to its intent: We do not reuse the ﬁ xture! It does not need to imply that the ﬁ xture is transient—only that it is not reused (Figure 9.1). Per- sistent Fresh Fixtures present several challenges not encountered with Transient Fresh Fixtures. In this chapter, we focus on the challenge posed by Unrepeatable Tests (see Erratic Test on page 228) caused by leftover Persistent Fresh Fixtures and Slow Tests (page 253) caused by Shared Fixtures (page 317).\n\nWhat Makes Fixtures Persistent?\n\nA ﬁ xture, fresh or otherwise, can become persistent for one of two reasons. The ﬁ rst reason is that the SUT is a stateful object and “remembers” how it was used in the past. This scenario most often occurs when the SUT includes a database,\n\n95\n\nwww.it-ebooks.info",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "96\n\nChapter 9 Persistent Fixture Management\n\nTransient Transient\n\nFresh Fresh Fixture Fixture\n\nPersistent Persistent\n\nShared Shared Fixture Fixture\n\nImmutable Immutable Shared Shared Fixture Fixture\n\nFigure 9.1 A Fresh Fixture can be either transient or persistent. We can apply a Fresh Fixture strategy even if the test ﬁ xture is naturally persistent but we must have a way to tear it down after each test.\n\nbut it can occur simply because the SUT uses class variables to hold some of its data. The second reason is that the Testcase Class (page 373) holds a reference to an otherwise Transient Fresh Fixture in a way that makes it survive across Test Method invocations.\n\nSome members of the xUnit family provide a mechanism to reload all classes at the beginning of each test run. This behavior may appear as an option—a check box labeled “Reload Classes”—or it may be automatic. Such a feature keeps the ﬁ xture from becoming persistent when it is referenced from a class variable; it does not prevent the Fresh Fixture from becoming persistent if either the SUT or the test puts the ﬁ xture into the ﬁ le system or a database.\n\nIssues Caused by Persistent Fresh Fixtures\n\nWhen ﬁ xtures are persistent, we may ﬁ nd that subsequent runs of the same Test Method try to recreate a ﬁ xture that already exists. This behavior may cause conﬂ icts between the preexisting and newly created resources. Although violat- ing unique key constraints in the database is the most common example of this problem, the conﬂ ict could be as simple as trying to create a ﬁ le with the same name as one that already exists. One way to avoid these Unrepeatable Tests is to tear down the ﬁ xture at the end of each test; another is to use Distinct Gen- erated Values (see Generated Value on page 723) for any identiﬁ ers that might cause conﬂ icts.\n\nwww.it-ebooks.info",
      "content_length": 1851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Managing Persistent Fresh Fixtures\n\nTearing Down Persistent Fresh Fixtures\n\nUnlike ﬁ xture setup code, which should help us understand the pre-conditions of the test, ﬁ xture teardown code is purely a matter of good housekeeping. It does not help us understand the behavior of the SUT but it has the potential to obscure the intent of the test or at least make it more difﬁ cult to understand. Therefore, the best kind of teardown code is the nonexistent kind. We should avoid writing teardown code whenever we can, which is why Garbage-Collected Teardown (page 500) is so preferable. Unfortunately, we cannot take advantage of Garbage-Collected Teardown if our Fresh Fixture is persistent.\n\nHand-Coded Teardown\n\nOne way to ensure that the ﬁ xture is destroyed after we are done with it is to include test-speciﬁ c teardown code within our Test Methods. This teardown mechanism might seem simple, but it is actually more complex than immediately meets the eye. Consider the following example:\n\npublic void testGetFlightsByOriginAirport_NoFlights()\n\nthrows Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); facade.removeAirport(outboundAirport); }\n\nThis Naive In-line Teardown (see In-line Teardown on page 509) will tear down the ﬁ xture when the test passes—but it won’t tear down the ﬁ xture if the test fails or ends with an error. This is because the calls to the Assertion Meth- ods (page 362) throw an exception; therefore, we may never make it to the teardown code. To ensure that the In-line Teardown code always executes, we must surround everything in the Test Method that might raise an exception with a try/catch control structure. Here’s the same test suitably modiﬁ ed:\n\npublic void testGetFlightsByOriginAirport_NoFlights_td() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); try { // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport);\n\nwww.it-ebooks.info\n\n97",
      "content_length": 2155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "98\n\nChapter 9 Persistent Fixture Management\n\n// Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { facade.removeAirport(outboundAirport); } }\n\nUnfortunately, the mechanism to ensure that the teardown code always runs in- troduces a fair bit of complication into the Test Method. Matters become even more complicated when we must tear down several resources: Even if our attempt to clean up one resource fails, we want to ensure that the other resources are still cleaned up. We can address part of this problem by using Extract Method [Fowler] refactoring to move the teardown code into a Test Utility Method (page 599) that we call from inside the error-handling construct. Although this Delegated Teardown (see In-line Teardown) hides the complexity of dealing with teardown errors, we still need to ensure that the method gets called even when test errors or test failures occur.\n\nMost members of the xUnit family solve this problem by supporting Implicit Teardown (page 516). The Test Automation Framework (page 298) calls a spe- cial tearDown method after each Test Method regardless of whether the test passed or failed. This approach avoids placing the error-handling code within the Test Method but imposes two requirements on our tests. First, the ﬁ xture must be accessible from the tearDown method, so we must use instance variables (pre- ferred), class variables, or global variables to hold the ﬁ xture. Second, we must ensure that the tearDown method works properly with each of the Test Methods regardless of which ﬁ xture it sets up.1\n\nMatching Setup with Teardown Code Organization\n\nGiven the three ways of organizing our setup code—In-line Setup (page 408), Delegated Setup (page 411), and Implicit Setup (page 424)—and the three ways of organizing our teardown code—In-line Teardown, Delegated Teardown, and Implicit Teardown—nine different combinations are available to us. Choosing the right one turns out to be an easy decision because it is not important for the teardown code to be visible to the test reader. We simply choose the most appropriate setup code organization and either the equivalent or more hidden version of teardown (Table 9.1). For example, it is appropriate to use Implicit Teardown even with In-line Setup or Delegated Setup; it is almost never a good\n\n1 This is less of an issue with Testcase Class per Fixture (page 631) because the ﬁ xture should always be the same. With other Testcase Class organizations, we may need to include Teardown Guard Clauses (see In-line Teardown) within the tearDown method to ensure that it doesn’t produce errors when it runs.\n\nwww.it-ebooks.info",
      "content_length": 2645,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Managing Persistent Fresh Fixtures\n\nidea to use In-line Teardown with anything other than In-line Setup, and even then it should probably be avoided!\n\nTable 9.1 The Compatibility of Various Fixture Setup and Teardown Strate- gies for Persistent Test Fixtures\n\nTeardown Mechanism\n\nSetup Mechanism\n\nIn-line Teardown Delegated Teardown\n\nImplicit Teardown\n\nIn-line Setup\n\nNot recommended Acceptable\n\nRecommended\n\nDelegated Setup\n\nNot recommended Acceptable\n\nRecommended\n\nImplicit Setup\n\nNot recommended Not recommended\n\nRecommended\n\nAutomated Teardown\n\nHand-coded teardown is associated with two problems: Extra work is required to write the tests, and the teardown code is hard to get right and even harder to test. When the teardown goes wrong, it may lead to Erratic Tests caused by Resource Leakage because the test that fails as a result is often different from the one that didn’t clean up properly.\n\nIn languages that support garbage collection, tearing down a Transient Fresh Fixture should be pretty much automatic. As long as our ﬁ xtures are referenced only by instance variables that go out of scope when our Testcase Object (page 382) is destroyed, garbage collection will clean them up. Garbage collection won’t work, however, if we use class variables or if our ﬁ xtures include persistent objects such as ﬁ les or database rows. In those cases, we need to perform our own cleanup.\n\nNot surprisingly, this situation may inspire the lazy but creative programmer to come up with a way to automate the teardown logic. The important thing to note is that teardown code doesn’t help us understand the test so it is better for it to remain hidden.2 We can eliminate the need to write hand-crafted teardown code for each Test Method or Testcase Class by building an Automated Tear- down (page 503) mechanism. It consists of three parts:\n\n1. A well-tested mechanism to iterate over a list of objects that need to be deleted and catch/report any errors it encounters while ensuring that all the deletions are attempted.\n\n2. A dispatching mechanism that invokes the deletion code appropriate to the kind of object to be deleted. This mechanism is often imple- mented as a Command [GOF] object that wraps each object to be\n\n2 Unlike setup code, which is often very important for understanding the test.\n\nwww.it-ebooks.info\n\n99",
      "content_length": 2326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "100\n\nChapter 9 Persistent Fixture Management\n\ndeleted, but could be as simple as calling a delete method on the object itself or using a switch statement based on the object’s class.\n\n3. A registration mechanism to add newly created objects (suitably\n\nwrapped if necessary) to the list of objects to be deleted.\n\nOnce we have built our Automated Teardown mechanism, we can simply in- voke the registration method from our Creation Methods (page 415) and the cleanup method from the tearDown method. The latter operation can be speciﬁ ed in a Testcase Superclass (page 638) that all of our Testcase Classes inherit from. We can even extend this mechanism to delete objects created by the SUT as it is exercised. To do so, we use an observable Object Factory (see Dependency Lookup on page 686) inside the SUT and have our Testcase Superclass register itself as an Observer [GOF] of object creation.\n\nDatabase Teardown\n\nWhen our persistent Fresh Fixture has been built entirely in a relational database, we can take advantage of certain features of the database to implement its tear- down. Table Truncation Teardown (page 661) is a brute-force way to blow away the entire contents of a table with a single database command. Of course, it is appropriate only when each Test Runner (page 377) has its own Database Sand- box (page 650). A somewhat less drastic approach is to use Transaction Rollback Teardown (page 668) to undo all changes made within the context of the current test. This mechanism relies on the SUT having been designed using the Humble Transaction Controller pattern (see Humble Object on page 695) so that we can invoke the business logic from the test without having the SUT commit the trans- action automatically. Both of these database-speciﬁ c teardown patterns are most commonly implemented using Implicit Teardown to keep the teardown logic out of the Test Methods.\n\nAvoiding the Need for Teardown\n\nSo far, we have looked at ways to do ﬁ xture teardown. Now, let us look at ways to avoid ﬁ xture teardown.\n\nAvoiding Fixture Collisions\n\nWe need to do ﬁ xture teardown for three reasons:\n\n1. The accumulation of leftover ﬁ xture objects can cause tests to run\n\nslowly.\n\nwww.it-ebooks.info",
      "content_length": 2210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Managing Persistent Fresh Fixtures\n\n2. The leftover ﬁ xture objects can cause the SUT to behave differently or\n\nour assertions to report incorrect results.\n\n3. The leftover ﬁ xture objects can prevent us from creating the Fresh Fix-\n\nture required by our test.\n\nThe issue that is easiest to address is the ﬁ rst one: We can schedule a periodic cleansing of the persistence mechanism back to a known, minimalist state. Un- fortunately, this tactic is useful only if we can get the tests to run correctly in the presence of accumulated test detritus.\n\nThe second issue can be addressed by using Delta Assertions (page 485) rather than “absolute” assertions. Delta Assertions work by taking a snapshot of the ﬁ xture before the test is run and verifying that the expected differences have appeared after we exercise the SUT.\n\nThe third issue can be addressed by ensuring that each test generates a differ- ent set of ﬁ xture objects each time it is run. Thus any objects that the test needs to create must be given totally unique identiﬁ ers—that is, unique ﬁ lenames, unique keys, and so on. To do so, we can build a simple unique ID generator and create a new ID at the beginning of each test. We can then use that unique ID as part of the identity of each newly created ﬁ xture object. If the ﬁ xture is shared beyond a single Test Runner, we may need to include something about the user in the unique identiﬁ ers we create; the currently logged-in user ID is usually sufﬁ cient. Using Distinct Generated Values as keys offers another ben- eﬁ t: It allows us to implement a Database Partitioning Scheme (see Database Sandbox) in which we can use absolute assertions despite the presence of left- over ﬁ xture objects.\n\nAvoiding Fixture Persistence\n\nWe seem to be going to a lot of trouble to undo the side effects caused by a persistent Fresh Fixture. Wouldn’t it be nice if we could avoid all of this work? The good news is that we can. The bad news is that we need to make our Fresh Fixture nonpersistent to do so. When the SUT is to blame for the persistence of the ﬁ xture, one possibility is to replace the persistence mechanism with a Test Double (page 522) that the test can wipe out at will. A good example of this ap- proach is the use of a Fake Database (see Fake Object on page 551). When the test is to blame for ﬁ xture persistence, the solution is even easier: Just use a less persistent ﬁ xture reference mechanism.\n\nwww.it-ebooks.info\n\n101",
      "content_length": 2455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "102\n\nChapter 9 Persistent Fixture Management\n\nDealing with Slow Tests\n\nA major drawback of using a Persistent Fresh Fixture is speed or, more precisely, the lack thereof. File systems and databases are much slower than the processors used in modern computers. As a consequence, tests that interact with databases tend to run much more slowly than tests that run entirely in memory. Part of this difference arises because the SUT is accessing the ﬁ xture from disk—but this issue turns out to be only a small part of the reason for the slowdown. Setting up the Fresh Fixture at the beginning of each test and tearing it down at the end of each test typically takes many more disk accesses than those used by the SUT to access the ﬁ xture. As a result, tests that access the database often take 50 to 100 times3 longer to run than tests that run entirely in memory, all other things being equal.\n\nThe typical reaction to slow tests caused by Persistent Fresh Fixtures is to eliminate the ﬁ xture setup and teardown overhead by reusing the ﬁ xture across many tests. Assuming we have ﬁ ve disk accesses to set up and tear down the ﬁ xture for every disk access performed by the SUT, the absolute best4 we can do by switching to a Shared Fixture is somewhere around ten times as slow. Of course, this outcome is still too slow in most situations and it comes with a hefty price: The tests are no longer independent. That means we will likely have Interacting Tests (see Erratic Test), Lonely Tests (see Erratic Test), and Unre- peatable Tests on top of our Slow Tests!\n\nA much better solution is to eliminate the need to have a disk-based database under the application. With a small amount of effort, we should be able to re- place the disk-based database with an In-Memory Database (see Fake Object) or a Fake Database. This decision is best made early in the project while the effort is still low. Yes, there are some challenges, such as dealing with stored procedures, but they are all surmountable.\n\nThis tactic isn’t the only way to deal with Slow Tests, of course. The side- bar “Faster Tests Without Shared Fixtures” (page 319) explores some other strategies.\n\n3 This is two orders of magnitude! 4 Your mileage may vary.\n\nwww.it-ebooks.info",
      "content_length": 2244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Managing Shared Fixtures\n\nManaging Shared Fixtures\n\nManaging Shared Fixtures has a lot in common with managing Persistent Fresh Fixtures, except that we deliberately choose not to tear the ﬁ xture down after every test so that we can reuse it in subsequent tests (Figure 9.2). This implies two things. First, we must be able to access the ﬁ xture in the other tests. Second, we must have a way of triggering both the construction and the teardown of the ﬁ xture.\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nExercise Exercise\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nVerify Verify\n\nTeardown Teardown\n\nFigure 9.2 A Shared Fixture with two Test Methods that share it. A Shared Fixture is set up once and used by two or more tests that may interact, either deliberately or accidentally, as a result. Note the lack of a ﬁ xture setup phase for the second test.\n\nAccessing Shared Fixtures\n\nRegardless of how and when we choose to build the Shared Fixture, the tests need a way to ﬁ nd the test ﬁ xture they are to reuse. The choices available to us depend on the nature of the ﬁ xture. When the ﬁ xture is stored in a database (the most common usage of a Shared Fixture), tests may access it directly with- out making direct references to the ﬁ xture objects as long as they know about the database. There may be a temptation to use Hard-Coded Values (see Literal Value on page 714) in database lookups to access the ﬁ xture objects. This is al- most always a bad idea, however, because it leads to a close coupling between tests and the ﬁ xture implementation and because it has poor documentation value (Obscure Test; page 186). To avoid these potential problems, we can use Finder Methods (see Test Utility Method) with Intent-Revealing Names [SBPP] to access\n\nwww.it-ebooks.info\n\n103",
      "content_length": 1790,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "104\n\nChapter 9 Persistent Fixture Management\n\nthe ﬁ xture. These Finder Methods may have names that are very similar to those of Creation Methods, but they return references to existing ﬁ xture objects rather than building brand new ones.\n\nWe have a range of possible solutions when the ﬁ xture is stored in memory. If all tests that need to share the ﬁ xture are in the same Testcase Class, we can use a ﬁ xture holding class variable to hold the reference to the ﬁ xture. As long as we give the variable an Intent-Revealing Name, the test reader should be able to understand the pre-conditions of the test. Another alternative is to use a Finder Method.\n\nIf we need to share the ﬁ xture across many Testcase Classes, we must use a more sophisticated technique. We could, of course, let one class declare the ﬁ xture holding class variable and have the other tests access the ﬁ xture via that variable. Unfortunately, this approach may create unnecessary coupling between the tests. Another alternative is to move the declaration to a well-known object—namely, a Test Fixture Registry (see Test Helper on page 643). This Registry [PEAA] object could be something like a test database or it could merely be a class. It can expose various parts of a ﬁ xture via discrete ﬁ xture holding class variables or via Finder Methods. When the Test Fixture Registry has only Finder Methods that know how to access the objects but don’t hold references to them, we call it a Test Helper.\n\nTriggering Shared Fixture Construction\n\nFor a test ﬁ xture to be shared, it must be built before any Test Method needs it. This construction could take place as late as right before the Test Method’s logic is run, just before the entire test suite is run, or at some earlier time (Figure 9.3). This leads us to the basic patterns of Shared Fixture creation.\n\nShared Shared Fixture Fixture\n\nPrebuilt Prebuilt Fixture Fixture\n\nLazy Lazy Setup Setup\n\nSetup Setup Decorator Decorator\n\nSuite Suite Fixture Fixture Setup Setup\n\nChained Chained Tests Tests\n\nShared Fixture Setup Shared Fixture Setup\n\nFigure 9.3 The plethora of ways to manage a Shared Fixture. A Shared Fixture can be set up at a variety of times; the decision is based on how many tests need to reuse the ﬁ xture and how many times they need to do so.\n\nwww.it-ebooks.info",
      "content_length": 2310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Managing Shared Fixtures\n\nIf we are happy with the idea of creating the test ﬁ xture the ﬁ rst time any test needs it, we can use Lazy Setup (page 435) in the setUp method of the corre- sponding Testcase Class to create it as part of running the ﬁ rst test. Subsequent tests will then see that the ﬁ xture already exists and reuse it. Because there is no obvious signal that the last test in a test suite (or Suite of Suites; see Test Suite Object on page 387) has been run, we won’t know when to tear down the ﬁ x- ture after each test run. This can lead to Unrepeatable Tests because the ﬁ xture may survive across test runs (depending on how the various tests access it).\n\nIf we need to share the ﬁ xture more broadly, we could include a Fixture Set- up Testcase at the beginning of the test suite. This is a special case of Chained Tests and suffers from the same problem as Lazy Setup—speciﬁ cally, we don’t know when it is time to tear down the ﬁ xture. It also depends on the ordering of tests within a suite, so it works best with Test Enumeration (page 399).\n\nIf we need to be able to tear down the test ﬁ xture after running a test suite, we must use a ﬁ xture management mechanism that tells us when the last test has been run. Several members of the xUnit family support the concept of a setUp method that runs just once for the test suite created from a single Testcase Class. This Suite Fixture Setup (page 441) method has a corresponding tearDown method that is called when the last Test Method has ﬁ nished running.5 We can then guarantee that a new ﬁ xture is built for each test run. The ﬁ xture is not left over to cause problems with subsequent test runs, which prevents Unrepeatable Tests; it does not prevent Interacting Tests within the test run, however. This capability could be added as an extension to any member of the xUnit family. When it isn’t supported or when we need to share the ﬁ xture beyond a single Testcase Class, we can resort to using a Setup Decorator (page 447) to bracket the running of a test suite with the execution of the ﬁ xture setUp and tearDown logic. The biggest drawback of Setup Decorator is that tests that depend on the decorator cannot be run by themselves; they are Lonely Tests.\n\nThe ﬁ nal option is to build the ﬁ xture well before the tests are run—that is, to employ a Prebuilt Fixture (page 429). This approach offers the most options regarding how the test ﬁ xture is actually constructed because the ﬁ xture setup need not be executable from within xUnit. For example, it could be set up manu- ally, by using database scripts, by copying a “golden” database, or by running a data generation program. The major disadvantage with a Prebuilt Fixture is that if any tests are Unrepeatable Tests, we will need to perform a Manual Intervention (page 250) before each test run. As a result, a Prebuilt Fixture is of- ten used in combination with a Fresh Fixture to construct an Immutable Shared Fixture (see Shared Fixture).\n\n5 Think of it as a built-in decorator for a single Testcase Class.\n\nwww.it-ebooks.info\n\n105",
      "content_length": 3077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "106\n\nChapter 9 Persistent Fixture Management\n\nWhat’s Next?\n\nNow that we’ve determined how we will set up and tear down our ﬁ xtures, we are ready to turn our attention to exercising the SUT and verifying that the expected outcome has occurred using calls to Assertion Methods. This process is described in more detail in Chapter 10, Result Veriﬁ cation.\n\nwww.it-ebooks.info",
      "content_length": 373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Chapter 10\n\nResult Veriﬁ cation\n\nAbout This Chapter\n\nChapter 8, Transient Fixture Management, and Chapter 9, Persistent Fixture Management, described how to set up the test ﬁ xture and how to tear it down after exercising the SUT. This chapter introduces a variety of options for verify- ing that the SUT has behaved correctly, including exercising the SUT and com- paring the actual outcome with the expected outcome.\n\nMaking Tests Self-Checking\n\nOne of the key characteristics of tests automated using xUnit is that they can be (and should be) Self-Checking Tests (see Goals of Test Automation on page 21). This characteristic makes them cost-effective enough to be run very frequently. Most members of the xUnit family come with a collection of built-in Assertion Methods (page 362) and some documentation that tells us which one to use when. On the surface this sounds pretty simple—but there’s a lot more to writ- ing good tests than just calling the built-in Assertion Methods. We also need to learn key techniques for making tests easy to understand and for avoiding and removing Test Code Duplication (page 213).\n\nA key challenge in coding the assertions is getting access to the information we want to compare with the expected results. This is where observation points come into play; they provide a window into the state or behavior of the SUT so that we can pass it to the Assertion Methods. Observation points for infor- mation accessible via synchronous method calls are relatively straightforward; observation points for other kinds of information can be quite challenging, which is precisely what makes automated unit testing so interesting.\n\nAssertions are usually—but not always—called from within the Test Method (page 348) body right after the SUT has been exercised. Some test automaters put\n\n107\n\nwww.it-ebooks.info",
      "content_length": 1837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "108\n\nChapter 10 Result Verification\n\nassertions after the ﬁ xture setup phase of the test to ensure that the ﬁ xture is set up correctly. This practice almost always contributes to Obscure Tests (page 186), so I would rather write unit tests for the Test Utility Methods (page 599).1 Some styles of testing do require us to set up our expectations before we exercise the SUT; this topic is discussed in more detail in Chapter 11, Using Test Doubles. We’ll see several examples of calling Assertion Methods from within Test Utility Methods in this chapter.\n\nOne possible—though rarely used—place to put calls to Assertion Methods is in the tearDown method used in Implicit Teardown (page 516). Because this method is run for every test, whether that test passed or failed (as long as the setUp method succeeded), one can put assertions here. This scheme involves the same trade-off as usingImplicit Setup (page 424) for building our test ﬁ xture; it’s less visible but done automatically. See the sidebar “Using Delta Assertions to Detect Data Leakage” (page 487) for an example of putting assertions in the tearDown method used by Implicit Teardown of a superclass to detect when tests leave leftover test objects in the database.\n\nVerify State or Behavior?\n\nUltimately, test automation is about verifying the behavior of the SUT. Some aspects of the SUT’s behavior can be veriﬁ ed directly; the value returned by a function is a good example. Other aspects of the behavior are more easily veri- ﬁ ed indirectly by looking at the state of some object. We can verify the actual behavior of the SUT in our tests in two ways:\n\n1. We can verify the states of various objects affected by the SUT by extracting each state using an observation point and using assertions to compare it to the expected state.\n\n2. We can verify the behavior of the SUT directly by using observation points inserted between the SUT and its depended-on component (DOC) to monitor its interactions (in the form of the method calls it makes) and comparing those method calls with what we expected.\n\nState Veriﬁ cation (page 462) is done using assertions and is the simpler of the two approaches. Behavior Veriﬁ cation (page 468) is more complicated and builds on the assertion techniques we use for verifying state.\n\n1 The one exception is when we must use a Shared Fixture (page 317); it may be worth- while to use a Guard Assertion (page 490) to document what the test requires from it and to produce a test failure if the ﬁ xture is corrupted. We could also do so from within the Finder Methods (see Test Utility Method) that we use to retrieve the objects in the Shared Fixture (page 317) we will use in our tests.\n\nwww.it-ebooks.info",
      "content_length": 2708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "State Verification\n\nState Veriﬁ cation\n\nThe “normal” way to verify the expected outcome has occurred is called State Veriﬁ cation (Figure 10.1). First we exercise the SUT; then we examine the post- exercise state of the SUT using assertions. We may also examine anything returned by the SUT as a result of the method call we made to exercise it. What is most notable is what we do not do: We do not instrument the SUT in any way to detect how it interacts with other components of the system. That is, we inspect only direct outputs and we use only direct method calls as our observation points.\n\nSetup Setup\n\nFixture Fixture Behavior Behavior (Indirect (Indirect Outputs) Outputs)\n\nDOC DOC\n\nExercise Exercise\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nGet State Get State\n\nB B\n\nA A\n\nC C\n\nTeardown Teardown\n\nFigure 10.1 State Veriﬁ cation. In State Veriﬁ cation, we assert that the SUT and any objects it returns are in the expected state after we have exercised the SUT. We “pay no attention to the man behind the curtain.”\n\nState Veriﬁ cation can be done in two slightly different ways. Procedural State Veriﬁ cation (see State Veriﬁ cation) involves writing a sequence of assertions that pick apart the end state of the SUT and verify that it is as expected. Expected Object (see State Veriﬁ cation) is a way of describing the expected state in such a way that it can be compared with a single Assertion Method call; this approach minimizes Test Code Duplication and increases test clarity (more on this later in this chapter). With both strategies, we can use either “built-in” assertions or Custom Assertions (page 474).\n\nwww.it-ebooks.info\n\n109",
      "content_length": 1651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "110\n\nChapter 10 Result Verification\n\nUsing Built-in Assertions\n\nWe use the assertions provided by our testing framework to specify what should be and depend on them to tell us when it isn’t so! But simply using the built-in assertions is only a small part of the story.\n\nThe simplest form of result veriﬁ cation is the assertion in which we specify what should be true. Most members of the xUnit family support a range of dif- ferent Assertion Methods, including the following:\n\nStated Outcome Assertions (see Assertion Method) such as assertTrue (aBooleanExpression)\n\nSimple Equality Assertions such as assertEquals(expected, actual)\n\nFuzzy Equality Assertions such as assertEquals(expected, actual, tolerance), which are used for comparing ﬂ oats\n\nOf course, the test programming language has some inﬂ uence on the nature of the assertions. In JUnit, SUnit, CppUnit, NUnit, and CsUnit, most of the Equal- ity Assertions take a pair of Objects as their parameters. Some languages support “overloading” of method parameter types so we can have different implemen- tations of an assertion for different types of objects. Some languages—C, for example—don’t support objects, so we cannot compare objects, only values.\n\nThere are several issues to consider when using Assertion Methods. Naturally, the ﬁ rst priority is the veriﬁ cation of all things that should be true. The better our assertions, the ﬁ ner our Safety Net (see page 24) and the higher our conﬁ - dence in our code. The second priority is the documentation value of the asser- tions. Each test should make it very clear that “When the system is in state S1 and I do X, the result should be R and the system should be in state S2.” We put the system into state S1 in our ﬁ xture setup logic. “I do X” corresponds to the exercise SUT phase of the test. “The result is R” and “the system is in state S2” are implemented using assertions. Thus we want to write our assertions in such a way that they succinctly describe “R” and “S2.”\n\nAnother thing to consider is that when the test fails, we want the failure message to tell us enough to enable us to identify the problem.2 Therefore, we should almost always include an Assertion Message (page 370) as the optional message parameter (assuming our xUnit family member has one!). This tactic avoids the possibility of us playing Assertion Roulette (page 224), in which we cannot even tell which assertion is failing without running the test interactively;\n\n2 In his book [TDD-APG], Dave Astels claims he never/rarely used the Eclipse Debugger while writing the code samples because the assertions always told him enough about what was wrong. This is what we strive for!\n\nwww.it-ebooks.info",
      "content_length": 2698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "State Verification\n\nit makes Integration Build [SCM] failures much easier to reproduce and ﬁ x. It also makes troubleshooting broken tests easier by telling us what should have happened; the actual outcome tells us what did happen!\n\nWhen we use a Stated Outcome Assertion (such as JUnit’s assertTrue), the failure messages tend to be unhelpful (e.g., “Assertion failed”). We can make the assertion output much more speciﬁ c by using an Argument-Describing Mes- sage (see Assertion Message) constructed by incorporating useful bits of data into the message. A good start is to include each of the values in the expression passed as the Assertion Method’s arguments.\n\nDelta Assertions\n\nWhen using a Shared Fixture (page 317), we may ﬁ nd that we have Interacting Tests (see Erratic Test on page 228) because each test adds more objects/rows into the database and we can never be certain exactly what should be there af- ter the SUT has been exercised. One way to deal with this uncertainty is to use Delta Assertions (page 485) to verify only the newly added objects/rows. In this approach, we take some sort of “snapshot” of the relevant tables/classes at the beginning of the test; we then remove these tables/classes from the collection of actual objects/rows produced at the end of the test before comparing them to the Expected Objects. Although this tactic can introduce signiﬁ cant extra complexity into the tests, the added complexity can be refactored into Custom Assertions and/or Veriﬁ cation Methods (see Custom Assertion). The “before” snapshot may be taken on an in-line basis within the Test Method or in the setUp method if all setup occurs before the Test Method is invoked [e.g., Implicit Setup, a Shared Fixture, or a Prebuilt Fixture (page 429)].\n\nExternal Result Veriﬁ cation\n\nThus far we have described only conventional “in-memory” veriﬁ cation of the expected results. In fact, another approach is possible—one that involves storing the expected and actual results in ﬁ les and using an external comparison pro- gram to report on any differences. This is, in effect, a form of Custom Assertion that uses a “deep compare” on two ﬁ le references. The comparison program often needs to be told which parts of the ﬁ les to ignore (or these parts need to be stripped out ﬁ rst), effectively making this a Fuzzy Equality Assertion.\n\nExternal result veriﬁ cation is particularly appropriate for automating accep- tance tests for regression-testing an application that hasn’t changed very much. The major disadvantage of this approach is that we almost always end up with a Mystery Guest (see Obscure Test) from the test reader’s perspective because the\n\nwww.it-ebooks.info\n\n111",
      "content_length": 2692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "112\n\nChapter 10 Result Verification\n\nexpected results are not visible inside the test. One way to avoid this problem is to have the test write the contents of the expected ﬁ le, thereby making the con- tents visible to the test reader. This step is practical only if the amount of data is quite small—another argument in favor of a Minimal Fixture (page 302).\n\nVerifying Behavior\n\nVerifying behavior is more complicated than verifying state because behavior is dynamic. We have to catch the SUT “in the act” as it generates indirect outputs to the objects it depends on (Figure 10.2). Two basic styles of behavior veriﬁ ca- tion are worth discussing: Procedural Behavior Veriﬁ cation and Expected Behavior. Both require a mechanism to access the outgoing method calls of the SUT (its indirect outputs). This and other uses of Test Doubles (page 522) are described in more detail in Chapter 11, Using Test Doubles.\n\nSetup Setup\n\nFixture Fixture Behavior Behavior (Indirect (Indirect Outputs) Outputs)\n\nDOC DOC\n\nExercise Exercise\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nA A\n\nB B\n\nC C\n\ny y f f i i r r e e V V\n\nTeardown Teardown\n\nFigure 10.2 Behavior Veriﬁ cation. In Behavior Veriﬁ cation, we focus our assertions on the indirect outputs (outgoing interfaces) of the SUT. This typically involves replacing the DOC with something that facilitates observing and verifying the outgoing calls.\n\nwww.it-ebooks.info",
      "content_length": 1411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "Verifying Behavior\n\nProcedural Behavior Veriﬁ cation\n\nIn Procedural Behavior Veriﬁ cation, we capture the behavior of the SUT as it executes and save that data for later retrieval. The test then compares each out- put of the SUT (one by one) with the corresponding expected output. Thus, in Procedural Behavior Veriﬁ cation, the test executes a procedure (a set of steps) to verify the behavior.\n\npublic void testRemoveFlightLogging_recordingTestStub() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // Test Double setup AuditLogSpy logSpy = new AuditLogSpy(); facade.setAuditLog(logSpy); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertEquals(\"number of calls\", 1, logSpy.getNumberOfCalls()); assertEquals(\"action code\", Helper.REMOVE_FLIGHT_ACTION_CODE, logSpy.getActionCode()); assertEquals(\"date\", helper.getTodaysDateWithoutTime(), logSpy.getDate()); assertEquals(\"user\", Helper.TEST_USER_NAME, logSpy.getUser()); assertEquals(\"detail\", expectedFlightDto.getFlightNumber(), logSpy.getDetail()); }\n\nThe key challenge in Procedural Behavior Veriﬁ cation is capturing the behavior as it occurs and saving it until the test is ready to use this information. This task is accomplished by conﬁ guring the SUT to use a Test Spy (page 538) or a Self Shunt (see Hard-Coded Test Double on page 568)3 instead of the depended-on class. After the SUT has been exercised, the test retrieves the recording of the behavior and veriﬁ es it using assertions.\n\nExpected Behavior Speciﬁ cation\n\nIf we can build an Expected Object and compare it with the actual object returned by the SUT for verifying state, can we do something similar for verifying\n\n3 A Test Spy built into the Testcase Class (page 373).\n\nwww.it-ebooks.info\n\n113",
      "content_length": 1861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "114\n\nChapter 10 Result Verification\n\nbehavior? Yes, we can and do. Expected Behavior is often used in conjunction with layer-crossing tests to verify the indirect outputs of an object or compo- nent. We conﬁ gure a Mock Object (page 544) with the method calls we expect the SUT to make to it and install this object before exercising the SUT.\n\npublic void testRemoveFlight_JMock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // mock conﬁguration Mock mockLog = mock(AuditLog.class); mockLog.expects(once()).method(\"logMessage\") .with(eq(helper.getTodaysDateWithoutTime()), eq(Helper.TEST_USER_NAME), eq(Helper.REMOVE_FLIGHT_ACTION_CODE), eq(expectedFlightDto.getFlightNumber())); // mock installation facade.setAuditLog((AuditLog) mockLog.proxy()); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify // verify() method called automatically by JMock }\n\nReducing Test Code Duplication\n\nOne of the most common test smells is Test Code Duplication. With every test we write, there is a good chance we have introduced some duplication, but especially if we used “cut and paste” to create a new test from an existing test. Some will argue that duplication in test code is not nearly as bad as duplication in production code. Test Code Duplication is bad if it leads to some other smell such as Fragile Test (page 239), Fragile Fixture (see Fragile Test), or High Test Maintenance Cost (page 265) because too many tests are too closely coupled to the Standard Fixture (page 305) or the API of the SUT. In addition, Test Code Duplication may sometimes be a symptom of another problem—namely, the intent of the tests being obscured by too much code (i.e., an Obscure Test).\n\nIn result veriﬁ cation logic, Test Code Duplication usually shows up as a set of repeated assertions. Several techniques are available to reduce the number of assertions in such cases:\n\nwww.it-ebooks.info",
      "content_length": 2009,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Reducing Test Code Duplication\n\nExpected Objects\n\nCustom Assertions\n\nVeriﬁ cation Methods\n\nExpected Objects\n\nOften, we will ﬁ nd ourselves doing a series of assertions on different ﬁ elds of the same object. If we begin repeating this group of assertions (whether multiple times in a single test or in multiple tests), we should look for a way to reduce the Test Code Duplication. The next listing shows one Test Method that compares several attributes of a single object. Many other Test Methods probably require the same sequence of assertions.\n\npublic void testInvoice_addLineItem7() { LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\nThe most obvious alternative is to use a single Equality Assertion to compare two whole objects to each other rather than using many Equality Assertion calls to compare them ﬁ eld by ﬁ eld. If the values are stored in individual variables, we may need to create a new object of the appropriate class and initialize its ﬁ elds with those values. This technique works as long as we have an equals method that compares only those ﬁ elds and we have the ability to create the Expected Object at will.\n\npublic void testInvoice_addLineItem8() { LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); LineItem actual = (LineItem)lineItems.get(0); assertEquals(\"Item\", expItem, actual); }\n\nBut what if we don’t want to compare all the ﬁ elds in an object or the equals method looks for identity rather than equality? What if we want test-speciﬁ c\n\nwww.it-ebooks.info\n\n115",
      "content_length": 1917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "116\n\nChapter 10 Result Verification\n\nequality? What if we cannot create an instance of the Expected Object because no constructor exists? In this scenario, we have two options: We can implement a Custom Assertion that deﬁ nes equality the way we want it or we can imple- ment our test-speciﬁ c equality in the equals method of the class of the Expected Object we pass to the Assertion Method. This class doesn’t need to be the same class as that of the actual object; it just needs to implement equals to compare itself with an instance of the actual object’s class. Therefore, it can be a simple Data Transfer Object [CJ2EEP] or it can be a Test-Speciﬁ c Subclass (page 579) of the real (production) class with just the equals method overridden.\n\nSome test automaters don’t think we should ever rely on the equals method of the SUT when making assertions because it could change, thereby causing tests that depend on this method to fail (or to miss important differences). I pre- fer to be pragmatic about this decision. If it seems reasonable to use the equals deﬁ nition supplied by the SUT, then I do so. If I need something else, I deﬁ ne a Custom Assertion or a test-speciﬁ c Expected Object class. I also ask myself how hard it would be to change my strategy if the equals method should later change. For example, in statically typed languages that support parameter type overloading (such as Java), we can add a Custom Assertion that uses different parameter types to override the default implementation when speciﬁ c types are used. This code can often be retroﬁ tted quite easily if a change to equals causes problems at a later date.\n\nCustom Assertions\n\nA Custom Assertion is a domain-speciﬁ c assertion we write ourselves. Custom Assertions hide the procedure for verifying the results behind a declarative name, making our result veriﬁ cation logic more intent-revealing. They also prevent Obscure Tests by eliminating of a lot of potentially distracting code. Another beneﬁ t of moving the code into a Custom Assertion is that the assertion logic can now be unit-tested by writing Custom Assertion Tests (see Custom Asser- tion). The assertions are no longer Untestable Test Code (see Hard-to-Test Code on page 209)!\n\nstatic void assertLineItemsEqual( String msg, LineItem exp, LineItem act) { assertEquals (msg+\" Inv\", exp.getInv(), act.getInv()); assertEquals (msg+\" Prod\", exp.getProd(), act.getProd()); assertEquals (msg+\" Quan\", exp.getQuantity(), act.getQuantity()); }\n\nThere are two ways to create Custom Assertions: (1) by refactoring existing complex test code to reduce Test Code Duplication and (2) by coding calls to\n\nwww.it-ebooks.info",
      "content_length": 2662,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Reducing Test Code Duplication\n\nnonexistent Assertion Methods as we write tests and then ﬁ lling in the method bodies with the appropriate logic once we land on the suite of Custom Assertions needed by a set of Test Methods. The latter technique is a good way of reminding ourselves what we expect the outcome of exercising the SUT to be, even though we haven’t yet written the code to verify it. Either way, the deﬁ nition of a set of Custom Assertions is the ﬁ rst step toward creating a Higher-Level Language (see page 41) for specifying our tests.\n\nWhen refactoring to Custom Assertions, we simply use Extract Method [Fowler] on the repeated assertions and give the new method an Intent-Revealing Name [SBPP]. We pass in the objects used by the existing veriﬁ cation logic as arguments and include an Assertion Message to differentiate between calls to the same assertion method.\n\nOutcome-Describing Veriﬁ cation Method\n\nAnother technique that is born from ruthless refactoring of test code is the “out- come-describing” Veriﬁ cation Method. Suppose we ﬁ nd that a group of tests all have identical exercise SUT and verify outcome sections. Only the setup portion is different for each test. If we do an Extract Method refactoring on the common code and give it a meaningful name, we need less code, achieve more understand- able tests, and produce testable veriﬁ cation logic all at the same time! If this isn’t a worthwhile reason for refactoring code, then I don’t know what else could be.\n\nvoid assertInvoiceContainsOnlyThisLineItem( Invoice inv, LineItem expItem) { List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\",expItem, actual); }\n\nThe major difference between a Veriﬁ cation Method and a Custom Assertion is that the latter only makes assertions, while the former also interacts with the SUT (typically for the purpose of exercising it). Another difference is that Custom Assertions typically have a standard Equality Assertion signature: assertSomething(message, expected, actual). In contrast, Veriﬁ cation Methods may have completely arbitrary parameters because they require additional param- eters to pass into the SUT. They are, in essence, halfway between a Custom Assertion and a Parameterized Test (page 607).\n\nwww.it-ebooks.info\n\n117",
      "content_length": 2369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "118\n\nChapter 10 Result Verification\n\nParameterized and Data-Driven Tests\n\nWe can go even further in factoring out the commonality between tests. If the logic to set up the test ﬁ xture is the same but uses different data, we can extract the common ﬁ xture setup, exercise SUT, and verify outcome phases of the test into a new Parameterized Test method. This Parameterized Test is not called automatically by the Test Automation Framework (page 298) because it requires arguments; instead, we deﬁ ne very simple Test Methods for each test, which then call the Parameterized Test and pass in the data required to make this test unique. This data may include that required for ﬁ xture setup, exercising the SUT, and the corresponding expected result. In the following tests, the method generateAndVerifyHtml is the Parameterized Test.\n\ndef test_extref sourceXml = \"<extref id='abc' />\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<extref>\") end\n\ndef test_testterm_normal sourceXml = \"<testterm id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<testterm>\") end\n\ndef test_testterm_plural sourceXml = \"<testterms id='abc'/>\" expectedHtml = \"<a href='abc.html'>abcs</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<plural>\") end\n\nIn a Data-Driven Test (page 288), the test case is completely generic and directly executable by the framework; it reads the arguments from a test data ﬁ le as it executes. Think of a Data-Driven Test as a Parameterized Test turned inside out: A Test Method passes test-speciﬁ c data to a Parameterized Test; a Data-Driven Test is the Test Method and reads the test-speciﬁ c data from a ﬁ le. The contents of the ﬁ le are a Higher-Level Language for testing; the Data-Driven Test method is the Interpreter [GOF] of that language. This scheme is the xUnit equivalent of a Fit test. A simple example of a Data-Driven Test method is shown in this code sample written in Ruby:\n\ndef test_crossref executeDataDrivenTest \"CrossrefHandlerTest.txt\" end\n\ndef executeDataDrivenTest ﬁlename dataFile = File.open(ﬁlename)\n\nwww.it-ebooks.info",
      "content_length": 2153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Avoiding Conditional Test Logic\n\ndataFile.each_line do | line | desc, action, part2 = line.split(\",\") sourceXml, expectedHtml, leftOver = part2.split(\",\") if \"crossref\"==action.strip generateAndVerifyHtml sourceXml, expectedHtml, desc else # new \"verbs\" go before here as elsif's report_error( \"unknown action\" + action.strip ) end end end\n\nHere is the comma-delimited data ﬁ le that the Data-Driven Test method reads:\n\nID, Action, SourceXml, ExpectedHtml Extref,crossref,<extref id='abc'/>,<a href='abc.html'>abc</a> TTerm,crossref,<testterm id='abc'/>,<a href='abc.html'>abc</a> TTerms,crossref,<testterms id='abc'/>,<a href='abc.html'>abcs</a>\n\nAvoiding Conditional Test Logic\n\nAnother thing we want to avoid in our tests is conditional logic. Conditional Test Logic (page 200) is bad because the same test may execute differently in different circumstances. Conditional Test Logic reduces our trust in the tests because the code in our Test Methods is Untestable Test Code. Why is this important? Because the only way we can verify our Test Method is to manually edit the SUT so that it produces the error we want to be detected. If the Test Method has many paths through it, we need to make sure each path is coded correctly. Isn’t it so much simpler just to have only one possible execution path through the test? Let us look at some reasons why we might include conditional logic in our tests:\n\nWe don’t want to execute certain assertions because their execution doesn’t make sense given what we have already discovered at this point in the test (typically a failure condition).\n\nWe have to allow for various situations in the actual results that we are\n\ncomparing to the expected results.\n\nWe are trying to reuse a Test Method in several different circumstances\n\n(essentially merging several tests into a single Test Method).\n\nThe problem with using Conditional Test Logic in the ﬁ rst two cases is that it makes the code hard to read and may mask cases of reusing test methods via Flexible Tests (see Conditional Test Logic). The last “reason” is just a bad idea,\n\nwww.it-ebooks.info\n\n119",
      "content_length": 2097,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "120\n\nChapter 10 Result Verification\n\nplain and simple. There are much better ways of reusing test logic than trying to reuse the Test Method itself. We have already seen some of these reuse tech- niques elsewhere in this chapter (in Reducing Test Code Duplication), and we will see other ways elsewhere in this book. Just say “no”!\n\nThe good news is that it is relatively straightforward to remove all legitimate\n\nuses of Conditional Test Logic from our tests.\n\nEliminating “if” Statements\n\nWhat should we do when we don’t want to execute an assertion because we know it will result in a test error and we would prefer to have a more meaning- ful test failure message? The normal reaction is to place the assertion inside an “if” statement, as shown in the following listing. Unfortunately, this approach results in Conditional Test Logic, which we would dearly like to avoid because we want exactly the same code to run each time we run the test.\n\nList lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem expected = new LineItem(invoice, product,5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"invoice\", expected, actItem); } else { fail(\"Invoice should have exactly one line item\"); }\n\nThe preferred solution is to use a Guard Assertion (page 490) as shown in this revised version of the test code:\n\nList lineItems = invoice.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem expected = new LineItem(invoice, product, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"invoice\", expected, actItem);\n\nThe nice thing about Guard Assertions is that they keep us from hitting the as- sertion that would cause a test error but without introducing Conditional Test Logic. Once we get used to them, these assertions are fairly obvious and intuitive to read. We may even ﬁ nd ourselves wanting to assert the pre-conditions of our methods in our production code!\n\nwww.it-ebooks.info",
      "content_length": 2052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Other Techniques\n\nEliminating Loops\n\nConditional Test Logic may also appear as loops that verify the content of a col- lection returned by the SUT matches what we expected. Putting loops directly into the Test Method creates three problems:\n\nIt introduces Untestable Test Code because the looping code, which is part of the test, cannot be tested with Fully Automated Tests (see page 26).\n\nIt leads to Obscure Tests because all that looping code obscures the real\n\nintent: Does or doesn’t the collection match?\n\nIt can lead to the project-level smell Developers Not Writing Tests (page 263) because the complexity of writing the loops may dis- courage the developer from writing the Self-Checking Test.\n\nA better solution is to delegate this logic to a Test Utility Method with an Intent- Revealing Name, which can be both tested and reused.\n\nOther Techniques\n\nThis section outlines some other techniques for writing easy-to-understand tests.\n\nWorking Backward, Outside-In\n\nA useful little trick for writing very intent-revealing code is to work backward. This is an application of Stephen Covey’s idea, “Start with the end in mind.” To do so, we write the last line of the function or test ﬁ rst. For a function, its whole reason for existence is to return a value; for a procedure, it is to produce one or more side effects by modifying something. For a test, the raison d’ tre is to verify that the expected outcome has occurred (by making assertions).\n\nWorking backward means we write these assertions ﬁ rst. We assert on the values of suitably named local variables to ensure that the assertion is intent-revealing. The rest of writing the test simply consists of ﬁ lling in whatever is needed to execute those assertions: We declare variables to hold the assertion arguments and initialize them with the appropriate content. Because at least one argument should have been retrieved from the SUT, we must, of course, invoke the SUT. To do so, we may need some variables to use as SUT arguments. Declaring and initializing a variable after it has been used forces us to understand the variable better when we introduce it. This scheme also results in better variable names and avoids meaningless names like invoice1 and invoice2.\n\nwww.it-ebooks.info\n\n121",
      "content_length": 2258,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "122\n\nChapter 10 Result Verification\n\nWorking “outside-in” (or “top-down” as it is sometimes called) means staying at a consistent level of abstraction. The Test Method should focus on what we need to have in place to induce the relevant behavior in the SUT. The mechanics of how we reach that place should be delegated to a “lower layer” of test soft- ware. In practice, we code this behavior as calls to Test Utility Methods, which allows us to stay focused on the requirements of the SUT as we write each Test Method. We don’t need to worry about how we will create that object or verify that outcome; we merely need to describe what that object or outcome should be. The utility method we just used but haven’t yet deﬁ ned acts as a placeholder for the unﬁ nished test automation logic.4 We can move on to writing the other tests we need for this SUT while they are still fresh in our minds. Later, we can switch to our “toolsmith” hat and implement the Test Utility Methods.\n\nUsing Test-Driven Development to Write Test Utility Methods\n\nOnce we are ﬁ nished writing the Test Method(s) that used the Test Utility Method, we can start the process of writing the Test Utility Method itself. Along the way, we can take advantage of test-driven development by writing Test Utility Tests (see Test Utility Method). It doesn’t take very long to write these unit tests that verify the behavior of our Test Utility Methods and we will have much more conﬁ dence in them.\n\nWe start with the simple case (say, asserting the equality of two identical collections that hold the same item) and work up to the most complicated case that the Test Methods actually require (say, two collections that contain the same two items but in different order). TDD helps us ﬁ nd the minimal implemen- tation of the Test Utility Method, which may be much simpler than a complete generic solution. There is no point in writing generic logic that handles cases that aren’t actually needed but it may be worthwhile to include a Guard Assertion or two inside the Custom Assertion to fail tests in cases it doesn’t support.\n\nWhere to Put Reusable Veriﬁ cation Logic?\n\nSuppose we have decided to use Extract Method refactorings to create some reus- able Custom Assertions or we have decided to write our tests in an intent-revealing way using Veriﬁ cation Methods. Where should we put these bits of reusable test\n\n4 We should always give this method an Intent-Revealing Name and stub it out with a call to the fail assertion to remind ourselves that we still need to write the method’s body.\n\nwww.it-ebooks.info",
      "content_length": 2581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "What’s Next?\n\nlogic? The most obvious place is in the Testcase Class (page 373) itself. We can allow this logic to be reused more broadly by using a Pull-Up Method [Fowler] refactoring to move them up to a Testcase Superclass (page 638) or a Move Method [Fowler] refactoring to move them into a Test Helper (page 643). This issue is dis- cussed in more detail in Chapter 12, Organizing Our Tests.\n\nWhat’s Next?\n\nThis discussion of techniques for verifying the expected outcome concludes our introduction to the basic techniques of automating tests using xUnit. Chapter 11, Using Test Doubles, introduces some advanced techniques involving the use of Test Doubles.\n\nwww.it-ebooks.info\n\n123",
      "content_length": 688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Chapter 11\n\nUsing Test Doubles\n\nAbout This Chapter\n\nThe last few chapters concluding with Chapter 10, Result Veriﬁ cation, intro- duced the basic mechanisms of running tests using the xUnit family of Test Automation Frameworks (page 298). For the most part we assumed that the SUT was designed such that it could be tested easily in isolation of other pieces of soft- ware. When a class does not depend on any other classes, testing it is relatively straightforward and the techniques described in this chapter are unnecessary. When a class does depend on other classes, we have two choices: We can test it together with all the other classes it depends on or we can try to isolate it from the other classes so that we can test it by itself. This chapter introduces techniques for isolating the SUT from the other software components on which it depends.\n\nWhat Are Indirect Inputs and Outputs?\n\nThe problem with testing classes in groups or clusters is that it becomes very hard to cover all the paths through the code. The depended-on component (DOC) may return values or throw exceptions that affect the behavior of the SUT, but it may prove difﬁ cult or impossible to cause certain cases to occur. The indirect inputs received from the DOC may be unpredictable (such as the system clock or cal- endar). In other cases, the DOC may not be available in the test environment or may not even exist. How can we test dependent classes in these circumstances?\n\nIn other cases, we need to verify that certain side effects of executing the SUT have, indeed, occurred. If it is too difﬁ cult to monitor these indirect outputs of the SUT (or if it is too expensive to retrieve them), the effectiveness of our automated testing may be compromised.\n\nAs you will no doubt have guessed from the title of this chapter, the solution to these problems is often the use of a Test Double (page 522). We will start by\n\n125\n\nwww.it-ebooks.info",
      "content_length": 1924,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "126\n\nChapter 11 Using Test Doubles\n\nlooking at how we can use Test Doubles to test indirect inputs and outputs. We will then describe a few other uses of these helpful mechanisms.\n\nWhy Do We Care about Indirect Inputs?\n\nCalls to DOCs often return objects or values, update their arguments or even throw exceptions. Many of the execution paths within the SUT are intended to deal with these return values and to handle the possible exceptions. Leaving these paths un- tested leads to Untested Code (see Production Bugs on page 268). These paths can be the most challenging to test effectively but are also among the most likely to lead to catastrophic failures if exercised for the very ﬁ rst time in production.\n\nWe certainly would rather not have the exception-handling code execute for the ﬁ rst time in production. What if it was coded incorrectly? Clearly, it would be high- ly desirable to have automated tests for such code. The testing challenge is to some- how cause the DOC to throw an exception so that the error path can be tested. The exception we expect the DOC to throw is a good example of an indirect input test condition (Figure 11.1). Our means of injecting this input is a control point.\n\nFixture Fixture\n\nSetup Setup\n\nDOC DOC\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nIndirect Indirect Input Input\n\nTeardown Teardown\n\nFigure 11.1 An indirect input being received by the SUT from a DOC. Not all inputs of the SUT come from the test. Some indirect inputs come from other components called by the SUT in the form of return values, updated parameters, or exceptions thrown.\n\nWhy Do We Care about Indirect Outputs?\n\nThe concept of encapsulation often directs us to not care about how some- thing is implemented. After all, that is the whole purpose of encapsulation—to alleviate the need for clients of our interface to care about our implementation.\n\nwww.it-ebooks.info",
      "content_length": 1887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "What Are Indirect Inputs and Outputs?\n\nWhen testing, we try to verify the implementation precisely so our clients do not have to care about it.\n\nConsider for a moment a component that has a method in its API that returns nothing—or at least nothing that can be used to determine whether it has performed its function correctly. In this situation, we have no choice but to test through the back door. A good example of this is a message logging system. Calls to the API of a logger rarely return anything that indicates it did its job correctly. The only way to determine whether the message logging system is working as expected is to interact with it through some other interface—one that allows us to retrieve the logged messages.\n\nA client of the logger may specify that the logger be called when certain con- ditions are met. These calls will not be visible on the client’s interface but would typically be a requirement that the client needs to satisfy and, therefore, would be something we want to test. The circumstances that should result in a messag- ing being logged are indirect output test conditions (Figure 11.2) for which we need to write tests so that we can avoid having Untested Requirements (see Pro- duction Bugs). Our means of seeing this output is an observation point.\n\nFixture Fixture\n\nSetup Setup\n\nDOC DOC\n\nExercise Exercise\n\nSUT SUT\n\nIndirect Indirect Output Output\n\nVerify Verify\n\nTeardown Teardown\n\nFigure 11.2 An indirect output being received by the SUT. Not all outputs of the SUT are directly visible to the test. Some indirect outputs are sent to other components in the form of method calls or messages.\n\nIn other cases, the SUT does produce visible behavior that can be veriﬁ ed through the front door but also has some expected side effects. Both outputs need to be veriﬁ ed in our tests. Sometimes this testing is simply a matter of adding assertions for the indirect outputs to the existing tests to verify the Untested Requirement.\n\nwww.it-ebooks.info\n\n127",
      "content_length": 1995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "128\n\nChapter 11 Using Test Doubles\n\nHow Do We Control Indirect Inputs?\n\nTesting with indirect inputs is a bit simpler than testing with indirect outputs because the techniques used to test outputs build on those used to test inputs. Let’s delve into indirect inputs ﬁ rst.\n\nTo test the SUT with indirect inputs, we must be able to control th e DOC well enough to cause it to return every possible kind of return value. That implies the availability of a suitable control point.\n\nExamples of the kinds of indirect inputs we want to be able to induce via this\n\ncontrol point include\n\nReturn values of methods/functions\n\nValues of updatable arguments\n\nExceptions that could be thrown\n\nOften, the test can interact with the DOC to set up how it will respond to requests. For example, if a component provides access to data in a database, then we can use Back Door Setup (see Back Door Manipulation on page 327) to insert speciﬁ c values into a database that cause the component to respond in the desired ways (e.g., no items found, one item found, many items found). (See Figure 11.3.) In this speciﬁ c case, we can use the database itself as a control point.\n\nSetup Setup Setup Setup\n\nExercise Exercise Exercise Exercise\n\nSUT SUT SUT SUT\n\nData Data Data Data Fixture Fixture Fixture Fixture\n\nVerify Verify Verify Verify\n\nTeardown Teardown Teardown Teardown\n\nFigure 11.3 Using Back Door Manipulation to indirectly control and observe the SUT. When the SUT stores its state in another component, we may be able to manipulate that state by having the test interact directly with the other com- ponent via a “back door.”\n\nIn most cases, however, this approach is neither practical nor even possible. We might not be able to use the real component for the following reasons:\n\nwww.it-ebooks.info",
      "content_length": 1786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "What Are Indirect Inputs and Outputs?\n\nThe real component cannot be manipulated to produce the desired indirect input. Only a true software error within the real component would result in the desired input to the SUT.\n\nThe real component could be manipulated to make the input occur but\n\ndoing so would not be cost-effective.\n\nThe real component could be manipulated to make the input occur but\n\ndoing so could have unacceptable side effects.\n\nThe real component is not yet available for use.\n\nIf we cannot use the real component as a control point, then we have to replace it with one that we can control. This replacement can be done in a number of different ways, which are the focus of the section Installing the Test Double later in this chapter. The most common approach is to conﬁ gure a Test Stub (page 529) with a set of values to return from its functions and then to install this Test Stub into the SUT. During execution of the SUT, the Test Stub receives the calls and returns the previously conﬁ gured responses (Figure 11.4). It has become our control point.\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nCreation Creation\n\nTest Test Stub Stub\n\nInstallation Installation\n\nExercise Exercise\n\nSUT SUT\n\nReturn Return Values Values\n\nVerify Verify\n\nIndirect Indirect Input Input\n\nTeardown Teardown\n\nFigure 11.4 Using a Test Stub as a control point for indirect inputs. One way to use a control point to inject indirect inputs into the SUT is to install a Test Stub in place of the DOC. Before exercising the SUT, we tell the Test Stub what it should return to the SUT when it is called. This strategy allows us to force the SUT through all its code paths.\n\nwww.it-ebooks.info\n\n129",
      "content_length": 1683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "130\n\nChapter 11 Using Test Doubles\n\nHow Do We Verify Indirect Outputs?\n\nIn normal usage, as the SUT is exercised, it interacts naturally with the component(s) upon which it depends. To test the indirect outputs, we must be able to observe the calls that the SUT makes to the API of the DOC (Figure 11.5). Furthermore, if we need the test to progress beyond that point, we need to be able to control the val- ues returned (as was discussed in the discussion of indirect inputs).\n\nSetup Setup\n\nFixture Fixture Behavior Behavior (Indirect (Indirect Outputs) Outputs)\n\nDOC DOC\n\nExercise Exercise\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nA A\n\nB B\n\nC C\n\ny y f f i i r r e e V V\n\nTeardown Teardown\n\nFigure 11.5 Using Behavior Veriﬁ cation to verify the indirect outputs of the SUT. When we care about exactly what calls our SUT makes to other components, we may have to do Behavior Veriﬁ cation rather than simply verifying the post-test state of the SUT.\n\nIn many cases, the test can use the DOC as an observation point to ﬁ nd out how it has been used. For example:\n\nWe can ask the ﬁ le system for the contents of a ﬁ le that the SUT has writ- ten to verify that it exists and was written with the expected contents.\n\nWe can ask the database for the contents of a table or speciﬁ c record to\n\nverify that the SUT wrote the expected records to the database.\n\nWe can interact directly with the e-mail sending component to ask\n\nwhether the SUT had asked it to send a particular e-mail.\n\nThese are all examples of Back Door Veriﬁ cation (see Back Door Manipulation on page 327). Some DOCs allow us to conﬁ gure their behavior in such a way that we can use them to keep the test informed of how they are being used:\n\nwww.it-ebooks.info",
      "content_length": 1728,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "What Are Indirect Inputs and Outputs?\n\nWe can ask the ﬁ le system to notify the test whenever a ﬁ le is created or\n\nmodiﬁ ed so we can verify its contents.\n\nWe can use a database trigger to notify the test when a record is written\n\nor deleted.\n\nWe can conﬁ gure the e-mail sending component to deliver all outgoing\n\ne-mail to the test.\n\nSometimes, as we have seen with indirect inputs, it is not practical to use the real component as an observation point. When all else fails, we may need to replace the real component with a test-speciﬁ c alternative. For example, we might need to do this for the following reasons:\n\nThe calls to (or the internal state of) the DOC cannot be queried.\n\nThe real component can be queried but doing so is cost-prohibitive.\n\nThe real component can be queried but doing so has unacceptable side\n\neffects.\n\nThe real component is not yet available for use.\n\nThe replacement of the real component can be done in a number of different ways, as will be discussed in Installing the Test Double.\n\nTwo basic styles of indirect output veriﬁ cation are available. Procedural Behav- ior Veriﬁ cation (see Behavior Veriﬁ cation) captures the calls to a DOC (or their re- sults) during SUT execution and then compares them with the expected calls after the SUT has ﬁ nished executing. This veriﬁ cation involves replacing a substitutable dependency with a Test Spy (page 538). During execution of the SUT, the Test Spy receives the calls and records them. After the Test Method (page 348) has ﬁ nished exercising the SUT, it retrieves the actual calls from the Test Spy and uses Assertion Methods (page 362) to compare them with the expected calls (Figure 11.6).\n\nExpected Behavior (see Behavior Veriﬁ cation) involves building a “behavior speciﬁ cation” during the ﬁ xture setup phase of the test and then comparing the actual behavior with this Expected Behavior. It is typically done by loading a Mock Object (page 544) with a set of expected procedure call descriptions and installing this object into the SUT (Figure 11.7). During execution of the SUT, the Mock Object receives the calls and compares them to the previously deﬁ ned expected calls (the “behavior speciﬁ cation”). As the test proceeds, if the Mock Object receives an unexpected call, it fails the test immediately. The test failure traceback will show the exact location in the SUT where the problem occurred because the Assertion Methods are called from the Mock Object, which is in turn called by the SUT. We can also see exactly where in the Test Method the SUT was being exercised.\n\nwww.it-ebooks.info\n\n131",
      "content_length": 2598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "132\n\nChapter 11 Using Test Doubles\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nCreation Creation\n\nTest Spy Test Spy\n\nExercise Exercise\n\nInstallation Installation\n\nExercise Exercise\n\nSUT SUT\n\nIndirect Indirect Output Output\n\nIndirect Indirect Outputs Outputs\n\nVerify Verify\n\nTeardown Teardown\n\nFigure 11.6 Using a Test Spy as an observation point for indirect outputs of the SUT. One way to implement Behavior Veriﬁ cation is to install a Test Spy in place of the target of the indirect outputs. After exercising the SUT, the test asks the Test Spy for information about how it was used and compares that information to the expected behavior using assertions.\n\nSetup Setup\n\nCreation Creation\n\nFixture Fixture\n\nMock Mock Object Object\n\nDOC DOC\n\nInstallation Installation\n\nExpectations Expectations\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nSUT SUT\n\nFinal Verification Final Verification\n\nIndirect Indirect Output Output\n\ny y f f i i r r e e V V\n\nTeardown Teardown\n\nFigure 11.7 Using a Mock Object as an observation point for indirect outputs of the SUT. Another way to implement Behavior Veriﬁ cation is to install a Mock Object in place of the target of the indirect outputs. As the SUT makes calls to the DOC, the Mock Object uses assertions to compare the actual calls and arguments with the expected calls and arguments.\n\nwww.it-ebooks.info",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Testing with Doubles\n\nWhen we use a Test Spy or a Mock Object, we may also have to employ it as a control point for any indirect inputs on which the SUT depends after the Test Spy or Mock Object has been called to allow test execution to continue.\n\nTesting with Doubles\n\nBy now you are probably wondering about how to replace those inﬂ exible and uncooperative real components with something that makes it easier to control the indirect inputs and to verify the indirect outputs.\n\nAs we have seen, to test the indirect inputs, we must be able to control the DOC well enough to cause it to return every possible kind of return value (valid, invalid, and exception). To test indirect outputs, we must be able to track the calls the SUT makes to other components. A Test Double is a type of object that is much more cooperative and lets us write tests the way we want to.\n\nTypes of Test Doubles\n\nA Test Double is any object or component that we install in place of the real component for the express purpose of running a test. Depending on the reason why we are using it, a Test Double can behave in one of four ways (summarized in Figure 11.8):\n\nA Dummy Object (page 728) is a placeholder object that is passed to the SUT as an argument (or an attribute of an argument) but is never actually used.\n\nA Test Stub is an object that replaces a real component on which the SUT depends so that the test can control the indirect inputs of the SUT. It allows the test to force the SUT down paths it might not otherwise exercise. A Test Spy, which is a more capable version of a Test Stub, can be used to verify the indirect outputs of the SUT by giving the test a way to inspect them after exercising the SUT.\n\nA Mock Object is an object that replaces a real component on which\n\nthe SUT depends so that the test can verify its indirect outputs.\n\nwww.it-ebooks.info\n\n133",
      "content_length": 1859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "134\n\nChapter 11 Using Test Doubles\n\nA Fake Object (page 551) (or just “Fake” for short) is an object that replaces the functionality of the real DOC with an alternative imple- mentation of the same functionality.\n\nTest Test Double Double\n\nDummy Dummy Object Object\n\nTest Test Stub Stub\n\nTest Test Spy Spy\n\nMock Mock Object Object\n\nFake Fake Object Object\n\nConfigurable Configurable Test Double Test Double\n\nHard-Coded Hard-Coded Test Double Test Double\n\nFigure 11.8 Several kinds of Test Doubles exist. Dummy Objects are really an alternative to the value patterns. Test Stubs are used to verify indirect inputs; Test Spies and Mock Objects are used to verify indirect outputs. Fake objects emulate the behavior of the real depended-on component, but with test-friendly characteristics.\n\nDummy Objects\n\nDummy Objects are a degenerate form of Test Double. They exist solely so that they can be passed around from method to method; they are never used. That is, Dummy Objects are not expected to do anything except exist. Often, we can get away with using “null” (or “nil” or “nothing”); at other times, we may be forced to create a real object because the code expects something non-null. In dynamically typed languages, almost any real object will do; in statically typed languages, we must make sure that the Dummy Object is “type-compatible” with the parameter it is being passed as or the variable to which it is being assigned.\n\nIn the following example, we pass an instance of DummyCustomer to the Invoice constructor to satisfy a mandatory argument. We do not expect the DummyCustomer to be used by the code we are testing here.\n\nwww.it-ebooks.info",
      "content_length": 1654,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "Testing with Doubles\n\npublic void testInvoice_addLineItem_DO() { ﬁnal int QUANTITY = 1; Product product = new Product(\"Dummy Product Name\", getUniqueNumber()); Invoice inv = new Invoice( new DummyCustomer() ); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\", expItem, actual); }\n\nNote that a Dummy Object is not the same as a Null Object [PLOPD3]. A Dummy Object is not used by the SUT, so its behavior is irrelevant. By contrast, a Null Object is used by the SUT but is designed to do nothing. That’s a small but very important distinction!\n\nDummy Objects are in a different league than the other Test Doubles; they are really an alternative to the attribute value patterns such as Literal Value (page 714), Generated Value (page 723), and Derived Value (page 718). Therefore, we don’t need to “conﬁ gure” them or “install” them. In fact, almost nothing we say about the other Test Doubles applies to Dummy Objects, so we won’t mention them again in this chapter.\n\nTest Stubs\n\nA Test Stub is an object that acts as a control point to deliver indirect inputs to the SUT when the Test Stub’s methods are called. Its use allows us to exercise Untested Code paths in the SUT that might otherwise be impossible to traverse during testing. A Responder (see Test Stub) is a basic Test Stub that is used to inject valid and invalid indirect inputs into the SUT via normal returns from method calls. A Saboteur (see Test Stub) is a special Test Stub that raises exceptions or errors to inject abnormal indirect inputs into the SUT. Because procedural programming languages do not support objects, they force us to use Procedural Test Stubs (see Test Stub).\n\nIn the following example, the Saboteur—implemented as an anonymous inner class in Java—throws an exception when the SUT calls the getTime method to allow us to verify that the SUT behaves correctly in this case:\n\npublic void testDisplayCurrentTime_exception() throws Exception { // Fixture setup\n\nwww.it-ebooks.info\n\n135",
      "content_length": 2202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "136\n\nChapter 11 Using Test Doubles\n\n// Deﬁne and instantiate Test Stub TimeProvider testStub = new TimeProvider() { // Anonymous inner Test Stub public Calendar getTime() throws TimeProviderEx { throw new TimeProviderEx(\"Sample\"); } }; // Instantiate SUT TimeDisplay sut = new TimeDisplay(); sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"error\\\">Invalid Time</span>\"; assertEquals(\"Exception\", expectedTimeString, result); }\n\nIn procedural programming languages, a Procedural Test Stub is either (1) a Test Stub implemented as a stand-in for an as-yet-unwritten procedure or (2) an alternative implementation of a procedure linked into the program instead of the real implementation of the procedure. Traditionally, Procedural Test Stubs are introduced to allow debugging to proceed while we are waiting for other code to be ready. They are rarely “swapped in” at runtime—this is hard to do in most procedural languages. If we do not mind introducing Test Logic in Pro- duction (page 217) code, we can implement a Procedural Test Stub using Test Hooks (page 709) such as if testing then ... else in the SUT. This is illustrated in the following listing:\n\npublic Calendar getTime() throws TimeProviderEx { Calendar theTime = new GregorianCalendar(); if (TESTING) { theTime.set(Calendar.HOUR_OF_DAY, 0); theTime.set(Calendar.MINUTE, 0);} else { // just return the calendar } return theTime; };\n\nThe key exception occurs in languages that support procedure variables.1 These variables allow us to implement dynamic binding as long as the client code ac- cesses the procedure to be replaced via a procedure variable.\n\n1 Also called function pointers.\n\nwww.it-ebooks.info",
      "content_length": 1786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Testing with Doubles\n\nTest Spies\n\nA Test Spy is an object that can act as an observation point for the indirect outputs of the SUT. To the capabilities of a Test Stub, it adds the ability to quietly record all calls made to its methods by the SUT. The veriﬁ cation part of the test performs Procedural Behavior Veriﬁ cation on those calls by using a series of assertions to compare the actual calls received by the Test Spy with the expected calls.\n\nThe following example uses the Retrieval Interface (see Test Spy) on the Test Spy to verify that the correct information was passed as arguments in the call to the logMessage method by the SUT (the removeFlight method of the facade).\n\npublic void testRemoveFlightLogging_recordingTestStub() throws Exception { // Fixture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // Test Double setup AuditLogSpy logSpy = new AuditLogSpy(); facade.setAuditLog(logSpy); // Exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // Verify state assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); // Verify indirect outputs using retrieval interface of spy assertEquals(\"number of calls\", 1, logSpy.getNumberOfCalls()); assertEquals(\"action code\", Helper.REMOVE_FLIGHT_ACTION_CODE, logSpy.getActionCode()); assertEquals(\"date\", helper.getTodaysDateWithoutTime(), logSpy.getDate()); assertEquals(\"user\", Helper.TEST_USER_NAME, logSpy.getUser()); assertEquals(\"detail\", expectedFlightDto.getFlightNumber(), logSpy.getDetail()); }\n\nMock Objects\n\nA Mock Object is also an object that can act as an observation point for the indirect outputs of the SUT. Like a Test Stub, it may need to return information in response to method calls. Also like a Test Spy, a Mock Object pays attention to how it was called by the SUT. It differs from a Test Spy, however, in that the\n\nwww.it-ebooks.info\n\n137",
      "content_length": 1975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "138\n\nChapter 11 Using Test Doubles\n\nMock Object compares actual calls received with the previously deﬁ ned expec- tations using assertions and fails the test on behalf of the Test Method. As a consequence, we can reuse the logic employed to verify the indirect outputs of the SUT across all tests that use the same Mock Object. Mock Objects come in two basic ﬂ avors:\n\nA strict Mock Object fails the test if the correct calls are received in a\n\ndifferent order than was speciﬁ ed.\n\nA lenient2 Mock Object tolerates out-of-order calls. Some lenient Mock Objects tolerate or even ignore unexpected calls or missed calls. That is, the Mock Object may verify only those actual calls that correspond to expected ones.\n\nThe following test conﬁ gures a Mock Object with the arguments of the expected call to logMessage. When the SUT (the removeFlight method) calls logMessage, the Mock Object asserts that each of the actual arguments equals the expected argu- ment. If it discovers that any wrong arguments were passed, the test fails.\n\npublic void testRemoveFlight_Mock() throws Exception { // Fixture setup FlightDto expectedFlightDto = createAnonRegFlight(); // Mock conﬁguration ConﬁgurableMockAuditLog mockLog = new ConﬁgurableMockAuditLog(); mockLog.setExpectedLogMessage( helper.getTodaysDateWithoutTime(), Helper.TEST_USER_NAME, Helper.REMOVE_FLIGHT_ACTION_CODE, expectedFlightDto.getFlightNumber()); mockLog.setExpectedNumberCalls(1); // Mock installation FlightManagementFacade facade = new FlightManagementFacadeImpl(); facade.setAuditLog(mockLog); // Exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // Verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); mockLog.verify(); }\n\n2 Lenient Mock Objects are sometimes called “nice,” but “lenient” is a more precise adjective.\n\nwww.it-ebooks.info",
      "content_length": 1883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Testing with Doubles\n\nLike Test Stubs, Mock Objects often support conﬁ guration with any indirect inputs required to allow the SUT to advance to the point where it would generate the indirect outputs they are verifying.\n\nFake Objects\n\nA Fake Object is quite different from a Test Stub or a Mock Object in that it is nei- ther directly controlled nor observed by the test. The Fake Object is used to replace the functionality of the real DOC in a test for reasons other than veriﬁ cation of indi- rect inputs and outputs. Typically, a Fake Object implements the same functionality or a subset of the functionality of the real DOC, albeit in a much simpler way. The most common reasons for using a Fake Object are that the real DOC has not yet been built, is too slow, or is not available in the test environment.\n\nThe sidebar “Faster Tests without Shared Fixtures” (page 319) describes how my team encapsulated all database access behind a persistence layer interface and then replaced the persistence layer component with one that used in-memory hash tables instead of a real database, thereby making our tests run 50 times faster. To do so, we used a Fake Database (see Fake Object) that was something like this one:\n\npublic class InMemoryDatabase implements FlightDao{ private List airports = new Vector(); public Airport createAirport(String airportCode, String name, String nearbyCity) throws DataException, InvalidArgumentException { assertParamtersAreValid( airportCode, name, nearbyCity); assertAirportDoesntExist( airportCode); Airport result = new Airport(getNextAirportId(), airportCode, name, createCity(nearbyCity)); airports.add(result); return result; } public Airport getAirportByPrimaryKey(BigDecimal airportId) throws DataException, InvalidArgumentException { assertAirportNotNull(airportId);\n\nAirport result = null; Iterator i = airports.iterator(); while (i.hasNext()) { Airport airport = (Airport) i.next(); if (airport.getId().equals(airportId)) { return airport; } } throw new DataException(\"Airport not found:\"+airportId); }\n\nwww.it-ebooks.info\n\n139",
      "content_length": 2072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "140\n\nChapter 11 Using Test Doubles\n\nProviding the Test Double\n\nThere are two approaches to providing a Test Double: a Hand-Built Test Dou- ble (see Conﬁ gurable Test Double on page 558), which is coded by the test automater, or a Dynamically Generated Test Double (see Conﬁ gurable Test Double), which is generated at runtime using a framework or toolkit provided by some other developer.3 All generated Test Doubles must be, by their very nature, Conﬁ gurable Test Doubles; these components are covered in more detail in the next section. Hand-Built Test Doubles, by contrast, tend to be Hard-Coded Test Doubles (page 568) but can also be made conﬁ gurable with some additional effort. The following code sample illustrates a hand-coded Inner Test Double (see Hard-Coded Test Double) that uses Java’s anonymous inner class construct:\n\npublic void testDisplayCurrentTime_AtMidnight_PS() throws Exception { // Fixture setup // Deﬁne and instantiate Test Stub TimeProvider testStub = new PseudoTimeProvider() { // Anonymous inner stub public Calendar getTime(String timeZone) { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.MINUTE, 0); myTime.set(Calendar.HOUR_OF_DAY, 0); return myTime; } }; // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Inject Test Stub into SUT sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nWe can greatly simplify the development of Hand-Built Test Doubles in statically typed languages such as Java and C# by providing a set of base classes called Pseudo-Objects (see Hard-Coded Test Double) from which to create sub- classes. Pseudo-Objects can reduce the number of methods we need to implement\n\n3 JMock and its ports to other languages are good examples of such toolkits. Other toolkits, such as EasyMock, implement Statically Generated Test Doubles (see Conﬁ gurable Test Double) by generating code that is then compiled just like a Hand-Built Test Double.\n\nwww.it-ebooks.info",
      "content_length": 2139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Testing with Doubles\n\nin each Test Stub, Test Spy, or Mock Object to just the ones we expect to be called. They are especially helpful when we are using Inner Test Doubles or Self Shunts (see Hard-Coded Test Double). The class deﬁ nition for the Pseudo-Object used in the previous example looks like this:\n\n/** * Base class for hand-coded Test Stubs and Mock Objects */ public class PseudoTimeProvider implements ComplexTimeProvider {\n\npublic Calendar getTime() throws TimeProviderEx { throw new PseudoClassException(); }\n\npublic Calendar getTimeDifference(Calendar baseTime, Calendar otherTime) throws TimeProviderEx { throw new PseudoClassException(); }\n\npublic Calendar getTime( String timeZone ) throws TimeProviderEx { throw new PseudoClassException(); } }\n\nConﬁ guring the Test Double\n\nSome Test Doubles (speciﬁ cally, Test Stubs and Mock Objects) need to be told which values to return and/or which values to expect. A Hard-Coded Test Double receives these instructions at design time from the test automater; a Conﬁ gurable Test Double is told this information at runtime by the test (Figure 11.9). A Test Stub or Test Spy needs to be conﬁ gured only with the values that will be returned by the methods that the SUT is expected to invoke. A Mock Object also needs to be conﬁ gured with the names and arguments of all methods we expect the SUT to invoke on it. In all cases, the test automater ultimately decides with which values to conﬁ gure the Test Double. Not surprisingly, the primary considerations when making this deci- sion are the understandability of the test and the potential reusability of the Test Double code.\n\nFake Objects do not need to be “conﬁ gured” at runtime because they are just used by the SUT; later outputs depend on the earlier calls by the SUT. Similarly, Dummy Objects do not need to be “conﬁ gured” because they should never be\n\nwww.it-ebooks.info\n\n141",
      "content_length": 1893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "142\n\nChapter 11 Using Test Doubles\n\nexecuted.4 Procedural Test Stubs are typically built as Hard-Coded Test Doubles. That is, they are hard-coded to return a particular value when the function is called—thus they are the simplest form of Test Double.\n\nExpectations, Expectations, Return Values Return Values\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nConfiguration Configuration\n\nInstallation Installation\n\nExpectations Expectations\n\nExercise Exercise\n\nSUT SUT\n\nReturn Return Values Values\n\nVerify Verify\n\nTeardown Teardown\n\nTest Test Double Double\n\nFigure 11.9 A Test Double being conﬁ gured by the test. We can avoid a proliferation of Hard-Coded Test Doubles classes by passing return values or expectation to the Conﬁ gurable Test Double at runtime.\n\nA Conﬁ gurable Test Double can provide either a Conﬁ guration Interface (see Conﬁ gurable Test Double) or a Conﬁ guration Mode (see Conﬁ gurable Test Double) that the test can use to conﬁ gure the Test Double with the values to return or expect. As a consequence, Conﬁ gurable Test Doubles are reusable across many tests. Use of these Conﬁ gurable Test Doubles also makes tests more understandable because the values used by the Test Double are visible within the test, thus avoiding the smell of a Mystery Guest (see Obscure Test on page 186).\n\nSo where should this conﬁ guration take place? The installation of the Test Double should be treated just like any other part of ﬁ xture setup. Alternatives such as In-line Setup (page 408), Implicit Setup (page 424), and Delegated Setup (page 411) are all available.\n\n4 A Dummy Object can be used as an observation point to verify that it was never used by ensuring that the Dummy Object throws an exception if any of its methods are called.\n\nwww.it-ebooks.info",
      "content_length": 1760,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Testing with Doubles\n\nInstalling the Test Double\n\nBefore we exercise the SUT, we need to “install” any Test Doubles on which our test depends. The term “install” here serves as a generic way to describe the process of telling the SUT to use our Test Double, regardless of the exact details regarding how we do it. The normal sequence is to instantiate the Test Double, conﬁ gure it if it is a Conﬁ gurable Test Double, and then tell the SUT to use the Test Double either before or as we exercise the SUT. There are several distinct ways to “install” the Test Double, and the choice between them may be as much a matter of style as of necessity if we are designing the SUT for testability. Our choices may be much more constrained, however, when we try to retroﬁ t our tests to an existing design.\n\nThe basic choices boil down to Dependency Injection (page 678), in which the client software tells the SUT which DOC to use; Dependency Lookup (page 686), in which the SUT delegates the construction or retrieval of the DOC to another object; and Test Hook, in which the DOC or the calls to it within the SUT are modiﬁ ed.\n\nIf an inversion of control framework is available in our language, our tests can substitute dependencies without much additional work on our part. This removes the need for building in the Dependency Injection or Dependency Lookup mechanism.\n\nDependency Injection\n\nDependency Injection is a class of design decoupling in which the client tells the SUT which DOC to use at runtime (Figure 11.10). The test-driven development (TDD) movement has greatly increased its popularity because Dependency Injec- tion makes for more easily tested designs. This pattern also makes it possible to reuse the SUT more broadly because it removes knowledge of the dependency from the SUT; often the SUT will be aware of only a generic interface that the DOC must implement. Dependency Injection comes in several speciﬁ c ﬂ avors, with the choice between them being largely a matter of taste:\n\nSetter Injection (see Dependency Injection): The SUT accesses the DOC through a public attribute (i.e., a variable or property). The test explicitly sets the attribute after instantiating the SUT to installing the Test Double. The SUT may have previously initialized the attribute with the real DOC in its constructor (in which case the test is replac- ing it) or the SUT may use Lazy Initialization [SBPP] to initialize the attribute (in which case the SUT will not bother to install the real DOC).\n\nwww.it-ebooks.info\n\n143",
      "content_length": 2521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "144\n\nChapter 11 Using Test Doubles\n\nClient Client\n\nCreation Creation\n\nDOC DOC\n\nUsage Usage\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nExercise Exercise\n\nCreation Creation\n\nSUT SUT\n\nUsage Usage\n\nTest Test Double Double\n\nFigure 11.10 A Test Double being “injected” into the SUT by a test. Using Test Doubles requires a means to replace the DOC. Using Dependency Injection involves having the caller supply the dependency to the SUT before or as it is used.\n\nConstructor Injection (see Dependency Injection): The SUT accesses the DOC through a private attribute. The test passes the Test Dou- ble to the SUT via a constructor that takes the DOC to be used as an explicit argument and initializes the attribute from it. This may be the primary constructor used by production code clients or it may be an alternative constructor. In the latter case, the primary constructor should call this constructor, passing the default DOC to it as an argument.\n\nParameter Injection (see Dependency Injection): The SUT receives the DOC as a method parameter. The test passes in a Test Double, whereas the production code passes in the real object.5 This approach works well when the API of the SUT takes as a parameter the object we need to replace. Although Mock Object aﬁ cionados might argue that designing APIs in this way improves the design of the SUT, it is not always possible or practical to pass everything required to each method.\n\nDependency Lookup\n\nWhen software is not designed for testability or when Dependency Injection is not appropriate, we may ﬁ nd it convenient to use Dependency Lookup. This pattern also removes the knowledge of exactly which DOC should be used from\n\n5 This approach was advocated in the original paper on Mock Objects [ET]. In this paper, Mock Objects passed as parameters to methods are called “Smart Handlers.”\n\nwww.it-ebooks.info",
      "content_length": 1877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Testing with Doubles\n\nthe SUT, but it does so by having the SUT ask another piece of software to create or ﬁ nd the DOC on its behalf (Figure 11.11). This opens the door to changing the DOC at runtime without modifying the SUT’s code. We do have to modify the behavior of the intermediary somehow, and this is where the speciﬁ c variants of Dependency Lookup differ from one another:\n\nConfiguration Configuration with Test Double with Test Double\n\nSetup Setup\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nFind or Create Find or Create\n\nCreation Creation\n\nor or\n\nDOC DOC\n\nTeardown Teardown\n\nCreation Creation\n\nClient Client\n\nUsage Usage\n\nSUT SUT\n\nUsage Usage\n\nUsage Usage\n\nTest Test Double Double\n\nFigure 11.11 A Service Locator being “conﬁ gured” by a test to return a Test Double to the SUT. Using Test Doubles requires a means to replace the DOC. Using Dependency Lookup involves having the SUT ask a well-known object to provide a reference to the DOC; the test can provide the Service Locator with a Test Double to return.\n\nObject Factory (see Dependency Lookup): The SUT creates the DOC by calling a Factory Method [GOF] on a well-known object instead of using an object constructor to create the DOC directly. The test explic- itly tells the Object Factory to create a Test Double instead of a normal DOC whenever this method is called .\n\nService Locator (see Dependency Lookup): The SUT retrieves a previ- ously created service object by asking a well-known Registry [PEAA] object for it. The test conﬁ gures the Service Locator to return the Test Double when the SUT requests the DOC.\n\nThe line between these two patterns can become quite blurry when we use Lazy Initialization to create the object being returned by a Service Locator. Should it be called an Object Factory instead? Does it really matter which label we apply? Probably not—hence the generic name of Dependency Lookup.\n\nwww.it-ebooks.info\n\n145",
      "content_length": 1926,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "146\n\nChapter 11 Using Test Doubles\n\nRetroﬁ tting Testability Using a Test-Speciﬁ c Subclass\n\nEven when none of these mechanisms is built into the SUT, we may be able to retroﬁ t them relatively easily by using a Test-Speciﬁ c Subclass.\n\nThe use of Singletons [GOF] speciﬁ cally to act as an Object Factory or Service Locator is common. If the Singleton has hard-coded behavior, we may have to turn it into a Substitutable Singleton (see Test-Speciﬁ c Subclass on page 579) to enable overriding the normally returned DOC with our Test Double. The use of Singletons can be avoided through the use of an IOC tool or a manually coded Dependency Injection mechanism. Both of these choices are preferable because they make the test’s dependency on a Test Double more obvious. Singletons used for other purposes almost always cause headaches when we are writing tests and should be avoided if possible.\n\nOur test can instantiate a Test-Speciﬁ c Subclass of the SUT to add a Depen- dency Injection mechanism or to replace other methods of the SUT with test-spe- ciﬁ c behavior; see Figure 11.12. We can override any logic used to access a DOC, thereby making it possible to return a Test Double instead of the normal DOC without modifying the production code. We can also replace the implementations of any methods being called from the method we are testing with Test Stub-like behavior, thereby turning the SUT into its own Subclassed Test Double (see Test- Speciﬁ c Subclass). This is one way to inject indirect inputs into the SUT.\n\nSUT SUT\n\nExercise Exercise\n\nMethod Under Test Method Under Test\n\nSetup Setup\n\nCreate Create\n\nInternal Method Internal Method\n\nExercise Exercise\n\nVerify Verify\n\nSet State Set State\n\nGet State Get State\n\nTest- Test- Specific Specific Subclass Subclass\n\nInternal Method Internal Method\n\nOverridden Overridden Self Call Self Call\n\nTeardown Teardown\n\nFigure 11.12 Using a Test-Speciﬁ c Subclass of the SUT. When all else fails, we can always try subclassing the SUT to change or expose functionality we need to enable testing\n\nwww.it-ebooks.info",
      "content_length": 2069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "Testing with Doubles\n\nThe main prerequisite of using a Test-Speciﬁ c Subclass of the SUT is that the SUT must use Self-Calls [WWW] to nonprivate methods that implement any functionality we need to override from the test. Small, single-purpose methods rule! The main drawback of this approach is that it is possible to accidentally override parts of the behavior we are intending to test.\n\nWe can also subclass the DOC to insert test-speciﬁ c behavior, effectively turning it into a Subclassed Test Double (Figure 11.13). This strategy is some- what safer than subclassing the SUT because it avoids the possibility of acci- dentally overriding those parts of the SUT that we are testing. The trick, however, is to get the SUT to use the Test-Speciﬁ c Subclass instead of the DOC. In practice, this implies that we must use one of the Dependency Injection or Dependency Lookup techniques, unless the DOC is a Singleton. When the SUT uses a Singleton by calling a static soleInstance method on a hard-coded class name, the test can cause the soleInstance method to return an instance of a Test Double by subclassing the Singleton class and initializing the real Singleton’s soleInstance class variable to hold an instance of the Test Double. The returned Test Double may need to be a Subclassed Test Double if the type of the vari- able used to hold the Singleton’s sole instance is hard-coded as the Singleton’s class. Although we often use this technique to get a Service Locator to return a different service, but we can also use a Subclassed Test Double directly with- out an intermediary Service Locator.\n\nFixture Fixture\n\nSetup Setup\n\nDOC DOC\n\nExercise Exercise\n\nVerify Verify\n\nTeardown Teardown\n\nSUT SUT\n\nSub- Sub- classed classed Test Test Double Double\n\nFigure 11.13 Using A Test Double subclassed from the DOC. One way to build a Test Double is to subclass the real class and override the implementation of any methods we need to control the indirect inputs or verify indirect outputs.\n\nwww.it-ebooks.info\n\n147",
      "content_length": 2017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "148\n\nChapter 11 Using Test Doubles\n\nOther Ways of Retroﬁ tting Testability\n\nAll is not lost when none of the techniques described thus far can be used to introduce testability. We still have a few tricks left up our sleeves.\n\nTest Hooks are the “elephant in the room” that no one wants to talk about because they may lead to Test Logic in Production. Test Hooks, however, are a perfectly legitimate way to get legacy code under test when it is too hard or dangerous to introduce one of the techniques described earlier. They are best used as a “transition” strategy to allow Scripted Tests (page 285) or Recorded Tests (page 278) to be automated to provide a Safety Net (see page 24) while large-scale refactoring is undertaken to improve testability. Ideally, once the code has been made more testable, better tests can be prepared using the tech- niques described earlier and the Test Hooks can be removed.\n\nMichael Feathers [WEwLC] has described several other techniques to replace dependencies with test-speciﬁ c code under the general heading of ﬁ nd- ing “object seams.” For example, we can replace a depended-on library with a library designed speciﬁ cally for testing. A seemingly hard-coded dependency can be broken this way. Most of these techniques are less applicable when we need to dynamically replace dependencies within individual tests than either Dependency Injection or Dependency Lookup because they require changes to the environment. Object seams are, however, an excellent way to place legacy code under test so that it can be refactored to introduce either of the previously mentioned dependency-breaking techniques.\n\nWe can use aspect-oriented programming (AOP) to install the Test Double behavior by deﬁ ning a test point-cut that matches the place where the SUT calls the DOC and we would rather have it call the Test Double. Although we need an AOP-enabled development environment to do this, we do not need to deploy the AOP-generated code into a production environ- ment. As a consequence, this technique may be used even in AOP-hostile environments.\n\nOther Uses of Test Doubles\n\nSo far, we have covered the testing of indirect inputs and indirect outputs. Now let’s look at some other uses of Test Doubles.\n\nwww.it-ebooks.info",
      "content_length": 2257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "Other Uses of Test Doubles\n\nEndoscopic Testing\n\nTim Mackinnon et al. introduced the concept of endoscopic testing [ET] in their initial Mock Objects paper. Endoscopic testing focuses on testing the SUT from the inside by passing in a Mock Object as an argument to the method under test. This allows veriﬁ cation of certain internal behaviors of the SUT that may not always be visible from the outside.\n\nThe classic example that Mackinnon and colleagues cite is the use of a mock collection class preloaded with all of the expected members of the collection. When the SUT tries to add an unexpected member, the mock collection’s asser- tion fails. The full stack trace of the internal call stack then becomes visible in the xUnit failure report. If our IDE supports breaking on speciﬁ ed exceptions, we can also inspect the local variables at the point of failure.\n\nNeed-Driven Development\n\nA reﬁ nement of endoscopic testing is “need-driven development” [MRNO], in which the dependencies of the SUT are deﬁ ned as the tests are written. This “outside-in” approach to writing and testing software combines the conceptual elegance of the traditional “top-down” approach to writing code with modern TDD techniques supported by Mock Objects. It allows us to build and test the software layer by layer, starting at the outermost layer before we have imple- mented the lower layers.\n\nNeed-driven development combines the beneﬁ ts of test-driven development (specifying all software with tests before we build them) with a highly incre- mental approach to design that removes the need for any speculation about how a depended-on class might be used.\n\nSpeeding Up Fixture Setup\n\nAnother application of Test Doubles is to reduce the runtime cost of Fresh Fix- ture (page 311) setup. When the SUT needs to interact with other objects that are difﬁ cult to create because they have many dependencies, a single Test Dou- ble can be created instead of the complex network of objects. When applied to networks of entity objects, this technique is called Entity Chain Snipping (see Test Stub).\n\nwww.it-ebooks.info\n\n149",
      "content_length": 2103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "150\n\nChapter 11 Using Test Doubles\n\nSpeeding Up Test Execution\n\nTest Doubles may also be used to speed up tests by replacing slow compo- nents with faster ones. Replacing a relational database with an in-memory Fake Object, for example, can reduce test execution times by an order of magnitude! The extra effort required to code the Fake Database is more than offset by the re- duced waiting time and the quality improvement due to the more timely feedback that comes from running the tests more frequently. Refer to the sidebar “Faster Tests without Shared Fixtures” on page 319 for a more detailed discussion of this issue.\n\nOther Considerations\n\nBecause many of our tests will involve replacing a real DOC with a Test Double, how do we know that the production code will work properly when it uses the real DOC? Of course, we would expect our customer tests to verify behavior with the real DOCs in place (except, possibly, when the real DOCs are interfaces to other systems that need to be stubbed out during single-system testing). We should write a special form of Constructor Test (see Test Method)— a “substitutable initialization test”—to verify that the real DOC is installed properly. The trigger for writing this test is performing the ﬁ rst test that replaces the DOC with a Test Double—that point is often when the Test Double installation mechanism is introduced.\n\nFinally, we want to be careful that we don’t fall into the “new hammer trap.”6 Overuse of Test Doubles (and especially Mock Objects or Test Stubs) can lead to Overspeciﬁ ed Software (see Fragile Test on page 239) by encoding implementation-speciﬁ c information about the design in our tests. The design may be then much more difﬁ cult to change if many tests are affected by the change simply because they use a Test Double that has been affected by the design change.\n\n6 “When you have a new hammer, everything looks like a nail.”\n\nwww.it-ebooks.info",
      "content_length": 1931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "What’s Next?\n\nWhat’s Next?\n\nIn this chapter, we examined techniques for testing software with indirect inputs and indirect outputs. In particular, we explored the concept of Test Doubles and various techniques for installing them. In Chapter 12, Organizing Our Tests, we will turn our attention to strategies for organizing the test code into Test Methods and Test Utility Methods (page 599) implemented on Testcase Classes (page 373) and Test Helpers (page 643).\n\nwww.it-ebooks.info\n\n151",
      "content_length": 488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "Chapter 12\n\nOrganizing Our Tests\n\nAbout This Chapter\n\nIn the chapters concluding with Chapter 11, Using Test Doubles, we looked at various techniques for interacting with the SUT for the purpose of verifying its behavior. In this chapter, we turn our attention to the question of how to orga- nize the test code to make it easy to ﬁ nd and understand.\n\nThe basic unit of test code organization is the Test Method (page 348). Deciding what to put in the Test Method and where to put it is central to the topic of test organization. When we have only a few tests, how we organize them isn’t terribly important. By contrast, when we have hundreds of tests, test organization becomes a critical factor in keeping our tests easy to understand and ﬁ nd.\n\nThis chapter begins by discussing what we should and should not include in a Test Method. Next, it explores how we can decide on which Testcase Classes (page 373) to put our Test Methods. Test naming depends heavily on how we have organized our tests, so we will talk about this issue next. We will then consider how to organize the Testcase Classes into test suites and where to put test code. The ﬁ nal topic is test code reuse—speciﬁ cally, where to put reusable test code.\n\nBasic xUnit Mechanisms\n\nThe xUnit family of Test Automation Frameworks (page 298) provides a num- ber of features to help us organize our tests. The basic question, “Where do I code my tests?”, is answered by putting our test code into a Test Method on a Testcase Class. We then use either Test Discovery (page 393) or Test Enumera- tion (page 399) to create a Test Suite Object (page 387) containing all the tests from the Testcase Class. The Test Runner (page 377) invokes a method on the Test Suite Object to run all the Test Methods.\n\n153\n\nwww.it-ebooks.info",
      "content_length": 1789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "154\n\nChapter 12 Organizing Our Tests\n\nRight-Sizing Test Methods\n\nA test condition is something we need to prove the SUT really does; it can be described in terms of what the starting state of the SUT is, how we exercise the SUT, how we expect the SUT to respond, and what the ending state of the SUT is expected to be. A Test Method is a sequence of statements in our test scripting language that exercises one or more test conditions (Figure 12.1). What should we include in a single Test Method?\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nTest Test\n\nSetup Setup\n\nExercise Exercise\n\nExercise Exercise\n\nFixture Fixture\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nSuite Suite Object Object\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nFigure 12.1 The four phases of a typical test. Each Test Method implements a Four-Phase Test (page 358) that ideally veriﬁ es a single test condition. Not all phases of the Four-Phase Test need be in the Test Method.\n\nMany xUnit purists prefer to Verify One Condition per Test (see page 45) because it gives them good Defect Localization (see page 22). That is, when a test fails, they know exactly what is wrong in the SUT because each test veriﬁ es exactly one test condition. This is very much in contrast with manual testing, where one tends to build long, involved multiple-condition tests because of the overhead involved in setting up each test’s pre-conditions. When creating xUnit- based automated tests, we have many ways of dealing with this frequently re- peated ﬁ xture setup (as described in Chapter 8, Transient Fixture Management), so we tend to Verify One Condition per Test. We call a test that veriﬁ es too many test conditions an Eager Test (see Assertion Roulette on page 224) and consider it a code smell.\n\nA test that veriﬁ es a single test condition executes a single code path through the SUT and it should execute exactly the same path each time it runs; that is what makes it a Repeatable Test (see page 26). Yes, that means we need as\n\nwww.it-ebooks.info",
      "content_length": 2246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Test Methods and Testcase Classes\n\nmany test methods as we have paths through the code—but how else can we expect to achieve full code coverage? What makes this pattern manageable is that we Isolate the SUT (see page 43) when we write unit tests for each class so we only have to focus on paths through a single object. Also, because each test should verify only a single path through the code, each test method should con- sist of strictly sequential statements that describe what should happen on that one path.1 Another reason we Verify One Condition per Test (see page 45) is to Minimize Test Overlap (see page 44) so that we have fewer tests to modify if we later modify the behavior of the SUT.\n\nBrian Marrick has developed an interesting compromise that I call “While We’re at It,”2 which leverages the test ﬁ xture we already have set up to run some additional checks and assertions. Marrick clearly marks these elements with comments to indicate that if changes to the SUT obsolete that part of the test, they can be easily deleted. This strategy minimizes the effort needed to maintain the extra test code.\n\nTest Methods and Testcase Classes\n\nA Test Method needs to live on a Testcase Class. Should we put all our Test Methods onto a single Testcase Class for the application? Or should we create a Testcase Class for each Test Method? Of course, the right answer lies somewhere between these two extremes, and it will change over the life of our project.\n\nTestcase Class per Class\n\nWhen we write our ﬁ rst few Test Methods, we can put them all onto a single Testcase Class. As the number of Test Methods increases, we will likely want to split the Testcase Class so that one Testcase Class per Class (page 617) is tested, which reduces the number of Test Methods per class (Figure 12.2). As those Testcase Classes get too big, we usually split the classes further. In that case, we need to decide which Test Methods to include in each Testcase Class.\n\n1 A Test Method that contains Conditional Test Logic (page 200) is a sign of a test trying to accommodate different circumstances because it does not have control of all indirect inputs of the SUT or because it is trying to verify complex expected states on an in-line basis within the Test Method. 2 He calls it “Just for Laughs” but I don’t ﬁ nd that name very intent-revealing.\n\nwww.it-ebooks.info\n\n155",
      "content_length": 2368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "156\n\nChapter 12 Organizing Our Tests\n\nTestcaseClass TestcaseClass\n\nCreation Creation\n\ntestMethod_A_1 testMethod_A_1\n\nFixture A Fixture A\n\ntestMethod_A_2 testMethod_A_2\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nExercise Exercise\n\nfeature_2 feature_2\n\ntestMethod_B_1 testMethod_B_1\n\ntestMethod_B_2 testMethod_B_2\n\nCreation Creation\n\nFixture B Fixture B\n\nFigure 12.2 A production class with a single Testcase Class. With the Testcase Class per Class pattern, a single Testcase Class holds all the Test Methods for all the behavior of our SUT class. Each Test Method may need to create a different ﬁ xture either in-line or by delegating that task to a Creation Method (page 415).\n\nTestcase Class per Feature\n\nOne school of thought is to put all Test Methods that verify a particular feature of the SUT—where a “feature” is deﬁ ned as one or more methods and attributes that collectively implement some capability of the SUT—into a single Testcase Class (Figure 12.3). This makes it easy to see all test conditions for that feature. (Use of appropriate Test Naming Conventions helps achieve this clarity.) It can, however, result in similar ﬁ xture setup code being required in each Testcase Class.\n\nTestcase Class per Fixture\n\nThe opposing view is that one should group all Test Methods that require the same test ﬁ xture (same pre-conditions) into one Testcase Class per Fixture (page 631; see Figure 12.4). This facilitates putting the test ﬁ xture setup code into the setUp method (Implicit Setup; see page 424) but can result in scattering of the test conditions for each feature across many Testcase Classes.\n\nwww.it-ebooks.info",
      "content_length": 1650,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Test Methods and Testcase Classes\n\nFeature1TestcaseClass Feature1TestcaseClass\n\nCreation Creation\n\ntestMethod_A testMethod_A\n\nFixture A Fixture A\n\ntestMethod_B testMethod_B\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nFeature2TestcaseClass Feature2TestcaseClass\n\nExercise Exercise\n\nfeature_2 feature_2\n\ntestMethod_A testMethod_A\n\ntestMethod_B testMethod_B\n\nCreation Creation\n\nFixture B Fixture B\n\nFigure 12.3 A production class with one Testcase Class for each feature. With the Testcase Class per Feature pattern, we have one Testcase Class for each major capability or feature supported by our SUT class. The Test Methods on that test class exercise various aspects of that feature after building whatever test ﬁ xture they require.\n\nFixtureATestcaseClass FixtureATestcaseClass FixtureATestcaseClass FixtureATestcaseClass\n\nsetUp setUp\n\ntestMethod_1 testMethod_1\n\nCreation Creation\n\nFixture A Fixture A\n\ntestMethod_2 testMethod_2\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nFixtureBTestcaseClass FixtureBTestcaseClass\n\nExercise Exercise\n\nfeature_2 feature_2\n\nsetUp setUp\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nCreation Creation\n\nFixture B Fixture B\n\nFigure 12.4 A production class with one Testcase Class for each ﬁ xture. With the Testcase Class per Fixture pattern, we have one Testcase Class for each possible test ﬁ xture (test pre-condition) of our SUT class. The Test Methods on that test class exercise various features from the common starting point.\n\nwww.it-ebooks.info\n\n157",
      "content_length": 1535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "158\n\nChapter 12 Organizing Our Tests\n\nChoosing a Test Method Organization Strategy\n\nClearly, there is no single “best practice” we can always follow; the best prac- tice is the one that is most appropriate for the particular circumstance. Testcase Class per Fixture is commonly used when we are writing unit tests for stateful objects and each method needs to be tested in each state of the object. Testcase Class per Feature (page 624) is more appropriate when we are writing customer tests against a Service Facade [CJ2EEP]; it enables us to keep all the tests for a customer-recognizable feature together. This pattern is also more commonly used when we rely on a Prebuilt Fixture (page 429) because ﬁ xture setup logic is not required in each test. When each test needs a slightly different ﬁ xture, the right answer may be to select the Testcase Class per Feature pattern and use a Delegated Setup (page 411) to facilitate setting up the ﬁ xtures.\n\nTest Naming Conventions\n\nThe names we give to our Testcase Classes and Test Methods are crucial in mak- ing our tests easy to ﬁ nd and understand. We can make the test coverage more obvious by naming each Test Method systematically based on which test condi- tion it veriﬁ es. Regardless of which test method organization scheme we use, we would like the combination of the names of the test package, the Testcase Class, and the Test Method to convey at least the following information:\n\nThe name of the SUT class\n\nThe name of the method or feature being exercised\n\nThe important characteristics of any input values related to the exercising\n\nof the SUT\n\nAnything relevant about the state of the SUT or its dependencies\n\nThese items are the “input” part of the test condition. Obviously, this is a lot to communicate in just two names but the reward is high if we can achieve it: We can tell exactly what test conditions we have tests for merely by looking at the names of the classes and methods in an outline view of our IDE. Figure 12.5 provides an example.\n\nwww.it-ebooks.info",
      "content_length": 2034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Test Naming Conventions\n\nFigure 12.5 A production class with one Testcase Class for each test ﬁ xture. When we use the Testcase Class per Fixture pattern, the class name can describe the ﬁ xture, leaving the method name available for describing the inputs and expected outputs.\n\nFigure 12.5 also shows how useful it is to include the “expectations” side of the test condition:\n\nThe outputs (responses) expected when exercising the SUT\n\nThe expected post-exercise state of the SUT and its dependencies\n\nThis information can be included in the name of the Test Method preﬁ xed by “should.” If this nomenclature makes the names too long,3 we can always access the expected outcome by looking at the body of the Test Method.\n\n3 Many xUnit variants “encourage” us to start all our Test Method names with “test” so that these methods can be automatically detected and added to the Test Suite Object. This constrains our naming somewhat compared to variants that indicate test methods via method attributes or annotations.\n\nwww.it-ebooks.info\n\n159",
      "content_length": 1040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "160\n\nChapter 12 Organizing Our Tests\n\nOrganizing Test Suites\n\nThe Testcase Class acts as a Test Suite Factory (see Test Enumeration) when it returns a Test Suite Object containing a collection of Testcase Objects (page 382), each representing a Test Method (Figure 12.6). This is the default organization mechanism provided by xUnit. Most Test Runners allow any class to act as a Test Suite Factory by implementing a Factory Method [GOF], which is typi- cally called suite.\n\nTestcase Class Testcase Class\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nFixture Fixture\n\ntestMethod_1 testMethod_1\n\nImplicit tearDown Implicit tearDown\n\nTest Test Suite Suite Factory Factory\n\nCreation Creation\n\nTest Test Suite Suite Object Object\n\nSUT SUT\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\ntestMethod_n testMethod_n\n\nImplicit tearDown Implicit tearDown\n\nFigure 12.6 A Testcase Class acting as a Test Suite Factory. By default, the Testcase Class acts as a Test Suite Factory to produce the Test Suite Object that the Test Runner requires to execute our tests. We can also enumerate a speciﬁ c set of tests we want to run by providing a Test Suite Factory that returns a Test Suite Object containing only the desired tests.\n\nRunning Groups of Tests\n\nWe often want to run groups of tests (i.e., a test suite) but we don’t want this decision to constrain how we organize them. A popular convention is to create a special Test Suite Factory called AllTests for each package of tests. We don’t need to stop there, however: We can create Named Test Suites (page 592) for any collection of tests we want to run together. A good example is a Subset Suite (see Named Test Suite) that allows us to run just those tests that need software\n\nwww.it-ebooks.info",
      "content_length": 1774,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Organizing Test Suites\n\ndeployed to the Web server (or not deployed to the Web server!). We usually have at least a Subset Suite for all the unit tests and another Subset Suite for just the customer tests (they often take a long time to execute). Some variants of xUnit support Test Selection (page 403), which we can use instead of deﬁ ning Subset Suites.\n\nSuch runtime groupings of tests often reﬂ ect the environment in which they need to run. For example, we might have one Subset Suite that includes all tests that can be run without the database and another Subset Suite that includes all tests that depend on the database. Likewise, we might have separate Subset Suites for tests that do, and do not, rely on the Web server. If our test package includes these various kinds of test suites, we can deﬁ ne AllTests as a Suite of Suites (see Test Suite Object) composed of these Subset Suites. Then any test that is added to one of the Subset Suites will also be run in AllTests without incurring extra test maintenance effort.\n\nRunning a Single Test\n\nSuppose a Test Method fails in our Testcase Class. We decide to put a break- point on a particular method—but that method is called in every test. Our ﬁ rst reaction might be to just muddle through by clicking “Go” each time the breakpoint is hit until we are being called from the test of interest. One possibility is to disable (by commenting out) the other Test Methods so they are not run. Another option is to rename the other Test Methods so that the xUnit Test Discovery mechanism will not recognize them as tests. In variants of xUnit that use method attributes or annotations, we can add the “Ignore” attribute to a test method instead. Each of these approaches introduces the potential problem of a Lost Test (see Production Bugs on page 268), although the “Ignore” approach does remind us that some tests are being ignored. In members of the xUnit family that provide a Test Tree Explorer (see Test Run- ner), we can simply select a single test to be run from the hierarchy view of the test suite, as shown in Figure 12.7.\n\nWhen none of these options is available, we can use a Test Suite Factory to run a single test. Wait a minute! Aren’t test suites all about running groups of tests that live in different Testcase Classes? Well, yes, but that doesn’t mean we can’t use them for other purposes. We can deﬁ ne a Single Test Suite4 (see Named Test Suite) that runs a particular test. To do so, we call the constructor of the Testcase Class with the speciﬁ c Test Method’s name as an argument.\n\n4 I usually call it MyTest.\n\nwww.it-ebooks.info\n\n161",
      "content_length": 2615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "162\n\nChapter 12 Organizing Our Tests\n\nFigure 12.7 A Test Tree Explorer showing the structure of the tests in our suite. We can use the Test Tree Explorer to drill down into the runtime structure of the test suite and run individual tests or subsuites.\n\nTest Code Reuse\n\nTest Code Duplication (page 213) can signiﬁ cantly increase the cost of writing and maintaining tests. Luckily, a number of techniques for reusing test logic are available to us. The most important consideration is that any reuse not compromise the value of the Tests as Documentation (see page 23). I don’t recommend reuse of the actual Test Method in different circumstances (e.g., with different ﬁ xtures), as this kind of reuse is typically a sign of a Flexible Test (see Conditional Test Logic on page 200) that tests different things in dif- ferent circumstances. Most test code reuse is achieved either through Implicit Setup or Test Utility Methods (page 599). The major exception is the reuse of Test Doubles (page 522) by many tests; we can treat these Test Double classes as a special kind of Test Helper (page 643) when thinking about where to put them.\n\nwww.it-ebooks.info",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Test Code Reuse\n\nTest Utility Method Locations\n\nTestcase Testcase Superclass Superclass\n\nTest Utility Test Utility Method Method\n\nFixture Fixture\n\nSUT SUT\n\ntestMethod_1 testMethod_1\n\nTest Helper Test Helper\n\ntestMethod_n testMethod_n\n\nTest Utility Test Utility Method Method\n\nTest Utility Test Utility Method Method\n\nTestcase Testcase Class Class\n\nFigure 12.8 The various places we can put Test Utility Methods. The primary decision-making criterion is the desired scope of reusability of the Test Methods.\n\nMany variants of xUnit provide a special Testcase Superclass (page 638)—typically called “TestCase”—from which all Testcase Classes should (and, in some cases, must) inherit either directly or indirectly (Figure 12.8). If we have useful utility methods on our Testcase Class that we want to reuse in other Testcase Classes, we may ﬁ nd it helpful to create one or more Testcase Superclasses from which to inherit instead of “TestCase.” If we take this step, we need to be careful if those methods need to see types or classes that reside in various packages within the SUT—our root Testcase Superclass should not depend on those types or classes directly, as that is likely to result in a cyclical dependency graph. We may be able to create a Testcase Superclass for each test package to keep our test class de- pendencies noncyclic. The alternative is to create a Test Helper for each domain package and put the various Test Helpers in the appropriate test packages. This way, a Testcase Class is not forced to choose a single Testcase Superclass; it can merely “use” the appropriate Test Helpers.\n\nTestCase Inheritance and Reuse\n\nThe most commonly used reason for inheriting methods from a Testcase Super- class is to access Test Utility Methods. Another use is when testing frameworks\n\nwww.it-ebooks.info\n\n163",
      "content_length": 1820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "164\n\nChapter 12 Organizing Our Tests\n\nand their plug-ins; it can be useful to create a conformance test that speciﬁ es the general behavior of the plug-in via a Template Method [GOF] that calls meth- ods provided by a subclass speciﬁ c to the kind of plug-in being tested to check speciﬁ c details of the plug-in. This scenario is rare enough that I won’t describe it further here; please refer to [FaT] for a more complete description.\n\nTest File Organization\n\nNow we face a new question: Where should we put our Testcase Classes? Obviously, these classes should be stored in the source code repository [SCM] along with the production code. Beyond that criterion, we have quite a range of choices. The test packaging strategy we choose will very much depend on our environment—many IDEs include constraints that make certain strate- gies unworkable. The key issue is to Keep Test Logic Out of Production Code (see page 45) and yet to be able to ﬁ nd the corresponding test for each piece of code or functionality.\n\nBuilt-in Self-Test\n\nWith a built-in self-test, the tests are included with the production code and can be run at any time. No provision is made for keeping them separate. Many orga- nizations want to Keep Test Logic Out of Production Code so built-in self-tests may not be a good option for them. This consideration is particularly important in memory-constrained environments where we don’t want test code taking up valuable space.\n\nSome development environments encourage us to keep the tests and the pro- duction code together. For example, SAP’s ABAP Unit supports the keyword “For Testing,” which tells the system to disable the tests when the code is trans- ported into the production environment.\n\nTest Packages\n\nIf we decide to put the Testcase Classes into separate test packages, we can organize them in several ways. We can keep the tests separate by putting them into one or more test packages while keeping them in the same source tree, or we can put the tests into the same logical package but physically store them in a parallel source tree. The latter approach is frequently used in Java because it avoids the problem of tests not being able to see “package-protected” methods\n\nwww.it-ebooks.info",
      "content_length": 2228,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "What’s Next?\n\non the SUT.5 Some IDEs may reject using this approach by insisting that a pack- age be wholly contained within a single folder or project. When we use test packages under each production code package, we may need to use a build-time test stripper to exclude them from production builds.\n\nTest Dependencies\n\nHowever we decide to store and manage the source code, we need to ensure that we eliminate any Test Dependency in Production (see Test Logic in Production on page 217) because even a test stripper cannot remove the tests if production code needs them to be present to run. This requirement makes paying attention to our class dependencies important. We also don’t want to have any Test Logic in Production because it means we aren’t testing the same code that we will eventu- ally run in production. This issue is discussed in more detail in Chapter 6, Test Automation Strategy.\n\nWhat’s Next?\n\nNow that we’ve looked at how to organize our test code, we should become familiar with a few more testing patterns. These patterns are introduced in Chapter 13, Testing with Databases.\n\n5 Java offers another way to get around the visibility issue: We can deﬁ ne our own test Security Manager to allow tests to access all methods on the SUT, not just the “package- protected” ones. This approach solves the problem in a general way but requires a good understanding of Java class loaders. Other languages may not have the equivalent functionality (or problem!).\n\nwww.it-ebooks.info\n\n165",
      "content_length": 1500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Chapter 13\n\nTesting with Databases\n\nAbout This Chapter\n\nIn Chapter 12, Organizing Our Tests, we looked at techniques for organizing our test code. In this chapter, we explore the issues that arise when our appli- cation includes a database. Applications with databases present some special challenges when writing automated tests. Databases are much slower than the processors used in modern computers. As a result, tests that interact with databases tend to run much, much more slowly than tests that can run entirely in memory.\n\nEven ignoring the potential for Slow Tests (page 253), databases are a ripe source for many test smells in our automated test suites. Some of these smells are a direct consequence of the persistent nature of the database, while others result from our choice to share the ﬁ xture instance between tests. These smells were introduced in Chapter 9, Persistent Fixture Management. This chapter expands on them and provides a more focused treatment of testing with databases.\n\nTesting with Databases\n\nHere is my ﬁ rst, and most critical, piece of advice on this subject:\n\nWhen there is any way to test without a database, test without the database!\n\nThis seems like pretty strong advice but it is phrased this way for a reason. Data- bases introduce all sorts of complications into our applications and especially into our tests. Tests that require a database run, on average, two orders of magnitude slower than the same tests that run without a database.\n\n167\n\nwww.it-ebooks.info",
      "content_length": 1507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "168\n\nChapter 13 Testing with Databases\n\nWhy Test with Databases?\n\nMany applications include a database to persist objects or data into longer-term storage. The database is a necessary part of the application, so verifying that the database is used properly is a necessary part of building the application. Therefore, the use of a Database Sandbox (page 650) to isolate developers and testers from production (and each other) is a fundamental practice on almost every project (Figure 13.1).\n\nDeveloper 1 Developer 1\n\nDeveloper 2 Developer 2\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nSUT SUT\n\nSUT SUT\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nFixture Fixture\n\nFixture Fixture\n\nDatabase Database\n\nDatabase Database\n\nFigure 13.1 A Database Sandbox for each developer. Sharing a Database Sandbox among developers is false economy. Would you make a plumber and an electrician work in the same wall at the same time?\n\nIssues with Databases\n\nA database introduces a number of issues that complicate test automation. Many of these issues relate to the fact that the ﬁ xture is persistent. These issues were introduced in Chapter 9, Persistent Fixture Management, and are summarized brieﬂ y here.\n\nPersistent Fixtures\n\nApplications with databases present some special challenges when we are writing automated tests. Databases are much slower than the processors used in modern computers. As a consequence, tests that interact with a database tend to run much more slowly than tests that can run entirely in memory. But even ignoring the Slow Tests issue, databases are a prime source of test smells in our automated test suites. Commonly encountered smells include Erratic\n\nwww.it-ebooks.info",
      "content_length": 1726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Testing without Databases\n\nTests (page 228) and Obscure Tests (page 186). Because the data in a database may potentially persist long after we run our test, we must pay special atten- tion to this data to avoid creating tests that can be run only once or tests that interact with one another. These Unrepeatable Tests (see Erratic Test) and Interacting Tests (see Erratic Test) are a direct consequence of the persistence of the test ﬁ xture and can result in more expensive maintenance of our tests as the application evolves.\n\nShared Fixtures\n\nPersistence of the ﬁ xture is one thing; choosing to share it is another. Deliberate sharing of the ﬁ xture can result in Lonely Tests (see Erratic Test) if some tests depend on other tests to set up the ﬁ xture for them—a situation called Chained Tests (page 454). If we haven’t provided each developer with his or her own Database Sandbox, we might spark a Test Run War (see Erratic Test) between developers. This problem arises when the tests being run from two or more Test Runners (page 377) interact by virtue of their accessing the same ﬁ xture objects in the shared database instance. Each of these behavior smells is a direct conse- quence of the decision to share the test ﬁ xture. The degree of persistence and the scope of ﬁ xture sharing directly affect the presence or absence of these smells.\n\nGeneral Fixtures\n\nAnother problem with tests that rely on databases is that databases tend to evolve into a large General Fixture (see Obscure Test) that many tests use for different pur- poses. This outcome is particularly likely when we use a Prebuilt Fixture (page 429) to avoid setting up the ﬁ xture in each test. It can also result from the decision to use a Standard Fixture (page 305) when we employ a Fresh Fixture (page 311) strategy. This approach makes it difﬁ cult to determine exactly what each test is specifying. In effect, the database appears as a Mystery Guest (see Obscure Test) in all of the tests.\n\nTesting without Databases\n\nModern layered software architecture [DDD, PEAA, WWW] opens up the pos- sibility of testing the business logic without using the database at all. We can test the business logic layer in isolation from the other layers of the system by using Layer Tests (page 337) and replacing the data access layer with a Test Double (page 522); see Figure 13.2.\n\nwww.it-ebooks.info\n\n169",
      "content_length": 2375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "170\n\nChapter 13 Testing with Databases\n\nLayer1TestcaseClass Layer1TestcaseClass testMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nLayernTestcaseClass LayernTestcaseClass testMethod_1 testMethod_1 testMethod_2 testMethod_2\n\nLayer n Layer n\n\nLayer 1 Layer 1\n\nTest Double Test Double\n\nTest Double Test Double\n\nDOC DOC\n\nFigure 13.2 A pair of Layer Tests, each of which tests a different layer of the system. Layer Tests allow us to build each layer independently of the other layers. They are especially useful when the persistence layer can be replaced by a Test Double that reduces the Context Sensitivity (see Fragile Test on page 239) of the tests.\n\nIf our architecture is not sufﬁ ciently layered to allow for Layer Tests, we may still be able to test without a real database by using either a Fake Database (see Fake Object on page 551) or an In-Memory Database (see Fake Object). An In-Memory Database is a database but stores its tables in memory; this structure makes it run much faster than a disk-based database. A Fake Database isn’t really a database at all; it is a data access layer that merely pretends to be one. As a rule, it is easier to ensure independence of tests by using a Fake Database because we typically cre- ate a new one as part of our ﬁ xture setup logic, thereby implementing a Transient Fresh Fixture (see Fresh Fixture) strategy. Nevertheless, both of these strategies allow our tests to run at in-memory speeds, thereby avoiding Slow Tests. We don’t introduce too much knowledge of the SUT’s structure as long as we continue to write our tests as round-trip tests.\n\nReplacing the database with a Test Double works well as long as we use the database only as a data repository. Things get more interesting if we use any vendor-speciﬁ c functionality, such as sequence number generation or stored pro- cedures. Replacing the database then becomes a bit more challenging because it requires more attention to creating a design for testability. The general strategy is to encapsulate all database interaction within the data access layer. Where the\n\nwww.it-ebooks.info",
      "content_length": 2099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "Testing the Database\n\ndata access layer provides data access functionality, we can simply delegate these duties to the “database object.” We must provide test-speciﬁ c implementations for any parts of the data access layer interface that implement the vendor-speciﬁ c functionality—a task for which a Test Stub (page 529) ﬁ ts the bill nicely.\n\nIf we are taking advantage of vendor-speciﬁ c database features such as sequence number generation, we will need to provide this functionality when executing the tests in memory. Typically, we will not need to substitute a Test Double for any functionality-related object because the functionality happens behind the scenes within the database. We can add this functionality into the in-memory version of the application using a Strategy [GOF] object, which by default is initialized to a null object [PLOPD3]. When run in production, the null object does nothing; when run in memory, the strategy object provides the missing functionality. As an added beneﬁ t, we will ﬁ nd it easier to change to a different database vendor once we have taken this step because the hooks to provide this functionality al- ready exist.1\n\nReplacing the database (or the data access layer) via an automated test implies that we have a way to instruct the SUT to use the replacement object. This is com- monly done in one of two ways: through direct Dependency Injection (page 678) or by ensuring that the business logic layer uses Dependency Lookup (page 686) to ﬁ nd the data access layer.\n\nTesting the Database\n\nAssuming we have found ways to test most of our software without using a database, then what? Does the need to test the database disappear? Of course not! We should ensure that the database functions correctly, just like any other soft- ware we write. We can, however, focus our testing of the database logic so as to reduce the number and kinds of tests we need to write. Because tests that involve the database will run much more slowly than our in-memory tests, we want to keep the number of these tests to the bare minimum.\n\nWhat kinds of database tests will we require? The answer to this question depends on how our application uses the database. If we have stored proce- dures, we should write unit tests to verify their logic. If a data access layer hides the database from the business logic, we should write tests for the data access functionality.\n\n1 Just one more example of how design for testability improves the design of our applications.\n\nwww.it-ebooks.info\n\n171",
      "content_length": 2520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "172\n\nChapter 13 Testing with Databases\n\nTesting Stored Procedures\n\nWe can write tests for stored procedures in one of two ways. A Remote Stored Procedure Test (see Stored Procedure Test on page 654) is written in the same programming language and framework as we write all of our other unit tests. It accesses the stored procedure via the same invocation mechanism as used within the application logic (i.e., by some sort of Remote Proxy [GOF], Facade [GOF], or Command object [GOF]). Alternatively, we can write In-Database Stored Procedure Tests (see Stored Procedure Test) in the same language as the stored procedure itself; these tests will run inside the database (Figure 13.3). xUnit fam- ily members are available for several of the most common stored procedure languages; utPLSQL is just one example.\n\nApplication Environment Application Environment\n\nTestcase Class Testcase Class\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nStored Stored Procedure Procedure Proxy Proxy\n\nTestcase Class Testcase Class\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nDatabase Database\n\nStored Stored Procedure Procedure\n\nFigure 13.3 Testing a stored procedure using Self-Checking Tests (see page 26). There is great value in having automated regression test for stored procedures, but we must take care to make them repeatable and robust.\n\nTesting the Data Access Layer\n\nWe also want to write some unit tests for the data access layer. For the most part, these data access layer tests can be round-trip tests. Nevertheless, it is useful to have a few layer-crossing tests to ensure that we are putting information into the correct columns. This can be done using xUnit framework extensions for\n\nwww.it-ebooks.info",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Testing with Databases (Again!)\n\ndatabase testing (e.g., DbUnit for Java) to insert data directly into the database (for “Read” tests) or to verify the post-test contents of the database (for “Cre- ate/Update/Delete” tests).\n\nA useful trick for keeping our ﬁ xture from becoming persistent during data access layer testing is to use Transaction Rollback Teardown (page 668). To do so, we rely on the Humble Transaction Controller (see Humble Object on page 695) DFT pattern when constructing our data access layer. That is, the code that reads or writes the database should never commit a transaction; this allows the code to be exercised by a test that rolls back the transaction to pre- vent any of the changes made by the SUT from being applied.\n\nAnother way to tear down any changes made to the database during the ﬁ xture setup and exercise SUT phases of the test is Table Truncation Tear- down (page 661). This “brute force” technique for deleting data works only when each developer has his or her own Database Sandbox and we want to clear out all the data in one or more tables.\n\nEnsuring Developer Independence\n\nTesting the database means we need to have the real database available for running these tests. During this testing process, every developer needs to have his or her ownDatabase Sandbox. Trying to share a single sandbox among several or all developers is a false economy; the developers will simply end up tripping over one another and wasting a lot of time.2 I have heard many different excuses for not giving each developer his or her own sandbox, but frankly none of them holds water. The most legitimate concern relates to the cost of a database license for each developer—but even this obstacle can be surmounted by choosing one of the “virtual sandbox” variations. If the database technology supports it, we can use a DB Schema per TestRunner (see Database Sandbox); otherwise, we have to use a Database Partitioning Scheme (see Database Sandbox).\n\nTesting with Databases (Again!)\n\nSuppose we have done a good job layering our system and achieved our goal of running most of our tests without accessing the real database. Now what kinds of tests should we run against the real database? The answer is simple: “As few as possible, but no fewer!” In practice, we want to run at least a representative sample of our customer tests against the database to ensure that the SUT behaves\n\n2 Can you image asking a team of carpenters to share a single hammer?\n\nwww.it-ebooks.info\n\n173",
      "content_length": 2502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "174\n\nChapter 13 Testing with Databases\n\nthe same way with a database as without one. These tests need not access the busi- ness logic via the user interface unless some particular user interface functionality depends on the database; Subcutaneous Tests (see Layer Test) should be adequate in most circumstances.\n\nWhat’s Next?\n\nIn this chapter, we looked at special techniques for testing with databases. This discussion has merely scratched the surface of the interactions between agile software development and databases.3 Chapter 14, A Roadmap to Effective Test Automation, summarizes the material we have covered thus far and makes some suggestions about how a project team should come up to speed on developer test automation.\n\n3 For a more complete treatment of the topic, refer to [RDb].\n\nwww.it-ebooks.info",
      "content_length": 813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Chapter 14\n\nA Roadmap to Effective Test Automation\n\nAbout This Chapter\n\nChapter 13, Testing with Databases, introduced a set of patterns speciﬁ c to testing applications that have a database. These patterns built on the techniques described in Chapter 6, Test Automation Strategy; Chapter 9, Persistent Fixture Manage- ment; and Chapter 11, Using Test Doubles. This was a lot of material to become familiar with before we could test effectively with and without databases!\n\nThis raises an important point: We don’t become experts in test automa- tion overnight—these skills take time to develop. It also takes time to learn the various tools and patterns at our disposal. This chapter provides something of a roadmap for how to learn the patterns and acquire the skills. It introduces the concept of “test automation maturity,” which is loosely based on the SEI’s Capability Maturity Model (CMM).\n\nTest Automation Difﬁ culty\n\nSome kinds of tests are harder to write than others. This difﬁ culty arises partly because the techniques are more involved and partly because they are less well known and the tools to do this kind of test automation are less readily avail- able. The following common kinds of tests are listed in approximate order of difﬁ culty, from easiest to most difﬁ cult:\n\n1. Simple entity objects (Domain Model [PEAA])\n\nSimple business classes with no dependencies\n\nComplex business classes with dependencies\n\n175\n\nwww.it-ebooks.info",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "176\n\nChapter 14 A Roadmap to Effective Test Automation\n\n2. Stateless service objects\n\nIndividual components via component tests\n\nThe entire business logic layer via Layer Tests (page 337)\n\n3. Stateful service objects\n\nCustomer tests via a Service Facade [CJ2EEP] using Subcutaneous\n\nTests (see Layer Test)\n\nStateful components via component tests\n\n4. “Hard-to-test” code\n\nUser interface logic exposed via Humble Dialog (see Humble\n\nObject on page 695)\n\nDatabase logic\n\nMulti-threaded software\n\n5. Object-oriented legacy software (software built without any tests)\n\n6. Non-object-oriented legacy software\n\nAs we move down this list, the software becomes increasingly more challenging to test. The irony is that many teams “get their feet wet” by trying to retroﬁ t tests onto an existing application. This puts them in one of the last two categories in this list, which is precisely where the most experience is required. Unfortunately, many teams fail to test the legacy software successfully, which may then prejudice them against trying automated testing, with or without test-driven development. If you ﬁ nd your- self trying to learn test automation by retroﬁ tting tests onto legacy software, I have two pieces of advice for you: First, hire someone who has done it before to help you through this process. Second, read Michael Feathers’ excellent book [WEwLC]; he covers many techniques speciﬁ cally applicable to retroﬁ tting tests.\n\nRoadmap to Highly Maintainable Automated Tests\n\nGiven that some kinds of tests are much harder to write than others, it makes sense to focus on learning to write the easier tests ﬁ rst before we move on to the more difﬁ cult kinds of tests. When teaching automated testing to developers, I introduce the techniques in the following sequence. This roadmap is based on Maslow’s hierarchy of needs [HoN], which says that we strive to meet the higher- level needs only after we have satisﬁ ed the lower-level needs.\n\nwww.it-ebooks.info",
      "content_length": 1972,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Roadmap to Highly Maintainable Automated Tests\n\n1. Exercise the happy path code\n\nSet up a simple pre-test state of the SUT\n\nExercise the SUT by calling the method being tested\n\n2. Verify direct outputs of the happy path\n\nCall Assertion Methods (page 362) on the SUT’s responses\n\nCall Assertion Methods on the post-test state\n\n3. Verify alternative paths\n\nVary the SUT method arguments\n\nVary the pre-test state of the SUT\n\nControl indirect inputs of the SUT via a Test Stub (page 529)\n\n4. Verify indirect output behavior\n\nUse Mock Objects (page 544) or Test Spies (page 538) to intercept\n\nand verify outgoing method calls\n\n5. Optimize test execution and maintainability\n\nMake the tests run faster\n\nMake the tests easy to understand and maintain\n\nDesign the SUT for testability\n\nReduce the risk of missed bugs\n\nThis ordering of needs isn’t meant to imply that this is the order in which we might think about implementing any speciﬁ c test.1 Rather, it is likely to be the order in which a project team might reasonably expect to learn about the tech- niques of test automation.\n\nLet’s look at each of these points in more detail.\n\nExercise the Happy Path Code\n\nTo run the happy path through the SUT, we must automate one Simple Success Test (see Test Method on page 348) as a simple round-trip test through the SUT’s API. To get this test to pass, we might simply hard-code some of the logic in the\n\n1 Although it can also be used that way, I ﬁ nd it better to write the assertions ﬁ rst and then work back from there.\n\nwww.it-ebooks.info\n\n177",
      "content_length": 1541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "178\n\nChapter 14 A Roadmap to Effective Test Automation\n\nSUT, especially where it might call other components to retrieve information it needs to make decisions that would drive the test down the happy path. Before exercising the SUT, we need to set up the test ﬁ xture by initializing the SUT to the pre-test state. As long as the SUT executes without raising any errors, we consider the test as having passed; at this level of maturity we don’t check the actual results against the expected results.\n\nVerify Direct Outputs of the Happy Path\n\nOnce the happy path is executing successfully, we can add result veriﬁ cation logic to turn our test into a Self-Checking Test (see page 26). This involves adding calls to Assertion Methods to compare the expected results with what actually oc- curred. We can easily make this change for any objects or values returned to the test by the SUT (e.g., “return values,” “out parameters”). We can also call other methods on the SUT or use public ﬁ elds to access the post-test state of the SUT; we can then call Assertion Methods on these values as well.\n\nVerify Alternative Paths\n\nAt this point the happy path through the code is reasonably well tested. The alternative paths through the code are still Untested Code (see Production Bugs on page 268) so the next step is to write tests for these paths (whether we have already written the production code or we are striving to automate the tests that would drive us to implement them). The question to ask here is “What causes the alternative paths to be exercised?” The most common causes are as follows:\n\nDifferent values passed in by the client as arguments\n\nDifferent prior state of the SUT itself\n\nDifferent results of invoking methods on components on which the\n\nSUT depends\n\nThe ﬁ rst case can be tested by varying the logic in our tests that calls the SUT methods we are exercising and passing in different values as arguments. The second case involves initializing the SUT with a different starting state. Neither of these cases requires any “rocket science.” The third case, however, is where things get interesting.\n\nControlling Indirect Inputs\n\nBecause the responses from other components are supposed to cause the SUT to exercise the alternative paths through the code, we need to get control\n\nwww.it-ebooks.info",
      "content_length": 2314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "Roadmap to Highly Maintainable Automated Tests\n\nover these indirect inputs. We can do so by using a Test Stub that returns the value that should drive the SUT into the desired code path. As part of ﬁ xture setup, we must force the SUT to use the stub instead of the real component. The Test Stub can be built two ways: as a Hard-Coded Test Stub (see Test Stub), which contains hand-written code that returns the speciﬁ c values, or as a Conﬁ gurable Test Stub (see Test Stub), which is conﬁ gured by the test to return the desired values. In both cases, the SUT must use the Test Stub instead of the real component.\n\nMany of these alternative paths result in “successful” outputs from the SUT; these tests are considered Simple Success Tests and use a style of Test Stub called a Responder (see Test Stub). Other paths are expected to raise errors or excep- tions; they are considered Expected Exception Tests (see Test Method) and use a style of stub called a Saboteur (see Test Stub).\n\nMaking Tests Repeatable and Robust\n\nThe act of replacing a real depended-on component (DOC) with a Test Stub has a very desirable side effect: It makes our tests both more robust and more repeat- able.2 By using a Test Stub, we replace a possibly nondeterministic component with one that is completely deterministic and under test control. This is a good example of the Isolate the SUT principle (see page 43).\n\nVerify Indirect Output Behavior\n\nThus far we have focused on getting control of the indirect inputs of the SUT and verifying readily visible direct outputs by inspecting the post-state test of the SUT. This kind of result veriﬁ cation is known as State Veriﬁ cation (page 462). Sometimes, however, we cannot conﬁ rm that the SUT has behaved correctly simply by looking at the post-test state. That is, we may still have some Untested Requirements (see Production Bugs) that can only be veriﬁ ed by doing Behavior Veriﬁ cation (page 468).\n\nWe can build on what we already know how to do by using one of the close relatives of the Test Stub to intercept the outgoing method calls from our SUT. A Test Spy “remembers” how it was called so that the test can later retrieve the usage information and use Assertion Method calls to compare it to the expected usage. A Mock Object can be loaded with expectations during ﬁ xture setup, which it subsequently compares with the actual calls as they occur while the SUT is being exercised.\n\n2 See Robust Test (see page 29) and Repeatable Test (see page 26) for a more detailed description.\n\nwww.it-ebooks.info\n\n179",
      "content_length": 2552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "180\n\nChapter 14 A Roadmap to Effective Test Automation\n\nOptimize Test Execution and Maintenance\n\nAt this point we should have automated tests for all the paths through our code. We may, however, have less than optimal tests:\n\nWe may have Slow Tests (page 253).\n\nThe tests may contain Test Code Duplication (page 213) that makes\n\nthem hard to understand.\n\nWe may have Obscure Tests (page 186) that are hard to understand\n\nand maintain.\n\nWe may have Buggy Tests (page 260) that are caused by unreliable Test Utility Methods (page 599) or Conditional Test Logic (page 200).\n\nMake the Tests Run Faster\n\nSlow Tests is often the ﬁ rst behavior smell we need to address. To make tests run faster, we can reuse the test ﬁ xture across many tests—for example, by using some form of Shared Fixture (page 317). Unfortunately, this tactic typically produces its own share of problems. Replacing a DOC with a Fake Object (page 551) that is functionally equivalent but executes much faster is almost always a better solution. Use of a Fake Object builds on the techniques we learned for verifying indirect inputs and outputs.\n\nMake the Tests Easy to Understand and Maintain\n\nWe can make Obscure Tests easier to understand and remove a lot of Test Code Duplication by refactoring our Test Methods to call Test Utility Methods that contain any frequently used logic instead of doing everything on an in-line basis. Creation Methods (page 415), Custom Assertions (page 474), Finder Methods (see Test Utility Method), and Parameterized Tests (page 607) are all examples of this approach.\n\nIf our Testcase Classes (page 373) are getting too big to understand, we can reorganize these classes around ﬁ xtures or features. We can also better commu- nicate our intent by using a systematic way of naming Testcase Classes and Test Methods that exposes the test conditions we are verifying in them.\n\nReduce the Risk of Missed Bugs\n\nIf we are having problems with Buggy Tests or Production Bugs, we can reduce the risk of false negatives (tests that pass when they shouldn’t) by encapsulating complex test logic. When doing so, we should use intent-revealing names for our\n\nwww.it-ebooks.info",
      "content_length": 2167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "What’s Next?\n\nTest Utility Methods. We should verify the behavior of nontrivial Test Utility Methods using Test Utility Tests (see Test Utility Method).\n\nWhat’s Next?\n\nThis chapter concludes Part I, The Narratives. Chapters 1–14 have provided an overview of the goals, principles, philosophies, patterns, smells, and coding idioms related to writing effective automated tests. Part II, The Test Smells, and Part III, The Patterns, contain detailed descriptions of each of the smells and patterns introduced in these narrative chapters, complete with code samples.\n\nwww.it-ebooks.info\n\n181",
      "content_length": 588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "PART II\n\nThe Test Smells\n\nwww.it-ebooks.info\n\nThe Test Smells",
      "content_length": 61,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "Chapter 15\n\nCode Smells\n\nSmells in This Chapter\n\nObscure Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\n\nConditional Test Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200\n\nHard-to-Test Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n\nTest Code Duplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n\nTest Logic in Production . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n\n185\n\nwww.it-ebooks.info\n\nCode Smells",
      "content_length": 547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "186\n\nObscure Test\n\nAlso known as: Long Test, Complex Test, Verbose Test\n\nChapter 15 Code Smells\n\nObscure Test\n\nIt is difﬁ cult to understand the test at a glance.\n\nAutomated tests should serve at least two purposes. First, they should act as documentation of how the system under test (SUT) should behave; we call this Tests as Documentation (see page 23). Second, they should be a self-verifying executable speciﬁ cation. These two goals are often contradictory because the level of detail needed for tests to be executable may make the test so verbose as to be difﬁ cult to understand.\n\nSymptoms\n\nWe are having trouble understanding what behavior a test is verifying.\n\nImpact\n\nThe ﬁ rst issue with an Obscure Test is that it makes the test harder to understand and therefore maintain. It will almost certainly preclude achieving Tests as Doc- umentation, which in turn can lead to High Test Maintenance Cost (page 265). The second issue with an Obscure Test is that it may allow bugs to slip through because of test coding errors hidden in the Obscure Test. This can re- sult in Buggy Tests (page 260). Furthermore, a failure of one assertion in an Eager Test may hide many more errors that simply aren’t run, leading to a loss of test debugging data.\n\nCauses\n\nParadoxically, an Obscure Test can be caused by either too much information in the Test Method (page 348) or too little information. Mystery Guest is an example of too little information; Eager Test and Irrelevant Information are examples of too much information.\n\nThe root cause of an Obscure Test is typically a lack of attention to keeping the test code clean and simple. Test code is just as important as the production code, and it needs to be refactored just as often. A major contributor to an Obscure Test is a “just do it in-line” mentality when writing tests. Putting code in-line results in large, complex Test Methods because some things just take a lot of code to do.\n\nThe ﬁ rst few causes of Obscure Test discussed here relate to having the\n\nwrong information in the test:\n\nwww.it-ebooks.info",
      "content_length": 2069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "Obscure Test\n\nEager Test: The test veriﬁ es too much functionality in a single Test Obscure Test\n\nEager Test: The test veriﬁ es too much functionality in a single Test Obscure Test\n\nMystery Guest: The test reader is not able to see the cause and effect between ﬁ xture and veriﬁ cation logic because part of it is done outside the Test Method.\n\nThe general problem of Verbose Tests—tests that use too much code to say what they need to say—can be further broken down into a number of root causes:\n\nGeneral Fixture: The test builds or references a larger ﬁ xture than is\n\nneeded to verify the functionality in question.\n\nIrrelevant Information: The test exposes a lot of irrelevant details about the ﬁ xture that distract the test reader from what really affects the be- havior of the SUT.\n\nHard-Coded Test Data: Data values in the ﬁ xture, assertions, or argu- ments of the SUT are hard-coded in the Test Method, obscuring cause– effect relationships between inputs and expected outputs.\n\nIndirect Testing: The Test Method interacts with the SUT indirectly via\n\nanother object, thereby making the interactions more complex.\n\nCause: Eager Test\n\nThe test veriﬁ es too much functionality in a single Test Method.\n\nSymptoms\n\nThe test goes on and on verifying this, that, and “everything but the kitchen sink.” It is hard to tell which part is ﬁ xture setup and which part is exercising the SUT.\n\npublic void testFlightMileage_asKm2() throws Exception { // set up ﬁxture // exercise constructor Flight newFlight = new Flight(validFlightNumber); // verify constructed object assertEquals(validFlightNumber, newFlight.number); assertEquals(\"\", newFlight.airlineCode); assertNull(newFlight.airline); // set up mileage newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); // verify results int expectedKilometres = 1810;\n\nwww.it-ebooks.info\n\n187",
      "content_length": 1893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "188\n\nObscure Test\n\nChapter 15 Code Smells\n\nassertEquals( expectedKilometres, actualKilometres); // now try it with a canceled ﬂight newFlight.cancel(); try { newFlight.getMileageAsKm(); fail(\"Expected exception\"); } catch (InvalidRequestException e) { assertEquals( \"Cannot get cancelled ﬂight mileage\", e.getMessage()); } }\n\nRoot Cause\n\nWhen executing tests manually, it makes sense to chain a number of logically distinct test conditions into a single test case to reduce the setup overhead of each test. This works because we have liveware (an intelligent human being) executing the tests, and this person can decide at any point whether it makes sense to keep going or whether the failure of a step is severe enough to abandon the execution of the test.\n\nPossible Solution\n\nWhen the tests are automated, it is better to have a suite of independent Single- Condition Tests (see page 45) as these provide much better Defect Localization (see page 22).\n\nCause: Mystery Guest\n\nThe test reader is not able to see the cause and effect between ﬁ xture and veriﬁ - cation logic because part of it is done outside the Test Method.\n\nSymptoms\n\nTests invariably require passing data to the SUT. The data used in the ﬁ xture setup and exercise SUT phases of the Four-Phase Test (page 358) deﬁ ne the pre- conditions of the SUT and inﬂ uence how it should behave. The post-conditions (the expected outcomes) are reﬂ ected in the data passed as arguments to the Assertion Methods (page 362) in the verify outcome phase of the test.\n\nWhen either the ﬁ xture setup or the result veriﬁ cation part of a test depends on information that is not visible within the test and the test reader ﬁ nds it dif- ﬁ cult to understand the behavior that is being veriﬁ ed without ﬁ rst ﬁ nding and inspecting the external information, we have a Mystery Guest on our hands. Here’s an example where we cannot tell what the ﬁ xture looks like, making it difﬁ cult to relate the expected outcome to the pre-conditions of the test:\n\nwww.it-ebooks.info",
      "content_length": 2018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Obscure Test\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight_mg() throws Exception { loadAirportsAndFlightsFromFile(\"test-ﬂights.csv\"); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirportCode( \"YYC\"); // Verify Outcome assertEquals( 1, ﬂightsAtOrigin.size()); FlightDto ﬁrstFlight = (FlightDto) ﬂightsAtOrigin.get(0); assertEquals( \"Calgary\", ﬁrstFlight.getOriginCity()); }\n\nImpact\n\nThe Mystery Guest makes it hard to see the cause–effect relationship between the test ﬁ xture (the pre-conditions of the test) and the expected outcome of the test. As a consequence, the tests don’t fulﬁ ll the role of Tests as Docu- mentation. Even worse, someone may modify or delete the external resource without realizing the impact this action will have when the tests are run. This behavior smell has its own name: Resource Optimism (see Erratic Test on page 228)!\n\nIf the Mystery Guest is a Shared Fixture (page 317), it may also lead to Erratic\n\nTests if other tests modify it.\n\nRoot Cause\n\nA test depends on mysterious external resources, making it difﬁ cult to under- stand the behavior that it is verifying. Mystery Guests may take many forms:\n\nA ﬁ lename of an existing external ﬁ le is passed to a method of the SUT; the contents of the ﬁ le should determine the behavior of the SUT.\n\nThe contents of a database record identiﬁ ed by a literal key are read\n\ninto an object that is then used by the test or passed to the SUT.\n\nThe contents of a ﬁ le are read and used in calls to Assertion Methods to\n\nverify the expected outcome.\n\nA Setup Decorator (page 447) is used to create a Shared Fixture, and objects in this ﬁ xture are then referenced via variables within the result veriﬁ cation logic.\n\nA General Fixture is set up using Implicit Setup (page 424), and the Test Methods then access them via instance variables or class variables.\n\nAll of these scenarios share a common outcome: It is hard to see the cause–effect relationship between the test ﬁ xture and the expected outcome of the test because\n\nwww.it-ebooks.info\n\n189\n\nObscure Test",
      "content_length": 2071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "190\n\nObscure Test\n\nChapter 15 Code Smells\n\nthe relevant data are not visible in the tests. If the contents of the data are not clearly described by the names we give to the variables and ﬁ les that contain them, we have a Mystery Guest.\n\nPossible Solution\n\nUsing a Fresh Fixture (page 311) built using In-line Setup (page 408) is the obvious solution for a Mystery Guest. When applied to the ﬁ le example, this would involve creating the contents of the ﬁ le as a string within our test so that the contents are visible and then writing them out to the ﬁ le system [Setup External Resource (page 772) refactoring] or putting it into a ﬁ le sys- tem Test Stub (page 529) as part of the ﬁ xture setup.1 To avoid Irrelevant Information, we may want to hide the details of the construction behind one or more evocatively named Creation Methods (page 415) that append to the ﬁ le’s contents.\n\nIf we must use a Shared Fixture or Implicit Setup, we should consider using evocatively named Finder Methods (see Test Utility Method on page 599) to access the objects in the ﬁ xture. If we must use external resources such as ﬁ les, we should put them into a special folder or directory and give them names that make it obvious what kind of data they hold.\n\nCause: General Fixture\n\nThe test builds or references a larger ﬁ xture than is needed to verify the func- tionality in question.\n\nSymptoms\n\nThere seems to be a lot of test ﬁ xture being built—much more than would appear to be necessary for any particular test. It is hard to understand the cause–effect relationship between the ﬁ xture, the part of the SUT being exercised, and the expected outcome of a test.\n\nConsider the following set of tests:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { setupStandardAirportsAndFlights(); FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome\n\n1 See In-line Resource (page 736) refactoring for details.\n\nwww.it-ebooks.info",
      "content_length": 2071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "Obscure Test\n\nassertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { setupStandardAirportsAndFlights(); FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nFrom reading the exercise SUT and veriﬁ ng outcome parts of the tests, it would appear that they need very different ﬁ xtures. Even though these tests are using a Fresh Fixture setup strategy, they are using the same ﬁ xture setup logic by calling the setupStandardAirportsAndFlights method. The name of the method is a clue to this classic but easily recognized example of a General Fixture. A more difﬁ cult case to diagnose would be if each test created the Standard Fixture (page 305) in-line or if each test created a somewhat different ﬁ xture but each ﬁ xture con- tained much more than was needed by each individual test.\n\nWe may also be experiencing Slow Tests (page 253) or a Fragile Fixture (see\n\nFragile Test on page 239).\n\nRoot Cause\n\nThe most common cause of this problem is a test that uses a ﬁ xture that is designed to support many tests. Examples include the use of Implicit Setup or a Shared Fix- ture across many tests with different ﬁ xture requirements. This problem results in the ﬁ xture becoming large and difﬁ cult to understand. The ﬁ xture may also grow larger over time. The root cause is that both approaches rely on a Standard Fix- ture that must meet the requirements of all tests that use it. The more diverse the needs of those tests, the more likely we are to create a General Fixture.\n\nImpact\n\nWhen the test ﬁ xture is designed to support many different tests, it can be very difﬁ cult to understand how each test uses the ﬁ xture. This complexity reduces the likelihood of using Tests as Documentation and can result in a Fragile Fixture as people alter the ﬁ xture so that it can handle new tests. It can also result in Slow\n\nwww.it-ebooks.info\n\n191\n\nObscure Test",
      "content_length": 2209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "192\n\nObscure Test\n\nChapter 15 Code Smells\n\nTests because a larger ﬁ xture takes more time to build, especially if a ﬁ le system or database is involved.\n\nPossible Solution\n\nWe need to move to a Minimal Fixture (page 302) to address this problem. To do so, we can use a Fresh Fixture for each test. If we must use a Shared Fixture, we should consider applying the Make Resource Unique (page 737) refactoring to create a virtual Database Sandbox (page 650) for each test.2\n\nCause: Irrelevant Information\n\nThe test exposes a lot of irrelevant details about the ﬁ xture that distract the test reader from what really affects the behavior of the SUT.\n\nSymptoms\n\nAs test readers, we ﬁ nd it hard to determine which of the values passed to objects actually affect the expected outcome:\n\npublic void testAddItemQuantity_severalQuantity_v10(){ // Set Up Fixture Address billingAddress = createAddress( \"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); Address shippingAddress = createAddress( \"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); Customer customer = createCustomer( 99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); Product product = createProduct( 88,\"SomeWidget\",new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, 5); // Verify Outcome LineItem expected = new LineItem(invoice, product,5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\n2 Switching to an Immutable Shared Fixture (see Shared Fixture) does not fully address the core of this problem because it does not help us determine which parts of the ﬁ xture are needed by each test; only the parts that are modiﬁ ed are so identiﬁ ed!\n\nwww.it-ebooks.info",
      "content_length": 1788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "Obscure Test\n\nFixture setup logic may seem very long and complicated as it weaves together many interrelated objects. This makes it hard to determine what the test is veri- fying because the reader doesn’t understand the pre-conditions of the test:\n\npublic void testGetFlightsByOriginAirport_TwoOutboundFlights() throws Exception { FlightDto expectedCalgaryToSanFran = new FlightDto(); expectedCalgaryToSanFran.setOriginAirportId(calgaryAirportId); expectedCalgaryToSanFran.setOriginCity(CALGARY_CITY); expectedCalgaryToSanFran.setDestinationAirportId(sanFranAirportId); expectedCalgaryToSanFran.setDestinationCity(SAN_FRAN_CITY); expectedCalgaryToSanFran.setFlightNumber( facade.createFlight(calgaryAirportId,sanFranAirportId)); FlightDto expectedCalgaryToVan = new FlightDto(); expectedCalgaryToVan.setOriginAirportId(calgaryAirportId); expectedCalgaryToVan.setOriginCity(CALGARY_CITY); expectedCalgaryToVan. setDestinationAirportId(vancouverAirportId); expectedCalgaryToVan.setDestinationCity(VANCOUVER_CITY); expectedCalgaryToVan.setFlightNumber(facade.createFlight( calgaryAirportId, vancouverAirportId));\n\nThe code that veriﬁ es the expected outcome of a test can also be too complicated to understand:\n\nList lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 2); // verify ﬁrst item LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem1.getInv(), actual.getInv()); assertEquals(expItem1.getProd(), actual.getProd()); assertEquals(expItem1.getQuantity(), actual.getQuantity()); // verify second item actual = (LineItem)lineItems.get(1); assertEquals(expItem2.getInv(), actual.getInv()); assertEquals(expItem2.getProd(), actual.getProd()); assertEquals(expItem2.getQuantity(), actual.getQuantity()); }\n\nRoot Cause\n\nA test contains a lot of data, either as Literal Values (page 714) or as variables. Irrelevant Information often occurs in conjunction with Hard-Coded Test Data or a General Fixture but can also arise because we make visible all data the test needs to execute rather than focusing on the data the test needs to be un- derstood. When writing tests, the path of least resistance is to use whatever methods are available (on the SUT and other objects) and to ﬁ ll in all parameters with values, whether or not they are relevant to the test.\n\nwww.it-ebooks.info\n\n193\n\nObscure Test",
      "content_length": 2341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "194\n\nObscure Test\n\nChapter 15 Code Smells\n\nAnother possible cause is when we include all the code needed to verify the outcome using Procedural State Veriﬁ cation (see State Veriﬁ cation on page 462) rather than using a much more compact “declarative” style to spec- ify the expected outcome.\n\nImpact\n\nIt is hard to achieve Tests as Documentation if the tests contain many seemingly random bits of Obscure Test that don’t clearly link the pre-conditions with the post-conditions. Likewise, wading through many steps of ﬁ xture setup or result veriﬁ cation logic can result in High Test Maintenance Cost and can increase the likelihood of Production Bugs (page 268) or Buggy Tests.\n\nPossible Solution\n\nThe best way to get rid of Irrelevant Information in ﬁ xture setup logic is to replace direct calls to the constructor or Factory Methods [GOF] with calls to Parameterized Creation Methods (see Creation Method) that take only the rel- evant information as parameters. Fixture values that do not matter to the test (i.e., those that do not affect the expected outcome) should be defaulted within Creation Methods or replaced by Dummy Objects (page 728). In this way we say to the test reader, “The values you don’t see don’t affect the expected out- come.” We can replace ﬁ xture values that appear in both the ﬁ xture setup and outcome veriﬁ cation parts of the test with suitably initialized named constants as long as we are using a Fresh Fixture approach to ﬁ xture setup.\n\nTo hide Irrelevant Information in result veriﬁ cation logic, we can use asser- tions on entire Expected Objects (see State Veriﬁ cation), rather than asserting on individual ﬁ elds, and we can create Custom Assertions (page 474) that hide complex procedural veriﬁ cation logic.\n\nCause: Hard-Coded Test Data\n\nData values in the ﬁ xture, assertions, or arguments of the SUT are hard-coded in the Test Method, obscuring cause–effect relationships between inputs and expected outputs.\n\nSymptoms\n\nAs test readers, we ﬁ nd it difﬁ cult to determine how various hard-coded (i.e., literal) values in the test are related to one another and which values should affect the behavior of the SUT. We may also encounter behavior smells such as Erratic Tests.\n\nwww.it-ebooks.info",
      "content_length": 2242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "Obscure Test\n\npublic void testAddItemQuantity_severalQuantity_v12(){ // Set Up Fixture Customer cust = createACustomer(new BigDecimal(\"30\")); Product prod = createAProduct(new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(cust); // Exercise SUT invoice.addItemQuantity(prod, 5); // Verify Outcome LineItem expected = new LineItem(invoice, prod, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); }\n\nThis speciﬁ c example isn’t so bad because there aren’t very many literal values. If we aren’t good at doing math in our heads, however, we might miss the relation- ship between the unit price ($19.99), the item quantity (5), the discount (30%), and the total price ($69.96).\n\nRoot Cause\n\nHard-Coded Test Data occurs when a test contains a lot of seemingly unrelated Literal Values. Tests invariably require passing data to the SUT. The data used in the ﬁ xture setup and exercise SUT phases of the Four-Phase Test deﬁ ne the pre- conditions of the SUT and inﬂ uence how it should behave. The post-conditions (the expected outcomes) are reﬂ ected in the data passed as arguments to the Assertion Methods in the verify outcome phase of the test. When writing tests, the path of least resistance is to use whatever methods are available (on the SUT and other objects) and to ﬁ ll in all parameters with values, whether or not they are relevant to the test.\n\nWhen we use “cut-and-paste” reuse of test logic, we ﬁ nd ourselves replicat-\n\ning the literal values to the derivative tests.\n\nImpact\n\nIt is hard to achieve Tests as Documentation if the tests contain many seemingly random bits of Obscure Test that don’t clearly link the pre-conditions with the post-conditions. A few literal parameters might not seem like a bad thing—after all, they don’t require us to make that much more effort to understand a test. As the number of literal values grows, however, it can become much more difﬁ cult to understand a test. This is especially true when the signal-to-noise ratio drops dramatically because the majority of the values are irrelevant to the test.\n\nThe second major impact occurs when collisions between tests occur because the tests are using the same values. This situation happens only when we use a Shared Fixture because a Fresh Fixture strategy shouldn’t litter the scene with any objects with which a subsequent test can collide.\n\nwww.it-ebooks.info\n\n195\n\nObscure Test",
      "content_length": 2439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "196\n\nObscure Test\n\nChapter 15 Code Smells\n\nPossible Solution\n\nThe best way to get rid of the Obscure Test smell is to replace the literal constants with something else. Fixture values that determine which scenario is being ex- ecuted (e.g., type codes) are probably the only ones that are reasonable to leave as literals—but even these values can be converted to named constants.\n\nFixture values that do not matter to the test (i.e., those that do not affect the expected outcome) should be defaulted within Creation Methods. In this way we say to the test reader, “The values you don’t see don’t affect the expected outcome.” We can replace ﬁ xture values that appear in both the ﬁ xture setup and outcome veriﬁ cation parts of the test with suitably initialized named con- stants as long as we are using a Fresh Fixture approach to ﬁ xture setup.\n\nValues in the result veriﬁ cation logic that are based on values used in the ﬁ x- ture or that are used as arguments of the SUT should be replaced with Derived Values (page 718) to make those calculations obvious to the test reader.\n\nIf we are using any variant of Shared Fixture, we should try to use Distinct Generated Values (see Generated Value on page 723) to ensure that each time a test is run, it uses a different value. This consideration is especially important for ﬁ elds that serve as unique keys in databases. A common way of encapsulating this logic is to use Anonymous Creation Methods (see Creation Method).\n\nCause: Indirect Testing\n\nThe Test Method interacts with the SUT indirectly via another object, thereby making the interactions more complex.\n\nSymptoms\n\nA test interacts primarily with objects other than the one whose behavior it purports to verify. The test must construct and interact with objects that contain references to the SUT rather than with the SUT itself. Testing business logic through the presentation layer is a common example of Indirect Testing.\n\nprivate ﬁnal int LEGAL_CONN_MINS_SAME = 30; public void testAnalyze_sameAirline_LessThanConnectionLimit() throws Exception { // setup FlightConnection illegalConn = createSameAirlineConn( LEGAL_CONN_MINS_SAME - 1); // exercise FlightConnectionAnalyzerImpl sut = new FlightConnectionAnalyzerImpl();\n\nwww.it-ebooks.info",
      "content_length": 2255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Obscure Test\n\nString actualHtml = sut.getFlightConnectionAsHtmlFragment( illegalConn.getInboundFlightNumber(), illegalConn.getOutboundFlightNumber()); // veriﬁcation StringBuffer expected = new StringBuffer(); expected.append(\"<span class=\"boldRedText\">\"); expected.append(\"Connection time between ﬂight \"); expected.append(illegalConn.getInboundFlightNumber()); expected.append(\" and ﬂight \"); expected.append(illegalConn.getOutboundFlightNumber()); expected.append(\" is \"); expected.append(illegalConn.getActualConnectionTime()); expected.append(\" minutes.</span>\"); assertEquals(\"html\", expected.toString(), actualHtml); }\n\nImpact\n\nIt may not be possible to test “anything that could possibly break” in the SUT via the intermediate object. Indeed, such tests are unlikely to be very clear or understandable. They certainly will not result in Tests as Documentation.\n\nIndirect Testing may result in Fragile Tests because changes in the intermediate objects may require modiﬁ cation of the tests even when the SUT is not modiﬁ ed.\n\nRoot Cause\n\nThe SUT may be “private” to the class being used to access it from the test. It may not be possible to create the SUT directly because the constructors them- selves are private. This problem is just one sign that the software is not designed for testability.\n\nIt may be that the actual outcome of exercising the SUT cannot be observed directly. In such a case, the expected outcome of the test must be veriﬁ ed through an intermediate object.\n\nPossible Solution\n\nIt may be necessary to improve the design-for-testability of the SUT to remove this smell. We might be able to expose the SUT directly to the test by using an Extract Testable Component refactoring (a variant of the Sprout Class [WEwLC] refac- toring). This approach may result in an untestable Humble Object (page 695) and an easily tested object that contains most or all of the actual logic.\n\npublic void testAnalyze_sameAirline_EqualsConnectionLimit() throws Exception { // setup Mock ﬂightMgntStub = mock(FlightManagementFacade.class);\n\nwww.it-ebooks.info\n\n197\n\nObscure Test",
      "content_length": 2087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "198\n\nObscure Test\n\nChapter 15 Code Smells\n\nFlight ﬁrstFlight = createFlight(); Flight secondFlight = createConnectingFlight( ﬁrstFlight, LEGAL_CONN_MINS_SAME); ﬂightMgntStub.expects(once()).method(\"getFlight\") .with(eq(ﬁrstFlight.getFlightNumber())) .will(returnValue(ﬁrstFlight)); ﬂightMgntStub.expects(once()).method(\"getFlight\") .with(eq(secondFlight.getFlightNumber())) .will(returnValue(secondFlight)); // exercise FlightConnAnalyzer theConnectionAnalyzer = new FlightConnAnalyzer(); theConnectionAnalyzer.facade = (FlightManagementFacade)ﬂightMgntStub.proxy(); FlightConnection actualConnection = theConnectionAnalyzer.getConn( ﬁrstFlight.getFlightNumber(), secondFlight.getFlightNumber()); // veriﬁcation assertNotNull(\"actual connection\", actualConnection); assertTrue(\"IsLegal\", actualConnection.isLegal()); }\n\nSometimes we may be forced to interact with the SUT indirectly because we cannot refactor the code to expose the logic we are trying to test. In these cases, we should encapsulate the complex logic forced by Indirect Testing behind suit- ably named Test Utility Methods. Similarly, ﬁ xture setup can be hidden behind Creation Methods and result veriﬁ cation can be hidden by Veriﬁ cation Methods (see Custom Assertion). Both are examples of SUT API Encapsulation (see Test Utility Method).\n\npublic void testAnalyze_sameAirline_LessThanConnLimit() throws Exception { // setup FlightConnection illegalConn = createSameAirlineConn( LEGAL_CONN_MINS_SAME - 1); FlightConnectionAnalyzerImpl sut = new FlightConnectionAnalyzerImpl(); // exercise SUT String actualHtml = sut.getFlightConnectionAsHtmlFragment( illegalConn.getInboundFlightNumber(), illegalConn.getOutboundFlightNumber()); // veriﬁcation assertConnectionIsIllegal(illegalConn, actualHtml); }\n\nThe following Custom Assertion hides the ugliness of extracting the business result from the presentation noise. It was created by doing a simple Extract Method [Fowler] refactoring on the test. Of course, this example would be more robust\n\nwww.it-ebooks.info",
      "content_length": 2029,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Obscure Test\n\nif it searched inside the HTML for key strings rather than building up the entire expected string and comparing it all at once. Other Presentation Layer Tests (see Layer Test on page 337) might then verify that the presentation logic is format- ting the HTML string properly.\n\nprivate void assertConnectionIsIllegal( FlightConnection conn, String actualHtml) { // set up expected value StringBuffer expected = new StringBuffer(); expected.append(\"<span class=\"boldRedText\">\"); expected.append(\"Connection time between ﬂight \"); expected.append(conn.getInboundFlightNumber()); expected.append(\" and ﬂight \"); expected.append(conn.getOutboundFlightNumber()); expected.append(\" is \"); expected.append(conn.getActualConnectionTime()); expected.append(\" minutes.</span>\"); // veriﬁcation assertEquals(\"html\", expected.toString(), actualHtml); }\n\nSolution Patterns\n\nA good test strategy helps keep the test code understandable. Nevertheless, just as “no battle plan survives the ﬁ rst contact with the enemy,” no test infrastruc- ture can anticipate all needs of all tests. We should expect the test infrastructure to evolve as the software matures and our test automation skills improve.\n\nWe can reuse test logic for several scenarios by having several tests call Test Utility Methods or by asking a common Parameterized Test (page 607) to pass in the already built test ﬁ xture or Expected Objects.\n\nWriting tests in an “outside-in” way can minimize the likelihood of produc- ing an Obscure Test that might then need to be refactored. This approach starts by outlining the Four-Phase Test using calls to nonexistent Test Utility Meth- ods. Once we are satisﬁ ed with these tests, we can write the utility methods needed to run them. By writing the tests ﬁ rst, we gain a better understanding of what the utility methods need to do for us to make writing the tests as simple as possible. The “test-infected” will, of course, write Test Utility Tests (see Test Utility Method) before writing the Test Utility Methods.\n\nwww.it-ebooks.info\n\n199\n\nObscure Test",
      "content_length": 2064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "200\n\nConditional Test Logic\n\nAlso known as: Indented Test Code\n\nChapter 15 Code Smells\n\nConditional Test Logic\n\nA test contains code that may or may not be executed.\n\nA Fully Automated Test (see page 26) is just code that veriﬁ es the behavior of other code. But if this code is complicated, how do we verify that it works prop- erly? We could write tests for our tests—but when would this recursion stop? The simple answer is that Test Methods (page 348) must be simple enough to not need tests.\n\nConditional Test Logic is one factor that makes tests more complicated than\n\nthey really should be.\n\nSymptoms\n\nAs a code smell, Conditional Test Logic may not produce any behavioral symp- toms but its presence should be reasonably obvious to the test reader. View any control structures within a Test Method with extreme suspicion! The test reader may also wonder which code path is the one that is being executed. The following is an example of Conditional Test Logic that involves both looping and if statements:\n\n// verify Vancouver is in the list actual = null; i = ﬂightsFromCalgary.iterator(); while (i.hasNext()) { FlightDto ﬂightDto = (FlightDto) i.next(); if (ﬂightDto.getFlightNumber().equals( expectedCalgaryToVan.getFlightNumber())) { actual = ﬂightDto; assertEquals(\"Flight from Calgary to Vancouver\", expectedCalgaryToVan, ﬂightDto); break; } } }\n\nThis code begs the question, “What is this test code doing and how do we know that it is doing it correctly?” One behavioral symptom may be the pres- ence of the related project-level smell High Test Maintenance Cost (page 265), which may be caused by the complexity introduced by the Conditional Test Logic.\n\nwww.it-ebooks.info",
      "content_length": 1688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Conditional Test Logic\n\nImpact\n\nConditional Test Logic makes it difﬁ cult to know exactly what a test is going to do when it really matters. Code that has only a single execution path al- ways executes in exactly the same way. Code that has multiple execution paths presents much greater challenges and does not inspire as much conﬁ dence about its outcome.\n\nTo increase our conﬁ dence in production code, we can write Self-Checking Tests (see page 26) that exercise the code. How can we increase our conﬁ dence in the test code if it executes differently each time we run it? It is hard to know (or prove) that the test is verifying the behavior we want it to verify. A test that has branches or loops, or that uses different values each time it is run, can be very difﬁ cult to debug simply because it isn’t completely deterministic.\n\nA related issue is that Conditional Test Logic makes writing the test correctly a more difﬁ cult task. Because the test itself cannot be tested easily, how do we know that it will actually detect the bugs it is intended to catch? [This is a gen- eral problem with Obscure Tests (page 186); they are more likely to result in Buggy Tests (page 260) than simple code.]\n\nCauses\n\nTest automaters may introduce Conditional Test Logic for several reasons:\n\nThey may use if statements to steer execution to a fail statement or to avoid executing certain pieces of test code when the SUT fails to return valid data.\n\nThey may use loops to verify the contents of collections of objects (Conditional Veriﬁ cation Logic). This may also result in an Obscure Test.\n\nThey may use Conditional Test Logic to verify complex objects or polymorphic data structures (another form of Conditional Veriﬁ cation Logic). This is just a Foreign Method [Fowler] implementation of the equals method.\n\nThey may use Conditional Test Logic to initialize the test ﬁ xture or Expected Object (see State Veriﬁ cation on page 462) so they can reuse a single test to verify several different cases (Flexible Test).\n\nThey may use if statements to avoid tearing down nonexistent ﬁ xture objects (Complex Teardown).\n\nwww.it-ebooks.info\n\n201\n\nConditional Test Logic",
      "content_length": 2161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "202\n\nConditional Test Logic\n\nChapter 15 Code Smells\n\nSome of these causes are worth examining in more detail.\n\nCause: Flexible Test\n\nThe test code veriﬁ es different functionality depending on when or where it is run.\n\nSymptoms\n\nThe test contains conditional logic that does different things depending on the current environment. Most commonly this functionality takes the form of Con- ditional Test Logic to build different versions of the expected results based on some factor external to the test.\n\nConsider the following test, which gets the current time so that it can deter-\n\nmine what the output of the SUT should be:\n\npublic void testDisplayCurrentTime_whenever() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify outcome Calendar time = new DefaultTimeProvider().getTime(); StringBuffer expectedTime = new StringBuffer(); expectedTime.append(\"<span class=\\\"tinyBoldText\\\">\");\n\nif ((time.get(Calendar.HOUR_OF_DAY) == 0) && (time.get(Calendar.MINUTE) <= 1)) { expectedTime.append( \"Midnight\"); } else if ((time.get(Calendar.HOUR_OF_DAY) == 12) && (time.get(Calendar.MINUTE) == 0)) { // noon expectedTime.append(\"Noon\"); } else { SimpleDateFormat fr = new SimpleDateFormat(\"h:mm a\"); expectedTime.append(fr.format(time.getTime())); } expectedTime.append(\"</span>\");\n\nassertEquals( expectedTime, result); }\n\nRoot Cause\n\nA Flexible Test is caused by a lack of control of the environment. The test automater probably wasn’t able to decouple the SUT from its dependencies and decided to adapt the test logic based on the state of the environment.\n\nwww.it-ebooks.info",
      "content_length": 1651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Conditional Test Logic\n\nImpact\n\nThe ﬁ rst issue is that using a Flexible Test makes the test harder to understand and therefore to maintain. The second issue is that we don’t know which test scenarios are actually being exercised and whether all scenarios are, in fact, exercised regularly. For example, in our sample test, is the midnight scenario ever exercised? How often? Probably rarely, if ever, because the test would have to be run at exactly midnight—an unlikely event, even if we timed the nightly build such that it ran over midnight.\n\nPossible Solution\n\nA Flexible Test is best addressed by decoupling the SUT from whatever depen- dencies prompted the test automater to make the test ﬂ exible. This involves refactoring the SUT to support substitutable dependency. We can then replace the dependency with a Test Double (page 522), such as a Test Stub (page 529) or Mock Object (page 544), and write separate tests for each circumstance previ- ously covered by the Flexible Test.\n\nCause: Conditional Veriﬁ cation Logic\n\nConditional Test Logic (page 200) may also create problems when it is used to verify the expected outcome. This issue usually arises when the tester tries to pre- vent the execution of assertions if the SUT fails to return the right objects or uses loops to verify the contents of collections returned by the SUT.\n\n// verify Vancouver is in the list actual = null; i = ﬂightsFromCalgary.iterator(); while (i.hasNext()) { FlightDto ﬂightDto = (FlightDto) i.next(); if (ﬂightDto.getFlightNumber().equals( expectedCalgaryToVan.getFlightNumber())) { actual = ﬂightDto; assertEquals(\"Flight from Calgary to Vancouver\", expectedCalgaryToVan, ﬂightDto); break; } } }\n\nPossible Solution\n\nWe can replace the if statements that steer execution to a call to fail with a Guard Assertion (page 490) that causes the test to fail before we reach the code we don’t\n\nwww.it-ebooks.info\n\n203\n\nConditional Test Logic",
      "content_length": 1928,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "204\n\nConditional Test Logic\n\nChapter 15 Code Smells\n\nwant to execute. This works well unless the test is an Expected Exception Test (see Test Method.) In the latter case, we should use the standard Expected Exception Test coding idiom for the xUnit family member and language.\n\nWe can replace Conditional Test Logic for veriﬁ cation of complex objects with an Equality Assertion (see Assertion Method on page 362) on an Expected Object. If the production code’s equals method is too strict, we can use a Custom Assertion (page 474) to deﬁ ne test-speciﬁ c equality.\n\nWe should move any loops in the veriﬁ cation logic to a Custom Assertion. We can then verify this assertion’s behavior by using Custom Assertion Tests (see Custom Assertion).\n\nWe can reuse test logic in several tests by calling a Test Utility Method (page 599) or a common Parameterized Test (page 607) that passes in the already built test ﬁ xture and Expected Objects.\n\nCause: Production Logic in Test\n\nSymptoms\n\nSome forms of Conditional Test Logic are found in the result veriﬁ cation section of our tests. Let us look more closely inside the loops of this test:\n\npublic void testCombinationsOfInputValues() { // Set up ﬁxture Calculator sut = new Calculator(); int expected; // TBD inside loops\n\nfor (int i = 0; i < 10; i++) { for (int j = 0; j < 10; j++) { // Exercise SUT int actual = sut.calculate( i, j );\n\n// Verify result if (i==3 & j==4) // special case expected = 8; else expected = i+j;\n\nassertEquals(message(i,j), expected, actual); } } }\n\nprivate String message(int i, int j) { return \"Cell( \" + String.valueOf(i)+ \",\" + String.valueOf(j) + \")\"; }\n\nwww.it-ebooks.info",
      "content_length": 1650,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "Conditional Test Logic\n\nThe nested loops in this Loop-Driven Test (see Parameterized Test) exercise the SUT with various combinations of values of i and j as inputs. Here we will focus on the Conditional Test Logic inside the loop.\n\nRoot Cause\n\nThis Production Logic in Test is a direct result of wanting to verify multiple test conditions in a single Test Method. Given that multiple input values are passed to the SUT, we should also have multiple expected results. It is hard to enumerate the expected result for each set of inputs if we pass in many com- binations of several input arguments to the SUT in nested loops. A common solution to this problem is to use a Calculated Value (see Derived Value on page 718) based on the inputs. The potential downfall (as we see here) is that we ﬁ nd ourselves replicating the expected SUT logic inside our test to calculate the expected values for assertions.\n\nPossible Solution\n\nIf at all possible, it is better to enumerate the sets of precalculated values with which to test the SUT. The following example tests the same logic using a (smaller) set of enumerated values:\n\npublic void testMultipleValueSets() { // Set Up Fixture Calculator sut = new Calculator(); TestValues[] testValues = { new TestValues(1,2,3), new TestValues(2,3,5), new TestValues(3,4,8), // special case! new TestValues(4,5,9) };\n\nfor (int i = 0; i < testValues.length; i++) { TestValues values = testValues[i]; // Exercise SUT int actual = sut.calculate( values.a, values.b); // Verify Result assertEquals(message(i), values.expectedSum, actual); } }\n\nprivate String message(int i) { return \"Row \"+ String.valueOf(i); }\n\nwww.it-ebooks.info\n\n205\n\nConditional Test Logic",
      "content_length": 1690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "206\n\nConditional Test Logic\n\nChapter 15 Code Smells\n\nCause: Complex Teardown\n\nSymptoms\n\nComplex ﬁ xture teardown code is more likely to leave the test environment cor- rupted if it does not clean up after itself correctly. It is hard to verify that tear- down code has been written correctly, and such code can easily result in “data leaks” that may later cause this or other tests to fail for no apparent reason. Consider this example:\n\npublic void testGetFlightsByOrigin_NoInboundFlight_SMRTD() throws Exception { // Set Up Fixture BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expFlightDto = createTestFlight(outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { try { facade.removeFlight(expFlightDto.getFlightNumber()); } ﬁnally { try { facade.removeAirport(inboundAirport); } ﬁnally { facade.removeAirport(outboundAirport); } } } }\n\nRoot Cause\n\nTeardown is typically required only when we use persistent resources that are beyond the reach of our garbage collection system. Complex Teardown occurs when many such resources are used in the same Test Method.\n\nPossible Solution\n\nTo avoid complex teardown logic, we should use Implicit Teardown (page 516), which will make the code both reusable and testable, or Automated Tear- down (page 503), which can be veriﬁ ed with automated unit tests. We can\n\nwww.it-ebooks.info",
      "content_length": 1598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "Conditional Test Logic\n\nalso eliminate the need to tear down any ﬁ xture objects by using a Fresh Fixture (page 311) strategy and by avoiding the use of any persistent objects in our tests by using some sort of Test Double.\n\nCause: Multiple Test Conditions\n\nSymptoms\n\nA test tries to apply the same test logic to many sets of input values, each with its own corresponding expected result. In the following example, the test iterates over a collection of test values and applies the test logic to each set:\n\npublic void testMultipleValueSets() { // Set Up Fixture Calculator sut = new Calculator(); TestValues[] testValues = { new TestValues(1,2,3), new TestValues(2,3,5), new TestValues(3,4,8), // special case! new TestValues(4,5,9) };\n\nfor (int i = 0; i < testValues.length; i++) { TestValues values = testValues[i]; // Exercise SUT int actual = sut.calculate( values.a, values.b); // Verify Outcome assertEquals(message(i), values.expectedSum, actual); } }\n\nprivate String message(int i) { return \"Row \"+ String.valueOf(i); }\n\nRoot Cause\n\nThe test automater is trying to test many test conditions using the same test logic in a single Test Method. In the preceding example, it is fairly simple Conditional Test Logic. Matters could be a lot worse if the code contained multiple nested loops and maybe even if statements to calculate different cases of the expected values.\n\nPossible Solution\n\nOf all sources of Conditional Test Logic, Multiple Test Conditions is prob- ably the most innocuous. Other than scaring the test reader, the main impact of such a test is that it stops executing at the ﬁ rst failure and doesn’t provide Defect Localization (see page 22) when a bug is introduced into the code. The\n\nwww.it-ebooks.info\n\n207\n\nConditional Test Logic",
      "content_length": 1758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "208\n\nConditional Test Logic\n\nChapter 15 Code Smells\n\nreadability issue can easily be addressed by using an Extract Method [Fowler] refactoring to create a Parameterized Test call from within the loop. The lack of Defect Localization can be addressed by calling the Parameterized Test from a separate Test Method for each test condition. For large sets of values, a Data-Driven Test (page 288) might be a better solution.\n\nwww.it-ebooks.info",
      "content_length": 440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "Hard-to-Test Code\n\nHard-to-Test Code\n\nCode is difﬁ cult to test.\n\nAutomated testing is a powerful tool that helps us develop software quickly even when we have a large code base to maintain. Of course, it provides these beneﬁ ts only if most of our code is protected by Fully Automated Tests (see page 26). The effort of writing these tests must be added to the effort of writing the product code they verify. Not surprisingly, we would prefer to make it easy to write the automated tests.3\n\nHard-to-Test Code is one factor that makes it difﬁ cult to write complete,\n\ncorrect automated tests in a cost-efﬁ cient manner.\n\nSymptoms\n\nSome kinds of code are inherently difﬁ cult to test—GUI components, multi- threaded code, and test code, for example. It may be difﬁ cult to get at the code to be tested because it is not visible to a test. It may be problematic to compile a test because the code is too highly coupled to other classes. It may be hard to create an instance of the object because the constructors don’t exist, are private, or take too many other objects as parameters.\n\nImpact\n\nWhenever we have Hard-to-Test Code, we cannot easily verify the quality of that code in an automated way. While manual quality assessment is often pos- sible, it doesn’t scale very well because the effort to perform this assessment after each code change usually means it doesn’t get done. Nor is this strategy readily repeated without a large test documentation cost.\n\nSolution Patterns\n\nA better solution is to make the code more amenable to testing. This topic is big enough that it warrants a whole chapter of its own, but this section covers a few of the highlights.\n\n3 We would also like to recoup this cost by reducing effort somewhere else. The best way to achieve this is to avoid Frequent Debugging (page 248) by writing the tests ﬁ rst and achieving Defect Localization (see page 22).\n\nwww.it-ebooks.info\n\n209\n\nHard-to-Test Code",
      "content_length": 1931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "210\n\nHard-to-Test Code\n\nAlso known as: Hard-Coded Dependency\n\nChapter 15 Code Smells\n\nCauses\n\nThere are a number of reasons for Hard-to-Test Code; the most common causes are discussed here.\n\nCause: Highly Coupled Code\n\nSymptoms\n\nA class cannot be tested without also testing several other classes.\n\nImpact\n\nCode that is highly coupled to other code is very difﬁ cult to unit test because it won’t execute in isolation.\n\nRoot Cause\n\nHighly Coupled Code can be caused by many factors, including poor design, lack of object-oriented design experience, and lack of a reward structure that encourages decoupling.\n\nPossible Solution\n\nThe key to testing overly coupled code is to break the coupling. This happens naturally when we are doing test-driven development.\n\nA technique that we often use to decouple code for the purpose of testing is a Test Double (page 522) or, more speciﬁ cally, a Test Stub (page 529) or Mock Object (page 544). This topic is covered in much more detail in Chapter 11, Using Test Doubles.\n\nRetroﬁ tting tests onto existing code is a more challenging task, especially when we are dealing with a legacy code base. This is a big enough topic that Michael Feathers wrote a whole book on techniques for doing this, titled Work- ing Effectively with Legacy Code [WEwLC].\n\nCause: Asynchronous Code\n\nSymptoms\n\nA class cannot be tested via direct method calls. The test must start an execut- able (such as a thread, process, or application) and wait until its start-up has ﬁ nished before interacting with the executable.\n\nwww.it-ebooks.info",
      "content_length": 1555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "Hard-to-Test Code\n\nImpact\n\nCode that has an asynchronous interface is hard to test because the tests of these elements must coordinate their execution with that of the SUT. This requirement can add a lot of complexity to the tests and causes them to take much, much longer to run. The latter issue is a major concern with unit tests, which must run very quickly to ensure that developers will run them frequently.\n\nRoot Cause\n\nThe code that implements the algorithm we wish to test is highly coupled to the active object in which it normally executes.\n\nPossible Solution\n\nThe key to testing asynchronous code is to separate the logic from the asynchronous access mechanism. The design-for-testability pattern Humble Object (page 695; including Humble Dialog and Humble Executable) is a good example of a way to restructure otherwise asynchronous code so it can be tested in a synchronous manner.\n\nCause: Untestable Test Code\n\nSymptoms\n\nThe body of a Test Method (page 348) is obscure enough (Obscure Test; see page 186) or contains enough Conditional Test Logic (page 200) that we wonder whether the test is correct.\n\nImpact\n\nAny Conditional Test Logic within a Test Method has a higher probability of producing Buggy Tests (page 260) and will likely result in High Test Mainte- nance Cost (page 265). Too much code in the test method body can make the test hard to understand and hard to construct correctly.\n\nRoot Cause\n\nThe code within the body of the Test Method is inherently hard to test using a Self-Checking Test (see page 26). To do so, we would have to replace the SUT with a Test Double that injects the target error and then run the test method inside another Expected Exception Test (see Test Method) method—much too much trouble to bother with in all but the most unusual circumstances.\n\nwww.it-ebooks.info\n\n211\n\nHard-to-Test Code",
      "content_length": 1844,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "212\n\nHard-to-Test Code\n\nChapter 15 Code Smells\n\nPossible Solution\n\nWe can remove the need to test the body of a Test Method by making it extremely simple and relocating any Conditional Test Logic from it into Test Utility Methods (page 599), for which we can easily write Self-Checking Tests.\n\nwww.it-ebooks.info",
      "content_length": 312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "Test Code Duplication\n\nTest Code Duplication\n\nThe same test code is repeated many times.\n\nMany of the tests in a suite need to do similar things. For example, tests often exercise scenarios that are variations on the same theme. Tests may require simi- lar ﬁ xture setup or result veriﬁ cation logic. In some cases, even the exercise SUT phase of many tests involves repeating the same nontrivial logic.\n\nThe need for tests to do similar things often results in Test Code Duplication.\n\nSymptoms\n\nSeveral tests may contain a common subset of essentially the same statements, as in the following example:\n\npublic void testInvoice_addOneLineItem_quantity1_b() { // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); // Verify only item LineItem expItem = new LineItem(inv, product, QUANTITY); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\npublic void testRemoveLineItemsForProduct_oneOfTwo() { // Set up Invoice inv = createAnonInvoice(); inv.addItemQuantity(product, QUANTITY); inv.addItemQuantity(anotherProduct, QUANTITY); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.removeLineItemForProduct(anotherProduct); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\nwww.it-ebooks.info\n\n213\n\nTest Code Duplication",
      "content_length": 1754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "214\n\nTest Code Duplication\n\nChapter 15 Code Smells\n\nA single test may also contain repeated groups of similar statements:\n\npublic void testInvoice_addTwoLineItems_sameProduct() { Invoice inv = createAnonInvoice(); LineItem expItem1 = new LineItem(inv, product, QUANTITY1); LineItem expItem2 = new LineItem(inv, product, QUANTITY2); // Exercise inv.addItemQuantity(product, QUANTITY1); inv.addItemQuantity(product, QUANTITY2); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 2); // Verify ﬁrst item LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem1.getInv(), actual.getInv()); assertEquals(expItem1.getProd(), actual.getProd()); assertEquals(expItem1.getQuantity(), actual.getQuantity()); // Verify second item actual = (LineItem)lineItems.get(1); assertEquals(expItem2.getInv(), actual.getInv()); assertEquals(expItem2.getProd(), actual.getProd()); assertEquals(expItem2.getQuantity(), actual.getQuantity()); }\n\nBoth of the preceding examples exhibit Test Code Duplication that is easily noticed. By comparison, it is more challenging to identify duplication when it occurs across Test Methods (page 348) that reside in different Testcase Classes (page 373).\n\nImpact\n\n“Cut and paste” often results in many copies of the same code. This code must be maintained every time the SUT is modiﬁ ed in a way that affects the seman- tics (e.g., number of arguments, argument attributes, returned object attributes, calling sequences) of its methods. This necessity can greatly increase the cost to introduce new functionality (High Test Maintenance Cost; see page 265) because of the effort involved in updating all tests that contain copies of the affected code.\n\nCauses\n\nCause: Cut-and-Paste Code Reuse\n\n“Cut and paste” is a powerful tool for writing code fast but it results in many copies of the same code, each of which must be maintained in parallel.\n\nwww.it-ebooks.info",
      "content_length": 1934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Test Code Duplication\n\nRoot Cause\n\nCut-and-Paste Code Reuse is often the default way to reuse logic. Developers who focus on details of “how” to do something will often repeat the same code many times because they cannot (or do not take the time to) focus on the big picture (the intent) of the test.\n\nA contributing factor may be a lack of refactoring skills or refactoring expe- rience that keeps developers from extracting the big picture from the detailed code they have written. Of course, time pressure may also be the culprit that keeps the refactoring from occurring. As a result, test code grows more compli- cated over time rather than becoming simpler.\n\nPossible Solution\n\nOnce Test Code Duplication has occurred, the best solution is to use an Extract Method [Fowler] refactoring to create a Test Utility Method (page 599) from one of the examples and then to generalize that method to handle each of the copies. When the Test Code Duplication consists of ﬁ xture setup logic, we end up with Creation Methods (page 415) or Finder Methods (see Test Utility Method). When the logic carries out result veriﬁ cation, we end up with Custom Assertions (page 474) or Veriﬁ cation Methods (see Custom Assertion).\n\nWe can use an Introduce Parameter [JBrains] refactoring to convert any lit- eral constants inside the extracted method into parameters that can be passed in to customize the method’s behavior for each test that calls it.\n\nMore simply, we can avoid most Test Code Duplication by writing the Test Methods in an “outside-in” manner, focusing on their intent. Whenever we need to do something that involves several lines of code, we simply call a nonexis- tent Test Utility Method to do it. We write all our tests this way and then ﬁ ll in implementations of the Test Utility Methods to get the tests to compile and run. (Modern IDEs facilitate this process by providing automatic method skeleton generation at a click of the mouse.)\n\nCause: Reinventing the Wheel\n\nWhile Cut-and-Paste Code Reuse deliberately makes copies of existing code to reduce the effort of writing tests, it is also possible to accidentally write the same sequence of statements in different tests.\n\nRoot Cause\n\nThis problem is primarily caused by a lack of awareness of which Test Utility Methods are available. It can also be caused by a predisposition to write one’s own code rather than reuse code written by others.\n\nwww.it-ebooks.info\n\n215\n\nTest Code Duplication",
      "content_length": 2455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "216\n\nTest Code Duplication\n\nChapter 15 Code Smells\n\nPossible Solution\n\nThe technical solution is largely the same as for Cut-and-Paste Code Reuse but the process solution is somewhat different. The test automater must look around more places to discover which Test Utility Methods are available before reinventing the wheel (i.e., writing new code).\n\nFurther Reading\n\nTest Code Duplication was ﬁ rst described in a paper at XP2001 called “Refac- toring Test Code” [RTC].\n\nwww.it-ebooks.info",
      "content_length": 490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "Test Logic in Production\n\nTest Logic in Production\n\nThe code that is put into production contains logic that should be exercised only during tests.\n\nThe SUT may contain logic that cannot be run in a test environment. Tests may require the SUT to behave in speciﬁ c ways to allow full test coverage.\n\nSymptoms\n\nThe logic in the SUT is there solely to support testing. This logic may be “extra stuff” that the tests require to gain access to the SUT’s internal state for ﬁ xture setup or result veriﬁ cation purposes. It may also consist of changes that the logic of the system undergoes when it detects that it is being tested.\n\nImpact\n\nWe would prefer not to end up with Test Logic in Production, as it can make the SUT more complex and opens the door to additional kinds of bugs that we would like to avoid. A system that behaves one way in the test lab and an entirely different way in production is a recipe for disaster!\n\nCauses\n\nCause: Test Hook\n\nConditional logic within the SUT determines whether the “real” code or test- speciﬁ c logic is run.\n\nSymptoms\n\nWith this code smell, either there may be no behavioral symptoms or something may go wrong in production. We may see snippets of code in the SUT that look something like this:\n\nif (testing) { return hardCodedCannedData; } else { // the real logic ... return gatheredData; }\n\nwww.it-ebooks.info\n\n217\n\nTest Logic in Production",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "218\n\nTest Logic in Production\n\nChapter 15 Code Smells\n\nAriane\n\nThe maiden ﬂ ight of the Ariane 5 rocket was a complete disaster: The rocket blew up only 37 seconds after takeoff. The culprit was a seem- ingly innocuous bit of code that was used only while the rocket was on the ground but unfortunately was left running for the ﬁ rst 40 seconds of ﬂ ight. When it tried to assign a 64-bit number representing the sideways velocity of the rocket to a 16-bit ﬁ eld, the navigation computer decided that the rocket was going the wrong way! It tried to correct the course, but the sudden change in direction tore the booster rocket apart. While this is not quite an example of Test Logic in Production (page 217), it certainly does illustrate the risks associated with this type of error.\n\nCould this disaster have been prevented by use of automated tests? While it is difﬁ cult to say with certainty, and one could certainly claim that any number of process changes could have detected this problem before it occurred, it is conceivable that automated tests could have averted this catastrophe.\n\nIn particular, a test should have addressed the boundary condition— namely, what happens when a number exceeds the maximum value stor- able. Such a test would have prevented an exception from occurring for the ﬁ rst time ever in production.\n\nIn addition, the presence of the tests from the Ariane 4 version of the rocket would have documented the maximum down-range velocity. It is quite possible that these tests would have been updated when the Ariane 5 software was being developed and that the new tests would have failed because of the new rocket’s higher speed.\n\nFor a slightly more detailed (and very interesting) description of “the little bug that could,” visit http://www.around.com/ariane.html.\n\nImpact\n\nCode that was not designed to work in production and that has not been veri- ﬁ ed to work properly in the production environment could accidentally be run in production and create serious problems.\n\nThe Ariane 5 rocket blew up 37 seconds after takeoff on its maiden ﬂ ight because a piece of code that was used only while the rocket was on the ground was left running for the ﬁ rst 40 seconds of ﬂ ight. This code tried to assign a 64-bit number representing the sideways velocity of the rocket to a 16-bit\n\nwww.it-ebooks.info",
      "content_length": 2334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "Test Logic in Production\n\nﬁ eld—an operation that convinced the rocket’s navigation computer that it was going the wrong way. (See the sidebar on Ariane on page 218 for more details.) While we believe the Test Hook would never be exercised in produc- tion, do we really want to take this kind of chance?\n\nRoot Cause\n\nIn some cases, the Test Logic in Production is introduced to make the behavior of the SUT more deterministic by returning known (hard-coded) values. In other cases, the Test Logic in Production may have been introduced to avoid execut- ing code that cannot be run in a test environment. Unfortunately, this approach can result in failure to execute that code in the production environment if some- thing is misconﬁ gured.\n\nIn some cases, tests may require that the SUT execute additional code that would otherwise be executed by a depended-on component. For example, code run from a trigger in a database will not run if the database is replaced by a Fake Database (see Fake Object on page 551); thus the test needs to ensure that the equivalent logic is executed from somewhere within the SUT.\n\nPossible Solution\n\nInstead of adding test logic into the production code directly, we can move logic into a substitutable dependency. We can put code that should be run in only pro- duction into a Strategy [GOF] object that is installed by default and replaced by a Null Object [PLOPD3] when running our tests. In contrast, code that should be run only during tests can be put into a Strategy [GOF] object that is conﬁ gured as a Null Object by default. Then, when we want the SUT to execute extra code during testing, we can replace this Strategy object with a test-speciﬁ c version. To ensure this mechanism is conﬁ gured properly, we should have a Constructor Test (see Test Method on page 348) to verify that any variables holding references to Strategy objects are initialized correctly when they are not overridden by the test. It may also be possible to override speciﬁ c methods of the SUT in a Test- Speciﬁ c Subclass (page 579) if the production logic we want to circumvent is localized in overridable methods. This ability is enabled by Self-Calls [WWW].\n\nCause: For Tests Only\n\nCode exists in the SUT strictly for use by tests.\n\nSymptoms\n\nSome of the methods of the SUT are used only by tests. Some of the attributes are public when they really should be private.\n\nwww.it-ebooks.info\n\n219\n\nTest Logic in Production",
      "content_length": 2439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "220\n\nTest Logic in Production\n\nChapter 15 Code Smells\n\nImpact\n\nSoftware that is added to the SUT For Tests Only makes the SUT more complex. It can confuse potential clients of the software’s interface by introducing addi- tional methods that should not be used by any code other than the tests. These methods may have been tested only in very speciﬁ c circumstances, so they might not work in the typical usage patterns used by real client software.\n\nRoot Cause\n\nThe test automater may need to add methods to a class that expose information needed by the test or methods that provide greater control over initialization (such as for the installation of a Test Double; see page 522). Test-driven devel- opment will lead to the creation of these additional methods even though they aren’t really needed by clients. When retroﬁ tting tests onto legacy code, the test automater may need access to information or functionality that is not already exposed.\n\nFor Tests Only can also result when a SUT is used asymmetrically in real life. Automated tests (especially round-trip tests) typically use software in a more symmetric fashion and hence may need methods that the real software clients do not need.\n\nPossible Solution\n\nWe can assure that tests have access to private information by creating a Test- Speciﬁ c Subclass of the SUT, which then provides methods to expose the needed attributes or initialization logic. A test needs to be able to create instances of the subclass instead of the SUT class for this approach to work.\n\nIf for some reason the extra methods cannot be moved to a Test-Speciﬁ c Sub- class, they should be clearly labeled For Tests Only. This can be done by adopt- ing a naming convention such as starting the names with “FTO_”.\n\nCause: Test Dependency in Production\n\nProduction executables depend on test executables.\n\nSymptoms\n\nWe cannot build only the production code; some test code must be included in the build to allow the production code to compile. Alternatively, we might notice that we cannot run the production code if the test executables are not present.\n\nwww.it-ebooks.info",
      "content_length": 2108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "Test Logic in Production\n\nImpact\n\nEven if the production modules do not contain any test code, problems can arise if any of these modules depends on a test module. At minimum, this dependency increases the size of the executable even if none of the test code is actually used in production scenarios. It also opens the door to accidental execution of test code during production.\n\nRoot Cause\n\nTest Dependency in Production is usually caused by a lack of attention to inter-module dependencies. It may also arise when a built-in self-test requires access to parts of the test automation infrastructure, such as Test Utility Methods (page 599) or the Test Automation Framework (page 298), to report test results.\n\nPossible Solution\n\nWe must manage our dependencies carefully to ensure that no production code depends on test code even for innocuous things such as type deﬁ nitions.\n\nAnything required by both test and production code should live in a production\n\nmodule or class that is accessible to both.\n\nCause: Equality Pollution\n\nAnother cause of Test Logic in Production is the implementation of test-speciﬁ c equality in the equals method of the SUT.\n\nSymptoms\n\nEquality Pollution can be difﬁ cult to spot once it has occurred—what is notable is that the SUT doesn’t actually need the equals method to be implemented. In other cases, behavioral symptoms may appear, such as test failure when the equals method is modiﬁ ed to support the speciﬁ c needs of a test or when the deﬁ - nition of equals changes within the SUT as part of a new feature or user story.\n\nImpact\n\nWe may write unnecessary equals methods simply to satisfy tests. We may also change the deﬁ nition of equals so that it no longer satisﬁ es the business requirements.\n\nEquality Pollution may make it difﬁ cult to introduce the equals logic pre- scribed by some new requirement if it already exists to support test-speciﬁ c equality for another test.\n\nwww.it-ebooks.info\n\n221\n\nTest Logic in Production",
      "content_length": 1973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "222\n\nTest Logic in Production\n\nChapter 15 Code Smells\n\nRoot Cause\n\nEquality Pollution is caused by a lack of awareness of the concept of test-speciﬁ c equality. Some early versions of dynamic Mock Object (page 544) generation tools forced us to use the SUT’s deﬁ nition of equals, which led to Equality Pollution.\n\nPossible Solution\n\nWhen a test requires test-speciﬁ c equality, we should use a Custom Asser- tion (page 474) instead of modifying the equals method just so that we can use a built-in Equality Assertion (see Assertion Method on page 362).\n\nWhen using dynamic Mock Object generation tools, we should use a Com- parator [WWW] rather than relying on the equals method supplied by the SUT. We can also implement the equals method on a Test-Speciﬁ c Subclass of an Expected Object (see State Veriﬁ cation on page 462) to avoid adding it to a production class directly.\n\nFurther Reading\n\nFor Tests Only and Equality Pollution were ﬁ rst introduced in a paper at XP2001 called “Refactoring Test Code” [RTC].\n\nwww.it-ebooks.info",
      "content_length": 1035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "Chapter 16\n\nBehavior Smells\n\nSmells in This Chapter\n\nAssertion Roulette . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\n\nErratic Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\n\nFragile Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n\nFrequent Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\n\nManual Intervention. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n\nSlow Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\n\n223\n\nwww.it-ebooks.info\n\nBehavior Smells",
      "content_length": 668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "224\n\nAssertion Roulette\n\nChapter 16 Behavior Smells\n\nAssertion Roulette\n\nIt is hard to tell which of several assertions within the same test method caused a test failure.\n\nSymptoms\n\nA test fails. Upon examining the output of the Test Runner (page 377), we cannot determine exactly which assertion failed.\n\nImpact\n\nWhen a test fails during an automated Integration Build [SCM], it may be hard to tell exactly which assertion failed. If the problem cannot be reproduced on a developer’s machine (as may be the case if the problem is caused by environ- mental issues or Resource Optimism; see Erratic Test on page 228) ﬁ xing the problem may be difﬁ cult and time-consuming.\n\nCauses\n\nCause: Eager Test\n\nA single test veriﬁ es too much functionality.\n\nSymptoms\n\nA test exercises several methods of the SUT or calls the same method several times interspersed with ﬁ xture setup logic and assertions.\n\npublic void testFlightMileage_asKm2() throws Exception { // set up ﬁxture // exercise constructor Flight newFlight = new Flight(validFlightNumber); // verify constructed object assertEquals(validFlightNumber, newFlight.number); assertEquals(\"\", newFlight.airlineCode); assertNull(newFlight.airline); // set up mileage newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); // verify results int expectedKilometres = 1810; assertEquals( expectedKilometres, actualKilometres);\n\nwww.it-ebooks.info",
      "content_length": 1445,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Assertion Roulette\n\n// now try it with a canceled ﬂight newFlight.cancel(); try { newFlight.getMileageAsKm(); fail(\"Expected exception\"); } catch (InvalidRequestException e) { assertEquals( \"Cannot get cancelled ﬂight mileage\", e.getMessage()); } }\n\nAnother possible symptom is that the test automater wants to modify the Test Automation Framework (page 298) to keep going after an assertion has failed so that the rest of the assertions can be executed.\n\nRoot Cause\n\nAn Eager Test is often caused by trying to minimize the number of unit tests (whether consciously or unconsciously) by verifying many test conditions in a single Test Method (page 348). While this is a good practice for manu- ally executed tests that have “liveware” interpreting the results and adjusting the tests in real time, it just doesn’t work very well for Fully Automated Tests (see page 26).\n\nAnother common cause of Eager Tests is using xUnit to automate customer tests that require many steps, thereby verifying many aspects of the SUT in each test. These tests are necessarily longer than unit tests but care should be taken to keep them as short as possible (but no shorter!).\n\nPossible Solution\n\nFor unit tests, we break up the test into a suite of Single-Condition Tests (see page 45) by teasing apart the Eager Test. It may be possible to do so by using one or more Extract Method [Fowler] refactorings to pull out independent pieces into their own Test Methods. Sometimes it is easier to clone the test once for each test condition and then clean up each Test Method by removing any code that is not required for that particular test conditions. Any code required to set up the ﬁ xture or put the SUT into the correct starting state can be ex- tracted into a Creation Method (page 415). A good IDE or compiler will then help us determine which variables are no longer being used.\n\nIf we are automating customer tests using xUnit, and this effort has resulted in many steps in each test because the work ﬂ ows require complex ﬁ xture setup, we could consider using some other way to set up the ﬁ xture for the latter parts of the test. If we can use Back Door Setup (see Back Door Manipulation on page 327) to create the ﬁ xture for the last part of the test independently of the\n\nwww.it-ebooks.info\n\n225\n\nAssertion Roulette",
      "content_length": 2309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "226\n\nAssertion Roulette\n\nChapter 16 Behavior Smells\n\nﬁ rst part, we can break one test into two, thereby improving our Defect Local- ization (see Goals of Test Automation). We should repeat this process as many times as it takes to make the tests short enough to be readable at a single glance and to Communicate Intent (see page 41) clearly.\n\nCause: Missing Assertion Message\n\nSymptoms\n\nA test fails. Upon examining the output of the Test Runner, we cannot deter- mine exactly which assertion failed.\n\nRoot Cause\n\nThis problem is caused by the use of Assertion Method (page 362) calls with identical or missing Assertion Messages (page 370). It is most commonly encountered when running tests using a Command-Line Test Runner (see Test Runner) or a Test Runner that is not integrated with the program text editor or development environment.\n\nIn the following test, we have a number of Equality Assertions (see Assertion\n\nMethod):\n\npublic void testInvoice_addLineItem7() { LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\nWhen an assertion fails, will we know which one it was? An Equality Assertion typically prints out both the expected and the actual values—but it may prove difﬁ cult to tell which assertion failed if the expected values are similar or print out cryptically. A good rule of thumb is to include at least a minimal Assertion Message whenever we have more than one call to the same kind of Assertion Method.\n\nPossible Solution\n\nIf the problem occurred while we were running a test using a Graphical Test Runner (see Test Runner) with IDE integration, we should be able to click on the appropriate line in the stack traceback to have the IDE highlight the failed\n\nwww.it-ebooks.info",
      "content_length": 2014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "Assertion Roulette\n\nassertion. Failing this, we can turn on the debugger and single-step through the test to see which assertion statement fails.\n\nIf the problem occurred while we were running a test using a Command- Line Test Runner, we can try running the test from a Graphical Test Runner with IDE integration to determine the offending assertion. If that doesn’t work, we may have to resort to using line numbers (if available) or apply a process of elimination to deduce which of the assertions it couldn’t be to narrow down the possibilities. Of course, we could just bite the bullet and add a unique Assertion Message (even just a number!) to each call to an Assertion Method.\n\nFurther Reading\n\nAssertion Roulette and Eager Test were ﬁ rst described in a paper presented at XP2001 called “Refactoring Test Code” [RTC].\n\nwww.it-ebooks.info\n\n227\n\nAssertion Roulette",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "228\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nErratic Test\n\nOne or more tests behave erratically; sometimes they pass and sometimes they fail.\n\nSymptoms\n\nWe have one or more tests that run but give different results depending on when they are run and who is running them. In some cases, the Erratic Test will con- sistently give the same results when run by one developer but fail when run by someone else or in a different environment. In other cases, the Erratic Test will give different results when run from the same Test Runner (page 377).\n\nImpact\n\nWe may be tempted to remove the failing test from the suite to “keep the bar green” but this would result in an (intentional) Lost Test (see Production Bugs on page 268). If we choose to keep the Erratic Test in the test suite despite the failures, the known failure may obscure other problems, such as another issue detected by the same tests. Just having a test fail can cause us to miss additional failures because it is much easier to see the change from a green bar to a red bar than to notice that two tests are failing instead of just the one we expected.\n\nTroubleshooting Advice\n\nErratic Tests can be challenging to troubleshoot because so many potential causes exist. If the cause cannot be easily determined, it may be necessary to collect data systematically over a period of time. Where (in which environments) did the tests pass, and where did they fail? Were all the tests being run or just a subset of them? Did any change in behavior occur when the test suite was run several times in a row? Did any change in behavior occur when it was run from several Test Runners at the same time?\n\nOnce we have some data, it should be easier to match up the observed symp- toms with those listed for each of the potential causes and to narrow the list of possibilities to a handful of candidates. Then we can collect some more data focusing on differences in symptoms between the possible causes. Figure 16.1 summarizes the process for determining which cause of an Erratic Test we are dealing with.\n\nwww.it-ebooks.info",
      "content_length": 2078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Erratic Test\n\nDifferent Different Results Every Results Every Run? Run?\n\nYes Yes\n\nNo No\n\nOnly with Only with Multiple Test Multiple Test Runners? Runners?\n\nYes Yes\n\nResults Results Vary for Tests Vary for Tests vs. Suites? vs. Suites?\n\nNo No\n\nYes Yes\n\nProbably Test Probably Test Run War Run War\n\nNo No\n\nYes Yes\n\nProbably Probably Resource Resource Leakage Leakage\n\nGets Gets Worse with Worse with Time? Time?\n\nNo No\n\nProbably Non- Probably Non- Deterministic Deterministic Test Test\n\nHappens Happens When Test Run When Test Run Alone? Alone?\n\nYes Yes\n\nNo No\n\nDifferent Different Results for First Results for First Run? Run?\n\nYes Yes\n\nNo No\n\nProbably Probably Unrepeatable Unrepeatable Test Test\n\nYes Yes\n\nResults Results Vary by Vary by Location? Location?\n\nNo No\n\nProbably Probably Lonely Test Lonely Test\n\nProbably Probably Interacting Interacting Tests or Suites Tests or Suites\n\nProbably Probably Resource Resource Optimism Optimism\n\nHire an Hire an xUnit xUnit Expert! Expert!\n\nFigure 16.1 Troubleshooting an Erratic Test.\n\nCauses\n\nTests may behave erratically for a number of reasons. The underlying cause can usually be determined through some persistent sleuthing by paying attention to patterns regarding how and when the tests fail. Some of the causes are common enough to warrant giving them names and speciﬁ c advice for rectifying them.\n\nCause: Interacting Tests\n\nTests depend on other tests in some way. Note that Interacting Test Suites and Lonely Test are speciﬁ c variations of Interacting Tests.\n\nSymptoms\n\nA test that works by itself suddenly fails in the following circumstances:\n\nAnother test is added to (or removed from) the suite.\n\nAnother test in the suite fails (or starts to pass).\n\nThe test (or another test) is renamed or moved in the source ﬁ le.\n\nA new version of the Test Runner is installed.\n\nRoot Cause\n\nInteracting Tests usually arise when tests use a Shared Fixture (page 317), with one test depending in some way on the outcome of another test. The cause of Interacting Tests can be described from two perspectives:\n\nwww.it-ebooks.info\n\n229\n\nErratic Test",
      "content_length": 2093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "230\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nThe mechanism of interaction\n\nThe reason for interaction\n\nThe mechanism for interaction could be something blatantly obvious—for example, testing an SUT that includes a database—or it could be more subtle. Anything that outlives the lifetime of the test can lead to interactions; static variables can be depended on to cause Interacting Tests and, therefore, should be avoided in both the SUT and the Test Automation Framework (page 298)! See the sidebar “There’s Always an Exception” on page 384 for an exam- ple of the latter problem. Singletons [GOF] and Registries [PEAA] are good examples of things to avoid in the SUT if at all possible. If we must use them, it is best to include a mechanism to reinitialize their static variables at the beginning of each test.\n\nTests may interact for a number of reasons, either by design or by accident:\n\nDepending on the ﬁ xture constructed by the ﬁ xture setup phase of\n\nanother test\n\nDepending on the changes made to the SUT during the exercise SUT\n\nphase of another test\n\nA collision caused by some mutually exclusive action (which may be either of the problems mentioned above) between two tests run in the same test run\n\nThe dependencies may suddenly cease to be satisﬁ ed if the depended-on test\n\nIs removed from the suite,\n\nIs modiﬁ ed to no longer change the state of the SUT,\n\nFails in its attempt to change the state of the SUT, or\n\nIs run after the test in question (because it was renamed or moved to a\n\ndifferent Testcase Class; see page 373).\n\nSimilarly, collisions may start occurring when the colliding test is\n\nAdded to the suite,\n\nPasses for the ﬁ rst time, or\n\nRuns before the dependent test.\n\nIn many of these cases, multiple tests will fail. Some of the tests may fail for a good reason—namely, the SUT is not doing what it is supposed to do. Depen- dent tests may fail for the wrong reason—because they were coded to depend\n\nwww.it-ebooks.info",
      "content_length": 1956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "Erratic Test\n\non other tests’ success. As a result, they may be giving a “false-positive” (false- failure) indication.\n\nIn general, depending on the order of test execution is not a wise approach because of the problems described above. Most variants of the xUnit frame- work do not make any guarantees about the order of test execution within a test suite. (TestNG, however, promotes interdependencies between tests by pro- viding features to manage the dependencies.)\n\nPossible Solution\n\nUsing a Fresh Fixture (page 311) is the preferred solution for Interacting Tests; it is almost guaranteed to solve the problem. If we must use a Shared Fixture, we should consider using an Immutable Shared Fixture (see Shared Fixture) to pre- vent the tests from interacting with one another through changes in the ﬁ xture by creating from scratch those parts of the ﬁ xture that they intend to modify.\n\nIf an unsatisﬁ ed dependency arises because another test does not create the expected objects or database data, we should consider using Lazy Setup (page 435) to create the objects or data in both tests. This approach ensures that the ﬁ rst test to execute creates the objects or data for both tests. We can put the ﬁ xture setup code into a Creation Method (page 415) to avoid Test Code Duplication (page 213). If the tests are on different Testcase Classes, we can move the ﬁ xture setup code to a Test Helper (page 643).\n\nSometimes the collision may be caused by objects or database data that are created in our test but not cleaned up afterward. In such a case, we should con- sider implementing Automated Fixture Teardown (see Automated Teardown on page 503) to remove them safely and efﬁ ciently.\n\nA quick way to ﬁ nd out whether any tests depend on one another is to run the tests in a different order than the normal order. Running the entire test suite in reverse order, for example, would do the trick nicely. Doing so regularly would help avoid accidental introduction of Interacting Tests.\n\nCause: Interacting Test Suites\n\nIn this special case of Interacting Tests, the tests are in different test suites.\n\nSymptoms\n\nA test passes when it is run in its own test suite but fails when it is run within a Suite of Suites (see Test Suite Object on page 387).\n\nSuite1.run()--> Green Suite2.run()--> Green Suite(Suite1,Suite2).run()--> Test C in Suite2 fails\n\nwww.it-ebooks.info\n\n231\n\nErratic Test",
      "content_length": 2397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "232\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nRoot Cause\n\nInteracting Test Suites usually occur when tests in separate test suites try to cre- ate the same resource. When they are run in the same suite, the ﬁ rst one succeeds but the second one fails while trying to create the resource.\n\nThe nature of the problem may be obvious just by looking at the test failure or by reading the failed Test Method (page 348). If it is not, we can try remov- ing other tests from the (nonfailing) test suite, one by one. When the failure stops occurring, we simply examine the last test we removed for behaviors that might cause the interactions with the other (failing) test. In particular, we need to look at anything that might involve a Shared Fixture, including all places where class variables are initialized. These locations may be within the Test Method itself, within a setUp method, or in any Test Utility Methods (page 599) that are called.\n\nWarning: There may be more than one pair of tests interacting in the same test suite! The interaction may also be caused by the Suite Fixture Setup (page 441) or Setup Decorator (page 447) of several Testcase Classes clashing rather than by a conﬂ ict between the actual Test Methods!\n\nVariants of xUnit that use Testcase Class Discovery (see Test Discovery on page 393), such as NUnit, may appear to not use test suites. In reality, they do—they just don’t expect the test automaters to use a Test Suite Factory (see Test Enumeration on page 399) to identify the Test Suite Object to the Test Runner.\n\nPossible Solution\n\nWe could, of course, eliminate this problem entirely by using a Fresh Fixture. If this solution isn’t within our scope, we could try using an Immutable Shared Fixture to prevent the tests’ interaction.\n\nIf the problem is caused by leftover objects or database rows created by one test that conﬂ ict with the ﬁ xture being created by a later test, we should con- sider using Automated Teardown to eliminate the need to write error-prone cleanup code.\n\nCause: Lonely Test\n\nA Lonely Test is a special case of Interacting Tests. In this case, a test can be run as part of a suite but cannot be run by itself because it depends on something in a Shared Fixture that was created by another test (e.g., Chained Tests; see page 454) or by suite-level ﬁ xture setup logic (e.g., a Setup Decorator).\n\nWe can address this problem by converting the test to use a Fresh Fixture or\n\nby adding Lazy Setup logic to the Lonely Test to allow it to run by itself.\n\nwww.it-ebooks.info",
      "content_length": 2529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "Erratic Test\n\nCause: Resource Leakage\n\nTests or the SUT consume ﬁ nite resources.\n\nSymptoms\n\nTests run more and more slowly or start to fail suddenly. Reinitializing the Test Runner, SUT, or Database Sandbox (page 650) clears up the problem—only to have it reappear over time.\n\nRoot Cause\n\nTests or the SUT consume ﬁ nite resources by allocating those resources and failing to free them afterward. This practice may make the tests run more slowly. Over time, all the resources are used up and tests that depend on them start to fail.\n\nThis problem can be caused by one of two types of bugs:\n\nThe SUT fails to clean up the resources properly. The sooner we detect\n\nthis behavior, the sooner we can track it down and ﬁ x it.\n\nThe tests themselves cause the resource leakage by allocating resources as part of ﬁ xture setup and failing to clean them up during ﬁ xture teardown.\n\nPossible Solution\n\nIf the problem lies in the SUT, then the tests have done their job and we can ﬁ x the bug. If the tests are causing the resource leakage, then we must eliminate the source of the leaks. If the leaks are caused by failure to clean up properly when tests fail, we may need to ensure that all tests do Guaranteed In-line Teardown (see In-line Teardown on page 509) or convert them to use Automated Teardown.\n\nIn general, it is a good idea to set the size of all resource pools to 1. This choice will cause the tests to fail much sooner, allowing us to more quickly determine which tests are causing the leak(s).\n\nCause: Resource Optimism\n\nA test that depends on external resources has nondeterministic results depending on when or where it is run.\n\nSymptoms\n\nA test passes when it is run in one environment and fails when it is run in another environment.\n\nwww.it-ebooks.info\n\n233\n\nErratic Test",
      "content_length": 1786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "234\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nRoot Cause\n\nA resource that is available in one environment is not available in another environment.\n\nPossible Solution\n\nIf possible, we should convert the test to use a Fresh Fixture by creating the resource as part of the test’s ﬁ xture setup phase. This approach ensures that the resource exists wherever it is run. It may necessitate the use of relative address- ing of ﬁ les to ensure that the speciﬁ c location in the ﬁ le system exists regardless of where the SUT is executed.\n\nIf an external resource must be used, the resources should be stored in the source code repository [SCM] so that all Test Runners run in the same en- vironment.\n\nCause: Unrepeatable Test\n\nA test behaves differently the ﬁ rst time it is run compared with how it behaves on subsequent test runs. In effect, it is interacting with itself across test runs.\n\nSymptoms\n\nEither a test passes the ﬁ rst time it is run and fails on all subsequent runs, or it fails the ﬁ rst time and passes on all subsequent runs. Here’s an example of what “Pass-Fail-Fail” might look like:\n\nSuite.run()--> Green Suite.run()--> Test C fails Suite.run()--> Test C fails User resets something Suite.run()--> Green Suite.run()--> Test C fails\n\nHere’s an example of what “Fail-Pass-Pass” might look like:\n\nSuite.run()--> Test C fails Suite.run()--> Green Suite.run()--> Green User resets something Suite.run()--> Test C fails Suite.run()--> Green\n\nBe forewarned that if our test suite contains several Unrepeatable Tests, we may see results that look more like this:\n\nSuite.run()--> Test C fails Suite.run()--> Test X fails Suite.run()--> Test X fails\n\nwww.it-ebooks.info",
      "content_length": 1674,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Erratic Test\n\nUser resets something Suite.run()--> Test C fails Suite.run()--> Test X fails\n\nTest C exhibits the “Fail-Pass-Pass” behavior, while test X exhibits the “Pass- Fail-Fail” behavior at the same time. It is easy to miss this problem because we see a red bar in each case; we notice the difference only if we look closely to see which tests fail each time we run them.\n\nRoot Cause\n\nThe most common cause of an Unrepeatable Test is the use—either deliberate or accidental—of a Shared Fixture. A test may be modifying the test ﬁ xture such that, during a subsequent run of the test suite, the ﬁ xture is in a different state. Although this problem most commonly occurs with a Prebuilt Fixture (see Shared Fixture), the only true prerequisite is that the ﬁ xture outlasts the test run.\n\nThe use of a Database Sandbox may isolate our tests from other developers’ tests but it won’t prevent the tests we run from colliding with themselves or with other tests we run from the same Test Runner.\n\nThe use of Lazy Setup to initialize a ﬁ xture holding class variable can result in the test ﬁ xture not being reinitialized on subsequent runs of the same test suite. In effect, we are sharing the test ﬁ xture between all runs started from the same Test Runner.\n\nPossible Solution\n\nBecause a persistent Shared Fixture is a prerequisite for an Unrepeatable Test, we can eliminate the problem by using a Fresh Fixture for each test. To fully isolate the tests, we must make sure that no shared resource, such as a Database Sandbox, outlasts the lifetimes of the individual tests. One option is to replace a database with a Fake Database (see Fake Object on page 551). If we must work with a persistent data store, we should use Distinct Generated Values (see Generated Value on page 723) for all database keys to ensure that we create different objects for each test and test run. The other alternative is to implement Automated Teardown to remove all newly created objects and rows safely and efﬁ ciently.\n\nCause: Test Run War\n\nTest failures occur at random when several people are running tests simultaneously.\n\nwww.it-ebooks.info\n\n235\n\nErratic Test",
      "content_length": 2147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "236\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nSymptoms\n\nWe are running tests that depend on some shared external resource such as a database. From the perspective of a single person running tests, we might see something like this:\n\nSuite.run() --> Test 3 fails Suite.run() --> Test 2 fails Suite.run() --> All tests pass Suite.run() --> Test 1 fails\n\nUpon describing our problem to our teammates, we discover that they are having the same problem at the same time. When only one of us runs tests, all of the tests pass.\n\nImpact\n\nA Test Run War can be very frustrating because the probability of it occurring increases the closer we get to a code cutoff deadline. This isn’t just Murphy’s law kicking in: It really does happen more often at this point! We tend to commit smaller changes at more frequent intervals as the deadline approaches (think “last-minute bug ﬁ xing”!). This, in turn, increases the likelihood that someone else will be running the test suite at the same time, which itself increases the like- lihood of test collisions between test runs occurring at the same time.\n\nRoot Cause\n\nA Test Run War can happen only when we have a globally Shared Fixture that various tests access and sometimes modify. This shared ﬁ xture could be a ﬁ le that must be opened or read by either a test or the SUT, or it could consist of the records in a test database.\n\nDatabase contention can be caused by the following activities:\n\nTrying to update or delete a record while another test is also updating\n\nthe same record\n\nTrying to update or delete a record while another test has a read lock\n\n(pessimistic locking) on the same record\n\nFile contention can be caused by an attempt to access a ﬁ le that has already been opened by another instance of the test running from a different Test Runner.\n\nPossible Solution\n\nUsing a Fresh Fixture is the preferred solution for a Test Run War. An even sim- pler solution is to give each Test Runner his or her own Database Sandbox. This\n\nwww.it-ebooks.info",
      "content_length": 1996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "Erratic Test\n\nshould not involve making any changes to the tests but will completely eliminate the possibility of a Test Run War. It will not, however, eliminate other sources of Erratic Tests because the tests can still interact through the Shared Fixture (the Database Sandbox). Another option is to switch to an Immutable Shared Fixture by having each test create new objects whenever it plans to change those objects. This approach does require changes to the Test Methods.\n\nIf the problem is caused by leftover objects or database rows created by one test that pollutes the ﬁ xture of a later test, another solution is using Automated Teardown to clean up after each test safely and efﬁ ciently. This measure, by itself, is unlikely to completely eliminate a Test Run War but it might reduce its frequency.\n\nCause: Nondeterministic Test\n\nTest failures occur at random, even when only a single Test Runner is running tests.\n\nSymptoms\n\nWe are running tests and the results vary each time we run them, as shown here:\n\nSuite.run() --> Test 3 fails Suite.run() --> Test 3 crashes Suite.run() --> All tests pass Suite.run() --> Test 3 fails\n\nAfter comparing notes with our teammates, we rule out a Test Run War either because we are the only person running tests or because the test ﬁ xture is not shared between users or computers.\n\nAs with an Unrepeatable Test, having multiple Nondeterministic Tests in the same test suite can make it more difﬁ cult to detect the failure/error pat- tern: It looks like different tests are failing rather than a single test producing different results.\n\nImpact\n\nDebugging Nondeterministic Tests can be very time-consuming and frustrating because the code executes differently each time. Reproducing the failure can be problematic, and characterizing exactly what causes the failure may require many attempts. (Once the cause has been characterized, it is often a straight- forward process to replace the random value with a value known to cause the problem.)\n\nwww.it-ebooks.info\n\n237\n\nErratic Test",
      "content_length": 2032,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "238\n\nErratic Test\n\nChapter 16 Behavior Smells\n\nRoot Cause\n\nNondeterministic Tests are caused by using different values each time a test is run. Sometimes, of course, it is a good idea to use different values each time the same test is run. For example, Distinct Generated Values may legitimately be used as unique keys for objects stored in a database. Use of generated values as input to an algorithm where the behavior of the SUT is expected to differ for different values can cause Nondeterministic Tests, however, as in the following examples:\n\nInteger values where negative (or even zero) values are treated differ- ently by the system, or where there is a maximum allowable value. If we generate a value at random, the test could fail in some test runs and pass on others.\n\nString values where the length of a string has minimum or maximum allowed values. This problem often occurs accidentally when we gener- ate a random or unique numeric value and then convert it to a string representation without using an explicit format that guarantees the length is constant.\n\nIt might seem like a good idea to use random values because they would improve our test coverage. Unfortunately, this tactic decreases our understanding of the test coverage and the repeatability of our tests (which violates the Repeatable Test principle; see page 26).\n\nAnother potential cause of Nondeterministic Tests is the use of Conditional Test Logic (page 200) in our tests. Its inclusion can result in different code paths being executed on different test runs, which in turn makes our tests non- deterministic. A common “reason” cited for doing so is the Flexible Test (see Conditional Test Logic). Anything that makes the tests less than completely deterministic is a bad idea!\n\nPossible Solution\n\nThe ﬁ rst step is to make our tests repeatable by ensuring that they execute in a completely linear fashion by removing any Conditional Test Logic. Then we can go about replacing any random values with deterministic values. If this results in poor test coverage, we can add more tests for the interesting cases we aren’t cov- ering. A good way to determine the best set of input values is to use the bound- ary values of the equivalence classes. If their use results in a lot of Test Code Duplication, we can extract a Parameterized Test (page 607) or put the input val- ues and the expected results into a ﬁ le read by a Data-Driven Test (page 288).\n\nwww.it-ebooks.info",
      "content_length": 2453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "Fragile Test\n\nFragile Test\n\nA test fails to compile or run when the SUT is changed in ways that do not affect the part the test is exercising.\n\nSymptoms\n\nWe have one or more tests that used to run and pass but now either fail to compile and run or fail when they are run. When we have changed the behavior of the SUT in question, such a change in test results is expected. When we don’t think the change should have affected the tests that are fail- ing or we haven’t changed any production code or tests, we have a case of Fragile Tests.\n\nPast efforts at automated testing have often run afoul of the “four sensitivities” of automated tests. These sensitivities are what cause Fully Automated Tests (see page 26) that previously passed to suddenly start failing. The root cause for tests failing can be loosely classiﬁ ed into one of these four sensitivities. Although each sensitivity may be caused by a variety of speciﬁ c test coding behaviors, it is useful to understand the sensitivities in their own right.\n\nImpact\n\nFragile Tests increase the cost of test maintenance by forcing us to visit many more tests each time we modify the functionality of the system or the ﬁ xture. They are particularly deadly when projects rely on highly incremental delivery, as in agile development (such as eXtreme Programming).\n\nTroubleshooting Advice\n\nWe need to look for patterns in how the tests fail. We ask ourselves, “What do all of the broken tests have in common?” The answer to this question should help us understand how the tests are coupled to the SUT. Then we look for ways to minimize this coupling.\n\nFigure 16.2 summarizes the process for determining which sensitivity we are\n\ndealing with.\n\nwww.it-ebooks.info\n\n239\n\nFragile Test",
      "content_length": 1733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "240\n\nFragile Test\n\nChapter 16 Behavior Smells\n\nAre the Tests Are the Tests Compiling? Compiling?\n\nNo No\n\nProbably Interface Probably Interface Sensitivity Sensitivity\n\nYes Yes\n\nPossibly Interface Possibly Interface Sensitivity Sensitivity\n\nYes Yes\n\nAre the Tests Are the Tests Erroring? Erroring?\n\nNo No\n\nPossibly Not Possibly Not Fragile Test Fragile Test\n\nYes Yes\n\nHave the Failing Have the Failing Tests Changed? Tests Changed?\n\nNo No\n\nProbably Behavior Probably Behavior Sensitivity Sensitivity\n\nYes Yes\n\nHas Some Has Some Code Changed? Code Changed?\n\nNo No\n\nProbably Data Probably Data Sensitivity Sensitivity\n\nYes Yes\n\nHas the Test Has the Test Data Changed? Data Changed?\n\nNo No\n\nProbably Context Probably Context Sensitivity Sensitivity\n\nFigure 16.2 Troubleshooting a Fragile Test.\n\nThe general sequence is to ﬁ rst ask ourselves whether the tests are failing to compile; if so, Interface Sensitivity is likely to blame. With dynamic languages we may see type incompatibility test errors at runtime—another sign of Interface Sensitivity.\n\nIf the tests are running but the SUT is providing incorrect results, we must ask ourselves whether we have changed the code. If so, we can try backing out of the latest code changes to see if that ﬁ xes the problem. If that tactic stops the failing tests,1 then we had Behavior Sensitivity.\n\nIf the tests still fail with the latest code changes backed out, then something else must have changed and we must be dealing with either Data Sensitiv- ity or Context Sensitivity. The former occurs only when we use a Shared Fix- ture (page 317) or we have modiﬁ ed ﬁ xture setup code; otherwise, we must have a case of Context Sensitivity.\n\nWhile this sequence of asking questions isn’t foolproof, it will give the right\n\nanswer probably nine times out of ten. Caveat emptor!\n\nCauses\n\nFragile Tests may be the result of several different root causes. They may be a sign of Indirect Testing (see Obscure Test on page 186)—that is, using the objects we modiﬁ ed to access other objects—or they may be a sign that we have Eager Tests (see Assertion Roulette on page 224) that are verifying too much functionality. Fragile Tests may also be symptoms of overcoupled software that is hard to test in small pieces (Hard-to-Test Code; see page 209) or our lack of experience with unit testing using Test Doubles (page 522) to test pieces in isola- tion (Overspeciﬁ ed Software).\n\n1 Other tests may fail because we have removed the code that made them pass—but at least we have established which part of the code they depend on.\n\nwww.it-ebooks.info",
      "content_length": 2579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "Fragile Test\n\nRegardless of their root cause, Fragile Tests usually show up as one of the four sensitivities. Let’s start by looking at them in a bit more detail; we’ll then examine some more detailed examples of how speciﬁ c causes change test output.\n\nCause: Interface Sensitivity\n\nInterface Sensitivity occurs when a test fails to compile or run because some part of the interface of the SUT that the test uses has changed.\n\nSymptoms\n\nIn statically typed languages, Interface Sensitivity usually shows up as a failure to compile. In dynamically typed languages, it shows up only when we run the tests. A test written in a dynamically typed language may experience a test error when it invokes an application programming interface (API) that has been modi- ﬁ ed (via a method name change or method signature change). Alternatively, the test may fail to ﬁ nd a user interface element it needs to interact with the SUT via a user interface. Recorded Tests (page 278) that interact with the SUT through a user interface2 are particularly prone to this problem.\n\nPossible Solution\n\nThe cause of the failures is usually reasonably apparent. The point at which the test fails (to compile or execute) will usually point out the location of the prob- lem. It is rare for the test to continue to run beyond the point of change—after all, it is the change itself that causes the test error.\n\nWhen the interface is used only internally (within the organization or applica- tion) and by automated tests, SUT API Encapsulation (see Test Utility Method on page 599) is the best solution for Interface Sensitivity. It reduces the cost and impact of changes to the API and, therefore, does not discourage necessary changes from being made. A common way to implement SUT API Encapsula- tion is through the deﬁ nition of a Higher-Level Language (see page 41) that is used to express the tests. The verbs in the test language are translated into the appropriate method calls by the encapsulation layer, which is then the only soft- ware that needs to be modiﬁ ed when the interface is altered in somewhat back- ward-compatible ways. The “test language” can be implemented in the form of Test Utility Methods such as Creation Methods (page 415) and Veriﬁ cation Methods (see Custom Assertion on page 474) that hide the API of the SUT from the test.\n\n2 Often called “screen scraping.”\n\nwww.it-ebooks.info\n\n241\n\nFragile Test",
      "content_length": 2404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "242\n\nFragile Test\n\nChapter 16 Behavior Smells\n\nThe only other way to avoid Interface Sensitivity is to put the interface under strict change control. When the clients of the interface are external and anonymous (such as the clients of Windows DLLs), this tactic may be the only viable alternative. In these cases, a protocol usually applies to mak- ing changes to interfaces. That is, all changes must be backward compatible; before older versions of methods can be removed, they must be deprecated, and deprecated methods must exist for a minimum number of releases or elapsed time.\n\nCause: Behavior Sensitivity\n\nBehavior Sensitivity occurs when changes to the SUT cause other tests to fail.\n\nSymptoms\n\nA test that once passed suddenly starts failing when a new feature is added to the SUT or a bug is ﬁ xed.\n\nRoot Cause\n\nTests may fail because the functionality they are verifying has been modiﬁ ed. This outcome does not necessarily signal a case of Behavior Sensitivity because it is the whole reason for having regression tests. It is a case of Behavior Sensitivity in any of the following circumstances:\n\nThe functionality the regression tests use to set up the pre-test state of\n\nthe SUT has been modiﬁ ed.\n\nThe functionality the regression tests use to verify the post-test state of\n\nthe SUT has been modiﬁ ed.\n\nThe code the regression tests use to tear down the ﬁ xture has been\n\nchanged.\n\nIf the code that changed is not part of the SUT we are verifying, then we are dealing with Context Sensitivity. That is, we may be testing too large a SUT. In such a case, what we really need to do is to separate the SUT into the part we are verifying and the components on which that part depends.\n\nPossible Solution\n\nAny newly incorrect assumptions about the behavior of the SUT used during ﬁ xture setup may be encapsulated behind Creation Methods. Similarly, assump- tions about the details of post-test state of the SUT can be encapsulated in Cus- tom Assertions or Veriﬁ cation Methods. While these measures won’t eliminate\n\nwww.it-ebooks.info",
      "content_length": 2048,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "Fragile Test\n\nthe need to update test code when the assumptions change, they certainly do reduce the amount of test code that needs to be changed.\n\nCause: Data Sensitivity\n\nData Sensitivity occurs when a test fails because the data being used to test the SUT has been modiﬁ ed. This sensitivity most commonly arises when the con- tents of the test database change.\n\nSymptoms\n\nA test that once passed suddenly starts failing in any of the following circum- stances:\n\nData is added to the database that holds the pre-test state of the SUT.\n\nRecords in the database are modiﬁ ed or deleted.\n\nThe code that sets up a Standard Fixture (page 305) is modiﬁ ed.\n\nA Shared Fixture is modiﬁ ed before the ﬁ rst test that uses it.\n\nIn all of these cases, we must be using a Standard Fixture, which may be either a Fresh Fixture (page 311) or a Shared Fixture such as a Prebuilt Fixture (see Shared Fixture).\n\nRoot Cause\n\nTests may fail because the result veriﬁ cation logic in the test looks for data that no longer exists in the database or uses search criteria that accidentally include newly added records. Another potential cause of failure is that the SUT is being exercised with inputs that reference missing or modiﬁ ed data and, therefore, the SUT behaves differently.\n\nIn all cases, the tests make assumptions about which data exist in the data-\n\nbase—and those assumptions are violated.\n\nPossible Solution\n\nIn those cases where the failures occur during the exercise SUT phase of the test, we need to look at the pre-conditions of the logic we are exercising and make sure they have not been affected by recent changes to the database.\n\nIn most cases, the failures occur during result veriﬁ cation. We need to examine the result veriﬁ cation logic to ensure that it does not make any un- reasonable assumptions about which data exists. If it does, we can modify the veriﬁ cation logic.\n\nwww.it-ebooks.info\n\n243\n\nFragile Test",
      "content_length": 1923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "244\n\nFragile Test\n\nChapter 16 Behavior Smells\n\nWhy Do We Need 100 Customers?\n\nA software development coworker of mine was working on a project as an analyst. One day, the manager she was working for came into her ofﬁ ce and asked, “Why have you requested 100 unique customers be cre- ated in the test database instance?”\n\nAs a systems analyst, my coworker was responsible for helping the busi- ness analysts deﬁ ne the requirements and the acceptance tests for a large, complex project. She wanted to automate the tests but had to overcome several hurdles. One of the biggest hurdles was the fact that the SUT got much of its data from an upstream system—it was too complex to try to generate this data manually.\n\nThe systems analyst came up with a way to generate XML from tests captured in spreadsheets. For the ﬁ xture setup part of the tests, she trans- formed the XML into QaRun (a Record and Playback Test tool—see Recorded Test on page 278) scripts that would load the data into the upstream system via the user interface. Because it took a while to run these scripts and for the data to make its way downstream to the SUT, the systems analyst had to run these scripts ahead of time. This meant that a Fresh Fixture (page 311) strategy was unachievable; a Prebuilt Fix- ture (page 429) was the best she could do. In an attempt to avoid the Interacting Tests (see Erratic Test on page 228) that were sure to result from a Shared Fixture (page 317), the systems analyst decided to imple- ment a virtual Database Sandbox (page 650) using a Database Partition- ing Scheme based on a unique customer number for each test. This way, any side effects of one test couldn’t affect any other tests.\n\nGiven that she had about 100 tests to automate, the systems analyst needed about 100 test customers deﬁ ned in the database. And that’s what she told her manager.\n\nThe failure can show up in the result veriﬁ cation logic even if the problem is that the inputs of the SUT refer to nonexistent or modiﬁ ed data. This may require ex- amining the “after” state of the SUT (which differs from the expected post-test state) and tracing it back to discover why it does not match our expectations. This should expose the mismatch between SUT inputs and the data that existed before the test started executing.\n\nThe best solution to Data Sensitivity is to make the tests independent of the existing contents of the database—that is, to use a Fresh Fixture. If this is not possible, we can try using some sort of Database Partitioning Scheme\n\nwww.it-ebooks.info",
      "content_length": 2548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "Fragile Test\n\n(see Database Sandbox on page 650) to ensure that the data modiﬁ ed for one test does not overlap with the data used by other tests. (See the sidebar “Why Do We Need 100 Customers?” on page 244 for an example.)\n\nAnother solution is to verify that the right changes have been made to the data. Delta Assertions (page 485) compare before and after “snapshots” of the data, thereby ignoring data that hasn’t changed. They eliminate the need to hard-code knowledge about the entire ﬁ xture into the result veriﬁ cation phase of the test.\n\nCause: Context Sensitivity\n\nContext Sensitivity occurs when a test fails because the state or behavior of the context in which the SUT executes has changed in some way.\n\nSymptoms\n\nA test that once passed suddenly starts failing for mysterious reasons. Unlike with an Erratic Test (page 228), the test produces consistent results when run repeatedly over a short period of time. What is different is that it consistently fails regardless of how it is run.\n\nRoot Cause\n\nTests may fail for two reasons:\n\nThe functionality they are verifying depends in some way on the time\n\nor date.\n\nThe behavior of some other code or system(s) on which the SUT\n\ndepends has changed.\n\nA major source of Context Sensitivity is confusion about which SUT we are intending to verify. Recall that the SUT is whatever piece of software we are intend- ing to verify. When unit testing, it should be a very small part of the overall system or application. Failure to isolate the speciﬁ c unit (e.g., class or method) is bound to lead to Context Sensitivity because we end up testing too much software all at once. Indirect inputs that should be controlled by the test are then left to chance. If someone then modiﬁ es a depended-on component (DOC), our tests fail.\n\nTo eliminate Context Sensitivity, we must track down which indirect input to the SUT has changed and why. If the system contains any date- or time-related logic, we should examine this logic to see whether the length of the month or other similar factors could be the cause of the problem.\n\nIf the SUT depends on input from any other systems, we should examine these inputs to see if anything has changed recently. Logs of previous interactions\n\nwww.it-ebooks.info\n\n245\n\nFragile Test",
      "content_length": 2271,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "246\n\nFragile Test\n\nAlso known as: Overcoupled Test\n\nChapter 16 Behavior Smells\n\nwith these other systems are very useful for comparison with logs of the failure scenarios.\n\nIf the problem comes and goes, we should look for patterns related to when it passes and when it fails. See Erratic Test for a more detailed discussion of possible causes of Context Sensitivity.\n\nPossible Solution\n\nWe need to control all the inputs of the SUT if our tests are to be deterministic. If we depend on inputs from other systems, we may need to control these inputs by using a Test Stub (page 529) that is conﬁ gured and installed by the test. If the system contains any time- or date-speciﬁ c logic, we need to be able to control the system clock as part of our testing. This may necessitate stubbing out the system clock with a Virtual Clock [VCTP] that gives the test a way to set the starting time or date and possibly to simulate the passage of time.\n\nCause: Overspeciﬁ ed Software\n\nA test says too much about how the software should be structured or behave. This form of Behavior Sensitivity (see Fragile Test on page 239) is associated with the style of testing called Behavior Veriﬁ cation (page 468). It is characterized by extensive use of Mock Objects (page 544) to build layer-crossing tests. The main issue is that the tests describe how the software should do something, not what it should achieve. That is, the tests will pass only if the software is implemented in a particular way. This problem can be avoided by applying the principle Use the Front Door First (see page 40) whenever possible to avoid encoding too much knowledge about the implementation of the SUT into the tests.\n\nCause: Sensitive Equality\n\nObjects to be veriﬁ ed are converted to strings and compared with an expected string. This is an example of Behavior Sensitivity in that the test is sensitive to behavior that it is not in the business of verifying. We could also think of it as a case of Interface Sensitivity where the semantics of the interface have changed. Either way, the problem arises from the way the test was coded; using the string representations of objects for verifying them against expected values is just asking for trouble.\n\nCause: Fragile Fixture\n\nWhen a Standard Fixture is modiﬁ ed to accommodate a new test, several other tests fail. This is an alias for either Data Sensitivity or Context Sensitivity depending on the nature of the ﬁ xture in question.\n\nwww.it-ebooks.info",
      "content_length": 2471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "Fragile Test\n\nFurther Reading\n\nSensitive Equality and Fragile Fixture were ﬁ rst described in [RTC], which was the ﬁ rst paper published on test smells and refactoring test code. The four sen- sitivities were ﬁ rst described in [ARTRP], which also described several ways to avoid Fragile Tests in Recorded Tests.\n\nwww.it-ebooks.info\n\n247\n\nFragile Test",
      "content_length": 351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "248\n\nFrequent Debugging\n\nAlso known as: Manual Debugging\n\nChapter 16 Behavior Smells\n\nFrequent Debugging\n\nManual debugging is required to determine the cause of most test failures.\n\nSymptoms\n\nA test run results in a test failure or a test error. The output of the Test Run- ner (page 377) is insufﬁ cient for us to determine the problem. Thus we have to use an interactive debugger (or sprinkle print statements throughout the code) to determine where things are going wrong.\n\nIf this case is an exception, we needn’t worry about it. If most test fail- ures require this kind of debugging, however, we have a case of Frequent Debugging.\n\nCauses\n\nFrequent Debugging is caused by a lack of Defect Localization (see page 22) in our suite of automated tests. The failed tests should tell us what went wrong either through their individual failure messages (see Assertion Message on page 370) or through the pattern of test failures. If they don’t:\n\nWe may be missing the detailed unit tests that would point out a logic\n\nerror inside an individual class.\n\nWe may be missing the component tests for a cluster of classes (i.e., a component) that would point out an integration error between the indi- vidual classes. This can happen when we use Mock Objects (page 544) extensively to replace depended-on objects but the unit tests of the depended-on objects don’t match the way the Mock Objects are pro- grammed to behave.\n\nI’ve encountered this problem most frequently when I wrote higher-level (func- tional or component) tests but failed to write all the unit tests for the individual methods. (Some people would call this approach storytest-driven development to distinguish it from unit test-driven development, in which every little bit of code is pulled into existence by a failing unit test.)\n\nFrequent Debugging can also be caused by Infrequently Run Tests (see Pro- duction Bugs on page 268). If we run our tests after every little change we make to the software, we can easily remember what we changed since the last time we ran the tests. Thus, when a test fails, we don’t have to spend a lot\n\nwww.it-ebooks.info",
      "content_length": 2118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "Frequent Debugging\n\nof time troubleshooting the software to discover where the bug is—we know where it is because we remember putting it there!\n\nImpact\n\nManual debugging is a slow, tedious process. It is easy to overlook subtle indi- cations of a bug and spend many hours tracking down a single logic error. Fre- quent Debugging reduces productivity and makes development schedules much less predictable because a single manual debugging session could extend the time required to develop the software by half a day or more.\n\nSolution Patterns\n\nIf we are missing the customer tests for a piece of functionality and manual user testing has revealed a problem not exposed by any automated tests, we probably have a case of Untested Requirements (see Production Bugs). We can ask our- selves, “What kind of automated test would have prevented the manual debug- ging session?” Better yet, once we have identiﬁ ed the problem, we can write a test that exposes it. Then we can use the failing test to do test-driven bug ﬁ xing. If we suspect this to be a widespread problem, we can create a development task to identify and write any additional tests that would be required to ﬁ ll the gap we just exposed.\n\nDoing true test-driven development is the best way to avoid the circumstances that lead to Frequent Debugging. We should start as close as possible to the skin of the application and do storytest-driven development—that is, we should write unit tests for individual classes as well as component tests for the collec- tions of related classes to ensure we have good Defect Localization.\n\nwww.it-ebooks.info\n\n249\n\nFrequent Debugging",
      "content_length": 1631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "250\n\nManual Intervention\n\nChapter 16 Behavior Smells\n\nManual Intervention\n\nA test requires a person to perform some manual action each time it is run.\n\nSymptoms\n\nThe person running the test must do something manually either before the test is run or partway through the test run; otherwise, the test fails. The Test Runner may need to verify the results of the test manually.\n\nImpact\n\nAutomated tests are all about getting early feedback on problems introduced into the software. If the cost of getting that feedback is too high—that is, if it takes the form of Manual Intervention—we likely won’t run the tests very often and we won’t get the feedback very often. If we don’t get that feedback very often, we’ll probably introduce lots of problems between test runs, which will ultimately lead to Frequent Debugging (page 248) and High Test Maintenance Cost (page 265).\n\nManual Intervention also makes it impractical to have a fully automated\n\nIntegration Build [SCM] and regression test process.\n\nCauses\n\nThe causes of Manual Intervention are as varied as the kinds of things our soft- ware does or encounters. The following are some general categories of the kinds of issues that require Manual Intervention. This list is by no means exhaustive, though.\n\nCause: Manual Fixture Setup\n\nSymptoms\n\nA person has to set up the test environment manually before the automated tests can be run. This activity may take the form of conﬁ guring servers, starting server processes, or running scripts to set up a Prebuilt Fixture (page 429).\n\nRoot Cause\n\nThis problem is typically caused by a lack of attention to automating the ﬁ xture setup phase of the test. It may also be caused by excessive coupling between\n\nwww.it-ebooks.info",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "Manual Intervention\n\ncomponents in the SUT that prevents us from testing a majority of the code in the system inside the development environment.\n\nPossible Solution\n\nWe need to make sure that we are writing Fully Automated Tests. This may require opening up test-speciﬁ c APIs to allow tests to set up the ﬁ xture. Where the issue is related to an inability to run the software in the development envi- ronment, we may need to refactor the software to decouple the SUT from the steps that would otherwise need to be done manually.\n\nCause: Manual Result Veriﬁ cation\n\nSymptoms\n\nWe can run the tests but they almost always pass—even when we know that the SUT is not returning the correct results.\n\nRoot Cause\n\nIf the tests we write are not Self-Checking Tests (see page 26), we can be given a false sense of security because tests will fail only if an error/exception is thrown.\n\nPossible Solution\n\nWe can ensure that our tests are all self-checking by including result veriﬁ ca- tion logic such as calls to Assertion Methods (page 362) within the Test Meth- ods (page 348).\n\nCause: Manual Event Injection\n\nSymptoms\n\nA person must intervene during test execution to perform some manual action before the test can proceed.\n\nRoot Cause\n\nMany events in a SUT are hard to generate under program control. Examples include unplugging network cables, bringing down database connections, and clicking buttons on a user interface.\n\nImpact\n\nIf a person needs to do something manually, it both increases the effort to run the test and ensures that the test cannot be run unattended. This torpedoes any attempt to do a fully automated build-and-test cycle.\n\nwww.it-ebooks.info\n\n251\n\nManual Intervention",
      "content_length": 1688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "252\n\nManual Intervention\n\nChapter 16 Behavior Smells\n\nPossible Solution\n\nThe best solution is to ﬁ nd ways to test the software that do not require a real person to do the manual actions. If the events are reported to the SUT through asynchronous events, we can have the Test Method invoke the SUT directly, passing it a simulated event object. If the SUT experiences the situation as a syn- chronous response from some other part of the system, we can get control of the indirect inputs by replacing some part of the SUT with a Test Stub (page 529) that simulates the circumstances to which we want to expose the SUT.\n\nFurther Reading\n\nRefer to Chapter 11, Using Test Doubles, for a much more detailed description of how to get control of the indirect inputs of the SUT.\n\nwww.it-ebooks.info",
      "content_length": 791,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "Slow Tests\n\nSlow Tests\n\nThe tests take too long to run.\n\nSymptoms\n\nThe tests take long enough to run that developers don’t run them every time they make a change to the SUT. Instead, the developers wait until the next coffee break or another interruption before running them. Or, whenever they run the tests, they walk around and chat with other team members (or play Doom or surf the Internet or . . .).\n\nImpact\n\nSlow Tests obviously have a direct cost: They reduce the productivity of the person running the test. When we are test driving the code, we’ll waste precious seconds every time we run our tests; when it is time to run all the tests before we commit our changes, we’ll have an even more signiﬁ cant wait time.\n\nSlow Tests also have many indirect costs:\n\nThe bottleneck created by holding the “integration token” longer because\n\nwe need to wait for the tests to run after merging all our changes.\n\nThe time during which other people are distracted by the person wait-\n\ning for his or her test run to ﬁ nish.\n\nThe time spent in debuggers ﬁ nding a problem that was inserted sometime after the last time we ran the test. The longer it has been since the test was run, the less likely we are to remember exactly what we did to break the test. This cost is a result of the breakdown of the rapid feedback that automated unit tests provide.\n\nA common reaction to Slow Tests is to immediately go for a Shared Fix- ture (page 317). Unfortunately, this approach almost always results in other problems, including Erratic Tests (page 228). A better solution is to use a Fake Object (page 551) to replace slow components (such as the database) with faster ones. However, if all else fails and we must use some kind of Shared Fixture, we should make it immutable if at all possible.\n\nTroubleshooting Advice\n\nSlow Tests can be caused either by the way the SUT is built and tested or by the way the tests are designed. Sometimes the problem is obvious—we can just\n\nwww.it-ebooks.info\n\n253\n\nSlow Tests",
      "content_length": 1999,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "254\n\nSlow Tests\n\nChapter 16 Behavior Smells\n\nwatch the green bar grow as we run the tests. There may be notable pauses in the execution; we may see explicit delays coded in a Test Method (page 348). If the cause is not obvious, however, we can run different subsets (or subsuites) of tests to see which ones run quickly and which ones take a long time to run.\n\nA proﬁ ling tool can come in handy to see where we are spending the extra time in test execution. Of course, xUnit gives us a simple means to build our own mini-proﬁ ler: We can edit the setUp and tearDown methods of our Testcase Superclass (page 638). We then write out the start/end times or test duration into a log ﬁ le, along with the name of the Testcase Class (page 373) and Test Method. Finally, we import this ﬁ le into a spreadsheet, sort by duration, and voila—we have found the culprits. The tests with the longest execution times are the ones on which it will be most worthwhile to focus our efforts.\n\nCauses\n\nThe speciﬁ c cause of the Slow Tests could lie either in how we built the SUT or in how we coded the tests themselves. Sometimes, the way the SUT was built forces us to write our tests in a way that makes them slow. This is particularly a problem with legacy code or code that was built with a “test last” perspective.\n\nCause: Slow Component Usage\n\nA component of the SUT has high latency.\n\nRoot Cause\n\nThe most common cause of Slow Tests is interacting with a database in many of the tests. Tests that have to write to a database to set up the ﬁ xture and read a database to verify the outcome (a form of Back Door Manipulation; see page 327) take about 50 times longer to run than the same tests that run against in-memory data structures. This is an example of the more general problem of using slow components.\n\nPossible Solution\n\nWe can make our tests run much faster by replacing the slow components with a Test Double (page 522) that provides near-instantaneous responses. When the slow component is the database, the use of a Fake Database (see Fake Object) can make the tests run on average 50 times faster! See the sidebar “Faster Tests Without Shared Fixtures” on page 319 for other ways to skin this cat.\n\nwww.it-ebooks.info",
      "content_length": 2220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "Slow Tests\n\nCause: General Fixture\n\nSymptoms\n\nTests are consistently slow because each test builds the same over-engineered ﬁ xture.\n\nRoot Cause\n\nEach test constructs a large General Fixture each time a Fresh Fixture (page 311) is built. Because a General Fixture contains many more objects than a Mini- mal Fixture (page 302), it naturally takes longer to construct. Fresh Fixture involves setting up a brand-new instance of the ﬁ xture for each Testcase Object (page 382), so multiply “longer” by the number of tests to get an idea of the magnitude of the slowdown!\n\nPossible Solution\n\nOur ﬁ rst inclination is often to implement the General Fixture as a Shared Fix- ture to avoid rebuilding it for each test. Unless we can make this Shared Fixture immutable, however, this approach is likely to lead to Erratic Tests and should be avoided. A better solution is to reduce the amount of ﬁ xture setup performed by each test.\n\nCause: Asynchronous Test\n\nSymptoms\n\nA few tests take inordinately long to run; those tests contain explicit delays.\n\nRoot Cause\n\nDelays included within a Test Method slow down test execution considerably. This slow execution may be necessary when the software we are testing spawns threads or processes (Asynchronous Code; see Hard-to-Test Code on page 209) and the test needs to wait for them to launch, run, and verify whatever side ef- fects they were expected to have. Because of the variability in how long it takes for these threads or processes to be started, the test usually needs to include a long delay “just in case”—that is, to ensure it passes consistently. Here’s an example of a test with delays:\n\nwww.it-ebooks.info\n\n255\n\nSlow Tests",
      "content_length": 1676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "256\n\nSlow Tests\n\nChapter 16 Behavior Smells\n\npublic class RequestHandlerThreadTest extends TestCase { private static ﬁnal int TWO_SECONDS = 3000;\n\npublic void testWasInitialized_Async() = throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); // Exercise sut.start(); // Verify Thread.sleep(TWO_SECONDS); assertTrue(sut.initializedSuccessfully()); }\n\npublic void testHandleOneRequest_Async() throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); sut.start(); // Exercise enqueRequest(makeSimpleRequest()); // Verify Thread.sleep(TWO_SECONDS); assertEquals(1, sut.getNumberOfRequestsCompleted()); assertResponseEquals(makeSimpleResponse(), getResponse()); } }\n\nImpact\n\nA two-second delay might not seem like a big deal. But consider what happens when we have a dozen such tests: It would take almost half a minute to run these tests. In contrast, we can run several hundred normal tests each second.\n\nPossible Solution\n\nThe best way to address this problem is to avoid asynchronicity in tests by test- ing the logic synchronously. This may require us to do an Extract Testable Com- ponent (page 767) refactoring to implement a Humble Executable (see Humble Object on page 695).\n\nCause: Too Many Tests\n\nSymptoms\n\nThere are so many tests that they are bound to take a long time to run regardless of how fast they execute.\n\nwww.it-ebooks.info",
      "content_length": 1417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "Slow Tests\n\nRoot Cause\n\nThe obvious cause of this problem is having so many tests. Perhaps we have such a large system that the large number of tests really is necessary, or perhaps we have too much overlap between tests.\n\nThe less obvious cause is that we are running too many of the tests too fre-\n\nquently!\n\nPossible Solution\n\nWe don’t have to run all the tests all the time! The key is to ensure that all tests are run regularly. If the entire suite is taking too long to run, consider creating a Subset Suite (see Named Test Suite on page 592) with a suitable cross section of tests; run this subsuite before every commit operation. The rest of the tests can be run regularly, albeit less often, by scheduling them to run overnight or at some other convenient time. Some people call this technique a “build pipeline.” For more on this and other ideas, see the sidebar “Faster Tests Without Shared Fixtures” on page 319.\n\nIf the system is large in size, it is a good idea to break it into a number of fairly independent subsystems or components. This allows teams work- ing on each component to work independently and to run only those tests specific to their own component. Some of those tests should act as proxies for how the other components would use the component; they must be kept up-to-date if the interface contract changes. Hmmm, Tests as Documenta- tion (see page 23); I like it! Some end-to-end tests that exercise all the com- ponents together (likely a form of storytests) would be essential, but they don’t need to be included in the pre-commit suite.\n\nwww.it-ebooks.info\n\n257\n\nSlow Tests",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "Chapter 17\n\nProject Smells\n\nSmells in This Chapter\n\nBuggy Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260\n\nDevelopers Not Writing Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n\nHigh Test Maintenance Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . 265\n\nProduction Bugs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\n\n259\n\nwww.it-ebooks.info\n\nProject Smells",
      "content_length": 457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "260\n\nBuggy Tests\n\nChapter 17 Project Smells\n\nBuggy Tests\n\nBugs are regularly found in the automated tests.\n\nFully Automated Tests (see page 26) are supposed to act as a “safety net” for teams doing iterative development. But how can we be sure the safety net actually works?\n\nBuggy Tests is a project-level indication that all is not well with our auto-\n\nmated tests.\n\nSymptoms\n\nA build fails, and a failed test is to blame. Upon closer inspection, we discover that the code being testing works correctly, but the test indicated it was broken. We encountered Production Bugs (page 268) despite having tests that verify the speciﬁ c scenario in which the bug was found. Root-cause analysis indicates the test contains a bug that precluded catching the error in the production code.\n\nImpact\n\nTests that give misleading results are dangerous! Tests that pass when they shouldn’t (a false negative, as in “nothing wrong here”) give a false sense of security. Tests that fail when they shouldn’t (a false positive) discredit the tests. They are like the little boy who cried, “Wolf!”; after a few occurrences, we tend to ignore them.\n\nCauses\n\nBuggy Tests can have many causes. Most of these problems also show up as code or behavior smells. As project managers, we are unlikely to see these un- derlying smells until we speciﬁ cally look for them.\n\nCause: Fragile Test\n\nBuggy Tests may just be project-level symptoms of a Fragile Test (page 239). For false-positive test failures, a good place to start is the “four sensitivities”: Interface Sensitivity (see Fragile Test), Behavior Sensitivity (see Fragile Test), Data Sensi- tivity (see Fragile Test), and Context Sensitivity (see Fragile Test). Each of these sensitivities could be the change that caused the test to fail. Removing the sensi- tivities by using Test Doubles (page 522) and refactoring can be challenging but ultimately it will make the tests much more dependable and cost-effective.\n\nwww.it-ebooks.info",
      "content_length": 1966,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "Buggy Tests\n\nCause: Obscure Test\n\nA common cause of false-negative test results (tests that pass when they shouldn’t) is an Obscure Test (page 186), which is difﬁ cult to get right—especially when we are modifying existing tests that were broken by a change we made. Because automated tests are hard to test, we don’t often verify that a modiﬁ ed test still catches all the bugs it was initially designed to trap. As long as we see a green bar, we think we are “good to go.” In reality, we may have created a test that never fails.\n\nObscure Tests are best addressed through refactoring of tests to focus on the reader of the tests. The real goal is Tests as Documentation (see page 23)— anything less will increase the likelihood of Buggy Tests.\n\nCause: Hard-to-Test Code\n\nAnother common cause of Buggy Tests, especially with “legacy software” (i.e., any software that doesn’t have a complete suite of automated tests), is that the design of the software is not conducive to automated testing. This Hard-to-Test Code (page 209) may force us to use Indirect Testing (see Obscure Test), which in turn may result in a Fragile Test.\n\nThe only way Hard-to-Test Code will become easy to test is if we refactor the code to improve its testability. (This transformation is described in Chapter 6, Test Automation Strategy, and Chapter 11, Using Test Doubles.) If this is not an option, we may be able to reduce the amount of test code affected by a change by applying SUT API Encapsulation (see Test Utility Method on page 599).\n\nTroubleshooting Advice\n\nWhen we have Buggy Tests, it is important to ask lots of questions. We must ask the “ﬁ ve why’s” [TPS] to get to the bottom of the problem—that is, we must determine exactly which code and/or behavior smells are causing the Buggy Tests and ﬁ nd the root cause of each smell.\n\nSolution Patterns\n\nThe solution depends very much on why the Buggy Tests occurred. Refer to the underlying behavior and code smells for possible solutions.\n\nAs with all “project smells,” we should look for project-level causes. These\n\ninclude not giving developers enough time to perform the following activities:\n\nwww.it-ebooks.info\n\n261\n\nBuggy Tests",
      "content_length": 2173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "262\n\nBuggy Tests\n\nChapter 17 Project Smells\n\nLearn to write the tests properly\n\nRefactor the legacy code to make test automation easier and more robust\n\nWrite the tests ﬁ rst\n\nFailure to address these project-level causes guarantees that the problems will recur in the near future.\n\nwww.it-ebooks.info",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "Developers Not Writing Tests\n\nDevelopers Not Writing Tests\n\nDevelopers aren’t writing automated tests.\n\nSymptoms\n\nWe hear that our developers aren’t writing tests. Or maybe we have observed Production Bugs (page 268) and asked, “Why are so many bugs getting through?”, only to be told, “Because we aren’t writing tests to cover that part of the software.”\n\nImpact\n\nIf the team isn’t writing automated tests for every piece of software “that could possibly break,” it is mortgaging its future. The current pace of software develop- ment will not be sustainable over the long haul because the system will be in test debt. It will take longer and longer to add new functionality, and refactoring the code to improve its design will be fraught with peril (so it will happen less and less frequently). This problem marks the beginning of a trip down the proverbial “slippery slope” to traditional paranoid, non-agile development. If that is where we aspire to be, we should stay the course. Otherwise, it is time to take action.\n\nCauses\n\nCause: Not Enough Time\n\nDevelopers may have trouble writing tests in the time they are given to do the development. This problem could be caused by an overly aggressive devel- opment schedule or supervisors/team leaders who instruct developers, “Don’t waste time writing tests.” Alternatively, developers may not have the skills needed to write tests efﬁ ciently and may not be allocated the time required to work their way up the learning curve.\n\nIf time is what the developers need, managers need to adjust the proj- ect schedule to give them that time. This extension need be only a temporary adjustment while the developers learn the skills and test automation infrastructure that will enable them to write the tests more quickly. In my experience, once developers have internalized the process, they can write the tests and the code in the same amount of time it once took them to write and debug just the code. The time spent writing the tests is more than compensated for by the time not spent in the debugger.\n\nwww.it-ebooks.info\n\n263\n\nDevelopers Not Writing Tests",
      "content_length": 2105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "264\n\nDevelopers Not Writing Tests\n\nChapter 17 Project Smells\n\nCause: Hard-to-Test Code\n\nA common cause of Developers Not Writing Tests, especially with “legacy soft- ware” (i.e., any software that doesn’t have a complete suite of automated tests), is that the design of the software is not conducive to automated testing. This situa- tion is described in more detail in its own smell, Hard-to-Test Code (page 209).\n\nCause: Wrong Test Automation Strategy\n\nAnother cause of Developers Not Writing Tests may be a test environment or test automation strategy that leads to Fragile Tests (page 239) or Obscure Tests (page 186) that take too long to write. We need to ask the “ﬁ ve why’s” [TPS] to ﬁ nd the root causes. Then we can address those causes and get the ship back on course.\n\nTroubleshooting Advice\n\nProject-level smells such as Developers Not Writing Tests are more likely to be detected by a project manager, scrum master, or team leader than by a developer. As managers, we may not know how to ﬁ x the problem, but our awareness and recognition of it is what matters. This unique perspective allows managers to ask the development team questions about why they aren’t writing tests, in which circumstances, and how long it takes to write tests when they do so. Then managers can encourage and empower the developers to come up with ways of addressing the root causes so that they write all the necessary tests.\n\nOf course, managers must give the developers their full support in carrying out whatever improvement plan they come up with. That support must include enough time to learn the requisite skills and build or set up the necessary test infrastructure. And managers shouldn’t expect things to turn around overnight. They might set a process improvement goal for each iteration, such as “20% reduction in code not tested” or “20% improvement in code coverage.” These goals should be reasonable and at a high-enough level that they encourage the right behavior, as opposed to just making the numbers look good. (A goal of 205 more tests written, for example, could be achieved without increasing the test coverage one iota simply by splitting tests into smaller pieces or cloning tests.)\n\nwww.it-ebooks.info",
      "content_length": 2220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "High Test Maintenance Cost\n\nHigh Test Maintenance Cost\n\nToo much effort is spent maintaining existing tests.\n\nTest code needs to be maintained along with the production code it veriﬁ es. As an application evolves, we will likely have to revisit our tests on a regular basis whenever we change the SUT classes to add new functionality or whenever we refactor the tests to simplify those classes. High Test Maintenance Cost occurs when the tests become overly difﬁ cult to understand and maintain.\n\nSymptoms\n\nDevelopment of new functionality slows down. Every time we add some new functionality, we need to make extensive changes to the existing tests. Develop- ers or test automaters may tell the project manager or coach that they need a “test refactoring/cleanup iteration.”\n\nIf we have been tracking the amount of time we spend writing the new tests and modifying existing tests separately from the time we spend implementing the code to make the tests pass, we notice that most of the time is spent modify- ing the existing tests.\n\nMost test maintainability issues are accompanied by other smells, such as the\n\nfollowing:\n\nA Fragile Test (page 239) indicates that tests are too closely coupled to\n\nthe SUT.\n\nA Fragile Fixture (see Fragile Test) signals that too many tests depend on the same ﬁ xture design (Standard Fixture on page 305), which leads to High Test Maintenance Cost.\n\nAn Erratic Test (page 228) may be a sign that a Shared Fixture (page 317)\n\nis causing our problem.\n\nImpact\n\nTeam productivity drops signiﬁ cantly because the tests take so much effort to main- tain. Developers may be agitating to “cut and run” (remove the affected tests from the test suites). While writing the production code is mandatory, maintaining the tests is completely optional (at least to the uninformed). If nothing is done about this problem, the entire test automation effort may be wasted when the team or manage- ment decides that test automation just “doesn’t work” and abandons the tests.\n\nwww.it-ebooks.info\n\n265\n\nHigh Test Maintenance Cost",
      "content_length": 2045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "266\n\nHigh Test Maintenance Cost\n\nChapter 17 Project Smells\n\nCauses\n\nThe root cause of High Test Maintenance Cost is failing to pay attention to the principles described in Chapter 5, Principles of Test Automation. A more immediate cause is often too much Test Code Duplication (page 213) and tests that are too closely coupled to the API of the SUT.\n\nCause: Fragile Test\n\nTests that fail because minor changes were made to the SUT are called Fragile Tests. They result in High Test Maintenance Cost because they need to be revis- ited and “giggled” after all manner of minor changes that really shouldn’t affect them.\n\nThe root cause of this failure can be any of the “four sensitivities”: Inter- face Sensitivity (see Fragile Test), Behavior Sensitivity (see Fragile Test), Data Sensitivity (see Fragile Test), and Context Sensitivity (see Fragile Test). We can reduce the High Test Maintenance Cost by protecting the tests against as many of these sensitivities as possible through the use of Test Doubles (page 522) and by refactoring the system into smaller components and classes that can be tested individually.\n\nCause: Obscure Test\n\nObscure Tests (page 186) are a major contributor to High Test Maintenance Cost because they take longer to understand each time they are visited. When they need to be modiﬁ ed, they take more effort to adjust and are much less likely to “work the ﬁ rst time,” resulting in more debugging of tests. Obscure Tests are also more likely to end up not catching conditions they were intended to detect, which can lead to Buggy Tests (page 260).\n\nObscure Tests are best addressed by refactoring tests to focus on the reader of the tests. The real goal is Tests as Documentation (see page 23)—anything less will increase the likelihood of High Test Maintenance Cost.\n\nCause: Hard-to-Test Code\n\n“Legacy software” (i.e., any software that doesn’t have a complete suite of auto- mated tests) can be hard to test because we typically write the tests “last” (after the software already exists). If the design of the software is not conducive to automated testing, we may be forced to use Indirect Testing (see Obscure Test) via awkward interfaces that involve a lot of accidental complexity; that effort may result in Fragile Tests.\n\nwww.it-ebooks.info",
      "content_length": 2279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "High Test Maintenance Cost\n\nIt will take both time and effort to refactor the code to improve its testability. Nevertheless, that time and effort are well spent if they eliminate the High Test Maintenance Cost. If refactoring is not an option, we may be able to reduce the amount of test code affected by a change by doing SUT API Encapsulation (see Test Utility Method on page 599) using Test Utility Methods. For example, Creation Methods (page 415) encapsulate the constructors, thereby rendering the tests less susceptible to changes in constructor signatures or semantics.\n\nTroubleshooting Advice\n\nAs a project-level smell, High Test Maintenance Cost is as likely to be detected by a project manager, scrum master, or team leader as by a developer. While managers may not have the technical depth needed to troubleshoot and ﬁ x the problem, the fact that they become aware of it is what is important. This aware- ness allows the manager to question the development team about how long it is taking to maintain tests, how often test maintenance occurs, and why it is neces- sary. Then the manager can challenge the developers to ﬁ nd a better way—one that won’t result in such High Test Maintenance Costs!\n\nOf course, the developers will need the manager’s support to carry out whatever improvement plan they come up with. That support must include time to conduct the investigations (spikes), learning/training time, and time to do the actual work. Managers can make time for this activity by having “test refactoring stories,” adjusting the velocity to reduce the new functionality com- mitted to the customer, or other means. Regardless of how managers carve out this time, they must remember that if they don’t give the development team the resources needed to ﬁ x the problem now, the problem will simply get worse and become even more challenging to ﬁ x in the future when the team has twice as many tests.\n\nwww.it-ebooks.info\n\n267\n\nHigh Test Maintenance Cost",
      "content_length": 1969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "268\n\nProduction Bugs\n\nChapter 17 Project Smells\n\nProduction Bugs\n\nWe ﬁ nd too many bugs during formal tests or in production.\n\nSymptoms\n\nWe have put a lot of effort into writing automated tests, yet the number of bugs showing up in formal (i.e., system) testing or production remains too high.\n\nImpact\n\nIt takes longer to troubleshoot and ﬁ x bugs found in formal testing than those found in development, and even longer to troubleshoot and ﬁ x bugs found in production. We may be forced to delay shipping the product or putting the application into production to allow time for the bug ﬁ xes and retesting. This time and effort translate directly into monetary costs and consume resources that might otherwise be used to add more functionality to the product or to build other products. The delay may also damage the organization’s credibility in the eyes of its customers. Poor quality has an indirect cost as well, in that it lowers the value of the product or service we are supplying.\n\nCauses\n\nBugs may slip through to production for several reasons, including Infrequently Run Tests or Untested Code. The latter problem may result from Missing Unit Tests or Lost Tests.\n\nBy specifying that “enough tests” be run, we mean the test coverage should be adequate, rather than that some speciﬁ c number of tests must be carried out. Changes to Untested Code are more likely to result in Production Bugs because there are no automated tests to tell the developers when they have introduced problems. Untested Requirements aren’t being veriﬁ ed every time the tests are run, so we don’t know for sure what is working. Both of these problems are related to Developers Not Writing Tests (page 263).\n\nCause: Infrequently Run Tests\n\nSymptoms\n\nWe hear that our developers aren’t running the tests very often. When we ask some questions, we discover that running the tests takes too long(Slow Tests; see page 253) or produces too many extraneous failures (Buggy Tests; see page 260).\n\nwww.it-ebooks.info",
      "content_length": 1995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "Production Bugs\n\nWe see test failures in the daily Integration Build [SCM]. When we dig deeper, we ﬁ nd that developers often commit their code without running the tests on their own machines.\n\nRoot Cause\n\nOnce they’ve seen the beneﬁ ts of working with the safety net of automated tests, most developers will continue using these tests unless something gets in the way. The most common impediments are Slow Tests that slow down the pre-integration regression testing or Unrepeatable Tests (see Erratic Test on page 228) that force developers to restart their test environment or do Manual Intervention (page 250) before running the tests.\n\nPossible Solution\n\nIf the root cause is Unrepeatable Tests, we can try switching to a Fresh Fix- ture (page 311) strategy to make the tests more deterministic. If the cause is Slow Tests, we must put more effort into speeding up the test run.\n\nCause: Lost Test\n\nSymptoms\n\nThe number of tests being executed in a test suite has declined (or has not increased as much as expected). We may notice this directly if we are paying attention to test counts. Alternatively, we may ﬁ nd a bug that should have been caused by a test that we know exists but, upon poking around, we discover that the test has been disabled.\n\nRoot Cause\n\nLost Tests can be caused by either a Test Method (page 348) or a Testcase Class (page 373) that has been disabled or has never been added to the AllTests Suite (see Named Test Suite on page 592).\n\nTests can be accidentally left out (i.e., never run) of test suite in the following\n\ncircumstances:\n\nWe forget to add the [test] attribute to the Test Method, or we acci- dentally use a method name that doesn’t match the naming convention used by the Test Discovery (page 393) mechanism.\n\nWe forget to add a call to suite.addTest to add the Test Method to the Test Suite Object (page 387) when we are automating tests in a Test Automation Framework (page 298) that supports only Test Enumeration (page 399).\n\nwww.it-ebooks.info\n\n269\n\nProduction Bugs",
      "content_length": 2012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "270\n\nProduction Bugs\n\nChapter 17 Project Smells\n\nWe forget to add a call to the Test Method explicitly in the Test Suite Procedure (see Test Suite Object) in procedural-language variations of xUnit.\n\nWe forget to add the test suite to the Suite of Suites (see Test Suite Object) or forget to add the [Test Fixture] attribute to the Testcase Class.\n\nTests that ran in the past may have been disabled in any of the following ways:\n\nWe renamed the Test Method to not match the pattern that causes Test Discovery to include the test in the test suite (e.g., the method name starts with “test . . .”).\n\nWe added an [Ignore] attribute in variants of xUnit that use method attributes to indicate Test Methods.\n\nWe commented out (or deleted) the code that adds the test (or suite) to\n\nthe suite explicitly.\n\nTypically, a Lost Test occurs when a test is failing and someone disables it to avoid having to wade through the failing tests when running other tests. It may also occur accidentally, of course.\n\nPossible Solution\n\nThere are a number of ways to avoid introducing Lost Tests.\n\nWe can use a Single Test Suite (see Named Test Suite) to run a single Test Method instead of disabling the failing or slow test. We can use the Test Tree Explorer (see Test Runner on page 377) to drill down and run a single test from within a test suite. Both of these techniques are made difﬁ cult by Chained Tests (page 454)—a deliberate form of Interacting Tests (see Erratic Test)—so this is just one more reason to avoid them.\n\nIf our variant of xUnit supports it, we can use the provided mechanism to ignore1 a test. It will typically remind us of the number of tests not being run so we don’t forget to re-enable them. We can also conﬁ gure our continuous integration tool to fail the build if the number of tests “ignored” exceeds a certain threshold.\n\nWe can compare the number of tests we have after check-in with the number of tests that existed in the code branch immediately before we started integra- tion. We simply verify that this count has increased by the number of tests we have added.\n\n1 For example, NUnit lets us put the attribute [Ignore] on a Test Method to keep it from being run.\n\nwww.it-ebooks.info",
      "content_length": 2203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "Production Bugs\n\nWe can implement or take advantage of Test Discovery if our programming\n\nlanguage supports reﬂ ection.\n\nWe can use a different strategy for ﬁ nding the tests to run in the Integration Build. Some build tools (such as Ant) let us ﬁ nd all ﬁ les that match a name pat- tern (e.g., those ending in “Test”). We won’t lose entire test suites if we use this capability to pick up all the tests.\n\nCause: Missing Unit Test\n\nSymptoms\n\nAll the unit tests pass but a customer test continues to fail. At some point, the customer test passed—but no unit tests were written to verify the behavior of the individual classes. Then, a subsequent code change modiﬁ ed the behavior of one of the classes, which broke its functionality.\n\nRoot Cause\n\nMissing Unit Tests often happen when a team focuses on writing the customer tests but fails to do test-driven development using unit tests. The team members may have built enough functionality to pass the customer tests, but a subsequent refactoring broke it. Unit tests would likely have prevented the code change from reaching the Integration Build.\n\nMissing Unit Tests can also arise during test-driven development when devel- opers get ahead of themselves and write some code without having a failing test to guide them.\n\nPossible Solution\n\nThe trite answer is to write more unit tests. Of course, this is easier said than done, and it isn’t always effective. Doing true test-driven development is the best way to avoid having Missing Unit Tests without writing unnecessary tests merely to get the test count up.\n\nCause: Untested Code\n\nSymptoms\n\nWe may just “know” that some piece of code in the SUT is not being exercised by any tests. Perhaps we have never seen that code execute, or perhaps we used code coverage tools to prove this fact beyond a doubt. In the following example, how can we test that when timeProvider throws an exception, this exception is handled correctly?\n\nwww.it-ebooks.info\n\n271\n\nProduction Bugs",
      "content_length": 1972,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "272\n\nProduction Bugs\n\nChapter 17 Project Smells\n\npublic String getCurrentTimeAsHtmlFragment() throws TimeProviderEx { Calendar currentTime; try { currentTime = getTimeProvider().getTime(); } catch (Exception e) { return e.getMessage(); } // etc.\n\nRoot Cause\n\nThe most common cause of Untested Code is that the SUT includes code paths that react to particular ways that a depended-on component (DOC) behaves and we haven’t found a way to exercise those paths. Typically, the DOC is being called synchronously and either returns certain values or throws excep- tions. During normal testing, only a subset of the possible equivalence classes of indirect inputs are actually encountered.\n\nAnother common cause of Untested Code is incompleteness of the test suite caused by incomplete characterization of the functionality exposed via the SUT’s interface.\n\nPossible Solution\n\nIf the Untested Code is caused by an inability to control the indirect inputs of the SUT, the most common solution is to use a Test Stub (page 529) to feed the various kinds of indirect inputs into the SUT to cover all the code paths. Other- wise, it may be sufﬁ cient to conﬁ gure the DOC to cause it to return the various indirect inputs required to fully test the SUT.\n\nCause: Untested Requirement\n\nSymptoms\n\nWe may just “know” that some piece of functionality is not being tested. Alter- natively, we may be trying to test a piece of software but cannot see any visible functionality that can be tested via the public interface of the software. All the tests we have written pass, however.\n\nWhen doing test-driven development, we know we need to add some code to handle a requirement. However, we cannot ﬁ nd a way to express the need for code to log the action in a Fully Automated Test (see page 26) such as this:\n\nwww.it-ebooks.info",
      "content_length": 1810,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Production Bugs\n\npublic void testRemoveFlight() throws Exception { // set up FlightDto expectedFlightDto = createARegisteredFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight should not exist after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); }\n\nNote that this test does not verify that the correct logging action has been done. It will pass regardless of whether the logging was implemented correctly—or even at all. Here’s the code that this test is verifying, complete with the indirect output of the SUT that has not been implemented correctly:\n\npublic void removeFlight(BigDecimal ﬂightNumber) throws FlightBookingException { System.out.println(\" removeFlight(\"+ﬂightNumber+\")\"); dataAccess.removeFlight(ﬂightNumber); logMessage(\"CreateFlight\", ﬂightNumber); // Bug! }\n\nIf we plan to depend on the information captured by logMessage when maintain- ing the application in production, how can we ensure that it is correct? Clearly, it is desirable to have automated tests verify this functionality.\n\nImpact\n\nPart of the required behavior of the SUT could be accidentally disabled without causing any tests to fail. Buggy software could be delivered to the customer. The fear of introducing bugs could discourage ruthless refactoring or deletion of code suspected to be unneeded (i.e., dead code).\n\nRoot Cause\n\nThe most common cause of Untested Requirements is that the SUT includes behavior that is not visible through its public interface. It may have expected “side effects” that cannot be observed directly by the test (such as writing out a ﬁ le or record or calling a method on another object or component)—in other words, it may have indirect outputs.\n\nWhen the SUT is an entire application, the Untested Requirement may be a result of not having a full suite of customer tests that verify all aspects of the visible behavior of the SUT.\n\nwww.it-ebooks.info\n\n273\n\nProduction Bugs",
      "content_length": 2043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "274\n\nProduction Bugs\n\nChapter 17 Project Smells\n\nPossible Solution\n\nIf the problem is missing customer tests, we need to write at least enough cus- tomer tests to ensure that all components are integrated properly. This may require improving the design-for-testability of the application by separating the presentation layer from the business logic layer.\n\nWhen we have indirect outputs that we need to verify, we can do Behavior Veriﬁ cation (page 468) through the use of Mock Objects (page 544). Testing of indirect outputs is covered in Chapter 11, Using Test Doubles.\n\nCause: Neverfail Test\n\nSymptoms\n\nWe may just “know” that some piece of functionality is not working, even though the tests for that functionality pass. When doing test-driven develop- ment, we have added a test for functionality we have not yet written but we cannot get the test to fail.\n\nImpact\n\nIf a test won’t fail even when the code to implement the functionality doesn’t exist, how useful is it for Defect Localization (see page 22)? Not very!\n\nRoot Cause\n\nThis problem can be caused by improperly coded assertions such as assertTrue- (aVariable, true) instead of assertEquals(aVariable, true) or just assertTrue(aVariable). Another cause is more sinister: When we have asynchronous tests, failures thrown in the other thread or process may not be seen or reported by the Test Runner.\n\nPossible Solution\n\nWe can implement cross-thread failure detection mechanisms to ensure that asynchronous tests do, indeed, fail. An even better solution is to refactor the code to support a Humble Executable (see Humble Object on page 695).\n\nwww.it-ebooks.info",
      "content_length": 1626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "PART III\n\nThe Patterns\n\nwww.it-ebooks.info\n\nThe Patterns",
      "content_length": 56,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "Chapter 18\n\nTest Strategy Patterns\n\nPatterns in This Chapter\n\nTest Automation Strategy\n\nRecorded Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\n\nScripted Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285\n\nData-Driven Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\n\nTest Automation Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\n\nTest Fixture Strategy\n\nMinimal Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302\n\nStandard Fixture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305\n\nFresh Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311\n\nShared Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n\nSUT Interaction Strategy\n\nBack Door Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\n\nLayer Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\n\n277\n\nwww.it-ebooks.info\n\nTest Strategy Patterns",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "278\n\nRecorded Test\n\nAlso known as: Record and Playback Test, Robot User Test, Capture/ Playback Test\n\nChapter 18 Test Strategy Patterns\n\nRecorded Test\n\nHow do we prepare automated tests for our software?\n\nWe automate tests by recording interactions with the application and playing them back using a test tool.\n\nFixture Fixture\n\nTest Test Recorder Recorder\n\nInputs Inputs\n\nOutputs Outputs\n\nInputs Inputs\n\nOutputs Outputs\n\nSUT SUT\n\nInputs Inputs\n\nOutputs Outputs\n\nTest Script Repository Test Script Repository\n\nTest Test Script 1 Script 1\n\nTest Test Script 2 Script 2\n\nTest Test Script n Script n\n\nAutomated tests serve several purposes. They can be used for regression testing software after it has been changed. They can help document the behavior of the software. They can specify the behavior of the software before it has been writ- ten. How we prepare the automated test scripts affects which purposes they can be used for, how robust they are to changes in the SUT, and how much skill and effort it takes to prepare them.\n\nRecorded Tests allow us to rapidly create regression tests after the SUT has\n\nbeen built and before it is changed.\n\nHow It Works\n\nWe use a tool that monitors our interactions with the SUT as we work with it. This tool keeps track of most of what the SUT communicates to us and our responses to the SUT. When the recording session is done, we can save the ses- sion to a ﬁ le for later playback. When we are ready to run the test, we start up\n\nwww.it-ebooks.info",
      "content_length": 1490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "Recorded Test\n\nthe “playback” part of the tool and point it at the recorded session. It starts up the SUT and feeds it our recorded inputs in response to the SUT’s outputs. It may also compare the SUT’s outputs with the SUT’s responses during the recording session. A mismatch may be cause for failing the test.\n\nSome Recorded Test tools allow us to adjust the sensitivity of the compari- sons that the tool makes between what the SUT said during the recording ses- sion and what it said during the playback. Most Recorded Test tools interact with the SUT through the user interface.\n\nWhen to Use It\n\nOnce an application is up and running and we don’t expect a lot of changes to it, we can use Recorded Tests to do regression testing. We could also use Recorded Tests when an existing application needs to be refactored (in anticipa- tion of modifying the functionality) and we do not have Scripted Tests (page 285) available to use as regression tests. It is typically much quicker to produce a set of Recorded Tests than to prepare Scripted Tests for the same functionality. In theory, the test recording can be done by anyone who knows how to operate the application; very little technical expertise should be required. In practice, many of the commercial tools have a steep learning curve. Also, some technical expertise may be required to add “checkpoints,” to adjust the sensitivity of the playback tool, or to adjust the test script if the recording tool became confused and recorded the wrong information.\n\nMost Recorded Test tools interact with the SUT through the user interface. This approach makes them particularly prone to fragility if the user interface of the SUT is evolving (Interface Sensitivity; see Fragile Test on page 239). Even small changes such as changing the internal name of a button or ﬁ eld may be enough to cause the playback tool to stumble. The tools also tend to record information at a very low and detailed level, making the tests hard to understand (Obscure Test; page 186); as a result, they are also difﬁ cult to repair by hand if they are broken by changes to the SUT. For these reasons, we should plan on rerecording the tests fairly regularly if the SUT will continue to evolve.\n\nIf we want to use the Tests as Documentation (see page 23) or if we want to use the tests to drive new development, we should consider using Scripted Tests. These goals are difﬁ cult to address with commercial Recorded Test tools because most do not let us deﬁ ne a Higher-Level Language (see page 41) for the test recording. This issue can be addressed by building the Recorded Test capability into the application itself or by using Refactored Recorded Test.\n\nwww.it-ebooks.info\n\n279\n\nRecorded Test",
      "content_length": 2723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "280\n\nRecorded Test\n\nChapter 18 Test Strategy Patterns\n\nVariation: Refactored Recorded Test\n\nA hybrid of the two strategies is to use the “record, refactor, playback”1 sequence to extract a set of “action components” or “verbs” from the newly Recorded Tests and then rewire the test cases to call these “action components” instead of having detailed in-line code. Most commercial capture/replay tools provide the means to turn Literal Values (page 714) into parameters that can be passed into the “action component” by the main test case. When a screen changes, we simply rerecord the “action component”; all the test cases continue to function by automatically using the new “action component” deﬁ nition. This strategy is effectively the same as using Test Utility Methods (page 599) to interact with the SUT in unit tests. It opens the door to using the Refactored Recorded Test com- ponents as a Higher-Level Language in Scripted Tests. Tools such as Mercury Interactive’s BPT2 use this paradigm for scripting tests in a top-down manner; once the high-level scripts are developed and the components required for the test steps are speciﬁ ed, more technical people can either record or hand-code the individual components.\n\nImplementation Notes\n\nWe have two basic choices when using a Recorded Test strategy: We can either acquire third-party tools that record the communication that occurs while we interact with the application or we can build a “record and playback” mecha- nism right into our application.\n\nVariation: External Test Recording\n\nMany test recording tools are available commercially, each of which has its own strengths and weaknesses. The best choice will depend on the nature of the user interface of the application, our budget, the complexity of the functionality to be veriﬁ ed, and possibly other factors.\n\nIf we want to use the tests to drive development, we need to pick a tool that uses a test-recording ﬁ le format that is editable by hand and easily understood. We’ll need to handcraft the contents—this situation is really an example of a Scripted Test even if we are using a “record and playback” tool to execute the tests.\n\n1 The name “record, refactor, playback” was coined by Adam Geras. 2 BPT is short for “Business Process Testing.”\n\nwww.it-ebooks.info",
      "content_length": 2289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "Recorded Test\n\nVariation: Built-In Test Recording\n\nIt is also possible to build a Recorded Test capability into the SUT. In such a case, the test scripting “language” can be deﬁ ned at a fairly high level—high enough to make it possible to hand-script the tests even before the system is built. In fact, it has been reported that the VBA macro capability of Microsoft’s Excel spreadsheet started out as a mechanism for automated testing of Excel.\n\nExample: Built-In Test Recording\n\nOn the surface, it doesn’t seem to make sense to provide a code sample for a Recorded Test because this pattern deals with how the test is produced, not how it is represented. When the test is played back, it is in effect a Data-Driven Test (page 288). Likewise, we don’t often refactor to a Recorded Test because it is often the ﬁ rst test automation strategy attempted on a project. Nevertheless, we might introduce a Recorded Test after attempting Scripted Tests if we discover that we have too many Missing Tests (page 268) because the cost of manual auto- mation is too high. In that case, we would not be trying to turn existing Scripted Tests into Recorded Tests; we would just record new tests.\n\nHere’s an example of a test recorded by the application itself. This test was used to regression-test a safety-critical application after it was ported from C on OS2 to C++ on Windows. Note how the recorded information forms a domain- speciﬁ c Higher-Level Language that is quite readable by a user.\n\n<interaction-log> <commands> <!-- more commands omitted --> <command seqno=\"2\" id=\"Supply Create\"> <ﬁeld name=\"engineno\" type=\"input\"> <used-value>5566</used-value> <expected></expected> <actual status=\"ok\"/> </ﬁeld> <ﬁeld name=\"direction\" type=\"selection\"> <used-value>SOUTH</used-value> <expected> <value>SOUTH</value> <value>NORTH</value> </expected> <actual> <value status=\"ok\">SOUTH</value> <value status=\"ok\">NORTH</value> </actual> </ﬁeld> </command> <!-- more commands omitted --> </commands> </interaction-log>\n\nwww.it-ebooks.info\n\n281\n\nRecorded Test",
      "content_length": 2046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "282\n\nRecorded Test\n\nChapter 18 Test Strategy Patterns\n\nThis sample depicts the output of having played back the tests. The actual elements were inserted by the built-in playback mechanism. The status attributes indicate whether these elements match the expected values. We applied a style sheet to these ﬁ les to format them much like a Fit test with color-coded results. The business users on the project then handled the recording, replaying, and result analysis.\n\nThis recording was made by inserting hooks in the presentation layer of the software to record the lists of choices offered the user and the user’s responses. An example of one of these hooks follows:\n\nif (playback_is_on()) { choice = get_choice_for_playback(dialog_id, choices_list); } else { choice = display_dialog(choices_list, row, col, title, key); }\n\nif (recording_is_on()) { record_choice(dialog_id, choices_list, choice, key); }\n\nThe method get_choice_for_playback retrieves the contents of the used-value element instead of asking the user to pick from the list of choices. The method record_choice generates the actual element and makes the “assertions” against the expected elements, recording the result in the status attribute of each element. Note that recording_is_on() returns true whenever we are in playback mode so that the test results can be recorded.\n\nExample: Commercial Record and Playback Test Tool\n\nAlmost every commercial testing tool uses a “record and playback” metaphor. Each tool also deﬁ nes its own Recorded Test ﬁ le format, most of which are very verbose. The following is a “short” excerpt from a test recorded using Mercury Interactive’s QuickTest Professional [QTP] tool. It is shown in “Expert View,” which exposes what is really recorded: a VbScript program! The example includes comments (preceded by “@@”) that were inserted manually to clarify what this test is doing; these comments would be lost if the test were rerecorded after a change to the application caused the test to no longer run.\n\n@@ @@ GoToPageMaintainTaxonomy() @@ Browser(\"Inf\").Page(\"Inf\").WebButton(\"Login\").Click Browser(\"Inf\").Page(\"Inf_2\").Check CheckPoint(\"Inf_2\") Browser(\"Inf\").Page(\"Inf_2\"\").Link(\"TAXONOMY LINKING\").Click Browser(\"Inf\").Page(\"Inf_3\").Check CheckPoint(\"Inf_3\") Browser(\"Inf\").Page(\"Inf_3\").Link(\"MAINTAIN TAXONOMY\").Click Browser(\"Inf\").Page(\"Inf_4\").Check CheckPoint(\"Inf_4\") @@\n\nwww.it-ebooks.info",
      "content_length": 2403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "Recorded Test\n\n@@ AddTerm(\"A\",\"Top Level\", \"Top Level Deﬁnition\") @@ Browser(\"Inf\").Page(\"Inf_4\").Link(\"Add\").Click wait 4 Browser(\"Inf_2\").Page(\"Inf\").Check CheckPoint(\"Inf_5\") Browser(\"Inf_2\").Page(\"Inf\").WebEdit(\"childCodeSufﬁx\").Set \"A\" Browser(\"Inf_2\").Page(\"Inf\"). WebEdit(\"taxonomyDto.descript\").Set \"Top Level\" Browser(\"Inf_2\").Page(\"Inf\"). WebEdit(\"taxonomyDto.deﬁniti\").Set \"Top Level Deﬁnition\" Browser(\"Inf_2\").Page(\"Inf\").WebButton(\"Save\").Click wait 4 Browser(\"Inf\").Page(\"Inf_5\").Check CheckPoint(\"Inf_5_2\") @@ @@ SelectTerm(\"[A]-Top Level\") @@ Browser(\"Inf\").Page(\"Inf_5\"). WebList(\"selectedTaxonomyCode\").Select \"[A]-Top Level\" @@ @@ AddTerm(\"B\",\"Second Top Level\", \"Second Top Level Deﬁnition\") @@ Browser(\"Inf\").Page(\"Inf_5\").Link(\"Add\").Click wait 4 Browser(\"Inf_2\").Page(\"Inf_2\").Check CheckPoint(\"Inf_2_2\") infoﬁle_;_Inform_Alberta_21.inf_;_hightlight id_; _Browser(\"Inf_2\").Page(\"Inf_2\")_;_ @@ @@ and it goes on, and on, and on ....\n\nNote how the test describes all inputs and outputs in terms of the user interface of the application. It suffers from two main issues: Obscure Tests (caused by the detailed nature of the recorded information) and Interface Sensitivity (resulting in Fragile Tests).\n\nRefactoring Notes\n\nWe can make this test more useful as documentation, reduce or avoid High Test Maintenance Cost (page 265), and support composition of other tests from a Higher-Level Language by using a series of Extract Method [Fowler] refactorings.\n\nExample: Refactored Commercial Recorded Test\n\nThe following example shows the same test refactored to Communicate Intent (see page 41):\n\nGoToPage_MaintainTaxonomy() AddTerm(\"A\",\"Top Level\", \"Top Level Deﬁnition\") SelectTerm(\"[A]-Top Level\") AddTerm(\"B\",\"Second Top Level\", \"Second Top Level Deﬁnition\")\n\nwww.it-ebooks.info\n\n283\n\nRecorded Test",
      "content_length": 1819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "284\n\nRecorded Test\n\nChapter 18 Test Strategy Patterns\n\nNote how much more intent revealing this test has become. The Test Utility Methods we extracted look like this:\n\nMethod GoToPage_MaintainTaxonomy() Browser(\"Inf\").Page(\"Inf\").WebButton(\"Login\").Click Browser(\"Inf\").Page(\"Inf_2\").Check CheckPoint(\"Inf_2\") Browser(\"Inf\").Page(\"Inf_2\").Link(\"TAXONOMY LINKING\").Click Browser(\"Inf\").Page(\"Inf_3\").Check CheckPoint(\"Inf_3\") Browser(\"Inf\").Page(\"Inf_3\").Link(\"MAINTAIN TAXONOMY\").Click Browser(\"Inf\").Page(\"Inf_4\").Check CheckPoint(\"Inf_4\") End\n\nMethod AddTerm( code, name, description) Browser(\"Inf\").Page(\"Inf_4\").Link(\"Add\").Click wait 4 Browser(\"Inf_2\").Page(\"Inf\").Check CheckPoint(\"Inf_5\") Browser(\"Inf_2\").Page(\"Inf\"). WebEdit(\"childCodeSufﬁx\").Set code Browser(\"Inf_2\").Page(\"Inf\"). WebEdit(\"taxonomyDto.descript\").Set name Browser(\"Inf_2\").Page(\"Inf\"). WebEdit(\"taxonomyDto.deﬁniti\").Set description Browser(\"Inf_2\").Page(\"Inf\").WebButton(\"Save\").Click wait 4 Browser(\"Inf\").Page(\"Inf_5\").Check CheckPoint(\"Inf_5_2\") end\n\nMethod SelectTerm( path ) Browser(\"Inf\").Page(\"Inf_5\"). WebList(\"selectedTaxonomyCode\").Select path Browser(\"Inf\").Page(\"Inf_5\").Link(\"Add\").Click wait 4 end\n\nThis example is one I hacked together to illustrate the similarities to what we do in xUnit. Don’t try running this example at home—it is probably not syntactically correct.\n\nFurther Reading\n\nThe paper “Agile Regression Testing Using Record and Playback” [ARTRP] describes our experiences building a Recorded Test mechanism into an applica- tion to facilitate porting it to another platform.\n\nwww.it-ebooks.info",
      "content_length": 1601,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "Scripted Test\n\nScripted Test\n\nHow do we prepare automated tests for our software?\n\nWe automate the tests by writing test programs by hand.\n\nFixture Fixture\n\nTest Test Development Development\n\nInputs Inputs\n\nOutputs Outputs\n\nSUT SUT\n\nInputs Inputs\n\nExpected Expected Outputs Outputs\n\nTest Script Repository Test Script Repository\n\nTest Test Script 1 Script 1\n\nTest Test Script 2 Script 2\n\nTest Test Script n Script n\n\nAutomated tests serve several purposes. They can be used for regression testing software after it has been changed. They can help document the behavior of the software. They can specify the behavior of the software before it has been written. How we prepare the automated test scripts affects which purpose they can be used for, how robust they are to changes in the SUT, and how much skill and effort it takes to prepare them.\n\nScripted Tests allow us to prepare our tests before the software is developed\n\nso they can help drive the design.\n\nHow It Works\n\nWe automate our tests by writing test programs that interact with the SUT for the purpose of exercising its functionality. Unlike Recorded Tests (page 278), these tests can be either customer tests or unit tests. These test programs are often called “test scripts” to distinguish them from the production code they test.\n\nwww.it-ebooks.info\n\n285\n\nScripted Test\n\nAlso known as: Hand-Written Test, Hand- Scripted Test, Programmatic Test, Automated Unit Test",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "286\n\nScripted Test\n\nChapter 18 Test Strategy Patterns\n\nWhen to Use It\n\nWe almost always use Scripted Tests when preparing unit tests for our software. This is because it is easier to access the individual units directly from software written in the same programming language. It also allows us to exercise all the code paths, including the “pathological” cases.\n\nCustomer tests are a slightly more complicated picture; we should use a Scripted Test whenever we use automated storytests to drive the develop- ment of software. Recorded Tests don’t serve this need very well because it is difﬁ cult to record tests without having an application from which to record them. Preparing Scripted Tests takes programming experience as well as experience in testing techniques. It is unlikely that most business users on a project would be interested in learning how to prepare Scripted Tests. An alternative to scripting tests in a programming language is to deﬁ ne a Higher- Level Language (see page 41) for testing the SUT and then to implement the language as a Data-Driven Test (page 288) Interpreter [GOF]. An open- source framework for deﬁ ning Data-Driven Tests is Fit and its wiki-based cousin, FitNesse. Canoo WebTest is another tool that supports this style of testing.\n\nIn case of an existing legacy application,3 we can consider using Recorded Tests as a way of quickly creating a suite of regression tests that will protect us while we refactor the code to introduce testability. We can then prepare Scripted Tests for our now testable application.\n\nImplementation Notes\n\nTraditionally, Scripted Tests were written as “test programs,” often using a spe- cial test scripting language. Nowadays, we prefer to write Scripted Tests using a Test Automation Framework (page 298) such as xUnit in the same language as the SUT. In this case, each test program is typically captured in the form of a Test Method (page 348) on a Testcase Class (page 373). To minimize Manual Intervention (page 250), each test method should implement a Self-Checking Test (see page 26) that is also a Repeatable Test (see page 26).\n\n3 Among test drivers, a legacy application is any system that lacks a safety net of auto- mated tests.\n\nwww.it-ebooks.info",
      "content_length": 2233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "Scripted Test\n\nExample: Scripted Test\n\nThe following is an example of a Scripted Test written in JUnit:\n\npublic void testAddLineItem_quantityOne(){ ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE; ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE; // Set Up Fixture Customer customer = createACustomer(NO_CUST_DISCOUNT); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(PRODUCT, QUAN_ONE); // Verify Outcome LineItem expected = createLineItem( QUAN_ONE, NO_CUST_DISCOUNT, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\npublic void testChangeQuantity_severalQuantity(){ ﬁnal int ORIGINAL_QUANTITY = 3; ﬁnal int NEW_QUANTITY = 5; ﬁnal BigDecimal BASE_PRICE = UNIT_PRICE.multiply( new BigDecimal(NEW_QUANTITY)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUST_DISCOUNT_PC.movePointLeft(2))); // Set Up Fixture Customer customer = createACustomer(CUST_DISCOUNT_PC); Invoice invoice = createInvoice(customer); Product product = createAProduct( UNIT_PRICE); invoice.addItemQuantity(product, ORIGINAL_QUANTITY); // Exercise SUT invoice.changeQuantityForProduct(product, NEW_QUANTITY); // Verify Outcome LineItem expected = createLineItem( NEW_QUANTITY, CUST_DISCOUNT_PC, EXTENDED_PRICE, PRODUCT, invoice); assertContainsExactlyOneLineItem( invoice, expected ); }\n\nAbout the Name\n\nAutomated test programs are traditionally called “test scripts,” probably due to the heritage of such test programs—originally they were implemented in interpreted test scripting languages such as Tcl. The downside of calling them Scripted Tests is that this nomenclature opens the door to confusion with the kind of script a person would follow during manual testing as opposed to unscripted testing such as exploratory testing.\n\nFurther Reading\n\nMany books have been written about the process of writing Scripted Tests and using them to drive the design of the SUT. A good place to start would be [TDD-BE] or [TDD-APG].\n\nwww.it-ebooks.info\n\n287\n\nScripted Test",
      "content_length": 2031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "288\n\nData-Driven Test\n\nChapter 18 Test Strategy Patterns\n\nData-Driven Test\n\nHow do we prepare automated tests for our software? How do we reduce Test Code Duplication?\n\nWe store all the information needed for each test in a data ﬁ le and write an interpreter that reads the ﬁ le and executes the tests.\n\nFixture Fixture\n\nTest 1 Test 1 Data Data\n\nSetup Setup\n\nTest 2 Test 2 Data Data data data\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nTeardown Teardown\n\nTest n Test n Data Data\n\nTesting can be very repetitious not only because we must run the same test over and over again, but also because many of the tests differ only slightly. For example, we might want to run essentially the same test with slightly different system inputs and verify that the actual output varies accordingly. Each of these tests would consist of exactly the same steps. While having so many tests is an excellent way to ensure good coverage of functionality, it is not so good for test maintainability because any change made to the algorithm of one of these tests must be propagated to all of the similar tests.\n\nA Data-Driven Test is one way to get excellent coverage while minimizing\n\nthe amount of test code we need to write and maintain.\n\nHow It Works\n\nWe write a Data-Driven Test interpreter that contains all the common logic from the tests. We put the data that varies from test to test into the Data-Driven Test ﬁ le that the interpreter reads to execute the tests. For each test it performs the same sequence of actions to implement the Four-Phase Test (page 358). First,\n\nwww.it-ebooks.info",
      "content_length": 1578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "Data-Driven Test\n\nthe interpreter retrieves the test data from the ﬁ le and sets up the test ﬁ xture us- ing the data from the ﬁ le. Second, it exercises the SUT with whatever arguments the ﬁ le speciﬁ es. Third, it compares the actual results produced by the SUT (e.g., returned values, post-test state) with the expected results from the ﬁ le. If the results don’t match, it marks the test as failed; if the SUT throws an exception, it catches the exception and marks the test accordingly and continues. Fourth, the interpreter does any ﬁ xture teardown that is necessary and then moves on to the next test in the ﬁ le.\n\nA test that might otherwise require a series of complex steps can be reduced to a single line of data in the Data-Driven Test ﬁ le. Fit is a popular example of a framework for writing Data-Driven Tests.\n\nWhen to Use It\n\nA Data-Driven Test is an alternative strategy to a Recorded Test (page 278) and a Scripted Test (page 285). It can also be used as part of a Scripted Test strategy, however, and Recorded Tests are, in fact, Data-Driven Tests when they are played back. A Data-Driven Test is an ideal strategy for getting business people involved in writing automated tests. By keeping the format of the data ﬁ le simple, we make it possible for the business person to populate the ﬁ le with data and execute the tests without having to ask a technical person to write test code for each test.\n\nWe can consider using a Data-Driven Test as part of a Scripted Test strategy whenever we have a lot of different data values with which we wish to exercise the SUT where the same sequence of steps must be executed for each data value. Usually, we discover this similarity over time and refactor ﬁ rst to a Parameterized Test (page 607) and then to a Data-Driven Test. We may also want to arrange a standard set of steps in different sequences with different data values much like in an Incremental Tabular Test (see Parameterized Test). This approach gives us the best coverage with the least amount of test code to maintain and makes it very easy to add more tests as they are needed.\n\nAnother consideration when deciding whether to use Data-Driven Tests is whether the behavior we are testing is hard-coded or driven by conﬁ guration data. If we automate tests for data-driven behavior using Scripted Tests, we must update the test programs whenever the conﬁ guration data changes. This behavior is just plain unnatural because it implies that we must commit changes to our source code repository [SCM] whenever we change the data in our conﬁ guration database.4 By making the tests data-driven, changes to the conﬁ guration data or\n\n4 Of course, we should be managing our test data in a version-controlled Repository, too—but that topic could ﬁ ll another book; see [RDb] for details.\n\nwww.it-ebooks.info\n\n289\n\nData-Driven Test",
      "content_length": 2850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "290\n\nData-Driven Test\n\nChapter 18 Test Strategy Patterns\n\nmeta-objects are then driven by changes to the Data-Driven Tests—a much more natural relationship.\n\nImplementation Notes\n\nOur implementation options depend on whether we are using a Data-Driven Test as a distinct test strategy or as part of an xUnit-based strategy. Using a Data-Driven Test as a stand-alone test strategy typically involves using open- source tools such as Fit or commercial Recorded Test tools such as QTP. Using a Data-Driven Test as part of a Scripted Test strategy may involve implementing a Data-Driven Test interpreter within xUnit.\n\nRegardless of which strategy we elect to follow, we should use the appropri- ate Test Automation Framework (page 298) if one is available. By doing so, we effectively convert our tests into two parts: the Data-Driven Test interpreter and the Data-Driven Test ﬁ les. Both of these assets should be kept under ver- sion control so that we can see how they have evolved over time and to allow us to back out any misguided changes. It is particularly important to store the Data-Driven Test ﬁ les in some kind of Repository, even though this concept may be foreign to business users. We can make this operation transparent by provid- ing the users with a Data-Driven Test ﬁ le-authoring tool such as FitNesse, or we can set up a “user-friendly” Repository such as a document management system that just happens to support version control as well.\n\nIt is also important to run these tests as part of the continuous integration process to conﬁ rm that tests that once passed do not suddenly begin to fail. Failing to do so can result in bugs creeping into the software undetected and signiﬁ cant troubleshooting effort once the bugs are detected. Including the cus- tomer tests in the continuous integration process requires some way to keep track of which customer tests were already passing, because we don’t insist that all customer tests pass before any code is committed. One option is to keep two sets of input ﬁ les, migrating tests that pass from the “still red” ﬁ le into the “all green” ﬁ le that is used for regression testing as part of the automatic build process.\n\nVariation: Data-Driven Test Framework (Fit)\n\nWe should consider using a prebuilt Data-Driven Test framework when we are using Data-Driven Tests as a test strategy. Fit is a framework originally conceived by Ward Cunningham as a way of involving business users in the automation of tests. Although Fit is typically used to automate customer tests, it can also be used for unit tests if the number of tests warrants building the necessary ﬁ x- tures. Fit consists of two parts: the framework and a user-created ﬁ xture. The Fit\n\nwww.it-ebooks.info",
      "content_length": 2733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "Data-Driven Test\n\nFramework is a generic Data-Driven Test interpreter that reads the input ﬁ le and ﬁ nds all tables in it. It looks in the top-left cell of each table for a ﬁ xture classname and then searches our test executable for that class. When it ﬁ nds a class, it creates an instance of the class and passes control to that instance as it reads each row and column of the table. We can override methods deﬁ ned by the framework to specify what should happen for each cell in the table. A Fit ﬁ xture, then, is an adapter that Fit calls to interpret a table of data and invoke methods on the SUT.\n\nThe Fit table can also contain expected results from the SUT. Fit compares the speciﬁ ed values with the actual values returned by the SUT. Unlike Asser- tion Methods (page 362) in xUnit, however, Fit does not abandon a test at the ﬁ rst value that does not match the expected value. Instead, it colors in each cell in the table, with green cells indicating actual values that matched the expected values and red cells indicating wrong or unexpected values.\n\nUsing Fit offers several advantages:\n\nThere is much less code to write than when we build our own test\n\nInterpreter [GOF].\n\nThe output makes sense to a business person, not just a technical person.\n\nThe tests don’t stop at the ﬁ rst failed assertion. Fit has a way of com- municating multiple failures/errors in a way that allows us to see the failure patterns very easily.\n\nThere are a plethora of ﬁ xture types available to subclass or use as is.\n\nSo why wouldn’t we use Fit for all our unit testing instead of xUnit? The main disadvantages of using Fit are described here:\n\nThe test scenarios need to be very well understood before we can build the Fit ﬁ xture. We then need to translate each test’s logic into a tabular representation; this isn’t always a good ﬁ t, especially for developers who are used to thinking procedurally. While it may be appropriate to have testers who can write the Fit ﬁ xtures for customer tests, this approach wouldn’t be appropriate for true unit tests unless we had close to a 1:1 tester-to-developer ratio.\n\nThe tests need to employ the same SUT interaction logic in each test.5 To run several different styles of tests, we would probably have to build one or more different ﬁ xtures for each style of test. Building a new ﬁ xture is typically more complex than writing a few Test Methods (page 348).\n\n5 The tabular data must be injected into the SUT during the ﬁ xture setup or exercise SUT phases or retrieved from the SUT during the result veriﬁ cation phase.\n\nwww.it-ebooks.info\n\n291\n\nData-Driven Test",
      "content_length": 2606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "292\n\nData-Driven Test\n\nChapter 18 Test Strategy Patterns\n\nAlthough many different ﬁ xture types are available to subclass or use as is, their use in this way is yet another thing that developers would be required to learn to do their jobs. Even then, not all unit tests are ame- nable to automation using Fit.\n\nFit tests aren’t normally integrated into developers’ regression tests that are run via xUnit. Instead, these tests must be run separately—which introduces the possibility that they will not be run at each check-in. Some teams include Fit tests as part of their continuous integration build process to partially mitigate this issue. Other teams have reported great success having a second “customer” build service or server that runs all the customer tests.\n\nEach of these issues is potentially surmountable, of course. In general, xUnit is a more appropriate framework for unit testing than Fit; the reverse is true for customer tests.\n\nVariation: Naive xUnit Test Interpreter\n\nWhen we have a small number of Data-Driven Tests that we wish to run as part of an xUnit-based Scripted Test strategy, the simplest implementation is to write a Test Method containing a loop that reads one set of input data values from the ﬁ le along with the expected results. This is the equivalent of converting a single Parameterized Test and all its callers into a Tabular Test (see Parameterized Test). As with a Tabular Test, this approach to building the Data-Driven Test interpreter will result in a single Testcase Object (page 382) with many asser- tions. This has several ramiﬁ cations:\n\nThe entire set of Data-Driven Tests will count as a single test. Hence, converting a set of Parameterized Tests into a single Data-Driven Test will reduce the count of tests executed.\n\nWe will stop executing the Data-Driven Test on the ﬁ rst failure or error. As a consequence, we will lose a lot of our Defect Localization (see page 22). Some variants of xUnit do allow us to specify that failed assertions shouldn’t abort execution of the Test Method.\n\nWe need to make sure our assertion failures tell us which subtest we\n\nwere executing when the failure occurred.\n\nWe could address the last two issues by including a try/catch statement inside the loop but surrounding the test logic and then continuing the code’s execution. Nevertheless, we still need to ﬁ nd a way to report the test results in a meaningful way (e.g., “Failed subtests 1, 3, and 6 with . . .”).\n\nwww.it-ebooks.info",
      "content_length": 2477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "Data-Driven Test\n\nTo make it easier to extend the Data-Driven Test interpreter to handle sev- eral different kinds of tests in the same data ﬁ le, we can include a “verb” or “action word” as part of each entry in the data ﬁ le. The interpreter can then dispatch to a different Parameterized Test based on the action word.\n\nVariation: Test Suite Object Generator\n\nWe can avoid the “stop on ﬁ rst failure” problem associated with a Naive xUnit Test Interpreter by having the suite method on the Test Suite Factory (see Test Enumeration on page 399) fabricate the same Test Suite Object (page 387) structure as the built-in mechanism for Test Discovery (page 393). To do so, we build a Testcase Object for each entry in the Data-Driven Test ﬁ le and ini- tialize each object with the test data for the particular test.6 That object knows how to execute the Parameterized Test with the data loaded into it when the test suite was built. This ensures that the Data-Driven Test continues execut- ing even after the ﬁ rst Testcase Object encounters an assertion failure. We can then let the Test Runner (page 377) count the tests, errors, and failures in the normal way.\n\nVariation: Test Suite Object Simulator\n\nAn alternative to building the Test Suite Object is to create a Testcase Object that behaves like one. This object reads the Data-Driven Test ﬁ le and iterates over all the tests when asked to run. It must catch any exceptions thrown by the Parameterized Test and continue executing the subsequent tests. When ﬁ nished, the Testcase Object must report the correct number of tests, failures, and errors back to the Test Runner. It also needs to implement any other meth- ods on the standard test interface on which the Test Runner depends, such as returning the number of tests in the “suite,” returning the name and status of each test in the suite (for the Graphical Test Tree Explorer, see Test Runner), and so forth.\n\nMotivating Example\n\nLet’s assume we have a set of tests as follows: def test_extref sourceXml = \"<extref id='abc' />\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<extref>\")\n\n6 This is very similar to how xUnit’s built-in Test Method Discovery (see Test Discovery) mechanism works, except that we are passing in the test data in addition to the Test Method name.\n\nwww.it-ebooks.info\n\n293\n\nData-Driven Test",
      "content_length": 2376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "294\n\nData-Driven Test\n\nChapter 18 Test Strategy Patterns\n\nend\n\ndef test_testterm_normal sourceXml = \"<testterm id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<testterm>\") end\n\ndef test_testterm_plural sourceXml = \"<testterms id='abc'/>\" expectedHtml = \"<a href='abc.html'>abcs</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<plural>\") end\n\nThe succinctness of these tests is made possible by deﬁ ning the Parameterized Test as follows:\n\ndef generateAndVerifyHtml( sourceXml, expectedHtml, message, &block) mockFile = MockFile.new sourceXml.delete!(\"\\t\") @handler = setupHandler(sourceXml, mockFile ) block.call unless block == nil @handler.printBodyContents actual_html = mockFile.output assert_equal_html( expectedHtml, actual_html, message + \"html output\") actual_html end\n\nThe main problem with these tests is that they are still written in code when, in fact, the only difference between them is the data used as input.\n\nRefactoring Notes\n\nThe solution, of course, is to extract the common logic of the Parameterized Tests into a Data-Driven Test interpreter and to collect all sets of parameters into a single data ﬁ le that can be edited by anyone. We need to write a “main” test that knows which ﬁ le to read the test data from and a bit of logic to read and parse the test ﬁ le. This logic can call our existing Parameterized Test logic and let xUnit keep track of the test execution statistics for us.\n\nExample: xUnit Data-Driven Test with XML Data File\n\nIn this example, we will use XML as our ﬁ le representation. Each test consists of a test element with three main parts:\n\nwww.it-ebooks.info",
      "content_length": 1663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "Data-Driven Test\n\nAn action that tells the Data-Driven Test interpreter which test logic to run (e.g., crossref)\n\nThe input to be passed to the SUT—in this case, the sourceXml element\n\nThe HTML we expect the SUT to produce (in the expectedHtml element)\n\nThese three components are wrapped up in a testsuite element.\n\n<testsuite id=\"CrossRefHandlerTest\"> <test id=\"extref\"> <action>crossref</action> <sourceXml> <extref id='abc'/> </sourceXml> <expectedHtml> <a href='abc.html'>abc</a> </expectedHtml> </test> <test id=\"TestTerm\"> <action>crossref</action> <sourceXml> <testterm id='abc'/> </sourceXml> <expectedHtml> <a href='abc.html'>abc</a> </expectedHtml> </test> <test id=\"TestTerm Plural\"> <action>crossref</action> <sourceXml> <testterms id='abc'/> </sourceXml> <expectedHtml> <a href='abc.html'>abcs</a> </expectedHtml> </test> </testsuite>\n\nThis XML ﬁ le could be edited by anyone with an XML editor without any concern for introducing test logic errors. All the logic for verifying the expected outcome is encapsulated by the Data-Driven Test interpreter in much the same way as it would be by a Parameterized Test. For viewing purposes we could hide the XML structure from the user by deﬁ ning a style sheet. In addition, many XML editors will turn the XML into a form-based input to simplify editing.\n\nTo avoid dealing with the complexities of manipulating XML, the interpreter\n\ncan also use a CSV ﬁ le as input.\n\nwww.it-ebooks.info\n\n295\n\nData-Driven Test",
      "content_length": 1467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "296\n\nData-Driven Test\n\nChapter 18 Test Strategy Patterns\n\nExample: xUnit Data-Driven Test with CSV Input File\n\nThe test in the previous example would look like this as a CSV ﬁ le:\n\nID, Action, SourceXml, ExpectedHtml Extref,crossref,<extref id='abc'/>,<a href='abc.html'>abc</a> TTerm,crossref,<testterm id='abc'/>,<a href='abc.html'>abc</a> TTerms,crossref,<testterms id='abc'/>,<a href='abc.html'>abcs</a>\n\nThe interpreter is relatively simple and is built on the logic we had already devel- oped for our Parameterized Test. This version reads the CSV ﬁ le and uses Ruby’s split function to parse each line.\n\ndef test_crossref executeDataDrivenTest \"CrossrefHandlerTest.txt\" end\n\ndef executeDataDrivenTest ﬁlename dataFile = File.open(ﬁlename) dataFile.each_line do | line | desc, action, part2 = line.split(\",\") sourceXml, expectedHtml, leftOver = part2.split(\",\") if \"crossref\"==action.strip generateAndVerifyHtml sourceXml, expectedHtml, desc else # new \"verbs\" go before here as elsif's report_error( \"unknown action\" + action.strip ) end end end\n\nUnless we changed the implementation of generateAndVerifyHtml to catch assertion failures and increment a failure counter, this Data-Driven Test will stop executing at the ﬁ rst failed assertion. While this behavior would be acceptable for regres- sion testing, it would not provide very good Defect Localization.\n\nExample: Data-Driven Test Using Fit Framework\n\nIf we wanted to have even more control over what the user can do, we could create a Fit “column ﬁ xture” with the columns “id,” “action,” “source XML,” and “expected Html()” and let the user edit an HTML Web page instead (Figure 18.1).\n\nwww.it-ebooks.info",
      "content_length": 1671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "Data-Driven Test\n\n[Figure 18.1: ‘CrossrefHandlerFitTest.vsd’]\n\nFigure 18.1 A Data-Driven test built using the Fit framework.\n\nWhen using Fit, the test interpreter is the Fit framework extended by the Fit ﬁ xture class speciﬁ c to the test:\n\npublic class CrossrefHandlerFixture extends ColumnFixture { // Input columns public String id; public String action; public String sourceXML;\n\n// Output columns public String expectedHtml() { return generateHtml(sourceXML); } }\n\nThe methods of this ﬁ xture class are called by the Fit framework for each cell in each line in the Fit table based on the column headers. Simple names are interpreted as the instance variable of the ﬁ xture (e.g., “id,” “source XML”). Column names ending in “()” signify a function that Fit calls and then compares its result with the contents of the cell.\n\nThe resulting output is shown in Figure 18.2. This colored-in table allows us\n\nto get an overview of the results of running one ﬁ le of tests at a single glance.\n\nFigure 18.2 The results of executing the Fit test.\n\nwww.it-ebooks.info\n\n297\n\nData-Driven Test",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "298\n\nTest Automation Framework\n\nChapter 18 Test Strategy Patterns\n\nTest Automation Framework\n\nHow do we make it easy to write and run tests written by different people?\n\nWe use a framework that provides all the mechanisms needed to run the test logic so the test writer needs to provide only the test-speciﬁ c logic.\n\nTest Automation Framework Test Automation Framework\n\nFixture Fixture\n\nTest Runner Test Runner\n\nSetup Setup\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nInputs Inputs\n\nExpected Expected Outputs Outputs\n\nTest Test Script 1 Script 1\n\nTest Test Script 2 Script 2\n\nTest Test Script n Script n\n\nTest Automation Infrastructure Test Automation Infrastructure\n\nWriting and running automated tests involves several steps, but many of these steps are the same for every test. If every test had to include an implementation of these steps, writing automated tests would be very tedious, time-consuming, prone to errors, and expensive.\n\nUsing a Test Automation Framework is a way to minimize the effort of writing\n\nFully Automated Tests (see page 26).\n\nHow It Works\n\nWe build a framework that implements all the mechanisms required to run suites of tests and record the results. These mechanisms include the ability to ﬁ nd in- dividual tests, assemble them into a test suite, execute each test in turn, verify expected outcomes, collect and report any test failures or errors, and clean up when failures or errors do occur. The framework provides a way to plug in and run the test-speciﬁ c behavior that test automaters write.\n\nwww.it-ebooks.info",
      "content_length": 1551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "Test Automation Framework\n\nWhy We Do This\n\nBuilding Fully Automated Tests that are repeatable and robust is a much more complicated process than just writing a test script that invokes the SUT. We need to handle success cases and error cases, both expected and unexpected. We need to set up and tear down test ﬁ xtures. We need to specify which test(s) to run. We also need to report on the results after we have run a suite of tests.\n\nThe amount of effort required to build Fully Automated Tests can act as a serious deterrent to automation of tests. We can reduce the cost of getting started signiﬁ cantly by providing a framework that implements the most com- mon functionality—the only entry cost is then incurred while learning to use the framework. This cost, in turn, can be reduced if the framework implements a common protocol such as xUnit that makes it easier for us to learn a second or third framework once we have experience with the ﬁ rst.\n\nUsing a framework also helps isolate the implementation of the logic re- quired to run the tests from the logic of the tests. This approach can help reduce Test Code Duplication (page 213) and minimize the occurrence of Obscure Tests (page 186). It also ensures that test written by different test automaters can be run easily in a single test run with a single report on the test results.\n\nImplementation Notes\n\nMany kinds of Test Automation Frameworks are available, from both com- mercial vendors and open-source resources. They can be classiﬁ ed into two main categories: “robot user” test tools and Scripted Tests (page 285). The latter category can be further subdivided into the xUnit and Data-Driven Tests (page 288) families of Test Automation Frameworks.\n\nVariation: Robot User Test Frameworks\n\nA large number of third-party test automation tools are designed to test applica- tions via the user interface. Most of them use the “record and playback” test metaphor. This metaphor leads to some very seductive marketing materials, because it makes test automation seem as simple as running some tests manu- ally while recording the test session. Such a robot user test tool consists of two major parts: the “test recorder,” which monitors and records the interactions between the user and the SUT, and the “test runner,” which executes the Recorded Tests (page 278). Most of these test automation tools are also frameworks that support a number of “widget recognizer” plug-ins. Most commercial tools come with a gaggle of built-in widget recognizers.\n\nwww.it-ebooks.info\n\n299\n\nTest Automation Framework",
      "content_length": 2566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "300\n\nTest Automation Framework\n\nChapter 18 Test Strategy Patterns\n\nVariation: The xUnit Family of Test Automation Frameworks\n\nMost unit-testing tools belong to the xUnit family of testing frameworks designed for automating Hand-Scripted Tests (see Scripted Test). xUnit has been ported to (or developed from scratch for) most current programming languages. The xUnit family of unit-testing frameworks consists of several major components. The most visible is the Test Runner (page 377), which can be invoked either from the command line or as a Graphical Test Runner (see Test Runner). It builds the Testcase Objects (page 382), collects them into Test Suite Objects (page 387), and invokes each of the Test Methods (page 348). The other major component of the xUnit frameworks is the library of built-in Assertion Methods (page 362) that are used within the Test Methods to specify the expected outcome of each test.\n\nVariation: Data-Driven Test Frameworks\n\nA Data-Driven Test framework provides a way to plug in interpreters that know how to execute a speciﬁ c kind of test step. This ﬂ exibility, in effect, extends the format of the input ﬁ le with new “verbs” and objects. Such a framework also provides a test runner that reads in the ﬁ le, passes control to the plug-ins when their corresponding data formats are encountered, and keeps track of statistics for the test run. The most notable member of the Data-Driven Test Frameworks family is Fit, which enables test automaters to write tests in tabular form and to “plug in” ﬁ xture classes that know how to interpret speciﬁ c formats of tables.\n\nExample: Test Automation Framework\n\nThe Test Automation Framework looks somewhat different for each of the possible ways to automate tests. To see these variations, refer to Recorded Test, Scripted Test, and Data-Driven Test for examples of the respective Test Auto- mation Frameworks.\n\nFurther Reading\n\nSome of the more popular examples of Test Automation Frameworks for xUnit are JUnit (Java), SUnit (Smalltalk), CppUnit (C++), NUnit (all .NET languages), runit (Ruby), PyUnit (Python), and VbUnit (Visual Basic). A more complete and up-to-date list can be found at http://xprogramming.com, along with a list of the available extensions (e.g., HttpUnit, Cactus).\n\nwww.it-ebooks.info",
      "content_length": 2289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "Test Automation Framework\n\nOther open-source Test Automation Frameworks include Fit, Canoo Web- Test, and Watir. Commercial Test Automation Frameworks include QTP, BPT, and eCATT, among many others.\n\nIn Test-Driven Development—By Example [TDD-BE], Kent Beck illustrates TDD by building a Test Automation Framework in Python. In an approach he likens to “doing brain surgery on yourself,” he uses the emerging Test Automa- tion Framework to run the tests he writes for each new capability. This applica- tion is a very good example of both TDD and bootstrapping.\n\nwww.it-ebooks.info\n\n301\n\nTest Automation Framework",
      "content_length": 613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "302\n\nMinimal Fixture\n\nAlso known as: Minimal Context\n\nChapter 18 Test Strategy Patterns\n\nMinimal Fixture\n\nWhich ﬁ xture strategy should we use?\n\nWe use the smallest and simplest ﬁ xture possible for each test.\n\nFixture Fixture\n\nsetUp setUp\n\ntest_1 test_1\n\nFixture Fixture\n\ntest_2 test_2\n\nSUT SUT\n\ntest_n test_n\n\nTestcase Class Testcase Class\n\nEvery test needs some kind of test ﬁ xture. A key part of understanding a test is understanding the test ﬁ xture and recognizing how it inﬂ uences the expected outcome of the test. Tests are much easier to understand if the ﬁ xture is small and simple.\n\nWhy We Do This\n\nA Minimal Fixture is important for achieving Tests as Documentation (see page 23) and for avoiding Slow Tests (page 253). A test that uses a Minimal Fixture will always be easier to understand than one that uses a ﬁ xture contain- ing unnecessary or irrelevant information. This is true whether we are using a Fresh Fixture (page 311) or a Shared Fixture (page 317), although the effort to build a Minimal Fixture is typically higher with a Shared Fixture because it must be designed to handle several tests. Deﬁ ning a Minimal Fixture is much easier for a Fresh Fixture because it need serve only a single test.\n\nwww.it-ebooks.info",
      "content_length": 1245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "Minimal Fixture\n\nImplementation Notes\n\nWe design a ﬁ xture that includes only those objects that are absolutely necessary to express the behavior that the test veriﬁ es. Another way to phrase this is “If the object is not important to understand the test, it is important not to include it in the ﬁ xture.”\n\nTo build a Minimal Fixture, we ruthlessly remove anything from the ﬁ xture that does not help the test communicate how the SUT should behave. Two forms of “minimization” can be considered:\n\nWe can eliminate objects entirely. That is, we don’t even build the objects as part of the ﬁ xture. If the object isn’t necessary to prove something about how the SUT behaves, we don’t include it at all.\n\nWe can hide unnecessary attributes of the object when they don’t con-\n\ntribute to the understanding of the expected behavior.\n\nA simple way to ﬁ nd out whether an object is necessary as part of the ﬁ xture is to remove it. If the test fails as a result, the object was probably necessary in some way. Of course, it may have been necessary only as an argument to some method we are not interested in or as an attribute that is never used (even though the object to which the attribute belongs is required for some reason). Including these kinds of objects as part of ﬁ xture setup deﬁ nitely contributes to Obscure Tests (page 186). We can eliminate these unnecessary objects in one of two ways: (1) by hiding them or (2) by eliminating the need for them by passing in Dummy Objects (page 728) or using Entity Chain Snipping (see Test Stub on page 529). If the SUT actually accesses the object as it is executing the logic under test, however, we may be forced to include the object as part of the test ﬁ xture.\n\nHaving determined that the object is necessary for the execution of the test, we must now ask whether the object is helpful in understanding the test. If we were to initialize it “off-stage,” would that make it harder to understand the test? Would the object lead to an Obscure Test by acting as a Mystery Guest (see Obscure Test)? If so, we want to keep the object visible. Boundary values are a good example of a case in which we do want to keep the objects and at- tributes that take on the boundary values visible.\n\nIf we have established that the object or attribute isn’t necessary for understanding the test, we should make every effort to eliminate it from the Test Method (page 348), albeit not necessarily from the test ﬁ xture. Creation Methods (page 415) are a common way of achieving this goal. We can hide the attributes of objects that don’t affect the outcome of the test but that are needed for construction of the object by using Creation Methods to ﬁ ll in all\n\nwww.it-ebooks.info\n\n303\n\nMinimal Fixture",
      "content_length": 2736,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "304\n\nMinimal Fixture\n\nChapter 18 Test Strategy Patterns\n\nthe “don’t care” attributes with meaningful default values. We can also hide the creation of necessary depended-on objects within the Creation Methods. A good example of this occurs when we write tests that require badly formed objects as input (for testing the SUT with invalid inputs). In this case we don’t want to confuse the issue by showing all valid attributes of the object being passed to the SUT; there could be many of these extraneous attributes. Instead, we want to focus on the invalid attribute. To do so, we can use the One Bad Attribute pattern (see Derived Value on page 718) to build malformed objects with a minimum of code by calling a Creation Method to construct a valid object and then replacing a single attribute with the invalid value that we want to verify the SUT will handle correctly.\n\nwww.it-ebooks.info",
      "content_length": 892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "Standard Fixture\n\nStandard Fixture\n\nWhich ﬁ xture strategy should we use?\n\nWe reuse the design of the text ﬁ xture across the many tests.\n\nSetup Setup\n\nExercise Exercise\n\nStandard Fixture Standard Fixture Setup Logic Setup Logic\n\nFixture Fixture\n\nVerify Verify\n\nSUT SUT\n\nSetup Setup Teardown Teardown\n\nExercise Exercise\n\nFixture Fixture\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nTo execute an automated test, we require a text ﬁ xture that is well understood and completely deterministic. Designing a custom test ﬁ xture for each test requires extra effort. A Standard Fixture offers a way to reuse the same ﬁ xture design in several tests without necessarily sharing the same ﬁ xture instance.\n\nHow It Works\n\nA Standard Fixture is more about attitude than about technology. It requires us to decide early on in the testing process that we will design a Standard Fixture that can be used by several or many tests rather than mining a common ﬁ xture from tests that were designed independently. In a sense, a Standard Fixture is the result of “Big Design Upfront” of the test ﬁ xture for a whole suite of tests. We then deﬁ ne our speciﬁ c tests using this common test ﬁ xture design.\n\nThe choice of a Standard Fixture is independent of the choice between a Fresh Fixture (page 311) and a Shared Fixture (page 317). A Shared Fixture is, by deﬁ nition, a Standard Fixture. The reverse is not true, however, because a Standard Fixture focuses on reuse of the ﬁ xture’s design—not the time when the ﬁ xture is built or its visibility. Having chosen to use a Standard Fixture, we still need to decide whether each test will build its own instance of the Standard\n\nwww.it-ebooks.info\n\n305\n\nStandard Fixture\n\nAlso known as: Standard Context",
      "content_length": 1735,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "306\n\nStandard Fixture\n\nChapter 18 Test Strategy Patterns\n\nFixture (a Fresh Fixture) or whether we will build it once as a Shared Fixture and reuse it across many tests.\n\nWhen to Use It\n\nWhen I was reviewing an early draft of this book with Series Editor Martin Fowler, he asked me, “Do people actually do this?” This question exempliﬁ es the philosophical divide of ﬁ xture design. Coming from an agile background, Martin lets each test pull a ﬁ xture into existence. If several tests happen to need the same ﬁ xture, then it makes sense to factor it out into the setUp method and split the class into one Testcase Class per Fixture (page 631). It doesn’t even occur to Martin to design a Standard Fixture that all tests can use. So who uses them? Standard Fixtures are something of a tradition in the testing (quality assess- ment) community. It is very commonplace to deﬁ ne a large Standard Fixture that is then used as a test bed for testing activities. This approach makes a lot of sense in the context of manual execution of many customer tests because it eliminates the need for each tester to spend a lot of time setting up the test environment for each customer test and it allows several testers to work in the same test environ- ment at the same time. Some test automaters also use Standard Fixtures when deﬁ ning their automated customer tests. This strategy is especially prevalent when test automaters use a Shared Fixture, for obvious reasons.\n\nIn the xUnit community, use of a Standard Fixture simply to avoid designing a Minimal Fixture (page 302) for each test is considered undesirable and has been given the name General Fixture (see Obscure Test on page 186). A more accepted example is the use of Implicit Setup (page 424) in conjunction with Testcase Class per Fixture because only a few Test Methods (page 348) share the design of the ﬁ xture and they do so because they need the same design. As we make a Standard Fixture more reusable across many tests with disparate needs, it tends to grow larger and more complex. This trend can lead to a Fragile Fixture (see Fragile Test on page 239) as the needs of new tests introduce changes that break existing clients of the Standard Fixture. Depending on how we go about building the Standard Fixture, we may also ﬁ nd ourselves entertaining a Mystery Guest (see Obscure Test) if the cause–effect relationships between the ﬁ xture and out- come are not easy to discern either because the ﬁ xture setup is hidden from the test or because it is not clear which characteristics of the referenced part of the Standard Fixture serve as pre-conditions for the test.\n\nA Standard Fixture will also take longer to build than a Minimal Fixture because there is more ﬁ xture to construct. When we are building a Fresh Fixture for each Testcase Object (page 382), this effort can lead to Slow Tests (page 253), especially if the ﬁ xture setup involves a database. (See the sidebar “Unit Test Rulz”\n\nwww.it-ebooks.info",
      "content_length": 2975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "Standard Fixture\n\nUnit Test Rulz\n\nMichael Feathers of Object Mentor writes:\n\nI’ve used these rules with a large number of teams. They encour- age good design and rapid feedback and they seem to help teams avoid a lot of trouble.\n\nA test is not a unit test if:\n\nIt talks to the database.\n\nIt communicates across the network.\n\nIt touches the ﬁ le system.\n\nIt can’t run correctly at the same time as any of your other unit\n\ntests.\n\nYou have to do special things to your environment (such as\n\nediting conﬁ g ﬁ les) to run it.\n\nTests that do these things aren’t bad. Often they are worth writ- ing, and they can be written in a unit test harness. However, it is important to be able to separate them from true unit tests so that we can keep a set of tests that we can run fast whenever we make our changes.\n\nhttp://www.objectmentor.com\n\nfor an opinion about what kinds of behavior are acceptable for a unit test.) For these reasons, we may be better off using a Minimal Fixture to avoid the extra ﬁ xture setup overhead associated with creating objects that are only needed in other tests.\n\nImplementation Notes\n\nAs mentioned earlier, we can use a Standard Fixture as either a Fresh Fixture or a Shared Fixture, and we can set it up using either Implicit Setup or Delegated Setup (page 411).7 When using it as a Fresh Fixture, we can deﬁ ne a Test Utility Method (page 599) (function or procedure) that builds the Standard Fixture; we can then call the Test Utility Method from each test that needs this particular design of ﬁ xture. Alternatively, we can take advantage of xUnit support for Implicit Setup by putting all of the ﬁ xture construction logic in the setUp method.\n\n7 Doing it with In-line Setup (page 408) would be silly—we would have to copy the code to construct the Standard Fixture to every Test Method.\n\nwww.it-ebooks.info\n\n307\n\nStandard Fixture",
      "content_length": 1858,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "308\n\nStandard Fixture\n\nChapter 18 Test Strategy Patterns\n\nWhen building a Standard Fixture for use as a Shared Fixture, we can employ any of the Shared Fixture setup patterns including Suite Fixture Setup (page 441), Lazy Setup (page 435), and Setup Decorator (page 447).\n\nMotivating Example\n\nAs mentioned earlier, we are most likely to end up using a Standard Fixture because we started that way—and we probably started that way as the result of the background of one of the project participants. We probably would not refactor our tests to use a Standard Fixture when those tests are already written to use a Minimal Fixture unless we were refactoring to create a Testcase Class per Fixture. For the sake of illustration, let’s assume that we did want to get to “here” from “there.” The following example uses Creation Methods (page 415) to build a custom Fresh Fixture for each test:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight_c() throws Exception { FlightDto outboundFlight = createOneOutboundFlightDto(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights_c() throws Exception { FlightDto[] outboundFlights = createTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nTo keep this test short, we have used Delegated Setup to populate the SUT with the Minimal Fixture needed for each test. We could have included the ﬁ xture setup code in-line in each method, but that choice would take us down the road toward an Obscure Test.\n\nwww.it-ebooks.info",
      "content_length": 1899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "Standard Fixture\n\nRefactoring Notes\n\nTechnically speaking, converting a pile of tests to a Standard Fixture isn’t really a “refactoring” because we actually change the behavior of these tests. The biggest challenge is designing the reusable Standard Fixture in such a way that each Test Method can ﬁ nd some part of the ﬁ xture that serves its needs. This means synthe- sizing all of the individual purpose-built Minimal Fixtures into a single “jack of all trades” ﬁ xture. Not surprisingly, this reworking of the code can be a nontrivial exercise when we have a lot of tests.\n\nThe easy and mechanical part of the refactoring is to convert the logic in each test that constructs the ﬁ xture into calls to Finder Methods (see Test Utility Method) that retrieve the appropriate part of the Standard Fixture. This transfor- mation is most easily done as a series of steps. First, we extract the in-line ﬁ xture construction logic in each Test Method into one or more Creation Methods with Intent-Revealing Names [SBPP]. Next, we do a global replace on the “create” part of each call to “ﬁ nd.” Finally, we generate (either manually or using our IDE’s “quick ﬁ x” capability) the Finder Methods needed to get the calls to compile. Inside each Finder Methods we add in code to return the relevant part of the Standard Fixture.\n\nExample: Standard Fixture\n\nHere’s the example given earlier converted to use a Standard Fixture:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { setupStandardAirportsAndFlights(); FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { setupStandardAirportsAndFlights(); FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome\n\nwww.it-ebooks.info\n\n309\n\nStandard Fixture",
      "content_length": 2163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "310\n\nStandard Fixture\n\nChapter 18 Test Strategy Patterns\n\nassertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nTo make the use of a Standard Fixture really obvious, this example shows a Fresh Fixture that is created explicitly in each test by calling the same Creation Method to set up the Standard Fixture (i.e., using Delegated Setup). We could have achieved the same effect by putting the ﬁ xture construction logic into the setUp method, thus using Implicit Setup. The resulting test would look identical to one that uses a Shared Fixture.\n\nwww.it-ebooks.info",
      "content_length": 600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "Fresh Fixture\n\nFresh Fixture\n\nWhich ﬁ xture strategy should we use?\n\nEach test constructs its own brand-new test ﬁ xture for its own private use.\n\nSetup Setup\n\nFixture Fixture\n\nExercise Exercise\n\nSetup Setup\n\nVerify Verify\n\nFixture Fixture\n\nSUT SUT\n\nExercise Exercise\n\nTeardown Teardown\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nEvery test needs a test ﬁ xture. It deﬁ nes the state of the test environment before the test. The choice of whether to build the ﬁ xture from scratch each time the test is run or to reuse a ﬁ xture built earlier is a key test automation decision.\n\nWhen each test creates a Fresh Fixture, Erratic Tests (page 228) are less likely and the testing effort is more likely to result in Tests as Documentation (see page 23).\n\nHow It Works\n\nWe design and build the test ﬁ xture such that only a single run of a single test will use it. We construct the ﬁ xture as part of running the test and tear down the ﬁ xture when the test has ﬁ nished. We do not reuse any ﬁ xture left over by other tests or other test runs. This way, we start and end every test with a “clean slate.”\n\nwww.it-ebooks.info\n\n311\n\nFresh Fixture\n\nAlso known as: Fresh Context, Private Fixture",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "312\n\nFresh Fixture\n\nChapter 18 Test Strategy Patterns\n\nWhen to Use It\n\nWe should use a Fresh Fixture whenever we want to avoid any interdependencies between tests that can result in Erratic Tests such as Lonely Tests (see Erratic Test) or Interacting Tests (see Erratic Test). If we cannot use a Fresh Fixture because it slows the tests down too much, we should consider using an Immutable Shared Fixture (see Shared Fixture on page 317) before resorting to a Shared Fixture. Note that using a Database Partitioning Scheme (see Database Sandbox on page 650) to create a private Database Sandbox for the test that no other tests will touch does not result in a Fresh Fixture because subsequent test runs could use the same ﬁ xture.\n\nImplementation Notes\n\nA ﬁ xture is considered a Fresh Fixture if we intend to use it a single time. Whether the Fresh Fixture is transient or persistent depends on the nature of the SUT and how the tests are written (Figure 18.3). While the intent is the same, the implemen- tation considerations are somewhat different when the Fresh Fixture is persistent. Fixture setup is largely unaffected, so it is discussed as a feature common to all such ﬁ xtures. Fixture teardown is speciﬁ c to the particular variation.\n\nTransient Transient\n\nFresh Fresh Fixture Fixture\n\nPersistent Persistent\n\nShared Shared Fixture Fixture\n\nImmutable Immutable Shared Shared Fixture Fixture\n\nFigure 18.3 Test ﬁ xture strategies. A ﬁ xture can be either Fresh, Shared, or a combination of the two (the immutable Shared Fixture) based on whether some, or all, of it persists between tests.\n\nwww.it-ebooks.info",
      "content_length": 1617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "Fresh Fixture\n\nWhy Does a Fixture Persist?\n\nThe ﬁ xture we construct may hang around after the Test Method (page 348) has ﬁ nished executing for one of two reasons. First, if the ﬁ xture primarily consists of the state of some other objects or components on which the SUT depends, its persistence is determined by whether those other objects are them- selves persistent. A database is one such beast. That’s because as soon as some code persists the ﬁ xture objects into a database, the objects “hang around” long after our test is done. Their existence in the database opens the door to collisions between multiple runs of our own test (Unrepeatable Test; see Erratic Test). Other tests may also be able to access those objects, which can result in other forms of Erratic Tests such as Interacting Tests and Test Run Wars. If we must use a database or other form of object persistence, we should take extra steps to keep the ﬁ xture private. In addition, we should tear down the ﬁ xture after each Test Method.\n\nThe second reason that a ﬁ xture might persist lies within the control of our tests—namely, which kind of variable we choose to hold the reference to the ﬁ xture. Local variables naturally go out of scope when the Test Method ﬁ nishes executing; therefore any ﬁ xture held in a local variable will be destroyed by garbage collection. Instance variables go out of scope when the Testcase Object is destroyed8 and require explicit teardown only if the xUnit framework doesn’t recreate the Testcase Objects during each test run. By contrast, class variables usually result in persistent ﬁ xtures that can outlive a single test method or even a test run and should therefore be avoided when using a Fresh Fixture.\n\nIn practice, our ﬁ xture will not normally be persistent in unit tests9 unless we have tightly coupled our application logic to the database. A ﬁ xture is more likely to be persistent when we are writing customer tests or possibly compo- nent tests.\n\nFresh Fixture Setup\n\nConstruction of the ﬁ xture is largely unaffected by whether it is persistent or tran- sient. The primary consideration is the location of the code to set up the ﬁ xture. We can use In-line Setup (page 408) if the ﬁ xture setup is relatively simple. For more complex ﬁ xtures, we generally prefer using Delegated Setup (page 411) when our\n\n8 Most members of the xUnit family create a separate Testcase Object (page 382) for each Test Method. A few do not, however. This difference can trip up unwary test automaters when they ﬁ rst start using these members of the family because instance variables may unexpectedly act like class variables. For a detailed description of this issue, see the sidebar “There’s Always an Exception” (page 384). 9 The sidebar “Unit Test Rulz” (page 307) explains what constitutes a unit test.\n\nwww.it-ebooks.info\n\n313\n\nFresh Fixture",
      "content_length": 2858,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "314\n\nFresh Fixture\n\nChapter 18 Test Strategy Patterns\n\nTest Methods are organized using Testcase Class per Class (page 617) or Testcase Class per Feature (page 624). We can use Implicit Setup (page 424) to build the ﬁ xture if we have used the Testcase Class per Fixture (page 631) organization.\n\nVariation: Transient Fresh Fixture\n\nIf we need to refer to the ﬁ xture from several places in the test, we should use only local variables or instance variables to refer to the ﬁ xture. In most cases we can depend on Garbage-Collected Teardown (page 500) to destroy the ﬁ xture without any effort on our part.\n\nNote that a Standard Fixture (page 305) can also be a Fresh Fixture if the ﬁ xture is built from scratch before each Test Method is run. This approach reuses the design of the ﬁ xture rather than the instance. It is commonly encountered when we use Implicit Setup but we are not using Testcase Class per Fixture to organize our Test Methods.\n\nVariation: Persistent Fresh Fixture\n\nIf we do end up using a Persistent Fresh Fixture, either we need to tear down the ﬁ xture or we need to take special measures to avoid the need for its teardown. We can tear down the ﬁ xture using In-line Teardown (page 509), Implicit Tear- down (page 516), Delegated Teardown (see In-line Teardown), or Automated Teardown (page 503) to leave the test environment in the same state as when we entered it.\n\nTo avoid ﬁ xture teardown, we can use a Distinct Generated Value (see Generated Value on page 723) for each ﬁ xture object that must be unique. This strategy can become the basis of a Database Partitioning Scheme that seeks to isolate the tests and test runners from one another. It would prevent Resource Leakage (see Erratic Test) in case our teardown process fails. We can also combine this approach with one of the teardown patterns to be doubly sure that no Unrepeatable Tests or Interacting Tests exist.\n\nNot surprisingly, this additional work has some drawbacks: It makes tests more complicated to write and it often leads to Slow Tests (page 253). A natural reaction is to take advantage of the persistence of the ﬁ xture by reusing it across many tests, thereby avoiding the overhead of setting it up and tearing it down. Unfortunately, this choice has many undesirable ramiﬁ cations because it violates one of our major principles: Keep Tests Independent (see page 42). The result- ing Shared Fixture invariably leads to Interacting Tests and Unrepeatable Tests, if not immediately, then at some point down the road. We should not venture down this road without fully understanding the consequences!\n\nwww.it-ebooks.info",
      "content_length": 2623,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "Fresh Fixture\n\nMotivating Example\n\nHere’s an example of a Shared Fixture:\n\nstatic Flight ﬂight; public void setUp() { if (ﬂight == null) { // Lazy SetUp Airport departAirport = new Airport(\"Calgary\", \"YYC\"); Airport destAirport = new Airport(\"Toronto\", \"YYZ\"); ﬂight = new Flight( ﬂightNumber, departAirport, destAirport); } }\n\npublic void testGetStatus_inital_S() { // implicit setup // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // teardown } public void testGetStatus_cancelled() { // implicit setup partially overridden ﬂight.cancel(); // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // teardown }\n\nBased on the code that actually sets up the ﬁ xture as shown here, it is a normal Shared Fixture, but we could have just as easily used a Prebuilt Fixture (page 429) for this motivating example. Either way, these tests could start interacting at any time.\n\nRefactoring Notes\n\nSuppose we are using a Shared Fixture (same design, single copy) and decide to refactor it to use a Fresh Fixture. We can start by refactoring the test to use a fresh Standard Fixture (same design, many copies). Then we can decide whether we want to further evolve the test so that it builds a Minimal Fixture (page 302) by pruning the ﬁ xture setup logic to the bare minimum using a Minimize Data (page 738) refactoring. This point would also be good time to group Test Methods that need the same type of test ﬁ xture into a Testcase Class per Fixture and use Implicit Setup; this use of a Standard Fixture would reduce the number of Minimal Fixtures we need to design and build.\n\nwww.it-ebooks.info\n\n315\n\nFresh Fixture",
      "content_length": 1686,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "316\n\nFresh Fixture\n\nChapter 18 Test Strategy Patterns\n\nExample: Fresh Fixture\n\nHere’s the same test converted to a Fresh Fixture to avoid any possibility of Interacting Tests:\n\npublic void testGetStatus_inital() { // setup Flight ﬂight = createAnonymousFlight(); // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // teardown // garbage-collected }\n\npublic void testGetStatus_cancelled2() { // setup Flight ﬂight = createAnonymousCancelledFlight(); // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // teardown // garbage-collected }\n\nNote the use of Anonymous Creation Methods (see Creation Method on page 415) to construct the appropriate state Flight object in each test.\n\nwww.it-ebooks.info",
      "content_length": 774,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "Shared Fixture\n\nShared Fixture\n\nHow can we avoid Slow Tests? Which ﬁ xture strategy should we use?\n\nWe reuse the same instance of the test ﬁ xture across many tests.\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nExercise Exercise\n\nVerify Verify\n\nSUT SUT\n\nTeardown Teardown\n\nVerify Verify\n\nTeardown Teardown\n\nTo execute an automated test, we require a text ﬁ xture that is well understood and completely deterministic. Setting up a Fresh Fixture (page 311) can be time- consuming, especially when we are dealing with complex system state stored in a test database.\n\nWe can make our tests run faster by reusing the same ﬁ xture for several or\n\nmany tests.\n\nHow It Works\n\nThe basic concept is pretty simple: We create a Standard Fixture (page 305) ﬁ xture that outlasts the lifetime of a single Testcase Object (page 382). This approach allows multiple tests to reuse the same test ﬁ xture without destroying that ﬁ xture and recreating it between tests. A Shared Fixture can be either a Prebuilt Fixture that is reused by one or more tests in many test runs or a ﬁ xture that is created by one test and reused by another test within the same test run. In either case, the key consideration is that many tests do not create their own ﬁ xtures but rather reuse a ﬁ xture “left over” from some other activity. The tests run faster because they have less ﬁ xture setup to perform, which may result in the test automater having to do less work to deﬁ ne the ﬁ xture for each test.\n\nwww.it-ebooks.info\n\n317\n\nShared Fixture\n\nAlso known as: Shared Context, Leftover Fixture, Reused Fixture, Stale Fixture",
      "content_length": 1598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "318\n\nShared Fixture\n\nChapter 18 Test Strategy Patterns\n\nWhen to Use It\n\nRegardless of why we use them, Shared Fixtures come with some baggage that we should understand before we head down this path. The major issue with a Shared Fixture is that it can lead to interactions between tests, possibly resulting in Erratic Tests (page 228) if some tests depend on the outcomes of other tests. Another potential problem is that a ﬁ xture designed to serve many tests is bound to be much more complicated than the Minimal Fixture (page 302) needed for a single test. This greater complexity will typically take more effort to design and can lead to a Fragile Fixture (see Fragile Test on page 239) later on down the road when we need to modify the ﬁ xture.\n\nA Shared Fixture will often result in an Obscure Test (page 186) because the ﬁ xture is not constructed inside the test. This potential disadvantage can be mitigated by using Finder Methods (see Test Utility Method on page 599) with Intent-Revealing Names [SBPP] to access the relevant parts of the ﬁ xture.\n\nThere are some valid reasons for using a Shared Fixture and some misguided ones. Many of the variations have been devised primarily to mitigate the negative consequences of using a Shared Fixture. So, what are good reasons for using a Shared Fixture?\n\nVariation: Slow Tests\n\nWe can use a Shared Fixture when we cannot afford to build a new Fresh Fixture for each test. Typically, this scenario will occur when it takes too much processing to build a new ﬁ xture for each test, which often leads to Slow Tests (page 253). It most commonly occurs when we are testing with real test databases due to the high cost of creating each of the records. This growth in overhead tends to be exacerbated when we use the API of the SUT to create the reference data, because the SUT often does a lot of input validation, which may involve reading some of the just-written records.\n\nA better solution is to make the tests run faster by not interacting with the database at all. For a more complete list of options, see the solutions to Slow Tests and the sidebar “Faster Tests Without Shared Fixtures” (page 319).\n\nwww.it-ebooks.info",
      "content_length": 2178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "Shared Fixture\n\nFaster Tests Without Shared Fixtures\n\nThe ﬁ rst reaction to Slow Tests (page 253) is often to switch to a Shared Fixture (page 317) approach. Several other solutions are available, how- ever. This sidebar describes some experiences on several projects.\n\nFake Database On one of our early XP projects, we wrote a lot of tests that accessed the database. At ﬁ rst we used a Shared Fixture. When we encountered Interacting Tests (see Erratic Test on page 228) and later Test Run Wars (see Erratic Test), however, we changed to a Fresh Fixture (page 311) approach. Because these tests needed a fair bit of reference data, they were taking a long time to run. On average, for every read or write the SUT did to or from the database, each test did several more. It was tak- ing 15 minutes to run the full test suite of several hundred tests, which greatly impeded our ability to integrate our work quickly and often.\n\nAt the time, we were using a data access layer to keep the SQL out of our code. We soon discovered that it allowed us to replace the real data- base with a functionally equivalent Fake Database (see Fake Object on page 551). We started out by using simple HashTables to store the objects against a key. This approach allowed us to run many of our simpler tests “in memory” rather than against the database. And that bought us a sig- niﬁ cant drop in test execution time.\n\nOur persistence framework supported an object query interface. We were able to build an interpreter of the object queries that ran against our HashTable database implementation and that allowed the majority of our tests to work entirely in memory. On average, our tests ran about 50 times faster in memory than with the database. For example, a test suite that took 10 minutes to run with the database took 10 seconds to run in memory.\n\nThis approach was so successful that we have reused the same testing infrastructure on many of our subsequent projects. Using the faked-out persistence framework also means we don’t have to bother with building a “real database” until our object models stabilize, which can be several months into the project.\n\nIncremental Speedups Ted O’Grady and Joseph King are agile team leads on a large (50-plus developers, subject matter experts, and testers) eXtreme Programming project. Like many project teams building database-centric applications,\n\nContinued...\n\nwww.it-ebooks.info\n\n319\n\nShared Fixture",
      "content_length": 2434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "320\n\nShared Fixture\n\nChapter 18 Test Strategy Patterns\n\nthey suffered from Slow Tests. But they found a way around this problem: As of late 2005, their check-in test suite ran in less than 8 minutes com- pared to 8 hours for a full test run against the database. That is a pretty impressive speed difference. Here is their story:\n\nCurrently we have about 6,700 tests that we run on a regular basis. We’ve actually tried a few things to speed up the tests and they’ve evolved over time.\n\nIn January 2004, we were running our tests directly against a database via Toplink.\n\nIn June 2004, we modiﬁ ed the application so we could run tests against an in-memory, in-process Java database (HSQL). This cut the time to run in half.\n\nIn August 2004, we created a test-only framework that allowed Toplink to work without a database at all. That cut the time to run all the tests by a factor of 10.\n\nIn July 2005, we built a shared “check-in” test execution server that allowed us to run tests remotely. This didn’t save any time at ﬁ rst but it has proven to be quite useful nonetheless.\n\nIn July 2005, we also started using a clustering framework that al- lowed us to run tests distributed across a network. This cut the time to run the tests in half.\n\nIn August 2005, we removed the GUI and Master Data (reference data crud) tests from the “check-in suite” and ran them only from Cruise Control. This cut the time to run by approximately 15% to 20%.\n\nSince May 2004, we have also had Cruise Control run all the tests against the database at regular intervals. The time it takes Cruise Control to complete [the build and run the tests] has grown with the number of tests from an hour to nearly 8 hours now.\n\nWhen a threshold has been met that prevents the developers from (a) running [the tests] frequently when developing and (b) creat- ing long check-in queues as people wait for the token to check in, we have adapted by experimenting with new techniques. As a rule we try to keep the running of the tests under 5 minutes, with any- thing over 8 minutes being a trigger to try something new.\n\nWe have resisted thus far the temptation to run only a subset of the tests and instead focused on ways to speed up running all the tests—although as you can see, we have begun removing the tests\n\nwww.it-ebooks.info",
      "content_length": 2302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "Shared Fixture\n\ndevelopers must run continuously (e.g., Master Data and GUI test suites are not required to check in, as they are run by Cruise Control and are areas that change infrequently).\n\nTwo of the most interesting solutions recently (aside from the in- memory framework) are the test server and the clustering frame- work.\n\nThe test server (named the “check-in” box here) is actually quite useful and has proven to be reliable and robust. We bought an Opteron box that is roughly twice as fast as the development boxes (really, the fastest box we could ﬁ nd). The server has an account set up for each development machine in the pit. Using the UNIX tool rsynch, the Eclipse workspace is synchronized with the user’s corresponding server account ﬁ le system. A series of shell scripts then recreates the database on the server for the remote account and runs all the development tests. When the tests have completed, a list of times to run each test is dumped to the console, along with a MyTestSuite.java class containing all the test failures, which the developer can use to run locally to ﬁ x any tests that have broken. The biggest advantage the remote server has provided is that it makes running a large number of tests feel fast again, because the developer can continue working while he or she waits for the results of the test server to come back.\n\nThe clustering framework (based on Condor) was quite fast but had the defect that it had to ship the entire workspace (11MB) to all the nodes on the network (×20), which had a signiﬁ cant cost, especially when a dozen pairs are using it. In comparison, the test server uses rsynch, which copies only the ﬁ les that are new or different in the developer’s workspace. The clustering framework also proved to be less reliable than the server solution, frequently not returning any status of the test run. There were also some tests that would not run reliably on the framework. Since it gave us roughly the same perfor- mance as the “check-in” test server, we have put this solution on the back burner.\n\nFurther Reading A more detailed description of the ﬁ rst experience can be found at http:// FasterTestsPaper.gerardmeszaros.com.\n\nwww.it-ebooks.info\n\n321\n\nShared Fixture",
      "content_length": 2235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "322\n\nShared Fixture\n\nChapter 18 Test Strategy Patterns\n\nVariation: Incremental Tests\n\nWe may also use Shared Fixtures when we have a long, complex sequence of actions, each of which depends on the previous actions. In customer tests, this may show up as a work ﬂ ow; in unit tests, it may be a sequence of method calls on the same object. This case might be tested using a single Eager Test (see As- sertion Roulette on page 224). The alternative is to put each distinct action into a separate Test Method (page 348) that builds upon the actions of a previous test operating on a Shared Fixture. This approach, which is an example of Chained Tests (page 454), is how testers in the “testing” (i.e., QA) community often operate: They set up a ﬁ xture and then run a sequence of tests, each of which builds upon the ﬁ xture. The testers do have one signiﬁ cant advantage over our Fully Automated Tests (see page 26): When a test partway through the chain fails, they are available to make decisions about how to recover or whether it is worth proceeding at all. In contrast, our automated tests just keep running, and many of them will generate test failures or errors because they did not ﬁ nd the ﬁ xture as expected and, therefore, the SUT behaved (probably correctly) differ- ently. The resulting test results can obscure the real cause of the failure in a sea of red. With some experience it is often possible to recognize the failure pattern and deduce the root cause.10\n\nThis troubleshooting can be made simpler by starting each Test Method with one or more Guard Assertions (page 490) that document the assumptions the Test Method makes about the state of the ﬁ xture. When these assertions fail, they tell us to look elsewhere—either at tests that failed earlier in the test suite or at the order in which the tests were run.\n\nImplementation Notes\n\nA key implementation question with Shared Fixtures is, How do tests know about the objects in the Shared Fixture so they can (re)use them? Because the point of a Shared Fixture is to save execution time and effort by having multiple tests use the same instance of the test ﬁ xture, we’ll need to keep a reference to the ﬁ xture we create. That way, we can ﬁ nd the ﬁ xture if it already exists and we can inform other tests that it now exists once we have constructed it. We have more choices available to us with Per-Run Fixtures because we can “remember” the ﬁ xture we set up in code more easily than a Prebuilt Fixture (page 429) set up by a different program. Although we could just hard-code the identiﬁ ers (e.g., database keys) of the ﬁ xture objects into all our tests, that technique would result in a Fragile Fix- ture. To avoid this problem, we need to keep a reference to the ﬁ xture when we create it and we need to make it possible for all tests to access that reference.\n\n10 It may not be as simple as looking at the ﬁ rst test that failed.\n\nwww.it-ebooks.info",
      "content_length": 2932,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "Shared Fixture\n\nVariation: Per-Run Fixture\n\nThe simplest form of Shared Fixture is the Per-Run Fixture, in which we set up the ﬁ xture at the beginning of a test run and allow it to be shared by the tests within the run. Ideally, the ﬁ xture won’t outlive the test run and we don’t have to worry about interactions between test runs such as Unrepeatable Tests (a cause of Erratic Tests). If the ﬁ xture is persistent, such as when it is stored in a database, we may need to do explicit ﬁ xture teardown.\n\nIf a Per-Run Fixture is shared only within a single Testcase Class (page 373), the simplest solution is to use a class variable for each ﬁ xture object we need to hold a reference to and then use either Lazy Setup (page 435) or Suite Fixture Setup (page 441) to initialize the objects just before we run the ﬁ rst test in the suite. If we want to share the test ﬁ xture between many Testcase Classes, we’ll need to use a Setup Decorator (page 447) to hold the setUp and tearDown methods and a Test Fixture Registry (see Test Helper on page 643) (which could just be the test database) to access the ﬁ xture.\n\nVariation: Immutable Shared Fixture\n\nThe problem with Shared Fixtures is that they lead to Erratic Tests if tests modify the Shared Fixture (page 317). Shared Fixtures violate the Independent Test prin- ciple (see page 42). We can avoid this problem by making the Shared Fixture immutable; that is, we partition the ﬁ xture needed by tests into two logical parts. The ﬁ rst part is the stuff every test needs to have present but is never modiﬁ ed by any tests—that is, the Immutable Shared Fixture. The second part is the objects that any test needs to modify or delete; these objects should be built by each test as Fresh Fixtures.\n\nThe most difﬁ cult part of applying an Immutable Shared Fixture is deciding what constitutes a change to an object. The key guideline is this: If any test per- ceives something done by another test as a change to an object in the Immutable Shared Fixture, then that change shouldn’t be allowed in any test with which it shares the ﬁ xture. Most commonly, the Immutable Shared Fixture consists of reference data that is needed by the actual per-test ﬁ xtures. The per-test ﬁ xtures can then be built as Fresh Fixtures on top of the Immutable Shared Fixture.\n\nMotivating Example\n\nThe following example shows a Testcase Class setting up the test ﬁ xture via Implicit Setup (page 424). Each Test Method uses an instance variable to access the contents of the ﬁ xture.\n\nwww.it-ebooks.info\n\n323\n\nShared Fixture",
      "content_length": 2552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "324\n\nShared Fixture\n\nChapter 18 Test Strategy Patterns\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { setupStandardAirportsAndFlights(); FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { setupStandardAirportsAndFlights(); FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nNote that the setUp method is run once for each Test Method. If the ﬁ xture setup is fairly complex and involves accessing a database, this approach could result in Slow Tests.\n\nRefactoring Notes\n\nTo convert a Testcase Class from a Standard Fixture to a Shared Fixture, we simply convert the instance variables into class variables to make the ﬁ xture outlast the creating Testcase Object. We then need to initialize the class vari- ables just once to avoid recreating them for each Test Method; Lazy Setup is an easy way to accomplish this task. Of course, other ways to set up the Shared Fixture are also possible, such as Setup Decorator or Suite Fixture Setup.\n\nExample: Shared Fixture\n\nThis example shows the ﬁ xture converted to a Shared Fixture set up using Lazy Setup.\n\nprotected void setUp() throws Exception { if (sharedFixtureInitialized) { return;\n\nwww.it-ebooks.info",
      "content_length": 1725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "Shared Fixture\n\n} facade = new FlightMgmtFacadeImpl(); setupStandardAirportsAndFlights(); sharedFixtureInitialized = true; }\n\nprotected void tearDown() throws Exception { // We cannot delete any objects because we don't know // whether this is the last test }\n\nThe Lazy Initialization [SBPP] logic in the setUp method ensures that the Shared Fixture is created whenever the class variable is uninitialized. The Test Methods have also been modiﬁ ed to use a Finder Method to access the contents of the ﬁ xture:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nThe details of how the Test Utility Methods such as setupStandardAirportsAndFlights are implemented are not shown here, because they are not important for under- standing this example. It should be enough to understand that these methods create the airports and ﬂ ights and store references to them in static variables so that all Test Methods can access the same ﬁ xture either directly or via Test Utility Methods.\n\nwww.it-ebooks.info\n\n325\n\nShared Fixture",
      "content_length": 1688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "326\n\nShared Fixture\n\nChapter 18 Test Strategy Patterns\n\nExample: Immutable Shared Fixture\n\nHere’s an example of Shared Fixture “pollution”:\n\npublic void testCancel_proposed_p()throws Exception { // shared ﬁxture BigDecimal proposedFlightId = ﬁndProposedFlight(); // exercise SUT facade.cancelFlight(proposedFlightId); // verify outcome try{ assertEquals(FlightState.CANCELLED, facade.ﬁndFlightById(proposedFlightId)); } ﬁnally { // teardown // try to undo the damage; hope this works! facade.overrideStatus( proposedFlightId, FlightState.PROPOSED); } }\n\nWe can avoid this problem by making the Shared Fixture immutable; that is, we partition the ﬁ xture needed by tests into two logical parts. The ﬁ rst part is the stuff every test needs to have present but is never modiﬁ ed by any tests—that is, the Immutable Shared Fixture. The second part is the objects that any test needs to modify or delete; these objects should be built by each test as Fresh Fixtures. Here’s the same test modiﬁ ed to use an Immutable Shared Fixture. We simply\n\ncreated our own mutableFlight within the test.\n\npublic void testCancel_proposed() throws Exception { // ﬁxture setup BigDecimal mutableFlightId = createFlightBetweenInsigiﬁcantAirports(); // exercise SUT facade.cancelFlight(mutableFlightId); // verify outcome assertEquals( FlightState.CANCELLED, facade.ﬁndFlightById(mutableFlightId)); // teardown // None required because we let the SUT create // new IDs for each ﬂight. We might need to clean out // the database eventually. }\n\nNote that we don’t need any ﬁ xture teardown logic in this version of the test because the SUT uses a Distinct Generated Value (see Generated Value on page 723)—that is, we do not supply a ﬂ ight number. We also use the predeﬁ ned dummyAirport1 and dummyAirport2 to avoid changing the number of ﬂ ights for airports used by other tests. Therefore, the mutable ﬂ ights can accumulate in the database trouble-free.\n\nwww.it-ebooks.info",
      "content_length": 1953,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "Back Door Manipulation\n\nBack Door Manipulation\n\nHow can we verify logic independently when we cannot use a round-trip test?\n\nWe set up the test ﬁ xture or verify the outcome by going through a back door (such as direct database access).\n\nSetup Setup\n\nExercise Exercise\n\nSUT SUT\n\nData Data Fixture Fixture\n\nVerify Verify\n\nTeardown Teardown\n\nEvery test requires a starting point (the test ﬁ xture) and an expected ﬁ nishing point (the expected results). The “normal” approach is to set up the ﬁ xture and verify the outcome by using the API of the SUT itself. In some circumstances this is either not possible or not desirable.\n\nIn some situations we can use Back Door Manipulation to set up the ﬁ xture\n\nand/or verify the SUT’s state.\n\nHow It Works\n\nThe state of the SUT comes in many ﬂ avors. It can be stored in memory, on disk as ﬁ les, in a database, or in other applications with which the SUT interacts. Whatever form it takes, the pre-conditions of a test typically require that the state of the SUT is not just known but is a speciﬁ c state. Likewise, at the end of the test we often want to do State Veriﬁ cation (page 462) of the SUT’s state.\n\nIf we have access to the state of the SUT from outside the SUT, the test can set up the pre-test state of the SUT by bypassing the normal API of the SUT and interacting directly with whatever is holding that state via a “back door.” When exercising of the SUT has been completed, the test can similarly access\n\nwww.it-ebooks.info\n\n327\n\nBack Door Manipulation\n\nAlso known as: Layer-Crossing Test",
      "content_length": 1547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "328\n\nBack Door Manipulation\n\nChapter 18 Test Strategy Patterns\n\nthe post-test state of the SUT via a back door to compare it with expected outcome. For customer tests, the back door is most commonly a test database, but it could also be some other component on which the SUT depends, including a Registry [PEAA] object or even the ﬁ le system. For unit tests, the back door is some other class or object or an alternative interface of the SUT (or a Test-Speciﬁ c Subclass; page 579) that exposes the state in a way “normal” clients wouldn’t use. We can also replace a depended-on component (DOC) with a suitably conﬁ gured Test Double (page 522) instead of using the real thing if that makes the job easier.\n\nWhen to Use It\n\nWe might choose to use Back Door Manipulation for several reasons which we’ll examine in more detail shortly. A prerequisite for using this technique is that some sort of back door to the state of the system must exist. The main drawback of Back Door Manipulation is that our tests—or the Test Utility Methods (page 599) they call—become much more closely coupled to the design decisions we make about how to represent the state of the SUT. If we need to change those decisions, we may encounter Fragile Tests (page 239). We need to decide whether this price is acceptable on a case-by-case basis. We can greatly reduce the impact of the close coupling by encapsulating all Back Door Manipulation in Test Utility Methods.\n\nUsing Back Door Manipulation can also lead to Obscure Tests (page 186) by hiding the relationship of the test outcome to the test ﬁ xture. We can avoid this problem by including the test data being passed to the Back Door Manipulation mechanism within the Testcase Class (page 373), or at least mitigate it by using Finder Methods (see Test Utility Method) to refer to the objects in the ﬁ xture via intent-revealing names.\n\nA common application of Back Door Manipulation involves testing basic CRUD (Create, Read, Update, Delete) operations on the SUT’s state. In such a case, we want to verify that the information persisted and can be recovered in the same form. It is difﬁ cult to write round-trip tests for “Read” without also testing “Create”; likewise, it is difﬁ cult to test “Update” or “Delete” without testing both “Create” and “Read.” We can certainly test these operations by using round-trip tests, but this kind of testing won’t detect certain types of systemic problems, such as putting information into the wrong database column. One solution is to con- duct layer-crossing tests that use Back Door Manipulation to set up or verify the contents of the database directly. For a “Read” test, the test sets up the contents of the database using Back Door Setup and then asks the SUT to read the data. For a “Write” test, the test asks the system to write certain objects and then uses Back Door Veriﬁ cation on the contents of the database.\n\nwww.it-ebooks.info",
      "content_length": 2922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "Back Door Manipulation\n\nVariation: Back Door Setup\n\nOne reason for doing Back Door Manipulation is to make tests run faster. If a system does a lot of processing before putting data into its data store, the time it takes for a test to set up the ﬁ xture via the SUT’s API could be quite signiﬁ cant. One way to make the tests run faster is to determine what those data stores should look like and then create a means to set them up via the back door rather than through the API. Unfortunately, this technique introduces its own problem: Because Back Door Setup bypasses enforcement of the object creation business rules, we may ﬁ nd ourselves creating ﬁ xtures that are not realistic and possibly even invalid. This problem may creep in over time as the business rules are modiﬁ ed in response to changing business needs. At the same time, this ap- proach may allow us to create test scenarios that the SUT will not let us set up through its API.\n\nWhen we share a database between our SUT and another application, we need to verify that we are using the database correctly and that we can handle all possible data conﬁ gurations the other applications might create. Back Door Setup is a good way to establish these conﬁ gurations—and it may be the only way if the SUT either doesn’t write those tables or writes only speciﬁ c (and valid) data conﬁ gurations. Back Door Setup lets us create those “impossible” conﬁ gurations easily so we can verify how the SUT behaves in these situations.\n\nVariation: Back Door Veriﬁ cation\n\nBack Door Veriﬁ cation involves sneaking in to do State Veriﬁ cation of the SUT’s post-exercise state via a back door; it is mostly applicable to customer tests (or functional tests, as they are sometimes called). The back door is typically an alternative way to examine the objects in the database, usually through a stan- dard API such as SQL or via data exports that can then be examined with a ﬁ le comparison utility program.\n\nAs mentioned earlier, Back Door Manipulation can make tests run faster. If the only way to get at the SUT’s state is to invoke an expensive operation (such as a complex report) or an operation that further modiﬁ es the SUT’s state, we may be better off using Back Door Manipulation.\n\nAnother reason for doing Back Door Manipulation is that other systems expect the SUT to store its state in a speciﬁ c way, which they can then access directly. This is a form of indirect output. In this situation, standard round-trip tests cannot prove that the SUT’s behavior is correct because they cannot detect a systematic problem if the “Write” and “Read” operations make the same mistake, such as putting information into the wrong database column. The solution is a layer-crossing test that looks at the contents of the database\n\nwww.it-ebooks.info\n\n329\n\nBack Door Manipulation",
      "content_length": 2826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "330\n\nBack Door Manipulation\n\nChapter 18 Test Strategy Patterns\n\ndirectly to verify that the information is stored correctly. For a “Write” test, the test asks the system to write certain objects and then inspects the contents of the database via the back door.\n\nVariation: Back Door Teardown\n\nWe can also use Back Door Manipulation to tear down a Fresh Fixture (page 311) that is stored in a test database. This ability is especially beneﬁ cial if we can use bulk database commands to wipe clean whole tables, as in Table Truncation Teardown (page 661) or Transaction Rollback Teardown (page 668).\n\nImplementation Notes\n\nHow we implement Back Door Manipulation depends on where the ﬁ xture lives and how easily we can access the state of the SUT. It also depends on why we are doing Back Door Manipulation. This section lists the most common imple- mentations, but feel free to use your imagination and come up with other ways to use this pattern.\n\nVariation: Database Population Script\n\nWhen the SUT stores its state in a database that it accesses as it runs, the easiest way to do Back Door Manipulation is to load data directly into that database before invoking the SUT. This approach is most commonly required when we are writing customer tests, but it may also be required for unit tests if the classes we are testing interact directly with the database. We must ﬁ rst determine the pre-conditions of the test and, from that information, identify the data that the test requires for its ﬁ xture. We then deﬁ ne a database script that inserts the cor- responding records directly into the database bypassing the SUT logic. We use this Database Population Script whenever we want to set up the test ﬁ xture—a decision that depends on which test ﬁ xture strategy we have chosen. (See Chap- ter 6, Test Automation Strategy, for more on that topic.)\n\nWhen deciding to use a Database Population Script, we will need to maintain both the Database Population Script and the ﬁ les it takes as input whenever we modify either the structure of the SUT’s data stores or the semantics of the data in them. This requirement can increase the maintenance cost of the tests.\n\nVariation: Data Loader\n\nA Data Loader is a special program that loads data into the SUT’s data store. It differs from a Database Population Script in that the Data Loader is written in a programming language rather than a database language. This gives us a bit\n\nwww.it-ebooks.info",
      "content_length": 2445,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "Back Door Manipulation\n\nmore ﬂ exibility and allows us to use the Data Loader even when the system state is stored somewhere other than a relational database.\n\nIf the data store is external to the SUT, such as in a relational database, the Data Loader can be “just another application” that writes to that data store. It would use the database in much the same way as the SUT but would get its inputs from a ﬁ le rather than from wherever the SUT normally gets its inputs (e.g., other “upstream” programs). When we are using an object relational mapping (ORM) tool to access the database from our SUT, a simple way to build the Data Loader is to use the same domain objects and mappings in our Data Loader. We just create the desired objects in memory and commit the ORM’s unit of work to save them into the database.\n\nIf the SUT stores data in internal data structures (e.g., in memory), the Data Loader may need to be an interface provided by the SUT itself. The following characteristics differentiate it from the normal functionality provided by the SUT:\n\nIt is used only by the tests.\n\nIt reads the data from a ﬁ le rather than wherever the SUT normally gets\n\nthe data.\n\nIt bypasses a lot of the “edit checks” (input validation) normally done\n\nby the SUT.\n\nThe input ﬁ les may be simple ﬂ at ﬁ les containing comma- or tab-delimited text, or they could be structured using XML. DbUnit is an extension of JUnit that implements Data Loader for ﬁ xture setup.\n\nVariation: Database Extraction Script\n\nWhen the SUT stores its state in a database that it accesses as it runs, we can take advantage of this structure to do Back Door Veriﬁ cation. We simply use a database script to extract data from the test database and verify that it contains the right data either by comparing it to previously prepared “extract” ﬁ les or by ensuring that speciﬁ c queries return the right number of records.\n\nVariation: Data Retriever\n\nA Data Retriever is the analog of a Data Loader that retrieves the state from the SUT when doing Back Door Veriﬁ cation. Like a trusty dog, it “fetches” the data so that we can compare it with our expected results within our tests. DbUnit is an extension of JUnit that implements Data Retriever to support result veriﬁ cation.\n\nwww.it-ebooks.info\n\n331\n\nBack Door Manipulation",
      "content_length": 2297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "332\n\nBack Door Manipulation\n\nChapter 18 Test Strategy Patterns\n\nVariation: Test Double as Back Door\n\nSo far, all of the implementation techniques described here have involved inter- acting with a DOC of the SUT to set up or tear down the ﬁ xture or to verify the expected outcome. Probably the most common form of Back Door Manipula- tion involves replacing the DOC with a Test Double. One option is to use a Fake Object (page 551) that we have preloaded with some data as though the SUT had already been interacting with it; this strategy allows us to avoid using the SUT to set up the SUT’s state. The other option is to use some kind of Con- ﬁ gurable Test Double (page 558), such as a Mock Object (page 544) or a Test Stub (page 529). Either way, we can completely avoid Obscure Tests by making the state of the Test Double visible within the Test Method (page 348).\n\nWhen we want to perform Behavior Veriﬁ cation (page 468) of the calls made by the SUT to one or more DOCs, we can use a layer-crossing test that replaces the DOC with a Test Spy (page 538) or a Mock Object. When we want to verify that the SUT behaves a speciﬁ c way when it receives indirect inputs from a DOC (or when in some speciﬁ c external state), we can replace the DOC with a Test Stub.\n\nMotivating Example\n\nThe following round-trip test veriﬁ es the basic functionality of removing a ﬂ ight by interacting with the SUT only via the front door. But it does not verify the indirect outputs of the SUT—namely, that the SUT is expected to call a logger to log each time a ﬂ ight is removed along with the day/time when the request was made and the user ID of the requester. In many systems, this would be an ex- ample of “layer-crossing behavior”: The logger is part of a generic infrastructure layer, while the SUT is an application-speciﬁ c behavior.\n\npublic void testRemoveFlight() throws Exception { // setup FlightDto expectedFlightDto = createARegisteredFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight should not exist after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); }\n\nwww.it-ebooks.info",
      "content_length": 2231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "Back Door Manipulation\n\nRefactoring Notes\n\nWe can convert this test to use Back Door Veriﬁ cation by adding result veriﬁ cation code to access and verify the logger’s state. We can do so either by reading that state from the logger’s database or by replacing the logger with a Test Spy that saves the state for easy access by the tests.\n\nExample: Back Door Result Veriﬁ cation Using a Test Spy\n\nHere’s the same test converted to use a Test Spy to access the post-test state of the logger:\n\npublic void testRemoveFlightLogging_recordingTestStub() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // Test Double setup AuditLogSpy logSpy = new AuditLogSpy(); facade.setAuditLog(logSpy); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertEquals(\"number of calls\", 1, logSpy.getNumberOfCalls()); assertEquals(\"action code\", Helper.REMOVE_FLIGHT_ACTION_CODE, logSpy.getActionCode()); assertEquals(\"date\", helper.getTodaysDateWithoutTime(), logSpy.getDate()); assertEquals(\"user\", Helper.TEST_USER_NAME, logSpy.getUser()); assertEquals(\"detail\", expectedFlightDto.getFlightNumber(), logSpy.getDetail()); }\n\nThis approach would be the better way to verify the logging if the logger’s data- base contained so many entries that it wasn’t practical to verify the new entries using Delta Assertions (page 485).\n\nExample: Back Door Fixture Setup\n\nThe next example shows how we can set up a ﬁ xture using the database as a back door to the SUT. The test inserts a record into the EmailSubscription table and then asks the SUT to ﬁ nd it. It then makes assertions on various ﬁ elds of the object returned by the SUT to verify that the record was read correctly.\n\nwww.it-ebooks.info\n\n333\n\nBack Door Manipulation",
      "content_length": 1836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "334\n\nBack Door Manipulation\n\nChapter 18 Test Strategy Patterns\n\nstatic ﬁnal String TABLE_NAME = \"EmailSubscription\"; static ﬁnal BigDecimal RECORD_ID = new BigDecimal(\"111\");\n\nstatic ﬁnal String LOGIN_ID = \"Bob\"; static ﬁnal String EMAIL_ID = \"bob@foo.com\";\n\npublic void setUp() throws Exception { String xmlString = \"<?xml version='1.0' encoding='UTF-8'?>\" + \"<dataset>\" + \" <\" + TABLE_NAME + \" EmailSubscriptionId='\" + RECORD_ID + \"'\" + \" UserLoginId='\" + LOGIN_ID + \"'\" + \" EmailAddress='\" + EMAIL_ID + \"'\" + \" RecordVersionNum='62' \" + \" CreateByUserId='MappingTest' \" + \" CreateDateTime='2004-03-01 00:00:00.0' \" + \" LastModByUserId='MappingTest' \" + \" LastModDateTime='2004-03-01 00:00:00.0'/>\" + \"</dataset>\"; insertRowsIntoDatabase(xmlString); }\n\npublic void testRead_Login() throws Exception { // exercise EmailSubscription subs = EmailSubscription.ﬁndInstanceWithId(RECORD_ID); // verify assertNotNull(\"Email Subscription\", subs); assertEquals(\"User Name\", LOGIN_ID, subs.getUserName()); }\n\npublic void testRead_Email() throws Exception { // exercise EmailSubscription subs = EmailSubscription.ﬁndInstanceWithId(RECORD_ID); // verify assertNotNull(\"Email Subscription\", subs); assertEquals(\"Email Address\", EMAIL_ID, subs.getEmailAddress()); }\n\nThe XML document used to populate the database is built within the Testcase Class so as to avoid the Mystery Guest (see Obscure Test) that would have been created if we had used an external ﬁ le for loading the database [the discussion of the In-line Resource (page 736) refactoring explains this approach]. To make the test clearer, we call intent-revealing methods that hide the details of how we use DbUnit to load the database and clean it out at the end of the test using Table Truncation Teardown. Here are the bodies of the Test Utility Methods used in this example:\n\nwww.it-ebooks.info",
      "content_length": 1848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "Back Door Manipulation\n\nprivate void insertRowsIntoDatabase(String xmlString) throws Exception { IDataSet dataSet = new FlatXmlDataSet(new StringReader(xmlString)); DatabaseOperation.CLEAN_INSERT. execute( getDbConnection(), dataSet); }\n\npublic void tearDown() throws Exception{ emptyTable(TABLE_NAME); }\n\npublic void emptyTable(String tableName) throws Exception { IDataSet dataSet = new DefaultDataSet(new DefaultTable(tableName)); DatabaseOperation.DELETE_ALL. execute(getDbConnection(), dataSet); }\n\nOf course, the implementations of these methods are speciﬁ c to DbUnit; we must change them if we use some other member of the xUnit family.\n\nSome other observations on these tests: To avoid an Eager Test (see Assertion Roulette on page 224), the assertion on each ﬁ eld appears in a separate test. This structure could result in Slow Tests (page 253) because these tests interact with a database. We could use Lazy Setup (page 435) or Suite Fixture Setup (page 441) to avoid setting up the ﬁ xture more than once as long as the resulting Shared Fixture (page 317) was not modiﬁ ed by any of the tests. (I chose not to further complicate this example by taking this tack.)\n\nFurther Reading\n\nSee the sidebar “Database as SUT API?” on page 336 for an example of when the back door is really a front door.\n\nwww.it-ebooks.info\n\n335\n\nBack Door Manipulation",
      "content_length": 1355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "336\n\nBack Door Manipulation\n\nChapter 18 Test Strategy Patterns\n\nDatabase as SUT API?\n\nA common technique for setting up test ﬁ xtures is Back Door Setup (see Back Door Manipulation on page 327); for verifying test outcomes, Back Door Veriﬁ cation (see Back Door Manipulation) is a popular option. But when is a test that interacts directly with the database behind a SUT not considered to be going through the back door?\n\nOn a recent project, some friends were struggling with this very question, though at ﬁ rst they didn’t realize it. One of their analysts (who was also a power user) seemed overly focused on the database schema. At ﬁ rst, they put this narrow focus down to the analyst’s Powerbuilder background and tried to break him of the habit. That didn’t work. The analyst just dug in his heels. The developers tried explaining that on agile projects it was important not to try to deﬁ ne the whole data schema at the begin- ning of the project; instead, the schema evolved as the requirements were implemented.\n\nOf course, the analyst complained every time they modiﬁ ed the database schema because the changes broke all his queries. As the project unfold- ed, the other team members slowly started to understand that the analyst really did need a stable database against which to run queries. It was his way to verify the correctness of the data generated by the system.\n\nOnce they recognized this requirement, the developers were able to treat the query schema as a formal interface provided by the system. Customer tests were written against this interface and developers had to ensure that those tests still passed whenever they changed the database. To minimize the impact of database refactorings, they deﬁ ned a set of query views that implemented this interface. This approach allowed them to refactor the database as needed.\n\nWhen might you ﬁ nd yourself in this situation? Any time your customer applies reporting tools (such as Crystal Reports) to your database, an argument can be made as to whether part of the requirements is a stable reporting interface. Similarly, if the customer uses scripts (such as DTS or SQL) to load data into the database, there may be a requirement for a stable data loading interface.\n\nwww.it-ebooks.info",
      "content_length": 2257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "Layer Test\n\nLayer Test\n\nHow can we verify logic independently when it is part of a layered architecture?\n\nWe write separate tests for each layer of the layered architecture.\n\nLayer1TestcaseClass Layer1TestcaseClass testMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nLayernTestcaseClass LayernTestcaseClass testMethod_1 testMethod_1 testMethod_2 testMethod_2\n\nLayer n Layer n\n\nLayer 1 Layer 1\n\nTest Double Test Double\n\nTest Double Test Double\n\nDOC DOC\n\nIt is difﬁ cult to obtain good test coverage when testing an entire application in a top-to-bottom fashion; we are bound to end up doing Indirect Testing (see Obscure Test on page 186) on some parts of the application. Many applications use a Layered Architecture [DDD, PEAA, WWW] to separate the major technical concerns. Most applications have some kind of presentation (user interface) lay- er, a business logic layer or domain layer, and a persistence layer. Some layered architectures have even more layers.\n\nAn application with a layered architecture can be tested more effectively by\n\ntesting each layer in isolation.\n\nHow It Works\n\nWe design the SUT using a layered architecture that separates the presentation logic from the business logic and from any persistence mechanism or interfaces\n\nwww.it-ebooks.info\n\n337\n\nLayer Test\n\nAlso known as: Single Layer Test, Testing by Layers, Layered Test",
      "content_length": 1356,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "338\n\nLayer Test\n\nChapter 18 Test Strategy Patterns\n\nto other systems.11 We put all business logic into a Service Layer [PEAA] that exposes the application functionality to the presentation layer as an API. We treat each layer of the architecture as a separate SUT. We write component tests for each layer independent of the other layers of the architecture. That is, for layer n of the architecture, the tests will take the place of layer n+1; we may op- tionally replace layer n-1 with a Test Double (page 522).\n\nWhen to Use It\n\nWe can use a Layer Test whenever we have a layered architecture and we want to provide good test coverage of the logic in each layer. It can be much simpler to test each layer independently than it is to test all the layers at once. This is especially true when we want to do defensive coding for return values of calls across the layer boundary. In software that is working correctly, these errors “should never happen”; in real life, they do. To make sure our code handles these errors, we can inject these “never happen” scenarios as indirect inputs to our layer.\n\nLayer Tests are very useful when we want to divide up the project team into subteams based on the technology in which the team members specialize. Each layer of an architecture tends to require different knowledge and often uses different technologies; therefore, the layer boundaries serve as natural team boundaries. Layer Tests can be a good way to nail down and document the semantics of the layer interfaces.\n\nEven when we choose to use a Layer Test strategy, it is a good idea to include a few “top-to-bottom” tests just to verify that the various layers are integrated correctly. These tests need to cover only one or two basic scenarios; we don’t need to test every business test condition because all of them have already been tested in the Layer Tests for at least one of the layers.\n\nMost of the variations on this pattern reﬂ ect which layer is being tested inde-\n\npendently of the other layers.\n\nVariation: Presentation Layer Test\n\nOne could write a whole book just on patterns of presentation layer testing. The speciﬁ c patterns depend on the nature of the presentation layer technology (e.g., graphical user interface, traditional Web interface, “smart” Web interface, Web services). Regardless of the technology, the key is to test the presentation logic separately from the business logic so that we don’t have to worry about changes\n\n11 Not all presentation logic relates to the user interface; this logic can also appear in a messaging interface used by another application.\n\nwww.it-ebooks.info",
      "content_length": 2612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "Layer Test\n\nin the underlying logic affecting our presentation layer tests. (They are hard enough to automate well as it is!)\n\nAnother consideration is to design the presentation layer so that its logic can be tested independently of the presentation framework. Humble Dialog (see Humble Object on page 695) is the key design-for-testability pattern to apply here. In effect, we are deﬁ ning sublayers within the presentation layer; the layer containing the Humble Dialogs is the “presentation graphic layer” and the layer we have made testable is the “presentation behavior layer.” This separation of layers allows us to verify that buttons are activated, menu items are grayed out, and so on, without instantiating any of the real graphical objects.\n\nVariation: Service Layer Test\n\nThe Service Layer is where most of our unit tests and component tests are traditionally concentrated. Testing the business logic using customer tests is a bit more challenging because testing the Service Layer via the presentation layer often involves Indirect Testing and Sensitive Equality (see Fragile Test on page 239), either of which can lead to Fragile Tests and High Test Maintenance Cost (page 265). Testing the Service Layer directly helps avoid these problems. To avoid Slow Tests (page 253), we usually replace the persistence layer with a Fake Database (see Fake Object on page 551) and then run the tests. In fact, most of the impetus behind a layered architecture is to isolate this code from the other, harder-to-test layers. Alistair Cockburn puts an interesting spin on this idea in his description of a Hexagonal Architecture at http://alistair.cockburn.us [WWW].\n\nThe Service Layer may come in handy for other uses. It can be used to run the application in “headless” mode (without a presentation layer attached), such as when using macros to automate frequently done tasks in Microsoft Excel.\n\nVariation: Persistence Layer Test\n\nThe persistence layer also needs to be tested. Round-trip tests will often sufﬁ ce if the application is the only one that uses the data store. But these tests won’t catch one kind of programming error: when we accidentally put information into the wrong columns. As long as the data type of the interchanged columns is compatible and we make the same error when reading the data, our round-trip tests will pass! This kind of bug won’t affect the operation of our application but it might make support more difﬁ cult and it will cause problems in interactions with other applications.\n\nWhen other applications also use the data store, it is highly advisable to imple- ment at least a few layer-crossing tests that verify information is put into the\n\nwww.it-ebooks.info\n\n339\n\nLayer Test",
      "content_length": 2719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "340\n\nLayer Test\n\nChapter 18 Test Strategy Patterns\n\ncorrect columns of tables. We can use Back Door Manipulation (page 327) to either set up the database contents or to verify the post-test database contents.\n\nVariation: Subcutaneous Test\n\nA Subcutaneous Test is a degenerate form of Layer Test that bypasses the pre- sentation layer of the system to interact directly with the Service Layer. In most cases, the Service Layer is not isolated from the layer(s) below; therefore, we test everything except the presentation. Use of a Subcutaneous Test does not require as strict a separation of concerns as does a Service Layer Test, which makes Subcutaneous Test easier to use when we are retroﬁ tting tests onto an application that wasn’t designed for testability. We should use a Subcutaneous Test whenever we are writing customer tests for an application and we want to ensure our tests are robust. A Subcutaneous Test is much less likely to be broken by changes to the application12 because it does not interact with the application via the presentation layer; as a consequence, a whole category of changes won’t affect it.\n\nVariation: Component Test\n\nA Component Test is the most general form of Layer Test, in that we can think of the layers being made up of individual components that act as “micro-layers.” Component Tests are a good way to specify or document the behavior of indi- vidual components when we are doing component-based development and some of the components must be modiﬁ ed or built from scratch.\n\nImplementation Notes\n\nWe can write our Layer Tests as either round-trip tests or layer-crossing tests. Each has advantages. In practice, we typically mix both styles of tests. The round-trip tests are easier to write (assuming we already have a suitable Fake Object available to use for layer n-1). We need to use layer-crossing tests, how- ever, when we are verifying the error-handling logic in layer n.\n\nRound-Trip Tests\n\nA good starting point for Layer Tests is the round-trip test, as it should be sufﬁ cient for most Simple Success Tests (see Test Method on page 348). These tests can be written such that they do not care whether we have fully isolated the layer of interest from the layers below. We can either leave the real com- ponents in place so that they are exercised indirectly, or we can replace them\n\n12 Less likely than a test that exercises the logic via the presentation layer, that is.\n\nwww.it-ebooks.info",
      "content_length": 2448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "Layer Test\n\nwith Fake Objects. The latter option is particularly useful when by a database or asynchronous mechanisms in the layer below lead to Slow Tests.\n\nControlling Indirect Inputs\n\nWe can replace a lower layer of the system with a Test Stub (page 529) that returns “canned” results based on what the client layer passes in a request (e.g., Customer 0001 is a valid customer, 0002 is a dormant customer, 0003 has three accounts). This technique allows us to test the client logic with well-understood indirect inputs from the layer below. It is particularly useful when we are auto- mating Expected Exception Tests (see Test Method) or when we are exercising behavior that depends on data that arrives from an upstream system.13 The alternative is to use Back Door Manipulation to set up the indirect inputs.\n\nVerifying Indirect Outputs\n\nWhen we want to verify the indirect outputs of the layer of interest, we can use a Mock Object (page 544) or Test Spy (page 538) to replace the components in the layer below the SUT. We can then compare the actual calls made to the DOC with the expected calls. The alternative is to use Back Door Manipulation to verify the indirect outputs of the SUT after they have occurred.\n\nMotivating Example\n\nWhen trying to test all layers of the application at the same time, we must verify the correctness of the business logic through the presentation layer. The following test is a very simple example of testing some trivial business logic through a trivial user interface:\n\nprivate ﬁnal int LEGAL_CONN_MINS_SAME = 30; public void testAnalyze_sameAirline_LessThanConnectionLimit() throws Exception { // setup FlightConnection illegalConn = createSameAirlineConn( LEGAL_CONN_MINS_SAME - 1); // exercise FlightConnectionAnalyzerImpl sut = new FlightConnectionAnalyzerImpl(); String actualHtml = sut.getFlightConnectionAsHtmlFragment( illegalConn.getInboundFlightNumber(), illegalConn.getOutboundFlightNumber());\n\n13 Typically this data goes directly into a shared database or is injected via a “data pump.”\n\nwww.it-ebooks.info\n\n341\n\nLayer Test",
      "content_length": 2079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "342\n\nLayer Test\n\nChapter 18 Test Strategy Patterns\n\n// veriﬁcation StringBuffer expected = new StringBuffer(); expected.append(\"<span class=\"boldRedText\">\"); expected.append(\"Connection time between ﬂight \"); expected.append(illegalConn.getInboundFlightNumber()); expected.append(\" and ﬂight \"); expected.append(illegalConn.getOutboundFlightNumber()); expected.append(\" is \"); expected.append(illegalConn.getActualConnectionTime()); expected.append(\" minutes.</span>\"); assertEquals(\"html\", expected.toString(), actualHtml); }\n\nThis test contains knowledge about the business layer functionality (what makes a connection illegal) and presentation layer functionality (how an illegal connec- tion is presented). It also depends on the database because the FlightConnections are retrieved from another component. If any of these areas change, this test must be revisited as well.\n\nRefactoring Notes\n\nWe can split this test into two separate tests: one to test the business logic (What constitutes an illegal connection?) and one to test the presentation layer (Given an illegal connection, how should it be displayed to the user?). We would typically do so by duplicating the entire Testcase Class (page 373), stripping out the presentation layer logic veriﬁ cation from the business layer Test Methods, and stubbing out the business layer object(s) in the presentation layer Test Methods.\n\nAlong the way, we will probably ﬁ nd that we can reduce the number of tests in at least one of the Testcase Classes because few test conditions exist for that layer. In this example, we started out with four tests (the combinations of same/ different airlines and time periods), each of which tested both the business and presentation layers; we ended up with four tests in the business layer (the origi- nal combinations but tested directly) and two tests in the presentation layer (formatting of legal and illegal connections).14 Therefore, only the latter two tests need to be concerned with the details of the string formatting and, when a test fails, we know which layer holds the bug.\n\nWe can take our refactoring even further by using a Replace Dependency with Test Double (page 739) refactoring to turn this Subcutaneous Test into a true Service Layer Test.\n\n14 I’m glossing over the various error-handling tests to simplify this discussion, but note that a Layer Test also makes it easier to exercise the error-handling logic.\n\nwww.it-ebooks.info",
      "content_length": 2444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "Layer Test\n\nExample: Presentation Layer Test\n\nThe following example shows the earlier test refactored to verify the behavior of the presentation layer when an illegal connection is requested. It stubs out the FlightConnAnalyzer and conﬁ gures it with the illegal connection to return to the HtmlFacade when it is called. This technique gives us complete control over the indirect input of the SUT.\n\npublic void testGetFlightConnAsHtml_illegalConnection() throws Exception { // setup FlightConnection illegalConn = createIllegalConnection(); Mock analyzerStub = mock(IFlightConnAnalyzer.class); analyzerStub.expects(once()).method(\"analyze\") .will(returnValue(illegalConn)); HTMLFacade htmlFacade = new HTMLFacade( (IFlightConnAnalyzer)analyzerStub.proxy()); // exercise String actualHtmlString = htmlFacade.getFlightConnectionAsHtmlFragment( illegalConn.getInboundFlightNumber(), illegalConn.getOutboundFlightNumber()); // verify StringBuffer expected = new StringBuffer(); expected.append(\"<span class=\"boldRedText\">\"); expected.append(\"Connection time between ﬂight \"); expected.append(illegalConn.getInboundFlightNumber()); expected.append(\" and ﬂight \"); expected.append(illegalConn.getOutboundFlightNumber()); expected.append(\" is \"); expected.append(illegalConn.getActualConnectionTime()); expected.append(\" minutes.</span>\"); assertEquals(\"returned HTML\", expected.toString(), actualHtmlString); }\n\nWe must compare the string representations of the HTML to determine whether the code has generated the correct response. Fortunately, we need only two such tests to verify the basic behavior of this component.\n\nExample: Subcutaneous Test\n\nHere’s the original test converted into a Subcutaneous Test that bypasses the presentation layer to verify that the connection information is calculated cor- rectly. Note the lack of any string manipulation in this test.\n\nwww.it-ebooks.info\n\n343\n\nLayer Test",
      "content_length": 1902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "344\n\nLayer Test\n\nChapter 18 Test Strategy Patterns\n\nprivate ﬁnal int LEGAL_CONN_MINS_SAME = 30; public void testAnalyze_sameAirline_LessThanConnectionLimit() throws Exception { // setup FlightConnection expectedConnection = createSameAirlineConn( LEGAL_CONN_MINS_SAME -1); // exercise IFlightConnAnalyzer theConnectionAnalyzer = new FlightConnAnalyzer(); FlightConnection actualConnection = theConnectionAnalyzer.getConn( expectedConnection.getInboundFlightNumber(), expectedConnection.getOutboundFlightNumber()); // veriﬁcation assertNotNull(\"actual connection\", actualConnection); assertFalse(\"IsLegal\", actualConnection.isLegal()); }\n\nWhile we have bypassed the presentation layer, we have not attempted to isolate the Service Layer from the layers below. This omission could result in Slow Tests or Erratic Tests (page 228).\n\nExample: Business Layer Test\n\nThe next example shows the same test converted into a Service Layer Test that is fully isolated from the layers below it. We have used JMock to replace these components with Mock Objects that verify the correct ﬂ ights are being looked up and that inject the corresponding ﬂ ight constructed into the SUT.\n\npublic void testAnalyze_sameAirline_EqualsConnectionLimit() throws Exception { // setup Mock ﬂightMgntStub = mock(FlightManagementFacade.class); Flight ﬁrstFlight = createFlight(); Flight secondFlight = createConnectingFlight( ﬁrstFlight, LEGAL_CONN_MINS_SAME); ﬂightMgntStub.expects(once()).method(\"getFlight\") .with(eq(ﬁrstFlight.getFlightNumber())) .will(returnValue(ﬁrstFlight)); ﬂightMgntStub.expects(once()).method(\"getFlight\") .with(eq(secondFlight.getFlightNumber())) .will(returnValue(secondFlight)); // exercise FlightConnAnalyzer theConnectionAnalyzer = new FlightConnAnalyzer(); theConnectionAnalyzer.facade = (FlightManagementFacade)ﬂightMgntStub.proxy(); FlightConnection actualConnection = theConnectionAnalyzer.getConn( ﬁrstFlight.getFlightNumber(), secondFlight.getFlightNumber());\n\nwww.it-ebooks.info",
      "content_length": 1985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "Layer Test\n\n// veriﬁcation assertNotNull(\"actual connection\", actualConnection); assertTrue(\"IsLegal\", actualConnection.isLegal()); }\n\nThis test runs very quickly because the Service Layer is fully isolated from any underlying layers. It is also likely to be much more robust because it tests much less code.\n\nwww.it-ebooks.info\n\n345\n\nLayer Test",
      "content_length": 345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "Chapter 19\n\nxUnit Basics Patterns\n\nPatterns in This Chapter\n\nTest Deﬁ nition\n\nTest Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\n\nFour-Phase Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358\n\nAssertion Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\n\nAssertion Message . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\n\nTestcase Class. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\n\nTest Execution\n\nTest Runner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\n\nTestcase Object. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n\nTest Suite Object. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387\n\nTest Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n\nTest Enumeration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399\n\nTest Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403\n\n347\n\nwww.it-ebooks.info\n\nxUnit Basics Patterns",
      "content_length": 1183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "348\n\nTest Method\n\nChapter 19 xUnit Basics Patterns\n\nTest Method\n\nWhere do we put our test code?\n\nWe encode each test as a single Test Method on some class.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\nCreate Create\n\ntestMethod_n testMethod_n\n\nTestcase Testcase Class Class\n\nFully Automated Tests (see page 26) consist of test logic. That logic has to live somewhere before we can compile and execute it.\n\nHow It Works\n\nWe deﬁ ne each test as a method, procedure, or function that implements the four phases (see Four-Phase Test on page 358) necessary to realize a Fully Automated Test. Most notably, the Test Method must include assertions if it is to be a Self- Checking Test (see page 26).\n\nWe organize the test logic following one of the standard Test Method templates to make the type of test easily recognizable by test readers. In a Simple Success Test, we have a purely linear ﬂ ow of control from ﬁ xture setup through exercis- ing the SUT to result veriﬁ cation. In an Expected Exception Test, language-based structures direct us to error-handling code. If we reach that code, we pass the test; if we don’t, we fail it. In a Constructor Test, we simply instantiate an object and make assertions against its attributes.\n\nwww.it-ebooks.info",
      "content_length": 1528,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "Test Method\n\nWhy We Do This\n\nWe have to encode the test logic somewhere. In the procedural world, we would encode each test as a test case procedure located in a ﬁ le or module. In object - oriented programming languages, the preferred option is to encode them as methods on a suitable Testcase Class (page 373) and then to turn these Test Methods into Testcase Objects (page 382) at runtime using either Test Discovery (page 393) or Test Enumeration (page 399).\n\nWe follow the standard test templates to keep our Test Methods as simple as possible. This greatly increases their utility as system documentation (see page 23) by making it easier to ﬁ nd the description of the basic behavior of the SUT. It is a lot easier to recognize which tests describe this basic behavior if only Expected Exception Tests contain error-handling language constructs such as try/catch.\n\nImplementation Notes\n\nWe still need a way to run all the Test Methods tests on the Testcase Class. One solution is to deﬁ ne a static method on the Testcase Class that calls each of the test methods. Of course, we would also have to deal with counting the tests and determining how many passed and how many failed. Because this functionality is needed for a test suite anyway, a simple solution is to instantiate a Test Suite Object (page 387) to hold each Test Method.1 This approach is easy to imple- ment if we create an instance of the Testcase Class for each Test Method using either Test Discovery or Test Enumeration.\n\nIn statically typed languages such as Java and C#, we may have to include a throws clause as part of the Test Method declaration so the compiler won’t complain about the fact that we are not handling the checked exceptions that the SUT has declared it may throw. In effect, we tell the compiler that “The Test Runner (page 377) will deal with the exceptions.”\n\nOf course, different kinds of functionality need different kinds of Test Methods. Nevertheless, almost all tests can be boiled down to one of three basic types.\n\nVariation: Simple Success Test\n\nMost software has an obvious success scenario (or “happy path”). A Simple Success Test veriﬁ es the success scenario in a simple and easily recognized way.\n\n1 See the sidebar “There’s Always an Exception” (page 384) for an explanation of when this isn’t the case.\n\nwww.it-ebooks.info\n\n349\n\nTest Method",
      "content_length": 2354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "350\n\nTest Method\n\nChapter 19 xUnit Basics Patterns\n\nWe create an instance of the SUT and call the method(s) that we want to test. We then assert that the expected outcome has occurred. In other words, we follow the normal steps of a Four-Phase Test. What we don’t do is catch any exceptions that could happen. Instead, we let the Test Automation Framework (page 298) catch and report them. Doing otherwise would result in Obscure Tests (page 186) and would mislead the test reader by making it appear as if exceptions were expected. See Tests as Documentation for the rationale behind this approach. Another beneﬁ t of avoiding try/catch-style code is that when errors do occur, it is a lot easier to track them down because the Test Automation Framework reports the location where the actual error occurred deep in the SUT rather than the place in our test where we called an Assertion Method (page 362) such as fail or assertTrue. These kinds of errors turn out to be much easier to trouble- shoot than assertion failures.\n\nVariation: Expected Exception Test\n\nWriting software that passes the Simple Success Test is pretty straightforward. Most of the defects in software appear in the various alternative paths—especially the ones that relate to error scenarios, because these scenarios are often Untested Requirements (see Production Bugs on page 268) or Untested Code (see Produc- tion Bugs). An Expected Exception Test helps us verify that the error scenarios have been coded correctly. We set up the test ﬁ xture and exercise the SUT in each way that should result in an error. We ensure that the expected error has occurred by using whatever language construct we have available to catch the error. If the error is raised, ﬂ ow will pass to the error-handling block. This diver- sion may be enough to let the test pass, but if the type or message contents of the exception or error is important (such as when the error message will be shown to a user), we can use an Equality Assertion (see Assertion Method) to verify it. If the error is not raised, we call fail to report that the SUT failed to raise an error as expected.\n\nWe should write an Expected Exception Test for each kind of exception that the SUT is expected to raise. It may raise the error because the client (i.e., our test) has asked it to do something invalid, or it may translate or pass through an error raised by some other component it uses. We should not write an Expected Exception Test for exceptions that the SUT might raise but that we cannot force to occur on cue, because these kinds of errors should show up as test failures in the Simple Success Tests. If we want to verify that these kinds of errors are handled properly, we must ﬁ nd a way to force them to occur. The most common way to do so is to use a Test Stub (page 529) to control the indirect input of the SUT and raise the appropriate errors in the Test Stub.\n\nwww.it-ebooks.info",
      "content_length": 2927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "Test Method\n\nException tests are very interesting to write about because of the different ways the xUnit frameworks express them. JUnit 3.x provides a special Expected- Exception class to inherit from. This class forces us to create a Testcase Class for each Test Method (page 348), however, so it really doesn’t save any effort over coding a try/catch block and does result in a large number of very small Testcase Classes. Later versions of JUnit and NUnit (for .NET) provide a special ExpectedException method attribute (called an annotation in Java) to tell the Test Automation Framework to fail the test if that exception isn’t raised. This method attribute allows us to include message text if we want to specify exactly which text to expect in addition to the type of the exception.\n\nLanguages that support blocks, such as Smalltalk and Ruby, can provide special assertions to which we pass the block of code to be executed as well as the expected exception/error object. The Assertion Method implements the error-handling logic required to determine whether the error has, in fact, occurred. This makes our Test Methods much simpler, even though we may need to examine the names of the assertions more closely to see which type of test we have.\n\nVariation: Constructor Test\n\nWe would have a lot of Test Code Duplication (page 213) if every test we wrote had to verify that the objects it creates in its ﬁ xture setup phase are correctly instantiated. We avoid this step by testing the constructor(s) separately from other Test Methods whenever the constructor contains anything more com- plex than a simple ﬁ eld assignment from the constructor parameters. These Constructor Tests provide better Defect Localization (see page 22) than includ- ing constructor logic veriﬁ cation in other tests. We may need to write one or more tests for each constructor signature. Most Constructor Tests will follow a Simple Success Test template; however, we can use an Expected Exception Test to verify that the constructor correctly reports invalid arguments by raising an exception.\n\nWe should verify each attribute of the object or data structure regardless of whether we expect it to be initialized. For attributes that should be initialized, we can use an Equality Assertion to specify the correct value. For attributes that should not be initialized, we can use a Stated Outcome Assertion (see Assertion Method) appropriate to the type of the attribute [e.g., assertNull(anObjectReference) for object variables or pointers]. Note that if we are organizing our tests with one Testcase Class per Fixture (page 631), we can put each assertion into a sepa- rate Test Method to give optimal Defect Localization.\n\nwww.it-ebooks.info\n\n351\n\nTest Method",
      "content_length": 2744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "352\n\nTest Method\n\nChapter 19 xUnit Basics Patterns\n\nVariation: Dependency Initialization Test\n\nWhen we have an object with a substitutable dependency, we need to make sure that the attribute that holds the reference to the depended-on component (DOC) is initialized to the real DOC when the software is run in production. A Depen- dency Initialization Test is a Constructor Test that asserts that this attribute is initialized correctly. It is often done in a different Test Method from the normal Constructor Tests to improve its visibility.\n\nExample: Simple Success Test\n\nThe following example illustrates a test where the novice test automater has included code to catch exceptions that he or she knows might occur (or that the test automater might have encountered while debugging the code).\n\npublic void testFlightMileage_asKm() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); try { // exercise SUT newFlight.setMileage(1122); // verify results int actualKilometres = newFlight.getMileageAsKm(); int expectedKilometres = 1810; // verify results assertEquals( expectedKilometres, actualKilometres); } catch (InvalidArgumentException e) { fail(e.getMessage()); } catch (ArrayStoreException e) { fail(e.getMessage()); } }\n\nThe majority of the code is unnecessary and just obscures the intent of the test. Luckily for us, all of this exception handling can be avoided. xUnit has built-in support for catching unexpected exceptions. We can rip out all the exception- handling code and let the Test Automation Framework catch any unexpected exception that might be thrown. Unexpected exceptions are counted as test errors because the test terminates in a way we didn’t anticipate. This is useful information and is not considered to be any more severe than a test failure.\n\npublic void testFlightMileage_asKm() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm();\n\nwww.it-ebooks.info",
      "content_length": 2063,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "Test Method\n\n// verify results int expectedKilometres = 1810; assertEquals( expectedKilometres, actualKilometres); }\n\nThis example is in Java (a statically typed language), so we had to declare that the SUT may throw an exception as part of the Test Method signature.\n\nExample: Expected Exception Test Using try/catch\n\nThe following example is a partially complete test to verify an exception case. The novice test automater has set up the right test condition to cause the SUT to raise an error.\n\npublic void testSetMileage_invalidInput() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); // exercise SUT newFlight.setMileage(-1122); // invalid // how do we verify an exception was thrown? }\n\nBecause the Test Automation Framework will catch the exception and fail the test, the Test Runner will not exhibit the green bar even though the SUT’s behavior is correct. We can introduce an error-handling block around the exercise phase of the test and use it to invert the pass/fail criteria (pass when the exception is thrown; fail when it is not). Here’s how to verify that the SUT fails as expected in JUnit 3.x:\n\npublic void testSetMileage_invalidInput() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); try { // exercise SUT newFlight.setMileage(-1122); fail(\"Should have thrown InvalidInputException\"); } catch( InvalidArgumentException e) { // verify results assertEquals( \"Flight mileage must be positive\", e.getMessage()); } }\n\nThis style of try/catch can be used only in languages that allow us to specify exactly which exception to catch. It won’t work if we want to catch a generic exception or the same exception that the Assertion Method fail throws, because these excep- tions will send us into the catch clause. In these cases we need to use the same style of Expected Exception Test as used in tests of Custom Assertions (page 474).\n\nwww.it-ebooks.info\n\n353\n\nTest Method",
      "content_length": 1957,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "354\n\nTest Method\n\nChapter 19 xUnit Basics Patterns\n\npublic void testSetMileage_invalidInput2() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); try { // exercise SUT newFlight.setMileage(-1122); // cannot fail() here if SUT throws same kind of exception } catch( AssertionFailedError e) { // verify results assertEquals( \"Flight mileage must be positive\", e.getMessage()); return; } fail(\"Should have thrown InvalidInputException\"); }\n\nExample: Expected Exception Test Using Method Attributes\n\nNUnit provides a method attribute that lets us write an Expected Exception Test without forcing us to code a try/catch block explicitly.\n\n[Test] [ExpectedException(typeof( InvalidArgumentException), \"Flight mileage must be > zero\")] public void testSetMileage_invalidInput_AttributeWithMessage() { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); // exercise SUT newFlight.setMileage(-1122); }\n\nThis approach does make the test much more compact but doesn’t provide a way to specify anything but the type of the exception or the message it contains. If we want to make any assertions on other contents of the exception (to avoid Sensitive Equality; see Fragile Test on page 239), we’ll need to use try/catch.\n\nExample: Expected Exception Test Using Block Closure\n\nSmalltalk’s SUnit provides another mechanism to achieve the same thing:\n\ntestSetMileageWithInvalidInput self should: [Flight new mileage: -1122] raise: RuntimeError new 'Should have raised error'\n\nBecause Smalltalk supports block closures, we pass the block of code to be executed to the method should:raise: along with the expected Exception object. Ruby’s Test::Unit uses the same approach:\n\nwww.it-ebooks.info",
      "content_length": 1725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "Test Method\n\ndef testSetMileage_invalidInput ﬂight = Flight.new(); assert_raises( RuntimeError, \"Should have raised error\") do ﬂight.setMileage(-1122) end end\n\nThe code between the do/end pair is a closure that is executed by the assert_raises method. If it doesn’t raise an instance of the ﬁ rst argument (the class RuntimeError), the test fails and presents the error message supplied.\n\nExample: Constructor Test\n\nIn this example, we need to build a ﬂ ight to test the conversion of the ﬂ ight distance from miles to kilometers. First, we’ll make sure the ﬂ ight is constructed properly.\n\npublic void testFlightMileage_asKm2() throws Exception { // set up ﬁxture // exercise constructor Flight newFlight = new Flight(validFlightNumber); // verify constructed object assertEquals(validFlightNumber, newFlight.number); assertEquals(\"\", newFlight.airlineCode); assertNull(newFlight.airline); // set up mileage newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); // verify results int expectedKilometres = 1810; assertEquals( expectedKilometres, actualKilometres); // now try it with a canceled ﬂight newFlight.cancel(); try { newFlight.getMileageAsKm(); fail(\"Expected exception\"); } catch (InvalidRequestException e) { assertEquals( \"Cannot get cancelled ﬂight mileage\", e.getMessage()); } }\n\nThis test is not a Single-Condition Test (see page 45) because it examines both object construction and distance conversion behavior. If object construction fails, we won’t know which issue was the cause of the failure until we start debugging the test.\n\nwww.it-ebooks.info\n\n355\n\nTest Method",
      "content_length": 1642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "356\n\nTest Method\n\nChapter 19 xUnit Basics Patterns\n\nIt would be better to separate this Eager Test (see Assertion Roulette on page 224) into two tests, each of which is a Single-Condition Test. This is most easily done by cloning the Test Method, renaming each copy to reﬂ ect what it would do if it were a Single-Condition Test, and then removing any code that doesn’t satisfy that goal.\n\nHere’s an example of a simple Constructor Test:\n\npublic void testFlightConstructor_OK() throws Exception { // set up ﬁxture // exercise SUT Flight newFlight = new Flight(validFlightNumber); // verify results assertEquals( validFlightNumber, newFlight.number ); assertEquals( \"\", newFlight.airlineCode ); assertNull( newFlight.airline ); }\n\nWhile we are at it, we might as well specify what should occur if an invalid argument is passed to the constructor by using the Expected Exception Test template for our Constructor Test:\n\npublic void testFlightConstructor_badInput() { // set up ﬁxture BigDecimal invalidFlightNumber = new BigDecimal(-1023); // exercise SUT try { Flight newFlight = new Flight(invalidFlightNumber); fail(\"Didn't catch negative ﬂight number!\"); } catch (InvalidArgumentException e) { // verify results assertEquals( \"Flight numbers must be positive\", e.getMessage()); } }\n\nNow that we know that our constructor logic is well tested, we are ready to write our Simple Success Test for our mileage translation functionality. Note how much simpler it has become because we can focus on verifying the business logic:\n\npublic void testFlightMileage_asKm() throws Exception { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); // verify results int expectedKilometres = 1810; assertEquals( expectedKilometres, actualKilometres); }\n\nwww.it-ebooks.info",
      "content_length": 1881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "Test Method\n\nSo what happens if the constructor logic is defective? This test will likely fail because its output depends on the value passed to the constructor. The con- structor test will also fail. That failure will tell us to look at the constructor logic ﬁ rst. Once that problem is ﬁ xed, this test will likely pass. If it doesn’t, then we can focus on ﬁ xing the getMileageAsKm method logic. This is a good example of Defect Localization.\n\nwww.it-ebooks.info\n\n357\n\nTest Method",
      "content_length": 483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "358\n\nFour-Phase Test\n\nChapter 19 xUnit Basics Patterns\n\nFour-Phase Test\n\nHow do we structure our test logic to make what we are testing obvious?\n\nWe structure each test with four distinct parts executed in sequence.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test\n\nSuite Suite Object Object\n\nSetup Setup\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTeardown Teardown\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nHow It Works\n\nWe design each test to have four distinct phases that are executed in sequence: ﬁ xture setup, exercise SUT, result veriﬁ cation, and ﬁ xture teardown.\n\nIn the ﬁ rst phase, we set up the test ﬁ xture (the “before” picture) that is required for the SUT to exhibit the expected behavior as well as any- thing you need to put in place to be able to observe the actual outcome (such as using a Test Double; see page 522).\n\nIn the second phase, we interact with the SUT.\n\nIn the third phase, we do whatever is necessary to determine whether\n\nthe expected outcome has been obtained.\n\nIn the fourth phase, we tear down the test ﬁ xture to put the world back\n\ninto the state in which we found it.\n\nWhy We Do This\n\nThe test reader must be able to quickly determine what behavior the test is verifying. It can be very confusing when various behaviors of the SUT are being invoked—some to set up the pre-test state (ﬁ xture) of the SUT, others to exercise\n\nwww.it-ebooks.info",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "Four-Phase Test\n\nthe SUT, and yet others to verify the post-test state of the SUT. Clearly identifying the four phases makes the intent of the test much easier to see.\n\nThe ﬁ xture setup phase of the test establishes the SUT’s state prior to the test, which is an important input to the test. The exercise SUT phase is where we actu- ally run the software we are testing. When reading the test, we need to see which software is being run. The result veriﬁ cation phase of the test is where we specify the expected outcome. The ﬁ nal phase, ﬁ xture teardown, is all about housekeeping. We wouldn’t want to obscure the important test logic with it because it is com- pletely irrelevant from the perspective of Tests as Documentation (see page 23). We should avoid the temptation to test as much functionality as pos- sible in a single Test Method (page 348) because that can result in Obscure Tests (page 186). In fact, it is preferable to have many small Single-Condition Tests (see page 45). Using comments to mark the phases of a Four-Phase Test is a good source of self-discipline, in that it makes it very obvious when our tests are not Single-Condition Tests. It will be self-evident if we have multiple exercise SUT phases separated by result veriﬁ cation phases or if we have inter- spersed ﬁ xture setup and exercise SUT phases. Sure, the tests may work—but they will provide less Defect Localization (see page 22) than if we have a bunch of independent Single-Condition Tests.\n\nImplementation Notes\n\nWe have several options for implementing the Four-Phase Test. In the simplest case, each test is completely free-standing. All four phases of the test are con- tained within the body of the Test Method. This structure implies we are using In-line Setup (page 408) and either Garbage-Collected Teardown (page 500) or In-line Teardown (page 509). It is the most appropriate choice when we are using Testcase Class per Class (page 617) or Testcase Class per Feature (page 624) to organize our Test Methods.\n\nThe other choice is to take advantage of the Test Automation Framework’s (page 298) support for Implicit Setup (page 424) and Implicit Teardown (page 516). We factor out the common ﬁ xture setup and ﬁ xture teardown logic into setUp and tearDown methods on the Testcase Class (page 373). This leaves only the exercise SUT and result veriﬁ cation phases in the Test Method. This approach is an appropriate choice when we are using Testcase Class per Fixture (page 631). We can also use this approach to set up common parts of the ﬁ xture when using Testcase Class per Class (page 617) or Testcase Class per Feature or to tear down the ﬁ xture when using Automated Teardown (page 503).\n\nwww.it-ebooks.info\n\n359\n\nFour-Phase Test",
      "content_length": 2738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "360\n\nFour-Phase Test\n\nChapter 19 xUnit Basics Patterns\n\nExample: Four-Phase Test (In-line)\n\nHere is an example of a test that is clearly a Four-Phase Test:\n\npublic void testGetFlightsByOriginAirport_NoFlights_inline() throws Exception { // Fixture setup NonTxFlightMngtFacade facade =new NonTxFlightMngtFacade(); BigDecimal airportId = facade.createTestAirport(\"1OF\"); try { // Exercise system List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(airportId); // Verify outcome assertEquals( 0, ﬂightsAtDestination1.size() ); } ﬁnally { // Fixture teardown facade.removeAirport( airportId ); } }\n\nAll four phases of the Four-Phase Test are included as in-line code. Because the calls to Assertion Methods (page 362) raise exceptions, we need to surround the ﬁ xture teardown part of the Test Method with a try/ﬁ nally construct to ensure that it is run in all cases.\n\nExample: Four-Phase Test (Implicit Setup/Teardown)\n\nHere is the same Four-Phase Test with the ﬁ xture setup and ﬁ xture teardown logic moved out of the Test Method:\n\nNonTxFlightMngtFacade facade = new NonTxFlightMngtFacade(); private BigDecimal airportId;\n\nprotected void setUp() throws Exception { // Fixture setup super.setUp(); airportId = facade.createTestAirport(\"1OF\"); }\n\npublic void testGetFlightsByOriginAirport_NoFlights_implicit() throws Exception { // Exercise SUT List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(airportId); // Verify outcome assertEquals( 0, ﬂightsAtDestination1.size() ); }\n\nprotected void tearDown() throws Exception {\n\nwww.it-ebooks.info",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "Four-Phase Test\n\n// Fixture teardown facade.removeAirport(airportId); super.tearDown(); }\n\nBecause the tearDown method is called automatically even after test failures, we don’t need the try/ﬁ nally construct inside the Test Method. The downside, however, is that references to our ﬁ xture must be held in instance variables rather than local variables.\n\nwww.it-ebooks.info\n\n361\n\nFour-Phase Test",
      "content_length": 395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "362\n\nAssertion Method\n\nChapter 19 xUnit Basics Patterns\n\nAssertion Method\n\nHow do we make tests self-checking?\n\nWe call a utility method to evaluate whether an expected outcome has been achieved.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nTest failed Test failed\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nSetup Setup\n\nExercise Exercise\n\nTest Test Suite Suite Object Object\n\nVerify Verify\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTeardown Teardown\n\nTestcase Testcase Object Object\n\nAssertion Assertion Method Method\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nA key part of writing Fully Automated Tests (see page 26) is to make them Self- Checking Tests (see page 26) to avoid having to inspect the outcome of each test for correctness each time it is run. This strategy involves ﬁ nding a way to express the expected outcome so that it can be veriﬁ ed automatically by the test itself.\n\nAssertion Methods give us a way to express the expected outcome in a way that is both executable by the computer and useful to the human reader, who can then use the Tests as Documentation (see page 23).\n\nHow It Works\n\nWe encode the expected outcome of the test as a series of assertions that state what should be true for the test to pass. The assertions are realized as calls to Assertion Methods that encapsulate the mechanism that causes the test to fail. The Assertion Methods may be provided by the Test Automation Framework (page 298) or by the test automater as Custom Assertions (page 474).\n\nwww.it-ebooks.info",
      "content_length": 1669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "Assertion Method\n\nWhy We Do This\n\nEncoding the expected outcome using Conditional Test Logic (page 200) is very verbose and makes tests hard to read and understand. It is also much more likely to lead to Test Code Duplication (page 213) and Buggy Tests (page 260). Asser- tion Methods help us avoid these issues by moving that complexity into reusable Test Utility Methods (page 599); these methods can then be veriﬁ ed as working correctly using Test Utility Tests (see Test Utility Method).\n\nImplementation Notes\n\nAlthough all members of the xUnit family provide Assertion Methods, they do so with a fair degree of variability. The key implementation considerations are as follows:\n\nHow to call the Assertion Methods\n\nHow to choose the best Assertion Method to call\n\nWhat information to include in the Assertion Message (page 370)\n\nCalling Built-in Assertion Methods\n\nThe way the Assertion Methods are called from within the Test Method (page 348) varies from language to language and from framework to framework. The lan- guage features determine what is possible and preferable, while the framework builders chose which options to use. The names these developers chose for the Assertion Methods were inﬂ uenced by how they chose to access them. Here are the most common options for accessing the Assertion Methods:\n\nTheAssertion Methods are inherited from a Testcase Superclass (page 638) provided by the framework. Such methods may be invoked as though they were provided locally on the Testcase Class (page 373). The original version of Java’s JUnit, for example, used this approach by providing a Testcase Superclass that inherits from the class Assert, which contains the actual Assertion Methods.\n\nThe Assertion Methods are provided via a globally accessible class or module. They are invoked using the class or module name to fully qualify the Assertion Method name. NUnit, for example, uses this approach [e.g., Assert.isTrue(x);]. JUnit does allow assertions to be invoked as static\n\nwww.it-ebooks.info\n\n363\n\nAssertion Method",
      "content_length": 2037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "364\n\nAssertion Method\n\nChapter 19 xUnit Basics Patterns\n\nmethods on the Assert class [e.g., Assert.assertTrue(x)] but this is not usually necessary because they are inherited via the Testcase Superclass.\n\nThe Assertion Methods are provided as “mix-ins” or macros. Ruby’s Test:: Unit, for example, provides the Assertion Methods in a module called Assert that can be included in any class,2 thereby allowing the Assertion Methods to be used as though deﬁ ned within the Testcase Class [e.g., assert_equal(a,b)]. CppUnit, by contrast, deﬁ nes the Assertion Methods as macros, which are expanded before the compiler sees the code.\n\nAssertion Messages\n\nAssertion Methods typically take an optional Assertion Message as a text param- eter that is included in the output when the assertion fails. This structure allows the test automater to explain to the test maintainer exactly which Assertion Method failed and to better explain what should have occurred. The error detected by the test will be much easier to debug if the Assertion Method provides more informa- tion about why it failed. Choosing the right Assertion Method goes a long way toward achieving this goal because many of the built-in Assertion Methods provide useful diagnostic information about the values of the arguments. This is especially true for Equality Assertions.\n\nOne of the biggest differences between members of the xUnit family is where the optional Assertion Message appears in the argument list. Most members tack it on to the end as an optional argument. JUnit, however, makes the Asser- tion Message the ﬁ rst argument when it is present.\n\nChoosing the Right Assertion\n\nWe have two goals for the calls to Assertion Methods in our Test Methods:\n\nFail the test when something other than the expected outcome occurs\n\nDocument how the SUT is supposed to behave (i.e., Tests as Documen-\n\ntation)\n\nTo achieve these goals we must strive to use the most appropriate Assertion Method. While the syntax and naming conventions vary from one member of the xUnit family to the next, most provide a basic set of assertions that fall into the following categories:\n\n2 This approach is particularly useful when we are building Mock Objects (page 544) because these objects are outside the Testcase Class but need to invoke Assertion Methods.\n\nwww.it-ebooks.info",
      "content_length": 2323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "Assertion Method\n\nSingle-Outcome Assertions such as fail; these take no arguments because they always behave the same way.\n\nStated Outcome Assertions such as assertNotNull(anObjectReference) and assertTrue(aBooleanExpression); these compare a single argument to an outcome implied by the method name.\n\nExpected Exception Assertions such as assert_raises(expectedError) { codeToExecute }; these evaluate a block of code and a single expected exception argument.\n\nEquality Assertions such as assertEqual(expected, actual); these compare two objects or values for equality.\n\nFuzzy Equality Assertions such as assertEqual(expected, actual, tolerance); these determine whether two values are “close enough” to each other by using a “tolerance” or “comparison mask.”\n\nVariation: Equality Assertion\n\nEquality Assertions are the most common examples of Assertion Methods. They are used to compare the actual outcome with an expected outcome that is expressed in the form of a constant Literal Value (page 714) or an Expected Object (see State Veriﬁ cation on page 462). By convention, the expected value is speci- ﬁ ed ﬁ rst and the actual value follows it. The diagnostic message that is generated by the Test Automation Framework makes sense only when they are provided in this order. The equality of the two objects is usually determined by invoking the equals method on the expected object. If the SUT’s deﬁ nition of equals is not what we want to use in our tests, either we can make Equality Assertions on individual ﬁ elds of the object or we can implement our test-speciﬁ c equality on a Test-Speciﬁ c Subclass (page 579) of the Expected Object.\n\nVariation: Fuzzy Equality Assertion\n\nWhen we cannot guarantee an exact match due to variations in precision or expected variations in value, it may be appropriate to use a Fuzzy Equality Assertion. Typically, these assertions look just like Equality Assertions with the addition of an extra “tolerance” or “comparison map” parameter that speciﬁ es how close the actual argument must be to the expected one. The most common example of a Fuzzy Equality Assertion is the comparison of ﬂ oating-point num- bers where the limitations of arithmetic precision need to be accounted for by providing a tolerance (the maximum acceptable distance between the two values).\n\nwww.it-ebooks.info\n\n365\n\nAssertion Method",
      "content_length": 2350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "366\n\nAssertion Method\n\nChapter 19 xUnit Basics Patterns\n\nWe use the same approach when comparing XML documents where direct string comparisons may result in failure owing to certain ﬁ elds having unpre- dictable content. In this case, the “fuzz” speciﬁ cation is a “comparison schema” that speciﬁ es which ﬁ elds need to match or which ﬁ elds should be ignored. This kind of Equality Assertion is very similar to asserting that a string conforms to a regular expression or other form of pattern matching.\n\nVariation: Stated Outcome Assertion\n\nStated Outcome Assertions are a way of saying exactly what the outcome should be without passing an expected value as an argument. The outcome must be common enough to warrant a special Assertion Method. The most common examples are as follows:\n\nassertTrue(aBooleanExpression), which fails if the expression evaluates to FALSE\n\nassertNotNull(anObjectReference), which fails if the objectReference doesn’t refer to a valid object\n\nStated Outcome Assertions are often used as Guard Assertions (page 490) to avoid Conditional Test Logic.\n\nVariation: Expected Exception Assertion\n\nIn languages that support block closures, we can use a variation of a Stated Outcome Assertion that takes an additional parameter specifying the kind of exception we expect. We can use this Expected Exception Assertion to say, “Run this block and verify that the following exception is thrown.” This format is more compact than using a try/catch construct. Some typical examples follow:\n\nshould: [aBlockToExecute] raise: expectedException in Smalltalk’s SUnit\n\nassert_raises( expectedError) { codeToExecute } in Ruby’s Test::Unit\n\nVariation: Single-Outcome Assertion\n\nA Single-Outcome Assertion always behaves the same way. The most commonly used Single-Outcome Assertion is fail, which causes a test to be treated as a failure. It is typically used in two circumstances:\n\nAs an Unﬁ nished Test Assertion (page 494) when a test is ﬁ rst identiﬁ ed and implemented as a nearly empty Test Method. By including a call to fail, we can have the Test Runner (page 377) remind us that we still have a test to ﬁ nish writing.\n\nwww.it-ebooks.info",
      "content_length": 2157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "Assertion Method\n\nAs part of a try/catch (or equivalent) block in an Expected Exception Test (see Test Method) by including a call to fail in the try block immediately after the call that is expected to throw an exception. If we don’t want to assert something about the exception that was caught, we can avoid an empty catch block by using the Single-Outcome Assertion success to document that this is the expected outcome. Assertion Method\n\nOne circumstance in which we really should not use Single-Outcome Assertions is in Conditional Test Logic. There is almost never a good reason to include conditional logic in a Test Method, as there is usually a more declarative way to handle this situation using other styles of Assertion Methods. For example, use of Guard Assertions results in tests that are more easily understood and less likely to yield incorrect results.\n\nMotivating Example\n\nThe following example illustrates the kind of code that would be required for each item we wanted to verify if we did not have Assertion Methods. All we really want to do is this:\n\nif (x.equals(y)) { throw new AssertionFailedError( \"expected: <\" + x.toString() + \"> but found: <\" + y.toString() + \">\"); } else { // Okay, continue // ... }\n\nUnfortunately, this code will cause a NullPointerException if x is null, and it would be hard to distinguish this exception from an error in the SUT. Thus we need to put some guard clauses around this functionality so that we always throw an AssertionFailedException:\n\nif (x == null) { // cannot do null.equals(null) if (y == null ) { // they are both null so equal return; } else { throw new AssertionFailedError( \"expected null but found: <\" + y.toString() +\">\"); } } else if (!x.equals(y)) { // comparable but not equal! throw new AssertionFailedError( \"expected: <\" + x.toString() + \"> but found: <\" + y.toString() + \">\");\n\n} // equal\n\nwww.it-ebooks.info\n\n367",
      "content_length": 1895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "368\n\nAssertion Method\n\nChapter 19 xUnit Basics Patterns\n\nYikes! That got pretty messy. And we’ll have to do the same thing for every attribute we want to verify? This is not good. There must be a better way.\n\nRefactoring Notes\n\nLuckily for us, the inventors of xUnit recognized this problem and did the requisite Extract Method [Fowler] refactoring to create a library of Assertion Methods that we can call instead. We simply replace the mess of in-line if statements and thrown exceptions with a call to the appropriate Assertion Method. The next example is the code for the JUnit assertEquals method. Although the intent of this example is the same as the code we wrote earlier, it has been rewritten in terms of guard clauses that identify when things are equal.\n\n/** * Asserts that two objects are equal. If they are not, * an AssertionFailedError is thrown with the given message. */ static public void assertEquals(String message, Object expected, Object actual) { if (expected == null && actual == null) return; if (expected != null && expected.equals(actual)) return; failNotEquals(message, expected, actual); }\n\nThe method failNotEquals is a Test Utility Method that fails the test and provides a diagnostic assertion message.\n\nExample: Equality Assertion\n\nHere is the same assertion logic recoded to take advantage of JUnit’s Equality Assertion:\n\nassertEquals( x, y );\n\nHere is the same assertion coded in C#. Note the classname qualiﬁ er and the resulting difference in the method name:\n\nAssert.AreEqual( x, y );\n\nExample: Fuzzy Equality Assertion\n\nTo compare two ﬂ oating-point numbers (which are rarely ever really equal), we specify the acceptable differences using a Fuzzy Equality Assertion:\n\nwww.it-ebooks.info",
      "content_length": 1727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "Assertion Method\n\nassertEquals( 3.1415, diameter/2/radius, 0.001); assertEquals( expectedXml, actualXml, elementsToCompare );\n\nExample: Stated Outcome Assertion\n\nTo insist that a particular outcome has occurred, we use a Stated Outcome Assertion:\n\nassertNotNull( a ); assertTrue( b > c ); assertNonZero( b );\n\nExample: Expected Exception Assertion\n\nHere is an example of how we verify that the correct exception was raised when we have blocks. In Smalltalk’s SUnit, it looks like this:\n\nself should: [Flight new mileage: -1122] raise: RuntimeError new 'Should have raised error'\n\nThe should: indicates the block of code to run (surrounded by square brackets), while the raise: speciﬁ es the expected exception object. In Ruby, it looks like this:\n\nassert_raises( RuntimeError, \"Should have raised error\") {ﬂight.setMileage(-1122) }\n\nThe Ruby language syntax also lets us use this “control structure”-style syntax by delimiting the block using do/end instead of curly braces:\n\nassert_raises( RuntimeError, \"Should have raised error\") do ﬂight.setMileage(-1122) end\n\nExample: Single-Outcome Assertion\n\nTo fail the test, use the Single Outcome Assertion:\n\nfail( \"Expected an exception\" ); unﬁnishedTest();\n\nwww.it-ebooks.info\n\n369\n\nAssertion Method",
      "content_length": 1245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "370\n\nAssertion Message\n\nChapter 19 xUnit Basics Patterns\n\nAssertion Message\n\nHow do we structure our test logic to know which assertion failed?\n\nWe include a descriptive string argument in each call to an Assertion Method.\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nTest Failed: Test Failed: message message\n\nSuite Suite\n\ntestMethod_1 testMethod_1\n\nrun run\n\nCreate Create\n\nSetup Setup Exercise Exercise Test Test Verify Verify Suite Suite Teardown Teardown Object Object\n\ntestMethod_n testMethod_n\n\nTestcase Testcase Class Class\n\nmessage message\n\nmessage message\n\nAssertion Assertion Method Method\n\nTestcase Testcase Object Object testMethod_n testMethod_n\n\nFixture Fixture\n\nSUT SUT\n\nWe make tests Self-Checking (see page 26) by including calls to Assertion Meth- ods (page 362) that specify the expected outcome. When a test fails, the Test Runner (page 377) writes an entry to the test result log.\n\nA well-crafted Assertion Message makes it very easy to determine which asser-\n\ntion failed and exactly what the symptoms were when the failure happened.\n\nHow It Works\n\nEvery Assertion Method takes an optional string parameter that is included in the failure log. When the condition being asserted is not true, the Assertion Message is output to the Test Runner’s log along with whatever output the assertion method normally generates.\n\nWhen to Use It\n\nThere are two schools of thought on this subject. Test drivers who belong to the “single assertion per Test Method” school believe that they don’t need to include Assertion Messages because only one assertion can possibly fail and, therefore, they always know exactly which assertion happened. They count on the Assertion Method to include the arguments (e.g., expected “x” but was “y”) but they don’t need to include a message.\n\nwww.it-ebooks.info",
      "content_length": 1844,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "Assertion Message\n\nConversely, people who ﬁ nd themselves coding several or many assertion method calls in their tests should strongly consider including a message that at least distinguishes which assertion failed. This information is especially impor- tant if the tests are frequently run using a Command-Line Test Runner (see Test Runner), which rarely provides failure location information.\n\nImplementation Notes\n\nIt is easy to state that we need a message for each assertion method call—but what should we say in the message? It is useful to take a moment as we write each assertion and ask ourselves what the person reading the failure log would hope to get out of it.\n\nVariation: Assertion-Identifying Message\n\nWhen we include several assertions of the same type in the same Test Method (page 348), we make it more difﬁ cult to determine exactly which one failed the test. By including some unique text in each Assertion Message, we can make it very easy to determine which assertion method call failed. A common practice is to use the name of the variable or attribute being asserted on as the message. This technique is very simple and requires very little thought. Another option is to number the assertions. This information would certainly be unique but understanding it may be less intuitive as we would have to look at the code to determine which assertion was failing.\n\nVariation: Expectation-Describing Message\n\nWhen a test fails, we know what has actually happened. The big question is, “What should have happened?” There are several ways of documenting the expected behavior for the test reader. For example, we could place comments in the test code. A better solution is to include a description of the expectation in the Assertion Message. While this is done automatically for an Equality Asser- tion (see Assertion Method), we need to provide this information ourselves for any Stated Outcome Assertions (see Assertion Method).\n\nVariation: Argument-Describing Message\n\nSome types of Assertion Methods provide less helpful failure messages than others. Among the worst are Stated Outcome Assertions such as assertTrue (aBooleanExpression). When they fail, all we know is that the stated outcome did not occur. In these cases we can include the expression that was being evalu- ated (including the actual values) as part of the Assertion Message text. The\n\nwww.it-ebooks.info\n\n371\n\nAssertion Message",
      "content_length": 2418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "372\n\nAssertion Message\n\nChapter 19 xUnit Basics Patterns\n\ntest maintainer can then examine the failure log and determine what was being evaluated and why it caused the test to fail.\n\nMotivating Example\n\nassertTrue( a > b ); assertTrue( b > c );\n\nThis code emits a failure message—something like “Assertion Failed.” From this output, we cannot even tell which of the two Assertion Messages failed. Not very useful, is it?\n\nRefactoring Notes\n\nFixing this problem is a simple matter of adding one more parameter to each Assertion Method call. In this case, we want to communicate that we are expecting “a” to be greater than “b.” Of course, it would also be useful to be able to see what the values of “a” and “b” actually were. We can add both pieces of information into the Assertion Message through some judicious string concatenation.\n\nExample: Expectation-Describing Message\n\nHere is the same test with the Argument-Describing Message added:\n\nassertTrue( \"Expected a > b but a was '\" + a.toString() + \"' and b was '\" + b.toString() + \"'\", a.gt(b) ); assertTrue( \"Expected b > c but b was '\" + b.toString() + \"' and c was '\" + c.toString + \"'\", b > c );\n\nThis will now result in a useful failure message:\n\nAssertion Failed. Expected a > b but a was '17' and b was '19'.\n\nOf course, this output would be even more meaningful if the variables had Intent-Revealing Names [SBPP]!\n\nwww.it-ebooks.info",
      "content_length": 1396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "Testcase Class\n\nTestcase Class\n\nWhere do we put our test code?\n\nWe group a set of related Test Methods on a single Testcase Class.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nWe put our test logic into Test Methods (page 348) but those Test Methods need to be associated with a class. A Testcase Class gives us a place to host those methods that we can later turn into Testcase Objects (page 382).\n\nHow It Works\n\nWe collect all Test Methods that are related in some way onto a special kind of class, the Testcase Class. At runtime, the Testcase Class acts as a Test Suite Factory (see Test Enumeration on page 399) that creates a Testcase Object for each Test Method. It adds all of these objects to a Test Suite Object (page 387) that the Test Runner (page 377) will use to run them all.\n\nWhy We Do This\n\nIn object-oriented languages, we prefer to put our Test Methods onto a class rather than having them as global functions or procedures (even if that is allowed). By making them instance methods of a Testcase Class, we can create a Testcase Object for each test by instantiating the Testcase Class once for each Test Method. This strategy allows us to manipulate the Test Methods at runtime.\n\nwww.it-ebooks.info\n\n373\n\nTestcase Class\n\nAlso known as: Test Fixture",
      "content_length": 1621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "374\n\nTestcase Class\n\nChapter 19 xUnit Basics Patterns\n\nClass–Instance Duality\n\nBack in high school physics, we learned about the “wave–particle duality” of light. Sometimes light acts like a particle (e.g., going through a small aperture), and sometimes it acts like a wave (e.g., rainbows). The behavior of Testcase Classes (page 373) sometimes reminds me of this concept. Let me explain why.\n\nDevelopers new to xUnit often ask, “Why is the class we subclass called TestCase when we have several Test Methods on it? Shouldn’t it be called TestSuite?” These questions make a lot of sense when we are focused primarily on the view of the class when we are writing the test code as opposed to when we are running the code.\n\nWhen we are writing test code, we concentrate on the Test Methods. The Testcase Class is primarily just a place to put the methods. About the only time we think of objects is when we use Implicit Setup (page 424) and need to create ﬁ elds (instance variables) to hold them between the invo- cation of the setUp method and when they are used in the Test Method. When developers new to xUnit test automation are writing their ﬁ rst tests, they tend to code by example. Following an existing example is a good way to get something working quickly but it doesn’t necessarily help the developer understand what is really going on.\n\nAt runtime, the xUnit framework typically creates one instance of the Testcase Class for each Test Method. The Testcase Class acts as a Test Suite Factory (see Test Enumeration on page 399) that builds a Test Suite Object (page 387) containing all the instances of itself, one instance for each Test Method. Now, it’s not very often that a static method on a class returns an instance of another class containing many instances of itself. If this behavior wasn’t odd enough, the fact that xUnit reports the test failures using the Test Method name can be enough to obscure from many test automaters the existence of “objects inside.”\n\nWhen we examine the object relationships at runtime, things become a bit clearer. The Test Suite Object returned by the Test Suite Factory contains one or more Testcase Objects (page 382). So far, so good. Each of these objects is an instance of our Testcase Class. Each instance is conﬁ gured to run one of the Test Methods. More importantly, each will run a differ- ent Test Method. (How this happens is described in more detail in Test Discovery on page 393.) So each instance of our Testcase Class is, indeed, a test case. The Test Methods are just how we tell each instance what it should test.\n\nwww.it-ebooks.info",
      "content_length": 2603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "Testcase Class\n\nFurther Reading Martin Fowler has a great piece on his blog about this issue called “JUnit New Instance” [JNI].\n\nWe could, of course, implement each Test Method on a separate class—but that creates additional overhead and clutters the class namespace. It also makes it harder (although not impossible) to reuse functionality between tests.\n\nImplementation Notes\n\nMost of the complexity of writing tests involves how to write the Test Methods: what to include in-line and what to factor out into Test Utility Methods (page 599), how to Isolate the SUT (see page 43), and so on.\n\nThe real magic associated with the Testcase Class occurs at runtime and is described in Testcase Object and Test Runner. As far as we are concerned, all we have to do is write some Test Methods that contain our test logic and let the Test Runner work its magic. We can avoid Test Code Duplica- tion (page 213) by using an Extract Method [Fowler] refactoring to factor out common code into Test Utility Methods. These methods can be left on the Testcase Class or they can be moved to an Abstract Testcase superclass (see Test- case Superclass on page 638), a Test Helper class (page 643), or a Test Helper Mixin (see Testcase Superclass).\n\nExample: Testcase Class\n\nHere is an example of a simple Testcase Class:\n\npublic class TestScheduleFlight extends TestCase {\n\npublic void testUnscheduled_shouldEndUpInScheduled() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.schedule(); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); }\n\npublic void testScheduledState_shouldThrowInvalidRequestEx() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.schedule(); fail(\"not allowed in scheduled state\");\n\nwww.it-ebooks.info\n\n375\n\nTestcase Class",
      "content_length": 1830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "376\n\nTestcase Class\n\nChapter 19 xUnit Basics Patterns\n\n} catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testAwaitingApproval_shouldThrowInvalidRequestEx() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.schedule(); fail(\"not allowed in schedule state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } } }\n\nFurther Reading\n\nIn some variants of xUnit (most notably VbUnit and NUnit), the Testcase Class is called a test ﬁ xture. This usage should not be confused with the test ﬁ xture (or test context) that consists of everything we need to have in place before we can start exercising the SUT.3 Neither should it be confused with the ﬁ xture term as used by the Fit framework, which is the Adapter [GOF] that interacts with the Fit table and thereby implements a Data-Driven Test (page 288) Interpreter [GOF].\n\n3 These are the pre-conditions of the test.\n\nwww.it-ebooks.info",
      "content_length": 1214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "Test Runner\n\nTest Runner\n\nHow do we run the tests?\n\nWe deﬁ ne an application that instantiates a Test Suite Object and executes all the Testcase Objects it contains.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nFixture Fixture\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nAssuming we have deﬁ ned our Test Methods (page 348) on one or more Testcase Classes (page 373), how do we actually cause the Test Automation Frameworks (page 298) to run our tests?\n\nHow It Works\n\nEach member of the xUnit family of Test Automation Frameworks provides some form of command-line or graphical application that can be used to run our automated tests and report on the results. The Test Runner uses Test Enu- meration (page 399), Test Discovery (page 393), or Test Selection (page 403) to obtain a Composite [GOF] test object. The latter may either be a single Testcase Object (page 382), a Test Suite Object (page 387), or a Composite test suite (a Suite of Suites; see Test Suite Object). Because all of these objects implement the same interface, the Test Runner need not care whether it is dealing with a single test or a multilevel suite. The Test Runner keeps track of, and reports on, how many tests it has run, how many tests had failed assertions, and how many tests raised errors or exceptions.\n\nwww.it-ebooks.info\n\n377\n\nTest Runner",
      "content_length": 1619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "378\n\nTest Runner\n\nChapter 19 xUnit Basics Patterns\n\nWhy We Do This\n\nWe wouldn’t want each test automater to have to provide a special means of running his or her own test suites. That requirement would just get in the way of our ability to simultaneously run all the tests automated by different people. By providing a standard Test Runner, we encourage developers to make it easy to run tests written by different people. We can also provide different ways of running the same tests.\n\nImplementation Notes\n\nSeveral styles of Test Runners are available. The most common variations are running tests from within an IDE and running tests from the command line. All of these schemes depend on the fact that all Testcase Objects implement a standard interface.\n\nStandard Test Interface\n\nStatically typed languages (such as Java and C#) typically include an interface type (fully abstract class) that deﬁ nes the interface that all Testcase Objects and Test Suite Objects must implement. Some languages (such as C# and Java 5.0) “mix” in the implementation by using class attributes or annotations on the Testcase Class. In dynamically typed languages, this interface may not exist explicitly. Instead, each implementation class simply implements the standard interface methods. Typically, the standard test interface includes methods on it to count the available tests and to run the tests. Where the framework supports Test Enumeration, each Testcase Class and test suite class must also implement the Test Suite Factory method (see Test Enumeration on page 399), which is typically called suite.\n\nVariation: Graphical Test Runner\n\nA Graphical Test Runner is typically a desktop application or part of an IDE (either built-in or a plug-in) for running tests. At least one, IeUnit, runs inside a Web browser rather than an IDE. The most common feature of the Graphical Test Runner is some sort of real-time progress indicator. This monitor typically includes a running count of test failures and errors and often includes a colored progress bar that starts off green and turns red as soon as an error or failure is encountered. Some members of the xUnit family include a graphical Test Tree Explorer as a means to drill down and run a single test from within a Suite of Suites.\n\nwww.it-ebooks.info",
      "content_length": 2293,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "Test Runner\n\nHere is the Graphical Test Runner from the JUnit plug-in for Eclipse:\n\nThe red bar near the top indicates that at least one test has failed. The upper text pane shows a list of test failures and test errors. The lower pane shows the traceback from the failed test selected in the upper pane.\n\nVariation: Command-Line Test Runner\n\nCommand-Line Test Runners are designed to be used from an operating system command line or from batch ﬁ les or shell scripts. They are very handy when working remotely via remote shells or when running the tests from a build script such as “make,” Ant, or a continuous integration tool such as “Cruise Control.” The following example shows how to run an runit (one of the xUnit imple- mentations for the Ruby programming language) test from the command line:\n\nwww.it-ebooks.info\n\n379\n\nTest Runner",
      "content_length": 839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "380\n\nTest Runner\n\nChapter 19 xUnit Basics Patterns\n\n>ruby testrunner.rb c:/examples/tests/SmellHandlerTest.rb Loaded suite SmellHandlerTest Started ..... Finished in 0.016 seconds. 5 tests, 6 assertions, 0 failures, 0 errors >Exit code: 0\n\nThe ﬁ rst line is the invocation at the command prompt. In this example we are running the tests deﬁ ned in a single Testcase Class, SmellHandlerTest. The next two lines are the initial feedback as the tests begin. The series of dots indicates the tests’ progress, one per test completed. This particular Command-Line Test Runner replaces the dot with an “E” or an “F” if the test produces an error or fails. The last three lines are summary statistics that provide an overview of what happened. Typically, the exit code is set to the total number of failed/error tests so that a non-zero exit code can be interpreted easily as a build failure when run from an automated build tool.\n\nVariation: File System Test Runner\n\nSome Command-Line Test Runners provide the option of searching a speciﬁ ed directory for all ﬁ les that are tests and running them all at once. This automated Testcase Class Discovery (see Test Discovery) avoids the need to build the Suite of Suites in code (Test Enumeration) and helps avoid Lost Tests (see Production Bugs on page 268).\n\nIn addition, some external tools will search the ﬁ le system for ﬁ les matching speciﬁ c patterns and then invoke an arbitrary command against the matched ﬁ les. These ﬁ les can be passed to the Test Runner from a build tool.\n\nVariation: Test Tree Explorer\n\nMembers of the xUnit family that turn each Test Method into a Testcase Object can manipulate the tests easily. Many of them provide a graphical representation of the Suite of Suites and allow the user to select an entire Test Suite Object or a single Testcase Object to run. This eliminates the need to create a Single Test Suite (see Named Test Suite on page 592) class to run a single test.\n\nHere is the Test Tree Explorer of JUnit plug-in for Eclipse shown “popped\n\nout” over other Eclipse views:\n\nwww.it-ebooks.info",
      "content_length": 2077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "Test Runner\n\nThe left pane of the IDE is the JUnit view within Eclipse. The progress bar ap- pears at the top of the view, the upper pane is the Test Tree Explorer, and the lower pane is the traceback for the currently selected test failure. Note that some Test Suite Objects in the Test Tree Explorer are “open,” revealing their contents; others are closed down. The colored annotation next to each Testcase Object shows its status; the annotations for each Test Suite Object indicate whether any contained Testcase Objects failed or produced an error. The Test Suite Object called “Test for com.clrstream.ex8.test” is a Suite of Suites for the package “com. clrstream.ex8.test”; “Test for allJUnitTests” is the topmost Suite of Suites for run- ning all the tests.\n\nwww.it-ebooks.info\n\n381\n\nTest Runner",
      "content_length": 803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "382\n\nTestcase Object\n\nChapter 19 xUnit Basics Patterns\n\nTestcase Object\n\nHow do we run the tests?\n\nWe create a Command object for each test and call the run method when we wish to execute it.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nThe Test Runner (page 377) needs a way to ﬁ nd and invoke the appropriate Test Methods (page 348) and to present the results to the user. Many Graphical Test Runners (see Test Runner) let the user drill down into the tree of tests and pick individual tests to run. This capability requires that the Test Runner be able to inspect and manipulate the tests at runtime.\n\nHow It Works\n\nWe instantiate a Command [GOF] object to represent each Test Method that should execute. We use the Testcase Class (page 373) as a Test Suite Factory to create a Test Suite Object (page 387) to hold all the Testcase Objects for a particular Testcase Class. We can use either Test Discovery (page 393) or Test Enumeration to create the Testcase Objects.\n\nwww.it-ebooks.info",
      "content_length": 1345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "Testcase Object\n\nWhy We Do This\n\nTreating tests as ﬁ rst-class objects opens up many new possibilities that are not available to us if we treat the tests as simple procedures. It is a lot easier for the Test Runner of the Test Automation Framework (page 298) to manipulate tests when they are objects. We can hold them in collections (Test Suite Objects), iterate over them, invoke them, and so on.\n\nMost members of the xUnit family create a separate Testcase Object for each test to isolate the tests from one another as prescribed by Independent Test (see page 42). Unfortunately, there is always an exception (see the sidebar “There’s Always an Exception” on page 384), and users of the affected Test Automation Frameworks need to be a bit more cautious.\n\nImplementation Notes\n\nEach Testcase Object implements a standard test interface so that the Test Runner does not need to know the speciﬁ c interface for each test. This scheme allows each Testcase Object to act as a Command object [GOF]. This allows us to build collections of these Testcase Objects, which we can then iterate across to do counting, running, displaying, and other operations.\n\nIn most programming languages, we need to create a class to deﬁ ne the behavior of the Testcase Objects. We could create a separate Testcase Class for each Testcase Object. It is more convenient to host many Test Methods on a single Testcase Class, however, as this strategy results in fewer classes to manage and facilitates reuse of Test Utility Methods (page 599). This approach requires that each Testcase Object of the Testcase Class have a way to deter- mine which Test Method it should invoke. Pluggable Behavior [SBPP] is the most common way to do this. The constructor of the Testcase Class takes the name of the method to be invoked as a parameter and stores this name in an instance variable. When the Test Runner invokes the run method on the Test- case Object, it uses reﬂ ection to ﬁ nd and invoke the method whose name is in the instance variable.\n\nwww.it-ebooks.info\n\n383\n\nTestcase Object",
      "content_length": 2057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "384\n\nTestcase Object\n\nChapter 19 xUnit Basics Patterns\n\nThere’s Always an Exception\n\nWhether we are learning to conjugate verbs in a new language or looking for patterns in how software is built, there’s always an exception!\n\nOne of the most notable exceptions in the xUnit family relates to the use of a Testcase Object (page 382) to represent each Test Meth- od (page 348) at runtime. This key design feature of xUnit offers a way to achieve an Independent Test (see page 42). The only members of the xUnit family that don’t follow this scheme are TestNG and NUnit (version 2.x). For the reasons described below, the builders of NUnit 2.0 chose to stray from the well-worn path of one Testcase Object per Test Method and create only a single instance of the Test- case Class (page 373). This instance, which they call the test ﬁ xture, is then reused for each Test Method. One of the authors of NUnit 2.0, James Newkirk, writes:\n\nI think one of the biggest screw-ups that was made when we wrote NUnit V2.0 was to not create a new instance of the test ﬁ xture class for each contained test method. I say “we” but I think this one was my fault. I did not quite understand the reasoning in JUnit for cre- ating a new instance of the test ﬁ xture for each test method. I look back now and see that reusing the instance for each test method allows someone to store a member variable from one test and use it in another. This can introduce execution-order dependencies, which for this type of testing is an anti-pattern. It is much better to fully isolate each test method from the others. This requires that a new object be created for each test method.\n\nUnfortunately, this has some very interesting—and undesirable— consequences when one is familiar with the “JUnit New Instance Behav- ior” of a separate Testcase Object per method. Because the object is reused, any objects it refers to via an instance variable are available to all subse- quent tests. This results in an implicit Shared Fixture (page 317) along with all the forms of Erratic Tests (page 228) that go with it. James goes on to say:\n\nSince it would be difﬁ cult to change the way that NUnit works now, and too many people would complain, I now make all of the mem- ber variables in test ﬁ xture classes static. It’s almost like truth in advertising. The result is that there is only one instance of this variable, no matter how many test ﬁ xture objects are created. If the variable is static, then someone who may not be familiar with\n\nwww.it-ebooks.info",
      "content_length": 2521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "Testcase Object\n\nhow NUnit executes would not assume that a new one is created before each test is executed. This is the closest I can get to how JUnit works without changing the way that NUnit executes test methods.\n\nMartin Fowler felt this exception was important enough that he wrote an article about why JUnit’s approach is correct. See http://martinfowler. com/bliki/JunitNewInstance.html.\n\nExample: Testcase Object\n\nThe main evidence of the existence of Testcase Objects appears in the Test Tree Explorer (see Test Runner) when we “drill down” into the Test Suite Object to expose the Testcase Objects it contains. Let’s look at an example from the JUnit Graphical Test Runner that is built into Eclipse. Here’s the list of objects created from the sample code from the write-up of Testcase Class:\n\nTestSuite(\"...ﬂightstate.featuretests.AllTests\") TestSuite(\"...ﬂightstate.featuretests.TestApproveFlight\") TestApproveFlight(\"testScheduledState_shouldThrowIn..ReEx\") TestApproveFlight(\"testUnsheduled_shouldEndUpInAwai..oval\") TestApproveFlight(\"testAwaitingApproval_shouldThrow..stEx\") TestApproveFlight(\"testWithNullArgument_shouldThrow..ntEx\") TestApproveFlight(\"testWithInvalidApprover_shouldTh..ntEx\") TestSuite(\"...ﬂightstate.featuretests.TestDescheduleFlight\") TestDescheduleFlight(\"testScheduled_shouldEndUpInSc..tate\") TestDescheduleFlight(\"testUnscheduled_shouldThrowIn..stEx\") TestDescheduleFlight(\"testAwaitingApproval_shouldTh..stEx\") TestSuite(\"...ﬂightstate.featuretests.TestRequestApproval\") TestRequestApproval(\"testScheduledState_shouldThrow..stEx\") TestRequestApproval(\"testUnsheduledState_shouldEndU..oval\") TestRequestApproval(\"testAwaitingApprovalState_shou..stEx\") TestSuite(\"...ﬂightstate.featuretests.TestScheduleFlight\") TestScheduleFlight(\"testUnscheduled_shouldEndUpInSc..uled\") TestScheduleFlight(\"testScheduledState_shouldThrowI..stEx\") TestScheduleFlight(\"testAwaitingApproval_shouldThro..stEx\")\n\nThe name outside the parentheses is the name of the class; the string inside the parentheses is the name of the object created from that class. By convention, the name of the Test Method4 to be run is used as the name of the Testcase Object, and the name of a Test Suite Object is whatever string was passed to the Test Suite Object constructor. In this example we’ve used the full package and class- name of the Testcase Class.\n\n4 I replaced part of the name with “..” to keep each line within the page width limit.\n\nwww.it-ebooks.info\n\n385\n\nTestcase Object",
      "content_length": 2491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "386\n\nTestcase Object\n\nChapter 19 xUnit Basics Patterns\n\nThis is what this scheme might look like when viewed in a Test Tree Explorer:\n\nwww.it-ebooks.info",
      "content_length": 153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "Test Suite Object\n\nTest Suite Object\n\nHow do we run the tests when we have many tests to run?\n\nWe deﬁ ne a collection class that implements the standard test interface and use it to run a set of related Testcase Objects.\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nSuite Suite\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nRun Run\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nRun Run\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nTestcase Testcase Class Class\n\nCreate Create\n\nGiven that we have created Test Methods (page 348) containing our test logic and placed them on a Testcase Class (page 373) so we can construct a Testcase Object (page 382) for each test, it would be nice to be able to run these tests as a single user operation.\n\nHow It Works\n\nWe deﬁ ne a Composite [GOF] Testcase Object called a Test Suite Object to hold the collection of individual Testcase Objects to execute. When we want to run all tests in the test suite at once, the Test Runner (page 377) asks the Test Suite Object to run all its tests.\n\nWhy We Do This\n\nTreating test suites as ﬁ rst-class objects makes it easier for the Test Runner of the Test Automation Framework (page 298) to manipulate tests in the test suite. With or without a Test Suite Object, the Test Runner would have to hold some kind of collection of Testcase Objects (so that we could iterate over them, count them, and so on). When we make the collection “smart,” it becomes a simple matter to add other uses such as the Suite of Suites.\n\nwww.it-ebooks.info\n\n387\n\nTest Suite Object",
      "content_length": 1697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "388\n\nTest Suite Object\n\nChapter 19 xUnit Basics Patterns\n\nVariation: Testcase Class Suite\n\nTo run all the Test Methods in a single Testcase Class, we simply build a Test Suite Object for the Testcase Class and add one Testcase Object for each Test Method. This allows us to run all the Test Methods in the Testcase Class simply by passing the name of the Testcase Class to the Test Runner.\n\nVariation: Suite of Suites\n\nWe can build up larger Named Test Suites (page 592) by organizing smaller test suites into a tree structure. The Composite pattern makes this organization invisible to the Test Runner, allowing it to treat a Suite of Suites exactly the same way it treats a simple Testcase Class Suite or a single Testcase Object.\n\nImplementation Notes\n\nAs a Composite object, each Test Suite Object implements the same interface as a simple Testcase Object. Thus neither the Test Runner nor the Test Suite Object needs to be aware of whether it is holding a reference to a single test or an entire suite. This makes it easier to implement any operations that involve iterating across all the tests such as counting, running, and displaying.\n\nBefore we can do anything with our Test Suite Object, we must construct it.\n\nWe can choose from several options to do so:\n\nTest Discovery (page 393): We can let the Test Automation Framework\n\ndiscover our Testcase Classes and Test Methods for us.\n\nTest Enumeration (page 399): We can write code that enumerates which Test Methods we want to include in a Test Suite Object. This usually involves creating a Test Suite Factory (see Test Enumeration).\n\nTest Selection (page 403): We can specify which subset of the Testcase\n\nObjects we want to include from an existing Test Suite Object.\n\nVariation: Test Suite Procedure\n\nSometimes we have to write code in programming or scripting languages that do not support objects. Given that we have written a number of Test Methods, we need to give the Test Runner some way to ﬁ nd the tests. A Test Suite Procedure allows us to enumerate all the tests we want to run by invoking each test in turn. The calls to each test are hard-coded within the body of the Test Suite Object. Of course, a Test Suite Procedure may call several other Test Suite Procedures to realize a Suite of Suites.\n\nwww.it-ebooks.info",
      "content_length": 2290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "Test Suite Object\n\nThe major disadvantage of this approach is that it forces us into Test Enumeration, which increases both the effort required to write tests and the likelihood of Lost Tests (see Production Bugs on page 268). Because we do not treat our code as “data,” we lose the ability to manipulate the code at runtime. As a consequence, it is more difﬁ cult to build a Graphical Test Runner (see Test Runner) with a hierarchy (tree) view of our Suite of Suites.\n\nExample: Test Suite Object\n\nMost members of the xUnit family implement Test Discovery, so there isn’t much of an example of Test Suite Object to see. The main evidence of the existence of Test Suite Objects appears in the Test Tree Explorer (see Test Runner) when we “drill down” into the Test Suite Object to expose the Testcase Objects it contains. Here’s an example from the JUnit Graphical Test Runner built into Eclipse:\n\nExample: Suite of Suites Built Using Test Enumeration\n\nHere is an example of using Test Enumeration to construct a Suite of Suites:\n\npublic class AllTests {\n\npublic static Test suite() {\n\nwww.it-ebooks.info\n\n389\n\nTest Suite Object",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "390\n\nTest Suite Object\n\nChapter 19 xUnit Basics Patterns\n\nTestSuite suite = new TestSuite(\"Test for allJunitTests\"); suite.addTestSuite( com.clrstream.camug.example.test.InvoiceTest.class); suite.addTest(com.clrstream.ex7.test.AllTests.suite()); suite.addTest(com.clrstream.ex8.test.AllTests.suite()); suite.addTestSuite(com.xunitpatterns.guardassertion.Example.class); return suite; } }\n\nThe ﬁ rst and last lines add the Test Suite Objects created from a single Testcase Class. Each of the middle two lines calls the Test Suite Factory for another Suite of Suites. The Test Suite Object we return is likely at least three levels deep:\n\n1. The Test Suite Object we instantiated and populated before returning\n\n2. The AllTests Test Suite Objects returned by the two calls to factory methods\n\n3. The Test Suite Objects for each of the Testcase Classes aggregated into\n\nthose Test Suite Objects\n\nThis is illustrated in the following tree of objects:\n\nTestSuite(\"Test for allJunitTests\"); TestSuite(\"com.clrstream.camug.example.test.InvoiceTest\") TestCase(\"testInvoice_addLineItem\") ... TestCase(\"testRemoveLineItemsForProduct_oneOfTwo\") TestSuite(\"com.clrstream.ex7.test.AllTests\") TestSuite(\"com.clrstream.ex7.test.TimeDisplayTest\") TestCase(\"testDisplayCurrentTime_AtMidnight\") TestCase(\"testDisplayCurrentTime_AtOneMinAfterMidnight\") TestCase(\"testDisplayCurrentTime_AtOneMinuteBeforeNoon\") TestCase(\"testDisplayCurrentTime_AtNoon\") ... TestSuite(\"com.clrstream.ex7.test.TimeDisplaySolutionTest\") TestCase(\"testDisplayCurrentTime_AtMidnight\") TestCase(\"testDisplayCurrentTime_AtOneMinAfterMidnight\") TestCase(\"testDisplayCurrentTime_AtOneMinuteBeforeNoon\") TestCase(\"testDisplayCurrentTime_AtNoon\") ... TestSuite(\"com.clrstream.ex8.test.AllTests\") TestSuite(\"com.clrstream.ex8.FlightMgntFacadeTest\") TestCase(\"testAddFlight\") TestCase(\"testAddFlightLogging\") TestCase(\"testRemoveFlight\") TestCase(\"testRemoveFlightLogging\") ... TestSuite(\"com.xunitpatterns.guardassertion.Example\") TestCase(\"testWithConditionals\") TestCase(\"testWithoutConditionals\") ...\n\nwww.it-ebooks.info",
      "content_length": 2074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "Test Suite Object\n\nNote that this class doesn’t subclass any other class. It does need to import TestSuite and the classes it is using as Test Suite Factories.\n\nExample: Test Suite Procedure\n\nIn the early days of agile software development, before any agile project manage- ment tools were available, I built a set of Excel spreadsheets for managing tasks and user stories. To make life simpler, I automated frequently performed tasks such as sorting all stories by release and iteration, sorting tasks by iteration and status, and so on. Eventually, I got bold enough to write a macro (a program, really) that would sum up the estimated and actual effort of all tasks for each story. At this point, the code was becoming somewhat complex and was more challenging to maintain. In particular, if one of the named ranges used by the sorting macros was accidentally deleted, the macro would produce an error.\n\nUnfortunately, there was no xUnit framework for VBA at the time, so all of this work was done without Tests as Safety Net (see page 24). Here is the main program of the reporting macro. All output was written to a new sheet in the workbook.\n\n'Main Macro\n\nSub summarizeActivities() Call VerifyVersionCompatability Call initialize Call SortByActivity\n\nFor row = ﬁrstTaskDataRow To lastTaskDataRow If numberOfNumberlessTasks < MaxNumberlessTasks Then thisActivity = ActiveSheet.Cells(row, TaskActivityColumn).Value\n\nIf thisActivity <> currentActivity Then Call ﬁnalizeCurrentActivityTotals currentActivity = thisActivity Call initializeCurrentActivityTotals End If\n\nCall accumulateActivityTotals(row) Else lastTaskDataRow = row ' end the For loop right away End If Next row Call cleanUp End Sub\n\nwww.it-ebooks.info\n\n391\n\nTest Suite Object",
      "content_length": 1742,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "392\n\nTest Suite Object\n\nChapter 19 xUnit Basics Patterns\n\nWithout any tests or Test Automation Framework, I had to do what I could to introduce some kind of regression testing. In this case, it was enough of a challenge (and a win) just to be able to exercise all the macros. If they ran to completion, it was a much better indication that I hadn’t broken anything major than not running the macros at all. Because VBA is based on Visual Basic 5, it has no classes. Thus we have no Testcase Class and no runtime Testcase Objects. The following is an example of the various Test Suite Procedures and the Test Methods my tests called:\n\nSub TestAll() Call TestAllStoryMacros Call TestAllTaskMacros Call TestReportingMacros Call TestToolbarMenus 'All The Same End Sub\n\nSub TestAllStoryMacros() Call TestActivitySorting Call TestStoryHiding Call ReportSuccess(\"All Story Macros\") End Sub\n\nSub TestActivitySorting() Call SortStoriesbyAreaAndNumber Call SortActivitiesByIteration Call SortActivitiesByIterationAndOrder Call SortActivitiesByNumber Call SortActivitiesByPercentDone End Sub\n\nSub TestReportingMacros() Call summarizeActivities End Sub\n\nThe ﬁ rst Test Suite Procedure is a Suite of Suites; the second Test Suite Procedure is the equivalent of a single Test Suite Object. The third Sub is the Test Method for exercising all of the sorting macros. The last Sub exercises the summarizeActivities macro using a Prebuilt Fixture (page 429). 5\n\n5 For those who might be wondering what happened to the verify outcome phase of the test, there isn’t one in this test. It is neither a Self-Checking Test nor a Single-Condition Test. Shame on me!\n\nwww.it-ebooks.info",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "Test Discovery\n\nTest Discovery\n\nHow does the Test Runner know which tests to run?\n\nThe Test Automation Framework discovers all tests that belong to the test suite automatically.\n\nTest Test Discovery Discovery Mechanism Mechanism\n\nCreate Create\n\ntestMethod_1 testMethod_1\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\nTest Runner Test Runner\n\nCreate Create\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nGiven that we have written a number of Test Methods (page 348) on one or more Testcase Classes (page 373), we need to give the Test Runner (page 377) some way to ﬁ nd the tests. Test Discovery eliminates most of the hassles associated with Test Enumeration (page 399).\n\nHow It Works\n\nThe Test Automation Framework (page 298) uses runtime reﬂ ection (or com- pile-time knowledge) to discover all Test Methods that belong to the test suite and/or all Test Suite Objects (page 387) that belong to a Suite of Suites (see Test Suite Object). It then builds up the Test Suite Objects containing the corresponding Testcase Objects (page 382) and other Test Suite Objects in preparation for running all the tests.\n\nWhen to Use It\n\nWe should use Test Discovery whenever our Test Automation Framework supports it. This pattern reduces the effort required to automate tests and greatly reduces the possibility of Lost Tests (see Production Bugs on page 268). The\n\nwww.it-ebooks.info\n\n393\n\nTest Discovery",
      "content_length": 1637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "394\n\nTest Discovery\n\nChapter 19 xUnit Basics Patterns\n\nonly times to consider using Test Enumeration are (1) when our framework does not support Test Discovery and (2) when we wish to deﬁ ne a Named Test Suite (page 592) that consists of a subset of tests6 chosen from other test suites and the Test Automation Framework does not support Test Selection (page 403). It is not uncommon to combine Test Suite Enumeration (see Test Enumeration) with Test Method Discovery; the reverse is less common.\n\nImplementation Notes\n\nBuilding the Suite of Suites to be executed by the Test Runner involves two steps. First, we must ﬁ nd all Test Methods to be included in each Test Suite Object. Second, we must ﬁ nd all Test Suite Objects to be included in the test run, albeit not necessarily in this order. Each of these steps may be done manually via Test Method Enumeration (see Test Enumeration) and Test Suite Enumeration or automatically via Test Method Discovery and Testcase Class Discovery.\n\nVariation: Testcase Class Discovery\n\nTestcase Class Discovery is the process by which the Test Automation Frame- work discovers the Testcase Classes on which it should do Test Method Dis- covery. One solution involves tagging each Testcase Class by subclassing a Testcase Superclass (page 638) or implementing a Marker Interface [PJV1]. Another alternative, used in the .NET languages and newer versions of JUnit, is to use a class attribute (e.g., \"[Test Fixture]\") or annotation (e.g., \"@Testcase\") to identify each Testcase Class. Yet another solution is to put all Testcase Classes into a common directory and point the Test Runner or some other program at this directory. A fourth solution is to follow a Testcase Class naming convention and use an external program to ﬁ nd all ﬁ les matching this naming pattern. Whichever way we choose to perform this task, once a Testcase Class has been discovered we can proceed to either Test Method Discovery or Test Method Enumeration.\n\nVariation: Test Method Discovery\n\nTest Method Discovery involves providing a way for the Test Automation Frame- work to discover the Test Methods in our Testcase Classes. There are two basic ways to indicate that a method of a Testcase Class is a Test Method. The more traditional approach is to use a Test Method naming convention such as “starts with ‘test’.” The Test Automation Framework then iterates over all methods of the Testcase Class, selects those that start with the string “test” (e.g., testCounters),\n\n6 A Smoke Test [SCM] suite is a good example.\n\nwww.it-ebooks.info",
      "content_length": 2554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "Test Discovery\n\nand calls the one-argument constructor to create the Testcase Object for that Test Method. The other alternative, which is used in the .NET languages and newer versions of JUnit, is to use a method attribute (e.g., “[Test]”) or annotation (e.g., “@Test”) to identify each Test Method.\n\nMotivating Example\n\nThe following example illustrates the kind of code that would be required for each Test Method to do Test Method Enumeration if we did not have Test Discovery available:\n\npublic: static CppUnit::Test *suite() { CppUnit::TestSuite *suite = new CppUnit::TestSuite( \"ComplexNumberTest\" ); suite>addTest( new CppUnit::TestCaller<ComplexNumberTest>( \"testEquality\", &ComplexNumberTest::testEquality ) ); suite>addTest( new CppUnit::TestCaller<ComplexNumberTest>( \"testAddition\", &ComplexNumberTest::testAddition ) ); return suite; }\n\nThis example is from the tutorial for an earlier version of CppUnit. Newer versions no longer require this approach.\n\nRefactoring Notes\n\nLuckily for the users of existing xUnit family members, the inventors of xUnit realized the importance of Test Discovery. Therefore all we have to do is follow their advice on how to identify our test methods. If the developers of our xUnit version used a naming convention, we may have to do a Rename Method [Fowler] refactoring to get xUnit to discover our Test Method. If they implemented method attributes, we just add the appropriate attribute to our Test Methods.\n\nExample: Test Method Discovery (Using Method Naming and Compiler Macro)\n\nWhen the programming language is capable of managing the tests as objects and invoking the methods but cannot easily ﬁ nd all methods to use as tests, we\n\nwww.it-ebooks.info\n\n395\n\nTest Discovery",
      "content_length": 1726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "396\n\nTest Discovery\n\nChapter 19 xUnit Basics Patterns\n\nmay need to give it a small push as encouragement to do so. Newer versions of CppUnit provide a macro that ﬁ nds all Test Methods at compile time and generates the code to build the test suite as illustrated in the previous example. The following code snippet triggers the Test Method Discovery:\n\nCPPUNIT_TEST_SUITE_REGISTRATION( FlightManagementFacadeTest );\n\nThis macro uses a method naming convention to determine which methods (“member functions”) it should turn into Testcase Objects by wrapping each with a TestCaller, much like in the manual example we saw earlier.\n\nExample: Test Method Discovery (Using Method Naming)\n\nThe following examples are more notable for the code that is missing than for the code that is present. Note that there is no code to add the Test Methods to the Test Suite Object.\n\nIn this Java example, the framework automatically runs all test methods that\n\nstart with “test” and have no arguments (a total of two):\n\npublic class TimeDisplayTest extends TestCase { public void testDisplayCurrentTime_AtMidnight() throws Exception { // Set up SUT TimeDisplay theTimeDisplay = new TimeDisplay(); // Exercise SUT String actualTimeString = theTimeDisplay.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( \"Midnight\", expectedTimeString, actualTimeString); }\n\npublic void testDisplayCurrentTime_AtOneMinuteAfterMidnight() throws Exception { // Set up SUT TimeDisplay actualTimeDisplay = new TimeDisplay(); // Exercise SUT String actualTimeString = actualTimeDisplay.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">12:01 AM</span>\"; assertEquals( \"12:01 AM\", expectedTimeString, actualTimeString); } }\n\nwww.it-ebooks.info",
      "content_length": 1850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "Test Discovery\n\nExample: Test Method Discovery (Using Method Attributes)\n\nIn this C# example, the tests are labeled with the method attribute [Test]. Both CsUnit and NUnit use this way of identifying Test Methods.\n\n[Test] public void testFlightMileage_asKm() { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); int expectedKilometres = 1810; // verify results Assert.AreEqual( expectedKilometres, actualKilometres); }\n\n[Test] [ExpectedException(typeof(InvalidArgumentException))] public void testSetMileage_invalidInput_attribute() { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); // exercise SUT newFlight.setMileage(-1122); }\n\nExample: Testcase Class Discovery (Using Class Attributes)\n\nHere is an example of using a class attribute to identify a Testcase Class (called a “Test Fixture” in NUnit) to the Test Runner:\n\n[TestFixture] public class SampleTestcase {\n\n}\n\nExample: Testcase Class Discovery (Using Common Location and Testcase Superclass)\n\nThe following Ruby example ﬁ nds all ﬁ les with the .rb extension in the “tests” directory and requires them from this ﬁ le. This causes Test::Unit to look for all tests in each ﬁ le because the Testcase Class in each ﬁ le extends Test::Unit::TestCase.\n\nDir['tests/*.rb'].each do |each| require each end\n\nwww.it-ebooks.info\n\n397\n\nTest Discovery",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "398\n\nTest Discovery\n\nChapter 19 xUnit Basics Patterns\n\nThe Dir['tests/*.rb'] returns a collection of ﬁ les over which the each method iter- ates with the block containing “require each” to implement Testcase Class Dis- covery. The Ruby interpreter and Test::Unit ﬁ nish the job by doing Test Method Discovery on each required class.\n\nwww.it-ebooks.info",
      "content_length": 352,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "Test Enumeration\n\nTest Enumeration\n\nHow does the Test Runner know which tests to run?\n\nThe test automater manually writes the code that enumerates all tests that belong to the test suite.\n\nTest Test Suite Suite Factory Factory\n\nCreate Create\n\ntestMethod_1 testMethod_1\n\nCreate Create\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\nSuite Suite\n\nTest Runner Test Runner\n\nCreate Create\n\nRun Run\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nGiven that we have written a number of Test Methods (page 348) on one or more Testcase Classes (page 373), we need to give the Test Runner (page 377) some way to ﬁ nd the tests. Test Enumeration is the way we do so when we lack support for Test Discovery (page 393).\n\nHow It Works\n\nThe test automater manually writes the code that enumerates all Test Methods that belong to the test suite and/or all Test Suite Objects (page 387) that belong to a Suite of Suites (see Test Suite Object). This is typically done by implement- ing the method suite either on a Testcase Class for Test Method Enumeration or on a Test Suite Factory for Test Suite Enumeration.\n\nWhen to Use It\n\nWe need to use Test Enumeration if our Test Automation Framework (page 298) does not support Test Discovery. We can also choose to use Test Enumeration\n\nwww.it-ebooks.info\n\n399\n\nTest Enumeration\n\nAlso known as: Test Suite Factory",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "400\n\nTest Enumeration\n\nChapter 19 xUnit Basics Patterns\n\nwhen we wish to deﬁ ne a Named Test Suite (page 592) that consists of a subset of tests7 chosen from other test suites and the framework does not support Test Selection (page 403).\n\nMany members of the xUnit family support Test Discovery at the Test Method\n\nlevel but force us to use Test Enumeration at the Testcase Class level.\n\nImplementation Notes\n\nBuilding the Suite of Suites to be executed by the Test Runner involves two steps. First, we must ﬁ nd all Test Methods to be included in each Test Suite Object. Second, we must ﬁ nd all Test Suite Objects to be included in the test run, albeit not necessarily in this order. Each of these steps may be done manually via Test Method Enumeration and Test Suite Enumeration or automatically via Test Method Discovery (see Test Discovery) and Testcase Class Discovery (see Test Discovery). When done manually, we typically use a “Test Suite Factory” that returns the Test Suite Object.\n\nVariation: Test Suite Enumeration\n\nMany members of the xUnit family require that we provide a Test Suite Factory that builds the top-level Suite of Suites (often called “AllTests”) as means to specify which Test Suite Objects we would like to include in a test run. We do so by providing a class method on a factory class; this Factory Method [GOF] is called suite in most members of the xUnit family. Inside the suite method we use calls to methods such as addTest to add each nested Test Suite Object to the suite we are building.\n\nAlthough this approach is fairly ﬂ exible, it can result in Lost Tests (see Production Bugs on page 268). The alternative is to let the development tools build the AllTests Suite (see Named Test Suite) automatically or to use a Test Runner that ﬁ nds all test suites in a ﬁ le system directory automatically. For example, NUnit provides a built-in mechanism that implements Testcase Class Discovery at the assembly level. We can also use third-party tools such as Ant to ﬁ nd all Testcase Class ﬁ les in a directory structure.\n\nEven in statically typed languages such as Java, the Test Suite Factory (see Test Enumeration on page 399) does not need to subclass a speciﬁ c class or implement a speciﬁ c interface. Instead, the only dependencies are on the generic Test Suite Object class it returns and the Testcase Classes or Test Suite Factories it asks for the nested suites.\n\n7 A Smoke Test [SCM] suite is a good example.\n\nwww.it-ebooks.info",
      "content_length": 2472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "Test Enumeration\n\nVariation: Test Method Enumeration\n\nMany members of the xUnit family now support Test Method Discovery. If we happen to be using a version that does not, we need to ﬁ nd all Test Methods in a Testcase Class, turn them into Testcase Objects (page 382), and put them into a Test Suite Object. We implement Test Method Enumeration by providing a class method, typically called suite, on the Testcase Class itself.\n\nThe capability to construct an object that calls an arbitrary method is often in- herited from the Test Automation Framework via a Testcase Superclass (page 638) or mixed in via a class attribute or Include directive. In some members of the xUnit family, this Pluggable Behavior [SBPP] capability is provided by a separate class (see the CppUnit example below).\n\nVariation: Direct Test Method Invocation\n\nIn the pure procedural world where we cannot treat a Test Method as an object or data item, we have no choice but to hand-code a Test Suite Procedure (see Test Suite Object) for each test suite. This procedure then calls each Test Method (or other Test Suite Procedures) one by one.\n\nExample: Test Method Enumeration in CppUnit\n\nEarly versions of most xUnit family members required that the test automater add each Test Method manually. Those versions that cannot use reﬂ ection still have this requirement. Here is an example from an older version of CppUnit that uses this approach:\n\npublic: static CppUnit::Test *suite() { CppUnit::TestSuite *suite = new CppUnit::TestSuite( \"ComplexNumberTest\" ); suite>addTest( new CppUnit::TestCaller<ComplexNumberTest>( \"testEquality\", &ComplexNumberTest::testEquality ) ); suite>addTest( new CppUnit::TestCaller<ComplexNumberTest>( \"testAddition\", &ComplexNumberTest::testAddition ) ); return suite; }\n\nThis example also illustrates how CppUnit wraps each Test Method with an instance of a class (TestCaller) to turn it into a Testcase Object.\n\nwww.it-ebooks.info\n\n401\n\nTest Enumeration",
      "content_length": 1962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "402\n\nTest Enumeration\n\nChapter 19 xUnit Basics Patterns\n\nExample: Test Method Invocation (Hard-Coded)\n\nThe following example is from a test suite for a program written in VBA (Visual Basic for Applications, the macro language used in Microsoft Ofﬁ ce products), which lacks support for objects:\n\nSub TestAllStoryMacros() Call TestActivitySorting Call TestStoryHiding Call ReportSuccess(\"All Story Macros\") End Sub\n\nExample: Test Suite Enumeration\n\nWe can use Test Suite Enumeration when the Test Automation Framework does not support Test Discovery or when we want to deﬁ ne a Named Test Suite that includes only a subset of the tests.\n\nThe main drawback of using Test Suite Enumeration for running all tests is the potential for Lost Tests if we forget to include a new test suite in the AllTests Suite. This risk can be reduced by paying attention to the number of tests that were run when we ﬁ rst checked out the code and ensuring that the number run just before check-in goes up by the number of new tests we added.\n\npublic class AllTests {\n\npublic static Test suite() { TestSuite suite = new TestSuite(\"Test for allJunitTests\"); //$JUnit-BEGIN$ suite.addTestSuite( com.clrstream.camug.example.test.InvoiceTest.class); suite.addTest(com.clrstream.ex7.test.AllTests.suite()); suite.addTest(com.clrstream.ex8.test.AllTests.suite()); suite.addTestSuite( com.xunitpatterns.guardassertion.Example.class); //$JUnit-END$ return suite; } }\n\nIn this example, we take advantage of the IDE’s ability to (re)generate the AllTests suite for us. (Eclipse will regenerate the code between the two marker comments whenever we request it to do so.) We still need to remember to regenerate the suite occasionally, but this approach goes a long way toward avoiding Lost Tests in the absence of Test Discovery.\n\nwww.it-ebooks.info",
      "content_length": 1815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "Test Selection\n\nTest Selection\n\nHow does the Test Runner know which tests to run?\n\nThe Test Automation Framework selects the Test Methods to be run at runtime based on attributes of the tests.\n\nTest Test Selection Selection Mechanism Mechanism\n\nTest Runner Test Runner\n\nGet Get\n\nCreate Create\n\nAdd Add\n\nCreate Create\n\nTest Test Suite Suite Object Object\n\nTestcase Testcase Object Object\n\ntestMethod_1 testMethod_1\n\nExercise Exercise\n\nFixture Fixture\n\nSUT SUT\n\nRun Run\n\nSubset Subset Suite Suite Object Object\n\nTestcase Testcase Object Object\n\nExercise Exercise\n\ntestMethod_n testMethod_n\n\nCreate Create\n\nTestcase Testcase Class Class\n\nGiven that we have written a number of Test Methods (page 348) on one or more Testcase Classes (page 373), we need to give the Test Runner (page 377) some way to ﬁ nd those tests. Test Selection is a way to pick subsets of tests dynamically.\n\nHow It Works\n\nThe test automater speciﬁ es the subset of tests to be run when invoking the Test Runner by providing test selection criteria. These selection criteria may be based on implicit or explicit attributes of the Testcase Classes or Test Methods.\n\nWhen to Use It\n\nWe should use Test Selection when we wish to run a subset of tests chosen from other test suites and we do not want to maintain a separate structure built using Test Enumeration (page 399). A Smoke Test [SCM] suite is a common usage; see Named Test Suite (page 592) for other uses.\n\nwww.it-ebooks.info\n\n403\n\nTest Selection",
      "content_length": 1472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "404\n\nTest Selection\n\nChapter 19 xUnit Basics Patterns\n\nImplementation Notes\n\nTest Selection can be implemented either by creating a Subset Suite (see Named Test Suite) from an existing Test Suite Object (page 387) or by skip- ping some of the tests within the Test Suite Object as we execute the Testcase Objects (page 382) it contains.\n\nAs with Test Discovery (page 393) and Test Enumeration, Test Selection can be applied at two different levels: selecting Testcase Classes or selecting Test Methods. Test Selection can be built into the Test Automation Frame- work (page 298) or it can be implemented more crudely as part of the build task.\n\nVariation: Testcase Class Selection\n\nWe can select the Testcase Classes to be examined for Test Methods in several ways. The crudest way to do Testcase Class Selection is simply to place the Test- case Classes into test packages based on some criteria. Unfortunately, this strategy works only for a single test classiﬁ cation scheme and is likely to reduce the value of Tests as Documentation (see page 23). A somewhat more ﬂ exible approach is to use a naming convention such as “contains ‘WebServer’” to select only those classes that verify the behavior of certain parts of the system. This, too, is some- what constrained in its utility.\n\nThe most ﬂ exible way to implement Test Selection is within the Test Auto- mation Framework. We can use class attributes (.NET) or annotations (Java) to indicate characteristics of the Testcase Class. The same technique can also be applied at the Test Method level.\n\nVariation: Test Method Selection\n\nWhen implemented as part of the Test Automation Framework, Test Method Selection can be done by specifying the “category” (or categories) to which a Test Method belongs. This usually requires language support for method attributes (.NET) or annotations (Java). It could also be based on a method name scheme, although this approach is not as ﬂ exible and would require tighter coupling to the Test Runner.\n\nExample: Testcase Class Selection Using Class Attributes\n\nThe following example of Testcase Class Selection is from NUnit. The class attribute Category(“FastSuite”) indicates that all tests in this Testcase Class should be included (or excluded) when the category “FastSuite” is speciﬁ ed in the Test Runner.\n\nwww.it-ebooks.info",
      "content_length": 2324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "Test Selection\n\n[TestFixture] [Category(\"FastSuite\")] public class CategorizedTests { [Test] public void testFlightConstructor_OK() // Methods omitted }\n\nExample: Test Method Selection Using Method Attributes\n\nThis example of Test Method Selection is from NUnit. The method attribute Category(“SmokeTest”) indicates that this Test Method should be included (or excluded) when the category “SmokeTest” is speciﬁ ed in the Test Runner.\n\n[Test] [Category(\"SmokeTests\")] public void testFlightMileage_asKm() { // set up ﬁxture Flight newFlight = new Flight(validFlightNumber); newFlight.setMileage(1122); // exercise mileage translator int actualKilometres = newFlight.getMileageAsKm(); int expectedKilometres = 1810; // verify results Assert.AreEqual( expectedKilometres, actualKilometres); }\n\nwww.it-ebooks.info\n\n405\n\nTest Selection",
      "content_length": 830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 469,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "Chapter 20\n\nFixture Setup Patterns\n\nPatterns in This Chapter\n\nFresh Fixture Setup\n\nIn-line Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n\nDelegated Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\n\nCreation Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n\nImplicit Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424\n\nShared Fixture Construction\n\nPrebuilt Fixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\n\nLazy Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435\n\nSuite Fixture Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\n\nSetup Decorator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447\n\nChained Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454\n\n407\n\nwww.it-ebooks.info\n\nFixture Setup Patterns",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "408\n\nIn-line Setup\n\nChapter 20 Fixture Setup Patterns\n\nIn-line Setup\n\nHow do we construct the Fresh Fixture?\n\nEach Test Method creates its own Fresh Fixture by calling the appropriate constructor methods to build exactly the test ﬁ xture it requires.\n\nsetUp setUp\n\nSetup Setup\n\nFixture Fixture\n\ntest_1 test_1\n\ntest_2 test_2\n\nSUT SUT\n\ntest_n test_n\n\nTestcase Class Testcase Class\n\nTo execute an automated test, we require a text ﬁ xture that is well understood and completely deterministic. We can use the Fresh Fixture (page 311) approach to build a Minimal Fixture (page 302) for the use of this one test. Setting up the test ﬁ xture on an in-line basis in each test is the most obvious way to build it.\n\nHow It Works\n\nEach Test Method (page 348) sets up its own test ﬁ xture by directly calling what- ever SUT code is required to construct exactly the test ﬁ xture it requires. We put the code that creates the ﬁ xture, the ﬁ rst phase of the Four-Phase Test (page 358), at the top of each Test Method.\n\nwww.it-ebooks.info",
      "content_length": 1024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "In-line Setup\n\nWhen to Use It\n\nWe can use In-line Setup when the ﬁ xture setup logic is very simple and straightforward. As soon as the ﬁ xture setup gets at all complex, we should consider using Delegated Setup (page 411) or Implicit Setup (page 424) for part or all of the ﬁ xture setup.\n\nWe can also use In-line Setup when we are writing a ﬁ rst draft of tests and haven’t yet ﬁ gured out which part of the ﬁ xture setup will be repeated from test to test. This is an example of applying the “Red–Green–Refactor” process pattern to the tests themselves. Nevertheless, we need to be careful when we refactor the tests to ensure that we don’t break the tests in ways that are undetectable.\n\nA third occasion to use In-line Setup is when refactoring obtuse ﬁ xture setup code. A ﬁ rst step may be to use In-line Method [Fowler] refactorings on all Creation Methods (page 415) and the setUp method. Then we can try using a series of Extract Method [Fowler] refactorings to deﬁ ne a new set of Creation Methods that are more intent-revealing and reusable.\n\nImplementation Notes\n\nIn practice, most ﬁ xture setup logic will include a mix of styles, such as In-line Setup building on top of Implicit Setup or Delegated Setup interspersed with In-line Setup.\n\nExample: In-line Setup\n\nHere’s an example of simple in-line setup. Everything each Test Method needs for exercising the SUT is included in-line.\n\npublic void testStatus_initial() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // tearDown: // garbage-collected }\n\npublic void testStatus_cancelled() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\");\n\nwww.it-ebooks.info\n\n409\n\nIn-line Setup",
      "content_length": 1987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "410\n\nIn-line Setup\n\nChapter 20 Fixture Setup Patterns\n\nFlight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); ﬂight.cancel(); // still part of setup // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // tearDown: // garbage-collected }\n\nRefactoring Notes\n\nIn-line Setup is normally the starting point for refactoring, not the end goal. Sometimes, however, we ﬁ nd ourselves with tests that are too hard to under- stand because of all the stuff happening behind the scenes, which is a form of Mystery Guest (see Obscure Test on page 186). At other times, we may ﬁ nd ourselves modifying the previously setup ﬁ xture in many of the tests.\n\nBoth of these situations are indications it may be time to refactor our test class into multiple classes based on the ﬁ xture they build. First, we use an In- line Method refactoring on the code to produce an In-line Setup. Next, we reorganize the tests using an Extract Class [Fowler] refactoring. Finally, we use a series of Extract Method refactorings to deﬁ ne a more understandable set of ﬁ xture setup methods.\n\nwww.it-ebooks.info",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "Delegated Setup\n\nDelegated Setup\n\nHow do we construct the Fresh Fixture?\n\nEach Test Method creates its own Fresh Fixture by calling Creation Methods from within the Test Methods.\n\nsetUp setUp\n\nUtility Utility Method Method\n\nSetup Setup\n\nFixture Fixture\n\ntest_1 test_1\n\nUtility Utility Method Method\n\ntest_2 test_2\n\nSUT SUT\n\ntest_n test_n\n\nTestcase Class Testcase Class\n\nTo execute an automated test, we require a text ﬁ xture that is well understood and completely deterministic. We are using a Fresh Fixture (page 311) approach to build a Minimal Fixture (page 302) for the use of this one test and we’d like to avoid Test Code Duplication (page 213).\n\nDelegated Setup lets us reuse the code to set up the ﬁ xture without compromis-\n\ning our goal of Tests as Documentation (see page 23).\n\nHow It Works\n\nEach Test Method (page 348) sets up its own test ﬁ xture by calling one or more Creation Methods (page 415) to construct exactly the test ﬁ xture it requires. To ensure Tests as Documentation, we build a Minimal Fixture using Creation Methods that build fully formed objects that are ready for use by the test. We strive to ensure that the method calls will convey the “big picture” to the test reader by passing in only those values that affect the behavior of the SUT.\n\nwww.it-ebooks.info\n\n411\n\nDelegated Setup",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "412\n\nDelegated Setup\n\nChapter 20 Fixture Setup Patterns\n\nWhen to Use It\n\nWe can use a Delegated Setup when we want to avoid the Test Code Duplication caused by having to set up similar ﬁ xtures for several tests and we want to keep the nature of the ﬁ xture visible within the Test Methods. A reasonable goal is to encapsulate the essential but irrelevant steps of setting up the ﬁ xture and leave only the steps and values essential to understanding the test within the Test Meth- od. This scheme helps us achieve Tests as Documentation by ensuring that excess In-line Setup (page 408) code does not obscure the intent of the test. It also avoids the Mystery Guest problem (see Obscure Test on page 186) by leaving the Intent- Revealing Name [SBPP] of the Creation Method call within the Test Method.\n\nFurthermore, Delegated Setup allows us to use whatever organization scheme we want for our Test Methods. In particular, we are not forced to put Test Methods that require the same test ﬁ xture into the same Testcase Class (page 373) just to reuse the setUp method as we would have to when using Implicit Setup (page 424). Furthermore, Delegated Setup helps prevent Fragile Tests (page 239) by moving much of the nonessential interaction with the SUT out of the very numerous Test Methods and into a much smaller number of Creation Method bodies, where it is easier to maintain.\n\nImplementation Notes\n\nWith modern refactoring tools, we can often create the ﬁ rst cut of a Creation Method by performing a simple Extract Method [Fowler] refactoring. As we are writing a set of tests using “clone and twiddle,” we must watch for any Test Code Duplication in the ﬁ xture setup logic within our tests. For each object that needs to be veriﬁ ed in the veriﬁ cation logic, we extract a Creation Method that takes only those attributes as parameters that affect the outcome of the test.\n\nInitially, we can leave the Creation Method on our Testcase Class. If we need to share them with another class, however, we can move the Creation Methods to an Abstract Testcase class (see Testcase Superclass on page 638) or a Test Helper (page 643) class.\n\nMotivating Example\n\nSuppose we are testing the state model of the Flight class. In each test, we need to have a ﬂ ight in the right state. Because a ﬂ ight needs to connect at least two airports, we need to create airports before we can create a ﬂ ight. Of course, air- ports are typically associated with cities or states/provinces. To keep the example manageable, let’s assume that our airports require only a city name and an air- port code.\n\nwww.it-ebooks.info",
      "content_length": 2604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": "Delegated Setup\n\npublic void testStatus_initial() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight(ﬂightNumber, departureAirport, destinationAirport); // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // teardown // garbage-collected }\n\npublic void testStatus_cancelled() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); ﬂight.cancel(); // still part of setup // Exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // teardown // garbage-collected }\n\nThese tests contain a fair amount of Test Code Duplication.\n\nRefactoring Notes\n\nWe can refactor the ﬁ xture setup logic by using an Extract Method refactoring to remove any frequently repeated code sequences into utility methods with Intent-Revealing Names. We leave the calls to the methods in the test, however, so that the reader can see what is being done. The method calls that remain within the test will convey the “big picture” to the test reader. The utility meth- od bodies contain the irrelevant mechanics of carrying out the intent. If we need to share the Delegated Setups with another Testcase Class, we can use either a Pull Up Method [Fowler] refactoring to move them to a Testcase Superclass or a Move Method [Fowler] refactoring to move them to a Test Helper class.\n\nExample: Delegated Setup\n\nIn this version of the test, we use a method that hides the fact that we need two airports instead of creating the two airports needed by the ﬂ ight within each Test Method. We could produce this version of the tests either through refactoring or by writing the test in this intent-revealing style right off the bat.\n\nwww.it-ebooks.info\n\n413\n\nDelegated Setup",
      "content_length": 1977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "414\n\nDelegated Setup\n\nChapter 20 Fixture Setup Patterns\n\npublic void testGetStatus_initial() { // setup Flight ﬂight = createAnonymousFlight(); // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // teardown // garbage-collected }\n\npublic void testGetStatus_cancelled2() { // setup Flight ﬂight = createAnonymousCancelledFlight(); // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // teardown // garbage-collected }\n\nThe simplicity of these tests was made possible by the following Creation Methods, which hide the “necessary but irrelevant” steps from the test reader:\n\nprivate int uniqueFlightNumber = 2000;\n\npublic Flight createAnonymousFlight(){ Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( new BigDecimal(uniqueFlightNumber++), departureAirport, destinationAirport); return ﬂight; } public Flight createAnonymousCancelledFlight(){ Flight ﬂight = createAnonymousFlight(); ﬂight.cancel(); return ﬂight; }\n\nwww.it-ebooks.info",
      "content_length": 1106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "Creation Method\n\nCreation Method\n\nHow do we construct the Fresh Fixture?\n\nWe set up the test ﬁ xture by calling methods that hide the mechanics of building ready-to-use objects behind Intent-Revealing Names.\n\nSetup Setup\n\nCreation Creation Method Method\n\nFixture Fixture\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nTeardown Teardown\n\nFixture setup usually involves the creation of a number of objects. In many cases, the details of those objects (i.e., the attribute values) are unimportant but must be speciﬁ ed to satisfy each object’s constructor method. Including all of this unnecessary complexity within the ﬁ xture setup part of the test can lead to Obscure Tests (page 186) and certainly doesn’t help us achieve Tests as Docu- mentation (see page 23)!\n\nHow can a properly initialized object be created without having to clutter the test with In-line Setup (page 408)? The answer, of course, is to encapsulate this complexity. Delegated Setup (page 411) moves the mechanics of the ﬁ xture setup into other methods but leaves overall control and coordination within the test itself. But what to delegate to? A Creation Method is one way we can encapsulate the mechanics of object creation so that irrelevant details do not distract the reader.\n\nwww.it-ebooks.info\n\n415\n\nCreation Method",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "416\n\nCreation Method\n\nChapter 20 Fixture Setup Patterns\n\nHow It Works\n\nAs we write tests, we don’t bother asking whether a desired utility function exists; we just use it! (It helps to pretend that we have a loyal helper sitting next to us who will quickly ﬁ ll in the bodies of any functions we call that do not exist as yet.) We write our tests in terms of these magic functions with Intent-Revealing Names [SBPP], passing as parameters only those things that will be veriﬁ ed in the assertions or that should affect the outcome of the test. Once we’ve written the test in this very intent-revealing style, we must implement all of the magic functions that we’ve been calling. The functions that create objects are our Creation Methods; they encapsulate the complexity of object creation. The simple ones call the appropriate constructor, passing it suitable default values for anything needed but not supplied as a parameter. If any of the constructor argu- ments are other objects, the Creation Method will ﬁ rst create those depended-on objects before calling the constructor.\n\nThe Creation Method may be placed in all the same places where we put Test Utility Methods (page 599). As usual, the decision is based on the expected scope of reuse and the Creation Method’s dependencies on the API of the SUT. A related pattern is Object Mother (see Test Helper on page 643), which is a combination of Creation Method, Test Helper, and optionally Automated Tear- down (page 503).\n\nWhen to Use It\n\nWe should use a Creation Method whenever constructing a Fresh Fixture (page 311) requires signiﬁ cant complexity and we value Tests as Documentation. Another key indicator for using Creation Method is that we are building the system in a highly incremental way and we expect the API of the system (and especially the object constructors) to change frequently. Encapsulating knowl- edge of how to create a ﬁ xture object is a special case of SUT API Encapsulation (see Test Utility Method), and it helps us avoid both Fragile Tests (page 239) and Obscure Tests.\n\nThe main drawback of a Creation Method is that it creates another API for test automaters to learn. This isn’t much of a problem for the initial test devel- opers because they are typically involved in building this API but it can create “one more thing” for new additions to the team to learn. Even so, this API should be pretty easy to understand because it is just a set of Factory Methods [GOF] organized in some way.\n\nIf we are using a Prebuilt Fixture (page 429), we should use Finder Methods (see Test Utility Method) to locate the prebuilt objects. At the same time, we\n\nwww.it-ebooks.info",
      "content_length": 2657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "Creation Method\n\nmay still use Creation Methods to lay mutable objects that we plan to modify on top of an Immutable Shared Fixture (see Shared Fixture on page 317).\n\nSeveral variations of Creation Method are worth exploring.\n\nVariation: Parameterized Creation Method\n\nWhile it is possible (and often very desirable) for Creation Methods to take no parameters whatsoever, many tests will require some customization of the cre- ated object. A Parameterized Creation Method allows the test to pass in some attributes to be used in the creation of the object. In such a case, we should pass only those attributes that are expected to affect (or those we want to demon- strate do not affect) the test’s outcome; otherwise, we could be headed down the slippery slope to Obscure Tests.\n\nVariation: Anonymous Creation Method\n\nAn Anonymous Creation Method automatically creates a Distinct Generated Value (see Generated Value on page 723) as the unique identiﬁ er for the object it is creating even though the arguments it receives may not be unique. This behav- ior is invaluable for avoiding Unrepeatable Tests (see Erratic Test on page 228) because it ensures that every object we create is unique, even across multiple test runs. If the test cares about some attributes of the object to be created, it can pass them as parameters of the Creation Method; this behavior turns the Anony- mous Creation Method into a Parameterized Anonymous Creation Method.\n\nVariation: Parameterized Anonymous Creation Method\n\nA Parameterized Anonymous Creation Method is a combination of several other variations of Creation Method in that we pass in some attributes to be used in the creation of the object but let the Creation Method create the unique identi- ﬁ er for it. A Creation Method could also take zero parameters if the test doesn’t care about any of the attributes.\n\nVariation: Named State Reaching Method\n\nSome SUTs are essentially stateless, meaning we can call any method at any time. By contrast, when the SUT is state-rich and the validity or behavior of methods is affected by the state of the SUT, it is important to test each method from each possible starting state. We could chain a bunch of such tests together in a single Test Method (page 348), but that approach would create an Eager Test (see Assertion Roulette on page 224). It is better to use a series of Single- Condition Tests (see page 45) for this purpose. Unfortunately, that leaves us\n\nwww.it-ebooks.info\n\n417\n\nCreation Method",
      "content_length": 2490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "418\n\nCreation Method\n\nChapter 20 Fixture Setup Patterns\n\nwith the problem of how to set up the starting state in each test without a lot of Test Code Duplication (page 213).\n\nOne obvious solution is to put all tests that depend on the same starting state into the same Testcase Class (page 373) and to create the SUT in the appropri- ate state in the setUp method using Implicit Setup (page 424) (called Testcase Class per Fixture; see page 631). The alternative is to use Delegated Setup by calling a Named State Reaching Method; this approach allows us to choose some other way to organize our Testcase Classes.\n\nEither way, the code that sets up the SUT will be easier to understand if it is short and sweet. That’s where a Named State Reaching Method comes in handy. By encapsulating the logic required to create the test objects in the cor- rect state in a single place (whether on the Testcase Class or a Test Helper), we reduce the amount of code we must update if we need to change how we put the test object into that state.\n\nVariation: Attachment Method\n\nSuppose we already have a test object and we want to modify it in some way. We ﬁ nd ourselves performing this task in enough tests to want to code this modiﬁ ca- tion once and only once. The solution in this case is an Attachment Method. The main difference between this variation and the original Creation Method pattern is that we pass in the object to be modiﬁ ed (one that was probably returned by another Creation Method) and the object we want to set one of its attributes to; the Attachment Method does the rest of the work for us.\n\nImplementation Notes\n\nMost Creation Methods are created by doing an Extract Method [Fowler] refac- toring on parts of an existing test. When we write tests in an “outside-in” man- ner, we assume that the Creation Methods already exist and ﬁ ll in the method bodies later. In effect, we deﬁ ne a Higher-Level Language (see page 41) for deﬁ n- ing our ﬁ xtures. Nevertheless, there is another, completely different way to deﬁ ne Creation Methods.\n\nVariation: Reuse Test for Fixture Setup\n\nWe can set up the ﬁ xture by calling another Test Method to do the ﬁ xture setup for us. This assumes that we have some way of accessing the ﬁ xture that the other test created, either through a Registry [PEAA] object or through instance variables of the Testcase Object (page 382).\n\nIt may be appropriate to implement a Creation Method in this way when we already have tests that depend on other tests to set up their test ﬁ xture but\n\nwww.it-ebooks.info",
      "content_length": 2547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "Creation Method\n\nwe want to reduce the likelihood that a change in the test execution order of Chained Tests (page 454) will cause tests to fail. Mind you, the tests will run more slowly because each test will call all the preceding tests it depends on each time each test is run rather than each test being run only once per test run. Of course, each test needs to call only the speciﬁ c tests it actually depends on, not all tests in the test suite. This slowdown won’t be very noticeable if we have replaced any slow components, such as a database, with a Fake Object (page 551).\n\nWrapping the Test Method in a Creation Method is a better option than calling the Test Method directly from the client Test Method because most Test Methods are named based on which test condition(s) they verify, not what (ﬁ x- ture) they leave behind. The Creation Method lets us put a nice Intent-Revealing Name between the client Test Method and the implementing Test Method. It also solves the Lonely Test (see Erratic Test) problem because the other test is run explicitly from within the calling test rather than just assuming that it was already run. This scheme makes the test less fragile and easier to understand but it won’t solve the Interacting Tests (see Erratic Test) problem: If the test we call fails and leaves the test ﬁ xture in a different state than we expected, our test will likely fail as well, even if the functionality we are testing is still working.\n\nMotivating Example\n\nIn the following example, the testPurchase test requires a Customer to ﬁ ll the role of the buyer. The ﬁ rst and last names of the buyer have no bearing on the act of pur- chasing, but are required parameters of the Customer constructor; we do care that the Customer’s credit rating is good (“G”) and that he or she is currently active.\n\npublic void testPurchase_ﬁrstPurchase_ICC() { Customer buyer = new Customer(17, \"FirstName\", \"LastName\", \"G\",\"ACTIVE\"); // ... } public void testPurchase_subsequentPurchase_ICC() { Customer buyer = new Customer(18, \"FirstName\", \"LastName\", \"G\",\"ACTIVE\"); // ... }\n\nThe use of constructors in tests can be problematic, especially when we are building an application incrementally. Every change to the parameters of the constructor will force us to revisit a lot of tests or jump through hoops to keep the constructor signatures backward compatible for the sake of the tests.\n\nwww.it-ebooks.info\n\n419\n\nCreation Method",
      "content_length": 2437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "420\n\nCreation Method\n\nChapter 20 Fixture Setup Patterns\n\nRefactoring Notes\n\nWe can use an Extract Method refactoring to remove the direct call to the construc- tor. We can give the new Creation Method an appropriate Intent-Revealing Name such as createCustomer based on the style of Creation Method we have created.\n\nExample: Anonymous Creation Method\n\nIn the following example, instead of making that direct call to the Customer constructor, we now use the Customer Creation Method. Notice that the coupling between the ﬁ xture setup code and the constructor has been removed. If another parameter such as phone number is added to the Customer constructor, only the Customer Creation Method must be updated to provide a default value; the ﬁ xture setup code remains insulated from the change thanks to encapsulation.\n\npublic void testPurchase_ﬁrstPurchase_ACM() { Customer buyer = createAnonymousCustomer(); // ... } public void testPurchase_subsequentPurchase_ACM() { Customer buyer = createAnonymousCustomer(); // ... }\n\nWe call this pattern an Anonymous Creation Method because the identity of the customer does not matter. The Anonymous Creation Method might look something like this:\n\npublic Customer createAnonymousCustomer() { int uniqueid = getUniqueCustomerId(); return new Customer(uniqueid, \"FirstName\" + uniqueid, \"LastName\" + uniqueid, \"G\", \"ACTIVE\"); }\n\nNote the use of a Distinct Generated Value to ensure that each anonymous Customer is slightly different to avoid accidentally creating an identical Customer.\n\nExample: Parameterized Creation Method\n\nIf we wanted to supply some of the Customer’s attributes as parameters, we could deﬁ ne a Parameterized Creation Method:\n\npublic void testPurchase_ﬁrstPurchase_PCM() { Customer buyer = createCreditworthyCustomer(\"FirstName\", \"LastName\");\n\nwww.it-ebooks.info",
      "content_length": 1825,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "Creation Method\n\n// ... } public void testPurchase_subsequentPurchase_PCM() { Customer buyer = createCreditworthyCustomer(\"FirstName\", \"LastName\"); // ... }\n\nHere’s the corresponding Parameterized Creation Method deﬁ nition:\n\npublic Customer createCreditworthyCustomer( String ﬁrstName, String lastName) { int uniqueid = getUniqueCustomerId(); Customer customer = new Customer(uniqueid,ﬁrstName,lastName,\"G\",\"ACTIVE\"); customer.setCredit(CreditRating.EXCELLENT); customer.approveCredit(); return customer; }\n\nExample: Attachment Method\n\nHere’s an example of a test that uses an Attachment Method to associate two customers to verify that both get the best discount either of them has earned or negotiated:\n\npublic void testPurchase_relatedCustomerDiscount_AM() { Customer buyer = createCreditworthyCustomer(\"Related\", \"Buyer\"); Customer discountHolder = createCreditworthyCustomer(\"Discount\", \"Holder\"); createRelationshipBetweenCustomers( buyer, discountHolder); // ... }\n\nBehind the scenes, the Attachment Method does whatever it takes to establish the relationship:\n\nprivate void createRelationshipBetweenCustomers( Customer buyer, Customer discountHolder) { buyer.addToRelatedCustomersList( discountHolder ); discountHolder.addToRelatedCustomersList( buyer ); }\n\nAlthough this example is relatively simple, the call to this method is still easier to understand than reading both the method calls of which it consists.\n\nwww.it-ebooks.info\n\n421\n\nCreation Method",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "422\n\nCreation Method\n\nChapter 20 Fixture Setup Patterns\n\nExample: Test Reused for Fixture Setup\n\nWe can reuse other tests to set up the ﬁ xture for our test. Here is an example of how not to do it:\n\nprivate Customer buyer; private AccountManager sut = new AccountManager(); private Account account;\n\npublic void testCustomerConstructor_SRT() { // Exercise buyer = new Customer(17, \"First\", \"Last\", \"G\", \"ACTIVE\"); // Verify assertEquals( \"First\", buyer.ﬁrstName(), \"ﬁrst\"); // ... } public void testPurchase_SRT() { testCustomerConstructor_SRT(); // Leaves in ﬁeld \"buyer\" account = sut.createAccountForCustomer( buyer ); assertEquals( buyer.name, account.customerName, \"cust\"); // ... }\n\nThe problem here is twofold. First, the name of the Test Method we are calling describes what it veriﬁ es (e.g., a name) and not what it leaves behind (i.e., a Customer in the buyer ﬁ eld. Second, the test does not return a Customer; it leaves the Customer in an instance variable. This scheme works only because the Test Method we want to reuse is on the same Testcase Class; if it were on an unrelated class, we would have to do a few backﬂ ips to access the buyer. A better way to accomplish this goal is to encapsulate this call behind a Creation Method:\n\nprivate Customer buyer; private AccountManager sut = new AccountManager(); private Account account;\n\npublic void testCustomerConstructor_RTCM() { // Exercise buyer = new Customer(17, \"First\", \"Last\", \"G\", \"ACTIVE\"); // Verify assertEquals( \"First\", buyer.ﬁrstName(), \"ﬁrst\"); // ... } public void testPurchase_RTCM() { buyer = createCreditworthyCustomer(); account = sut.createAccountForCustomer( buyer ); assertEquals( buyer.name, account.customerName, \"cust\"); // ... } public Customer createCreditworthyCustomer() { testCustomerConstructor_RTCM();\n\nwww.it-ebooks.info",
      "content_length": 1819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "Creation Method\n\nreturn buyer; // ... }\n\nNotice how much more readable this test has become? We can see where the buyer came from! This was easy to do because both Test Methods were on the same class. If they were on different classes, our Creation Method would have to create an instance of the other Testcase Class before it could run the test. Then it would have to ﬁ nd a way to access the buyer instance variable so that it could return it to the calling Test Method.\n\nwww.it-ebooks.info\n\n423\n\nCreation Method",
      "content_length": 514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "424\n\nImplicit Setup\n\nAlso known as: Hooked Setup, Framework- Invoked Setup, Shared Setup Method\n\nChapter 20 Fixture Setup Patterns\n\nImplicit Setup\n\nHow do we construct the Fresh Fixture?\n\nWe build the test ﬁ xture common to several tests in the setUp method.\n\nsetup setup\n\nSetup Setup\n\nFixture Fixture\n\ntest_1 test_1\n\ntest_2 test_2\n\nSUT SUT\n\ntest_n test_n\n\nTestcase Class Testcase Class\n\nTo execute an automated test, we require a text ﬁ xture that is well understood and completely deterministic. We are using a Fresh Fixture (page 311) approach to build the Minimal Fixture (page 302) for the use of this one test.\n\nImplicit Setup is a way to reuse the ﬁ xture setup code for all Test Meth-\n\nods (page 348) in a Testcase Class (page 373).\n\nHow It Works\n\nAll tests in a Testcase Class create identical Fresh Fixtures by doing test ﬁ xture setup in a special setUp method on the Testcase Class. The setUp method is called automatically by the Test Automation Framework (page 298) before it calls each Test Method. This allows the ﬁ xture setup code placed in the setUp method to be reused without reusing the same instance of the test ﬁ xture. This approach is called “implicit” setup because the calls to the ﬁ xture setup logic are not explicit within the Test Method, unlike with In-line Setup (page 408) and Delegated Set- up (page 411).\n\nwww.it-ebooks.info",
      "content_length": 1361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "Implicit Setup\n\nWhen to Use It\n\nWe can use Implicit Setup when several Test Methods on the same Testcase Class need an identical Fresh Fixture. If all Test Methods need the exact same ﬁ xture, then the entire Minimal Fixture needed by each test can be set up in the setUp method. This form of Test Method organization is known as Testcase Class per Fixture (page 631).\n\nWhen the Test Methods need different ﬁ xtures because we are using a Testcase Class per Feature (page 624) or Testcase Class per Class (page 617) scheme, it is more difﬁ cult to use Implicit Setup and still build a Minimal Fixture. We can use the setUp method only to set up the part of the ﬁ xture that does not cause any problems for the other tests. A reasonable compromise is to use Implicit Setup to set up the parts of the ﬁ xture that are essential but irrelevant and leave the setup of critical (and different from test to test) parts of the ﬁ xture to the individual Test Methods. Examples of “essential but irrelevant” ﬁ xture setup include ini- tializing variables with “don’t care” values and initializing hidden “plumbing” such as database connections. Fixture setup logic that directly affects the state of the SUT should be left to the individual Test Methods unless every Test Method requires the same starting state.\n\nThe obvious alternatives for creating a Fresh Fixture are In-line Setup, in which we include all setup logic within each Test Method without factoring out any common code, and Delegated Setup, in which we move all common ﬁ xture setup code into a set of Creation Methods (page 415) that we can call from within the setup part of each Test Method.\n\nImplicit Setup removes a lot of Test Code Duplication (page 213) and helps prevent Fragile Tests (page 239) by moving much of the nonessential interaction with the SUT out of the very numerous tests and into a much smaller num- ber of places where it is easier to maintain. It can, however, lead to Obscure Tests (page 186) when a Mystery Guest makes the test ﬁ xture used by each test less obvious. It can also lead to a Fragile Fixture (see Fragile Test) if all tests in the class do not really need identical test ﬁ xtures.\n\nImplementation Notes\n\nThe main implementation considerations for Implicit Setup are as follows:\n\nHow do we cause the ﬁ xture setUp method to be called?\n\nHow do we tear the ﬁ xture down?\n\nHow do the Test Methods access the ﬁ xture?\n\nwww.it-ebooks.info\n\n425\n\nImplicit Setup",
      "content_length": 2452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 489,
      "content": "426\n\nImplicit Setup\n\nChapter 20 Fixture Setup Patterns\n\nCalling the Setup Code\n\nA setUp method is the most common way to handle Implicit Setup; it consists of having the Test Automation Framework call the setUp method before each Test Method. Strictly speaking, the setUp method is not the only form of implicit ﬁ x- ture setup. Suite Fixture Setup (page 441), for example, is used to set up and tear down a Shared Fixture (page 317) that is reused by the Test Methods on a single Testcase Class. In addition, Setup Decorator (page 447) moves the setUp method to a Decorator [GOF] object installed between the Test Suite Object (page 387) and the Test Runner (page 377). Both are forms of Implicit Setup because the setUp logic is not explicit within the Test Method.\n\nTearing Down the Fixture\n\nThe ﬁ xture teardown counterpart of Implicit Setup is Implicit Teardown (page 516). Anything that we set up in the setUp method that is not automatically cleaned up by Automated Teardown (page 503) or garbage collection should be torn down in the corresponding tearDown method.\n\nAccessing the Fixture\n\nThe Test Methods need to be able to access the test ﬁ xture built in the setUp method. When they were used in the same method, local variables were sufﬁ cient. To communicate between the setUp method and the Test Method, how- ever, the local variables must be changed into instance variables. We must be careful not to make them class variables as this will result in the potential for a Shared Fixture. (See the sidebar “There’s Always an Exception” on page 384 for a description of when instance variations do not provide this level of isolation.)\n\nMotivating Example\n\nIn the following example, each test needs to create a ﬂ ight between a pair of airports.\n\npublic void testStatus_initial() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); // teardown // garbage-collected\n\nwww.it-ebooks.info",
      "content_length": 2149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "Implicit Setup\n\n}\n\npublic void testStatus_cancelled() { // in-line setup Airport departureAirport = new Airport(\"Calgary\", \"YYC\"); Airport destinationAirport = new Airport(\"Toronto\", \"YYZ\"); Flight ﬂight = new Flight( ﬂightNumber, departureAirport, destinationAirport); ﬂight.cancel(); // still part of setup // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); // teardown // garbage-collected }\n\nRefactoring Notes\n\nThese tests contain a fair amount of Test Code Duplication. We can remove this duplication by refactoring this Testcase Class to use Implicit Setup. There are two refactoring cases to consider.\n\nFirst, when we discover that all tests are doing similar work to set up their test ﬁ xtures but are not sharing a setUp method, we can do an Extract Meth- od [Fowler] refactoring of the ﬁ xture setup logic in one of the tests to create our setUp method. We will also need to convert any local variables to instance variables (ﬁ elds) that hold the references to the resulting ﬁ xture until the Test Method can access it.\n\nSecond, when we discover that a Testcase Class already uses the setUp method to build the ﬁ xture and has tests that need a different ﬁ xture, we can use an Extract Class [Fowler] refactoring to move all Test Methods that need a different setup method to a different class. We need to ensure any instance variables that are used to convey knowledge of the ﬁ xture from the setup method to the Test Methods are transferred along with the setUp method. Sometimes it is simpler to clone the Testcase Class and delete each test from one or the other copy of the class; we can then delete from each class any instance variables that are no longer being used.\n\nExample: Implicit Setup\n\nIn this modiﬁ ed example, we have moved all common ﬁ xture setup code to the setUp method of our Testcase Class. This avoids the need to repeat this code in each test and makes each test much shorter—which is a good thing.\n\nAirport departureAirport; Airport destinationAirport;\n\nwww.it-ebooks.info\n\n427\n\nImplicit Setup",
      "content_length": 2070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "428\n\nImplicit Setup\n\nChapter 20 Fixture Setup Patterns\n\nFlight ﬂight;\n\npublic void setUp() throws Exception{ super.setUp(); departureAirport = new Airport(\"Calgary\", \"YYC\"); destinationAirport = new Airport(\"Toronto\", \"YYZ\"); BigDecimal ﬂightNumber = new BigDecimal(\"999\"); ﬂight = new Flight( ﬂightNumber , departureAirport, destinationAirport); }\n\npublic void testGetStatus_initial() { // implicit setup // exercise SUT and verify outcome assertEquals(FlightState.PROPOSED, ﬂight.getStatus()); }\n\npublic void testGetStatus_cancelled() { // implicit setup partially overridden ﬂight.cancel(); // exercise SUT and verify outcome assertEquals(FlightState.CANCELLED, ﬂight.getStatus()); }\n\nThis approach has several disadvantages, which arise because we are not organizing our Test Methods around a Testcase Class per Fixture. (We are using Testcase Class per Feature here.) All the Test Methods on the Testcase Class must be able to make do with the same ﬁ xture (at least as a starting point), as evidenced by the partially overridden ﬁ xture setup in the second test in the exam- ple. The ﬁ xture is also not very obvious in these tests. Where does the ﬂ ight come from? Is there anything special about it? We cannot even rename the instance variable to communicate the nature of the ﬂ ight better because we are using it to hold ﬂ ights with different characteristics in each test.\n\nwww.it-ebooks.info",
      "content_length": 1403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "Prebuilt Fixture\n\nPrebuilt Fixture\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nWe build the Shared Fixture separately from running the tests.\n\nSetup Setup\n\nSetup Setup\n\nFixture Fixture\n\nTest Runner Test Runner\n\nExercise Exercise\n\nSUT SUT\n\nExercise Exercise\n\nVerify Verify\n\nTeardown Teardown\n\nVerify Verify\n\nTeardown Teardown\n\nWhen we choose to use a Shared Fixture (page 317), whether it be for reasons of convenience or out of necessity, we need to create the Shared Fixture before we use it.\n\nHow It Works\n\nWe create the ﬁ xture sometime before running the test suite. We can create the ﬁ xture a number of different ways that we’ll discuss later. The most important point is that we don’t need to build the ﬁ xture each time the test suite is run because the ﬁ xture outlives both the mechanism used to build it and any one test run that uses it.\n\nWhen to Use It\n\nWe can reduce the overhead of creating a Shared Fixture each time a test suite is run by creating the ﬁ xture only occasionally. This pattern is especially appropri- ate when the cost of constructing the Shared Fixture is extremely high or cannot be automated easily.\n\nwww.it-ebooks.info\n\n429\n\nPrebuilt Fixture\n\nAlso known as: Prebuilt Context, Test Bed",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "430\n\nPrebuilt Fixture\n\nChapter 20 Fixture Setup Patterns\n\nBecause of the Manual Intervention (page 250) required to (re)build the ﬁ xture before the tests are run, we’ll probably end up using the same ﬁ xture several times, which can lead to Erratic Tests (page 228) caused by shared ﬁ xture pollution. We may be able to avoid these problems by treating the Prebuilt Fixture as an Immu- table Shared Fixture (see Shared Fixture) and building a Fresh Fixture (page 311) for anything we plan to modify.\n\nThe alternatives to a Prebuilt Fixture are a Shared Fixture that is built once per test run and a Fresh Fixture. Shared Fixtures can be constructed using Suite Fixture Setup (page 441), Lazy Setup (page 435), or Setup Decorator (page 447). Fresh Fixtures can be constructed using In-line Setup (page 408), Implicit Setup (page 424), or Delegated Setup (page 411).\n\nVariation: Global Fixture\n\nA Global Fixture is a special case of Prebuilt Fixture where we shared the ﬁ xture between multiple test automaters. The key difference is that the ﬁ xture is globally visible and not “private” to a particular user. This pattern is most commonly em- ployed when we are using a single shared Database Sandbox (page 650) without using some form of Database Partitioning Scheme (see Database Sandbox).\n\nThe tests themselves can be the same as those used for a basic Prebuilt Fix- ture; likewise, the ﬁ xture setup is the same as that for a Prebuilt Fixture. What’s different here are the kinds of problems we can encounter. Because the ﬁ xture is now shared among multiple users, each of whom is running a separate Test Runner (page 377) on a different CPU, we may experience all sorts of multipro- cessing-related issues. The most common problem is a Test Run War (see Erratic Test) where we see seemingly random results. We can avoid this possibility by adopting some kind of Database Partitioning Scheme or by using Distinct Generated Values (see Generated Value on page 723) for any ﬁ elds with unique key constraints.\n\nImplementation Notes\n\nThe tests themselves look identical to a basic Shared Fixture. What’s different is how the ﬁ xture is set up. The test reader won’t be able to ﬁ nd any sign of it either within the Testcase Class (page 373) or in a Setup Decorator or Suite Fixture Setup method. Instead, the ﬁ xture setup is most probably performed manually via some kind of database copy operation, by using a Data Loader (see Back Door Manipulation on page 327) or by running a database population script. In these examples of Back Door Setup (see Back Door Manipulation), we bypass the SUT and interact with its database directly. (See the sidebar “Database as\n\nwww.it-ebooks.info",
      "content_length": 2687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "Prebuilt Fixture\n\nSUT API?” on page 336 for an example of when the back door really is a front door.) Another option is to use a Fixture Setup Testcase (see Chained Tests on page 454) run from a Test Runner either manually or on a regular schedule.\n\nAnother difference is how the Finder Methods (see Test Utility Method on page 599) are implemented. We cannot just store the results of creating the objects in a class variable or an in-memory Test Fixture Registry (see Test Helper on page 643) because we aren’t setting the ﬁ xture up in code within the test run. Two of the more commonly used options available to us are (1) to store the unique identiﬁ ers generated during ﬁ xture construction in a persistent Test Fixture Registry (such as a ﬁ le) as we build the ﬁ xture so that the Finder Meth- ods can retrieve them later and (2) to hard-code the identiﬁ ers in the Finder Methods. We could search for objects/records that meet the Finder Methods’ criteria at runtime, but that approach might result in Nondeterministic Tests (see Erratic Test) because each test run could end up using a different object/re- cord from the Prebuilt Fixture. This strategy may be a good idea if each test run modiﬁ es the objects such that they no longer satisfy the criteria. Nevertheless, it may make debugging a failing test rather difﬁ cult, especially if the failures occur intermittently because some other attribute of the selected object is different.\n\nMotivating Example\n\nThe following example shows the construction of a Shared Fixture using Lazy Setup:1\n\nprotected void setUp() throws Exception { if (sharedFixtureInitialized) { return; } facade = new FlightMgmtFacadeImpl(); setupStandardAirportsAndFlights(); sharedFixtureInitialized = true; }\n\nprotected void tearDown() throws Exception { // Cannot delete any objects because we don't know // whether this is the last test }\n\nNote the call to setupStandardAirports in the setUp method. The tests use this ﬁ xture by calling Finder Methods that return objects from the ﬁ xture that match certain criteria:\n\n1 Of course, there are other ways to set up the Shared Fixture, such as Setup Decorator and Suite Fixture Setup.\n\nwww.it-ebooks.info\n\n431\n\nPrebuilt Fixture",
      "content_length": 2214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "432\n\nPrebuilt Fixture\n\nChapter 20 Fixture Setup Patterns\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nRefactoring Notes\n\nOne way to convert a Testcase Class from a Standard Fixture (page 305) to a Prebuilt Fixture is to do an Extract Class [Fowler] refactoring so that the ﬁ xture is set up in one class and the Test Methods (page 348) are located in another class. Of course, we need to provide a way for the Finder Methods to deter- mine which objects or records exist in the structure because we won’t be able to guarantee that any instance or class variables will bridge the time gap between ﬁ xture construction and ﬁ xture usage.\n\nExample: Prebuilt Fixture Test\n\nHere is the resulting Testcase Class that contains the Test Methods. Note that it looks almost identical to the basic Shared Fixture tests.\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome\n\nwww.it-ebooks.info",
      "content_length": 1766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 496,
      "content": "Prebuilt Fixture\n\nassertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nWhat’s different is how the ﬁ xture is set up and how the Finder Methods are implemented.\n\nExample: Fixture Setup Testcase\n\nWe may ﬁ nd it to be convenient to set up our Prebuilt Fixture using xUnit. This is simple to do if we already have the appropriate Creation Methods (page 415) or constructors already deﬁ ned and we have a way to easily persist the objects into the Database Sandbox. In the following example, we call the same method as in the previous example from the setUp method, except that now the method lives in the setUp method of a Fixture Setup Testcase that can be run whenever we want to regenerate the Prebuilt Fixture:\n\npublic class FlightManagementFacadeSetupTestcase extends AbstractFlightManagementFacadeTestCase { public FlightManagementFacadeSetupTestcase(String name) { super(name); }\n\nprotected void setUp() throws Exception { facade = new FlightMgmtFacadeImpl(); helper = new FlightManagementTestHelper(); setupStandardAirportsAndFlights(); saveFixtureInformation(); }\n\nprotected void tearDown() throws Exception { // Leave the Prebuilt Fixture for later use }\n\n}\n\nwww.it-ebooks.info\n\n433\n\nPrebuilt Fixture",
      "content_length": 1613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "434\n\nPrebuilt Fixture\n\nChapter 20 Fixture Setup Patterns\n\nNote that there are no Test Methods on this Testcase Class and the tearDown method is empty. Here we want to do only the setup—nothing else.\n\nOnce we created the objects, we saved the information to the database using the call to saveFixtureInformation; this method persists the objects and saves the various keys in a ﬁ le so that we can reload them for use from the subsequent real test runs. This approach avoids the need to hard-code knowledge of the ﬁ xture into Test Methods or Test Utility Methods. In the interest of space, I’ll spare you the details of how we ﬁ nd the “dirty” objects and save the key infor- mation; there is more than one way to handle this task and any of these tactics will sufﬁ ce.\n\nExample: Prebuilt Fixture Setup Using a Data Population Script\n\nThere are as many ways to build a Prebuilt Fixture in a Database Sandbox as there are programming languages—everything from SQL scripts to Pearl and Ruby programs. These scripts can contain the data or they can read the data from a collection of ﬂ at ﬁ les. We can even copy the contents of a “golden” data- base into our Database Sandbox. I’ll leave it as an exercise for you to ﬁ gure out what’s most appropriate in your particular circumstance.\n\nwww.it-ebooks.info",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "Lazy Setup\n\nLazy Setup\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nWe use Lazy Initialization of the ﬁ xture to create it in the ﬁ rst test that needs it.\n\nTestcase Testcase Object Object Implicit setUp Implicit setUp\n\nsetUp setUp\n\nTest Test Suite Suite Object Object\n\ntestMethod_1 testMethod_1\n\nIs Fixture Set Is Fixture Set Up Yet? Up Yet?\n\nNo No\n\nCreate Create\n\nFixture Fixture\n\nTestcase Testcase Object Object Implicit setUp Implicit setUp\n\nExercise Exercise\n\nSUT SUT\n\ntestMethod_n testMethod_n\n\nShared Fixtures (page 317) are often used to speed up test execution by reducing the number of times a complex ﬁ xture needs to be created. Unfortunately, a test that depends on other tests to set up the ﬁ xture cannot be run by itself; it is a Lonely Test (see Erratic Test on page 228)\n\nWe can avoid this problem by having each test use Lazy Setup to set up the\n\nﬁ xture if it is not already set up.\n\nHow It Works\n\nWe use Lazy Initialization [SBPP] to construct the ﬁ xture in the ﬁ rst test that needs it and then store a reference to the ﬁ xture in a class variable that every test can access. All subsequently run tests will discover that the ﬁ xture is already created and that they can reuse it, thereby avoiding the effort of constructing the ﬁ xture anew.\n\nwww.it-ebooks.info\n\n435\n\nLazy Setup",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "436\n\nLazy Setup\n\nChapter 20 Fixture Setup Patterns\n\nWhen to Use It\n\nWe can use Lazy Setup whenever we need to create a Shared Fixture yet still want to be able to run each test by itself. We can also use Lazy Setup instead of other techniques such as Setup Decorator (page 447) and Suite Fixture Set- up (page 441) if it is not crucial that the ﬁ xture be torn down. For example, we could use Lazy Setup when we are using a ﬁ xture that can be torn down by Garbage-Collected Teardown (page 500). We might also use Lazy Setup when we are using Distinct Generated Values (see Generated Value on page 723) for all database keys and aren’t worried about leaving extra records lying around after each test; Delta Assertions (page 485) make this approach possible.\n\nThe major disadvantage of Lazy Setup is the fact that while it is easy to discover that we are running the ﬁ rst test and need to construct the ﬁ xture, it is difﬁ cult to determine that we are running the last test and the ﬁ xture should be destroyed. Most members of the xUnit family of Test Automation Frameworks (page 298) do not provide any way to determine this fact other than by using a Setup Decorator for the entire test suite. A few members of the xUnit family support Suite Fixture Setup (NUnit, VbUnit, and JUnit 4.0 and newer, to name a few), which provides setUp/tearDown “bookends” for a Testcase Class (page 373). Unfortunately, this ability won’t help us if we are writing our tests in Ruby, Python, or PLSQL!\n\nSome IDEs and Test Runners (page 377) automatically reload our classes every time the test suite is run. This causes the original class variable to go out of scope, and the ﬁ xture will be garbage-collected before the new version of the class is run. In these cases there may be no negative consequence of using Lazy Setup.\n\nA Prebuilt Fixture (page 429) is another alternative to setting up the Shared Fixture for each test run. Its use can lead to Unrepeatable Tests (see Erratic Test) if the ﬁ xture is corrupted by some of the tests.\n\nImplementation Notes\n\nBecause Lazy Setup makes sense only with Shared Fixtures, Lazy Setup carries all the same baggage that comes with Shared Fixtures.\n\nNormally, Lazy Setup is used to build a Shared Fixture to be used by a single Testcase Class. The reference to the ﬁ xture is held in a class variable. Things get a bit trickier if we want to share the ﬁ xture across several Testcase Classes. We could move both the Lazy Initialization logic and the class variable to a Testcase Superclass (page 638) but only if our language supports inheritance of class variables. The other alternative is to move the logic and variables to a Test Helper (page 643).\n\nwww.it-ebooks.info",
      "content_length": 2704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "Lazy Setup\n\nOf course, we could use an approach such as reference counting as a way to know whether all Test Methods (page 348) have run. The challenge would be to know how many Testcase Objects (page 382) are in the Test Suite Object (page 387) so that we can compare this number with the number of times the tearDown method has been called. I have never seen anyone do this so I won’t call it a pattern! Adding logic to the Test Runner to invoke a tearDown method at the Test Suite Object level would amount to implementing Suite Fixture Setup.\n\nMotivating Example\n\nIn this example, we have been building a new ﬁ xture for each Testcase Object:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { setupStandardAirportsAndFlights(); FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { setupStandardAirportsAndFlights(); FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nNot surprisingly, these tests are slow because creating the airports and ﬂ ights involves a database. We can try refactoring these tests to set up the ﬁ xture in the setUp method (Implicit Setup; see page 424):\n\nprotected void setUp() throws Exception { facade = new FlightMgmtFacadeImpl(); helper = new FlightManagementTestHelper(); setupStandardAirportsAndFlights(); oneOutboundFlight = ﬁndOneOutboundFlight(); }\n\nwww.it-ebooks.info\n\n437\n\nLazy Setup",
      "content_length": 1889,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "438\n\nLazy Setup\n\nChapter 20 Fixture Setup Patterns\n\nprotected void tearDown() throws Exception { removeStandardAirportsAndFlights(); }\n\npublic void testGetFlightsByOriginAirport_NoFlights_td() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); try { // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { facade.removeAirport(outboundAirport); } }\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( oneOutboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", oneOutboundFlight, ﬂightsAtOrigin); }\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nThis doesn’t speed up our tests one bit because the Test Automation Framework calls the setUp and tearDown methods for each Testcase Object. All we have done is moved the code. We need to ﬁ nd a way to set up the ﬁ xture only once per test run.\n\nwww.it-ebooks.info",
      "content_length": 1445,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "Lazy Setup\n\nRefactoring Notes\n\nWe can reduce the number of times we set up the ﬁ xture by converting this test to Lazy Setup. Because the ﬁ xture setup is already handled by the setUp method, we need simply insert the Lazy Initialization logic into the setUp method so that only the ﬁ rst test will cause it to be run. We must not forget to remove the tearDown logic, because it will render the Lazy Initialization logic useless if it removes the ﬁ xture after each Test Method has run! Sorry, but there is nowhere that we can move this logic to so that it will be run after the last Test Method has completed if our xUnit family member doesn’t support Suite Fixture Setup.\n\nExample: Lazy Setup\n\nHere is the same test refactored to use Lazy Setup:\n\nprotected void setUp() throws Exception { if (sharedFixtureInitialized) { return; } facade = new FlightMgmtFacadeImpl(); setupStandardAirportsAndFlights(); sharedFixtureInitialized = true; }\n\nprotected void tearDown() throws Exception { // Cannot delete any objects because we don't know // whether this is the last test }\n\nWhile there is a tearDown method on AirportFixture, there is no way to know when to call it! That’s the main consequence of using Lazy Setup. Because the variables are static, they will not go out of scope; hence the ﬁ xture will not be garbage col- lected until the class is unloaded or reloaded.\n\nThe tests are unchanged from the Implicit Setup version:\n\npublic void testGetFlightsByFromAirport_OneOutboundFlight() throws Exception { FlightDto outboundFlight = ﬁndOneOutboundFlight(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlight.getOriginAirportId()); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", outboundFlight, ﬂightsAtOrigin); }\n\nwww.it-ebooks.info\n\n439\n\nLazy Setup",
      "content_length": 1811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "440\n\nLazy Setup\n\nChapter 20 Fixture Setup Patterns\n\npublic void testGetFlightsByFromAirport_TwoOutboundFlights() throws Exception { FlightDto[] outboundFlights = ﬁndTwoOutboundFlightsFromOneAirport(); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( outboundFlights[0].getOriginAirportId()); // Verify Outcome assertExactly2FlightsInDtoList( \"Flights at origin\", outboundFlights, ﬂightsAtOrigin); }\n\nwww.it-ebooks.info",
      "content_length": 444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "Suite Fixture Setup\n\nSuite Fixture Setup\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nWe build/destroy the shared ﬁ xture in special methods called by the Test Automation Framework before/after the ﬁ rst/last Test Method is called.\n\nSuiteFixture setUp SuiteFixture setUp\n\nSetup Setup\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nFixture Fixture\n\nInline Setup Inline Setup Exercise Exercise Verify Verify Inline Teardown Inline Teardown\n\nTestSuite Object TestSuite Object for for Testcase Class Testcase Class\n\nImplicit tearDown Implicit tearDown\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nSUT SUT\n\nInline Setup Inline Setup Exercise Exercise Verify Verify Inline Teardown Inline Teardown\n\nImplicit tearDown Implicit tearDown\n\nSuiteFixture tearDown SuiteFixture tearDown\n\nShared Fixtures (page 317) are commonly used to reduce the amount of per-test overhead required to set up the ﬁ xture. Sharing a ﬁ xture involves extra test program- ming effort because we must create the ﬁ xture and have a way of discovering the ﬁ xture in each test. Regardless of how the ﬁ xture is accessed, it must be initialized (constructed) before it is used.\n\nSuite Fixture Setup is one way to initialize the ﬁ xture if all the Test Meth- ods (page 348) that need it are deﬁ ned on the same Testcase Class (page 373).\n\nHow It Works\n\nWe implement or override a pair of methods that the Test Automation Frame- work (page 298) calls automatically. The name or annotation of these methods varies between members of the xUnit family but all work the same way: The framework calls the Suite Fixture Setup method before it calls the setUp method for the ﬁ rst Test Method; it calls the Suite Fixture Teardown method after it calls the tearDown method for the ﬁ nal Test Method. (I would have preferred to\n\nwww.it-ebooks.info\n\n441\n\nSuite Fixture Setup",
      "content_length": 1911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "442\n\nSuite Fixture Setup\n\nChapter 20 Fixture Setup Patterns\n\nsay, “method on the ﬁ rst/ﬁ nal Testcase Object” but that isn’t true: NUnit, unlike other members of the xUnit family, creates only a single Testcase Object. See the sidebar “There’s Always an Exception” on page 384 for details.)\n\nWhen to Use It\n\nWe can use Suite Fixture Setup when we have a test ﬁ xture we wish to share between all Test Methods of a single Testcase Class and our variant of xUnit sup- ports this feature. This pattern is particularly useful if we need to tear down the ﬁ xture after the last test is run. At the time of writing this book, only VbUnit, NUnit, and JUnit 4.0 supported Suite Fixture Setup “out of the box.” Nevertheless, it is not difﬁ cult to add this capability in most variants of xUnit.\n\nIf we need to share the ﬁ xture more widely, we must use either a Prebuilt Fixture (page 429), a Setup Decorator (page 447), or Lazy Setup (page 435). If we don’t want to share the actual instance of the ﬁ xture but we do want to share the code to set up the ﬁ xture, we can use Implicit Setup (page 424) or Delegated Setup (page 411).\n\nThe main reason for using a Shared Fixture, and hence Suite Fixture Setup, is to overcome the problem of Slow Tests (page 253) caused by too many test ﬁ xture objects being created each time every test is run. Of course, a Shared Fixture can lead to Interacting Tests (see Erratic Test on page 228) or even a Test Run War (see Erratic Test); the sidebar “Faster Tests Without Shared Fixtures” (page 319) describes other ways to solve this problem.\n\nImplementation Notes\n\nFor Suite Fixture Setup to work properly, we must ensure that the ﬁ xture is remembered between calls to the Test Methods. This criterion implies we need to use a class variable, Registry [PEAA], or Singleton [GOF] to hold the references to the ﬁ xture (except in NUnit; see the sidebar “There’s Always an Exception” on page 384). The exact implementation varies from one member of the xUnit family to the next. Here are a few highlights:\n\nIn VbUnit, we implement the interface IFixtureFrame in the Testcase Class, thereby causing the Test Automation Framework (1) to call the IFixture Frame_Create method before the ﬁ rst Test Method is called and (2) to call the IFixtureFrame_Destroy method after the last Test Method is called.\n\nIn NUnit, the attributes [TestFixtureSetUp] and [TestFixtureTearDown] are used inside a test ﬁ xture to designate the methods to be called (1) once\n\nwww.it-ebooks.info",
      "content_length": 2495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "Suite Fixture Setup\n\nprior to executing any of the tests in the ﬁ xture and (2) once after all tests are completed.\n\nIn JUnit 4.0 and later, the attribute @BeforeClass is used to indicate that a method should be run once before the ﬁ rst Test Method is executed. The method with the attribute @AfterClass is run after the last Test Method is run. JUnit allows these methods to be inherited and overridden; the subclass’s methods are run between the superclass’s methods. Suite Fixture Setup\n\nIn JUnit 4.0 and later, the attribute @BeforeClass is used to indicate that a method should be run once before the ﬁ rst Test Method is executed. The method with the attribute @AfterClass is run after the last Test Method is run. JUnit allows these methods to be inherited and overridden; the subclass’s methods are run between the superclass’s methods. Suite Fixture Setup\n\nMotivating Example\n\nSuppose we have the following test:\n\n[SetUp] protected void setUp() { helper.setupStandardAirportsAndFlights(); }\n\n[TearDown] protected void tearDown() { helper.removeStandardAirportsAndFlights(); }\n\n[Test] public void testGetFlightsByOriginAirport_2OutboundFlights(){ FlightDto[] expectedFlights = helper.ﬁndTwoOutboundFlightsFromOneAirport(); long originAirportId = expectedFlights[0].OriginAirportId; // Exercise System IList ﬂightsAtOrigin = facade.GetFlightsByOriginAirport(originAirportId); // Verify Outcome AssertExactly2FlightsInDtoList( expectedFlights[0], expectedFlights[1], ﬂightsAtOrigin, \"Flights at origin\"); }\n\n[Test] public void testGetFlightsByOriginAirport_OneOutboundFlight(){ FlightDto expectedFlight = helper.ﬁndOneOutboundFlight(); // Exercise System\n\nwww.it-ebooks.info\n\n443",
      "content_length": 1686,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "444\n\nSuite Fixture Setup\n\nChapter 20 Fixture Setup Patterns\n\nIList ﬂightsAtOrigin = facade.GetFlightsByOriginAirport( expectedFlight.OriginAirportId); // Verify Outcome AssertOnly1FlightInDtoList( expectedFlight, ﬂightsAtOrigin, \"Outbound ﬂight at origin\"); }\n\nFigure 20.1 is the console generated by an instrumented version of these tests.\n\n-------------------- setUp setupStandardAirportsAndFlights testGetFlightsByOriginAirport_OneOutboundFlight tearDown removeStandardAirportsAndFlights -------------------- setUp setupStandardAirportsAndFlights testGetFlightsByOriginAirport_TwoOutboundFlights tearDown removeStandardAirportsAndFlights -------------------- Figure 20.1 The calling sequence of Implicit Setup and Test Methods. The setupStandardAirportsAndFlights method is called before each Test Method. The hori- zontal lines delineate the Test Method boundaries.\n\nRefactoring Notes\n\nSuppose we want to refactor this example to a Shared Fixture. If we don’t care about destroying the ﬁ xture when the test run is ﬁ nished, we could use Lazy Setup. Otherwise, we can convert this example to a Suite Fixture Setup strategy by simply moving our code from the setUp and tearDown methods to the suiteFixtureSetUp and suiteFixtureTearDown methods, respectively.\n\nIn NUnit, we use the attributes [TestFixtureSetUp] and [TestFixtureTearDown] to indicate these methods to the Test Automation Framework. If we don’t want to leave anything in our setUp/tearDown methods, we can simply change the attributes from [Setup] and TearDown to [TestFixtureSetUp] and [TestFixtureTearDown], respectively.\n\nExample: Suite Fixture Setup\n\nHere’s the result of our refactoring to Suite Fixture Setup:\n\n[TestFixtureSetUp] protected void suiteFixtureSetUp() {\n\nwww.it-ebooks.info",
      "content_length": 1759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "Suite Fixture Setup\n\nhelper.setupStandardAirportsAndFlights(); }\n\n[TestFixtureTearDown] protected void suiteFixtureTearDown() { helper.removeStandardAirportsAndFlights(); }\n\n[SetUp] protected void setUp() { }\n\n[TearDown] protected void tearDown() { }\n\n[Test] public void testGetFlightsByOrigin_TwoOutboundFlights(){ FlightDto[] expectedFlights = helper.ﬁndTwoOutboundFlightsFromOneAirport(); long originAirportId = expectedFlights[0].OriginAirportId; // Exercise System IList ﬂightsAtOrigin = facade.GetFlightsByOriginAirport(originAirportId); // Verify Outcome AssertExactly2FlightsInDtoList( expectedFlights[0], expectedFlights[1], ﬂightsAtOrigin, \"Flights at origin\"); }\n\n[Test] public void testGetFlightsByOrigin_OneOutboundFlight() { FlightDto expectedFlight = helper.ﬁndOneOutboundFlight(); // Exercise System IList ﬂightsAtOrigin = facade.GetFlightsByOriginAirport( expectedFlight.OriginAirportId); // Verify Outcome AssertOnly1FlightInDtoList( expectedFlight, ﬂightsAtOrigin, \"Outbound ﬂight at origin\"); }\n\nNow when various methods of the Testcase Class are called, the console looks like Figure 20.2.\n\nwww.it-ebooks.info\n\n445\n\nSuite Fixture Setup",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "446\n\nSuite Fixture Setup\n\nChapter 20 Fixture Setup Patterns\n\nsuiteFixtureSetUp setupStandardAirportsAndFlights -------------------- setUp testGetFlightsByOriginAirport_OneOutboundFlight tearDown -------------------- setUp testGetFlightsByOriginAirport_TwoOutboundFlights tearDown -------------------- suiteFixtureTearDown removeStandardAirportsAndFlights\n\nFigure 20.2 The calling sequence of Suite Fixture Setup and Test Methods. The setupStandardAndAirportsAndFlights method is called once only for the Testcase Class rather than before each Test Method. The horizontal lines delineate the Test Method boundaries.\n\nThe setUp method is still called before each Test Method, along with the suite FixtureSetUp method where we are now calling setupStandardAirportsAndFlights to set up our ﬁ xture. So far, this is no different than Lazy Setup; the difference arises in that removeStandardAirportsAndFlights is called after the last of our Test Methods.\n\nAbout the Name\n\nNaming this pattern was tough because each variant of xUnit that implements it has a different name for it. Complicating matters is the fact that the Microsoft camp uses “test ﬁ xture” to mean more than what the Java/Pearl/Ruby/etc. camp means. I landed on Suite Fixture Setup by focusing on the scope of the Shared Fixture; it is shared across the test suite for one Testcase Class that spawns a single Test Suite Object (page 387). The ﬁ xture that is built for the Test Suite Object could be called a “SuiteFixture.”\n\nFurther Reading\n\nSee http://www.vbunit.com/doc/Advanced.htm for more information on Suite Fixture Setup as implemented in VbUnit. See http://nunit.org for more informa- tion on Suite Fixture Setup as implemented in NUnit.\n\nwww.it-ebooks.info",
      "content_length": 1729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 510,
      "content": "Setup Decorator\n\nSetup Decorator\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nWe wrap the test suite with a Decorator that sets up the shared test ﬁ xture before running the tests and tears it down after all tests are done.\n\nsetUp setUp\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nFixture Fixture\n\nFixture Fixture Setup Setup Decorator Decorator\n\nTestSuite TestSuite Object Object\n\nInline Setup Inline Setup Exercise Exercise Verify Verify Inline Teardown Inline Teardown\n\nImplicit tearDown Implicit tearDown\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nSUT SUT\n\nInline Setup Inline Setup Exercise Exercise Verify Verify Inline Teardown Inline Teardown\n\nImplicit tearDown Implicit tearDown\n\ntearDown tearDown\n\nIf we have chosen to use a Shared Fixture (page 317), whether for reasons of convenience or out of necessity, and we have chosen not to use a Prebuilt Fix- ture (page 429), we will need to ensure that the ﬁ xture is built before each test run. Lazy Setup (page 435) is one strategy we could employ to create the test ﬁ xture “just in time” for the ﬁ rst test. But if it is critical to tear down the ﬁ xture after the last test, how do we know that all tests have been completed?\n\nHow It Works\n\nA Setup Decorator works by “bracketing” the execution of the entire test suite with a set of matching setUp and tearDown “bookends.” The pattern Decorator [GOF] is just what we need to make this happen. We construct a Setup Decora- tor that holds a reference to the Test Suite Object (page 387) we wish to decorate and then pass our Decorator to the Test Runner (page 377). When it is time to\n\nwww.it-ebooks.info\n\n447\n\nSetup Decorator",
      "content_length": 1725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "448\n\nSetup Decorator\n\nChapter 20 Fixture Setup Patterns\n\nrun the test, the Test Runner calls the run method on our Setup Decorator rather than the run method on the actual Test Suite Object. The Setup Decorator performs the ﬁ xture setup before calling the run method on the Test Suite Object and tears down the ﬁ xture after it returns.\n\nWhen to Use It\n\nWe can use a Setup Decorator when it is critical that a Shared Fixture be set up before every test run and that the ﬁ xture is torn down after the run is complete. This behavior may be critical because tests are using Hard-Coded Values (see Literal Value on page 714) that would cause the tests to fail if they are run again without cleaning up after each run (Unrepeatable Tests; see Erratic Test on page 228). Alternatively, this behavior may be necessary to avoid the incremental consumption of some limited resource, such as our database slowly ﬁ lling up with data from repeated test runs.\n\nWe might also use a Setup Decorator when the tests need to change some global parameter before exercising the SUT and then need to change this parameter back when they are ﬁ nished. Replacing the database with a Fake Database (see Fake Object on page 551) in an effort to avoid Slow Tests (page 253) is one common reason for taking this approach; setting global switches to a particular conﬁ gura- tion is another. Setup Decorators are installed at runtime, so nothing stops us from using several different decorators on the same test suite at different times (or even the same time).\n\nAs an alternative to a Setup Decorator, we can use Suite Fixture Setup (page 441) if we only want to share the ﬁ xture across the tests in a single Testcase Class (page 373) and our member of the xUnit family supports this behavior. If it is not essential that the ﬁ xture be torn down after every test run, we could use Lazy Setup instead.\n\nImplementation Notes\n\nA Setup Decorator consists of an object that sets up the ﬁ xture, delegates test execution to the test suite to be run, and then executes the code to tear down the ﬁ xture. To better line up with the normal xUnit calling conventions, we typically put the code that constructs the test ﬁ xture into a method called setUp and the code that tears down the ﬁ xture into a method called tearDown. Then our Setup Decorator’s run logic consists of three lines of code:\n\nwww.it-ebooks.info",
      "content_length": 2382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": "Setup Decorator\n\nvoid run() { setup(); decoratedSuite.run(); teardown(); }\n\nThere are several ways to build the Setup Decorator.\n\nVariation: Abstract Setup Decorator\n\nMany members of the xUnit family of Test Automation Frameworks (page 298) provide a reusable superclass that implements a Setup Decorator. This class usually implements the setUp/run/tearDown sequence as a Template Method [GOF]. All we have to do is to subclass this class and implement the setUp and tearDown methods as we would in a normal Testcase Class. When instantiating our Setup Decorator class, we pass the Test Suite Object we are decorating as the construc- tor argument.\n\nVariation: Hard-Coded Setup Decorator\n\nIf we need to build our Setup Decorator from scratch, the “simplest thing that could possibly work” is to hard-code the name of the decorated class in the suite method of the Setup Decorator. This allows the Setup Decorator class to act as the Test Suite Factory (see Test Enumeration on page 399) for the decorated suite.\n\nVariation: Parameterized Setup Decorator\n\nIf we want to reuse the Setup Decorator for different test suites, we can param- eterize its constructor method with the Test Suite Object to be run. This means that the setup and teardown logic can be coded within the Setup Decorator, thereby eliminating the need for a separate Test Helper (page 643) class just to reuse the setup logic across tests.\n\nVariation: Decorated Lazy Setup\n\nOne of the main drawbacks of using a Setup Decorator is that tests cannot be run by themselves because they depend on the Setup Decorator to set up the ﬁ xture. We can work around this requirement by augmenting the Setup Decorator with Lazy Setup in the setUp method so that an undecorated Testcase Object (page 382) can construct its own ﬁ xture. The Testcase Object can also remember that it built its own ﬁ xture and destroy it in the tearDown method. This functionality could be implemented on a generic Testcase Superclass (page 638) so that it has to be built and tested just once.\n\nwww.it-ebooks.info\n\n449\n\nSetup Decorator",
      "content_length": 2072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "450\n\nSetup Decorator\n\nChapter 20 Fixture Setup Patterns\n\nThe only other alternative is to use a Pushdown Decorator. That would negate any test speedup the Shared Fixture bought us, however, so this approach can be used only in those cases when we use the Setup Decorator for reasons other than setting up a Shared Fixture.\n\nVariation: Pushdown Decorator\n\nOne of the main drawbacks of using a Setup Decorator is that tests cannot be run by themselves because they depend on the Setup Decorator to set up the ﬁ xture. One way we can circumvent this obstacle is to provide a means to push the decorator down to the level of the individual tests rather than the whole test suite. This step requires a few modiﬁ cations to the TestSuite class to allow the Setup Decorator to be passed down to where the individual Testcase Objects are constructed during the Test Discovery (page 393) process. As each object is created from the Test Method (page 348), it is wrapped in the Setup Decorator before it is added to the Test Suite Object’s collection of tests.\n\nOf course, this negates one of the major sources of the speed advantage created by using a Setup Decorator by forcing a new test ﬁ xture to be built for each test. See the sidebar “Faster Tests Without Shared Fixtures” on page 319 for other ways to address the test execution speed issue.\n\nMotivating Example\n\nIn this example, we have a set of tests that use Lazy Setup to build the Shared Fixture and Finder Methods (see Test Utility Method on page 599) to ﬁ nd the objects in the ﬁ xture. We have discovered that the leftover ﬁ xture is causing Unrepeatable Tests, so we want to clean up properly after the last test has ﬁ n- ished running.\n\nprotected void setUp() throws Exception { if (sharedFixtureInitialized) { return; } facade = new FlightMgmtFacadeImpl(); setupStandardAirportsAndFlights(); sharedFixtureInitialized = true; }\n\nprotected void tearDown() throws Exception { // Cannot delete any objects because we don't know // whether this is the last test }\n\nwww.it-ebooks.info",
      "content_length": 2038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 514,
      "content": "Setup Decorator\n\nBecause there is no easy way to accomplish this goal with Lazy Setup, we must change our ﬁ xture setup strategy. One option is to use a Setup Decorator instead.\n\nRefactoring Notes\n\nWhen creating a Setup Decorator, we can reuse the exact same ﬁ xture setup logic; we just need to call it at a different time. Thus this refactoring consists mostly of moving the call to the ﬁ xture setup logic from the setUp method on the Testcase Class to the setUp method of a Setup Decorator class. Assuming we have an Abstract Setup Decorator available to subclass, we can create our new sub- class and provide concrete implementations of the setUp and tearDown methods.\n\nIf our instance of xUnit does not support Setup Decorator directly, we can create our own Setup Decorator superclass by building a single-purpose Setup Decorator and then introducing a constructor parameter and instance variable to hold the test suite to be run. Finally, we do an Extract Superclass [Fowler] refactoring to create our reusable superclass.\n\nExample: Hard-Coded Setup Decorator\n\nIn this example, we have moved all of the setup logic to the setUp method of a Setup Decorator that inherits its basic functionality from an Abstract Setup Decorator. We have also written some ﬁ xture teardown logic in the tearDown method so that we clean up the ﬁ xture after the entire test suite has been run.\n\npublic class FlightManagementTestSetup extends TestSetup { private FlightManagementTestHelper helper;\n\npublic FlightManagementTestSetup() { // Construct the Test Suite Object to be decorated and // pass it to our Abstract Setup Decorator superclass super( SafeFlightManagementFacadeTest.suite() ); helper = new FlightManagementTestHelper(); }\n\npublic void setUp() throws Exception { helper.setupStandardAirportsAndFlights(); }\n\npublic void tearDown() throws Exception { helper.removeStandardAirportsAndFlights(); }\n\nwww.it-ebooks.info\n\n451\n\nSetup Decorator",
      "content_length": 1939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 515,
      "content": "452\n\nSetup Decorator\n\nChapter 20 Fixture Setup Patterns\n\npublic static Test suite() { // Return an instance of this decorator class return new FlightManagementTestSetup(); } }\n\nBecause this is a Hard-Coded Setup Decorator, the call to the Test Suite Factory that builds the actual Test Suite Object is hard-coded inside the constructor. The suite method just calls the constructor.\n\nExample: Parameterized Setup Decorator\n\nTo make our Setup Decorator reusable with several different test suites, we need to do an Introduce Parameter [JBrains] refactoring on the name of the Test Suite Factory inside the constructor:\n\npublic class ParameterizedFlightManagementTestSetup extends TestSetup {\n\nprivate FlightManagementTestHelper helper = new FlightManagementTestHelper();\n\npublic ParameterizedFlightManagementTestSetup( Test testSuiteToDecorate) { super(testSuiteToDecorate); }\n\npublic void setUp() throws Exception { helper.setupStandardAirportsAndFlights(); }\n\npublic void tearDown() throws Exception { helper.removeStandardAirportsAndFlights(); } }\n\nTo make it easy for the Test Runner to create our test suite, we also need to cre- ate a Test Suite Factory that calls the Setup Decorator’s constructor with the Test Suite Object to be decorated:\n\npublic class DecoratedFlightManagementFacadeTestFactory { public static Test suite() { // Return a new Test Suite Object suitably decorated return new ParameterizedFlightManagementTestSetup( SafeFlightManagementFacadeTest.suite()); } }\n\nwww.it-ebooks.info",
      "content_length": 1503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 516,
      "content": "Setup Decorator\n\nWe will need one of these Test Suite Factories for each test suite we want to be able to run by itself. Even so, this is a small price to pay for reusing the actual Setup Decorator.\n\nExample: Abstract Decorator Class\n\nHere’s what the Abstract Decorator Class looks like:\n\npublic class TestSetup extends TestCase { Test decoratedSuite;\n\nAbstractSetupDecorator(Test testSuiteToDecorate) { decoratedSuite = testSuiteToDecorate; }\n\npublic void setUp() throws Exception { // subclass responsibility } public void tearDown() throws Exception { // subclass responsibility }\n\nvoid run() { setup(); decoratedSuite.run(); teardown(); } }\n\nwww.it-ebooks.info\n\n453\n\nSetup Decorator",
      "content_length": 686,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 517,
      "content": "454\n\nChained Tests\n\nChapter 20 Fixture Setup Patterns\n\nChained Tests\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nWe let the other tests in a test suite set up the test ﬁ xture.\n\nFixture Fixture Fixture Fixture\n\nExercise Exercise\n\nVerify Verify\n\nTest Runner Test Runner\n\nExercise Exercise\n\nVerify Verify\n\nSUT SUT\n\nExercise Exercise\n\nVerify Verify\n\nShared Fixtures (page 317) are commonly used to reduce the amount of per- test overhead required to set up the ﬁ xture. Sharing a ﬁ xture involves extra test programming effort because we need to create the ﬁ xture and have a way of discovering the ﬁ xture in each test. Regardless of how the ﬁ xture is accessed, it must be initialized (constructed) before it is used.\n\nChained Tests offer a way to reuse the test ﬁ xture left over from one test and\n\nthe Shared Fixture of a subsequent test.\n\nHow It Works\n\nChained Tests take advantage of the objects created by the tests that run before our current test in the test suite. This approach is very similar to how a human tester tests a large number of test conditions in a single test—by building up a complex test ﬁ xture through a series of actions, with the outcome of each action ﬁ rst being veriﬁ ed. We can achieve a similar result with automated tests by building a set of Self-Checking Tests (see page 26) that do not perform any ﬁ xture setup but instead\n\nwww.it-ebooks.info",
      "content_length": 1428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 518,
      "content": "Chained Tests\n\nrely on the “leftovers” of the test(s) that run before them. Unlike with the Reuse Test for Fixture Setup pattern (see Creation Method on page 415), we don’t actu- ally call another Test Method (page 348) from within out test; we just assume that it has been run and has left behind something we can use as a test ﬁ xture.\n\nWhen to Use It\n\nChained Tests is a ﬁ xture strategy that people either love or hate. Those who hate it do so because this approach is simply the test smell Interacting Tests (see Erratic Test on page 228) recast as a pattern. Those who love it typically do so because it solves a nasty problem introduced by using Shared Fixtures to deal with Slow Tests (page 253). Either way, it is a valid strategy for refactoring exist- ing tests that are overly long and contain many steps that build on one another. Such tests will stop executing when the ﬁ rst assertion fails. We can refactor such tests into a set of Chained Tests fairly quickly because this strategy doesn’t require determining exactly which test ﬁ xture we need to build for each test. This may be the ﬁ rst step in evolving the tests into a set of Independent Tests (see page 42).\n\nChained Tests help prevent Fragile Tests (page 239) because they are a crude form of SUT API Encapsulation (see Test Utility Method on page 599). Our test doesn’t need to interact with the SUT to set up the ﬁ xture because we let another test that was already using the same API set up the ﬁ xture for us. Fragile Fixtures (see Fragile Test) may be a problem, however; if one of the preceding tests is modiﬁ ed to create a different ﬁ xture, the depending test will probably fail. This is also true if some of the earlier tests fail or have errors; they may leave the Shared Fixture in a different state from what the current test expects.\n\nOne of the key problems with Chained Tests is the nondeterminism of the order in which xUnit executes tests in a test suite. Most members of the family make no guarantees about this order (TestNG is an exception). Thus tests could start to fail when a new version of xUnit is installed or even when one of the Test Methods is renamed [if the xUnit implementation happens to sort the Test- case Objects (page 382) by method name].\n\nAnother problem is that Chained Tests are Lonely Tests (see Erratic Test) because the current test depends on the tests that precede it to set up the test ﬁ xture. If we run the test by itself, it will likely fail because the test ﬁ xture it assumes is not set up for it. As a consequence, we cannot run just the one test when we are debugging failures it exposes.\n\nDepending on other tests to set up the test ﬁ xture invariably results in tests that are more difﬁ cult to understand because the test ﬁ xture is invisible to the\n\nwww.it-ebooks.info\n\n455\n\nChained Tests",
      "content_length": 2823,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 519,
      "content": "456\n\nChained Tests\n\nChapter 20 Fixture Setup Patterns\n\ntest reader—a classic case of a Mystery Guest (see Obscure Test on page 186). This problem can be at least partially mitigated through the use of appropri- ately named Finder Methods (see Test Utility Method) to access the objects in the Shared Fixture. It is less of an issue if all the Test Methods are on the same Testcase Class (page 373) and are listed in the same order as they are executed.\n\nVariation: Fixture Setup Testcase\n\nIf we need to set up a Shared Fixture and we cannot use any of the other tech- niques to set it up [e.g., Lazy Setup (page 435), Suite Fixture Setup (page 441), or Setup Decorator (page 447)], we can arrange to have a Fixture Setup Testcase run as the ﬁ rst test in the test suite. This is simple to do if we are using Test Enu- meration (page 399); we just include the appropriate addTest method call in our Test Suite Factory (see Test Enumeration). This variation is a degenerate form of the Chained Tests pattern in that we are chaining a test suite behind a single Fixture Setup Testcase.\n\nImplementation Notes\n\nThere are two key challenges in implementing Chained Tests:\n\nGetting tests in the test suite to run in the desired order\n\nAccessing the ﬁ xture leftover by the previous test(s)\n\nWhile a few members of the xUnit family provide an explicit mechanism for deﬁ ning the order of tests, most members make no such guarantees about this order. We can probably ﬁ gure out what order the xUnit member uses by per- forming a few experiments. Most commonly, we will discover that it is either the order in which the Test Methods appear in the ﬁ le or alphabetical order by Test Method name (in which case, the easiest solution is to include a test sequence number in the test name). In the worst-case scenario, we could always revert to Test Method Enumeration (see Test Enumeration) to ensure that Testcase Objects are added to the test suite in the correct order.\n\nTo refer to the objects created by the previous tests, we need to use one of the ﬁ xture object access patterns. If the preceding tests are Test Methods on the same Testcase Class, it is sufﬁ cient for each test to store any object references that subse- quent tests will use to access the ﬁ xture in a ﬁ xture holding class variable. (Fixture holding instance variables typically won’t work here because each test runs on a separate Testcase Object and, therefore, the tests don’t share instance variables. See the sidebar “There’s Always an Exception” on page 384 for a description of when instance variations do not behave this way.)\n\nwww.it-ebooks.info",
      "content_length": 2617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 520,
      "content": "Chained Tests\n\nIf our test depends on a Test Method on a different Testcase Class being run as a part of a Suite of Suites (see Test Suite Object on page 387), neither of these solutions will work. Our best bet will be to use a Test Fixture Registry (see Test Helper on page 643) as the means to store references to the objects used by the tests. A test database is a good example.\n\nObviously, we don’t want the test we are depending on to clean up after itself—that would leave nothing for us to reuse as our test ﬁ xture. That re- quirement makes Chained Tests incompatible with the Fresh Fixture (page 311) approach.\n\nMotivating Example\n\nHere’s an example of an incremental Tabular Test (see Parameterized Test on page 607) provided by Clint Shank on his blog:\n\npublic class TabularTest extends TestCase { private Order order = new Order(); private static ﬁnal double tolerance = 0.001;\n\npublic void testGetTotal() { assertEquals(\"initial\", 0.00, order.getTotal(), tolerance); testAddItemAndGetTotal(\"ﬁrst\", 1, 3.00, 3.00); testAddItemAndGetTotal(\"second\",3, 5.00, 18.00); // etc. }\n\nprivate void testAddItemAndGetTotal( String msg, int lineItemQuantity, double lineItemPrice, double expectedTotal) { // setup LineItem item = new LineItem( lineItemQuantity, lineItemPrice); // exercise SUT order.addItem(item); // verify total assertEquals(msg,expectedTotal,order.getTotal(),tolerance); } }\n\nThis test begins by building an empty order, veriﬁ es the total is zero, and then proceeds to add several items verifying the total after each item (Figure 20.3). The main issue with this test is that if one of the subtests fails, all subsequent subtests don’t get run. For example, suppose a rounding error makes the total after the second item incorrect: Wouldn’t we like to see whether the fourth, ﬁ fth, and six items are still correct?\n\nwww.it-ebooks.info\n\n457\n\nChained Tests",
      "content_length": 1875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 521,
      "content": "458\n\nChained Tests\n\nChapter 20 Fixture Setup Patterns\n\nFigure 20.3 Tabular Test results. The lower pane shows the details of the ﬁ rst failure inside the single Tabular Test method listed in the upper pane. Because of the failure, the rest of the test method is not executed.\n\nRefactoring Notes\n\nWe can convert this Tabular Test to a set of Chained Tests simply by breaking up the single Test Method into one Test Method per subtest. One way to do so is to use a series of Extract Method [Fowler] refactorings to create the Test Methods. This will force us to use an Introduce Field [JetBrains] refactoring for any local variables before the ﬁ rst Extract Method refactoring operation. Once we have de- ﬁ ned all of the new Test Methods, we simply delete the original Test Method and let the Test Automation Framework (page 298) call our new methods directly.2\n\nWe need to ensure the tests run in the same order. Because JUnit seems to sort the Testcase Objects by method name, we can force them into the right order by including a sequence number in the Test Method name.\n\nFinally, we need to convert our Fresh Fixture into a Shared Fixture. We do so by changing our order ﬁ eld (instance variable) into a class variable (a static variable in Java) so that all of the Testcase Objects use the same Order.\n\n2 If we don’t have a refactoring tool handy, no worries. Just end the Test Method after each subtest and type in the signature of the next Test Method before the next subtest. We then move any Shared Fixture variables out of the ﬁ rst Test Method.\n\nwww.it-ebooks.info",
      "content_length": 1574,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 522,
      "content": "Chained Tests\n\nExample: Chained Tests\n\nHere’s the simple example turned into three separate tests:\n\nprivate static Order order = new Order(); private static ﬁnal double tolerance = 0.001;\n\npublic void test_01_initialTotalShouldBeZero() { assertEquals(\"initial\", 0.00, order.getTotal(), tolerance); }\n\npublic void test_02_totalAfter1stItemShouldBeOnlyItemAmount(){ testAddItemAndGetTotal( \"ﬁrst\", 1, 3.00, 3.00); }\n\npublic void test_03_totalAfter2ndItemShouldBeSumOfAmounts() { testAddItemAndGetTotal( \"second\",3, 5.00, 18.00); }\n\nprivate void testAddItemAndGetTotal( String msg, int lineItemQuantity, double lineItemPrice, double expectedTotal) { // create a line item LineItem item = new LineItem(lineItemQuantity, lineItemPrice); // add line item to order order.addItem(item); // verify total assertEquals(msg,expectedTotal,order.getTotal(),tolerance); }\n\nThe Test Runner (page 377) gives us a better overview of what is wrong and what is working (Figure 20.4).\n\nUnfortunately, we will not be able to run any of the tests by themselves while we debug this problem (except for the very ﬁ rst test) because of the interdepen- dencies between the tests; they are Lonely Tests.\n\nwww.it-ebooks.info\n\n459\n\nChained Tests",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 523,
      "content": "460\n\nChained Tests\n\nChapter 20 Fixture Setup Patterns\n\nFigure 20.4 Chained Tests result. The upper pane shows the three test methods with two tests passing. The lower pane shows the details of the one failing Test Method.\n\nwww.it-ebooks.info",
      "content_length": 241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 524,
      "content": "Chapter 21\n\nResult Veriﬁ cation Patterns\n\nPatterns in This Chapter\n\nVeriﬁ cation Strategy\n\nState Veriﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462\n\nBehavior Veriﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468\n\nAssertion Method Styles\n\nCustom Assertion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474\n\nDelta Assertion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485\n\nGuard Assertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490\n\nUnﬁ nished Test Assertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494\n\n461\n\nwww.it-ebooks.info\n\nResult Veriﬁ cation Patterns",
      "content_length": 733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 525,
      "content": "462\n\nAlso known as: State-Based Testing\n\nState Veriﬁ cation\n\nChapter 21 Result Verification Patterns\n\nState Veriﬁ cation\n\nHow do we make tests self-checking when there is state to be veriﬁ ed?\n\nWe inspect the state of the system under test after it has been exercised and compare it to the expected state.\n\nFixture Fixture\n\nSetup Setup\n\nDOC DOC\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nGet State Get State\n\nSUT SUT\n\nB B\n\nBehavior Behavior (Indirect (Indirect Outputs) Outputs)\n\nA A\n\nC C\n\nTeardown Teardown\n\nA Self-Checking Test (see page 26) must verify that the expected outcome has occurred without manual intervention by whoever is running the test. But what do we mean by “expected outcome”? The SUT may or may not be “stateful”; if it is stateful, it may or may not have a different state after it has been exercised. As test automaters, it is our job to determine whether our expected outcome is a change of ﬁ nal state or whether we need to be more speciﬁ c about what occurs while the SUT is being exercised.\n\nState Veriﬁ cation involves inspecting the state of the SUT after it has been\n\nexercised.\n\nHow It Works\n\nWe exercise the SUT by invoking the methods of interest. Then, as a separate step, we interact with the SUT to retrieve its post-exercise state and compare it with the expected end state by calling Assertion Methods (page 362).\n\nwww.it-ebooks.info",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 526,
      "content": "State Verification\n\nNormally, we can access the state of the SUT simply by calling methods or functions that return its state. This is especially true when we are doing test-driven development because the tests will have ensured that the state is easily accessible. When we are retroﬁ tting tests, however, we may ﬁ nd it more challenging to access the relevant state information. In these cases, we may need to use a Test-Speciﬁ c Subclass (page 579) or some other technique to expose the state without introduc- ing Test Logic in Production (page 217).\n\nA related question is “Where is the state of the SUT stored?” Sometimes, the state is stored within the actual SUT; in other cases, the state may be stored in another component such as a database. In the latter case, State Veriﬁ cation may involve accessing the state within the other component (essentially a layer-crossing test). By contrast, Behavior Veriﬁ cation (page 468) would involve verifying the interactions between the SUT and the other component.\n\nWhen to Use It\n\nWe should use State Veriﬁ cation when we care about only the end state of the SUT—not how the SUT got there. Taking such a limited view helps us maintain encapsulation of the implementation of the SUT.\n\nState Veriﬁ cation comes naturally when we are building the software inside out. That is, we build the innermost objects ﬁ rst and then build the next layer of objects on top of them. Of course, we may need to use Test Stubs (page 529) to control the indirect inputs of the SUT to avoid Production Bugs (page 268) caused by untested code paths. Even then, we are choosing not to verify the indirect outputs of the SUT.\n\nWhen we do care about the side effects of exercising the SUT that are not visible in its end state (its indirect outputs), we can use Behavior Veriﬁ cation to observe the behavior directly. We must be careful, however, not to create Fragile Tests (page 239) by overspecifying the software.\n\nImplementation Notes\n\nThere are two basic styles of implementing State Veriﬁ cation.\n\nVariation: Procedural State Veriﬁ cation\n\nWhen doing Procedural State Veriﬁ cation, we simply write a series of calls to Assertion Methods that pick apart the state information into pieces and com- pare those bits of information to individual expected values. Most people who are new to automating tests take such a “path of least resistance.” The major disadvantage of this approach is that it can result in Obscure Tests (page 186)\n\nwww.it-ebooks.info\n\n463\n\nState Veriﬁ cation",
      "content_length": 2511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 527,
      "content": "464\n\nAlso known as: Expected Object\n\nState Veriﬁ cation\n\nChapter 21 Result Verification Patterns\n\nowing to the number of assertions it may take to specify the expected outcome. When the same sequence of assertions must be carried out in many tests or many times within a single Test Method (page 348), we also have Test Code Duplica- tion (page 213).\n\nVariation: Expected State Speciﬁ cation\n\nWhen doing Expected State Speciﬁ cation, we construct a speciﬁ cation for the post-exercise state of the SUT in the form of one or more objects populated with the expected attributes. We then compare the actual state directly with these objects using a single call to an Equality Assertion (see Assertion Method). This tends to result in more concise and readable tests. We can use an Expected State Speciﬁ cation whenever we need to verify several attributes and it is possible to construct an object that looks like the object we expect the SUT to return. The more attributes we have that need to be compared and the more tests that need to compare them, the more compelling the argument for using an Expected State Speciﬁ cation. In the most extreme cases, when we have a lot of data to verify, we can construct an “expected table” and verify that the SUT contains it. Fit’s “row ﬁ xtures” offer a good way to do this in customer tests; tools such as DbUnit are a good way to use Back Door Manipulation (page 327) for this purpose.\n\nWhen constructing the Expected State Speciﬁ cation, we may prefer to use a Parameterized Creation Method (see Creation Method on page 415) so that the reader is not distracted by all the necessary but unimportant attributes of the Expected State Speciﬁ cation. The Expected State Speciﬁ cation is most often an instance of the same class that we expect to get back from the SUT. We may have difﬁ culty using an Expected State Speciﬁ cation if the object doesn’t imple- ment equality in a way that involves comparing the values of attributes (e.g., by comparing the object references with each other) or if our test-speciﬁ c deﬁ nition of equality differs from that implemented by the equals method.\n\nIn these cases, we can still use an Expected State Speciﬁ cation if we create a Custom Assertion (page 474) that implements test-speciﬁ c equality. Alterna- tively, we can build the Expected State Speciﬁ cation from a class that imple- ments our test-speciﬁ c equality. This class can either be a Test-Speciﬁ c Subclass that overrides the equals method or a simple Data Transfer Object [CJ2EEP] that implements equals(TheRealObjectClass other). Both of these measures are preferable to modifying (or introducing) the equals method on the production class, as that would be a form of Equality Pollution (see Test Logic in Production). When the class is difﬁ cult to instantiate, we can deﬁ ne a Fake Object (page 551) that has the necessary attributes plus an equals method that implements test-speciﬁ c equality. These last few “tricks” are made possible by the fact that Equality\n\nwww.it-ebooks.info",
      "content_length": 3029,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 528,
      "content": "State Verification\n\nAssertions usually ask the Expected State Speciﬁ cation to compare itself to the actual result, rather than the reverse.\n\nWe can build the Expected State Speciﬁ cation either during the result veriﬁ - cation phase of the test immediately before it is used in the Equality Assertion or during the ﬁ xture setup phase of the test. The latter strategy allows us to use attributes of the Expected State Speciﬁ cation as parameters passed to the SUT or as the base for Derived Values (page 718) when building other objects in the test ﬁ xture. This makes it easier to see the cause–effect relationship between the ﬁ xture and the Expected State Speciﬁ cation, which in turn helps us achieve Tests as Documentation (see page 23). It is particularly useful when the Ex- pected State Speciﬁ cation is created out of sight of the test reader such as when using Creation Methods to do the construction.\n\nMotivating Example\n\nThis simple1 example features a test that exercises the code that adds a line item to an invoice. Because it contains no assertions, it is not a Self-Checking Test.\n\npublic void testInvoice_addOneLineItem_quantity1() { // Exercise inv.addItemQuantity(product, QUANTITY); }\n\nWe have chosen to create the invoice and product in the setUp method, an approach called Implicit Setup (page 424).\n\npublic void setUp() { product = createAnonProduct(); anotherProduct = createAnonProduct(); inv = createAnonInvoice(); }\n\nRefactoring Notes\n\nThe ﬁ rst refactoring we can do is not really a refactoring at all, because we are changing the behavior of the tests (for the better): We introduce some assertions that specify the expected outcome. This results in an example of Procedural State Veriﬁ cation because we make this change within the Test Method as a series of calls to built-in Assertion Methods.\n\n1 The natural example for this pattern is not very good at illustrating the difference between State Veriﬁ cation and Behavior Veriﬁ cation. For this purpose, refer to Behavior Veriﬁ cation, which provides a second example of State Veriﬁ cation that is more directly comparable.\n\nwww.it-ebooks.info\n\n465\n\nState Veriﬁ cation",
      "content_length": 2152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 529,
      "content": "466\n\nState Veriﬁ cation\n\nChapter 21 Result Verification Patterns\n\nWe can further simplify the Test Method by refactoring it to use an Expected Object. First, we build an Expected Object by constructing an object of the expected class, or a suitable Test Double (page 522), and initializing it with the values that were previously speciﬁ ed in the assertions. Then we replace the series of assertions with a single Equality Assertion that compares the actual result with an Expected Object. We may have to use a Custom Assertion if we need test-speciﬁ c equality.\n\nExample: Procedural State Veriﬁ cation\n\nHere we have added the assertions to the Test Method to turn it into a Self- Checking Test. Because several steps must be carried out to verify the expected outcome, this test suffers from a mild case of Obscure Test.\n\npublic void testInvoice_addOneLineItem_quantity1() { // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); // Verify only item LineItem actual = (LineItem) lineItems.get(0); assertEquals(inv, actual.getInv()); assertEquals(product, actual.getProd()); assertEquals(QUANTITY, actual.getQuantity()); }\n\nExample: Expected Object\n\nIn this simpliﬁ ed version of the test, we use the Expected Object with a single Equality Assertion instead of a series of assertions on individual attributes:\n\npublic void testInvoice_addLineItem1() { LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity( expItem.getProd(), expItem.getQuantity()); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem) lineItems.get(0); assertEquals(\"Item\", expItem, actual); }\n\nwww.it-ebooks.info",
      "content_length": 1791,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 530,
      "content": "State Verification\n\nBecause we are also using some of the attributes as arguments of the SUT, we have chosen to build the Expected Object during the ﬁ xture setup phase of the test and to use the attributes of the Expected Object as the SUT arguments.\n\nwww.it-ebooks.info\n\n467\n\nState Veriﬁ cation",
      "content_length": 296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 531,
      "content": "468\n\nAlso known as: Interaction Testing\n\nBehavior Veriﬁ cation\n\nChapter 21 Result Verification Patterns\n\nBehavior Veriﬁ cation\n\nHow do we make tests self-checking when there is no state to verify?\n\nWe capture the indirect outputs of the SUT as they occur and compare them to the expected behavior.\n\nFixture Fixture\n\nSetup Setup\n\nDOC DOC\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nSUT SUT\n\nB B\n\nBehavior Behavior (Indirect (Indirect Outputs) Outputs)\n\nA A\n\nC C\n\nTeardown Teardown\n\nA Self-Checking Test (see page 26) must verify that the expected outcome has occurred without manual intervention by whoever is running the test. But what do we mean by “expected outcome”? The SUT may or may not be “stateful”; if it is stateful, it may or may not be expected to end up in a different state after it has been exercised. The SUT may also be expected to invoke methods on other objects or components.\n\nBehavior Veriﬁ cation involves verifying the indirect outputs of the SUT as it\n\nis being exercised.\n\nHow It Works\n\nEach test speciﬁ es not only how the client of the SUT interacts with it during the exercise SUT phase of the test, but also how the SUT interacts with the compo- nents on which it should depend. This ensures that the SUT really is behaving as speciﬁ ed rather than just ending up in the correct post-exercise state.\n\nwww.it-ebooks.info\n\ny y f f i i r r e e V V",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 532,
      "content": "Behavior Verification\n\nBehavior Veriﬁ cation almost always involves interacting with or replacing a depended-on component (DOC) with which the SUT interacts at runtime. The line between Behavior Veriﬁ cation and State Veriﬁ cation (page 462) can get a bit blurry when the SUT stores its state in the DOC because both forms of veriﬁ cation involve layer-crossing tests. We can distinguish between the two cases based on whether we are verifying the post-test state in the DOC (State Veriﬁ cation) or whether we are verifying the method calls made by the SUT on the DOC (Behavior Veriﬁ cation).\n\nWhen to Use It\n\nBehavior Veriﬁ cation is primarily a technique for unit tests and component tests. We can use Behavior Veriﬁ cation whenever the SUT calls methods on other objects or components. We must use Behavior Veriﬁ cation whenever the expected outputs of the SUT are transient and cannot be determined simply by looking at the post-exercise state of the SUT or the DOC. This forces us to monitor these indirect outputs as they occur.\n\nA common application of Behavior Veriﬁ cation is when we are writing our code in an “outside-in” manner. This approach, which is often called need-driven development, involves writing the client code before we write the DOC. It is a good way to ﬁ nd out exactly what the interface provided by the DOC needs to be based on real, concrete examples rather than on speculation. The main objection to this approach is that we need to use a lot of Test Doubles (page 522) to write these tests. That could result in Fragile Tests (page 239) because each test knows so much about how the SUT is implemented. Because the tests specify the behavior of the SUT in terms of its interactions with the DOC, a change in the implementation of the SUT could break a lot of tests. This kind of Overspeciﬁ ed Software (see Fragile Test) could lead to High Test Maintenance Cost (page 265).\n\nThe jury is still out on whether Behavior Veriﬁ cation is a better approach than State Veriﬁ cation. In most cases, State Veriﬁ cation is clearly necessary; in some cases, Behavior Veriﬁ cation is clearly necessary. What has yet to be deter- mined is whether Behavior Veriﬁ cation should be used in all cases or whether we should use State Veriﬁ cation most of the time and resort to Behavior Veriﬁ - cation only when State Veriﬁ cation falls short of full test coverage.\n\nImplementation Notes\n\nBefore we exercise the SUT by invoking the methods of interest, we must ensure that we have a way of observing its behavior. Sometimes the mechanisms that the\n\nwww.it-ebooks.info\n\n469\n\nBehavior Veriﬁ cation",
      "content_length": 2609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 533,
      "content": "470\n\nBehavior Veriﬁ cation\n\nAlso known as: Expected Behavior\n\nChapter 21 Result Verification Patterns\n\nSUT uses to interact with the components surrounding it make such observation possible; when this is not the case, we must install some sort of Test Double to monitor the SUT’s indirect outputs. We can use a Test Double as long as we have a way to replace the DOC with the Test Double. This could be via Dependency Injection (page 678) or by Dependency Lookup (page 686).\n\nThere are two fundamentally different ways to implement Behavior Veriﬁ ca- tion, each with its own proponents. The Mock Object (page 544) community has been very vocal about the use of “mocks” as an Expected Behavior Speciﬁ cation, so it is the more commonly used approach. Nevertheless, Mock Objects are not the only way of doing Behavior Veriﬁ cation.\n\nVariation: Procedural Behavior Veriﬁ cation\n\nIn Procedural Behavior Veriﬁ cation, we capture the method calls made by the SUT as it executes and later get access to them from within the Test Method (page 348). Then we use Equality Assertions (see Assertion Method on page 362) to compare them with the expected results.\n\nThe most common way of trapping the indirect outputs of the SUT is to install a Test Spy (page 538) in place of the DOC during the ﬁ xture setup phase of the Four-Phase Test (page 358). During the result veriﬁ cation phase of the test, we ask the Test Spy how it was used by the SUT during the exercise SUT phase. Use of a Test Spy does not require any advance knowledge of how the methods of the DOC will be called.\n\nThe alternative is to ask the real DOC how it was used. Although this scheme is not always feasible, when it is, it avoids the need to use a Test Double and minimizes the degree to which we have Overspeciﬁ ed Software.\n\nWe can reduce the amount of code in the Test Method (and avoid Test Code Duplication; see page 213) by deﬁ ning Expected Objects (see State Veriﬁ cation) for the arguments of method calls or by delegating the veriﬁ cation of them to Custom Assertions (page 474).\n\nVariation: Expected Behavior Speciﬁ cation\n\nExpected Behavior Speciﬁ cation is a different way of doing Behavior Veriﬁ cation. Instead of waiting until after the fact to verify the indirect outputs of the SUT by using a sequence of assertions, we load the Expected Behavior Speciﬁ cation into a Mock Object and let it verify that the method calls are correct as they are received.\n\nWe can use an Expected Behavior Speciﬁ cation when we know exactly what should happen ahead of time and we want to remove all Procedural Behavior Veriﬁ cation from the Test Method. This pattern variation tends to make the\n\nwww.it-ebooks.info",
      "content_length": 2677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 534,
      "content": "Behavior Verification\n\ntest shorter (assuming we are using a compact representation of the expected behavior) and can be used to cause the test to fail on the ﬁ rst deviation from the expected behavior if we so choose.\n\nOne distinct advantage of using Mock Objects is that Test Double generation tools are available for many members of the xUnit family. They make imple- menting Expected Behavior Speciﬁ cation very easy because we don’t need to manually build a Test Double for each set of tests. One drawback of using a Mock Object is that it requires that we can predict how the methods of the DOC will be called and what arguments will be passed to it in the method calls.\n\nMotivating Example\n\nThe following test is not a Self-Checking Test because it does not verify that the expected outcome has actually occurred; it contains no calls to Assertion Methods, nor does it set up any expectations on a Mock Object. Because we are testing the logging functionality of the SUT, the state that interests us is actu- ally stored in the logger rather than within the SUT itself. The writer of this test hasn’t found a way to access the state we are trying to verify.\n\npublic void testRemoveFlightLogging_NSC() throws Exception { // setup FlightDto expectedFlightDto = createARegisteredFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify // have not found a way to verify the outcome yet // Log contains record of Flight removal }\n\nTo verify the outcome, whoever is running the tests must access the database and the log console and compare what was actually output to what should have been output.\n\nOne way to make the test Self-Checking is to enhance the test with Expected\n\nState Speciﬁ cation (see State Veriﬁ cation) of the SUT as follows:\n\npublic void testRemoveFlightLogging_ESS() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacadeImplTI facade = new FlightManagementFacadeImplTI(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify\n\nwww.it-ebooks.info\n\n471\n\nBehavior Veriﬁ cation",
      "content_length": 2178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 535,
      "content": "472\n\nBehavior Veriﬁ cation\n\nChapter 21 Result Verification Patterns\n\nassertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); }\n\nUnfortunately, this test does not verify the logging function of the SUT in any way. It also illustrates one reason why Behavior Veriﬁ cation came about: Some functionality of the SUT is not visible within the end state of the SUT itself, but can be seen only if we intercept the behavior at an internal observation point between the SUT and the DOC or if we express the behavior in terms of state changes for the objects with which the SUT interacts.\n\nRefactoring Notes\n\nWhen we made the changes in the second code sample in the “Motivating Example,” we weren’t really refactoring; instead, we added veriﬁ cation logic to make the tests behave differently. There are, however, several refactoring cases that are worth discussing.\n\nTo refactor from State Veriﬁ cation to Behavior Veriﬁ cation, we must do a Replace Dependency with Test Double (page 522) refactoring to gain visibility of the indirect outputs of the SUT via a Test Spy or Mock Object.\n\nTo refactor from an Expected Behavior Speciﬁ cation to Procedural Behavior Veriﬁ cation, we install a Test Spy instead of the Mock Object. After exercising the SUT, we make assertions on values returned by the Test Spy and compare them with the expected values that were originally used as arguments when we initially conﬁ gured the Mock Object (the one that we just converted into a Test Spy).\n\nTo refactor from Procedural Behavior Veriﬁ cation to an Expected Behavior Speciﬁ cation, we conﬁ gure a Mock Object with the expected values from the assertions made on values returned by the Test Spy and install the Mock Object instead of the Test Spy.\n\nExample: Procedural Behavior Veriﬁ cation\n\nThe following test veriﬁ es the basic functionality of creating a ﬂ ight but uses Procedural Behavior Veriﬁ cation to verify the indirect outputs of the SUT. That is, it uses a Test Spy to capture the indirect outputs and then veriﬁ es those out- puts are correct by making in-line calls to the Assertion Methods.\n\npublic void testRemoveFlightLogging_recordingTestStub() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl();\n\nwww.it-ebooks.info",
      "content_length": 2376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 536,
      "content": "Behavior Verification\n\n// Test Double setup AuditLogSpy logSpy = new AuditLogSpy(); facade.setAuditLog(logSpy); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertEquals(\"number of calls\", 1, logSpy.getNumberOfCalls()); assertEquals(\"action code\", Helper.REMOVE_FLIGHT_ACTION_CODE, logSpy.getActionCode()); assertEquals(\"date\", helper.getTodaysDateWithoutTime(), logSpy.getDate()); assertEquals(\"user\", Helper.TEST_USER_NAME, logSpy.getUser()); assertEquals(\"detail\", expectedFlightDto.getFlightNumber(), logSpy.getDetail()); }\n\nExample: Expected Behavior Speciﬁ cation\n\nIn this version of the test, we use the JMock framework to deﬁ ne the expected behavior of the SUT. The method expects on mockLog conﬁ gures the Mock Object with the Expected Behavior Speciﬁ cation (speciﬁ cally, the expected log message).\n\npublic void testRemoveFlight_JMock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // mock conﬁguration Mock mockLog = mock(AuditLog.class); mockLog.expects(once()).method(\"logMessage\") .with(eq(helper.getTodaysDateWithoutTime()), eq(Helper.TEST_USER_NAME), eq(Helper.REMOVE_FLIGHT_ACTION_CODE), eq(expectedFlightDto.getFlightNumber())); // mock installation facade.setAuditLog((AuditLog) mockLog.proxy()); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify // verify() method called automatically by JMock }\n\nwww.it-ebooks.info\n\n473\n\nBehavior Veriﬁ cation",
      "content_length": 1540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 537,
      "content": "474\n\nAlso known as: Bespoke Assertion\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\nCustom Assertion\n\nHow do we make tests self-checking when we have test-speciﬁ c equality logic? How do we reduce Test Code Duplication when the same assertion logic appears in many tests? How do we avoid Conditional Test Logic?\n\nWe create a purpose-built Assertion Method that compares only those attributes of the object that deﬁ ne test-speciﬁ c equality.\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nTeardown Teardown\n\nCustom Custom Assertion Assertion\n\nAssertion Assertion Method Method\n\nAssertion Assertion Method Method\n\nMost members of the xUnit family provide a reasonably rich set of Assertion Methods (page 362). But sooner or later, we inevitably ﬁ nd ourselves saying, “This test would be so much easier to write if I just had an assertion that did . . . .” So why not write it ourselves?\n\nThe reasons for writing a Custom Assertion are many, but the technique is pretty much the same regardless of our goal. We hide the complexity of whatever it takes to prove the system is behaving correctly behind an Assertion Method with an Intent-Revealing Name [SBPP].\n\nHow It Works\n\nWe encapsulate the mechanics of verifying that something is true (an assertion) behind an Intent-Revealing Name. To do so, we factor out all the common assertion code within the tests into a Custom Assertion that implements the\n\nwww.it-ebooks.info",
      "content_length": 1461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 538,
      "content": "CUSTOM ASSERTION\n\nveriﬁ cation logic. A Custom Equality Assertion takes two parameters: an Expected Object (see State Veriﬁ cation on page 462) and the actual object.\n\nA key characteristic of Custom Assertions is that they receive everything they need to pass or fail the test as parameters. Other than causing the test to fail, they have no side effects.\n\nTypically, we create Custom Assertions through refactoring by identifying common patterns of assertions in our tests. When test driving, we might just go ahead and call a nonexistent Custom Assertion because it makes writing our test easier; this tactic lets us focus on the part of the SUT that needs to be tested rather than the mechanics of how the test would be carried out.\n\nWhen to Use It\n\nWe should consider creating a Custom Assertion whenever any of the following statements are true:\n\nWe ﬁ nd ourselves writing (or cloning) the same assertion logic in test\n\nafter test (Test Code Duplication; see page 213).\n\nWe ﬁ nd ourselves writing Conditional Test Logic (page 200) in the result veriﬁ cation part of our tests. That is, our calls to Assertion Methods are embedded in if statements or loops.\n\nThe result veriﬁ cation parts of our tests suffer from Obscure Test (page 186) because we use procedural rather than declarative result veriﬁ cation in the tests.\n\nWe ﬁ nd ourselves doing Frequent Debugging (page 248) whenever\n\nassertions fail because they do not provide enough information.\n\nA key reason for moving the assertion logic out of the tests and into Custom Assertions is to Minimize Untestable Code (see page 44). Once the veriﬁ cation logic has been moved into a Custom Assertion, we can write Custom Assertion Tests (see Custom Assertion on page 474) to prove the veriﬁ cation logic is work- ing properly. Another important beneﬁ t of using Custom Assertions is that they help avoid Obscure Tests and make tests Communicate Intent (see page 41). That, in turn, will help produce robust, easily maintained tests.\n\nIf the veriﬁ cation logic must interact with the SUT to determine the actual outcome, we can use a Veriﬁ cation Method (see Custom Assertion) instead of a Custom Assertion. If the setup and exercise parts of the tests are also the same except for the values of the actual/expected objects, we should consider using a Parameterized Test (page 607). The primary advantage of Custom Assertions over both of these techniques is reusability; the same Custom Assertion can be\n\nwww.it-ebooks.info\n\n475\n\nCustom Assertion",
      "content_length": 2503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 539,
      "content": "476\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\nreused in many different circumstances because it is independent of its context (its only contact with the outside world occurs through its parameter list).\n\nWe most commonly write Custom Assertions that are Equality Assertions (see Assertion Method), but there is no reason why we cannot write other kinds as well.\n\nVariation: Custom Equality Assertion\n\nFor custom equality assertions, the Custom Assertion must be passed an Expected Object and the actual object to be veriﬁ ed. It should also take an Assertion Mes- sage (page 370) to avoid playing Assertion Roulette (page 224). Such an assertion is essentially an equals method implemented as a Foreign Method [Fowler].\n\nVariation: Object Attribute Equality Assertion\n\nWe often run across Custom Assertions that take one actual object and several different Expected Objects that need to be compared with speciﬁ c attributes of the actual object. (The set of attributes to be compared is implied by the name of the Custom Assertion.) The key difference between these Custom Assertions and a Veriﬁ cation Method is that the latter interacts with the SUT while the Object Attribute Equality Assertion looks only at the objects passed in as parameters.\n\nVariation: Domain Assertion\n\nAll of the built-in Assertion Methods are domain independent. Custom Equal- ity Assertions implement test-speciﬁ c equality but still compare only two objects. Another style of Custom Assertion helps contribute to the deﬁ nition of a “domain-speciﬁ c” Higher-Level Language (see page 41)—namely, the Domain Assertion.\n\nA Domain Assertion is a Stated Outcome Assertion (see Assertion Method) that states something that should be true in domain-speciﬁ c terms. It helps elevate the test into “business-speak.”\n\nVariation: Diagnostic Assertion\n\nSometimes we ﬁ nd ourselves doing Frequent Debugging whenever a test fails because the assertions tell us only that something is wrong but do not identify the speciﬁ c problem (e.g., the assertions indicate these two objects are not equal but it isn’t clear what isn’t equal about the object). We can write a special kind of Custom Assertion that may look just like one of the built-in assertions but pro- vide more information about what is different between the expected and actual values than a built-in assertion because it is speciﬁ c to our types. (For example, it might tell us which attributes are different or where long strings differ.)\n\nwww.it-ebooks.info",
      "content_length": 2505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 540,
      "content": "Custom Assertion\n\nOn one project, we were comparing string variables containing XML. Whenever a test failed, we had to bring up two string inspectors and scroll through them looking for the difference. Finally, we got smart and included the logic in a Custom Assertion that told us where the ﬁ rst difference between the two XML strings occurred. The small amount of time we spent writing the diagnostic custom assertion was paid back many times over as we ran our tests.\n\nVariation: Veriﬁ cation Method\n\nIn customer tests, a lot of the complexity of verifying the outcome is related to interacting with the SUT. Veriﬁ cation Methods are a form of Custom Asser- tions that interact directly with the SUT, thereby relieving their callers from this task. This simpliﬁ es the tests signiﬁ cantly and leads to a more “declarative” style of outcome speciﬁ cation. After the Custom Assertion has been written, we can write subsequent tests that result in the same outcome much more quickly. In some cases, it may be advantageous to incorporate even the exercise SUT phase of the test into the Veriﬁ cation Method. This is one step short of a full Parameterized Test that incorporates all the test logic in a reusable Test Utility Method (page 599).\n\nImplementation Notes\n\nThe Custom Assertion is typically implemented as a set of calls to the various built-in Assertion Methods. Depending on how we plan to use it in our tests, we may also want to include the standard Equality Assertion template to ensure correct behavior with null parameters. Because the Custom Assertion is itself an Assertion Method, it should not have any side effects, nor should it call the SUT. (If it needs to do so, it would be a Veriﬁ cation Method.)\n\nVariation: Custom Assertion Test\n\nTesting zealots would also write a Custom Assertion Test (a Self-Checking Test— see page 26—for Custom Assertions) to verify the Custom Assertion. The beneﬁ t from doing so is obvious: increased conﬁ dence in our tests. In most cases, writing Custom Assertion Tests isn’t particularly difﬁ cult because Assertion Methods take all their arguments as parameters.\n\nWe can treat the Custom Assertion as the SUT simply by calling it with various arguments and verifying that it fails in the right cases. Single-Outcome Assertions (see Assertion Method) need only a single test because they don’t take any parameters (other than possibly an Assertion Message). Stated Outcome Assertions need one\n\nwww.it-ebooks.info\n\n477\n\nCustom Assertion",
      "content_length": 2491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 541,
      "content": "478\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\ntest for each possible value (or boundary value). Equality Assertions need one test that compares two objects deemed to be equivalent, one test that compares an object with itself, and one test for each attribute whose inequality should cause the assertion to fail. Attributes that don’t affect equality can be veriﬁ ed in one additional test because the Equality Assertion should not raise an error for any of them.\n\nThe Custom Assertions follow the normal Simple Success Test (see Test Method on page 348) and Expected Exception Test (see Test Method) tem- plates with one minor difference: Because the Assertion Method is the SUT, the exercise SUT and verify outcome phases of the Four-Phase Test (page 358) are combined into a single phase.\n\nEach test consists of setting up the Expected Object and the actual object and then calling the Custom Assertion. If the objects should be equivalent, that’s all there is to it. (The Test Automation Framework described on page 298 would catch any assertion failures and fail the test.) For the tests where we expect the Custom Assertion to fail, we can write the test as an Expected Exception Test (except that the exercise SUT and verify outcome phases of the Four-Phase Test are combined into the single call to the Custom Assertion).\n\nThe simplest way to build the objects to be compared for a speciﬁ c test is to do something similar to One Bad Attribute (see Derived Value on page 718)— that is, build the ﬁ rst object and make a deep copy of it. For successful tests, modify any of the attributes that should not be compared. For each test failure, modify one attribute that should be grounds for failing the assertion.\n\nA brief warning about a possible complication in a few members of the xUnit family: If all of the test failure handling does not occur in the Test Runner (page 377), calls to fail or built-in assertions may add messages to the failure log even if we catch the error or exception in our Custom Assertion Test. The only way to circumvent this behavior is to use an “Encapsulated Test Runner” to run each test by itself and verify that the one test failed with the expected error message.\n\nMotivating Example\n\nIn the following example, several test methods repeat the same series of assertions:\n\npublic void testInvoice_addOneLineItem_quantity1_b() { // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1);\n\nwww.it-ebooks.info",
      "content_length": 2557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 542,
      "content": "Custom Assertion\n\n// Verify only item LineItem expItem = new LineItem(inv, product, QUANTITY); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\npublic void testRemoveLineItemsForProduct_oneOfTwo() { // Setup Invoice inv = createAnonInvoice(); inv.addItemQuantity(product, QUANTITY); inv.addItemQuantity(anotherProduct, QUANTITY); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.removeLineItemForProduct(anotherProduct); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem.getInv(), actual.getInv()); assertEquals(expItem.getProd(), actual.getProd()); assertEquals(expItem.getQuantity(), actual.getQuantity()); }\n\n// // Adding TWO line items //\n\npublic void testInvoice_addTwoLineItems_sameProduct() { Invoice inv = createAnonInvoice(); LineItem expItem1 = new LineItem(inv, product, QUANTITY1); LineItem expItem2 = new LineItem(inv, product, QUANTITY2); // Exercise inv.addItemQuantity(product, QUANTITY1); inv.addItemQuantity(product, QUANTITY2); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 2); // Verify ﬁrst item LineItem actual = (LineItem)lineItems.get(0); assertEquals(expItem1.getInv(), actual.getInv()); assertEquals(expItem1.getProd(), actual.getProd()); assertEquals(expItem1.getQuantity(), actual.getQuantity()); // Verify second item actual = (LineItem)lineItems.get(1); assertEquals(expItem2.getInv(), actual.getInv()); assertEquals(expItem2.getProd(), actual.getProd()); assertEquals(expItem2.getQuantity(), actual.getQuantity()); }\n\nwww.it-ebooks.info\n\n479\n\nCustom Assertion",
      "content_length": 1842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 543,
      "content": "480\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\nNote that the ﬁ rst test ends with a series of three assertions and the second test repeats the series of three assertions twice, once for each line item. This is clearly a bad case of Test Code Duplication.\n\nRefactoring Notes\n\nRefactoring zealots can probably see that the solution is to do an Extract Meth- od [Fowler] refactoring on these tests. If we pull out all the common calls to Assertion Methods, we will be left with only the differences in each test. The extracted method is our Custom Assertion. We may also need to introduce an Expected Object to hold all the values that were being passed to the individual Assertion Methods on a single object to be passed to the Custom Assertion.\n\nExample: Custom Assertion\n\nIn this test, we use a Custom Assertion to verify that LineItem matches the expected LineItem(s). For one reason or another, we have chosen to implement a test-speciﬁ c equality rather than using a standard Equality Assertion.\n\npublic void testInvoice_addOneLineItem_quantity1_() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); // Verify only item LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"LineItem\", expItem, actual); }\n\npublic void testAddItemQuantity_sameProduct_() { Invoice inv = createAnonInvoice(); LineItem expItem1 = new LineItem(inv, product, QUANTITY1); LineItem expItem2 = new LineItem(inv, product, QUANTITY2); // Exercise inv.addItemQuantity(product, QUANTITY1); inv.addItemQuantity(product, QUANTITY2); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 2); // Verify ﬁrst item LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"Item 1\",expItem1,actual); // Verify second item\n\nwww.it-ebooks.info",
      "content_length": 1988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 544,
      "content": "Custom Assertion\n\nactual = (LineItem)lineItems.get(1); assertLineItemsEqual(\"Item 2\",expItem2, actual); }\n\nThe tests have become signiﬁ cantly smaller and more intent-revealing. We have also chosen to pass a string indicating which item we are examining as an argu- ment to the Custom Assertion to avoid playing Assertion Roulette when a test fails.\n\nThis simpliﬁ ed test was made possible by having the following Custom\n\nAssertion available to us:\n\nstatic void assertLineItemsEqual( String msg, LineItem exp, LineItem act) { assertEquals(msg+\" Inv\", exp.getInv(),act.getInv()); assertEquals(msg+\" Prod\", exp.getProd(), act.getProd()); assertEquals(msg+\" Quan\", exp.getQuantity(), act.getQuantity()); }\n\nThis Custom Assertion compares the same attributes of the object as we were comparing on an in-line basis in the previous version of the test; thus the seman- tics of the test haven’t changed. We also concatenate the name of the attribute being compared with the message parameter to get a unique failure message, which allows us to avoid playing Assertion Roulette when a test fails.\n\nExample: Domain Assertion\n\nIn this next version of the test, we have further elevated the level of the asser- tions to better communicate the expected outcome of the test scenarios:\n\npublic void testAddOneLineItem_quantity1() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity( product, QUANTITY); // Verify assertInvoiceContainsOnlyThisLineItem( inv, expItem); }\n\npublic void testRemoveLineItemsForProduct_oneOfTwo_() { Invoice inv = createAnonInvoice(); inv.addItemQuantity( product, QUANTITY); inv.addItemQuantity( anotherProduct, QUANTITY); LineItem expItem = new LineItem( inv, product, QUANTITY); // Exercise inv.removeLineItemForProduct( anotherProduct ); // Verify assertInvoiceContainsOnlyThisLineItem( inv, expItem); }\n\nwww.it-ebooks.info\n\n481\n\nCustom Assertion",
      "content_length": 1936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 545,
      "content": "482\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\nThis simpliﬁ ed version of the test was made possible by extracting the following Domain Assertion method:\n\nvoid assertInvoiceContainsOnlyThisLineItem( Invoice inv, LineItem expItem) { List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"item\",expItem, actual); }\n\nThis example chose to forgo passing a message to the Domain Assertion to save a bit of space. In real life, we would typically include a message string in the parameter list and concatenate the messages of the individual assertions to one passed in. See Assertion Message (page 370) for more details.\n\nExample: Veriﬁ cation Method\n\nIf the exercise SUT and result veriﬁ cation phases of several tests are pretty much identical, we can incorporate both phases into our reusable Custom Assertion. Because this approach changes the semantics of the Custom Assertion from being just a function free of side effects to an operation that changes the state of the SUT, we usually give it a more distinctive name starting with “verify”.\n\nThis version of the test merely sets up the test ﬁ xture before calling a Veriﬁ ca- tion Method that incorporates both the exercise SUT and verify outcome phases of the test. It is most easily recognized by the lack of a distinct “exercise” phase in the calling test and the presence of calls to methods that modify the state of one of the objects passed as a parameter of the Veriﬁ cation Method.\n\npublic void testAddOneLineItem_quantity2() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise & Verify verifyOneLineItemCanBeAdded(inv, product, QUANTITY, expItem); }\n\nThe Veriﬁ cation Method for this example looks like this:\n\npublic void verifyOneLineItemCanBeAdded( Invoice inv, Product product, int QUANTITY, LineItem expItem) { // Exercise inv.addItemQuantity(product, QUANTITY); // Verify assertInvoiceContainsOnlyThisLineItem(inv, expItem); }\n\nwww.it-ebooks.info",
      "content_length": 2086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 546,
      "content": "Custom Assertion\n\nThis Veriﬁ cation Method calls the “pure” Custom Assertion, although it could just as easily have included all the assertion logic if we didn’t have the other Cus- tom Assertion to call. Note the call to addItemQuantity on the parameter inv; this is what changes if from a Custom Assertion to a Veriﬁ cation Method.\n\nExample: Custom Assertion Test\n\nThis Custom Assertion isn’t particularly complicated, so we may feel comfort- able without having any automated tests for it. If there is anything complex about it, however, we may ﬁ nd it worthwhile to write tests like these:\n\npublic void testassertLineItemsEqual_equivalent() { Invoice inv = createAnonInvoice(); LineItem item1 = new LineItem(inv, product, QUANTITY1); LineItem item2 = new LineItem(inv, product, QUANTITY1); // exercise/verify assertLineItemsEqual(\"This should not fail\",item1, item2); }\n\npublic void testassertLineItemsEqual_differentInvoice() { Invoice inv1 = createAnonInvoice(); Invoice inv2 = createAnonInvoice(); LineItem item1 = new LineItem(inv1, product, QUANTITY1); LineItem item2 = new LineItem(inv2, product, QUANTITY1); // exercise/verify try { assertLineItemsEqual(\"Msg\",item1, item2); } catch (AssertionFailedError e) { assertEquals(\"e.getMsg\", \"Invoice-expected: <123> but was <124>\", e.getMessage()); return; } fail(\"Should have thrown exception\"); }\n\npublic void testassertLineItemsEqual_differentQuantity() { Invoice inv = createAnonInvoice(); LineItem item1 = new LineItem(inv, product, QUANTITY1); LineItem item2 = new LineItem(inv, product, QUANTITY2); // exercise/verify try { assertLineItemsEqual(\"Msg\",item1, item2); } catch (AssertionFailedError e) { pass(); // to indicate that no assertion is needed return; } fail(\"Should have thrown exception\"); }\n\nwww.it-ebooks.info\n\n483\n\nCustom Assertion",
      "content_length": 1806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 547,
      "content": "484\n\nCustom Assertion\n\nChapter 21 Result Verification Patterns\n\nThis example includes a few of the Custom Assertion Tests needed for this Custom Assertion. Note that the code includes one “equivalent” and several “different” tests (one for each attribute whose difference should cause the test to fail). We have to use the second form of the Expected Exception Test template in those cases where the assertion was expected to fail, because fail throws the same exception as our assertion method. In one of the “different” tests, we have included sample logic for asserting on the exception message. (Although I’ve abridged it to save space, the example here should give you an idea of where to assert on the message.)\n\nwww.it-ebooks.info",
      "content_length": 737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 548,
      "content": "Delta Assertion\n\nDelta Assertion\n\nHow do we make tests self-checking when we cannot control the initial contents of the ﬁ xture?\n\nWe specify assertions based on differences between the pre- and post-exercise state of the SUT.\n\n1. Get Pre-test State 1. Get Pre-test State\n\nSetup Setup\n\nBefore Before\n\nExercise Exercise\n\nSUT SUT\n\nFixture Fixture Data Data\n\nVerify Verify\n\nAfter After\n\n2. Get Post-test State 2. Get Post-test State\n\nTeardown Teardown\n\n3. Compare with 3. Compare with Expected Difference Expected Difference\n\nWhen we are using a Shared Fixture (page 317) such as a test database, it can be challenging to code the assertions that state what the content of the ﬁ xture should be after the SUT has been exercised. This is because other tests may have created objects in the ﬁ xture that our assertions may detect and that may cause our assertions to fail. One solution is to isolate the current test from all other tests by using a Database Partitioning Scheme (see Database Sandbox on page 650). But what can we do if this option is not available to us?\n\nUsing Delta Assertions allows us to be less dependent on which data already\n\nexist in the Shared Fixture.\n\nHow It Works\n\nBefore exercising the SUT, we take a snapshot of relevant parts of the Shared Fixture. After exercising the SUT, we specify our assertions relative to the saved snapshot. The Delta Assertions typically verify that the number of objects has changed by the right number and that the contents of collections of objects returned by the SUT in response to our queries have been augmented by the expected objects.\n\nwww.it-ebooks.info\n\n485\n\nDelta Assertion",
      "content_length": 1637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 549,
      "content": "486\n\nDelta Assertion\n\nChapter 21 Result Verification Patterns\n\nWhen to Use It\n\nWe can use a Delta Assertion whenever we don’t have full control over the test ﬁ xture and we want to avoid Interacting Tests (see Erratic Test on page 228). Using Delta Assertions will help make our tests more resilient to changes in the ﬁ xture. We can also use Delta Assertions in concert with Implicit Teardown (page 516) to detect memory or data leaks in the code that we are testing. See the sidebar “Using Delta Assertions to Detect Data Leakage” on page 487 for a more detailed description.\n\nDelta Assertions work well when tests are run one after another from the same Test Runner (page 377). Unfortunately, they cannot prevent a Test Run War (see Erratic Test) because such a problem arises when tests are run at the same time from different processes. Delta Assertions work whenever the state of the SUT and the ﬁ xture are modiﬁ ed only by our own test. If other tests are running in parallel (not before or after the current test, but at the same time), a Delta Assertion won’t be sufﬁ cient to avoid the Test Run War problem.\n\nImplementation Notes\n\nWhen saving the pre-test state of the Shared Fixture or SUT, we must make sure that the SUT cannot change our snapshot. For example, if our snapshot consists of a collection of objects returned by the SUT in response to a query, we must perform a deep copy; a shallow copy copies only the Collection object and not the objects to which it refers. Shallow copying would allow the SUT to modify the very objects it returned to us as we exercise it; as a consequence, we would lose the reference snapshot with which we are comparing the post-test state.\n\nWe can ensure that we have the correct post-test state in several different ways. Assuming that our test adds any new objects it plans to modify, one approach is to ﬁ rst check that the result collection (1) has the right number of items, (2) con- tains all the pre-test items, and (3) contains the new Expected Objects (see State Veriﬁ cation on page 462). Another approach is to remove all the saved items from the result collection and then compare what remains with the collection of new expected objects. Both of these approaches can be hidden behind a Custom Assertion (page 474) or a Veriﬁ cation Method (see Custom Assertion).\n\nwww.it-ebooks.info",
      "content_length": 2348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 550,
      "content": "Delta Assertion\n\nUsing Delta Assertions to Detect Data Leakage\n\nA long time ago, on a project far away, we were experimenting with different ways to clean up our test ﬁ xtures after the customer tests. Our tests were accessing a database and leaving objects behind. This behavior caused all sorts of problems with Unrepeatable Tests (see Erratic Test on page 228) and Interacting Tests (see Erratic Test). We were also suffering from Slow Tests (page 253).\n\nEventually we hit upon the idea of keeping track of all the objects we were creating in our tests by registering them with an Automated Tear- down (page 503) mechanism. Then we found a way to stub out the data- base with a Fake Database (see Fake Object on page 551). Next we made it possible to run the same test against either the fake database or the real one. This solved many of the interaction problems when running against the fake database, although those problems still occurred when we ran the tests against the real database—tests still left objects behind, and we wanted to know why. But ﬁ rst we had to determine precisely which tests were at fault.\n\nThe solution turned out to be pretty simple. In our Fake Database—which was implemented using simple hash tables—we added a method to count the total number of objects. We simply saved this value in an instance variable in the setUp method and used it as the expected value passed to an Equality Assertion (see Assertion Method on page 362) called in the tear- Down method to verify that we had cleaned up all objects properly. [This is an example of using Delta Assertions (page 485).] Once we implemented this little trick, we quickly found out which tests were suffering from the Data Leak (see Erratic Test). We could then focus our efforts on a much smaller number of tests.\n\nEven today, we ﬁ nd it useful to be able to run the same test against the database and in memory. Similarly, we still occasionally see a test fail when the tearDown method inherited from our company-speciﬁ c Testcase Superclass (page 638) has a Delta Assertion failure. Perhaps the same idea could be applied to checking for memory leaks in programming lan- guages with manual memory management (such as C++).\n\nwww.it-ebooks.info\n\n487\n\nDelta Assertion",
      "content_length": 2254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 551,
      "content": "488\n\nDelta Assertion\n\nChapter 21 Result Verification Patterns\n\nMotivating Example\n\nThe following test retrieves some objects from the SUT. It then compares the objects it actually found with the objects it expected to ﬁ nd.\n\npublic void testGetFlightsByOriginAirport_OneOutboundFlight() throws Exception { FlightDto expectedFlightDto = createNewFlightBetweenExistingAirports(); // Exercise System facade.createFlight( expectedFlightDto.getOriginAirportId(), expectedFlightDto.getDestinationAirportId()); // Verify Outcome List ﬂightsAtOrigin = facade.getFlightsByOriginAirport( expectedFlightDto.getOriginAirportId()); assertOnly1FlightInDtoList( \"Outbound ﬂight at origin\", expectedFlightDto, ﬂightsAtOrigin); }\n\nUnfortunately, because this test used a Shared Fixture, other tests that ran before it may have added objects as well. That behavior could cause the current test to fail if we encounter additional, unexpected objects.\n\nRefactoring Notes\n\nTo convert the test to use a Delta Assertion, we must ﬁ rst take a snapshot of the data (or collection of objects) we will later be asserting on. Next, we need to modify our assertions to focus on the difference between the pre-test data/ objects and the post-test data/objects. To avoid introducing Conditional Test Logic (page 200) into the Test Method (page 348), we may want to introduce a new Custom Assertion. Although we may be able to use existing assertions (custom or otherwise) as a starting point, we’ll probably have to modify them to take the pre-test data into account.\n\nExample: Delta Assertion\n\nIn this version of the test, we use a Delta Assertion to verify the objects added when we exercised the SUT. Here we are verifying that we have one more object than before and that the collection of objects returned by the SUT includes the new Expected Object and all objects that it previously contained.\n\npublic void testCreateFlight_Delta() throws Exception { FlightDto expectedFlightDto = createNewFlightBetweenExistingAirports();\n\nwww.it-ebooks.info",
      "content_length": 2018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 552,
      "content": "Delta Assertion\n\n// Remember prior state List ﬂightsBeforeCreate = facade.getFlightsByOriginAirport( expectedFlightDto.getOriginAirportId()); // Exercise system facade.createFlight( expectedFlightDto.getOriginAirportId(), expectedFlightDto.getDestinationAirportId()); // Verify outcome relative to prior state List ﬂightsAfterCreate = facade.getFlightsByOriginAirport( expectedFlightDto.getOriginAirportId()); assertFlightIncludedInDtoList( \"new ﬂight \", expectedFlightDto, ﬂightsAfterCreate); assertAllFlightsIncludedInDtoList( \"previous ﬂights\", ﬂightsBeforeCreate, ﬂightsAfterCreate); assertEquals( \"Number of ﬂights after create\", ﬂightsBeforeCreate.size()+1, ﬂightsAfterCreate.size()); }\n\nBecause the SUT returns Data Transfer Objects [CJ2EEP], we can be assured that the objects we saved before exercising the SUT cannot possibly change. We have modiﬁ ed our Custom Assertions to ignore the pre-test objects (by not insisting that the Expected Object is the only one) and have written a new Cus- tom Assertion that ensures all pre-test objects are also present. Another way to accomplish this task is to remove the pre-test objects from the result collection and then verify that only the new Expected Objects are left.\n\nI’ve omitted the implementation of the Custom Assertions, as it is purely an exercise in comparing objects and is not salient to understanding the Delta Assertion pattern. The “test infected” among us would, of course, write the Custom Assertions driven by some Custom Assertion Tests (see Custom Assertion).\n\nwww.it-ebooks.info\n\n489\n\nDelta Assertion",
      "content_length": 1577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 553,
      "content": "490\n\nGuard Assertion\n\nChapter 21 Result Verification Patterns\n\nGuard Assertion\n\nHow do we avoid Conditional Test Logic?\n\nWe replace an if statement in a test with an assertion that fails the test if not satisﬁ ed.\n\nFixture Fixture\n\nSetup Setup\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nobj obj\n\nAssertNotNull AssertNotNull\n\nTeardown Teardown\n\nobj.attr obj.attr\n\nAssert Assert\n\nSome veriﬁ cation logic may fail because information returned by the SUT is not initialized as expected. When a test encounters an unexpected problem, it may produce a test error rather than a test failure. While the Test Runner (page 377) does its best to provide useful diagnostic information, the test automater can of- ten do better by checking for the particular condition and reporting it explicitly. A Guard Assertion is a good way to do so without introducing Conditional\n\nTest Logic (page 200).\n\nHow It Works\n\nTests either pass or fail. We fail tests by calling Assertion Methods (page 362) that stop the test from executing further if the assertion’s condition is not met. Alternatively, we can replace Conditional Test Logic that is used to avoid ex- ecuting assertions when they would cause test errors with assertions that fail the test instead. This pattern also documents the fact that we expect the condition of the Guard Assertion to be true. A failure of the Guard Assertion makes it very\n\nwww.it-ebooks.info",
      "content_length": 1405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 554,
      "content": "Guard Assertion\n\nclear that some condition we expected to be true was not; it eliminates the effort needed to infer the test result from the conditional logic.\n\nWhen to Use It\n\nWe should use a Guard Assertion whenever we want to avoid executing state- ments in our Test Method (page 348) because they would cause an error if they were executed when some condition related to values returned by the SUT is not true. We take this step instead of putting an if then else fail code construct around the sensitive statements. Normally, a Guard Assertion is placed between the exer- cise SUT and the verify outcome phases of a Four-Phase Test (page 358).\n\nVariation: Shared Fixture State Assertion\n\nWhen the test uses a Shared Fixture (page 317), a Guard Assertion can also be useful at the beginning of the test (before the exercise SUT phase) to verify that the Shared Fixture satisﬁ es the test’s needs. It also makes it clearer to the test reader which parts of the Shared Fixture this test actually uses; the greater clar- ity improves the likelihood of achieving Tests as Documentation (see page 23).\n\nImplementation Notes\n\nWe can use Stated Outcome Assertions (see Assertion Method) like assertNotNil and Equality Assertions (see Assertion Method) like assertEquals as Guard Assertions that fail the test and prevent execution of other statements that would cause test errors.\n\nMotivating Example\n\nConsider the following example:\n\npublic void testWithConditionals() throws Exception { String expectedLastname = \"smith\"; List foundPeople = PeopleFinder. ﬁndPeopleWithLastname(expectedLastname); if (foundPeople != null) { if (foundPeople.size() == 1) { Person solePerson = (Person) foundPeople.get(0); assertEquals( expectedLastname,solePerson.getName()); } else { fail(\"list should have exactly one element\"); } } else { fail(\"list is null\"); } }\n\nwww.it-ebooks.info\n\n491\n\nGuard Assertion",
      "content_length": 1889,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 555,
      "content": "492\n\nGuard Assertion\n\nChapter 21 Result Verification Patterns\n\nThis example includes plenty of conditional statements that the author might get wrong—things like writing (foundPeople == null) instead of (foundPeople != null). In C-based languages, one might mistakenly use = instead of ==, which would result in the test always passing!\n\nRefactoring Notes\n\nWe can use a Replace Nested Conditional with Guard Clauses [Fowler] refactoring to transform this spider web of Conditional Test Logic into a nice linear sequence of statements. (In a test, even a single conditional statement is considered too much and hence “nested”!) We can use Stated Outcome Assertions to check for null object references and Equality Assertions to verify the number of objects in the collection. If each assertion is satisﬁ ed, the test proceeds. If they are not satisﬁ ed, the test ends in failure before it reaches the next statement.\n\nExample: Simple Guard Assertion\n\nThis simpliﬁ ed version of the test replaces all conditional statements with asser- tions. It is shorter than the original test and much easier to read.\n\npublic void testWithoutConditionals() throws Exception { String expectedLastname = \"smith\"; List foundPeople = PeopleFinder. ﬁndPeopleWithLastname(expectedLastname); assertNotNull(\"found people list\", foundPeople); assertEquals( \"number of people\", 1, foundPeople.size() ); Person solePerson = (Person) foundPeople.get(0); assertEquals( \"last name\", expectedLastname, solePerson.getName() ); }\n\nWe now have a single linear execution path through this Test Method (page 348); it should improve our conﬁ dence in the correctness of this test immensely!\n\nExample: Shared Fixture Guard Assertion\n\nHere’s an example of a test that depends on a Shared Fixture. If a previous test (or even this test in a previous test run) modiﬁ es the state of the ﬁ xture, our SUT could return unexpected results. It might take a fair bit of effort to determine that the problem lies in the test’s pre-conditions rather than being a bug in the SUT. We can avoid all of this trouble by making the assumptions of this test explicit through the use of a Guard Assertion during the ﬁ xture lookup phase of the test.\n\nwww.it-ebooks.info",
      "content_length": 2214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 556,
      "content": "Guard Assertion\n\npublic void testAddFlightsByFromAirport_OneOutboundFlight_GA() throws Exception { // Fixture Lookup List ﬂights = facade.getFlightsByOriginAirport( ONE_OUTBOUND_FLIGHT_AIRPORT_ID ); // Guard Assertion on Fixture Contents assertEquals( \"# ﬂights precondition\", 1, ﬂights.size()); FlightDto ﬁrstFlight = (FlightDto) ﬂights.get(0); // Exercise System BigDecimal ﬂightNum = facade.createFlight( ﬁrstFlight.getOriginAirportId(), ﬁrstFlight.getDestAirportId()); // Verify Outcome FlightDto expFlight = (FlightDto) ﬁrstFlight.clone(); expFlight.setFlightNumber( ﬂightNum ); List actual = facade.getFlightsByOriginAirport( ﬁrstFlight.getOriginAirportId()); assertExactly2FlightsInDtoList( \"Flights at origin\", ﬁrstFlight, expFlight, actual); }\n\nWe now have a way to determine that the assumptions were violated without extensive debugging! This is another way we achieve Defect Localization (see page 22). This time the defect is in the tests’ assumptions on the previously run tests’ behavior.\n\nwww.it-ebooks.info\n\n493\n\nGuard Assertion",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 557,
      "content": "494\n\nUnﬁ nished Test Assertion\n\nChapter 21 Result Verification Patterns\n\nUnﬁ nished Test Assertion\n\nHow do we structure our test logic to avoid leaving tests unﬁ nished?\n\nWe ensure that incomplete tests fail by executing an assertion that is guaranteed to fail.\n\nvoid testSomething() { // Outline: // create a ﬂight in ... state // call the ... method // verify ﬂight is in ... state fail(\"Unﬁnished Test!\"); }\n\nWhen we start deﬁ ning the tests for a particular piece of code, it is useful to “rough in” the tests by deﬁ ning Test Methods (page 348) on the appropriate Testcase Class (page 373) as we think of the test conditions. We do, however, want to ensure that we don’t accidentally forget to ﬁ ll in the bodies of these tests if we get distracted. We want the tests to fail until we ﬁ nish coding them.\n\nIncluding an Unﬁ nished Test Assertion is a good way to make sure we don’t\n\nforget.\n\nHow It Works\n\nWe put a single call to fail in each Test Method as we deﬁ ne it. The fail method is a Single-Outcome Assertion (see Assertion Method on page 362) that always fails. We include the Assertion Message (page 370) “Unﬁ nished Test” as a reminder of why the test fails when we do run the tests.\n\nWhen to Use It\n\nWe should not deliberately write any tests that might accidentally pass. A failing test makes a good reminder that we still have work to do. We can remind our- selves of this work by putting an Unﬁ nished Test Assertion at the end of every test we write and by removing it only when we are satisﬁ ed that the test is coded properly. There is no real cost to doing so, but a lot of beneﬁ t. It is just a matter of getting into the habit. Some IDEs even help us out by letting us put the Unﬁ n- ished Test Assertion into the code generation template for a Test Method\n\nIf we need to check in the tests before all code is working, we shouldn’t remove the tests or the Unﬁ nished Test Assertions just to get a green bar, as this could\n\nwww.it-ebooks.info",
      "content_length": 1967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 558,
      "content": "Unfinished Test Assertion\n\nresult in Lost Tests (see Production Bugs on page 268). Instead, we can add an [Ignore] attribute to the test if our member of the xUnit family supports it, rename the test method if xUnit uses name-based Test Discovery (page 393), or exclude the entire Testcase Class from the AllTests Suite (see Named Test Suite on page 592) if we are using Test Enumeration (page 399) at the suite level.\n\nImplementation Notes\n\nMost members of the xUnit family have a fail method already deﬁ ned. If the member that we are using doesn’t include it, we should avoid the temptation to sprinkle assertTrue(false) throughout our code. This kind of code is obtuse and easy to get wrong because it is counter-intuitive. Instead, we should take a minute to write this method ourselves as a Custom Assertion (page 474) and write the Custom Assertion Test (see Custom Assertion) for it ﬁ rst to make sure we got it right.\n\nSome IDEs include the ability to customize code generation templates. Some even include a template for a Test Method that includes an Unﬁ nished Test Assertion.\n\nMotivating Example\n\nConsider the following Testcase Class that we are roughing in:\n\npublic void testPull_emptyStack() {\n\n}\n\npublic void testPull_oneItemOnStack () {\n\n}\n\npublic void testPull_twoItemsOnStack () { //To do: Write this test }\n\npublic void testPull_oneItemsRemainingOnStack () { //To do: Write this test }\n\nIncluding the // To do: ... comments may remind us that the test still needs work if our IDE supports that feature—but it won’t remind us of the unﬁ nished work when we run the tests. Running this Testcase Class will result in a green bar even though we may not have implemented our stack at all!\n\nwww.it-ebooks.info\n\n495\n\nUnﬁ nished Test Assertion",
      "content_length": 1756,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 559,
      "content": "496\n\nUnﬁ nished Test Assertion\n\nChapter 21 Result Verification Patterns\n\nRefactoring Notes\n\nTo implement Unﬁ nished Test Assertion all we need to do is add the following line to each test as we rough it in:\n\nfail(\"Unﬁnished Test!\");\n\nThe exclamation mark is optional. It might be even better to create a Custom Assertion such as this one:\n\nprivate void unﬁnishedTest() { fail(\"Test not implemented!\"); }\n\nThis would allow us to ﬁ nd all the Unﬁ nished Test Assertions easily by using the “search for references” feature of our IDE.\n\nExample: Unﬁ nished Test Assertion\n\nHere are the tests with an Unﬁ nished Test Assertion added to each one:\n\npublic void testPull_emptyStack() { unﬁnishedTest(); }\n\npublic void testPull_oneItemOnStack () { unﬁnishedTest(); }\n\npublic void testPull_twoItemsOnStack() { unﬁnishedTest(); }\n\npublic void testPull_oneItemsRemainingOnStack () { unﬁnishedTest(); }\n\nNow we have a Testcase Class that is guaranteed to fail until we ﬁ nish writing the code. The failing tests act as a “to do” list for writing the tests.\n\nExample: Unﬁ nished Test Method Generation from Template\n\nEclipse (version 3.0) is an example of an IDE that includes the ability to custom- ize templates. Its testmethod template inserts the following code into our Testcase Class:\n\npublic void testname() throws Exception { fail(\"ClassName::testname not implemented\"); }\n\nwww.it-ebooks.info",
      "content_length": 1386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 560,
      "content": "Unfinished Test Assertion\n\nThe strings “ClassName” and “testname” are placeholders for the names of the Testcase Class and Test Method, respectively; they are ﬁ lled in automatically by the IDE. As we modify the test name in the signature, the test name in the fail statement is adjusted automatically. All we have to do to insert a new Test Method into a class is to type “testmethod” and then press CTRL-SPACEBAR.\n\nwww.it-ebooks.info\n\n497\n\nUnﬁ nished Test Assertion",
      "content_length": 467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 561,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 562,
      "content": "Chapter 22\n\nFixture Teardown Patterns\n\nPatterns in This Chapter\n\nTeardown Strategy\n\nGarbage-Collected Teardown. . . . . . . . . . . . . . . . . . . . . . . . . . . 500\n\nAutomated Teardown. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503\n\nCode Organization\n\nIn-line Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n\nImplicit Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516\n\n499\n\nwww.it-ebooks.info\n\nFixture Teardown Patterns",
      "content_length": 514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 563,
      "content": "500\n\nGarbage- Collected Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nGarbage-Collected Teardown\n\nHow do we tear down the Test Fixture?\n\nWe let the garbage collection mechanism provided by the programming language clean up after our test.\n\nTestcase Class Testcase Class\n\nSetup Setup\n\nSetup Setup\n\nFixture Fixture\n\ntest_method_1 test_method_1\n\ntest_method_2 test_method_2\n\nSUT SUT\n\nSetup Setup\n\ntest_method_n test_method_n\n\ntearDown tearDown\n\nGarbage Garbage Collection Collection\n\nTeardown Teardown\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ x- ture is torn down after each test and a new one created for the next test run. This strategy is known as a Fresh Fixture (page 311). In languages that provide garbage collection, much of the teardown can happen automatically if we refer to resources via local and instance variables.\n\nHow It Works\n\nMany of the objects created during the course of our test (including both ﬁ xture setup and exercising the SUT) are transient objects that are kept alive only as long as there is a reference to them somewhere in the program that created them. The garbage collection mechanisms of modern languages use various algorithms to detect “garbage.” What is most important, though, is how they determine that something is not garbage: Any object that is reachable from any other live object or from global (i.e., static) variables will not be garbage collected.\n\nWhen running our tests, the Test Automation Framework (page 298) creates a Testcase Object (page 382) for each Test Method (page 348) in our Testcase\n\nwww.it-ebooks.info",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 564,
      "content": "Garbage-Collected Teardown\n\nClass (page 373) and adds those objects to a Test Suite Object (page 387). When- ever a new test run is started, the framework typically throws away the existing test suite and builds a new one (to be sure everything is fresh). When the old test suite is discarded, any objects referenced only by instance variables in those tests become candidates for garbage collection.\n\nWhen to Use It\n\nWe should use Garbage-Collected Teardown whenever we possibly can because it will save us a lot of effort!\n\nIf our programming takes place in an environment that doesn’t support garbage collection, or if we have resources that aren’t garbage collected automatically (e.g., ﬁ les, sockets, records in a database), we’ll need to destroy or free those resources explicitly. If we are using a Shared Fixture (page 317), we won’t be able to use Garbage-Collected Teardown unless we do something fancy to hold the reference to the ﬁ xture in such a way that it will go out of scope when our test suite has ﬁ nished running.\n\nWe can use In-line Teardown (page 509), Implicit Teardown (page 516), or\n\nAutomated Teardown (page 503) to ensure that they are released properly.\n\nImplementation Notes\n\nSome members of the xUnit family and some IDEs go so far as to replace the classes each time the test suite is run. We may see this behavior show up as an option called “Reload Classes” or it may be forced upon us. We must be care- ful if we decide to take advantage of this feature to perform Garbage-Collected Teardown with ﬁ xture holding class variables, as our tests may stop running if we change IDEs or try running our tests from the command line (e.g., from “Cruise Control” or a build script.)\n\nMotivating Example\n\nThe following test creates some in-memory objects during ﬁ xture setup and explicitly destroys them using In-line Teardown. (We could have used Implicit Teardown in this example but that just makes it harder for readers to see what is going on.)\n\npublic void testCancel_proposed_UIT() { // ﬁxture setup Flight proposedFlight = createAnonymousProposedFlight(); // exercise SUT proposedFlight.cancel();\n\nwww.it-ebooks.info\n\n501\n\nGarbage- Collected Teardown",
      "content_length": 2185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 565,
      "content": "502\n\nGarbage- Collected Teardown\n\nChapter 22 Fixture Teardown Patterns\n\n// verify outcome try{ assertEquals( FlightState.CANCELLED, proposedFlight.getStatus()); } ﬁnally { // teardown proposedFlight.delete(); proposedFlight = null; } }\n\nBecause these objects are not persistent, the code to delete the proposedFlight is unnecessary and just makes the test more complicated and harder to understand.\n\nRefactoring Notes\n\nTo convert to Garbage-Collected Teardown, we need only remove the unneces- sary cleanup code. If we had been using a class variable to hold the reference to the object, we would have had to convert it to either an instance variable or a local variable, both of which would have moved us from a Shared Fixture to a Fresh Fixture.\n\nExample: Garbage-Collected Teardown\n\nIn this reworked test, we let Garbage-Collected Teardown do the job for us.\n\npublic void testCancel_proposed_GCT() { // ﬁxture setup Flight proposedFlight = createAnonymousProposedFlight(); // exercise SUT proposedFlight.cancel(); // verify outcome assertEquals( FlightState.CANCELLED, proposedFlight.getStatus()); // teardown // Garbage collected when proposedFlight goes out of scope }\n\nNote how much simpler the test has become!\n\nwww.it-ebooks.info",
      "content_length": 1237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 566,
      "content": "Automated Teardown\n\nAutomated Teardown\n\nHow do we tear down the Test Fixture?\n\nWe keep track of all resources that are created in a test and automatically destroy/free them during teardown.\n\nTestcase Class Testcase Class\n\nCreation Creation\n\nCreation Creation\n\nFixture Fixture\n\ntest_method_1 test_method_1\n\ntest_method_2 test_method_2\n\nSUT SUT\n\nRegister Register Test Object Test Object\n\ntest_method_n test_method_n\n\nTeardown Teardown\n\ntearDown tearDown\n\nDestroy All Destroy All\n\nTest Object Test Object Registry Registry\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ xture is torn down after each test and a new one created for the next test run. This strategy is known as a Fresh Fixture (page 311). Leftover objects and database records, as well as open ﬁ les and connections, can at best cause performance degradations and at worst cause tests to fail or systems to crash. While some of these resources may be cleaned up automatically by garbage collection, others may be left hanging if they are not torn down explicitly.\n\nWriting teardown code that can be relied upon to clean up properly in all possible circumstances is challenging and time-consuming. It involves understanding what could be left over for each possible outcome of the test and writing code to deal with that scenario. This Complex Teardown (see Obscure Test on page 186) introduces a fair bit of Conditional Test Logic (page 200) and—worst of all—Untestable Test Code (see Hard-to-Test Code on page 209).\n\nA better solution is to let the test infrastructure track the objects created and\n\nclean them up auto-magically when the test is complete.\n\nwww.it-ebooks.info\n\n503\n\nAlso known as: Test Object Registry\n\nAutomated Teardown",
      "content_length": 1734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 567,
      "content": "504\n\nAutomated Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nHow It Works\n\nThe core of the solution is a mechanism to register each persistent item (i.e., object, record, connection, and so on) we create in the test. We maintain a list (or lists) of registered objects that need some action to be taken to destroy them. This can be as simple as tossing each object into a collection. At the end of the test, we traverse the collection and destroy each object. We will want to catch any errors that we encounter so that one object’s cleanup error will not cause the rest of the cleanup to be aborted.\n\nWhen to Use It\n\nWe can use Automated Teardown whenever we have persistent resources that need to be cleaned up to keep the test environment functioning. (This happens more often in customer tests than in unit tests.) This pattern addresses both Unrepeatable Tests (see Erratic Test on page 228) and Interacting Tests (see Erratic Test) by keeping the objects created in one test from lingering into the execution of a subsequent test.\n\nAutomated Teardown isn’t very difﬁ cult to build, and it will save us a large amount of grief and effort. Once we have built it for one project, we should be able to reuse the teardown logic on subsequent projects for very little effort.\n\nImplementation Notes\n\nAutomated Teardown comes in two ﬂ avors. The basic ﬂ avor tears down only objects that were created as part of ﬁ xture setup. The more advanced version also destroys any objects that were created by the SUT while it was being exercised.\n\nVariation: Automated Fixture Teardown\n\nThe simplest solution is to register the objects we create in our Creation Methods (page 415). Although this pattern will not tear down the objects created by the SUT, by dealing with our ﬁ xture it reduces the effort and likelihood of errors signiﬁ cantly.\n\nThere are two key challenges with this variation:\n\nFinding a generic way to clean up the registered objects\n\nEnsuring that our Automated Teardown code is run for each registered\n\nobject\n\nGiven that the latter challenge is the easier problem, let us deal with it ﬁ rst. When we are tearing down a Persistent Fresh Fixture (see Fresh Fixture), the\n\nwww.it-ebooks.info",
      "content_length": 2201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 568,
      "content": "Automated Teardown\n\nsimplest solution is to put the call to the Automated Teardown mechanism into the tearDown method on our Testcase Class (page 373). This method is called regardless of whether the test passes or fails as long as the setUp method succeeds. When we are tearing down a Shared Fixture (page 317), we want the tearDown method to run only after all the Test Methods (page 348) have been run. In this case, we can use either Suite Fixture Setup (page 441), if our member of the xUnit family supports it, or a Setup Decorator (page 447).\n\nNow let’s go back to the harder problem: the generic mechanism for cleaning up the resources. We have at least two options here. First, we can ensure that all persistent (non-garbage-collected) objects implement a generic cleanup mechanism that we can call from within the Automated Teardown mechanism. Alternatively, we can wrap each object in another object that knows how to clean up the object in question. The latter strategy is an example of the Command [GOF] pattern.\n\nIf we build our Automated Teardown mechanism in a completely generic way, we can include it in the Testcase Superclass (page 638) on which we can base all our Testcase Classes. Otherwise, we may need to put it onto a Test Helper (page 643) that is visible from all Testcase Classes that need it. A Test Helper that both creates ﬁ xture objects and tears them down automatically is sometimes called an Object Mother (see Test Helper).\n\nBeing a nontrivial (and very critical) piece of code, the Automated Teardown mechanism deserves its own unit tests. Because it is now outside the Test Method, we can write Self-Checking Tests (see page 26) for it! If we want to be really careful (some might say paranoid), we can use Delta Assertions (page 485) to verify that any objects that persist after the teardown operation really existed before the test was performed.\n\nVariation: Automated Exercise Teardown\n\nWe can make the tests even more “self-cleaning” by also cleaning up the objects created by the SUT. This effort involves designing the SUT using an observable Object Factory (see Dependency Lookup on page 686) so that we can automati- cally register any objects created by the SUT while it is being exercised. During the teardown phase we can delete these objects, too.\n\nMotivating Example\n\nIn this example, we create several objects using Creation Methods and need to tear them down when the test in complete. To do so, we introduce a try/ﬁ nally block to ensure that our In-line Teardown (page 509) code executes even when the assertions fail.\n\nwww.it-ebooks.info\n\n505\n\nAutomated Teardown",
      "content_length": 2620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 569,
      "content": "506\n\nAutomated Teardown\n\nChapter 22 Fixture Teardown Patterns\n\npublic void testGetFlightsByOrigin_NoInboundFlight_SMRTD() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expFlightDto = createTestFlight(outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { try { facade.removeFlight(expFlightDto.getFlightNumber()); } ﬁnally { try { facade.removeAirport(inboundAirport); } ﬁnally { facade.removeAirport(outboundAirport); } } } }\n\nNote that we must use nested try/ﬁ nally constructs within the ﬁ nally block to ensure that any errors in the teardown don’t keep us from ﬁ nishing the job.\n\nRefactoring Notes\n\nIntroducing Automated Teardown involves two steps. First, we add the Automated Teardown mechanism to our Testcase Class. Second, we remove any In-line Teardown code from our tests.\n\nAutomated Teardown can be implemented on a speciﬁ c Testcase Class or it can be inherited (or mixed in) via a generic class. Either way, we need to make sure we register all of our newly created objects so that the mechanism knows to delete them when the test is ﬁ nished. This is most easily done inside Creation Methods that already exist. Alternatively, we can use an Extract Method [Fowler] refactoring to move the direct constructor calls into newly created Creation Methods and add the registration.\n\nThe generic Automated Teardown mechanism should be invoked from the tearDown method. Although this can be done on our own Testcase Class, it is almost always better to put this method on a Testcase Superclass that all our Testcase Classes inherit from. If we don’t already have a Testcase Superclass,\n\nwww.it-ebooks.info",
      "content_length": 1923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 570,
      "content": "Automated Teardown\n\nwe can easily create one by doing an Extract Class [Fowler] refactoring and then doing a Pull Up Method [Fowler] refactoring on any methods (and ﬁ elds) associated with the Automated Teardown mechanism.\n\nExample: Automated Teardown\n\nThere is not much to see in this refactored test because all of the teardown code has been removed.\n\npublic void testGetFlightsByOriginAirport_OneOutboundFlight() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = createTestAirport(\"1IF\"); FlightDto expectedFlightDto = createTestFlight( outboundAirport, inboundAirport); // Exercise System List ﬂightsAtOrigin = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertOnly1FlightInDtoList( \"Flights at origin\", expectedFlightDto, ﬂightsAtOrigin); }\n\nHere is where all the work gets done! The Creation Method has been modiﬁ ed to register the object it just created.\n\nprivate List allAirportIds; private List allFlights;\n\nprotected void setUp() throws Exception { allAirportIds = new ArrayList(); allFlights = new ArrayList(); }\n\nprivate BigDecimal createTestAirport(String airportName) throws FlightBookingException { BigDecimal newAirportId = facade.createAirport( airportName, \" Airport\" + airportName, \"City\" + airportName); allAirportIds.add(newAirportId); return newAirportId; }\n\nNext comes the actual Automated Teardown logic. In this example, it lives on our Testcase Class and is called from the tearDown method. To keep this example very simple, this logic has been written speciﬁ cally to handle airports and ﬂ ights. More typically, it would live in the Testcase Superclass, where it could be used\n\nwww.it-ebooks.info\n\n507\n\nAutomated Teardown",
      "content_length": 1744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 571,
      "content": "508\n\nAutomated Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nby all Testcase Classes, and would use a generic object destruction mechanism so that it would not have to care what types of objects it was deleting.\n\nprotected void tearDown() throws Exception { removeObjects(allAirportIds, \"Airport\"); removeObjects(allFlights, \"Flight\"); }\n\npublic void removeObjects(List objectsToDelete, String type) { Iterator i = objectsToDelete.iterator(); while (i.hasNext()) { try { BigDecimal id = (BigDecimal) i.next(); if (\"Airport\"==type) { facade.removeAirport(id); } else { facade.removeFlight(id); } } catch (Exception e) { // do nothing if the remove failed } } }\n\nIf we were tearing down a Shared Fixture, we would annotate our tearDown method with the suitable annotation or attribute (e.g., @afterClass or [TestFixtureTearDown]) or move it to a Setup Decorator.\n\nExample: Automated Exercise Teardown\n\nIf we wanted to take the next step and automatically tear down any objects created within the SUT, we could modify the SUT to use an observable Object Factory. In our test, we would add the following code:\n\nResourceTracker tracker;\n\npublic void setUp() { tracker = new ResourceTracker(); ObjectFactory.addObserver(tracker); }\n\npublic void tearDown() { tracker.cleanup(); ObjectFactory.removeObserver(tracker); }\n\nThis last example assumes that the Automated Teardown logic has been moved into the cleanup method on the ResourceTracker.\n\nwww.it-ebooks.info",
      "content_length": 1457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 572,
      "content": "In-line Teardown\n\nIn-line Teardown\n\nHow do we tear down the Test Fixture?\n\nWe include teardown logic at the end of the Test Method immediately after the result veriﬁ cation.\n\nTestcase Class Testcase Class\n\nFixture Fixture\n\ntest_method_1 test_method_1\n\ntest_method_2 test_method_2\n\nTeardown Teardown\n\nSUT SUT\n\ntest_method_n test_method_n\n\nTeardown Teardown\n\ntearDown tearDown\n\nTeardown Teardown\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ xture is torn down after each test and a new one created for the next test run. This strategy is known as a Fresh Fixture (page 311). Leftover objects and database records, as well as open ﬁ les and connections, can at best cause performance degradations and at worst cause tests to fail or systems to crash. While some of these resources may be cleaned up automatically by garbage collection, others may be left hanging if they are not torn down explicitly.\n\nAt a minimum, we should write In-line Teardown code that cleans up\n\nresources left over after our test.\n\nHow It Works\n\nAs we write a test, we mentally keep track of all objects the test creates that will not be cleaned up automatically. After writing the code to exercise the SUT and verify the outcome, we add logic to the end of the Test Method (page 348) to destroy any objects that will not be cleaned up automatically by the garbage collector. We use the relevant language feature to ensure that the teardown code is run regardless of the outcome of the test.\n\nwww.it-ebooks.info\n\n509\n\nIn-line Teardown",
      "content_length": 1541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 573,
      "content": "510\n\nIn-line Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nWhen to Use It\n\nWe should use some form of teardown logic whenever we have resources that will not be freed automatically after the Test Method is run; we can use In-line Teardown when each test has different objects to clean up. We may discover that objects need to be cleaned up because we have Unrepeatable Tests (see Erratic Test on page 228) or Slow Tests (page 253) caused by the accumulation of detritus from many test runs.\n\nUnlike ﬁ xture setup, the teardown logic is not important from the perspective of Tests as Documentation (see page 23). Use of any form of teardown logic may potentially contribute to High Test Maintenance Cost (page 265) and should be avoided if at all possible. Thus the only real beneﬁ t of including the teardown logic on an in-line basis is that it may make it easier to maintain the teardown logic—a pretty slim beneﬁ t, indeed. It is almost always better to strive for Automated Tear- down (page 503) or to use Implicit Teardown (page 516) if we are using Testcase Class per Fixture (page 631), where all tests in a Testcase Class (page 373) have the same test ﬁ xture.\n\nWe can also use In-line Teardown as a steppingstone to Implicit Teardown, thereby following the principle of “the simplest thing that could possibly work.” First, we learn how to do In-line Teardown for each Test Method; next, we extract the common logic from those tests into the tearDown method. We should not use In-line Teardown if the objects created by the test are subject to automated memory management. In such a case, we should use Garbage- Collected Teardown (page 500) instead because it is much less error-prone and keeps the tests easier to understand and maintain.\n\nImplementation Notes\n\nThe primary consideration in In-line Teardown is ensuring that the teardown code actually runs even when the test is failed by an Assertion Method (page 362) or ends in an error in either the SUT or the Test Method. A secondary consider- ation is ensuring that the teardown code does not introduce additional errors.\n\nThe key to doing In-line Teardown correctly is to use language-level constructs to ensure that the teardown code is run. Most modern languages include some sort of error/exception-handling construct that will attempt the execution of a block of code with the guarantee that a second block of code will be run regard- less of how the ﬁ rst block terminates. In Java, this construct takes the form of a try block with an associated ﬁ nally block.\n\nwww.it-ebooks.info",
      "content_length": 2557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 574,
      "content": "In-line Teardown\n\nVariation: Teardown Guard Clause\n\nTo protect against a failure caused by trying to tear down a resource that doesn’t exist, we can put a “guard clause” around the logic. Its inclusion reduces the likelihood of a test error caused by the teardown logic.\n\nVariation: Delegated Teardown\n\nWe can move much of the teardown logic out of the Test Method by calling a Test Utility Method (page 599). Although this strategy reduces the amount of teardown logic cluttering the test, we still need to place an error-handling con- struct around at least the assertions and the exercising of the SUT to ensure that it gets called. Using Implicit Teardown is almost always a better solution.\n\nVariation: Naive In-line Teardown\n\nNaive In-line Teardown is what we have when we forget to put the equivalent of a try/ﬁ nally block around our test logic to ensure that our teardown logic always executes. It leads to Resource Leakage (see Erratic Test), which in turn may lead to Erratic Tests.\n\nMotivating Example\n\nThe following test creates a persistent object (airport) as part of the ﬁ xture. Because the object is stored in a database, it is not subject to Garbage-Collected Teardown and must be explicitly destroyed. If we do not include teardown logic in the test, each time the test is run it will create another object in the database. This may lead to Unrepeatable Tests unless the test uses Distinct Generated Values (see Generated Value on page 723) to ensure that the created objects do not violate any unique key constraints.\n\npublic void testGetFlightsByOriginAirport_NoFlights_ntd() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); }\n\nwww.it-ebooks.info\n\n511\n\nIn-line Teardown",
      "content_length": 1894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 575,
      "content": "512\n\nIn-line Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nExample: Naive In-line Teardown\n\nIn this naive solution to this problem, we added a line after the assertion to destroy the airport created in the ﬁ xture setup.\n\npublic void testGetFlightsByOriginAirport_NoFlights() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); facade.removeAirport(outboundAirport); }\n\nUnfortunately, this solution isn’t really adequate because the teardown logic won’t be exercised if the SUT encounters an error or if the assertions fail. We could try moving the ﬁ xture cleanup before the assertions but this still wouldn’t address the issue with errors occurring inside the SUT. Clearly, we need a more general solution.\n\nRefactoring Notes\n\nWe need either to place an error-handling construct around the exercising of the SUT and the assertions or to move the teardown code into the tearDown method. Either way, we need to ensure that all the teardown code runs, even if some parts of it fail. This usually involves the judicious use of try/ﬁ nally control structures around each step of the teardown process.\n\nExample: In-line Teardown\n\nIn this Java example, we have introduced a try/ﬁ nally block around the exercise SUT and result veriﬁ cation phases of the test to ensure that our teardown code is run.\n\npublic void testGetFlightsByOriginAirport_NoFlights_td() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); try { // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size());\n\nwww.it-ebooks.info",
      "content_length": 1832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 576,
      "content": "In-line Teardown\n\n} ﬁnally { facade.removeAirport(outboundAirport); } }\n\nNow the exercising of the SUT and the assertions both appear in the try block and the teardown logic is found in the ﬁ nally block. This separation is crucial to making In-line Teardown work properly. We should not include a catch block unless we are writing an Expected Exception Test (see Test Method).\n\nExample: Teardown Guard Clause\n\nHere, we’ve added a Teardown Guard Clause to the teardown code to ensure it isn’t run if the airport doesn’t exist:\n\npublic void testGetFlightsByOriginAirport_NoFlights_TDGC() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); try { // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(outboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { if (outboundAirport!=null) { facade.removeAirport(outboundAirport); } } }\n\nExample: Multiresource In-line Teardown (Java)\n\nIf multiple resources need to be cleaned up in the same test, we must ensure that all the teardown code runs even if some of the teardown statements contain errors. This goal can be accomplished by nesting each subsequent teardown step inside another block of guaranteed code, as in this Java example:\n\npublic void testGetFlightsByOrigin_NoInboundFlight_SMRTD() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expFlightDto = createTestFlight(outboundAirport, inboundAirport);\n\nwww.it-ebooks.info\n\n513\n\nIn-line Teardown",
      "content_length": 1654,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 577,
      "content": "514\n\nIn-line Teardown\n\nChapter 22 Fixture Teardown Patterns\n\n// Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { try { facade.removeFlight(expFlightDto.getFlightNumber()); } ﬁnally { try { facade.removeAirport(inboundAirport); } ﬁnally { facade.removeAirport(outboundAirport); } } } }\n\nThis scheme gets very messy in a hurry if we must clean up more than a few resources. In such a situation, it makes more sense to organize the resources into an array or list and then to iterate over that array or list. At that point we are halfway to implementing Automated Teardown.\n\nExample: Delegated Teardown\n\nWe can also delegate the teardown from within the Test Method if we don’t believe we can come up with a completely generic way cleanup strategy that will work for all tests.\n\npublic void testGetFlightsByOrigin_NoInboundFlight_DTD() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expectedFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expectedFlightDto = createTestFlight( outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { teardownFlightAndAirports( outboundAirport, inboundAirport, expectedFlightDto); } }\n\nwww.it-ebooks.info",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 578,
      "content": "In-line Teardown\n\nprivate void teardownFlightAndAirports( BigDecimal ﬁrstAirport, BigDecimal secondAirport, FlightDto ﬂightDto) throws FlightBookingException { try { facade.removeFlight( ﬂightDto.getFlightNumber() ); } ﬁnally { try { facade.removeAirport(secondAirport); } ﬁnally { facade.removeAirport(ﬁrstAirport); } } }\n\nThe optimizers among us will notice that the two ﬂ ight numbers are actually available as attributes of the ﬂ ightDto. The paranoid will counter that because the teardownFlightAndAirports method could be called before the ﬂ ightDto is constructed, we cannot count on using it to access the Airports. Hence we must pass the Airports in individually. The need to think this way is why a generic Automated Teardown is so attractive; it avoids having to think at all!\n\nwww.it-ebooks.info\n\n515\n\nIn-line Teardown",
      "content_length": 830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 579,
      "content": "516\n\nAlso known as: Hooked Teardown, Framework- Invoked Teardown, Teardown Method\n\nImplicit Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nImplicit Teardown\n\nHow do we tear down the Test Fixture?\n\nThe Test Automation Framework calls our cleanup logic in the tearDown method after every Test Method.\n\nTestcase Class\n\nFixture\n\ntest_1\n\ntest_2\n\nSUT\n\ntest_n\n\nTeardown\n\nTeardown\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ xture is torn down after each test and a new one created for the next test run. This strategy is known as a Fresh Fixture (page 311). Leftover objects and database records, as well as open ﬁ les and connections, can at best cause performance degradations and at worst cause tests to fail or systems to crash. When we can’t take advantage of Garbage-Collected Teardown (page 500) and we have several tests with the same objects to tear down, we can put the teardown logic into a special tearDown method that the Test Automation Framework (page 298) calls after each Test Method (page 348) is run.\n\nHow It Works\n\nAnything that needs to be cleaned up can be freed or destroyed in the ﬁ nal phase of the Four-Phase Test (page 358)—namely, the ﬁ xture teardown phase. Most members of the xUnit family of Test Automation Frameworks support\n\nwww.it-ebooks.info",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 580,
      "content": "Implicit Teardown\n\nthe concept of Implicit Teardown, wherein they call the tearDown method of each Testcase Object (page 382) after the Test Method has been run.\n\nThe tearDown method is called regardless of whether the test passes or fails. This scheme ensures that we have the opportunity to clean up, undisturbed by any failed assertions. Be aware, however, that many members of the xUnit family do not call tearDown if the setUp method raises an error.\n\nWhen to Use It\n\nWe can use Implicit Teardown whenever several tests with the same resources need to be destroyed or freed explicitly after the test has been completed and those resources will not be destroyed or freed automatically. We may discover this require- ment because we have Unrepeatable Tests (see Erratic Test on page 228) or Slow Tests (page 253) caused by the accumulation of detritus from many test runs.\n\nIf the objects created by the test are internal resources and subject to automated memory management, then Garbage-Collected Teardown may eliminate a lot of work for us. If each test has a completely different set of objects to tear down, then In-line Teardown (page 509) may be more appropriate. In many cases, we can completely avoid manually written teardown logic by using Automated Tear- down (page 503).\n\nImplementation Notes\n\nThe teardown logic in the tearDown method is most often created by refactoring from tests that had In-line Teardown. The tearDown method may need to be “ﬂ exible” or “accommodating” for several reasons:\n\nWhen a test fails or when a test error occurs, the Test Method may not\n\nhave created all the ﬁ xture objects.\n\nIf all the Test Methods in the Testcase Class (page 373) don’t use identical ﬁ xtures,1 there may be different sets of objects to clean up for different tests.\n\nVariation: Teardown Guard Clause\n\nWe can avoid arbitrarily Conditional Test Logic (page 200) if we deal with the case where only a subset of the objects to be torn down are actually present by putting a guard clause (a simple if statement) around each teardown operation\n\n1 That is, they augment the Implicit Teardown with some additional In-line Setup (page 408) or Delegated Setup (page 411).\n\nwww.it-ebooks.info\n\n517\n\nImplicit Teardown",
      "content_length": 2224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 581,
      "content": "518\n\nImplicit Teardown\n\nChapter 22 Fixture Teardown Patterns\n\nto guard against the resource not being present. With this technique, a suitably coded tearDown method can tear down various ﬁ xture conﬁ gurations. Contrast this with the setUp method, which can set up only the lowest common denominator ﬁ xture for the Test Methods that share it.\n\nMotivating Example\n\nThe following test creates several standard objects during ﬁ xture setup. Because the objects are persisted in a database, they must be cleaned up explicitly after every test. Each test (only one of several is shown here) contains the same in-line teardown logic to delete the objects.\n\npublic void testGetFlightsByOrigin_NoInboundFlight_SMRTD() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expFlightDto = createTestFlight(outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { try { facade.removeFlight(expFlightDto.getFlightNumber()); } ﬁnally { try { facade.removeAirport(inboundAirport); } ﬁnally { facade.removeAirport(outboundAirport); } } } }\n\nThere is enough Test Code Duplication (page 213) here to warrant converting these tests to Implicit Teardown.\n\nRefactoring Notes\n\nFirst, we ﬁ nd the most representative example of teardown in all the tests. Next, we do an Extract Method [Fowler] refactoring on that code and call the resulting method tearDown. Finally, we delete the teardown logic in each of the other tests.\n\nwww.it-ebooks.info",
      "content_length": 1722,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 582,
      "content": "Implicit Teardown\n\nWe may need to introduce Teardown Guard Clauses around any teardown logic that may not be needed in every test. We should also surround each teardown attempt with a try/ﬁ nally block to ensure that the remaining teardown logic executes even if an earlier attempt fails.\n\nExample: Implicit Teardown\n\nThis example shows the same tests with the teardown logic removed to the tearDown method. Note how much smaller the tests have become.\n\nBigDecimal outboundAirport; BigDecimal inboundAirport; FlightDto expFlightDto;\n\npublic void testGetFlightsByAirport_NoInboundFlights_NIT() throws Exception { // Fixture Setup outboundAirport = createTestAirport(\"1OF\"); inboundAirport = createTestAirport(\"1IF\"); expFlightDto = createTestFlight( outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); }\n\nprotected void tearDown() throws Exception { try { facade.removeFlight( expFlightDto.getFlightNumber() ); } ﬁnally { try { facade.removeAirport(inboundAirport); } ﬁnally { facade.removeAirport(outboundAirport); } } }\n\nNote that there is no try/ﬁ nally block around the exercising of the SUT and the assertions. This structure helps the test reader understand that this is not an Expected Exception Test (see Test Method). Also, we didn’t need to put a Guard Clause [SBPP] in front of each operation because the try/ﬁ nally block ensures that a failure is noncatastrophic; thus there is no real harm in trying to perform the operation. We did have to convert our ﬁ xture holding local variables into instance variables to allow the tearDown method to access the ﬁ xture.\n\nwww.it-ebooks.info\n\n519\n\nImplicit Teardown",
      "content_length": 1759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 583,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 584,
      "content": "Chapter 23\n\nTest Double Patterns\n\nPatterns in This Chapter\n\nTest Double. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522\n\nTest Double Usage\n\nTest Stub. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529\n\nTest Spy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538\n\nMock Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544\n\nFake Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551\n\nTest Double Construction\n\nConﬁ gurable Test Double. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558\n\nHard-Coded Test Double . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568\n\nTest-Speciﬁ c Subclass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n\n521\n\nwww.it-ebooks.info\n\nTest Double Patterns",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 585,
      "content": "522\n\nAlso known as: Imposter\n\nTest Double\n\nChapter 23 Test Double Patterns\n\nTest Double\n\nHow can we verify logic independently when code it depends on is unusable? How can we avoid Slow Tests?\n\nWe replace a component on which the SUT depends with a “test-speciﬁ c equivalent.”\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nTest Test Double Double\n\nExercise Exercise\n\nSUT SUT\n\nVerify Verify\n\nTeardown Teardown\n\nSometimes it is just plain hard to test the SUT because it depends on other components that cannot be used in the test environment. Such a situation may arise because those components aren’t available, because they will not return the results needed for the test, or because executing them would have unde- sirable side effects. In other cases, our test strategy requires us to have more control over or visibility of the internal behavior of the SUT.\n\nWhen we are writing a test in which we cannot (or choose not to) use a real depended-on component (DOC), we can replace it with a Test Double. The Test Double doesn’t have to behave exactly like the real DOC; it merely has to provide the same API as the real DOC so that the SUT thinks it is the real one!\n\nwww.it-ebooks.info",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 586,
      "content": "Test Double\n\nHow It Works\n\nWhen the producers of a movie want to ﬁ lm something that is potentially risky or dangerous for the leading actor to carry out, they hire a “stunt double” to take the place of the actor in the scene. The stunt double is a highly trained individual who is capable of meeting the speciﬁ c requirements of the scene. The stunt double may not be able to act, but he or she knows how to fall from great heights, crash a car, or do whatever the scene calls for. How closely the stunt double needs to resemble the actor depends on the nature of the scene. Usually, things can be arranged such that someone who vaguely resembles the actor in stature can take the actor’s place.\n\nFor testing purposes, we can replace the real DOC (not the SUT!) with our equivalent of the “stunt double”: the Test Double. During the ﬁ xture setup phase of our Four-Phase Test (page 358), we replace the real DOC with our Test Double. Depending on the kind of test we are executing, we may hard-code the behavior of the Test Double or we may conﬁ gure it during the setup phase. When the SUT interacts with the Test Double, it won’t be aware that it isn’t talking to the real McCoy, but we will have achieved our goal of making impossible tests possible.\n\nRegardless of which variation of Test Double we choose to use, we must keep in mind that we don’t need to implement the entire interface of the DOC. Instead, we provide only the functionality needed for our particular test. We can even build different Test Doubles for different tests that involve the same DOC.\n\nWhen to Use It\n\nWe might want to use some sort of Test Double during our tests in the following circumstances:\n\nIf we have an Untested Requirement (see Production Bugs on page 268) because neither the SUT nor its DOCs provide an observation point for the SUT’s indirect output that we need to verify using Behavior Veriﬁ - cation (page 468)\n\nIf we have Untested Code (see Production Bugs) and a DOC does not provide the control point to allow us to exercise the SUT with the nec- essary indirect inputs\n\nIf we have Slow Tests (page 253) and we want to be able to run our\n\ntests more quickly and hence more often\n\nEach of these scenarios can be addressed in some way by using a Test Double. Of course, we have to be careful when using Test Doubles because we are testing\n\nwww.it-ebooks.info\n\n523\n\nTest Double",
      "content_length": 2376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 587,
      "content": "524\n\nTest Double\n\nChapter 23 Test Double Patterns\n\nthe SUT in a different conﬁ guration from the one that will be used in production. For this reason, we really should have at least one test that veriﬁ es the SUT works without a Test Double. We need to be careful that we don’t replace the parts of the SUT that we are trying to verify because that practice can result in tests that test the wrong software! Also, excessive use of Test Doubles can result in Fragile Tests (page 239) as a result of Overspeciﬁ ed Software.\n\nTest Doubles come in several major ﬂ avors, as summarized in Figure 23.1. The implementation variations of these patterns are described in more detail in the corresponding pattern write-ups.\n\nTest Test Double Double\n\nDummy Dummy Object Object\n\nTest Test Stub Stub\n\nTest Test Spy Spy\n\nMock Mock Object Object\n\nFake Fake Object Object\n\nFigure 23.1 Types of Test Doubles. Dummy Objects are really an alternative to the value patterns. Test Stubs are used to verify indirect inputs; Test Spies and Mock Objects are used to verify indirect outputs. Fake objects provide an alternative implementation.\n\nThese variations are classiﬁ ed based on how/why we use the Test Double. We will deal with variations around how we build the Test Doubles in the “Imple- mentation” section.\n\nVariation: Test Stub\n\nWe use a Test Stub (page 529) to replace a real component on which the SUT depends so that the test has a control point for the indirect inputs of the SUT. Its inclusion allows the test to force the SUT down paths it might not otherwise execute. We can further classify Test Stubs by the kind of indirect inputs they are used to inject into the SUT. A Responder (see Test Stub) injects valid values, while a Saboteur (see Test Stub) injects errors or exceptions.\n\nSome people use the term “test stub” to mean a temporary implementation that is used only until the real object or procedure becomes available. I prefer to call this usage a Temporary Test Stub (see Test Stub) to avoid confusion.\n\nwww.it-ebooks.info",
      "content_length": 2030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 588,
      "content": "Test Double\n\nVariation: Test Spy\n\nWe can use a more capable version of a Test Stub, the Test Spy (page 538), as an observation point for the indirect outputs of the SUT. Like a Test Stub, a Test Spy may need to provide values to the SUT in response to method calls. The Test Spy, however, also captures the indirect outputs of the SUT as it is exercised and saves them for later veriﬁ cation by the test. Thus, in many ways, the Test Spy is “just a” Test Stub with some recording capability. While a Test Spy is used for the same fundamental purpose as a Mock Object (page 544), the style of test we write using a Test Spy looks much more like a test written with a Test Stub.\n\nVariation: Mock Object\n\nWe can use a Mock Object as an observation point to verify the indirect outputs of the SUT as it is exercised. Typically, the Mock Object also includes the func- tionality of a Test Stub in that it must return values to the SUT if it hasn’t already failed the tests but the emphasis1 is on the veriﬁ cation of the indirect outputs. Therefore, a Mock Object is a lot more than just a Test Stub plus assertions: It is used in a fundamentally different way.\n\nVariation: Fake Object\n\nWe use a Fake Object (page 551) to replace the functionality of a real DOC in a test for reasons other than veriﬁ cation of indirect inputs and outputs of the SUT. Typically, a Fake Object implements the same functionality as the real DOC but in a much simpler way. While a Fake Object is typically built speciﬁ cally for testing, the test does not use it as either a control point or an observation point.\n\nThe most common reason for using a Fake Object is that the real DOC is not available yet, is too slow, or cannot be used in the test environment because of deleterious side effects. The sidebar “Faster Tests Without Shared Fixtures” (page 319) describes how we encapsulated all database access behind a persistence layer interface and then replaced the database with in-memory hash tables and made our tests run 50 times faster. Chapter 6, Test Automation Strategy, and Chapter 11, Using Test Doubles, provide an overview of the vari- ous techniques available for making our SUT easier to test.\n\n1 My mother grew up in Hungary and has retained a part of her Hungarian accent—think Zsa Zsa Gabor—all her life. She says, “It is important to put the emphasis on the right syllable.”\n\nwww.it-ebooks.info\n\n525\n\nTest Double",
      "content_length": 2407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 589,
      "content": "526\n\nTest Double\n\nChapter 23 Test Double Patterns\n\nVariation: Dummy Object\n\nSome method signatures of the SUT may require objects as parameters. If neither the test nor the SUT cares about these objects, we may choose to pass in a Dummy Object (page 728), which may be as simple as a null object ref- erence, an instance of the Object class, or an instance of a Pseudo-Object (see Hard-Coded Test Double on page 568). In this sense, a Dummy Object isn’t really a Test Double per se but rather an alternative to the value patterns Literal Value (page 714), Derived Value (page 718), and Generated Value (page 723).\n\nVariation: Procedural Test Stub\n\nA Test Double implemented in a procedural programming language is often called a “test stub,” but I prefer to call it a Procedural Test Stub (see Test Stub) to distinguish this usage from the modern Test Stub variation of Test Doubles. Typically, we use a Procedural Test Stub to allow testing/debugging to proceed while waiting for other code to become available. It is rare for these objects to be “swapped in” at runtime but sometimes we make the code conditional on a “Debugging” ﬂ ag—a form of Test Logic in Production (page 217).\n\nImplementation Notes\n\nSeveral considerations must be taken into account when we are building the Test Double (Figure 23.2):\n\nWhether the Test Double should be speciﬁ c to a single test or reusable\n\nacross many tests\n\nWhether the Test Double should exist in code or be generated on-the-ﬂ y\n\nHow we tell the SUT to use the Test Double (installation)\n\nThe ﬁ rst and last points are addressed here. The discussion of Test Double gen- eration is left to the section on Conﬁ gurable Test Doubles.\n\nBecause the techniques for building Test Doubles are pretty much independent of their behavior (e.g., they apply to both Test Stubs and Mock Objects), I’ve chosen to split out the descriptions of the various ways we can build Hard-Coded Test Doubles and Conﬁ gurable Test Doubles (page 558) into separate patterns.\n\nwww.it-ebooks.info",
      "content_length": 2011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 590,
      "content": "Test Double\n\nTest Test Double Double\n\nDummy Dummy Object Object\n\nTest Test Stub Stub\n\nTest Test Spy Spy\n\nMock Mock Object Object\n\nFake Fake Object Object\n\nConfigurable Configurable Test Double Test Double\n\nHard-Coded Hard-Coded Test Double Test Double\n\nFigure 23.2 Types of Test Doubles with implementation choices. Only Test Stubs, Test Spies, and Mock Objects need to be hard-coded or conﬁ gured by the test. Dummy Objects have no implementation; Fake Objects are installed but not controlled by the test.\n\nVariation: Unconﬁ gurable Test Doubles\n\nNeither Dummy Objects nor Fake Objects need to be conﬁ gured, each for their own reason. Dummies should never be used by the receiver so they need no “real” implementation. Fake Objects, by contrast, need a “real” implementa- tion but one that is much simpler or “lighter” than the object that they replace. Therefore, neither the test nor the test automater will need to conﬁ gure “canned” responses or expectations; we just install the Test Double and let the SUT use it as if it were real.\n\nVariation: Hard-Coded Test Double\n\nWhen we plan to use a speciﬁ c Test Double in only a single test, it is often sim- plest to just hard-code the Test Double to return speciﬁ c values (for Test Stubs) or expect speciﬁ c method calls (Mock Objects). Hard-Coded Test Doubles are typically hand-built by the test automater. They come in several forms, including the Self Shunt (see Hard-Coded Test Double), where the Testcase Class (page 373) acts as the Test Double; the Anonymous Inner Test Double (see Hard-Coded Test Double), where language features are used to create the Test Double inside the Test Method (page 348); and the Test Double implemented as separate Test Double Class (see Hard-Coded Test Double). Each of these options is discussed in more detail in Hard-Coded Test Double.\n\nwww.it-ebooks.info\n\n527\n\nTest Double",
      "content_length": 1870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 591,
      "content": "528\n\nTest Double\n\nChapter 23 Test Double Patterns\n\nVariation: Conﬁ gurable Test Double\n\nWhen we want to use the same Test Double implementation in many tests, we will typically prefer to use a Conﬁ gurable Test Double. Although the test auto- mater can manually build these objects, many members of the xUnit family have reusable toolkits available for generating Conﬁ gurable Test Doubles.\n\nInstalling the Test Double\n\nBefore we can exercise the SUT, we must tell it to use the Test Double instead of the object that the Test Double replaces. We can use any of the substitutable dependency patterns to install the Test Double during the ﬁ xture setup phase of our Four-Phase Test. Conﬁ gurable Test Doubles need to be conﬁ gured before we exercise the SUT, and we typically perform this conﬁ guration before we install them.\n\nExample: Test Double\n\nBecause there are a wide variety of reasons for using the variations of Test Dou- bles, it is difﬁ cult to provide a single example that characterizes the motivation behind each style. Please refer to the examples in each of the more detailed pat- terns referenced earlier.\n\nwww.it-ebooks.info",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 592,
      "content": "Test Stub\n\nTest Stub\n\nHow can we verify logic independently when it depends on indirect inputs from other software components?\n\nWe replace a real object with a test-speciﬁ c object that feeds the desired indirect inputs into the system under test.\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nCreation Creation\n\nTest Test Stub Stub\n\nInstallation Installation\n\nExercise Exercise\n\nSUT SUT\n\nReturn Return Values Values\n\nVerify Verify\n\nIndirect Indirect Input Input\n\nTeardown Teardown\n\nIn many circumstances, the environment or context in which the SUT operates very much inﬂ uences the behavior of the SUT. To get adequate control over the indirect inputs of the SUT, we may have to replace some of the context with something we can control—namely, a Test Stub.\n\nHow It Works\n\nFirst, we deﬁ ne a test-speciﬁ c implementation of an interface on which the SUT depends. This implementation is conﬁ gured to respond to calls from the SUT with the values (or exceptions) that will exercise the Untested Code (see Production Bugs on page 268) within the SUT. Before exercising the SUT, we install the Test Stub so that the SUT uses it instead of the real implementation. When called by\n\nwww.it-ebooks.info\n\n529\n\nAlso known as: Stub\n\nTest Stub",
      "content_length": 1228,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 593,
      "content": "530\n\nTest Stub\n\nChapter 23 Test Double Patterns\n\nthe SUT during test execution, the Test Stub returns the previously deﬁ ned values. The test can then verify the expected outcome in the normal way.\n\nWhen to Use It\n\nA key indication for using a Test Stub is having Untested Code caused by our inability to control the indirect inputs of the SUT. We can use a Test Stub as a control point that allows us to control the behavior of the SUT with vari- ous indirect inputs and we have no need to verify the indirect outputs. We can also use a Test Stub to inject values that allow us to get past a particular point in the software where the SUT calls software that is unavailable in our test environment.\n\nIf we do need an observation point that allows us to verify the indirect out- puts of the SUT, we should consider using a Mock Object (page 544) or a Test Spy (page 538). Of course, we must have a way of installing a Test Double (page 522) into the SUT to be able to use any form of Test Double.\n\nVariation: Responder\n\nA Test Stub that is used to inject valid indirect inputs into the SUT so that it can go about its business is called a Responder. Responders are commonly used in “happy path” testing when the real component is uncontrollable, is not yet available, or is unusable in the development environment. The tests will invari- ably be Simple Success Tests (see Test Method on page 348).\n\nVariation: Saboteur\n\nA Test Stub that is used to inject invalid indirect inputs into the SUT is often called a Saboteur because its purpose is to derail whatever the SUT is trying to do so that we can see how the SUT copes under these circumstances. The “derailment” might be caused by returning unexpected values or objects, or it might result from raising an exception or causing a runtime error. Each test may be either a Simple Success Test or an Expected Exception Test (see Test Method), depending on how the SUT is expected to behave in response to the indirect input.\n\nVariation: Temporary Test Stub\n\nA Temporary Test Stub stands in for a DOC that is not yet available. This kind of Test Stub typically consists of an empty shell of a real class with hard-coded return statements. As soon as the real DOC is available, it replaces the Tempo- rary Test Stub. Test-driven development often requires us to create Temporary\n\nwww.it-ebooks.info",
      "content_length": 2346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 594,
      "content": "Test Stub\n\nTest Stubs as we write code from the outside in; these shells evolve into the real classes as we add code to them. In need-driven development, we tend to use Mock Objects because we want to verify that the SUT calls the right methods on the Temporary Test Stub; in addition, we typically continue using the Mock Object even after the real DOC becomes available.\n\nVariation: Procedural Test Stub\n\nA Procedural Test Stub is a Test Stub written in a procedural programming lan- guage. It is particularly challenging to create in procedural programming languages that do not support procedure variables (also known as function pointers). In most cases, we must put if testing then hooks into the production code (a form of Test Logic in Production; see page 217).\n\nVariation: Entity Chain Snipping\n\nEntity Chain Snipping (see Test Stub on page 529) is a special case of a Responder that is used to replace a complex network of objects with a single Test Stub that pretends to be the network of objects. Its inclusion can make ﬁ x- ture setup go much more quickly (especially when the objects would normally have to be persisted into a database) and can make the tests much easier to understand.\n\nImplementation Notes\n\nWe must be careful when using Test Stubs because we are testing the SUT in a different conﬁ guration from the one that will be used in production. We really should have at least one test that veriﬁ es the SUT works without a Test Stub. A common mistake made by test automaters who are new to stubs is to replace a part of the SUT that they are trying to test. For this reason, it is important to be really clear about what is playing the role of SUT and what is playing the role of test ﬁ xture. Also, note that excessive use of Test Stubs can result in Overspeci- ﬁ ed Software (see Fragile Test on page 239).\n\nTest Stubs may be built in several different ways depending on our speciﬁ c\n\nneeds and the tools we have on hand.\n\nVariation: Hard-Coded Test Stub\n\nA Hard-Coded Test Stub has its responses hard-coded within its program logic. These Test Stubs tend to be purpose-built for a single test or a very small number of tests. See Hard-Coded Test Double (page 568) for more information.\n\nwww.it-ebooks.info\n\n531\n\nTest Stub",
      "content_length": 2251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 595,
      "content": "532\n\nTest Stub\n\nChapter 23 Test Double Patterns\n\nVariation: Conﬁ gurable Test Stub\n\nWhen we want to avoid building a different Hard-Coded Test Stub for each test, we can use a Conﬁ gurable Test Stub (see Conﬁ gurable Test Double on page 558). A test conﬁ gures the Conﬁ gurable Test Stub as part of its ﬁ xture setup phase. Many members of the xUnit family offer tools with which to generate Conﬁ gurable Test Doubles (page 558), including Conﬁ gurable Test Stubs.\n\nMotivating Example\n\nThe following test veriﬁ es the basic functionality of a component that formats an HTML string containing the current time. Unfortunately, it depends on the real system clock so it rarely ever passes!\n\npublic void testDisplayCurrentTime_AtMidnight() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result); }\n\nWe could try to address this problem by making the test calculate the expected results based on the current system time as follows:\n\npublic void testDisplayCurrentTime_whenever() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify outcome Calendar time = new DefaultTimeProvider().getTime(); StringBuffer expectedTime = new StringBuffer(); expectedTime.append(\"<span class=\\\"tinyBoldText\\\">\");\n\nif ((time.get(Calendar.HOUR_OF_DAY) == 0) && (time.get(Calendar.MINUTE) <= 1)) { expectedTime.append( \"Midnight\"); } else if ((time.get(Calendar.HOUR_OF_DAY) == 12) && (time.get(Calendar.MINUTE) == 0)) { // noon expectedTime.append(\"N3oon\"); } else { SimpleDateFormat fr = new SimpleDateFormat(\"h:mm a\"); expectedTime.append(fr.format(time.getTime())); }\n\nwww.it-ebooks.info",
      "content_length": 1855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 596,
      "content": "Test Stub\n\nexpectedTime.append(\"</span>\");\n\nassertEquals( expectedTime, result); }\n\nThis Flexible Test (see Conditional Test Logic on page 200) introduces two prob- lems. First, some test conditions are never exercised. (Do you want to come in to work to run the tests at midnight to prove the software works at midnight?) Second, the test needs to duplicate much of the logic in the SUT to calculate the expected results. How do we prove the logic is actually correct?\n\nRefactoring Notes\n\nWe can achieve proper veriﬁ cation of the indirect inputs by getting control of the time. To do so, we use the Replace Dependency with Test Double (page 522) refactoring to replace the real system clock (represented here by TimeProvider) with a Virtual Clock [VCTP]. We then implement it as a Test Stub that is conﬁ g- ured by the test with the time we want to use as the indirect input to the SUT.\n\nExample: Responder (as Hand-Coded Test Stub)\n\nThe following test veriﬁ es one of the happy path test conditions using a Responder to get control over the indirect inputs of the SUT. Based on the time injected into the SUT, the expected result can be hard-coded safely.\n\npublic void testDisplayCurrentTime_AtMidnight() throws Exception { // Fixture setup // Test Double conﬁguration TimeProviderTestStub tpStub = new TimeProviderTestStub(); tpStub.setHours(0); tpStub.setMinutes(0); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Test Double installation sut.setTimeProvider(tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nwww.it-ebooks.info\n\n533\n\nTest Stub",
      "content_length": 1738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 597,
      "content": "534\n\nTest Stub\n\nChapter 23 Test Double Patterns\n\nThis test makes use of the following hand-coded configurable Test Stub implementation:\n\nprivate Calendar myTime = new GregorianCalendar(); /** * The complete constructor for the TimeProviderTestStub * @param hours speciﬁes the hours using a 24-hour clock * (e.g., 10 = 10 AM, 12 = noon, 22 = 10 PM, 0 = midnight) * @param minutes speciﬁes the minutes after the hour * (e.g., 0 = exactly on the hour, 1 = 1 min after the hour) */ public TimeProviderTestStub(int hours, int minutes) { setTime(hours, minutes); }\n\npublic void setTime(int hours, int minutes) { setHours(hours); setMinutes(minutes); }\n\n// Conﬁguration interface public void setHours(int hours) { // 0 is midnight; 12 is noon myTime.set(Calendar.HOUR_OF_DAY, hours); }\n\npublic void setMinutes(int minutes) { myTime.set(Calendar.MINUTE, minutes); } // Interface used by SUT public Calendar getTime() { // @return the last time that was set return myTime; }\n\nExample: Responder (Dynamically Generated)\n\nHere’s the same test coded using the JMock Conﬁ gurable Test Double frame- work:\n\npublic void testDisplayCurrentTime_AtMidnight_JM() throws Exception { // Fixture setup TimeDisplay sut = new TimeDisplay(); // Test Double conﬁguration Mock tpStub = mock(TimeProvider.class); Calendar midnight = makeTime(0,0); tpStub.stubs().method(\"getTime\"). withNoArguments(). will(returnValue(midnight));\n\nwww.it-ebooks.info",
      "content_length": 1421,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 598,
      "content": "Test Stub\n\n// Test Double installation sut.setTimeProvider((TimeProvider) tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nThere is no Test Stub implementation to examine for this test because the JMock framework implements the Test Stub using reﬂ ection. Thus we had to write a Test Utility Method (page 599) called makeTime that contains the logic to construct the Calendar object to be returned. In the hand-coded Test Stub, this logic appeared inside the getTime method.\n\nExample: Saboteur (as Anonymous Inner Class)\n\nThe following test uses a Saboteur to inject invalid indirect inputs into the SUT so we can see how the SUT copes under these circumstances.\n\npublic void testDisplayCurrentTime_exception() throws Exception { // Fixture setup // Deﬁne and instantiate Test Stub TimeProvider testStub = new TimeProvider() { // Anonymous inner Test Stub public Calendar getTime() throws TimeProviderEx { throw new TimeProviderEx(\"Sample\"); } }; // Instantiate SUT TimeDisplay sut = new TimeDisplay(); sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"error\\\">Invalid Time</span>\"; assertEquals(\"Exception\", expectedTimeString, result); }\n\nIn this case, we used an Inner Test Double (see Hard-Coded Test Double) to throw an exception that we expect the SUT to handle gracefully. One interest- ing thing about this test is that it uses the Simple Success Test method template rather than the Expected Exception Test template, even though we are injecting an exception as the indirect input. The rationale behind this choice is that we are expecting the SUT to catch the exception and change the string formatting; we are not expecting the SUT to throw an exception.\n\nwww.it-ebooks.info\n\n535\n\nTest Stub",
      "content_length": 1996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 599,
      "content": "536\n\nTest Stub\n\nChapter 23 Test Double Patterns\n\nExample: Entity Chain Snipping\n\nIn this example, we are testing the Invoice but require a Customer to instantiate the Invoice. The Customer requires an Address, which in turn requires a City. Thus we ﬁ nd ourselves creating numerous additional objects just to set up the ﬁ xture. Suppose the behavior of the invoice depends on some attribute of the Customer that is calculated from the Address by calling the method get_zone on the Customer.\n\npublic void testInvoice_addLineItem_noECS() { ﬁnal int QUANTITY = 1; Product product = new Product(getUniqueNumberAsString(), getUniqueNumber()); State state = new State(\"West Dakota\", \"WD\"); City city = new City(\"Centreville\", state); Address address = new Address(\"123 Blake St.\", city, \"12345\"); Customer customer= new Customer(getUniqueNumberAsString(), getUniqueNumberAsString(), address); Invoice inv = new Invoice(customer); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); LineItem expItem = new LineItem(inv, product, QUANTITY); assertLineItemsEqual(\"\",expItem, actual); }\n\nIn this test, we want to verify only the behavior of the invoice logic that depends on this zone attribute—not the way this attribute is calculated from the Customer’s address. (There are separate Customer unit tests to verify the zone is calculated correctly.) All of the setup of the address, city, and other information merely distracts the reader.\n\nHere’s the same test using a Test Stub instead of the Customer. Note how much\n\nsimpler the ﬁ xture setup has become as a result of Entity Chain Snipping!\n\npublic void testInvoice_addLineItem_ECS() { ﬁnal int QUANTITY = 1; Product product = new Product(getUniqueNumberAsString(), getUniqueNumber()); Mock customerStub = mock(ICustomer.class); customerStub.stubs().method(\"getZone\").will(returnValue(ZONE_3)); Invoice inv = new Invoice((ICustomer)customerStub.proxy()); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify\n\nwww.it-ebooks.info",
      "content_length": 2127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 600,
      "content": "Test Stub\n\nList lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); LineItem expItem = new LineItem(inv, product, QUANTITY); assertLineItemsEqual(\"\", expItem, actual); }\n\nWe have used JMock to stub out the Customer with a customerStub that returns ZONE_3 when getZone is called. This is all we need to verify the Invoice behavior, and we have managed to get rid of all that distracting extra object construction. It is also much clearer from reading this test that invoicing behavior depends only on the value returned by get_zone and not any other attributes of the Customer or Address.\n\nFurther Reading\n\nAlmost every book on automated testing using xUnit has something to say about Test Stubs, so I won’t list those resources here. As you are reading other books, however, keep in mind that the term Test Stub is often used to refer to a Mock Object. Mocks, Fakes, Stubs, and Dummies (in Appendix B) contains a more thorough comparison of the terminology used in various books and articles.\n\nSven Gorts describes a number of different ways we can use a Test Stub [UTwHCM]. I have adopted many of his names and adapted a few to better ﬁ t into this pattern language. Paolo Perrotta wrote a pattern describing a com- mon example of a Responder called Virtual Clock. He uses a Test Stub as a Decorator [GOF] for the real system clock that allows the time to be “frozen” or resumed. Of course, we could use a Hard-Coded Test Stub or a Conﬁ gu- rable Test Stub just as easily for most tests.\n\nwww.it-ebooks.info\n\n537\n\nTest Stub",
      "content_length": 1604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 601,
      "content": "538\n\nAlso known as: Spy, Recording Test Stub\n\nTest Spy\n\nChapter 23 Test Double Patterns\n\nTest Spy\n\nHow do we implement Behavior Veriﬁ cation? How can we verify logic independently when it has indirect outputs to other software components?\n\nWe use a Test Double to capture the indirect output calls made to another component by the SUT for later veriﬁ cation by the test.\n\nFixture Fixture\n\nDOC DOC\n\nCreation Creation\n\nTest Spy Test Spy\n\nSetup Setup\n\nExercise Exercise\n\nInstallation Installation\n\nExercise Exercise\n\nSUT SUT\n\nIndirect Indirect Output Output\n\nIndirect Indirect Outputs Outputs\n\nVerify Verify\n\nTeardown Teardown\n\nIn many circumstances, the environment or context in which the SUT operates very much inﬂ uences the behavior of the SUT. To get adequate visibility of the indirect outputs of the SUT, we may have to replace some of the context with something we can use to capture these outputs of the SUT.\n\nUse of a Test Spy is a simple and intuitive way to implement Behavior Veriﬁ - cation (page 468) via an observation point that exposes the indirect outputs of the SUT so they can be veriﬁ ed.\n\nHow It Works\n\nBefore we exercise the SUT, we install a Test Spy as a stand-in for a DOC used by the SUT. The Test Spy is designed to act as an observation point by recording the method calls made to it by the SUT as it is exercised. During the\n\nwww.it-ebooks.info",
      "content_length": 1372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 602,
      "content": "Test Spy\n\nresult veriﬁ cation phase, the test compares the actual values passed to the Test Spy by the SUT with the values expected by the test.\n\nWhen to Use It\n\nA key indication for using a Test Spy is having an Untested Requirement (see Production Bugs on page 268) caused by an inability to observe the side effects of invoking methods on the SUT. Test Spies are a natural and intuitive way to extend the existing tests to cover these indirect outputs because the calls to the Assertion Methods (page 362) are invoked by the test after the SUT has been exercised just like in “normal” tests. The Test Spy merely acts as the observation point that gives the Test Method (page 348) access to the values recorded during the SUT execution.\n\nWe should use a Test Spy in the following circumstances:\n\nWe are verifying the indirect outputs of the SUT and we cannot predict the values of all attributes of the interactions with the SUT ahead of time.\n\nWe want the assertions to be visible in the test and we don’t think the way in which the Mock Object (page 544) expectations are established is sufﬁ ciently intent-revealing. Test Spy\n\nOur test requires test-speciﬁ c equality (so we cannot use the standard deﬁ nition of equality as implemented in the SUT) and we are using tools that generate the Mock Object but do not give us control over the Assertion Methods being called.\n\nA failed assertion cannot be reported effectively back to the Test Run- ner (page 377). This might occur if the SUT is running inside a contain- er that catches all exceptions and makes it difﬁ cult to report the results or if the logic of the SUT runs in a different thread or process from the test that invokes it. (Both of these cases really beg refactoring to allow us to test the SUT logic directly, but that is the subject of another chapter.)\n\nWe would like to have access to all the outgoing calls of the SUT before\n\nmaking any assertions on them.\n\nIf none of these criteria apply, we may want to consider using a Mock Object. If we are trying to address Untested Code (see Production Bugs) by controlling the indirect inputs of the SUT, a simple Test Stub (page 529) may be all we need.\n\nwww.it-ebooks.info\n\n539",
      "content_length": 2196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 603,
      "content": "540\n\nTest Spy\n\nAlso known as: Loopback\n\nChapter 23 Test Double Patterns\n\nUnlike a Mock Object, a Test Spy does not fail the test at the ﬁ rst deviation from the expected behavior. Thus our tests will be able to include more detailed diagnostic information in the Assertion Message (page 370) based on informa- tion gathered after a Mock Object would have failed the test. At the point of test failure, however, only the information within the Test Method itself is avail- able to be used in the calls to the Assertion Methods. If we need to include information that is accessible only while the SUT is being exercised, either we must explicitly capture it within our Test Spy or we must use a Mock Object.\n\nOf course, we won’t be able to use any Test Doubles (page 522) unless the\n\nSUT implements some form of substitutable dependency.\n\nImplementation Notes\n\nThe Test Spy itself can be built as a Hard-Coded Test Double (page 568) or as a Conﬁ gurable Test Double (page 558). Because detailed examples appear in the discussion of those patterns, only a quick summary is provided here. Likewise, we can use any of the substitutable dependency patterns to install the Test Spy before we exercise the SUT.\n\nThe key characteristic in how a test uses a Test Spy relates to the fact that as- sertions are made from within the Test Method. Therefore, the test must recover the indirect outputs captured by the Test Spy before it can make its assertions, which can be done in several ways.\n\nVariation: Retrieval Interface\n\nWe can deﬁ ne the Test Spy as a separate class with a Retrieval Interface that exposes the recorded information. The Test Method installs the Test Spy instead of the normal DOC as part of the ﬁ xture setup phase of the test. After the test has exercised the SUT, it uses the Retrieval Interface to retrieve the actual indi- rect outputs of the SUT from the Test Spy and then calls Assertion Methods with those outputs as arguments.\n\nVariation: Self Shunt\n\nWe can collapse the Test Spy and the Testcase Class (page 373) into a single object called a Self Shunt. The Test Method installs itself, the Testcase Object (page 382), as the DOC into the SUT. Whenever the SUT delegates to the DOC, it is actually calling methods on the Testcase Object, which implements the methods by saving the actual values into instance variables that can be accessed by the Test Method. The methods could also make assertions in the Test Spy methods, in which case the Self Shunt is a variation on a Mock Object rather than a Test Spy. In stati- cally typed languages, the Testcase Class must implement the outgoing interface\n\nwww.it-ebooks.info",
      "content_length": 2640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 604,
      "content": "Test Spy\n\n(the observation point) on which the SUT depends so that the Testcase Class is type-compatible with the variables that are used to hold the DOC.\n\nVariation: Inner Test Double\n\nA popular way to implement the Test Spy as a Hard-Coded Test Double is to code it as an anonymous inner class or block closure within the Test Method and to have this class or block save the actual values into instance or local variables that are accessible by the Test Method. This variation is really another way to implement a Self Shunt (see Hard-Coded Test Double).\n\nVariation: Indirect Output Registry\n\nYet another possibility is to have the Test Spy store the actual parameters in a well-known place where the Test Method can access them. For example, the Test Spy could save those values in a ﬁ le or in a Registry [PEAA] object.\n\nMotivating Example\n\nThe following test veriﬁ es the basic functionality of removing a ﬂ ight but does not verify the indirect outputs of the SUT—namely, the fact that the SUT is expected to log each time a ﬂ ight is removed along with the date/time and user- name of the requester.\n\npublic void testRemoveFlight() throws Exception { // setup FlightDto expectedFlightDto = createARegisteredFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight should not exist after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); }\n\nRefactoring Notes\n\nWe can add veriﬁ cation of indirect outputs to existing tests using a Replace Dependency with Test Double (page 522) refactoring. It involves adding code to the ﬁ xture setup logic of the tests to create the Test Spy, conﬁ guring the Test Spy with any values it needs to return, and installing it. At the end of the test, we add assertions comparing the expected method names and arguments of the\n\nwww.it-ebooks.info\n\n541\n\nTest Spy",
      "content_length": 1941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 605,
      "content": "542\n\nTest Spy\n\nChapter 23 Test Double Patterns\n\nindirect outputs with the actual values retrieved from the Test Spy using the Retrieval Interface.\n\nExample: Test Spy\n\nIn this improved version of the test, logSpy is our Test Spy. The statement facade. setAuditLog(logSpy) installs the Test Spy using the Setter Injection pattern (see Dependency Injection on page 678). The methods getDate, getActionCode, and so on are the Retrieval Interface used to access the actual arguments of the call to the logger.\n\npublic void testRemoveFlightLogging_recordingTestStub() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnUnregFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // Test Double setup AuditLogSpy logSpy = new AuditLogSpy(); facade.setAuditLog(logSpy); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); assertEquals(\"number of calls\", 1, logSpy.getNumberOfCalls()); assertEquals(\"action code\", Helper.REMOVE_FLIGHT_ACTION_CODE, logSpy.getActionCode()); assertEquals(\"date\", helper.getTodaysDateWithoutTime(), logSpy.getDate()); assertEquals(\"user\", Helper.TEST_USER_NAME, logSpy.getUser()); assertEquals(\"detail\", expectedFlightDto.getFlightNumber(), logSpy.getDetail()); }\n\nThis test depends on the following deﬁ nition of the Test Spy:\n\npublic class AuditLogSpy implements AuditLog { // Fields into which we record actual usage information private Date date; private String user; private String actionCode; private Object detail; private int numberOfCalls = 0;\n\nwww.it-ebooks.info",
      "content_length": 1678,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 606,
      "content": "Test Spy\n\n// Recording implementation of real AuditLog interface public void logMessage(Date date, String user, String actionCode, Object detail) { this.date = date; this.user = user; this.actionCode = actionCode; this.detail = detail;\n\nnumberOfCalls++; }\n\n// Retrieval Interface public int getNumberOfCalls() { return numberOfCalls; } public Date getDate() { return date; } public String getUser() { return user; } public String getActionCode() { return actionCode; } public Object getDetail() { return detail; } }\n\nOf course, we could have implemented the Retrieval Interface by making the various ﬁ elds of our spy public and thereby avoided the need for accessor methods. Please refer to the examples in Hard-Coded Test Double for other implementation options.\n\nwww.it-ebooks.info\n\n543\n\nTest Spy",
      "content_length": 799,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 607,
      "content": "544\n\nMock Object\n\nChapter 23 Test Double Patterns\n\nMock Object\n\nHow do we implement Behavior Veriﬁ cation for indirect outputs of the SUT? How can we verify logic independently when it depends on indirect inputs from other software components?\n\nWe replace an object on which the SUT depends on with a test-speciﬁ c object that veriﬁ es it is being used correctly by the SUT.\n\nSetup Setup\n\nCreation Creation\n\nFixture Fixture\n\nMock Mock Object Object\n\nDOC DOC\n\nInstallation Installation\n\nExpectations Expectations\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nSUT SUT\n\nFinal Verification Final Verification\n\nIndirect Indirect Output Output\n\ny y f f i i r r e e V V\n\nTeardown Teardown\n\nIn many circumstances, the environment or context in which the SUT operates very much inﬂ uences the behavior of the SUT. In other cases, we must peer “inside”2 the SUT to determine whether the expected behavior has occurred. A Mock Object is a powerful way to implement Behavior Veriﬁ cation (page 468) while avoiding Test Code Duplication (page 213) between similar tests. It works by delegating the job of verifying the indirect outputs of the SUT entirely to a Test Double (page 522).\n\n2 Technically, the SUT is whatever software we are testing and doesn’t include anything it depends on; thus “inside” is somewhat of a misnomer. It is better to think of the DOC that is the destination of the indirect outputs as being “behind” the SUT and part of the ﬁ xture.\n\nwww.it-ebooks.info",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 608,
      "content": "Mock Object\n\nHow It Works\n\nFirst, we deﬁ ne a Mock Object that implements the same interface as an object on which the SUT depends. Then, during the test, we conﬁ gure the Mock Object with the values with which it should respond to the SUT and the method calls (complete with expected arguments) it should expect from the SUT. Before exer- cising the SUT, we install the Mock Object so that the SUT uses it instead of the real implementation. When called during SUT execution, the Mock Object com- pares the actual arguments received with the expected arguments using Equality Assertions (see Assertion Method on page 362) and fails the test if they don’t match. The test need not make any assertions at all!\n\nWhen to Use It\n\nWe can use a Mock Object as an observation point when we need to do Behavior Veriﬁ cation to avoid having an Untested Requirement (see Production Bugs on page 268) caused by our inability to observe the side effects of invoking meth- ods on the SUT. This pattern is commonly used during endoscopic testing [ET] or need-driven development [MRNO]. Although we don’t need to use a Mock Object when we are doing State Veriﬁ cation (page 462), we might use a Test Stub (page 529) or Fake Object (page 551). Note that test drivers have found other uses for the Mock Object toolkits, but many of these are actually examples of using a Test Stub rather than a Mock Object.\n\nTo use a Mock Object, we must be able to predict the values of most or all arguments of the method calls before we exercise the SUT. We should not use a Mock Object if a failed assertion cannot be reported back to the Test Runner (page 377) effectively. This may be the case if the SUT runs inside a container that catches and eats all exceptions. In these circumstances, we may be better off using a Test Spy (page 538) instead.\n\nMock Objects (especially those created using dynamic mocking tools) often use the equals methods of the various objects being compared. If our test-speciﬁ c equality differs from how the SUT would interpret equals, we may not be able to use a Mock Object or we may be forced to add an equals method where we didn’t need one. This smell is called Equality Pollution (see Test Logic in Production on page 217). Some implementations of Mock Objects avoid this problem by allow- ing us to specify the “comparator” to be used in the Equality Assertions.\n\nMock Objects can be either “strict” or “lenient” (sometimes called “nice”). A “strict” Mock Object fails the test if the calls are received in a different order than was speciﬁ ed when the Mock Object was programmed. A “lenient” Mock Object tolerates out-of-order calls.\n\nwww.it-ebooks.info\n\n545\n\nMock Object",
      "content_length": 2681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 609,
      "content": "546\n\nMock Object\n\nChapter 23 Test Double Patterns\n\nImplementation Notes\n\nTests written using Mock Objects look different from more traditional tests be- cause all the expected behavior must be speciﬁ ed before the SUT is exercised. This makes the tests harder to write and to understand for test automation neophytes. This factor may be enough to cause us to prefer writing our tests using Test Spies. The standard Four-Phase Test (page 358) is altered somewhat when we use Mock Objects. In particular, the ﬁ xture setup phase of the test is broken down into three speciﬁ c activities and the result veriﬁ cation phase more or less dis- appears, except for the possible presence of a call to the “ﬁ nal veriﬁ cation” method at the end of the test.\n\nFixture setup:\n\nTest constructs Mock Object.\n\nTest conﬁ gures Mock Object. This step is omitted for Hard-Coded Test\n\nDoubles (page 568).\n\nTest installs Mock Object into SUT.\n\nExercise SUT:\n\nSUT calls Mock Object; Mock Object does assertions.\n\nResult veriﬁ cation:\n\nTest calls “ﬁ nal veriﬁ cation” method.\n\nFixture teardown:\n\nNo impact.\n\nLet’s examine these differences a bit more closely:\n\nConstruction\n\nAs part of the ﬁ xture setup phase of our Four-Phase Test, we must construct the Mock Object that we will use to replace the substitutable dependency. Depend- ing on which tools are available in our programming language, we can either build the Mock Object class manually, use a code generator to create a Mock Object class, or use a dynamically generated Mock Object.\n\nConﬁ guration with Expected Values\n\nBecause the Mock Object toolkits available in many members of the xUnit family typically create Conﬁ gurable Mock Objects (page 544), we need\n\nwww.it-ebooks.info",
      "content_length": 1720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 610,
      "content": "Mock Object\n\nto conﬁ gure the Mock Object with the expected method calls (and their parameters) as well as the values to be returned by any functions. (Some Mock Object frameworks allow us to disable veriﬁ cation of the method calls or just their parameters.) We typically perform this conﬁ guration before we install the Test Double.\n\nThis step is not needed when we are using a Hard-Coded Test Double such\n\nas an Inner Test Double (see Hard-Coded Test Double).\n\nInstallation\n\nOf course, we must have a way of installing a Test Double into the SUT to be able to use a Mock Object. We can use whichever substitutable dependency pattern the SUT supports. A common approach in the test-driven development community is Dependency Injection (page 678); more traditional developers may favor Dependency Lookup (page 686).\n\nUsage\n\nWhen the SUT calls the methods of the Mock Object, these methods compare the method call (method name plus arguments) with the expectations. If the method call is unexpected or the arguments are incorrect, the assertion fails the test im- mediately. If the call is expected but came out of sequence, a strict Mock Object fails the test immediately; by contrast, a lenient Mock Object notes that the call was received and carries on. Missed calls are detected when the ﬁ nal veriﬁ cation method is called.\n\nIf the method call has any outgoing parameters or return values, the Mock Object needs to return or update something to allow the SUT to continue executing the test scenario. This behavior may be either hard-coded or conﬁ gured at the same time as the expectations. This behavior is the same as for Test Stubs, except that we typically return happy path values.\n\nFinal Veriﬁ cation\n\nMost of the result veriﬁ cation occurs inside the Mock Object as it is called by the SUT. The Mock Object will fail the test if the methods are called with the wrong arguments or if methods are called unexpectedly. But what happens if the expected method calls are never received by the Mock Object? The Mock Object may have trouble detecting that the test is over and it is time to check for unfulﬁ lled expectations. Therefore, we need to ensure that the ﬁ nal veriﬁ cation method is called. Some Mock Object toolkits have found a way to invoke this\n\nwww.it-ebooks.info\n\n547\n\nMock Object",
      "content_length": 2303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 611,
      "content": "548\n\nMock Object\n\nChapter 23 Test Double Patterns\n\nmethod automatically by including the call in the tearDown method.3 Many other toolkits require us to remember to call the ﬁ nal veriﬁ cation method ourselves.\n\nMotivating Example\n\nThe following test veriﬁ es the basic functionality of creating a ﬂ ight. But it does not verify the indirect outputs of the SUT—namely, the SUT is expected to log each time a ﬂ ight is created along with the date/time and username of the requester.\n\npublic void testRemoveFlight() throws Exception { // setup FlightDto expectedFlightDto = createARegisteredFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight should not exist after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); }\n\nRefactoring Notes\n\nVeriﬁ cation of indirect outputs can be added to existing tests by using a Replace Dependency with Test Double (page 522) refactoring. This involves adding code to the ﬁ xture setup logic of our test to create the Mock Object; conﬁ guring the Mock Object with the expected method calls, arguments, and values to be returned; and installing it using whatever substitutable dependency mechanism is provided by the SUT. At the end of the test, we add a call to the ﬁ nal veriﬁ cation method if our Mock Object framework requires one.\n\nExample: Mock Object (Hand-Coded)\n\nIn this improved version of the test, mockLog is our Mock Object. The method setExpectedLogMessage is used to program it with the expected log message. The statement facade.setAuditLog(mockLog) installs the Mock Object using the Setter Injection (see Dependency Injection) test double-installation pattern. Finally, the verify() method ensures that the call to logMessage() was actually made.\n\n3 This usually requires that we subclass our testcase from a special MockObjectTestCase class.\n\nwww.it-ebooks.info",
      "content_length": 1960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 612,
      "content": "Mock Object\n\npublic void testRemoveFlight_Mock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); // mock conﬁguration ConﬁgurableMockAuditLog mockLog = new ConﬁgurableMockAuditLog(); mockLog.setExpectedLogMessage( helper.getTodaysDateWithoutTime(), Helper.TEST_USER_NAME, Helper.REMOVE_FLIGHT_ACTION_CODE, expectedFlightDto.getFlightNumber()); mockLog.setExpectedNumberCalls(1); // mock installation FlightManagementFacade facade = new FlightManagementFacadeImpl(); facade.setAuditLog(mockLog); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); mockLog.verify(); }\n\nThis approach was made possible by use of the following Mock Object. Here we have chosen to use a hand-built Mock Object. In the interest of space, just the logMessage method is shown:\n\npublic void logMessage( Date actualDate, String actualUser, String actualActionCode, Object actualDetail) { actualNumberCalls++;\n\nAssert.assertEquals(\"date\", expectedDate, actualDate); Assert.assertEquals(\"user\", expectedUser, actualUser); Assert.assertEquals(\"action code\", expectedActionCode, actualActionCode); Assert.assertEquals(\"detail\", expectedDetail,actualDetail); }\n\nThe Assertion Methods are called as static methods. In JUnit, this approach is required because the Mock Object is not a subclass of TestCase; thus it does not inherit the assertion methods from Assert. Other members of the xUnit family may provide different mechanisms to access the Assertion Methods. For exam- ple, NUnit provides them only as static methods on the Assert class, so even Test Methods (page 348) need to access the Assertion Methods this way. Test::Unit,\n\nwww.it-ebooks.info\n\n549\n\nMock Object",
      "content_length": 1823,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 613,
      "content": "550\n\nMock Object\n\nChapter 23 Test Double Patterns\n\nthe xUnit family member for the Ruby programming language, provides them as mixins; as a consequence, they can be called in the normal fashion.\n\nExample: Mock Object (Dynamically Generated)\n\nThe last example used a hand-coded Mock Object. Most members of the xUnit family, however, have dynamic Mock Object frameworks available. Here’s the same test rewritten using JMock:\n\npublic void testRemoveFlight_JMock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // mock conﬁguration Mock mockLog = mock(AuditLog.class); mockLog.expects(once()).method(\"logMessage\") .with(eq(helper.getTodaysDateWithoutTime()), eq(Helper.TEST_USER_NAME), eq(Helper.REMOVE_FLIGHT_ACTION_CODE), eq(expectedFlightDto.getFlightNumber())); // mock installation facade.setAuditLog((AuditLog) mockLog.proxy()); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); // verify() method called automatically by JMock }\n\nNote how JMock provides a “ﬂ uent” Conﬁ guration Interface (see Conﬁ gurable Test Double) that allows us to specify the expected method calls in a fairly readable fashion. JMock also allows us to specify the comparator to be used by the asser- tions; in this case, the calls to eq cause the default equals method to be called.\n\nFurther Reading\n\nAlmost every book on automated testing using xUnit has something to say about Mock Objects, so I won’t list those resources here. As you are reading other books, keep in mind that the term Mock Object is often used to refer to a Test Stub and sometimes even to Fake Objects. Mocks, Fakes, Stubs, and Dummies (in Appendix B) contains a more thorough comparison of the terminology used in various books and articles.\n\nwww.it-ebooks.info",
      "content_length": 1965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 614,
      "content": "Fake Object\n\nFake Object\n\nHow can we verify logic independently when depended-on objects cannot be used? How can we avoid Slow Tests?\n\nWe replace a component that the SUT depends on with a much lighter-weight implementation.\n\nSetup Setup\n\nInstallation Installation\n\nCreation Creation\n\nFixture Fixture\n\nDOC DOC\n\nFake Fake Object Object\n\nSetup Setup\n\nExercise Exercise\n\nExercise Exercise\n\nSUT SUT\n\nData Data\n\nVerify Verify\n\nTeardown Teardown\n\nThe SUT often depends on other components or systems. Although the inter- actions with these other components may be necessary, the side effects of these interactions as implemented by the real DOC may be unnecessary or even detrimental.\n\nA Fake Object is a much simpler and lighter-weight implementation of the functionality provided by the DOC without the side effects we choose to do without.\n\nHow It Works\n\nWe acquire or build a very lightweight implementation of the same functionality as provided by a component on which the SUT depends and instruct the SUT to use it instead of the real DOC. This implementation need not have any of the\n\nwww.it-ebooks.info\n\n551\n\nAlso known as: Dummy\n\nFake Object",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 615,
      "content": "552\n\nFake Object\n\nChapter 23 Test Double Patterns\n\n“-ilities” that the real DOC needs to have (such as scalability); it need provide only the equivalent services to the SUT so that the SUT remains unaware it isn’t using the real DOC.\n\nA Fake Object is a kind of Test Double (page 522) that is similar to a Test Stub (page 529) in many ways, including the need to install into the SUT a substitutable dependency. Whereas a Test Stub acts as a control point to inject indirect inputs into the SUT, however, the Fake Object does not: It merely provides a way for the interactions to occur in a self-consistent manner. These interactions (i.e., between the SUT and the Fake Object) will typically be many, and the values passed in as arguments of earlier method calls will often be returned as results of later method calls. Contrast this behavior with that of Test Stubs and Mock Objects (page 544), where the responses are either hard-coded or conﬁ gured by the test.\n\nWhile the test does not normally conﬁ gure a Fake Object, complex ﬁ xture setup that would typically involve initializing the state of the DOC may also be done with the Fake Object directly using Back Door Manipulation (page 327). Techniques such as Data Loader (see Back Door Manipulation) and Back Door Setup (see Back Door Manipulation) can be used quite successfully with less fear of Overspeciﬁ ed Software (see Fragile Test on page 239) because they sim- ply bind us to the interface between the SUT and the Fake Object; the interface used to conﬁ gure the Fake Object is a test-only concern.\n\nWhen to Use It\n\nWe should use a Fake Object whenever the SUT depends on other components that are unavailable or that make testing difﬁ cult or slow (e.g., Slow Tests; see page 253) and the tests need more complex sequences of behavior than are worth implement- ing in a Test Stub or Mock Object. It must also be easier to create a lightweight implementation than to build and program suitable Mock Objects, at least in the long run, if building a Fake Object is to be worthwhile.\n\nUsing a Fake Object helps us avoid Overspeciﬁ ed Software because we do not encode the exact calling sequences expected of the DOC within the test. The SUT can vary how many times the methods of the DOC are called without causing tests to fail.\n\nIf we need to control the indirect inputs or verify the indirect outputs of the\n\nSUT, we should probably use a Mock Object or Test Stub instead.\n\nSome speciﬁ c situations where we replace the real component with a Fake\n\nObject are described next.\n\nwww.it-ebooks.info",
      "content_length": 2561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 616,
      "content": "Fake Object\n\nVariation: Fake Database\n\nWith the Fake Database pattern, the real database or persistence layer is replaced by a Fake Object that is functionally equivalent but that has much better perfor- mance characteristics. An approach we have often used involves replacing the database with a set of in-memory HashTables that act as a very lightweight way of retrieving objects that have been “persisted” earlier in the test.\n\nVariation: In-Memory Database\n\nAnother example of a Fake Object is the use of a small-footprint, diskless database instead of a full-featured disk-based database. This kind of In-Memory Database will improve the speed of tests by at least an order of magnitude while giving up less functionality than a Fake Database.\n\nVariation: Fake Web Service\n\nWhen testing software that depends on other components that are accessed as Web services, we can build a small hard-coded or data-driven implementation that can be used instead of the real Web service to make our tests more robust and to avoid having to create a test instance of the real Web service in our development environment.\n\nVariation: Fake Service Layer\n\nWhen testing user interfaces, we can avoid Data Sensitivity (see Fragile Test) and Behavior Sensitivity (see Fragile Test) of the tests by replacing the component that implements the Service Layer [PEAA] (including the domain layer) of our application with a Fake Object that returns remembered or data-driven results. This approach allows us to focus on testing the user interface without having to worry about the data being returned changing over time.\n\nImplementation Notes\n\nIntroducing a Fake Object involves two basic concerns:\n\nBuilding the Fake Object implementation\n\nInstalling the Fake Object\n\nBuilding the Fake Object\n\nMost Fake Objects are hand-built. Often, the Fake Object is used to replace a real implementation that suffers from latency issues owing to real messaging\n\nwww.it-ebooks.info\n\n553\n\nFake Object",
      "content_length": 1966,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 617,
      "content": "554\n\nFake Object\n\nChapter 23 Test Double Patterns\n\nor disk I/O with a much lighter in-memory implementation. With the rich class libraries available in most object-oriented programming languages, it is usually possible to build a fake implementation that is sufﬁ cient to satisfy the needs of the SUT, at least for the purposes of speciﬁ c tests, with relatively little effort.\n\nA popular strategy is to start by building a Fake Object to support a speciﬁ c set of tests where the SUT requires only a subset of the DOC’s services. If this proves successful, we may consider expanding the Fake Object to handle addi- tional tests. Over time, we may ﬁ nd that we can run all of our tests using the Fake Object. (See the sidebar “Faster Tests Without Shared Fixtures” on page 319 for a description of how we faked out the entire database with hash tables and made our tests run 50 times faster.)\n\nInstalling the Fake Object\n\nOf course, we must have a way of installing the Fake Object into the SUT to be able to take advantage of it. We can use whichever substitutable dependency pattern the SUT supports. A common approach in the test-driven development community is Dependency Injection (page 678); more traditional developers may favor Dependency Lookup (page 686). The latter technique is also more appropriate when we introduce a Fake Database (see Fake Object on page 551) in an effort to speed up execution of the customer tests; Dependency Injection doesn’t work so well with these kinds of tests.\n\nMotivating Example\n\nIn this example, the SUT needs to read and write records from a database. The test must set up the ﬁ xture in the database (several writes), the SUT interacts (reads and writes) with the database several more times, and then the test removes the records from the database (several deletes). All of this work takes time—several seconds per test. This very quickly adds up to minutes, and soon we ﬁ nd that our developers aren’t running the tests quite so frequently. Here is an example of one of these tests:\n\npublic void testReadWrite() throws Exception{ // Setup FlightMngtFacade facade = new FlightMgmtFacadeImpl(); BigDecimal yyc = facade.createAirport(\"YYC\", \"Calgary\", \"Calgary\"); BigDecimal lax = facade.createAirport(\"LAX\", \"LAX Intl\", \"LA\"); facade.createFlight(yyc, lax); // Exercise List ﬂights = facade.getFlightsByOriginAirport(yyc);\n\nwww.it-ebooks.info",
      "content_length": 2389,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 618,
      "content": "Fake Object\n\n// Verify assertEquals( \"# of ﬂights\", 1, ﬂights.size()); Flight ﬂight = (Flight) ﬂights.get(0); assertEquals( \"origin\", yyc, ﬂight.getOrigin().getCode()); }\n\nThe test calls createAirport on our Service Facade [CJ2EEP], which calls, among other things, our data access layer. Here is the actual implementation of several of the methods we are calling:\n\npublic BigDecimal createAirport( String airportCode, String name, String nearbyCity) throws FlightBookingException{ TransactionManager.beginTransaction(); Airport airport = dataAccess. createAirport(airportCode, name, nearbyCity); logMessage(\"Wrong Action Code\", airport.getCode());//bug TransactionManager.commitTransaction(); return airport.getId(); }\n\npublic List getFlightsByOriginAirport( BigDecimal originAirportId) throws FlightBookingException {\n\nif (originAirportId == null) throw new InvalidArgumentException( \"Origin Airport Id has not been provided\", \"originAirportId\", null); Airport origin = dataAccess.getAirportByPrimaryKey(originAirportId); List ﬂights = dataAccess.getFlightsByOriginAirport(origin);\n\nreturn ﬂights; }\n\nThe calls to dataAccess.createAirport, dataAccess.createFlight, and TransactionManager. commitTransaction cause our test to slow down the most. The calls to dataAccess. getAirportByPrimaryKey and dataAccess.getFlightsByOriginAirport are a lesser factor but still contribute to the slow test.\n\nRefactoring Notes\n\nThe steps for introducing a Fake Object are very similar to those for adding a Mock Object. If one doesn’t already exist, we use a Replace Dependency with Test Double (page 522) refactoring to introduce a way to substitute the Fake Object for the DOC—usually a ﬁ eld (attribute) to hold the reference to it. In statically typed languages, we may have to do an Extract Interface [Fowler] refactoring before we\n\nwww.it-ebooks.info\n\n555\n\nFake Object",
      "content_length": 1861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 619,
      "content": "556\n\nFake Object\n\nChapter 23 Test Double Patterns\n\ncan introduce the fake implementation. Then, we use this interface as the type of variable that holds the reference to the substitutable dependency.\n\nOne notable difference is that we do not need to conﬁ gure the Fake Object with\n\nexpectations or return values; we merely set up the ﬁ xture in the normal way.\n\nExample: Fake Database\n\nIn this example, we’ve created a Fake Object that replaces the database—that is, a Fake Database implemented entirely in memory using hash tables. The test doesn’t change a lot, but the test execution occurs much, much faster.\n\npublic void testReadWrite_inMemory() throws Exception{ // Setup FlightMgmtFacadeImpl facade = new FlightMgmtFacadeImpl(); facade.setDao(new InMemoryDatabase()); BigDecimal yyc = facade.createAirport(\"YYC\", \"Calgary\", \"Calgary\"); BigDecimal lax = facade.createAirport(\"LAX\", \"LAX Intl\", \"LA\"); facade.createFlight(yyc, lax); // Exercise List ﬂights = facade.getFlightsByOriginAirport(yyc); // Verify assertEquals( \"# of ﬂights\", 1, ﬂights.size()); Flight ﬂight = (Flight) ﬂights.get(0); assertEquals( \"origin\", yyc, ﬂight.getOrigin().getCode()); }\n\nHere’s the implementation of the Fake Database:\n\npublic class InMemoryDatabase implements FlightDao{ private List airports = new Vector(); public Airport createAirport(String airportCode, String name, String nearbyCity) throws DataException, InvalidArgumentException { assertParamtersAreValid( airportCode, name, nearbyCity); assertAirportDoesntExist( airportCode); Airport result = new Airport(getNextAirportId(), airportCode, name, createCity(nearbyCity)); airports.add(result); return result; } public Airport getAirportByPrimaryKey(BigDecimal airportId) throws DataException, InvalidArgumentException { assertAirportNotNull(airportId);\n\nAirport result = null; Iterator i = airports.iterator(); while (i.hasNext()) {\n\nwww.it-ebooks.info",
      "content_length": 1901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 620,
      "content": "Fake Object\n\nAirport airport = (Airport) i.next(); if (airport.getId().equals(airportId)) { return airport; } } throw new DataException(\"Airport not found:\"+airportId); }\n\nNow all we need is the implementation of the method that installs the Fake Database into the facade to make our developers more than happy to run all the tests after every code change.\n\npublic void setDao(FlightDao) { dataAccess = dao; }\n\nFurther Reading\n\nThe sidebar “Faster Tests Without Shared Fixtures” on page 319 provides a more in-depth description of how we faked out the entire database with hash tables and made our tests run 50 times faster. Mocks, Fakes, Stubs, and Dum- mies (in Appendix B) contains a more thorough comparison of the terminology used in various books and articles.\n\nwww.it-ebooks.info\n\n557\n\nFake Object",
      "content_length": 804,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 621,
      "content": "558\n\nAlso known as: Conﬁ gurable Mock Object, Conﬁ gurable Test Spy, Conﬁ gurable Test Stub\n\nConﬁ gurable Test Double\n\nChapter 23 Test Double Patterns\n\nConﬁ gurable Test Double\n\nHow do we tell a Test Double what to return or expect?\n\nWe conﬁ gure a reusable Test Double with the values to be returned or veriﬁ ed during the ﬁ xture setup phase of a test.\n\nExpectations, Expectations, Return Values Return Values\n\nFixture Fixture\n\nDOC DOC\n\nSetup Setup\n\nConfiguration Configuration\n\nInstallation Installation\n\nExpectations Expectations\n\nExercise Exercise\n\nSUT SUT\n\nReturn Return Values Values\n\nVerify Verify\n\nTeardown Teardown\n\nTest Test Double Double\n\nSome tests require unique values to be fed into the SUT as indirect inputs or to be veriﬁ ed as indirect outputs of the SUT. This approach typically requires the use of Test Doubles (page 522) as the conduit between the test and the SUT; at the same time, the Test Double somehow needs to be told which values to return or verify. AConﬁ gurable Test Double is a way to reduce Test Code Duplication (page 213) by reusing a Test Double in many tests. The key to its use is to conﬁ gure the Test Double’s values to be returned or expected at runtime.\n\nHow It Works\n\nThe Test Double is built with instance variables that hold the values to be returned to the SUT or to serve as the expected values of arguments to method calls. The test initializes these variables during the setup phase of the test by calling the appropri- ate methods on the Test Double’s interface. When the SUT calls the methods on the Test Double, the Test Double uses the contents of the appropriate variable as the value to return or as the expected value in assertions.\n\nwww.it-ebooks.info",
      "content_length": 1711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 622,
      "content": "Configurable Test Double\n\nWhen to Use It\n\nWe can use a Conﬁ gurable Test Double whenever we need similar but slightly different behavior in several tests that depend on Test Doubles and we want to avoid Test Code Duplication or Obscure Tests (page 186)—in the latter case, we need to see what values the Test Double is using as we read the test. If we expect only a single usage of a Test Double, we can consider using a Hard-Coded Test Double (page 568) if the extra effort and complexity of building a Conﬁ gurable Test Double are not warranted.\n\nImplementation Notes\n\nA Test Double is a Conﬁ gurable Test Double because it needs to provide a way for the tests to conﬁ gure it with values to return and/or method arguments to expect. Conﬁ gurable Test Stubs (page 529) and Test Spies (page 538) simply require a way to conﬁ gure the responses to calls on their methods; conﬁ gurable Mock Objects (page 544) also require a way to conﬁ gure their expectations (which methods should be called and with which arguments).\n\nConﬁ gurable Test Doubles may be built in many ways. Deciding on a par- ticular implementation involves making two relatively independent decisions: (1) how the Conﬁ gurable Test Double will be conﬁ gured and (2) how the Conﬁ gurable Test Double will be coded.\n\nThere are two common ways to conﬁ gure a Conﬁ gurable Test Double. The most popular approach is to provide a Conﬁ guration Interface that is used only by the test to conﬁ gure the values to be returned as indirect inputs and the expected values of the indirect outputs. Alternatively, we may build the Conﬁ gurable Test Double with two modes. The Conﬁ guration Mode is used during ﬁ xture setup to install the indirect inputs and expected indirect out- puts by calling the methods of the Conﬁ gurable Test Double with the expected arguments. Before the Conﬁ gurable Test Double is installed, it is put into the normal (“usage” or “playback”) mode.\n\nThe obvious way to build a Conﬁ gurable Test Double is to create a Hand- Built Test Double. If we are lucky, however, someone will have already built a tool to generate a Conﬁ gurable Test Double for us. Test Double genera- tors come in two ﬂ avors: code generators and tools that fabricate the object at runtime. Developers have built several generations of “mocking” tools, and several of these have been ported to other programming languages; check out http://xprogramming.com to see what is available in your programming language of choice. If the answer is “nothing,” you can hand-code the Test Double your- self, although this does take somewhat more effort.\n\nwww.it-ebooks.info\n\n559\n\nConﬁ gurable Test Double",
      "content_length": 2646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 623,
      "content": "560\n\nConﬁ gurable Test Double\n\nChapter 23 Test Double Patterns\n\nVariation: Conﬁ guration Interface\n\nA Conﬁ guration Interface comprises a separate set of methods that the Conﬁ gurable Test Double provides speciﬁ cally for use by the test to set each value that the Conﬁ gurable Test Double returns or expects to receive. The test simply calls these methods during the ﬁ xture setup phase of the Four-Phase Test (page 358). The SUT uses the “other” methods on the Conﬁ gurable Test Double (the “normal” interface). It isn’t aware that the Conﬁ guration Interface exists on the object to which it is delegating.\n\nConﬁ guration Interfaces come in two ﬂ avors. Early toolkits, such as Mock- Maker, generated a distinct method for each value we needed to conﬁ gure. The collection of these setter methods made up the Conﬁ guration Interface. More recently introduced toolkits, such as JMock, provide a generic interface that is used to build an Expected Behavior Speciﬁ cation (see Behavior Veriﬁ cation on page 468) that the Conﬁ gurable Test Double interprets at runtime. A well-designed ﬂ uent interface can make the test much easier to read and understand.\n\nVariation: Conﬁ guration Mode\n\nWe can avoid deﬁ ning a separate set of methods to conﬁ gure the Test Double by providing a Conﬁ guration Mode that the test uses to “teach” the Conﬁ gurable Test Double what to expect. At ﬁ rst glance, this means of conﬁ guring the Test Double can be confusing: Why does the Test Method (page 348) call the methods of this other object before it calls the methods it is exercising on the SUT? When we come to grips with the fact that we are doing a form of “record and play- back,” this technique makes a bit more sense.\n\nThe main advantage of using a Conﬁ guration Mode is that it avoids creating a separate set of methods for conﬁ guring the Conﬁ gurable Test Double because we reuse the same methods that the SUT will be calling. (We do have to pro- vide a way to set the values to be returned by the methods, so we have at least one additional method to add.) On the ﬂ ip side, each method that the SUT is expected to call now has two code paths through it: one for the Conﬁ guration Mode and another for the “usage mode.”\n\nVariation: Hand-Built Test Double\n\nA Hand-Built Test Double is one that was deﬁ ned by the test automater for one or more speciﬁ c tests. A Hard-Coded Test Double is inherently a Hand-Built Test Double, while a Conﬁ gurable Test Double can be either hand-built or gener- ated. This book uses Hand-Built Test Doubles in a lot of the examples because it is easier to see what is going on when we have actual, simple, concrete code to look at. This is the main advantage of using a Hand-Built Test Double; indeed,\n\nwww.it-ebooks.info",
      "content_length": 2747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 624,
      "content": "Configurable Test Double\n\nsome people consider this beneﬁ t to be so important that they use Hand-Built Test Doubles exclusively. We may also use a Hand-Built Test Double when no third-party toolkits are available or if we are prevented from using those tools by project or corporate policy.\n\nVariation: Statically Generated Test Double\n\nThe early third-party toolkits used code generators to create the code for Stati- cally Generated Test Doubles. The code is then compiled and linked with our handwritten test code. Typically, we will store the code in a source code repository [SCM]. Whenever the interface of the target class changes, of course, we must regenerate the code for our Statically Generated Test Doubles. It may be advan- tageous to include this step as part of the automated build script to ensure that it really does happen whenever the interface changes.\n\nInstantiating a Statically Generated Test Double is the same as instantiating a Hand-Built Test Double. That is, we use the name of the generated class to construct the Conﬁ gurable Test Double.\n\nAn interesting problem arises during refactoring. Suppose we change the interface of the class we are replacing by adding an argument to one of the methods. Should we then refactor the generated code? Or should we regener- ate the Statically Generated Test Double after the code it replaces has been refactored? With modern refactoring tools, it may seem easier to refactor the generated code and the tests that use it in a single step; this strategy, however, may leave the Statically Generated Test Double without argument veriﬁ cation logic or variables for the new parameter. Therefore, we should regenerate the Statically Generated Test Double after the refactoring is ﬁ nished to ensure that the refactored Statically Generated Test Double works properly and can be recreated by the code generator.\n\nVariation: Dynamically Generated Test Double\n\nNewer third-party toolkits generate Conﬁ gurable Test Doubles at runtime by using the reﬂ ection capabilities of the programming language to examine a class or interface and build an object that is capable of understanding all calls to its methods. These Conﬁ gurable Test Doubles may interpret the behavior speciﬁ cation at runtime or they may generate executable code; nevertheless, there is no source code for us to generate and maintain or regenerate. The down side is simply that there is no code to look at—but that really isn’t a disadvantage unless we are particularly suspicious or paranoid.\n\nMost of today’s tools generate Mock Objects because they are the most fashionable and widely used options. We can still use these objects as Test Stubs,\n\nwww.it-ebooks.info\n\n561\n\nConﬁ gurable Test Double",
      "content_length": 2729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 625,
      "content": "562\n\nConﬁ gurable Test Double\n\nChapter 23 Test Double Patterns\n\nhowever, because they do provide a way of setting the value to be returned when a particular method is called. If we aren’t particularly interested in verifying the methods being called or the arguments passed to them, most toolkits provide a way to specify “don’t care” arguments. Given that most toolkits generate Mock Objects, they typically don’t provide a Retrieval Interface (see Test Spy).\n\nMotivating Example\n\nHere’s a test that uses a Hard-Coded Test Double to give it control over the time:\n\npublic void testDisplayCurrentTime_AtMidnight_HCM() throws Exception { // Fixture Setup // Instantiate hard-code Test Stub: TimeProvider testStub = new MidnightTimeProvider(); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Inject Stub into SUT sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify Direct Output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nThis test is hard to understand without seeing the deﬁ nition of the Hard-Coded Test Double. It is easy to see how this lack of clarity can lead to a Mystery Guest (see Obscure Test) if the deﬁ nition is not close at hand.\n\nclass MidnightTimeProvider implements TimeProvider { public Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.HOUR_OF_DAY, 0); myTime.set(Calendar.MINUTE, 0); return myTime; } }\n\nWe can solve the Obscure Test problem by using a Self Shunt (see Hard-Coded Test Double) to make the Hard-Coded Test Double visible within the test:\n\npublic class SelfShuntExample extends TestCase implements TimeProvider { public void testDisplayCurrentTime_AtMidnight() throws Exception { // Fixture Setup\n\nwww.it-ebooks.info",
      "content_length": 1841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 626,
      "content": "Configurable Test Double\n\nTimeDisplay sut = new TimeDisplay(); // Mock Setup sut.setTimeProvider(this); // self shunt installation // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify Direct Output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\npublic Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.MINUTE, 0); myTime.set(Calendar.HOUR_OF_DAY, 0); return myTime; } }\n\nUnfortunately, we will need to build the Test Double behavior into each Testcase Class (page 373) that requires it, which results in Test Code Duplication.\n\nRefactoring Notes\n\nRefactoring a test that uses a Hard-Coded Test Double to become a test that uses a third-party Conﬁ gurable Test Double is relatively straightforward. We simply follow the directions provided with the toolkit to instantiate the Conﬁ gurable Test Double and conﬁ gure it with the same values as we used in the Hard-Coded Test Double. We may also have to move some of the logic that was originally hard-coded within the Test Double into the Test Method and pass it in to the Test Double as part of the conﬁ guration step.\n\nConverting the actual Hard-Coded Test Double into a Conﬁ gurable Test Double is a bit more complicated, but not overly so if we need to capture only simple behavior. (For more complex behavior, we’re probably better off examining one of the existing toolkits and porting it to our environment if it is not yet available.) First we need to introduce a way to set the values to be returned or expected. The best choice is to start by modifying the test to see how we want to interact with the Conﬁ gurable Test Double. After instantiating it during the ﬁ xture setup part of the test, we then pass the test-speciﬁ c values to the Conﬁ gurable Test Double using the emerging Conﬁ guration Interface or Conﬁ guration Mode. Once we’ve seen how we want to use the Conﬁ gurable Test Double, we can use an Introduce Field [JetBrains] refactoring to create the instance variables of the Conﬁ gurable Test Double to hold each of the previ- ously hard-coded values.\n\nwww.it-ebooks.info\n\n563\n\nConﬁ gurable Test Double",
      "content_length": 2218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 627,
      "content": "564\n\nConﬁ gurable Test Double\n\nChapter 23 Test Double Patterns\n\nExample: Conﬁ guration Interface Using Setters\n\nThe following example shows how a test would use a simple hand-built Conﬁ guration Interface using Setter Injection:\n\npublic void testDisplayCurrentTime_AtMidnight() throws Exception { // Fixture setup // Test Double conﬁguration TimeProviderTestStub tpStub = new TimeProviderTestStub(); tpStub.setHours(0); tpStub.setMinutes(0); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Test Double installation sut.setTimeProvider(tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify Outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nThe Conﬁ gurable Test Double is implemented as follows:\n\nclass TimeProviderTestStub implements TimeProvider { // Conﬁguration Interface public void setHours(int hours) { // 0 is midnight; 12 is noon myTime.set(Calendar.HOUR_OF_DAY, hours); }\n\npublic void setMinutes(int minutes) { myTime.set(Calendar.MINUTE, minutes); } // Interface Used by SUT public Calendar getTime() { // @return the last time that was set return myTime; } }\n\nExample: Conﬁ guration Interface Using Expression Builder\n\nNow let’s contrast the Conﬁ guration Interface we deﬁ ned in the previous example with the one provided by the JMock framework. JMock generates Mock Objects dynamically and provides a generic ﬂ uent interface for conﬁ guring the Mock Object in an intent-revealing style. Here’s the same test converted to use JMock:\n\nwww.it-ebooks.info",
      "content_length": 1600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 628,
      "content": "Configurable Test Double\n\npublic void testDisplayCurrentTime_AtMidnight_JM() throws Exception { // Fixture setup TimeDisplay sut = new TimeDisplay(); // Test Double conﬁguration Mock tpStub = mock(TimeProvider.class); Calendar midnight = makeTime(0,0); tpStub.stubs().method(\"getTime\"). withNoArguments(). will(returnValue(midnight)); // Test Double installation sut.setTimeProvider((TimeProvider) tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify Outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nHere we have moved some of the logic to construct the time to be returned into the Testcase Class because there is no way to do it in the generic mocking frame- work; we’ve used a Test Utility Method (page 599) to construct the time to be returned. This next example shows a conﬁ gurable Mock Object complete with multiple expected parameters:\n\npublic void testRemoveFlight_JMock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); FlightManagementFacade facade = new FlightManagementFacadeImpl(); // mock conﬁguration Mock mockLog = mock(AuditLog.class); mockLog.expects(once()).method(\"logMessage\") .with(eq(helper.getTodaysDateWithoutTime()), eq(Helper.TEST_USER_NAME), eq(Helper.REMOVE_FLIGHT_ACTION_CODE), eq(expectedFlightDto.getFlightNumber())); // mock installation facade.setAuditLog((AuditLog) mockLog.proxy()); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); // verify() method called automatically by JMock }\n\nThe Expected Behavior Speciﬁ cation is built by calling expression-building methods such as expects, once, and method to describe how the Conﬁ gurable\n\nwww.it-ebooks.info\n\n565\n\nConﬁ gurable Test Double",
      "content_length": 1933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 629,
      "content": "566\n\nConﬁ gurable Test Double\n\nChapter 23 Test Double Patterns\n\nTest Double should be used and what it should return. JMock supports the speciﬁ cation of much more sophisticated behavior (such as multiple calls to the same method with different arguments and return values) than does our hand-built Conﬁ gurable Test Double.\n\nExample: Conﬁ guration Mode\n\nIn the next example, the test has been converted to use a Mock Object with a Conﬁ guration Mode:\n\npublic void testRemoveFlight_ModalMock() throws Exception { // ﬁxture setup FlightDto expectedFlightDto = createAnonRegFlight(); // mock conﬁguration (in Conﬁguration Mode) ModalMockAuditLog mockLog = new ModalMockAuditLog(); mockLog.logMessage(Helper.getTodaysDateWithoutTime(), Helper.TEST_USER_NAME, Helper.REMOVE_FLIGHT_ACTION_CODE, expectedFlightDto.getFlightNumber()); mockLog.enterPlaybackMode(); // mock installation FlightManagementFacade facade = new FlightManagementFacadeImpl(); facade.setAuditLog(mockLog); // exercise facade.removeFlight(expectedFlightDto.getFlightNumber()); // verify assertFalse(\"ﬂight still exists after being removed\", facade.ﬂightExists( expectedFlightDto. getFlightNumber())); mockLog.verify(); }\n\nHere the test calls the methods on the Conﬁ gurable Test Double during the ﬁ xture setup phase. If we weren’t aware that this test uses a Conﬁ gurable Test Double mock, we might ﬁ nd this structure confusing at ﬁ rst glance. The most obvious clue to its intent is the call to the method enterPlaybackMode, which tells the Conﬁ gurable Test Double to stop saving expected values and to start asserting on them.\n\nThe Conﬁ gurable Test Double used by this test is implemented like this:\n\nprivate int mode = record;\n\npublic void enterPlaybackMode() { mode = playback; }\n\npublic void logMessage( Date date, String user, String action, Object detail) {\n\nwww.it-ebooks.info",
      "content_length": 1854,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 630,
      "content": "Configurable Test Double\n\nif (mode == record) { Assert.assertEquals(\"Only supports 1 expected call\", 0, expectedNumberCalls); expectedNumberCalls = 1; expectedDate = date; expectedUser = user; expectedCode = action; expectedDetail = detail; } else { Assert.assertEquals(\"Date\", expectedDate, date); Assert.assertEquals(\"User\", expectedUser, user); Assert.assertEquals(\"Action\", expectedCode, action); Assert.assertEquals(\"Detail\", expectedDetail, detail); } }\n\nThe if statement checks whether we are in record or playback mode. Because this simple hand-built Conﬁ gurable Test Double allows only a single value to be stored, a Guard Assertion (page 490) fails the test if it tries to record more than one call to this method. The rest of the then clause saves the parameters into variables that it uses as the expected values of the Equality Assertions (see Assertion Method on page 362) in the else clause.\n\nwww.it-ebooks.info\n\n567\n\nConﬁ gurable Test Double",
      "content_length": 958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 631,
      "content": "568\n\nAlso known as: Hard-Coded Mock Object, Hard-Coded Test Stub, Hard-Coded Test Spy\n\nHard-Coded Test Double\n\nChapter 23 Test Double Patterns\n\nHard-Coded Test Double\n\nHow do we tell a Test Double what to return or expect?\n\nWe build the Test Double by hard-coding the return values and/or expected calls.\n\nFixture Fixture\n\nDOC DOC\n\nCreation Creation\n\nSetup Setup\n\nExpectations Expectations\n\nInstallation Installation\n\nExercise Exercise\n\nSUT SUT\n\nReturn Return Values Values\n\nVerify Verify\n\nTeardown Teardown\n\nTest Test Double Double\n\nTest Doubles (page 522) are used for many reasons during the development of Fully Automated Tests (see page 26). The behavior of the Test Double may vary from test to test, and there are many ways to deﬁ ne this behavior.\n\nWhen the Test Double is very simple or very speciﬁ c to a single test, the sim-\n\nplest solution is often to hard-code the behavior into the Test Double.\n\nHow It Works\n\nThe test automater hard-codes all of the Test Double’s behavior into the Test Double. For example, if the Test Double needs to return a value for a method call, the value is hard-coded into the return statement. If it needs to verify that a certain parameter had a speciﬁ c value, the assertion is hard-coded with the value that is expected.\n\nwww.it-ebooks.info",
      "content_length": 1286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 632,
      "content": "Hard-Coded Test Double\n\nWhen to Use It\n\nWe typically use a Hard-Coded Test Double when the behavior of the Test Double is very simple or is very speciﬁ c to a single test or Testcase Class (page 373). The Hard-Coded Test Double can be either a Test Stub (page 529), a Test Spy (page 538), or a Mock Object (page 544), depending on what we encode in the method(s) called by the SUT.\n\nBecause each Hard-Coded Test Double is purpose-built by hand, its construction may take more effort than using a third-party Conﬁ gurable Test Double (page 558). It can also result in more test code to maintain and refactor as the SUT changes. If different tests require that the Test Double behave in different ways and the use of Hard-Coded Test Doubles results in too much Test Code Duplication (page 213), we should consider using a Conﬁ gurable Test Double instead.\n\nImplementation Notes\n\nHard-Coded Test Doubles are inherently Hand-Built Test Doubles (see Conﬁ gurable Test Double) because there tends to be no point in generating Hard-Coded Test Doubles automatically. Hard-Coded Test Doubles can be implemented with dedicated classes, but they are most commonly used when the programming language supports blocks, closures, or inner classes. All of these language features help to avoid the ﬁ le/class overhead associated with creating a Hard-Coded Test Double; they also keep the Hard-Coded Test Double’s behavior visible within the test that uses it. In some languages, this can make the tests a bit more difﬁ cult to read. This is especially true when we use anonymous inner classes, which require a lot of syntactic overhead to deﬁ ne the class in-line. In languages that support blocks directly, and in which developers are very familiar with their usage idioms, using Hard-Coded Test Doubles can actually make the tests easier to read.\n\nThere are many different ways to implement a Hard-Coded Test Double,\n\neach of which has its own advantages and disadvantages.\n\nVariation: Test Double Class\n\nWe can implement the Hard-Coded Test Double as a class distinct from either the Testcase Class or the SUT. This allows the Hard-Coded Test Double to be reused by several Testcase Classes but may result in an Obscure Test (page 186; caused by a Mystery Guest) because it moves important indirect inputs or indi- rect outputs of the SUT out of the test to somewhere else, possibly out of sight of the test reader. Depending on how we implement the Test Double Class, it may also result in code proliferation and additional Test Double classes to maintain.\n\nwww.it-ebooks.info\n\n569\n\nHard-Coded Test Double",
      "content_length": 2593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 633,
      "content": "570\n\nHard-Coded Test Double\n\nAlso known as: Loopback, Testcase Class as Test Double\n\nChapter 23 Test Double Patterns\n\nOne way to ensure that the Test Double Class is type-compatible with the component it will replace is to make the Test Double Class a subclass of that component. We then override any methods whose behavior we want to change.\n\nVariation: Test Double Subclass\n\nWe can also implement the Hard-Coded Test Double by subclassing the real DOC and overriding the behavior of the methods we expect the SUT to call as we exercise it. Unfortunately, this approach can have unpredictable consequences if the SUT calls other DOC methods that we have not overridden. It also ties our test code very closely to the implementation of the DOC and can result in Over- speciﬁ ed Software (see Fragile Test on page 239). Using a Test Double Subclass may be a reasonable option in very speciﬁ c circumstances (e.g., while doing a spike or when it is the only option available to us), but this strategy isn’t recom- mended on a routine basis.\n\nVariation: Self Shunt\n\nWe can implement the methods that we want the SUT to call on the Testcase Class and install the Testcase Object (page 382) into the SUT as the Test Double to be used. This approach is called a Self Shunt.\n\nThe Self Shunt can be either a Test Stub, a Test Spy, or a Mock Object, depending on what the method called by the SUT does. In each case, it will need to access instance variables of the Testcase Class to know what to do or expect. In statically typed languages, the Testcase Class must also implement the interface on which the SUT depends.\n\nWe typically use a Self Shunt when we need a Hard-Coded Test Double that is very speciﬁ c to a single Testcase Class. If only a single Test Method (page 348) requires the Hard-Coded Test Double, using an Inner Test Double may result in greater clarity if our language supports it.\n\nVariation: Inner Test Double\n\nA popular way to implement a Hard-Coded Test Double is to code it as an anonymous inner class or block closure within the Test Method. This strategy gives the Test Double access to instance variables and constants of the Testcase Class and even the local variables of the Test Method, which can eliminate the need to conﬁ gure the Test Double.\n\nWhile the name of this variation is based on the name of the Java language construct of which it takes advantage, many programming languages have an equivalent mechanism for deﬁ ning code to be run later using blocks or closures.\n\nwww.it-ebooks.info",
      "content_length": 2519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 634,
      "content": "Hard-Coded Test Double\n\nWe typically use an Inner Test Double when we are building a Hard-Coded Test Double that is relatively simple and is used only within a single Test Method. Many people ﬁ nd the use of a Hard-Coded Test Double more intuitive than using a Self Shunt because they can see exactly what is going on within the Test Method. Readers who are unfamiliar with the syntax of anonymous inner classes or blocks may ﬁ nd the test difﬁ cult to understand, however.\n\nVariation: Pseudo-Object\n\nOne challenge facing writers of Hard-Coded Test Doubles is that we must implement all the methods in the interface that the SUT might call. In statically typed languages such as Java and C#, we must at least implement all methods declared in the interface implied by the class or type associated with however we access the DOC. This often “forces” us to subclass from the real DOC to avoid providing dummy implementations for these methods.\n\nOne way of reducing the programming effort is to provide a default class that implements all the interface methods and throws a unique error. We can then implement a Hard-Coded Test Double by subclassing this concrete class and overriding just the one method we expect the SUT to call while we are exercising it. If the SUT calls any other methods, the Pseudo-Object throws an error, thereby failing the test.\n\nMotivating Example\n\nThe following test veriﬁ es the basic functionality of the component that formats an HTML string containing the current time. Unfortunately, it depends on the real system clock, so it rarely passes!\n\npublic void testDisplayCurrentTime_AtMidnight() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result); }\n\nwww.it-ebooks.info\n\n571\n\nHard-Coded Test Double",
      "content_length": 1938,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 635,
      "content": "572\n\nHard-Coded Test Double\n\nChapter 23 Test Double Patterns\n\nRefactoring Notes\n\nThe most common transition is from using the real component to using a Hard-Coded Test Double.4 To make this transition, we need to build the Test Double itself and install it from within our Test Method. We may also need to introduce a way to install the Test Double using one of the Dependency Injection patterns (page 678) if the SUT does not already support this installation. The process for doing so is described in the Replace Dependency with Test Double (page 522) refactoring.\n\nExample: Test Double Class\n\nHere’s the same test modiﬁ ed to use a Hard-Coded Test Double class to allow control over the time:\n\npublic void testDisplayCurrentTime_AtMidnight_HCM() throws Exception { // Fixture setup // Instantiate hard-coded Test Stub TimeProvider testStub = new MidnightTimeProvider(); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Inject Test Stub into SUT sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nThis test is hard to understand without seeing the deﬁ nition of the Hard-Coded Test Double. We can readily see how this approach might lead to an Obscure Test caused by a Mystery Guest if the Hard-Coded Test Double is not close at hand.\n\nclass MidnightTimeProvider implements TimeProvider { public Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.HOUR_OF_DAY, 0); myTime.set(Calendar.MINUTE, 0); return myTime; } }\n\n4 We rarely move from a Conﬁ gurable Test Double to a Hard-Coded Test Double because we generally seek to make the Test Double more—not less—reusable.\n\nwww.it-ebooks.info",
      "content_length": 1836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 636,
      "content": "Hard-Coded Test Double\n\nDepending on the programming language, this Test Double Class can be deﬁ ned in a number of different places, including within the body of the Testcase Class (an inner class) and as a separate free-standing class either in the same ﬁ le as the test or in its own ﬁ le. Of course, the farther away the Test Double Class resides from the Test Method, the more of a Mystery Guest it becomes.\n\nExample: Self Shunt/Loopback\n\nHere’s a test that uses a Self Shunt to allow control over the time:\n\npublic class SelfShuntExample extends TestCase implements TimeProvider { public void testDisplayCurrentTime_AtMidnight() throws Exception { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // mock setup sut.setTimeProvider(this); // self shunt installation // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\npublic Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.MINUTE, 0); myTime.set(Calendar.HOUR_OF_DAY, 0); return myTime; } }\n\nNote how both the Test Method that installs the Hard-Coded Test Double and the implementation of the getTime method called by the SUT are members of the same class. We used the Setter Injection pattern (see Dependency Injection) to install the Hard-Coded Test Double. Because this example is written in a statically typed language, we had to add the clause implements TimeProvider to the Testcase Class declaration so that the sut.setTimeProvider(this) statement will compile. In a dynamically typed language, this step is unnecessary.\n\nExample: Subclassed Inner Test Double\n\nHere’s a JUnit test that uses a Subclassed Inner Test Double using Java’s “Anon- ymous Inner Class” syntax:\n\nwww.it-ebooks.info\n\n573\n\nHard-Coded Test Double",
      "content_length": 1892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 637,
      "content": "574\n\nHard-Coded Test Double\n\nChapter 23 Test Double Patterns\n\npublic void testDisplayCurrentTime_AtMidnight_AIM() throws Exception { // Fixture setup // Deﬁne and instantiate Test Stub TimeProvider testStub = new TimeProvider() { // Anonymous inner stub public Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.MINUTE, 0); myTime.set(Calendar.HOUR_OF_DAY, 0); return myTime; } }; // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Inject Test Stub into SUT sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nHere we used the name of the real depended-on class (TimeProvider) in the call to new for the deﬁ nition of the Hard-Coded Test Double. By including a deﬁ nition of the method getTime within curly braces after the classname, we are actually creating an anonymous Subclassed Test Double inside the Test Method.\n\nExample: Inner Test Double Subclassed from Pseudo-Class\n\nSuppose we have replaced one implementation of a method with another imple- mentation that we need to leave around for backward-compatibility purposes, but we want to write tests to ensure that the old method is no longer called. This is easy to do if we already have the following Pseudo-Object deﬁ nition:\n\n/** * Base class for hand-coded Test Stubs and Mock Objects */ public class PseudoTimeProvider implements ComplexTimeProvider {\n\npublic Calendar getTime() throws TimeProviderEx { throw new PseudoClassException(); }\n\npublic Calendar getTimeDifference(Calendar baseTime, Calendar otherTime) throws TimeProviderEx { throw new PseudoClassException();\n\nwww.it-ebooks.info",
      "content_length": 1798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 638,
      "content": "Hard-Coded Test Double\n\n}\n\npublic Calendar getTime( String timeZone ) throws TimeProviderEx { throw new PseudoClassException(); } }\n\nWe can now write a test that ensures the old version of the getTime method is not called by subclassing and overriding the newer version of the method (the one we expect to be called by the SUT):\n\npublic void testDisplayCurrentTime_AtMidnight_PS() throws Exception { // Fixture setup // Deﬁne and instantiate Test Stub TimeProvider testStub = new PseudoTimeProvider() { // Anonymous inner stub public Calendar getTime(String timeZone) { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.MINUTE, 0); myTime.set(Calendar.HOUR_OF_DAY, 0); return myTime; } }; // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Inject Test Stub into SUT: sut.setTimeProvider(testStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nIf any of the other methods are called, the base class methods are invoked and throw an exception. Therefore, if we run this test and one of the methods we didn’t override is called, we will see the following output as the ﬁ rst line of the JUnit stack trace for this test error:\n\ncom..PseudoClassEx: Unexpected call to unsupported method. at com..PseudoTimeProvider.getTime(PseudoTimeProvider.java:22) at com..TimeDisplay.getCurrentTimeAsHtmlFragment(TimeDisplay.java:64) at com..TimeDisplayTestSolution. testDisplayCurrentTime_AtMidnight_PS( TimeDisplayTestSolution.java:247)\n\nwww.it-ebooks.info\n\n575\n\nHard-Coded Test Double",
      "content_length": 1670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 639,
      "content": "576\n\nHard-Coded Test Double\n\nChapter 23 Test Double Patterns\n\nWhat’s in a (Pattern) Name?\n\nThe Importance of Good Names Names are important because they are a key part of how we communicate. Names are labels we attach to concepts. Good names help us communi- cate those concepts. This is true when we are communicating with people who already know the names, but especially when we are communicating with people who don’t. Consider the following example.\n\nEarly in my pattern-writing days, I attended the very ﬁ rst Pattern Languages of Programs (PLoP) conference (http://www.hillside.net/ conferences/plop). At the conference, the well-known author Jim Co- plien (“Cope,” to his friends) had a pattern language of organizational patterns being workshopped. One of the patterns was called “Buffalo Mountain”; another was called “Architect Also Implements.” These two pattern names are at opposite ends of the spectrum as far as pat- tern names are concerned.\n\nThe gist of “Architect Also Implements” can be gleaned from the pattern name even if a person has not read the actual pattern. The name is both a placeholder for the pattern and meaningful in its own right.\n\nThe name “Buffalo Mountain,” by contrast, does not readily communi- cate its underlying meaning. To this day I can still remember the story behind the name—but I cannot remember the actual focus of the pattern. The name was based on a graph that plotted some data related to the pattern. An early reviewer thought it resembled the proﬁ le of a nearby mountain called Buffalo Mountain. Thus, while the pattern name is mem- orable, it is not very evocative.\n\nCloser to home, Self Shunt (see Hard-Coded Test Double on page 568) is an example of a name that is less than evocative because the term “shunt” is not widely used except in a few specialized ﬁ elds. Michael Feathers does a good job explaining the background of the name in his description of the pattern. Unless you’ve read that description, however, the name is “just a name.” A more evocative name might be something like “Testcase Class as Test Double” or “Loopback” but even the latter suffers from ambiguity because it isn’t clear what is being looped back. So the name Self Shunt survives because it is in common use.\n\nwww.it-ebooks.info",
      "content_length": 2269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 640,
      "content": "Hard-Coded Test Double\n\nOther Naming Considerations People might ask why I sometimes propose alternative names for some patterns. The preceding story highlights one of the reasons. Another reason is that in a larger collection of patterns (such as this book), it is important that there exists a “system of names.”\n\nLet me illustrate this second reason with an example. Many people advocate the use of a setUp method to create the test ﬁ xture. This approach moves the ﬁ xture setup logic out of each individual Test Method (page 348) and into a single place where it can be reused. Many people might refer to this pattern as “Shared Setup Method.” But in this pattern language, I’ve chosen to call it Implicit Setup (page 424). Why?\n\nIt comes down to the names of other patterns in the language. On the one hand, “Shared Setup Method” could easily be confused with the existing pattern Shared Fixture (page 317). (The former pattern deals with sharing code, whereas the latter pattern focuses on sharing the runtime objects in the ﬁ xture.) On the other hand, the two major alternatives to Implicit Setup are called In-line Setup (page 408) and Delegated Setup (page 411). Wouldn’t you agree that “In-line Setup, Delegated Setup, Implicit Setup” forms a better “system of names” than “In-line Setup, Delegated Setup, Shared Setup Method”? The connection between the pattern names is much more obvious when we consider all the major alternative patterns when choosing the system of names.\n\nWhy Standardize Testing Patterns? The last part of this soapbox highlights why I think it is important for us to standardize the names of the test automation patterns, especially those related to Test Stubs (page 529) and Mock Objects (page 544). The key issue here relates to succinctness of communication.\n\nWhen someone tells you, “Put a mock in it” (pun intended!), what advice is that person giving you? Depending on what the person means by a “mock,” he or she could be suggesting that you control the indirect inputs of your SUT using a Test Stub or that you replace your database with a Fake Database (see Fake Object on page 551) that will reduce test inter- actions and speed up your tests by a factor of 50. (Yes, 50! See the sidebar “Faster Tests Without Shared Fixtures” on page 319.) Or perhaps the person is suggesting that you verify that your SUT calls the correct meth- ods by installing an Eager Mock Object (see Mock Object) preconﬁ gured\n\nContinued...\n\nwww.it-ebooks.info\n\n577\n\nHard-Coded Test Double",
      "content_length": 2510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 641,
      "content": "578\n\nHard-Coded Test Double\n\nChapter 23 Test Double Patterns\n\nwith the Expected Behavior (see Behavior Veriﬁ cation on page 468). If everyone used “mock” to mean a Mock Object—no more or less—then the advice would be pretty clear. As I write this, the advice is very murky because we have taken to calling just about any Test Double (page 522) a “mock object” (despite the objections of the authors of the original paper on Mock Objects [ET]).\n\nFurther Reading If you want to ﬁ nd out what “Buffalo Mountain” is really about, go to http://www1.bell-labs.com/user/cope/Patterns/Process/section29.html.\n\nYou can ﬁ nd “Architect Also Implements” at http://www1.bell-labs.com/ user/cope/Patterns/Process/section16.html.\n\nInterestingly, Alistair Cockburn wrote a similar comparison of pattern names in an article on his Web site (http://alistair.cockburn.us) and chose exactly the same two pattern names in his comparison. Coincidence or pattern?\n\nIn addition to failing the test, this scheme makes it very easy to see exactly which method was called. The bonus is that it works for calls to all unexpected methods with no additional effort.\n\nFurther Reading\n\nMany of the “how to” books on test-driven development provide examples of Self Shunt, including [TDD-APG], [TDD-BE], [UTwJ], [PUT], and [JuPG]. The original write-up was by Michael Feathers and is accessible at http://www.objectmentor. com/resources/articles/SelfShunPtrn.pdf\n\nThe original “Shunt” pattern is written up at http://http://c2.com/cgi/wiki? ShuntPattern, along with a list of alternative names including “Loopback.” See the sidebar “What’s in a (Pattern) Name?” on page 576 for a discussion of how to select meaningful and evocative pattern names.\n\nThe Pseudo-Object pattern is described in the paper “Pseudo-Classes: Very Simple and Lightweight Mock Object-like Classes for Unit-Testing” available at http://www.devx.com/Java/Article/22599/1954?pf=true.\n\nwww.it-ebooks.info",
      "content_length": 1942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 642,
      "content": "Test-Specific Subclass\n\nTest-Speciﬁ c Subclass\n\nHow can we make code testable when we need to access private state of the SUT?\n\nWe add methods that expose the state or behavior needed by the test to a subclass of the SUT.\n\nSUT SUT\n\nExercise Exercise\n\nMethod Under Test Method Under Test\n\nSetup Setup\n\nCreate Create\n\nInternal Method Internal Method\n\nExercise Exercise\n\nVerify Verify\n\nSet State Set State\n\nGet State Get State\n\nTest- Test- Specific Specific Subclass Subclass\n\nInternal Method Internal Method\n\nOverridden Overridden Self Call Self Call\n\nTeardown Teardown\n\nIf the SUT was not designed speciﬁ cally to be testable, we may ﬁ nd that the test cannot gain access to a state that it must initialize or verify at some point in the test.\n\nA Test-Speciﬁ c Subclass is a simple yet very powerful way to open up the\n\nSUT for testing purposes without modifying the code of the SUT itself.\n\nHow It Works\n\nWe deﬁ ne a subclass of the SUT and add methods that modify the behavior of the SUT just enough to make it testable by implementing control points and observation points. This effort typically involves exposing instance variables using setters and getters or perhaps adding a method to put the SUT into a speciﬁ c state without moving through its entire life cycle.\n\nwww.it-ebooks.info\n\n579\n\nAlso known as: Test-Speciﬁ c Extension\n\nTest-Speciﬁ c Subclass",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 643,
      "content": "580\n\nTest-Speciﬁ c Subclass\n\nAlso known as: Subclassed Test Double\n\nChapter 23 Test Double Patterns\n\nBecause the Test-Speciﬁ c Subclass would be packaged together with the tests that use it, the use of a Test-Speciﬁ c Subclass does not change how the SUT is seen by the rest of the application.\n\nWhen to Use It\n\nWe should use a Test-Speciﬁ c Subclass whenever we need to modify the SUT to improve its testability but doing so directly would result in Test Logic in Produc- tion (page 217). Although we can use a Test-Speciﬁ c Subclass for a number of purposes, all of those scenarios share a common goal: They improve testability by letting us get at the insides of the SUT more easily. A Test-Speciﬁ c Subclass can be a double-edged sword, however. By breaking encapsulation, it allows us to tie our tests even more closely to the implementation, which can in turn result in Fragile Tests (page 239).\n\nVariation: State-Exposing Subclass\n\nIf we are doing State Veriﬁ cation (page 462), we can subclass the SUT (or some component of it) so that we can see the internal state of the SUT for use in Assertion Methods (page 362). Usually, this effort involves adding accessor methods for pri- vate instance variables. We may also allow the test to set the state as a way to avoid Obscure Tests (page 186) caused by Obscure Setup (see Obscure Test) logic.\n\nVariation: Behavior-Exposing Subclass\n\nIf we want to test the individual steps of a complex algorithm individually, we can subclass the SUT to expose the private methods that implement the Self- Calls [WWW]. Because most languages do not allow for relaxing the visibility of a method, we often have to use a different name in the Test-Speciﬁ c Subclass and make a call to the superclass’s method.\n\nVariation: Behavior-Modifying Subclass\n\nIf the SUT contains some behavior that we do not want to occur when testing, we can override whatever method implements the behavior with an empty method body. This technique works best when the SUT uses Self-Calls (or a Template Method [GOF]) to delegate the steps of an algorithm to methods on itself or subclasses.\n\nVariation: Test Double Subclass\n\nTo ensure that a Test Double (page 522) is type-compatible with a DOC we wish to replace, we can make the Test Double a subclass of that component. This may\n\nwww.it-ebooks.info",
      "content_length": 2318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 644,
      "content": "Test-Specific Subclass\n\nbe the only way we can build a Test Double that the compiler will accept when variables are statically typed using concrete classes.5 (We should not have to take this step with dynamically typed languages such as Ruby, Python, Perl, and JavaScript.) We then override any methods whose behavior we want to change and add any methods we require to transform the Test Double into a Conﬁ gu- rable Test Double (page 558) if we so desire.\n\nUnlike the Behavior-Modifying Subclass, the Test Double Subclass does not just “tweak” the behavior of the SUT (or a part thereof) but replaces it entirely with canned behavior.\n\nVariation: Substituted Singleton\n\nThe Substituted Singleton is a special case of Test Double Subclass. We use it when we want to replace a DOC with a Test Double and the SUT does not sup- port Dependency Injection (page 678) or Dependency Lookup (page 686).\n\nImplementation Notes\n\nThe use of a Test-Speciﬁ c Subclass brings some challenges:\n\nFeature granularity: ensuring that any behavior we want to override or expose is in its own single-purpose method. It is enabled through copi- ous use of small methods and Self-Calls.\n\nFeature visibility: ensuring that subclasses can access attributes and be- havior of the SUT class. It is primarily an issue in statically typed lan- guages such as Java, C#, and C++; dynamically typed languages typically do not enforce visibility.\n\nAs with Test Doubles, we must be careful to ensure that we do not replace any of the behavior we are actually trying to test.\n\nIn languages that support class extensions without the need for subclassing (e.g., Smalltalk, Ruby, JavaScript, and other dynamic languages), a Test-Speciﬁ c Subclass can be implemented as a class extension in the test package. We need to be aware, however, whether the extensions will make it into production; doing so would introduce Test Logic in Production.\n\n5 That is, by using a concrete class as the type of the variable rather than an abstract class or interface.\n\nwww.it-ebooks.info\n\n581\n\nAlso known as: Subclassed Singleton, Substitutable Singleton\n\nTest-Speciﬁ c Subclass",
      "content_length": 2124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 645,
      "content": "582\n\nTest-Speciﬁ c Subclass\n\nChapter 23 Test Double Patterns\n\nVisibility of Features\n\nIn languages that enforce scope (visibility) of variables and methods, we may need to change the visibility of the variables to allow subclasses to access them. While such a change affects the actual SUT code, it would typically be con- sidered much less intrusive or misleading than changing the visibility to public (thereby allowing any code in the application to access the variables) or adding the test-speciﬁ c methods directly to the SUT.\n\nFor example, in Java, we might change the visibility of instance variables from private to protected to allow the Test-Speciﬁ c Subclass to access them. Similarly, we might change the visibility of methods to allow the Test-Speciﬁ c Subclass to call them.\n\nGranularity of Features\n\nLong methods are difﬁ cult to test because they often bring too many dependen- cies into play. By comparison, short methods tend to be much simpler to test because they do only one thing. Self-Call offers an easy way to reduce the size of methods. We delegate parts of an algorithm to other methods implemented on the same class. This strategy allows us to test these methods independently. We can also conﬁ rm that the calling method calls these methods in the right sequence by overriding them in a Test Double Subclass (see Test-Speciﬁ c Subclass on page 579).\n\nSelf-Call is a part of good object-oriented code design in that it keeps methods small and focused on implementing a single responsibility of the SUT. We can use this pattern whenever we are doing test-driven development and have control over the design of the SUT. We may ﬁ nd that we need to introduce Self-Call when we encounter long methods where some parts of the algorithm depend on things we do not want to exercise (e.g., database calls). This likelihood is especially high, for example, when the SUT is built using a Transaction Script [PEAA] architecture. Self-Call can be retroﬁ tted easily using the Extract Method [Fowler] refactoring supported by most modern IDEs.\n\nMotivating Example\n\nThe test in the following example is nondeterministic because it depends on the time. Our SUT is an object that formats the time for display as part of a Web page. It gets the time by asking a Singleton called TimeProvider to retrieve the time from a calendar object that it gets from the container.\n\npublic void testDisplayCurrentTime_AtMidnight() throws Exception { // Set up SUT\n\nwww.it-ebooks.info",
      "content_length": 2481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 646,
      "content": "Test-Specific Subclass\n\nTimeDisplay theTimeDisplay = new TimeDisplay(); // Exercise SUT String actualTimeString = theTimeDisplay.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( \"Midnight\", expectedTimeString, actualTimeString); }\n\npublic void testDisplayCurrentTime_AtOneMinuteAfterMidnight() throws Exception { // Set up SUT TimeDisplay actualTimeDisplay = new TimeDisplay(); // Exercise SUT String actualTimeString = actualTimeDisplay.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">12:01 AM</span>\"; assertEquals( \"12:01 AM\", expectedTimeString, actualTimeString); }\n\nThese tests rarely pass, and they never pass in the same test run! The code within the SUT looks like this:\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar timeProvider; try { timeProvider = getTime(); } catch (Exception e) { return e.getMessage(); } // etc. }\n\nprotected Calendar getTime() { return TimeProvider.getInstance().getTime(); }\n\nThe code for the Singleton follows:\n\npublic class TimeProvider { protected static TimeProvider soleInstance = null;\n\nprotected TimeProvider() {};\n\npublic static TimeProvider getInstance() {\n\nwww.it-ebooks.info\n\n583\n\nTest-Speciﬁ c Subclass",
      "content_length": 1311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 647,
      "content": "584\n\nTest-Speciﬁ c Subclass\n\nChapter 23 Test Double Patterns\n\nif (soleInstance==null) soleInstance = new TimeProvider(); return soleInstance; }\n\npublic Calendar getTime() { return Calendar.getInstance(); } }\n\nRefactoring Notes\n\nThe precise nature of the refactoring employed to introduce a Test-Speciﬁ c Subclass depends on why we are using one. When we are using a Test-Speciﬁ c Subclass to expose “private parts” of the SUT or override undesirable parts of its behavior, we merely deﬁ ne the Test-Speciﬁ c Subclass as a subclass of the SUT and create an instance of the Test-Speciﬁ c Subclass to exercise in the setup ﬁ xture phase of our Four-Phase Test (page 358).\n\nWhen we are using the Test-Speciﬁ c Subclass to replace a DOC of the SUT, however, we need to use a Replace Dependency with Test Double (page 522) refactoring to tell the SUT to use our Test-Speciﬁ c Subclass instead of the real DOC.\n\nIn either case, we either override existing methods or add new methods to the Test-Speciﬁ c Subclass using our language-speciﬁ c capabilities (e.g., subclass- ing or mixins) as required by our tests.\n\nExample: Behavior-Modifying Subclass (Test Stub)\n\nBecause the SUT uses a Self-Call to the getTime method to ask the TimeProvider for the time, we have an opportunity to use a Subclassed Test Double to control the time.6 Based on this idea we can take a stab at writing our tests as follows (I have shown only one test here):\n\npublic void testDisplayCurrentTime_AtMidnight() { // Fixture setup TimeDisplayTestStubSubclass tss = new TimeDisplayTestStubSubclass(); TimeDisplay sut = tss; // Test Double conﬁguration tss.setHours(0); tss.setMinutes(0); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment();\n\n6 This decision is enabled by the fact that getTime was deﬁ ned to be protected; we would not be able to do this if it was private.\n\nwww.it-ebooks.info",
      "content_length": 1875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 648,
      "content": "Test-Specific Subclass\n\n// Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result ); }\n\nNote that we have used the Test-Speciﬁ c Subclass class for the variable that receives the instance of the SUT; this approach ensures that the methods of the Conﬁ gura- tion Interface (see Conﬁ gurable Test Double) deﬁ ned on the Test-Speciﬁ c Subclass are visible to the test.7 For documentation purposes, we have then assigned the Test-Speciﬁ c Subclass to the variable sut; this is a safe cast because the Test-Speciﬁ c Subclass class is a subclass of the SUT class. This technique also helps us avoid the Mystery Guest (see Obscure Test) problem caused by hard-coding an important indirect input of our SUT inside the Test Stub (page 529).\n\nNow that we have seen how it will be used, it is a simple matter to imple-\n\nment the Test-Speciﬁ c Subclass:\n\npublic class TimeDisplayTestStubSubclass extends TimeDisplay {\n\nprivate int hours; private int minutes;\n\n// Overridden method protected Calendar getTime() { Calendar myTime = new GregorianCalendar(); myTime.set(Calendar.HOUR_OF_DAY, this.hours); myTime.set(Calendar.MINUTE, this.minutes); return myTime; } /* * Conﬁguration Interface */ public void setHours(int hours) { this.hours = hours; }\n\npublic void setMinutes(int minutes) { this.minutes = minutes; } }\n\nThere’s no rocket science here—we just had to implement the methods used by the test.\n\n7 We could have used a Hard-Coded Test Double (page 568) subclass instead, but that tactic would have required a different Test-Speciﬁ c Subclass for each time we want to test with. Each subclass would simply hard-code the return value of the getTime method.\n\nwww.it-ebooks.info\n\n585\n\nTest-Speciﬁ c Subclass",
      "content_length": 1775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 649,
      "content": "586\n\nTest-Speciﬁ c Subclass\n\nChapter 23 Test Double Patterns\n\nExample: Behavior-Modifying Subclass (Substituted Singleton)\n\nSuppose our getTime method was declared to beprivate8 or static, ﬁ nal or sealed, and so on.9 Such a declaration would prevent us from overriding the method’s behavior in our Test-Speciﬁ c Subclass. What could we do to address our Nondeterministic Tests (see Erratic Test on page 228)?\n\nBecause the design uses a Singleton [GOF] to provide the time, a simple solution is to replace the Singleton during test execution with a Test Double Subclass. We can do so as long as it is possible for a subclass to access its soleInstance variable. We use the Introduce Local Extension [Fowler] refactoring (speciﬁ cally, the subclass variant of it) to create the Test-Speciﬁ c Subclass. Writ- ing the tests ﬁ rst helps us understand the interface we want to implement.\n\npublic void testDisplayCurrentTime_AtMidnight() { TimeDisplay sut = new TimeDisplay(); // Install test Singleton TimeProviderTestSingleton timeProvideSingleton = TimeProviderTestSingleton.overrideSoleInstance(); timeProvideSingleton.setTime(0,0); // Exercise SUT String actualTimeString = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, actualTimeString ); }\n\nNow that we have a test that uses the Substituted Singleton, we can proceed to implement it by subclassing the Singleton and deﬁ ning the methods the tests will use.\n\npublic class TimeProviderTestSingleton extends TimeProvider { private Calendar myTime = new GregorianCalendar(); private TimeProviderTestSingleton() {};\n\n// Installation Interface static TimeProviderTestSingleton overrideSoleInstance() { // We could save the real instance ﬁrst, but we won't! soleInstance = new TimeProviderTestSingleton(); return (TimeProviderTestSingleton) soleInstance; }\n\n// Conﬁguration Interface used by the test\n\n8 A private method cannot be seen or overridden by a subclass. 9 This choice prevents a subclass from overriding the method’s behavior.\n\nwww.it-ebooks.info",
      "content_length": 2119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 650,
      "content": "Test-Specific Subclass\n\npublic void setTime(int hours, int minutes) { myTime.set(Calendar.HOUR_OF_DAY, hours); myTime.set(Calendar.MINUTE, minutes); }\n\n// Usage Interface used by the client public Calendar getTime() { return myTime; } }\n\nHere the Test Double is a subclass of the real component and has overridden the instance method called by the clients of the Singleton.\n\nExample: Behavior-Exposing Subclass\n\nSuppose we wanted to test the getTime method directly. Because getTime is protected and our test is in a different package from the TimeDisplay class, our test cannot call this method. We could try making our test a subclass of TimeDisplay or we could put it into the same package as TimeDisplay. Unfortunately, both of these solutions come with baggage and may not always be possible.\n\nA more general solution is to expose the behavior using a Behavior-Exposing Subclass. We can do so by deﬁ ning a Test-Speciﬁ c Subclass and adding a public method that calls this method.\n\npublic class TimeDisplayBehaviorExposingTss extends TimeDisplay {\n\npublic Calendar callGetTime() { return super.getTime(); } }\n\nWe can now write the test using the Behavior-Exposing Subclass as follows:\n\npublic void testGetTime_default() { // create SUT TimeDisplayBehaviorExposingTss tsSut = new TimeDisplayBehaviorExposingTss(); // exercise SUT // want to do // Calendar time = sut.getTime(); // have to do Calendar time = tsSut.callGetTime(); // verify outcome assertEquals( defaultTime, time ); }\n\nwww.it-ebooks.info\n\n587\n\nTest-Speciﬁ c Subclass",
      "content_length": 1536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 651,
      "content": "588\n\nTest-Speciﬁ c Subclass\n\nChapter 23 Test Double Patterns\n\nExample: Deﬁ ning Test-Speciﬁ c Equality (Behavior-Modifying Subclass)\n\nHere is an example of a very simple test that fails because the object we pass to assertEquals does not implement test-speciﬁ c equality. That is, the default equals method returns false even though our test considers the two objects to be equals.\n\nprotected void setUp() throws Exception { oneOutboundFlight = ﬁndOneOutboundFlightDto(); }\n\npublic void testGetFlights_OneFlight() throws Exception { // Exercise System List ﬂights = facade.getFlightsByOriginAirport( oneOutboundFlight.getOriginAirportId()); // Verify Outcome assertEquals(\"Flights at origin - number of ﬂights: \", 1, ﬂights.size()); FlightDto actualFlightDto = (FlightDto)ﬂights.get(0); assertEquals(\"Flight DTOs at origin\", oneOutboundFlight, actualFlightDto); }\n\nOne option is to write a Custom Assertion (page 474). Another option is to use a Test-Speciﬁ c Subclass to add a more appropriate deﬁ nition of equality for our test purposes alone. We can change our ﬁ xture setup code slightly to create the Test-Speciﬁ c Subclass as our Expected Object (see State Veriﬁ cation).\n\nprivate FlightDtoTss oneOutboundFlight;\n\nprivate FlightDtoTss ﬁndOneOutboundFlightDto() { FlightDto realDto = helper.ﬁndOneOutboundFlightDto(); return new FlightDtoTss(realDto) ; }\n\nFinally, we implement the Test-Speciﬁ c Subclass by copying and comparing only those ﬁ elds that we want to use for our test-speciﬁ c equality.\n\npublic class FlightDtoTss extends FlightDto { public FlightDtoTss(FlightDto realDto) { this.destAirportId = realDto.getDestinationAirportId(); this.equipmentType = realDto.getEquipmentType(); this.ﬂightNumber = realDto.getFlightNumber(); this.originAirportId = realDto.getOriginAirportId(); }\n\nwww.it-ebooks.info",
      "content_length": 1819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 652,
      "content": "Test-Specific Subclass\n\npublic boolean equals(Object obj) { FlightDto otherDto = (FlightDto) obj; if (otherDto == null) return false; if (otherDto.getDestAirportId()!= this.destAirportId) return false; if (otherDto.getOriginAirportId()!= this.originAirportId) return false; if (otherDto.getFlightNumber()!= this.ﬂightNumber) return false; if (otherDto.getEquipmentType() != this.equipmentType ) return false; return true; } }\n\nIn this case we copied the ﬁ elds from the real DTO into our Test-Speciﬁ c Subclass, but we could just as easily have used the Test-Speciﬁ c Subclass as a wrapper for the real DTO. There are other ways we could have created the Test-Speciﬁ c Subclass; the only real limit is our imagination.\n\nThis example also assumes that we have a reasonable toString implementa- tion on our base class that prints out the values of the ﬁ elds being compared. It is needed because assertEquals will use that implementation when the equals method returns false. Otherwise, we will have no idea of why the objects are considered unequal.\n\nExample: State-Exposing Subclass\n\nSuppose we have the following test, which requires a Flight to be in a particular state:\n\nprotected void setUp() throws Exception { super.setUp(); scheduledFlight = createScheduledFlight(); }\n\nFlight createScheduledFlight() throws InvalidRequestException{ Flight newFlight = new Flight(); newFlight.schedule(); return newFlight; }\n\npublic void testDeschedule_shouldEndUpInUnscheduleState() throws Exception { scheduledFlight.deschedule(); assertTrue(\"isUnsched\", scheduledFlight.isUnscheduled()); }\n\nwww.it-ebooks.info\n\n589\n\nTest-Speciﬁ c Subclass",
      "content_length": 1631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 653,
      "content": "590\n\nTest-Speciﬁ c Subclass\n\nChapter 23 Test Double Patterns\n\nSetting up the ﬁ xture for this test requires us to call the method schedule on the ﬂ ight:\n\npublic class Flight{ protected FlightState currentState = new UnscheduledState();\n\n/** * Transitions the Flight from the <code>unscheduled</code> * state to the <code>scheduled</code> state. * @throws InvalidRequestException when an invalid state * transition is requested */ public void schedule() throws InvalidRequestException{ currentState.schedule(); } }\n\nThe Flight class uses the State [GOF] pattern and delegates handling of the schedule method to whatever State object is currently referenced by currentState. This test will fail during ﬁ xture setup if schedule does not work yet on the default content of currentState. We can avoid this problem by using a State-Exposing Subclass that provides a method to move directly into the state, thereby making this an Inde- pendent Test (see page 42).\n\npublic class FlightTss extends Flight {\n\npublic void becomeScheduled() { currentState = new ScheduledState(); } }\n\nBy introducing a new method becomeScheduled on the Test-Speciﬁ c Subclass, we ensure that we will not accidentally override any existing behavior of the SUT. Now all we have to do is instantiate the Test-Speciﬁ c Subclass in our test instead of the base class by modifying our Creation Method (page 415).\n\nFlight createScheduledFlight() throws InvalidRequestException{ FlightTss newFlight = new FlightTss(); newFlight.becomeScheduled(); return newFlight; }\n\nNote how we still declare that we are returning an instance of the Flight class when we are, in fact, returning an instance of the Test-Speciﬁ c Subclass that has the additional method.\n\nwww.it-ebooks.info",
      "content_length": 1738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 654,
      "content": "Chapter 24\n\nTest Organization Patterns\n\nPatterns in This Chapter\n\nNamed Test Suite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592\n\nTest Code Reuse\n\nTest Utility Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599\n\nParameterized Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607\n\nTestcase Class Structure\n\nTestcase Class per Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617\n\nTestcase Class per Feature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n\nTestcase Class per Fixture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631\n\nUtility Method Location\n\nTestcase Superclass. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638\n\nTest Helper. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643\n\n591\n\nwww.it-ebooks.info\n\nTest Organization Patterns",
      "content_length": 935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 655,
      "content": "592\n\nNamed Test Suite\n\nChapter 24 Test Organization Patterns\n\nNamed Test Suite\n\nHow do we run the tests when we have arbitrary groups of tests to run?\n\nWe deﬁ ne a test suite, suitably named, that contains a set of tests that we wish to be able to run as a group.\n\nTestcase Class Testcase Class\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\nFixture Fixture\n\ntestMethod_1 testMethod_1\n\nImplicit tearDown Implicit tearDown\n\nTest Test Suite Suite Factory Factory\n\nCreation Creation\n\nTest Test Suite Suite Object Object\n\nSUT SUT\n\nTestcase Object Testcase Object Implicit setUp Implicit setUp\n\ntestMethod_n testMethod_n\n\nImplicit tearDown Implicit tearDown\n\nWhen we have a large number of tests, we need to organize them in a systematic way. A test suite allows us to group tests that have related functionality close to each other. Although we want to be able to run all the tests for the entire applica- tion or component easily, we also want to be able to run only those tests applicable to speciﬁ c subsets of the functionality or subcomponents of the system. In other situations, we want to run only a subset of all the tests we have deﬁ ned.\n\nNamed Test Suites give us a way to choose which predeﬁ ned subset of the\n\ntests we want to run.\n\nHow It Works\n\nFor each group of related tests that we would like to be able to run as a group, we can deﬁ ne a special Test Suite Factory (see Test Enumeration on page 399) with an Intent-Revealing Name. The Factory Method [GOF] can use any of several\n\nwww.it-ebooks.info",
      "content_length": 1530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 656,
      "content": "Named Test Suite\n\ntest suite construction techniques to return a Test Suite Object (page 387) containing only the speciﬁ c Testcase Objects (page 382) we wish to execute.\n\nWhen to Use It\n\nAlthough we often want to run all the tests with a single command, sometimes we want to run only a subset of the tests. The most common reason for doing so is time; for this purpose, running the AllTests Suite for a speciﬁ c context is prob- ably our best bet. When our member of xUnit doesn’t support Test Selection and the tests we want to run are scattered across multiple contexts and some contexts contain tests we deﬁ nitely don’t want run, we can use a Subset Suite.\n\nVariation: AllTests Suite\n\nWe often want to run all the tests we have available. With smaller systems, it may be standard practice to run the AllTests Suite after checking out a new code base (to ensure we start at a known point) and before every check-in (to ensure all our code works). We typically have an AllTests Suite for each package or namespace of software so that we can run subsets of the tests after each code change as part of the “red–green–refactor” cycle.\n\nVariation: Subset Suite\n\nDevelopers often do not want to run tests because they are Slow Tests (page 253). Tests that exercise components that access a database will inevitably run much more slowly than tests that run entirely in memory. By deﬁ ning one Named Test Suite for the database tests and another Named Test Suite for the in-memory tests, we can choose not to run the database tests simply by choosing to run the in-memory Subset Suite.\n\nAnother common reason given for not running tests is because the context they need to run is not available. For example, if we don’t have a Web server running on our development desktop, or if deploying our software to the Web server takes too long, we won’t want to run the tests of components that require the Web server to be running (they would just take extra time to run, and we know they will fail and spoil our chances of achieving a green bar).\n\nVariation: Single Test Suite\n\nThe degenerate form of a Subset Suite is the Single Test Suite, in which we instanti- ate a single Testcase Object so that we can run a single Test Method (page 348). This variation is particularly useful when we don’t have a Test Tree Explorer (see Test Runner on page 377) available or when the Test Method requires some\n\nwww.it-ebooks.info\n\n593\n\nNamed Test Suite",
      "content_length": 2433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 657,
      "content": "594\n\nNamed Test Suite\n\nChapter 24 Test Organization Patterns\n\nform of Setup Decorator (page 447) to run properly. Some test automaters keep a “MyTest” Testcase Class (page 373) open in their workspace at all times speciﬁ - cally for this purpose.\n\nImplementation Notes\n\nThe concept of running named sets of tests is independent of how we build the Named Test Suites. For example, we can use Test Enumeration to build up our suites of tests explicitly or we can use Test Discovery (page 393) to ﬁ nd all tests in a particular place (e.g., a namespace or assembly). We can also do Test Selec- tion (page 403) from within a suite of tests to create a smaller suite dynamically. Some members of the xUnit family require us to deﬁ ne the AllTests Suites for each test package or subsystem manually; others, such as NUnit, automatically create a Test Suite Object for each namespace.\n\nWhen we are using Test Enumeration and have Named Test Suites for various subsets of the tests, it is better to deﬁ ne our AllTests Suite in terms of these subsets. When we implement the AllTests Suite as a Suite of Suites (see Test Suite Object), we need to add a new Testcase Class to only a single Named Test Suite; this collection of tests is then rolled up into the AllTests Suite for the local context as well as the Named Test Suite and the next higher context.\n\nRefactoring Notes\n\nThe steps to refactor existing code to a Named Test Suite are highly dependent on the variant of Named Test Suite we are using. For this reason, I’ll dispense with the motivating example and skip directly to examples of Named Test Suites.\n\nExample: AllTests Suite\n\nAn AllTests Suite helps us run all the tests for different subsets of the functional- ity of our choosing. For each subcomponent or context (e.g., a Java package), we deﬁ ne a special test suite (and its corresponding Test Suite Factory) called AllTests. In the suite Factory Method on the Test Suite Factory, we add all the tests in the current context and all the Named Test Suites from any nested con- texts (such as nested Java packages). That way, when the top-level Named Test Suite is run, all Named Test Suites for the nested contexts will be run as well.\n\nThe following example illustrates the kind of code that would be required to\n\nrun all the tests in most members of the xUnit family:\n\npublic class AllTests {\n\npublic static Test suite() {\n\nwww.it-ebooks.info",
      "content_length": 2405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 658,
      "content": "Named Test Suite\n\nTestSuite suite = new TestSuite(\"Test for allJunitTests\"); //$JUnit-BEGIN$ suite.addTestSuite( com.clrstream.camug.example.test.InvoiceTest.class); suite.addTest(com.clrstream.ex7.test.AllTests.suite()); suite.addTest(com.clrstream.ex8.test.AllTests.suite()); suite.addTestSuite( com.xunitpatterns.guardassertion.Example.class); //$JUnit-END$ return suite; } }\n\nWe had to use a mix of methods in this case because we are adding other Named Test Suites as well as Test Suite Objects representing a single Testcase Class. In JUnit, we use different methods to do this. Other members of the xUnit family, however, may use the same method signature.\n\nThe other notable aspect of this example is the JUnit-start and JUnit-end com- ments. The IDE (in this case, Eclipse) helps us out by automatically regener- ating the list between these two comments—a semi-automated form of Test Discovery.\n\nExample: Special-Purpose Suite\n\nSuppose we have three major packages (A, B, and C) containing business logic. Each package contains both in-memory objects and database access classes. We would then have corresponding test packages for each of the three packages. Some tests in each package would require the database, while others could run purely in memory.\n\nWe want to be able to run the following sets of tests for the entire system,\n\nand for each package (A, B, and C):\n\nAll tests\n\nAll database tests\n\nAll in-memory tests\n\nThis implies a total of 12 named sets of tests (three named sets for each of four contexts).\n\nIn each of the three packages (A, B, and C), we should deﬁ ne the following\n\nNamed Test Suites:\n\nwww.it-ebooks.info\n\n595\n\nNamed Test Suite",
      "content_length": 1665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 659,
      "content": "596\n\nNamed Test Suite\n\nChapter 24 Test Organization Patterns\n\n\n\nAllDbTests, by adding all the Testcase Classes containing database tests\n\n\n\nAllInMemoryTests, by adding all the Testcase Classes containing in-memory tests\n\n\n\nAllTests, by combining AllDbTests and AllInMemoryTests\n\nThen, at the top-level testing context, we deﬁ ne Named Test Suites by the same names as follows:\n\n\n\nAllDbTests, by composing all the AllDbTests Testcase Classes from pack- ages A, B, and C\n\n\n\nAllInMemoryTests, by composing all the AllInMemoryTests Testcase Classes from packages A, B, and C\n\n\n\nAllTests, by composing all the AllTests Testcase Classes from packages A, B, and C (This is just the normal AllTests Suite.)\n\nIf we ﬁ nd ourselves needing to include some tests from a single Testcase Class in both Named Test Suites, we should split the class into one class for each context (e.g., database tests and in-memory tests).\n\nExample: Single Test Suite\n\nIn some circumstances—especially when we are using a debugger—it is highly desirable to not run all the tests in a Testcase Class. One way to run only a subset of these tests is to use the Test Tree Explorer provided by some Graphical Test Run- ners (see Test Runner). When this capability isn’t available, a common practice is to disable the tests we don’t want run by either commenting them out, copying the entire Testcase Class and deleting most of the tests, or changing the names or attri- butes of the test that cause them to be included by the Test Discovery algorithm.\n\npublic class LostTests extends TestCase { public LostTests(String name) { super(name); }\n\npublic void xtestOne() throws Exception { fail(\"test not implemented\"); }\n\n/* public void testTwo() throws Exception { fail(\"test not implemented\"); } */\n\nwww.it-ebooks.info",
      "content_length": 1780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 660,
      "content": "Named Test Suite\n\npublic void testSeventeen() throws Exception { assertTrue(true); } }\n\nAll of these approaches suffer from the potential for Lost Tests (see Produc- tion Bugs on page 268) if the means of running a single test is not reversed properly when the situation requiring this testing strategy has passed. A Sin- gle Test Suite makes it possible to run the speciﬁ c test(s) without making any changes to the Testcase Class in question. This technique takes advantage of the fact that most implementations of xUnit require a one-argument constructor on our Testcase Class; this argument consists of the name of the method that this instance of the class will invoke using reﬂ ection. The one-argument construc- tor is called once for each Test Method on the class, and the resulting Testcase Object is added to the Test Suite Object. (This is an example of the Pluggable Behavior [SBPP] pattern.)\n\nWe can run a single test by implementing a Test Suite Factory class with a single method suite that creates an instance of the desired Testcase Class by call- ing the one-argument constructor with the name of the one Test Method to be run. By returning a Test Suite Object containing only this one Testcase Object from suite, we achieve the desired result (running a single test) without touching the target Testcase Class.\n\npublic class MyTest extends TestCase {\n\npublic static Test suite() { return new LostTests(\"testSeventeen\"); } }\n\nI like to keep a Single Test Suite class around all the time and just plug in what- ever test I want to run by changing the import statements and the suite method. Often, I maintain several Single Test Suite classes so I can ﬂ ip back and forth between different tests very quickly. I ﬁ nd this technique easier to do than drill- ing down in the Test Tree Explorer and picking the speciﬁ c test to run manually. (Your mileage may vary!)\n\nExample: Smoke Test Suite\n\nWe can take the idea of a Special-Purpose Suite and combine it with the imple- mentation technique of a Single Test Suite to create a Smoke Test [SCM] suite. This strategy involves picking a representative test or two from each of the major areas of the system and including those tests in a single Test Suite Object.\n\nwww.it-ebooks.info\n\n597\n\nNamed Test Suite",
      "content_length": 2269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 661,
      "content": "598\n\nNamed Test Suite\n\nChapter 24 Test Organization Patterns\n\npublic class SmokeTestSuite extends TestCase { public static Test suite() { TestSuite mySuite = new TestSuite(\"Smoke Tests\");\n\nmySuite.addTest( new LostTests(\"testSeventeen\") ); mySuite.addTest( new SampleTests(\"testOne\") ); mySuite.addTest( new FlightManagementFacadeTest( \"testGetFlightsByOriginAirports_TwoOutboundFlights\")); // add additional tests here as needed... return mySuite; } }\n\nThis scheme won’t test our system thoroughly, but it is a quick way to ﬁ nd out whether some part of the core functionality is broken.\n\nwww.it-ebooks.info",
      "content_length": 608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 662,
      "content": "Test Utility Method\n\nTest Utility Method\n\nHow do we reduce Test Code Duplication?\n\nWe encapsulate the test logic we want to reuse behind a suitably named utility method.\n\nSetup Setup\n\nTest Utility Test Utility Methods Methods Creation Creation Method Method Finder Finder Method Method\n\nFixture Fixture\n\nExercise Exercise\n\nEncapsulation Encapsulation Method Method\n\nSUT SUT SUT SUT\n\nVerify Verify\n\nVerification Verification Method Method\n\nTeardown Teardown\n\nCustom Custom Assertion Assertion\n\nCleanup Cleanup Method Method\n\nAs we write tests, we will invariably ﬁ nd ourselves needing to repeat the same logic in many, many tests. Initially, we will just “clone and twiddle” as we write additional tests that need the same logic. Sooner or later, however, we will come to the realization that this Test Code Duplication (page 213) is starting to cause prob- lems. This point is a good time to think about introducing a Test Utility Method.\n\nHow It Works\n\nThe subroutine and the function were two of the earliest ways devised to reuse logic in several places within a program. A Test Utility Method is just the same principle applied to object-oriented test code. We move any logic that appears in more than one test into a Test Utility Method; we can then call this method from various tests or even several times from within a single test. Of course, we will want to pass in anything that varies from usage to usage as arguments to the Test Utility Method.\n\nwww.it-ebooks.info\n\n599\n\nTest Utility Method",
      "content_length": 1503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 663,
      "content": "600\n\nTest Utility Method\n\nChapter 24 Test Organization Patterns\n\nWhen to Use It\n\nWe should use a Test Utility Method whenever test logic appears in several tests and we want to be able to reuse that logic. We might also use a Test Utility Method because we want to be very sure that the logic works as expected. The best way to achieve that kind of certainty is to write Self-Checking Tests (unit tests—see page 26) for the reusable test logic. Because the Test Methods (page 348) cannot easily be tested, it is best to do this by moving the logic out of the test methods and into Test Utility Methods, where it can be more easily tested.\n\nThe main drawback of using the Test Utility Method pattern is that it creates another API that the test automaters must build and understand. This extra ef- fort can be largely mitigated through the use of Intent-Revealing Names [SBPP] for the Test Utility Methods and through the use of refactoring as the means for deﬁ ning the Test Utility Methods.\n\nThere are as many different kinds of Test Utility Methods as there are kinds of logic in a Test Method. Next, we brieﬂ y summarize some of the most popu- lar kinds. Some of these variations are important enough to warrant their own pattern write-ups in the corresponding section of this book.\n\nVariation: Creation Method\n\nCreation Methods (page 415) are used to create ready-to-use objects as part of ﬁ xture setup. They hide the complexity of object creation and interdependencies from the test. Creation Method has enough variants to warrant addressing this pattern in its own section.\n\nVariation: Attachment Method\n\nAn Attachment Method (see Creation Method) is a special form of Creation Method used to amend already-created objects as part of ﬁ xture setup.\n\nVariation: Finder Method\n\nWe can encapsulate any logic required to retrieve objects from a Shared Fix- ture (page 317) within a function that returns the object(s). We then give this function an Intent-Revealing Name so that anyone reading the test can easily understand the ﬁ xture we are using in this test.\n\nWe should use a Finder Method whenever we need to ﬁ nd an existing Shared Fixture object that meets some criteria and we want to avoid a Fragile Fixture (see Fragile Test on page 239) and High Test Maintenance Cost (page 265). Finder Methods can be used in either a pure Shared Fixture strategy or a hybrid strategy such as Immutable Shared Fixture (see Shared Fixture). Finder\n\nwww.it-ebooks.info",
      "content_length": 2465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 664,
      "content": "Test Utility Method\n\nMethods also help prevent Obscure Tests (page 186) by encapsulating the mechanism of how the required objects are found and exactly which objects to use, thereby enabling the reader to focus on understanding why a particular object is being used and how it relates to the expected outcome described in the assertions. This helps us move toward Tests as Documentation (see page 23).\n\nAlthough most Finder Methods return a single object reference, that object may be the root of a tree of objects (e.g., an invoice might refer to the customer and various addresses as well as containing a list of line items). In some circum- stances, we may choose to deﬁ ne a Finder Method that returns a collection (Array or Hash) of objects, but the use of this type of Finder Method is less common. Finder Methods may also update parameters to pass additional objects back to the test that called them, although this approach is not as intent-revealing as use of a function. I do not recommend initialization of instance variables as a way of passing back objects because it is obscure and keeps us from moving the Finder Method to a Test Helper (page 643) later.\n\nThe Finder Method can ﬁ nd objects in the Shared Fixture in several ways: by using direct references (instance variables or class variables initialized in the ﬁ xture setup logic), by looking the objects up using known keys, or by search- ing for the objects using speciﬁ c criteria. Using direct references or known keys has the advantage of always returning exactly the same object each time the test is run. The main drawback is that some other test may have modiﬁ ed the object such that it may no longer match the criteria implied by the Finder Method’s name. Searching by criteria can avoid this problem, though the resulting tests may take longer to run and might be less deterministic if they use different objects each time they are run. Either way, we must modify the code in fewer places whenever the Shared Fixture is modiﬁ ed (compared to when the objects are used directly within the Test Method).\n\nVariation: SUT Encapsulation Method\n\nAnother reason for using a Test Utility Method is to encapsulate unnecessary knowl- edge of the API of the SUT. What constitutes unnecessary? Any method we call on the SUT that is not the method being tested creates additional coupling between the test and the SUT. Creation Methods and Custom Assertions (page 474) are common enough examples of SUT Encapsulation Methods to warrant their own write-ups as separate patterns. This section focuses on the less common uses of SUT Encapsulation Methods. For example, if the method that we are exercising (or that we use for verifying the outcome) has a complicated signature, we increase the amount of work involved to write and maintain the test code and may\n\nwww.it-ebooks.info\n\n601\n\nTest Utility Method\n\nAlso known as: SUT API Encapsulation",
      "content_length": 2911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 665,
      "content": "602\n\nTest Utility Method\n\nChapter 24 Test Organization Patterns\n\nmake it harder to understand the tests (Obscure Test). We can avoid this problem by wrapping these calls in SUT Encapsulation Methods that are intent-revealing and may have simpler signatures.\n\nVariation: Custom Assertion\n\nCustom Assertions are used to specify test-speciﬁ c equality in a way that is reusable across many tests. They hide the complexity of comparing the expected outcome with the actual outcome. Custom Assertions are typically free of side effects in that they do not interact with the SUT to retrieve the outcome; that task is left to the caller.\n\nVariation: Veriﬁ cation Method\n\nVeriﬁ cation Methods (see Custom Assertion) are used to verify that the expected outcome has occurred. They hide the complexity of verifying the outcome from the test. Unlike Custom Assertions, Veriﬁ cation Methods interact with the SUT.\n\nVariation: Parameterized Test\n\nThe most complete form of the Test Utility Method pattern is the Parameterized Test (page 607). It is, in essence, an almost complete test that can be reused in many circumstances. We simply provide the data that varies from test to test as a parameter and let the Parameterized Test execute all the stages of the Four- Phase Test (page 358) for us.\n\nVariation: Cleanup Method\n\nCleanup Methods1 are used during the ﬁ xture teardown phase of the test to clean up any resources that might still be allocated after the test ends. Refer to the pattern Automated Teardown (page 503) for a more detailed discussion and examples.\n\nImplementation Notes\n\nThe main objection some people have to using Test Utility Methods is that this pattern removes some of the logic from the test, which may make the test harder to read. One way we can avoid this problem when using Test Utility Methods is to give Intent-Revealing Names to the Test Utility Methods. In fact, well-chosen names can make the tests even easier to understand because they\n\n1 One could call this pattern a “Teardown Method,” but that name might be confused with the method used in Implicit Teardown (page 516).\n\nwww.it-ebooks.info",
      "content_length": 2119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 666,
      "content": "Test Utility Method\n\nhelp prevent Obscure Tests by deﬁ ning a Higher Level Language (see page 41) for deﬁ ning tests. It is also helpful to keep the Test Utility Methods relatively small and self-contained. We can achieve this goal by passing all arguments to these methods explicitly as parameters (rather than using instance variables) and by returning any objects that the tests will require as explicit return values or updated parameters.\n\nTo ensure that the Test Utility Methods have Intent-Revealing Names, we should let the tests pull the Test Utility Methods into existence rather than just inventing Test Utility Methods that we think may be needed later. This “out- side-in” approach to writing code avoids “borrowing tomorrow’s trouble” and helps us ﬁ nd the minimal solution.\n\nWriting the reusable Test Utility Method is relatively straightforward. The trickier question is where we would put this method. If the Test Utility Method is needed only in Test Methods in a single Testcase Class (page 373), then we can put it onto that class. If we need the Test Utility Method in several classes, however, the solution becomes a bit more complicated. The key issue relates to type visibility. The client classes need to be able to see the Test Utility Method, and the Test Utility Method needs to be able to see all the types and classes on which it depends. When it doesn’t depend on many types/classes or when everything it depends on is visible from a single place, we can put the Test Utility Method into a common Testcase Superclass (page 638) that we deﬁ ne for our project or company. If it depends on types/classes that cannot be seen from a single place that all the clients can see, then we may need to put the Test Utility Method on a Test Helper in the appropriate test package or subsystem. In larger systems with many groups of domain objects, it is common practice to have one Test Helper for each group (package) of related domain objects.\n\nVariation: Test Utility Test\n\nOne major advantage of using Test Utility Methods is that otherwise Untestable Test Code (see Hard-to-Test Code on page 209) can now be tested with Self- Checking Tests. The exact nature of such tests varies based on the kind of Test Utility Method being tested but a good example is a Custom Assertion Test (see Custom Assertion).\n\nMotivating Example\n\nThe following example shows a test as many novice test automaters would ﬁ rst write it:\n\npublic void testAddItemQuantity_severalQuantity_v1(){ Address billingAddress = null;\n\nwww.it-ebooks.info\n\n603\n\nTest Utility Method",
      "content_length": 2569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 667,
      "content": "604\n\nTest Utility Method\n\nChapter 24 Test Organization Patterns\n\nAddress shippingAddress = null; Customer customer = null; Product product = null; Invoice invoice = null; try { // Fixture Setup billingAddress = new Address(\"1222 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); shippingAddress = new Address(\"1333 1st St SW\", \"Calgary\", \"Alberta\", \"T2N 2V2\", \"Canada\"); customer = new Customer( 99, \"John\", \"Doe\", new BigDecimal(\"30\"), billingAddress, shippingAddress); product = new Product( 88, \"SomeWidget\", new BigDecimal(\"19.99\")); invoice = new Invoice( customer ); // Exercise SUT invoice.addItemQuantity( product, 5 ); // Verify Outcome List lineItems = invoice.getLineItems(); if (lineItems.size() == 1) { LineItem actItem = (LineItem) lineItems.get(0); assertEquals(\"inv\", invoice, actItem.getInv()); assertEquals(\"prod\", product, actItem.getProd()); assertEquals(\"quant\", 5, actItem.getQuantity()); assertEquals(\"discount\", new BigDecimal(\"30\"), actItem.getPercentDiscount()); assertEquals(\"unit price\", new BigDecimal(\"19.99\"), actItem.getUnitPrice()); assertEquals(\"extended\", new BigDecimal(\"69.96\"), actItem.getExtendedPrice()); } else { assertTrue(\"Invoice should have 1 item\", false); } } ﬁnally { // Teardown deleteObject(invoice); deleteObject(product); deleteObject(customer); deleteObject(billingAddress); deleteObject(shippingAddress); } }\n\nThis test is difﬁ cult to understand because it exhibits many code smells, includ- ing Obscure Test and Hard-Coded Test Data (see Obscure Test).\n\nwww.it-ebooks.info",
      "content_length": 1534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 668,
      "content": "Test Utility Method\n\nRefactoring Notes\n\nWe often create Test Utility Methods by mining existing tests for reusable logic when we are writing new tests. We can use an Extract Method [Fowler] refac- toring to pull the code for the Test Utility Method out of one Test Method and put it onto the Testcase Class as a Test Utility Method. From there, we may choose to move the Test Utility Method to a superclass by using a Pull Up Method [Fowler] refactoring or to another class by using a Move Method [Fowler] refactoring.\n\nExample: Test Utility Method\n\nHere’s the refactored version of the earlier test. Note how much simpler this test is to understand than the original version. And this is just one example of what we can achieve by using Test Utility Methods!\n\npublic void testAddItemQuantity_severalQuantity_v13(){ ﬁnal int QUANTITY = 5; ﬁnal BigDecimal CUSTOMER_DISCOUNT = new BigDecimal(\"30\"); // Fixture Setup Customer customer = ﬁndActiveCustomerWithDiscount(CUSTOMER_DISCOUNT); Product product = ﬁndCurrentProductWith3DigitPrice( ); Invoice invoice = createInvoice(customer); // Exercise SUT invoice.addItemQuantity(product, QUANTITY); // Verify Outcome ﬁnal BigDecimal BASE_PRICE = product.getUnitPrice(). multiply(new BigDecimal(QUANTITY)); ﬁnal BigDecimal EXTENDED_PRICE = BASE_PRICE.subtract(BASE_PRICE.multiply( CUSTOMER_DISCOUNT.movePointLeft(2))); LineItem expected = createLineItem( QUANTITY, CUSTOMER_DISCOUNT, EXTENDED_PRICE, product, invoice); assertContainsExactlyOneLineItem(invoice, expected); }\n\nLet’s go through the changes step by step. First, we replaced the code to create the Customer and the Product with calls to Finder Methods that retrieve those objects from an Immutable Shared Fixture. We altered the code in this way because we don’t plan to change these objects.\n\nprotected Customer ﬁndActiveCustomerWithDiscount( BigDecimal percentDiscount) { return CustomerHome.ﬁndCustomerById( ACTIVE_CUSTOMER_WITH_30PC_DISCOUNT_ID); }\n\nwww.it-ebooks.info\n\n605\n\nTest Utility Method",
      "content_length": 2002,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 669,
      "content": "606\n\nTest Utility Method\n\nChapter 24 Test Organization Patterns\n\nNext, we introduced a Creation Method for the Invoice to which we plan to add the LineItem.\n\nprotected Invoice createInvoice(Customer customer) { Invoice newInvoice = new Invoice(customer); registerTestObject(newInvoice); return newInvoice; }\n\nList testObjects; protected void registerTestObject(Object testObject) { testObjects.add(testObject); }\n\nTo avoid the need for In-line Teardown (page 509), we registered each of the objects we created with our Automated Teardown mechanism, which we call from the tearDown method.\n\nprivate void deleteTestObjects() { Iterator i = testObjects.iterator(); while (i.hasNext()) { try { deleteObject(i.next()); } catch (RuntimeException e) { // Nothing to do; we just want to make sure // we continue on to the next object in the list. } } }\n\npublic void tearDown() { deleteTestObjects(); }\n\nFinally, we extracted a Custom Assertion to verify that the correct LineItem has been added to the Invoice.\n\nvoid assertContainsExactlyOneLineItem( Invoice invoice, LineItem expected) { List lineItems = invoice.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actItem = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\",expected, actItem); }\n\nwww.it-ebooks.info",
      "content_length": 1288,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 670,
      "content": "Parameterized Test\n\nParameterized Test\n\nHow do we reduce Test Code Duplication when the same test logic appears in many tests?\n\nWe pass the information needed to do ﬁ xture setup and result veriﬁ cation to a utility method that implements the entire test life cycle.\n\nData Data\n\nTest Test Method1 Method1\n\nData Data\n\nFixture Fixture\n\nTest Test Method2 Method2\n\nSetup Setup\n\nExercise Exercise\n\nSUT SUT\n\nData Data\n\nVerify Verify\n\nTest Test Method n Method n\n\nTeardown Teardown\n\nTesting can be very repetitious not only because we must run the same test over and over again, but also because many of the tests differ only slightly from one another. For example, we might want to run essentially the same test with slightly different system inputs and verify that the actual output varies accord- ingly. Each of these tests would consist of the exact same steps. While having a large number of tests is an excellent way to ensure good code coverage, it is not so attractive from a test maintainability standpoint because any change made to the algorithm of one of the tests must be propagated to all similar tests.\n\nA Parameterized Test offers a way to reuse the same test logic in many Test\n\nMethods (page 348).\n\nwww.it-ebooks.info\n\n607\n\nParame- terized Test",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 671,
      "content": "608\n\nParame- terized Test\n\nChapter 24 Test Organization Patterns\n\nHow It Works\n\nThe solution, of course, is to factor out the common logic into a utility method. When this logic includes all four parts of the entire Four-Phase Test (page 358) life cycle—that is, ﬁ xture setup, exercise SUT, result veriﬁ cation, and ﬁ xture teardown—we call the resulting utility method a Parameterized Test. This kind of test gives us the best coverage with the least code to maintain and makes it very easy to add more tests as they are needed.\n\nIf the right utility method is available to us, we can reduce a test that would otherwise require a series of complex steps to a single line of code. As we detect similarities between our tests, we can factor out the commonalities into a Test Utility Method (page 599) that takes only the information that differs from test to test as its arguments. The Test Methods pass in as parameters any information that the Parameterized Test requires to run and that varies from test to test.\n\nWhen to Use It\n\nWe can use a Parameterized Test whenever Test Code Duplication (page 213) results from several tests implementing the same test algorithm but with slightly different data. The data that differs becomes the arguments passed to the Param- eterized Test, and the logic is encapsulated by the utility method. A Parameterized Test also helps us avoid Obscure Tests (page 186); by reducing the number of times the same logic is repeated, it can make the Testcase Class (page 373) much more compact. A Parameterized Test is also a good steppingstone to a Data- Driven Test (page 288); the name of the Parameterized Test maps to the verb or “action word” of the Data-Driven Test, and the parameters are the attributes.\n\nIf our extracted utility method doesn’t do any ﬁ xture setup, it is called a Veriﬁ cation Method (see Custom Assertion on page 474). If it also doesn’t exercise the SUT, it is called a Custom Assertion.\n\nImplementation Notes\n\nWe need to ensure that the Parameterized Test has an Intent-Revealing Name [SBPP] so that readers of the test will understand what it is doing. This name should imply that the test encompasses the whole life cycle to avoid any con- fusion. One convention is to start or end the name in “test”; the presence of parameters conveys the fact that the test is parameterized. Most members of the xUnit family that implement Test Discovery (page 393) will create only Testcase Objects (page 382) for “no arg” methods that start with “test,” so this restriction shouldn’t prevent us from starting our Parameterized Test names with “test.” At least one member of the xUnit family—MbUnit—implements\n\nwww.it-ebooks.info",
      "content_length": 2679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 672,
      "content": "Parameterized Test\n\nParameterized Tests at the Test Automation Framework (page 298) level. Extensions are becoming available for other members of the xUnit family, with DDSteps for JUnit being one of the ﬁ rst to appear.\n\nTesting zealots would advocate writing a Self-Checking Test (see page 26) to verify the Parameterized Test. The beneﬁ ts of doing so are obvious—including increased conﬁ dence in our tests—and in most cases it isn’t that hard to do. It is a bit harder than writing unit tests for a Custom Assertion because of the inter- action with the SUT. We will likely need to replace the SUT2 with a Test Double so that we can observe how it is called and control what it returns.\n\nVariation: Tabular Test\n\nSeveral early reviewers of this book wrote to me about a variation of Param- eterized Test that they use regularly: the Tabular Test. The essence of this test is the same as that for a Parameterized Test, except that the entire table of values resides in a single Test Method. Unfortunately, this approach makes the test an Eager Test (see Assertion Roulette on page 224) because it veriﬁ es many test conditions. This issue isn’t a problem when all of the tests pass, but it does lead to a lack of Defect Localization (see page 22) when one of the “rows” fails.\n\nAnother potential problem is that “row tests” may depend on one another either on purpose or by accident because they are running on the same Testcase Object; see Incremental Tabular Test for an example of this behavior.\n\nDespite these potential issues, Tabular Tests can be a very effective way to test. At least one member of the xUnit family implements Tabular Tests at the framework level: MbUnit provides an attribute [RowTest] to indicate that a test is a Parameterized Test and another attribute [Row(x,y,...)] to specify the parameters to be passed to it. Perhaps it will be ported to other members of the xUnit family? (Hint, hint!)\n\nVariation: Incremental Tabular Test\n\nAn Incremental Tabular Test is a variant of the Tabular Test pattern in which we deliberately build on the ﬁ xture left over by the previous rows of the test. It is identical to a deliberate form of Interacting Tests (see Erratic Test on page 228)\n\n2 The terminology of SUT becomes very confusing in this case because we cannot replace the SUT with a Test Double if it truly is the SUT. Strictly speaking, we are replacing the object that would normally be the SUT with respect to this test. Because we are actually verifying the behavior of the Parameterized Test, whatever normally plays the role of SUT for this test now becomes a DOC. (My head is starting to hurt just describing this; fortunately, it really isn’t very complicated and will make a lot more sense when you actually try it out.)\n\nwww.it-ebooks.info\n\n609\n\nAlso known as: Row Test\n\nParame- terized Test",
      "content_length": 2831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 673,
      "content": "610\n\nParame- terized Test\n\nChapter 24 Test Organization Patterns\n\ncalled Chained Tests (page 454), except that all the tests reside within the same Test Method. The steps within the Test Method act somewhat like the steps of a “DoFixture” in Fit but without individual reporting of failed steps.3\n\nVariation: Loop-Driven Test\n\nWhen we want to test the SUT with all the values in a particular list or range, we can call the Parameterized Test from within a loop that iterates over the values in the list or range. By nesting loops within loops, we can verify the behavior of the SUT with combinations of input values. The main requirement for doing this type of testing is that we must either enumerate the expected result for each input value (or combination) or use a Calculated Value (see Derived Value on page 718) without introducing Production Logic in Test (see Conditional Test Logic on page 200). A Loop-Driven Test suffers from many of the same issues associated with a Tabular Test, however, because we are hiding many tests inside a single Test Method (and, therefore, Testcase Object).\n\nMotivating Example\n\nThe following example includes some of the runit (Ruby Unit) tests from the Web site publishing infrastructure I built in Ruby while writing this book. All of the Simple Success Tests (see Test Method) for my cross-referencing tags went through the same sequence of steps: deﬁ ning the input XML, deﬁ ning the expected HTML, stubbing out the output ﬁ le, setting up the handler for the XML, extracting the resulting HTML, and comparing it with the expected HTML.\n\ndef test_extref # setup sourceXml = \"<extref id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\" mockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) # execute @handler.printBodyContents # verify assert_equals_html( expectedHtml, mockFile.output, \"extref: html output\") end\n\ndef testTestterm_normal sourceXml = \"<testterm id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\"\n\n3 This is because most members of the xUnit terminate the Test Method on the ﬁ rst failed assertion.\n\nwww.it-ebooks.info",
      "content_length": 2102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 674,
      "content": "Parameterized Test\n\nmockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) @handler.printBodyContents assert_equals_html( expectedHtml, mockFile.output, \"testterm: html output\") end\n\ndef testTestterm_plural sourceXml =\"<testterms id='abc'/>\" expectedHtml = \"<a href='abc.html'>abcs</a>\" mockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) @handler.printBodyContents assert_equals_html( expectedHtml, mockFile.output, \"testterms: html output\") end\n\nEven though we have already factored out much of the common logic into the setupHandler method, some Test Code Duplication remains. In my case, I had at least 20 tests that followed this same pattern (with lots more on the way), so I felt it was worthwhile to make these tests really easy to write.\n\nRefactoring Notes\n\nRefactoring to a Parameterized Test is a lot like refactoring to a Custom Asser- tion. The main difference is that we include the calls to the SUT made as part of the exercise SUT phase of the test within the code to which we apply the Extract Method [Fowler] refactoring. Because these tests are virtually identical once we have deﬁ ned our ﬁ xture and expected results, the rest can be extracted into the Parameterized Test.\n\nExample: Parameterized Test\n\nIn the following tests, we have reduced each test to two steps: initializing two variables and calling a utility method that does all the real work. This utility method is a Parameterized Test.\n\ndef test_extref sourceXml = \"<extref id='abc' />\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<extref>\") end\n\ndef test_testterm_normal sourceXml = \"<testterm id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<testterm>\")\n\nwww.it-ebooks.info\n\n611\n\nParame- terized Test",
      "content_length": 1815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 675,
      "content": "612\n\nParame- terized Test\n\nChapter 24 Test Organization Patterns\n\nend\n\ndef test_testterm_plural sourceXml = \"<testterms id='abc'/>\" expectedHtml = \"<a href='abc.html'>abcs</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<plural>\") end\n\nThe succinctness of these tests is made possible by deﬁ ning the Parameterized Test as follows:\n\ndef generateAndVerifyHtml( sourceXml, expectedHtml, message, &block) mockFile = MockFile.new sourceXml.delete!(\"\\t\") @handler = setupHandler(sourceXml, mockFile ) block.call unless block == nil @handler.printBodyContents actual_html = mockFile.output assert_equal_html( expectedHtml, actual_html, message + \"html output\") actual_html end\n\nWhat distinguishes this Parameterized Test from a Veriﬁ cation Method is that it contains the ﬁ rst three phases of the Four-Phase Test (from setup to verify), whereas the Veriﬁ cation Method performs only the exercise SUT and verify re- sult phases. Note that our tests did not need the teardown phase because we are using Garbage-Collected Teardown (page 500).\n\nExample: Independent Tabular Test\n\nHere’s an example of the same tests coded as a single Independent Tabular Test:\n\ndef test_a_href_Generation row( \"extref\" ,\"abc\",\"abc.html\",\"abc\" ) row( \"testterm\" ,'abc',\"abc.html\",\"abc\" ) row( \"testterms\",'abc',\"abc.html\",\"abcs\") end\n\ndef row( tag, id, expected_href_id, expected_a_contents) sourceXml = \"<\" + tag + \" id='\" + id + \"'/>\" expectedHtml = \"<a href='\" + expected_href_id + \"'>\" + expected_a_contents + \"</a>\" msg = \"<\" + tag + \"> \" generateAndVerifyHtml( sourceXml, expectedHtml, msg) end\n\nwww.it-ebooks.info",
      "content_length": 1596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 676,
      "content": "Parameterized Test\n\nIsn’t this a nice, compact representation of the various test conditions? I simply did an In-line Temp [Fowler] refactoring on the local variables sourceXml and expectedHtml in the argument list of generateAndVerify and “munged” the various Test Methods together into one. Most of the work involved something we won’t have to do in real life: squeeze the table down to ﬁ t within the page-width limit for this book. That constraint forced me to abridge the text in each row and rebuild the HTML and the expected XML within the row method. I chose the name row to better align this example with the MbUnit example provided later in this section but I could have called it something else like test_element.\n\nUnfortunately, from the Test Runner’s (page 377) perspective, this is a single test, unlike the earlier examples. Because the tests all reside within the same Test Method, a failure in any row other than the last will cause a loss of infor- mation. In this example, we need not worry about Interacting Tests because generateAndVerify builds a new test ﬁ xture each time it is called. In the real world, however, we have to be aware of that possibility.\n\nExample: Incremental Tabular Test\n\nBecause a Tabular Test is deﬁ ned in a single Test Method, it will run on a single Testcase Object. This opens up the possibility of building up series of actions. Here’s an example provided by Clint Shank on his blog:\n\npublic class TabularTest extends TestCase { private Order order = new Order(); private static ﬁnal double tolerance = 0.001;\n\npublic void testGetTotal() { assertEquals(\"initial\", 0.00, order.getTotal(), tolerance); testAddItemAndGetTotal(\"ﬁrst\", 1, 3.00, 3.00); testAddItemAndGetTotal(\"second\",3, 5.00, 18.00); // etc. }\n\nprivate void testAddItemAndGetTotal( String msg, int lineItemQuantity, double lineItemPrice, double expectedTotal) { // setup LineItem item = new LineItem( lineItemQuantity, lineItemPrice); // exercise SUT order.addItem(item); // verify total assertEquals(msg,expectedTotal,order.getTotal(),tolerance); } }\n\nwww.it-ebooks.info\n\n613\n\nParame- terized Test",
      "content_length": 2110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 677,
      "content": "614\n\nParame- terized Test\n\nChapter 24 Test Organization Patterns\n\nNote how each row of the Incremental Tabular Test builds on what was already done by the previous row.\n\nExample: Tabular Test with Framework Support (MbUnit)\n\nHere’s an example from the MbUnit documentation that shows how to use the [RowTest] attribute to indicate that a test is a Parameterized Test and another attribute [Row(x,y,...)] to specify the parameters to be passed to it.\n\n[RowTest()] [Row(1,2,3)] [Row(2,3,5)] [Row(3,4,8)] [Row(4,5,9)] public void tAdd(Int32 x, Int32 y, Int32 expectedSum) { Int32 Sum; Sum = this.Subject.Add(x,y); Assert.AreEqual(expectedSum, Sum); }\n\nExcept for the syntactic sugar of the [Row(x,y,...)] attributes, this code sure looks similar to the previous example. It doesn’t suffer from the loss of Defect Local- ization, however, because each row is considered a separate test. It would be a simple matter to convert the previous example to this format using the “ﬁ nd and replace” feature in a text editor.\n\nExample: Loop-Driven Test (Enumerated Values)\n\nThe following test uses a loop to exercise the SUT with various sets of input values:\n\npublic void testMultipleValueSets() { // Set up ﬁxture Calculator sut = new Calculator(); TestValues[] testValues = { new TestValues(1,2,3), new TestValues(2,3,5), new TestValues(3,4,8), // special case! new TestValues(4,5,9) };\n\nfor (int i = 0; i < testValues.length; i++) { TestValues values = testValues[i]; // Exercise SUT int actual = sut.calculate( values.a, values.b); // Verify result\n\nwww.it-ebooks.info",
      "content_length": 1560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 678,
      "content": "Parameterized Test\n\nassertEquals(message(i), values.expectedSum, actual); } }\n\nprivate String message(int i) { return \"Row \"+ String.valueOf(i); }\n\nIn this case we enumerated the expected value for each set of test inputs. This strategy avoids Production Logic in Test.\n\nExample: Loop-Driven Test (Calculated Values)\n\nThis next example is a bit more complex:\n\npublic void testCombinationsOfInputValues() { // Set up ﬁxture Calculator sut = new Calculator(); int expected; // TBD inside loops\n\nfor (int i = 0; i < 10; i++) { for (int j = 0; j < 10; j++) { // Exercise SUT int actual = sut.calculate( i, j );\n\n// Verify result if (i==3 & j==4) // Special case expected = 8; else expected = i+j;\n\nassertEquals(message(i,j), expected, actual); } } }\n\nprivate String message(int i, int j) { return \"Cell( \" + String.valueOf(i)+ \",\" + String.valueOf(j) + \")\"; }\n\nUnfortunately, it suffers from Production Logic in Test because of the need to deal with the special case.\n\nFurther Reading\n\nSee the documentation for MbUnit for more information on the [RowTest] and [Row()] attributes. Likewise, see http://www.ddsteps.org for a description of the DDSteps extension for JUnit; while its name suggests a tool that supports\n\nwww.it-ebooks.info\n\n615\n\nParame- terized Test",
      "content_length": 1259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 679,
      "content": "616\n\nParame- terized Test\n\nChapter 24 Test Organization Patterns\n\nData-Driven Testing, the examples given are Parameterized Tests. More argu- ments for Tabular Test can be found on Clint Shank’s blog at http://clintshank. javadevelopersjournal.com/tabulartests.htm.\n\nwww.it-ebooks.info",
      "content_length": 285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 680,
      "content": "Testcase Class per Class\n\nTestcase Class per Class\n\nHow do we organize our Test Methods onto Testcase Classes?\n\nWe put all the Test Methods for one SUT class onto a single Testcase Class.\n\nTestcaseClass TestcaseClass\n\nCreation Creation\n\ntestMethod_A_1 testMethod_A_1\n\nFixture A Fixture A\n\ntestMethod_A_2 testMethod_A_2\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nExercise Exercise\n\nfeature_2 feature_2\n\ntestMethod_B_1 testMethod_B_1\n\ntestMethod_B_2 testMethod_B_2\n\nCreation Creation\n\nFixture B Fixture B\n\nAs the number of Test Methods (page 348) grows, we need to decide on which Testcase Class (page 373) to put each Test Method. Our choice of a test organi- zation strategy affects how easily we can get a “big picture” view of our tests. It also affects our choice of a ﬁ xture setup strategy.\n\nUsing a Testcase Class per Class is a simple way to start off organizing our\n\ntests.\n\nHow It Works\n\nWe create a separate Testcase Class for each class we wish to test. Each Testcase Class acts as a home to all the Test Methods that are used to verify the behavior of the SUT class.\n\nwww.it-ebooks.info\n\n617\n\nTestcase Class per Class",
      "content_length": 1148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 681,
      "content": "618\n\nTestcase Class per Class\n\nChapter 24 Test Organization Patterns\n\nWhen to Use It\n\nUsing a Testcase Class per Class is a good starting point when we don’t have very many Test Methods or we are just starting to write tests for our SUT. As the number of tests increases and we gain a better understanding of our test ﬁ xture require- ments, we may want to split the Testcase Class into multiple classes. This choice will result in either Testcase Class per Fixture (page 631; if we have a small number of frequently used starting points for our tests) or Testcase Class per Feature (page 624; if we have several distinct features to test). As Kent Beck would say, “Let the code tell you what to do!”\n\nImplementation Notes\n\nChoosing a name for the Testcase Class is pretty simple: Just use the SUT class- name, possibly preﬁ xed or sufﬁ xed with “Test.” The method names should try to capture at least the starting state (ﬁ xture) and the feature (method) being exercised, along with a summary of the parameters to be passed to the SUT. Given these requirements, we likely won’t have “room” for the expected outcome in the method name, so the test reader must look at the Test Method body to determine the expected outcome.\n\nThe creation of the ﬁ xture is the primary implementation concern when using a Testcase Class per Class. Conﬂ icting ﬁ xture requirements will inevitably arise among the various Test Methods, which makes use of Implicit Setup (page 424) difﬁ cult and forces us to use either In-line Setup (page 408) or Delegated Set- up (page 411). A second consideration is how to make the nature of the ﬁ x- ture visible within each test method so as to avoid Obscure Tests (page 186). Delegated Setup (using Creation Methods; see page 415) tends to lead to more readable tests unless the In-line Setup is very simple.\n\nExample: Testcase Class per Class\n\nHere’s an example of using the Testcase Class per Class pattern to structure the Test Methods for a Flight class that has three states (Unscheduled, Scheduled, and AwaitingApproval) and four methods (schedule, requestApproval, deSchedule, and approve. Because the class is stateful, we need at least one test for each state for each method.\n\npublic class FlightStateTest extends TestCase {\n\npublic void testRequestApproval_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper.getAnonymousFlightInScheduledState();\n\nwww.it-ebooks.info",
      "content_length": 2418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 682,
      "content": "Testcase Class per Class\n\ntry { ﬂight.requestApproval(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testRequestApproval_FromUnsheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.requestApproval(); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); }\n\npublic void testRequestApproval_FromAwaitingApprovalState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.requestApproval(); fail(\"not allowed in awaitingApproval state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testSchedule_FromUnscheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.schedule(); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); }\n\npublic void testSchedule_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.schedule(); fail(\"not allowed in scheduled state\");\n\nwww.it-ebooks.info\n\n619\n\nTestcase Class per Class",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 683,
      "content": "620\n\nTestcase Class per Class\n\nChapter 24 Test Organization Patterns\n\n} catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testSchedule_FromAwaitingApprovalState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.schedule(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testDeschedule_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); ﬂight.deschedule(); assertTrue(\"isUnscheduled()\", ﬂight.isUnscheduled()); }\n\npublic void testDeschedule_FromUnscheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); try { ﬂight.deschedule(); fail(\"not allowed in unscheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"deschedule\", e.getRequest()); assertTrue(\"isUnscheduled()\", ﬂight.isUnscheduled()); } }\n\npublic void testDeschedule_FromAwaitingApprovalState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try {\n\nwww.it-ebooks.info",
      "content_length": 1432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 684,
      "content": "Testcase Class per Class\n\nﬂight.deschedule(); fail(\"not allowed in awaitingApproval state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"deschedule\", e.getRequest()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testApprove_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.approve(\"Fred\"); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"approve\", e.getRequest()); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testApprove_FromUnsheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); try { ﬂight.approve(\"Fred\"); fail(\"not allowed in unscheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"approve\", e.getRequest()); assertTrue( \"isUnscheduled()\", ﬂight.isUnscheduled()); } }\n\npublic void testApprove_FromAwaitingApprovalState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); ﬂight.approve(\"Fred\"); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); }\n\npublic void testApprove_NullArgument() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState();\n\nwww.it-ebooks.info\n\n621\n\nTestcase Class per Class",
      "content_length": 1462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 685,
      "content": "622\n\nTestcase Class per Class\n\nChapter 24 Test Organization Patterns\n\ntry { ﬂight.approve(null); fail(\"Failed to catch no approver\"); } catch (InvalidArgumentException e) { assertEquals(\"e.getArgumentName()\", \"approverName\", e.getArgumentName()); assertNull( \"e.getArgumentValue()\", e.getArgumentValue()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testApprove_InvalidApprover() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.approve(\"John\"); fail(\"Failed to validate approver\"); } catch (InvalidArgumentException e) { assertEquals(\"e.getArgumentName()\", \"approverName\", e.getArgumentName()); assertEquals(\"e.getArgumentValue()\", \"John\", e.getArgumentValue()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } } }\n\nThis example uses Delegated Setup of a Fresh Fixture (page 311) to achieve a more declarative style of ﬁ xture construction. Even so, this class is getting rather large and keeping track of the Test Methods is becoming a bit of a chore. Even the “big picture” provided by our IDE is not that illuminating; we can see the test conditions being exercised but cannot tell what the expected outcome should be without looking at the method bodies (Figure 24.1).\n\nwww.it-ebooks.info",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 686,
      "content": "Testcase Class per Class\n\nFigure 24.1 Testcase Class per Class example as seen in the Package Explorer of the Eclipse IDE. Note how both the starting state and event are included in the Test Method names.\n\nwww.it-ebooks.info\n\n623\n\nTestcase Class per Class",
      "content_length": 255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 687,
      "content": "624\n\nTestcase Class per Feature\n\nChapter 24 Test Organization Patterns\n\nTestcase Class per Feature\n\nHow do we organize our Test Methods onto Testcase Classes?\n\nWe group the Test Methods onto Testcase Classes based on which testable feature of the SUT they exercise.\n\nFeature1TestcaseClass Feature1TestcaseClass\n\nCreation Creation\n\ntestMethod_A testMethod_A\n\nFixture A Fixture A\n\ntestMethod_B testMethod_B\n\nOutputs) Outputs)\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nFeature2TestcaseClass Feature2TestcaseClass\n\nExercise Exercise\n\nfeature_2 feature_2\n\ntestMethod_A testMethod_A\n\ntestMethod_B testMethod_B\n\nCreation Creation\n\nFixture B Fixture B\n\nAs the number of Test Methods (page 348) grows, we need to decide on which Testcase Class (page 373) to put each Test Method. Our choice of a test organi- zation strategy affects how easily we can get a “big picture” view of our tests. It also affects our choice of a ﬁ xture setup strategy.\n\nUsing a Testcase Class per Feature gives us a systematic way to break up a large Testcase Class into several smaller ones without having to change our Test Methods.\n\nHow It Works\n\nWe group our Test Methods onto Testcase Classes based on which feature of the Testcase Class they verify. This organizational scheme allows us to have smaller Testcase Classes and to see at a glance all the test conditions for a particular feature of the class.\n\nwww.it-ebooks.info",
      "content_length": 1419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 688,
      "content": "Testcase Class per Feature\n\nWhen to Use It\n\nWe can use a Testcase Class per Feature when we have a signiﬁ cant number of Test Methods and we want to make the speciﬁ cation of each feature of the SUT more obvious. Unfortunately, Testcase Class per Feature does not make each individual Test Method any simpler or easier to understand; only Testcase Class per Fixture (page 631) helps on that front. Likewise, it doesn’t make much sense to use Testcase Class per Feature when each feature of the SUT requires only one or two tests; in that case, we can stick with a single Testcase Class per Class (page 617).\n\nNote that having a large number of features on a class is a “smell” indicating the possibility that the class might have too many responsibilities. We typically use Testcase Class per Feature when we are writing customer tests for methods on a service Facade [GOF].\n\nVariation: Testcase Class per Method\n\nWhen a class has methods that take a lot of different parameters, we may have many tests for the one method. We can group all of these Test Methods onto a single Testcase Class per Method and put the rest of the Test Methods onto one or more other Testcase Classes.\n\nVariation: Testcase Class per Feature\n\nAlthough a “feature” of a class is typically a single operation or function, it may also be a set of related methods that operate on the same instance variable of the object. For example, the set and get methods of a Java Bean would be con- sidered a single (and trivial) “feature” of the class that contains those methods. Similarly, a Data Access Object [CJ2EEP] would provide methods to both read and write objects. It is difﬁ cult to test these methods in isolation, so we can treat the reading and writing of one kind of object as a feature.\n\nVariation: Testcase Class per User Story\n\nIf we are doing highly incremental development (such as we might do with eXtreme Programming), it can be useful to put the new Test Methods for each story into a different Testcase Class. This practice prevents commit-related conﬂ icts when dif- ferent people are working on different stories that affect the same SUT class. The Testcase Class per User Story pattern may or may not end up being the same as Testcase Class per Feature or Testcase Class per Method, depending on how we partition our user stories.\n\nwww.it-ebooks.info\n\n625\n\nTestcase Class per Feature",
      "content_length": 2374,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 689,
      "content": "626\n\nTestcase Class per Feature\n\nChapter 24 Test Organization Patterns\n\nImplementation Notes\n\nBecause each Testcase Class represents the requirements for a single feature of the SUT, it makes sense to name the Testcase Class based on the feature it veri- ﬁ es. Similarly, we can name each test method based on which test condition of the SUT is being veriﬁ ed. This nomenclature allows us to see all the test condi- tions at a glance by merely looking at the names of the Test Methods of the Testcase Class.\n\nOne consequence of using Testcase Class per Feature is that we end up with a larger number of Testcase Classes for a single production class. Because we still want to run all the tests for this class, we should put these Testcase Classes into a single nested folder, package, or namespace. We can use an AllTests Suite (see Named Test Suite on page 592) to aggregate all of the Testcase Classes into a single test suite if we are using Test Enumeration (page 399).\n\nMotivating Example\n\nThis example uses the Testcase Class per Class pattern to structure the Test Methods for a Flight class that has three states (Unscheduled, Scheduled, and Await- ingApproval) and four methods (schedule, requestApproval, deSchedule, and approve. Because the class is stateful, we need at least one test for each state for each method. (In the interest of saving trees, I’ve omitted many of the method bodies; please refer to Testcase Class per Class for the full listing.)\n\npublic class FlightStateTest extends TestCase {\n\npublic void testRequestApproval_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.requestApproval(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testRequestApproval_FromUnsheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState();\n\nwww.it-ebooks.info",
      "content_length": 2078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 690,
      "content": "Testcase Class per Feature\n\nﬂight.requestApproval(); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); }\n\npublic void testRequestApproval_FromAwaitingApprovalState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.requestApproval(); fail(\"not allowed in awaitingApproval state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testSchedule_FromUnscheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.schedule(); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); }\n\npublic void testSchedule_FromScheduledState() throws Exception { // I've omitted the bodies of the rest of the tests to // save a few trees } }\n\nThis example uses Delegated Setup (page 411) of a Fresh Fixture (page 311) to achieve a more declarative style of ﬁ xture construction. Even so, this class is getting rather large and keeping track of the Test Methods is becoming a bit of a chore. Because the Test Methods on this Testcase Class require four distinct methods, it is a good example of a test that can be improved through refactoring to Testcase Class per Feature.\n\nRefactoring Notes\n\nWe can reduce the size of each Testcase Class and make the names of the Test Methods more meaningful by converting them to follow the Testcase Class per Feature pattern. First, we determine how many classes we want to create and\n\nwww.it-ebooks.info\n\n627\n\nTestcase Class per Feature",
      "content_length": 1639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 691,
      "content": "628\n\nTestcase Class per Feature\n\nChapter 24 Test Organization Patterns\n\nwhich Test Methods should go into each one. If some Testcase Classes will end up being smaller than others, it makes the job easier if we start by building the smaller classes. Next, we do an Extract Class [Fowler] refactoring to create one of the new Testcase Classes and give it a name that describes the feature it exercises. Then, we do a Move Method [Fowler] refactoring (or a simple “cut and paste”) on each Test Method that belongs in this new class along with any instance variables it uses.\n\nWe repeat this process until we are down to just one feature in the original Testcase Class; we then rename that class based on the feature it exercises. At this point, each of the Testcase Classes should compile and run—but we still aren’t completely done. To get the full beneﬁ t of the Testcase Class per Feature pattern, we have one ﬁ nal step to carry out. We should do a Rename Method [Fowler] refactoring on each of the Test Methods to better reﬂ ect what the Test Method is verifying. As part of this refactoring, we can remove any mention of the feature being exercised from each Test Method name—that informa- tion should be captured in the name of the Testcase Class. This leaves us with “room” to include both the starting state (the ﬁ xture) and the expected result in the method name. If we have multiple tests for each feature with different method arguments, we’ll need to ﬁ nd a way to include those aspects of the test conditions in the method name, too.\n\nAnother way to perform this refactoring is simply to make copies of the orig- inal Testcase Class and rename them as described above. Then we simply delete the Test Methods that aren’t relevant for each class. We do need to be careful that we don’t delete all copies of a Test Method; a less critical oversight is to leave a copy of the same method in several Testcase Classes. We can avoid both of the potential errors by making one copy of the original Testcase Class for each of the features and rename them as described above. Then we simply de- lete the Test Methods that aren’t relevant for each class. When we are done, we simply delete the original Testcase Class.\n\nExample: Testcase Class per Feature\n\nIn this example, we have converted the previously mentioned set of tests to use Testcase Class per Feature.\n\npublic class TestScheduleFlight extends TestCase {\n\npublic void testUnscheduled_shouldEndUpInScheduled() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.schedule(); assertTrue( \"isScheduled()\", ﬂight.isScheduled());\n\nwww.it-ebooks.info",
      "content_length": 2652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 692,
      "content": "Testcase Class per Feature\n\n}\n\npublic void testScheduledState_shouldThrowInvalidRequestEx() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.schedule(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testAwaitingApproval_shouldThrowInvalidRequestEx() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.schedule(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue( \"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } } }\n\nExcept for their names, the Test Methods really haven’t changed here. Because the names include the pre-conditions (ﬁ xture), the feature being exercised, and the expected outcome, they help us see the big picture when we look at the list of tests in our IDE’s “outline view” (see Figure 24.2). This satisﬁ es our need for Tests as Documentation (see page 23).\n\nwww.it-ebooks.info\n\n629\n\nTestcase Class per Feature",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 693,
      "content": "630\n\nTestcase Class per Feature\n\nChapter 24 Test Organization Patterns\n\nFigure 24.2 Testcase Class per Feature example as seen in the Package Explorer of the Eclipse IDE. Note how we do not need to include the starting state in the Test Method names, leaving room for the name of the method being called and the expected end state.\n\nwww.it-ebooks.info",
      "content_length": 351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 694,
      "content": "Testcase Class per Fixture\n\nTestcase Class per Fixture\n\nHow do we organize our Test Methods onto Testcase Classes?\n\nWe organize Test Methods into Testcase Classes based on commonality of the test ﬁ xture.\n\nFixtureATestcaseClass FixtureATestcaseClass FixtureATestcaseClass FixtureATestcaseClass\n\nsetUp setUp\n\ntestMethod_1 testMethod_1\n\nCreation Creation\n\nFixture A Fixture A\n\ntestMethod_2 testMethod_2\n\nExercise Exercise\n\nfeature_1 feature_1\n\nSUT Class SUT Class\n\nFixtureBTestcaseClass FixtureBTestcaseClass\n\nExercise Exercise\n\nfeature_2 feature_2\n\nsetUp setUp\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nCreation Creation\n\nFixture B Fixture B\n\nAs the number of Test Methods (page 348) grows, we need to decide on which Testcase Class (page 373) to put each Test Method. Our choice of a test organi- zation strategy affects how easily we can get a “big picture” view of our tests. It also affects our choice of a ﬁ xture setup strategy.\n\nUsing a Testcase Class per Fixture lets us take advantage of the Implicit Setup (page 424) mechanism provided by the Test Automation Framework (page 298).\n\nHow It Works\n\nWe group our Test Methods onto Testcase Classes based on which test ﬁ xture they require as a starting point. This organization allows us to use Implicit Setup to move the entire ﬁ xture setup logic into the setUp method, thereby allow- ing each test method to focus on the exercise SUT and verify outcome phases of the Four-Phase Test (page 358).\n\nwww.it-ebooks.info\n\n631\n\nTestcase Class per Fixture",
      "content_length": 1518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 695,
      "content": "632\n\nTestcase Class per Fixture\n\nChapter 24 Test Organization Patterns\n\nWhen to Use It\n\nWe can use the Testcase Class per Fixture pattern whenever we have a group of Test Methods that need an identical ﬁ xture and we want to make each test method as simple as possible. If each test needs a unique ﬁ xture, using Testcase Class per Fixture doesn’t make a lot of sense because we will end up with a large number of single-test classes; in such a case, it would be better to use either Testcase Class per Feature (page 624) or simply Testcase Class per Class (page 617).\n\nOne beneﬁ t of Testcase Class per Fixture is that we can easily see whether we are testing all the operations from each starting state. We should end up with the same lineup of test methods on each Testcase Class, which is very easy to see in an “outline view” or “method browser” of an IDE. This attribute makes the Testcase Class per Fixture pattern particularly useful for discovering Missing Unit Tests (see Production Bugs on page 268) long before we go into production.\n\nTestcase Class per Fixture is a key part of the behavior-driven development style of testing/speciﬁ cation. It leads to very short test methods, often featuring only a single assertion per test method. When combined with a test method naming con- vention that summarizes the expected outcome of the test, this pattern leads to Tests as Documentation (see page 23).\n\nImplementation Notes\n\nBecause we set up the ﬁ xture in a method called by the Test Automation Frame- work (the setUp method), we must use an instance variable to hold a reference to the ﬁ xture we created. In such a case, we must be careful not to use a class vari- able, as it can lead to a Shared Fixture (page 317) and the Erratic Tests (page 228) that often accompany this kind of ﬁ xture. [The sidebar “There’s Always an Exception” on page 384 lists xUnit members that don’t guarantee Independent Tests (see page 42) when we use instance variables.]\n\nBecause each Testcase Class represents a single test ﬁ xture conﬁ guration, it makes sense to name the Testcase Class based on the ﬁ xture it creates. Similarly, we can name each test method based on the method of the SUT being exercised, the characteristics of any arguments passed to the SUT method, and the expected outcome of that method call.\n\nOne side effect of using Testcase Class per Fixture is that we end up with a larger number of Testcase Classes. We may want to ﬁ nd a way to group the various Testcase Classes that verify a single SUT class. One way to do so is to create a nested folder, package, or namespace to hold just these test classes. If we are using Test Enumeration (page 399), we’ll also want to create an AllTests\n\nwww.it-ebooks.info",
      "content_length": 2730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 696,
      "content": "Testcase Class per Fixture\n\nSuite (see Named Test Suite on page 592) to aggregate all the Testcase Class per Fixtures into a single suite.\n\nAnother side effect is that the tests for a single feature of the SUT are spread across several Testcase Classes. This distribution may be a good thing if the features are closely related to one another because it highlights their interdepen- dency. Conversely, if the features are somewhat unrelated, their dispersal may be disconcerting. In such a case, we can either refactor to use Testcase Class per Feature or apply an Extract Class [Fowler] refactoring on the SUT if we decide that this symptom indicates that the class has too many responsibilities.\n\nMotivating Example\n\nThe following example uses Testcase Class per Class to structure the Test Methods for a Flight class that has three states (Unscheduled, Scheduled, and AwaitingApproval) and four methods (schedule, requestApproval, deSchedule, and approve). Because the class is stateful, we need at least one test for each state for each method. (In the interest of saving trees, I’ve omitted many of the method bodies; please refer to Testcase Class per Class for the full listing.)\n\npublic class FlightStateTest extends TestCase {\n\npublic void testRequestApproval_FromScheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInScheduledState(); try { ﬂight.requestApproval(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isScheduled()\", ﬂight.isScheduled()); } }\n\npublic void testRequestApproval_FromUnsheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.requestApproval(); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); }\n\npublic void testRequestApproval_FromAwaitingApprovalState() throws Exception {\n\nwww.it-ebooks.info\n\n633\n\nTestcase Class per Fixture",
      "content_length": 2002,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 697,
      "content": "634\n\nTestcase Class per Fixture\n\nChapter 24 Test Organization Patterns\n\nFlight ﬂight = FlightTestHelper. getAnonymousFlightInAwaitingApprovalState(); try { ﬂight.requestApproval(); fail(\"not allowed in awaitingApproval state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isAwaitingApproval()\", ﬂight.isAwaitingApproval()); } }\n\npublic void testSchedule_FromUnscheduledState() throws Exception { Flight ﬂight = FlightTestHelper. getAnonymousFlightInUnscheduledState(); ﬂight.schedule(); assertTrue( \"isScheduled()\", ﬂight.isScheduled()); }\n\npublic void testSchedule_FromScheduledState() throws Exception { // I've omitted the bodies of the rest of the tests to // save a few trees } }\n\nThis example uses Delegated Setup (page 411) of a Fresh Fixture (page 311) to achieve a more declarative style of ﬁ xture construction. Even so, this class is getting rather large and keeping track of the Test Methods is becoming a bit of a chore. Because the Test Methods on this Testcase Class require three distinct test ﬁ xtures (one for each state the ﬂ ight can be in), it is a good example of a test that can be improved through refactoring to Testcase Class per Fixture.\n\nRefactoring Notes\n\nWe can remove Test Code Duplication (page 213) in the ﬁ xture setup and make the Test Methods easier to understand by converting them to use the Testcase Class per Fixture pattern. First, we determine how many classes we want to cre- ate and which Test Methods should go into each one. If some Testcase Classes will end up being smaller than others, it will reduce our work if we start with the smaller ones. Next, we do an Extract Class refactoring to create one of the Testcase Classes and give it a name that describes the ﬁ xture it requires. Then, we do a Move Method [Fowler] refactoring on each Test Method that belongs in this new class, along with any instance variables it uses.\n\nwww.it-ebooks.info",
      "content_length": 1988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 698,
      "content": "Testcase Class per Fixture\n\nWe repeat this process until we are down to just one ﬁ xture in the original class; we can then rename that class based on the ﬁ xture it creates. At this point, each of the Testcase Classes should compile and run—but we still aren’t com- pletely done. To get the full beneﬁ t of the Testcase Class per Fixture pattern, we have two more steps to complete. First, we should factor out any common ﬁ xture setup logic from each of the Test Methods into the setUp method, result- ing in an Implicit Setup. This type of setup is made possible because the Test Methods on each class have the same ﬁ xture requirements. Second, we should do a Rename Method [Fowler] refactoring on each of the Test Methods to bet- ter reﬂ ect what the Test Method is verifying. We can remove any mention of the starting state from each Test Method name, because that information should be captured in the name of the Testcase Class. This refactoring leaves us with “room” to include both the action (the method being called plus the nature of the arguments) and the expected result in the method name.\n\nAs described in Testcase Class per Fixture, we can also refactor to this pat- tern by making one copy of the Testcase Class (suitably named) for each ﬁ xture, deleting the unnecessary Test Methods from each one, and ﬁ nally deleting the old Testcase Class.\n\nExample: Testcase Class per Fixture\n\nIn this example, the earlier set of tests has been converted to use the Testcase Class per Fixture pattern. (In the interest of saving trees, I’ve shown only one of the resulting Testcase Classes; the others look pretty similar.)\n\npublic class TestScheduledFlight extends TestCase {\n\nFlight scheduledFlight;\n\nprotected void setUp() throws Exception { super.setUp(); scheduledFlight = createScheduledFlight(); }\n\nFlight createScheduledFlight() throws InvalidRequestException{ Flight newFlight = new Flight(); newFlight.schedule(); return newFlight; }\n\npublic void testDeschedule_shouldEndUpInUnscheduleState() throws Exception { scheduledFlight.deschedule(); assertTrue(\"isUnsched\", scheduledFlight.isUnscheduled()); }\n\nwww.it-ebooks.info\n\n635\n\nTestcase Class per Fixture",
      "content_length": 2172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 699,
      "content": "636\n\nTestcase Class per Fixture\n\nChapter 24 Test Organization Patterns\n\npublic void testRequestApproval_shouldThrowInvalidRequestEx(){ try { scheduledFlight.requestApproval(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"requestApproval\", e.getRequest()); assertTrue(\"isScheduled()\", scheduledFlight.isScheduled()); } }\n\npublic void testSchedule_shouldThrowInvalidRequestEx() { try { scheduledFlight.schedule(); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"schedule\", e.getRequest()); assertTrue(\"isScheduled()\", scheduledFlight.isScheduled()); } }\n\npublic void testApprove_shouldThrowInvalidRequestEx() throws Exception { try { scheduledFlight.approve(\"Fred\"); fail(\"not allowed in scheduled state\"); } catch (InvalidRequestException e) { assertEquals(\"InvalidRequestException.getRequest()\", \"approve\", e.getRequest()); assertTrue(\"isScheduled()\", scheduledFlight.isScheduled()); } } }\n\nNote how much simpler each Test Method has become! Because we have used Intent-Revealing Names [SBPP] for each of the Test Methods, we can use the Tests as Documentation. By looking at the list of methods in the “outline view” of our IDE, we can see the starting state (ﬁ xture), the action (method being called), and the expected outcome (what it returns or the post-test state)—all without even opening up the method body (Figure 24.3).\n\nwww.it-ebooks.info",
      "content_length": 1520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 700,
      "content": "Testcase Class per Fixture\n\nFigure 24.3 The tests for our Testcase Class per Fixture as seen in the Package Explorer of the Eclipse IDE. Note how we do not need to include the name of the method being called in the Test Method names, leaving room for the starting state and the expected end state.\n\nThis “big picture” view of our tests makes it clear that we are only testing the approve method arguments when the Flight is in the awaitingApproval state. We can now decide whether that limitation is a shortcoming of the tests or part of the speciﬁ cation (i.e., the result of calling approve is “undeﬁ ned” for some states of the Flight).\n\nwww.it-ebooks.info\n\n637\n\nTestcase Class per Fixture",
      "content_length": 692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 701,
      "content": "638\n\nAlso known as: Abstract Testcase, Abstract Test Fixture, Testcase Baseclass\n\nTestcase Superclass\n\nChapter 24 Test Organization Patterns\n\nTestcase Superclass\n\nWhere do we put our test code when it is in reusable Test Utility Methods?\n\nWe inherit reusable test-speciﬁ c logic from an abstract Testcase Super class.\n\nTestcase Testcase Superclass Superclass\n\nTest Utility Test Utility Method_1 Method_1\n\nTest Utility Test Utility Method_2 Method_2\n\nFixture Fixture\n\nSUT SUT\n\ntestMethod_1 testMethod_1\n\ntestMethod_n testMethod_n\n\nTestcase Testcase Class Class\n\nAs we write tests, we will invariably ﬁ nd ourselves needing to repeat the same logic in many, many tests. Initially, we may just “clone and twiddle” as we write addi- tional tests that need the same logic. Ultimately, we may introduce Test Utility Meth- ods (page 599) to hold this logic—but where do we put the Test Utility Methods? A Testcase Superclass is one option as a home for our Test Utility Methods.\n\nHow It Works\n\nWe deﬁ ne an abstract superclass to hold the reusable Test Utility Method that should be available to several Testcase Classes (page 373). We make the methods that will be reused visible to subclasses (e.g., protected in Java). We then use this abstract class as the superclass (base class) for any tests that wish to reuse the logic. The logic can be accessed simply by calling the method as though it were deﬁ ned on the Testcase Class itself.\n\nwww.it-ebooks.info",
      "content_length": 1452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 702,
      "content": "Testcase Superclass\n\nWhen to Use It\n\nWe can use a Testcase Superclass if we wish to reuse Test Utility Methods between several Testcase Classes and can ﬁ nd or deﬁ ne a Testcase Superclass from which we can subclass all tests that require the logic.\n\nThis pattern assumes that our programming language supports inheritance, we are not already using inheritance for some other conﬂ icting purpose, and the Test Utility Method doesn’t need access to speciﬁ c types that are not visible from the Testcase Superclass.\n\nThe decision between a Testcase Superclass and a Test Helper (page 643) comes down to type visibility. The client classes need to see the Test Utility Method, and the Test Utility Method needs to see the types and classes it depends on. When it doesn’t depend on many types/classes or when everything it depends on is visible from a single place, we can put the Test Utility Method into a common Testcase Superclass we deﬁ ne for our project or company. If the Test Utility Method depends on types/classes that cannot be seen from a single place that all clients can access, it may be necessary to put it on a Test Helper in the appropriate test package or subsystem.\n\nVariation: Test Helper Mixin\n\nIn languages that support mixins, Test Helper Mixins give us the best of both worlds. As with a Test Helper, we can choose which Test Helper Mixins to in- clude without being constrained by a single-inheritance hierarchy. As with a Test Helper Object (see Test Helper), we can hold a test-speciﬁ c state in the mixin but we don’t have to instantiate and delegate that task to a separate object. As with a Testcase Superclass, we can access everything as methods and attributes on self.\n\nImplementation Notes\n\nIn variants of xUnit that require all Testcase Classes to be subclasses of a Test- case Superclass provided by the Test Automation Framework (page 298), we deﬁ ne that class as the superclass of our Testcase Superclass. In variants that use annotations or method attributes to identify the Test Method (page 348), we can subclass any class that we ﬁ nd useful.\n\nWe can implement the methods on the Testcase Superclass either as class methods or as instance methods. For any stateless Test Utility Methods, it is perfectly reasonable to use class methods. If it isn’t possible to use class meth- ods for some reason, we can work with instance methods. Either way, because the methods are inherited, we can access them as though they were deﬁ ned on the Testcase Class itself. If our language supports managing the visibility\n\nwww.it-ebooks.info\n\n639\n\nTestcase Superclass",
      "content_length": 2592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 703,
      "content": "640\n\nTestcase Superclass\n\nChapter 24 Test Organization Patterns\n\nof methods, we must ensure that we make the methods visible enough (e.g., protected in Java).\n\nMotivating Example\n\nThe following example shows a Test Utility Method that is on the Testcase Class:\n\npublic class TestRefactoringExample extends TestCase { public void testAddOneLineItem_quantity1() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify assertInvoiceContainsOnlyThisLineItem(inv, expItem); }\n\nvoid assertInvoiceContainsOnlyThisLineItem( Invoice inv, LineItem expItem) { List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\",expItem, actual); } }\n\nThis Test Utility Method is not reusable outside this particular class or its subclasses.\n\nRefactoring Notes\n\nWe can make the Test Utility Method more reusable by moving it to a Testcase Superclass by using a Pull Up Method [Fowler] refactoring. Because the method is inherited by our Testcase Class, we can access it as if the method were deﬁ ned locally. If the Test Utility Method accesses any instance variables, we must perform a Pull Up Field [Fowler] refactoring to move those variables to a place where the Test Utility Method can see them. In languages that have visibility restrictions, we may need to make the ﬁ elds visible to subclasses (e.g., default or protected in Java) if Test Methods on the Testcase Class need to access the ﬁ elds as well.\n\nwww.it-ebooks.info",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 704,
      "content": "Testcase Superclass\n\nExample: Testcase Superclass\n\nBecause the method is inherited by our Testcase Class, we can access it as if it were deﬁ ned locally. Thus the usage looks identical.\n\npublic class TestRefactoringExample extends OurTestCase { public void testAddItemQuantity_severalQuantity_v12(){ // Fixture Setup Customer cust = createACustomer(new BigDecimal(\"30\")); Product prod = createAProduct(new BigDecimal(\"19.99\")); Invoice invoice = createInvoice(cust); // Exercise SUT invoice.addItemQuantity(prod, 5); // Verify Outcome LineItem expected = new LineItem(invoice, prod, 5, new BigDecimal(\"30\"), new BigDecimal(\"69.96\")); assertContainsExactlyOneLineItem(invoice, expected); } }\n\nThe only difference is the class in which the method is deﬁ ned and its visibility:\n\npublic class OurTestCase extends TestCase { void assertContainsExactlyOneLineItem(Invoice invoice, LineItem expected) { List lineItems = invoice.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actItem = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\",expected, actItem); } }\n\nExample: Test Helper Mixin\n\nHere are some tests written in Ruby using Test::Unit:\n\ndef test_extref # setup sourceXml = \"<extref id='abc'/>\" expectedHtml = \"<a href='abc.html'>abc</a>\" mockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) # execute @handler.printBodyContents # verify assert_equals_html( expectedHtml, mockFile.output, \"extref: html output\") end\n\ndef testTestterm_normal sourceXml = \"<testterm id='abc'/>\"\n\nwww.it-ebooks.info\n\n641\n\nTestcase Superclass",
      "content_length": 1570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 705,
      "content": "642\n\nTestcase Superclass\n\nChapter 24 Test Organization Patterns\n\nexpectedHtml = \"<a href='abc.html'>abc</a>\" mockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) @handler.printBodyContents assert_equals_html( expectedHtml, mockFile.output, \"testterm: html output\") end\n\ndef testTestterm_plural sourceXml =\"<testterms id='abc'/>\" expectedHtml = \"<a href='abc.html'>abcs</a>\" mockFile = MockFile.new @handler = setupHandler(sourceXml, mockFile) @handler.printBodyContents assert_equals_html( expectedHtml, mockFile.output, \"testterms: html output\") end\n\nThese tests contain a fair bit of Test Code Duplication (page 213). We can address this issue by using an Extract Method [Fowler] refactoring to create a Test Utility Method. We can then make the Test Utility Method more reusable by moving it to a Test Helper Mixin using a Pull Up Method refactoring. Because the mixed-in functionality is considered part of our Testcase Class, we can access it as if it were deﬁ ned locally. Thus the usage looks identical.\n\nclass CrossrefHandlerTest < Test::Unit::TestCase include HandlerTest\n\ndef test_extref sourceXml = \"<extref id='abc' />\" expectedHtml = \"<a href='abc.html'>abc</a>\" generateAndVerifyHtml(sourceXml,expectedHtml,\"<extref>\") end\n\nThe only difference is the location where the method is deﬁ ned and its visibility. In particular, Ruby requires mixins to be deﬁ ned in a module rather than a class.\n\nmodule HandlerTest def generateAndVerifyHtml( sourceXml, expectedHtml, message, &block) mockFile = MockFile.new sourceXml.delete!(\"\\t\") @handler = setupHandler(sourceXml, mockFile ) block.call unless block == nil @handler.printBodyContents actual_html = mockFile.output assert_equal_html( expectedHtml, actual_html, message + \"html output\") actual_html end\n\nwww.it-ebooks.info",
      "content_length": 1798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 706,
      "content": "Test Helper\n\nTest Helper\n\nWhere do we put our test code when it is in reusable Test Utility Methods?\n\nWe deﬁ ne a helper class to hold any Test Utility Methods we want to reuse in several tests.\n\nTest Helper Test Helper\n\nFixture Fixture\n\ntestMethod_1 testMethod_1\n\nTest Utility Test Utility Method_1 Method_1\n\ntestMethod_n testMethod_n\n\nTest Utility Test Utility Method_2 Method_2\n\nSUT SUT\n\nTestcase Testcase Class Class\n\nAs we write tests, we will invariably ﬁ nd ourselves needing to repeat the same logic in many, many tests. Initially, we may just “clone and twiddle” as we write additional tests that need the same logic. Ultimately, we may introduce Test Utility Methods (page 599) to hold this logic—but where should we put such reusable logic?\n\nA Test Helper is one possible choice of home for reusable test logic.\n\nHow It Works\n\nWe deﬁ ne a separate class to hold the reusable Test Utility Methods that should be available to several Testcase Classes (page 373). In each test that wishes to use this logic, we access the logic either using static method calls or via an instance created speciﬁ cally for the purpose.\n\nWhen to Use It\n\nWe can use a Test Helper if we wish to share logic or variables between several Testcase Classes and cannot (or choose not to) ﬁ nd or deﬁ ne a Testcase Super- class (page 638) from which we might otherwise subclass all tests that require this logic. We might pursue this course in several circumstances: Perhaps our\n\nwww.it-ebooks.info\n\n643\n\nTest Helper",
      "content_length": 1497,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 707,
      "content": "644\n\nTest Helper\n\nChapter 24 Test Organization Patterns\n\nprogramming language doesn’t support inheritance (e.g., Visual Basic 5 or 6), perhaps we are already using inheritance for some other conﬂ icting purpose, or perhaps the Test Utility Method needs access to speciﬁ c types that are not visible from the Testcase Superclass.\n\nThe decision between a Test Helper and a Testcase Superclass comes down to type visibility. The client classes need to see the Test Utility Method, and the Test Utility Method needs to see all the types and classes it depends on. When it doesn’t depend on many types/classes or when everything it depends on is visible from a single place, we can put the Test Utility Method into a common Testcase Superclass we deﬁ ne for our project or company. If the Test Utility Method depends on types/classes that cannot be seen from a single place that all clients can access, it may be necessary to put it on a Test Helper in the appropri- ate test package or subsystem. In larger systems with many groups of domain objects, it is common practice to have one Test Helper for each group (package) of related domain objects.\n\nVariation: Test Fixture Registry\n\nA Registry [PEAA] is a well-known object that can be accessed from anywhere in a program. We can use the Registry to store and retrieve objects from dif- ferent parts of our program or tests. (Registry objects are often confused with Singletons [GOF], which are also well known but have only a single instance. With a Registry object, there may be one or more instances—we don’t really care.) A Test Fixture Registry gives the tests the ability to access the same ﬁ xture as other tests in the same test run. Depending on how we implement our Test Helper, we may choose to provide a different instance of the Test Fixture Regis- try for each Test Runner (page 377) in an effort to prevent a Test Run War (see Erratic Test on page 228). A common example of a Test Fixture Registry is the Database Sandbox (page 650).\n\nA Test Fixture Registry is typically used with a Setup Decorator (page 447) or with Lazy Setup (page 435); it isn’t needed with Suite Fixture Setup (page 441) because only tests on the same Testcase Class need to share the ﬁ xture. In such a case, using a ﬁ xture holding class variable works well for this purpose.\n\nVariation: Object Mother\n\nThe Object Mother pattern is simply an aggregate of several other patterns, each of which makes a small but signiﬁ cant contribution to making the test ﬁ xture easier to manage. The Object Mother consists of one or more Test Helpers that provide Creation Methods (page 415) and Attachment Methods (see Creation Method), which our tests then use to create ready-to-use test ﬁ xture objects. Object Mothers\n\nwww.it-ebooks.info",
      "content_length": 2764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 708,
      "content": "Test Helper\n\noften provide several Creation Methods that create instances of the same class, where each method results in a test object in a different starting state (a Named State Reaching Method; see Creation Method). The Object Mother may also have the ability to delete the objects it creates automatically—an example of Automated Teardown (page 503).\n\nBecause there is no single, crisp deﬁ nition of what someone means by “Object Mother,” it is advisable to refer to the individual patterns (such as Automated Teardown) when referring to speciﬁ c capabilities of the Object Mother.\n\nImplementation Notes\n\nThe methods on the Test Helper can be implemented as either class methods or instance methods depending on the degree to which we want to keep the tests from interacting.\n\nVariation: Test Helper Class\n\nIf all of the Test Utility Methods are stateless, the simplest approach is to imple- ment the functionality of the Test Helper as class methods and then to have the tests access those methods using the ClassName.methodName (or equivalent) notation. If we need to hold references to ﬁ xture objects, we could place them in class variables. We need to be careful to avoid inadvertently creating a Shared Fixture (page 317), however—unless, of course, that is exactly what we are trying to do. In such a case, we are actually building a Test Fixture Registry.\n\nVariation: Test Helper Object\n\nIf we can’t use class methods for some reason, we can work with instance meth- ods instead. In this case, the client test will need to create an instance of the Test Helper class and store it in an instance variable; the methods can then be accessed via this variable. This pattern is a good approach when the Test Helper holds references to ﬁ xture or SUT objects and we want to make sure that we don’t creep into a Shared Fixture situation. It is also useful when the Test Helper stores expec- tations for a set of Mock Objects (page 544), because this pattern ensures that we can verify the calls are interleaved between the Mock Objects correctly.\n\nMotivating Example\n\nThe following example shows a Test Utility Method that is on the Testcase Class:\n\npublic class TestUtilityExample extends TestCase {\n\npublic void testAddOneLineItem_quantity1() {\n\nwww.it-ebooks.info\n\n645\n\nTest Helper",
      "content_length": 2290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 709,
      "content": "646\n\nTest Helper\n\nChapter 24 Test Organization Patterns\n\nInvoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify assertInvoiceContainsOnlyThisLineItem(inv, expItem); }\n\nvoid assertInvoiceContainsOnlyThisLineItem( Invoice inv, LineItem expItem) { List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\",expItem, actual); } }\n\nThis Test Utility Method is not reusable outside this particular class.\n\nRefactoring Notes\n\nWe can make a Test Utility Method more reusable by moving it to a Test Helper class. This transformation is often as simple as doing a Move Method [Fowler] refactoring to our Test Helper class. One potential problem arises when we have used instance variables to pass arguments to or return data from the Test Utility Method. These “global data” need to be converted to explicit arguments and return values before we can perform the Move Method refactoring.\n\nExample: Test Helper with Class Methods\n\nIn this modiﬁ ed version of the preceding test, we have turned the Test Utility Method into a class method on a Test Helper Class so we can access it via the classname without creating an instance:\n\npublic class TestUtilityExample extends TestCase { public void testAddOneLineItem_quantity1_staticHelper() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify TestHelper.assertContainsExactlyOneLineItem(inv, expItem); } }\n\nwww.it-ebooks.info",
      "content_length": 1664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 710,
      "content": "Test Helper\n\nExample: Test Helper with Instance Methods\n\nIn this example, we have moved the Test Utility Method to a Test Helper as an instance method. Note that we must now access the method via an object refer- ence (a variable that holds an instance of the Test Helper).\n\npublic class TestUtilityExample extends TestCase { public void testAddOneLineItem_quantity1_instanceHelper() { Invoice inv = createAnonInvoice(); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify TestHelper helper = new TestHelper(); helper.assertInvContainsExactlyOneLineItem(inv, expItem); } }\n\nwww.it-ebooks.info\n\n647\n\nTest Helper",
      "content_length": 677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 711,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 712,
      "content": "CHAPTER 25\n\nDatabase Patterns\n\nPatterns in This Chapter\n\nDatabase Sandbox. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650\n\nStored Procedure Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654\n\nTable Truncation Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661\n\nTransaction Rollback Teardown . . . . . . . . . . . . . . . . . . . . . . . . . . . 668\n\n649\n\nwww.it-ebooks.info\n\nDatabase Patterns",
      "content_length": 473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 713,
      "content": "650\n\nDatabase Sandbox\n\nChapter 25 Database Patterns\n\nDatabase Sandbox\n\nHow do we develop and test software that depends on a database?\n\nWe provide a separate test database for each developer or tester.\n\nDeveloper 1 Developer 1\n\nDeveloper 2 Developer 2\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nSUT SUT\n\nSUT SUT\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nFixture Fixture\n\nFixture Fixture\n\nDatabase Database\n\nDatabase Database\n\nMany applications use a database to store the persistent state of the application. At least some of the tests for such an application will require accessing the data- base. Unfortunately, a database is a primary cause of Erratic Tests (page 228) due to the fact that data may persist between tests. A major goal in keeping tests from interacting is ensuring that the test ﬁ xtures used by each test do not overlap. This is especially difﬁ cult when the development environment contains only a single test database and all tests run by all developers run against the same database.\n\nA Database Sandbox is one way to keep the tests from interacting by acciden-\n\ntally accessing the same records in the database.\n\nHow It Works\n\nWe provide each user with a separate, self-consistent sandbox in which to work. This sandbox includes the user’s own copy of any code plus—most importantly—the user’s own copy of the database. Such an arrangement allows each user to modify the database in any way he or she sees ﬁ t and to exercise the application with tests without worrying about any interactions between the user’s own tests and the tests conducted by other users.\n\nWhen to Use It\n\nWe should use a Database Sandbox whenever we are building or modifying an application that depends on a database for a signiﬁ cant portion of its functionality.\n\nwww.it-ebooks.info",
      "content_length": 1826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 714,
      "content": "Database Sandbox\n\nThis need is especially evident if we have chosen to use a Shared Fixture (page 317). Using a Database Sandbox will help us avoid Test Run Wars (see Erratic Test) between different users of the database. Depending on how we have chosen to implement the Database Sandbox, it may or may not allow different users to modify the structure of the database. A Database Sandbox will not prevent Un- repeatable Tests (see Erratic Test) or Interacting Tests (see Erratic Test), however, because it merely separates different users (and their test runs) from one another; tests within a single test run may continue to share a test ﬁ xture.\n\nImplementation Notes\n\nThe application needs to be made conﬁ gurable so that the database to be used in testing can be changed without modifying the code. Typically, this goal is accomplished by reading the database conﬁ guration information from a proper- ties ﬁ le that is customized in each user’s environment.\n\nADatabase Sandbox can be implemented in many different ways. Fundamentally, the choice comes down to whether we give each user a separate database instance or just simulate one. In general, giving each user a real separate database instance is the preferred choice. This scheme may not always be feasible, however—especially if the database vendor’s licensing structure makes it cost prohibitive.\n\nVariation: Dedicated Database Sandbox\n\nWe give each developer, tester, or test user a separate database instance. This is typically accomplished by installing a lightweight database technology in each user’s test environment. Examples of lightweight database technologies include MySql and Personal Oracle. The database instance can be installed on the user’s own machine, on a shared test server, or on a dedicated “virtual server” running on shared server hardware.\n\nA Dedicated Database Sandbox is the preferred solution because it provides the greatest ﬂ exibility. It allows a developer to modify the database schema, load his or her own test data, and so on.\n\nVariation: DB Schema per Test Runner\n\nWith DB Schema per Test Runner, we give each developer, tester, or test user what appears to be a separate database instance by using built-in database sup- port for multiple schemas.\n\nOne considerable advantage that the DB Schema per Test Runner pattern enjoys relative to the Dedicated Database Sandbox pattern is that we can share an Immutable Shared Fixture (see Shared Fixture) deﬁ ned in a common schema and put each user’s mutable ﬁ xture in his or her own private schema. Note that\n\nwww.it-ebooks.info\n\n651\n\nDatabase Sandbox",
      "content_length": 2598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 715,
      "content": "652\n\nDatabase Sandbox\n\nChapter 25 Database Patterns\n\nthis scheme does not allow the user to modify the structure of the database (at least not to the same degree as is possible with a Dedicated Database Sandbox). It also forces all users, including both developers and testers, to use the same database structure. This can create logistical issues when database structure upgrades need to be rolled out.\n\nVariation: Database Partitioning Scheme\n\nWe give each developer, tester, or test user a separate set of data within a single Database Sandbox. Each user can modify that data as he or she sees ﬁ t but is not allowed to modify the data assigned to other users.\n\nThis approach requires less database administration overhead but more data administration overhead than with the other ways to implement a Database Sandbox. Because it does not allow developers to modify the database schema, a Database Partitioning Scheme is not appropriate for evolutionary database development. It is, however, appropriate for preventing Interacting Tests when applied to different tests run from the same Test Runner. That is, we give each test a unique key such as a CustomerNumber that it uses for all data. As a conse- quence, other tests within the same test run use different data. This pattern can be combined with many of the other variations of Database Sandbox to prevent Interacting Tests when using a Shared Fixture. Note that this pattern does not prevent Unrepeatable Tests unless we also use Distinct Generated Values (see Generated Value on page 723).\n\nMotivating Example\n\nThe following test uses Literal Values for the arguments to a constructor of a Product that is persisted into a database instance shared among several developers. The name of the Product must be unique:\n\npublic void testProductPrice_HCV() { // Setup Product product = new Product( 88, // ID \"Widget\", // Name new BigDecimal(\"19.99\")); // Price // Exercise SUT // ... }\n\nUnfortunately, we may end up with a Test Run War when we run this test against a shared database instance regardless of how effectively we tear down the Product after each test. This is because we are trying to create the same Product that the same test run from another Test Runner might be in the process of using at the same time.\n\nwww.it-ebooks.info",
      "content_length": 2296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 716,
      "content": "Database Sandbox\n\nRefactoring Notes\n\nThere are no code changes required of our test when we create a Dedicated Database Sandbox for each developer and tester. Therefore, tests should not have to do anything special to run completely independently of tests being run from other Test Runners (page 377). There is a small change required of the SUT, however, to allow the SUT to connect to different database instances based on conﬁ guration data. How we make this change varies with the technology we use and is beyond the scope of this book.\n\nWe can convert the test to use a Database Partitioning Scheme by replacing the Literal Values with calls to the appropriate getUnique method passing an ID speciﬁ c to the Test Runner as a seed.\n\nExample: Database Partitioning Scheme\n\nHere is the same test using a Database Partitioning Scheme to ensure that each test uses a different set of products. For the getUniqueString method, we pass a string based on the MAC address of our computer.\n\npublic void testProductPrice_DPS() { // Setup Product product = new Product( getUniqueInt(), // ID getUniqueString(getMacAddress()), // Name new BigDecimal(\"19.99\")); // Price // Exercise SUT // ... }\n\nstatic int counter = 0;\n\nint getUniqueInt() { counter++; return counter; }\n\nBigDecimal getUniqueBigDecimal() { return new BigDecimal(getUniqueInt()); }\n\nString getUniqueString(String baseName) { return baseName.concat(String.valueOf( getUniqueInt())); }\n\nThis test can now be run from several different computers against the same shared database instance without fear of a Test Run War.\n\nwww.it-ebooks.info\n\n653\n\nDatabase Sandbox",
      "content_length": 1617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 717,
      "content": "654\n\nStored Procedure Test\n\nChapter 25 Database Patterns\n\nStored Procedure Test\n\nHow can we verify logic independently when we have stored procedures?\n\nWe write Fully Automated Tests for each stored procedure.\n\nApplication Environment Application Environment\n\nTestcase Class Testcase Class\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nStored Stored Procedure Procedure Proxy Proxy\n\nTestcase Class Testcase Class\n\ntestMethod_1 testMethod_1\n\ntestMethod_2 testMethod_2\n\nDatabase Database\n\nStored Stored Procedure Procedure\n\nMany applications that use a database to store the persistent state of the appli- cation also use stored procedures and triggers to improve performance and do common processing on updates.\n\nA Stored Procedure Test is a way to apply automated testing practices to this\n\ncode that lives inside the database.\n\nHow It Works\n\nWe write unit tests for the stored procedures independent of the client application software. These tests may be layer-crossing tests or round-trip tests, depending on the nature of the store procedure(s) being tested.\n\nWhen to Use It\n\nWe should write Stored Procedure Tests whenever we have nontrivial logic in stored procedures. This pattern will help us verify that the stored procedures—our\n\nwww.it-ebooks.info",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 718,
      "content": "Stored Procedure Test\n\nSUT for the purposes of these tests—are working properly independently of the client application. This consideration is particularly important when more than one application will use the stored procedures or when the stored procedures are being developed by a different development team. Stored Procedure Tests are particularly important when we cannot ensure the procedures are tested ade- quately simply by exercising the application software (a form of Indirect Testing; see Obscure Test on page 186). Using Stored Procedure Tests also helps us to enumerate all the conditions under which the stored procedure could be called and what should happen in each circumstance. The very act of thinking about these circumstances is likely to improve the design—a common result of doing test-ﬁ rst development.\n\nImplementation Notes\n\nThere are two fundamentally different ways to implement Stored Procedure Tests: (1) We can write the tests in the same programming language as the stored proce- dure and run them in the database or (2) we can write the tests in our application programming language and access the stored procedure via a Remote Proxy [GOF]. We might even write tests both ways. For example, the stored-procedure developers might write unit tests in the database programming language, whereas the applica- tion developers might prepare some acceptance tests in the application programming language to run as part of the application build.\n\nEither way, we need to decide how the test will set up the ﬁ xture (the “before” state of the database) and verify the expected outcome (the “after” state of the database as well as any expected actions such as cascading deletes). The test may interact directly with the database to insert/verify the data (a form of Back Door Manipulation; see page 327) or it could use another stored procedure (a form of round-trip test).\n\nVariation: In-Database Stored Procedure Test\n\nOne advantage of the xUnit approach to automated testing is that the tests are written in the same language as the code we are testing. This makes it easier for the developers to learn how to automate the tests without learning a new program- ming language, debugger, and so on. Extending this idea to its logical conclusion, it makes sense to test stored procedures using tests that are written in the stored- procedure programming language. Naturally, we will need to run these tests inside the database. Unfortunately, that requirement may make it hard to run them as part of the Integration Build [SCM].\n\nThis variation on the Stored Procedure Test pattern is appropriate when we have more experience writing code in the stored-procedure language and/or\n\nwww.it-ebooks.info\n\n655\n\nStored Procedure Test",
      "content_length": 2750,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 719,
      "content": "656\n\nStored Procedure Test\n\nChapter 25 Database Patterns\n\nenvironment than in the application environment and it is not essential that all tests be run from a single place. For example, a database or data services team that is writing stored procedures for use by other teams would ﬁ nd this approach attractive. Another circumstance in which it would be appropriate to use In-Database Stored Procedure Tests arises when the procedures are stored in a different source code repository than the application logic. Using In-Database Stored Procedure Test allows us to store the tests in the same repository as the SUT (in this case, the stored procedures).\n\nIn-Database Stored Procedure Tests may allow somewhat more thorough unit testing (and test-driven development) of the stored procedures because we may have better access to implementation details of the stored procedure from our tests. Of course, this violation of encapsulation could result in Overspeciﬁ ed Software (see Fragile Test on page 239). If the client code uses a data access layer, we must still write unit tests for that software in the application programming language to ensure that we handle errors correctly (e.g., failure to connect).\n\nSome databases support several programming languages. In such a case, we can choose to use the more test-friendly programming language for our tests but write the stored procedures in the more traditional stored-procedure programming language. For example, Oracle databases support both PLSQL and Java, so we could use JUnit tests to verify our PLSQL stored procedures. Likewise, Microsoft’s SQL Server supports C#, so we could use NUnit tests written in C# to verify the stored procedures written in Transact-SQL.\n\nVariation: Remoted Stored Procedure Test\n\nThe purpose of Remoted Stored Procedure Tests is to allow us to write the tests in the same language as the unit tests for the client application logic. We must access the stored procedure via a Remote Proxy [GOF] that hides the mechanics of inter- acting with that procedure. This proxy can be structured as either a Service Facade [CJ2EEP] or a Command [GOF] (such as Java’s JdbcOdbcCallableStatement).\n\nRemoted Stored Procedure Tests are, in effect, component tests in that they treat the stored procedure as a “black box” component. Because Remoted Stored Procedure Tests do not run inside the database, we are more likely to write them as round-trip tests (calling other stored procedures to set up the ﬁ xture, verify the outcome, and perform other necessary tasks) unless we have an easy way to insert or verify data. Some members of the xUnit family have extensions that are speciﬁ cally intended to facilitate this behavior (e.g., DbUnit for Java and NDbUnit for .NET languages).\n\nThis solution is more appropriate if we want to keep all our tests in a single programming language. The Remoted Stored Procedure Test pattern makes it easier to run all the tests every time we check in changes to the application code.\n\nwww.it-ebooks.info",
      "content_length": 3014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 720,
      "content": "Stored Procedure Test\n\nTesting Stored Procedures with JUnit\n\nOn an early XP project, our application was mandated to use stored procedures being developed by another group. It seemed that every time we integrated our Java with those developers’ PLSQL code, we found serious bugs in the fundamental behavior of their stored procedures. We were writing automated tests using JUnit for our code. Although we were sure that writing unit tests for the stored procedures would clarify the interface contract and improve the quality of the other group’s code, we couldn’t force the other team to write unit tests. Nor had utPLSQL even been invented at that point.\n\nWe decided to try writing unit tests for the stored procedures in the xUnit family member we were comfortable with: JUnit. Because we had to write JDBC code to access the stored procedures anyway, we deﬁ ned JUnit tests for each stored procedure via the JDBC PreparedStatement classes that we had built. The tests exercised the basic behavior of the stored behaviors and a few of the more obvious failure cases. Whenever we received a new version of the stored procedures, we would run the JUnit tests before we even tried to exercise the procedures from our application code. Needless to say, many of the tests failed.\n\nWe sat down with the developers who were building the stored proce- dures and showed them our tests—including how they were failing left, right, and center. Needless to say, the developers were a bit embarrassed but they agreed that our tests were correct. They went off to ﬁ x the stored procedures and gave us a new version to test. The revision fared somewhat better but still produced some failures. Then a very important thing hap- pened: The members of the other group asked for a copy of the tests we had written and instructions on how to run them for themselves. Before long, these developers were writing their own PLSQL unit tests in JUnit!\n\nThis capability is particularly useful if the stored procedures are being writ- ten and/or modiﬁ ed by the same team that is developing the client code. We can also use Remoted Stored Procedure Tests when another team is provid- ing the stored procedures and we are not conﬁ dent in those developers’ ability to write defect-free code (probably because they are not writing In-Database Stored Procedure Tests for their code). In this situation, we can use the Remot- ed Stored Procedure Tests as a form of acceptance test for their code. See the sidebar “Testing Stored Procedures with JUnit” for an illustration of how this setup worked on one project.\n\nwww.it-ebooks.info\n\n657\n\nStored Procedure Test",
      "content_length": 2633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 721,
      "content": "658\n\nStored Procedure Test\n\nChapter 25 Database Patterns\n\nOne disadvantage of using Remoted Stored Procedure Tests is that they will likely cause the test suite to run more slowly because the tests require the database to be available and populated with data. The tests for the stored procedures can be put into a separate Subset Suite (see Named Test Suite on page 592) so that they need not be run with all the in-memory tests. This can signiﬁ cantly speed up test execution, thereby avoiding Slow Tests (page 253).\n\nRemoted Stored Procedure Tests also come in handy when logic written in our programming language of choice already has unit tests and we need to move that logic into the database. By using a Remoted Stored Procedure Test, we can avoid rewriting the tests in a different programming language and Test Automation Framework (page 298), which can in turn save time and money. This pattern also enables us to avoid any translation errors when recoding the logic, so we can be sure the recoded logic really does produce the same results.\n\nMotivating Example\n\nHere is an example of a stored procedure written in PLSQL:\n\nCREATE OR REPLACE PROCEDURE calc_secs_between ( date1 IN DATE, date2 IN DATE, secs OUT NUMBER ) IS BEGIN secs := (date2 - date1) * 24 * 60 * 60; END; /\n\nThis sample was taken from the examples that come with the utPLSQL tool. In real life we might not bother testing this code because it is so simple (but then again, maybe not?) but it will work just ﬁ ne to illustrate how we could go about testing it.\n\nRefactoring Notes\n\nThis example doesn’t deal so much with refactoring as with adding a missing test. Let’s ﬁ nd a way to write one. We will see what is involved by using the two main variants: In-Database Stored Procedure Test and Remote Stored Procedure Test.\n\nExample: In-Database Stored Procedure Test\n\nThis example uses utPLSQL, the xUnit family member for PLSQL, to automate tests that run inside the database:\n\nwww.it-ebooks.info",
      "content_length": 1973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 722,
      "content": "Stored Procedure Test\n\nCREATE OR REPLACE PACKAGE BODY ut_calc_secs_between IS PROCEDURE ut_setup IS BEGIN NULL; END;\n\nPROCEDURE ut_teardown IS BEGIN NULL; END;\n\n-- For each program to test... PROCEDURE ut_CALC_SECS_BETWEEN IS secs PLS_INTEGER; BEGIN CALC_SECS_BETWEEN ( DATE1 => SYSDATE ' DATE2 => SYSDATE ' SECS => secs );\n\nutAssert.eq ( 'Same dates', secs, 0 ); END ut_CALC_SECS_BETWEEN;\n\nEND ut_calc_secs_between; /\n\nThis test uses many of the familiar xUnit patterns. It is one of several tests we would normally write for this stored procedure—one test for each possible scenario. (This sample was taken from the examples that come with the utPLSQL tool. Not being a PLSQL programmer, I did not want to mess with the formatting in case it mattered!)\n\nExample: Remoted Stored Procedure Test\n\nTo test this stored procedure in our normal programming and test execution environment, we must ﬁ rst ﬁ nd or create a Remote Proxy for it in our unit-testing environment of choice. Then we can write our unit tests in the usual manner.\n\nwww.it-ebooks.info\n\n659\n\nStored Procedure Test",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 723,
      "content": "660\n\nStored Procedure Test\n\nChapter 25 Database Patterns\n\nThe following test uses JUnit to automate tests that run outside the database and call our PLSQL stored procedure remotely:\n\npublic class StoredProcedureTest extends TestCase { public void testCalcSecsBetween_SameTime() { // Setup TimeCalculatorProxy SUT = new TimeCalculatorProxy(); Calendar cal = new GregorianCalendar(); long now = cal.getTimeInMillis(); // Exercise long timeDifference = SUT.calc_secs_between(now,now); // Verify assertEquals( 0, timeDifference ); } }\n\nWe have reduced the complexity of the original test to a simple test of a function by hiding the JdbcOdbcCallableStatement behind a Service Facade. Looking at this example, it is difﬁ cult to tell that we are not testing a Java method. We would prob- ably have additional Expected Exception Tests (see Test Method on page 348) to verify failed connections and other problems.\n\nwww.it-ebooks.info",
      "content_length": 927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 724,
      "content": "Table Truncation Teardown\n\nTable Truncation Teardown\n\nHow do we tear down the Test Fixture when it is in a relational database?\n\nWe truncate the tables modiﬁ ed during the test to tear down the ﬁ xture.\n\nSetup Setup\n\nInsert Insert\n\nFixture Fixture\n\nExercise Exercise\n\nData Data\n\nVerify Verify\n\nExercise Exercise\n\nSUT SUT\n\nUpdate Update\n\nTeardown Teardown\n\nTruncate Truncate\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ xture is torn down after each test. Leftover objects and database records, as well as open ﬁ les and connections, can at best cause performance degradation and at worst cause tests to fail or systems to crash. While some of these resources may be cleaned up automatically by garbage collection, others may be left hanging if they are not torn down explicitly.\n\nWriting teardown code that can be relied upon to clean up properly in all pos- sible circumstances is challenging and time-consuming. It involves understand- ing what could be left over for each possible outcome of the test and writing code to deal with that possibility. This Complex Teardown (see Obscure Test on page 186) introduces a fair bit of Conditional Test Logic (page 200) and—worst of all—Untestable Test Code (see Hard-to-Test Code on page 209).\n\nWhen testing a system that uses a relational database, we can take advantage of the database’s capabilities by using the TRUNCATE command to remove all data from a table we have modiﬁ ed.\n\nHow It Works\n\nWhen we no longer need a persistent ﬁ xture, we issue a TRUNCATE command for each table in the ﬁ xture. It blasts all data out of the tables very efﬁ ciently with no side effects (e.g., triggers).\n\nwww.it-ebooks.info\n\n661\n\nTable Truncation Teardown",
      "content_length": 1726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 725,
      "content": "662\n\nTable Truncation Teardown\n\nChapter 25 Database Patterns\n\nWhen to Use It\n\nWe often turn to Table Truncation Teardown when we are using a Persistent Fresh Fixture (see Fresh Fixture on page 311) strategy with an SUT that includes a database. It is rarely our ﬁ rst choice, however. That distinction goes to Transac- tion Rollback Teardown (page 668). Nevertheless, Table Truncation Teardown is a better choice for use with a Shared Fixture (page 317), as this type of ﬁ xture, by deﬁ nition, outlives any one test. By contrast, using Transaction Rollback Teardown with a Shared Fixture would require a very long-running transaction. While not impossible, such a long-lived transaction is troublesome.\n\nBefore we can use Table Truncation Teardown, we must satisfy a couple of criteria. The ﬁ rst requirement is that we really want all data in the affected tables removed. The second requirement is that each Test Runner (page 377) has its own Database Sandbox (page 650). Table Truncation Teardown will not work if we are using a Database Partitioning Scheme (see Database Sandbox) to isolate users or tests from one another. It is ideally suited for use with a DB Schema per Test Runner (see Database Sandbox), especially when we are implementing an Immutable Shared Fixture (see Shared Fixture) as a separate shared schema in the database. This allows us to blast away all the Fresh Fixture data in our own Database Sandbox without affecting the Immutable Shared Fixture.\n\nIf we are not using a transactional database, the closest approximation is Automated Teardown (page 503), which deletes only those records that were created by the test. Automated Teardown does not depend on the database transactions to do the work for it, but it does involve more development work on our part. We can also avoid the need to do teardown entirely by using Delta Assertions (page 485).\n\nImplementation Notes\n\nBesides the usual “Where do we put the teardown code?” decision, implementa- tion of Table Truncation Teardown needs to deal with the following questions:\n\nHow do we actually delete the data—that is, which database commands\n\ndo we use?\n\nHow do we deal with foreign key constraints and triggers?\n\nHow do we ensure consistency when we are using an object-relational\n\nmapping (ORM)?\n\nSome databases support the TRUNCATE command directly. Where this is the case, the obvious choice is to use this command. Oracle, for example, supports TRUNCATE.\n\nwww.it-ebooks.info",
      "content_length": 2462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 726,
      "content": "Table Truncation Teardown\n\nOtherwise, we may have to use a DELETE * FROM table-name command instead. The TRUN- CATE or DELETE commands can be issued using In-line Teardown (page 509—called from within each Test Method; see page 348) or Implicit Teardown (page 516— called from the tearDown method). Some people prefer to use this command with Lazy Teardown because it ensures that the tables are empty at the beginning of the test in cases where those tables would be affected by extraneous data.\n\nDatabase foreign key constraints can be a problem for Table Truncation Teardown if our database does not offer something similar to Oracle’s ON DELETE CASCADE option. In Oracle, if the command to truncate a table includes the ON DELETE CASCADE option, then rows dependent on the truncated table rows are deleted as well. If our database does not cascade deletes, we must ensure that the tables are truncated in the order required by the schema. Schema changes can invalidate this order, resulting in failures in the teardown code. Fortunately, such failures are easy to detect: A test error tells us that our teardown needs adjusting. Correction is fairly straightforward—typically, we just need to reor- der the TRUNCATE commands. We could, of course, come up with a way to issue the TRUNCATE commands in the correct order dynamically based on the dependen- cies between the tables. Usually, however, it is enough to encapsulate this trun- cation logic behind a Test Utility Method (page 599).\n\nIf we want to avoid the side effects of triggers and other complications for databases where TRUNCATE is not supported, we can disable the constraints and/or triggers for the duration of the test. We should take this step only if other tests exercise the SUT with the constraints and triggers in place.\n\nIf we are using an ORM layer such as Toplink, (N)Hibernate, or EJB 3.0, we may need to force the ORM to clear its cache of objects already read from the database so that subsequent object lookups do not ﬁ nd the recently deleted objects. For example, NHibernate provides the ClearAllCaches method on the TransactionManager for this purpose.\n\nVariation: Lazy Teardown\n\nA teardown technique that works with only a few styles of Shared Fixtures is Lazy Teardown. With this pattern, the ﬁ xture must be destroyable at an arbitrary point in time. Thus we cannot depend on “remembering” what needs to be torn down; it must be obvious without any “memory.” Table Truncation Teardown ﬁ ts the bill because how we perform teardown is exactly the same whenever we choose to do it. We simply issue the table truncation commands during ﬁ xture setup before setting up the new ﬁ xture.\n\nwww.it-ebooks.info\n\n663\n\nTable Truncation Teardown",
      "content_length": 2722,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 727,
      "content": "664\n\nTable Truncation Teardown\n\nChapter 25 Database Patterns\n\nMotivating Example\n\nThe following test attempts to use Guaranteed In-line Teardown (see In-line Teardown) to remove all the records it created:\n\n[Test] public void TestGetFlightsByOrigin_NoInboundFlights() { // Fixture Setup long OutboundAirport = CreateTestAirport(\"1OF\"); long InboundAirport = CreateTestAirport(\"1IF\"); FlightDto ExpFlightDto = null; try { ExpFlightDto = CreateTestFlight(OutboundAirport, InboundAirport); // Exercise System IList FlightsAtDestination1 = Facade.GetFlightsByOriginAirport( InboundAirport); // Verify Outcome Assert.AreEqual( 0, FlightsAtDestination1.Count ); } ﬁnally { Facade.RemoveFlight( ExpFlightDto.FlightNumber ); Facade.RemoveAirport( OutboundAirport ); Facade.RemoveAirport( InboundAirport ); } }\n\nThis code is neither easy to write nor correct!1 Trying to keep track of the many objects the SUT has created and then tear them down one by one in a safe man- ner is very tricky.\n\nRefactoring Notes\n\nWe can avoid most of the issues with coordinating In-line Teardown of mul- tiple resources in a safe way by using Table Truncation Teardown and blasting away all the airports in one fell swoop.2 Most of the refactoring work involves deleting the existing teardown code from the ﬁ nally clause and inserting a call to cleanDatabase. We then implement this method using the truncation commands.\n\n1 See In-line Teardown for an explanation of what is wrong here. 2 This assumes that we start with no airports and want to end with no airports. If we want to delete just these speciﬁ c airports, we cannot use Table Truncation Teardown.\n\nwww.it-ebooks.info",
      "content_length": 1653,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 728,
      "content": "Table Truncation Teardown\n\nExample: Table Truncation (Delegated) Teardown Test\n\nThis is what the test looks like when we are done:\n\npublic void TestGetFlightsByOrigin_NoInboundFlight_TTTD() { // Fixture Setup long OutboundAirport = CreateTestAirport(\"1OF\"); long InboundAirport = 0; FlightDto ExpectedFlightDto = null; try { InboundAirport = CreateTestAirport(\"1IF\"); ExpectedFlightDto = CreateTestFlight( OutboundAirport,InboundAirport); // Exercise System IList FlightsAtDestination1 = Facade.GetFlightsByOriginAirport(InboundAirport); // Verify Outcome Assert.AreEqual(0,FlightsAtDestination1.Count); } ﬁnally { CleanDatabase(); } }\n\nThis example uses Delegated Teardown (see In-line Teardown) to keep the teardown code visible. Normally, however, we would use Implicit Teardown by putting this logic into the tearDown method. The try/catch ensures that clean- Database is run but it does not ensure that a failure inside cleanDatabase will not prevent the teardown from completing.\n\nExample: Lazy Teardown Test\n\nHere is the same example converted to use Lazy Teardown:\n\n[Test] public void TestGetFlightsByOrigin_NoInboundFlight_LTD() { // Lazy Teardown CleanDatabase(); // Fixture Setup long OutboundAirport = CreateTestAirport(\"1OF\"); long InboundAirport = 0; FlightDto ExpectedFlightDto = null; InboundAirport = CreateTestAirport(\"1IF\"); ExpectedFlightDto = CreateTestFlight( OutboundAirport, InboundAirport); // Exercise System\n\nwww.it-ebooks.info\n\n665\n\nTable Truncation Teardown",
      "content_length": 1486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 729,
      "content": "666\n\nTable Truncation Teardown\n\nChapter 25 Database Patterns\n\nIList FlightsAtDestination1 = Facade.GetFlightsByOriginAirport(InboundAirport); // Verify Outcome Assert.AreEqual(0,FlightsAtDestination1.Count); }\n\nBy moving the call to cleanDatabase to the front of the Test Method, we ensure that the database is in the state we expect it. This code cleans up whatever the last test did, regardless of whether that test provided proper teardown. It also takes care of anything added to the relevant tables since the last test was run. It has the added beneﬁ t of eliminating the need for the try/ﬁ nally construct, thereby making the test simpler and easier to understand.\n\nExample: Table Truncation Teardown Using SQL\n\nThis implementation of the cleanDatabase method uses SQL statements constructed within the code:\n\npublic static void CleanDatabase() { string[] tablesToTruncate = new string[] {\"Airport\",\"City\",\"Airline_Cd\",\"Flight\"}; IDbConnection conn = getCurrentConnection(); IDbTransaction txn = conn.BeginTransaction(); try { foreach (string eachTableToTruncate in tablesToTruncate) { TruncateTable(txn, eachTableToTruncate); } txn.Commit(); conn.Close(); } catch (Exception e) { txn.Rollback(); } ﬁnally { conn.Close(); } }\n\nprivate static void TruncateTable( IDbTransaction txn, string tableName) { const string C_DELETE_SQL = \"DELETE FROM {0}\";\n\nIDbCommand cmd = txn.Connection.CreateCommand(); cmd.Transaction = txn; cmd.CommandText = string.Format(C_DELETE_SQL, tableName);\n\ncmd.ExecuteNonQuery(); }\n\nwww.it-ebooks.info",
      "content_length": 1531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 730,
      "content": "Table Truncation Teardown\n\nBecause we are using SQL Server as the database, we had to implement our own TruncateTable method that issues a Delete * from ... SQL command. We would not have to take this step if our database implemented TRUNCATE directly.\n\nExample: Table Truncation Teardown Using ORM\n\nHere is the implementation of the cleanDatabase method using NHibernate, an ORM layer:\n\npublic static void CleanDatabase() { ISession session = TransactionManager.Instance.CurrentSession; TransactionManager.Instance.BeginTransaction(); try { // We need to delete only the root classes because // cascade rules will delete all related child entities session.Delete(\"from Airport\"); session.Delete(\"from City\"); session.Flush(); TransactionManager.Instance.Commit(); } catch (Exception e) { Console.Write(e); throw e; } ﬁnally { TransactionManager.Instance.CloseSession(); } }\n\nWhen using an ORM, we read, write, and delete domain objects; the tool deter- mines which underlying tables they map to and takes the appropriate actions. Because we have chosen to make City and Airport “root” (parent) objects, any subordinate (child) objects such as the Flights are deleted automatically when the root is deleted. This approach further decouples us from the details of the table implementations.\n\nwww.it-ebooks.info\n\n667\n\nTable Truncation Teardown",
      "content_length": 1341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 731,
      "content": "668\n\nTransaction Rollback Teardown\n\nChapter 25 Database Patterns\n\nTransaction Rollback Teardown\n\nHow do we tear down the Test Fixture when it is in a relational database?\n\nWe roll back the uncommitted test transaction as part of the teardown.\n\nSetup Setup\n\nStart Transaction Start Transaction\n\nFixture Fixture\n\nInsert Insert\n\nExercise Exercise\n\nData Data\n\nVerify Verify\n\nExercise Exercise\n\nSUT SUT\n\nUpdate Update\n\nTeardown Teardown\n\nRollback Rollback\n\nA large part of making tests repeatable and robust is ensuring that the test ﬁ x- ture is torn down after each test. Leftover objects and database records, as well as open ﬁ les and connections, can at best cause performance degradation and at worst cause tests to fail or systems to crash. While some of these resources may be cleaned up automatically by garbage collection, others may be left hanging if they are not torn down explicitly.\n\nWriting teardown code that can be relied upon to clean up properly in all possible circumstances is challenging and time-consuming. It involves under- standing what could be left over for each possible outcome of the test and writing code to deal with this case. This Complex Teardown (see Obscure Test on page 186) introduces a fair bit of Conditional Test Logic (page 200) and— worst of all—Untestable Test Code (see Hard-to-Test Code on page 209).\n\nWe can avoid making any lasting changes to the database contents by not committing the transaction and taking advantage of the rollback capabilities of the database.\n\nHow It Works\n\nOur test starts a new test transaction, sets up the ﬁ xture, exercises the SUT, and veriﬁ es the outcome of the test. Each of these steps may involve interacting with\n\nwww.it-ebooks.info",
      "content_length": 1713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 732,
      "content": "Transaction Rollback Teardown\n\nthe database. At the end of the test, the test rolls back the test transaction, which prevents any of the changes from becoming persistent.\n\nWhen to Use It\n\nWe can use Transaction Rollback Teardown when we are using a Fresh Fix- ture (page 311) approach with an SUT that includes a database that supports rolling back a transaction. There are, however, some prerequisites for using Transaction Rollback Teardown.\n\nIn particular, the SUT must expose methods that are normally called in the context of an existing transaction by a Humble Transaction Controller (see Humble Object on page 695). That is, the methods should not start their own transaction and must never commit a transaction. If we are doing test-driven development, this design will come about as a result of applying the Transac- tion Rollback Teardown pattern as we write our code. If we are retroﬁ tting the tests to existing software, we may need to refactor the code to use a Humble Transaction Controller before we can use Transaction Rollback Teardown.\n\nThe nice thing about Transaction Rollback Teardown is that it leaves the database in exactly the same state as it was when we started the test, regard- less of what changes we made to the database contents during the test. As a result, we do not need to determine what needs to be cleaned up and what does not. Changes to the database schema or contents do not affect our teardown logic. Clearly, this pattern is much simpler to apply than Table Truncation Tear- down (page 661).\n\nThe usual caveats apply to any tests that run against a real database; such tests will take approximately 50 (yes, 50!) times as long to run as tests that do not access the database. This testing approach will almost surely result in Slow Tests (page 253) unless we replace the real database with an In-Memory Database (see Fake Object on page 551) for most of our tests. Because we are depending on the transactional properties of the database, a simple Fake Data- base (see Fake Object) will probably not be sufﬁ cient unless it supports ACID.\n\nAnother prerequisite with Transaction Rollback Teardown is that we cannot do anything that results in a commit anywhere in the tests or the code they exer- cise. The sidebar “Transaction Rollback Pain” on page 670 describes examples of where commits can sneak in and cause havoc.\n\nwww.it-ebooks.info\n\n669\n\nTransaction Rollback Teardown",
      "content_length": 2419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 733,
      "content": "670\n\nTransaction Rollback Teardown\n\nChapter 25 Database Patterns\n\nTransaction Rollback Pain\n\nJohn Hurst sent me an e-mail in which he described some of the issues his team had encountered using Transaction Rollback Teardown. He writes:\n\nWe used Transaction Rollback Teardown for our database integra- tion tests for a while, after a discussion on TheServerSide during which Rod Johnson advocated the approach. I gathered his main motivation for using it was for performance; a rollback is usually a lot faster than repriming the database in a new transaction for the next test. Indeed, we did ﬁ nd it somewhat faster than our pre- vious approach. We used Spring’s excellent AbstractTransactionalData- SourceSpringContextTests base class, which supports most of what you need to do for this pattern out of the box.\n\nHowever, I chose to abandon this pattern after a few months. Here are the drawbacks I came across with this approach:\n\n1. You lose some test isolation. In the way we implemented this pattern, anyway, each test assumed the database was in a cer- tain base starting condition, and the rollback would revert it to that condition. In our current model, each test is respon- sible—usually via a base class’s setUp()—for priming the data- base into a known state.\n\n2. You can’t see what’s in the database when something goes wrong. If your test fails, you usually want to examine the database to see what happened. If you’ve rolled back all the changes, it makes it harder to ﬁ nd the bug.\n\n3. You have to be very careful not to inadvertently commit during your test. Yes, the code under test has declarative transaction management, and does nothing surprising. But we occasionally would need to do things in the test setup like drop and recreate a sequence to reset its value. This, being DDL, commits any outstanding transaction—and confused programmers.\n\n4. You can’t easily mix in tests that do need to commit changes. Lately I have added some PLSQL stored procedures and tests. Some of the stored procedures do explicit commits. I cannot mix these in the same JUnit suite with tests that assume the database always remains in a certain state.\n\nI apologize if my terminology isn’t consistent with what’s in your book. Also, my experience is probably a little limited; I’ve only\n\nwww.it-ebooks.info",
      "content_length": 2310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 734,
      "content": "Transaction Rollback Teardown\n\ntried this approach in a Spring environment and I prefer to do most things in a “Spring” way. Finally, I am sure these limitations can be and are worked around in various ways. It’s just that, for our team, this pattern turned out to be more trouble than it was worth.\n\nDon’t get me wrong—I DO think the pattern should be included. I just think the consequences should be noted, and maybe it isn’t for everyone.\n\nImplementation Notes\n\nA few members of the xUnit family support Transaction Rollback Teardown directly; open-source extensions may be available for other members. If nothing is available, coding this teardown logic is not very complicated. The more signiﬁ - cant implementation consideration is giving the tests access to nontransactional methods on the SUT. Most domain model objects are nontransactional, so this requirement should not be a problem for unit tests of domain objects. We are more likely to experience a problem when we are writing Subcutaneous Tests (see Layer Test on page 337) against a Service Facade [CJ2EEP] because these methods often perform transaction control. If this is the case, we will need to expose a nontransactional version of the methods by refactoring to the Humble Transaction Controller pattern. We could either use a transactional Decorator [GOF] as a separate object or simply have the transactional methods delegate to the nontransactional versions of the methods on self. This approach is called a Poor Man’s Humble Object (see Humble Object).\n\nIf the methods exist but are not visible to the client, we will need to expose them to the test. We can do so either by making the methods to be tested pub- lic or by exposing them indirectly via a Test-Speciﬁ c Subclass (page 579). We could also do an Extract Testable Component (page 735) refactoring to move the nontransactional versions of the methods to a different class and make them visible to the test from there.\n\nAny reading of the updated data in the database must occur within the context of the same transaction. This normally is not a problem except when we are trying to simulate or test concurrency. If we are using an ORM layer such as Toplink, (N)Hibernate, or EJB 3.0, we may need to force the ORM to write the changes made to the objects to the database so that methods that read the database directly (from within the same transactional context) can see them. For example, EJB 3.0 provides the EntityManager.ﬂ ush static method for exactly this purpose.\n\nwww.it-ebooks.info\n\n671\n\nTransaction Rollback Teardown",
      "content_length": 2562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 735,
      "content": "672\n\nTransaction Rollback Teardown\n\nChapter 25 Database Patterns\n\nMotivating Example\n\nThe following test attempts to use Guaranteed In-line Teardown (see In-line Teardown on page 509) to remove all the records it created:\n\npublic void testGetFlightsByOriginAirport_NoInboundFlights() throws Exception { // Fixture Setup BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = createTestAirport(\"1IF\"); FlightDto expFlightDto = null; try { expFlightDto = createTestFlight(outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport( inboundAirport); // Verify Outcome assertEquals( 0, ﬂightsAtDestination1.size() ); } ﬁnally { facade.removeFlight( expFlightDto.getFlightNumber() ); facade.removeAirport( outboundAirport ); facade.removeAirport( inboundAirport ); } }\n\nThis code is neither easy to write nor correct!3 Trying to keep track of all objects the SUT has created and then tear them down one by one in a safe manner is very tricky.\n\nRefactoring Notes\n\nWe can avoid most of the issues related to coordinating In-line Teardown of multiple resources in a safe way by using Transaction Rollback Teardown and blasting away all changes to the objects in one fell swoop. Most of the refactor- ing work consists of deleting the existing teardown code from the ﬁ nally clause and inserting a call to the abortTransaction method. We also need to make the call to beginTransaction before we do any ﬁ xture setup, and we have to modify the Creation Methods (page 415) to ensure that they do not commit a transaction. To do so, we have them call a nontransactional version of the methods on the Service Facade.\n\n3 See In-line Teardown for an explanation of what is wrong here.\n\nwww.it-ebooks.info",
      "content_length": 1772,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 736,
      "content": "Transaction Rollback Teardown\n\nExample: Object Transaction Rollback Teardown\n\nHere is what the test looks like when we are done:\n\npublic void testGetFlightsByOrigin_NoInboundFlight_TRBTD() throws Exception { // Fixture Setup TransactionManager.beginTransaction(); BigDecimal outboundAirport = createTestAirport(\"1OF\"); BigDecimal inboundAirport = null; FlightDto expectedFlightDto = null; try { inboundAirport = createTestAirport(\"1IF\"); expectedFlightDto = createTestFlight( outboundAirport, inboundAirport); // Exercise System List ﬂightsAtDestination1 = facade.getFlightsByOriginAirport(inboundAirport); // Verify Outcome assertEquals(0,ﬂightsAtDestination1.size()); } ﬁnally { TransactionManager.abortTransaction(); } }\n\nIn this refactored test, we have replaced the multiple lines of teardown code in the ﬁ nally clause with a single call to abortTransaction. We still need the ﬁ nally clause because this example is using In-line Teardown; we could easily move this call to the TransactionManager to the tearDown method because it is so generic.\n\nIn this example, Transaction Rollback Teardown undoes the ﬁ xture setup performed by the various Creation Methods we called earlier in the test. The ﬁ xture objects have not yet been committed to the database. Because getFlights- FromAirport is being called within the context of the transaction, however, it returns the newly added but not yet committed ﬂ ights. (That is the “C” for “consistent” in ACID working on our behalf!)\n\nprivate BigDecimal createTestAirport(String airportName) throws FlightBookingException { BigDecimal newAirportId = facade._createAirport( airportName, \" Airport\" + airportName, \"City\" + airportName); return newAirportId; }\n\nwww.it-ebooks.info\n\n673\n\nTransaction Rollback Teardown",
      "content_length": 1762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 737,
      "content": "674\n\nTransaction Rollback Teardown\n\nChapter 25 Database Patterns\n\nThe creation method calls the nontransactional version of the facade method (an example of a Poor Man’s Humble Object):\n\npublic BigDecimal createAirport( String airportCode, String name, String nearbyCity) throws FlightBookingException{ TransactionManager.beginTransaction(); BigDecimal airportId = _createAirport(airportCode, name, nearbyCity); TransactionManager.commitTransaction(); return airportId; }\n\n// private, nontransactional version for use by tests BigDecimal _createAirport( String airportCode, String name, String nearbyCity) throws DataException, InvalidArgumentException { Airport airport = dataAccess.createAirport(airportCode,name,nearbyCity); logMessage(\"CreateFlight\", airport.getCode()); return airport.getId(); }\n\nIf the method we were exercising (e.g., getFlightsFromAirport) did modify the state of the SUT and did begin and end its own transaction, we would have to do a similar refactoring on it as well.\n\nExample: Database Transaction Rollback Teardown\n\nThe ﬁ rst example hid the database from the code behind a data access layer that returned or accepted objects. This is common practice when using the Domain Model [PEAA] pattern for organizing the business logic. Transaction Rollback Teardown is typically used when manipulating the database directly in our ap- plication logic (a style known as a Transaction Script [PEAA]). The following example illustrates this approach using .NET row sets (or something similar):\n\n[TestFixture] public class TransactionRollbackTearDownTest { private SqlConnection _Connection; private SqlTransaction _Transaction;\n\npublic TransactionRollbackTearDownTest() { }\n\n[SetUp]\n\nwww.it-ebooks.info",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 738,
      "content": "Transaction Rollback Teardown\n\npublic void Setup() { string dbConnectionString = ConﬁgurationSettings. AppSettings.Get(\"DbConnectionString\"); _Connection = new SqlConnection(dbConnectionString); _Connection.Open(); _Transaction = _Connection.BeginTransaction(); }\n\n[TearDown] public void TearDown() { _Transaction.Rollback(); _Connection.Close(); // Avoid NUnit \"instance behavior\" bug _Transaction = null; _Connection = null; }\n\n[Test] public void AnNUnitTest() { const string C_INSERT_SQL = \"INSERT INTO Invoice(Amount, Tax, CustomerId)\" + \" VALUES({0}, {1}, {2})\"; SqlCommand cmd = _Connection.CreateCommand(); cmd.Transaction = _Transaction; cmd.CommandText = string.Format( C_INSERT_SQL, new object[] {\"100.00\", \"7.00\", 2001}); // Exercise SUT cmd.ExecuteNonQuery(); // Verify result // etc. } } }\n\nThis example uses Implicit Setup (page 424) to establish the connection and start the transaction. After the Test Method (page 348) has run, it uses Implicit Teardown (page 516) to roll back the transaction and close the connection. We assign null to the instance variables because NUnit does not create a separate Testcase Object (page 382) for each Test Method, unlike most other members of xUnit. See the sidebar “There’s Always an Exception” on page 384 for details.\n\nwww.it-ebooks.info\n\n675\n\nTransaction Rollback Teardown",
      "content_length": 1330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 739,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 740,
      "content": "Chapter 26\n\nDesign-for-Testability Patterns\n\nPatterns in This Chapter\n\nDependency Injection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 678\n\nDependency Lookup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 686\n\nHumble Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 695\n\nTest Hook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 709\n\n677\n\nwww.it-ebooks.info\n\nDesign-for- Testability Patterns",
      "content_length": 521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 741,
      "content": "678\n\nDependency Injection\n\nChapter 26 Design-for-Testability Patterns\n\nDependency Injection\n\nHow do we design the SUT so that we can replace its dependencies at runtime?\n\nThe client provides the depended-on object to the SUT.\n\nClient Client\n\nCreation Creation\n\nDOC DOC\n\nUsage Usage\n\nUsage Usage\n\nSetup Setup Exercise Exercise Verify Verify Teardown Teardown\n\nExercise Exercise\n\nCreation Creation\n\nSUT SUT\n\nUsage Usage\n\nTest Test Double Double\n\nAlmost every piece of code depends on some other classes, objects, modules, or procedures. To unit-test a piece of code properly, we would like to isolate the code from its dependencies. This isolation is difﬁ cult to achieve if those depen- dencies are hard-coded in the form of literal classnames.\n\nDependency Injection is a way to allow the normal coupling between a SUT\n\nand its dependencies to be broken during automated testing.\n\nHow It Works\n\nWe avoid hard-coding the names of classes on which we depend into our code by providing some other means for the client or system conﬁ guration to tell the SUT which objects to use for each dependency as it is executed. As part of the design of the SUT, we arrange to pass the dependency in to the SUT through the “front door.” That is, the means to specify the dependency becomes part of the API of the SUT. We can include it as an argument with each method call, include it on the constructor, or make it a settable attribute (property).\n\nWhen to Use It\n\nWe need to provide a means to substitute a depended-on component (DOC) to make it easy to use a Test Double (page 522) while testing our code. Static\n\nwww.it-ebooks.info",
      "content_length": 1620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 742,
      "content": "Dependency Injection\n\nbinding—that is, specifying exact types or classes at compile time—severely limits our options regarding how the software is conﬁ gured as it runs. Dynamic binding creates much more ﬂ exible software by deferring the decision of exactly which type or class to use until runtime. Dependency Injection is a good choice for com- municating which class to use when we are designing the software from scratch. It offers a natural way to design the code when we are doing test-driven development (TDD) because many of the tests we write for dependent objects seek to replace a DOC with a Test Double.\n\nWhen we don’t have complete control over the code we are testing, such as when we are retroﬁ tting tests to existing code,1 we may need to use some other means to introduce the Test Doubles. If the SUT uses Dependency Lookup (page 686) to ﬁ nd the DOC, we can override the lookup mechanism to return the Test Double. We can also use a Test-Speciﬁ c Subclass (page 579) of the SUT to return a Test Double as long as access to the DOC remains encapsulated behind a method call.\n\nImplementation Notes\n\nIntroducing Dependency Injection requires solving two problems. First, we must be able to use a Test Double wherever the real DOC is used. This constraint is primarily an issue in statically typed languages because we must convince the compiler to allow us to pass off a Test Double as the real thing. Second, we must provide a way to tell the SUT to use the Test Double.\n\nType Compatibility\n\nWhichever way we choose to install the dependency into the SUT, we must also ensure that the Test Double we want to replace it with is “type compatible” with the code that uses the Test Double. This is most easily accomplished if both the real component and the Test Double implement the same interface (in statically typed languages) or have the same signature (in dynamically typed languages). A quick way to introduce a Test Double into existing code is to do an Extract Interface [Fowler] refactoring on the real DOC and then have the Test Double implement the new interface.\n\nInstalling the Test Double\n\nThere are a number of different ways to tell the SUT to use the Test Double, but they all involve replacing a hard-coded name with a mechanism that determines the type of object to use at execution time. The three basic options are as follows:\n\n1 “If it ain’t broke, don’t change it (even to improve the testability)” is a common, albeit somewhat misguided, constraint in these circumstances.\n\nwww.it-ebooks.info\n\n679\n\nDependency Injection",
      "content_length": 2558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 743,
      "content": "680\n\nDependency Injection\n\nChapter 26 Design-for-Testability Patterns\n\nParameter Injection: We pass the dependency directly to the SUT as we\n\ninvoke it.\n\nConstructor Injection: We tell the SUT which DOC to use when we\n\nconstruct it.\n\nSetter Injection: We tell the SUT about the DOC sometime between\n\nwhen we construct it and when we exercise it.\n\nEach of these three variations of Dependency Injection can be hand-coded. Another option is to use an “Inversion of Control” (IoC) framework to link the various components together at runtime. This scheme avoids superﬂ uous diversity in how Dependency Injection is implemented across the application and can simplify the process of reconﬁ guring the application for different deployment models.\n\nVariation: Parameter Injection\n\nParameter Injection is a form of Dependency Injection in which the SUT does not keep or initialize a reference to the DOC; instead, it is passed in as an argument of the method being called on the SUT. All clients of the SUT—whether they are tests or production code—supply the DOC. As a consequence, the SUT is more indepen- dent of the context because it makes no assumptions about the dependency other than its usage interface. The main drawback is that Parameter Injection forces the client to know about the dependency, which is more appropriate in some circum- stances than in others. Most of the other variants of Dependency Injection move this knowledge somewhere other than the client or at least make it optional.\n\nParameter Injection is advocated by the original paper on Mock Objects (page 544) [ET]. It is especially effective when we are doing true TDD because that’s when we have the greatest control over the design. It is possible to introduce Parameter In- jection in an optional fashion by providing an alternative signature for the method in question with the extra parameter; we can then have the more traditional style method create the instance of the dependency and call the method that takes the de- pendency as a parameter.\n\nVariation: Constructor Injection\n\nBoth Constructor Injection and Setter Injection involve storing a reference to the DOC as an attribute (ﬁ eld or instance variable) of the SUT. With Dependency Injection, the ﬁ eld is initialized from a constructor argument. The SUT may optionally provide a simpler constructor that calls this constructor with the value normally used in production. When a test wants to replace the real DOC with a Test Double, it passes in the Test Double to the constructor when it builds the SUT.\n\nwww.it-ebooks.info",
      "content_length": 2563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 744,
      "content": "Dependency Injection\n\nThis approach to introducing Dependency Injection works well when the code includes only one or two constructors and they have small argument lists. Constructor Injection is the only approach that works if the DOC is an active object that creates its own thread of execution during construction; such behavior would make for Hard-to-Test Code (page 209), and we should probably consider turning it into a Humble Executable (see Humble Object on page 695). If we have a large number of dependencies as constructor argu- ments, we probably need to refactor the code to remove this code smell.\n\nVariation: Setter Injection\n\nAs with Constructor Injection, the SUT holds a reference to the DOC as an attri- bute (ﬁ eld) that is initialized in the constructor. Where it differs is that the attribute is exposed to the client either as a public attribute or via a “setter” method. When a test wants to replace the real DOC with a Test Double, it assigns to the exposed attribute (or calls the setter with) an instance of the Test Double. This approach works well when constructing the real DOC has no unpleasant side effects and assuming that nothing can happen automatically between the constructor call and the point at which the test calls the setter for the property. Setter Injection cannot be used if the SUT performs any signiﬁ cant processing in the constructor that relies on the dependency. In that case, we must use Constructor Injection. If constructing the real DOC has deleterious side effects, we can avoid creating it via the construc- tor by modifying the SUT to use Lazy Initialization [SBPP] to instantiate the DOC the ﬁ rst time the SUT needs to use it.\n\nRetroﬁ tting Dependency Injection\n\nWhen the SUT does not support any of these options “out of the box,” we may be able to retroﬁ t this capability via a Test-Speciﬁ c Subclass. If the actual class to be used is normally retrieved from conﬁ guration data, this retrieval should be done by some component other than the SUT and the class then passed to the SUT using Dependency Injection. Such a use of the Humble Object pattern for the client or conﬁ guration decouples the SUT from the environment and ensures that tests do not need to set up some external dependency (the conﬁ guration ﬁ le) to introduce the Test Double.\n\nAnother possibility is to use aspect-oriented programming (AOP) to insert the Dependency Injection mechanism into the development environment. For example, we might inject the decision to use the Test Double or inject the test- speciﬁ c logic—the Test Double—directly into the SUT. I don’t think we have enough experience with using AOP to call this a pattern just yet.\n\nwww.it-ebooks.info\n\n681\n\nDependency Injection",
      "content_length": 2730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 745,
      "content": "682\n\nDependency Injection\n\nChapter 26 Design-for-Testability Patterns\n\nMotivating Example\n\nThe following test cannot be made to pass “as is”:\n\npublic void testDisplayCurrentTime_AtMidnight() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result); }\n\nThis test almost always fails because it depends on the current time being returned to the SUT by a DOC. The test cannot control the values being returned by that component, the DefaultTimeProvider. Therefore, this test will pass only when the system time is exactly midnight.\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar currentTime; try { currentTime = new DefaultTimeProvider().getTime(); } catch (Exception e) { return e.getMessage(); } // etc. }\n\nBecause the SUT is hard-coded to use a particular class to retrieve the time, we cannot replace the DOC with a Test Double. That constraint makes this test nondeterministic and pretty much useless. We need to ﬁ nd a way to gain control over the indirect inputs of the SUT.\n\nRefactoring Notes\n\nWe can use a Replace Dependency with Test Double (page 522) refactoring to gain control over the time. Setter Injection can be introduced into existing code if we have control over the code and the method in question is not widely used or if we have refactoring tools that support the Introduce Parameter [JBrains] refactor- ing. Failing that, we can use an Extract Method [Fowler] refactoring to create the new method signature that takes the Dependency Injection as an argument and leave the old method as an Adapter [GOF] that calls the new method.\n\nwww.it-ebooks.info",
      "content_length": 1788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 746,
      "content": "Dependency Injection\n\nExample: Parameter Injection\n\nHere’s the test rewritten to use Parameter Injection:\n\npublic void testDisplayCurrentTime_AtMidnight_PI() { // Fixture setup // Test Double instantiation TimeProvider tpStub = new MidnightTimeProvider(); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Exercise SUT using Test Double String result = sut.getCurrentTimeAsHtmlFragment(tpStub); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nIn this case, only the test will use the new signature. The existing code can use the old signature and the method adapter instantiates the real dependency object before passing it in.\n\npublic String getCurrentTimeAsHtmlFragment( TimeProvider timeProviderArg) { Calendar currentTime; try { currentTime = timeProviderArg.getTime(); } catch (Exception e) { return e.getMessage(); } // etc. }\n\nExample: Constructor Injection\n\nHere’s the same test rewritten to use Constructor Injection:\n\npublic void testDisplayCurrentTime_AtMidnight_CI() throws Exception { // Fixture setup // Test Double instantiation TimeProvider tpStub = new MidnightTimeProvider(); // Instantiate SUT injecting Test Double TimeDisplay sut = new TimeDisplay(tpStub); // Exercise SUT String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">12:01 AM</span>\"; // Verify outcome\n\nwww.it-ebooks.info\n\n683\n\nDependency Injection",
      "content_length": 1446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 747,
      "content": "684\n\nDependency Injection\n\nChapter 26 Design-for-Testability Patterns\n\nassertEquals(\"12:01 AM\", expectedTimeString, sut.getCurrentTimeAsHtmlFragment()); }\n\nTo convert the SUT to use Constructor Injection, we can do an Introduce Field [JetBrains] refactoring to hold the DOC in a ﬁ eld that is initialized in the existing constructor. We can then do an Introduce Parameter refactoring to modify all callers of the existing constructor so that they pass the real DOC as a parameter of the constructor. If we cannot or do not want to modify all existing callers of the constructor, we can deﬁ ne a new constructor that takes the DOC as a parameter and modify the existing constructor to instantiate the real DOC and pass it in to our new constructor.\n\npublic class TimeDisplay {\n\nprivate TimeProvider timeProvider;\n\npublic TimeDisplay() { // backwards compatible constructor timeProvider = new DefaultTimeProvider(); } public TimeDisplay(TimeProvider timeProvider) { // new constructor this.timeProvider = timeProvider; }\n\nAnother approach is to do an Extract Method refactoring on the call to the con- structor and then use Move Method [Fowler] refactoring to move it to an Object Factory (see Dependency Lookup). That would result in Dependency Lookup.\n\nExample: Setter Injection\n\nHere is the same test refactored to use Setter Injection:\n\npublic void testDisplayCurrentTime_AtMidnight_SI() throws Exception { // Fixture setup // Test Double instantiation TimeProvider tpStub = new MidnightTimeProvider(); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Test Double installation sut.setTimeProvider(tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nwww.it-ebooks.info",
      "content_length": 1855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 748,
      "content": "Dependency Injection\n\nNote the call to setTimeProvider to install the Hard-Coded Test Double (page 568). If we had used a Conﬁ gurable Test Double (page 558), its conﬁ guration would occur immediately before the call to setTimeProvider.\n\nTo refactor the SUT to support Setter Injection, we can do an Introduce Field refactoring to hold the DOC in a variable that is initialized in the exist- ing constructor and call the DOC via this ﬁ eld. We can then expose the ﬁ eld either directly or via a setter so that the test can override its value. Here is the refactored version of the SUT:\n\npublic class TimeDisplay {\n\nprivate TimeProvider timeProvider;\n\npublic TimeDisplay() { timeProvider = new DefaultTimeProvider(); } public void setTimeProvider(TimeProvider provider) { this.timeProvider = provider; } public String getCurrentTimeAsHtmlFragment() throws TimeProviderEx { Calendar currentTime; try { currentTime = getTimeProvider().getTime(); } catch (Exception e) { return e.getMessage(); } // etc.\n\nHere we chose to use a getter to retrieve the DOC. We could just as easily have used the timeProvider ﬁ eld directly.\n\nwww.it-ebooks.info\n\n685\n\nDependency Injection",
      "content_length": 1165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 749,
      "content": "686\n\nAlso known as: Service Locator, Object Factory, Component Broker, Component Registry\n\nDependency Lookup\n\nChapter 26 Design-for-Testability Patterns\n\nDependency Lookup\n\nHow do we design the SUT so that we can replace its dependencies at runtime?\n\nThe SUT asks another object to return the depended-on object before it uses it.\n\nConfiguration Configuration with Test Double with Test Double\n\nSetup Setup\n\nExercise Exercise\n\nVerify Verify\n\nExercise Exercise\n\nFind or Create Find or Create\n\nCreation Creation\n\nor or\n\nDOC DOC\n\nTeardown Teardown\n\nCreation Creation\n\nClient Client\n\nUsage Usage\n\nSUT SUT\n\nUsage Usage\n\nUsage Usage\n\nTest Test Double Double\n\nAlmost every piece of code depends on some other classes, objects, modules, or procedures. To unit-test a piece of code properly, we would like to isolate it from its dependencies. Such isolation is difﬁ cult to achieve, however, if those depen- dencies are hard-coded within the code in the form of literal classnames.\n\nDependency Lookup is a way to allow the normal coupling between a SUT\n\nand its dependencies to be broken during automated testing.\n\nHow It Works\n\nWe avoid hard-coding the names of classes on which the SUT depends into our code because static binding severely limits our options regarding how the software is conﬁ gured as it runs. Instead, we hard-code that name of a “compo- nent broker” that returns a ready-to-use object. The component broker provides some means for the client software or perhaps a system conﬁ guration manager to tell the SUT in question which objects to use for each component request.\n\nwww.it-ebooks.info",
      "content_length": 1602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 750,
      "content": "Dependency Lookup\n\nWhen to Use It\n\nDependency Lookup is most appropriate when we need to retrieve DOCs from deep inside the system and it would be too messy to pass the Test Double (page 522) in from the client. A good example of such a situation is when we want to replace the data access layer of the system with a Fake Database (see Fake Object on page 551) or In-Memory Database (see Fake Object) to speed up execution of the automated customer tests. It would be too complex for each Subcutaneous Test (see Layer Test on page 337) to pass the Fake Database in through the Service Facade [CJ2EEP] and all the way down to the data access layer. Using Dependency Lookup allows the test or even a Setup Decorator (page 447) to use a “conﬁ guration facade” to install the Fake Database, which the SUT can magically use without any further ado. Jeremy Miller writes:\n\nYou cannot understate the value of using a Service Locator for automated testing. We routinely use alternative dependencies in testing, both to deal with difﬁ cult dependencies and for test performance. For example, in a functional test we’ll collapse a Web site and a backing application server into a single process for better performance.\n\nDependency Lookup tends to be a lot simpler to retroﬁ t onto existing legacy software because it affects only those places where object construction actually occurs; we do not need to modify every intermediate object or method, as we might have to do with Dependency Injection (page 678). It is also much simpler to retroﬁ t existing round-trip tests so that they use a Fake Object to speed them up by wrapping them in a Setup Decorator. With this scheme, we do not have to change each test; instead, we can create new instances of the SUT in each test and still have the test use the same Fake Object because the Service Locator remembers it across tests.2\n\nThe main alternative to Dependency Lookup is to provide a substitution mechanism within the SUT using Dependency Injection. This approach is gen- erally preferable for unit tests because it makes the replacement of the DOC more obvious and directly connected to exercising the SUT. Another option is to use AOP to install test-speciﬁ c logic using the development tools rather than modifying the design of the software. The least preferred solution is to use a Test Hook (page 709) within the SUT to avoid calling the DOC or within the DOC so that it behaves in a test-speciﬁ c way.\n\n2 We call these tests “bimodal” or “multimodal” because they can be run with both real and fake DOCs.\n\nwww.it-ebooks.info\n\n687\n\nDependency Lookup",
      "content_length": 2598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 751,
      "content": "688\n\nDependency Lookup\n\nChapter 26 Design-for-Testability Patterns\n\nThe well-known intermediary may be called a “Service Locator,” “Object Factory,” “Component Broker,” or “Component Registry.” While these names imply different semantics (new versus existing objects), this need not be the case. For performance reasons, we may choose to return new objects from a “Service Locator” or “previously enjoyed” objects from an Object Factory. To simplify this discussion, the term “Component Broker” is used here.\n\nImplementation Notes\n\nA desire to use a Test Double when testing our code implies a need to make DOCs substitutable. This constraint rules out hard-coding the names of classes on which we depend into our code because static binding severely limits our options regard- ing how the software is conﬁ gured as it runs. One way to avoid this issue is to have the SUT delegate DOC fabrication to another object. Of course, this scheme implies we need a way to get a reference to that object. We solve this recursive problem by having a well-known object act as an intermediary between the test and the DOC. This well-known object is referenced by a hard-coded classname. To be useful for installing Test Doubles, this well-known object must supply a mechanism by which the test can specify the object to be returned.\n\nDependency Lookup has the following characteristics:\n\nEither a Singleton [GOF], a Registry [PEAA], or some kind of Thread-\n\nSpeciﬁ c Storage [POSA2]\n\nAn interface that fully encapsulates which implementation we are using\n\nA built-in substitution mechanism for replacing the returned object\n\nwith a Test Double\n\nAccess via well-known global name\n\nThe Dependency Lookup mechanism returns an object that can be used directly by the client. The nature of the actual object returned determines whether it is more appropriate to call it a “Service Locator” or an “Object Factory.” Once the object is retrieved, the SUT uses it directly. During testing, the test arranges for the Dependency Lookup mechanism to return a test-speciﬁ c object.\n\nEncapsulated Implementation\n\nA major requirement of Dependency Lookup is the existence of a well-known object to which we can delegate our requests for DOCs. This well-known\n\nwww.it-ebooks.info",
      "content_length": 2251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 752,
      "content": "Dependency Lookup\n\nobject could be a Singleton, a Registry, or some kind of Thread-Speciﬁ c Storage mechanism.3\n\nThe “Component Broker” should encapsulate its implementation from the client (our SUT). That is, the interface provided by the “Component Broker” should not expose whether it is a Singleton or a Registry or whether some type of Thread-Speciﬁ c Storage mechanism is in use under the covers. In fact, the test environment may want to provide a different implementation speciﬁ cally to avoid issues caused by Singletons in tests, such as a Substitutable Singleton (see Test-Speciﬁ c Subclass on page 579).\n\nSubstitution Mechanism\n\nWhen a test wants to replace the real DOC with a Test Double, it needs a way to tell the “Component Broker” that a Test Double should be returned instead of the real component. The “Component Broker” may provide a conﬁ guration interface to conﬁ gure it with the object to be returned or the test can replace the component Registry with a suitable Test-Speciﬁ c Subclass. It may also need to provide a way to restore the original or default conﬁ guration of the broker so that the conﬁ guration used in one test does not “leak” into another test, effec- tively changing the “Component Broker” into a Shared Fixture (page 317).\n\nA less desirable conﬁ guration alternative is to have the “Component Broker” read the classnames to be constructed for each request from a conﬁ guration ﬁ le. This approach poses several problems, however. First, the test must write the ﬁ le as part of ﬁ xture setup unless the test offers a way to replace the ﬁ le access mechanism. This is sure to result in Slow Tests (page 253). Second, this scheme will not work with Conﬁ gurable Test Doubles (page 558) unless the conﬁ gura- tion ﬁ le can also provide initialization data for the object. Finally, the need to write a ﬁ le opens the door to Interacting Tests (see Erratic Test on page 228) because different tests may need different conﬁ guration information.\n\nIf the “Component Broker” must return objects based on conﬁ guration data, a better solution is to have a separate Humble Object (page 695) read the ﬁ le and call a conﬁ guration interface on the “Component Broker.” The test can then use this same interface to conﬁ gure the broker on a per-test basis.\n\n3 The main difference is that a Singleton has only a single instance, whereas a Registry makes no such promise. Thread-Speciﬁ c Storage allows objects to access “global” data via a well-known object, where the data accessed is speciﬁ c to a particular thread; the same object might retrieve different data depending on which thread is being run.\n\nwww.it-ebooks.info\n\n689\n\nDependency Lookup",
      "content_length": 2678,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 753,
      "content": "690\n\nDependency Lookup\n\nChapter 26 Design-for-Testability Patterns\n\nMotivating Example\n\nThe following test cannot be made to pass “as is”:\n\npublic void testDisplayCurrentTime_AtMidnight() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result); }\n\nThis test almost always fails because it assumes that the current time will be returned to the SUT by a DOC. The test cannot control which values are returned by that component (the DefaultTimeProvider), however, so this test will pass only when the system time is exactly midnight.\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar currentTime; try { currentTime = new DefaultTimeProvider().getTime(); } catch (Exception e) { return e.getMessage(); } // etc. }\n\nBecause the SUT is hard-coded to use a particular class to retrieve the time, we cannot replace the DOC with a Test Double. That makes this test nondeter- ministic and pretty much useless. We need to ﬁ nd a way to gain control over the indirect inputs of the SUT.\n\nRefactoring Notes\n\nThe ﬁ rst step to making this behavior testable is to replace the hard-coded classname with a call to a “Service Locator”:\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar currentTime; try { TimeProvider timeProvider = (TimeProvider) ServiceLocator.getInstance(). ﬁndService(\"Time\"); currentTime = timeProvider.getTime(); } catch (Exception e) { return e.getMessage(); } // etc.\n\nwww.it-ebooks.info",
      "content_length": 1617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 754,
      "content": "Dependency Lookup\n\nAlthough we could have provided a class method to avoid the chained method calls, that step would just move the getInstance into the class method. The next refactoring step depends on whether we have a conﬁ guration interface on our “Service Locator.” If it makes sense to conﬁ gure the production version of the “Service Locator,” we can introduce the conﬁ guration mechanism directly into it (as illustrated in the next example). Otherwise, we can simply override what the Service Locator returns in a Test-Speciﬁ c Subclass (as illustrated in the sec- ond example).\n\nExample: Conﬁ gurable Registry\n\nThis version of the test has been modiﬁ ed to use the conﬁ guration interface on the “Service Locator” to install a Test Double:\n\npublic void testDisplayCurrentTime_AtMidnight_CSL() { // Fixture setup // Test Double conﬁguration MidnightTimeProvider tpStub = new MidnightTimeProvider(); // Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Test Double installation ServiceLocator.getInstance().registerServiceForName(tpStub, \"Time\"); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nThe code in the SUT was described previously. The code for the Conﬁ guration Interface (see Conﬁ gurable Test Double) of the Conﬁ gurable Registry follows:\n\npublic class ServiceLocator { protected ServiceLocator() {};\n\nprotected static ServiceLocator soleInstance = null;\n\npublic static ServiceLocator getInstance() { if (soleInstance==null) soleInstance = new ServiceLocator(); return soleInstance; }\n\nprivate HashMap providers = new HashMap();\n\npublic ServiceProvider ﬁndService(String serviceName) { return (ServiceProvider) providers.get(serviceName);\n\nwww.it-ebooks.info\n\n691\n\nDependency Lookup",
      "content_length": 1879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 755,
      "content": "692\n\nDependency Lookup\n\nChapter 26 Design-for-Testability Patterns\n\n}\n\n// conﬁguration interface public void registerServiceForName( ServiceProvider provider, String serviceName) { providers.put( serviceName, provider); } }\n\nThe interesting thing about this example is our use of a Conﬁ guration Interface on a production class rather than a Test Double. In fact, the Conﬁ gurable Registry avoids the need to use a Test Double by providing the test with a mechanism to alter the service component the Conﬁ gurable Registry returns.\n\nExample: Substituted Singleton\n\nThis version of the test deals with a nonconﬁ gurable Dependency Lookup mechanism by replacing the soleInstance of the “Service Locator” with a Sub- stituted Singleton (see Test-Speciﬁ c Subclass). To ensure the reusability of the conﬁ guration interface of the Substituted Singleton, we pass the TimeProvider Test Stub (page 529) as an argument to overrideSoleInstance.\n\npublic void testDisplayCurrentTime_AtMidnight_TSS() { // Fixture setup // Test Double conﬁguration MidnightTimeProvider tpStub = new MidnightTimeProvider();\n\n// Instantiate SUT TimeDisplay sut = new TimeDisplay(); // Test Double installation // Replaces the entire Service Locator with one that // always returns our Test Stub ServiceLocatorTestSingleton.overrideSoleInstance(tpStub); // Exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // Verify outcome String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals(\"Midnight\", expectedTimeString, result); }\n\nNote how the test overrides the object normally returned by getInstance with an instance of a Test-Speciﬁ c Subclass. The code for the Singleton follows:\n\npublic class ServiceLocator { protected ServiceLocator() {};\n\nprotected static ServiceLocator soleInstance = null;\n\nwww.it-ebooks.info",
      "content_length": 1830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 756,
      "content": "Dependency Lookup\n\npublic static ServiceLocator getInstance() { if (soleInstance==null) soleInstance = new ServiceLocator(); return soleInstance; }\n\nprivate HashMap providers = new HashMap();\n\npublic ServiceProvider ﬁndService(String serviceName) { return (ServiceProvider) providers.get(serviceName); } }\n\nNote that we had to make the constructor and soleInstance protected rather than private to allow them to be overridden by the subclass. Finally, here is the code for the Substituted Singleton:\n\npublic class ServiceLocatorTestSingleton extends ServiceLocator { private ServiceProvider tpStub;\n\nprivate ServiceLocatorTestSingleton(TimeProvider newTpStub) { this.tpStub = newTpStub; };\n\n// Installation interface static ServiceLocatorTestSingleton overrideSoleInstance(TimeProvider tpStub) { // We could save the real instance before reassigning // soleInstance so we could restore it later, but we'll // forego that complexity for this example soleInstance = new ServiceLocatorTestSingleton( tpStub); return (ServiceLocatorTestSingleton) soleInstance; }\n\n// Overridden superclass method public ServiceProvider ﬁndService(String serviceName) { return tpStub; // Hard-coded; ignores serviceName } }\n\nBecause it cannot see the private HashMap of providers, this code simply returns the contents of the tpStub ﬁ eld that it initialized in the constructor.\n\nAbout the Name\n\nChoosing a name for this pattern was tough. Service Locator and Component Broker were already in widespread use. Both are good names for use in their particular circumstance. Unfortunately, neither name can encompass the other, so I had to come up with another name that uniﬁ ed the two major variants.\n\nwww.it-ebooks.info\n\n693\n\nDependency Lookup",
      "content_length": 1720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 757,
      "content": "694\n\nDependency Lookup\n\nChapter 26 Design-for-Testability Patterns\n\nThe name Dependency Injection was already in widespread use for the alter- nate pattern; a desire for consistency with that name led to using Dependency Lookup. See the sidebar “What’s in a (Pattern) Name?” on page 576 for more on this decision-making process.\n\nwww.it-ebooks.info",
      "content_length": 348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 758,
      "content": "Humble Object\n\nHumble Object\n\nHow can we make code testable when it is too closely coupled to its environment?\n\nWe extract the logic into a separate, easy-to-test component that is decoupled from its environment.\n\nFixture Fixture\n\nHumble Humble Object Object\n\nImpossible Impossible Dependency Dependency\n\nSetup Setup\n\nExercise Exercise\n\nTestable Testable\n\nVerify Verify\n\nComponent Component\n\nTeardown Teardown\n\nWe are often faced with trying to test software that is closely coupled to some kind of framework. Examples include visual components (e.g., widgets, dialogs) and transactional component plug-ins. Testing these objects is difﬁ cult because constructing all the objects with which our SUT needs to interact may be expensive—or even impossible. In other cases, objects may be hard to test because they run asynchronously; examples include active objects (e.g., threads, processes, Web servers) and user interfaces. These objects’ asynchronicity intro- duces uncertainty, a requirement for interprocess coordination, and the need for delays into tests. Faced with these thorny issues, developers often just give up on testing this kind of code. The result: Production Bugs (page 268) caused by Untested Code and Untested Requirements.\n\nHumble Object is a way to bring the logic of these hard-to-instantiate objects\n\nunder test in a cost-effective manner.\n\nwww.it-ebooks.info\n\n695\n\nHumble Object",
      "content_length": 1402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 759,
      "content": "696\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nHow It Works\n\nWe extract all the logic from the hard-to-test component into a component that is testable via synchronous tests. This component implements a service interface consisting of methods that expose the logic of the untestable component; the only difference is that these methods are accessible via synchronous method calls. As a result, the Humble Object component becomes a very thin adapter layer that contains very little code. Each time the framework calls the Humble Object, this object delegates its responsibilities to the testable component. If the testable component needs any information from the context, the Humble Object is responsible for retrieving it and passing it to the testable component. The Humble Object code is typically so simple that we often don’t bother writing tests for it because it can be quite difﬁ cult to set up the environment needed to run those tests.\n\nWhen to Use It\n\nWe can and should introduce a Humble Object whenever we have nontrivial logic in a component that is problematic to instantiate because it depends on a framework or can be accessed only asynchronously. There are lots of reasons for objects being hard to test; consequently, there are lots of variations in how we break the dependencies that are required. The following variations are the most common examples of Humble Object—but we shouldn’t be surprised if we sometimes need to invent our own variation.\n\nVariation: Humble Dialog\n\nGraphical user interface (GUI) frameworks require us to provide objects to represent our pages and controls. These objects provide logic to translate user actions into the underlying system actions and to translate the system responses back into user recognizable behavior. This logic may involve invoking the application behind the user interface and/or modifying the state of this or other visual objects.\n\nVisual objects are very difﬁ cult to test efﬁ ciently because they are tightly coupled to the presentation framework that invokes them. To be effective, a test would need to simulate that environment to provide the visual object with all the information and facilities it requires. Further complicating the issue is the fact that these frameworks often run in their own thread of control, which means that we must use asynchronous tests. These tests are challenging to write, and they often result in Slow Tests (page 253) and Nondeterministic Tests (see Erratic Test on page 228). Under these circumstances, we may beneﬁ t by using\n\nwww.it-ebooks.info",
      "content_length": 2575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 760,
      "content": "Humble Object\n\na Humble Object to move all of the controller and view-updating logic out of the framework-dependent object and into a testable object.\n\nVariation: Humble Executable\n\nMany programs contain active objects. Active objects have their own thread of execution so they can do things in parallel with other activities of the system. Examples of active objects include anything that runs in a separate process (e.g., Windows applications in .exe ﬁ les) or thread (in Java, any object that imple- ments Runnable). These objects may be launched directly by the client, or they may be started automatically, process requests from a queue, and send replies via a return message. Either way, we must write asynchronous tests (complete with interprocess coordination and/or explicit delays and Neverfail Tests; see Production Bugs) to verify their behavior.\n\nThe Humble Executable pattern provides a way to bring the logic of the exe- cutable under test without incurring the delays that might otherwise lead to Slow Tests and Nondeterministic Tests. We extract all the logic from the executable into a component that is testable via synchronous tests. This component implements a service interface consisting of methods that expose all logic of the executable; the only difference is that these methods are accessible via synchronous method calls. The testable component may be a Windows DLL, a Java JAR containing a Service Facade [CJ2EEP] class, or some other language component or class that exposes the services of the executable in a testable way.\n\nThe Humble Executable component itself contains very little code. All it does in its thread of control is to load the testable component (if a True Hum- ble Object) and delegate to it. As a result, the Humble Executable requires only one or two tests to verify that it performs this load/delegate function correctly. Although these tests still take seconds to execute, they have a much smaller impact on the overall test suite execution time because so few of them exist. Given that this code will not change very often, these tests can even be omitted from the suite of tests that developers execute before check-in to speed up test suite execution times. Of course, we would still prefer to run the Humble Executable tests as part of the automated build process.\n\nVariation: Humble Transaction Controller\n\nMany applications use databases to persist their state. Fixture setup with databases can be slow and complex, and leftover ﬁ xtures can wreak havoc with subsequent tests and test runs. If we are using a Shared Fixture (page 317), the ﬁ xture’s persis- tence may lead to Erratic Tests. Humble Transaction Controller facilitates testing of the logic that runs within the transaction by making it possible for the test to control\n\nwww.it-ebooks.info\n\n697\n\nHumble Object",
      "content_length": 2830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 761,
      "content": "698\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nthe transaction. As a consequence, we can exercise the logic, verify the outcome, and then abort the transaction, leaving no trace of our activity in the database.\n\nTo implement Humble Transaction Controller, we use an Extract Method [Fowler] refactoring to move all the logic we want to test out of the code that controls the transaction and into a separate method that knows nothing about transaction control and that can be called by the test. Because the caller con- trols the transaction, the test can start, commit (if it so chooses), and (most commonly) roll back the transaction. In this case, the behavior—not the dependencies—causes us to bypass the Humble Object when we are testing the business logic. As a result, we are more likely to be able to get away with a Poor Man’s Humble Object.\n\nAs for the Humble Object, it contains no business logic. Thus the only behavior that needs to be tested is whether the Humble Object commits and rolls back the transaction properly based on the outcome of the methods it calls. We can write a test that replaces the testable component with a Test Stub (page 529) that throws an exception and then verify that this activity results in a rollback of the transac- tion. If we are using a Poor Man’s Humble Object, the stub would be implemented as a Subclassed Test Double (see Test-Speciﬁ c Subclass on page 579) that overrides the “real” methods with methods that throw exceptions.\n\nMany of the major application server technologies support this pattern either directly or indirectly by taking transaction control away from the business objects that we write. If we are building our software without using a transaction control framework, we may need to implement our own Humble Transaction Controller. See the “Implementation Notes” section for some ideas on how we can enforce the separation.\n\nVariation: Humble Container Adapter\n\nSpeaking of “containers,” we often have to implement speciﬁ c interfaces to allow our objects to run inside an application server (e.g., the “EJB session bean” interface). Another variation on the Humble Object pattern is to design our objects to be container-independent and then have a Humble Container Adapter adapt them to the interface required by container. This strategy makes our logic components easy to test outside the container, which dramatically reduces the time required for an “edit–compile–test” cycle.\n\nImplementation Notes\n\nWe can make the logic that normally runs inside the Humble Object testable in several different ways. All of these techniques share one commonality: They in- volve exposing the logic so that it can be veriﬁ ed using synchronous tests. They\n\nwww.it-ebooks.info",
      "content_length": 2749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 762,
      "content": "Humble Object\n\nvary, however, in terms of how the logic is exposed. Regardless of how logic ex- posure occurs, test-driven purists would prefer that tests verify that the Humble Object is calling the extracted logic properly. This can be done by replacing the real logic methods with some kind of Test Double (page 522) implementation.\n\nVariation: Poor Man’s Humble Object\n\nThe simplest way to isolate and expose each piece of logic we want to verify is to place it into a separate method. We can do so by using an Extract Method refactoring on in-line logic and then making the resulting method visible from the test. Of course, this method cannot require anything from the context. Ideally everything the method needs to do its work will be passed in as arguments but this information could also be placed in ﬁ elds. Problems may arise if the testable com- ponent needs to call methods to access information it needs and those methods are dependent on the (nonexistent/faked) context, as this dependency makes writing the tests more complex.\n\nThis approach, which constitutes the “poor man’s” Humble Object, works well if no obstacles prevent the instantiation of the Humble Object (e.g., automatically starting its thread, no public constructor, unsatisﬁ able dependencies). Use of a Test- Speciﬁ c Subclass can also help break these dependencies by providing a test-friendly constructor and exposing private methods to the test.\n\nWhen testing a Subclassed Humble Object or a Poor Man’s Humble Object, we can build the Test Spy (page 538) as a Subclassed Test Double of the Humble Object to record when the methods in question were called. We can then use assertions within the Test Method (page 348) to verify that the values recorded match the values expected.\n\nVariation: True Humble Object\n\nAt the other extreme, we can put the logic we want to test into a separate class and have the Humble Object delegate to an instance of it. This approach, which was implied in the introduction to this pattern, will work in almost any circum- stance where we have complete control over the code.\n\nSometimes the host framework requires that its objects hold certain responsi- bilities that we cannot move elsewhere. For example, a GUI framework expects its view objects to contain data for the controls of the GUI and the data that those controls display on the screen. In these cases we must either give the test- able object a reference to the Humble Object and have it manipulate the data for that object or put some minimal update logic in the Humble Object and accept that it won’t be covered by automated tests. The former approach is almost always possible and is always preferable.\n\nwww.it-ebooks.info\n\n699\n\nHumble Object",
      "content_length": 2724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 763,
      "content": "700\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nTo refactor to a True Humble Object, we normally do a series of Extract Method refactorings to decouple the public interface of the Humble Object from the implementation logic we plan to delegate. Then we do an Extract Class [Fowler] refactoring to move all the methods—except the ones that deﬁ ne the public interface of the Humble Object—to the new “testable” class. We introduce an attribute (a ﬁ eld) to hold a reference to an instance of the new class and initial- ize it to an instance of the new class either as part of the constructor or using Lazy Initialization [SBPP] in each interface method.\n\nWhen testing a True Humble Object (where the Humble Object delegates to a separate class), we typically use a Lazy Mock Object (see Mock Object on page 544) or Test Spy to verify that the extracted class is called correctly. By contrast, using the more common Active Mock Object (see Mock Object) is problematic in this situation because the assertions are made on a different thread from the Testcase Object (page 382) and failures won’t be detected unless we ﬁ nd a way to channel them back to the test thread.\n\nTo ensure that the extracted testable component is instantiated properly, we can use an observable Object Factory (see Dependency Lookup on page 686) to construct the extracted component. The test can register as a listener to verify the correct method is called on the factory. We can also use a regular factory object and replace it during the test with a Mock Object or Test Stub to monitor which factory method was called.\n\nVariation: Subclassed Humble Object\n\nIn between the extremes of the Poor Man’s Humble Object and the True Humble Object are approaches that involve clever use of subclassing to put the logic into separate classes while still allowing them to be on a single object. A number of different ways to do this are possible, depending on whether the Humble Object class needs to subclass a speciﬁ c framework class. I won’t go into a lot of detail here as this technique is very speciﬁ c to the language and runtime environment. Nevertheless, you should recognize that the basic options are either having the framework-dependent class inherit the logic to be tested from a superclass or having the class delegate to an abstract method that is implemented by a subclass.\n\nMotivating Example (Humble Executable)\n\nIn this example, we are testing some logic that runs in its own thread and processes each request as it arrives. In each test, we start up the thread, send it some messages, and wait long enough so that our assertions pass. Unfortu- nately, it takes several seconds for the thread to start up, become initialized,\n\nwww.it-ebooks.info",
      "content_length": 2748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 764,
      "content": "Humble Object\n\nand process the ﬁ rst request. Thus the test fails sporadically unless we include a two-second delay after starting the thread.\n\npublic class RequestHandlerThreadTest extends TestCase { private static ﬁnal int TWO_SECONDS = 3000;\n\npublic void testWasInitialized_Async() throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); // Exercise sut.start(); // Verify Thread.sleep(TWO_SECONDS); assertTrue(sut.initializedSuccessfully()); }\n\npublic void testHandleOneRequest_Async() throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); sut.start(); // Exercise enqueRequest(makeSimpleRequest()); // Verify Thread.sleep(TWO_SECONDS); assertEquals(1, sut.getNumberOfRequestsCompleted()); assertResponseEquals(makeSimpleResponse(), getResponse()); } }\n\nIdeally, we would like to test the thread with each kind of transaction individu- ally to achieve better Defect Localization (see page 22). Unfortunately, if we did so our test suite would take many minutes to run because each test includes a delay of several seconds. Another problem is that the tests won’t result in an error if our active object has an exception in its own thread.\n\nA two-second delay may not seem like a big deal, but consider what happens when we have a dozen such tests. It would take us almost half a minute to run these tests. Contrast this performance with that of normal tests—we can run several hundred of those tests each second. Testing via the executable is affecting our productivity negatively. For the record, here’s the code for the executable:\n\npublic class RequestHandlerThread extends Thread { private boolean _initializationCompleted = false; private int _numberOfRequests = 0;\n\npublic void run() { initializeThread(); processRequestsForever(); }\n\nwww.it-ebooks.info\n\n701\n\nHumble Object",
      "content_length": 1861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 765,
      "content": "702\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\npublic boolean initializedSuccessfully() { return _initializationCompleted; }\n\nvoid processRequestsForever() { Request request = nextMessage(); do { Response response = processOneRequest(request); if (response != null) { putMsgOntoOutputQueue(response); } request = nextMessage(); } while (request != null); } }\n\nTo avoid the distraction of the business logic, I have already used an Extract Method refactoring to move the real logic into the method processOneRequest. Likewise, the actual initialization logic is not shown here; sufﬁ ce it to say that this logic sets the variable _initializationCompleted when it ﬁ nishes successfully.\n\nRefactoring Notes\n\nTo create a Poor Man’s Humble Object, we expose the methods to make them visible from the test. (If the code used in-line logic, we would do an Extract Method refactoring ﬁ rst.) If there were any dependencies on the context, we would need to do an Introduce Parameter [JBrains] refactoring or an Introduce Field [JetBrains] refactoring so that the processOneRequest method need not access anything from the context.\n\nTo create a true Humble Object, we can do an Extract Class refactoring on the executable to create the testable component, leaving behind just the Humble Object as an empty shell. This step typically involves doing the Extract Method refactoring described above to separate the logic we want to test (e.g., the initializeThread method and the processOneRequest method) from the logic that interacts with the context of the executable. We then do an Extract Class refactoring to introduce the testable component class (essentially a single Strategy [GOF] object) and move all methods except the public interface methods over to it. The Extract Class refac- toring includes introducing a ﬁ eld to hold a reference to the new object and creating an instance. It also includes ﬁ xing all of the public methods so that they call the methods that were moved to the new testable class.\n\nwww.it-ebooks.info",
      "content_length": 2038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 766,
      "content": "Humble Object\n\nExample: Poor Man’s Humble Executable\n\nHere is the same set of tests rewritten as a Poor Man’s Humble Object:\n\npublic void testWasInitialized_Sync() throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); // Exercise sut.initializeThread(); // Verify assertTrue(sut.initializedSuccessfully()); }\n\npublic void testHandleOneRequest_Sync() throws InterruptedException { // Setup RequestHandlerThread sut = new RequestHandlerThread(); // Exercise Response response = sut.processOneRequest(makeSimpleRequest()); // Verify assertEquals(1, sut.getNumberOfRequestsCompleted()); assertResponseEquals(makeSimpleResponse(), response); }\n\nHere, we have made the methods initializeThread and processOneRequest public so that we can call them synchronously from the test. Note the absence of a delay in this test. This approach works well as long as we can instantiate the executable component easily.\n\nExample: True Humble Executable\n\nHere is the code for our SUT refactored to use a True Humble Executable:\n\npublic class HumbleRequestHandlerThread extends Thread implements Runnable { public RequestHandler requestHandler;\n\npublic HumbleRequestHandlerThread() { super(); requestHandler = new RequestHandlerImpl(); }\n\npublic void run() { requestHandler.initializeThread(); processRequestsForever(); }\n\npublic boolean initializedSuccessfully() {\n\nwww.it-ebooks.info\n\n703\n\nHumble Object",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 767,
      "content": "704\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nreturn requestHandler.initializedSuccessfully(); }\n\npublic void processRequestsForever() { Request request = nextMessage(); do { Response response = requestHandler.processOneRequest(request); if (response != null) { putMsgOntoOutputQueue(response); } request = nextMessage(); } while (request != null); }\n\nHere, we have moved the method processOneRequest to a separate class that we can instantiate easily. Below is the same test rewritten to take advantage of the extracted component. Note the absence of a delay in this test.\n\npublic void testNotInitialized_Sync() throws InterruptedException { // Setup/Exercise RequestHandler sut = new RequestHandlerImpl(); // Verify assertFalse(\"init\", sut.initializedSuccessfully()); }\n\npublic void testWasInitialized_Sync() throws InterruptedException { // Setup RequestHandler sut = new RequestHandlerImpl(); // Exercise sut.initializeThread(); // Verify assertTrue(\"init\", sut.initializedSuccessfully()); }\n\npublic void testHandleOneRequest_Sync() throws InterruptedException { // Setup RequestHandler sut = new RequestHandlerImpl(); // Exercise Response response = sut.processOneRequest( makeSimpleRequest() ); // Verify assertEquals( 1, sut.getNumberOfRequestsDone()); assertResponseEquals( makeSimpleResponse(), response); }\n\nBecause we have introduced delegation to another object, we should probably verify that the delegation occurs properly. The next test veriﬁ es that the Humble\n\nwww.it-ebooks.info",
      "content_length": 1514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 768,
      "content": "Humble Object\n\nObject calls the initializeThread method and the processOneRequest method on the newly created testable component:\n\npublic void testLogicCalled_Sync() throws InterruptedException { // Setup RequestHandlerRecordingStub mockHandler = new RequestHandlerRecordingStub(); HumbleRequestHandlerThread sut = new HumbleRequestHandlerThread(); // Mock Installation sut.setHandler( mockHandler ); sut.start(); // Exercise enqueRequest(makeSimpleRequest()); // Verify Thread.sleep(TWO_SECONDS); assertTrue(\"init\", mockHandler.initializedSuccessfully() ); assertEquals( 1, mockHandler.getNumberOfRequestsDone() ); }\n\nNote that this test does require at least a small delay to allow the thread to start up. The delay is shorter, however, because we have replaced the real logic component with a Test Double that responds instantly and only one test now requires the delay. We could even move this test to a separate test suite that is run less frequently (e.g., only during the automated build process) to ensure that all tests performed before each check-in run quickly.\n\nThe other signiﬁ cant thing to note is that we are using a Test Spy rather than a Mock Object. Because the assertions done by the Mock Object would be raised in a different thread from the Test Method, the Test Automation Frame- work (page 298)—in this example, JUnit—won’t catch them. As a consequence, the test might indicate “pass” even though assertions in the Mock Object are failing. By making the assertions in the Test Method, we avoid having to do something special to relay the exceptions thrown by the Mock Object back to the thread in which the Test Method is executing.\n\nThe preceding test veriﬁ ed that our Humble Object actually delegates to the Test Spy that we have installed. It would also be a good idea to verify that our Humble Object actually initializes the variable holding the delegate to the appropriate class. Here’s a simple way to do so:\n\npublic void testConstructor() { // Exercise HumbleRequestHandlerThread sut = new HumbleRequestHandlerThread(); // Verify String actualDelegateClass = sut.requestHandler.getClass().getName(); assertEquals( RequestHandlerImpl.class.getName(), actualDelegateClass); }\n\nwww.it-ebooks.info\n\n705\n\nHumble Object",
      "content_length": 2246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 769,
      "content": "706\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nThis Constructor Test (see Test Method) veriﬁ es that a speciﬁ c attribute has been initialized.\n\nExample: Humble Dialog\n\nMany development environments let us build the user interface visually by dragging and dropping various objects (“widgets”) onto a canvas. They let us add behavior to these visual objects by selecting one of several possible actions or events speciﬁ c to that visual object and typing logic into the code window presented by the IDE. This logic may involve invoking the application behind the user interface or it may involve modifying the state of this or some other visual object.\n\nVisual objects are very difﬁ cult to test efﬁ ciently because they are tightly coupled to the presentation framework that invokes them. To provide the visual object with all the information and facilities it requires, the test would need to simulate that environment—quite a challenge. This makes testing very complicated, so much so that many development teams don’t bother testing the presentation logic at all. This lack of testing, not surprisingly, often leads to Production Bugs caused by untested code and Untested Requirements.\n\nTo create the Humble Dialog, we extract all the logic from the view com- ponent into a nonvisual component that is testable via synchronous tests. If this component needs to update the view object’s (Humble Dialog’s) state, the Humble Dialog is passed in as an argument. When testing the nonvisual com- ponent, we typically replace the Humble Dialog with a Mock Object that is conﬁ gured with the indirect input values and the expected behavior (indirect outputs). In GUI frameworks that require the Humble Dialog to register itself with the framework for each event it wishes to see, the nonvisual component can register itself instead of the Humble Dialog (as long as that doesn’t introduce unmanageable dependencies on the context). This ﬂ exibility makes the Humble Dialog even simpler because the events go directly to the nonvisual component and require no delegation logic.\n\nThe following code sample is taken from a VB view component (.ctl) that includes some nontrivial logic. It is part of a custom plug-in we built for Mercury Interactive’s TestDirector tool.\n\n' Interface method, TestDirector will call this method ' to display the results. Public Sub ShowResultEx(TestSetKey As TdTestSetKey, _ TSTestKey As TdTestKey, _ ResultKey As TdResultKey)\n\nwww.it-ebooks.info",
      "content_length": 2485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 770,
      "content": "Humble Object\n\nDim RpbFiles As OcsRpbFiles Set RpbFiles = getTestResultFileNames(ResultKey) ResultsFileName = RpbFiles.ActualResultFileName ShowFileInBrowser ResultsFileName End Sub\n\nFunction getTestResultFileNames(ResultKey As Variant) As OcsRpbFiles On Error GoTo Error Dim Attachments As Collection Dim thisTest As Run Dim RpbFiles As New OcsRpbFiles\n\nCall EnsureConnectedToTd\n\nSet Attachments = testManager.GetAllAttachmentsOfRunTest(ResultKey) Call RpbFiles.LoadFromCollection(Attachments, \"RunTest\") Set getTestResultFileNames = RpbFiles Exit Function Error: ' do something ... End Function\n\nIdeally, we would like to test the logic. Unfortunately, we cannot construct the objects passed in as parameters because they don’t have public constructors. Passing in objects of some other type isn’t possible either, because the types of the function parameters are hard-coded to be speciﬁ c concrete classes.\n\nWe can do an Extract Testable Component (page 735) refactoring on the ex- ecutable to create the testable component, leaving behind just the Humble Dialog as an empty shell. This approach typically involves doing several Extract Method refactorings (already done in the original example to make the refactoring easier to understand), one for each chunk of logic that we want to move. We then do an Extract Class refactoring to create our new testable component class. The Extract Class refactoring may include both Move Method [Fowler] and Move Field [Fowler] refactorings to move the logic and the data it requires out of the Humble Dialog and into the new testable component. Here’s the same view converted to a Humble Dialog:\n\n' Interface method, TestDirector will call this method ' to display the results. Public Sub ShowResultEx(TestSetKey As TdTestSetKey, _ TSTestKey As TdTestKey, _ ResultKey As TdResultKey) Dim RpbFiles As OcsRpbFiles Call EnsureImplExists Set RpbFiles = Implementation.getTestResultFileNames(ResultKey)\n\nwww.it-ebooks.info\n\n707\n\nHumble Object",
      "content_length": 1981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 771,
      "content": "708\n\nHumble Object\n\nChapter 26 Design-for-Testability Patterns\n\nResultsFileName = RpbFiles.ActualResultFileName ShowFileInBrowser ResultsFileName End Sub\n\nPrivate Sub EnsureImplExists() If Implementation Is Nothing Then Set Implementation = New OcsScriptViewerImpl End If End Sub\n\nHere’s the testable component OcsScriptViewerImpl that the Humble Object calls:\n\n' ResultViewer Implementation: Public Function getTestResultFileNames(ResultKey As Variant) As OcsRpbFiles On Error GoTo Error\n\nDim Attachments As Collection Dim thisTest As Run Dim RpbFiles As New OcsRpbFiles\n\nCall EnsureConnectedToTd\n\nSet Attachments = testManager.GetAllAttachmentsOfRunTest(ResultKey) Call RpbFiles.LoadFromCollection(Attachments, \"RunTest\") Set getTestResultFileNames = RpbFiles Exit Function Error: ' do something ... End Function\n\nWe could now instantiate this OcsScriptViewerImpl class easily and write VbUnit tests for it. I’ve omitted the tests for space reasons because they don’t really show anything particularly interesting.\n\nExample: Humble Transaction Controller\n\nTransaction Rollback Teardown (page 668) contains an example of writing tests that bypass the Humble Transaction Controller.\n\nFurther Reading\n\nSee http://www.objectmentor.com/resources/articles/TheHumbleDialogBox.pdf for Michael Feathers’ original write-up of the Humble Dialog pattern.\n\nwww.it-ebooks.info",
      "content_length": 1364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 772,
      "content": "Test Hook\n\nTest Hook\n\nHow do we design the SUT so that we can replace its dependencies at runtime?\n\nWe modify the SUT to behave differently during the test.\n\nSUT SUT\n\nDOC DOC\n\nSetup Setup\n\nExercise Exercise\n\nExercise Exercise\n\nNo No\n\nIf If Testing? Testing?\n\nYes Yes\n\nUsage Usage\n\nNo No\n\nIf If Testing? Testing?\n\nYes Yes\n\nVerify Verify\n\nProduction Production Logic Logic\n\nTest- Test- Specific Specific Logic Logic\n\nProduction Production Logic Logic\n\nTest- Test- Specific Specific Logic Logic\n\nTeardown Teardown\n\nAlmost every piece of code depends on some other classes, objects, modules, or procedures. To unit-test a piece of code properly, we would like to isolate it from its dependencies. Such isolation is difﬁ cult to achieve if those dependencies are hard-coded within the code in the form of literal classnames.\n\nTest Hook is a “method of last resort” for introducing test-speciﬁ c behavior\n\nduring automated testing.\n\nHow It Works\n\nWe modify the behavior of the SUT to support testing by putting a hook directly into the SUT or into a DOC. This approach implies that we use some kind of testing ﬂ ag that can be checked in the appropriate place.\n\nWhen to Use It\n\nSometimes it is appropriate to use this “pattern of last resort” when we cannot use either Dependency Injection (page 678) or Dependency Lookup (page 686). In this situation, we use a Test Hook because we have no other way to address the Untested Code (see Production Bugs on page 268) caused by a Hard-Coded Dependency (see Hard-to-Test Code on page 209).\n\nwww.it-ebooks.info\n\n709\n\nTest Hook",
      "content_length": 1564,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 773,
      "content": "710\n\nTest Hook\n\nChapter 26 Design-for-Testability Patterns\n\nA Test Hook may be the only way to introduce Test Double (page 522) behavior when we are programming in a procedural language that does not support objects, function pointers, or any other form of dynamic binding.\n\nTest Hooks can be used as a transition strategy to bring legacy code under the testing umbrella. We can introduce testability using the Test Hooks and then use those Tests as Safety Net (see page 24) while we refactor for even more test- ability. At some point we should be able to discard the initial round of tests that required the Test Hooks because we have enough “modern” tests to protect us.\n\nImplementation Notes\n\nThe essence of the Test Hook pattern is that we insert some code into the SUT that lets us test it. Regardless of how we insert this code into the SUT, the code itself can either\n\nDivert control to a Test Double instead of the real object, or\n\nBe the Test Double within the real object, or\n\nBe a test-speciﬁ c Decorator [GOF] that delegates to the real object\n\nwhen in production.\n\nThe ﬂ ag that indicates testing is in progress can be a compile-time constant, which may, for example, cause the compiler to optimize out all the testing logic. In languages that support preprocessors or compiler macros, such constructs may also be used to remove the Test Hook before the code enters the production phase. The value of the ﬂ ag can also be read in from conﬁ guration data or stored in a global variable that the test sets directly.\n\nMotivating Example\n\nThe following test cannot be made to pass “as is”:\n\npublic void testDisplayCurrentTime_AtMidnight() { // ﬁxture setup TimeDisplay sut = new TimeDisplay(); // exercise SUT String result = sut.getCurrentTimeAsHtmlFragment(); // verify direct output String expectedTimeString = \"<span class=\\\"tinyBoldText\\\">Midnight</span>\"; assertEquals( expectedTimeString, result); }\n\nThis test almost always fails because it depends on a DOC to return the current time to the SUT. The test cannot control the values returned by that component,\n\nwww.it-ebooks.info",
      "content_length": 2097,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 774,
      "content": "Test Hook\n\nthe DefaultTimeProvider. As a consequence, this test will pass only when the system time is exactly midnight.\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar currentTime; try { currentTime = new DefaultTimeProvider().getTime(); } catch (Exception e) { return e.getMessage(); } // etc. }\n\nBecause the SUT is hard-coded to use a particular class to retrieve the time, we cannot replace the DOC with a Test Double. As a result, this test is nondeter- ministic and pretty much useless. We need to ﬁ nd a way to gain control over the indirect inputs of the SUT.\n\nRefactoring Notes\n\nWe can introduce a Test Hook by creating a ﬂ ag that can be checked into the SUT. We then wrap the production code with an if/then/else control structure and put the test-speciﬁ c logic into the then clause.\n\nExample: Test Hook in System Under Test\n\nHere’s the production code modiﬁ ed to accommodate testing via a Test Hook:\n\npublic String getCurrentTimeAsHtmlFragment() { Calendar theTime; try { if (TESTING) { theTime = new GregorianCalendar(); theTime.set(Calendar.HOUR_OF_DAY, 0); theTime.set(Calendar.MINUTE, 0);} else { theTime = new DefaultTimeProvider().getTime(); } } catch (Exception e) { return e.getMessage(); } // etc.\n\nHere we have implemented the testing ﬂ ag as global constant, which we can edit as necessary. This ﬂ exibility implies a separate build step is necessary for versions of the system to be tested. Such a strategy is somewhat safer than using a dynamic conﬁ guration parameter or member variable because many compilers will optimize this hook right out of the object code.\n\nwww.it-ebooks.info\n\n711\n\nTest Hook",
      "content_length": 1636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 775,
      "content": "712\n\nTest Hook\n\nChapter 26 Design-for-Testability Patterns\n\nExample: Test Hook in Depended-on Component\n\nWe can also introduce a Test Hook by putting the hook into a DOC rather than into the SUT:\n\npublic Calendar getTime() throws TimeProviderEx { Calendar theTime = new GregorianCalendar(); if (TESTING) { theTime.set(Calendar.HOUR_OF_DAY, 0); theTime.set(Calendar.MINUTE, 0);} else { // just return the calendar } return theTime; };\n\nThis approach is somewhat better because we are not modifying the SUT as we test it.\n\nwww.it-ebooks.info",
      "content_length": 539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 776,
      "content": "Chapter 27\n\nValue Patterns\n\nPatterns in This Chapter\n\nLiteral Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 714\n\nDerived Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718\n\nGenerated Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723\n\nDummy Object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 728\n\n713\n\nwww.it-ebooks.info\n\nValue Patterns",
      "content_length": 494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 777,
      "content": "714\n\nAlso known as: Hard-Coded Value, Constant Value\n\nLiteral Value\n\nChapter 27 Value Patterns\n\nLiteral Value\n\nHow do we specify the values to be used in tests?\n\nWe use literal constants for object attributes and assertions.\n\nBigDecimal expectedTotal = new BigDecimal(\"99.95\");\n\nThe values we use for the attributes of objects in our test ﬁ xture and the expect- ed outcome of our test are often related to one another in a way that is deﬁ ned in the requirements. Getting these values—and, in particular, the relationship between the pre-conditions and the post-conditions—right is crucial because it drives the correct behavior into the SUT.\n\nLiteral Values are a popular way to specify the values of attributes of objects\n\nin a test.\n\nHow It Works\n\nWe use a literal constant of the appropriate type for each attribute of an object or for use as an argument of a method call to the SUT or an Assertion Method (page 362). The expected values are calculated by hand, calculator, or spreadsheet and hard-coded within the test as Literal Values.\n\nWhen to Use It\n\nUsing a Literal Value in-line makes it very clear which value is being used; there is no doubt about the value’s identity because it is right in front of our face. Unfortunately, using Literal Values can make it difﬁ cult to see the relationships between the values used in various places in the test, which may in turn lead to Obscure Tests (page 186). It certainly makes sense to use Literal Values if the testing requirements specify which values are to be used and we want to make it clear that we are, in fact, using those values. [We might sometimes consider us- ing a Data-Driven Test (page 288) instead to avoid the effort and transcription errors associated with copying the data into test methods.]\n\nOne downside of using a Literal Value is that we might use the same value for two unrelated attributes; if the SUT happens to use the wrong one, tests may pass even though they should not. If the Literal Value is a ﬁ lename or a key used to access a database, the meaning of the value is lost—the content of the ﬁ le or record actually drives the behavior of the SUT. Using a Literal Value as the key does nothing to help the reader understand the test in such a case, and we are likely to suffer from Obscure Tests.\n\nwww.it-ebooks.info",
      "content_length": 2307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 778,
      "content": "Literal Value\n\nIf the values in the expected outcome can be derived from the values in the ﬁ xture setup logic, we will be more likely to use the Tests as Documentation (see page 23) if we use Derived Values (page 718). Conversely, if the values are not important to the speciﬁ cation of the logic being tested, we should consider using Generated Values (page 723).\n\nImplementation Notes\n\nThe most common way to use a Literal Value is with literal constants within the code. When the same value needs to be used in several places in the test (typically during ﬁ xture setup and result veriﬁ cation), this approach can obscure the relationship between the test pre-conditions and post-conditions. Introducing an evocatively named symbolic constant can make this relationship much clearer. Likewise, if we cannot use a self-describing value, we can still make the code easier to use by deﬁ ning a suitably named symbolic constant and using it wherever we would have used the Literal Value.\n\nVariation: Symbolic Constant\n\nWhen we need to use the same Literal Value in several places in a single Test Method (page 348) or within several distinct tests, it is a good practice to use a Symbolic Constant instead of a Literal Value. A Symbolic Constant is function- ally equivalent to a Literal Value but reduces the likelihood of High Test Mainte- nance Cost (page 265).\n\nVariation: Self-Describing Value\n\nWhen several attributes of an object need the same kind of value, using different values provides advantages by helping us to prove that the SUT is working with the correct attribute. When an attribute or argument is an unconstrained string, it can be useful to choose a value that describes the role of the value in the test (a Self-Describing Value). For example, using “Not an existing customer” for the name of a customer might be more helpful to the reader than using “Joe Blow,” especially when we are debugging or when the attributes are included in the test failure output.\n\nExample: Literal Value\n\nBecause Literal Value is usually the starting point when writing tests, I’ll dis- pense with a motivating example and cut straight to the chase. Here’s an example of the Literal Value pattern in action. Note the use of Literal Values in both the ﬁ xture setup logic and the assertion.\n\nwww.it-ebooks.info\n\n715\n\nLiteral Value",
      "content_length": 2331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 779,
      "content": "716\n\nLiteral Value\n\nChapter 27 Value Patterns\n\npublic void testAddItemQuantity_1() throws Exception { Product product = new Product(\"Widget\", 19.95); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, 1); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); assertEquals(new BigDecimal(\"19.95\"), actualItem.getExtendedPrice()); }\n\nThe Product constructor requires both a name and a cost. The assertion on the extendedCost of the lineItem requires a value for the total cost of the product for that line item. In this example, we included these values as hard-coded literal constants. In the next example, we’ll use symbolic constants instead.\n\nRefactoring Notes\n\nWe can reduce the Test Code Duplication (page 213) in the form of the hard- coded Literal Value of 19.95 by doing a Replace Magic Number with Symbolic Constant [Fowler] refactoring.\n\nExample: Symbolic Constant\n\nThis refactored version of the original test replaces the duplicated Literal Value of the widget’s price (19.95) with a suitably named Symbolic Constant that is used during ﬁ xture setup as well as result veriﬁ cation:\n\npublic void testAddItemQuantity_1s() throws Exception { BigDecimal widgetPrice = new BigDecimal(\"19.95\"); Product product = new Product(\"Widget\", widgetPrice); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, 1); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); assertEquals(widgetPrice, actualItem.getExtendedPrice()); }\n\nwww.it-ebooks.info",
      "content_length": 1591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 780,
      "content": "Literal Value\n\nExample: Self-Describing Value\n\nThis refactored version of the test provides a Self-Describing Value for the mandatory name argument passed to the Product constructor. This value is not used by the method we are testing; it is merely stored for later access by another method we are not testing here.\n\npublic void testAddItemQuantity_1b() throws Exception { BigDecimal widgetPrice = new BigDecimal(\"19.95\"); Product product = new Product(\"Irrelevant product name\", widgetPrice); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, 1); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); assertEquals(widgetPrice, actualItem.getExtendedPrice()); }\n\nExample: Distinct Value\n\nThis test needs to verify that the item’s name is taken from the product’s name. We’ll use a Distinct Value for the name and the SKU so we can tell them apart.\n\npublic void testAddItemQuantity_1c() throws Exception { BigDecimal widgetPrice = new BigDecimal(\"19.95\"); String name = \"Product name\"; String sku = \"Product SKU\"; Product product = new Product(name, sku, widgetPrice); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, 1); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); assertEquals(name, actualItem.getName()); }\n\nThis also happens to be an example of a self-describing value.\n\nwww.it-ebooks.info\n\n717\n\nLiteral Value",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 781,
      "content": "718\n\nAlso known as: Calculated Value\n\nDerived Value\n\nChapter 27 Value Patterns\n\nDerived Value\n\nHow do we specify the values to be used in tests?\n\nWe use expressions to calculate values that can be derived from other values.\n\nBigDecimal expectedTotal = itemPrice.multiply(QUANTITY);\n\nThe values we use for the attributes of objects in our test ﬁ xtures and the result veriﬁ cation parts of our tests are often related to one another in a way that is deﬁ ned in the requirements. Getting these values—and, in particular, the rela- tionship between the pre-conditions and the post-conditions—right is crucial because it drives the correct behavior into the SUT and helps the tests act as documentation of our software.\n\nOften, some of these values can be derived from other values in the same test. In these cases the beneﬁ ts from using our Tests as Documentation (see page 23) are improved if we show the derivation by calculating the values using the appro- priate expression.\n\nHow It Works\n\nComputers are really good at math and string concatenation. We can avoid doing the math in our head (or with a calculator) by coding the math for expected results as arguments of the Assertion Method (page 362) calls directly into the tests. We can also use Derived Values as arguments for ﬁ xture object creation and as method arguments when exercising the SUT.\n\nDerived Values, by their very nature, encourage us to use variables or symbolic constants to hold the values. These variables/constants can be initialized at com- pile time (constants), during class or Testcase Object (page 382) initialization, during ﬁ xture setup, or within the body of the Test Method (page 348).\n\nWhen to Use It\n\nWe should use a Derived Value whenever we have values that can be derived in some deterministic way from other values in our tests. The main drawback of using Derived Values is that the same math error (e.g., rounding errors) could appear in both the SUT and the tests. To be safe, we might want to code a few of the patho- logical test cases using Literal Values (page 714) just in case such a problem might be present. If the values we are using must be unique or don’t affect the logic in the SUT, we may be better off using Generated Values (page 723) instead.\n\nwww.it-ebooks.info",
      "content_length": 2274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 782,
      "content": "Derived Value\n\nWe can use a Derived Value either as part of ﬁ xture setup (Derived Input or One Bad Attribute) or when determining the expected values to be com- pared with those generated by the SUT (Derived Expectation). These uses are described in a bit more detail later in this section.\n\nVariation: Derived Input\n\nSometimes our test ﬁ xture contains similar values that the SUT might compare or use to base its logic on the difference between them. For example, a Derived Input might be calculated in the ﬁ xture setup portion of the test by adding the difference to a base value. This operation makes the relationship between the two values explicit. We can even put the value to be added in a symbolic constant with an Intent-Revealing Name [SBPP] such as MAXIMUM_ALLOWABLE_TIME_DIFFERENCE.\n\nVariation: One Bad Attribute\n\nA Derived Input is often employed when we need to test a method that takes a complex object as an argument. For example, thorough “input validation” testing requires that we exercise the method with each of the attributes of the object set to one or more possible invalid values to ensure that it handles all of these cases cor- rectly. Because the ﬁ rst rejected value could cause termination of the method, we must verify each bad attribute in a separate call to the SUT; each of these calls, in turn, should be done in a separate test method (each should be a Single-Condition Test; see page 45). We can instantiate the invalid object easily by ﬁ rst creating a valid object and then replacing one of its attributes with an invalid value. It is best to create the valid object using a Creation Method (page 415) so as to avoid Test Code Duplication (page 213).\n\nVariation: Derived Expectation\n\nWhen some value produced by the SUT should be related to one or more of the values we passed in to the SUT as arguments or as values in the ﬁ xture, we can often derive the expected value from the input values as the test executes rather than using precalculated Literal Values. We then use the result as the expected value in an Equality Assertion (see Assertion Method).\n\nMotivating Example\n\nThe following test doesn’t use Derived Values. Note the use of Literal Values in both the ﬁ xture setup logic and the assertion.\n\npublic void testAddItemQuantity_2a() throws Exception { BigDecimal widgetPrice = new BigDecimal(\"19.99\");\n\nwww.it-ebooks.info\n\n719\n\nDerived Value",
      "content_length": 2395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 783,
      "content": "720\n\nDerived Value\n\nChapter 27 Value Patterns\n\nProduct product = new Product(\"Widget\", widgetPrice); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, 5); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); assertEquals(new BigDecimal(\"99.95\"), actualItem.getExtendedPrice()); }\n\nTest readers may have to do some math in their heads to fully appreciate the relationship between the values in the ﬁ xture setup and the value in the result veriﬁ cation part of the test.\n\nRefactoring Notes\n\nTo make this test more readable, we can replace any Literal Values that are actu- ally derived from other values with formulas that calculate these values.\n\nExample: Derived Expectation\n\nThe original example contained only one line item for ﬁ ve instances of the prod- uct. We therefore calculated the expected value of the extended price attribute by multiplying the unit price by the quantity, which makes the relationship between the values explicit.\n\npublic void testAddItemQuantity_2b() throws Exception { BigDecimal widgetPrice = new BigDecimal(\"19.99\"); BigDecimal numberOfUnits = new BigDecimal(\"5\"); Product product = new Product(\"Widget\", widgetPrice); Invoice invoice = new Invoice(); // Exercise invoice.addItemQuantity(product, numberOfUnits); // Verify List lineItems = invoice.getLineItems(); LineItem actualItem = (LineItem)lineItems.get(0); BigDecimal totalPrice = widgetPrice.multiply(numberOfUnits); assertEquals(totalPrice, actualItem.getExtendedPrice()); }\n\nNote that we have also introduced symbolic constants for the unit price and quantity to make the expression even more obvious and to reduce the effort of changing the values later.\n\nwww.it-ebooks.info",
      "content_length": 1745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 784,
      "content": "Derived Value\n\nExample: One Bad Attribute\n\nSuppose we have the following Customer Factory Method [GOF], which takes a CustomerDto object as an argument. We want to write tests to verify what occurs when we pass in invalid values for each of the attributes in the CustomerDto. We could create the CustomerDto in-line in each Test Method with the appropriate attribute initialized to some invalid value.\n\npublic void testCreateCustomerFromDto_BadCredit() { // ﬁxture setup CustomerDto customerDto = new CustomerDto(); customerDto.ﬁrstName = \"xxx\"; customerDto.lastName = \"yyy\"; // etc. customerDto.address = createValidAddress(); customerDto.creditRating = CreditRating.JUNK; // exercise the SUT try { sut.createCustomerFromDto(customerDto); fail(\"Expected an exception\"); } catch (InvalidInputException e) { assertEquals( \"Field\", \"Credit\", e.ﬁeld ); } }\n\npublic void testCreateCustomerFromDto_NullAddress() { // ﬁxture setup CustomerDto customerDto = new CustomerDto(); customerDto.ﬁrstName = \"xxx\"; customerDto.lastName = \"yyy\"; // etc. customerDto.address = null; customerDto.creditRating = CreditRating.AAA; // exercise the SUT try { sut.createCustomerFromDto(customerDto); fail(\"Expected an exception\"); } catch (InvalidInputException e) { assertEquals( \"Field\", \"Address\", e.ﬁeld ); } }\n\nThe obvious problem with this code is that we end up with a lot of Test Code Duplication because we need at least one test per attribute. The problem becomes even worse if we are doing incremental development: We will require more tests for each newly added attribute, and we will have to revisit all existing tests to add the new attribute to the Factory Method signature.\n\nwww.it-ebooks.info\n\n721\n\nDerived Value",
      "content_length": 1706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 785,
      "content": "722\n\nDerived Value\n\nChapter 27 Value Patterns\n\nThe solution is to deﬁ ne a Creation Method that produces a valid instance of the CustomerDto (by doing an Extract Method [Fowler] refactoring on one of the tests) and uses it in each test to create a valid DTO. Then we simply replace one of the attributes with an invalid value in each of the tests. Each test now has an object with One Bad Attribute, with each one invalid in a slightly different way.\n\npublic void testCreateCustomerFromDto_BadCredit_OBA() { CustomerDto customerDto = createValidCustomerDto(); customerDto.creditRating = CreditRating.JUNK; try { sut.createCustomerFromDto(customerDto); fail(\"Expected an exception\"); } catch (InvalidInputException e) { assertEquals( \"Field\", \"Credit\", e.ﬁeld ); } }\n\npublic void testCreateCustomerFromDto_NullAddress_OBA() { CustomerDto customerDto = createValidCustomerDto(); customerDto.address = null; try { sut.createCustomerFromDto(customerDto); fail(\"Expected an exception\"); } catch (InvalidInputException e) { assertEquals( \"Field\", \"Address\", e.ﬁeld ); } }\n\nwww.it-ebooks.info",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 786,
      "content": "Generated Value\n\nGenerated Value\n\nHow do we specify the values to be used in tests?\n\nWe generate a suitable value each time the test is run.\n\nBigDecimal uniqueCustomerNumber = getUniqueNumber();\n\nWhen initializing the objects in the test ﬁ xture, one issue that must be dealt with is the fact that most objects have various attributes (ﬁ elds) that need to be supplied as arguments to the constructor. Sometimes the exact values to be used affect the outcome of the test. More often than not, however, it is important only that each object use a different value. When the precise values of these attributes are not important to the test, it is important not to have them visible within the test!\n\nGenerated Values are used in conjunction with Creation Methods (page 415)\n\nto help us remove this potentially distracting information from the test.\n\nHow It Works\n\nInstead of deciding which values to use in our tests while we are coding the tests, we generate the values when we actually execute the tests. We can then pick values to satisfy speciﬁ c criteria such as “must be unique in the database” that can be determined only as the test run unfolds.\n\nWhen to Use It\n\nWe use a Generated Value whenever we cannot or do not want to specify the test values until the test is executing. Perhaps the value of an attribute is not expected to affect the outcome of the test and we don’t want to be bothered to deﬁ ne Literal Values (page 714), or perhaps we need to ensure some quality of the attribute that can be determined only at runtime. In some cases, the SUT requires the value of an attribute to be unique; using a Generated Value can ensure that this criterion is satisﬁ ed and thereby prevent Unrepeatable Tests (see Erratic Test on page 228) and Test Run Wars (see Erratic Test) by reducing the likelihood of a test conﬂ icting with its parallel incarnation in another test run. Optionally, we can use this distinct value for all attributes of the object; object recognition then becomes very easy when we inspect the object in a debugger.\n\nOne thing to be wary of is that different values could expose different bugs. For example, a single-digit number may be formatted correctly, whereas a multidigit number might not (or vice versa). Generated Values can result\n\nwww.it-ebooks.info\n\n723\n\nGenerated Value",
      "content_length": 2310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 787,
      "content": "724\n\nGenerated Value\n\nChapter 27 Value Patterns\n\nin Nondeterministic Tests (see Erratic Test); if we encounter nondeterminism (sometimes the test passes and then fails during the very next run), we must check the SUT code to see whether differences in value could be the root cause. In general, we shouldn’t use a Generated Value unless the value must be unique because of the nondeterminism such a value may introduce. The obvi- ous alternative is to use a Literal Value. A less obvious alternative is to use a Derived Value (page 718), especially when we must determine the expected results of a test.\n\nImplementation Notes\n\nWe can generate values in a number of ways. The appropriateness of each tech- nique depends on the circumstance.\n\nVariation: Distinct Generated Value\n\nWhen we need to ensure that each test or object uses a different value, we can take advantage of Distinct Generated Values. In such a case, we can create a set of util- ity functions that will return unique values of various types (e.g., integers, strings, ﬂ oating-point numbers). The various getUnique methods can all be built upon an integer sequence number generator. For numbers that must be unique within the scope of a shared database, we can use database sequences or a sequence table. For numbers that must be unique within the scope of a particular test run, we can use an in-memory sequence number generator (e.g., use a Java static variable that is incremented before usage). In-memory sequence numbers that start from the number 1 each time a test suite is run offer a useful quality: The values generated in each test are the same for each run and can simplify debugging.\n\nVariation: Random Generated Value\n\nOne way to obtain good test coverage without spending a lot of time analyzing the behavior and generating test conditions is to use different values each time we run the tests. Using a Random Generated Value is one way to accomplish this goal. While use of such values may seem like a good idea, it makes the tests nondeterministic (Nondeterministic Tests) and can make debugging failed tests very difﬁ cult. Ideally, when a test fails, we want to be able to repeat that test failure on demand. To do so, we can log the Random Generated Value as the test is run and show it as part of the test failure. We then need to ﬁ nd a way to force the test to use that value again while we are troubleshooting the failed test. In most cases, the effort required outweighs the potential beneﬁ t. Of course, when we need this technique, we really need it.\n\nwww.it-ebooks.info",
      "content_length": 2564,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 788,
      "content": "Generated Value\n\nVariation: Related Generated Value\n\nAn optional enhancement is to combine a Generated Value with a Derived Value by using the same generated integer as the root for all attributes of a single object. This result can be accomplished by calling getUniqueInt once and then using that value to build unique strings, ﬂ oating-point numbers, and other values. With a Related Generated Value, all ﬁ elds of the object contain “related” data, which makes the object easier to recognize when debugging. Another option is to sepa- rate the generation of the root from the generation of the values by calling gener- ateNewUniqueRoot explicitly before calling getUniqueInt, getUniqueString, and so on.\n\nAnother nice touch for strings is to pass a role-describing argument to the function that is combined with the unique integer key to make the code more intent-revealing. Although we could also pass such arguments to the other functions, of course we wouldn’t be able to build them into an integer value.\n\nMotivating Example\n\nThe following test uses Literal Values for the arguments to a constructor:\n\npublic void testProductPrice_HCV() { // Setup Product product = new Product( 88, // ID \"Widget\", // Name new BigDecimal(\"19.99\")); // Price // Exercise // ... }\n\nRefactoring Notes\n\nWe can convert the test to use Distinct Generated Values by replacing the Literal Values with calls to the appropriate getUnique method. These methods simply increment a counter each time they are called and use that counter value as the root for construction of an appropriately typed value.\n\nExample: Distinct Generated Value\n\nHere is the same test using a Distinct Generated Value. For the getUniqueString method, we’ll pass a string describing the role (“Widget Name”).\n\npublic void testProductPrice_DVG() { // Setup Product product =\n\nwww.it-ebooks.info\n\n725\n\nGenerated Value",
      "content_length": 1870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 789,
      "content": "726\n\nGenerated Value\n\nChapter 27 Value Patterns\n\nnew Product( getUniqueInt(), // ID getUniqueString(\"Widget\"), // Name getUniqueBigDecimal()); // Price // Exercise // ... }\n\nstatic int counter = 0;\n\nint getUniqueInt() { counter++; return counter; }\n\nBigDecimal getUniqueBigDecimal() { return new BigDecimal(getUniqueInt()); }\n\nString getUniqueString(String baseName) { return baseName.concat(String.valueOf( getUniqueInt())); }\n\nThis test uses a different generated value for each argument of the constructor call. The numbers generated in this way are consecutive but the test reader still needs to look at a speciﬁ c attribute when debugging to get a consistent view. We probably should not generate the price value if the logic we were testing was related to price calculation because that would force our veriﬁ cation logic to accommodate different total costs.\n\nExample: Related Generated Value\n\nWe can ensure that all values used by the test are obviously related by separating the generation of the root value from the construction of the individual values. In the following example, we’ve moved the generation of the root to the setUp method so each test method gets a new value only once. The methods that retrieve the various values (e.g., getUniqueString) simply use the previously gener- ated root when deriving the Generated Values.\n\npublic void testProductPrice_DRVG() { // Setup Product product = new Product( getUniqueInt(), // ID getUniqueString(\"Widget\"), // Name getUniqueBigDecimal()); // Price // Exercise // ... }\n\nwww.it-ebooks.info",
      "content_length": 1555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 790,
      "content": "Generated Value\n\nstatic int counter = 0;\n\npublic void setUp() { counter++; }\n\nint getUniqueInt() { return counter; }\n\nString getUniqueString(String baseName) { return baseName.concat(String.valueOf( getUniqueInt())); }\n\nBigDecimal getUniqueBigDecimal() { return new BigDecimal(getUniqueInt()); }\n\nIf we looked at this object in an object inspector or database or if we dumped part of it to a log, we could readily tell which object we were looking at regard- less of which ﬁ eld we happened to see.\n\nwww.it-ebooks.info\n\n727\n\nGenerated Value",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 791,
      "content": "728\n\nAlso known as: Dummy, Dummy Parameter, Dummy Value, Placeholder, Stub\n\nDummy Object\n\nChapter 27 Value Patterns\n\nDummy Object\n\nHow do we specify the values to be used in tests when the only usage is as irrelevant arguments of SUT method calls?\n\nWe pass an object that has no implementation as an argument of a method called on the SUT.\n\nInvoice inv = new Invoice( new DummyCustomer() );\n\nGetting the SUT into the right state to start a test often requires calling other methods of the SUT. These methods commonly take as arguments objects that are stored in instance variables for later use. Often, these objects (or at least some attributes of these objects) are never used in the code that we are actu- ally testing. Instead, we create them solely to conform to the signature of some method we must call to get the SUT into the right state. Constructing these objects can be nontrivial and adds unnecessary complexity to the test.\n\nIn these cases, a Dummy Object can be passed as an argument, eliminating\n\nthe need to build a real object.\n\nHow It Works\n\nWe create an instance of some object that can be instantiated easily and with no dependencies; we then pass that instance as the argument of the method of the SUT. Because it won’t actually be used within the SUT, we don’t need any implementation for this object. If any of the methods of the Dummy Object are invoked, the test really should throw an error. Trying to invoke a nonexistent method will typically produce that result.\n\nWhen to Use It\n\nWe can use Dummy Objects whenever we need to use objects as attributes of other objects or arguments of methods on the SUT or other ﬁ xture objects. Using Dummy Objects helps us avoid Obscure Tests (page 186) by leaving out the irrelevant code that would be necessary to build real objects and by making it clear which objects and values are not used by the SUT.\n\nIf we need to control the indirect inputs or verify the indirect outputs of the SUT, we should probably use a Test Stub (page 529) or a Mock Object (page 544) instead. If the object will be used by the SUT but we cannot provide the real object, we should consider providing a Fake Object (page 551) that provides just enough behavior for the test to execute.\n\nwww.it-ebooks.info",
      "content_length": 2251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 792,
      "content": "Dummy Object\n\nWe can use one of the value patterns when the SUT really does need to use the object in some way. Either a Literal Value (page 714), a Generated Value (page 723), or a Derived Value (page 718) may be appropriate, depend- ing on the circumstance.\n\nVariation: Dummy Argument\n\nWe can use a Dummy Argument whenever methods of the SUT take objects as arguments1 and those objects are not relevant to the test.\n\nVariation: Dummy Attribute\n\nWe can use a Dummy Attribute whenever we are creating objects that will be used as part of the ﬁ xture or as arguments of SUT methods, and some of the attributes of those objects are not relevant to the test.\n\nImplementation Notes\n\nThe simplest implementation of a Dummy Object is to pass a null value as the argument. This approach works even in a statically typed language such as Java, albeit only if the method being called doesn’t check for null arguments. If the method complains when we pass it null, we’ll need to employ a slightly more sophisticated implementation. The biggest disadvantage to using null is that it is not very descriptive.\n\nIn dynamically typed languages such as Ruby, Perl, and Python, the actual type of the object will never be checked (because it will never be used), so we can use any class such as String or Object. In such a case, it is useful to give the object a Self-Describing Value (see Literal Value) such as “Dummy Customer.” In statically typed languages (such as Java, C#, and C++), we must ensure that the Dummy Object is type compatible with the parameter it is to match. Type compatibility is much easier to achieve if the parameter has an abstract type (e.g., an Interface in Java) because we can create our own trivial implementation of the type or pass a suitable Pseudo-Object (see Hard-Coded Test Double on page 568). If the parameter type is a concrete class, we may be able to create\n\n1 From Wikipedia: Parameters are also commonly referred to as arguments, although ar- guments are more properly thought of as the actual values or references assigned to the parameter variables when the subroutine is called at runtime. When discussing code that is calling into a subroutine, any values or references passed into the subroutine are the arguments, and the place in the code where these values or references are given is the parameter list. When discussing the code inside the subroutine deﬁ nition, the variables in the subroutine’s parameter list are the parameters, while the values of the parameters at runtime are the arguments.\n\nwww.it-ebooks.info\n\n729\n\nDummy Object",
      "content_length": 2572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 793,
      "content": "730\n\nDummy Object\n\nChapter 27 Value Patterns\n\na trivial instance of it or we may need to create an instance of a Test-Speciﬁ c Subclass (page 579) within our test.\n\nSome Mock Object frameworks have Test Utility Methods (page 599) that will generate a Dummy Object for a speciﬁ ed class that takes a String argument for a Self-Describing Value.\n\nWhile the Dummy Object may, in fact, be null, it is not the same as a Null Object [PLOPD3]. A Dummy Object is not used by the SUT, so its behavior is either irrelevant or it should throw an exception when executed. In contrast, a Null Object is used by the SUT but is designed to do nothing. That’s a small but very important distinction!\n\nMotivating Example\n\nIn this example, we are testing the Invoice but we require a Customer to instantiate the invoice. The Customer requires an Address, which in turn requires a City. Thus we ﬁ nd ourselves creating several additional objects just to set up the ﬁ xture. But if we know that the behavior we are testing should not access the Customer at all, why do we need to create it and all the objects on which it depends?\n\npublic void testInvoice_addLineItem_noECS() { ﬁnal int QUANTITY = 1; Product product = new Product(getUniqueNumberAsString(), getUniqueNumber()); State state = new State(\"West Dakota\", \"WD\"); City city = new City(\"Centreville\", state); Address address = new Address(\"123 Blake St.\", city, \"12345\"); Customer customer= new Customer(getUniqueNumberAsString(), getUniqueNumberAsString(), address); Invoice inv = new Invoice(customer); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); LineItem expItem = new LineItem(inv, product, QUANTITY); assertLineItemsEqual(\"\",expItem, actual); }\n\nThis test is quite cluttered as a result of the extra object creation. How is the behavior we are testing related to the Address and City? From this test, we can only assume that there is some relation. But this misleads the test reader!\n\nwww.it-ebooks.info",
      "content_length": 2102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 794,
      "content": "Dummy Object\n\nRefactoring Notes\n\nIf the objects in the ﬁ xture are not relevant to the test, they should not be visible in the test. Therefore, we should try to eliminate the need to create all these objects. We could try passing in null for the Customer. In this case, the constructor checks for null and rejects it, so we have to ﬁ nd another way.\n\nThe solution is to replace the object that is not important to our test with a Dummy Object. In dynamically typed languages, we could just pass in a string. In statically typed languages such as Java and C#, however, we must pass in a type-compatible object. In this case, we have chosen to do an Extract Interface [Fowler] refactoring on Customer to create a new interface and then create a new implementation class called DummyCustomer. Of course, as part of the Extract Inter- face refactoring, we must replace all references to Customer with the new interface name so that the DummyCustomer will be acceptable. A less intrusive option would be to use a Test-Speciﬁ c Subclass of Customer that adds a test-friendly constructor.\n\nExample: Dummy Values and Dummy Objects\n\nHere’s the same test using a Dummy Object instead of the Product name and the Customer. Note how much simpler the ﬁ xture setup has become!\n\npublic void testInvoice_addLineItem_DO() { ﬁnal int QUANTITY = 1; Product product = new Product(\"Dummy Product Name\", getUniqueNumber()); Invoice inv = new Invoice( new DummyCustomer() ); LineItem expItem = new LineItem(inv, product, QUANTITY); // Exercise inv.addItemQuantity(product, QUANTITY); // Verify List lineItems = inv.getLineItems(); assertEquals(\"number of items\", lineItems.size(), 1); LineItem actual = (LineItem)lineItems.get(0); assertLineItemsEqual(\"\", expItem, actual); }\n\nUsing a Dummy Object for the name of the Product was simple because it is a string and has no uniqueness requirement. Thus we were able to use a Self- Describing Value. We were not able to use a Dummy Object for the Product number because it must be unique, so we left it as a Generated Value. The Customer was a bit trickier because the LineItem’s constructor expected a non- null object. Because this example is written in Java, the method parameter is strongly typed; for this reason, we needed to create an alternative implemen- tation of the ICustomer interface with a no-argument constructor to simplify in-line construction. Because the DummyCustomer is never used, we have created\n\nwww.it-ebooks.info\n\n731\n\nDummy Object",
      "content_length": 2482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 795,
      "content": "732\n\nDummy Object\n\nChapter 27 Value Patterns\n\nit in-line rather than declaring a variable to hold it. This choice reduces the ﬁ xture setup code by one line, and the presence of the in-line constructor call within the call to the Invoice constructor reinforces the message that we need the Dummy Object only for the constructor call and not for the rest of the test. Here is the code for the DummyCustomer:\n\npublic class DummyCustomer implements ICustomer {\n\npublic DummyCustomer() { // Real simple; nothing to initialize! }\n\npublic int getZone() { throw new RuntimeException(\"This should never be called!\"); } }\n\nWe have implemented the DummyCustomer class with just those methods declared in the interface; because each method throws an exception, we know when it is hit. We could also have used a Pseudo-Object for the DummyCustomer. In other circum- stances we might have been able to simply pass in null or construct a dummy instance of the real class. The major problem with the latter technique is that we won’t know for sure if the Dummy Object is actually used.\n\nFurther Reading\n\nWhen [UTwJ] refers to a “dummy object,” these authors are referring to what this book terms a Test Stub. See Mocks, Fakes, Stubs, and Dummies in Appen- dix B for a more thorough comparison of the terminology used in various books and articles. The JMock and NMock frameworks for testing with Mock Objects support auto-generation of Dummy Objects.\n\nwww.it-ebooks.info",
      "content_length": 1455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 796,
      "content": "PART IV\n\nAppendixes\n\n733\n\nwww.it-ebooks.info\n\nAppendixes",
      "content_length": 56,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 797,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 798,
      "content": "Appendix A\n\nTest Refactorings\n\nExtract Testable Component\n\nYou want to be able to test the logic easily but the component is too closely tied to its context to allow such testing.\n\nExtract the logic you want to test into a separate component that is designed for testability and is independent of the context in which it is run.\n\nImplementation Notes\n\nWe extract the logic from the untestable component into a component that is testable via synchronous tests, leaving behind all the ties to the context. This usually means that anything required by the testable component logic from the context is retrieved by the untestable component and passed in to the testable component as arguments of the methods under test or constructor methods. The untestable component then contains very little code and is considered to be a Humble Object (page 695). It simply retrieves the information the testable component requires from the context, instantiates the testable component, and delegates to it. All interactions with the new testable component consist of synchronous method calls.\n\nThe testable component may be a Windows DLL, a Java JAR containing a Service Facade [CJ2EEP] class, or some other language component or class that exposes the services of the executable in a testable way. The untestable code may be an executable, a dialog box or some other presentation compo- nent, logic that is executed inside a transaction, or even a complex test method. Extraction of the testable component should leave behind a Humble Object that requires very little, if any, testing.\n\n735\n\nwww.it-ebooks.info\n\nExtract Testable Component\n\nAlso known as: Sprout Class [WEwLC]",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 799,
      "content": "736\n\nIn-line Resource\n\nAppendix A Test Refactorings\n\nDepending on the nature of the untestable component, we may choose to write tests for the delegation logic or we may be unable to do so because the logic is so closely tied to the context. If we do write tests for it, we require only one or two tests to verify that the instantiation and delegation occur correctly. Because this code will not change very often, these tests are much less critical than other tests and can even be omitted from the suite of tests that developers execute before check-in if we want to speed up test suite execution times. Of course, we would still prefer to run them from the automated build process.\n\nFurther Reading\n\nThis refactoring is similar to an Extract Interface [Fowler] refactoring and an Extract Implementer [Fowler] refactoring, except that Extract Testable Component does not require keeping the same interface. It can also be viewed as a special case of the Extract Class [Fowler] refactoring.\n\nIn-line Resource\n\nTests that depend on an unseen external resource create a Mystery Guest problem.\n\nMove the contents of an external resource into the ﬁ xture setup logic of the test.\n\nFrom [RTC]:\n\nTo remove the dependency between a test method and some external resource, we incorporate that resource in the test code. This is done by setting up a ﬁ xture in the test code that holds the same contents as the resource. This ﬁ xture is then used instead of the resource to run the test. A simple example of this refactoring is putting the contents of a ﬁ le that is used into some string in the test code.\n\nIf the contents of the resource are large, chances are high that you are also suffering from Eager Tests (see Assertion Roulette on page 224). Consider applying an Extract Method [Fowler] refactoring or a Minimize Data (page 738) refactoring.\n\nwww.it-ebooks.info",
      "content_length": 1862,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 800,
      "content": "Make Resource Unique\n\nImplementation Notes\n\nThe problem with tests that depend on an external resource is that we cannot see the pre-conditions of the test. The resource may be a ﬁ le sitting in the ﬁ le system, the contents of a database, or some other object created outside the test. None of these Prebuilt Fixtures (page 429) is visible to the test reader. The solution is to make them visible by including the resource in-line within the test. The simplest way to do so is to create the resource from within the test itself. For example, we could build the contents of a text ﬁ le by writing to the ﬁ le rather than just referring to a preexisting ﬁ le. If we delete the ﬁ le at the end of the test, this step also moves us from a Prebuilt Fixture approach to a Persistent Fresh Fixture (see Fresh Fixture on page 311) approach. As a result, our tests may execute somewhat more slowly.\n\nA more innovative way to in-line the external resource is to replace the actual resource with a Test Stub (page 529) that is initialized within the test. The contents of the resource then become visible to the test reader. When the system under test (SUT) executes, it uses the Test Stub instead of the real resource.\n\nAnother option is to refactor the design of the SUT so as to improve its test- ability. We can apply the Extract Testable Component (page 735) refactoring to the part of the SUT that uses the contents of the resource so that it can be tested directly without actually accessing an external resource. That is, the test passes the contents of the resource to the logic that uses it. We can also test the Humble Object (page 695) that reads the resource independently by replacing the extracted component with a Test Stub or Mock Object (page 544).\n\nMake Resource Unique\n\nSeveral tests are accidentally creating or using the same resource in a Shared Fixture.\n\nMake the name of any resources used by a test unique.\n\nFrom [RTC]:\n\nA lot of problems originate from the use of overlapping resource names, either between different tests run by the same user or between simultaneous test runs done by different users.\n\nSuch problems can easily be prevented (or repaired) by using unique identiﬁ ers for all resources that are allocated—for example, by including a time stamp. When you also include the name of the test responsible for\n\nwww.it-ebooks.info\n\n737\n\nMake Resource Unique",
      "content_length": 2383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 801,
      "content": "738\n\nMinimize Data\n\nAlso known as: Reduce Data\n\nAppendix A Test Refactorings\n\nallocating the resource in this identiﬁ er, you will have fewer problems ﬁ nding tests that do not properly release their resources.\n\nImplementation Notes\n\nWe make the name of any resources used by a test unique across all tests by using a Distinct Generated Value (see Generated Value on page 723) as part of the name. Ideally, the name should include the name of the test that “owns” the resource. To avoid Interacting Tests (see Erratic Test on page 228), we include a time stamp in the name of any resources created by the tests and use Automated Teardown (page 503) to delete those resources at the end of the test.\n\nMinimize Data\n\nThe test ﬁ xture is too large, making the test hard to understand.\n\nWe remove things from the ﬁ xture until we have a Minimal Fixture.\n\nFrom [RTC]:\n\nMinimize the data that is set up in ﬁ xtures to the bare essentials. This will have two advantages: (1) It makes them more suitable as documentation, and (2) your tests will be less sensitive to changes.\n\nImplementation Notes\n\nReducing the data in our test ﬁ xture to the bare minimum results in a Minimal Fix- ture (page 302) that helps the tests achieve Tests as Documentation (see page 23). How we do this depends on how our Test Methods (page 348) are organized into Testcase Classes (page 373).\n\nWhen our Test Methods are organized via the Testcase Class per Fixture pat- tern (page 631) and we believe we have a General Fixture (see Obscure Test on page 186), we can remove the ﬁ xture setup logic for any parts of the ﬁ xture that we suspect are not used by the tests. It is best to remove this logic incrementally so that if a test fails, we can undo our most recent change and try again.\n\nWhen our Test Methods are organized as a Testcase Class per Feature (page 624) or a Testcase Class per Class (page 617), Minimize Data may also involve copying ﬁ xture setup logic from the setUp method of a Testcase Class or Setup Decora- tor (page 447) into each test that needs the ﬁ xture. Assuming the collection of\n\nwww.it-ebooks.info",
      "content_length": 2101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 802,
      "content": "Replace Dependency with Test Double\n\nobjects in the Shared Fixture (page 317) is overkill for any one test, we can use a series of Extract Method [Fowler] refactorings to create a set of Creation Methods (page 415), which we then call from the tests. Next, we remove the calls to the Creation Methods from the setUp method and put them into only those Test Methods that require that part of the original ﬁ xture. The ﬁ nal step would be to convert any ﬁ xture-holding instance variables into local variables.\n\nReplace Dependency with Test Double\n\nThe dependencies of an object being tested get in the way of running tests.\n\nBreak the dependency by replacing a depended-on component with a Test Double.\n\nImplementation Notes\n\nThe ﬁ rst step is to choose the form of dependency substitution. Dependency Injection (page 678) is the best option for unit tests, whereas Dependency Look- up (page 686) often works better for customer tests. We then refactor the SUT to support this choice or design the capability into the SUT as we do test-driven development. The next decision is whether to use a Fake Object (page 551), a Test Stub (page 529), a Test Spy (page 538), or a Mock Object (page 544) based on how the Test Double will be used by the test. This decision is described in Chapter 11, Using Test Doubles.\n\nIf we are using a Test Stub or Mock Object, we must decide whether we want to use a Hard-Coded Test Double (page 568) or a Conﬁ gurable Test Double (page 558). The trade-offs are discussed in Chapter 11 and in the detailed descriptions of the patterns. That decision then dictates the shape of our test—for example, Tests that use Mock Objects are more “front-loaded” by the construction of the Mock Object.\n\nFinally, we modify our test to construct, optionally conﬁ gure, and then install the Mock Object. We may also have to add a call to the veriﬁ cation method for some kinds of Mock Objects. In statically typed languages, we may have to do an Extract Interface [Fowler] refactoring before we can introduce the fake imple- mentation. We then use this interface as the type of the variable that holds the reference to the substitutable dependency.\n\nwww.it-ebooks.info\n\n739\n\nReplace Dependency with Test Double",
      "content_length": 2223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 803,
      "content": "740\n\nSetup External Resource\n\nAppendix A Test Refactorings\n\nSetup External Resource\n\nThe SUT depends on the contents of an external resource that is acting as a Mystery Guest in our test.\n\nCreate an external resource within the ﬁ xture setup logic of the test rather than using a predeﬁ ned resource.\n\nFrom [RTC]:\n\nIf it is necessary for a test to rely on external resources, such as directories, databases, or ﬁ les, make sure the test that uses them explicitly creates or allocates these resources before testing, and releases them when done (take precautions to ensure the resource is also released when tests fail).\n\nImplementation Notes\n\nWhen our SUT must use an external resource such as a ﬁ le and we absolutely, positively cannot replace the access mechanism with a Test Stub (page 529) or Fake Object (page 551), we may need to live with the fact that we have to use an external resource. The problems with external resources are obvious: The test reader can- not tell what they contain; those resources may disappear unexpectedly, causing tests to fail because of Resource Optimism (see Erratic Test on page 228); and the resources may result in Interacting Tests (see Erratic Test) and Test Run Wars (see Erratic Test). Setup External Resource does not help us with the last problem but it does avoid the problems of a Mystery Guest (see Obscure Test on page 186) and Resource Optimism.\n\nTo implement the Setup External Resource refactoring, we simply pull the contents of the external resource into our Test Method (page 348), setUp method, or a Test Utility Method (page 599) called by them. Using the contents we con- struct the external resource within our test code, thereby making it evident to the test reader exactly what the test depends on. This approach also guarantees that the resource exists because we create it in every test run.\n\nwww.it-ebooks.info",
      "content_length": 1876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 804,
      "content": "Appendix B\n\nxUnit Terminology\n\nMocks, Fakes, Stubs, and Dummies\n\nAre you confused about what someone means when that individual says “test stub” or “mock object”? Do you sometimes feel that the person you are talking to is using a very different deﬁ nition? Well, you are not alone!\n\nThe terminology for the various kinds of Test Doubles (page 522) is confusing and inconsistent. Different authors use different terms to mean the same thing. And sometimes they mean different things even when they use the same term! Ouch! (See the sidebar “What’s in a (Pattern) Name?” on page 576 for why I think names are important.)\n\nPart of my reason for writing this book was to try to establish some consistency in the terminology, thereby giving people a set of names with clear deﬁ nitions of what they mean. In this appendix, I provide a list of the current sources and cross- reference the terminology they use with the terminology used in this book.\n\nRole Descriptions\n\nThe table on page 742 is a summary of what I mean by each of the major Test Double pattern names.\n\n741\n\nwww.it-ebooks.info\n\nMocks, Fakes, Stubs, and Dummies",
      "content_length": 1121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 805,
      "content": "Pattern\n\nPurpose\n\nTest Double (page 522)\n\nGeneric name for family\n\nDummy Object Attribute or (page 728)\n\nmethod parameter\n\nTest Stub (page 529)\n\nVerify indirect inputs of SUT\n\nTest Spy (page 538)\n\nVerify indirect outputs of SUT\n\nMock Object (page 544)\n\nVerify indirect outputs of SUT\n\nFake Object (page 551)\n\nRun (unrunnable) tests (faster)\n\nTemporary Test Stub (see Test Stub)\n\nStand in for procedural code not yet written\n\nHas Behavior\n\nNo\n\nYes\n\nYes\n\nYes\n\nYes\n\nYes\n\nD e s c r i p t i o n s\n\nR o l e\n\nInjects Indirect Handles Indirect Values Provided Examples Inputs into SUT Outputs of SUT\n\nby Test(er)\n\nNo, never called\n\nNo, never called\n\nNo\n\nNull, “Ignored String,” new Object()\n\nYes\n\nIgnores them\n\nInputs\n\nOptional\n\nCaptures them for later veriﬁ cation\n\nInputs (optional)\n\nOptional\n\nVeriﬁ es correctness against expectations\n\nInputs (optional) and expected outputs.\n\nNo\n\nUses them\n\nNone\n\nIn-memory database emulator\n\nNo\n\nUses them\n\nNone\n\nIn-memory database emulator\n\nwww.it-ebooks.info\n\n7 4 2\n\nA p p e n d\n\ni x B\n\nx U n i t T e r m i n o l o g y",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 806,
      "content": "Terminology Cross-Reference\n\nTerminology Cross-Reference\n\nThe following table lists some sources of conﬂ icting deﬁ nitions just to make it clear what the mapping is to the pattern names used in this book.\n\nSources and Names Used in Them\n\nPattern Astels Beck Feathers Fowler jMock UTWJ OMG Pragmatic Recipes\n\nTest Double\n\nDouble or stand-in\n\nDummy Stub Object\n\nDummy\n\nStub\n\nTest Stub\n\nFake\n\nFake\n\nStub\n\nStub\n\nDummy\n\nMock\n\nFake\n\nTest Spy\n\nDummy\n\nSpy\n\nMock Mock Object\n\nMock Mock Mock Mock\n\nMock\n\nMock\n\nFake Object\n\nDummy\n\nTempo- rary Test Stub\n\nStub\n\nOMG’s CORBA Stub\n\nStub\n\nUnit Testing with Java [UTwJ] uses the term “Dummy Object” to refer\n\nto what this book calls a “Fake Object.”\n\nPragmatic Unit Testing [PUT] describes a “Stub” as an empty imple- mentation of a method. This is a common interpretation in the proce- dural world; in the object world, however, it is typically called a Null Object [PLOPD3].\n\nSome of the early Mock Objects literature could be interpreted to equate a “Stub” with a “Mock Object.” The distinction between the two has since been clariﬁ ed in [MRNO] and [MAS].\n\nwww.it-ebooks.info\n\n743\n\nTerminology Cross- Reference",
      "content_length": 1148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 807,
      "content": "744\n\nxUnit Terminology Cross- Reference\n\nAppendix B xUnit Terminology\n\nThe CORBA standard1 and other remote-procedure call speciﬁ cations use the terms “stubs” and “skeletons” to refer to the automatically generated code for the near- and far-end implementations of a remote interface deﬁ ned in IDL. (I’ve included this information here because it is another use of a term that is commonly used in the TDD and auto- mated developer testing community.)\n\nThe sources quoted in the preceding table are provided here:\n\nSource\n\nDescription\n\nCitation\n\nPublisher\n\nAstels\n\nBook: Test-Driven Development\n\n[TDD-APG] Prentice Hall\n\nBeck\n\nBook: Test-Driven Development\n\n[TDD-BE]\n\nAddison-Wesley\n\nFeathers\n\nBook: Working Effectively with Legacy Code\n\n[WEwLC]\n\nPrentice Hall\n\nFowler\n\nBlog: Mocks Aren’t Stubs\n\n[MAS]\n\nmartinfowler.com\n\njMock\n\nPaper: Mock Roles, Not Objects\n\n[MRNO]\n\nACM (OOPSLA)\n\nUTWJ\n\nBook: Unit Testing in Java\n\n[UTwJ]\n\nMorgan Kaufmann\n\nOMG\n\nObject Management Group’s CORBA speciﬁ cations\n\nOMG\n\nPragmatic\n\nBook: Pragmatic Unit Testing with NUnit\n\n[PUT]\n\nPragmatic Pro- grammers\n\nRecipes\n\nBook: JUnit Recipes\n\nManning\n\nxUnit Terminology Cross-Reference\n\nThe following table maps the terminology used in this book to the terminology used by speciﬁ c members of the xUnit family. This list is not intended to be exhaustive but rather is meant to illustrate the adaptations of the standard xUnit terminology to the idioms and culture of each language and community.\n\n1 CORBA is an acronym for Common Object Request Broker Architecture. This standard is defined by the Object Management Group.\n\nwww.it-ebooks.info",
      "content_length": 1613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 808,
      "content": "Tool\n\nLanguage\n\nJava 1.4\n\nJava 5\n\n.NET\n\n.NET\n\n.NET\n\n.NET\n\n.NET\n\nPHP\n\nBook Term\n\nxUnit Member\n\nTestcase Class\n\nJUnit 3.8.2 Subclass of\n\nTestCase\n\nJUnit 4.0+\n\nimport org. junit.Test\n\nCsUnit\n\n[TestFixture]\n\nNUnit 2.0\n\n[TestFixture]\n\nNUnit 2.1+\n\n[TestFixture]\n\nMbUnit 2.0 [TestFixture]\n\nMSTest\n\n[TestClass]\n\nPHPUnit\n\nSubclass of TestCase\n\nTest Suite Factory\n\nTest Method\n\nFixture setup\n\nFixture teardown\n\nSuite Fixture Setup\n\nSuite Fixture Teardown Test\n\nExpected Exception\n\nstatic suite() testXxx() setUp()\n\ntearDown() Not\n\nNot\n\nSubclass of\n\napplicable applicable Expected\n\nException Test\n\nstatic suite() @Test\n\n@Before @After\n\n@Before @After Class\n\nClass\n\n@Exception\n\n[Suite]\n\n[Test]\n\n[SetUp]\n\n[TearDown] Not\n\nNot\n\n[Expected\n\napplicable applicable Exception()]\n\n[Suite]\n\n[Test]\n\n[SetUp]\n\n[TearDown] Not\n\nNot\n\n[Expected\n\napplicable applicable Exception()]\n\n[Suite]\n\n[Test]\n\n[SetUp]\n\n[TearDown]\n\n[Test Fixture SetUp]\n\n[TestFixture [Expected TearDown] Exception()]\n\n[Suite]\n\n[Test]\n\n[SetUp]\n\n[TearDown]\n\n[Fixture Setup]\n\n[Fixture Teardown] Exception()]\n\n[Expected\n\nNot applicable Method]\n\n[Test\n\n[Test Initialize] Cleanup]\n\n[Test\n\n[Class Initialize] Cleanup]\n\n[Class\n\n[Expected Exception()]\n\nstatic suite() testXxx() setUp()\n\ntearDown() Not\n\nNot\n\nSubclass of\n\napplicable applicable Expected\n\nException Test\n\nContinued...\n\nwww.it-ebooks.info\n\nR e f e r e n c e\n\nC r o s s -\n\nT e r m n o o g y\n\ni\n\nl\n\nx U n i t\n\nx U n i t T e r m i n o l o g y C r o s s - R e f e r e n c e\n\n7 4 5",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 809,
      "content": "Tool\n\nBook Term\n\nLanguage\n\nxUnit Member\n\nTestcase Class\n\nPython\n\nPyUnit\n\nSubclass of unittest. TestCase\n\nRuby\n\nTest::Unit\n\nSubclass of Test::Unit:: TestCase\n\nSmalltalk\n\nSUnit\n\nSuperclass: TestCase\n\nVB 6\n\nVbUnit\n\nImplements IFixture\n\nSAP ABAP ABAP Unit FOR\n\nTESTING\n\nTest Suite Factory\n\nTest Method\n\nFixture Fixture Setup\n\nTeardown Teardown\n\nTest Loader()\n\ntestXxx\n\nsetUp\n\ntearDown\n\nClassname. suite()\n\ntestXxx() setup()\n\nteardown\n\nTestSuite named:\n\ntestXxx\n\nsetUp\n\ntearDown\n\nImplements TestXxx() IFixture_ IFixture_ ISuite\n\nSetup()\n\nTearDown\n\nAutomatic Any\n\nsetup\n\nteardown\n\nwww.it-ebooks.info\n\nR e f e r e n c e\n\nC r o s s -\n\nT e r m n o o g y\n\ni\n\nl\n\nSuite Fixture Test\n\nSuite Fixture\n\nNot applicable applicable\n\nNot\n\nNot applicable applicable\n\nNot\n\nTo be determined determined\n\nTo be\n\nIFixture Frame_ Create()\n\nIFixture Frame_ Destroy\n\nclass_setup class_\n\nteardown\n\nx U n i t\n\nExpected Exception\n\nassert raise\n\nassert_raise\n\nshould:raise:\n\non error...\n\nTo be determined\n\n7 4 6\n\nA p p e n d\n\ni x B\n\nx U n i t T e r m i n o l o g y",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 810,
      "content": "Appendix C\n\nxUnit Family Members\n\nThis (incomplete) list of members of the xUnit family of test automation frameworks is included here to illustrate the diversity of the family and the extent to which automated unit testing is supported in various programming languages. This appendix also includes comments about speciﬁ c capabilities of some members of the family. A much more complete and up-to-date list can be found at http://xprogramming.com/software.htm.\n\nABAP Object Unit\n\nThe member of the xUnit family for SAP’s ABAP programming language. ABAP Object Unit is more or less a direct port of JUnit to ABAP except for the fact that it cannot catch exceptions encountered within the system under test (SUT).\n\nABAP Object Unit is available for download at http://www.abapunittests. com, along with articles about unit testing in ABAP. See ABAP Unit for versions of SAP/ABAP starting with 6.40.\n\nABAP Unit\n\nThe member of the xUnit family for versions of SAP’s ABAP programming lan- guage starting with Basis version 6.40 (NetWeaver 2004s). The most notable aspect of ABAP Unit is its special support that allows tests to be stripped from the code as the code is “transported” from the acceptance test environment to the production environment.\n\nABAP Unit is available directly from SAP AG as part of the NetWeaver 2004s development tools. More information on unit testing in ABAP is available in the SAP documentation and from http://www.abapunittests.com. See ABAP Object Unit for versions of SAP/ABAP prior to Basis version 6.40 (NetWeaver 2004s).\n\n747\n\nwww.it-ebooks.info\n\nxUnit Family Members",
      "content_length": 1599,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 811,
      "content": "748\n\nxUnit Family Members\n\nAppendix C xUnit Family Members\n\nCppUnit\n\nThe member of the xUnit family for the C++ programming language. It is available for download from http://cppunit.sourceforge.net. Another option for some .NET programmers is NUnit.\n\nCsUnit\n\nThe member of the xUnit family for the C# programming language. It is available from http://www.csunit.org. Another option for .NET programmers is NUnit.\n\nCUnit\n\nThe member of the xUnit family for the C programming language. Details can be found at http://cunit.sourceforge.net/doc/index.html.\n\nDbUnit\n\nAn extension of the JUnit framework intended to simplify testing of databases. It can be downloaded from http://www.dbunit.org/.\n\nIeUnit\n\nThe member of the xUnit family for testing Web pages rendered in Microsoft’s Internet Explorer browser using JavaScript and DHTML. It can be downloaded from http://ieunit.sourceforge.net/.\n\nJBehave\n\nOne of the ﬁ rst of a new generation of xUnit members designed to make tests written as part of TDD more useful Tests as Speciﬁ cation. The main difference between JBehave and more traditional members of the xUnit family is that JBehave eschews the “test” terminology and replaces it with terms more appro- priate for speciﬁ cation—that is, “ﬁ xture” becomes “context,” “assert” becomes “should,” and so on. JBehave is available at http://jbehave.codehaus.org. RSpec is the Ruby equivalent.\n\nJUnit\n\nThe member of the xUnit family for the Java programming language. JUnit was rewritten in late 2005 to take advantage of the annotations introduced in Java 1.5. It can be downloaded from http://www.junit.org.\n\nwww.it-ebooks.info",
      "content_length": 1626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 812,
      "content": "xUnit Family Members\n\nMbUnit\n\nThe xUnit family member for the C# programming language. MbUnit’s main claim to fame is its direct support for Parameterized Tests. It is available from http://www.nunit.orgmbunit.com. Other options for .NET programmers include NUnit, CsUnit, and MSTest.\n\nMSTest\n\nMicrosoft’s member of xUnit family does not seem to have a formal name other than its namespace Microsoft.VisualStudio.TestTools.UnitTesting but most people refer to it as MSTest. Technically, it is just the name of the Command- Line Test Runner mstest.exe. MSTest’s main claim to fame is that it ships with Visual Studio 2005 Team System. It does not appear to be available in the less expensive versions of Visual Studio or for free download. MSTest includes a number of innovative features, such as direct support for Data-Driven Tests. Information is available on MSDN at http://msdn.microsoft.com/en-us/library/ ms182516.aspx. Other (and cheaper) options for .NET programmers include NUnit, CsUnit, and MbUnit.\n\nNUnit\n\nThe member of the xUnit family for the .NET programming languages. It is available from http://www.nunit.org. Other options for C# programmers in- clude CsUnit, MbUnit, and MSTest.\n\nPHPUnit\n\nThe member of the xUnit family for the PHP programming language. Accord- ing to Sebastian Bergmann, “PHPUnit is a complete port of JUnit 3.8. On top of this original feature set it adds out-of-the-box support for Mock Objects, Code Coverage, Agile Documentation, and Incomplete and Skipped Tests.” More information about PHPUnit can be found at http://www.phpunit.de, including the free book on PHPUnit.\n\nPyUnit\n\nThe member of the xUnit family written to support Python programmers. It is a full port of JUnit. More information can be found at http://pyunit.sourceforge.net/.\n\nwww.it-ebooks.info\n\n749\n\nxUnit Family Members",
      "content_length": 1831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 813,
      "content": "750\n\nxUnit Family Members\n\nAppendix C xUnit Family Members\n\nRSpec\n\nOne of the ﬁ rst of a new generation of xUnit members designed to make tests written as part of TDD more useful Tests as Speciﬁ cation. The main differ- ence between RSpec and more traditional members of the xUnit family is that RSpec eschews the “test” terminology and replaces it with terms more appropri- ate for speciﬁ cation—for example, “ﬁ xture” becomes “context,” Test Methods becomes “specify,” “assert” becomes “should,” and so on. RSpec is available at http://rspec.rubyforge.org. JBehave is the Java equivalent.\n\nrunit\n\nOne member of the xUnit family for the Ruby programming language. It is a wrapper on Test::Unit that adds additional functionality. It is available at www.rubypeople.org.\n\nSUnit\n\nThe self-proclaimed “mother of all unit-testing frameworks.” SUnit is the mem- ber of the xUnit family for the Smalltalk programming language. It is available for download at http://sunit.sourceforge.net.\n\nTest::Unit\n\nThe member of the xUnit family for the Ruby programming language. It is available for download from http://www.rubypeople.org and comes as part of the “Ruby Development Tools” feature for the Eclipse IDE framework.\n\nTestNG\n\nA member of the xUnit family for Java that behaves a bit differently from JUnit. TestNG speciﬁ cally supports dependencies between tests and the shar- ing of the test ﬁ xture between Test Methods. More information is available at http://testng.org.\n\nutPLSQL\n\nThe member of the xUnit family for the PLSQL database programming lan- guage. You can get more information and download the source for this tool at http://utplsql.sourceforge.net/. A plug-in that integrates utPLSQL into the Oracle toolset is available at http://www.ounit.com.\n\nwww.it-ebooks.info",
      "content_length": 1775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 814,
      "content": "xUnit Family Members\n\nVB Lite Unit\n\nAnother member of the xUnit family written to support Visual Basic and VBA (Visual Basic for Applications). “VB Lite Unit is a reliable, lightweight unit-testing tool for Visual Basic and VBA written by Steve Jorgensen. The driving principle behind VB Lite Unit was to create the simplest, most reliable unit-testing tool possible that would still do everything that usually matters for doing test-driven development in VB 6 or VBA. Things that don’t work or don’t work reliably in VB and VBA are avoided, such as attempts at introspection to identify the test methods.” Another option for VB and VBA programmers is VbUnit. For VB.NET programmers, options include NUnit, CsUnit, and MbUnit.\n\nVbUnit\n\nThe member of the xUnit family written to support Visual Basic 6.0. It was the ﬁ rst member of the xUnit family to support Suite Fixture Setup and introduced the concept of calling a Testcase Class “test ﬁ xture.”\n\nOne major quirk of VbUnit is that when an Assertion Method fails the test, it writes the messages into the failure log immediately rather than just raising an error that is then caught by the Test Runner. The practical implication of this behavior is that it becomes difﬁ cult to test Custom Assertions because the messages in the logs are not prevented by the normal Expected Exception Test construct. The work-around is to run the Custom Assertion Tests inside an “Encapsulated Test Runner.”\n\nAnother quirk is that VbUnit is one of the few members of the xUnit family that is not free (as in beer). It is available from http://www.vbunit.org. There used to be a free version available—who knows, it may reappear some day. Another option for VB and VBA programmers is VB Lite Unit. For VB.NET program- mers, options include NUnit, CsUnit, and MbUnit.\n\nxUnit\n\nThe generic name for any Test Automation Framework for unit testing that is patterned on JUnit or SUnit. The xUnit test framework for most languages can be found at http://xprogramming.com or http://en.wikipedia.org/wiki/XUnit. Another place to look for both unit test and customer test tools is http://www. opensourcetesting.org.\n\nwww.it-ebooks.info\n\n751\n\nxUnit Family Members",
      "content_length": 2188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 815,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 816,
      "content": "Appendix D\n\nTools\n\nThe following tools are mentioned at some point within this book. This section describes their purpose and how they relate to xUnit test automation in just a wee bit more detail.\n\nAnt\n\nA build automation tool used in the Java community. NAnt is the equivalent for .NET projects.\n\nAntHill\n\nA continuous integration tool used in the Java community.\n\nBPT\n\nA commercial Scripted Test tool that allows less technically advanced users to compose tests from reusable test components that are the result of Refactored Recorded Tests. It can also be used to specify reusable test components to be built by more technically oriented test automaters. More information can be found on Mercury Interactive’s Web site. As this book went to press, Mercury Interactive was in the process of being acquired by Hewlett-Packard, so the URL may have changed.\n\nCanoo WebTest\n\nA framework for preparing Scripted Tests written in XML. Conceptually, Canoo WebTest is similar to Fit in that it allows us to deﬁ ne our own domain-speciﬁ c testing language for deﬁ ning customer tests. More information can be found at http://webtest.canoo.com and http://webtest-community.canoo.com.\n\n753\n\nwww.it-ebooks.info\n\nTools",
      "content_length": 1207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 817,
      "content": "754\n\nTools\n\nAppendix D Tools\n\nCruise Control\n\nA continuous integration tool used in the Java community. Cruise Control.net is the equivalent for .NET projects.\n\nDDSteps\n\nA Data-Driven Test extension for JUnit. “DDSteps is a JUnit extension for building data driven test cases. In a nutshell, DDSteps lets you parameterize your test cases, and run them more than once using different data.” See http:// www.ddsteps.org for more information.\n\nEasyMock\n\nA static Mock Object generation toolkit for Java tests. Because EasyMock uses a Conﬁ guration Mode for specifying the expectations, the tests look a bit strange and may take a bit of getting used to. More information can be found at http:// www.easymock.org.\n\neCATT\n\nThe Recorded Test tool that comes with SAP’s development tools. More infor- mation can be found at http://www.sap.com and at http://www.sdn.sap.com.\n\nEclipse\n\nA Java integrated development environment (IDE) and platform for rich client applications. Eclipse was originally created by IBM and is now managed by the Eclipse Foundation. Several of the language-speciﬁ c plug-ins are integrated with the corresponding xUnit family member. For example, the Java IDE includes JUnit and the Ruby Development Tools IDE includes Test::Unit. Eclipse is available for download from http://www.eclipse.org.\n\nFit\n\nThe framework conceived by Ward Cunningham that made it possible for cus- tomers to write automated tests. Fit separates the work of deﬁ ning the tests using tables in Web pages or spreadsheets from the programming work of exercising the SUT. While Fit was once a particular tool, it is now a speciﬁ cation for a fam- ily of tools implemented in a variety of languages, including Java, .NET, Ruby,\n\nwww.it-ebooks.info",
      "content_length": 1736,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 818,
      "content": "and Python. Some members of the family are simply test execution frameworks; others, such as Fitnesse, include test authoring and versioning capabilities. All should implement the same set of standard ﬁ xtures. More information can be found at Ward’s Web site (http://ﬁ t.c2.com) or in the book [FitB] he co-wrote with Rick Mudgridge.\n\nFitNesse\n\nA Fit test authoring tool conceived by (Uncle) Bob Martin of Object Mentor. FitNesse provides a wiki-like test authoring system with a set of predeﬁ ned Fit ﬁ xtures that makes it possible for customers to write and run automated tests. More information can be found at http://www.ﬁ tnesse.org.\n\nHttpUnit\n\nA front end that layers on top of JUnit to allow tests to exercise a Web applica- tion via the HTTP protocol. HttpUnit bypasses the browser, so it is not suitable for use with applications that make extensive use of on-page scripting (e.g., AJAX). See http://httpunit.sourceforge.net for more information.\n\nIdea\n\nA Java IDE that offers rich support for refactoring. The Idea Web site [JBrains] contains fairly detailed descriptions of many of the refactorings. The same group also offers a very popular refactoring plug-in for Visual Studio, called ReSharper.\n\nJFCUnit\n\nA JUnit front end that layers on top of HttpUnit to allow tests to exercise a Web application via the HTTP protocol. JFCUnit provides a number of Test Utility Methods that form a Higher-Level Language for expressing tests of Web applications. Because it is a layer on top of HttpUnit, it bypasses the browser. Thus JFCUnit is not suitable for use with applications that make extensive use of on-page scripting (e.g., AJAX). See http://jfcunit.sourceforge.net for more information.\n\nJMock\n\nA widely used dynamic Mock Object framework for Java tests. The ﬂ uent Conﬁ guration Interface used for specifying the expectations makes the tests highly readable. More information can be found at http://www.jmock.org.\n\nwww.it-ebooks.info\n\nTools\n\n755\n\nTools",
      "content_length": 1969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 819,
      "content": "756\n\nTools\n\nAppendix D Tools\n\nNMock\n\nA widely used dynamic Mock Object framework for .NET tests. The ﬂ uent Conﬁ guration Interface used for specifying the expectations makes the tests highly readable. More information can be found at http://nmock.org.\n\nQTP (QuickTest Professional)\n\nA commercial Recorded Test tool that allows less technically advanced users to record tests as they use an application. In conjunction with the “Expert View” of the Recorded Tests, QTP can also be used to refactor the tests into reusable test components that are appropriate for use by less technically adept test automa- ters. More information can be found on Mercury Interactive’s Web site. As this book went to press, Mercury Interactive was in the process of being acquired by Hewlett-Packard, so the URL has probably changed.\n\nReSharper\n\nA refactoring plug-in for Visual Studio by JetBrains, the makers of the Idea IDE. Their Web site [JBrains] contains fairly detailed descriptions of many of the refactorings.\n\nVisual Studio\n\nMicrosoft’s integrated development environment intended for developing .NET applications software. Visual Studio comes in several versions (at various price points), some of which include MSTest and code/test refactoring support. Third- party plug-ins are also available for both refactoring (see [JBrains]) and xUnit (see CsUnit, MbUnit, and NUnit).\n\nWatir\n\n“Web Application Testing in Ruby.” This set of components allows us to drive Internet Explorer from Scripted Tests written in the Ruby programming language. More information can be found at http://wtr.rubyforge.org/.\n\nwww.it-ebooks.info",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 820,
      "content": "Appendix E\n\nGoals and Principles\n\nName\n\nPage\n\nRelation\n\nBase Name\n\nChapter\n\nBug Repellent\n\n22\n\nBug Repellent\n\nChapter 3, Goals of Test Automation\n\nCommunicate Intent\n\n41\n\nCommunicate Intent\n\nChapter 5, Principles of Test Automation\n\nDefect Localization\n\n22\n\nDefect Localization\n\nChapter 3, Goals of Test Automation\n\nDesign for Testability\n\n40\n\nDesign for Testability\n\nChapter 5, Principles of Test Automation\n\nDo No Harm\n\n24\n\nDo No Harm\n\nChapter 3, Goals of Test Automation\n\nDon’t Modify the SUT\n\n41\n\nDon’t Modify the SUT\n\nChapter 5, Principles of Test Automation\n\nEnsure Commen- surate Effort and Responsibility\n\n47\n\nEnsure Commen- Chapter 5, Principles surate Effort and Responsibility\n\nof Test Automation\n\nExecutable Speciﬁ cation\n\n22\n\nAlias\n\nTests as Speciﬁ cation\n\nChapter 3, Goals of Test Automation\n\nExpressive Tests\n\n28\n\nExpressive Tests\n\nChapter 3, Goals of Test Automation\n\nFront Door First\n\n40\n\nAlias\n\nUse the Front Door First\n\nChapter 5, Principles of Test Automation\n\nFully Automated Test\n\n26\n\nFully Automated Chapter 3, Goals of Test\n\nTest Automation\n\nHigher Level Language\n\n41\n\nAlias\n\nCommunicate Intent\n\nChapter 5, Principles of Test Automation\n\nContinued...\n\n757\n\nwww.it-ebooks.info\n\nGoals and Principles",
      "content_length": 1221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 821,
      "content": "758\n\nGoals and Principles\n\nAppendix E Goals and Principles\n\nName\n\nPage\n\nRelation\n\nBase Name\n\nChapter\n\nIndependent Test 42\n\nAlias\n\nKeep Tests Independent\n\nChapter 5, Principles of Test Automation\n\nIsolate the SUT\n\n43\n\nIsolate the SUT\n\nChapter 5, Principles of Test Automation\n\nKeep Test Logic Out of Production Code\n\n45\n\nKeep Test Logic Out of Production Code\n\nChapter 5, Principles of Test Automation\n\nKeep Tests Independent\n\n42\n\nKeep Tests Independent\n\nChapter 5, Principles of Test Automation\n\nMinimize Test Overlap\n\n44\n\nMinimize Test Overlap\n\nChapter 5, Principles of Test Automation\n\nMinimize Untestable Code\n\n44\n\nMinimize Untestable Code\n\nChapter 5, Principles of Test Automation\n\nNo Test Logic in 45 Production Code\n\nKeep Test Logic Out Chapter 5, Principles of of Production Code\n\nTest Automation\n\nNo Test Risk\n\n21\n\nAlias\n\nDo No Harm\n\nChapter 5, Principles of Test Automation\n\nRepeatable Test\n\n26\n\nRepeatable Test\n\nChapter 3, Goals of Test Automation\n\nRobust Test\n\n29\n\nRobust Test\n\nChapter 3, Goals of Test Automation\n\nSafety Net\n\n24\n\nAlias\n\nTests as Safety Net Chapter 3, Goals of\n\nTest Automation\n\nSelf-Checking Test\n\n26\n\nSelf-Checking Test\n\nChapter 3, Goals of Test Automation\n\nSeparation of Concerns\n\n28\n\nSeparation of Concerns\n\nChapter 3, Goals of Test Automation\n\nSimple Tests\n\n28\n\nSimple Tests\n\nChapter 3, Goals of Test Automation\n\nSingle Condition 45 Test\n\nAlias\n\nVerify One Condition per Test\n\nChapter 5, Principles of of Test Automation\n\nSingle Glance Readable\n\n41\n\nAlias\n\nCommunicate Intent\n\nChapter 5, Principles of Test Automation\n\nTest Concerns Separately\n\n47\n\nTest Concerns Separately\n\nChapter 5, Principles of Test Automation\n\nTest-Driven Development\n\n40\n\nAlias\n\nWrite the Tests First\n\nChapter 5, Principles of Test Automation\n\nwww.it-ebooks.info",
      "content_length": 1769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 822,
      "content": "Name\n\nTest First Development\n\nTests as Documentation\n\nTests as Safety Net\n\nTests as Speciﬁ cation\n\nUse the Front Door First\n\nVerify One Condition per Test\n\nWrite the Tests First\n\nPage\n\n40\n\n23\n\n24\n\n22\n\n40\n\n45\n\n40\n\nRelation\n\nBase Name\n\nAlias\n\nWrite the Tests First\n\nTests as Documentation\n\nTests as Safety Net\n\nTests as Speciﬁ cation\n\nUse the Front Door First\n\nVerify One Condition per Test\n\nWrite the Tests First\n\nwww.it-ebooks.info\n\nGoals and Principles\n\nChapter\n\nChapter 5, Principles of Test Automation\n\nChapter 3, Goals of Test Automation\n\nChapter 3, Goals of Test Automation\n\nChapter 3, Goals of Test Automation\n\nChapter 5, Principles of Test Automation\n\nChapter 5, Principles of Test Automation\n\nChapter 5, Principles of Test Automation\n\n759\n\nGoals and Principles",
      "content_length": 768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 823,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 824,
      "content": "Appendix F\n\nSmells, Aliases, and Causes\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nAssertion Roulette\n\n224\n\nAssertion Roulette\n\nChapter 16, Behavior Smells\n\nAsynchronous Code\n\n210\n\nCause of\n\nHard-to-Test Code\n\nChapter 15, Code Smells\n\nAsynchronous Test\n\n255\n\nCause of\n\nSlow Tests\n\nChapter 16, Behavior Smells\n\nBehavior Sensitivity\n\n242\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nBuggy Tests\n\n260\n\nBuggy Tests\n\nChapter 17, Project Smells\n\nComplex Teardown\n\n206\n\nCause of\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nComplex Test\n\n186\n\nAlias\n\nObscure Test\n\nChapter 15, Code Smells\n\nConditional Test 200 Logic\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nConditional Veriﬁ cation Logic\n\n203\n\nCause of\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nContext Sensitivity\n\n245\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nCut-and-Paste Code Reuse\n\n214\n\nCause of\n\nTest Code Duplication\n\nChapter 15, Code Smells\n\nData Sensitivity\n\n243\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nDevelopers Not Writing Tests\n\n263\n\nDevelopers Not Chapter 17, Project Writing Tests\n\nSmells\n\nContinued...\n\n761\n\nwww.it-ebooks.info\n\nSmells, Aliases, and Causes",
      "content_length": 1175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 825,
      "content": "762\n\nSmells, Aliases, and Causes\n\nAppendix F Smells, Aliases, and Causes\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nEager Test\n\n224\n\nCause of\n\nAssertion Roulette Chapter 16, Behavior\n\nSmells\n\nEquality Pollution\n\n221\n\nCause of\n\nTest Logic in Production\n\nChapter 15, Code Smells\n\nErratic Test\n\n228\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nFlexible Test\n\n202\n\nCause of\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nFor Tests Only\n\n219\n\nCause of\n\nTest Logic in Production\n\nChapter 15, Code Smells\n\nFragile Fixture\n\n246\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nFragile Test\n\n239\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nFrequent Debugging\n\n248\n\nFrequent Debugging\n\nChapter 16, Behavior Smells\n\nGeneral Fixture\n\n190\n\nCause of\n\nObscure Test\n\nChapter 15, Code Smells\n\nHard-to-Test Code\n\n209\n\nHard-to-Test Code\n\nChapter 15, Code Smells\n\nHard-Coded Dependency\n\n210\n\nAlias\n\nHard-to-Test Code\n\nChapter 15, Code Smells\n\nHard-Coded Test Data\n\n194\n\nCause of\n\nObscure Test\n\nChapter 15, Code Smells\n\nHigh Test Maintenance Cost\n\n265\n\nHigh Test Maintenance Cost\n\nChapter 17, Project Smells\n\nHighly Coupled 210 Code\n\nCause of\n\nHard-to-Test Code\n\nChapter 15, Code Smells\n\nIndented Test Code\n\n200\n\nAlias\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nIndirect Testing\n\n196\n\nCause of\n\nObscure Test\n\nChapter 15, Code Smells\n\nInfrequently Run Tests\n\n268\n\nCause of\n\nProduction Bugs\n\nChapter 17, Project Smells\n\nwww.it-ebooks.info",
      "content_length": 1432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 826,
      "content": "Name\n\nPage\n\nInteracting Test Suites\n\n231\n\nInteracting Tests\n\n229\n\nInterface Sensitivity\n\n241\n\nIrrelevant Information\n\n192\n\nLonely Test\n\n232\n\nLong Test\n\n186\n\nLost Test\n\n269\n\nManual Debugging\n\n248\n\nManual Event Injection\n\n281\n\nManual Fixture Setup\n\n250\n\nManual Intervention\n\n250\n\nManual Result Veriﬁ cation\n\n251\n\nMissing Assertion 226 Message\n\nMissing Unit Test\n\n271\n\nMultiple Test Conditions\n\n207\n\nMystery Guest\n\n188\n\nNeverfail Test\n\n274\n\nNondeterministic 237 Test\n\nSmells, Aliases, and Causes\n\nRelationship Base Name\n\nChapter\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nCause of\n\nObscure Test\n\nChapter 15, Code Smells\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nAlias\n\nObscure Test\n\nChapter 15, Code Smells\n\nCause of\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nAlias\n\nFrequent Debugging\n\nChapter 16, Behavior Smells\n\nCause of\n\nManual Intervention\n\nChapter 16, Behavior Smells\n\nCause of\n\nManual Intervention\n\nChapter 16, Behavior Smells\n\nManual Intervention\n\nChapter 16, Behavior Smells\n\nCause of\n\nManual Intervention\n\nChapter 16, Behavior Smells\n\nCause of\n\nAssertion Roulette\n\nChapter 16, Behavior Smells\n\nCause of\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nCause of\n\nConditional Test Chapter 15, Code Logic\n\nSmells\n\nCause of\n\nObscure Test\n\nChapter 15, Code Smells\n\nCause of\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nContinued...\n\nwww.it-ebooks.info\n\n763\n\nSmells, Aliases, and Causes",
      "content_length": 1571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 827,
      "content": "764\n\nSmells, Aliases, and Causes\n\nAppendix F Smells, Aliases, and Causes\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nNot Enough Time\n\n263\n\nCause of\n\nDevelopers Not Chapter 17, Project Writing Tests\n\nSmells\n\nObscure Test\n\n186\n\nObscure Test Smells\n\nChapter 15, Code\n\nOvercoupled Test\n\n246\n\nAlias\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nOverspeciﬁ ed Software\n\n246\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nProduction Bugs\n\n268\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nProduction Logic in Test\n\n204\n\nCause of\n\nConditional Test Logic\n\nChapter 15, Code Smells\n\nReinventing the Wheel\n\n215\n\nCause of\n\nTest Code Duplication\n\nChapter 15, Code Smells\n\nResource Leakage\n\n233\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nResource Optimism\n\n233\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nSensitive Equality\n\n246\n\nCause of\n\nFragile Test\n\nChapter 16, Behavior Smells\n\nSlow Component 254 Usage\n\nCause of\n\nSlow Tests\n\nChapter 16, Behavior Smells\n\nSlow Tests\n\n253\n\nSlow Tests\n\nChapter 16, Behavior Smells\n\nTest Code Duplication\n\n213\n\nTest Code Duplication\n\nChapter 15, Code Smells\n\nTest Dependency 220 in Production\n\nCause of\n\nTest Logic in Production\n\nChapter 15, Code Smells\n\nTest Logic in Production\n\n217\n\nTest Logic in Production\n\nChapter 15, Code Smells\n\nTest Run War\n\n235\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nToo Many Tests\n\n256\n\nCause of\n\nSlow Tests\n\nChapter 16, Behavior Smells\n\nUnrepeatable Test\n\n234\n\nCause of\n\nErratic Test\n\nChapter 16, Behavior Smells\n\nUntestable Test Code\n\n211\n\nCause of\n\nHard-to-Test Code\n\nChapter 15, Code Smells\n\nwww.it-ebooks.info",
      "content_length": 1605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 828,
      "content": "Name\n\nUntested Code\n\nUntested Requirement\n\nVerbose Test\n\nWrong Test Automation Strategy\n\nPage\n\n271\n\n272\n\n186\n\n264\n\nSmells, Aliases, and Causes\n\nRelationship Base Name\n\nChapter\n\nCause of\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nCause of\n\nProduction Bugs Chapter 17, Project\n\nSmells\n\nAlias\n\nObscure Test\n\nChapter 15, Code Smells\n\nCause of\n\nDevelopers Not Chapter 17, Project Writing Tests\n\nSmells\n\nwww.it-ebooks.info\n\n765\n\nSmells, Aliases, and Causes",
      "content_length": 453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 829,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 830,
      "content": "Appendix G\n\nPatterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nAbstract Setup Decorator\n\n449\n\nVariation\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nAbstract Test Fixture\n\n638\n\nAlias\n\nTestcase Superclass\n\nChapter 24, Test Organization Patterns\n\nAbstract Testcase\n\n638\n\nAlias\n\nTestcase Superclass\n\nChapter 24, Test Organization Patterns\n\nAllTests Suite\n\n593\n\nVariation\n\nNamed Test Suite Chapter 24, Test\n\nOrganization Patterns\n\nAnonymous Creation Method\n\n417\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nArgument- Describing Message\n\n371\n\nVariation\n\nAssertion Message Chapter 19, xUnit\n\nBasics Patterns\n\nAssertion- Identifying Message\n\n371\n\nVariation\n\nAssertion Message Chapter 19, xUnit\n\nBasics Patterns\n\nAssertion Message\n\n370\n\nAssertion Message Chapter 19, xUnit\n\nBasics Patterns\n\nAssertion Method\n\n362\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nAttachment Method\n\n418\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nContinued...\n\n767\n\nwww.it-ebooks.info\n\nPatterns, Aliases, and Variations",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 831,
      "content": "768\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nAutomated Exercise Teardown\n\n505\n\nVariation\n\nAutomated Teardown\n\nChapter 22, Fixture Teardown Patterns\n\nAutomated Fixture Teardown\n\n504\n\nVariation\n\nAutomated Teardown\n\nChapter 22, Fixture Teardown Patterns\n\nAutomated Teardown\n\n503\n\nAutomated Teardown\n\nChapter 22, Fixture Teardown Patterns\n\nAutomated Unit Test\n\n285\n\nAlias\n\nScripted Test\n\nChapter 18, Test Strategy Patterns\n\nBack Door Manipulation\n\n327\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nBack Door Setup\n\n329\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nBack Door Teardown\n\n330\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nBack Door Veriﬁ cation\n\n329\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nBehavior- Exposing Subclass\n\n580\n\nVariation\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nBehavior- Modifying Subclass\n\n580\n\nVariation\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nBehavior Veriﬁ cation\n\n468\n\nBehavior Veriﬁ cation\n\nChapter 21, Result Veriﬁ cation Patterns\n\nBespoke Assertion\n\n474\n\nAlias\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nBuilt-in Test Recording\n\n281\n\nVariation\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nCalculated Values\n\n718\n\nAlias\n\nDerived Value\n\nChapter 27, Value Patterns\n\nCapture/ Playback Test\n\n278\n\nAlias\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nChained Tests\n\n454\n\nChained Tests\n\nChapter 20, Fixture Setup Patterns\n\nCleanup Method 602\n\nVariation\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nwww.it-ebooks.info",
      "content_length": 1709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 832,
      "content": "Name\n\nPage\n\nCommand-Line Test Runner\n\n379\n\nComponent Broker\n\n686\n\nComponent Registry\n\n686\n\nComponent Test\n\n340\n\nConﬁ gurable Mock Object\n\n558\n\nConﬁ gurable Test Double\n\n558\n\nConﬁ gurable Test Spy\n\n558\n\nConﬁ gurable Test Stub\n\n558\n\nConﬁ guration Interface\n\n560\n\nConﬁ guration Mode\n\n560\n\nConstant Value\n\n714\n\nConstructor Injection\n\n680\n\nConstructor Test 351\n\nCreation Method 415\n\nCustom Assertion\n\n474\n\nCustom Assertion Test\n\n477\n\nCustom Equality 476 Assertion\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nVariation\n\nTest Runner\n\nChapter 19, xUnit Basics Patterns\n\nAlias\n\nDependency Lookup\n\nChapter 26, Design- for-Testability Patterns\n\nAlias\n\nDependency Lookup\n\nChapter 26, Design- for-Testability Patterns\n\nVariation\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nLiteral Value\n\nChapter 27, Value Patterns\n\nVariation\n\nDependency Injection\n\nChapter 26, Design- for-Testability Patterns\n\nVariation\n\nTest Method\n\nChapter 19, xUnit Basics Patterns\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n769\n\nPatterns, Aliases, and Variations",
      "content_length": 1708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 833,
      "content": "770\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nDB Schema per Test-Runner\n\n651\n\nVariation\n\nDatabase Sandbox Chapter 25, Database\n\nPatterns\n\nData Loader\n\n330\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nData Retriever\n\n331\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nData-Driven Test 288\n\nData-Driven Test Chapter 18, Test\n\nStrategy Patterns\n\nData-Driven Test 290 Framework (Fit)\n\nVariation\n\nData-Driven Test Chapter 18, Test Strategy Patterns\n\nData-Driven Test 300 Frameworks\n\nVariation\n\nTest Automation Chapter 18, Test Strategy Patterns Framework\n\nDatabase Extraction Script\n\n331\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nDatabase Partitioning Scheme\n\n652\n\nVariation\n\nDatabase Sandbox Chapter 25, Database\n\nPatterns\n\nDatabase Population Script\n\n330\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nDatabase Sandbox\n\n650\n\nDatabase Sandbox Chapter 25, Database\n\nPatterns\n\nDecorated Lazy Setup\n\n449\n\nVariation\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nDedicated Database Sandbox\n\n651\n\nVariation\n\nDatabase Sandbox Chapter 25, Database\n\nPatterns\n\nDelegated Setup\n\n411\n\nDelegated Setup\n\nChapter 20, Fixture Setup Patterns\n\nDelegated Teardown\n\n511\n\nVariation\n\nIn-line Teardown Chapter 22, Fixture Teardown Patterns\n\nDelta Assertion\n\n485\n\nDelta Assertion\n\nChapter 21, Result Veriﬁ cation Patterns\n\nDependency Initialization Test\n\n352\n\nVariation\n\nTest Method\n\nChapter 19, xUnit Basics Patterns\n\nDependency Injection\n\n678\n\nDependency Injection\n\nChapter 26, Design- for-Testability Patterns\n\nwww.it-ebooks.info",
      "content_length": 1704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 834,
      "content": "Name\n\nDependency Lookup\n\nDerived Expectation\n\nDerived Input\n\nDerived Value\n\nDiagnostic Assertion\n\nDirect Test Method Invocation\n\nDistinct Generated Value\n\nDomain Assertion\n\nDummy\n\nDummy Argument\n\nDummy Attribute\n\nDummy Object\n\nDummy Parameter\n\nDummy Value\n\nDynamically Generated Test Double\n\nEntity Chain Snipping\n\nEquality Assertion\n\nPage\n\n686\n\n719\n\n719\n\n718\n\n476\n\n401\n\n724\n\n476\n\n728\n\n729\n\n729\n\n728\n\n728\n\n728\n\n561\n\n531\n\n365\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nDependency Lookup\n\nChapter 26, Design- for-Testability Patterns\n\nVariation\n\nDerived Value\n\nChapter 27, Value Patterns\n\nVariation\n\nDerived Value\n\nChapter 27, Value Patterns\n\nDerived Value\n\nChapter 27, Value Patterns\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nVariation\n\nTest Enumeration Chapter 19, xUnit\n\nBasics Patterns\n\nVariation\n\nGenerated Value\n\nChapter 27, Value Patterns\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nAlias\n\nDummy Object\n\nChapter 27, Value Patterns\n\nVariation\n\nDummy Object\n\nChapter 27, Value Patterns\n\nVariation\n\nDummy Object\n\nChapter 27, Value Patterns\n\nDummy Object\n\nChapter 27, Value Patterns\n\nAlias\n\nDummy Object\n\nChapter 27, Value Patterns\n\nAlias\n\nDummy Object\n\nChapter 27, Value Patterns\n\nVariation\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nVariation\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n771\n\nPatterns, Aliases, and Variations",
      "content_length": 1530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 835,
      "content": "772\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nExpectation- Describing Message\n\n371\n\nVariation\n\nAssertion Message Chapter 19, xUnit\n\nBasics Patterns\n\nExpected Behavior\n\n470\n\nAlias\n\nBehavior Veriﬁ cation\n\nChapter 21, Result Veriﬁ cation Patterns\n\nExpected Behavior Speciﬁ cation\n\n470\n\nVariation\n\nBehavior Veriﬁ cation\n\nChapter 21, Result Veriﬁ cation Patterns\n\nExpected Exception Assertion\n\n366\n\nVariation\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nExpected Exception Test\n\n350\n\nVariation\n\nTest Method\n\nChapter 19, xUnit Basics Patterns\n\nExpected Object 464\n\nAlias\n\nState Veriﬁ cation Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nExpected State Speciﬁ cation\n\n464\n\nVariation\n\nState Veriﬁ cation Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nExternal Test Recording\n\n280\n\nVariation\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nFake Database\n\n553\n\nVariation\n\nFake Object\n\nChapter 23, Test Double Patterns\n\nFake Object\n\n551\n\nFake Object\n\nChapter 23, Test Double Patterns\n\nFake Service Layer\n\n553\n\nVariation\n\nFake Object\n\nChapter 23, Test Double Patterns\n\nFake Web Service\n\n553\n\nVariation\n\nFake Object\n\nChapter 23, Test Double Patterns\n\nFile System Test Runner\n\n380\n\nVariation\n\nTest Runner\n\nChapter 19, xUnit Basics Patterns\n\nFinder Method\n\n600\n\nVariation\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nFixture Setup Testcase\n\n456\n\nVariation\n\nChained Tests\n\nChapter 20, Fixture Setup Patterns\n\nFour-Phase Test\n\n358\n\nFour-Phase Test\n\nChapter 19, xUnit Basics Patterns\n\nFramework- Invoked Setup\n\n424\n\nAlias\n\nImplicit Setup\n\nChapter 20, Fixture Setup Patterns\n\nwww.it-ebooks.info",
      "content_length": 1683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 836,
      "content": "Name\n\nFramework- Invoked Teardown\n\nFresh Context\n\nFresh Fixture\n\nFuzzy Equality Assertion\n\nGarbage- Collected Teardown\n\nGenerated Value\n\nGlobal Fixture\n\nGraphical Test Runner\n\nGuard Assertion\n\nHand-Built Test Double\n\nHand-Scripted Test\n\nHand-Written Test\n\nHard-Coded Mock Object\n\nHard-Coded Setup Decorator\n\nHard-Coded Test Double\n\nHard-Coded Test Stub\n\nHard-Coded Value\n\nPage\n\n516\n\n311\n\n311\n\n365\n\n500\n\n723\n\n430\n\n378\n\n490\n\n560\n\n285\n\n285\n\n568\n\n449\n\n568\n\n568\n\n714\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nAlias\n\nImplicit Teardown Chapter 22, Fixture Teardown Patterns\n\nAlias\n\nFresh Fixture\n\nChapter 18, Test Strategy Patterns\n\nFresh Fixture\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nGarbage- Collected Teardown\n\nChapter 22, Fixture Teardown Patterns\n\nGenerated Value\n\nChapter 27, Value Patterns\n\nVariation\n\nPrebuilt Fixture\n\nChapter 20, Fixture Setup Patterns\n\nVariation\n\nTest Runner\n\nChapter 19, xUnit Basics Patterns\n\nGuard Assertion\n\nChapter 21, Result Veriﬁ cation Patterns\n\nVariation\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nScripted Test\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nScripted Test\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nLiteral Value\n\nChapter 27, Value Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n773\n\nPatterns, Aliases, and Variations",
      "content_length": 1625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 837,
      "content": "774\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nHooked Setup\n\n424\n\nAlias\n\nImplicit Setup\n\nChapter 20, Fixture Setup Patterns\n\nHooked Teardown\n\n516\n\nAlias\n\nImplicit Teardown Chapter 22, Fixture Teardown Patterns\n\nHumble Container Adapter\n\n698\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nHumble Dialog\n\n696\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nHumble Executable\n\n697\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nHumble Object\n\n695\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nHumble Transaction Controller\n\n697\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nImmutable Shared Fixture\n\n323\n\nVariation\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nImplicit Setup\n\n424\n\nImplicit Setup\n\nChapter 20, Fixture Setup Patterns\n\nImplicit Teardown\n\n516\n\nImplicit Teardown Chapter 22, Fixture Teardown Patterns\n\nImposter\n\n522\n\nAlias\n\nTest Double\n\nChapter 23, Test Double Patterns\n\nIn-Database Stored Procedure Test\n\n655\n\nVariation\n\nStored Procedure Chapter 25, Database Test\n\nPatterns\n\nIn-Memory Database\n\n553\n\nVariation\n\nFake Object\n\nChapter 23, Test Double Patterns\n\nIncremental Tabular Test\n\n609\n\nVariation\n\nParameterized Test Chapter 24, Result\n\nVeriﬁ cation Patterns\n\nIncremental Tests 322\n\nVariation\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nIndirect Output Registry\n\n541\n\nVariation\n\nTest Spy\n\nChapter 23, Test Double Patterns\n\nwww.it-ebooks.info",
      "content_length": 1574,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 838,
      "content": "Name\n\nPage\n\nIn-line Setup\n\n408\n\nIn-line Teardown 509\n\nInner Test Double\n\n570\n\nInteraction Testing\n\n468\n\nLayer Test\n\n337\n\nLayer-Crossing Test\n\n327\n\nLayered Test\n\n337\n\nLazy Setup\n\n435\n\nLazy Teardown\n\n663\n\nLeftover Fixture\n\n317\n\nLiteral Value\n\n714\n\nLoop-Driven Test\n\n610\n\nMinimal Fixture\n\n302\n\nMinimal Context\n\n302\n\nMock Object\n\n544\n\nNaive In-line Teardown\n\n511\n\nNaive xUnit Test Interpreter\n\n292\n\nNamed State Reaching Method\n\n417\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nIn-line Setup\n\nChapter 20, Fixture Setup Patterns\n\nIn-line Teardown Chapter 22, Fixture Teardown Patterns\n\nVariation\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nAlias\n\nBehavior Veriﬁ cation\n\nChapter 21, Result Veriﬁ cation Patterns\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nLazy Setup\n\nChapter 20, Fixture Setup Patterns\n\nVariation\n\nTable Truncation Chapter 25, Database Teardown\n\nPatterns\n\nAlias\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nLiteral Value\n\nChapter 27, Value Patterns\n\nVariation\n\nParameterized Test\n\nChapter 24, Result Veriﬁ cation Patterns\n\nMinimal Fixture\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nMinimal Fixture\n\nChapter 18, Test Strategy Patterns\n\nMock Object\n\nChapter 23, Test Double Patterns\n\nVariation\n\nIn-line Teardown Chapter 22, Fixture Teardown Patterns\n\nVariation\n\nData-Driven Test Chapter 18, Test Strategy Patterns\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n775\n\nPatterns, Aliases, and Variations",
      "content_length": 1634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 839,
      "content": "776\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nNamed Test Suite 592\n\nNamed Test Suite Chapter 24, Test\n\nOrganization Patterns\n\nObject Attribute 476 Equality Assertion\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nObject Factory\n\n686\n\nAlias\n\nDependency Lookup\n\nChapter 26, Design- for-Testability Patterns\n\nObject Mother\n\n644\n\nVariation\n\nTest Helper\n\nChapter 24, Test Organization Patterns\n\nOne Bad Attribute\n\n719\n\nVariation\n\nDerived Value\n\nChapter 27, Value Patterns\n\nParameter Injection\n\n680\n\nVariation\n\nDependency Injection\n\nChapter 26, Design- for-Testability Patterns\n\nParameterized Anonymous Creation Method\n\n417\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nParameterized Creation Method\n\n417\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nParameterized Setup Decorator\n\n449\n\nVariation\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nParameterized Test\n\n607\n\nParameterized Test Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nPer-Run Fixture\n\n323\n\nVariation\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nPersistence Layer 339 Test\n\nVariation\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nPersistent Fresh Fixture\n\n314\n\nVariation\n\nFresh Fixture\n\nChapter 18, Test Strategy Patterns\n\nPlaceholder\n\n728\n\nAlias\n\nDummy Object\n\nChapter 27, Value Patterns\n\nPoor Man’s Humble Object\n\n699\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nwww.it-ebooks.info",
      "content_length": 1533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 840,
      "content": "Name\n\nPage\n\nPrebuilt Context 429\n\nPrebuilt Fixture\n\n429\n\nPresentation Layer Test\n\n338\n\nPrivate Fixture\n\n311\n\nProcedural Behavior Veriﬁ cation\n\n470\n\nProcedural State 463 Veriﬁ cation\n\nProcedural Test Stub\n\n526\n\nProgrammatic Test\n\n285\n\nPseudo-Object\n\n571\n\nPushdown Decorator\n\n450\n\nRandom Generated Value\n\n724\n\nRecord and Playback Test\n\n278\n\nRecorded Test\n\n278\n\nRefactored Recorded Test\n\n280\n\nRelated Generated Value\n\n725\n\nRemoted Stored 656 Procedure Test\n\nResponder\n\n530\n\nRetrieval Interface\n\n540\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nAlias\n\nPrebuilt Fixture\n\nChapter 20, Fixture Setup Patterns\n\nPrebuilt Fixture\n\nChapter 20, Fixture Setup Patterns\n\nVariation\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nFresh Fixture\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nBehavior Veriﬁ cation\n\nChapter 21, Result Veriﬁ cation Patterns\n\nVariation\n\nState Veriﬁ cation Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nVariation\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nAlias\n\nScripted Test\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nVariation\n\nGenerated Value\n\nChapter 27, Value Patterns\n\nAlias\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nGenerated Value\n\nChapter 27, Value Patterns\n\nVariation\n\nStored Procedure Chapter 25, Database Test\n\nPatterns\n\nVariation\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nVariation\n\nTest Spy\n\nChapter 23, Test Double Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n777\n\nPatterns, Aliases, and Variations",
      "content_length": 1724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 841,
      "content": "778\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nReuse Test for Fixture Setup\n\n418\n\nVariation\n\nCreation Method Chapter 20, Fixture\n\nSetup Patterns\n\nReused Fixture\n\n317\n\nAlias\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nRobot User Test\n\n278\n\nAlias\n\nRecorded Test\n\nChapter 18, Test Strategy Patterns\n\nRobot User Test Framework\n\n299\n\nVariation\n\nTest Automation Chapter 18, Test Strategy Patterns Framework\n\nRow Test\n\n609\n\nAlias\n\nParameterized Test\n\nChapter 24, Test Organization Patterns\n\nSUT API Encapsulation\n\n601\n\nAlias\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nSUT Encapsulation Method\n\n601\n\nVariation\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nSaboteur\n\n530\n\nVariation\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nScripted Test\n\n285\n\nScripted Test\n\nChapter 18, Test Strategy Patterns\n\nSelf Shunt\n\n540\n\nVariation\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nSelf-Describing Value\n\n715\n\nVariation\n\nLiteral Value\n\nChapter 27, Value Patterns\n\nService Layer Test\n\n339\n\nVariation\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nService Locator\n\n686\n\nAlias\n\nDependency Lookup\n\nChapter 26, Design- for-Testability Patterns\n\nSetter Injection\n\n681\n\nVariation\n\nDependency Injection\n\nChapter 26, Design- for-Testability Patterns\n\nSetup Decorator\n\n447\n\nSetup Decorator\n\nChapter 20, Fixture Setup Patterns\n\nShared Context\n\n317\n\nAlias\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nShared Fixture\n\n317\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nwww.it-ebooks.info",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 842,
      "content": "Name\n\nPage\n\nShared Fixture State Assertion\n\n491\n\nShared Setup Method\n\n424\n\nSimple Success Test\n\n349\n\nSingle-Layer Test 337\n\nSingle-Outcome Assertion\n\n366\n\nSingle Test Suite\n\n593\n\nSlow Tests\n\n318\n\nSpy\n\n538\n\nStale Fixture\n\n317\n\nStandard Context 305\n\nStandard Fixture 305\n\nState-Exposing Subclass\n\n580\n\nState Veriﬁ cation 462\n\nState-Based Testing\n\n462\n\nStated Outcome 366 Assertion\n\nStatically Generated Test Double\n\n561\n\nStored Procedure 654 Test\n\nStub\n\n529\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nVariation\n\nGuard Assertion\n\nChapter 21, Result Veriﬁ cation Patterns\n\nAlias\n\nImplicit Setup\n\nChapter 20, Fixture Setup Patterns\n\nVariation\n\nTest Method\n\nChapter 19, xUnit Basics Patterns\n\nAlias\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nVariation\n\nNamed Test Suite Chapter 24, Test\n\nOrganization Patterns\n\nVariation\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nTest Spy\n\nChapter 23, Test Double Patterns\n\nAlias\n\nShared Fixture\n\nChapter 18, Test Strategy Patterns\n\nAlias\n\nStandard Fixture Chapter 18, Test Strategy Patterns\n\nStandard Fixture Chapter 18, Test Strategy Patterns\n\nVariation\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nState Veriﬁ cation Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nAlias\n\nState Veriﬁ cation Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nVariation\n\nAssertion Method Chapter 19, xUnit\n\nBasics Patterns\n\nVariation\n\nConﬁ gurable Test Double\n\nChapter 23, Test Double Patterns\n\nStored Procedure Chapter 25, Database Test\n\nPatterns\n\nAlias\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n779\n\nPatterns, Aliases, and Variations",
      "content_length": 1700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 843,
      "content": "780\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nStub\n\n728\n\nAlias\n\nDummy Object\n\nChapter 27, Value Patterns\n\nSubclassed Humble Object\n\n700\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nSubclassed Singleton\n\n581\n\nAlias\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nSubclassed Test Double\n\n581\n\nAlias\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nSubcutaneous Test\n\n340\n\nVariation\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nSubset Suite\n\n593\n\nVariation\n\nNamed Test Suite Chapter 24, Test\n\nOrganization Patterns\n\nSubstitutable Singleton\n\n581\n\nAlias\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nSubstituted Singleton\n\n581\n\nVariation\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nSuite of Suites\n\n388\n\nVariation\n\nTest Suite Object Chapter 19, xUnit\n\nBasics Patterns\n\nSuite Fixture Setup\n\n441\n\nSuite Fixture Setup\n\nChapter 20, Fixture Setup Patterns\n\nSymbolic Constant\n\n715\n\nVariation\n\nLiteral Value\n\nChapter 27, Value Patterns\n\nTable Truncation 661 Teardown\n\nTable Truncation Chapter 25, Database Teardown\n\nPatterns\n\nTabular Test\n\n609\n\nVariation\n\nParameterized Test\n\nChapter 24, Test Organization Patterns\n\nTeardown Guard Clause\n\n511\n\nVariation\n\nIn-line Teardown Chapter 22, Fixture Teardown Patterns\n\nTemporary Test Stub\n\n530\n\nVariation\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nTest Automation 298 Framework\n\nTest Automation Chapter 18, Test Strategy Patterns Framework\n\nTest Bed\n\n429\n\nAlias\n\nPrebuilt Fixture\n\nChapter 20, Fixture Setup Patterns\n\nwww.it-ebooks.info",
      "content_length": 1634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 844,
      "content": "Name\n\nPage\n\nTest Discovery\n\n393\n\nTest Double\n\n522\n\nTest Double Class\n\n569\n\nTest Double Subclass\n\n580\n\nTest Double as Back Door\n\n332\n\nTest Enumeration 399\n\nTest Fixture\n\n373\n\nTest Fixture Registry\n\n644\n\nTest Helper\n\n643\n\nTest Helper Class\n\n645\n\nTest Helper Mixin\n\n639\n\nTest Helper Object\n\n645\n\nTest Hook\n\n709\n\nTest Method\n\n348\n\nTest Method Discovery\n\n394\n\nTest Method Enumeration\n\n401\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nTest Discovery\n\nChapter 19, xUnit Basics Patterns\n\nTest Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nVariation\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nVariation\n\nBack Door Manipulation\n\nChapter 18, Test Strategy Patterns\n\nTest Enumeration Chapter 19, xUnit\n\nBasics Patterns\n\nAlias\n\nTestcase Class\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTest Helper\n\nChapter 24, Test Organization Patterns\n\nTest Helper\n\nChapter 24, Test Organization Patterns\n\nVariation\n\nTest Helper\n\nChapter 24, Test Organization Patterns\n\nVariation\n\nTestcase Superclass\n\nChapter 24, Test Organization Patterns\n\nVariation\n\nTest Helper\n\nChapter 24, Test Organization Patterns\n\nTest Hook\n\nChapter 26, Design- for-Testability Patterns\n\nTest Method\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTest Discovery\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTest Enumeration Chapter 19, xUnit\n\nBasics Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n781\n\nPatterns, Aliases, and Variations",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 845,
      "content": "782\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nTest Method Selection\n\n404\n\nVariation\n\nTest Selection\n\nChapter 19, xUnit Basics Patterns\n\nTest Object Registry\n\n503\n\nAlias\n\nAutomated Teardown\n\nChapter 22, Fixture Teardown Patterns\n\nTest Runner\n\n377\n\nTest Runner\n\nChapter 19, xUnit Basics Patterns\n\nTest Selection\n\n403\n\nTest Selection\n\nChapter 19, xUnit Basics Patterns\n\nTest Spy\n\n538\n\nTest Spy\n\nChapter 23, Test Double Patterns\n\nTest Spy\n\n568\n\nAlias\n\nHard-Coded Test Double\n\nChapter 23, Test Double Patterns\n\nTest Stub\n\n529\n\nTest Stub\n\nChapter 23, Test Double Patterns\n\nTest Suite Enumeration\n\n400\n\nVariation\n\nTest Enumeration Chapter 19, xUnit\n\nBasics Patterns\n\nTest Suite Factory\n\n399\n\nAlias\n\nTest Enumeration Chapter 19, xUnit\n\nBasics Patterns\n\nTest Suite Object 387\n\nTest Suite Object Chapter 19, xUnit\n\nBasics Patterns\n\nTest Suite Object 293 Generator\n\nVariation\n\nData-Driven Test Chapter 18, Test Strategy Patterns\n\nTest Suite Object 293 Simulator\n\nVariation\n\nData-Driven Test Chapter 18, Test Strategy Patterns\n\nTest Suite Procedure\n\n388\n\nVariation\n\nTest Suite Object Chapter 19, xUnit\n\nBasics Patterns\n\nTest Tree Explorer\n\n380\n\nVariation\n\nTest Runner\n\nChapter 19, xUnit Basics Patterns\n\nTest Utility Method\n\n599\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nTest Utility Test\n\n603\n\nVariation\n\nTest Utility Method\n\nChapter 24, Test Organization Patterns\n\nTest-Speciﬁ c Extension\n\n579\n\nAlias\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nTest-Speciﬁ c Subclass\n\n579\n\nTest-Speciﬁ c Subclass\n\nChapter 23, Test Double Patterns\n\nwww.it-ebooks.info",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 846,
      "content": "Name\n\nTestcase Class\n\nTestcase Class Discovery\n\nTestcase Class per Method\n\nTestcase Class per User Story\n\nTestcase Class Selection\n\nTestcase Class Suite\n\nTestcase Class per Class\n\nTestcase Class per Feature\n\nTestcase Class per Fixture\n\nTestcase Object\n\nTestcase Superclass\n\nTesting by Layers\n\nThe xUnit Family\n\nTransaction Rollback Teardown\n\nTransient Fresh Fixture\n\nPage\n\n373\n\n394\n\n625\n\n625\n\n404\n\n388\n\n617\n\n624\n\n631\n\n382\n\n638\n\n337\n\n300\n\n668\n\n314\n\nPatterns, Aliases, and Variations\n\nRelationship Base Name\n\nChapter\n\nTestcase Class\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTest Discovery\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTestcase Class per Feature\n\nChapter 24, Test Organization Patterns\n\nVariation\n\nTestcase Class per Feature\n\nChapter 24, Test Organization Patterns\n\nVariation\n\nTest Selection\n\nChapter 19, xUnit Basics Patterns\n\nVariation\n\nTest Suite Object Chapter 19, xUnit\n\nBasics Patterns\n\nTestcase Class per Class\n\nChapter 24, Test Organization Patterns\n\nTestcase Class per Feature\n\nChapter 24, Test Organization Patterns\n\nTestcase Class per Fixture\n\nChapter 24, Test Organization Patterns\n\nTestcase Object\n\nChapter 19, xUnit Basics Patterns\n\nTestcase Superclass\n\nChapter 24, Test Organization Patterns\n\nAlias\n\nLayer Test\n\nChapter 18, Test Strategy Patterns\n\nVariation\n\nTest Automation Chapter 18, Test Strategy Patterns Framework\n\nTransaction Rollback Teardown\n\nChapter 25, Database Patterns\n\nVariation\n\nFresh Fixture\n\nChapter 18, Test Strategy Patterns\n\nContinued...\n\nwww.it-ebooks.info\n\n783\n\nPatterns, Aliases, and Variations",
      "content_length": 1553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 847,
      "content": "784\n\nPatterns, Aliases, and Variations\n\nAppendix G Patterns, Aliases, and Variations\n\nName\n\nPage\n\nRelationship Base Name\n\nChapter\n\nTrue Humble Object\n\n699\n\nVariation\n\nHumble Object\n\nChapter 26, Design- for-Testability Patterns\n\nUnﬁ nished Test Assertion\n\n494\n\nUnﬁ nished Test Assertion\n\nChapter 21, Result Veriﬁ cation Patterns\n\nVeriﬁ cation Method\n\n477\n\nVariation\n\nCustom Assertion Chapter 21, Result\n\nVeriﬁ cation Patterns\n\nwww.it-ebooks.info",
      "content_length": 444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 848,
      "content": "Glossary\n\nThis glossary contains the author’s deﬁ nitions of the terms used throughout this book.\n\nacceptance test\n\nA customer test that the customer of the software plans to run to help the customer decide whether he or she will accept the software system. Acceptance tests are usually run manually after all automated customer tests have passed. They exercise all layers of the system—from the user interface back to the database—and should include any integration with other systems on which the application depends.\n\naccessor\n\nA method that provides access to an instance variable of an object either by returning its value or by providing a way to set its value.\n\nACID\n\nThe four qualities of transactions that modern databases ensure:\n\nAtomic: A transaction is all or nothing.\n\nConsistent: All operations within a transaction see the same view of the\n\nworld.\n\nIndependent: Transactions are independent of one another (no cross-\n\ntransaction leakage of changes).\n\nDurable: Once committed, the changes made within a transaction are\n\npermanent (they don’t just vanish for no reason!).\n\nagile method\n\nA method of executing projects (typically, but not always, restricted to software) that reduces the cost of change and allows customers of the software to have more control over how much they spend and what they get for their money. Agile\n\n785\n\nwww.it-ebooks.info\n\nAlso known as: user acceptance test (UAT)",
      "content_length": 1408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 849,
      "content": "786\n\nGlossary\n\nmethods include eXtreme Programming, SCRUM, Feature-Driven Development (FDD), and Dynamic Systems Development Method (DSDM), among many others. A core practice of most agile methods is the use of automated unit tests.\n\nannotation\n\nA way of indicating something about something. JUnit version 4.0 uses annota- tions to indicate which classes are Testcase Classes and which methods are Test Methods; NUnit uses .NET attributes for this purpose.\n\nanonymous inner class\n\nAn inner class in Java that is deﬁ ned without a unique name. Anonymous inner classes are often used when deﬁ ning Hard-Coded Test Doubles.\n\nanti-pattern\n\nA pattern that shouldn’t be used because it is known to produce less than optimal results. Code smells, or their underlying causes, are a kind of anti-pattern.\n\napplication programming interface (API)\n\nThe means by which other software can invoke some piece of functionality. In object-oriented software, an API consists of the classes and their publicly acces- sible methods. In procedural software, it consists of the module or package name plus the publicly accessible procedures.\n\naspect-oriented programming\n\nAn advanced software modularization technique that allows improved separa- tion of concerns by “weaving” cross-cutting concerns into code after the affected software has been built but before it is executed.\n\nassertion\n\nA statement that something should be true. In xUnit-style Test Automation Frameworks, an assertion takes the form of an Assertion Method that fails when the actual outcome passed to it does not match the expected outcome.\n\nwww.it-ebooks.info",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 850,
      "content": "Glossary\n\nasynchronous test\n\nA test that runs in a separate thread of control from the system under test (SUT) and interacts with it using asynchronous (i.e., “real”) messages. An asynchro- nous test must coordinate its steps with those of the SUT because this interac- tion is not managed automatically by the runtime system. An asynchronous test may have to include delays to give the SUT enough time to ﬁ nish execution before inspecting the outcome. Contrast this with a synchronous test, which interacts with the SUT via simple method calls.\n\nattribute\n\nA characteristic of something. The members of the xUnit family for the .NET languages use class and method attributes to indicate which classes are Testcase Classes and which methods are Test Methods. The term attribute is also a syn- onym for “instance variable” in some circles.\n\nback door\n\nAn alternative interface to a system under test (SUT) that test software can use to inject indirect inputs into the SUT. A database is a common example of a back door, but it could also be any component that can be either manipulated to return test-speciﬁ c values or replaced by a Test Double. Contrast this with the front door: the application programming interface (API).\n\nBDUF\n\n“Big design up front” is the classic “waterfall” approach to software design. In BDUF, all requirements must be understood early in the project, and the software is designed to support those requirements in a single design “phase.” Contrast this with the emergent design favored by agile projects.\n\nbehavior-driven development\n\nA variation on the test-driven development process wherein the focus of the tests is to clearly describe the expected behavior of the system under test (SUT). The emphasis is on Tests as Documentation rather than merely using tests for veriﬁ cation.\n\nBehavior-driven development can be done using traditional members of the xUnit family. New “members” of the family, however, have been built speciﬁ - cally to emphasize the change in focus. They include changes in terminology\n\nwww.it-ebooks.info\n\n787",
      "content_length": 2063,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 851,
      "content": "788\n\nGlossary\n\n(e.g., “test” becomes “spec”; “ﬁ xture” becomes “context”) and more explicit framework support for clarity of the speciﬁ cation.\n\nbehavior smell\n\nA test smell we encounter while compiling or running tests. We don’t have to be particularly observant to notice behavior smells, as they will present themselves to us via compile errors or test failures. See also: code smell, project smell.\n\nblack box\n\nA piece of software that we treat as an opaque object whose internal workings cannot be seen. Tests written for the black box can verify only externally visible behavior and are independent of the implementation inside the system under test (SUT).\n\nblock\n\nA block of code that can be run. Many programming languages (most notably, Smalltalk and Ruby) use blocks (also known as “block closures”) as a way of passing a chunk of code to a method, which can then run the code in its own context. Java’s anonymous inner classes are a way to achieve the same thing without direct support for blocks. C# uses delegates for the same purpose.\n\nblock closure\n\nSee block.\n\nboundary value\n\nAn input value for a system under test (SUT) that is immediately adjacent to the boundary between two equivalence classes. Tests using two adjacent boundary values help us verify that the behavior changes with exactly the right input and that we don’t have “off by one” problems.\n\nbuilt-in self-test\n\nA means of organizing test code in which the tests live inside the same module or class as the production code and are run automatically when the system is initialized.\n\nwww.it-ebooks.info",
      "content_length": 1582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 852,
      "content": "Glossary\n\nbusiness logic\n\nThe core logic related to the domain model of a business system. Because busi- ness logic usually reﬂ ects the results of many independent business decisions, it often seems anything but logical!\n\nclass attribute\n\nAn attribute that is placed on a class in the source code to tell the compiler or runtime system that this class is “special.” In some variants of xUnit, class at- tributes are used to indicate that a class is a Testcase Class.\n\nclass method\n\nA method that is associated with a class rather than an object. Class methods can be invoked using a classname.methodname notation [e.g., Assert.assertEquals(message, expected, actual);] and do not require an instance of the class to be invoked. Class methods cannot access instance methods or instance variables of objects; that is, they do not have access to self or this. In Java, a class method is called a static method. Other languages may use different names or keywords.\n\nclass variable\n\nA variable that is associated with a class rather than an instance of the class and is typically used to access information that all instances need to share. In some languages, class variables can be accessed using the syntax classname.variable- name (e.g., TestHelper.lineFeedCharacter;). That is, they do not need to be accessed via self or this. In Java, a class variable is called a static variable. Other lan- guages may use different names or keywords.\n\nclosure\n\nSee block.\n\ncode smell\n\nThe “classic” bad smell, as ﬁ rst described by Martin Fowler in [Ref]. Test au- tomaters must recognize code smells that arise as they maintain test code. Code smells typically affect maintenance cost of tests but may also be early warning signs of behavior smells to follow.\n\nSee also: test smell, behavior smell, project smell.\n\nwww.it-ebooks.info\n\n789",
      "content_length": 1826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 853,
      "content": "790\n\nGlossary\n\ncomponent\n\nA larger part of the overall system that is often separately deployable. Component- based development involves decomposing the overall functionality into a series of individual components that can be built and deployed separately. This allows sharing of the components between applications that need the same functionality. Each component is a consequence of one or more design decisions, although its behavior may also be traced back to some aspect of the requirements.\n\nComponents can take many forms, depending on the technology being employed. The Windows platform uses dynamic linked libraries (DLLs) or assemblies as components. The Java platform uses Java Archives (JARs). A service-oriented architecture (SOA) uses Web Services as its large-grained components. The components may implement front-end logic (e.g., a “File Open Dialog”) or back-end logic (e.g., a “Customer Persistence” component). A component can and should be veriﬁ ed using component tests before the overall application is tested using customer tests.\n\ncomponent test\n\nA test that veriﬁ es the behavior of some component of the overall system. The component is a consequence of one or more design decisions, although its be- havior may also be traced back to some aspect of the requirements. There is no need for component tests to be readable, recognizable, or veriﬁ able by the customer or business domain expert. Contrast this with a customer test, which is derived almost entirely from the requirements and should be veriﬁ able by the customer, and with a unit test, which veriﬁ es a much smaller component. A component test lies somewhere in between these two extremes.\n\nDuring test-driven development, component tests are written after the cus- tomer tests are written and the overall design is solidiﬁ ed. They are written as the architectural decisions are made but before the individual units are designed or coded. They are usually automated using a member of the xUnit family.\n\nconstructor\n\nA special method used in some object-oriented programming languages to con- struct a brand-new object. It often has the same name as the class and is typically called automatically by the runtime system whenever the special operation new is invoked. A Complete Constructor Method [SBPP] returns a ready-to-use object that requires no additional tweaking; this usually implies arguments must be passed to the constructor.\n\nwww.it-ebooks.info",
      "content_length": 2445,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 854,
      "content": "Glossary\n\ncontinuous integration\n\nThe agile software development practice of integrating software changes continu- ously. In practice, developers typically integrate their changes every few hours to days. Continuous integration often includes the practice of an automated build that is triggered by each check-in. The build process typically runs all automated tests and may even run tests that aren’t run before check-in because they take too long. The build is considered to have “failed” if any tests fail. When the build fails, teams typically consider getting the build working again to be the top priority; only code changes aimed at ﬁ xing the build are allowed until a successful build has occurred.\n\ncontrol point\n\nHow the test asks the system under test (SUT) to do something for it. A control point could be created for the purpose of setting up or tearing down the ﬁ xture or it could be used during the exercise SUT phase of the test. It is a kind of in- teraction point. Some control points are provided strictly for testing purposes; they should not be used by the production code because they bypass input validation or short-circuit the normal life cycle of the SUT or some object on which it depends.\n\ncustomer test\n\nA test that veriﬁ es the behavior of a slice of the visible functionality of the over- all system. The system under test (SUT) may consist of the entire system or a fully functional top-to-bottom slice (“module”) of the system. A customer test should be independent of the design decisions made while building the SUT. That is, we should require the same set of customer tests regardless of how we choose to build the SUT. (Of course, how the customer tests interact with the SUT may be affected by high-level software architecture decisions.)\n\ndata access layer\n\nA way of keeping data access logic from permeating the application code by put- ting it into a separate component that encapsulates the database.\n\ndepended-on component (DOC)\n\nAn individual class or a large-grained component on which the system under test (SUT) depends. The dependency is usually one of delegation via method\n\nwww.it-ebooks.info\n\n791\n\nAlso known as: data access object (DAO), data abstraction layer (DAL)",
      "content_length": 2220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 855,
      "content": "792\n\nAlso known as: DfT\n\nGlossary\n\ncalls. In test automation, the DOC is primarily of interest in that we need to be able to observe and control its interactions with the SUT to get complete test coverage.\n\ndesign pattern\n\nA pattern that we can use to solve a particular software design problem. Most design patterns are programming language independent; the language-speciﬁ c ones are typically called “coding idioms.” Design patterns were ﬁ rst popularized by the book Design Patterns [GOF].\n\ndesign for testability\n\nA way of ensuring that code is easily tested by making sure that testing require- ments are considered as the code is designed. When doing test-driven develop- ment, design for testability occurs as a natural side effect of development\n\ndeveloper test\n\nAnother name for an automated unit test that is prepared by someone playing the developer role on an eXtreme Programming project.\n\nDfT\n\nSee design for testability.\n\ndirect input\n\nA test may interact with the system under test (SUT) directly via its “front door” or public application programming interface (API) or indirectly via its “back door.” The stimuli injected by the test into the SUT via its front door are direct inputs of the SUT. Direct inputs may consist of method or function calls to an- other component or messages sent on a message channel (e.g., MQ or JMS) and the arguments or contents thereof.\n\ndirect output\n\nA test may interact with the system under test (SUT) directly via its “front door” or public application programming interface (API) or indirectly via its “back door.” The responses received by the test from the SUT via its front door are\n\nwww.it-ebooks.info",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 856,
      "content": "Glossary\n\ndirect outputs of the SUT. Direct outputs may consist of the return values of method or function calls, updated arguments passed by reference, exceptions raised by the SUT, or messages received on a message channel (e.g., MQ or JMS) from the SUT.\n\ndocument-driven development\n\nA development process that focuses on producing documents that describe how the code will be structured and then coding from those documents. Docu- ment-driven development is normally associated with “big design up front” (BDUF, also known as “waterfall”) software development. Contrast this with test-driven development, which focuses on producing working code one test at a time.\n\ndomain layer\n\nThe layer of a Layered Architecture [DDD, PEAA, WWW] that corresponds to the domain model. See Eric Evans’ book, Domain-Driven Design [DDD].\n\ndomain model\n\nA model of the problem domain that may form the basis of the object model in the business domain layer of a software application. See Eric Evans’ book, Domain-Driven Design [DDD].\n\nDTO\n\nShort for the Data Transfer Object [CJ2EEP] design pattern.\n\ndynamic binding\n\nDeferring the decision about which piece of software to transfer control to until execution time. The same method name can be used to invoke different behavior (method bodies) based on the class of the object on which it is invoked; the latter class is determined only at execution time. Dynamic binding is the opposite of static binding; it is also called polymorphism (from the Latin, meaning “taking on many shapes”).\n\nEDD\n\nSee example-driven development.\n\nwww.it-ebooks.info\n\n793",
      "content_length": 1587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 857,
      "content": "794\n\nAlso known as: domain object\n\nGlossary\n\nemergent design\n\nThe opposite of BDUF (big design up front). Emergent design involves letting the right design be discovered as the software slowly evolves to pass one test at a time during test-driven development.\n\nendoscopic testing\n\nA testing technique pioneered by the authors of the original Mock Object paper [ET], which involves testing software from the inside.\n\nentity object\n\nAn object that represents an entity concept from a domain. Entity objects typi- cally have a life cycle that is represented as their state. Contrast this with a service object, which has no single state. EJB Entity Beans are one example of an entity object.\n\nequivalence class\n\nA test condition identiﬁ cation technique that reduces the number of tests required by grouping together inputs that should result in the same output or that should exercise the same logic in the system. This organization allows us to focus our tests on key boundary values at which the expected output changes.\n\nexample-driven development (EDD)\n\nA reframing of the test-driven development process to focus on the “executable speciﬁ cation” aspect of the tests. The act of providing examples is more intuitive to many people; it doesn’t carry the baggage of “testing” software that doesn’t yet exist.\n\nexercise SUT\n\nAfter the ﬁ xture setup phase of testing, the test stimulates the system under test (SUT) logic that is to be tested. This phase of the testing process is called exercise SUT.\n\nwww.it-ebooks.info",
      "content_length": 1520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 858,
      "content": "Glossary\n\nexpectation\n\nWhat a test expects the system under test (SUT) to have done. When we are using Mock Objects to verify the indirect outputs of the SUT, we load each Mock Object with the expected method calls (including the expected argu- ments); these are called the expectations.\n\nexpected outcome\n\nThe outcome that we verify after exercising the system under test (SUT). A Self-Checking Test veriﬁ es the expected outcome using calls to Assertion Methods.\n\nexploratory testing\n\nInteractive testing of an application without a speciﬁ c script in hand. The tester “explores” the system, making up theories about how it should behave based on what the application has already done and then testing those theories to see if they hold up. While there is no rigid plan, exploratory testing is a disciplined activity that is more likely to ﬁ nd real bugs than rigidly scripted tests.\n\neXtreme Programming\n\nAn agile software development methodology that showcases pair programming, automated unit testing, and short iterations.\n\nfactory\n\nA method, object, or class that exists to build other objects.\n\nfalse negative\n\nA situation in which a test passes even though the system under test (SUT) is not working properly. Such a test is said to give a false-negative indication or a “false pass.”\n\nSee also: false positive.\n\nfalse positive\n\nA situation in which a test fails even though the system under test (SUT) is working properly. Such a test is said to give a false-positive indication or a\n\nwww.it-ebooks.info\n\n795",
      "content_length": 1518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 859,
      "content": "796\n\nGlossary\n\n“false failure.” The terminology comes from statistical science and relates to our attempt to calculate the probability of some observation error occurring. For example, in medicine we run tests to ﬁ nd out if a medical condition is pres- ent; if it is, the test is “positive.” It is useful to know the probability that a test might indicate that a condition (such as diabetes) is present when it is not—that is, a false “positive.” If we think of software tests as a way of determining whether a condition (a particular defect or bug) is present, a test that reports a defect (a test failure or error) when it is not, in fact, present is giving us a false positive.\n\nSee also: false negative. Wikipedia [Wp] has an extensive description under\n\nthe topic “Type I and type II errors.”\n\nfault insertion test\n\nA kind of test in which a deliberate fault is introduced in one part of the sys- tem to verify that another part reacts to the error appropriately. Initially, the faults were related to hardware but the same concept is now applied to software faults as well. Replacing a depended-on component (DOC) with a Saboteur that throws an exception is an example of a software fault insertion test.\n\nfeature\n\nA testable unit of functionality that can be built onto the evolving software sys- tem. In eXtreme Programming, a user story corresponds roughly to a feature.\n\nFit test\n\nA test that uses the Fit testing framework; most commonly a customer test.\n\nﬁ xture\n\nSee test ﬁ xture (disambiguation).\n\nﬁ xture (Fit)\n\nIn Fit, the Adapter [GOF] that interprets the Fit table and invokes methods on the system under test (SUT), thereby implementing a Data-Driven Test. For meanings in other contexts, see test ﬁ xture (disambiguation), test ﬁ xture (in xUnit), and test ﬁ xture (in NUnit).\n\nwww.it-ebooks.info",
      "content_length": 1817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 860,
      "content": "Glossary\n\nﬁ xture holding class variable\n\nA class variable of a Testcase Class that is used to hold a reference to the test ﬁ xture. It typically holds a reference to a Shared Fixture.\n\nﬁ xture holding instance variable\n\nAn instance variable of a Testcase Object that is used to hold a reference to the test ﬁ xture. It typically holds a reference to a Fresh Fixture that is set up using Implicit Setup.\n\nﬁ xture holding local variable\n\nA local variable of a Test Method that is used to hold a reference to the test ﬁ x- ture. It typically holds a reference to a Fresh Fixture that is set up within the test method using In-line Setup or returned from Delegated Setup.\n\nﬁ xture setup\n\nBefore the desired logic of the system under test (SUT) can be exercised, the pre- conditions of the test need to be set up. Collectively, all objects (and their states) are called the test ﬁ xture (or test context), and the phase of the test that sets up the test ﬁ xture is called ﬁ xture setup.\n\nﬁ xture teardown\n\nAfter a test is run, the test ﬁ xture that was built by the test should be destroyed. This phase of the test is called ﬁ xture teardown.\n\nﬂ uent interface\n\nA style of object constructor API that results in easy-to-understand statements. The Conﬁ guration Interface provided by the Mock Object toolkit JMock is an example of a ﬂ uent interface.\n\nfront door\n\nThe public application programming interface (API) of a piece of software. Con- trast this with the back door.\n\nwww.it-ebooks.info\n\n797",
      "content_length": 1494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 861,
      "content": "798\n\nAlso known as: procedure variable, delegate in .NET languages\n\nGlossary\n\nfunction pointer\n\nFrom Wikipedia [Wp]: “A function pointer is a type of pointer in C, C++, D, and other C-like programming languages. When dereferenced, a function pointer in- vokes a function, passing it zero or more arguments like a normal function.”\n\nfunctional test (common usage)\n\nA black-box test of the end-user functionality of an application. The agile com- munity is trying to avoid this usage of “functional test” because of the potential for confusion when talking about verifying functional (as opposed to nonfunctional or extra-functional properties) properties of a unit or component. This book uses the terms “customer test” and “acceptance test” for a functional test of the entire appli- cation and “unit test” for a functional test of an individual unit of the application.\n\nfunctional test (contrast with extra-functional test)\n\nA test that veriﬁ es the functionality implemented by a piece of software. De- pending on the scope of the software, a functional test may be a customer test, a unit test, or a component test.\n\nIn some circles a functional test is a customer test. This usage becomes con- fusing, however, when we talk about testing nonfunctional or extra-functional properties of the system under test (SUT). This book uses the terms “customer test” and “acceptance test” for a functional test of the entire application and “unit test” for a functional test of an individual unit of the application.\n\ngarbage collection\n\nA mechanism that automatically recovers the memory used by any objects that are no longer accessible. Many modern object-oriented programming environ- ments provide garbage collection.\n\nglobal variable\n\nA variable that is global to a whole program. A global variable is accessible from anywhere within the program and never goes out of scope, although the memory to which it refers can be deallocated explicitly.\n\ngreen bar\n\nMany Graphical Test Runners portray the progress of the test run using a prog- ress bar. As long as all tests have passed, the bar stays green. When any tests fail, the indicator changes to a red bar.\n\nwww.it-ebooks.info",
      "content_length": 2177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 862,
      "content": "Glossary\n\nGUI\n\nGraphical user interface.\n\nhappy path\n\nThe “normal” path of execution through a use case or through the software that implements it; also known as the “sunny day” scenario. Nothing goes wrong, nothing out of the ordinary happens, and we swiftly and directly achieve the user’s or caller’s goal.\n\nHollywood principle\n\nWhat directors in Hollywood tell aspiring actors at mass-casting calls: “Don’t call us; we’ll call you (if we want you).” In software, this concept is often called inversion of control (IOC).\n\nIDE\n\nIntegrated development environment. An environment that provides tools to edit, compile, execute, and (typically) test code within a single development tool.\n\nincremental delivery\n\nA method of building and deploying a software system in stages and releasing the software as each stage, called an “increment,” is completed. This approach results in earlier delivery to the user of a working system, where the capabilities of the system increase over time. In agile methods, the increment of functionality is the feature or user story. Incremental delivery goes beyond iterative develop- ment and incremental development, however, by actually putting the functional- ity into production on a regular basis. This idea is summarized by the following mantra: “Deliver early, deliver often.”\n\nincremental development\n\nA method of building a software system in stages such that the functionality built to date can be tested before the next stage is started. This approach allows for the earlier delivery to the user of a working system, where the capabilities of the system increase over time (see incremental delivery). In agile methods, the incre- ment of functionality is the feature or user story. Incremental development goes beyond iterative development, however, in that it promises to produce working,\n\nwww.it-ebooks.info\n\n799",
      "content_length": 1857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 863,
      "content": "800\n\nAlso known as: outgoing interface\n\nAlso known as: member variable\n\nGlossary\n\ntestable, and potentially deployable software with every iteration. With incre- mental delivery, we also promise to “Deliver early, deliver often.”\n\nindirect input\n\nWhen the behavior of the system under test (SUT) is affected by the values returned by another component whose services it uses, we call those values the indirect in- puts of the SUT. Indirect inputs may consist of actual return values of functions, updated (out) parameters of procedures or subroutines, and any errors or excep- tions raised by the depended-on component (DOC). Testing of the SUT behavior with indirect inputs requires the appropriate control point on the “back side” of the SUT. We often use a Test Stub to inject the indirect inputs into the SUT.\n\nindirect output\n\nWhen the behavior of the system under test (SUT) includes actions that cannot be observed through the public application programming interface (API) of the SUT but that are seen or experienced by other systems or application compo- nents, we call those actions the indirect outputs of the SUT. Indirect outputs may consist of method or function calls to another component, messages sent on a message channel (e.g., MQ or JMS), and records inserted into a database or written to a ﬁ le. Veriﬁ cation of the indirect output behaviors of the SUT requires the use of appropriate observation points on the “back side” of the SUT. Mock Objects are often used to implement the observation point by intercepting the indirect outputs of the SUT and comparing them to the expected values.\n\nSee also: outgoing interface.\n\ninner class\n\nA class in Java that is deﬁ ned inside another class. Anonymous inner classes are deﬁ ned inside a method, whereas inner classes are deﬁ ned outside a method. In- ner classes are often used when deﬁ ning Hard-Coded Test Doubles.\n\ninstance method\n\nA method that is associated with an object rather than the class of the object. An instance method is accessible only from within or via an instance of the class. It is typically used to access information that is expected to differ from one instance to another.\n\nwww.it-ebooks.info",
      "content_length": 2185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 864,
      "content": "Glossary\n\nThe exact syntax used to access an instance method varies from language to language. The most common syntax is objectReference.methodName(). When referenced from within other methods on the object, some languages require an explicit reference to the object (e.g., this.methodName() or self methodName); other languages simply assume that any unqualiﬁ ed references to methods are references to instance methods.\n\ninstance variable\n\nA variable that is associated with an object rather than the class of object. An instance variable is accessible only from within or via an instance of the class. It is typically used to access information that is expected to differ from one instance to another.\n\ninteraction point\n\nA point at which a test interacts with the system under test (SUT). An interac- tion point can be either a control point or an observation point.\n\ninterface\n\nIn general, a fully abstract class that deﬁ nes only the public methods that all im- plementers of the interface must provide. In Java, an interface is a type deﬁ nition that does not provide any implementation. In most single-inheritance languages, a class may implement any number of interfaces, even though it can extend (subclass) only one other class.\n\ninversion of control (IOC)\n\nA control paradigm that distinguishes software frameworks from “toolkits” or components. The framework calls the software plug-in (rather than the reverse). In the real world, inversion of control is often called the Hollywood principle. With the advent of automated unit testing, a class of framework known as an inversion of control framework has sprung up speciﬁ cally to simplify the re- placement of depended-on components (DOCs) with Test Doubles.\n\nIOC\n\nSee inversion of control.\n\nwww.it-ebooks.info\n\n801\n\nAlso known as: member function",
      "content_length": 1811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 865,
      "content": "802\n\nAlso known as: wetware, mushware\n\nGlossary\n\niterative development\n\nA method of building a software system using time-boxed “iterations.” Each iteration is planned and then executed. At the end of the “time box,” the status of all the work is reviewed and the next iteration is planned. The strict time- boxing prevents “runaway development,” where the state of the system is never assessed because nothing is ever ﬁ nished. Unlike incremental development, itera- tive development does not require working software to be delivered at the end of each iteration.\n\nlayer-crossing test\n\nA test that either sets up the ﬁ xture or veriﬁ es the outcome using Back Door Manipulation, which involves using a “back door” of the system under test (SUT) such as a database. Contrast this with a round-trip test.\n\nlegacy software\n\nIn the test-driven development community, any software that does not have a Safety Net of Fully Automated Tests.\n\nliveware\n\nThe people who use our software. They are usually assumed to be much more intelligent than either the software or the hardware but they can also be rather unpredictable.\n\nlocal variable\n\nA variable that is associated with a block of code rather than an object or class. A local variable is accessible only from within the code block; it goes out of scope when the block of code returns to its caller.\n\nmanual test\n\nA test that is executed by a person interacting with the system under test (SUT). The user may be following some sort of “test script” (not to be confused with a Scripted Test) or doing ad hoc or exploratory testing.\n\nwww.it-ebooks.info",
      "content_length": 1597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 866,
      "content": "Glossary\n\nmeta object\n\nAn object that holds data that controls the behavior of another object. A meta object protocol is the interface by which the meta object is constructed or con- ﬁ gured.\n\nmetatest\n\nA test that veriﬁ es the behavior of one or more tests. Such a test is mostly used during test-driven development, when we are writing tests as examples or course material and we want to ensure that tests are, indeed, failing to illustrate a par- ticular problem.\n\nmethod attribute\n\nAn attribute that is placed on a method in the source code to tell the compiler or runtime system that this method is “special.” In some xUnit family members, method attributes are used to indicate that a method is a Test Method.\n\nmixin\n\nFunctionality intended to be inherited by another class as part of that class’s implementation without implying specialization (“kind of” relationship) of the providing class.\n\n“The term mixin comes from an ice cream store in Somerville, Massachu- setts, where candies and cakes were mixed into the basic ice cream ﬂ avors. This seemed like a good metaphor to some of the object-oriented programmers who used to take a summer break there, especially while working with the object- oriented programming language SCOOPS” (SAMS Teach Yourself C++ in 21 Days, 4th ed., p. 458).\n\nmodule\n\nIn legacy programming environments (and probably a few current ones, too): An independently compilable unit of source code (e.g., the “ﬁ le I/O module”) that is later linked into the ﬁ nal executable. Unlike a component, this kind of module is typically not independently deployable. It may or may not have a cor- responding set of unit tests or component tests.\n\nWhen describing the functionality of a software system or application: A complete vertical chunk of the application that provides a particular piece of functionality (e.g., the “Customer Management Module”) that can be used\n\nwww.it-ebooks.info\n\n803",
      "content_length": 1919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 867,
      "content": "804\n\nGlossary\n\nsomewhat independently of the other modules. It would have a corresponding set of acceptance tests and may be the unit of incremental delivery.\n\nneed-driven development\n\nA variation on the test-driven development process where code is written from the outside in and all depended-on code is replaced by Mock Objects that verify the expected indirect outputs of the code being written. This approach ensures that the responsibilities of each software unit are well understood before they are coded, by virtue of having unit tests inspired by examples of real usage. The outermost layer of software is written using storytest-driven development. It should have examples of usage by real clients (e.g., a user interface driving the Service Facade [CJ2EEP]) in addition to the customer tests.\n\nobject-relational mapping (ORM)\n\nA middleware component that translates between the object-oriented domain model of an application and the table-oriented view presented by a relational database management system.\n\nobservation point\n\nThe means by which the test observes the behavior of the system under test (SUT). This kind of interaction point can be used to inspect the post-exercise state of the SUT or to monitor interactions between the SUT and its depended- on components. Some observation points are provided strictly for the tests; they should not be used by the production code because they may expose private implementation details of the SUT that cannot be depended on not to change.\n\nORM\n\nSee object-relational mapping.\n\noutgoing interface\n\nA component (e.g., a class or a collection of classes) often depends on other components to implement its behavior. The interfaces it uses to access these components are known as outgoing interfaces, and the inputs and outputs trans- mitted via test interfaces are called indirect inputs and indirect outputs. Outgoing interfaces may consist of method or function calls to another component, mes- sages sent on a message channel (e.g., MQ or JMS), or records inserted into a\n\nwww.it-ebooks.info",
      "content_length": 2053,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 868,
      "content": "Glossary\n\ndatabase or written to a ﬁ le. Testing the behavior of the system under test (SUT) with outgoing interfaces requires special techniques such as Mock Objects to intercept and verify the usage of outgoing interfaces.\n\npattern\n\nA solution to a recurring problem. A pattern has a context in which it is typically applied and forces that help you choose one pattern over another based on that context. Design patterns are a particular kind of pattern. Organizational pat- terns are not discussed in this book.\n\npattern language\n\nA collection of patterns that work together to lead the reader from a very high- level problem to a very detailed solution customized for his or her particular context. When a pattern language achieves this goal, it is said to be “genera- tive”; this characteristic differentiates a pattern language from a simple collec- tion of patterns. Refer to “A Pattern Language for Pattern Writing” [APLfPW] to learn more about how to write a pattern language.\n\npolymorphism\n\nDynamic binding. The word is derived from the Latin, meaning “taking on many shapes.”\n\npresentation layer\n\nThe part of a Layered Architecture [DDD, PEAA, WWW] that contains the presentation logic.\n\npresentation logic\n\nThe logic embedded in the presentation layer of a business system. It decides which screen to show, which items to put on menus, which items or buttons to enable or disable, and so on.\n\nprocedure variable\n\nA variable that refers to a procedure or function rather than a piece of data. It allows the code to be called to be determined at runtime (dynamic binding) rather than at compile time. The actual procedure to be invoked is assigned to\n\nwww.it-ebooks.info\n\n805\n\nAlso known as: function pointer, delegate (in .NET languages)",
      "content_length": 1748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 869,
      "content": "806\n\nAlso known as: pull system\n\nGlossary\n\nthe variable either during program initialization or during execution. Procedure variables were a precursor to true object-oriented programming languages (OOPLs). Early OOPLs such as C++ were built by using tables (arrays) of data structures containing procedure variables to implement the method (member function) dispatch tables for classes.\n\nproduction\n\nIn IT shops, the environment in which applications being used by real users run. This environment is distinguished from the various testing environments, such as “acceptance,” “integration,” “development,” and “qual” (short for “quality assessment or assurance”).\n\nproduction code\n\nIn IT shops, the environment in which applications run is often called produc- tion. Production code is the code that we are writing for eventual deployment to this environment, whether the code is to be shipped in a product or deployed into “production.” Compare to “test code.”\n\nprogrammer test\n\nA developer test.\n\nproject smell\n\nA symptom that something has gone wrong on the project. Its underlying root cause is likely to be one or more code smells or behavior smells. Because project managers rarely run or write tests, project smells are likely the ﬁ rst hint they have that something may be less than perfect in test automation land.\n\npull\n\nA concept from lean manufacturing that states that things should be produced only once a real demand for them exists. In a “pull system,” upstream (i.e., subcomponent) assembly lines produce only enough products to replace the items withdrawn from the pool that buffers them from the downstream assem- bly lines. In software development, this idea can be translated as follows: “We should only write methods that have already been called by other software and only handle those cases that the other software actually needs.” This approach avoids speculation and the writing of unnecessary software, which is one of\n\nwww.it-ebooks.info",
      "content_length": 1965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 870,
      "content": "Glossary\n\nsoftware development’s key forms of inventory (which is considered waste in lean systems).\n\nred bar\n\nMany Graphical Test Runners portray the progress of the test run using a prog- ress bar that starts off green in color. When any tests fail, this indicator changes to a red bar.\n\nrefactoring\n\nChanging the structure of existing code without changing its behavior. Refactor- ing is used to improve the design of existing code, often as a ﬁ rst step before add- ing new functionality. The authoritative source for information on refactoring is Martin Fowler’s book [Ref].\n\nreﬂ ection\n\nThe ability of a software program to examine its own structure as it is executing. Reﬂ ection is often used in software development tools to facilitate adding new capabilities.\n\nregression test\n\nA test that veriﬁ es that the behavior of a system under test (SUT) has not changed. Most regression tests are originally written as either unit tests or ac- ceptance tests, but are subsequently included in the regression test suite to keep that functionality from being accidentally changed.\n\nresult veriﬁ cation\n\nAfter the exercise SUT phase of the Four-Phase Test, the test veriﬁ es that the expected (correct) outcome has actually occurred. This phase of the test is called result veriﬁ cation.\n\nretrospective\n\nA process whereby a team reviews its processes and performance for the pur- pose of identifying better ways of working. Retrospectives are often conducted at the end of a project (called a project retrospective) to collect data and make recommendations for future projects. They have more impact if they are done\n\nwww.it-ebooks.info\n\n807\n\nAlso known as: postmortem, postpartum",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 871,
      "content": "808\n\nAlso known as: service component\n\nGlossary\n\nregularly during a project. Agile projects tend to do retrospectives after at least every release (called a release retrospective) and often after every iteration (called an iteration retrospective.)\n\nroot cause analysis\n\nA process wherein the cause of a failure or bug is traced back to all possible contributing factors. A root cause analysis helps us avoid treating symptoms by identifying the true sources of our problems. A number of techniques for doing root cause analysis exist, including Toyota’s “ﬁ ve why’s” [TPS].\n\nround-trip test\n\nA test that interacts only via the “front door” (public interface) of the system under test (SUT). Compare with layer-crossing test.\n\nservice object\n\nAn object that provides a service to other objects. Service objects typically do not have a life cycle of their own; any state they do contain tends to be an aggre- gate of the states of the entity objects that they vend. The interface of a service object is often deﬁ ned via a Service Facade [CJ2EEP] class. EJB Session Beans are one example of a service object.\n\nsetter\n\nA method provided by an object speciﬁ cally to set the value of one of its attri- butes. By convention, it either has the same name as the attribute or its name includes the preﬁ x “set” (e.g., setName).\n\nsmell\n\nA symptom of a problem. A smell doesn’t necessarily tell us what is wrong, be- cause it may have several possible causes. A smell must pass the “sniffability test”—that is, it must grab us by the nose and say, “Something is wrong here.” To ﬁ gure out exactly what the smell means, we must perform root cause analysis.\n\nWe classify smells based on where we ﬁ nd them. The most common kinds are (production) code smells, test smells, and project smells. Test smells may be either (test) code smells or behavior smells.\n\nwww.it-ebooks.info",
      "content_length": 1865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 872,
      "content": "Glossary\n\nspike\n\nIn agile methods such as eXtreme Programming, a time-boxed experiment used to obtain enough information to estimate the effort required to implement a new kind of functionality.\n\nstateless\n\nAn object that does not maintain any state between invocations of its opera- tions. That is, each request is self-contained and does not require that the same server object be used for a series of requests.\n\nstatic binding\n\nResolving exactly which piece of software we will transfer control to at compile time. Static binding is the opposite of dynamic binding.\n\nstatic method\n\nIn Java, a method that the compiler resolves at compile time (rather than at run- time using dynamic binding). This behavior is the opposite of dynamic (or virtual in C++). A static method is also a class method because only class methods can be resolved at compile time in Java. A static method is not necessarily a class method in all languages, however. For example:\n\nAssert.assertEquals(message, expected, actual);\n\nstatic variable\n\nIn Java, a variable (ﬁ eld) that the compiler resolves at compile time rather than at runtime using dynamic binding. A static variable is also a class variable be- cause only class variables can be resolved at compile time in Java. Being static (i.e., not dynamic) does not necessarily imply that something is associated with a class (rather than an instance) in all languages.\n\nSTDD\n\nSee storytest-driven development.\n\nstory\n\nSee user story.\n\nwww.it-ebooks.info\n\n809",
      "content_length": 1489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 873,
      "content": "810\n\nAlso known as: AUT, CUT, MUT, OUT\n\nGlossary\n\nstorytest\n\nA customer test that is the “conﬁ rmation” part of the user story “trilogy”: card, conversation, conﬁ rmation [XPC]. When the storytests are written before any software is developed, we call the process storytest-driven development.\n\nstorytest-driven development (STDD)\n\nA variation of the test-driven development process that entails writing (and usually automating) customer tests before the development of the correspond- ing functionality begins. This approach ensures that integration of the various software units veriﬁ ed by the unit tests results in a usable whole. The term “storytest-driven development” was ﬁ rst coined by Joshua Kerievsky as part of his methodology “Industrial XP” [IXP].\n\nSTTCPW\n\n“The simplest thing that could possibly work.” This approach is commonly used on XP projects when someone is over-engineering the software by trying to anticipate future requirements.\n\nsubstitutable dependency\n\nA software component may depend on any number of other components. If we are to test this component by itself, we must be able to replace the other components with Test Doubles—that is, each component must be a substitutable dependency. We can turn something into a substitutable dependency in several ways, including Dependency Injection, Dependency Lookup, and Test-Speciﬁ c Subclass.\n\nsynchronous test\n\nA test that interacts with the system under test (SUT) using normal (synchronous) method calls that return the results that the test will make assertions against. A synchronous test does not need to coordinate its steps with those of the SUT; this activity is managed automatically by the runtime system. Contrast this with an asynchronous test, which runs in a separate thread of control from the SUT.\n\nsystem under test (SUT)\n\nWhatever thing we are testing. The SUT is always deﬁ ned from the perspective of the test. When we are writing unit tests, the SUT is whatever class (also known\n\nwww.it-ebooks.info",
      "content_length": 1997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 874,
      "content": "Glossary\n\nas CUT), object (also known as OUT), or method (also known as MUT) we are testing; when we are writing customer tests, the SUT is probably the entire appli- cation (also known as AUT) or at least a major subsystem of it. The parts of the application that we are not verifying in this particular test may still be involved as a depended-on component (DOC).\n\ntask\n\nThe unit of work assignment (or volunteering) in eXtreme Programming. One or more tasks may be involved in delivering a user story (a feature).\n\nTDD\n\nSee test-driven development.\n\ntest\n\nA procedure, whether manually executed or automated, that can be used to verify that the system under test (SUT) is behaving as expected. Often called a test case.\n\ntest automater\n\nThe person or project role that is responsible for building the tests. Sometimes a “subject matter expert” may be responsible for coming up with the tests to be automated by the test automater.\n\ntest case\n\nUsually a synonym for “test.” In xUnit, it may also refer to a Testcase Class, which is actually a Test Suite Factory as well as a place to put a set of related Test Methods.\n\ntest code\n\nCode written speciﬁ cally to test other code (either production or other test code).\n\ntest condition\n\nA particular behavior of the system under test (SUT) that we need to verify. It can be described as the following collection of points:\n\nwww.it-ebooks.info\n\n811",
      "content_length": 1395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 875,
      "content": "812\n\nGlossary\n\nIf the SUT is in some state S1, and\n\nWe exercise the SUT in some way X, then\n\nThe SUT should respond with R and\n\nThe SUT should be in state S2.\n\ntest context\n\nEverything a system under test (SUT) needs to have in place so that we can ex- ercise the SUT for the purpose of verifying its behavior. For this reason, RSpec calls the test ﬁ xture (as used in xUnit) a “context.”\n\nContext: a set fruits with contents = {apple, orange, pear} Exercise: remove orange from the fruits set Verify: fruits set contents = {apple, pear}\n\nIn this example, the ﬁ xture consists of a single set and is created directly in the test. How we choose to construct the ﬁ xture has very far-reaching ramiﬁ cations for all aspects of test writing and maintenance.\n\ntest database\n\nA database instance that is used primarily for the execution of tests. It should not be the same database as is used in production!\n\ntest debt\n\nI ﬁ rst became aware of the concept of various kinds of debts via the Industrial XP mailing list on the Internet. The concept of “debt” is a metaphor for “not doing enough of” something. To get out of debt, we must put extra effort into the something we were not doing enough of. Test debt arises when we do not write all of the necessary tests. As a result, we have “unprotected code” in that the code could break without causing any tests to fail.\n\ntest-driven bug ﬁ xing\n\nA way of ﬁ xing bugs that entails writing and automating unit tests that reproduce each bug before we begin debugging the code and ﬁ xing the bug; the bug-ﬁ xing extension of test-driven development.\n\nwww.it-ebooks.info",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 876,
      "content": "Glossary\n\ntest-driven development (TDD)\n\nA development process that entails writing and automating unit tests before the development of the corresponding units begins. TDD ensures that the responsi- bilities of each software unit are well understood before they are coded. Unlike test-ﬁ rst development, test-driven development is typically meant to imply that the production code is made to work one test at a time (a characteristic called emergent design).\n\nSee also: storytest-driven development.\n\ntest driver\n\nA person doing test-driven development.\n\ntest driving\n\nThe act of doing test-driven development.\n\ntest error\n\nWhen a test is run, an error that keeps the test from running to completion. The error may be explicitly raised or thrown by the system under test (SUT) or by the test itself, or it may be thrown by the runtime system (e.g., operating system, virtual machine). In general, it is much easier to debug a test error than a test failure because the cause of the problem tends to be much more local to where the test error occurs. Compare with test failure and test success.\n\ntest failure\n\nWhen a test is run and the actual outcome does not match the expected out- come. Compare with test error and test success.\n\ntest-ﬁ rst development\n\nA development process that entails writing and automating unit tests before the development of the corresponding units begins. Test-ﬁ rst development ensures that the responsibilities of each software unit are well understood before that unit is coded. Unlike test-driven development, test-ﬁ rst development merely says that the tests are written before the production code; it does not imply that the production code is made to work one test at a time (emergent design). Test-ﬁ rst\n\nwww.it-ebooks.info\n\n813",
      "content_length": 1764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 877,
      "content": "814\n\nAlso known as: Testcase Class\n\nAlso known as: test context\n\nGlossary\n\ndevelopment may be applied at the unit test or customer test level, depending on which tests we have chosen to automate.\n\ntest ﬁ xture (disambiguation)\n\nIn generic xUnit: All the things we need to have in place to run a test and expect a particular outcome. The test ﬁ xture comprises the pre-conditions of the test; that is, it is the “before” picture of the SUT and its context. See also: test ﬁ xture (in xUnit) and test context.\n\nIn NUnit and VbUnit: The Testcase Class. See also: test ﬁ xture (in NUnit). In Fit: The adapter that interprets the Fit table and invokes methods on the\n\nsystem under test (SUT), thereby implementing a Data-Driven Test.\n\nSee also: ﬁ xture (Fit).\n\ntest ﬁ xture (in NUnit)\n\nIn NUnit (and in VbUnit and most .NET implementations of xUnit): The Test- case Class on which the Test Methods are implemented. We add the attribute [TestFixture] to the class that hosts the Test Methods.\n\nSome members of the xUnit family assume that an instance of the Testcase Class “is a” test context; NUnit is a good example. This interpretation assumes we are using the Testcase Class per Fixture approach to organizing the tests. When we choose to use a different way of organizing the tests, such as Testcase Class per Class or Testcase Class per Feature, this merging of the concepts of test context and Testcase Class can be confusing. This book uses “test ﬁ xture” to mean “the pre-conditions of the test” (also known as the test context) and Testcase Class to mean “the class that contains the Test Methods and any code needed to set up the test context.”\n\ntest ﬁ xture (in xUnit)\n\nIn xUnit: All the things we need to have in place to run a test and expect a par- ticular outcome (i.e., the test context). Some variants of xUnit keep the concept of the test context separate from the Testcase Class that creates it; JUnit and its direct ports fall into this camp. Setting up the test ﬁ xture is the ﬁ rst phase of the Four-Phase Test. For meanings of the term “test ﬁ xture” in other contexts, see test ﬁ xture (disambiguation).\n\nwww.it-ebooks.info",
      "content_length": 2142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 878,
      "content": "Glossary\n\ntest-last development\n\nA development process that entails executing unit tests after the development of the corresponding units is ﬁ nished. Unlike test-ﬁ rst development, test-last devel- opment merely says that testing should be done before the code goes into pro- duction; it does not imply that the tests are automated. Traditional QA (quality assurance) testing is inherently test-last development unless the tests are pre- pared as part of the requirements phase of the project and are shared with the development team.\n\ntest maintainer\n\nThe person or project role responsible for maintaining the tests as the system or application evolves. Most commonly, this person is enhancing the system with new functionality or ﬁ xing bugs. The test maintainer could also be who- ever is called in when the automated tests fail for whatever reason. If the test maintainer is doing the enhancements by writing tests ﬁ rst, he or she is also a test driver.\n\ntest package\n\nIn languages that provide packages or namespaces, a package or name that exists for the purpose of hosting Testcase Classes.\n\ntest reader\n\nAnyone who has reason to read tests, including a test maintainer or test driver. This individual may be reading the tests primarily for the purpose of under- standing what the system under test (SUT) is supposed to do (Tests as Docu- mentation) or as part of a test maintenance or software development activity.\n\ntest result\n\nA test or test suite can be run many times, each time yielding a different test result.\n\ntest run\n\nA test or test suite can be run many times, each time yielding a different test result. Some commercial test automation tools record the results of each test run for prosperity.\n\nwww.it-ebooks.info\n\n815",
      "content_length": 1742,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 879,
      "content": "816\n\nGlossary\n\ntest smell\n\nA symptom of a problem in test code. A smell doesn’t necessarily tell us what is wrong because it may have several possible causes. Like all smells, a test smell must pass the “sniffability test”—that is, it must grab us by the nose and say, “Something is wrong here.”\n\ntest-speciﬁ c equality\n\nTests and the system under test (SUT) may have different ideas about what con- stitutes equality of two objects. In fact, this understanding may differ from one test to another. It is not advisable to modify the deﬁ nition of equality within the SUT to match the tests’ expectations, as this practice leads to Equality Pollution. Making individual Equality Assertions on many attributes of an object is not the answer either, as it can result in Obscure Tests and Test Code Duplication. In- stead, build one or more Custom Assertions that meets your tests’ needs.\n\ntest stripper\n\nA step or program in the build process that removes all the test code from the compiled and linked executable.\n\ntest success\n\nA situation in which a test is run and all actual outcomes match the expected outcomes. Compare with test failure and test error.\n\ntest suite\n\nA way to name a collection of tests that we want to run together.\n\nUniﬁ ed Modeling Language (UML)\n\nFrom Wikipedia [Wp]: “[A] nonproprietary speciﬁ cation language for object modeling. UML is a general-purpose modeling language that includes a stan- dardized graphical notation used to create an abstract model of a system, referred to as a UML model.”\n\nwww.it-ebooks.info",
      "content_length": 1542,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 880,
      "content": "Glossary\n\nunit test\n\nA test that veriﬁ es the behavior of some small part of the overall system. What turns a test into a unit test is that the system under test (SUT) is a very small subset of the overall system and may be unrecognizable to someone who is not involved in building the software. The actual SUT may be as small as a single object or method that is a consequence of one or more design decisions, although its behav- ior may also be traced back to some aspect of the functional requirements. Unit tests need not be readable, recognizable, or veriﬁ able by the customer or business domain expert. Contrast this with a customer test, which is derived almost entirely from the requirements and which should be veriﬁ able by the customer. In eXtreme Programming, unit tests are also called developer tests or programmer tests.\n\nuse case\n\nA way of describing the functionality of a system in terms of what its users are trying to achieve and what the system needs to do to achieve their goals. Unlike user stories, use cases may cover many different scenarios yet are often not test- able independently.\n\nuser acceptance test (UAT)\n\nSee acceptance test.\n\nuser story\n\nThe unit of incremental development in eXtreme Programming. We must INVEST in good user stories—that is, each user story must be Independent, Negotiable, Valuable, Estimatable, Small, and Testable [XP123]. A user story corresponds roughly to a “feature” in non-eXtreme Programming terminology and is typically decomposed into one or more tasks to be carried out by project team members.\n\nverify outcome\n\nAfter the exercise SUT phase of the test, the test compares the actual outcome— including returned values, indirect outputs, and the post-test state of the system under test (SUT)—with the expected outcome. This phase of the test is called the verify outcome phase.\n\nwww.it-ebooks.info\n\n817\n\nAlso known as: story, feature",
      "content_length": 1901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 881,
      "content": "This page intentionally left blank\n\nwww.it-ebooks.info",
      "content_length": 54,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 882,
      "content": "References\n\n[AP] AntiPatterns: Refactoring Software, Architectures, and Projects in Crisis\n\nPublished by: John Wiley (1998) ISBN: 0-471-19713-0 By: William J. Brown et al.\n\nThis book describes common problems on software projects and suggests how to eliminate them by changing the architecture or project organization.\n\n[APLfPW] A Pattern Language for Pattern Writing\n\nIn: Pattern Languages of Program Design 3 [PLoPD3], pp. 529–574. Published by: Addison-Wesley (1998) By: Gerard Meszaros and James Doble\n\nAs the patterns community has accumulated experience in writing and reviewing patterns and pattern languages, we have begun to develop insight into pattern-writing techniques and approaches that have been observed to be particularly effective at addressing certain recurring problems. This pat- tern language attempts to capture some of these “best practices” of pattern writing, both by describing them in pattern form and by demonstrating them in action. As such, this pattern language is its own running example.\n\nFurther Reading\n\nFull text of this paper is available online in PDF form at http://Pattern- WritingPatterns.gerardmeszaros.com and in HTML form, complete with a hyperlinked table of contents, at http://hillside.net/patterns/writing/pattern- writingpaper.htm.\n\n819\n\nwww.it-ebooks.info",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 883,
      "content": "820\n\nReferences\n\n[ARTRP] Agile Regression Testing Using Record and Playback\n\nhttp://AgileRegressionTestPaper.gerardmeszaros.com By: Gerard Meszaros and Ralph Bohnet\n\nThis paper was presented at XP/Agile Universe 2003. It describes how we built a “record and playback” test mechanism into a safety-critical appli- cation to make it easier to regression test it as it was ported from OS2 to Windows.\n\n[CJ2EEP] Core J2EE™ Patterns, Second Edition: Best Practices and Design Strategies\n\nPublished by: Prentice Hall (2003) ISBN: 0-131-42246-4 By: Deepak Alur, Dan Malks, and John Crupi\n\nThis book catalogs the core patterns of usage of Enterprise Java Beans (EJB), which are a key part of the Java 2 Enterprise Edition. Examples include Session Facade [CJ2EEP].\n\n[DDD] Domain-Driven Design: Tackling Complexity in the Heart of Software\n\nPublished by: Addison-Wesley (2004) ISBN: 0-321-12521-5 By: Eric Evans\n\nThis book is a good introduction to the process of using a domain model as the heart of a software system.\n\nReaders learn how to use a domain model to make complex development effort more focused and dynamic. A core of best practices and standard patterns provides a common language for the development team.\n\nwww.it-ebooks.info",
      "content_length": 1232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 884,
      "content": "References\n\n[ET] Endo-Testing\n\nhttp://www.connextra.com/aboutUs/mockobjects.pdf By: Tim Mackinnon, Steve Freeman, and Philip Craig\n\nThis paper, which was presented at XP 2000 in Sardinia, describes the use of Mock Objects (page 544) to facilitate testing of the behavior of an object by monitoring its behavior while it is executing.\n\nUnit testing is a fundamental practice in eXtreme Programming, but most nontrivial code is difﬁ cult to test in isolation. It is hard to avoid writing test suites that are complex, incomplete, and difﬁ cult to maintain and interpret. Using Mock Objects for unit testing improves both domain code and test suites. These objects allow unit tests to be written for everything, simplify test structure, and avoid polluting domain code with testing infrastructure.\n\n[FaT] Frameworks and Testing\n\nIn: Proceedings of XP2002 http://www.agilealliance.org/articles/roockstefanframeworks/ﬁ le By: Stefan Roock\n\nThis paper is mandatory reading for framework builders. It describes four kinds of automated testing that should accompany a framework, including the ability to test a plug-in’s compliance with the framework’s protocol and a testing framework that makes it easier to test applications built on the framework.\n\nwww.it-ebooks.info\n\n821",
      "content_length": 1268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 885,
      "content": "822\n\nReferences\n\n[FitB] Fit for Developing Software\n\nPublished by: Addison-Wesley (2005) ISBN: 0-321-26934-9 By: Rick Mugridge and Ward Cunningham\n\nThis book is a great introduction to the use of Data-Driven Tests (page 288) for preparing customer tests, whether as part of agile or traditional projects. This is what I wrote for inclusion as “advance praise”:\n\nWow! This is the book I wish I had on my desk when I did my ﬁ rst storytest-driven development project. It explains the philosophy behind the Fit framework and a process for using it to interact with the customers to help deﬁ ne the requirements of the project. It makes Fit so easy and approachable that I wrote my ﬁ rst FitNesse tests before I even I ﬁ nished the book.\n\nFurther Reading\n\nMore information on Fit can be found at Ward’s Web site, http://ﬁ t.c2.com.\n\n[GOF] Design Patterns: Elements of Reusable Object-Oriented Software\n\nPublished by: Addison-Wesley (1995) ISBN: 0-201-63361-2 By: Erich Gamma, Richard Helm, Ralph Johnson, and John M.Vlissides\n\nThis book started the patterns movement. In it, the “Gang of Four” describe 23 recurring patterns in object-oriented software systems. Examples include Composite [GOF], Factory Method [GOF], and Facade [GOF].\n\nwww.it-ebooks.info",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 886,
      "content": "References\n\n[HoN] Hierarchy of Needs\n\nFrom Wikipedia [Wp]:\n\nMaslow’s hierarchy of needs is a theory in psychology that Abraham Maslow proposed in his 1943 paper “A Theory of Human Motivation,” which he subsequently extended. His theory contends that as humans meet “basic needs,” they seek to satisfy successively “higher needs” that occupy a set hierarchy. . . .\n\nMaslow’s hierarchy of needs is often depicted as a pyramid consisting of ﬁ ve levels: The four lower levels are grouped together as deﬁ ciency needs associated with physiological needs, while the top level is termed growth needs associated with psychological needs. While our deﬁ ciency needs must be met, our being needs are continually shaping our behavior. The basic concept is that the higher needs in this hierarchy only come into focus once all the needs that are lower down in the pyramid are mainly or entirely satisﬁ ed. Growth forces create upward movement in the hierarchy, whereas regressive forces push prepotent needs farther down the hierarchy.\n\n[IEAT] Improving the Effectiveness of Automated Tests\n\nhttp://FasterTestsPaper.gerardmeszaros.com. By: Gerard Meszaros, Shaun Smith, and Jennitta Andrea\n\nThis paper was presented at XP2001 in Sardinia, Italy. It describes a number of issues that reduce the speed and effectiveness of automated unit tests and suggests ways to address them.\n\n[IXP] Industrial XP\n\nhttp://ixp.industriallogic.com.\n\nIndustrial XP is a “branded” variant of eXtreme Programming created by Joshua Kerievsky of Industrial Logic. It includes a number of practices required to scale eXtreme Programming to work in larger enterprises, such as “Project Chartering.”\n\nwww.it-ebooks.info\n\n823",
      "content_length": 1687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 887,
      "content": "824\n\nReferences\n\n[JBrains] JetBrains\n\nhttp://www.jetbrains.com.\n\nJetBrains builds software development tools that automate (among other things) refactoring. Its Web site contains a list of all refactorings that the company’s various tools support, including some that are not described in [Ref].\n\n[JNI] JUnit New Instance\n\nhttp://www.martinfowler.com/bliki/JunitNewInstance.html\n\nThis article by Martin Fowler provides the background for why it makes sense for JUnit and many of its ports to create a new instance of the Testcase Class (page 373) for each Test Method (page 348).\n\n[JuPG] JUnit Pocket Guide\n\nPublished by: O’Reilly ISBN: 0-596-00743-4 By: Kent Beck\n\nThis 80-page, small-format book is an excellent summary of key features of JUnit and best practices for writing tests. Being small enough to ﬁ t in a pocket, it doesn’t go into much detail, but it does give us an idea of what is possible and where to look for details.\n\n[LSD] Lean Software Development : An Agile Toolkit\n\nPublished by: Addison-Wesley (2003) ISBN: 0-321-15078-3 By: Mary Poppendieck and Tom Poppendieck\n\nThis excellent book describes 22 “thinking tools” that are used to work quickly and effectively in many domains. The authors describe how to apply these tools to software development. If you want to understand why agile development methods work, this book is a must read!\n\nwww.it-ebooks.info",
      "content_length": 1377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 888,
      "content": "References\n\n[MAS] Mocks Aren’t Stubs\n\nhttp://www.martinfowler.com/articles/mocksArentStubs.html By: Martin Fowler\n\nThis article clariﬁ es the difference between Mock Objects (page 544) and Test Stubs (page 529). It goes on to describe the two fundamentally differ- ent approaches to test-driven development engendered by these differences: “classical TDD” versus “mockist TDD.”\n\n[MRNO] Mock Roles, Not Objects\n\nPaper presented at OOPSLA 2004 in Vancouver, British Columbia, Canada. By: Steve Freeman, Tim Mackinnon, Nat Pryce, and Joe Walnes\n\nThis paper describes the use of Mock Objects (page 544) to help the developer discover the signatures of the objects on which the class being designed and tested depends. This approach allows the design of the supporting classes to be deferred until after the client classes have been coded and tested. Members can obtain this paper at the ACM portal http://portal.acm.org/ft_gateway. cfm?id=1028765&type=pdf; nonmembers of the ACM can ﬁ nd it at http:// joe.truemesh.com/MockRoles.pdf.\n\n[PEAA] Patterns of Enterprise Application Architecture\n\nPublished by: Addison-Wesley (2003) ISBN: 0-321-12742-0 By: Martin Fowler\n\nThis book is an indispensable handbook of architectural patterns that are applicable to any enterprise application platform. It is a great way to understand how the various approaches to developing large business systems differ.\n\nwww.it-ebooks.info\n\n825",
      "content_length": 1415,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 889,
      "content": "826\n\nReferences\n\n[PiJV1] Patterns in Java, Volume 1: A Catalog of Reusable Design Patterns Illustrated with UML\n\nPublished by: Wiley Publishing (2002) ISBN: 0-471-22729-3 By: Mark Grand\n\nA catalog of design patterns commonly used in Java.\n\nFurther Reading http://www.markgrand.com/id1.html\n\n[PLoPD3] Pattern Languages of Program Design 3\n\nPublished by: Addison-Wesley (1998) ISBN: 0-201-31011-2 Edited by: Robert C. Martin, Dirk Riehle, and Frank Buschmann\n\nA collection of patterns originally workshopped at the Pattern Languages of Programs (PLoP) conferences.\n\n[POSA2] Pattern-Oriented Software Architecture, Volume 2: Patterns for Concurrent and Networked Objects\n\nPublished by: Wiley & Sons (2000) ISBN: 0-471-60695-2 By: Douglas Schmidt, Michael Stal, Hans Robert, and Frank Buschmann\n\nThis book is the second volume in the highly acclaimed Pattern-Oriented Software Architecture (POSA) series. POSA1 was published in 1996; hence this book is referred to as POSA2. It presents 17 interrelated pat- terns that cover core elements of building concurrent and networked sys- tems: service access and conﬁ guration, event handling, synchronization, and concurrency.\n\nwww.it-ebooks.info",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 890,
      "content": "References\n\n[PUT] Pragmatic Unit Testing\n\nPublished by: Pragmatic Bookshelf ISBN: 0-9745140-2-0 (In C# with NUnit) ISBN: 0-9745140-1-2 (In Java with JUnit) By: Andy Hunt and Dave Thomas\n\nThis book by the “pragmatic programmers” introduces the concept of automated unit testing in a very approachable way. Both versions lower the entry barriers by focusing on the essentials without belaboring the ﬁ ner points. They also include a very good section on how to determine which tests you need to write for a particular class or method.\n\n[RDb] Refactoring Databases: Evolutionary Database Design\n\nPublished by: Addison-Wesley (2006) ISBN: 0-321-29353-3 By: Pramodkumar J. Sadalage and Scott W. Ambler\n\nThis book is a good introduction to techniques for applying agile principles to development of database-dependent software. It describes techniques for eliminating the need to do “big design up front” on the database. It deserves to be on the bookshelf of every agile developer who needs to work with a database. A summary of the contents can be found at http://www. ambysoft.com/books/refactoringDatabases.html.\n\n[Ref] Refactoring: Improving the Design of Existing Code\n\nPublished by: Addison-Wesley (1999) ISBN: 0-201-48567-2 By: Martin Fowler et al.\n\nThis book offers a good introduction to the process of refactoring software. It introduces a number of “code smells” and suggests ways to refactor the code to eliminate those smells.\n\nwww.it-ebooks.info\n\n827",
      "content_length": 1459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 891,
      "content": "828\n\nReferences\n\n[RTC] Refactoring Test Code\n\nPaper presented at XP2001 in Sardinia, Italy By: Arie van Deursen, Leon Moonen, Alex van den Bergh, and Gerard Kok\n\nThis paper was the ﬁ rst to apply the concept of “code smells” to test code. It described a collection of 12 “test smells” and proposed a set of refac- torings that could be used to improve the code. The original paper can be found at http://homepages.cwi.nl/~leon/papers/xp2001/xp2001.pdf.\n\n[RtP] Refactoring to Patterns\n\nPublished by: Addison-Wesley (2005) ISBN: 0-321-21335-1 By: Joshua Kerievsky\n\nThis book deals with the marriage of refactoring (the process of improving the design of existing code) with patterns (the classic solutions to recurring design problems). Refactoring to Patterns suggests that using patterns to improve an existing design is a better approach than using patterns early in a new design, whether the code is years old or minutes old. We can improve designs with patterns by applying sequences of low-level design transfor- mations, known as refactorings.\n\n[SBPP] Smalltalk Best Practice Patterns\n\nPublished by: Prentice Hall (1997) ISBN: 0-13-476904-X By: Kent Beck\n\nThis book describes low-level programming patterns that are used in good object-oriented software. On the back cover, Martin Fowler wrote:\n\nKent’s Smalltalk style is the standard I aim to emulate in my work. This book does not just set that standard, but also explains why it is the standard. Every Smalltalk developer should have it close at hand.\n\nWhile Smalltalk is no longer the dominant object-oriented development language, many of the patterns established by Smalltalk programmers have been adopted as the standard way of doing things in the mainstream object- oriented development languages. The patterns in this book remain highly relevant even if the examples are in Smalltalk.\n\nwww.it-ebooks.info",
      "content_length": 1868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 892,
      "content": "References\n\n[SCMP] Software Conﬁ guration Management Patterns: Effective Teamwork, Practical Integration\n\nPublished by: Addison-Wesley (2003) ISBN: 0-201-74117-1 By: Steve Berczuk (with Brad Appleton)\n\nThis book describes, in pattern form, the how’s and why’s of using a source code conﬁ guration management system to synchronize the activities of multiple developers on a project. The practices described here are equally applicable to agile and traditional projects.\n\nFurther Reading\n\nhttp://www.scmpatterns.com\n\nhttp://www.scmpatterns.com/book/pattern-summary.html\n\n[SoC] Secrets of Consulting: A Guide to Giving and Getting Advice Successfully\n\nPublished by: Dorset House (1985) ISBN: 0-932633-01-3 By: Gerald M. Weinberg\n\nFull of Gerry’s laws and rules, such as “The Law of Raspberry Jam: The farther you spread it, the thinner it gets.”\n\n[TAM] Test Automation Manifesto\n\nhttp://TestAutomationManifesto.gerardmeszaros.com By: Shaun Smith and Gerard Meszaros\n\nThis paper was presented at the August 2003 XP/Agile Universe meeting in New Orleans, Louisiana. It describes a number of principles that should be followed to make automated testing using xUnit cost-effective.\n\nwww.it-ebooks.info\n\n829",
      "content_length": 1199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 893,
      "content": "830\n\nReferences\n\n[TDD-APG] Test-Driven Development: A Practical Guide\n\nPublished by: Prentice Hall (2004) ISBN: 0-13-101649-0 By: David Astels\n\nThis book provides a good introduction to the process of driving software development with unit tests. Part III of the book is an end-to-end example of using tests to drive a small Java project.\n\n[TDD-BE] Test-Driven Development: By Example\n\nPublished by: Addison-Wesley (2003) ISBN: 0-321-14653-0 By: Kent Beck\n\nThis book provides a good introduction to the process of driving software development with unit tests. In the second part of the book, Kent illustrates TDD by building a Test Automation Framework (page 298) in Python. In an approach he likens to “doing brain surgery on yourself,” he uses the emerg- ing framework to run the tests he writes for each new capability. It is a very good example of both TDD and bootstrapping.\n\n[TDD.Net] Test-Driven Development in Microsoft .NET\n\nPublished by: Microsoft Press (2004) ISBN: 0-735-61948-4 By: James W. Newkirk and Alexei A. Vorontsov\n\nThis book is a good introduction to the test-driven development process and the tools used to do it in Microsoft’s. Net development environment.\n\n[TI] Test Infected\n\nhttp://junit.sourceforge.net/doc/testinfected/testing.htm By: Eric Gamma and Kent Beck\n\nThis article was ﬁ rst published in the Java Report issue called “Test Infected— Programmers Love Writing Tests.” It has been credited by some as being what led to the meteoric rise in JUnit’s popularity. This article is an excellent intro- duction to the how’s and why’s of test automation using xUnit.\n\nwww.it-ebooks.info",
      "content_length": 1614,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 894,
      "content": "References\n\n[TPS] Toyota Production System: Beyond Large-Scale Production\n\nPublished by: Productivity Press (1995) ISBN: 0-915-2991-4-3 By: Taiichi Ohno\n\nThis book, which was written by the father of just-in-time manufacturing, describes how Toyota came up with the system driven by its need to pro- duce a small number of cars while realizing economies of scale. Among the techniques described here are “kanban” and the “ﬁ ve why’s.”\n\n[UTF] Unit Test Frameworks: Tools for High-Quality Software Development\n\nPublished by: O’Reilly (2004) ISBN: 0-596-00689-6 By: Paul Hamill\n\nThis book is a brief introduction to the most popular implementations of xUnit.\n\n[UTwHCM] Unit Testing with Hand-Crafted Mocks\n\nhttp://refactoring.be/articles/mocks/mocks.html By: Sven Gorts\n\nThis paper summarizes and names a number of idioms related to Hand-Built Test Doubles (see Conﬁ gurable Test Double on page 522)—speciﬁ cally, Test Stubs (page 529) and Mock Objects (page 544). Sven Gorts writes:\n\nMany of the unit tests I wrote over the last couple of years use mock objects in order to test the behavior of a component in isolation of the rest of the system. So far, despite the availability of various mocking frameworks, each of the mock classes I’ve used has been handwritten. In this article I do some retrospection and try to wrap up the mocking idioms I’ve found most useful.\n\nwww.it-ebooks.info\n\n831",
      "content_length": 1392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 895,
      "content": "832\n\nReferences\n\n[UTwJ] Unit Testing in Java: How Tests Drive the Code\n\nPublished by: Morgan Kaufmann ISBN: 1-55860-868-0 By: Johannes Link, with contributions by Peter Fröhlich\n\nThis book does a very nice job of introducing many of the concepts and techniques of unit testing. It uses intertwined narratives and examples to introduce a wide range of techniques. Unfortunately, due to the format, it can be difﬁ cult to ﬁ nd something at a later time.\n\n[VCTP] The Virtual Clock Test Pattern\n\nhttp://www.nusco.org/docs/virtual_clock.pdf By: Paolo Perrotta\n\nThis paper describes a common example of a Responder called Virtual Clock [VCTP]. The author uses the Virtual Clock Test Pattern as a Decorator [GOF] for the real system clock, which allows the time to be “frozen” or resumed. One could use a Hard-Coded Test Stub or a Conﬁ gurable Test Stub just as easily for most tests. Paolo Perrotta summarizes the thrust of his article:\n\nWe can have a hard time unit-testing code that depends on the system clock. This paper describes both the problem and a common, reusable solution.\n\n[WEwLC] Working Effectively with Legacy Code\n\nPublished by: Prentice Hall (2005) ISBN: 0-13-117705-2 By: Michael Feathers\n\nThis book describes how to get your legacy software system back under control by retroﬁ tting automated unit tests. A key contribution is a set of “dependency-breaking techniques”—mostly refactorings—that can help you isolate the software for the purpose of automated testing.\n\nwww.it-ebooks.info",
      "content_length": 1499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 896,
      "content": "Database Refactoring\n\n[Wp] Wikipedia\n\nFrom Wikipedia [Wp]: “Wikipedia is a multilingual, Web-based free con- tent encyclopedia project. The name Wikipedia is a blend of the words ‘wiki’ and ‘encyclopedia.’ Wikipedia is written collaboratively by volun- teers, allowing most articles to be changed by almost anyone with access to the Web site.”\n\n[WWW] World Wide Web\n\nA reference annotation of [WWW] indicates that the information was found on the World Wide Web. You can use your favorite search engine to ﬁ nd a copy by searching for it by the title.\n\n[XP123] XP123\n\nhttp://xp123.com Web site hosted by: William Wake\n\nA Web site hosting various resources for teams doing eXtreme Program- ming.\n\n[XPC] XProgramming.com\n\nhttp://xprogramming.com Web site hosted by: Ron Jeffries\n\nA Web site hosting various resources for teams doing eXtreme Program- ming. One of the better places to look for links to software downloads for unit test automation tools including members of the xUnit family.\n\nwww.it-ebooks.info\n\n833",
      "content_length": 1013,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 897,
      "content": "834\n\nReferences\n\n[XPE] eXtreme Programming Explained, Second Edition: Embrace Change\n\nPublished by: Addison-Wesley (2005) ISBN: 0-321-27865-8 By: Kent Beck\n\nThis book kick-started the eXtreme Programming movement. The ﬁ rst edi- tion (0-201-61641-6) described a recipe consisting of 12 practices backed by principles and values. The second edition focuses more on the values and principles. It breaks the practices into a primary set and a corollary set; the latter set should be attempted only after the primary practices are mas- tered. Among the practices both editions describe are pair programming and test-driven development.\n\nwww.it-ebooks.info",
      "content_length": 651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 898,
      "content": "Index\n\nA\n\nABAP Object Unit, 747 ABAP Unit, 747 Abstract Setup Decorator\n\ndeﬁ ned, 449 example, 453\n\nacceptance tests. See also\n\ncustomer tests deﬁ ned, 785 why test?, 19 accessor methods, 785 ACID, 785 acknowledgements, xxvii–xxviii action components, 280 agile method\n\ndeﬁ ned, 785–786 property tests, 52\n\nAllTests Suite\n\nexample, 594–595 introduction, 13 when to use, 593\n\nannotation\n\ndeﬁ ned, 786 Test Methods, 351\n\nAnonymous Creation Method\n\ndeﬁ ned, 417 example, 420\n\nHard-Coded Test Data\n\nsolution, 196\n\npreface, xxi\n\nanonymous inner class deﬁ ned, 786 Test Stub examples, 535–536\n\nAnt, 753 AntHill, 753 anti-pattern (AP) deﬁ ned, 786 test smells, xxxv\n\nAOP (aspect-oriented programming)\n\ndeﬁ ned, 786 Dependency Injection, 681 retroﬁ tting testability, 148 API (application programming inter-\n\nface)\n\nCreation Methods, 416 database as SUT, 336 deﬁ ned, 786 Test Utility Method, 600 architecture, design for testability.\n\nSee design-for-testability\n\narguments\n\nmessages describing, 371–372 as parameters (Dummy\n\nArguments), 729 role-describing, 725\n\n835\n\nwww.it-ebooks.info",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 899,
      "content": "836\n\nIndex\n\nArguments, Dummy, 729 Ariane 5 rocket, 218 aspect-oriented programming (AOP)\n\ndeﬁ ned, 786 Dependency Injection, 681 retroﬁ tting testability, 148\n\nimproperly coded in Neverfail\n\nTests, 274\n\nintroduction, 77 Missing Assertion Messages,\n\n226–227\n\nreducing Test Code Duplication,\n\nAssertion Message\n\n114–119\n\nof Assertion Method, 364 pattern description, 370–372\n\nAssertion Method\n\nAssertion Messages, 364 calling built-in, 363–364 choosing right, 364–365 Equality Assertions, 365 examples, 368–369 Expected Exception Assertions, 366\n\nFuzzy Equality Assertions,\n\n365–366\n\nimplementation, 363 as macros, 364 motivating example, 367–368 overview, 362–363 refactoring, 368 Single-Outcome Assertions,\n\n366–367\n\nStated Outcome Assertions, 366\n\nAssertion Roulette\n\nrefactoring, xlvi–xlix Self-Checking Tests, 107–108 unit testing, 6 Verify One Condition per Test,\n\n46–47\n\nassumptions, xxxix–xl Astels, Dave, 110 asynchronous tests deﬁ ned, 787 Hard-To-Test Code, 210–211 Humble Object, 696–697 Slow Tests, 255–256 testability, 70–71 Attachment Method deﬁ ned, 418 example, 421\n\nattributes\n\ndeﬁ ned, 787 dummy, 729 hiding unnecessary, 303–304 One Bad Attribute. See One\n\nEager Tests, 224–226 impact, 224 introduction, 14 Missing Assertion Message,\n\n226–227\n\nsymptoms, 224\n\nBad Attribute\n\nparameters as, 608 Suite Fixture Setup, 442–443 Test Discovery using, 397 Test Selection, 403–405 Automated Exercise Teardown\n\nassertions\n\nBuilt-in, 110–111 custom. See Custom Assertion deﬁ ned, 786 diagramming notation, xlii Domain Assertions, 476,\n\n481–482\n\ndeﬁ ned, 505 example, 508\n\nAutomated Fixture Teardown,\n\n504–505\n\nAutomated Teardown\n\nensuring Repeatable Tests, 27 examples, 507–508\n\nwww.it-ebooks.info",
      "content_length": 1703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 900,
      "content": "Index\n\nimplementation, 504–505 Interacting Test Suites, 232 Interacting Tests solution, 231 motivating example, 505–506 overview, 503–504 of persistent ﬁ xtures, 99–100 refactoring, 506–507 resource leakage solution, 233 when to use, 504 automated unit testing\n\nBeck, Kent, xxii\n\nsniff test, xxxviii Test Automation Frameworks,\n\n301\n\ntest smells, 9 Testcase Class per Class, 618 xUnit, 57 Behavior Sensitivity\n\ncause of Fragile Tests, 242–243 caused by Overspeciﬁ ed\n\nauthor’s motivation, xxiv–xxv fragile test problem, xxxi–xxxii introduction, xxx–xxxii\n\nSoftware, 246\n\ndeﬁ ned, xxxi smells, 14\n\nbehavior smells, 223–247\n\nB\n\nback door, deﬁ ned, 787 Back Door Manipulation\n\ncontrol/observation points, 66–67 database as SUT API, 336 Expected State Speciﬁ cation, 464 ﬁ xture setup, 333–335 implementation, 330–332 motivating example, 332 overview, 327–328 refactoring, 333 setup, 329 teardown, 330 veriﬁ cation, 329–330 veriﬁ cation using Test Spy, 333 when to use, 328\n\nBack Door Setup\n\ncontrolling indirect inputs, 128 ﬁ xture design, 59 Prebuilt Fixtures, 430–431 transient ﬁ xtures, 86\n\nAssertion Roulette. See Assertion Roulette deﬁ ned, 10–11, 788 Erratic Tests. See Erratic Test Fragile Tests. See Fragile Test Frequent Debugging. See Frequent Debugging Manual Intervention. See Manual Intervention\n\noverview, 13–15 Slow Tests. See Slow Tests\n\nBehavior Veriﬁ cation\n\napproach to Self-Checking\n\nTests, 108\n\nexamples, 472–473 implementation, 469–471 indirect outputs, 179–180 motivating example, 471–472 overview, 468–469 refactoring, 472 vs. state, 36 test results, 112–114 using Mock Objects. See\n\nBack Door Veriﬁ cation, 130–133 BDUF (big design upfront)\n\nMock Object\n\ndeﬁ ned, 787 design for testability, 65 test automation strategy, 49\n\nusing Test Spies. See Test Spy using Use the Front Door\n\nFirst, 40\n\nwww.it-ebooks.info\n\n837",
      "content_length": 1838,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 901,
      "content": "838\n\nIndex\n\nverifying indirect outputs,\n\nBPT (Business Process Testing)\n\n130–133\n\nwhen to use, 469\n\nbehavior-driven development\n\ndeﬁ ned, 753 Recorded Tests, 280 Test Automation\n\ndeﬁ ned, 787–788 Testcase Class per Fixture\n\nusage, 632\n\nFrameworks, 301\n\nBug Repellent, 22 Buggy Test\n\nBehavior-Exposing Subclass\n\nTest-Speciﬁ c Subclass\n\nexample, 587 when to use, 580\n\nintroduction, 12–13 reducing risk, 181 symptoms, 260–262\n\nBuilt-in Assertion\n\nBehavior-Modifying Subclass\n\nDeﬁ ning Test-Speciﬁ c Equality,\n\ncalling, 363–364 introduction, 110–111\n\n588–589\n\nSubstituted Singleton,\n\n586–587\n\nbuilt-in self-tests deﬁ ned, 788 test ﬁ le organization, 164\n\nTest Stub, 584–585 when to use, 580\n\nBespoke Assertion. See Custom\n\nbuilt-in test recording deﬁ ned, 281 example, 281–282\n\nAssertion\n\nbusiness logic\n\nbimodal tests, 687 binding, static\n\ndeﬁ ned, 809 Dependency Injection, 678–679\n\nblack box\n\ndeﬁ ned, 789 developer testing, xxx development process, 4–5 Layer Tests example, 344–345 testing without databases,\n\ndeﬁ ned, 788 Remoted Stored Procedure\n\n169–171\n\nBusiness Process Testing (BPT).\n\nTests, 656\n\nSee BPT (Business Process Testing)\n\nblock closures\n\ndeﬁ ned, 788 Expected Exception Tests,\n\nC\n\n354–355\n\nCalculated Value. See also Derived\n\nblocks\n\nValue\n\ncleaning up ﬁ xture teardown\n\nlogic, l–liv deﬁ ned, 788 try/ﬁ nally. See try/ﬁ nally block\n\nboundary values deﬁ ned, 788 erratic tests, 238 Minimal Fixtures, 303 result veriﬁ cation patterns, 478\n\nLoop-Driven Tests, 615 Production Logic in Test\n\nsolution, 205\n\nCanoo WebTest deﬁ ned, 753 Scripted Tests, 286 Test Automation\n\nFrameworks, 301\n\ntest automation tools, 53\n\nwww.it-ebooks.info",
      "content_length": 1645,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 902,
      "content": "Index\n\ncapacity tests, 52 Capture/Playback Test.\n\nsamples, xli–xlii writing tests, 27–29\n\nSee Recorded Test\n\ncode smells\n\nChained Test\n\ncustomer testing, 6 examples, 459–460 implementation, 456–457 motivating example, 457–458 overview, 454–455 refactoring, 458 Shared Fixture strategies, 64–65 Shared Fixtures, 104–105, 322 when to use, 455–456 xUnit introduction, 57\n\nConditional Test Logic. See Conditional Test Logic\n\ndeﬁ ned, 10–11, 789 Hard-To-Test Code. See Hard-To-Test Code\n\nobscure tests. See Obscure Test Test Code Duplication. See Test\n\nCode Duplication\n\nTest Logic in Production. See Test Logic in Production\n\ntypes of, 16–17\n\nclass attributes\n\ncoding idioms\n\ndeﬁ ned, 789 Test Discovery using, 397 Testcase Class Selection using,\n\ndeﬁ ned, xxxv design patterns, 792\n\ncollisions\n\n404–405 class methods\n\nInteracting Tests, 229–231 Shared Fixtures, 318\n\ndeﬁ ned, 789 with Test Helper, 645, 646\n\nclass variables\n\ndeﬁ ned, 789 Suite Fixture Setup, 442\n\nclasses\n\ndiagramming notation, xlii as ﬁ xtures, 59 Test Double, 569–570, 572–573 Testcase. See Testcase Class\n\nCommand object\n\nintroduction, 82 Testcase Object as, 382 Command-Line Test Runner Assertion Message, 371 deﬁ ned, 379–380 introduction, 79 Missing Assertion Message,\n\n226–227\n\ncommercial recorded tests\n\nclass-instance duality, 374 Cleanup Method, 602 closure, block\n\nrefactored, 283–284 tools, 282–283\n\ncommon location, Test Discovery,\n\ndeﬁ ned, 788 Expected Exception Tests,\n\n354–355 Cockburn, Alistair\n\n397–398\n\nCommunicate Intent deﬁ ned, 41 refactoring Recorded Tests to,\n\npattern naming, 578 service layer tests, 339\n\n283–284\n\ncompiler macro, Test Method\n\ncode\n\nDiscovery, 395–396\n\ninside-out development, 34–36 organization. See test\n\nComplex Teardown, 206–207 Complex Test. See Dependency\n\norganization\n\nLookup\n\nwww.it-ebooks.info\n\n839",
      "content_length": 1815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 903,
      "content": "840\n\nIndex\n\nComponent Broker. See Dependency\n\nLookup\n\nComponent Registry, 688 component tests deﬁ ned, 790 layer-crossing tests, 69 per-functionality, 52 test automation philosophies,\n\nConﬁ gurable Test Double examples, 564–567 implementation, 559–562 installing, 141–142 as kind of Test Double, 528 motivating example, 562–563 overview, 558 refactoring, 563 when to use, 559\n\n34–36\n\ntest strategy patterns, 340\n\nConﬁ gurable Test Stub. See also\n\ncomponents\n\ndeﬁ ned, 790 depended-on component. See\n\nConﬁ gurable Test Double implementation, 532 indirect input control, 179\n\nDOC (depended-on component)\n\nComposite object, deﬁ ned, 82 Concerns, Separation of, 28–29 concrete classes, 581 Condition Veriﬁ cation Logic, 203–204 Conditional Test Logic\n\nvs. Assertion Method, 363 avoidance, 119–121 avoiding via Custom\n\nAssertion, 475\n\nConﬁ guration Interface examples, 564–566 implementation, 560\n\nConﬁ guration Mode\n\nexample, 566–567 implementation, 560\n\nConstant Value. See Literal Value constants in Derived Value,\n\n718–722\n\nconstructing Mock Object, 546 Constructor Injection\n\navoiding via Guard Assertion,\n\n490–493\n\ncauses, 201–202 Complex Teardown, 206–207 Condition Veriﬁ cation Logic,\n\n203–204\n\nFlexible Tests, 202–203 impact, 201 introduction, 16 Multiple Test Conditions,\n\nexample, 683–684 implementation, 680–681 installing Test Doubles, 144\n\nConstructor Test deﬁ ned, 351 example, 355–357 introduction, 77\n\nconstructors\n\ndeﬁ ned, 790 problems with, 419\n\n207–208\n\ncontainers, Humble Container\n\nProduction Logic in Test,\n\nAdapter, 698\n\n204–205\n\nContext Sensitivity\n\nsymptoms, 200 Test Methods, 155\n\navoiding via Isolate the SUT,\n\n43–44\n\nConﬁ gurable Mock Object, 546–547. See also Conﬁ gurable Test Double\n\ndeﬁ ned, 245–246 introduction, xxxii, 14\n\nConﬁ gurable Registry, 691–692\n\ncontinuous design, xxxiii\n\nwww.it-ebooks.info",
      "content_length": 1831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 904,
      "content": "Index\n\ncontinuous integration\n\nCSV ﬁ les, xUnit Data-Driven\n\navoiding Lost Tests, 270 deﬁ ned, 791 impact of Data-Driven Tests, 290 steps, 14 control points\n\nTest, 296 CUnit, 748 Cunningham, Ward, xxv, 290 Custom Assertion\n\nas Conditional Veriﬁ cation\n\ndeﬁ ned, 791 testability, 66–67\n\nCoplien, Jim, 576 CORBA standards, 744 cost effectiveness, Self-Checking\n\nLogic solution, 204\n\nexamples, 480–484 implementation, 477–478 Indirect Testing solution,\n\n198–199\n\nTests, 107–108\n\nIrrelevant Information\n\ncosts, test automation, 20–21 Covey, Stephen, 121 CppUnit\n\ndeﬁ ned, 748 Test Automation Frameworks,\n\nsolution, 193\n\nmotivating example, 478–480 overview, 474–475 reducing Test Code Duplication,\n\n116–117\n\n300\n\nTest Method enumeration, 401\n\nCreation Method\n\nDelegated Setup, 89–91,\n\nrefactoring, 480 Test Utility Methods, 602 when to use, 475–477 writing simple tests, 28\n\n411–414\n\neliminating unnecessary\n\nobjects/attributes, 303–304\n\nCustom Assertion test example, 483–484 implementation, 477–478\n\nexamples, 420–423 as Hard-Coded Test Data\n\nCustom Equality Assertion, 476 customer tests\n\nsolution, 196 hybrid setup, 93 implementation, 418–419 motivating example, 419 overview, 415–416 persistent ﬁ xtures teardown, 100\n\ndeﬁ ned, 791 Eager Tests cause, 225 Missing Unit Test, 271 overview, 5–6 per-functionality, 51 as Scripted Test, 285–287\n\nCut and Paste code reuse,\n\npreface, xxiii refactoring, 420 as Test Utility Method, 600 when to use, 416–418 writing simple tests, 28 cross-functional tests, 52–53 cross-thread failure assertion, 274 Cruise Control, 754 CsUnit, 748\n\n214–215\n\nD\n\ndata access layer\n\ndatabase testing, 172–173 deﬁ ned, 791 Slow Tests with Shared\n\nFixtures, 319\n\nwww.it-ebooks.info\n\n841",
      "content_length": 1706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 905,
      "content": "842\n\nIndex\n\ndata leaks\n\navoiding with Delta Assertions,\n\n486–487\n\nComplex Teardown, 206\n\ndatabase testing, 167–174 overview, 167–169 persistent ﬁ xtures, 313 testing without databases,\n\nData Loader, Back Door Manipulation, 330–331 data minimization, 738–739 data population script, 434 Data Retriever, 331 Data Sensitivity\n\ndeﬁ ned, 243–245 introduction, xxxii, 14\n\n169–171\n\ntypes of, 171–174\n\nDatabase Transaction Rollback Tear-\n\ndown, 674–675\n\ndatabases\n\nfake. See Fake Database as SUT API, 336 teardown, 100\n\nData Transfer Object (DTO)\n\nData-Driven Test\n\ndeﬁ ned, 793 result veriﬁ cation, 116 Database Extraction Script, 331 Database Partitioning Scheme Data Sensitivity solution,\n\n244–245\n\ncustomer testing, 5 Fit framework example,\n\n296–297\n\nframeworks, 300 implementation, 290 implemented as Recorded\n\ndeveloper independence, 173 example, 653 Global Fixtures, 430 implementation, 652 database patterns, 649–675 Database Sandbox,\n\nTest, 281\n\nintroduction, 83 motivating example, 293–294 overview, 288–289 principles, 48 reducing Test Code Duplication,\n\n650–653\n\n118–119\n\nStored Procedure Test,\n\n654–660\n\nTable Truncation Teardown,\n\nrefactoring notes, 294 Test Suite Object Simulator, 293 using Fit framework,\n\n661–667\n\n290–292\n\nTransaction Rollback Teardown, 668–675 Database Population Script, 330 Database Sandbox\n\nvia Naive xUnit Test Interpreter,\n\n292–293\n\nvia Test Suite Object\n\nGenerator, 293\n\ndatabase testing, 168 design for testability, 7 pattern description, 650–653 as Test Run Wars solution,\n\nwhen to use, 289–290 xUnit with CSV input ﬁ le, 296 xUnit with XML data ﬁ le,\n\n294–295\n\n236–237\n\nDB Schema per Test Runner\n\nUnrepeatable Tests cause, 235 when to use, 650\n\ndeveloper independence, 173 implementation, 651–652\n\nwww.it-ebooks.info",
      "content_length": 1753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 906,
      "content": "Index\n\nDbUnit\n\nDelta Assertion\n\nBack Door Manipulation, 335 deﬁ ned, 748 Expected State Speciﬁ cation, 464\n\nDDSteps, 754 Decorated Lazy Setup, 449–450 Decorator\n\nAbstract Setup Decorator,\n\n449, 453\n\navoiding ﬁ xture collisions, 101 as Data Sensitivity solution, 245 detecting data leakage with, 487 examples, 488–489 introduction, 111 pattern description, 485–486 depended-on component (DOC). See DOC (depended-on component)\n\nParameterized Setup Decorator,\n\ndependencies\n\n452–453\n\nPushdown Decorator, 450 Setup. See Setup Decorator Test Hook as, 710\n\nInteracting Tests, 230–231 replacement with Test\n\nDoubles, 739\n\nreplacing using Test Hooks,\n\nDedicated Database Sandbox, 651 Defect Localization\n\ncustomer testing, 5 deﬁ ned, 22–23 Frequent Debugging, 248 Keep Tests Independent Tests, 43 right-sizing Test Methods, 154 test automation philosophies, 34 unit testing, 6 Verify One Condition per Test, 45\n\ndeﬁ ning tests\n\nintroduction, 76–78 suites of, 78–79\n\n709–712\n\nretroﬁ tting testability, 148 test automation philosophies, 34 Test Dependency in Production,\n\n220–221\n\ntest ﬁ le organization, 165 Dependency Initialization Test, 352 Dependency Injection\n\ndesign for testability, 7 examples, 683–685 implementation, 679–681 installing Test Doubles via,\n\n143–144\n\ndelays. See Slow Tests Delegated Setup\n\nexample, 413–414 introduction, 77 matching with teardown code,\n\nIsolate the SUT, 44 motivating example, 682 overview, 678 Persistent Fresh Fixtures\n\navoidance, 62–63\n\n98–99\n\noverview, 411–414 of transient ﬁ xtures, 89–91 when to use, 412 Delegated Teardown\n\nrefactoring, 682 testability improvement, 70 when database testing, 171 when to use, 678–679\n\nDependency Lookup\n\nexample, 514–515 overview, 511 of persistent ﬁ xtures, 98–99 Table Truncation Teardown, 665\n\ndesign for testability, 7 examples, 691–693 implementation, 688–689 installing Test Doubles, 144–145\n\nwww.it-ebooks.info\n\n843",
      "content_length": 1893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 907,
      "content": "844\n\nIndex\n\nIsolate the SUT, 44 motivating example, 690 names, 693–694 overview, 686 Persistent Fresh Fixtures,\n\ndeterministic values, 238 developer independence, 173 developer testing deﬁ ned, 792 introduction, xxx\n\n62–63\n\nrefactoring, 690–691 when database testing, 171 when to use, 687–688\n\nDerived Expectation example, 720 when to use, 719\n\nDevelopers Not Writing Tests, 13 development\n\nagile, 239 behavior driven, 632, 787–788 document-driven, 793 EDD. See EDD (example-driven\n\ndevelopment)\n\nDerived Input, 719 Derived Value\n\nexamples, 719–722 overview, 718 when to use, 718–719 design patterns, xxxv, 792 design-for-testability\n\ncontrol points and observation\n\nincremental, 33–34, 799–800 inside-out, 463 inside-out vs. outside in, 34–36 need-driven. See need-driven\n\ndevelopment outside-in, 469 process, 4–5 TDD. See TDD (test-driven\n\npoints, 66–67\n\ndevelopment)\n\ndeﬁ ned, 792 divide and test, 71–72 ensuring testability, 65 interaction styles and testability\n\npatterns, 67–71\n\noverview, 7 Separation of Concerns, 28–29 test automation philosophies.\n\nSee test automation philosophies\n\ntest automation principles, 40 test-driven testability, 66\n\ndesign-for-testability patterns,\n\n677–712\n\ntest-ﬁ rst. See test-ﬁ rst\n\ndevelopment\n\ntest-last. See test-last development\n\nDiagnostic Assertion, 476–477 diagramming notation, xlii Dialog, Humble. See Humble Dialog direct output\n\ndeﬁ ned, 792–793 veriﬁ cation, 178\n\nDirect Test Method Invocation, 401 disambiguation, test ﬁ xtures, 814 Discovery, Test. See Test Discovery Distinct Generated Values Anonymous Creation\n\nDependency Injection. See Dependency Injection Dependency Lookup. See Dependency Lookup\n\nMethods, 417\n\nDelegated Setup, 90 example, 725–726 Hard-Coded Test Data\n\nHumble Object. See Humble\n\nsolution, 196\n\nObject\n\nTest Hooks, 709–712\n\nimplementation, 724 Unrepeatable Tests solution, 235\n\nwww.it-ebooks.info",
      "content_length": 1873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 908,
      "content": "Index\n\nDistinct Values, 717 Do No Harm, 24–25 DOC (depended-on component)\n\ndynamic binding deﬁ ned, 793 use in Dependency Injection, 679\n\nBehavior Veriﬁ cation, 469 control points and observation\n\nDynamically Generated Mock\n\nObject, 550\n\npoints, 66–67 deﬁ ned, 791–792 outside-in development, 35 replacing with Test Double.\n\nDynamically Generated Test Double implementation, 561–562 providing, 140–141\n\nDynamically Generated Test Stub,\n\nSee Test Double\n\n534–535\n\nretrieving. See Dependency\n\nLookup\n\nterminology, xl–xli Test Hook in, 712 Documentation, Tests as.\n\nSee Tests as Documentation document-driven development,\n\n793\n\nE\n\nEager Test\n\nAssertion Roulette, 224–226 Fragile Tests, 240 Obscure Tests, 187–188 right-sizing Test Methods, 154\n\nDomain Assertion\n\nEasyMock\n\ndeﬁ ned, 476 example, 481–482\n\ndeﬁ ned, 754 Test Doubles, 140\n\ndomain layer\n\neCATT\n\ndeﬁ ned, 793 test strategy patterns, 337\n\ndeﬁ ned, 754 Test Automation Frameworks,\n\ndomain model, 793 Don’t Modify the SUT, 41–42 drivers, test\n\ndeﬁ ned, 813 lack of Assertion Messages,\n\n370\n\n301\n\nEclipse\n\nDebugger, 110 deﬁ ned, 754\n\neconomics of test automation, 20–21 EDD (example-driven development)\n\nDRY (don’t repeat yourself), 28 DTO (Data Transfer Object)\n\ndeﬁ ned, 794 tests as examples, 33\n\ndeﬁ ned, 793 result veriﬁ cation, 116\n\nefﬁ ciency, 11 emergent design\n\nDummy Argument, 729 Dummy Attribute, 729 Dummy Object\n\nvs. BDUF, 65 deﬁ ned, xxxiii, 794\n\nencapsulation\n\nconﬁ guring, 141–142 deﬁ ned, 133 as Test Double, 134–135, 526 as value pattern, 728–732 xUnit terminology, 741–744\n\nCreation Method. See Creation\n\nMethod\n\nDependency Lookup\n\nimplementation, 688–689\n\nindirect outputs and, 126\n\nwww.it-ebooks.info\n\n845",
      "content_length": 1680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 909,
      "content": "846\n\nIndex\n\nIndirect Testing solution, 198 SUT API. See SUT API\n\nEncapsulation\n\nusing Test Utility Methods. See Test Utility Method\n\nequivalence class\n\nBehavior Smells, 238 deﬁ ned, 794 Untested Code, 272\n\nErratic Test\n\nendoscopic testing (ET)\n\ndeﬁ ned, 794 Mock Objects, 545 Test Doubles, 149\n\nEnsure Commensurate Effort and\n\nResponsibility, 47–48 Entity Chain Snipping example, 536–537 testing with doubles, 149 when to use, 531\n\nAutomated Teardown and, 27 customer testing, 5 database testing, 168–169 impact, 228 Interacting Test Suites, 231–232 Interacting Tests, 229–231 introduction, 14–16 Lonely Tests, 232 Nondeterministic Tests,\n\n237–238\n\nentity object, 794 enumeration\n\ncustomer testing, 5 Suite of Suites built using, 389–391 test conditions in Loop-Driven\n\nTests, 614–615\n\nResource Leakage, 233 Resource Optimism, 233–234 symptoms, 228 Test Run Wars, 235–237 troubleshooting, 228–229 Unrepeatable Tests, 234–235\n\nTest Enumeration, 399–402 Test Suite Object built using, 388 xUnit organization mechanisms, 153\n\nEquality, Sensitivity\n\nFragile Tests, 246 test-ﬁ rst development, 32\n\nEquality Assertion\n\nessential but irrelevant ﬁ xture\n\nsetup, 425\n\nET (endoscopic testing)\n\ndeﬁ ned, 794 Mock Object use for, 149, 545 example-driven development (EDD)\n\ndeﬁ ned, 794 tests as examples, 33\n\nAssertion Methods, 365 Custom, 476 example, 368 Guard Assertion as, 491 introduction, 110 reducing Test Code Duplication, 115\n\nexamples, tests as, 33 exclamation marks, xlii Executable, Humble. See Humble\n\nExecutable\n\nExecutable Speciﬁ cation, 51 execution optimization, 180–181 exercise SUT\n\nunit testing, 6\n\nEquality Pollution, 221–222 equals method\n\ndeﬁ ned, 794 test phases, 359\n\nexpectations\n\nEquality Pollution, 221–222 Expected State Speciﬁ cation, 464 reducing Test Code Duplication,\n\n115–116\n\ndeﬁ ned, 795 Derived Expectations, 719, 720 messages describing, 371–372 naming conventions, 159\n\nwww.it-ebooks.info",
      "content_length": 1915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 910,
      "content": "Index\n\nExpected Behavior Speciﬁ cation\n\ndeﬁ ned, 470–471 example, 473\n\nin persistent ﬁ xture teardown, 98 refactoring Recorded Tests, 283 Extract Testable Component, 197,\n\nExpected Behavior Veriﬁ cation\n\n735–736\n\ndeﬁ ned, 112 indirect outputs, 131–132 Expected Exception Assertion\n\neXtreme Programming deﬁ ned, 795 projects affected by Slow Tests,\n\ndeﬁ ned as Assertion Method,\n\n319–321\n\n365–366 example, 369\n\neXtreme Programming Explained\n\n(Beck), xxii\n\nExpected Exception Test\n\nConditional Veriﬁ cation Logic\n\nsolution, 204 introduction, 77 as Test Method, 350–351 using block closure, 354–355 using method attributes, 354 using try/catch, 353–354\n\nF\n\nfactories\n\ndeﬁ ned, 795 Factory Method, 592–593 Object Factories, 145, 688\n\nfailed tests\n\nExpected Object\n\nreducing Test Code Duplication,\n\ndue to Unﬁ nished Test Assertions, 494–497\n\n115–116\n\nrefactoring tests, xlv–xlviii State Veriﬁ cations, 109, 466–467 unit testing, 6 expected outcome, 795 Expected State Speciﬁ cation,\n\n464–465\n\nimplementation, 80 “Fail-Pass-Pass”, 234–235 failure messages\n\nAssertion Messages, 370–372 Built-in Assertions, 110–111 removing “if” statements, 120 Single-Outcome Assertions,\n\nexpected values, 546–547 exploratory testing\n\n366–367 Fake Database\n\ncross-functionality, 53 deﬁ ned, 795 Scripted Tests, 287\n\nExpression Builders, 564–566 expressiveness gaps, 27–28 external resource setup, 740 external result veriﬁ cation, 111–112 external test recording, 280 Extract Method\n\navoiding persistence, 101 database testing, 170 example, 556–557 Slow Component Usage\n\nsolution, 254\n\nSlow Tests with Shared\n\nFixtures, 319 when to use, 553\n\nFake Object\n\nCreation Methods, 418 Custom Assertions, 117 Delegated Setup, 89 as Eager Tests solution, 225 example, xlvii\n\nconﬁ guring, 141–142 customer testing, 6 deﬁ ned, 134 examples, 556–557 implementation, 553–554\n\nwww.it-ebooks.info\n\n847",
      "content_length": 1863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 911,
      "content": "848\n\nIndex\n\nmotivating example, 554–555 optimizing test execution, 180 overview, 551–552 refactoring, 555–556 as Test Double, 139, 525 when to use, 552–553 xUnit terminology, 741–744\n\nFake Service Layer, 553 Fake Web Services, 553 false negative, 795 false positive, 795–796 fault insertion tests deﬁ ned, 796 per-functionality, 52\n\nFeathers, Michael, 40\n\nHighly Coupled Code\n\nFit\n\nData-Driven Test example,\n\n296–297\n\nData-Driven Test\n\nimplementation, 290–292\n\ndeﬁ ned, 754–755, 796 Expected State Speciﬁ cation, 464 ﬁ xture deﬁ nition, 59, 86 ﬁ xture vs. Testcase Class, 376 Scripted Tests\n\nimplementation, 286\n\nTest Automation Framework, 301\n\ntest automation tools, 54 tests as examples, 33 vs. xUnit, 57\n\nsolution, 210\n\nFitnesse\n\nHumble Object, 708 pattern naming, 576 retroﬁ tting testability, 148 Self Shunt, 578 test automation roadmap, 176 Unit Test Rulz, 307\n\nfeatures\n\nData-Driven Test\n\nimplementation, 290\n\ndeﬁ ned, 755 Scripted Test\n\nimplementation, 286\n\n“Five Whys”, 11 ﬁ xture design\n\ndeﬁ ned, 796 right-sizing Test Methods,\n\nupfront or test-by-test, 36 Verify One Condition per\n\n156–157\n\nTest, 46\n\nTestcase Class per. See Testcase\n\nxUnit sweet spot, 58\n\nClass per Feature\n\nvisibility/granularity in Test-Speciﬁ c Subclass, 581–582\n\nﬁ xture holding class variables, 797 ﬁ xture holding instance\n\nvariables, 797\n\nﬁ xture setup\n\nfeedback in test automation, xxix ﬁ le contention. See Test Run War File System Test Runner, 380 Finder Method\n\naccessing Shared Fixtures,\n\n103–104\n\nMystery Guests solution, 190 when to use, 600–601 ﬁ ne-grained testing, 33–34\n\nBack Door Manipulation, 329,\n\n333–335\n\ncleaning up, liv–lvii deﬁ ned, 797 Delegated Setup, 89–91 external resources, 740 Four-Phase Test, 358–361 Fresh Fixtures, 313–314 hybrid setup, 93\n\nwww.it-ebooks.info",
      "content_length": 1774,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 912,
      "content": "Index\n\nImplicit Setup, 91–93 In-Line Setup, 88–89 introduction, 77 matching with teardown code,\n\n98–99\n\nShared Fixtures, 104–105 speeding up with doubles,\n\n149–150 strategies, 60\n\nintroduction, 77 Lazy Setup problems, 439 persistent ﬁ xtures, 97–100 Persistent Fresh Fixtures, 314 refactoring, l–liv Shared Fixtures, 105 transient ﬁ xtures, 93–94 Verify One Condition per\n\nTest, 46\n\nﬁ xture setup patterns, 407–459\n\nﬁ xture teardown patterns, 499–519\n\nChained Test. See Chained Test Creation Method. See Creation\n\nAutomated Teardown,\n\n503–508\n\nMethod\n\nGarbage-Collected Teardown,\n\nDelegated Setup, 411–414 Implicit Setup, 424–428. See also\n\nImplicit Setup\n\nIn-line Setup, 408–410. See also\n\nIn-line Setup\n\nLazy Setup. See Lazy Setup Prebuilt Fixture. See Prebuilt\n\n500–502\n\nImplicit Teardown, 516–519. See also Implicit Teardown In-line Teardown, 509–515. See also In-line Teardown Table Truncation Teardown,\n\n661–667\n\nFixture\n\nTransaction Rollback\n\nSetup Decorator. See Setup\n\nDecorator\n\nTeardown. See Transaction Rollback Teardown\n\nSuite Fixture Setup. See Suite\n\nﬁ xtures\n\nFixture Setup\n\nFixture Setup Testcase, 456 ﬁ xture strategies\n\noverview, 58–61 persistent fresh ﬁ xtures, 62–63 shared ﬁ xture strategies, 63–65\n\nﬁ xture teardown\n\navoiding in persistent ﬁ xtures,\n\ncollisions, 100–101 database testing, 168–169 deﬁ ned, 796, 814 Four-Phase Test, 358–361 fresh. See Fresh Fixture introduction, 78 Minimal. See Minimal Fixture right-sizing Test Methods,\n\n100–101\n\n156–157\n\nBack Door Manipulation, 330 cleaning up, l–liv Complex Teardown, 206–207 data access layer testing, 173 deﬁ ned, 797 ﬁ xture strategies, 60 Four-Phase Test, 358–361 Implicit Setup, 426\n\nShared. See Shared Fixture speeding up setup with doubles,\n\n149–150\n\nStandard. See Standard Fixture Testcase Class as, 376 Testcase Class per Fixture.\n\nSee Testcase Class per Fixture transient. See transient ﬁ xtures\n\nwww.it-ebooks.info\n\n849",
      "content_length": 1907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 913,
      "content": "850\n\nIndex\n\nFlexible Test, 202–203 ﬂ uent interface, 797 For Tests Only, 219–220 foreign-key constraints, 663 forms, pattern, xxxiv–xxxv Four-Phase Test\n\nCustom Assertions, 478 ﬁ xture design, 59 introduction, 76–78 Mock Object patterns, 546 pattern description, 358–361 unit testing, 6 Verify One Condition per Test, 46\n\nData Sensitivity, 243–245 Fragile Fixture, 246–247 High Test Maintenance\n\nCost, 266 impact, 239 Interface Sensitivity, 241–242 introduction, xxiii, xxxi–xxxii,\n\n13–14\n\nOverspeciﬁ ed Software, 246 Sensitivity Equality, 246 symptoms, 239 troubleshooting, 239–240\n\nframeworks\n\nFowler, Martin, xxvi\n\ncode smells, 16 Creation Methods, 418 Custom Assertions, 117 Cut and Paste code reuse, 215 Delegated Setup, 89, 413 Eager Tests solution, 225 Multiple Test Conditions\n\nsolution, 208\n\npattern forms, xxxvi refactoring, xxxix refactoring Recorded Tests, 283 reusable test logic, 123 self-testing code, xxi Standard Fixtures, 306 state vs. behavior veriﬁ cation, 36\n\nFit. See Fit Test Automation Framework, 75,\n\n298–301 Frequent Debugging\n\navoidance with Custom\n\nAssertion, 475 causes, 248–249 impact, 249 introduction, 15 solution patterns, 249 symptoms, 248\n\nFresh Fixture\n\nCreation Method. See Creation\n\nMethod\n\nData Sensitivity solution,\n\n244–245\n\ntest smells, 9 Testcase Object exception, 385\n\nFragile Fixture\n\ndeﬁ ned, 246–247 introduction, 14, 16 setUp method misuse, 93\n\nFragile Test\n\nBehavior Sensitivity, 242–243 Buggy Tests, 260 causes, 240–241 Context Sensitivity, 245–246\n\nDelegated Setup, 411–414 example, 316 ﬁ xture strategies, 60–61 implementation, 312 Implicit Setup, 424–428 Interacting Tests solution, 231 motivating example, 315 Mystery Guests solution, 190 overview, 311 persistent, 62–63, 313–314. See also persistent ﬁ xtures\n\nrefactoring, 315\n\nwww.it-ebooks.info",
      "content_length": 1801,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 914,
      "content": "Index\n\nsetup, 313–314 test automation philosophies, 36 Test Run Wars solution, 236–237 transient, 61–62. See also\n\nmisuse of setUp method,\n\n92–93\n\nObscure Tests, 190–192 Slow Tests, 255\n\ntransient ﬁ xtures\n\nTransient Fresh Fixture, 314 when to use, 312\n\nfront door, 797 Front Door First\n\ndeﬁ ned, 40–41 Overspeciﬁ ed Software\n\nGenerated Value, 723–727 Geras, Adam, 280 Global Fixture, 430 global variables deﬁ ned, 798 instance variables as, 92\n\ngoals, test automation.\n\navoidance, 246 Fully Automated Test\n\nbehavior smells and, 15 Communicate Intent and, 41 Manual Fixture Setup\n\nSee test automation goals\n\nGorts, Sven, 537 granularity\n\ntest automation tools and,\n\n53–54\n\nsolution, 251\n\nTest-Speciﬁ c Subclass,\n\nminimizing untested code, 44–45 running, 25–26 unit testing, 6\n\n581–582\n\nGraphical Test Runner\n\nclicking through to test code,\n\nfunctional tests\n\n226–227\n\ndeﬁ ned, 798 per-functionality, 50–52\n\nFuzzy Equality Assertion deﬁ ned, 365–366 example, 368–369 external result veriﬁ cation,\n\n111–112\n\ndeﬁ ned, 378–379 green bar, 26 introduction, 79, 300 graphical user interface (GUI).\n\nSee GUI (graphical user interface)\n\ngreen bar, deﬁ ned, 798 Guaranteed In-Line Teardown,\n\nintroduction, 110\n\n233\n\nGuard Assertion\n\nG\n\nConditional Veriﬁ cation Logic\n\nsolution, 203–204\n\nGamma, Erich, 57 garbage collection, 798 Garbage-Collected Teardown design-for-testability, 7 pattern description, 500–502 persistent ﬁ xtures, 97 transient ﬁ xtures, 87–88\n\nGeneral Fixture\n\ndatabase testing, 169 deﬁ ned, 187\n\nintroduction, 80 pattern description, 490–493 removing “if” statements in\n\nTest Method, 120 GUI (graphical user interface)\n\ndeﬁ ned, 799 design for testability, 7 Interface Sensitivity, xxxii testing with Humble\n\nDialogs, 696\n\nwww.it-ebooks.info\n\n851",
      "content_length": 1754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 915,
      "content": "852\n\nIndex\n\nH\n\nHand-Built Test Double. See also\n\nHard-Coded Test Double\n\ntesting with, 140–142 when to use, 569 Hard-Coded Test Spy. See Hard-Coded Test Double\n\nConﬁ gurable Test Double,\n\n560–561\n\nproviding, 140–141\n\nHand-Coded Mock Object, 548–550 hand-coded teardown, 97–98 Hand-Coded Test Stub, 533–534 Hand-Scripted Test. See also\n\nScripted Test\n\nintroduction, 75 tools for automating, 53–54 Hand-Written Test. See Scripted Test happy path\n\ndeﬁ ned, 799 Responder use, 530 Simple Success Tests, 349–350 test automation roadmap,\n\n177–178\n\nHard-Coded Mock Object. See Hard-\n\nCoded Test Double\n\nHard-Coded Setup Decorator\n\nHard-Coded Test Stub. See also\n\nHard-Coded Test Double\n\nimplementation, 531–532 indirect input control, 179\n\nHard-Coded Value, 103 Hard-To-Test Code\n\nAsynchronous Code, 210–211 Buggy Tests, 261 code smells, 16 Developers Not Writing\n\nTests, 264\n\ndivide and test, 71–72 High Test Maintenance Cost,\n\n266–267\n\nHighly Coupled Code, 210 impact, 209 solution patterns, 209 symptoms, 209 Untestable Test Code, 211–212 hierarchy of test automation needs,\n\ndeﬁ ned, 449 example, 451–452 Hard-Coded Test Data\n\ncausing Obscure Tests, 194–196 deﬁ ned, 187 introduction, lv–lvii, 16\n\n176–177\n\nHigh Test Maintenance Cost\n\nConditional Test Logic, 200 In-Line Setup, 89 introduction, 12–13 smell description, 265–267\n\nHard-Coded Test Double conﬁ guring, 141–142 implementation, 527, 569–571 motivating example, 571 naming patterns, 576–578 overview, 568 refactoring, 572 Self Shunt/Loopback, 573 Subclassed Inner Test Double,\n\nHigher Level Language\n\nCustom Assertion, 117 Interface Sensitivity solution, 241 xUnit sweet spot, 58 Highly Coupled Code, 210 historical patterns and smells, xxxviii Hollywood principle deﬁ ned, 56, 799 test results, 79\n\n573–575, 578\n\nTest Double Class, 572–573\n\nHook, Test. See Test Hook HTML user interface sensitivity, xxxii\n\nwww.it-ebooks.info",
      "content_length": 1883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 916,
      "content": "Index\n\nHttpUnit, 755 Humble Container Adapter, 698 Humble Dialog\n\ndesign-for-testability, 7 example, 706–708 Hard-To-Test Code, 72 minimizing untested code, 45 when to use, 696–697\n\nHumble Executable\n\nasynchronous tests, 70–71 minimizing untested code, 44 motivating example, 700–702 Neverfail Test solution, 274 when to use, 697\n\nHumble Object\n\nAsynchronous Code solution, 211 Humble Dialog, 706–708 Humble Transaction\n\nIdea, 755 IeUnit\n\ndeﬁ ned, 748 Graphical Test Runner, 378\n\n“if” statements\n\nConditional Test Logic, 201 Guard Assertions, 490–491 removing, 120 IFixtureFrame, 442 ignoring tests, 270 Immutable Shared Fixture\n\ndeﬁ ned, 323 example, 326 Interacting Tests solution, 231 introduction, 61, 65 vs. Irrelevant Information, 192 Test Run Wars solution, 237\n\nimpact\n\nController, 708\n\nimplementation, 698–700 motivating example, 700–702 overview, 695–696 Poor Manís Humble Executable, 703\n\nAssertion Roulette, 224 Asynchronous Code, 211 Buggy Tests, 260 Conditional Test Logic, 201 Developers Not Writing\n\nTests, 263\n\nrefactoring, 702 True Humble Executable,\n\n703–706\n\nwhen to use, 696–698 Humble Transaction Controller\n\ndata access layer testing, 173 example, 708 when to use, 697–698\n\nEquality Pollution, 221 Erratic Tests, 228 Flexible Tests, 203 Fragile Tests, 239 Frequent Debugging, 249 General Fixtures, 191–192 Hard-Coded Test Data, 195 Hard-To-Test Code, 209 High Test Maintenance\n\nHurst, John, 670–671 hybrid setup, 93\n\nCost, 265\n\nI\n\nIDE (integrated development\n\nenvironment)\n\ndeﬁ ned, 799 introduction, 78 refactoring, xxxix\n\nHighly Coupled Code, 210 Indirect Testing, 197 Irrelevant Information, 193 Manual Intervention, 250 Mystery Guests, 189 Neverfail Tests, 274 Nondeterministic Tests, 237\n\nwww.it-ebooks.info\n\n853",
      "content_length": 1740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 917,
      "content": "854\n\nIndex\n\nObscure Tests, 186 Production Bugs, 268 Slow Tests, 253 Test Code Duplication, 214 Test Dependency in Production, 221\n\nTest Hooks, 218–219 Test Logic in Production, 217 Test Run Wars, 236 For Tests Only, 220 Untestable Test Code, 211 Untested Requirements, 273\n\nImplicit Setup\n\nvs. Four-Phase Test, 360–361 introduction, 7, 77 matching with teardown code,\n\nincremental tests, 322 In-Database Stored Procedure Test\n\ndatabase testing, 172 example, 658–659 implementation, 655–656 Independent Tabular Test, 612–613 independent testing. See Keep Tests\n\nIndependent indirect input\n\nalternative path veriﬁ cation, 179 controlling, 128–129 controlling in Layer Tests, 341 deﬁ ned, 800 importance of, 126 Test Doubles, 125–126\n\nindirect output\n\n98–99\n\nBehavior Veriﬁ cation.\n\npattern description, 424–428 pattern naming, 577 reusing test code with, 162 transient ﬁ xtures, 91–93\n\nImplicit Teardown\n\nComplex Teardown solution,\n\n206–207 database, 100 vs. Four-Phase Test, 360–361 pattern description, 516–519 persistent ﬁ xtures, 98–99 Self-Checking Tests with, 108\n\nImposter. See Test Double incremental delivery\n\nSee Behavior Veriﬁ cation\n\ndeﬁ ned, 800 importance of, 126–127 registries, 541 Test Doubles, 125–126 veriﬁ cation, 130–133, 178–180 verifying in Layer Tests, 341\n\nIndirect Testing deﬁ ned, 187 Fragile Tests cause, 240 Obscure Tests cause, 196–199 testability, 70–71 Infrequently Run Test\n\nFrequent Debugging cause,\n\nagile development, 239 deﬁ ned, 799\n\n248–249\n\nProduction Bugs cause, 268–269\n\nincremental development deﬁ ned, 799–800 test automation philosophies,\n\n33–34\n\ninheritance\n\nreusing test code, 164 reusing test ﬁ xtures, 62 injected values, Test Stub.\n\nIncremental Tabular Test\n\nSee Test Stub\n\nimplementation, 609–610 Parameterized Test patterns,\n\nInjection, Parameter. See Parameter\n\nInjection\n\n613–614\n\nin-line Four Phase Test, 360\n\nwww.it-ebooks.info",
      "content_length": 1881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 918,
      "content": "Index\n\nin-line resources, 736–737 In-line Setup\n\nMock Object, 547 retroﬁ tting testability,\n\nintroduction, 77 matching with teardown code,\n\n146–148\n\ninstance methods\n\n98–99\n\nMystery Guest solution, 190 pattern description, 408–410 transient ﬁ xtures, 88–89\n\nIn-line Teardown\n\ndeﬁ ned, 800–801 with Test Helper, 645, 647\n\ninstance variables\n\nconverting for Implicit Setup, 427 Data-Driven Tests using Fit\n\nexamples, 512–515 implementation, 510–511 motivating example, 511 Naive In-Line Teardown, 512 overview, 509 of persistent ﬁ xtures, 98–99 refactoring, 512 when to use, 510 In-Memory Database, 553 inner class\n\nFramework, 297\n\ndeﬁ ned, 801 Fresh Fixtures, 313 as global variables, 92 Reuse Tests for Fixture Setup,\n\n418–419\n\nwith Test Speciﬁ c Subclass, 558 Testcase Class per Fixture, 632\n\ninstances\n\nanonymous, 535–536, 786 deﬁ ned, 800\n\nreusing, 63 Testcase Object exception,\n\n384–385\n\nInner Test Double\n\nintegrated development environment\n\nexample, 573–574 Hard-Coded Test Double\n\n(IDE). See IDE (integrated development environment)\n\nimplementation, 570–571\n\nSubclassed from Pseudo-Class,\n\nIntegration Build, 4 Intent-Revealing Name\n\n574–575, 578\n\nTest Spy implementation, 541\n\ninput\n\nderived, 719 indirect. See indirect input naming conventions, 158–159\n\nCustom Assertion, 474–475 Implicit Setup, 92 Parameterized Test, 608 Test Utility Method, 602–603\n\nInteracting Test Suites,\n\n231–232\n\ninside-out development\n\nInteracting Tests\n\nvs. outside-in development, 34–36 State Veriﬁ cation, 463\n\navoiding with Database\n\nSandbox, 650–653\n\ninstalling Test Doubles, 528\n\navoiding with Delta Assertion,\n\nDependency Injection, 143–144,\n\n111, 486\n\n679–680\n\nDependency Lookup, 144–145 Fake Object, 554 introduction, 143\n\ncaused by Shared Fixture, 63 Chained Tests, 455 customer testing, 5–6 database testing, 169\n\nwww.it-ebooks.info\n\n855",
      "content_length": 1833,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 919,
      "content": "856\n\nIndex\n\nErratic Test cause, 229–231 introduction, 15 Keep Tests Independent, 43\n\nJBehave\n\ndeﬁ ned, 748 tests as examples, 33\n\ninteraction point, 801 interaction styles, 67–71 Interaction Testing. See Behavior\n\nVeriﬁ cation\n\nInterface Sensitivity\n\nJFCUnit, 755 JMock\n\nConﬁ guration Interface, 560 deﬁ ned, 755 Test Double implementation,\n\ndeﬁ ned, 241–242 introduction, xxxii, 13\n\ninterfaces\n\n140\n\nJohnson, Rod, 670 JUnit\n\nConﬁ guration Interface, 560 deﬁ ned, 801 GUI. See GUI (graphical user\n\ninterface)\n\noutgoing interface, 804–805 standard test, 378 Test Runner. See Test Runner Use the Front Door First, 40–41\n\ninternal recording tools, 56 interpreters in Data-Driven Tests.\n\nSee Data-Driven Test\n\ndeﬁ ned, 748 Expected Exception Test\n\nexpression, 351 ﬁ xture design, 59 language-speciﬁ c terminology, xl Suite Fixture Setup support,\n\n442–443\n\nTest Automation Framework, 300\n\ntest automation tools, 55 Testcase Object exception,\n\nIntervention, Manual. See Manual\n\n384–385\n\nIntervention\n\ntesting stored procedures, 657\n\nIntroduce Explaining Variable\n\nrefactoring, lvii–lviii\n\nK\n\nIoC (inversion of control) framework\n\ndeﬁ ned, 801 for Dependency Injection, 680\n\nKeep Test Logic Out of Production\n\nCode\n\nirrelevant information deﬁ ned, 187 Obscure Test, 192–194\n\nminimizing risk, 24 principle, 45 test code organization, 164–165\n\nIsolate the SUT, 43–44 iterative development, 802\n\nKeep Tests Independent\n\nrunning, 26 test automation principles,\n\nJ\n\n42–43\n\nusing Fake Object. See Fake\n\nJava\n\nObject\n\nlanguage-speciﬁ c xUnit\n\nterminology, xl\n\ntest code packaging, 165\n\nKerievsky, Joshua, xxxix keys, Literal Values as, 714 King, Joseph, 319–321\n\nwww.it-ebooks.info",
      "content_length": 1667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 920,
      "content": "Index\n\nL\n\nleakage, resource\n\nlanguages\n\nErratic Tests, 233 persistent ﬁ xtures, 99\n\nterminology, xl–xli variations in Built-in Assertions,\n\nlearning styles, xxxix–xl legacy software\n\n110–111\n\nxUnit implementations, 76 language-speciﬁ c xUnit terminology,\n\nBuggy Tests, 261–262 deﬁ ned, 802 tests as safety net, 24\n\nxl–xli\n\n“Law of Raspberry Jam”, xxv Layer Test\n\nlenient Mock Object deﬁ ned, 138 when to use, 545\n\nBusiness Layer Tests, 344–345 database testing, 169–171 implementation, 340–341 motivating example, 341–342 overview, 337–338 Presentation Layer Tests, 343 refactoring, 342 Subcutaneous Tests, 343–344 when to use, 338–340\n\nlayer-crossing tests deﬁ ned, 802 testability, 67–69 Layered Architecture\n\ndesign-for-testability, 7 layer-crossing tests, 67–69\n\nlightweight implementation using Fake Object. See Fake Object\n\nLiteral Value\n\nHard-Coded Test Data, 195 pattern description, 714–717\n\nlocal variables\n\nconverting in Implicit\n\nSetup, 427 deﬁ ned, 802 Fresh Fixtures, 313\n\nLonely Test\n\ncaused by Chained Test. See\n\nChained Test Erratic Tests, 232 Interacting Tests.\n\nLazy Initialization, 435 Lazy Setup\n\nDecorated, 449–450 examples, 439–440 implementation, 436–437 Interacting Tests solution, 231 motivating example, 437–438 overview, 435 vs. Prebuilt Fixtures, 431–432 refactoring, 439 Shared Fixture, 64, 105 when to use, 436\n\nSee Interacting Tests\n\nLong Tests. See Obscure Test Loopback. See Self Shunt Loop-Driven Test\n\nimplementation, 610 Parameterized Test, 614–615\n\nloops\n\nas Conditional Test Logic, 201 eliminating, 121 Production Logic in Test cause,\n\n204–205\n\nLazy Teardown\n\nexample, 665–666 implementation, 663–664\n\nLost Tests\n\navoiding, 597 Production Bugs cause,\n\n269–271\n\nwww.it-ebooks.info\n\n857",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 921,
      "content": "858\n\nIndex\n\nM\n\nMackinnon, Tim, 149 macros, Assertion Methods as, 364 maintenance\n\nHigh Test Maintenance Cost. See High Test Maintenance Cost\n\noptimizing, 180–181 test automation goals, 27–29 Manual Event Injection, 251–252 Manual Fixture Setup, 250–251 Manual Intervention impact, 250 introduction, 15 Manual Event Injection,\n\n251–252\n\nManual Fixture Setup, 250–251 Manual Result Veriﬁ cation, 251 symptoms, 250\n\nManual Result Veriﬁ cation, 251 manual testing\n\nmetatests, 803 method attributes deﬁ ned, 803 Expected Exception Tests, 354 Test Discovery using, 397 Test Method Selection\n\nusing, 405\n\nmethod names\n\nlanguage-speciﬁ c xUnit terminology, xl–xli\n\nTest Method Discovery, 395–396\n\nmethods\n\ndiagramming notation, xlii instance. See instance methods setUp. See setUp method static, 809 suite, 399 tearDown. See tearDown method Template Method, 164 test commands, 82 veriﬁ cation. See result\n\ndeﬁ ned, 802 right-sizing Test Methods, 154\n\nMarrick, Brian\n\nveriﬁ cation\n\nMiller, Jeremy, 687 Minimal Fixture\n\npurpose of tests, 51 right-sizing Test Methods, 155 tests as examples, 33\n\nMaslow, 176 MbUnit\n\ndeﬁ ned, 749 Parameterized Test\n\nexternal result veriﬁ cation, 112 General Fixtures solution, 192 minimizing data, 738–739 misuse of setUp method, 93 pattern description, 302–304 strategy, 62–63 test automation philosophies, 36\n\nimplementation, 608–609 Tabular Test with framework\n\nsupport, 614\n\nMessage, Assertion. See Assertion\n\nMinimize Test Overlap, 44 Minimize Untestable Code, 44–45 Missing Assertion Message, 226–227 Missing Unit Test\n\nMessage\n\nmessages, failure. See failure\n\nDefect Localization, 23 Production Bugs, 271\n\nmessages meta objects\n\nData-Driven Tests, 290 deﬁ ned, 803\n\nmixins\n\ndeﬁ ned, 803 Test Helper Mixins, 639,\n\n641–642\n\nwww.it-ebooks.info",
      "content_length": 1769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 922,
      "content": "Index\n\nMock Object\n\nN\n\nConﬁ gurable. See Conﬁ gurable\n\nTest Double\n\nconﬁ guring, 141–142 deﬁ ned, 133 examples, 548–550 Expected Behavior Speciﬁ cation,\n\nNaive In-line Teardown\n\ndeﬁ ned, 511 example, 512 of persistent ﬁ xtures, 97 Naive xUnit Test Interpreter,\n\n470–471\n\nimplementation, 546–548 motivating example, 548 Overspeciﬁ ed Software\n\n292–293\n\nNamed State Reaching Method,\n\n417–418\n\nNamed Test Suite\n\ncause, 246\n\noverview, 544–545 refactoring, 548 Test Double patterns, 525 Test Doubles, 137–139 unit testing, 6 vs. Use the Front Door First, 40 verifying indirect output,\n\nexamples, 594–598 implementation, 594 introduction, 160–161 overview, 592–593 refactoring, 594 Test Enumeration, 400 when to use, 593–594\n\nnames\n\n131–133\n\nwhen to use, 545 xUnit terminology, 741–744\n\nDependency Lookup, 693–694 intent-revealing. See\n\nIntent-Revealing Name\n\nMockMaker, 560 modules, 803–804 Move Method, 413 MSTest, 749 Mugridge, Rick, xxiv multimodal tests, 687 multiple-condition tests\n\nreferring to patterns and smells,\n\nxxxviii\n\nScripted Test, 287 Suite Fixture Setup, 446\n\nnaming conventions\n\nassertion-identifying\n\nmessages, 371\n\nConditional Test Logic,\n\n207–208\n\ndeﬁ ned, 45–47\n\nMultiresource In-line Teardown,\n\n513–514 MySql, 651 Mystery Guest\n\nmaking resources unique,\n\n737–738\n\npatterns, 576–578 vs. test code organization,\n\n158–159\n\nTest Method Discovery,\n\n395–396\n\ndeﬁ ned, 187 Obscure Test cause, 188–190\n\nTestcase Class per Class, 618 Testcase Class per Feature, 626 Testcase Class per Fixture, 632 For Tests Only solution, 220\n\nwww.it-ebooks.info\n\n859",
      "content_length": 1561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 923,
      "content": "860\n\nIndex\n\nneed-driven development\n\nObject Mother\n\nBehavior Veriﬁ cation, 469 deﬁ ned, 804 testing with doubles, 149 using Mock Objects, 545\n\nin Delegated Setup, 90–91 when to use, 644–645\n\nobject technology, xxxix–xl Object Transaction Rollback\n\nNeverfail Test, 274 New River Gorge bridge, xxvi Newkirk, James, 384–385 NMock, 756 No Test Risk, 24–25 Nondeterministic Test dangers of, 26–27 Erratic Test, 237–238 Generated Values cause, 723–724\n\nTeardown, 673–674\n\nobject-oriented programming\n\nlanguage (OOPL), 76\n\nobject-relational mapping (ORM).\n\nSee ORM (object-relational mapping)\n\nobjects\n\nCreation Method. See Creation\n\nMethod\n\nnotation, diagramming, xlii Null Object vs. Dummy Object, 730 null values in Dummy Objects,\n\n729–732\n\nNUnit\n\ndetermining necessary,\n\n303–304\n\ndiagramming notation, xlii fake. See Fake Object Test Suite Objects. See Test Suite\n\ndeﬁ ned, 749 Expected Exception Test\n\nObject\n\nTestcase. See Testcase Object\n\nexpression, 351 ﬁ xture design, 59 Interacting Test Suites, 232 Suite Fixture Setup support,\n\nObscure Test\n\navoiding with Custom Assertion,\n\n475\n\navoiding with Separation of Con-\n\n442–443\n\nTest Automation Frameworks,\n\n300\n\ntest automation ways and\n\nmeans, 55\n\ntest ﬁ xtures, 814 Testcase Classes, 376 Testcase Object exception,\n\ncerns, 28–29 Buggy Test, 261 causes, 186–187 vs. Communicate Intent, 41 customer testing, 5 database testing, 169 Eager Test, 187–188 General Fixture, 190–192 Hard-Coded Test Data,\n\n384–385\n\n194–196\n\nO\n\nHigh Test Maintenance Cost,\n\n266\n\nObject Attribute Equality Assertion,\n\n476\n\nObject Factory\n\nDependency Lookup, 688 installing Test Double, 145\n\nimpact, 186 Indirect Testing, 196–199 introduction, xlvi, 12–13, 16 Irrelevant Information, 192–194 Mystery Guests, 188–190\n\nwww.it-ebooks.info",
      "content_length": 1759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 924,
      "content": "Index\n\noptimizing test execution/\n\noverlapping tests\n\nmaintenance, 180\n\nsmells, 10 solution patterns, 199 symptoms, 186 observation points\n\nminimizing, 44 Too Many Tests, 256–257\n\nOverspeciﬁ ed Software\n\navoiding with Fake Objects,\n\n552\n\ndeﬁ ned, 804 test automation strategy, 66–67\n\nO’Grady, Ted, 319–321 One Bad Attribute\n\nFragile Tests, 246 testing with doubles, 150 Use the Front Door First, 40\n\nexample, 721–722 introduction, xxiii, 90 Minimal Fixtures, 304 when to use, 719 OOPL (object-oriented\n\nprogramming language), 76\n\noptimism, resource, 189, 233–234 order of tests, 456 organization, test. See test\n\nP\n\nParameter Injection\n\nexample, 683 implementation, 680 installing Test Doubles, 144 Parameterized Anonymous Creation\n\nMethod, 417\n\nParameterized Creation Method\n\norganization; test organization patterns\n\nORM (object-relational mapping)\n\ndeﬁ ned, 804 Table Truncation Teardown, 663 Table Truncation Teardown\n\ndeﬁ ned, 417 Delegated Setup, 90 example, xxiii, 420–421 Irrelevant Information\n\nsolution, 193\n\nParameterized Setup Decorator\n\nusing, 667\n\nTransaction Rollback\n\ndeﬁ ned, 449 example, 452–453\n\nTeardown, 671\n\nParameterized Test\n\nOutcome Assertions, Stated. See\n\nStated Outcome Assertion\n\noutcome veriﬁ cation patterns. See\n\nresult veriﬁ cation patterns\n\noutcome-describing Veriﬁ cation\n\nMethod, 117\n\nexample, 611–612 extracting. See Data-Driven Test further reading, 615–616 implementation, 608–610 Incremental Tabular Test,\n\n613–614\n\noutgoing interface, 804–805 out-of-order calls, 138 output, indirect. See indirect output outside-in development\n\nBehavior Veriﬁ cation, 469 vs. inside-out development,\n\nIndependent Tabular Test,\n\n612–613\n\nLoop-Driven Tests, 614–615 motivating example, 610–611 overview, 607–608 reducing Test Code Duplication,\n\n34–36\n\nOvercoupled Software, 40\n\n118–119\n\nrefactoring, 611\n\nwww.it-ebooks.info\n\n861",
      "content_length": 1851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 925,
      "content": "862\n\nIndex\n\nTabular Test with framework\n\nsupport, 614\n\nTest Utility Method, 602 when to use, 608\n\nparameters, arguments as, 729 “Pass-Fail-Fail”, 234–235 pattern language\n\ndeﬁ ned, xxxv–xxxvi, 805 pattern naming, 577\n\nmanaging, 103–105 overview, 95–96 Slow Tests cause, 102 Table Truncation Teardown. See Table Truncation Teardown teardown avoidance, 100–101 tearing down, 97–100 test strategy patterns, 313–314 what’s next, 106 Persistent Fresh Fixture\n\nPattern Languages of Programming\n\n(PLoP), 576\n\npatterns\n\naliases and variations, 767–784 database. See database patterns deﬁ ned, 805 design-for-testability. See\n\nbuilding, 88 deﬁ ned, 60–61 strategies, 62–63 Personal Oracle, 651 philosophy, test automation. See test\n\nautomation philosophies\n\ndesign-for-testability patterns ﬁ xture setup. See ﬁ xture setup\n\nPHPUnit, 749 PLoP (Pattern Languages of\n\npatterns\n\nProgramming), 576\n\nresult veriﬁ cation. See result\n\nPluggable Behavior\n\nveriﬁ cation patterns\n\ntest automation introduction,\n\nin Named Test Suites, 597 Testcase Object\n\nxxxiv–xxxviii\n\nimplementation, 383\n\nTest Double. See Test Double test organization. See test organization patterns\n\npollution\n\nEquality Pollution, 221–222 Shared Fixture, 326\n\ntest strategy. See test strategy\n\npatterns\n\ntestability, 67–71 value. See value patterns xUnit basics. See xUnit basics\n\npatterns peeling the onion, 11 per-functionality test, 50–52 Perrotta, Paolo, 537 Per-Run Fixtures, 323 persistence layer, 339–340 persistence resources, 504 persistent ﬁ xtures, 95–106\n\npolymorphism, 805 Poor Manís Humble Executable, 703\n\nPoor Man’s Humble Object implementation, 699 Transaction Rollback\n\nTeardown, 671\n\nPoppendieck, Mary, 51 Pragmatic Unit Testing, 743 Prebuilt Fixture\n\nexamples, 432–434 implementation, 430–431 motivating example,\n\ndatabase testing, 168–169 issues caused by, 96\n\n431–432\n\noverview, 429–430\n\nwww.it-ebooks.info",
      "content_length": 1879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 926,
      "content": "Index\n\nrefactoring, 432 Shared Fixture strategies, 64 Shared Fixtures, 104–105 Unrepeatable Tests cause, 235\n\npresentation layer\n\ndeﬁ ned, 805 Layer Tests example, 343 testing, 338–339\n\npresentation logic, 805 Preserve Whole Object refactoring,\n\nxlviii–xlix\n\nprinciples\n\nlist of, 757–759 patterns vs., xxxv–xxxvi test automation. See test automation principles Private Fixture. See Fresh Fixture private methods, 586 problem statements, xxxvi–xxxvii Procedural Behavior Veriﬁ cation\n\nMissing Unit Tests, 271 Neverfail Tests, 274 overview, 268 reducing risk, 181 Untested Code, 271–272 Untested Requirements, 272–274\n\nproduction code deﬁ ned, 806 keeping test logic out of, 45 Production Logic in Test, 204–205 proﬁ ling tools, 254 Programmatic Test. See Scripted Test programmer tests, 806 project smells, 259–274\n\nBuggy Tests, 260–262 deﬁ ned, 806 Developers Not Writing Tests,\n\n263–264\n\nHigh Test Maintenance Cost,\n\n265–267\n\ndeﬁ ned, 470 example, 472–473 indirect outputs, 131 introduction, 112–113 Test Spy usage, 137 Procedural State Veriﬁ cation\n\noverview, 12–13 Production Bugs. See Production\n\nBugs property tests, 52 Pseudo-Object\n\nHard-Coded Test Double\n\ndeﬁ ned, 463–464 example, 466 introduction, 109 Procedural Test Stub deﬁ ned, 526 introduction, 135–136 when to use, 531\n\nProcedure Test, Stored. See Stored\n\nProcedure Test\n\nprocedure variables, 805–806 production, 806 Production Bugs\n\nimplementation, 570–571 Inner Test Double Subclassed\n\nfrom Pseudo-Class, 574–575, 578\n\ntesting with doubles, 140–141\n\npull system, 806–807 Pull-Up Method refactoring Delegated Setup, 413 moving reusable test logic, 123 Testcase Superclass, 640\n\nPushdown Decorator, 450 PyUnit\n\nInfrequently Run Tests, 268–269 introduction, 12–13 Lost Tests, 269–271\n\ndeﬁ ned, 749 Test Automation Framework,\n\n300\n\nwww.it-ebooks.info\n\n863",
      "content_length": 1819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 927,
      "content": "864\n\nIndex\n\nQ\n\nQA (quality assurance), 22–23 QaRun, 244 QTP (QuickTest Professional) Data-Driven Tests, 290 deﬁ ned, 756 record and playback tools, 282 Test Automation Framework, 301\n\nred bar, 807 Refactored Recorded Tests commercial, 283–284 overview, 280\n\nrefactoring. See also test refactorings\n\nAssertion Message, 372 Assertion Method, 368 Automated Teardown,\n\n506–507\n\nquality assurance (QA), 22–23 QuickTest Professional (QTP).\n\nSee QTP (QuickTest Professional)\n\nR\n\nrandom values\n\nNondeterministic Tests, 238 Random Generated Values, 724\n\nRecord and Playback Test, 13 record and playback tools introduction, xxxi Recorded Tests, 282–283 xUnit sweet spot, 58\n\nRecorded Test\n\nbuilt-in test recording,\n\nBack Door Manipulation, 333 Chained Test, 458 Conﬁ gurable Test Double, 463 Creation Method, 420 Custom Assertion, 480 Database Sandbox, 653 Data-Driven Test, 294 deﬁ ned, 807 Delegated Setup, 413 Delta Assertion, 488 Dependency Injection, 682 Dependency Lookup, 690–691 Derived Value, 720 Dummy Object, 731 Fake Object, 555–556 Fresh Fixture, 315–316 Garbage-Collected Teardown, 502\n\n281–282\n\ncommercial record and\n\nplayback tool, 282–283\n\ncustomer testing, 5 Data-Driven Tests and, 289 implementation, 280–281 Interface Sensitivity, 241 overview, 278–279 refactored commercial recorded\n\ntests, 283–284\n\nvs. Scripted Tests, 286 smells, 10 tools, 56 tools for automating, 53–54 when to use, 279–280\n\nGenerated Value, 725 Guard Assertion, 492 Hard-Coded Test Double, 572 Humble Object, 702 Implicit Setup, 427 Implicit Teardown, 518–519 In-line Setup, 410 In-line Teardown, 512 Layer Test, 342 Lazy Setup, 439 Literal Value, 716 Mock Object, 548 Named Test Suite, 594 Parameterized Test, 611\n\nRecording Test Stub. See Test Spy\n\nwww.it-ebooks.info",
      "content_length": 1751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 928,
      "content": "Index\n\nPrebuilt Fixture, 432 Setup Decorator, 451 Shared Fixture, 324 Standard Fixture, 309–310 State Veriﬁ cation, 465–466 Stored Procedure Test, 658 Suite Fixture Setup, 444 Table Truncation Teardown,\n\nRelated Generated Values example, 726–727 implementation, 725 Remoted Stored Procedure Test\n\nexample, 659–660 implementation, 656–658 introduction, 172\n\nRepeatable Test\n\n664–665\n\nTest Discovery, 395 Test Helper, 646 Test Spy, 541–542 Test Stub, 533 Test Utility Method, 605 Testcase Class per Feature,\n\ndeﬁ ned, 26–27 indirect inputs control, 179 Replace Dependency with Test\n\nDouble refactoring\n\nBehavior Veriﬁ cation, 472 deﬁ ned, 739\n\nRepository\n\n627–628\n\nTestcase Class per Fixture,\n\n634–635\n\nData-Driven Test ﬁ les, 290 persistent objects, 90 source code, 24, 79, 234,\n\nTestcase Superclass, 640 Test-Speciﬁ c Subclass, 584 Transaction Rollback\n\n561, 656\n\ntest code, 164, 561 Requirement, Untested.\n\nTeardown, 672\n\nSee Untested Requirement\n\nUnﬁ nished Test Assertion, 496\n\nRefactoring: Improving the\n\nReSharper, 756 Resource Leakage\n\nDesign of Existing Code (Fowler), 9, 16\n\nErratic Tests, 233 persistent ﬁ xtures, 99\n\nreferences, 819–832 reﬂ ection\n\nResource Optimism, 189, 233–234 resources\n\ndeﬁ ned, 807 Test Discovery, 393 Testcase Object\n\nexternal, 740 in-line, 736–737 unique, 737–738\n\nimplementation, 383\n\nResponder\n\nRegistry\n\nconﬁ gurable, 691–692 in Dependency Lookup, 688–689 Interacting Tests, 230 Test Fixture, 644\n\nregression tests\n\ndeﬁ ned, 807 Recorded Tests. See Recorded\n\nTest\n\nScripted Tests, 285–287\n\ndeﬁ ned, 524 examples, 533–535 indirect input control, 179 introduction, 135 when to use, 530 response time tests, 52 result veriﬁ cation, 107–123\n\nBehavior Veriﬁ cation, 112–114 Conditional Test Logic avoidance, 119–121\n\nwww.it-ebooks.info\n\n865",
      "content_length": 1773,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 929,
      "content": "866\n\nIndex\n\nData Sensitivity, 243–245 deﬁ ned, 807 Four-Phase Test, 358–361 Mock Object, 547–548 other techniques, 121–122 reducing Test Code Duplication,\n\n114–119\n\nRobust Tests\n\ndeﬁ ned, 29 indirect inputs control, 179 role-describing arguments, 725 root cause analysis deﬁ ned, 808 smells, 11\n\nreusable test logic, 123 Self-Checking Tests, 107–108 State Veriﬁ cation, 109–112 result veriﬁ cation patterns, 461–497 Behavior Veriﬁ cation. See Behavior Veriﬁ cation\n\nround-trip tests\n\ndeﬁ ned, 808 introduction, 67–69 Layer Tests, 340–341\n\nrow tests. See Tabular Test RSpec\n\nCustom Assertion. See Custom\n\nAssertion\n\nDelta Assertion, 485–489 Guard Assertion, 490–493 State Veriﬁ cation. See State Veri-\n\nﬁ cation\n\ndeﬁ ned, 750 ﬁ xture design, 59 tests as examples, 33\n\nrunit\n\ndeﬁ ned, 750 Test Automation\n\nUnﬁ nished Test Assertion,\n\nFrameworks, 300\n\n494–497\n\nrunning tests\n\nresults, test\n\ndeﬁ ned, 815 introduction, 79–80 Retrieval Interface, 137, 540 retrospective, 807–808 reusable test logic\n\nintroduction, 79 structure, 81 test automation goals, 25–27\n\nruntime reﬂ ection, 393\n\nS\n\nCreation Method, 418–419 ﬁ xture setup patterns, 422–423 organization, 162–164 result veriﬁ cation, 123 Test Code Duplication, 214–215 Test Utility Method. See Test\n\nSaboteur\n\ndeﬁ ned, 135 example, 535–536 inside-out development, 35 Test Double patterns, 524 when to use, 530\n\nUtility Method\n\nReuse Tests for Fixture Setup, 90 Robot User Test. See Recorded Test robot user tools\n\ndeﬁ ned, 55–56 introduction, xxxi Test Automation Framework,\n\n299\n\nSafety Net\n\nBuggy Tests, 260 tests as, 24 sample code, xli–xlii screen scraping, 241 Scripted Test\n\nCommunicate Intent, 41 customer testing, 5\n\nwww.it-ebooks.info",
      "content_length": 1693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 930,
      "content": "Index\n\nData-Driven Tests and, 289 introduction, 75 pattern description, 285–287 vs. Recorded Tests, 279 smells, 10 UI, 55 Verify One Condition per\n\nTest, 46\n\nSelf Shunt\n\ndata. See Data Sensitivity interface. See Interface\n\nSensitivity\n\nSeparation of Concerns, 28–29 Service Facade, 71–72 service layers fake, 553 tests, 7, 339 Service Locator\n\nBehavior Veriﬁ cations, 113 example, 573 Hard-Coded Test Double implementation, 570\n\npattern naming, 576 Test Spy implementation,\n\nin Dependency Lookup.\n\nSee Dependency Lookup installing Test Doubles, 145\n\nservice objects, 808 Setter Injection\n\nConﬁ guration Interface\n\n540–541 Self-Call, 582 Self-Checking Test\n\nAssertion Method usage, 362 Conditional Test Logic\n\nsolution, 201\n\ndeﬁ ned, 80 happy path code, 178 introduction, 107–108 running, 26 Self-Describing Value example, 717 Literal Value patterns, 715\n\nself-testing code, xxi self-tests, built-in deﬁ ned, 788 test ﬁ le organization, 164\n\nusing, 564\n\nexample, 684–685 implementation, 681 installing Test Doubles, 143\n\nsetters, 808 setup, ﬁ xtures. See ﬁ xture setup Setup Decorator\n\nexamples, 451–453 implementation, 448–450 Implicit Setup, 426 motivating example, 450–451 overview, 447–448 refactoring, 451 Shared Fixture strategies, 64,\n\n104–105\n\nwhen to use, 448\n\nsetUp method\n\nSensitive Equality\n\nFragile Tests, 246 test-ﬁ rst development, 32\n\nsensitivities\n\nImplicit Setup, 91–92, 424–428 misuse of, 92–93 pattern naming, 577 Setup Decorator. See Setup\n\nautomated unit testing,\n\nDecorator\n\nxxxi–xxxii\n\nSuite Fixture Setup. See Suite\n\nbehavior. See Behavior Sensitivity Buggy Tests cause, 260 context. See Context Sensitivity\n\nFixture Setup\n\nshadows, diagramming notation, xlii Shank, Clint, 457–458, 613, 616\n\nwww.it-ebooks.info\n\n867",
      "content_length": 1740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 931,
      "content": "868\n\nIndex\n\nShared Fixture. See also Standard\n\nFixture\n\nintroduction, 77 pattern description, 349–350\n\nBehavior Veriﬁ cation, 108 Chained Test. See Chained Test customer testing, 5 Data Sensitivity cause, 243 database testing, 169 deﬁ ned, 60–61 Delta Assertions, 111 example, 324–325 Immutable. See Immutable\n\nThe simplest thing that could\n\npossibly work (STTCPW), 810\n\nSingle Glance Readable.\n\nSee Communicate Intent\n\nSingle Layer Test. See Layer Test Single Test Suite\n\nexample, 596–597 Lost Tests solution, 270 when to use, 593–594\n\nShared Fixture\n\nImmutable Shared Fixtures, 326 implementation, 322–323 incremental tests, 322 Interacting Tests cause, 229–231 introduction, 15, 63–65 Lazy Setup. See Lazy Setup managing, 103–105 motivating example, 323–324 in Nondeterministic Tests, 27 overview, 317 Prebuilt Fixture. See Prebuilt\n\nsingle tests, 161–162 Single-Condition Test\n\nEager Tests solution, 225–226 Obscure Tests solution, 188 principles. See Verify One\n\nCondition per Test\n\nunit testing, 6\n\nSingle-Outcome Assertion\n\nAssertion Method, 366–367 deﬁ ned, 365 example, 369\n\nSingleton\n\nFixture\n\nin Dependency Lookup,\n\nrefactoring, 324 Setup Decorator. See Setup\n\nDecorator\n\n688–689\n\nInteracting Tests, 230 retroﬁ tting testability, 146–147\n\nSlow Tests cause, 318–321 Suite Fixture Setup. See Suite\n\nFixture Setup\n\nSingleton, Substituted example, 586–587 when to use, 581\n\nTable Truncation Teardown.\n\nSee Table Truncation Teardown\n\nTest Run Wars cause, 236 Unrepeatable Tests cause, 235 using Finder Methods, 600–601 when to use, 318\n\nskeletons, 744 Slow Component Usage, 254 Slow Tests\n\nAsynchronous Tests, 255–256 avoiding with Shared Fixture,\n\n318–321\n\nShared Fixture Guard Assertion,\n\n492–493\n\nShared Fixture State Assertion, 491 Simple Success Test\n\ndatabase testing, 168 design for testability, 7 due to Transaction Rollback\n\nTeardown, 669\n\nexample, 352–353 happy path code, 177\n\nGeneral Fixtures, 255 impact, 253\n\nwww.it-ebooks.info",
      "content_length": 1947,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 932,
      "content": "Index\n\nintroduction, 15 optimizing execution, 180 persistent ﬁ xtures, 102 preventing with Fake Object.\n\nsolution patterns, code smells Asynchronous Code, 211 Conditional Veriﬁ cation Logic,\n\n203–204\n\nSee Fake Object preventing with Test\n\nDouble, 523\n\nSlow Component Usage, 254 symptoms, 253 Too Many Tests, 256–257 troubleshooting, 253–254\n\nsmells, test. See test smells Smith, Shaun, 39 Smoke Test\n\nCut and Paste code reuse, 215 Eager Test, 188 Equality Pollution, 222 Flexible Test, 203 General Fixture, 192 Hard-Coded Test Data, 196 Hard-To-Test Code, 209 Highly Coupled Code, 210 Indirect Testing, 197–199 Irrelevant Information, 193 Multiple Test Conditions,\n\ndevelopment process, 4 suites, 597–598 Test Discovery, 394\n\nsniff test\n\ndeﬁ ned, xxxviii test smells, 10\n\nsolution patterns, behavior smells Asynchronous Tests, 256 Behavior Sensitivity, 242–243 Context Sensitivity, 246 Data Sensitivity, 243–245 Eager Tests, 225–226 Frequent Debugging, 249 General Fixture, 255 Interacting Test Suites, 232 Interacting Tests, 231 Interface Sensitivity, 241–242 Manual Intervention,\n\n250–252\n\n207–208\n\nMystery Guests, 190 Obscure Tests, 199 Production Logic in Test, 205 Test Code Duplication, 115–216 Test Dependency in Production, 221\n\nTest Hook, 219 For Tests Only, 220 Untestable Test Code, 212\n\nsolution patterns, project smells Buggy Test, 261–262 Infrequently Run Test, 269 Lost Test, 270–271 Missing Unit Test, 271 Neverfail Test, 274 Untested Code, 272 Untested Requirements, 274\n\nMissing Assertion Messages,\n\n226–227\n\nSpecial-Purpose Suite, 595–596 speciﬁ cation\n\nResource Leakage, 233 Resource Optimism, 234 Slow Component Usage, 254 Test Run War, 236–237 Too Many Tests, 257 Unrepeatable Tests, 235\n\nExpected Behavior, 470–471 Expected Behavior\n\nexample, 473\n\nExpected Object example, 466 Expected State, 464–465 tests as, xxxiii, 22\n\nwww.it-ebooks.info\n\n869",
      "content_length": 1869,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 933,
      "content": "870\n\nIndex\n\nspikes, 809 Spy, Test. See Test Spy SQL, Table Truncation Teardown\n\nstatic binding\n\ndeﬁ ned, 809 Dependency Injection,\n\nusing, 666–667 Standard Fixture\n\nimplementation, 307–308 motivating example, 308 overview, 305–306 refactoring, 309–310 when to use, 306–307 standard test interface, 378 starbursts, diagramming\n\n678–679 static methods, 809 static variables, 809 Statically Generated Test\n\nDoubles, 561\n\nSTDD (storytest-driven development), 4, 810\n\nstop on ﬁ rst failure\n\nNaive xUnit Test Interpreter,\n\nnotation, xlii\n\n292–293\n\nstate, initializing via\n\nxUnit introduction, 57\n\nBack Door Manipulation.\n\nStored Procedure Test\n\nSee Back Door Manipulation Named State Reaching Method,\n\n417–418\n\nState Veriﬁ cation\n\nvs. behavior, 36 examples, 466–467 implementation, 463–465 indirect outputs, 179–180 introduction, 109–112 motivating example, 465 overview, 462–463 refactoring, 465–466 Self-Checking Tests, 108 Use the Front Door First, 41 when to use, 463 Stated Outcome Assertion\n\ndatabase testing, 172 examples, 658–660 implementation, 655–658 motivating example, 658 overview, 654 refactoring, 658 when to use, 654–655\n\nstorytest, 810 storytest-driven development\n\n(STDD), 4, 810\n\nstrategies, test automation. See test\n\nautomation strategies\n\nstress tests, cross-functionality, 52 strict Mock Object deﬁ ned, 138 when to use, 545\n\nAssertion Methods, 366 deﬁ ned, 365 example, 369 Guard Assertions as, 491 introduction, 110–111\n\nSTTCPW (The simplest thing that\n\ncould possibly work), 810\n\nStub, Test. See Test Stub Subclass, Test-Speciﬁ c. See\n\nTest-Speciﬁ c Subclass\n\nState-Exposing Subclass\n\nTest-Speciﬁ c Subclass, 289–590 when to use, 580\n\nSubclassed Humble Object, 700 Subclassed Inner Test Double,\n\n573–574\n\nstateless, 809 statements, “if”. See “if” statements\n\nSubclassed Singleton, 7 Subclassed Test Double, 146–147\n\nwww.it-ebooks.info",
      "content_length": 1855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 934,
      "content": "Index\n\nSubcutaneous Test\n\nSuites of Suites\n\ncustomer testing, 5 database testing, 174 design for testability, 7 Layer Tests, 343–344\n\nSubset Suite\n\nexample, 594–598 implementation, 594 introduction, 160–161 overview, 592 Too Many Tests solution, 257 when to use, 593 substitutable dependencies\n\nbuilding with Test enumeration,\n\n400\n\ndeﬁ ned, 388 example, 389–391 Interacting Test Suites, 231–232 introduction, 7, 15, 78\n\nSUnit\n\ndeﬁ ned, 750 Test Automation\n\nFrameworks, 300\n\nSuperclass, Testcase. See Testcase\n\nSuperclass\n\ndeﬁ ned, 810 Dependency Initialization\n\nSUT (system under test)\n\ncontrol points and observation\n\nTest, 352\n\npoints, 66–67\n\nusing Test Spy, 540 Substitutable Singleton\n\nin Dependency Lookup, 689 example, 586–587, 692–693 retroﬁ tting testability, 146–147 when to use, 581 substitution mechanisms,\n\n688–689\n\ndangers of modifying, 41–42 deﬁ ned, 810–811 Four-Phase Test, 358–361 interface sensitivity, xxxii isolation principle, 43–44 minimizing risk, 24–25 preface, xxii–xxiii replacing in Parameterized\n\nSuite Fixture Setup\n\nTest, 609\n\nexample, 444–446 implementation, 442–443 implicit, 426 motivating example, 443–444 overview, 441–442 refactoring, 444 Shared Fixture strategies, 64 Shared Fixtures, 104–105 when to use, 442\n\nresult veriﬁ cation. See result\n\nveriﬁ cation\n\nstate vs. behavior veriﬁ cation, 36 terminology, xl–xli test automation tools, 53–54 Test Hook in, 711–712 understanding with test\n\nautomation, 23\n\nsuite method, 399 suites\n\nNamed Test Suite. See Named\n\nTest Suite\n\nSUT API Encapsulation\n\nChained Tests as, 455 Indirect Testing solution, 198 Interface Sensitivity\n\ntest organization, 160–162 Test Suite Object. See Test Suite\n\nsolution, 241\n\nSUT Encapsulation Method,\n\nObject\n\n601–602\n\nwww.it-ebooks.info\n\n871",
      "content_length": 1754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 935,
      "content": "872\n\nIndex\n\nSymbolic Constants example, 716 Literal Value, 715 symptoms, behavior smells Assertion Roulette, 224 Asynchronous Tests, 255 Behavior Sensitivity, 242 Context Sensitivity, 245 Data Sensitivity, 243 Eager Tests, 224–225 Erratic Tests, 228 Fragile Tests, 239 Frequent Debugging, 248 General Fixtures, 255 Interacting Test Suites, 231 Interacting Tests, 229 Interface Sensitivity, 241 Manual Intervention, 250–252 Missing Assertion Messages, 226 Nondeterministic Tests, 237 Resource Leakage, 233 Resource Optimism, 233 Slow Tests, 253 Test Run Wars, 236 Too Many Tests, 256 Unrepeatable Tests, 234–235\n\nMystery Guests, 188–189 Obscure Tests, 186 Production Logic in Test,\n\n204–205\n\nTest Code Duplication, 213–214 Test Dependency in Production, 220\n\nTest Logic in Production, 217 test smells, 10 For Tests Only, 219 Untestable Test Code, 211\n\nsymptoms, project smells Buggy Tests, 260 Developers Not Writing Tests,\n\n263\n\nHigh Test Maintenance\n\nCost, 265\n\nInfrequently Run Tests, 268–269 Lost Tests, 269 Missing Unit Tests, 271 Neverfail Tests, 274 Production Bugs, 268 Untested Code, 271–272 Untested Requirements, 272–273\n\nsymptoms, test smells, 10 synchronous tests\n\nsymptoms, code smells\n\navoiding with Humble Object,\n\nAsynchronous Code, 210 Complex Teardown, 206 Conditional Test Logic, 200 Eager Tests, 187–188 Equality Pollution, 221 Flexible Tests, 202 General Fixtures, 190–191 Hard-Coded Test Data,\n\n696–697 deﬁ ned, 810\n\nsystem under test (SUT). See SUT\n\n(system under test)\n\nT\n\nTable Truncation Teardown\n\n194–195\n\nHard-To-Test Code, 209 Highly Coupled Code, 210 Indirect Testing, 196–197 Irrelevant Information, 192–193 Multiple Test Conditions, 207\n\ndata access layer testing, 173 deﬁ ned, 100 examples, 665–667 implementation, 662–664 motivating example, 664 overview, 661–662\n\nwww.it-ebooks.info",
      "content_length": 1817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 936,
      "content": "Index\n\nrefactoring, 664–665 when to use, 662\n\ntabular data, 291 Tabular Test\n\nChained Tests, 457–458 with framework support, 614 implementation, 609–610 Incremental, 613–614 Independent, 612–613\n\ntasks, 811 TDD (test-driven development)\n\ndeﬁ ned, 813 implementing utility methods,\n\n122\n\nintroduction, xxxiii–xxxiv Missing Unit Tests, 271 need-driven development, 149 process, 4–5 Test Automation\n\nFrameworks, 301\n\ntest automater, 811 test automation, xxix–xliii assumptions, xxxix–xl automated unit testing, xxx–xxxii brief tour, 3–8 code samples, xli–xlii developer testing, xxx diagramming notation, xlii feedback, xxix fragile test problem, xxxi–xxxii limitations, xliii overview, xxix patterns, xxxiv–xxxviii refactoring, xxxviii–xxxix terminology, xl–xli testing, xxx uses of, xxxiii–xxxiv Test Automation Framework\n\nintroduction, 75 pattern description, 298–301\n\ntest automation principles, 40\n\ntest automation goals, 19–29\n\nteardown, ﬁ xture. See ﬁ xture\n\nteardown\n\nTeardown Guard Clause\n\nexample, 513 Implicit Teardown, 517–518 In-line Teardown, 511\n\ntearDown method\n\nImplicit Teardown, 516–519 persistent ﬁ xtures, 98 Setup Decorator. See Setup\n\nDecorator Template Method, 164 Temporary Test Stub\n\nwhen to use, 530–531 xUnit terminology, 741–744\n\nterminology\n\ntest automation introduction,\n\nxl–xli\n\ntransient ﬁ xtures, 86–88 xUnit. See xUnit basics\n\nease of running, 25–27 improving quality, 22–23 list of, 757–759 objectives, 21–22 reducing risk, 23–25 system evolution, 29 understanding SUT, 23 why test?, 19–21 writing and maintaining, 27–29\n\nTest Automation Manifesto, 39 test automation philosophies, 31–37\n\nauthor’s, 37 differences, 32–36 importance of, 31–32\n\ntest automation principles, 39–48 Communicate Intent, 41 Design for Testability, 40 Don’t Modify the SUT, 41–42 Ensure Commensurate Effort and Responsibility, 47–48\n\nwww.it-ebooks.info\n\n873",
      "content_length": 1865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 937,
      "content": "874\n\nIndex\n\nIsolate the SUT, 43–44 Keep Test Logic Out of Production Code, 45 Keep Tests Independent,\n\ntools for, 53–58 transient fresh ﬁ xtures, 61–62 what’s next, 73 wrong, 264\n\n42–43\n\nMinimize Test Overlap, 44 Minimize Untestable Code,\n\n44–45\n\nTest Bed. See Prebuilt Fixture test cases, 811 test code, 811 Test Code Duplication\n\noverview, 39–40 Test Concerns Separately, 47 Use the Front Door First,\n\n40–41\n\ncauses, 214–215 Custom Assertions, 475 Delegated Setup, 412 High Test Maintenance\n\nVerify One Condition per Test,\n\n45–47\n\nWrite the Tests First, 40\n\ntest automation roadmap, 175–181 alternative path veriﬁ cation,\n\n178–179\n\ndifﬁ culties, 175–176 direct output veriﬁ cation, 178 execution and maintenance optimization, 180–181 happy path code, 177–178 indirect outputs veriﬁ cation,\n\n178–180\n\nmaintainability, 176–177 test automation strategies, 49–73\n\nbrief tour, 3–8 control points and observation\n\nCost, 266 impact, 214 In-Line Setup, 89 introduction, 16 possible solution, 216 reducing, 114–119 reducing with Conﬁ gurable\n\nTest Doubles. See Conﬁ gurable Test Double\n\nreducing with Parameterized Tests. See Parameterized Test\n\nreducing with Test Utility Methods. See Test Utility Method\n\nremoving with Testcase Class per Fixture. See Testcase Class per Fixture\n\npoints, 66–67\n\ncross-functional tests, 52–53 divide and test, 71–72 ensuring testability, 65 ﬁ xture strategies overview, 58–61 interaction styles and testability\n\npatterns, 67–71 overview, 49–50 per-functionality tests, 50–52 persistent fresh ﬁ xtures, 62–63 shared ﬁ xture strategies, 63–65 test-driven testability, 66\n\nreusing test code, 162 symptoms, 213–214\n\nTest Commands, 82 Test Concerns Separately, 47 test conditions, 154, 811–812 test database, 812 test debt, 812 Test Dependency in Production,\n\n220–221\n\nTest Discovery\n\nintroduction, 78 Lost Tests solution, 271\n\nwww.it-ebooks.info",
      "content_length": 1867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 938,
      "content": "Index\n\npattern description, 393–398 Test Suite Object Generator, 293 Test Suite Objects, 388 Test Double, 125–151, 521–590\n\nBack Door Manipulation, 332 Behavior Veriﬁ cation, 112 Conﬁ gurable Test Double.\n\nSee Conﬁ gurable Test Double\n\nconﬁ guring, 141–142 considerations, 150 customer testing, 5 database testing, 169–171 Dependency Injection.\n\nproviding, 140–141 retroﬁ tting testability,\n\n146–148\n\nreusing test code, 162 terminology, 741–744 vs. Test Hook, 709–712 Test Spy, 137, 538–543 Test Stub. See Test Stub Test-Speciﬁ c Subclass.\n\nSee Test-Speciﬁ c Subclass\n\ntypes of, 133–134 when to use, 523–526\n\nTest Double Class\n\nSee Dependency Injection\n\nDependency Lookup, 144–145 dependency replacement, 739 design for testability, 7 Don’t Modify the SUT, 41–42 Dummy Object, 134–135 example, 526–528 Fake Object. See Fake Object Fragile Test, 240 Hard-Coded Test Double.\n\nexample, 572–573 implementation, 569–570\n\nTest Double Subclass\n\nimplementation, 570 when to use, 580–581\n\ntest drivers\n\nAssertion Messages, 370 deﬁ ned, 813 test driving, 813 Test Enumeration\n\nSee Hard-Coded Test Double\n\nHighly Coupled Code\n\nintroduction, 153 pattern description, 399–402\n\nsolution, 210\n\nindirect input and output,\n\n125–126\n\nindirect input control, 128–129 indirect input, importance\n\nof, 126\n\ntest errors, 80, 813 test failure, 80, 813 test ﬁ rst development deﬁ ned, 813–814 process, 4–5 test automation philosophy,\n\nindirect output, importance of,\n\n32–33\n\n126–127\n\nvs. test-last development, xxxiv\n\nindirect output veriﬁ cation,\n\nTest Fixture Registry\n\n130–133\n\ninstalling, 143 minimizing risk, 25 Mock Object. See Mock Object other uses, 148–150 outside-in development, 35–36 overview, 522–523\n\naccessing Shared Fixtures, 104 Test Helper use, 644 test ﬁ xtures. See ﬁ xtures Test Helper\n\nAutomated Teardown, 505 introduction, xxiii pattern description, 643–647\n\nwww.it-ebooks.info\n\n875",
      "content_length": 1880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 939,
      "content": "876\n\nIndex\n\nTest Helper Mixin\n\nexample, 641–642 vs. Testcase Superclass, 639\n\nTest Hook\n\nimplementation, 349 invocation, 402 Lost Tests, 269–270 minimizing untested code,\n\npattern description, 709–712 in Procedural Test Stub,\n\n44–45\n\norganization, 7, 155–158. See\n\n135–136\n\nalso test organization patterns\n\nretroﬁ tting testability, 148 Test Logic in Production,\n\noverview, 348–349 persistent ﬁ xtures. See persistent\n\n217–219 testability, 70\n\nTest Logic, Conditional.\n\nSee Conditional Test Logic\n\nTest Logic in Production\n\nEquality Pollution, 221–222 impact, 217 introduction, 17 symptoms, 217 Test Dependency in Production,\n\n220–221\n\nTest Hooks, 148, 217–219 For Tests Only, 219–220\n\nﬁ xtures\n\nright-sizing, 154–155 running, 81 selection, 404–405 Simple Success Test, 349–350 Simple Success Test example,\n\n352–353\n\ntest automation philosophies, 34 Test Commands, 82 Test Concerns Separately, 47 Test Suite Objects, 82 Testcase Object implementation,\n\n384–385\n\ntest maintainer, 815 Test Method\n\ntransient ﬁ xture management.\n\nSee transient ﬁ xtures\n\ncalling Assertion. See Assertion\n\nMethod\n\nunit testing, 6 Verify One Condition per Test,\n\nConstructor Test example,\n\n46–47\n\n355–357\n\nwriting simple tests, 28\n\nConstructor Tests, 351 Dependency Initialization\n\nTests, 352\n\nTest Method Discovery\n\ndeﬁ ned, 394–395 examples, 395–397\n\nenumeration, 401 Expected Exception Test,\n\nTest Object Registry. See Automated\n\nTeardown\n\n350–351\n\nExpected Exception Test using\n\nblock closure, 354–355\n\nExpected Exception Test using\n\nmethod attributes, 354\n\nExpected Exception Test using\n\ntest organization, 153–165 code reuse, 162–164 introduction, 153 naming conventions, 158–159 overview, 7 right-sizing Test Methods,\n\ntry/catch, 353–354\n\n154–155\n\nﬁ xture design, 59\n\ntest ﬁ les, 164–165\n\nwww.it-ebooks.info",
      "content_length": 1792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 940,
      "content": "Index\n\nTest Methods and Testcase\n\nTest Runner\n\nClasses, 155–158 test suites, 160–162\n\nGraphical. See Graphical Test\n\nRunner\n\ntest organization patterns, 591–647 Named Test Suite. See Named\n\nTest Suite\n\nimplementation, 378–381 introduction, 79 Missing Assertion Messages,\n\nParameterized Test.\n\n226–227\n\nSee Parameterized Test\n\nTest Helper, 643–647 Test Utility Method. See Test\n\nUtility Method\n\nTestcase Class per Class.\n\noverview, 377–378 Test Automation Frameworks,\n\n300 test runs, 815 Test Selection\n\nSee Testcase Class per Class\n\nTestcase Class per Feature.\n\npattern description, 403–405 Test Suite Object, 388\n\nSee Testcase Class per Feature\n\ntest smells, 9–17\n\nTestcase Class per Fixture.\n\nSee Testcase Class per Fixture\n\nTestcase Superclass, 638–642\n\ntest packages\n\ndeﬁ ned, 815 test ﬁ le organization, 164–165\n\naliases and causes, 761–765 behavior. See behavior smells catalog of, 12–17 code smells. See code smells database testing. See database\n\ntesting\n\ntest readers, 815 test refactorings. See also refactoring Extractable Test Component,\n\n735–736\n\ndeﬁ ned, 808, 816 introduction, xxxvi overview, 9–11 patterns and principles vs.,\n\nIn-line Resource, 736–737 Make Resources Unique,\n\n737–738\n\nxxxv–xxxvi\n\nproject smells. See project smells reducing Test Code Duplication,\n\nMinimize Data, 738–739 Replace Dependency with Test\n\n114–119\n\nTest Spy\n\nDouble, 739\n\nSet Up External Resource, 740\n\ntest results\n\nBack Door Veriﬁ cation, 333 Behavior Veriﬁ cation, 113 Conﬁ gurable. See Conﬁ gurable\n\ndeﬁ ned, 815 introduction, 79–80 veriﬁ cation. See result veriﬁ cation\n\nTest Run War\n\nTest Double\n\nexamples, 542–543 implementation, 540–541 indirect outputs veriﬁ cation,\n\ndatabase testing, 169 Erratic Tests cause, 235–237 introduction, 15 vs. Shared Fixture strategy, 64\n\n179–180\n\nintroduction, 131–133,\n\n137, 525\n\nmotivating example, 541\n\nwww.it-ebooks.info\n\n877",
      "content_length": 1864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 941,
      "content": "878\n\nIndex\n\noverview, 538–539 Procedural Behavior Veriﬁ cation, 470 refactoring, 541–542 when to use, 539–540 xUnit terminology, 741–744\n\ntest strategy patterns, 277–345\n\nData-Driven Test. See Data-\n\nwhen to use, 530–531 xUnit terminology, 741–744\n\ntest success, 816 Test Suite Enumeration\n\ndeﬁ ned, 400 example, 402 Test Suite Factory, 232 Test Suite Object\n\nDriven Test\n\nFresh Fixture. See Fresh Fixture Layer Test. See Layer Test Minimal Fixture, 302–304 Recorded Test. See Recorded\n\nTest\n\nScripted Test, 285–287 Shared Fixture. See Shared\n\nFixture\n\nenumeration, 400 Interacting Test Suites, 231–232 introduction, 7, 82 pattern description, 387–392 Test Suite Object Generator, 293 Test Suite Object Simulator, 293 Test Suite Procedure\n\ndeﬁ ned, 388–389 example, 391–392\n\nStandard Fixture. See Standard\n\ntest suites\n\nFixture\n\nTest Automation Framework,\n\n298–301\n\ndeﬁ ned, 816 Lost Tests, 269–270 Named Test Suites. See Named\n\ntest strippers, 816 Test Stub\n\nTest Suite\n\nTest Tree Explorer, 161–162,\n\nBehavior-Modifying Subclass,\n\n380–381\n\n584–585\n\nTest Utility Method\n\nConﬁ gurable. See Conﬁ gurable\n\nTest Double\n\nconﬁ guring, 141–142 Context Sensitivity solution, 246 controlling indirect inputs, 129 creating in-line resources, 737 examples, 533–537 implementation, 531–532 indirect inputs control, 179 inside-out development, 34–35 introduction, 133, 135–136, 524 motivating example, 532–533 overview, 529–530 refactoring, 533 unit testing, 6\n\nCommunicate Intent, 41 eliminating loops, 121 example, 605–606 implementation, 602–603 introduction, xxiii, 16–17, 23,\n\n162–163\n\nmotivating example, 603–604 Obscure Tests solution, 199 overview, 599 reducing risk of bugs, 181 refactoring, 605 reusing, lviii–lix reusing via Test Helper, 643–647 reusing via Testcase Superclass,\n\n638–642\n\nwww.it-ebooks.info",
      "content_length": 1805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 942,
      "content": "Index\n\nusing TDD to write, 122 when to use, 600–602\n\nTest Utility Test, 603 testability, design for. See design-\n\nTestcase Superclass\n\npattern description, 638–642 reusing test code, 163–164 Test Discovery using, 397–398\n\nfor-testability Testcase Class\n\ntest-driven bug ﬁ xing, 812 test-driven development (TDD).\n\nintroduction, 78 organization, 7, 155–158 pattern description, 373–376 reusable test logic, 123 selection, 404–405\n\nTestcase Class Discovery\n\ndeﬁ ned, 394 example, 397–398\n\nTestcase Class per Class\n\nexample, 618–623 implementation, 618 overview, 617 when to use, 618 Testcase Class per Feature example, 628–630 implementation, 626 motivating example, 626–627 overview, 624 refactoring, 627–628 when to use, 625 Testcase Class per Fixture example, 635–637 implementation, 632–633 motivating example,\n\nSee TDD (test-driven development)\n\nTest-Driven Development: By\n\nExample (Beck), 301 test-driven testability, 66 Testing by Layers. See Layer Test testing terminology. See terminology test-last development deﬁ ned, 815 strategy, 65 test automation philosophy,\n\n32–33\n\nvs. test-ﬁ rst development, xxxiv\n\nTestNG\n\ndeﬁ ned, 750 Interacting Tests, 231 Testcase Object exception,\n\n384–385 vs. xUnit, 57\n\nTests as Documentation\n\nCommunicate Intent, 41 customer testing, 5 deﬁ ned, 23 reusing test code, 162 unit testing, 6\n\n633–634 overview, 631 refactoring, 634–635 Verify One Condition per Test,\n\nTests as Safety Net, 24, 260 Tests as Speciﬁ cation, xxxiii, 22 test-speciﬁ c equality, 588–589, 816 Test-Speciﬁ c Extension.\n\n46–47\n\nSee Test-Speciﬁ c Subclass\n\nwhen to use, 632\n\nTest-Speciﬁ c Subclass\n\nTestcase Class per Method, 625 Testcase Class per User Story, 625 Testcase Object\n\nBehavior-Exposing Subclass,\n\n587\n\nBehavior-Modifying Subclass\n\nintroduction, 81 pattern description, 382–386\n\n(Substituted Singleton), 586–587\n\nwww.it-ebooks.info\n\n879",
      "content_length": 1859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 943,
      "content": "880\n\nIndex\n\nBehavior-Modifying Subclass\n\ntransient ﬁ xtures, 85–94\n\n(Test Stub), 584–585\n\ndeﬁ ning Test-Speciﬁ c Equality,\n\n588–589\n\nDon’t Modify the SUT, 42 implementation, 581–582 Isolate the SUT, 44 motivating example, 582–584 overview, 579–580 refactoring, 584 retroﬁ tting testability, 146–147 State-Exposing Subclass,\n\n289–590\n\nFor Tests Only solution, 220 when to use, 580–581\n\nDelegated Setup, 89–91 hybrid setup, 93 Implicit Setup, 91–93 In-Line Setup, 88–89 overview, 85–86 vs. persistent ﬁ xtures, 96 tearing down, 93–94 terminology, 86–88 what’s next, 94 Transient Fresh Fixture\n\ndatabase testing, 170 deﬁ ned, 60–61, 314 vs. Shared Fixture, 61–62\n\ntroubleshooting\n\nTest::Unit, 750 Thread-Speciﬁ c Storage, 688–689 Too Many Tests, 256–257 tools\n\nautomated unit testing,\n\nxxx–xxxi\n\nBuggy Tests, 261 Developers Not Writing Tests,\n\n264\n\nErratic Tests, 228–229 Fragile Tests, 239–240 High Test Maintenance Cost,\n\ncommercial record and playback,\n\n267\n\n282–283\n\nSlow Tests, 253–254\n\nQTP. See QTP (QuickTest\n\nProfessional)\n\nrobot user. See robot user tools for test automation strategy,\n\nTrue Humble Executable, 703–706 True Humble Objects, 699–700 TRUNCATE command. See Table\n\nTruncation Teardown\n\n53–58\n\ntry/catch\n\ntypes of, 753–756\n\nExpected Exception Tests,\n\nTransaction Controller, Humble.\n\n353–354\n\nSee Humble Transaction Controller\n\nSingle-Outcome Assertions, 367\n\ntry/ﬁ nally block\n\nTransaction Rollback Teardown data access layer testing, 173 deﬁ ned, 100 examples, 673–675 implementation, 671 motivating example, 672 overview, 668–669 refactoring, 672 when to use, 669–671\n\ncleaning up ﬁ xture teardown\n\nlogic, l–liv\n\nImplicit Teardown, 519 In-line Teardown, 512–513\n\ntype compatibility, 679 type visibility\n\nTest Helper use, 644 Test Utility Methods, 603 Testcase Superclass use, 639\n\nwww.it-ebooks.info",
      "content_length": 1819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 944,
      "content": "Index\n\nU\n\nUntested Code\n\nUAT (user acceptance tests)\n\ndeﬁ ned, 817 principles, 42 UI (User Interface) tests\n\nasynchronous tests, 70–71 Hard-To-Test Code, 71–72 tools, 55\n\nUML (Uniﬁ ed Modeling\n\nLanguage), 816\n\nalternative path veriﬁ cation,\n\n178–179\n\nindirect inputs and, 126 Isolate the SUT, 43 minimizing, 44–45 preventing with Test Doubles,\n\n523\n\nProduction Bugs, 271–272 unit testing, 6 Untested Requirement\n\nUnconﬁ gurable Test Doubles, 527 unexpected exceptions, 352 Unﬁ nished Test Assertion, 494–497 Unﬁ nished Test Method from\n\nFrequent Debugging cause,\n\n249\n\nindirect output testing, 127 preventing with Test\n\nTemplate, 496–497\n\nUniﬁ ed Modeling Language\n\n(UML), 816\n\nunique resources, 737–738 Unit Testing with Java (Link), 743 unit tests\n\ndeﬁ ned, 817 introduction, 6 per-functionality, 51 rules, 307 Scripted Tests, 285–287 xUnit vs. Fit, 290–292 unnecessary object elimination,\n\nDoubles, 523\n\nProduction Bugs cause,\n\n272–274\n\nreducing via Isolate the\n\nSUT, 43 usability tests, 53 use cases, 817 Use the Front Door First deﬁ ned, 40–41 Overspeciﬁ ed Software\n\navoidance, 246 user acceptance tests (UAT)\n\n303–304\n\nUnrepeatable Test\n\ndeﬁ ned, 817 principles, 42 User Interface (UI) tests\n\ndatabase testing, 169 Erratic Test cause, 234–235 introduction, 15, 64 persistent fresh ﬁ xtures, 96 vs. Repeatable Test, 26–27\n\nasynchronous tests, 70–71 Hard-To-Test Code, 71–72 tools, 55\n\nuser story\n\nUntestable Test Code\n\navoiding Conditional Logic,\n\ndeﬁ ned, 817 Testcase Class per, 625 utility methods. See Test Utility\n\n119–121\n\nHard-To-Test Code, 211–212\n\nMethod\n\nutPLSQL, 750\n\nwww.it-ebooks.info\n\n881",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 945,
      "content": "882\n\nIndex\n\nV\n\nvisibility\n\nvalue patterns, 713–732\n\nDerived Values, 718–722 Dummy Objects, 728–732 Generated Values, 723–727 Literal Values, 714–717\n\nvariables\n\nin Derived Values, 718–722 global, 92, 798 instance. See instance variables local. See local variables procedure variables, 805–806 static, 809 VB Lite Unit, 751 VbUnit\n\nof SUT features from Test-\n\nSpeciﬁ c Subclass, 581–582\n\ntest ﬁ le organization, 165 type. See type visibility visual objects, Humble Dialog\n\nuse, 706\n\nVisual Studio, 756\n\nW\n\nwaterfall design, 65 Watir\n\ndeﬁ ned, 756 Test Automation Frameworks,\n\n301\n\ndeﬁ ned, 751 Suite Fixture Setup support, 442 Testcase Class terminology, 376 xUnit terminology, 300 Verbose Tests. See Obscure Test veriﬁ cation\n\nalternative path, 178–179 Back Door Manipulation,\n\n329–330\n\nBack Door using Test Spy, 333 cleaning up logic, xlvi–l direct output, 178 indirect outputs, 130–133,\n\n178–180\n\nstate vs. behavior, 36 test results. See result veriﬁ cation Verify One Condition per Test,\n\ntest automation tools, 53 Weinberg, Gerry, xxiv–xxv, 61–62 widgets\n\nHumble Dialog use, 706 recognizers, 299\n\nWikipedia, 729 Working Effectively with Legacy\n\nCode (Feathers), 210 Write the Tests First, 40 writing tests\n\nDevelopers Not Writing Tests\n\nproject smells, 263–264 development process, 4–5 goals, 27–29 philosophies. See test automation\n\nphilosophies\n\nprinciples. See test automation\n\n45–47\n\nprinciples\n\nVeriﬁ cation Method\n\ndeﬁ ned, 477, 602 example, 482–483\n\nX\n\nXML data ﬁ les, Data-Driven Tests,\n\nVerify One Condition per Test\n\n294–295\n\ndeﬁ ned, 40, 45–47 right-sizing Test Methods,\n\nxUnit\n\nData-Driven Tests with CSV\n\n154–155 verify outcome, 817 Virtual Clock, 246\n\ninput ﬁ le, 296\n\nData-Driven Tests with XML\n\ndata ﬁ le, 294–295\n\nwww.it-ebooks.info",
      "content_length": 1753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 946,
      "content": "deﬁ ned, 751 family members, 747–751 vs. Fit, 291–292 ﬁ xture deﬁ nitions, 86 Interacting Test Suites, 232 introduction, 56–57 language-speciﬁ c terminology,\n\nxl–xli\n\nﬁ xtures, 78 overview, 75–76 procedural world, 82–83 running Test Methods, 81 running tests, 79 Test Commands, 82 test results, 79–80 Test Suite Object, 82 xUnit basics patterns, 347–405\n\nmodern, 55 Naive xUnit Test Interpreter,\n\n292–293\n\nAssertion Message, 370–372 Assertion Method.\n\nproﬁ ling tools, 254 Suite Fixture Setup support,\n\n442–443\n\nsweet spot, 58 terminology, 741–746 Test Automation Frameworks,\n\nSee Assertion Method Four-Phase Test, 358–361 Test Discovery, 393–398 Test Enumeration, 399–402 Test Method.\n\nSee Test Method\n\n300\n\nTest Runner.\n\ntest ﬁ xtures, 814 test organization mechanisms,\n\n153\n\nSee Test Runner\n\nTest Selection, 403–405 Test Suite Object, 82,\n\nxUnit basics, 75–83\n\n387–392\n\ndeﬁ ning suites of tests, 78–79 deﬁ ning tests, 76–78\n\nTestcase Class, 373–376 Testcase Object, 382–386\n\nwww.it-ebooks.info\n\nIndex\n\n883",
      "content_length": 1008,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 947,
      "content": "List of Smells\n\nAssertion Roulette (224): It is hard to tell which of several assertions within the same test method caused a test failure. Includes Eager Test, Missing Assertion Message.\n\nBuggy Tests (260): Bugs are regularly found in the automated tests. Includes Fragile Test, Hard-to-Test Code, Obscure Test.\n\nConditional Test Logic (200): A test contains code that may or may not be executed. Includes Complex Teardown, Condi- tional Veriﬁ cation Logic, Flexible Test, Multiple Test Conditions, Production Logic in Test.\n\nDevelopers Not Writing Tests (263): Developers aren’t writing automated tests. Includes Hard-to-Test Code, Not Enough Time, Wrong Test Automation Strategy.\n\nErratic Test (228): One or more tests are behaving erratically; sometimes they pass and sometimes they fail. Includes Inter- acting Test Suites, Interacting Tests, Lonely Test, Nondeterministic Test, Resource Leakage, Resource Optimism, Test Run War, Unrepeatable Test.\n\nFragile Test (239): A test fails to compile or run when the SUT is changed in ways that do not affect the part the test is exer- cising. Includes Behavior Sensitivity, Context Sensitivity, Data Sensitivity, Fragile Fixture, Interface Sensitivity, Overspeciﬁ ed Software, Sensitive Equality.\n\nFrequent Debugging (248): Manual debugging is required to determine the cause of most test failures.\n\nHard-to-Test Code (209): Code is difﬁ cult to test. Includes Asynchronous Code, Hard-Coded Dependency, Highly Coupled Code, Untestable Test Code.\n\nHigh Test Maintenance Cost (265): Too much effort is spent maintaining existing tests. Includes Fragile Test, Hard-to-Test Code, Obscure Test.\n\nManual Intervention (250): A test requires a person to perform some manual action each time it is run. Includes Manual Event Injection, Manual Fixture Setup, Manual Result Veriﬁ cation.\n\nObscure Test (186): It is difﬁ cult to understand the test at a glance. Includes Eager Test, General Fixture, Hard-Coded Test Data, Indirect Testing, Irrelevant Information, Mystery Guest.\n\nProduction Bugs (268): We ﬁ nd too many bugs during formal test or in production. Includes Infrequently Run Tests, Lost Test, Missing Unit Test, Neverfail Test, Untested Code, Untested Requirement.\n\nSlow Tests (253): The tests take too long to run. Includes Asynchronous Test, General Fixture, Slow Component Usage, Too Many Tests.\n\nTest Code Duplication (213): The same test code is repeated many times. Includes Cut-and-Paste Code Reuse, Reinventing the Wheel.\n\nTest Logic in Production (217): The code that is put into production contains logic that should be exercised only during tests. Includes Equality Pollution, For Tests Only, Test Dependency in Production, Test Hook.\n\nwww.it-ebooks.info",
      "content_length": 2715,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 948,
      "content": "All Patterns Listed by the Problem They Solve\n\nHow do we prepare automated tests for our software?\n\nRecorded Test (278); Scripted Test (285); Data-Driven Test (288)\n\nHow do we make it easy to write and run tests?\n\nTest Automation Framework (298)\n\nWhere do we put our test code?\n\nTest Method (348); Testcase Class (373); Test Helper (643); Testcase Superclass (638)\n\nHow do we organize our Test Methods onto Testcase Classes?\n\nTestcase Class per Feature (624); Testcase Class per Fixture (631); Testcase Class per Class (617)\n\nHow do we make tests self-checking?\n\nState Veriﬁ cation (462); Behavior Veriﬁ cation (468); Assertion Method (362); Custom Assertion (474); Delta\n\nAssertion (485)\n\nHow do we structure our test logic?\n\nFour-Phase Test (358); Assertion Message (370); Unﬁ nished Test Assertion (494)\n\nHow do we reduce Test Code Duplication?\n\nData-Driven Test (288); Custom Assertion (474); Test Utility Method (599); Parameterized Test (607)\n\nHow do we run the tests?\n\nTest Runner (377); Testcase Object (382); Test Suite Object (387); Named Test Suite (592)\n\nHow does the Test Runner know which tests to run?\n\nTest Discovery (393); Test Enumeration (399); Test Selection (403)\n\nWhich ﬁ xture strategy should we use?\n\nMinimal Fixture (302); Standard Fixture (305); Fresh Fixture (311); Shared Fixture (317)\n\nHow do we construct the ﬁ xture?\n\nIn-line Setup (408); Delegated Setup (411); Creation Method (415); Implicit Setup (424)\n\nHow do we cause the Shared Fixture to be built before the ﬁ rst test method that needs it?\n\nPrebuilt Fixture (429); Lazy Setup (435); Suite Fixture Setup (441); Setup Decorator (447); Chained Tests (454)\n\nHow do we specify the values to be used in tests?\n\nDummy Object (728); Literal Value (714); Derived Value (718); Generated Value (723)\n\nHow do we tear down the Test Fixture?\n\nGarbage-Collected Teardown (500); In-line Teardown (509); Implicit Teardown (516); Automated Teardown (503);\n\nTable Truncation Teardown (661); Transaction Rollback Teardown (668)\n\nHow can we avoid Slow Tests?\n\nShared Fixture (317); Test Double (522); Fake Object (551)\n\nHow do we avoid Conditional Test Logic?\n\nCustom Assertion (474); Guard Assertion (490)\n\nHow can we verify logic independently?\n\nBack Door Manipulation (327); Layer Test (337); Test Double (522); Test Stub (529); Test Spy (538); Mock\n\nObject (544); Fake Object (551); Stored Procedure Test (654)\n\nHow do we implement Behavior Veriﬁ cation?\n\nTest Spy (538); Mock Object (544)\n\nHow do we tell a Test Double what to return or expect?\n\nConﬁ gurable Test Double (558); Hard-Coded Test Double (568)\n\nHow can we make code testable?\n\nHumble Object (695); Test-Speciﬁ c Subclass (579)\n\nHow do we design the SUT so that we can replace its dependencies at runtime? Dependency Injection (678); Dependency Lookup (686); Test Hook (709)\n\nwww.it-ebooks.info",
      "content_length": 2829,
      "extraction_method": "Unstructured"
    }
  ]
}