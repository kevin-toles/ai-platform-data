{
  "metadata": {
    "title": "Essential Test-Driven Development",
    "author": "Rob Myers",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 260,
    "conversion_date": "2025-12-25T18:13:13.239271",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Essential Test-Driven Development.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Thinking Test-Driven",
      "start_page": 31,
      "end_page": 50,
      "detection_method": "regex_chapter_title",
      "content": "Figure 1.1 A stereo system’s volume dial circa 1980. (Photo: atm2003/123rf)\n\nIn a brief whiteboard discussion, we decide we need a volume-control application programming interface (API) that accepts an integer from 0 to 10. Zero is effectively “off” and 10 is the maximum volume (not 11!1).\n\n1 https://en.wikipedia.org/wiki/This_Is_Spinal_Tap\n\nThe UI developer assures us that we will not receive an out-of-range integer. However, we agree that if this somehow happens, our code will throw an IllegalArgumentException. In Java, this could look like the following:\n\nif (volumeZeroToTen > 10 || volumeZeroToTen < 0)\n\nthrow new IllegalArgumentException(\"Volume cannot be \" + valueZeroToTen);\n\naudioManager.setStreamVolume(volumeZeroToTen);\n\nThere are three scenarios:\n\n1. When volumeZeroToTen is greater than 10\n\n2. When volumeZeroToTen is less than 0\n\n3. When volumeZeroToTen is within the acceptable range\n\nEach of those three scenarios is different because of the first if conditional, and each deserves its own distinct unit test. The behavior is more clearly described by expressly writing tests for those three examples.\n\nBehavioral Boundaries\n\nA behavioral boundary is the conceptual boundary separating two (or more) related scenarios. In practice, it is often the boundary between two similar input values that give different results.\n\nLet’s return to the volume dial example. Suppose that rather than having the dial’s API set the actual phone volume (as previously shown), we decide to take an event-driven approach instead. An observer of a Dial object will need to query the setting whenever the graphical dial is turned. What the observer wants to know is not which value was set on the dial, but what percent of the maximum volume the setting represents.\n\nA developer might be tempted to aim for the “happy path” (the scenario that typically avoids all edge cases and error conditions), write a simple implementation (for example, return currentSettingFromZeroToTen * 10; ), and be done with it:\n\n@Test public void levelAsPercent_HappyPath_RightDownTheMiddle() {\n\nDial dial = new Dial(\"Volume\"); dial.setTo(5); assertThat(dial.levelAsPercent()).isEqualTo(50);\n\n}\n\nThis is a good start, but there is another way to approach this kind of problem, particularly when you are working with something more sophisticated than a volume dial.\n\nTDD is first and foremost a practice that facilitates and supports a developer’s thinking and coding. Writing one test scenario often leads to an awareness of other related scenarios. Writing tests for those related scenarios is known as the “triangulation” technique; it is covered in detail in later chapters. In its most refined form, triangulation helps the developer identify behavioral boundaries by asking, “What value(s) will cause us to write smarter code?”\n\nLet’s review what we know about Dial:\n\nNegative numbers should throw an exception.\n\nNumbers greater than 10 should throw the same exception with a different message.\n\nZero is effectively “off.”\n\nAll other values represent a percentage of the maximum volume.\n\nThere is a behavioral boundary between –1 (an error) and 0 (off), and another behavioral boundary between 10 (maximum) and 11 (another error). However, there are no behavioral boundaries between any two of the integers between 1 and 10. The behavior—the complete code path—is the same for all: 1, 2, 5, 6, 9, and 10 must all exhibit the same behavior (in this case, a simple calculation) resulting in different answers.\n\nBecause 0 (off) could also be represented by 0 percent, is there a behavioral boundary between a dial setting of 0 and 1? It’s impossible to say without learning more about the product specification. If we are asked to have the software turn on a little green light whenever the sound system is turned on, that would call for a behavioral boundary between 0 and 1 (though not necessarily implemented by Dial). Until that behavior is requested, however, the test-driven developer would not build it.\n\nWhen you spot a behavioral boundary, you can write your tests for the values that represent the “edges” of that boundary:\n\n@BeforeEach public void initializeVolumeDial() {\n\ndial = new Dial(\"Volume\");\n\n}\n\n@Test public void dialThrowsExceptionForValueUnderMin() {\n\nassertThatIllegalArgumentException().isThrownBy(() -> { dial.setTo(-1); });\n\n}\n\n@Test public void levelAsPercent_WhenOff() {\n\ndial.setTo(0); assertThat(dial.levelAsPercent()).isEqualTo(0);\n\n}\n\n@Test public void levelAsPercent_AtMaximum() {\n\ndial.setTo(10); assertThat(dial.levelAsPercent()).isEqualTo(100);\n\n}\n\n@Test public void dialThrowsExceptionForValueOverMax() {\n\nassertThatIllegalArgumentException().isThrownBy(() -> { dial.setTo(11); });\n\n}\n\nWhat if you hadn’t noticed the boundaries before writing your tests, and you had written that earlier happy path test for the dial setting of 5? You could either leave the original test within the suite or delete it. If the happy path test provides even the tiniest additional clarity in the specification, leave it. It would take perhaps one additional microsecond for each run. Alternatively, you could adjust the happy path test to be right at the edge of the boundary by changing the 5 to a 0 or a 10 and by changing the expected outcome to 0 or 100, respectively.\n\nIf you don’t test the edges of the behavioral boundaries, your team’s safety net will not be as clear and thorough as possible. For example, if we use 42 instead of 11 as the erroneous input greater than 10, and later another developer mistakenly increases the maximum value to 30, our test suite wouldn’t catch this problem. Our team might not learn about the defect until some unlucky consumer receives a very unpleasant (and very loud) surprise.\n\nBecause thinking test-driven requires human thought, mistakes will still happen, but far less frequently. When TDD is used diligently, every software defect is either a missing test or a vaguely specified test (that is, a misunderstanding). In either case, the mistake is a gap in the team’s safety net. It often takes just one more test to close the gap.\n\nA Taste of TDD\n\nLet’s return to the Box software and give it the ability to indicate whether it is empty or not.\n\nThis example is written in JavaScript using Jasmine. In case you want to follow along, you won’t need anything except a text editor, a browser, and a download of the Jasmine library.\n\n1. Write a single test for a tiny bit of behavior:\n\ndescribe(\"box\", function() { it (\"starts out empty\", function() { var box = new Box(); expect(box.isEmpty()).toBeTruthy(); }); });\n\n2. Write just enough code so your test fails due to an assertion (also known as an expectation):\n\nBox = function() { };\n\nBox.prototype = { isEmpty: function() { return false; },\n\n};\n\n3. Run all of the tests—that is, develop the habit of running the whole suite each time. Make sure the new test fails with an informative message, as illustrated in Figure 1.2.\n\nFigure 1.2 Jasmine’s SpecRunner.html page showing the clean test failure.\n\nNote that Jasmine highlights the name of the test that has failed. The test (“spec”) name and failure message together describe the expected behavior. Seeing the test fail and reading the failure message is how you “test the test”: You know you’ve asked for behavior that has not yet been implemented.\n\n4. Write just enough code to get the test to pass as quickly as possible:\n\nisEmpty: function() { return true; },\n\nAll we did was change false to true. This technique, called Fake It, will be covered in detail in Chapter 2, “Basic Moves.”\n\n5. Run all of the tests again. They should all pass now, as shown in Figure 1.3.\n\nFigure 1.3 Jasmine’s test-results browser page showing the tests passing.\n\nWe’re not done with this behavior, but this seemingly trivial code is sufficient to pass all existing tests.\n\n6. Refactor diligently!\n\nBecause this is our very first test, there’s nothing to refactor here. Nevertheless, you should always take a moment to review both the tests and the implementation to see if you reduce any duplication or improve the clarity.\n\nBack to step 1!\n\n1. Write a single test\n\nWhere there’s a true, there’s usually a false—for example, when a Box isn’t empty. We’ll give Box an add() method:\n\ndescribe(\"box\", function() { it (\"isn’t empty after adding\", function() { var box = new Box(); box.add(\"red pen\"); expect(box.isEmpty()).toBeFalsy(); });\n\nit (\"starts out empty\", function() { var box = new Box(); expect(box.isEmpty()).toBeTruthy(); }); });\n\n2. Write just enough code to make your test fail cleanly. It won’t fail cleanly until it has a stub for add(). Resist the temptation to write an implementation for add() before you see the test fail: Box = function() { };\n\nBox.prototype = { add: function(item) { }, isEmpty: function() { return true;\n\n}, };\n\n3. Run all of the tests.\n\nOf course, the new test fails, as illustrated in Figure 1.4.\n\nFigure 1.4 A new failing test.\n\n4. Write just enough code to get both tests to pass:\n\nBox = function() { this.items = []; };\n\nBox.prototype = { add: function(item) { this.items.push(item); }, isEmpty: function() { return this.items.length === 0; }, };\n\n5. Run all of the tests to see them pass, as shown in Figure 1.5.\n\nFigure 1.5 Both tests passing.\n\n6. Refactor diligently!\n\nRefactoring the tests is as important as refactoring the implementation. Doing so often helps you write later tests.\n\nThere is a tiny bit of duplication between the two tests, and that’s enough. The duplicated code is highlighted in the following code:\n\ndescribe(\"box\", function() { it (\"isn’t empty after adding\", function() { var box = new Box(); box.add(\"red pen\"); expect(box.isEmpty()).toBeFalsy(); });\n\nit (\"starts out empty\", function() { var box = new Box(); expect(box.isEmpty()).toBeTruthy(); }); });\n\nAll unit-testing frameworks have a way to perform common setup for each test in the suite. In Jasmine, it’s called beforeEach(), and it runs once before each test within the containing describe block:\n\ndescribe(\"box\", function() { var box; beforeEach(function() { box = new Box(); });\n\nit (\"isn’t empty after adding\", function() { box.add(\"red pen\");\n\nexpect(box.isEmpty()).toBeFalsy(); });\n\nit (\"starts out empty\", function() { expect(box.isEmpty()).toBeTruthy(); }); });\n\nAnother run of the tests confirms that the changes made did not break any tests (Figure 1.6).\n\nFigure 1.6 Checking the refactoring.\n\nHere’s what we’ve accomplished with this tiny example:\n\nWe have some tested, production-quality code.\n\nWe have written part of an engineering specification.\n\nWe have written two distinct behaviors that this object provides.\n\nWe’ve reduced duplication in the tests.\n\nIn Chapter 2, we’ll walk through a richer example and explore more test- driven thinking and design choices.\n\nThe Future of Test-Driven Development\n\nOne of my favorite personal mottoes is “Never an absolutist.”2 Do I believe TDD is a practice that will never change and will never disappear?\n\n2 For the record, I was saying this long before we heard Obiwan Kenobi’s equally silly paradoxical utterance, “Only a Sith deals in absolutes.”\n\nOf course not: It’s already changing. What follows are some of those changes, along with their implications for the future of TDD. In each case, the TDD game is slightly modified, but the thought processes, techniques, and human activities remain essentially the same.\n\nAs far as TDD disappearing, well, that won’t happen as long as humans are needed to explain to a computer what it is that we want it to do for us.\n\nTDD and Behavior-Driven Development\n\nBehavior-Driven Development (BDD) is like TDD in many ways. It is the whole-team practice of building software by writing brief, human-readable scenarios, and getting them to work one at a time. BDD is defined as the practice of “exploring desired system behaviors with examples in conversations and formalizing those examples into automated tests to guide development.”3\n\n3 As defined in Behavior-Driven Development with Cucumber by Richard Lawrence and Paul Rayner (Addison-Wesley, 2019). distilled from the ideas of Liz Keogh and Daniel Terhorst- North.\n\nBDD isn’t merely a renaming of TDD. As Matt Wynne wrote in The Cucumber Book (Pragmatic Bookshelf, 2017), “BDD builds upon TDD by formalizing the good habits of the best TDD practitioners.”\n\nIn practice, BDD differs from TDD in the following ways:\n\n1. The tests are “business-facing.” That is, they express business rules and requests. The suite of tests (“scenarios” or “examples”) represents a runnable product specification, whereas the emphasis of TDD is often on a runnable engineering specification. To keep the scenarios readable to the entire team—whether technically inclined or otherwise—BDD examples are written in a ubiquitous domain language and use a handful of keywords.\n\n2. BDD is a whole-team activity and is best implemented with nearly continuous collaboration between the product and development specialists. Although this approach might seem burdensome on the\n\nsurface, BDD can reduce the need for many traditional meetings and hand-offs. The team works closely together to plan, specify, build, test, and demonstrate working software within hours or even minutes.\n\n3. TDD is included as a practice within the BDD cycle. There are often situations where finer-grained, “infrastructural” testing is required to complete a BDD scenario without cluttering the product specification with engineering details. Ergo, there’s no need to choose between BDD or TDD. They go together nicely, like chocolate and peanut butter.\n\nMany of the test-driven techniques described in this book apply equally to a team’s BDD scenarios: They favor developing more, smaller tests with fewer assertions; setting up just enough to test the business rule; reducing (for example, using Cucumber’s Gherkin Background duplication keyword); making replacing challenging dependencies with test doubles; and taking small, quick, and safe steps toward the team’s goal.\n\nthe scenarios\n\nreadable; strategically\n\nRecommended Reading\n\nTo learn more about BDD, check out any one of the following highly rated books:\n\nThe Cucumber Book by Matt Wynne, Aslak Hellesøy, and Steve Tooke (Pragmatic Bookshelf, 2017).\n\nBehavior-Driven Development with Cucumber by Richard Lawrence and Paul Rayner (Addison-Wesley, 2019).\n\nBDD in Action by John Ferguson Smart (Manning, 2023).\n\nTDD and Functional Programming\n\nWhen using Functional Programming (FP), whenever you create a type with a rigorous contract, you are effectively testing a lot of assumptions at compile-time. But there is also behavioral (runtime) code, and where there’s behavior, there’s the opportunity to get it wrong. Ergo, unit-testing FP behaviors is still beneficial, and building those behaviors by using a test- driven approach is as critical to quality FP as it is to object-oriented development.\n\nFP unit tests contain all the attributes of good tests, as described in Chapter 5, “Sustaining a Test-Driven Practice.” The following example gives a taste of unit testing in F# (a .Net FP language):\n\n[<Test>] let ``intersection of two sets should only contain common eleme\n\nlet left = initialSet |> add \"Rob\" |> add \"Awesome\"\n\nlet right = initialSet |> add \"Jason\" |> add \"Awesome\"\n\nlet result = left |> intersection right\n\nresult |> contains \"Awesome\" |> expectsToBeTrue\n\nresult |> contains \"Rob\" |> expectsToBeFalse\n\nresult |> contains \"Jason\" |> expectsToBeFalse\n\nHere is the passing implementation of intersection as another taste of FP syntax:\n\nlet intersection right left =\n\nleft |> List.filter(fun element -> right |> List.contains element )\n\nDaydreams of TDD, Quantum Computers, and No Implementation\n\nWe developers used to fear the day when computers could write their own code. We were worried not only because such computers might build Terminator robots (or worse, mountains of paperclips4) and take over the world, but also because we’d be out of a job.\n\n4 about https://en.wikipedia.org/wiki/Instrumental_convergence.\n\nRead\n\nthe\n\n“Paperclip\n\nProblem”\n\nFor many years, I’ve speculated that we would eventually see a computing breakthrough the entire implementation. At the time, I envisioned a quantum computer (QC) that could take a team’s specifications and search a “solution space” for a set of machine instructions that would pass all the tests.\n\nthat would allow\n\nthe computer\n\nto write\n\nIf the computer were fast enough, it could perhaps rewrite the entire implementation each time the team added a new test scenario. In other words, neither the team nor the development computer would ever need to read or refactor the implementation.\n\nTeams could then focus entirely on writing, refactoring, and maintaining the test suite. Future high-level programming languages could be limited to the syntax and structure needed for good test-writing (for example, no more loops or branching statements). Developers would write engineering specifications as unit tests, or co-author product scenarios while working side-by-side with product designers. Or perhaps the team roles of product designer, developer, and tester would blend and merge into something new. A test-driven approach would be the de facto standard for building and maintaining software.\n\nWhile I was daydreaming about the impacts of this hypothetical QC, a different computing breakthrough was happening. Noting the surging power and popularity of artificial intelligence5 (AI) and large language models (LLMs), I began to wonder whether these new tools would someday make my predictions a reality.\n\n5 Not to be confused with the “artificial intelligence” from the pre-LLM science fiction tales, which has been relabeled artificial general intelligence (AGI), and most likely does not exist at\n\nin\n\nthe time of this writing.\n\nThe Reality of TDD, Artificial Intelligence, and “Vibe Coding”\n\nUsing a rudimentary OpenAI script that contained a simple prompt including all my tests (I could easily add tests with each run of the script), I explored whether OpenAI could generate code to pass all my tests. And it did so, repeatedly and successfully.\n\nWith each new run of the script, my AI agent did not have access to any previous prompt, dialog, or existing implementation. There was nothing for me or the AI to refactor, because it always started from scratch and overwrote all the code with each run.\n\nMy experiment was only that: a simple proof-of-concept. I had the script build simple bits and pieces of various classroom exercises (for example, the Salvo game described in the Exercises Appendix at the end of this book).\n\nThere are several ways that AI tools are currently assisting real developers. Some offer AI “autocomplete” options that are quite good at predicting what the developer intended to write next, and the complete code appears much faster than the developer could have typed it. Others read all the existing code and will suggest refactorings. The developer merely needs to glance over the proffered code and press a single key to accept the changes.\n\nintegrated\n\ndevelopment\n\nenvironments\n\n(IDEs)\n\nAI agents can even write tests for you. However, the generated tests that I saw merely confirmed that the code did what it did, not necessarily that it did what was wanted. This is the gap in the workflow where humans are still needed: Someone needs to come up with descriptive and detailed examples of what they want the software to do.\n\nRight now, various methods for incorporating AI into the full development workflow are vying for our attention. One of those approaches is called “vibe coding,” which was described to me as allowing the AI agent to write the implementation without any review. The person receiving the code then tests it and gives the AI feedback. This is happening in a few different ways:\n\nThe human manually tests the application and then explains to the AI where it failed, often using an example. They do this repeatedly until the human is satisfied with the results. At least one person who used this method expressed occasional frustration with the AI agent’s tendency to incorrectly anticipate unspoken needs. to seemingly re-interpret older requirements or\n\nThe human lets the AI write its own tests, and the human reviews and runs those tests.\n\nThe human gives the AI examples, either one at a time or in a batch, and adds to those examples if either the human realizes there is a gap in the examples (a common and natural occurrence with a test-driven approach) or the AI delivers a solution that is in some way too generalized (also common when pairs or ensembles develop code using a test-driven approach). This style most closely resembles my early experiment with OpenAI.\n\nSome things I noted about these reported experiences:\n\nIn every case, clear, specific examples—either written by or approved by the person making the requests—were necessary at some point in the workflow.\n\nExcept for the time saved by having the AI write code (which was certainly a significant savings), little time was saved during interactions with the AI. Put another way, the person interacting with the AI still spent about the same amount of overall time explaining what they wanted.\n\nNone of the applications described to me could directly impact a user’s health, finances, or safety. I asked AI guru Scott Werner if he would use vibe coding for financial, medical, or safety applications, and he responded, “No, probably not anything life critical.”\n\nComputers can’t read our minds, and they don’t do well with ambiguous instructions. When it comes to our safety, health, finances, and other critical domains, we will need to describe—to the computer and to each other—all\n\ndesired outcomes using complete, descriptive, and detailed examples. That is exactly the practice of TDD.\n\nSummary\n\nThe real power of TDD comes from thinking about what the software needs to do and coming up with examples to represent that behavior. The red– green–clean steps are not meant to be mere mechanics, but rather a scaffolding to allow developers to explore, guide, and preserve those thoughts.\n\nThe next two chapters will use an example application to explore the thinking process in detail. Approach this material as you would approach the rules of a new game. These chapters provide the rules and basic strategies by walking you through a sample “game.” The example app is a simple one, and the resulting implementation will be trivial, but you will come away with an understanding of the power of test-driven thinking.\n\nOceanofPDF.com\n\nChapter 2. Basic Moves\n\nTest-Driven Development (TDD) is, in many ways, a cooperative game that you play with the computer. This isn’t meant to imply that developers are “goofing around” when they use TDD. Rather, it means that there are purposeful rules to the game (for example, no additional implementation code can be added unless a single test is failing), a prescribed order to how events unfold (as illustrated in Figure 2.1), and some tried-and-true winning strategies.\n\nThis chapter uses a simplified product request to provide a walk-through of the basic steps of TDD, while also exploring the thought process that TDD enables to create genuine, valuable software. First, though, you need to become familiar with a few of the “game pieces” and the “rules.”\n\nThe TDD Flowchart\n\nThe simple red–green–clean diagram in the Preface serves as a reminder of how straightforward TDD is as a practice, but there are a few detours that you can make along the way. The flowchart in Figure 2.1 identifies some of the most common detours.\n\nFigure 2.1 TDD flowchart with more detail.\n\nFollow this flowchart until it becomes a habit, and then you will know what to do at any point. Realize, however, that having a flowchart or detailed recipe does not limit your intellect or creativity.\n\nEven a simple flowchart can include details that may not be applicable to your situation. For example, consider the first diamond-shaped decision point, “Does everything compile?” At this point in the flow, the method you are testing might not yet exist, or you might have added a parameter. Regardless of whether you have a compiled language, at this point you write just enough implementation so that the test fails due to an assertion (note the second decision diamond), and not because something is undefined or throws a null-reference exception. In this way, you are “testing the test” to ensure that it accurately represents new behavior and that it can detect any mistakes in the future.\n\nDefinitions\n\nProduct advocate: This person collaborates closely with the rest of the development team to assure that customers’ most valuable software requests are delivered first and with impeccable quality. On Scrum teams, this is the “Product Owner”; on XP teams, it is the “Onsite Customer.”\n\nAssertion: A simple test function that compares an expected value or outcome with the actual value or outcome. Basic assertions are often included with unit-test frameworks. Assertions are also known as expectations. Here are some examples:\n\nUsing JUnit:\n\nassertEquals(expectedValue, actualValue);\n\nUsing rSpec:\n\nexpect(actualValue).to be(expectedValue)\n\nThinking in Tests\n\nTDD takes what is usually an unconscious skill and makes it conscious and repeatable. Developers, testers, and product advocates already converse in examples: “When this happens, that should happen” and “When this happens, that should not happen.” We take those examples and record them",
      "page_number": 31
    },
    {
      "number": 2,
      "title": "Basic Moves",
      "start_page": 51,
      "end_page": 80,
      "detection_method": "regex_chapter_title",
      "content": "as repeatable automated tests, thereby preserving the effort we’ve invested in that behavior.\n\nEach test is a single scenario describing—to yourself and future developers —how to use this behavior, and what you expect the new behavior to do for the caller.\n\nYou write this test, or specification, just a moment before you write that implementation. If you follow this practice regularly, eventually you will be able to type in a new test as rapidly as you can think of the new scenario. Writing the test and refining the scenario often occur together. In this way, the amount of “extra” time required to write the test before you write the the implementation approaches zero. After all, you cannot write implementation code without already knowing what it is supposed to do.\n\nGiven that the word “test” carries some cultural baggage, you might want to think of each test as being an example, specification, or scenario. Besides, it isn’t really a test until it passes for the first time. From then on, that test checks whether the behavior you’ve developed remains intact indefinitely.\n\nThe structure of each test is determined by the testing framework used. Nevertheless, each test must contain three parts:1\n\n1 You may know these components as “arrange,” “act,” and “assert.” I prefer given/when/then because those terms are linguistically natural and require little explanation. Also, alliterations can accidentally achieve alternative arrangements.\n\n1. “Given”: Set up what needs to be true in this scenario before the tested behavior can be exercised.\n\n2. “When”: Invoke the behavior you want to test.\n\n3. “Then”: Once the behavior has completed, check your expected outcomes.\n\nHere’s what these parts mean for unit testing in particular:\n\nGiven: The test creates the state necessary for this distinct scenario. For example, it might create an instance of the object under test and\n\nset the state on that object so it will have the resources it needs. That state makes this scenario unique.\n\nWhen: Call the method or function under development.\n\nThen: Use the test framework’s assertions to confirm that everything happened as expected. What does the tested code do? Does it return a value? Does it change its own state? Does it delegate to a dependency? The test verifies that the implementation performed those tasks correctly. Tool Tip\n\nThere is at least one Junit-style or rSpec-style testing framework for most programming languages. I have used the following and recommend them: JUnit for Java, MSTest and NUnit for .Net languages, rSpec for Ruby, Python’s built-in unittest framework, and Jasmine and Mocha for JavaScript.\n\nSome test frameworks, like Cucumber, explicitly delineate these three testing sections. In other frameworks, you could add given/when/then comments, or you could separate the three sections with a line of whitespace.\n\nCan you identify the given, when, and then in the following test?\n\nimport org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.*;\n\npublic class TrackerGroupTests {\n\n@Test void isEmptyWhenConstructed() { TrackerGroup trackers = new TrackerGroup(); assertThat(trackers.isEmpty()).isTrue(); }\n\n}\n\nThis test is so small that the given might elude the casual reader: It’s just the new TrackerGroup(). The when is the call to trackers.isEmpty(). The rest of that line is all then.\n\nYou could break the last line of the preceding test into two lines and use a temporary variable to hold the results. Alternatively, you could put the whole test on one line, as shown here:\n\nassertThat(new TrackerGroup().isEmpty()).isTrue;\n\nWhat you choose will depend on what you and your teammates consider to be clear and readable test code (a topic addressed in Chapter 5, “Sustaining a Test-Driven Practice”).\n\nOf the three parts of a test, the most often neglected or abused is then. To write a test, you must know what the outcomes should be; otherwise, you are doing nothing more than exercising the code and discarding the results, which doesn’t test anything. Even worse, a collection of such tests would give your team a false sense of security by artificially increasing code coverage.\n\nOne way to avoid this pitfall is to write the assertion first. What do you expect to be the results of this new behavior? If you don’t know that, you aren’t ready to write a test. You might need to use a calculator or a slide rule, search the Internet, or ask someone who knows what is expected, such as your product advocate.\n\nKey Lesson\n\nYou cannot write a test for a behavior until you know what the outcome(s) should be. You must know the answer!\n\nOnce you know what the outcomes should be, you will have a better idea of how the object should be called to invoke that behavior. In other words, writing the test guides your design of the object’s interface from the perspective of code that calls your code.\n\nDefinition\n\nInterface: The services an object offers to other objects, and the data types necessary for those services to operate correctly. This includes publicly accessible methods, functions, and constructors, as well as the order and type of parameters passed into these methods. In many programming languages, an object’s interface\n\nis primarily the method signatures marked with a public keyword.2\n\n2 There are exceptions: In Java, an object within the same package can access “package protected” methods of another object. Also, C# has redefined public as those methods accessible to code outside the DLL and has the internal keyword for parts of the interface that are accessible to other objects within the DLL.\n\nNot to be confused with the Java and C# interface keyword. Whenever I do refer to the interface keyword, I will make that distinction clear.\n\nThree Basic Techniques\n\nThe three primary “moves” (originally described by Kent Beck3) you play in the TDD “game” are as follows:\n\n3 Kent Beck, Test-Driven Development by Example (Addison-Wesley, 2003), p. 13.\n\nFake It (’Til You Make It): Adding implementation code that uses a constant or replaceable value just to get the test to pass with little effort, so you can move on to the next related test.\n\nTriangulation: Writing a test for a new part of the behavior you are adding, knowing that you haven’t covered enough scenarios to justify a complete implementation.\n\nObvious Implementation: Writing an implementation that works for all existing tests and is simple enough to write without mistakes. This is not necessarily the “final” implementation.\n\nTDD isn’t a thought-free practice, but it can look that way to an outside observer, because each move is small and quick. Your experience, intelligence, and creativity are all funneled through TDD. If you follow the rules of the game diligently, you will eventually stop thinking about the rules, and you will consistently write valuable, high-quality, maintainable code.\n\nRequests, Tasks, and a To-Do List\n\nTDD is a team effort. The team typically first breaks a request up into clear, easily digestible tasks.\n\nDefinition\n\nTask: A brief description of a short-term team activity or goal. Tasks differ from requests. Each task is a reminder for the team and contains a unique step required to complete the associated requests. By themselves, tasks don’t provide value to a user; therefore, tracking hourly estimates or actuals is unnecessary.\n\nTool Tip\n\nMy teams typically use large index cards (either 4 × 6 inches or 5 × 7 inches) for requests, but a request can also be a simplified “ticket” in your tracking software. We often use color-coding to differentiate between new requests and defects to repair. We use smaller (3 × 5 inches) index cards for tasks.\n\nFigure 2.2 illustrates an example of a request.\n\nFigure 2.2 Sample request.\n\nFigure 2.3 illustrates a related task.\n\nFigure 2.3 Sample team task.\n\nMy teams try to keep tasks down to a half-day or less, so we\n\nmight integrate the changes from one task before lunch and another before the end of the day. Pairs of developers volunteer for one task at a time, rather than having tasks be preassigned. If a task takes longer than a day, we inform the rest of the team, describe our lessons learned, and ask for help if necessary.\n\nOther examples of team tasks:\n\n“Refactor the duplicated adapter code so new adapters are easier to test”\n\n“Add an Accounts_Addresses join-table to the database schema and migrate existing relationships”\n\n“Set up a dev system for the new developer, including the IDE (do we need to purchase additional licenses?)”\n\nA team task is anything the team decides they must do soon\n\nthat isn’t already part of their process. In other words, “write tests” does not need to appear on a task because it is a routine part of how a test-driven team builds software.\n\nThe Modest To-Do List\n\nOne important TDD “game piece” that often gets overlooked is a short “to- do” list to capture future moves you don’t want to forget.4 This list holds test scenarios and refactorings for the very near future that relate to the task you are currently working on, and no other. These to-do items can occur to you before you start the task, or they can arise while you are working on a test and its implementation.\n\n4 Kent Beck referred to this as a “test list,” but it often identifies tests, refactorings, and other task-specific steps (e.g., a quick Internet search.\n\nCapture the creative thoughts that occur once you see what you’ve written and how the design is emerging. To stay focused and energized, always keep this list extremely short—maybe a dozen or fewer items.\n\nGetting Started\n\nBefore starting a task, take a moment to consider what you are about to build. Briefly describe a few small scenarios or behaviors and write them on your list.\n\nStart out with a few simple, quick wins. What sort of default (but correct) answers will this new method or function return right away? What’s the initial state of an object? How do the constructor’s parameters affect that state? For example, for an online shopping cart, no sales tax should be applied if there are no items in the cart.\n\nAnother source of quick wins is often unhappy paths, or error cases. For example, what happens when the shopper tries to add an unavailable item to the shopping cart?\n\nDefinitions\n\nPair programming: The practice of working on a development task with one other person, at one computer. Developers work as peers to complete a single task together. They can switch between driver and navigator roles whenever they choose.\n\nDriver: The person currently at the keyboard. Their responsibilities include writing clear tests, implementing those\n\nbehaviors in the most expedient way possible, noticing when a design issue is making either of those processes more difficult, and mentioning possible next refactorings or tests.\n\nNavigator: The person not at the keyboard. Their responsibilities include keeping the pair focused on the real goal of the current task, recording new ideas for tests and refactorings on the to-do list, suggesting which item to tackle next, and making sure the code’s design is headed in a reasonable direction based on what the task requires.\n\nRed: The state of having at least one test failing. “In the red” means you have caused a test to fail, either intentionally or accidentally.\n\nGreen: The state of having all tests passing, including your latest test. “Get to green” means to do something expediently to reach the “green” state. “In the green” means you are within that state, and you can safely discuss next steps, refactor, commit your changes, or write the next test.\n\nTool Tip: Managing Your To-Do List\n\nIt’s nearly impossible to derive all useful to-do items beforehand. Brainstorming scenarios as a pair or a team is a powerful technique, but you should be sensitive to the point when you have identified enough scenarios. When you feel confident enough to jump in and write that first test, it’s a good time to stop pondering and start coding. Allow more detailed scenarios and edge cases to arise as you write the code and add them to the to-do list as you think of them.\n\nIf you’re using pair programming, the navigator can write down ideas (mostly potential tests and refactorings) while the driver is writing some bit of code or test. The driver can also ask the navigator to add something they’ve noticed. Postpone lengthy discussions while there is currently a failing test (that is, while you are “in the red”). Wait until you are “in the green” to discuss next steps.\n\nThe to-do list can be maintained in a digital text file or on a piece of paper. Small index cards (3 × 5) work well. Whenever you think of something to add, draw an empty check box. Once the item is done, check the box. If you decide a to-do item is no longer relevant, X it out.\n\nBecause an index card has limited space, you won’t be tempted to include more items than you can accomplish in less than an hour. A long to-do list often contains some items you won’t really need.\n\nOnce your index card is full and all the check boxes are\n\nchecked or X-ed out, you can get another blank card, if necessary. If you’re working remotely with other developers, any simple, shared, synchronized format will work fine. A simple cloud-based text document works extremely well.\n\nMany integrated development environments (IDEs) have a\n\nway to keep track of to-do items by letting you add a “to-do comment.” Here’s an example:\n\n// TODO: refactor away the following duplicated code\n\nThat could be helpful, especially if you are coding solo. But please, never ever push a to-do comment to your\n\nrepository. To-do comments that are not done and are stored away in the repository will build up in your code base, much like ignored compiler warnings. Your latest insightful to-do reminder will become buried among the hordes of others, which defeats the purpose of having a to-do list in the first place. No one will know who is supposed to do that item, or when. In fact, it most likely will never get done. To avoid this outcome, decide whether the item really needs to be done, and then delete from the to-do list before you push to the repo.\n\nIf you are programming closely with others in a pair or\n\nensemble, don’t use to-do comments at all. As the driver, you can simply ask the navigator to record any to-do items that pop into your head, and you can discuss the merit of this new item once you’re back “in the green.”\n\nExercise Walk-Through\n\nImagine you’re building an application that tracks geotags attached to endangered mammalian populations (elephants, whales, tigers, ). The team has determined that they will need to be able to manipulate groups of tracker tags, each having a unique identification string. A group of tags represents some segment of a population—for example, “all right whales detected in the North Atlantic,” or “all right whales detected last winter.” The scientists using this application aim to manipulate these groups, seeking trends and behavioral patterns.\n\nHere is one developer task for this application:\n\nBuild a TrackerGroup class that can be combined with other TrackerGroup collections and can tell us if one group contains, or is contained by, another.\n\nThe following are a developer’s discussion notes from the team’s conversation with the product advocate:\n\n1. We need to create TrackerGroups from arrays of tag IDs (which will come from database queries).\n\n2. We want the TrackerGroup to be able to tell us if it’s empty.\n\n3. We need to be able to merge two groups (like an OR operator) or determine how they overlap (AND). Examples:\n\na. { A, B, C, D, E } AND { X, Y, Z, A, B, C } returns { A, B, C }.\n\nb. { A, B, C, D, E } OR { X, Y, Z, A, B, C } returns { A, B, C, D, E,\n\nX, Y, Z }.\n\nOperations should return a new TrackerGroup object containing those tag IDs.\n\n4. We need to know if one group contains all the IDs of another group.\n\n5. We need to know when two groups are the same—that is, when they contain all the same IDs.\n\nAlthough these requests could all fit on a single index card, the walk- through will break them down further across two chapters. First, we will start with these items:\n\n[cb] The TrackerGroup knows if it is empty.\n\n[cb] The TrackerGroup knows if a tracker ID is in the group.\n\nNote that our first to-do item comes directly from note #2, and the second to-do item is needed to support the behaviors outlined in notes #3 and #4. These are quick and easy wins that we can build upon.\n\nDefinitions\n\nImmutable object: An object whose state cannot be changed once it is created. In Java and C#, dates, strings, and integers are good examples of immutable objects. Immutables have many advantages (such as thread-safety) and very few drawbacks. Consider using immutables in your business model whenever applicable. For example, a postal Address object could have the ability to change; alternatively, you could make the object immutable, with a change of address then being represented by a fresh new object containing the new data.\n\nSprinting Baby: The term “baby steps” is already in common usage as a descriptor for taking very small steps, and all big software changes are really made up of many very small changes anyway. But “baby steps” can also imply going slowly.\n\n“Sprinting Baby” is a metaphor suggesting how small, careful steps can be done quickly, resulting in a highly productive “flow” state. Small, quick wins can also keep you motivated.\n\nAs you build confidence in your test-writing skills and in the\n\nsafety net of tests, you will find yourself adding behaviors and refactoring more quickly, taking small, quick, clear, and safe steps toward your goal. So, you’re taking baby steps, but fast. That’s the “Sprinting Baby.”\n\nStep 1: Write a Test for a New Behavior\n\nThe first step, whenever you’re doing TDD, is always the same: Write a test.\n\nTDD requires a mental shift from immediately writing the inner workings of a class or function to first crafting a precise, repeatable example of how that behavior is meant to work.\n\nThis approach might seem backwards at first. After all, in the preceding example, the list of TrackerGroup behaviors has already been decomposed into simple behaviors. You can probably guess at the implementation of each one. So, why test it before you’ve written it?\n\nMany developers have been trained to write the implementation first, then write unit tests if there’s time remaining—and there’s almost never enough time remaining. That’s one of the more obvious benefits of TDD: The implementation has tests, guaranteed.\n\nBut there’s an even more important reason to write the test first: Developers naturally think in terms of examples, and they often use examples when discussing solutions with colleagues or stakeholders. What’s new here is taking those thoughts and recording them as executable specifications— that is, as tests.\n\nIf you don’t feel comfortable writing a test first, practice writing down simple examples on a scrap of paper. For example, you might write: “When I create a TrackerGroup with an empty array, it knows it’s empty.” Then turn that example into test code without making it any more elaborate or comprehensive than what’s on the scrap of paper:\n\nimport org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.*;\n\npublic class TrackerGroupTests {\n\n@Test void isEmptyWhenGivenAnEmptyArray() { TrackerGroup emptyGroup = new TrackerGroup(new String[] assertThat(emptyGroup.isEmpty()).isTrue(); }\n\n}\n\nWhy the String array? This bit of code could have also been written as follows:\n\nTrackerGroup emptyGroup = new TrackerGroup();\n\nWith TDD, the way you write a test should match your expectations for objects and methods that will be useful in production. Early on, each new test will help you design the interface of the object. Be prepared to try out different styles of interfaces early, when altering the interface will not be challenging.\n\nIn this case, the list of developer notes presented earlier says that these objects will be created from the results of database queries. Tracker IDs could be added in the constructor either all at once or by using an add() method and requiring the caller to loop through the query results and add each one individually. When given that kind of choice, favor giving your object as much of its known state as possible in the constructor, and favor “immutable” objects—that is, once created, the objects do not change.\n\nBecause this is the very first test, it won’t even compile. The next step is to write enough code so the test compiles, runs, and fails “cleanly”—that is, it fails due to the assertion.\n\nStep 2: Make It Fail Cleanly\n\nWhat you need is just enough real code to make the test fail cleanly—that is, due to an assertion in the test, rather than because of a compile error or raised exception. This achievement is what we mean by “getting to red.”\n\nWriting a test for code that doesn’t exist yet might feel unnatural, particularly in a compiled language like Java. At this point, there’s no autocomplete to help you, and your IDE will flag a lot of what you type as a compiler error.\n\nThe following code makes the previous test fail cleanly:\n\npackage crittermaps; public class TrackerGroup {\n\npublic TrackerGroup(String[] trackerIDs) { }\n\npublic boolean isEmpty() { return false; }\n\n}\n\nModern IDEs will do a lot of the boilerplate coding for you. For example, IntelliJ wrote the skeletal TrackerGroup class and a stub for the method isEmpty(). Having the IDE do the rote work will save you time and help you stay focused on writing the interesting runtime behavior.\n\nIf you work in a language that has less syntactic overhead than Java or C#, you may find it just as easy to write all the code yourself.\n\nAlways run all tests, rather than just the new one. Read the failure message to ensure that the new test is, indeed, the test that’s failing.\n\nTool Tip\n\nRun all the tests. There’s usually a hotkey (or menu option) to run all tests in the project. Make a habit of using that hotkey. Run the tests whenever you feel the need for feedback from them. Perhaps run them twice before pushing your changes to the repository. If a single test fails, don’t commit your changes.\n\nMany modern IDEs will determine which tests are new and which tests are affected by any addition or refactoring, and will run them instantly whenever you save a change. This can provide you with continuous automated test feedback.\n\nStep 3: Make It Pass\n\nOnce you have confirmed that the test fails cleanly, you will want to make it pass as simply as possible. You want to “get to green”; that is, you want all tests to pass.\n\nReaching the point where all tests pass is a safe resting point. When you are “in the green,” you can have discussions about better implementations, refactorings, and next steps.\n\nWhen you are “in the red” (that is, you have one failing test), you need to act. If you don’t know how to get your latest test to pass, then delete it and\n\nwrite a smaller or simpler one—a test that you know how to implement.\n\nWe risk losing our way whenever we leave things in an uncertain state for too long. So, strive to take small, quick, simple, and safe steps.\n\nFor the first test describing part of a new behavior, a simple technique that Kent Beck called Fake It ‘Til You Make It will often be your best move.\n\nTechnique: Fake It\n\nThis technique for “getting to green” can seem counterintuitive, not because the implementation is difficult, but because it is absurdly easy. If the test represents the tiniest request you can make of your code, then you need only add a tiny bit of code to get it to pass.\n\nHere is the simplest possible implementation to get the test to pass. Anything more would include a small amount of untested—and therefore as-yet-unjustified—behavior:\n\npackage crittermaps; public class TrackerGroup {\n\npublic TrackerGroup(String[] arrayOfIDs) { }\n\npublic boolean isEmpty() { return true; }\n\n}\n\nMost developers, until they discover for themselves the advantages of TDD, just want to “write the code!”—meaning “all the code.” Resist that temptation with every fiber of your being!\n\nFake code is simple code. Typically, fake code is a single line of code, even just the return of a constant. If your fake code starts to look like an actual implementation, ask yourself: “Have I really tested all that behavior?”\n\nHere are a few notes about the Fake It technique:\n\n“Fake It” Until You’ve Justified It\n\nTo check whether the TrackerGroup is empty, you need to know the number of trackerIDs and compare that number to zero. You\n\ncould write that simple line of code in your sleep, right?\n\nBut we haven’t written enough test code to justify a complete implementation. Instead of writing more code than is currently needed, write down a few other examples on the to-do list, and wait to create a test from one of them until this test passes.\n\nImplementing everything now, without tests, would leave a tiny gap in the safety net. That gap might not seem like a big deal now, but most defects occur because a test is missing. Have you ever spotted a mistake in the code and thought, “How did we ever think that would work?” Save yourself hours of future debugging: Always write that test first.\n\nFake Code Is Real Code (But Temporary)\n\nThe term “fake” is misleading. It’s expedient code that gets the latest test to pass. It gets you to “green” rapidly, so you can do some refactoring, merge your changes, check your to-do list, write the next test, or take a lunch break.\n\nIn real-world development, fake code rarely survives more than a few seconds, but it does do something, and it isn’t broken.\n\nThe “Sprinting Baby” approach asks you to take smaller steps rapidly and to always stay in the realm of releasable quality, if not yet releasable scope.\n\nOnce you are safely “in the green,” you are ready for refactoring.\n\nStep 4: Refactor\n\nSo far in this walk-through, there’s nothing to refactor. However, you should get into the habit of taking a second or two to look around for something to improve, even if only slightly. If you neglect this step, the code smells will gradually accumulate, and a minor, simple refactoring will become a large, unpleasant refactoring.\n\nDefinition\n\nCode smell: A common pattern visible in the structure of your code that indicates the internal design needs to be improved.\n\nMany code smells have names, such as Long Method. You should rely on your team’s experience and preferences regarding the identification of code smells (for example, determining how long of a method is too long) and which code smells should be addressed first.\n\nThe reason why refactoring is an efficient way to design code is that it’s always easier to be a critic than an artist. Developers can quickly spot something they don’t like about a bit of code, even if they don’t immediately know what else can be done. Without the safety net of tests to protect existing behaviors, they risk breaking a behavior or making the design worse.\n\nTDD asks teams to “clean as you go” by identifying code smells and refactoring them away.\n\nRecommended Reading\n\nFor a more comprehensive study of refactoring and code smells, read Martin Fowler’s Refactoring: Changing the Design of Existing Code (Addison-Wesley Professional, 2018). The first edition contained timeless examples in Java, and the second edition expands upon those using JavaScript.\n\nRepeat the Cycle\n\nAfter refactoring, return to step 1 and write another test. Typically, the previous test suggests the next logical test. If you haven’t completed any small part of the functionality, you will want to continue working on it until it is done. Advancing the behavior you have started writing is known as the Triangulation technique.\n\nTechnique: Triangulation\n\nTriangulation is the technique you use to push a behavior further, encouraging your implementation to handle interesting edge cases or behavior that is beyond the default. This technique involves writing another test related to the same behavior as the previous test. If the previous\n\nimplementation was fake, this will cause it to be replaced with a more sophisticated implementation.\n\nDefinition\n\nBehavioral boundary: The imaginary dotted line separating two or more very closely related behaviors. Here are some common examples of such boundaries:\n\nDefault behavior and behavior that is altered by state\n\nHaving none of something and having at least one\n\nInvalid data and valid data\n\nMore will be said about behavioral boundaries in Chapter 5,\n\n“Sustaining a Test-Driven Practice.”\n\nWhen triangulating, you want to address the other “side” of the behavioral boundary you have just exposed. For example:\n\nAn empty TrackerGroup implies that a non-empty TrackerGroup could exist.\n\nA TrackerGroup that does not contain a tracker ID implies that a TrackerGroup could contain that tracker ID.\n\nAlways update your to-do list as you think of new tests to write:\n\n[cm] The TrackerGroup knows if it is empty.\n\n[cb] When *not* empty.\n\n[cb] The TrackerGroup knows if a tracker ID is *NOT* in the group.\n\n[cb] When the tracker ID *IS* in the group.\n\nTo triangulate, create a non-empty TrackerGroup:\n\n@Test void isNotEmptyWhenOneItemIncludedInConstructor() {\n\nString[] trackerIDs = { \"any ID\" }; TrackerGroup trackers = new TrackerGroup(trackerIDs); assertThat(trackers.isEmpty()).isFalse();\n\n}\n\nNote that the test already fails cleanly with the existing implementation. Don’t add anything that isn’t needed to get a clean failure; for example, don’t create a field for the tracker IDs:\n\npublic class TrackerGroup {\n\npublic TrackerGroup(String[] trackerIDs) { }\n\npublic boolean isEmpty() { return true; }\n\n}\n\nTo get this test to pass, you can now add an “Obvious Implementation.”\n\nTechnique: Obvious Implementation\n\nAn Obvious Implementation is the simplest implementation that is sufficient to pass the failing test without faking anything within that implementation.\n\nTo get this latest test to pass without breaking others, add some simple logic. Here is one possible Obvious Implementation:\n\npublic class TrackerGroup {\n\nprivate final String[] trackerIDs;\n\npublic TrackerGroup(String[] trackerIDs) { this.trackerIDs = trackerIDs; }\n\npublic boolean isEmpty() { return trackerIDs.length == 0; }\n\n}\n\nAn Obvious Implementation is often the code that you knew you needed all along. So, why not write it first? First, you might overlook a simple yet significant behavioral boundary, which could result in a defect during a future modification. Second—and this happens more frequently than you might think—you could make a nearly invisible mistake and spend an excessive amount of time trying to fix what you thought was “obvious.” For those reasons, take the rapid baby steps leading up to “obvious.”\n\nThis change included a replacement of the fake code. If you forget to swap out the fake code, the failing test will continue to remind you.\n\nAlways take a tiny step, run the tests, and repeat. No matter how large and complex your software becomes, small steps taken within the safety net of tests will help you complete the task faster and with fewer errors.\n\nIf you can resist taking bigger steps—either broader tests or extra implementation—eventually the TDD steps will become an embodied habit, and every step you take will be quick and safe.\n\nNext Moves\n\nA couple of items remain on the “quick wins” to-do list. We will wrap up that list and introduce some interface design trade-offs simultaneously.\n\n[cm] The TrackerGroup knows if it is empty.\n\n[cm] When *not* empty.\n\n[cb] The TrackerGroup knows if a tracker ID is *NOT* in the group.\n\n[cb] When the tracker ID *IS* in the group.\n\nYou want a method to determine whether a TrackerGroup contains a particular tracker ID. When you write the first test for this entirely new method, give some thought to the name of that method.\n\nYou could give your new method a long and descriptive name. With autocomplete built into almost every IDE and editor nowadays, you may never need to type the full name more than once. For example:\n\nboolean containsThisParticularTrackerID(String trackerID)\n\nOr, your team might prefer Java’s naming conventions, where methods that return a Boolean value usually start with “is”:\n\nboolean isTrackerAMember(String trackerID)\n\nOr you could use something short, yet clear:\n\nboolean has(String trackerID)\n\nThe key guideline is to make the method’s purpose obvious in the caller’s code. Because the first caller of a new method is a unit test, this is the best opportunity to try out different names. For example:\n\nassertThat(endangeredAfricanElephants.has(“Dumbo”)).isTrue();\n\nYou do not have to complete your to-do list in top-to-bottom order. As an example, we will implement the last item on our to-do list next:\n\n@Test void containsAnID() {\n\nString idToFind = \"the ID of interest\"; String[] trackerIDs = { idToFind }; TrackerGroup trackers = new TrackerGroup(trackerIDs);\n\nassertThat(trackers.contains(idToFind)).isTrue();\n\n}\n\nWhy \"the ID of interest\"? Unless the test data is being used to test parsing or validation of the data, favor using data that is obviously nonsensical. Unrealistic data is shorthand for “this value doesn’t affect the outcome of this test.”\n\nAlso, idToFind is used instead of duplicating the string \"the ID of interest\" in the assertion. This helps in the following ways:\n\nIt prevents typos from causing the test to fail for the wrong reasons.\n\nThe name of idToFind helps clarify how the value is being used in the test. You could use just id, but why abbreviate? It’s not any ID; it’s the idToFind.\n\nIf you let the IDE write the skeletal method implementation for you, most IDEs will use the name of the test’s local variable as the name of the method’s parameter.\n\nHere is the code to make that test fail cleanly:\n\npublic class TrackerGroup { // ...Previous code is still here; not shown for brevity...\n\npublic boolean contains(String idToFind) { return false;\n\n}\n\n}\n\nAnd here is the code to make it pass, using the Fake It technique:\n\npublic boolean contains(String idToFind) {\n\nreturn true;\n\n}\n\nChoosing Triangulate, Fake It, or Obvious Implementation\n\nOnce again, you probably know exactly how to implement contains() in your preferred programming language. Even so, let yourself complete the Triangulations around each behavioral boundary before implementing an Obvious Implementation. Each failing test earns you the chance to incrementally grow the code that you were already sure you needed.\n\nYou might wonder if you could replace the fake code with the Obvious Implementation as a “refactoring.” That would be appropriate only if it does not add anything to existing behavior. Later, you might forget to write the tests that would have earned you that implementation. Most of the (quite rare) defects found in test-driven code can be traced to a missed specification.\n\nWhen you play the TDD game by the rules, you will begin to feel more confident in your implementation. Once you’ve locked down part of a behavior with one or more passing tests, you won’t have to waste time wondering “Do I have it right? What happens if ?” Replace every “What happens if ?” with a Triangulation test.\n\nHere is the Triangulation test for contains():\n\n@Test void doesNotContainAnID() {\n\nTrackerGroup trackers = new TrackerGroup(new String[]{ \"not\n\nassertThat(trackers.contains(\"cannot be found\")).isFalse();\n\n}\n\nCould you still get this test to pass with a single, simple line of code? More to the point, is there an unspoken assumption written into the existing tests?\n\nThere will be times when the lines between the Fake It and Obvious Implementation techniques are blurry. What you always want to do is write the simplest code that will pass the tests, then find and write the triangulating test that helps you get closer to a completely implemented behavior.\n\nAs the TDD game unfolds, you will often notice that extremely brief and simple tests will get you most of the way home, but a richer and more specific test might be needed to force the code to handle the most general cases. For example, to get the currently failing triangulating test to pass, try this:\n\npublic boolean contains(String idToFind) { return trackerIDs[0].equals(idToFind); }\n\nThat makes the test pass, but it’s silly, right? It seems too complex to be fake and too wrong to be obvious. Yet it is the simplest code that makes the existing tests pass. This indicates that there is a small hole in your safety net: The tests so far have included only one tracker ID. When you spot something like this, immediately add it to your to-do list:\n\n[cm] The TrackerGroup knows if it is empty.\n\n[cm] When *not* empty.\n\n[cm] The TrackerGroup knows if a tracker ID is *NOT* in the group.\n\n[cm] When the tracker ID *IS* in the group.\n\n[cb] When the ID to be found by contains() is included among other\n\nIDs\n\nChoosing to Add or Modify Tests\n\nTo extend the testing, you could write a new test with a TrackerGroup that has more than one ID, or you could add another item to the initialization array in an existing test.\n\nThe “correct” move is always contextual. In this example, the cost of being wrong is extremely small. When you’re faced with similar decisions in your own work, here are some things to keep in mind:\n\nMore, smaller tests are typically easier to read and understand than fewer, larger tests.\n\nFavor what is most readable and maintainable to your current teammates.\n\nYou want a set of tests that will be descriptive enough to prevent future misunderstandings.\n\nIf you choose a path that leads to a poor design, how much refactoring will it take to make the design clearer?\n\nFavor clarity, expressiveness, and comprehensiveness in the collection of tests you choose to leave behind.\n\nYou could write this as a new test or modify containsAnID():\n\n@Test void containsAnIDWhenMoreThanOneToSearchThrough() {\n\nString idToFind = \"id of interest\"; TrackerGroup trackers = new TrackerGroup(new String[]{ \"some uninteresting id\", idToFind });\n\nassertThat(trackers.contains(idToFind)).isTrue();\n\n}\n\nNow to “get to green,” here is one possible Obvious Implementation:\n\npublic boolean contains(String idToFind) { for (String trackerID : trackerIDs) { if (trackerID.equals(idToFind)) return true; } return false;\n\n}\n\nIt’s time for some closure on this to-do list:\n\n[cm] The TrackerGroup knows if it is empty.\n\n[cm] When *not* empty.\n\n[cm] The TrackerGroup knows if a tracker ID is *NOT* in the group.\n\n[cm] When the tracker ID *IS* in the group.\n\n[cm] When the ID to be found by contains() is included among other\n\nIDs\n\nNever Skip Refactoring\n\nAs an example, let’s replace all occurrences of idToFind with trackerToFind. This would require a few straightforward “Rename Variable” or “Change Signature” refactorings. For example:\n\npublic boolean contains(String trackerToFind) {\n\nfor (String trackerID : trackerIDs) { if (trackerID.equals(trackerToFind)) return true; } return false;\n\n}\n\nRun your tests after each refactoring, even if the IDE did all the work for you or when it seems that “nothing could possibly break.” Stay “in the green” (that is, keep all tests passing) while the stakes are low. As the system becomes more complex, the more comprehensive your safety net is, the more confident you will be in all refactorings.\n\nSave Your Changes\n\nWhenever you feel like you’ve completed a small, but significant bit of functionality, run all your tests one more time to be sure they pass. Commit and push your tests and implementation to a repository (for example, with git). Then celebrate!\n\nSummary\n\nIn this chapter, you encountered many of the “essentials” for TDD. Later chapters will build upon these skills and will introduce useful ancillary\n\ntechniques. Here are some key takeaways from this chapter.\n\nDefining Test-Driven Development:\n\nPeople naturally think about and discuss software behaviors in terms of “given, when, then.” By writing each of those scenarios into a test, you will clarify your thoughts now, and those tests will protect those behaviors from unintended alterations in the future.\n\nThe “unit” you are testing with each unit test is a discrete behavior. You cannot always match a unit of behavior to consecutive lines of implementation code. Therefore, the only place the behavior is clearly described is in the test. Each test is a step-by-step recipe for one solitary path through the code.\n\nPlaying the TDD “Game”:\n\nYou are playing to win (to avoid defects) now (by writing the test) and in the future (whenever you need to refactor). With TDD, defects are usually due to missing tests, not incorrect tests.\n\nTake baby steps, but quickly. The full “turn” of the TDD cycle should take no more than 5 minutes. Notice whenever it takes longer, and give yourself time to step back and adjust your strategy for completing your current task. Write down some smaller steps on the to-do list, then tackle each individually.\n\nDo all thinking, planning, discussing, designing, and research while “in the green”—that is, when all tests are passing.\n\nWhen “in the red” (a test is failing), get to green (all tests passing) as quickly as possible. Do not linger in the danger zone. If you can’t get from red to green, consider reverting to the last point when all tests were passing.\n\nRefactor only when “in the green.” If a test fails during a refactoring, revert to the most recent known good state, then try taking a smaller step forward.\n\nWriting Clear Tests:\n\nEvery test has three parts: given (arranging what must be true before the behavior is invoked), when (invoking the behavior), and\n\nthen (asserting what should be true after the behavior has been invoked).\n\nYou will likely write more lines of test code than lines of implementation. Each test is a description of a single scenario. It will read like a script, with step-by-step code, and without branches or loops.\n\nDon’t guess: To write a complete test, you must know the precise intended outcomes of the behavior.\n\nNotice behavioral boundaries and remember to test all “sides” of a boundary.\n\nEach unit test will remain as a detailed engineering specification describing one scenario.\n\nFavor more tests with fewer assertions, so any future failures pinpoint and describe the behavior that broke.\n\nChapter 3, “Build upon Existing Behavior,” continues the TrackerGroup walk-through with a focus on using the behaviors you’ve tested and built so far to write further tests and implementation. Thinking test-driven helps developers avoid continuously rebuilding the same behaviors in different parts of the system, or duplicating those behaviors in the tests.\n\nThere is also a second meaning of the title of Chapter 3: We’ll be delving into what it means to think test-driven by learning from simple but common coding mistakes.\n\nOceanofPDF.com\n\nChapter 3. Build upon Existing Behavior\n\nIn this chapter, we continue the TrackerGroup walk-through from Chapter 2, “Basic Moves.” With the simplest but necessary behaviors built, it will be easier to build more complex behaviors.\n\nNext Steps\n\nEach time you preserve what you wrote (both tests and implementation) in a repository, you can step back and look at the business requests and modify or re-create your to-do list. Here is our original list of developer notes:\n\n1. We need to create TrackerGroups from arrays of tag IDs (which will come from database queries).\n\n2. We want the TrackerGroup to be able to tell us if it’s empty.\n\n3. We need to be able to merge two groups or determine how they overlap (AND). Examples: { A, B, C, D, E } AND { X, Y, Z, A, B, C } returns { A, B, C }. { A, B, C, D, E } OR { X, Y, Z, A, B, C } returns { A, B, C, D, E, X, Y, Z }. Operations return a new TrackerGroup object containing those tag IDs.\n\n4. We need to know if one group contains all the IDs of another group.\n\n5. We need to know when two groups are the same—that is, they contain all the same IDs.\n\nHere’s a fresh to-do list:\n\n[cb] Merge two groups\n\n[cb] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nGroup Tests by Behavior\n\nThere is no need to limit yourself to one test class per production class. Grouping tests into separate classes is a helpful way to categorize different responsibilities of a production class.\n\nDepending on your unit-testing framework, you might also have available a way to nest groups of behaviors within a single test file. For example, Ruby’s rSpec and JavaScript’s Jasmine and Mocha all have a describe clause that allows grouping and nesting of distinct behaviors. Java’s JUnit5 also introduced nesting.1\n\n1 I found Junit5’s nesting syntax overly noisy and unhelpful. Your mileage may vary.\n\nIn this walk-through, we’ll stick to using separate test classes.\n\npublic class CombiningTrackerGroups {\n\n@Test void combinedWithEmpty_HasSameIDs() { String foo = \"foo\"; String bar = \"bar\"; TrackerGroup someGroup = new TrackerGroup(new String[] TrackerGroup empty = new TrackerGroup(new String[]{});\n\nTrackerGroup combo = someGroup.combinedWith(empty);\n\nassertThat(combo.contains(foo)).isTrue(); assertThat(combo.contains(bar)).isTrue(); }\n\n}\n\nThis new test uses contains(), which was built in Chapter 2. Once you’ve thoroughly tested a behavior, you can use it in another test without again asserting that it works, because the tests for contains() also run every time you run all tests.\n\nTypically, you want to have only one assertion per test, but this test has multiple assertions. Including multiple assertions is a mild test smell. However, when they’re checking similar results of the same behavior, and it’s clear why you’re putting the assertions together, then you can leave them together.\n\nA stronger unit-testing guideline regarding multiple assertions is this: A unit test must not have assertions separated by more behavioral code. Put another way, a test should not have more than one when step. Multiple assertions with more than one given or when are a sure sign that multiple behaviors are being tested (see the “Test Smells and Refactorings” section in Chapter 5, “Sustaining a Test-Driven Practice.”)\n\nFake It\n\nThe previous test was written to be an easy win. Here’s the simplest passing implementation:\n\npublic TrackerGroup combinedWith(TrackerGroup otherTrackerGroup\n\nreturn this;\n\n}\n\nTriangulate\n\nFake It often reveals the other side of a behavioral boundary and suggests a simple Triangulation scenario. In this case, you could write a new test by swapping the object being called with the object being passed as a parameter, and—voila!—you have a failing test:\n\n@Test void emptyCombinedWithNonemptyGivesAllTagsFromNonemptyGroup() {\n\nString foo = \"foo\"; String bar = \"bar\"; TrackerGroup someGroup = new TrackerGroup(new String[] { foo, bar }); TrackerGroup empty = new TrackerGroup(new String[]{});\n\nTrackerGroup combo = empty.combinedWith(someGroup);\n\nassertThat(combo.contains(foo)).isTrue();",
      "page_number": 51
    },
    {
      "number": 3,
      "title": "Build upon Existing Behavior",
      "start_page": 81,
      "end_page": 109,
      "detection_method": "regex_chapter_title",
      "content": "assertThat(combo.contains(bar)).isTrue();\n\n}\n\nThese tests have a lot of duplication. We’ll add reducing the duplication to the to-do list for now, because there’s now a failing test. Always wait until all tests pass before refactoring either the tests or the implementation.\n\nThis test now justifies a minimal passing implementation.\n\nObvious Implementation\n\nYou could still “fake it”; that is, you could return the group that isn’t empty. On the one hand, a fake that has logic, or that has as many lines of code as the Obvious Implementation, is overkill. On the other hand, that a fake could still work suggests that perhaps the Triangulation test could have been more robust. But we’re “in the red” right now, so let’s “get to green” and consider our alternatives once all tests are passing.\n\nThe code to make the latest test pass might look like this:\n\npublic TrackerGroup combinedWith(TrackerGroup otherTrackerGroup\n\nList<String> allIDs = new ArrayList<String>(); allIDs.addAll(Arrays.asList(this.trackerIDs)); allIDs.addAll(Arrays.asList(otherTrackerGroup.trackerIDs)); return new TrackerGroup(allIDs.toArray(new String[0]));\n\n}\n\nIf all the internal shuffling between Java’s arrays and Lists bothers you, you’re not alone.2 Now that the tests are passing, let’s refactor the implementation.\n\n2 Merging two arrays is simpler in some languages: In Ruby, the + operator concatenates two arrays into a new one. In Python, however, + performs matrix algebra on the two arrays. That’s not what I expected! Good thing I wrote a test first.\n\nRefactoring from Obvious to Better\n\nWith the passing tests in place, replacing one simple but awkward implementation with a cleaner one is a matter of refactoring. If the refactoring occurs to you while you are writing your minimal passing implementation, add the refactoring step to your to-do list.\n\nThere are now two new items on our to-do list. I chose not to mark “merge two groups” as complete, because the tests are missing some richness:\n\n[cb] Refactor String[] to List<String> internally\n\n[cb] Merge two groups with non-empty groups!\n\n[cb] Clean up the combinedWith tests\n\n[cb] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nAdding examples and refactorings to the list allows you to weigh your options. You might choose to get more tests passing first and see where that leads you. Or, if writing more tests will be easier due to a refactoring—a very common outcome—you would want to refactor first.\n\nHere is a refactored TrackerGroup class that uses Lists internally but preserves the public constructor that takes an array:\n\nimport java.util.ArrayList; import java.util.Collections; import java.util.List;\n\npublic class TrackerGroup {\n\nprivate final List<String> trackerIDs;\n\nprivate TrackerGroup(List<String> listOfIDs) { this.trackerIDs = listOfIDs; }\n\npublic TrackerGroup(String[] trackerIDs) { this(new ArrayList<String>()); Collections.addAll(this.trackerIDs, trackerIDs); }\n\npublic TrackerGroup combinedWith(TrackerGroup otherTrackerG List<String> allIDs = new ArrayList<String>(); allIDs.addAll(this.trackerIDs); allIDs.addAll(otherTrackerGroup.trackerIDs); return new TrackerGroup(allIDs); }\n\npublic boolean isEmpty() { return trackerIDs.isEmpty(); }\n\npublic boolean contains(String trackerToFind) { return trackerIDs.contains(trackerToFind); }\n\n}\n\nThis refactoring of TrackerGroup left the public interface unchanged and the tests untouched. That won’t always be true. If, for example, you use a Rename Method refactoring on contains() to call it has(), you will change the tests as well.\n\nYour IDE’s built-in refactorings will take care of all details that will allow the code to compile and the tests to continue passing. For example, the Rename Class refactoring in a Java IDE will also rename the source file, so an automated refactoring is generally much safer than using a global find- and-replace operation.\n\nIf you think you are finished with a refactoring, but one or more tests fail, it’s a sign that the refactoring step was a larger one than you were prepared to take. Whenever that happens, revert to a state where tests were passing and take a smaller step toward your design goal.\n\nAlways give your to-do list a quick glance. Check the boxes you’ve completed, and X out any that no longer seem necessary. Checking a box on the to-do list is a micro-celebration. I wouldn’t be surprised to learn that there’s a tiny release of endorphins each time I check a to-do box.\n\n[cm] Refactor String[] to List<String> internally\n\n[cb] Merge two groups with non-empty groups!\n\n[cb] Clean up the combinedWith tests\n\n[cb] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nLet’s work on the next item in the list: “Merge two groups with non-empty groups!”\n\nImproving the Tests\n\nAn empty TrackerGroup was used for the following reasons:\n\nIt was easiest to create.\n\nThe initial implementation was easy to fake (but only once).\n\nIt might have seemed like a behavioral boundary.\n\nBut is there a behavioral boundary between an empty TrackerGroup and a non-empty TrackerGroup? Not really: When you triangulate around a true behavioral boundary, you will need to add or change behavioral code. We did not need to add extra code to handle the empty/non-empty boundary. Put another way, empty/not-empty is a test-data boundary, but not a behavioral boundary.\n\nThe implementation already handles the general case, but the tests do not reflect that general case. You have two options: write a new test with richer test data (one that does not use an empty TrackerGroup) or alter the existing tests.\n\nInvoke the Jetlagged Intern\n\nCan you imagine a scenario where someone on your team might try to simplify or optimize the code, but instead mistakenly breaks it?\n\nThe combinedWith(), and a pretty good Fake It move:\n\nfollowing\n\nis also a minimal passing\n\nimplementation\n\npublic TrackerGroup combinedWith(TrackerGroup otherTrackerGroup if (this.trackerIDs.size() > otherTrackerGroup.trackerIDs.s return this; else return otherTrackerGroup;\n\n}\n\nfor\n\nIt’s also wrong, but it passes all the existing tests. There’s a “gap” in our safety net.\n\nIf you spot this gap earlier, follow the standard TDD steps and Triangulate further. But take comfort: Even the best developers and testers will occasionally miss a good triangulating example until they examine their implementation.\n\nStep back on occasion, while “in the green,” and imagine yourself as an earnest but jetlagged intern or a new hire. See if there’s a way you could accidentally break the implementation without the tests detecting the error. Then fix the tests so that those potential mistakes cannot ever reach production.\n\nDefinition\n\nJetlagged Intern Mind: An ancillary TDD practice where the developers look over the existing tests and implementation and ask, “Did we miss anything? Do our tests truly represent the behaviors clearly, or did we take a shortcut that leaves us vulnerable to misinterpretation?”\n\nFirst, put yourself in the role of the “jetlagged intern”— someone new to the team who has just arrived from an overseas business trip and is attempting to be immediately useful by simplifying or optimizing parts of your code. Could the jetlagged intern make a reasonable but invalid assumption about the existing implementation?\n\nThen, ask yourself if the safety net would stop this person\n\nfrom introducing a defect. Do you have at least one fast automated test that will keep that new defect from reaching production? If not, there’s a gap in the safety net.\n\nFor each gap, write a new test or improve an existing test to represent the example that will protect you against that defect. Presumably, this new test will pass right away.\n\nStrengthening the safety net isn’t meant to defend against only\n\na jetlagged intern. It also protects the team from inadvertently introducing that same mistaken optimization or simplification during future critical refactorings. A detailed safety net keeps the\n\nteam from having to know every detail of every line of code in the code base.\n\nInvoking Jetlagged Intern Mind whenever your intuition warns you that there’s a gap in the safety net will also improve your TDD game overall by strengthening your ability to create realistic and well-targeted examples.\n\nYou could either add the following test or use it to replace or modify one of the previous combinedWith() tests. Before you run it, though, ask yourself if it will fail:\n\n@Test void combiningTwoGroups_GivesNewGroupContainingAllIDs() {\n\nString one = \"one\"; String dup = \"duplicate\"; String three = \"three\"; TrackerGroup a = new TrackerGroup(new String[] { one, dup } TrackerGroup b = new TrackerGroup(new String[] { dup, three TrackerGroup combo = b.combinedWith(a); assertThat(combo.contains(one)).isTrue(); assertThat(combo.contains(dup)).isTrue(); assertThat(combo.contains(three)).isTrue();\n\n}\n\nThis test already passes because earlier we wrote a minimal passing implementation. Invoking Jetlagged Intern Mind isn’t part of the original TDD game, but it is an important practice that allows you to notice your blind spots and will improve your test-driven skills over time.\n\nWhenever you have a nagging feeling that you’ve written more code than your existing tests require, take a moment to review and strengthen the safety net around the additional behavior.\n\nRefactoring the Tests\n\nIf you cannot readily tell the difference between two tests and what they’re testing, refactor them to make them clearly distinct. Doing so will make the tests easier and quicker for new team members to digest, and will often make future tests easier to write.\n\nRefactor based on code smells: Choose clear names for tests and test data, remove duplication, and ensure your test data avoids special domain values unless those values are intended to alter behavior.\n\nHere is the result of a few refactorings of this chapter’s test class:\n\npackage tests;\n\nimport crittermaps.TrackerGroup;\n\nimport org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test;\n\nimport static org.assertj.core.api.Assertions.*;\n\npublic class CombiningTrackerGroups { private TrackerGroup emptyGroup; private TrackerGroup groupA; private TrackerGroup groupB; private static final String IN_GROUP_A = \"one\"; private static final String IN_A_AND_B = \"duplicate\"; private static final String IN_GROUP_B = \"three\";\n\n@BeforeEach void createSampleGroups() { emptyGroup = new TrackerGroup(new String[]{}); groupA = new TrackerGroup( new String[] {IN_GROUP_A, IN_A_AND_B}); groupB = new TrackerGroup( new String[] {IN_A_AND_B, IN_GROUP_B}); }\n\nprivate void assertGroupContainsAll(TrackerGroup result, String[] expectedElemen for (String nextElement : expectedElements) { assertThat(result.contains(nextElement)).isTrue(); } }\n\n@Test void combiningTwoGroups_GivesNewGroupContainingAllIDs() { TrackerGroup result = groupB.combinedWith(groupA); assertGroupContainsAll(result, new String[] {IN_GROUP_A, IN_A_AND_B, IN_GROUP_ }\n\n@Test void emptyCombinedWithNonempty_HasAllFromNonempty() { TrackerGroup result = emptyGroup.combinedWith(groupA); assertGroupContainsAll(result, new String[] {IN_GROUP_A, IN_A_AND_B}); }\n\n}\n\nThis is what changed:\n\n1. Created constants for frequently used test values. This reduces duplication, improves readability, and avoids failures due to typos within a string—for example, \"RANDOM\" versus \"RAND0M\".3 3 Not a contrived example! Four developers all stared at a failure message that said “Expected ‘X3O4B’ but was ‘X304B’” for about 20 minutes, before someone figured it out. That’s one risk of using real airline confirmation numbers in unit-test data.\n\n2. Reduced duplication by creating a custom assertion via Extract Method.\n\n3. Removed one of the two tests using an empty group. It was redundant and could mislead a developer into thinking empty groups were special in a behavioral way.\n\nChapter 5 covers good test-writing practices, “test smells,” and other test refactorings in more detail.\n\nRemoving the Symptomless Defect\n\nTrackerGroup’s combinedWith() implementation has a potential flaw. The code copies every element of both TrackerGroups into the combo. What if an ID appears in both TrackerGroups? With the current implementation, duplicate entries would occur. That might not cause any visible problems with the requested behaviors now, but it could cause trouble later. Let’s add “prevent duplicate IDs” to the list and take care of it now:\n\n[cm] Refactor String[] to List<String> internally\n\n[cm] Merge two groups with non-empty groups!\n\n[cm] Clean up the combinedWith tests\n\n[cb] Prevent duplicate IDs\n\n[cb] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nLet’s ask TrackerGroup for a count of IDs. Our product advocate might not have a need for that functionality yet, but we need it to fix a defect. Avoid adding getters, setters, or behaviors just because “we might need it someday,” but recognize that sometimes you may need to add behavior to make testing simpler. Occasionally, if it’s useful for the tests, that behavior will prove useful for the product.\n\nFirst, here’s a quick test to build the simple behavior of counting the number of IDs:\n\n@Test void canCountNumberOfIDs() {\n\nTrackerGroup group = new TrackerGroup(new String[] {\"one\", assertThat(group.idCount()).isEqualTo(3);\n\n}\n\nAnd here’s the code to make it pass:\n\npublic int idCount() {\n\nreturn trackerIDs.size();\n\n}\n\nNow we add a new test that checks for accidental duplicate IDs:\n\n@Test void combiningTwoGroups_DoesNotAddDuplicates() {\n\nTrackerGroup result = groupB.combinedWith(groupA); assertThat(result.idCount()).isEqualTo(3);\n\n}\n\nUnsurprisingly, this results in the following failure:\n\norg.opentest4j.AssertionFailedError: Expecting:\n\n<4>\n\nto be equal to:\n\n<3>\n\nbut was not. Expected :3 Actual :4\n\nThere are many ways to fix this defect. One quick and simple way in Java is to replace ArrayList<String> with HashSet<String>. That approach will require a couple of refactorings, so at this point we would temporarily disable the combiningTwoGroups_DoesNotAddDuplicates() test.\n\nWe’ll first refactor every occurrence of “new ArrayList<String>()” into a private static factory method, freshStoreForIDs(), and change every other reference to the type of the tag ID storage to a Collection (the common ancestor of List and Set). That way the fix to avoid duplicates involves a one-line change to freshStoreForIDs().\n\nTypically, whether you do your refactoring before writing the test or after it passes is up to you. Always be sure you refactor only when all tests are passing. In this case, the easier path is to refactor first, then reactivate the test, then write the idCount() code to make it pass:\n\npackage crittermaps;\n\nimport java.util.*;\n\npublic class TrackerGroup {\n\nprivate static Collection<String> freshStoreForIDs() { return new HashSet<String>(); }\n\nprivate final Collection<String> trackerIDs;\n\nprivate TrackerGroup(Collection<String> listOfIDs) { this.trackerIDs = listOfIDs; }\n\npublic TrackerGroup(String[] trackerIDs) { this(freshStoreForIDs()); Collections.addAll(this.trackerIDs, trackerIDs); }\n\npublic TrackerGroup combinedWith(TrackerGroup otherTrackerG Collection<String> allIDs = freshStoreForIDs(); allIDs.addAll(this.trackerIDs);\n\nallIDs.addAll(otherTrackerGroup.trackerIDs); return new TrackerGroup(allIDs); }\n\npublic boolean isEmpty() { return trackerIDs.isEmpty(); }\n\npublic boolean contains(String trackerToFind) { return trackerIDs.contains(trackerToFind); }\n\npublic int idCount() { return trackerIDs.size(); }\n\n}\n\nTriangulating Away Side Effects\n\nImagine that every time you turned up the volume on your music player, it also advanced to the next song. That would be really irritating, right?\n\nThat particular side effect may be unlikely, but some classes of side effects can certainly creep into your system if you’re not thinking test-driven. The next request on the list, “Determine how two groups overlap,” demonstrates this possibility.\n\n[cm] Refactor String[] to List<String> internally\n\n[cm] Merge two groups with non-empty groups!\n\n[cm] Clean up the combinedWith tests\n\n[cm] Prevent duplicate IDs\n\n[cb] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nWe write a test:\n\n@Test void overlapContainsIDWhenInBothGroups() { TrackerGroup result = groupA.overlap(groupB); assertThat(result.contains(IN_A_AND_B)).isTrue(); }\n\nHere is some new TrackerGroup code to get the test to fail cleanly:\n\npublic TrackerGroup overlap(TrackerGroup otherTrackerGroup) {\n\nreturn new TrackerGroup(new String[] {});\n\n}\n\nAnd here’s some simple Fake It code to make the test pass:\n\npublic TrackerGroup overlap(TrackerGroup otherTrackerGroup) {\n\nreturn this;\n\n}\n\nDo you see the problem with this fake implementation? The result does, indeed, contain the expected IDs, but it also includes an undesirable extra ID.\n\nThe Jetlagged Intern Mind practice can help you avoid unexpected and undesirable side effects. You can’t test away all possible mistakes, but if you notice a likely erroneous side effect, write a test that assures this mistake never occurs.\n\nHere we Triangulate to make sure the “side effect” of including other IDs doesn’t happen:\n\n@Test void overlapDoesNOTContainIDsThatAreNOTInBothGroups() {\n\nTrackerGroup result = groupA.overlap(groupB); assertThat(result.contains(IN_GROUP_B)).isFalse(); assertThat(result.contains(IN_GROUP_A)).isFalse();\n\n}\n\nThen we write an Obvious Implementation (preferably obvious to all developers on your team):\n\npublic TrackerGroup overlap(TrackerGroup otherTrackerGroup) {\n\nCollection<String> foundInBoth = freshStoreForIDs(); otherTrackerGroup.trackerIDs.forEach( nextID -> { if (contains(nextID))\n\nfoundInBoth.add(nextID); }); return new TrackerGroup(foundInBoth);\n\n}\n\nDesign Detour: Let Design Choices Propagate Naturally\n\nA good design clearly communicates its intent to the team and is easy to maintain. “Good design” is contextual: It depends on the combined experience of team members with objects, functional programming, your programming languages, various schools of design wisdom (I recommend as starting points Daniel Terhorst North’s “CUPID” and Kent Beck’s “Four Rules of Simple Design”), and the intuitive wisdom of code smells.\n\nFor example, you may want to avoid using lambdas until all developers on the team are comfortable using lambdas. The most efficient way to share design knowledge is to play the TDD game in pairs or as an ensemble. My Extreme Programming (XP) teams typically change pairs frequently, which helps propagate new design idioms and technical knowledge throughout the code and the team.\n\nAvoid going through your code all at once and replacing an old idiom with a new one. Older code can be refactored when it needs to do something new. But whenever it is time to enhance old code, take the time to refactor it to conform to your team’s latest design preferences.\n\nA Picture Is Worth a Thousand Words\n\nSometimes, the tests, code, and to-do list aren’t enough to clarify all that needs to be done.\n\nLet’s use the next to-do item—“Does one group contain all the IDs of another group?”—as an example. Whenever I deliver my TDD course to developers, this is the point where roughly four out of five pairs of developers write tests that are less robust than they need to be.\n\n[cm] Refactor String[] to List<String> internally\n\n[cm] Merge two groups with non-empty groups!\n\n[cm] Clean up the combinedWith tests\n\n[cm] Prevent duplicate IDs\n\n[cm] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nLet’s skip a few details and jump to the typical results. Here are the tests:\n\npackage tests;\n\nimport crittermaps.TrackerGroup;\n\nimport org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test;\n\nimport static org.assertj.core.api.Assertions.*;\n\npublic class ContainsAll {\n\nprivate TrackerGroup smallGroup; private TrackerGroup biggerGroup;\n\n@BeforeEach void createGroups() { smallGroup = new TrackerGroup(new String[] { \"a\", \"b\", biggerGroup = new TrackerGroup(new String[] { \"a\", \"b\", }\n\n@Test void knowsWhenItDoesNOTContainAllIDs() { assertThat(smallGroup.containsAll(biggerGroup)).isFalse }\n\n@Test void knowsWhenItContainsAllIDs() { assertThat(biggerGroup.containsAll(smallGroup)).isTrue( }\n\n}\n\nHere is a typical implementation:\n\npublic boolean containsAll(TrackerGroup otherTrackerGroup) {\n\nfor (String nextElement : otherTrackerGroup.trackerIDs) { if (!contains(nextElement)) return false; } return true;\n\n}\n\nBut wait! Do you see how those previous two tests do not fully represent the implemented behavior? We jumped to Obvious Implementation too soon.\n\nTo illustrate, let’s instead implement containsAll() with the following Fake It move:\n\nreturn trackerIDs.size() > otherTrackerGroup.tr\n\nDo all tests still pass? Yes. Is this a minimal passing implementation? Sure. But is it correct? No. The implementation does what we asked it to do, but the tests missed an important behavioral boundary.\n\nYour testing challenges will be much more complex than this example, but here are a few suggestions:\n\n1. Take a 10-minute break. Take a walk or grab a snack. Don’t spend the break worrying about the challenge. Instead, do something that takes your mind off the challenge. Perhaps the solution will present itself.\n\n2. Hold a CRC card session with a few other teammates. (Chapter 5, “Sustaining a Test-Driven Practice,” covers CRC cards.)\n\n3. Discuss the challenge with another developer, and draw some simple diagrams on a whiteboard: a UI mock-up, a UML class diagram, a sequence diagram, a state diagram, or—in this case—a Venn diagram.\n\nFigure 3.1 shows the two TrackerGroups used in the previous tests.\n\nFigure 3.1 Insufficient example data.\n\nWhat is missing is a robust counterexample, as illustrated in Figure 3.2.\n\nFigure 3.2 Better example data.\n\nThe “other” TrackerGroup doesn’t have to be { x, y, z }. It could also be { a, b, x } or { a, b, c, x }. It needs at least one ID that does not appear in the “bigger” TrackerGroup.\n\nHere are the improved tests:\n\npublic class ContainsAll {\n\nprivate TrackerGroup smallGroup; private TrackerGroup biggerGroup; private TrackerGroup otherGroup;\n\n@BeforeEach void createGroups() { smallGroup = new TrackerGroup(new String[] { \"a\", \"b\", biggerGroup = new TrackerGroup(new String[] { \"a\", \"b\", otherGroup = new TrackerGroup(new String[] { \"x\", \"y\", }\n\n@Test void knowsWhenItDoesNOTContainAllIDs() { assertThat(biggerGroup.containsAll(otherGroup)).isFalse }\n\n@Test void knowsWhenItContainsAllIDs() { assertThat(biggerGroup.containsAll(smallGroup)).isTrue( }\n\n}\n\nThese tests will break the fake code and earn our original Obvious Implementation.\n\npublic boolean containsAll(TrackerGroup otherTrackerGroup) {\n\nfor (String nextElement : otherTrackerGroup.trackerIDs) { if (!contains(nextElement)) return false; } return true;\n\n}\n\nCombining Behaviors\n\nOne task remains. It involves an important design decision, and a delightfully simple conclusion:\n\n[cm] Refactor String[] to List<String> internally\n\n[cm] Merge two groups with non-empty groups!\n\n[cm] Clean up the combinedWith tests\n\n[cm] Prevent duplicate IDs\n\n[cm] Determine how two groups overlap\n\n[cm] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nYou are asked to provide a way to compare two TrackerGroups for equality. Should you override Java’s built-in equals() (Equals() in .Net) or create your own comparison method using your domain language?\n\nDesign Detour: Choosing equals(object other) or isEqualTo(TrackerGroup other)\n\nFor TrackerGroup—and most objects within a business application domain—if you need to know if two instances are equal, build a way to compare two instances without overriding any externally inherited methods. Use a name that is clear and a signature that requires a type within your business domain. For example:\n\npublic boolean isEqualTo(TrackerGroup other)\n\nWhen would you instead override Java’s equals() (.Net’s\n\nEquals())?\n\n1. When you are building a framework for use by developers, particularly when they are external to your team\n\n2. When instances could potentially be used within other common frameworks (for example, Collections)\n\n3. When the objects in your framework are considered “primitive”; that is, when they are used mostly as data- holders with limited behaviors or that rely upon familiar operators\n\nA classic example that meets all three criteria is if you were tasked with adding complex numbers (for example, 2.4 + 3.333i) to a programming language that doesn’t already support them (for example, Java).\n\nIf you do choose to override the built-in equals(), remember\n\nto test these edge cases:\n\n1. group.equals(null) should return false, not throw an exception.\n\n2. group.equals(\"I am a string\") should return false. The correct method boolean equals(Object other) in Java, and public override bool Equals(object other) in C#. is\n\n3. Two objects that are equal (but are not necessarily the same instance) must return the same hash code (in Java and .Net).\n\n4. Test that equals() is false when a TrackerGroup is compared against an object of a different type that has the same hash code. For example:\n\n@Test void whenNotEqualDespiteEqualHashcodes() {\n\nassertThat(group.equals(new Integer(group.hashCode())\n\n}\n\nIf you skip this test, a jetlagged intern could replace your\n\nequals() implementation with this faulty one:\n\nreturn this.hashCode() == other.hashCode();\n\nTool Tip\n\nA unit-test framework is a good workbench for running quick experiments on an unfamiliar framework or syntax. For example, to assure myself that an Integer’s hash code was the integer value itself, I wrote this test:\n\n@Test void integerHashcodeIsObvious() {\n\nint aHashCode = 16309; assertThat(new Integer(aHashCode).hashCode()).isEqual\n\n}\n\nLet’s build our domain-specific isEqualTo() method using TDD, starting with a fresh test class for this behavior:\n\npackage tests;\n\nimport crittermaps.TrackerGroup; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.*;\n\npublic class Equality {\n\nprivate TrackerGroup group;\n\n@BeforeEach void createInterestingGroup() { group = new TrackerGroup(new String[] { \"this\", \"is\", \" }\n\n@Test void equalWhenTheyHaveTheSameIDs() { TrackerGroup sameIDsDifferentOrder = new TrackerGroup(new String[] { \"interesting\", assertThat(group.isEqualTo(sameIDsDifferentOrder)).isTr }\n\n}\n\nMake it fail cleanly with either of the following additions to TrackerGroup.\n\nEither:\n\npublic boolean isEqualTo(TrackerGroup other) {\n\nreturn false;\n\n}\n\nOr, if your IDE will automatically create the tested method for you:\n\npublic boolean isEqualTo(TrackerGroup other) {\n\nthrow new NotImplementedException();\n\n}\n\nTo make it pass:\n\npublic boolean isEqualTo(TrackerGroup other) {\n\nreturn true;\n\n}\n\nNow create one more test:\n\n@Test void whenNotEqual() {\n\nTrackerGroup notTheSameGroup = new TrackerGroup(new String[] {\"not\", \"as\", \"intere assertThat(group.isEqualTo(notTheSameGroup)).isFalse();\n\n}\n\nOur reward for saving this task until the end: a one-line minimal passing implementation!\n\npublic boolean isEqualTo(TrackerGroup other) { return this.containsAll(other) && other.containsAll(thi }\n\nIt’s a great feeling to see the list completed!\n\n[cm] Refactor String[] to List<String> internally\n\n[cm] Merge two groups with non-empty groups!\n\n[cm] Clean up the combinedWith tests\n\n[cm] Prevent duplicate IDs\n\n[cm] Determine how two groups overlap\n\n[cm] Does one group contain all the IDs of another group?\n\n[cm] Are two groups equal?\n\nAnd all of the tests pass (Figure 3.3)!\n\nFigure 3.3 All tests passing!\n\nIt’s time to integrate your changes and celebrate.\n\nOptimize for the Right Reasons\n\nIn this example, the last method we wrote—isEqualTo()—could become a challenging performance bottleneck given enough data. isEqualTo() calls containsAll() twice, and containsAll() in turn calls contains() for each ID. Then contains() delegates to the enclosed HashSet, which is— thankfully—optimized for larger datasets.\n\nThere is likely an algorithm that would be more efficient for much larger datasets. Replacing the implementation with another, more efficient algorithm could be considered a reasonable refactoring. There are two important questions to ask yourself and your team before proceeding:\n\n1. Is it necessary?\n\nOptimizing parts of the code that are not the real bottleneck is unnecessary and could even make the real bottleneck worse. First, identify the actual bottleneck. Then, write a lightweight performance test isolating the area of code you plan to improve. This could be as simple as taking a timestamp at the beginning and end of a CPU-\n\nintensive call, then asserting that the difference is shorter than an acceptable maximum (include a margin of error; otherwise, the test may later fail on rare occasions and halt your productivity). Adjust the maximum so that the test fails consistently, then replace the inefficient implementation with a better one (incrementally, if possible).\n\n2. Is it maintainable?\n\nTypically, less code is better. For example, the one-line implementation of isEqualTo() is easy to understand. You could incur a maintenance cost for a more complicated implementation: A developer’s time is currently the most expensive part of software development.\n\nIf you spend a lot of time making every bit of your code ultra-efficient— memory-wise or clock-cycle-wise (or both)—you are likely spending time that would be better spent building much-needed functionality.\n\nSummary\n\nIn this chapter, we expanded upon the basics of the TDD game as well as what it means to think “test-driven.”\n\nKey Takeaways\n\nThe TDD game is best played using a Sprinting Baby strategy:\n\n1. Ask, “What is the simplest test we can write to earn the next bit of implementation?” where “simplest” does not always mean easiest to write. A copy-and-paste of previous tests will often lead to gaps in behavioral coverage.\n\n2. Write the minimum passing implementation for that test without breaking any other tests. Take care not to jump to Obvious Implementation too soon.\n\nInvoke Jetlagged Intern Mind whenever you feel you might have added more behavior than you’ve tested.\n\nCombine Sprinting Baby and Jetlagged Intern Mind to craft the most “leak-proof” safety net possible.\n\nAny behavior that is fully tested can be used in the implementation and build isEqualTo(TrackerGroup other), you could optionally replace the implementation of the custom assertion assertGroupContainsAll(). in\n\nUse new language features only after assuring that they won’t negatively impact customers (i.e., do they need to be upgraded, too?) or your fellow developers.\n\nWith TDD, you can still introduce a defect if your test data is too simple. Ask yourself, “How will different test data alter the behavior?” This might lead to a new or better test.\n\nTests deserve refactoring as much as the implementation. Preserve the intent of each test (the expected behavior), while clarifying and simplifying how it tests that single behavior.\n\nWhat to Try Next\n\n1. Build Your Own TrackerGroup\n\nStart over and rebuild TrackerGroup your way, following the TDD steps rigorously. Use the to-do list specifications in this and the previous chapter, but ignore the book’s tests and implementations. Perhaps write down the to-do items, then close the book.\n\n2. Change Something and Build It Again\n\nStart over, completely from scratch, but change something.\n\nTry working as a pair or ensemble. Be sure everyone sticks to the rules of the TDD game.\n\nTry a different programming language. C# and Java are too similar. Perhaps try JavaScript, Ruby, or Python. Choose a language that you are comfortable with or one that you would like to learn. TDD is a great way to learn a new programming language.\n\nShuffle the order of the to-do items. You can build software in any order, if—as you go—you test all behaviors and refactor away code smells and test smells.\n\nIf you’ve built TrackerGroup twice, consider or discuss the following questions:\n\nWas the second time around faster or slower? Why?\n\nWas it easier or harder, and why?\n\nDoes the implementation look different?\n\nDo the tests look different?\n\nChapter 4, “Exceptional Behaviors,” is a brief “addendum” to the TrackerGroup example. It covers how to test scenarios that require an exception to be thrown, and how to test the behavior of custom exception classes.\n\nOceanofPDF.com\n\nChapter 4. Exceptional Behaviors\n\nAll the code you write is important and deserving of tests. Whenever your code throws or catches an exception or raises an error, that too is your code’s behavior and can be tested.\n\nKey Lesson\n\nTest your code’s behaviors. This includes interacting with an external dependency, throwing an exception, or formatting a simple output string.\n\nYou rarely need to test simple data accessors and mutators (“getters” and “setters”) or .Net “properties.” Your compiler in a strongly typed language will be your first line of defense for simple getters and setters.\n\nHowever, if the setter can throw an exception when the data is\n\ninvalid, test that. If the getter formats a string based on one or more fields, test that.\n\nRaising/Throwing\n\nThrowing an exception (or “raising an error” in many languages) is intended to let a developer, or DevOps person, know that something technical is broken, not that an end user made a typo. This includes developer mistakes (they will happen), unavailability of a service, and so on.\n\nAn exception is merely an object. Raising or throwing an error or exception creates an alternative return path. Nothing terribly mysterious is happening. The thrown exception will bubble up the call stack, ignoring whatever else\n\nwas going on (if statements, loops, and the like) until it’s caught by a catch clause or until it reaches the top of the call stack and halts the app.\n\nMost of the web apps I have worked on using TDD followed similar guidelines regarding exceptions:\n\n1. Catch Java’s checked exceptions from external dependencies (for example, database access) immediately. For example, catch Java’s JDBCException rather than declaring that your domain method “throws JDBCException.”\n\n2. Never ignore or merely log an exception! Wrap the external exception in your team’s customized domain exception and throw that. Wrapping the original exception preserves its stack trace.\n\n3. Capture any relevant local state in your domain exception’s constructor parameters and add that data to the exception message.\n\n4. Be extra careful to avoid including sensitive user data in any log or error message. For example, no one except the user ever needs to see the user’s password, even if it’s mistyped.\n\n5. Automatically deliver each domain exception to the development or support team. Log it or email it to a well-monitored inbox. In at least one case, my team was able to log and email the stack trace and message within the domain exception’s own constructor, after calling the parent class constructor. If the log file or inbox becomes unmanageably large, the team needs to ask themselves why the system is throwing so many exceptions. After all, exceptions are meant to be exceptional.\n\n6. Catch the exception only when something can be done about it—for example, perform a database rollback or notify the user that the attempted action did not occur. Retry loops often create more problems than they solve.\n\n7. Allow the exception to bubble up the stack until it reaches the point where the app must alert the user. Generally, the audience for an exception message is the development, support, or ops teams, and not\n\nthe end user. Consider providing the user with the following information:\n\na. A message that what they attempted failed to happen\n\nb. Why it happened, if known (a high-level explanation, not a stack\n\ntrace)\n\nc. What the system is going to do next (e.g., report the error to\n\nsupport)\n\nd. Next steps to retry or resolve the issue\n\ne. A unique support ticket number should they choose to call support\n\nfor more help\n\nIn this way, each thrown exception will guide you through the specific code path that generated the error, and the exception’s message will reveal the data that played a role. Eventually, these exceptions will become rare and will either indicate an unstable external resource or a missed test scenario (also known as a defect or bug).\n\nA Test That Passes When the Code “Fails”\n\nContinuing the TrackerGroup example from Chapters 2 and 3, imagine that legitimate tracker IDs come from either a database or a barcode reader, but on rare occasions, a null value from a defective barcode reader makes it into the list.\n\nYou want the test to pass when the correct exception type is thrown. The test should fail if no exception is thrown or if the wrong exception type is .Net’s example, (for thrown NullReferenceException):\n\nJava’s NullPointerException\n\nor\n\n@Test void throwsOnNullID() { assertThrows(InvalidIDException.class, () -> new TrackerGroup(new String[]{ \"good ID before bad data\", null})); }",
      "page_number": 81
    },
    {
      "number": 4,
      "title": "Exceptional Behaviors",
      "start_page": 110,
      "end_page": 115,
      "detection_method": "regex_chapter_title",
      "content": "All modern assertion frameworks have some type of assertThrows(). The assertion takes a block of code (as a lambda function) and executes it. If that code throws the expected exception, the assertion passes; otherwise, the assertion fails.\n\nThe test will pass when you add the code to check for null:\n\npublic TrackerGroup(String[] trackerIDs) { this(freshStoreForIDs()); for (String candidate : trackerIDs) { if (candidate == null) { throw new InvalidIDException(candidate); } } Collections.addAll(this.trackerIDs, trackerIDs); }\n\nAfter a simple Extract Method refactoring, the code takes this form:\n\npublic TrackerGroup(String[] trackerIDs) {\n\nthis(freshStoreForIDs()); checkIDs(trackerIDs); Collections.addAll(this.trackerIDs, trackerIDs);\n\n}\n\nprivate void checkIDs(String[] trackerIDs) {\n\nfor (String candidate : trackerIDs) { if (candidate == null) { throw new InvalidIDException(candidate); } }\n\n}\n\nAlong with asserting that the correct exception type is thrown under the right circumstances, assertThrows() returns the thrown exception so the same test can assert something about the message. My preference, though, is to test the domain exception’s message-formatting behaviors separately.\n\nDesign Detour\n\nDomain exceptions tend to go together with some grouping of system behaviors. You will find dependencies easier to manage if those exceptions are contained within the same Java package or\n\nthe same .Net namespace and dynamic link library (DLL) as the parts of your system that throw them.\n\nMany teams I’ve coached tried to put all their domain exceptions into one Java package or .Net DLL. But because specific knowledge of other domain types (for example, TrackerGroup) is often useful, combining domain exceptions together “because they’re exceptions” will cause unnecessary coupling among most of your DLLs or JAR files.\n\nHere are two example tests for InvalidIDException. What we want from this exception is for the invalid value to appear in the message, clearly delineated with square brackets, and with quotes around string values (so we can distinguish the difference between null and \"null\"):\n\n@Test void messageFormatsNullReference() { InvalidIDException myException = new InvalidIDException(null); assertThat(myException.getMessage()).contains(\"[null]\");\n\n}\n\n@Test void messageContainsTheInvalidValue() {\n\nInvalidIDException myException = new InvalidIDException(\"bleep!\"); assertThat(myException.getMessage()).contains(\"[\\\"bleep!\\\"]\n\n}\n\nHere is the entire InvalidIDException class:\n\npackage crittermaps; public class InvalidIDException extends RuntimeException {\n\npublic InvalidIDException(String badID) { super(String.format(\"Invalid tracker ID value [%s]\", badID != null ? String.format(\"\\\"%s\\\"\", badID) : badID)); }\n\n}\n\nExternal Exceptions\n\nSimilarly, you can test what your code does with an exception thrown by an external dependency. Of course, to test that, you need to reliably cause the external dependency to throw the exception! Rather than trying something risky, slow, or expensive, replace the dependency with a “test double.” Chapter 6, “Test Doubles,” provides more details.\n\nSummary\n\nThe creation, throwing/raising, and handling of exceptions and errors are all significant behaviors deserving attention. Yet all too often, they are simply dismissed because they are not what we think of as “features.” Once we see that these behaviors are really not all that mysterious or special, they become easy to test and build:\n\nExceptions are not all that different from any other object, and can contain some simple behaviors.\n\nThrowing or “raising” will immediately break the code’s usual return path and—if not caught somewhere—will halt your software or at least the current transaction. That isn’t always a bad thing.\n\nCatching an exception is easy, but something besides merely logging the exception must happen. Otherwise, the software will continue on its normal path, pretending that nothing bad has happened. And that’s worse!\n\nWe now put the TrackerGroup example behind us. You now have all the “moves” of the TDD game.\n\nChapter 5, “Sustaining a Test-Driven Practice,” is packed with strategies for sustaining and winning the TDD “long game.” Keeping both the implementation and the test code flexible, changeable, and useful allows teams to extend and enhance their software indefinitely.\n\nOceanofPDF.com\n\nChapter 5. Sustaining a Test-Driven Practice\n\nIn this chapter, we cover challenges that arise over time. Those challenges include knowing where to start, testing code that is difficult to test, resolving “test smells,” and having to alter multiple tests. Expecting these issues and knowing ways to resolve them will give you the confidence to face them head-on. Also, learning that they are not nearly as bad as the situations that teams face when they do not maintain their safety net will give you the courage to resolve them even when others might ask you to take a shortcut.\n\nAt the beginning of a test-driven project, you might spend a seemingly disproportionate amount of time on test maintenance. For example, you might need to continuously reshape your object interfaces or change the number of objects involved in a behavior until the design feels right. Don’t be discouraged. As you progress, you will uncover your team’s design preferences, tolerance for various smells, and preferred ways to test your domain that will set you up for success in the future.\n\nTest-suite design and implementation design are quite different, but they also tests help shape a clean implementation. Conversely, whenever a particular behavior resists testing, you will likely need to first refactor the implementation, or the tests, or both.\n\nreflect each other: Well-written\n\nKey Lesson\n\nIf software is difficult to test, it’s not necessarily that testing is difficult, but rather that the current design of the code is resistant to testing. Often, refactoring is necessary before you can add new behaviors test-driven.\n\nOn longer-term test-driven projects, you may encounter a counterintuitive truth: The team spends more time maintaining the software’s safety net than maintaining the deliverable implementation. This is mostly because the test-driven team can refactor and add new behaviors with much greater confidence and speed. They don’t have to fearfully belabor over each new line of code.\n\nThe other reason for this time imbalance is a bit more subtle: The way system behaviors are performed is defined in the implementation, but those behaviors themselves are defined in the tests. A truly test-driven team is more focused on the “what” and less focused on the “how.”\n\nLet’s first examine what you’re aiming for, then consider the various challenges and how to overcome them.\n\nAttributes of a Good Unit Test\n\nUnit tests are part of your code and well deserving your attention. Test code differs from implementation code in purpose, design, smells, and refactorings, but it needs to be maintained with at least as much diligence as the implementation.\n\nThe sections that follow describe the attributes of good unit tests, which overlap and support each other.\n\nFast\n\nLonger test runs lead to wasteful context-switching. Stay alert for when your full test run exceeds 40 seconds. Anecdotally, that’s about as long as the typical developer will wait before getting bored and switching to other tasks.\n\nWith just a few unit tests, you’ll notice a bit of startup overhead with each run (up to 5 seconds on some IDEs). Once your team reaches tens of thousands of tests (it might take a year), you won’t notice the overhead anymore. If you notice the feedback loop rising toward 40 seconds, take steps to speed up slow tests.\n\nIn his book Working Effectively with Legacy Code (Pearson, 2004), Michael Feathers defines a “unit test” as one that runs without touching a database, file system, web API, or network. The short version of this definition is that the test runs entirely “in process”—that is, in memory.\n\nYou can achieve this by isolating slow external dependencies and replacing them with test doubles (see Chapter 6, “Test Doubles”). I’ve witnessed clients reduce their regression testing cycle from weeks or even months to mere minutes by substituting externals with test doubles.1\n\n1 This doesn’t mean they didn’t still do some manual testing. No amount of automated testing can replace Exploratory Testing by a talented professional tester. But if testing finds a defect, that broken behavior can be expressed and corrected through a much faster, repeatable test.\n\nClear\n\nA good test clearly describes the behavior being tested. Strive to make each test excruciatingly obvious regarding which behavior it is testing, and how it differs from its neighboring tests.\n\nDefinition\n\nExcruciatingly obvious: My term for code that is so clear that its purpose and behavior are unambiguous, even to someone new to the team. There’s no need to overthink this: If two developers understand the code, you’re headed in the right direction.\n\nAlthough unit tests are expressed in a programming language, they should also reflect the team’s ubiquitous business language and avoid unfamiliar technical jargon. You want both you and your teammates to be able to read a dozen or two tests rapidly and get the overall idea of which behaviors are described and tested.\n\nYou want each test to be readable in isolation, and you want a collection of tests—within a test class or describe clause, for example—to tell a coherent story about the described behaviors. Keep your audience in mind.\n\nChoose test data values that are simple and distinct. If necessary, assign each value to a local variable to name it.",
      "page_number": 110
    },
    {
      "number": 5,
      "title": "Sustaining a Test-Driven Practice",
      "start_page": 116,
      "end_page": 183,
      "detection_method": "regex_chapter_title",
      "content": "You want each test scenario to be brief but clear, so don’t skimp on keystrokes. Variable names such as “n” or “foo” do not improve readability.\n\nDon’t force yourself, or any future developer, to do math while reading the test, or to work out a complex Boolean conditional, or to guess at the meaning of a constant value (the Magic Number code smell).\n\nUnless a particular data value alters behavior, choose values that have no special meaning to the system. Instead of your CEO’s email address, use a example, fictitious “Zaphod.Beeblebrox@heartofgold.org.” If you need a date, perhaps choose a famous person’s birthdate:\n\ncharacter’s\n\naddress—for\n\nvar ladyGagasBirthday =\n\nnew SimpleDateFormat(\"dd MMM yyyy\").parse(\"28 Mar 1986\"\n\nValues that are unlikely to appear in your production environment will stand out clearly as test data much better than actual production data.\n\nConcise/Focused\n\nEach unit test should be as brief as reasonably possible and should test only one behavior. Nothing extra: No extra data, no extra calls, no unrelated assertions.\n\nA greater number of simple, precisely targeted tests offer you more information than fewer, broader (for example, end-to-end) tests. When a test fails, it should tell you precisely what is broken and under which scenario. If only one test is failing in an entire suite of concise tests, the tests that are passing tell you what is still working, which is also helpful.\n\nClear and concise tests also offer the team a detailed engineering specification. A developer can read through a group of related tests and glean the intended purpose of the code.\n\nTo check whether your tests are excruciatingly obvious, show them to someone without explaining the behaviors. Often when I’m delivering training, a developer will show me tests that were written earlier and try to explain what they are testing. I’ll usually ask them to stop. If the tests are not expressive, then explaining them while I’m reading them is distracting\n\nand potentially misleading. If they’re clear and concise tests, even someone who is unfamiliar with those tests can absorb the details and the business rules faster than anyone could explain them.\n\nLet your tests speak for themselves. If they fail to communicate intent, those tests need refactoring.\n\nIndependent\n\nA unit test must not rely on other tests to run first, or at all. Each test must be able to run individually or as part of the complete safety net.\n\nToday, separate tests run in parallel. In addition, some IDEs detect which tests need to be re-executed due to a code change and will run them without waiting for you to activate them.\n\nPut simply, a unit test should stand alone: All the state should be expressed within the test, or in a nearby BeforeEach block (or equivalent). Avoid fetching state from hidden or shared sources. Avoid the temptation to create a test database or a separate massive file of data for the unit tests. If someone reading your test suite must open another text file or manually execute a SQL query to understand what is being tested, then you don’t have independent unit tests. This can often be fixed by using test doubles for external dependencies (see Chapter 6, “Test Doubles”).\n\nRepeatable\n\nGiven that the underlying implementation being tested hasn’t changed, a test must give the same result every time, regardless of time of day, which other tests were run previously, speed of execution, or which system it’s running on.\n\nKeep the attributes just presented in mind as you investigate the testing challenges and test smells (code smells that pertain to test code) in the rest of this chapter.\n\nWhere to Start\n\nDevelopers naturally tend to think in tests, though they aren’t always aware they’re doing so.\n\nBefore you write a single programmatic statement, you already know why you need it. TDD turns those thoughts into concrete tests. You don’t think “I need a loop,” without first thinking “This needs to happen more than once.” You don’t think “I need a branch statement,” without first thinking “This only happens when that is true.”\n\nYou identify behavioral boundaries quite naturally. You think in tests.\n\nUnit tests are very concrete, low-level examples of how an object, method, or function is used. They are not product specifications. They are more akin to engineering specifications: You are describing and delineating the tolerances and responses of each part of the software’s behavior.\n\nWhen developers discuss behaviors with the business managers, these conversations are often at a level too high or abstract to begin thinking about behaviors at the “unit level.” Every team (not just TDD teams) must have a way to decompose requests before writing code. Fortunately, most people in the software industry—developers, but also product advocates, business analysts, and testers—will describe what the product should do, at some level, using examples.\n\nThere are excellent collaborative techniques beyond the scope of this book that can be used to effectively decompose requests to the point that they can be built in a test-driven manner. Two such techniques are Example Mapping and Behavior-Driven Development (BDD). BDD was discussed briefly in Chapter 1, “Thinking Test-Driven.”\n\nOnce the examples are sufficiently concrete and agreed upon, developers may need to break behaviors down further and distribute them across objects and functions. Where should the new behavior live? Which part of the whole system should you build first?\n\nFortunately, the inventor of TDD, Kent Beck, was already familiar with this issue. He and his frequent innovation partner, Ward Cunningham, gave us a\n\ntool to do just enough upfront design: CRC cards.\n\nCRC Cards\n\nOften teams know what they need to build, but they’re not sure which classes should own which behaviors. One lightweight and collaborative software design practice that can be applied in such a situation is CRC cards. CRC cards encourage teams to focus first on services and relationships, and to defer details such as algorithms and implementation until the team feels ready to build the implementation in a test-driven way.\n\nTeams often use physical index cards as CRC cards, with each card class–responsibilities– representing one collaborators. Those are the three areas of text included on each physical index card (Figure 5.1):\n\nclass. CRC\n\nstands\n\nfor\n\nClass: This is usually the name of the class in an object-oriented programming language. It can also be a concept, category, or abstraction until the team works through a few quick iterations of the technique and identifies a useful class name.\n\nResponsibilities: These are the behaviors of the class, which are usually limited to the services this class offers to other classes in the system. They can be stated as public method names. Ideally, though, you should favor descriptive sentence fragments or service names, to avoid overdesign prior to writing the first unit test for that behavior.\n\nCollaborators: These are the other classes and objects with which this class will need to interact to accomplish its responsibilities. These are usually limited to other objects within the business domain. In other words, avoid including primitive objects like Date.\n\nFigure 5.1 CRC card format: Each card has an area for the Class name, Responsibilities, and Collaborators.\n\nTool Tip\n\nPrior to 2020, the primary format for a team’s CRC cards session was to sit around a table with a stack of blank index cards, pencils, and erasers. Since 2020, many teams have continued to use CRC cards, but done so within collaborative online environments such as Miro or Mural. The position of the areas of text on a rectangular widget is less important than the clarity of each region’s purpose. In Miro, we’ve used rectangular “sticky notes” and placed the text in class–responsibilities–collaborators order, separating the three regions with a blank line or a few dashes.\n\nTo get started, developers get together with a subset of teammates (roughly 2 to 6 people). The teammates then work through a few scenarios from the request and experiment with which object should take on which responsibilities. Whenever someone identifies an interesting class, category, abstraction, or concept, the team makes a new CRC card to represent it.\n\nOne teammate volunteers to write down the class name and the new responsibility. If an existing card delegates to the new class or is its responsibility, the new class is recorded as a collaborator. This continues until the team has worked through at least one scenario.\n\nThe team may choose to run through a few other likely scenarios, with each developer playing the “role” of the cards in their possession. If something about the scenario feels awkward, the team can create new classes, break down and move responsibilities, and perhaps toss out cards that end up with no discernible responsibilities. This is why each session includes some kind of eraser.\n\nTeams should not try to complete one card before moving on to another. Instead, they should talk through each interaction scenario and identify what each class is responsible for within that interaction, and to which other classes it delegates other responsibilities (the collaborators). The person holding a particular card records each new responsibility or collaborator.\n\nAs an example, here are some tiny requests from one of my extended exercises. These come from a single-player, turn-based, text-based game of my youth, which I’ve renamed Super Nostalgic Trek to avoid litigation.\n\nRaise shield!: To protect key subsystems, the command “shield up” raises the energy shield, and “shield transfer n” moves n units of energy from ship reserves to the shield. Shield strength min 0, max 10,000.\n\nShield can be depleted: Enemy fire is absorbed by the shield (if raised) until the shield is depleted. Any remaining energy damages a random subsystem (e.g., a weapon, the engines, life-support, sensors, or even the shield generator itself).\n\nRest: So that I can recover from a brutal space battle, the command “REST 3” consumes 3 standard game “days” to repair every damaged subsystem by that much. (Note: Damage is measured in the number of standard days needed to effect repairs based on the vague estimates of Chief Engineer Monty McGuffin.)\n\nIn courses, I play the role of product advocate and share any missing context in conversations with teams. The sample results that follow contain knowledge gained from the written requests, from conversations with the product advocate, and from stepping through scenarios with each team member playing the role of one or more classes.\n\nHere are some scenarios that teams often come up with:\n\nWe’re attacked, and the shield withstands the attack.\n\nWe’re attacked, and the shield “buckles” because it is hit with more energy than it held.\n\nWe’re attacked, and the shield is down.\n\nWe rest, but only long enough to repair one of two damaged subsystems.\n\nWe rest needlessly, because no subsystems were damaged.\n\nFigure 5.2, Figure 5.3, and Figure 5.4 show what the resulting cards might look like.\n\nFigure 5.2 Sample CRC card for the game’s Shield class.\n\nFigure 5.3 Sample CRC card for the game’s Ship class.\n\nFigure 5.4 Sample CRC card for the game’s Subsystem abstraction.\n\nThe angle brackets < and > around “Subsystem” indicate that it is an abstraction. “Subsystem” is a term that groups a set of objects into a convenient category, based on similar behaviors. When implemented, Subsystem might be an abstract class, with Shield as one of its concrete subclasses, but that isn’t the only possible outcome.\n\nAvoid attempting to lock in the “perfect” design with your CRC cards. The sample cards in these images are one possible outcome, written by a specific team. As a counterexample, another team argued that the responsibilities given to the Ship class in the sample card don’t belong\n\nthere. Instead, that team had an abstraction called “Turn,” which represented enemy attacks and player commands.\n\nNotice that each responsibility has a check box. Once you’re done with a CRC card session, you will have numerous reasonable places to start using TDD. Each check box represents at least one behavioral boundary. If a collaborator’s class or responsibility hasn’t been written yet, you can stub out enough to get started or create test doubles for those incomplete responsibilities (see Chapter 6).\n\nTool Tip\n\nOn our XP teams, pairs of developers would volunteer to pick up each resulting CRC card, treating it as a task card and building it separately from the others. Because we worked very closely together, a question like “Hey, what are you calling the subsystem repair method?” would get an immediate answer, thus reducing merge conflicts during integration.\n\nCRC cards are neither code nor documentation. Each CRC session should start with a fresh set of blank cards. Avoid writing code fragments on your cards. Also, don’t preserve them or try to keep them in sync with the code.\n\nDon’t spend more time in a CRC card session than you need to get started with TDD. Use CRC cards to explore possible designs and interactions of just a few classes, by running through a few scenarios until your team feels ready to write unit tests based on those class responsibilities. At that point, go build your code and refactor it. If a concern arises, regroup and start another CRC card session to work things out, given the newfound feedback that only real, working code can provide.\n\nWhat to Test\n\nWhen building a new group of object behaviors, you will often have too many ideas and scenarios in your head, rather than too few. Give yourself the time to write them down on your short to-do list. Avoid writing multiple tests that don’t pass. That would require you to fully design the interface\n\n(the way in which others will call your object or function) without the useful feedback from your own working implementation.\n\nFor your first test of each behavior, make an educated guess as to which “side” of a behavioral boundary will be easiest to implement, so you can strengthen your safety net with little effort. “Default” behavior, if any, is usually a quick win. Handling “failure” scenarios is also typically much easier than implementing success scenarios. Consider this example from TrackerGroup: Testing that a tracker ID is not found is easier than implementing a search within TrackerGroup.\n\nTest Behaviors, Not Implementation\n\nBehaviors expects. caller—the Implementation is the code that makes this behavior happen. Therefore, the real description of a behavior lives in the tests as executable specifications, not in the code that implements it.\n\nare what\n\nthe\n\nclient—sees\n\nand\n\nImagine that you’ve been asked to build a new type of collection, called a Box. You write this test (in Ruby):\n\ndescribe 'box' do\n\nit 'knows what has been added' do box = Box.new box.add(\"red pen\") expect(box.has(\"red pen\")).to be_truthy end\n\nend\n\nNotice that you need to make two calls to box to test one behavior. You’re not just testing the code within add() and has(). You’re testing that what is added to a Box can be found within that Box.\n\nSimilarly, what causes you to write the next test is not that you still need to test add() or has() independently. Rather, now that you’ve pinned down one side of the behavioral boundary, you need to pin down the other side:\n\nit 'knows what it does not have' do box = Box.new box.add(\"red pen\")\n\nexpect(box.has(\"purple smartphone\")).to be_falsy end\n\nNotice that the implementation of Box is not shown. What type of data structure Box uses is an implementation detail. When you write tests first, you will often have an implementation in mind. That’s okay, but when you approach development with a test-driven mindset—when you focus on how the behavior will be used—you will either prove your assumptions or (often) uncover a simpler implementation.\n\nTest Behaviors, Not Data Structures\n\nChoose behaviors that will drive the design toward a particular data structure, rather than asserting that the data structure exists.\n\nThere is a lab in the Appendix called Salvo. In it, you are asked to build a software version of a very old pen-and-paper game of the same name.2\n\n2 A precursor of the familiar Milton-Bradley board game, Battleship.\n\nThe initial rules for Salvo read like a recipe of data structures. One rule tells you that each player has a 10 × 10 board. If you read that rule and immediately start writing a test, you might assume that you need a two- dimensional array to represent the 10 × 10 board. How would you test-drive that into your code? What behavior does a 10 × 10 board have?\n\nOften, developers will start with the following kind of test:\n\nrequire 'salvo' describe 'board but poorly' do\n\nit 'is 10x10' do board = Board.new expect(board.height).to be(10) expect(board.width).to be(10) end\n\nend\n\nThere are two problems with this approach. First, height and width will likely remain simple accessors (“getters”).3 Typically, there’s no behavior in the implementation. Second, this test establishes an interface that might never be useful. Will there ever be a call to height or width besides the\n\ncalls in the test itself? If not, then the test adds unnecessary complexity to Board’s interface. If height and width are eventually needed (perhaps while displaying the board), they can be added later using tests for more interesting behaviors.\n\n3 In Ruby, if you don’t have a parameter, you don’t need empty parentheses to call a method.\n\nInstead, identify and test behaviors that will drive the use of a data structure. How will the game or a player interact with a Board? How could you justify a 10 × 10 array through that interface? Always find something the software is truly being asked to do for the caller or user.\n\nFor example, in the Salvo game, ships need to get placed on boards. So, we could attempt to place a tiny single-cell ship onto a board. Using this little ship, you can test many behaviors of the board without any consideration for the size or orientation (vertical/horizontal) of the ship. Here are two things a developer might start with:\n\nrequire 'salvo'\n\ndescribe 'when placing ships on boards' do\n\nbefore(:each) do @board = Board.new @ship = Ship.new(size=1) end\n\nit 'will place a ship where asked' do @board.place(@ship, row=0, column=0) expect(@board.whats_at(row=0, column=0)).to eq(@ship) end\n\nit 'will NOT place a ship off the board' do expect{ @board.place(@ship, row= -1, column=0) }. to raise_error(\"Invalid board position!\") end\n\nend\n\nWe still need more tests to be sure the board knows about all four of its edges. What are some other important “off the board” examples? You want to assert each erroneous condition separately, without ambiguity. For example, location (42, –1) would be ambiguous because either the row or column value could trigger the error. Instead, we identify that the four sets\n\nof coordinates shown in Figure 5.5 hit the error-generating edge of each board boundary.\n\nFigure 5.5 Invalid Salvo ship-placement locations: (–1, 0), (0, 10), (10, 9), and (9, –1).\n\nDesign Detour\n\nDoes each new test need to be perfect and unchanging? Thankfully, no. Work iteratively until both your tests and your code are excruciatingly obvious.\n\nFor example, perhaps the “Invalid board position!” error message should list the offending coordinates. Would you add that now or later? At the very least, add it to your to-do list for later consideration.\n\nAnother example: The concept of ship orientation (vertical or\n\nhorizontal) could be added now or later, with an “orientation” parameter. If you think of it right away, add it to the interface and it won’t need to change later. The implementation could (should!) ignore the parameter until the Salvo tests include a larger ship.\n\nAlternatively, orientation would be easy to add later with an Add Parameter refactoring.4\n\n4 https://refactoring.guru/add-parameter\n\nLastly, should place() raise an error? In Chapter 4,\n\n“Exceptional Behaviors,” I recommended against raising errors to communicate invalid user input. So, does this example violate my recommendation?\n\nIn this case, imagine that this call to board.place() will be\n\ncalled after a check has been performed on the input values. More Salvo code appears later in this chapter, in the section, “Test Smells and Refactorings.” There, you’ll see why raising an error in board.place() could be appropriate.\n\nA test at (9, 9) would complete all edges of the board-size behavioral boundary. But would that be enough to justify adding a list or array?\n\nYou could get this far using the Fake It technique. Now consider these possible Triangulation tests to break the fake code:\n\nCheck that a successfully placed ship is not at a location where you didn’t place it.\n\nPlace two one-cell ships in two different locations and check that they are both where you placed them.\n\nWhat about testing other happy paths besides (0, 0) and (9,9)? Would you need to test position (5, 5)? Good question! Which leads to the next suggestion\n\nTest Behavioral Boundaries, Not Data\n\nIs there a need to create a unit test for location (5, 5), or for any other valid location other than (0, 0) and (9, 9)? Is there any behavioral difference between the tested locations and the other 98 valid board locations?\n\nRegarding the size of the Board, there is no behavioral difference in any of the other 98 valid board positions. Writing more unit tests that hit other\n\nlocations could clutter the engineering specification by having a cluster of tests that all test the same thing.\n\nWhenever this type of question arises, ask yourself whether you still need to (1) express some as-yet-unexpressed behavior or (2) prevent a reasonably intelligent but Jetlagged Intern from breaking something during a future refactoring attempt. If neither of those issues needs to be addressed, go on to your next to-do item. In Salvo, for example, the player can’t place two ships in the same spot on their board. That could certainly be your very next test, and it would justify using an appropriate data structure.\n\nTest Only What Is Known\n\nOn any software project where you are building something new, using new tools, or in a new environment, your job as a software developer will require extensive research. Unfortunately, you cannot determine all the things you will need to learn before you start coding. At the same time, you don’t want to have to go back and redesign your whole implementation.\n\nThe sections that follow describe ways to mitigate the risk associated with these unknowns.\n\nIsolate and Defer the Unknown\n\nWard Cunningham5 once said, “Code what you know.” The flip side of this statement is to avoid coding what you don’t yet know how to implement.\n\n5 https://en.wikipedia.org/wiki/Ward_Cunningham\n\nKeep the known and the unknown separated in the code. Objects and functions usually divide easily along the known/unknown boundary. If you guess incorrectly, a little refactoring will move behaviors into their proper homes.\n\nResearch the Unknown\n\nOften, the unknown is external to your system: hiding behind a third-party API, requiring a deeper discussion with the product advocate, or requiring the right search keywords to find a good example. In my experience, the\n\nsolution to the unknown is likely to be far simpler and narrower in scope than you had initially anticipated.\n\nAdd a research item to your to-do list. Ask the product advocate, an architect, or another teammate who is familiar with a new technology.\n\nAvoid collecting too many of these unknowns or deferring them for too long.\n\nExperiment with the Unknown\n\nPerhaps you will need to run a coding experiment. You can create a separate IDE project—that is, not part of your product or its tests—to perform these experiments. You can often use the unit-testing framework as your laboratory. “When I give this library these values, do I get back what I need?” When you have answers, apply what you’ve learned to the real product.\n\nZombies\n\nAnother lightweight approach to choosing what to test next is James Grenning’s “ZOMBIES.”6 ZOMBIES may be one of the best mnemonic acronyms in the software industry: It’s easy to remember, and all the letters are helpful. It is a reminder of very common test-writing heuristics that help you to stay on track and to unit-test all aspects of a behavior.\n\n6 https://blog.wingman-sw.com/tdd-guided-by-zombies\n\nAn old joke says there are only three numbers in computing: zero, one, and many. Unsurprisingly, those are the first three characters of ZOMBIES. Taken as a set, ZOM reminds us to start simple and add complexity incrementally.\n\nZero\n\nThe “zero” case might be a literal zero, or something that is empty, or something that does nothing by default. Or it could be any other initial state.\n\nThe “zero” case is often the quickest and easiest to build, and a great place to start. Even when you Fake It, you’re making progress on designing the interface. Plus, each passing test—no matter how trivial it might seem at first—“locks in” your winnings, protecting those basic behaviors while you tackle the more challenging scenarios. Grenning refers to this as “skillful procrastination.”\n\nHere’s an example: Imagine you’ve been tasked with building a library of various text filters. Perhaps one of these filters deletes suspicious scripting, another corrects your spelling, a third strips out all HTML tags, and so on. The “Zero” of ZOMBIES might prompt you to create a “pass-through” filter that lets everything through, or perhaps a “bit-bucket” type that filters everything, effectively throwing away all the text. Both would be easy to test and write.\n\nDesign Detour\n\nNull pointer/reference exceptions are evil!\n\nYou can avoid returning null (Ruby’s nil) and usually avoid checking for null values within your business domain. You will still need to check for null values returned from an external dependency or passed to you by an external caller. But within your own coding realm, every null return and every subsequent null check is avoidable.\n\nThe text filter example’s “pass-through” and “bit-bucket” filters could be “Null Objects.7” A Null Object is a concrete implementation of an abstraction (an abstract parent class or a Ruby module) that provides “default” behaviors—typically, doing nothing or returning a simple response to any request.\n\n7 https://en.wikipedia.org/wiki/Null_object_pattern\n\nReturning a Null Object is a better alternative than returning a\n\nnull pointer or reference. A method that responds with a null value pushes the responsibility to check for null onto the caller. If the caller instead tries to use that reference, a nasty null-reference exception will be thrown. Evil!\n\nOne\n\nNext is the case when there is one of something: one behavior, one object instance, one concrete class. Returning to the text filters example, that means it’s time to create a filter that does just one thing.\n\nMany (or More Complex)\n\nHere, your domain becomes more diverse and more useful. For example, perhaps you’ve already built a bit-bucket (which filters everything) and a pass-through (which filters nothing). Now, you will build something more complex. Perhaps that will be a filter that removes all HTML tags, highlights search keywords, compresses the text, or encrypts it. Or maybe it will be a filter that can combine any number of other filters.\n\nBoundaries and Behaviors\n\nAt this point, Grenning’s acronym ceases to be a stepwise succession and adds overarching reminders to the TDD thought process.\n\nSoftware behaviors exist because of boundary conditions and “edge cases.” The “B” in ZOMBIES reminds us to be vigilant about that fact, and not to neglect the other side of each behavioral boundary.\n\nInterface\n\nThe “I” component in ZOMBIES reminds us to shape our code from the interface inward. Each test is a client of the code, and each client is asking for something new. Design the interface (the service you’re providing) to suit the caller, not the implementation.\n\nExceptional Conditions\n\nThe “other side” of a behavioral boundary might be how your code deals with a programmatic error on the part of the caller, which requires that an exception be raised. Throwing that exception is the expected behavior in that scenario and should be tested.\n\nMixing the Interface concern with Exceptional conditions reminds us to give our exceptions meaningful names and to provide error messages that help other programmers correct the mistakes in their code.\n\nThe “E” in ZOMBIES isn’t only about throwing exceptions. Together with Boundaries, it reminds us to test our code’s responses to other exceptional conditions. For example, if your user’s full legal name is “III,” does your code handle this exceptional case gracefully?8\n\n8 An homage to the late great consultant and friend, III.\n\nSimple Scenarios and Simple Solutions\n\nSimple is never easy, so the “S” in ZOMBIES reminds us to take baby steps toward our solution. Each new test and each added behavior, when kept small and discrete, keeps both the code and the tests maintainable by us, the fallible human software developers.\n\nWhen Code Is Difficult to Test\n\nYou might find yourself wanting to write a test that is big or difficult to set up, or turns out awkward, complex, and difficult to understand. This dilemma is often a result of previous steps that were too large.\n\nKey Lesson\n\nThis key lesson from the beginning of this chapter deserves repeating: When code is difficult to test, it’s not because testing is difficult.\n\nInstead, your implementation is pushing back, telling you that\n\nyou have an opportunity to improve the design. Before writing that next test, step back and refactor the existing code or run a quick CRC card session.\n\nAny bit of isolated behavior does one of three things:\n\n1. Answers a query by returning a value (calculated or otherwise)\n\n2. Alters the state of the object\n\n3. Delegates to another object or function\n\nThe first behavior is the easiest to test: When a method or function like ftoc(int fahrenheit) returns a value, you can easily assert that the return value is what you expect. But because this type of behavior is so easy to test, developers may add return values to methods that weren’t meant to return anything.\n\nThe second type of behavior, changing state, can be somewhat more difficult. The state-change must have a purpose, so the test changes the state and expects that purpose to be fulfilled. Otherwise, it isn’t a fully realized behavior.\n\nThe third behavior is perhaps the trickiest to test. When our object delegates to another object, questions inevitably arise: Where did this other object come from? How will we know it was called correctly? Is that object fully tested?\n\nWhenever you feel even the slightest push-back from your code, take a step back (if you’re in green). Ask, “Why is this code being so stubbornly untestable?”\n\nBefore writing the next test, you may need to refactor a little. So, follow your nose: Find the code smells that are preventing you from testing, and clean those up.\n\nTest Smells and Refactorings\n\nYou know, Homer, it’s very easy to criticize.\n\n—Marge Simpson\n\nFun, too!\n\n—Homer Simpson\n\nThe Simpsons, season 9, episode 14, “Bart Star”\n\ntests slow down development? As with your Will maintaining implementation, if you make small changes and run them after each refactoring, your test-driven intuition and skills will continually improve, and maintenance of the tests will get easier. The process of running into a challenge and overcoming it will help you avoid that same challenge in the future.\n\nTeams that have been doing TDD for a while often report that they spend more time and creativity on their test suite than on their implementation— yet they do not see this as a problem. When a team takes the time to keep its specifications clear, simple, and passing, that well-crafted safety net will pay them back by making defect repairs and feature enhancements go much faster and more smoothly.\n\nOn a TDD project, the ratio of test code to implementation code is significant. My XP teams have generally had two to three times more lines of test code than implementation code. Again, this was not seen as a bad thing, for two reasons:\n\nThrough diligent, continuous refactoring, all duplication was wrung out of the code, leaving behind a smaller, clearer, and more maintainable implementation.\n\nGood test design and good object design differ: Good tests are “scripty” and self-contained. We want unit tests that are short, specific, and step-by-step scenarios, exercising only one isolated behavior. Contrast that to good object-oriented design, where objects often delegate anything outside of their primary purpose to another object, without being coupled to how that next object will accomplish its primary function.\n\nSo, good tests and good objects look very different, and their code smells are different, too. To clean up or avoid testing challenges, we need to be sensitive to test smells.\n\nWhat follows are some common test smells and tips on how to alleviate them. Many of these smells are similar, and there is considerable overlap among them. Read the section on each one independently and you’ll more readily notice the subtle differences.\n\nDesign Detour\n\nSoftware design and test design influence each other.\n\nWhen you write the test first, you design a part of the public\n\ninterface to your implementation. When you refactor the implementation, you often make it easier to test.\n\nThese two design considerations work together. For example, if you break out a small object from a big one, you can also factor out parts of the existing tests to test the extracted behaviors directly on the new object. Both the implementation and the tests become simpler and clearer.\n\nThis process can even reduce the number of tests for\n\nresponsibilities that have become unnecessarily entangled. When you decouple behaviors, you often reduce the number of distinct permutations that need testing.\n\nDependencies that create permutations can be decoupled using\n\none of the many forms of test doubles (see Chapter 6, “Test Doubles”). The result is low-level unit tests for all discrete behaviors, including interactions and delegations across objects. Using unit tests, 100 percent of the behavior can be covered with fewer tests.\n\nWe’ll be using an informal taxonomy for the condition of the code samples that follow. “Smelly” means that something can be cleaned up to make the tests clearer. There are degrees of smelly, which are part opinion and part experience (for example, “mildly smelly” and “very smelly”). “Better” tends to imply that the code is now less smelly, but there may be further alterations that could get it to “good.” “Alternative” is used if there are multiple ways to clean up the smell depending on the real intent of the test.\n\nSmell: Unhelpful Test Names\n\nHow you name or describe each test and each test class is important. It helps make the suite of tests serve as a good “engineering specification,” and that organization provides useful information whenever a test fails.\n\nWhen the test first “fails” as part of the TDD cycle, read the failure report. Does it describe the failing behavior sufficiently to explain what isn’t\n\nworking and how that behavior fits into the overall system? That might sound like a lot, but if the test fails in the future, that description is just the starting point. The developers who read the failure report will begin thinking about the problem using the context provided by the failure report.\n\nSmelly (in Ruby’s rSpec):\n\nrequire 'salvo'\n\ndescribe \"board tests\" do\n\nit \"place fails\" do # ... scenario code goes here end it \"place succeeds\" do # ... scenario code goes here end\n\nend\n\nSmelly (in Java’s JUnit):\n\npackage salvo;\n\npublic class BoardTests {\n\n@Test void testPlaceFails() { // ... scenario code goes here }\n\n@Test void testPlaceSucceeds() { // ... scenario code goes here }\n\n}\n\nIn the first versions of JUnit, you had to name each test starting with the word “test.” Thankfully, that hasn’t been true for a long time. Even so, you should avoid using the same term in so many places that it becomes mere noise to the reader. What’s important is what unique scenario is being tested.\n\nIn JUnit and MSTest, you have a lot of flexibility when choosing test names, and you have a lot of characters available. In Jasmine, Mocha, and rSpec, you have a description string where you can write whole sentences\n\nwithout using camel-case. Either way, you need to type your descriptive test name only once for each test—so make it a good one.\n\nYour team may want to experiment with different test-naming conventions. Recall that you do not have to limit yourself to one test class (or describe clause) per implementation class.\n\nInclude in each test name what makes it unique, even if that will be obvious in the body of the test. How does this test differ from its neighbors? Which side of a behavioral boundary does this test exercise?\n\nHere’s an example from the Salvo game (see the Appendix at the end of this book), written in Ruby’s rSpec:\n\nGood:\n\nrequire 'salvo'\n\ndescribe 'placing ships on the board' do it 'fails when two ships overlap' do # ... scenario code goes here end it 'works when there are empty cells around each ship' do # ... scenario code goes here end\n\nend\n\nIn Java’s JUnit, you might use the class name to describe the overall behavior, and the test-method names to describe each unique scenario.\n\nGood:\n\npackage salvo;\n\npublic class PlacingShipsOnTheBoard {\n\n@Test void fails_WhenTwoShipsOverlap() { // ... scenario code goes here }\n\n@Test void works_WhenThereAreEmptyCellsAroundEachShip() { // ... scenario code goes here\n\n}\n\n}\n\nSmell: Unhelpful Instance Names\n\nName your object instances with variable names that add clarity to the test. You might not come up with the right names when you first write the test. Once the test is passing, be willing to read over your tests and see if a few simple Rename refactorings will enhance the clarity.\n\nOften, developers will start out with something that doesn’t seem to be a problem. For example:\n\n@ship = Ship.new(size=3, Orientation::VERTICAL)\n\nThis is fine until the number of items lacking intention-revealing names increases as you write more tests.\n\nSmelly:\n\n@ship1 = Ship.new(size=3, Orientation::VERTICAL) @ship2 = Ship.new(size=4, Orientation::HORIZONTAL)\n\nWhenever there’s a clear distinction to be made between objects, think of a name that will make the tests clearer—for example, boardToBeTested, theOtherBoard, emptyBoard, crowdedBoard, tinyShip, and biggestShip.\n\nGood:\n\n@vertical_ship = Ship.new(size=3, Orientation::VERTICAL) @horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL\n\nSmell: Copy/Paste/Modify\n\nTests that triangulate related behaviors will look very similar, but each one should be unique in some significant and obvious way.\n\nDevelopers often like to copy and paste an existing test, then modify it to match the next triangulating scenario. After a while, the resulting tests start to grow larger and more complex than necessary.\n\nUse your IDE’s hotkeys and autocomplete features until they become second nature. Frequently reread the handful of tests you’ve written most recently, then refactor away duplication and remove unnecessary given information. When you truly embrace test-driven thinking, you’ll find yourself using copy/paste/modify far less, instead favoring writing each test “from scratch.”\n\nSmell: Duplicate Given\n\nWhen building something new, you will often end up with a handful of tests that use the same lines of setup (given) code in each test.\n\nSmelly (in Ruby’s rSpec):\n\ndescribe 'placing ships on the board' do it 'fails when two ships overlap' do board = Board.new vertical_ship = Ship.new(size=3, Orientation::VERTICAL) horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL) # ... scenario code goes here end it 'works when there are empty cells around each ship' do board = Board.new vertical_ship = Ship.new(size=3, Orientation::VERTICAL) horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL) # ... scenario code goes here end\n\nend\n\nSmelly (in Java’s JUnit):\n\npublic class PlacingShipsOnTheBoard {\n\n@Test void fails_WhenTwoShipsOverlap() { Board board = new Board(); Ship verticalShip = new Ship(3, Orientation.VERTICAL); Ship horizontalShip = new Ship(4, Orientation.HORIZONTA // ... scenario code goes here }\n\n@Test void works_WhenThereAreEmptyCellsAroundEachShip() { Board board = new Board(); Ship verticalShip = new Ship(3, Orientation.VERTICAL);\n\nShip horizontalShip = new Ship(4, Orientation.HORIZONTA // ... scenario code goes here }\n\n}\n\nEvery modern unit-testing framework has a “Before Each” mechanism for consolidating repeated given lines (henceforth referred to as BeforeEach). MSTest uses @Before, Jasmine uses beforeEach, rSpec uses before(:each), and JUnit uses @BeforeEach. A BeforeEach runs before each test within that test class or describe clause.\n\nFirst, find all duplicated given objects and assign them to fields with descriptive names. Then, move all duplicated setup into the BeforeEach block.\n\nBe sure you declare each object as a field in the test class and not as a method-scope local variable. Otherwise, the initialized objects will drop out of scope once the BeforeEach exits. Particularly when you are using JUnit or MSTest, be careful not to move both the declaration and the instantiation into @BeforeEach; otherwise, the local variables will shadow fields with the same name.\n\nGood (in Ruby’s rSpec):\n\ndescribe 'placing ships on the board' do\n\nbefore(:each) do @board = Board.new @vertical_ship = Ship.new(size=3, Orientation::VERTICAL) @horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL end it 'fails when two ships overlap' do # ... scenario code goes here end it 'works when there are empty cells around each ship' do # ... scenario code goes here end\n\nend\n\nGood (in Java’s JUnit):\n\npublic class PlacingShipsOnTheBoard {\n\nprivate Board board;\n\nprivate Ship verticalShip; private Ship horizontalShip;\n\n@BeforeEach void setupBoardAndShips() { board = new Board(); verticalShip = new Ship(3, Orientation.VERTICAL); horizontalShip = new Ship(4, Orientation.HORIZONTAL); }\n\n@Test void fails_WhenTwoShipsOverlap() { // ... scenario code goes here }\n\n@Test void works_WhenThereAreEmptyCellsAroundEachShip() { // ... scenario code goes here }\n\n}\n\nIt’s best to wait to use the BeforeEach mechanism until you truly have some duplicated code. Make sure that what you move into this area is useful to most tests in the test class/describe clause. If some tests ignore some of the BeforeEach, that’s a mild test smell. However, if a test needs to overwrite or undo part of that BeforeEach, then you have the Rejected Given test smell (discussed later in this chapter).\n\nRemove duplication as it occurs. Don’t wait until it grows out of control. If you follow the “rule of two”—cleaning up duplication whenever you have two chunks of test code that look very similar—the next test you think of will be that much easier to write.\n\nReducing duplication also helps whenever the tests need refactoring. Often, changes can be made to the BeforeEach rather than to every test. For example, if you add a parameter to the tested object’s constructor, it will likely be initialized in the BeforeEach.\n\nYou might think it paradoxical that you want self-contained and readable tests, yet you are moving duplicated code out of those tests. Optimize for comprehension of a related group of tests; that is, keep the given code close to the tests that use it. If you must scroll up a few screen lengths to see it,\n\nthat’s probably fine. If you have hundreds of tests in the same file, consider splitting them up. See the discussion of the Rejected Given test smell later in this chapter for further guidance.\n\nSmell: Excessive Given\n\nThis smell arises when a test has a lot of things happening in the given section—either appearing in the test code or loaded from a test database. If the given contains the assembly of a complex object graph, or if the tested behavior needs to make calls to more than a few ancillary objects, you are likely trying to test too much in a single unit test.\n\nSmelly:\n\nit 'does everything, everywhere, all at once' do\n\nboard1 = Board.new player1 = Player.new(board1) board2 = Board.new player2 = Player.new(board2)\n\nsub1 = Ship.new(size=1) destroyer1 = Ship.new(size=3) battleship1 = Ship.new(size=4)\n\nsub2 = Ship.new(size=1) destroyer2 = Ship.new(size=3) battleship2 = Ship.new(size=4)\n\nboard1.place(sub1, row=0, column=0, Orientation::HORIZONTAL board1.place(destroyer1, row=0, column=2, Orientation::HORI board1.place(battleship1, row=0, column=9, Orientation::VER\n\n# ...and so on... end\n\nThis test smell provides an example of why, whenever it’s difficult to test a behavior, there’s likely a need to improve the design of the implementation.\n\nIf this smell emerges before there’s much implementation, first run through a CRC card session and identify each object’s isolated behaviors. Build those units of behavior first, and then see whether you still need to test some aggregate behavior.\n\nIf this smell pops up later in development, and many of these objects have significant the implementation. When numerous objects are needed to play a role in a unit- testing scenario, there is often some unintended coupling across some of the objects. Generally, look for long blocks (for example, in Java or C#, more than a dozen lines of code between curly braces). Often script-like, “procedural” code is used to “manage” an interaction. There is likely a place in your domain for such transactional scripts, but that behavior might not belong in any of the stateful objects you’ve created within your given part.\n\nimplementation, first\n\nlook for code smells within\n\nAs always, first get back to green, then follow your nose and refactor away those smells. After that, you can approach new behaviors using objects that have fewer and simpler responsibilities.\n\nIf it turns out that the scenario you’re testing truly requires several objects (e.g., a list of ships), consider using a “Builder”9 to assemble the required object graph and make the given more readable.\n\n9 https://en.wikipedia.org/wiki/Builder_pattern\n\nBetter:\n\nit 'does everything, everywhere, all at once' do\n\nplayer1 = PlayerBuilder.new .addShip(size=1, 0, 0, Orientation::HORIZONTAL .addShip(size=3, 0, 2, Orientation::HORIZONTAL .addShip(size=4, 0, 9, Orientation::VERTICAL) .build\n\nplayer2 = PlayerBuilder.new .addShip(size=1, 3, 8, Orientation::HORIZONTAL .addShip(size=3, 1, 1, Orientation::VERTICAL) .addShip(size=4, 5, 9, Orientation::VERTICAL) .build\n\n# ...and so on...\n\nend\n\nSmell: Rejected Given\n\nOften teams will pile a lot of given code into a single BeforeEach method, resulting in setup code that isn’t used in many tests, or—worse—needs to be undone or replaced in some tests.\n\nSmelly (a simplified example):\n\ndescribe 'board status' do\n\nbefore(:each) do @board = Board.new @board.place( Ship.new(size=2), Orientation::HORIZONTAL, row=0, column=0) end\n\nit 'reports all NOT sunk when one ship afloat' do expect(@board.all_sunk?).to be false end\n\nit 'reports all sunk when no ships afloat' do @board.attacked_at(0, 0) @board.attacked_at(0, 1) expect(@board.all_sunk?).to be true end\n\nit 'reports all sunk when empty even though this cannot happe @board = Board.new expect(@board.all_sunk?).to be true end\n\nend\n\nNote that the last spec is resetting @Board to test an unusual (impossible?10) case. The refactoring needed is to divide the tests into separate test classes or describe clauses, grouping them by what is common in the given.\n\n10 This “impossible” scenario should probably assert that a programmatic error is raised, but I needed the example to be as simple as possible.\n\nSmell: Forced Return Value\n\nA trap that developers often fall into when starting out with TDD is trying to force one method to do everything necessary, including returning an\n\notherwise meaningless status value.\n\nWhile it’s true that a test without an assertion (the then part) is worse than no test at all, we want assertions that make sense within the test scenario. Recall the three things that software can do: return a value, change state, or delegate. This smell happens when a developer tries to avoid testing the latter two possibilities.\n\nSmelly:\n\nit 'will place a ship where asked - with FORCED RETURN VALUE' d expect(@board.place(@ship, row=0, column=0) ).to eq(Board::SU\n\nend\n\nit 'will NOT place a ship off the board - with FORCED RETURN VA\n\nexpect(@board.place(@ship, row= -1, column=0) ).to eq(Board::FAILURE_INVALID_POSITION)\n\nend\n\nGiving place() a return value gives it two important jobs: placing the ship, but first determining whether it can be placed there.\n\nAlthough the place() method could do both,11 the trouble is that these return values force the client to deal with an error after it has occurred. Also, when a method’s name doesn’t describe what it’s returning, the calling code will also be less clear.\n\n11 There is a lot of precedent for doing this: Most UNIX system calls return a value indicating success or failure.\n\nOne short-term “fix” is to rename the method based on what it does and what it returns.\n\nBetter:\n\ndef place_ship_and_return_success_or_failure( ship, starting_row, starting_column) # significant logic removed for brevity... SUCCESS end\n\nThis Rename Method refactoring doesn’t solve the problem, but it does make the smell more obvious. By adding _and_ to a method name, you send\n\na very clear signal to your team that a method is doing more than one thing. If you cannot immediately come up with a way to resolve this smell, at least the smell will be easier to detect in the future.\n\nHow do you resolve this test smell for good? Split up the behaviors. What follows is one example of how to split up this Board behavior into two calls. One call is a query; the other is a request.\n\nGood:\n\ndescribe 'placing ships on the board' do\n\nbefore(:each) do @board = Board.new @ship = Ship.new(size=1) end\n\nit 'knows if tiny ship can be placed at empty square' do expect(@board.can_place_ship_at( @ship, row=0, column=0)).to be_truthy end\n\nit 'knows that tiny ship CANNOT be placed off the board' do expect(@board.can_place_ship_at( @ship, row= -1, column=0)).to be_falsy end\n\nit 'will place a ship where asked' do @board.place(@ship, row=0, column=0) expect(@board.whats_at(row=0, column=0)).to eq(@ship) end\n\nit 'will NOT place a ship off the board' do expect{ @board.place(@ship, row= -1, column=0) }. to raise_error(\"Invalid board position!\") end\n\nend\n\nThe client implementation will need to call the query first instead of checking for errors after the call. For example:\n\nif player_board.can_place_ship_at(player_ship,\n\nselected_row, selected_column)\n\nplayer_board.place(player_ship, selected_row, selected_column\n\nelse\n\nnotify_player(\"Sorry! You cannot place your\" + \" #{player_ship.ship_type}\" + \" at #{selected_row},#{selected_column}\")\n\nend\n\nThe complexity of responding to error codes has been avoided by using the lesser complexity of calling board twice. This code can also be written using a test-driven approach: Either the ship is on the board in the location requested, or the player is notified of their invalid selection.\n\nNote that the “Invalid board position!” error (or exception) would not be raised unless the logic has a defect. In other words, it is exceptional: It should never happen, and if it does, it’s because a developer has made a coding or refactoring mistake.\n\nThe implementation of place() may also call can_place_ship_at() in a guard clause to avoid attempting the impossible:\n\ndef place(ship, starting_row, starting_column)\n\nunless can_place_ship_at(ship, starting_row, starting_column) raise(\"Invalid board position!\") end\n\n# ...logic placing the ship on the board goes here...\n\nend\n\nDefinition\n\nGuard clause: A conditional statement that, when true, exits the method immediately. A guard clause stops the typical flow of the method, allowing the rest of the method to be written without additional indentation.\n\nDesign Detour\n\nThe example implementation of place() that calls can_place_ship_at() might bother some developers. Why invoke can_place_ship_at() twice for the same user action?\n\nThe concern is often one related to speed or efficiency. If can_place_ship_at() were expensive (for example, using a lot of\n\ntime or CPU cycles), then certainly limiting it to one call would be of benefit. Otherwise, computational efficiency is not a priority. Our software exists to make at least one human’s life fractionally more pleasant, often by helping make a menial or repetitive task go faster and be less prone to human error. Our product’s source code exists primarily to make maintaining and extending that software easier for the development team. There is seldom a need to make the computer’s task faster unless it helps with those two goals.\n\nThe two calls to can_place_ship_at() serve different purposes\n\nand result in different actions. On the one hand, when the caller checks and then transacts, the check fails if the player entered a choice that isn’t allowed by the rules of the game. At that point, the caller will—presumably—be asked to enter different coordinates. There is no programmatic error. If, on the other hand, the “Invalid board position!” error somehow gets raised, that event would indicate a true programmatic error.\n\nIn the case of a stand-alone app, a programmatic error should cause the entire application to halt. In the case of a transactional web application, it should roll back or avoid committing the latest transaction. In a few mission-critical web applications I worked on, an exception’s stack trace (and other relevant local state) would be sent automatically to the development team, and an on- call developer would be immediately notified.\n\nSmell: Coupled Assertions\n\nYou might find yourself using the same sequence of assertions repeatedly, either in different tests or in the same test. This smell suggests a relationship among the assertions in the pattern. It tells you that some form of conceptual coupling is prompting you to group the assertions.\n\nSmelly:\n\ndescribe 'ship status' do\n\nbefore(:each) do @board = Board.new @sub = Ship.new(size=1) @destroyer = Ship.new(size=3)\n\n@battleship = Ship.new(size=4) @board.place(@sub, Orientation::HORIZONTAL, row=0, column=0 @board.place(@destroyer, Orientation::HORIZONTAL, row=0, co @board.place(@battleship, Orientation::VERTICAL, row=0, col end\n\nit 'reports undamaged if attack misses all ships' do @board.attacked_at(5, 4) expect(@battleship.damaged?).to be false expect(@battleship.sunk?).to be false\n\nexpect(@destroyer.damaged?).to be false expect(@destroyer.sunk?).to be false\n\nexpect(@sub.damaged?).to be false expect(@sub.sunk?).to be false end\n\nThis example repeatedly asserts against both the damaged? and sunk? methods. Thus, the notions of damaged and sunk are apparently related.\n\nAn aside: This example shows repetition in a single test, mostly to make the Coupled Assertions smell more obvious. You might also get a whiff of other test smells in this sample code. For example, assertions upon unaffected objects could be removed, leaving behind only those assertions for the object that changed (or that will change in the adjacent Triangulation test— for example, when the attack hits the @sub).\n\nThe refactoring depends on the reason for the relationship. If there’s a legitimate reason to check those same assertions in similar scenarios, then you could extract your own custom assertion.\n\nAlternative:\n\nit 'reports undamaged if attack misses all ships' do @board.attacked_at(5, 4) expect_ship_to_be_healthy(@battleship) expect_ship_to_be_healthy(@destroyer) expect_ship_to_be_healthy(@sub) end\n\ndef expect_ship_to_be_healthy(ship) expect(ship.damaged?).to be false\n\nexpect(ship.sunk?).to be false end\n\nYou can keep this custom assertion method local. Alternatively, if it’s useful in many test files (or describe clauses), then you could add it to your test framework.\n\nCoupled Assertions often indicate related behaviors that might be useful in the implementation. For example, perhaps one of Ship’s clients needs to know if a ship is both undamaged and still afloat:\n\nclass Ship\n\ndef undamaged? not ( damaged? or sunk? ) end # ...\n\nend\n\nIf so, then the tests could be altered to check that combined result.\n\nAlternative:\n\nit 'reports undamaged if attack misses all ships' do @board.attacked_at(5, 4) expect(@battleship.undamaged?).to be true expect(@destroyer.undamaged?).to be true expect(@sub.undamaged?).to be true end\n\nMost of the time, each assertion belongs in its own test. When you see two very different assertions that appear together, they are likely testing two different behaviors in the same test. If so, divide them into separate tests.\n\nGood:\n\nit 'reports undamaged if attack misses' do @board.attacked_at(5, 4) expect(@sub.damaged?).to be false end\n\nit 'reports not sunk if attack misses' do @board.attacked_at(5, 4) expect(@sub.sunk?).to be false end\n\nDesign Detour\n\nIf you use a testing framework that allows for nested describe clauses, consider consolidating a duplicated when step into a BeforeEach, leaving only the assertions in the tests. In our simple Salvo example, this technique might look absurd, but anytime you need to assert against two very different outcomes, this refactoring could help clarify those differences.\n\ndescribe 'ship status' do\n\nbefore(:each) do @board = Board.new @sub = Ship.new(size=1) @board.place(@sub, Orientation::HORIZONTAL, row=0, co end\n\ndescribe 'when attack misses' do before(:each) do @board.attacked_at(5, 4) end\n\nit 'reports undamaged' do expect(@sub.damaged?).to be false end\n\nit 'reports not sunk' do expect(@sub.sunk?).to be false end end\n\nend\n\nThe rSpec- and Jasmine-style frameworks are well designed to\n\ncarry out this particular refactoring, because the BeforeEach that includes the tested behavior—the when of the test; in this case, @board.attacked_at(5, 4)—is very close to the two tests that use it. If there’s too much unrelated test code between the when and the then, the tests will be much less clear.\n\nTalk with your teammates before using this refactoring.\n\nDevelopers either love it or hate it, and it’s not always as readable in other test frameworks.\n\nSmell: Extended Narrative\n\nYou might be tempted to write a test that tests more than one scenario. Perhaps one scenario logically follows the other, or two scenarios share the same given setup. You test a little behavior, and—because you’re already set up for it—you test a little more.\n\nThe resulting tests typically follow a given–when–then–when–then or given–when–then–given–when–then pattern.\n\nSmelly:\n\nit 'reports sunk if all cells have been hit' do\n\ndestroyer = Ship.new(size=2) @board.place(destroyer, Orientation::HORIZONTAL, row=0, colum\n\n@board.attacked_at(0, 0) expect(destroyer.damaged?).to be true expect(destroyer.sunk?).to be false\n\n@board.attacked_at(0, 1) expect(destroyer.sunk?).to be true\n\nend\n\nThis is problematic because if one of the earlier assertions fails, the test halts, and the other behavior is not tested. Also, the test is less concise, and the description matches only one of the two scenarios.\n\nTool Tip\n\nThe first assertion/expectation that fails halts the test. (Other tests in the test run should continue to execute as expected.) Any further assertions within that test will not be checked. This is a valuable feature of test frameworks, because once a scenario is known to be in an undesirable state, any further information we try to gather will be unreliable and possibly misleading.\n\nThis is the reason for the oft-quoted “rule” of “One assertion per test.” (I’ve always treated it as a design guideline rather than a rule.) Notice how many of these test smells arise from having more than one assertion in a single test.\n\nParticularly in frameworks that allow nested describe clauses,\n\neach with its own BeforeEach, it’s easy to create short, clear, concise tests with only one assertion.\n\nIdeally, each of your unit tests should fail for one and only one broken behavior. Then, whenever a test fails in the future, you’ll get more information from more tests with fewer assertions. The passing or failing of individual tests will help you triangulate the mistake.\n\nBetter:\n\nbefore(:each) do\n\n@board = Board.new @destroyer = Ship.new(size=2) @board.place(@destroyer, Orientation::HORIZONTAL, row=0, colu\n\nend\n\nit 'reports sunk if all cells have been hit' do\n\n@board.attacked_at(0, 0) @board.attacked_at(0, 1) expect(@destroyer.sunk?).to be true\n\nend\n\nit 'reports damaged if attack hits a cell' do\n\n@board.attacked_at(0, 0) expect(@destroyer.damaged?).to be true\n\nend\n\nit 'reports still afloat if NOT all cells hit' do\n\n@board.attacked_at(0, 0) expect(@destroyer.sunk?).to be false\n\nend\n\nSmell: Unrelated Assertions\n\nThis test smell is very similar to Coupled Assertions. In this case, though, you sense that the adjacent assertions are not at all related. Two flavors of this smell are most common: assertions upon different objects, and multiple assertions upon the state of the tested object.\n\nIf you assert upon more than one object in a test, consider testing those objects separately.\n\nSmelly:\n\nit 'will place a ship where asked' do\n\n@board.place(@sub, Orientation::HORIZONTAL, row=0, column=0) expect(@board.whats_at(row=0, column=0)).to eq(@sub) expect(@board.number_of_ships).to eq(1)\n\nend\n\nAre we testing that the board can tell us what has been placed on a particular location, or that it can count how many ships it contains, or both?\n\nPutting unrelated assertions information whenever one or both fail.\n\ninto separate\n\ntests will provide more\n\nBetter:\n\nit 'will place a ship where asked' do\n\n@board.place(@sub, Orientation::HORIZONTAL, row=0, column=0) expect(@board.whats_at(row=0, column=0)).to eq(@sub)\n\nend\n\nit 'knows how many ships it holds' do\n\n@board.place(@sub, Orientation::HORIZONTAL, row=0, column=0) expect(@board.number_of_ships).to eq(1)\n\nend\n\nSmell: Unfailing Assertion\n\nUnfailing Assertion is an assertion that is unrelated to the behavior being tested, usually testing something that couldn’t possibly fail. An unfailing assertion is misleading and confusing, and future readers will struggle to understand why it’s there.\n\nSmelly:\n\n@Test void theConstructorWorks() {\n\nShip shipToBeTested = new Ship(4); assertThat(shipToBeTested).isNotNull();\n\n}\n\nThe assertion that the reference pointer isn’t null cannot fail. Any condition that could conceivably leave shipToBeTested uninitialized would either\n\nthrow an exception (failing the test without reaching the assertion) or would crash the whole virtual machine.\n\nThe solution is to delete the Unfailing Assertion.\n\nSmell: Redundant Assertion\n\nThis test smell is a special case of Extended Narrative mixed with Unfailing Assertion. It arises whenever the same behavior and related assertion appears in different tests. This typically occurs when a developer writes a test without considering or trusting the existing tests and implementations. Perhaps the developer was uncertain of where to begin, or was not yet willing to rely on the safety net, or wrote the test using copy/paste/modify while thinking, “What’s the harm of an extra assertion?”\n\nIt’s acceptable—even commonplace—to use behaviors that are thoroughly tested elsewhere (within the same safety net) to set up a scenario (given) or to assert that something happened (then). But once you have tested a behavior elsewhere in your safety net, you do not need to assert that it is still working in those other scenarios.\n\nSmelly:\n\nit 'reports damaged if attack hits a cell' do\n\n@board.attacked_at(0, 0) expect(@destroyer.damaged?).to be true\n\nend\n\n# ...many other tests written here, perhaps over many days...\n\nit 'reports sunk if all cells have been hit' do\n\n@board.attacked_at(0, 0) expect(@destroyer.damaged?).to be true @board.attacked_at(0, 1) expect(@destroyer.sunk?).to be true\n\nend\n\nThe computer is not likely to stop computing correctly between the execution of one test and another. Yes, it is possible that a stray gamma particle will knock one of your system’s bits into the wrong state, and a test will fail randomly. But the odds are so slim that you’re better off running\n\nthe tests again before blaming gamma rays. If the same test fails twice, it was not caused by gamma rays.12\n\n12 https://radiolab.org/podcast/bit-flip\n\nThe solution: delete the smelly assertion. If you have doubts about the redundancy of the deleted assertion, run the tests with code-coverage afterward to confirm that the assertion was not testing some obscure bit of your implementation.\n\nSmell: Repeated Assertions\n\nWith this smell, you find multiple assertions in a test that are almost duplicates. This often occurs when arrays or collections are involved.\n\nMildly smelly:\n\nit 'finds the ship at all locations' do\n\nbattleship = Ship.new(size=4) @board.place(battleship, Orientation::HORIZONTAL, 3, 2) expect(@board.whats_at(3,2)).to eq(battleship) expect(@board.whats_at(3,3)).to eq(battleship) expect(@board.whats_at(3,4)).to eq(battleship) expect(@board.whats_at(3,5)).to eq(battleship)\n\nend\n\nThis is a mild smell because often the alternatives are worse. In this example, an alternative might be to loop through the coordinates and have the assertion occur within the loop. That smell, Branching Code (discussed later in this section), is worse.\n\nCreating a custom assertion could clarify the test or could further obfuscate the test. Try something; if it makes the test less clear, undo it and move on. Not every smell must be cleaned up.\n\nBetter:\n\nit 'finds the ship at all locations' do\n\nbattleship = Ship.new(size=4) @board.place(battleship, Orientation::HORIZONTAL, 3, 2) expectShipToOccupy(battleship, [[3,2],[3,3],[3,4],[3,5]])\n\nend\n\ndef expectShipToOccupy(ship, cells)\n\ncells.each do |cell| expect(@board.whats_at(cell[0],cell[1])).to eq(ship), \"@[#{cell[0]},#{cell[1]}]\" end\n\nend\n\nA loop in a custom assertion doesn’t smell as bad as a loop in the test, because the custom assertion method gives the loop an intention-revealing name and declutters the flow of the test scenario. A simple custom assertion is often reusable, reducing duplication across tests.\n\nSmell: Branching Code\n\nThis smell occurs whenever there is a branch or loop in the test. It happens when a developer tries to get a single test to test too much.\n\nGood tests are written in a simple, step-by-step format that describes a scenario. As a result, they are more linear and “scripty” than a well- decomposed implementation.\n\nThis smell comes in various flavors:\n\nIf statements: The desire to add an if statement to a test is a clear indicator that it is testing too much. Instead, break the test into two tests with unique descriptions.\n\nLoops: Strong arguments can be made for looping over a whole table of data, and some of the unit-testing frameworks have utility syntax to provide that kind of testing. That’s great for testing in general. However, when you are developing code using TDD, you are testing the edges of behavioral boundaries. Is there a change in the code paths for each row of the table? If not, each row is testing the same logic and calculations. Again, it might not be a bad test, but it isn’t a “unit test” for the TDD game. It's better to write a single test for each edge of the behavioral boundary. The description (the name of the test method, or the text string that describes the purpose of the test) should then clarify how each test is unique.\n\nTry-catch statements: This is the most dangerous of all test smells in this catalog. If the catch catches something generic like Exception or Throwable, the test might not fail even when the code is broken. The resulting safety net might lie to you.\n\nIf your code throws an unexpected exception, the test framework will tell you. If throwing the exception is the expected behavior, then use the assertion designed to check for thrown exceptions. The syntax for an exception-thrown assertion typically takes a block, or “lambda function,” so it can be tricky. Familiarize yourself with the correct assertion syntax for your framework.\n\nSmell: Calculations\n\nLike Branching Code, this smell indicates you are asking your test (and your reader) to do too much.\n\nSmelly:\n\ndescribe 'converter' do\n\nit 'converts Fahrenheit to Celsius' do converter = Converter.new expect(converter.f_to_c(70)).to eq(70 - 32 * 5 / 9) end\n\nend\n\nThe calculation for the conversion appears in the test, but it’s difficult to understand at a glance. Here’s the implementation:\n\nclass Converter def f_to_c(f) f - 32 * 5 / 9 end\n\nend\n\nThis test passes. The trouble is that both the test and the implementation are wrong! This can happen when a developer copies the test code into the implementation, or vice versa.\n\nEven if the implementation and the calculations in the tests were correct, those calculations still make the tests harder to read. To understand the test, the reader’s brain must parse and execute the calculation.\n\nTo resolve this test smell:\n\nAvoid mathematical operators in your tests. To write a good test, you must know the precise outcomes. Use a calculator, search the Internet, or ask a business analyst. If you’re working in a scientific field, ask your client scientists to get out their slide rules and tell you what the results should be.\n\nCreate a scenario with math so excruciatingly obvious that anyone who subsequently reads the test can do the math quickly without a calculator.\n\nAvoid “magic numbers” by assigning values to variables, constants, or fields with descriptive names.\n\nTest right at the edge of any behavioral boundaries. Since Fahrenheit-to-Celsius temperature conversion is a simple, linear, and well-known algorithm, it doesn’t really have any true behavioral boundaries. Even so, you can start with well-known values.\n\nBetter:\n\nit 'converts Fahrenheit to Celsius when water freezes' do converter = Converter.new expect(converter.f_to_c(32)).to eq(0) end\n\nit 'converts Fahrenheit to Celsius when water boils' do converter = Converter.new expect(converter.f_to_c(212)).to eq(100) end\n\nIn this case, two discrete values assure that the calculation is correct. They could be any two values.\n\nYou might prefer to use a temperature below freezing to confirm that your implementation allows for negative numbers. What follows is a truly nerdy example: absolute zero! It could be a behavioral boundary because it is the “lowest temperature possible.”13\n\n13 https://en.wikipedia.org/wiki/Absolute_zero\n\nit 'converts Fahrenheit to Celsius at absolute zero (approximat\n\nexpect(converter.f_to_c(-459)).to eq(-273)\n\nend\n\nSmell: Custom Parent Test Classes\n\nThe very earliest versions of JUnit relied heavily on subclassing junit.TestCase rather than making use of delegation. In turn, our early TDD teams would often create a project-specific abstract subclass of TestCase, which would act as parent to all of our actual TestCase classes. That got messy very quickly.\n\nThere are often good reasons, and good ways, to add custom assertions or object builders to your test framework. Research the best way to do that, given your specific domain and tools.\n\nSmell: End-to-End Tests\n\nSome code smells aren’t always bad. Every TDD project I’ve worked on supplemented the unit-test safety net with a small percentage of end-to-end tests. The latter were called “integration tests,” “navigation tests,” or “acceptance tests.”\n\nThe trouble occurs whenever these tests slow down the feedback loop between making a change and knowing that you broke something. When actively playing the TDD game, 40 seconds seems to be the absolute maximum. If you’re integrating only once every couple of hours—for example, when a task is complete—then 15 minutes (the length of a coffee break) is still the absolute maximum. If any testing takes longer than that, the efficacy of the safety net will rapidly deteriorate.\n\nHere’s an example. Imagine you find a manual test case that looks something like this:\n\n1. Log in as Richie Rich.\n\n2. Check that you are on the home page.\n\n3. Click on the Performance menu.\n\n4. Check that you are on the Performance page.\n\n5. Click on the Amazing Bookstore stock symbol.\n\n6. Check that you are on the personal performance page for Richie Rich’s Amazing Bookstore holdings.\n\n7. Assert that Richie Rich’s Amazing Bookstore performance is the correct percentage, given his various purchases of the stock over the years.\n\nThere are so many test smells here, and so much overhead just to get to the intended behavior under test—the personal investment performance calculation. Once this test is automated, there are at least a dozen reasons why it could fail. The system needs to authenticate the user, load all past purchases of the investment (from a database), obtain the current share price for the investment (from a third-party API), and perform the calculation correctly.\n\nLikely, all the code for loading assets from the database, calculating their current monetary value, and summing them up is located within one big smelly code block. The way to reduce this smell is to find discrete behaviors in the test and the implementation that can be tested in isolation —for example, the calculation of percent gain. You can then turn that part of the test into a small group of fast tests (see Chapter 7, “Testing Legacy Code”). Next, you should remove the related assertions from the integration test. Repeat as needed for authentication, navigation, database access, API access, and so on.\n\nWhat remains of your end-to-end test will be a test of the system’s integration points (presumably as deployed on a test server). You will benefit from testing the integration points of your system using small integration tests. But avoid forcing those tests to do double duty as tests of business logic.\n\nKey Lesson\n\nWhat you test separately, you can alter independently.\n\nOften, when I’ve seen teams develop and test with end-to-end\n\ntests, the implementation will contain large blocks of code that\n\nreference multiple external resources. In the Richie Rich example, you can imagine a block of code that extracts investments from the database, then fetches the current price, performs the calculations, and writes the results to the UI framework. When all that happens within a single set of curly braces, it inadvertently couples all referenced dependencies. As a result, it becomes more difficult to change one without possibly breaking access to the others.\n\nBy testing behaviors separately through unit tests, you avoid\n\nunintended behavioral coupling within your implementation.\n\nTo minimize this test smell, always look for ways to unit-test a behavior. Here are some examples that I’ve encountered (and that sometimes surprised me):\n\nSite navigation is a behavior and can be unit-tested without deploying the site.\n\nPage rendering might be a behavior that can be isolated, depending on your architecture.\n\nOn at least two applications I encountered, database access could be integration-tested using the unit-testing framework, with a few simple Create–Read–Update–Delete tests. In one case, we ran these “object relational mapping” tests against a table named something like WORST_CASE_TABLE_FOR_TESTING_ONLY.\n\nFinally, here is the behavior that the original manual test was supposed to test, now with clearly stated inputs (given) and results (then):\n\nit 'gives percent gain for multiple purchases' do\n\nticker = \"AMZBK\" big_gain = Purchase.new(ticker, any_timestamp(), shares=10, p zero_gain = Purchase.new(ticker, any_timestamp(), 10, 25) small_loss = Purchase.new(ticker, any_timestamp(), 10, 30) purchases = [ big_gain, zero_gain, small_loss ]\n\ncurrent_price = 25 expected_percent_gain = 15 expect(account.percent_gain(purchases, current_price))\n\n.to eq(expected_percent_gain)\n\nend\n\nSmell: Tear Down\n\nTesting frameworks typically have an AfterEach mechanism that cleans up after each test (or an AfterAll to wrap up the full test run). It is most often used to reset a database or log out of an authenticated session.\n\nWhen doing TDD, this mechanism is rarely needed. Most tests are thoroughly cleaned up by the garbage collector.\n\nUsing AfterEach is a test smell that indicates you no longer have an independent unit test. It tells you a relatively expensive and independent resource needs to be released after the test runs.\n\nFor example, suppose your test populates records in a database, but then needs to erase them after the test runs. You might have written a very useful test, but it’s not isolated enough to assure that the test will pass quickly and consistently.\n\nWhenever this test smell arises, a test is likely accessing an external dependency that can be replaced with a test double (see Chapter 6).\n\nSmell: Frequent Changes to Given\n\nThis smell occurs when you find yourself returning to previous tests and changing the example data in the given part. Typically, this happens when you are developing a new behavior and permutations arise as you add more complexity.\n\nAnother indicator of this smell is example data that becomes less clear as it becomes less specific to each individual specification. In essence, you wind up with a jumbled, generic blob of test data for all related scenarios.\n\nThis smell often crops up during the Password Exercise (see the Appendix). That exercise asks you to build a single call to determine whether a new password is strong or weak based on a set of rules. The first rule is that the password must have a length greater than 7.\n\npackage password;\n\nimport org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.*;\n\npublic class PasswordCheckerTests {\n\nPasswordChecker checker;\n\n@BeforeEach void InitializeChecker() { checker = new PasswordChecker(); }\n\n@Test void StrongWhenLongEnough() { assertThat(checker.isStrong(\"12345678\") }\n\n@Test void WeakWhenTooShort() { assertThat(checker.isStrong(\"1234567\")) }\n\n}\n\nThis example is a bit contrived and simplistic, but it rapidly demonstrates what can happen when the given data (in this case, the password string) contains combinations that alter behavior.\n\nAccording to Part 1 of the two-part exercise, a strong password also needs to have an uppercase letter, a digit, a lowercase letter, and a special character. You might find yourself repeatedly going back to each existing test to make the “strong” password stronger.\n\nAt the same time, you should be conscientiously (one hopes) editing each weak password so that each exercises one, and only one, of the required rules. For example, having “foo” as the weak password in all tests is not at all clarifying, because it breaks all but one of the password rules in Part 1.\n\nFor example:\n\n@Test void StrongPassword() {\n\nassertThat(checker.isStrong(\"Pa$$w0rd\")).isTrue();\n\n}\n\n@Test void WeakWhenTooShort() {\n\nassertThat(checker.isStrong(\"Pa$$w0r\")).isFalse();\n\n} @Test void WeakWhenMissingCapital() {\n\nassertThat(checker.isStrong(\"pa$$w0rd\")).isFalse();\n\n} @Test void WeakWhenMissingDigit() {\n\nassertThat(checker.isStrong(\"Pa$$word\")).isFalse();\n\n} @Test void WeakWhenMissingSpecialCharacter() {\n\nassertThat(checker.isStrong(\"Passw0rd\")).isFalse();\n\n}\n\n...\n\nThe Password Exercise is a contrived example designed to generate this very test smell quickly (and annoyingly). On a complex and realistic project, you might miss this smell until it becomes much more pungent. There’s a “tipping point” with code smells: Once they grow large enough, cleaning them up no longer seems cost-effective, and we then let them grow out of control.\n\nThis test smell always has a strong code smell mirrored in the implementation. Therefore, the solution involves refactoring both the tests and the implementation.\n\nPart 1 of the Password Exercise will let you get away with mildly smelly tests and implementation. Part 2 is designed to wreck everything. It simulates what happens when you are asked for unplanned enhancements that require invasive changes within your implementation. You’re asked not to read Part 2 until you’ve completed Part 1, to avoid spoilers! Once you have read Part 2’s requests, consider refactoring the existing tests and implementation before tackling those requests.\n\nWithout giving away too much of one possible “solution” to the Password Exercise, here are some things to consider when you encounter this test\n\nsmell while doing TDD in the real world:\n\nYour tested object is probably doing too much, which is causing you to churn through permutations in the test data.\n\nCould the called object delegate part of the overall behavior to a set of simpler objects? Look for which behaviors are changing most frequently across tests, extract those changing behaviors into their own isolated form using the most appropriate features of your programming language (for example, classes, functions, modules, or tuples), and test each discrete behavior in isolation. You will know you’re on the right track if you can test these new objects using simpler test data. For example, recall that the password length rule was originally tested with strong = “12345678” and weak = “1234567.” That wasn’t accidental: Those values tell you exactly where the behavioral boundary is for the length rule.\n\nTo uncover a name for those classes, functions, or objects, listen carefully in your conversations with teammates. When you talk about the business domain, does everyone keep using a noun that could represent a useful abstraction in your domain model? For example, the password strength checker applies multiple _______s (fill in the blank) to each password string.\n\nIf you used refactorings to remove excess behaviors from the original object and then wrote simpler unit tests for those new delegates, look at the remaining untested implementation of that object to determine what still needs testing. It won’t be trivial, but it won’t be subject to permutations.\n\nFinally, you can safely delete any redundant permutation tests.\n\nSmell: For Testing Only\n\nSometimes, a developer will add a method to the tested object to either inject test data or to have something to assert against. This is a noble effort, but if the method is never used elsewhere, then the test has expanded the object’s interface just to make it testable.\n\nSmelly:\n\ndescribe \"board\" do it \"is 10x10\" do board = Board.new expect(board.height).to be(10) expect(board.width).to be(10) end\n\nend\n\nEven experienced TDD developers will occasionally create this smell. It can happen when building new objects, particularly if developers skip using CRC cards beforehand. Sometimes your code will need to get a little smellier before the right refactorings reveal themselves. So, if this test smell is short-lived, there’s no need to worry.\n\nHowever, if you’re about to knowingly commit a For Testing Only test smell to your team’s repository, first make the smell even more pungent, so that someone on your team will easily detect it later and perhaps find a better solution. For example, rename the questionable methods to something excruciatingly obvious and a little obnoxious.\n\nVery smelly (on purpose):\n\ndescribe \"board\" do it \"is 10x10\" do board = Board.new expect(board.height_FOR_TESTING_ONLY_please_fix_me).to be expect(board.width_FOR_TESTING_ONLY_please_fix_me).to be( end\n\nend\n\nThe For Testing Only smell will dissipate only after you find a cleaner design through refactoring, a richer behavior to be tested, or a real business need for the For Testing Only methods.\n\nSmell: Intermittent Failure\n\n“This must be Thursday,” said Arthur to himself, sinking low over his beer. “I never could get the hang of Thursdays.”\n\n—Douglas Adams, The Hitchhiker’s Guide to the Galaxy14\n\n14 low\n\nwww.goodreads.com/quotes/13822-this-must-be-thursday-said-arthur-to-himself-sinking-\n\nTrue story: A client team had a test that would always fail on Thursdays. I asked them what they were doing about this test, and they said, “We wait until Friday and run it again.”\n\nThat level of unpredictability needs to be investigated. They had either a poorly conceived test, a race condition, or an unreliable data source.\n\nEvery Intermittent Failure represents a risky gap in your safety net.\n\nImagine you need to write a couple of stable unit tests for the following method (in C#):\n\npublic bool IsTodayThursday() { DateTime today = DateTime.Now; return today.DayOfWeek == DayOfWeek.Thursday; }\n\nThe trouble is caused by the external dependency DateTime.Now.\n\nFirst, perform the Extract Method refactoring on everything below the initialization of the external object(s). Make the newly extracted method internal so the tests can access it, but not public (unless and until a business need for the new method is found).\n\npublic bool IsTodayThursday() {\n\nDateTime today = DateTime.Now; return IsItThursday(today);\n\n}\n\ninternal bool IsItThursday(DateTime today) {\n\nreturn today.DayOfWeek == DayOfWeek.Thursday;\n\n}\n\nNow you can thoroughly test IsItThursday(DateTime today). Two tests are needed, one for each side of the behavioral boundary: one for Thursday and one for not-Thursday.\n\nHave you just traded Intermittent Failure for another test smell—for example, For Testing Only? Perhaps, but it’s a valuable trade-off: For Testing Only is a design issue, whereas Intermittent Failure is a quality issue.\n\nThat’s the nature of code smells. You clean up those that bother you the most, and sometimes you’re left with other smells that don’t demand your immediate attention.\n\nTool Tip\n\nDon’t neglect other forms of testing.\n\nTDD does a very nice job of covering very nearly 100 percent\n\nof your code. It will greatly reduce the need for other forms of testing, but it doesn’t eliminate them completely.\n\nThe Intermittent Failure example includes a line of non-unit-\n\ntestable code: the code in IsTodayThursday() that passes DateTime.Now to IsItThursday() and returns the results of that call. How would you test that?\n\nOne small, well-designed functional or integration test— written in any automated testing framework—could exercise quite a few external dependencies and test the code that calls them. For example, a single Cucumber scenario using the Playwright headless-browser library could run through just one full scenario and provide assurances that your application is deployed correctly (preferably to a test-only web server) and that all APIs, external libraries, and databases are accessible.\n\nSmell: Scattered Changes\n\nThe smell: Before writing a new test for a new behavior, you need to make the same change to a call or constructor in many existing tests. This often occurs when you are first developing an object’s interface. Perhaps you find a need to add a new parameter to a constructor or a method signature.\n\nScattered Changes is a very important smell, because it causes many developers to perceive test maintenance as more work than it’s worth. The key to overcoming this smell is to take baby steps.\n\nDefinitions\n\nMethod signature: A method’s “signature” is what makes that method uniquely identifiable to the compiler or runtime environment. A method’s signature includes its name and parameter types, but not the return type.\n\nOverload method: An overload method is a method with the same name but with a parameter list that differs in number or types. Overloads allow you to give related methods the same name but a unique method signature.\n\nContrast overload with override, which is a method in a\n\nsubclass that has the same signature but a different implementation that “overrides” the parent class’s behavior. Unless the parent’s method is abstract (having no implementation), override methods are considered a very stinky code smell.\n\nNote that Ruby does not support overload methods, per se. Instead, it allows for optional parameters with default values.\n\nContinuing with the Salvo game example, suppose you initially decided to add the notion of orientation to each ship, but now you realize it would be better as a parameter to the board.place() method.\n\nSmelly:\n\ndescribe 'placing ships on the board' do\n\nbefore(:each) do @board = Board.new end\n\nit 'knows two ships cannot overlap' do vertical_ship = Ship.new(size=3, Orientation::VERTICAL) horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL) @board.place(vertical_ship, 0, 0) expect(@board.can_place_ship_at(horizontal_ship, 0, 0)).to end\n\nit 'will work when there are empty cells around each ship' do vertical_ship = Ship.new(size=3, Orientation::VERTICAL) horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL) @board.place(vertical_ship, 0, 0)\n\nexpect(@board.can_place_ship_at(horizontal_ship, 0, 2)).to end\n\nend\n\nRather than changing everything all at once, you can take several baby steps that will lead you to the desired outcome. Which baby steps you use, and in which order, will be very contextual. The overall strategy is to keep the tests passing with each small change.\n\nStart by looking for Duplicate Given and move the duplication into a BeforeEach.\n\nBetter:\n\ndescribe 'placing ships on the board' do\n\nbefore(:each) do @board = Board.new @vertical_ship = Ship.new(size=3, Orientation::VERTICAL) @horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL end\n\nit 'knows two ships cannot overlap' do @board.place(@vertical_ship, 0, 0) expect(@board.can_place_ship_at(@horizontal_ship, 0, 0)) .to be false end\n\nit 'will work when there are empty cells around each ship' do @board.place(@vertical_ship, 0, 0) expect(@board.can_place_ship_at(@horizontal_ship, 0, 2)).to end\n\nend\n\nYour next step depends on your language and tools. If your IDE has a built- in Introduce Parameter refactoring to add a parameter to a method call, consider using that to pass in an Orientation argument.\n\nIf your IDE doesn’t support Introduce Parameter, you can add the parameter to the method implementation yourself and let compiler errors guide you to all the places where you need to add the value. As a temporary measure, perhaps make it an optional parameter with a default value, if there is a sensical default. C# and Ruby support optional parameters, and if\n\nthe parameter isn’t given in a call, the default value is used. Note that optional parameters must follow required parameters. (In this example, neither HORIZONTAL nor VERTICAL makes much sense as a default: The caller really must provide the value.)\n\nAn alternative way to incrementally add a parameter to a method is to create an overload method. If your language does not support overloaded methods, then you could create a method with a slightly different name.\n\nIn our example, you can first perform an Extract Method refactoring on almost then pass ship.Orientation from within the old method as the new argument value. You can then make your new method public and change the tests to call that method. This technique is particularly useful if you need to preserve the existing interface of the object—for example, if it’s part of a deployed API that must remain backward compatible.\n\neverything within\n\ncan_place_ship_at()\n\nand\n\nHere’s an example using Extract Method in C# to create an overload of CanPlaceShipAt().\n\nBefore Extract Method (in C#):\n\npublic bool CanPlaceShipAt(\n\nShip ship, int startingRow, int startingColumn)\n\n{\n\norientation = ship.Orientation; // most of the logic removed for clarity return result;\n\n}\n\nAfter Extract Method (in C#):\n\npublic bool CanPlaceShipAt(\n\nShip ship, int startingRow, int startingColumn)\n\n{\n\norientation = ship.Orientation; return CanPlaceShipAt(ship, startingRow, startingColumn, orientation);\n\n}\n\npublic bool CanPlaceShipAt(\n\nShip ship, int startingRow, int startingColumn, Orientation orientation)\n\n{\n\n// most of the logic removed for clarity return result;\n\n}\n\nThis refactoring keeps all previous calls intact. A possible next step could be to delete the original method, and then follow compiler errors (or test failures in Ruby) and add the orientation to each of those calls.\n\nGood:\n\ndescribe 'placing ships on the board' do\n\nbefore(:each) do @board = Board.new @vertical_ship = Ship.new(size=3) @horizontal_ship = Ship.new(size=4) end\n\nit 'knows two ships cannot overlap' do @board.place(@vertical_ship, 0, 0, Orientation::VERTICAL) expect(@board.can_place_ship_at(@horizontal_ship, 0, 0, Orientation::HORIZONTAL)) .to be false end\n\nit 'will work when there are empty cells around each ship' do @board.place(@vertical_ship, 0, 0, Orientation::VERTICAL) expect(@board.can_place_ship_at(@horizontal_ship, 0, 2, Orientation::HORIZONTAL)) .to be true end\n\nend\n\nIf you need to add a parameter to a constructor, the refactorings are similar. When one constructor calls another, they are referred to as “chained constructors.”\n\nSmell: Multiple Test Doubles\n\nTests that must train more than one test double are messy and hard to follow.15 The Multiple Test Doubles smell suggests that more than one “unit” of behavior is being tested. Related test smells are Excessive Given and End-to-End Tests.\n\n15 Given that a few of the test smell descriptions suggest test doubles to resolve the smells, it’s only fair to warn you here about two test smells related to test doubles. You might want to revisit these two smells, Multiple Test Doubles and Exposed Implementation, after reading Chapter 6.\n\nThe most pungent form of this smell occurs when one test double must return another test double. For example, it would occur if a mock java.sql.Statement returns a mock java.sql.ResultSet to the tested object.\n\nThis smell occurs most frequently when tests are written after the implementation. If you find yourself “patching” your safety net around previously untested code, you might have to live with this test smell for a while. See Chapter 7, “Testing Legacy Code,” for helpful guidance on adding tests to untested code.\n\nSmell: Exposed Implementation\n\nSpies are a particular type of test double that record the interactions that occur between the tested object and the spy (representing a dependency). Because the test must train the spy to expect certain calls, the test usually contains (and tests) implementation details, rather than focusing on outcomes.\n\nThe Exposed Implementation smell can often be reduced by further identifying and breaking up responsibilities across objects or methods, either via CRC cards or refactorings. However, as with most code smells and test smells, there’s often a trade-off. Spies (described in more detail in Chapter 6) have an important role to play when you need to add tests to existing untested code (the topic of Chapter 7).\n\nOther Common Testing Challenges\n\nThe scenarios described in the sections that follow occur mostly when code is written before tests. They are described here because they reflect an important facet of test maintenance: These challenges can be avoided by writing each test first.\n\nHow to Test a Private Method\n\nWhen playing the TDD game, private methods usually occur when some bit of behavior is extracted from the public behaviors. So, they’re fully tested due to your test-driven approach.\n\nIf you still feel that the private method could use some additional testing, your unease is a strong indication that the behavior lives in the wrong class. If it can be tested in isolation—that is, without a lot of the surrounding object state—then it is ripe for its own encapsulation. Look for opportunities to move the private method to a more appropriate home (with the Move Method refactoring) or create a new object (with the Extract Class refactoring).\n\nAfterward, you might want to clean up any Unrelated Assertions or Extended Narrative test smells that arose, as described earlier in this chapter.\n\nHow to Test an Abstract Class\n\nWith TDD, you typically wait until you have at least two similar objects before extracting an abstraction. Once you notice the duplication across objects, you can reduce it by moving the common behaviors to an abstract class (in Java or C#) or, in Ruby, perhaps a module.\n\nBut you don’t need to point the existing tests to the abstract class or module. Those behaviors are all tested through the concrete subclasses that you’ve already built using the test-driven approach.\n\nDesign Detour\n\nThere has long been a heuristic called the “Rule of Three.” When you have three of something, or when something occurs three times, it’s time to reduce the duplication across those three by some technique—for example, by creating a private method, an abstract parent class, or a Ruby module, or by delegating the behavior to a new dependency object.\n\nConsider being even more sensitive to duplication and playing\n\nthe TDD game by following the “Rule of Two.” It’s always\n\npossible to separate behaviors that are common from those that are unique and to reduce duplication. It’s very rarely the wrong thing to do. Even if you make a slight misstep and must backtrack a little, your safety net always gives you the freedom to experiment with alternative designs.\n\nHow to Test a Method That Creates Its Own Dependencies\n\nThis challenge could occur even with TDD, though it’s mostly avoidable: Avoid both creating and calling an object within the same block of code.\n\nBefore you write the next test, follow these steps:\n\n1. Move all initializations up closer to the beginning of the method.\n\n2. Perform an Extract Method refactoring on everything below those initializations. All challenging dependencies will be passed into the new method as arguments.\n\n3. Make that new extracted method accessible to the tests.\n\n4. Test the new method by passing the dependencies created by the test. The most challenging dependencies can be replaced with test doubles.\n\nOther common refactorings could also help—for example, extracting a Factory Method or Builder to encapsulate the creation code. Test them separately. Your production system can use the factory or builder to assure that objects are created with all the necessary dependencies and absolutely no null pointers anywhere.\n\nA few such refactoring examples appeared earlier in this chapter. Look again at the code that answered the question, “Is it Thursday?” (“Smell: Intermittent Failure”), and the C# example where we extracted an overload of CanPlaceShipAt() (“Smell: Scattered Changes”).\n\nHow to Test or Refactor Existing Untested Code\n\nUntested code is often a huge impediment to starting and sustaining a healthy TDD practice. This problem is so big that it warrants its own chapter (see Chapter 7). That chapter will show you how to patch the safety net whenever you need to alter existing untested code.\n\nSummary\n\nTeams that follow a test-driven approach with diligence will find that the safety net of tests need to be maintained if they want to continue receiving its benefits. Knowing what a good test looks like, how to write good tests, and how to tell when a single test or a collection of them could be made better are all part of the TDD long game.\n\nThe path to high-quality code is test-driven, and the path to an extensible design is refactoring—specifically, refactoring away code smells before they grow into mountains of technical debt. Frequently review your existing suite of tests and identify the most pungent or most common test smell currently wafting through the suite. Discuss it by name (if it’s in this chapter; otherwise, have your team choose their own name), and make an effort to refactor away the stink.\n\nThe chapters that follow describe ancillary but essential TDD practices that will help you with that maintenance. Chapter 6 will help you choose the right test doubles to keep the safety net running swiftly, providing the team with critical feedback within minutes instead of weeks. Chapter 7 addresses in detail the issues with existing untested code.\n\nOceanofPDF.com\n\nPart II: Ancillary Practices\n\nOceanofPDF.com\n\nChapter 6. Test Doubles\n\nIn the late 1990s, when my team first started experimenting with a test- driven approach, we ran into trouble unit-testing any object that interacted with an external dependency (Figure 6.1). A hardware driver presented the most painful challenge because it didn’t even exist yet, and we had to talk to the hardware team in a different time zone to learn how it might behave. This type of challenge occurs whenever your code must interact with a dependency that is slow, unpredictable, complex, expensive, or risky.\n\nFigure 6.1 The problem: Testing object “a” with test t(a) involves external dependency x, which is relatively slow and could give\n\nunpredictable results.\n\nA test double is an object that represents the external dependency in a test scenario but is not the actual production dependency (Figure 6.2). It is created and controlled by the test, rather than by the tested object. Test doubles are most useful when an external dependency complicates ease of testing. A test double plays a supporting role in the test scenario.\n\nFigure 6.2 The external dependency x has been replaced with test double x′ (“x-prime”) under the control of test t(a).\n\nKey Lesson\n\nTest-Driven Development (TDD) is intended for creating, designing, and testing your code’s behaviors. To do this effectively, you will occasionally need to isolate your efforts from other people’s code—that is, from external dependencies.\n\nThere are different flavors of test doubles for different types of software behaviors. This chapter covers many types and uses of test doubles, with examples in Java.\n\nThe Trouble with Dependencies\n\nOften, software requires a dependency that exhibits one or more of the following testing obstacles:\n\nSlow: Anytime a test accesses a disk drive or network, either directly or indirectly, that test will take a lot longer to run than a similar test running entirely in memory. A team might not even notice this slowdown in the first few hundred tests. But once a full test run exceeds even a minute, you start losing the benefits of TDD’s rapid feedback loop.\n\nUnpredictable: Many external dependencies will respond differently over time, which makes writing an independent and repeatable test more challenging. The system clock, a random number generator, and even a “for testing only” database could give you an undesirable response during a test run. Some external dependencies are also unreliable (for example, the Wi-Fi in my house). You will want to test how your software responds to an outage without having to physically disconnect the dependency’s hardware at just the right millisecond.\n\nComplex: Your object-under-test might have dependencies that have their own dependencies, in a cascading tree of interdependence.\n\nExpensive: Some\n\nthe software’s external environment could be expensive in terms of real money. For\n\ninteractions with",
      "page_number": 116
    },
    {
      "number": 6,
      "title": "Test Doubles",
      "start_page": 184,
      "end_page": 208,
      "detection_method": "regex_chapter_title",
      "content": "example, when testing stock-trading software, you don’t want your tests buying and selling real stocks.\n\nRisky: When testing a rocket’s navigation software, you don’t want to have to launch a real rocket.\n\nA Test Double Taxonomy\n\nTest doubles can alleviate the difficulties in even the most challenging testing scenarios. In essence, the test double acts as a replacement for the usual dependency, yet is under the direction of the test. It can simulate any kind of behavior needed to test your code’s behavior. Additionally, once the code under test is called, the test double can optionally be queried to verify that all interactions went as planned.\n\nThere are several types of useful test doubles. Gerard Meszaros collected and delineated various terms that people were using and coined the term “test double” as an umbrella term for these constructs.1 The sections that follow introduce pertinent parts of his taxonomy, plus a new addition, the Nullable, which was named and defined by James Shore.\n\n1 https://martinfowler.com/bliki/TestDouble.html\n\nFakes\n\nA fake2 is a substitute dependency that has real—but not production-ready —behavior. It will behave sufficiently similar to the actual production dependency, but is limited, less expensive, or easier to install.\n\n2 Not to be confused with the TDD Fake It technique. You don’t need a fake to Fake It.\n\nExamples include (but are limited only by your creativity):\n\nA mobile-device emulator\n\nA lightweight, unjournaled in-memory database\n\nA localhost network connection\n\nA for-testing-only web API account with a maximum budget of a few dollars\n\nA fast but obsolete encryption algorithm\n\nA virtual machine running a flavor of your operating system that is similar, but perhaps not the same as, the version used in production\n\nAll of these substitutes could be used to test your code’s behavior, but then you could choose not to use them when actually deploying your software.\n\nFakes can be either configured to be available to the test environment or constructed in each test and delivered to the system under test in the given part of the test.\n\nStubs\n\nA stub is a very simple replacement object that will return predetermined values. Stubs can return a constant value or values given to it by the test. They can also be told to throw an exception to simulate an outage or programmatic error.\n\nMock frameworks can create stubs for you, or you can “hand-craft” them in your test code. In strongly typed languages like Java and C#, a stub either implements an interface, subclasses an abstraction, or subclasses the dependency’s concrete class and overrides all required methods.\n\nStubs are typically “trained” to respond in the given portion of a test or group of tests. This is usually accomplished by passing the appropriate return values into the stub’s constructor, or with simple setters, or by using a stub Builder.3\n\n3 https://en.wikipedia.org/wiki/Builder_pattern\n\nConsider the Salvo game exercise found in the Appendix at the end of this book. There are two optional features: One allows the game to randomly place the player’s ships on the board, and another enables the game to play against the player. In either of those cases, the game needs a way to generate a random board position.\n\nTo test these features, your tests will need to have full control over a random number generator. The tests will replace the default random number generator with a simple stub. Each test “trains” the stub with the return values appropriate for that scenario.\n\nThe following example is a simple JUnit test using a hand-crafted stub. The stub definition that follows can be defined in its own file in the test project, or as an inner class within the JUnit test class.\n\n@Test void generatesRandomBoardPositions() {\n\n// Given: create the stub int expectedRow = 4; int expectedColumn = 2; java.util.Random randomNumbers = new MyRandomStub(new int[] {expectedRow, expectedCo // Given: introduce the stub to the tested object Player autopilot = new Player(randomNumbers);\n\n// When: int[] position = autopilot.selectBoardPosition();\n\n// Then: assertThat(position).isEqualTo( new int[] {expectedRow, expectedColumn});\n\n}\n\nAnd here is the stub, a hand-crafted subclass of java.util.Random:\n\npublic class MyRandomStub extends java.util.Random {\n\nprivate final int[] integersToReturn; private int indexToNextRandom = 0;\n\npublic MyRandomStub(int[] integersToReturn) { this.integersToReturn = integersToReturn; }\n\n@Override public int nextInt(int maximumExclusive) { return this.integersToReturn[indexToNextRandom++]; }\n\n}\n\nA few warnings about stubbing this way:\n\nStubs are test code. Be sure they exist only in the test project.\n\nThe stub in the example extends Random instead of implementing the RandomGenerator interface. To implement the interface, you would need to stub every method declared on the interface, even if you don’t use them all. This is disappointing because using an external interface within your implementation is often preferable. Notably, there are many broad “kitchen sink” interfaces out in the wild; for example, java.sql.ResultSet has almost 200 method declarations. if you\n\nConsider what would happen\n\nto alter your implementation to call nextLong() instead of nextInt() as a refactoring (that is, without first adding or changing a test). Because the stub already inherited nextLong() from java.util.Random, if you forgot to alter the stub, then tests would start to fail intermittently (and randomly). Mock frameworks (described later) solve this problem by causing a failure whenever the tested object calls a method that the mock wasn’t expecting. Most mock frameworks will also provide in the failure messages a description of the calls that were received by mistake.\n\nYou could mistakenly train your test double to return values that the real production dependency would never produce. For example, MyRandomStub should never be trained with a stubbed return value of maximumExclusive or greater in any test, because the Java Class Library implementation of Random would never do that. Avoid testing impossible scenarios; otherwise, you might feel compelled to write defensive code for that scenario, either in your implementation or in the stub itself.\n\nStubs should be simple. If your stub does contain some logic—for example, MyRandomStub increments its index with each call—you may want to unit-test the stub, too. Just be sure never to deploy the stub into production!\n\nMocks\n\nA mock4 is a test double that is generated by a mock-object framework, used in a test, and then discarded once the test is done.\n\n4 In conversation, many developers—me included—will use the term “mock” interchangeably with “test double” as the umbrella term. It’s simpler to say, it’s what we called these objects before a test double taxonomy or the mock frameworks existed, and it works as both noun and verb, as in “Let’s mock that with a mock.” The terminology that your team uses in conversation will depend on whether the team favors linguistic convenience or precision.\n\nMany of the interesting behavioral boundaries that occur when your code interacts with an external dependency are determined by all the myriad ways the dependency could respond. Could it throw an exception? Does it occasionally return a null pointer?\n\nMock-object frameworks are most helpful in such cases. A hand-crafted test double would become too complicated when used across multiple unique scenarios.\n\nThe mock object, its class definition (a temporary subclass of the dependency’s type), and its mock implementation are all created while the test is running. Each test “trains” its mock object, describing how it is to play its role in one unique scenario. At the end of the test, the mock object is discarded and its training forgotten.\n\nAn Example of a Mock as a Stub\n\nMocking involves four steps:\n\n1. Creating the mock object\n\n2. Training it\n\n3. Injecting it into the tested object\n\n4. Verifying that the tested object interacted with the mock as expected.\n\nYou can often skip the last step—verifying—and instead use assertions against other results.\n\nConsider the following example of how to test the Salvo board position- generator using Mockito (mock framework details are emphasized):\n\npackage salvo;\n\nimport org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.*; import org.mockito.*;\n\npublic class PlayerAutopilot {\n\n@Test void generatesRandomBoardPositions() { // Given: create the mock java.util.random.RandomGenerator randomNumbers = Mockito.mock(java.util.random.RandomGenerator.c\n\n// Given: introduce the mock to the tested object Player autopilot = new Player(randomNumbers); int expectedRow = 4; int expectedColumn = 2;\n\n// Given: train the mock Mockito.when(randomNumbers.nextInt(10)) .thenReturn(expectedRow) .thenReturn(expectedColumn);\n\n// When: int[] position = autopilot.selectBoardPosition(); // Then: assertThat(position).isEqualTo( new int[] {expectedRow, expectedColumn}); }\n\n}\n\nAny inexpensive mock framework can mock out anything that’s “virtual.” If there’s an interface available for the external dependency, you can give your future self a little more design flexibility by mocking the interface instead example, java.util.random.RandomGenerator rather than java.util.Random.\n\nof\n\na\n\nspecific\n\nconcrete\n\nclass—for\n\nDefinitions\n\nState-based: A test that compares object states against expected values is a state-based test.\n\nInteraction-based: A test that compares expected interactions (that is, calls to dependencies) with what occurred during the test run is an interaction-based test. You want to test your code’s behaviors, and part of that might be making sure it calls external dependencies at the right time, in the right order, or with the right values.\n\nSpies\n\nA spy records each method call made to it and the parameters passed to those methods. Once the scenario is finished, the test can “debrief” the spy. That is, it can verify that the calls to the spy occurred as expected. With a spy, you can determine whether multiple calls happened in the correct order, the correct number of times, and with the correct argument values.\n\nSpies are useful for interaction-based test scenarios, but hand-crafting your own spy may require a lot of custom test-double code. A mock object framework can provide spies that can be trained to expect different calls in each unique scenario.\n\nUsing a spy with TDD may feel awkward: How do you write a test that expects that implementation? Spies can cause us to write tests that are interaction-based and may contain the Exposed Implementation test smell (described in Chapter 5, “Sustaining a Test-Driven Practice”).\n\na\n\nparticular\n\nimplementation without\n\npresuming\n\nDesign Detour\n\nState-based tests tend to be more readable than interaction-based tests, as well as more resilient during refactoring. State-based tests describe behaviors from the external perspective of the caller, whereas interaction-based tests describe encapsulated implementation details.\n\nWhenever possible, favor tests that check for valuable outcomes (state-based) rather than prescribed interactions (interaction-based). Carefully consider which assertions you need to be sure those outcomes are what you wanted. You might have to exercise several objects, preferably using test doubles to replace only external dependencies.\n\nFurthermore, whenever a behavior is difficult to test, it’s not because testing is difficult; it’s due to initial design assumptions. Get out the CRC cards or brainstorm at a whiteboard and find a way to test separate behaviors separately.\n\nSpies are still practical whenever your code calls an external dependency that does not provide an immediate testable result—for example, if your\n\ncall gives the dependency the address of a call-back function. Spies are also useful if you need to write a test for an existing untested implementation— that is, if the implementation was not written test-driven but now needs to tests. The ancillary practice called have fast and comprehensive characterization testing will help you write those tests; it is described in Chapter 7, “Testing Legacy Code.”\n\nAn Example of a Mock as a Spy\n\nWhenever there’s an interaction between two objects that needs testing, but there is no convenient result to assert against, you can spy on the interaction. Also, if an external call is slow or expensive, and you want to avoid a mistakenly duplicated call, you can debrief (verify) the spy to confirm the correct number of calls occurred. This is a quick and easy way to defend your team against an expensive mistake by a jetlagged intern.\n\nThe following code is identical to the previous example test except for the addition of the verify() call:\n\n@Test void generatesRandomBoardPositions() { // Given: create the mock java.util.random.RandomGenerator randomNumbers = Mockito.mock(java.util.random.RandomGenerator.c\n\n// Given: introduce the mock to the tested object Player autopilot = new Player(randomNumbers); int expectedRow = 4; int expectedColumn = 2;\n\n// Given: train the mock Mockito.when(randomNumbers.nextInt(10)) .thenReturn(expectedRow) .thenReturn(expectedColumn);\n\n// When: int[] position = autopilot.selectBoardPosition();\n\n// Then: assertThat(position).isEqualTo( new int[] {expectedRow, expectedColumn}); Mockito.verify(randomNumbers, Mockito.times(2))\n\n.nextInt(Mockito.anyInt()); }\n\nThe mock framework will cause the test to fail if there were either more or fewer calls than expected. In addition, most frameworks will tell you why. For example, consider the test results in Figure 6.3, where I mistakenly told the test to expect only one call to nextInt() using Mockito.times(1):\n\nFigure 6.3 Mockito tells you when expected and actual calls to the mock did not match.\n\nTable 6.1 summarizes the pros and cons of using mock frameworks.\n\nTable 6.1 Mock Framework Pros and Cons\n\nAn alternative to spying with mocks that allows you to use state-based TDD was introduced by my old friend and colleague James Shore. He calls these test doubles Nullables.\n\nNullable\n\nA Nullable is a stub that exists within production code but is typically accessed only by tests. Or, as James Shore puts it in his abbreviated article on the topic:\n\nAt first glance, Nullables seem like test doubles, but they’re actually production code with an “off” switch.5\n\n5 www.jamesshore.com/v2/projects/nullables/a-light-introduction-to-nullables\n\nHere’s how Shore described Nullables to me in an email dated May 19, 2023:\n\nIt’s not just external dependencies that are Nullable; anything that has an external dependency anywhere in its dependency tree is also Nullable.\n\nA Nullable is a class with the ability to “turn off” its interaction with external systems. A Nullable that has been “turned off” has been Nulled. Nullables are production classes, so some people call them production doubles rather than test doubles.\n\nAny production class can be Nullable. Most Nullables delegate to Nullable dependencies, but at the lowest level, Nullables are implemented with the Null Object pattern.6 In statically-typed languages, this involves replacing the external dependency with an interface and two implementations: a wrapper around the actual external dependency, and a stub that “turns off” the external dependency. In normal operation, the external dependency is used, but when the class is Nulled, the stub is used.\n\n6 https://en.wikipedia.org/wiki/Null_object_pattern\n\nAlthough Nullables are mainly used for testing, they do have production uses. For example, imagine a command-line tool with a “--dry-run” option. When the option is used, the tool is supposed to run normally without actually doing anything. A Nullable could be used to implement that behavior.\n\nRecommended Reading\n\nA Light Introduction to Nullables by James Shore: www.jamesshore.com/v2/projects/nullables/a-light-introduction- to-nullables\n\nA Salvo sample test might look like the following. Note that the earlier sample MyRandomStub class could be used by the NulledBuilder once the stub class is moved into the production project and encapsulated within Player:\n\n@Test void generatesRandomBoardPositions_NullableVersion() { int expectedRow = 4; int expectedColumn = 2; int unusedAvoidsFakingIt = 6; int[] numberSequence = new int[] {expectedRow, expectedColumn, unusedAvoid Player autopilot = Player.NulledBuilder().withNumbers(n .create();\n\nint[] firstPosition = autopilot.selectBoardPosition(); assertThat(firstPosition).isEqualTo( new int[] {expectedRow, expectedColumn}); }\n\nTo me, the use of Nullables feels like an architectural pattern: something that a team should consider—and experiment with—prior to crafting a new group of objects related to one or more external dependencies. The advantages of fully state-based testing could be appealing.\n\nAdditional Recommendations\n\nTest doubles can befuddle even the most experienced developers until they become second nature. The sections follow provide some considerations to keep in mind when using any test double.\n\nthat\n\nFavor Dependency Injection\n\nIn all of the examples in this chapter that involve mocks and spies, the mock object was created by the test and then introduced to the tested object via its constructor. This type of “dependency injection” is very common in object-oriented code and tends to keep objects decoupled from their dependencies’ constructors and often their concrete types.\n\nExcessively coupled:\n\nprivate String name; private UserType user;\n\npublic Player() { this.user = new ConsoleUser();\n\nthis.name = user.queryResponse(\"What is your name?\"); }\n\nThe Player class uses (is coupled to) not only the UserType interface and its queryResponse() method, but also the concrete ConsoleUser type and its default constructor.\n\nIf you avoid both building and calling the same object within the same block of code, you give your system much more flexibility and testability.\n\nLess coupling:\n\nprivate String name; private UserType user;\n\npublic Player(UserType userType) { this.user = userType; this.name = user.queryResponse(\"What is your name?\"); }\n\nSomewhere, the code must decide which UserType is to be used, and there are many patterns and techniques to choose from.\n\nSimple static factories:\n\nprotected Player(UserType userType) { this.user = userType; this.name = user.queryResponse(\"What is your name?\"); }\n\npublic static Player createHumanPlayer() { return new Player(new ConsoleUser()); }\n\npublic static Player createAutopilotAdversary(\n\nRandomGenerator randomNumbers) {\n\nreturn new Player(new Autopilot(randomNumbers)); }\n\nDesign Detour\n\nAvoid calling a production factory in a test to create the tested object. Instead, favor calling a constructor on the tested object—\n\none that takes all challenging dependencies as parameters. That way, you can more easily configure the object for various tests. Often, the given part in your earlier unit tests can serve as a model for how to best construct an instance of the tested class in production code.\n\nYou can also write unit tests for most factory code. You’ll want to do so if that code can return different subclasses under different circumstances, or if it configures the returned object in some way.\n\nTool Tip\n\nIn Java, a constructor can be made protected or package- protected to prevent excessive use outside the factories. When the tests have the same package name (even though they live in a separate project), they can still use a protected constructor to introduce test doubles.\n\nIn C#, internal constructors are made accessible to tests by\n\nadding the tests’ assembly name to the implementation’s assembly configuration using InternalsVisibleTo():\n\n[assembly: InternalsVisibleTo(\"SalvoTests\")] ... internal Player(UserType userType)\n\nWrap External Dependencies\n\nTest doubles are great choices as substitutes for dependencies that are slow, unpredictable, complex, expensive, or risky. Typically, those are external dependencies. Another good, but seemingly contradictory, approach is to substitute for only those objects your team has control over.\n\nHow could you substitute for only external dependencies, but also classes within your control? By creating wrapper objects at the periphery of your architecture. In other words, you can hide each external dependency behind a thin wrapper—so thin that it contains no testable behavior. Then your tests can replace the wrappers with test doubles.\n\nThis consideration is especially important when your external dependencies are inconveniently designed. For example, java.sql.ResultSet has nearly 200 unique method signatures that all declare that they can throw checked exception SQLException.\n\nIf you’re building an API, don’t require an external caller to pass in a ResultSet. Instead, create your own Java interface and offer that as a way for your clients to interact with your system. The caller will need to wrap its ResultSet in a wrapper that implements your interface (Figure 6.4). You can then test your API using test doubles that also implement your interface (Figure 6.5).\n\nFigure 6.4 A thin delegating wrapper b(x) exposes only the methods of x that are used. This decouples x from your business domain and can reduce the number of methods that need to be stubbed.\n\nFigure 6.5 Wrapper b(x) is replaced with test double b′, greatly simplifying testing.\n\nIf, instead, you are calling an external API (for example, JDBC), consider wrapping the external services in simple delegating wrappers. Include only those method signatures that are used by your system, thereby shrinking both the interface and the cognitive dissonance associated with the bloated service. Your tests can then substitute a test double that extends your wrapper class.\n\nHere’s sample wrapper code for related classes from the java.sql package.7\n\n7 Important caveat: This example is meant to demonstrate the general technique, and not to suggest a generic solution for JDBC.\n\nResultSet:\n\npublic class DatabaseResult {\n\nprivate final java.sql.ResultSet realResultSet;\n\npublic DatabaseResult(java.sql.ResultSet wrapped) { this.realResultSet = wrapped; }\n\npublic boolean getBoolean(String columnLabel) { try { return realResultSet.getBoolean(columnLabel); } catch (java.sql.SQLException sqle) { throw new DatabaseException(columnLabel, sqle); } }\n\npublic int getInt(String columnLabel) { try { return realResultSet.getInt(columnLabel); } catch (java.sql.SQLException sqle) { throw new DatabaseException(columnLabel, sqle); } }\n\n}\n\nSQLException:\n\nimport java.sql.SQLException;\n\npublic class DatabaseException extends RuntimeException {\n\npublic DatabaseException( String localData, SQLException sqlException) { super( String.format(\"Local data: [%s]\", localData), sqlException); }\n\n}\n\nNotes about wrappers:\n\nWrappers can also wrap their results if necessary. But more than two layers of wrapper? That’s starting to smell odd. When I’ve worked with JDBC, for example, we instead isolated complete actions behind façades8 (for example, “Transaction” and “Query”). A query\n\nwould result in a complete collection of business objects, and SQLException was wrapped in fewer than a half dozen places.\n\n8 https://en.wikipedia.org/wiki/Facade_pattern\n\nIn typed languages such as Java and C#, nearly all code in a thin wrapper is sufficiently “tested” by compiling.\n\nThe try/catch/wrap blocks in the sample DatabaseResult wrapper are fully unit-testable. Given that you’re building a wrapper for ResultSet, which is causing the testing challenges in the first place, you would need to mock ResultSet with a mock framework just for those behaviors within DatabaseResult. Even a top chef uses a microwave from time to time.\n\nChapter 7, “Testing Legacy Code,” contains an example of a particularly useful wrapper, the “virtualizing proxy.”\n\nUse Simple Objects as Their Own Test Doubles\n\nObjects that are easy to create, such as Date (in Java) and DateTime (in C#), can act as their own test doubles. For example, you can create a Java Date from a string:\n\nDate b5goesLive =\n\nnew SimpleDateFormat(\"dd-MMM-yyyy\").parse(\"09-Mar-2256\"\n\nDefinition\n\nSociable test: A test that allows the tested object to access real collaborators to accomplish the expected outcomes.9\n\n9 https://martinfowler.com/bliki/UnitTest.html\n\nObjects such as Date, String, and Integer are immutable. That assures you that their state will not change during the execution of a test, no matter what your code asks them to do.\n\nIf your tested object is easy to construct out of other easily constructed objects, then you can test each unique behavior with a representative object graph. As an example, imagine you need to test a Filesystem API. You\n\nmight be asked to test whether the Filesystem can report on capacity used and capacity remaining, and that it can build a full pathname of a particular file. These behaviors can be tested by building a simple object graph, as demonstrated in Figure 6.6. Because the behaviors of the tested object and its collaborators are all exercised by each test, such tests would be considered “sociable tests.”\n\nFigure 6.6 A Filesystem with a few folders and files of various sizes.\n\nWould sociable tests still be “unit tests”? Certainly, if those supporting objects do not themselves access a troublesome external dependency. Once you have fully tested a behavior, it can be used confidently in other tests and implementations. The behavior of all production classes in the sample object graph would be exercised in those activities. Meanwhile, if the relevant File and Folder behaviors have already been tested, they would simply play their part while new Filesystem behaviors are developed.\n\nSummary: Use with Care\n\nreplacing challenging external for Test doubles are very useful dependencies. They can be used in suites of unit tests or Behavior-Driven Development’s scenarios, the suites sufficiently fast and repeatable.\n\nto make\n\nSometimes, though, they can be almost too powerful. Here are a few caveats.\n\nBe Sure to Deploy with Real Dependencies\n\nMake sure your deployed code receives a real dependency when it is created in production.\n\nFirst, be sure that no mock framework or stubbed class is referenced in your production-ready build. Keep these items in a separate test project that cannot get deployed beyond the build, integration, and test servers.\n\nSecond, avoid using null in all constructors, assignments, and parameter lists. Think of the null keyword as its own code smell—a very biting stench. Except for the need to check external values (for example, from an external caller or external API), endeavor to eliminate any need for the null keyword in your system.\n\nSupplement with Narrow Integration Tests\n\n“Narrow integration tests” (a term coined by James Shore) are not quite unit tests by this book’s definition, but they’re not whole end-to-end scenarios, either. They are tests of an external dependency itself, or the objects that directly interact with that dependency.\n\nIf there is some way to force the external dependency to give a predictable response, you can write tests for these narrow, isolated, external behaviors. As long as these tests remain few in number and tightly focused, you can keep your total build time below the “coffee break” standard, and you will feel more confident that all individual behaviors of your system have been fully and automatically tested.\n\nContinuing with the JDBC example, a few of my teams were able to refactor JDBC access to isolate and reduce actual interactions to a handful of services: create, read, update, and delete (“C.R.U.D.” code). This allowed us to write about a half-dozen tests that truly interacted with JDBC.\n\nWe used a fake database (a test database freshly erased for every dev, test, or integration test run). Eventually, in-memory database instances became an option. Today, a local test database running on a solid-state drive can provide the same convenience.\n\nHaving a small number of narrow integration tests is a valuable addition to your safety net:\n\nUnlike a full suite of end-to-end tests, these tests are limited to integration points. They may be slower than unit tests, but each code path is tested only once, so the full integration test suite might add just a few seconds to the feedback loop.\n\nThey confirm that dependencies continue to behave as expected. Their narrowness helps pinpoint any undesirable changes whenever the team upgrades to a newer version of an API or programming language.\n\nThey test the minimal behaviors of architectural wrappers. By testing wrappers without test doubles, narrow integration tests provide confidence that a production deployment will not result in null- pointer/null-reference exceptions.\n\nUse Mock Frameworks Sparingly\n\nMock frameworks make it easy to test complicated interactions. They’re particularly useful when managing previously untested code (see Chapter 7). But that ease-of-testing can render the team less motivated to refactor a smelly design. Poor designs left lying around, despite being tested, can still cause confusion and slow further progress.\n\nAs in a sushi kitchen, the typical definition of “clean enough” is not clean enough. If you use a mock to expedite development, be sure to sniff out new code smells and be ready to refactor.\n\nNow that you have test doubles in your toolbox, you are fully prepared for Chapter 7, “Testing Legacy Code”!\n\nOceanofPDF.com\n\nChapter 7. Testing Legacy Code\n\nAfter a Test-Driven Development course at Microsoft (circa 2005), a developer came up to me and said, “Great class! Unfortunately, I probably won’t get to use TDD until I move to another team. You see, our team has about 400,000 lines of untested C# code to maintain.” That was the day I realized I needed to include techniques to wrangle legacy code.\n\nAt the time, there were concerns about the applicability of TDD for anything but greenfield projects. There were misguided suggestions to simply avoid or rewrite legacy code: “If it ain’t broke, don’t fix it!”\n\nThe reality of software development, however, is that adding new behaviors to an existing system often requires changes to older code.\n\nDefinitions\n\nLegacy code: In his book Working Effectively with Legacy Code (Pearson, 2004), Michael Feathers defines legacy code very simply: “Code without tests” (p. xvi). Feathers wasn’t trying to place blame; he was drawing a clearly defined boundary between two types of code: tested and untested.\n\nIn Art of Agile Development (O’Reilly Media, 2021), James Shore defines legacy code as “code you’re afraid to change” (p. 300).\n\nI consider Feathers’s and Shore’s definitions to be equivalent. Greenfield: Adjective denoting that a project or product is\n\nbeing built with all new code.\n\nLegacy code presents a dilemma: The team wants to enhance the system and deliver on received requests. To do so, they need to refactor, and that requires a good testing safety net. But untested code usually resists testing. To make it more testable, we often need to refactor it first.\n\nA team with a poor safety net of tests, or a very slow test feedback loop, will be reluctant to change legacy code for fear of breaking something. Most teams still try to get around this risk by copying legacy code that is close to what they want, pasting it into another file, and modifying the pasted code to perform the new functionality. This copy/paste/modify technique does indeed preserve the behaviors of legacy code—usually “someone else’s code”—but results in more legacy code and increasingly unmaintainable designs.\n\nDesign Detour\n\nA cautionary tale about copy/paste/modify: I once met with a team that was asked to make a single change to a dynamic item on their website, and they unhappily realized they had to make the change to every one of their 100-plus ASP.Net pages. Then months later, the defect reports started to roll in: They had missed a few pages.\n\nThere is always a way to eliminate duplication, and it’s almost\n\nalways a good idea to do so.\n\nThe ASP.Net team had missed the opportunity to utilize an ASP.Net “User Control”—a separately tested custom widget that can appear on multiple pages yet provides a single point of maintenance. More generally, objects, functions, abstract classes (Java, C#), modules (Ruby), private methods, and many other programming language features are provided so that (1) the design can be understood by humans and (2) each change can occur with the least amount of risk.\n\nA fast, comprehensive safety net of tests prevents a team from breaking existing behavior while they are adding new behavior and refactoring the design. The safety net provides confidence, which in turn shortens average delivery time.\n\nLegacy code—code without fast tests—has huge holes in the safety net. Fortunately, there is a way to “patch” the holes: Characterization testing is an essential ancillary practice for your TDD toolbox. With a little imagination, it can be as enjoyable as using TDD to develop new features (Figure 7.1).\n\nFigure 7.1 Flowchart showing how TDD and characterization testing complement each other.\n\nOnce you’ve patched and strengthened your safety net with characterization tests—covering all behaviors in the areas of code that require change—you can confidently return to refactoring design and adding new functionality using TDD.",
      "page_number": 184
    },
    {
      "number": 7,
      "title": "Testing Legacy Code",
      "start_page": 209,
      "end_page": 236,
      "detection_method": "regex_chapter_title",
      "content": "Characterization Testing\n\nA characterization test is a test that characterizes—that is, captures and preserves—existing behaviors.\n\nimagine yourself as a “software When faced with paleontologist.” You are going to carefully sweep away the dust and dirt that has accreted over immeasurable time, then identify and preserve the fragile fossils of your system. Unlike in real paleontology, though, once you’ve identified and protected enough of the fossils, you can then turn a slow, plodding brontosaurus into a fast, intelligent velociraptor through refactoring and TDD.\n\nlegacy code,\n\nAs with TDD, the goal is to be able to confidently develop new and valuable features without damaging existing behavior. Unlike with TDD, however, you don’t decide which behavior to create by writing a failing test and then writing the correct code to make it pass. Instead, you write a test to explore and preserve what the existing code is already doing.\n\nThe characterization test recipe in brief is as follows:\n\n1. Ask the “Three Questions” (described in the next section).\n\n2. Start writing a test: Call a constructor and a method, or a static method. Pass in some test data and add at least one assertion. If this is impossible, use a “surgical refactoring” (also described later) to make testing possible.\n\n3. Run the test and learn from its failure: Did the tested object throw an exception? Did it delegate to an external dependency? Did it return an unexpected value?\n\n4. Update the test with what you learned in step 3. Add a mock object, update an assertion, and change the test’s expectations to meet what the object is doing with your test data.\n\n5. Repeat steps 3 and 4 until the test passes, and refactor the test until it is fast, independent, and repeatable.\n\n6. Return to step 2 for a different behavior. Continue until you have covered all code paths for the area you want covered.\n\nYou don’t need to cover all your legacy code all at once—just the code that is about to change. Apply characterization tests when and where you need them, rather than wasting precious time attempting to cover everything before you make a valuable change. Behavioral coverage will grow as needed, rather than looming over your team’s heads as one huge monolithic chore.\n\nThere are common legacy testing hurdles you can watch for and common ways to get over those hurdles. Still, you shouldn’t let the flowchart shown in Figure 7.1 and this brief recipe lure you into a sense of complacency. Characterization testing will require all your experience, intuition, and cleverness—so put on your paleontologist’s fedora and get digging.\n\nThe Three Questions\n\nFirst, examine your legacy code and roughly prioritize which areas you want to cover. You can use these questions as a guide, but they don’t have to be addressed as a sequential procedure. Instead, keep all of them in mind as you examine the code, and record answers to each as you notice something.\n\nQuestion 1. What does this class or method do? Write a brief description (not code!) of each behavior in a fresh to-do list. For now, write down one item for each separate path through the code. Characterization tests usually cover multiple behaviors, so do not become discouraged by the size of the list.\n\na. Look for loops. Will you need separate tests for zero, one, and\n\nmany times through the loop?\n\nb. Look for other forms of branching: if, if-else, and switch\n\nstatements.\n\nc. Be thorough. If the legacy code chooses an option, calculates a value, logs a message, throws an error, or calls an external dependency, write that down. You must assume that it does\n\neverything for a reason. Plan to lock down that behavior, no matter how trivial it might seem.\n\nQuestion 2. What is preventing you from easily writing a test for those behaviors? Note anything that will create a challenge. For example:\n\na. An important method call that provides no meaningful return value.\n\nb. Numerous external dependencies, all accessed from the same\n\nmonolithic block of code.\n\nc. A dependency that cannot be easily substituted with a test double (for example, a sealed .Net class, final Java class, or nonvirtual C++ method).\n\nQuestion 3. What can be done about the challenges listed under Question 2? As you encounter testing challenges, write down any ideas you have that will make testing easier. Much of this chapter is dedicated to some of the most common techniques.\n\nRecommended Reading\n\nThere is an entire, massive book that offers numerous characterization testing techniques: Working Effectively with Legacy Code by Michael Feathers. Sections in his book are titled with a question or concern that you might have, and while covering one solution, the author sometimes guides you to another section to cover a related technique. The book is so well organized by topic that I’m not sure I’ve ever read the whole thing, because I usually end up skipping around.\n\nThe rest of this chapter covers the techniques that I’ve found\n\nto be most useful in a wide variety of circumstances. Those techniques may suffice for your purposes. Nevertheless, Feathers’s book is an excellent companion to this one.\n\nStart Writing the First Test\n\nJust jump in! Start writing a test that calls the object’s constructor or calls one static method. You can start by passing nonsensical parameter values:\n\nnulls, minus one (–1), or perhaps “Inconceivable!” Follow the compiler’s and the test’s complaints to determine what else your test requires.\n\nIn the following example, we want to test openOrderReport(). However, it has a void return, so we may not know what to assert against. For now, we’re experimenting using the test framework:\n\n@Test void openOrderReport() {\n\nString fakeTestAccount = \"BOGUS\"; ReportGenerator generator = new ReportGenerator(fakeTestAcc generator.openOrderReport();\n\n}\n\nKey Lesson\n\nWhenever you’re using your test framework to experiment with a bit of legacy code or an external API, make sure your development/test system cannot reach a production version of any system beyond your local computer. You don’t want to buy 10,000 shares of a stock, or launch the $100 million rocket into space, or erase the production database. In Working Effectively with Legacy Code, Feathers suggests disconnecting the network while running the experiments. Sound advice, indeed!\n\nPerhaps unsurprisingly, most characterization tests do not pass right away:\n\njava.lang.RuntimeException: Account \"BOGUS\" not found\n\nat investors.FinancialDatabase.QueryAccount(Financ at legacy.ReportGenerator.<init>(ReportGenerator.j\n\nThe first test you write for a portion of legacy code tends to take the longest and requires the most mental effort, because most of the hurdles must be overcome in that first test. This is why teams often give up on characterization testing too soon: When the first test you write takes hours to complete, you might feel that it was wasted time. Not so!\n\nStep by step, you will encounter and overcome each hurdle preventing you from writing that first test. Afterward, the second test for the same block of\n\ncode will likely take just a few minutes, because you’ve already found a way around those testing challenges.\n\nTool Tip\n\nWhen adding characterization tests, teams can use any preferred automated testing framework. I recommend using the unit-testing framework—at least at first—for these reasons:\n\nYou don’t have to switch back and forth between tools or languages. Everything is within the developers’ IDE and written in a single programming language.\n\nYou’re using the tool to explore what a certain part of the code is doing, so you are digging up and preserving behavior possibly the system— behavior that is not always clearly associated with user- visible features. located deep within\n\nA language like Cucumber’s Gherkin scenarios is great at describing desired behaviors to a broader audience (including product managers, business analysts, testers, auditors, and developers). In contrast, legacy behavior could be a mystery, an accident, or even a defect, or—at the very least—poorly documented from the start. Declaring legacy behavior as persistent and desirable by writing Gherkin scenarios could be misleading to others.\n\nDivide and Conquer\n\nOne of the most frequently used characterization testing tools in your toolbox will likely be the Extract Method refactoring.\n\nHere’s the sample untested code we’re working with:\n\npublic class ReportGenerator { private Account account; public ReportGenerator(String accountNumber) { account = FinancialDatabase.QueryAccount(accountNumber) } public void openOrderReport() { Order[] openOrders = FinancialDatabase.QueryOrders(\n\n\"open\", account.accountNumber(), java.time.Instant.now()); if (openOrders.length == 0) { System.out.println(\"--- [You have no open orders]\") } else { System.out.format( \"| %8s | %8s | %8s | %8s | %8s | %8s |%n\", \"Order\", \"Type\", \"Status\", \"Symbol\", \"Shares\", \"Price\"); for (Order order : openOrders) { System.out.format( \"| %8s | %8s | %8s | %8s | %8d | %8d |%n\", order.id(), order.type(), order.status(), order.symbol(), order.shares(), order.appropriateDisplayPrice()); } } }\n\n}\n\nThe previous failure told us that this class is accessing FinancialDatabase (a test copy, we hope!) to fetch an Account.\n\nThe next step depends on what we intend to change once we have protected the existing behaviors with our improved safety net of tests. If we intend to allow this code to access a different database, we will need to wrap and substitute the database calls—a technique described later in this chapter. If we intend to change the format of this report (for example, adding a column or centering some of the displayed data), then we want to isolate and cover that behavior.\n\nGiven that the first attempt to test anything failed, we need to create what Feathers calls a “seam” in the code—that is, an entry point where we can test. That will require a refactoring.\n\nWe’ve already run into the most common dilemma: We want to refactor the code to make it testable, but we know it’s risky to change code that is not covered by tests. Without a sense of confidence and safety while refactoring legacy code, teams will often find ways to avoid changing that code,\n\ntypically by copying, pasting, and modifying it, and adding a branching statement that leads to their new code. Unfortunately, with this approach, the design continues to degrade, and maintenance costs continue to rise.\n\nInstead, when faced with this dilemma, you will need to apply surgical refactorings, just enough to make part of the code testable.\n\nDefinition\n\nSurgical refactoring: Any refactoring of legacy code that is relatively safe (behavior-preserving) despite a lack of tests around the code that is being changed.\n\nIf your IDE can perform a refactoring for you, you may consider that safe. If your programming language is strongly typed, some manual refactorings will cause a compiler error if you’ve made a mistake or forgotten to complete the refactoring (for example, Rename Method).\n\nHere are the results of a straightforward Extract Method that separates the behavior from the initialization:\n\npublic void openOrderReport() {\n\nOrder[] openOrders = FinancialDatabase.QueryOrders( \"open\", account.accountNumber(), java.time.Instant. formatOpenOrderReport(openOrders);\n\n}\n\nprivate void formatOpenOrderReport(Order[] openOrders) {\n\nif (openOrders.length == 0) { System.out.println(\"--- [You have no open orders]\"); } else { System.out.format( \"| %8s | %8s | %8s | %8s | %8s | %8s |%n\", \"Order\", \"Type\", \"Status\", \"Symbol\", \"Shares\", \"Price\"); for (Order order : openOrders) { System.out.format( \"| %8s | %8s | %8s | %8s | %8d | %8d |%n\", order.id(), order.type(), order.status(), order.symbol(), order.shares(),\n\norder.appropriateDisplayPrice()); } }\n\n}\n\nBy default, Extract Method creates a private method. There are a few ways to make it accessible to tests.\n\nYou could make the new method’s access public:\n\npublic void formatOpenOrderReport(Order[] openOrders)\n\nIn Java, if the test case and the tested class are designated as being in the same package (even if they are in different projects or JAR files), tests can access package-private methods (no modifier): void formatOpenOrderReport(Order[] openOrders)\n\nIn C# (and other .Net languages), you can use internal, which makes the method accessible to everything within the same DLL file, but not accessible (by default) to calls beyond the DLL. Then you can add the test suite’s assembly as InternalsVisibleTo() in the assembly information: [assembly: InternalsVisibleTo(\"LegacyReportGeneratorTests\")] … internal void FormatOpenOrderReport(Order[] openOrders)\n\nIt’s also possible to use reflection in Java and C# to test the private method, although this is a very smelly practice. The test gets obfuscated with reflection syntax, and the name of the method shows up in quotes and might get missed during a future refactoring. Most significantly, the test is now testing what has been designated as an encapsulated implementation detail, not a service of the object. Design Detour: Should I test a private method?\n\nA private method presents a potent testing challenge. Moreover, like most testing challenges, it tells you that there is a better design right around the corner.\n\nMost environments have power tools that will let a developer\n\nset or check private values or call a private method within the tests. That might help you test legacy code in the short term, but such tests can complicate future refactorings. Most legacy code needs to be refactored anyway, so that it will be more expressive and flexible. Rather than take the quick-and-dirty path, consider which surgical refactorings would improve testability.\n\nDevelopers often have reasonable misgivings about arbitrarily\n\nmaking something public—that is, openly accessible to other parts of the system and external callers. But the desire to test a private method means that some important behavior is disguised as an implementation detail (the original programmatic meaning of private). This behavior likely belongs within a different class, possibly all by itself.\n\nOnce the behavior is moved to another class (for example, via\n\nan Extract Class1 or Move Method2 refactoring), it can then be called through a public method (optionally internal in C#) on the new class, and it can be fully tested in isolation.\n\n1 https://refactoring.guru/extract-class\n\n2 https://refactoring.guru/move-method\n\nBuild Tests Incrementally\n\nIn the first of the Three Questions, you identified what the legacy code does. Now, you need to find ways to assert that all the effects of that behavior continue to occur correctly whenever the test runs.\n\nStart with the simplest cases. You might cover less consequential code, but you might also learn the most about how to test this code. In our example, there is a “zero” case:\n\n@Test void openOrderReport_WhenNoOpenOrders() {\n\nAccount unusedTestAccount = null; ReportGenerator generator = new ReportGenerator(unusedTestA generator.formatOpenOrderReport(new Order[] {});\n\nfail(\"We're not done yet!\");\n\n}\n\nThe call to org.assertj.core.api.Assertions.fail() reminds us that we have yet to set up a way to capture the output. Without this, the test will “pass” and a developer might be led into a false sense of confidence. Without a better assertion, there is nothing tested.\n\nNote that we’re passing in null, not as a String account number, but as an actual Account object. We want to be able to create a ReportGenerator while skipping the FinancialDatabase query. To get that change to compile, another straightforward surgical refactoring is needed that “chains” the constructors (that is, one constructor calls another). This is very much like Extract Method but focuses on a constructor.\n\npublic ReportGenerator(String accountNumber) { this( FinancialDatabase.QueryAccount(accountNumber) ); }\n\npublic ReportGenerator(Account account) { this.account = account; }\n\nWhen the test runs, the output \"--- [You have no open orders]\" goes to the console. We need a way to capture that output for the test, without adding new behaviors to the implementation. System.out is a PrintStream, which is fortunately not a final class. (If it were final, we would need to build a wrapper around it, as described later in the “Introduce Virtualizing Proxy” section).\n\nIn JUnit:\n\nprivate PrintStream originalConsole; private ByteArrayOutputStream actualOutputBuffer;\n\n@BeforeEach void redirectOutput() {\n\noriginalConsole = System.out; actualOutputBuffer = new ByteArrayOutputStream(); System.setOut(new PrintStream(actualOutputBuffer));\n\n} @AfterEach void restoreOutput() {\n\nSystem.setOut(originalConsole);\n\n}\n\n@Test void openOrderReport_WhenNoOpenOrders() {\n\nAccount unusedTestAccount = null; ReportGenerator generator = new ReportGenerator(unusedTestA generator.formatOpenOrderReport(new Order[] {}); assertThat(actualOutputBuffer.toString()).isEqualTo( \"--- [You have no open orders]\\n\");\n\n}\n\nIn MSTest for C#, temporarily redirecting console output to a string looks very similar:\n\nprivate TextWriter originalConsole; private StringWriter actualOutputBuffer;\n\n[TestInitialize] public void RedirectOutput() {\n\noriginalConsole = Console.Out; actualOutputBuffer = new StringWriter(); Console.SetOut(actualOutputBuffer);\n\n}\n\n[TestCleanup] public void RestoreOutput() {\n\nConsole.SetOut(originalConsole);\n\n}\n\nUse BeforeEach and AfterEach for this sort of runtime bracketing in case the tested legacy code surprises you with an unexpected exception. An exception will cause the test to fail, but AfterEach will still put your environment back into a stable state. Note that this technique does introduce the Tear Down test smell described in Chapter 5, but it need linger only until you refactor the implementation to a better design.\n\nContinue to build tests until you have full behavioral coverage. This example needs just one more test to cover all behaviors. Rather than attempting to manually assemble the complex expected report output, let a test failure tell you exactly what is expected:\n\n@Test void openOrderReport_WithRepresentativeSample() {\n\nAccount unusedTestAccount = null; ReportGenerator generator = new ReportGenerator(unusedTestA\n\nOrder[] variousOpenOrders = new Order[] { new Order(\"B3AAE\", \"MARKET\", \"OPEN\", \"LXEGF\", 1000, new Order(\"F5A62\", \"STOP\", \"OPEN\", \"LXODP\", 200, 22 new Order(\"6BE11\", \"LIMIT\", \"OPEN\", \"LXSPL\", 4, 186 }; generator.formatOpenOrderReport(variousOpenOrders); assertThat(actualOutputBuffer.toString()).isEqualTo( \"Inconceivable!\");\n\n}\n\nIt fails, of course. To do otherwise would be absolutely, totally, and in all other ways, “Inconceivable!”3 (Figure 7.2).\n\n3 https://en.wikipedia.org/wiki/The_Princess_Bride_(film)\n\nFigure 7.2 A failing characterization test reveals the actual results of the behavior.\n\nCopy and paste the actual results into the test and run it again. You might need to make minor formatting and whitespace adjustments so that it will pass:\n\n@Test void openOrderReport_WithRepresentativeSample() {\n\nAccount unusedTestAccount = null; ReportGenerator generator = new ReportGenerator(unusedTestA\n\nOrder[] variousOpenOrders = new Order[] { new Order(\"B3AAE\", \"MARKET\", \"OPEN\", \"LXEGF\", 1000, new Order(\"F5A62\", \"STOP\", \"OPEN\", \"LXODP\", 200, 22 new Order(\"6BE11\", \"LIMIT\", \"OPEN\", \"LXSPL\", 4, 186 }; generator.formatOpenOrderReport(variousOpenOrders); assertThat(actualOutputBuffer.toString()).isEqualTo(\n\n\"| Order | Type | Status | Symbol | Shares | Pr \"| B3AAE | MARKET | OPEN | LXEGF | 1000 | \"| F5A62 | STOP | OPEN | LXODP | 200 | 2 \"| 6BE11 | LIMIT | OPEN | LXSPL | 4 | 186\n\n);\n\n}\n\nOnce a test passes the first time, run it again. If a characterization test gives back different results each time, that indicates some dependency isn’t yet under the test’s control via a test double.\n\nof consistently, Both formatOpenOrderReport() are fully tested. It can now be refactored or altered using TDD. For example, if we wanted to center the “Type” column data, we could change the expected output in the test and then play the TDD game from that point forward.\n\ntests\n\nnow\n\npass\n\nand\n\nthe\n\nbehaviors\n\nOther Considerations\n\nWith characterization testing, you are patching your team’s safety net and shortening the length of the feedback loop between making a mistake and identifying that mistake, particularly if you were previously relying on manual test cases. Whenever you are writing characterization tests, keep the following considerations in mind.\n\nTest All Paths and All Results\n\nYour own legacy code will probably be much more complex than our simple example. Remember to iterate over all the behaviors you’ve identified with the first of the Three Questions, creating characterization tests that capture all the results of each behavior.\n\nMake a note of everything that happens within a single scenario. Add assertions for all results, including logging, state changes, events raised, console output, delegations, and so on.\n\nMaximize Coverage While Minimizing Effort\n\nIf you discover a way to increase behavioral coverage and shorten the test cycle with less of the team’s efforts, try it out.\n\nIt’s common for a characterization test to cover multiple behaviors. Suppose your code performs a complex calculation, chooses a code path based on those results (for example, a switch statement), writes the results to the console, and stores the results in a spreadsheet. That entire series of behaviors can be tested with as few tests as there are optional code paths (for example, the number of cases in a switch).\n\nYou can write each test to weave a different path through the code and cover different behavioral boundaries. Some behavioral boundaries might get tested more than once, and that’s fine.\n\nDepending on the complexity of the legacy code, you might not know which behavioral boundaries you are testing until the test passes. It’s not always easy to tell which branch of code will be taken based on the given parameters and the state of the tested object. Capture the actual results and use them as expected results. For your first test of a region of code, any path through the code is a good one.\n\nRefactor Characterization Tests, Too\n\nOnce one or two characterization tests are passing, look for test smells and refactor to eliminate them.\n\nCharacterization tests are often longer than unit tests because they test multiple behaviors. Even so, you should not leave behind test smells that are easy to repair. For example:\n\nMake any arbitrary inputs or mocked interactions more explanatory.\n\nAdjust the inputs so they are right up against the behavioral boundaries. For example, if you tested code that includes if (x > 12) with x set to 4 and 42, instead change these values to 12 and 13.\n\nRename the test based on the scenario you’ve captured.\n\nRecognize That This Is Not Test-Driven Development\n\nCharacterization testing sometimes feels like TDD. After all, you write a test that fails, and then you make it pass. However, when you’re writing a the the characterization implementation’s behaviors, not the other way around.\n\ntest, you continue\n\nto fix\n\ntest\n\nto match\n\nWear One Hat at a Time\n\nConceptually, you have a “test-driven hat,” a “refactoring hat,” and now a “characterization testing hat.” You can fit only one hat atop your head at any moment.\n\nRecall that with TDD, you wait until the tests are passing before you tackle refactoring. Similarly, whenever you’re engaged in characterization testing, you should avoid trying to do two things at once: Do not change or add any behavior to your system while writing characterization tests.\n\nThat includes adding logging or output buffering to the implementation to facilitate the testing. In the ReportGenerator legacy example, we didn’t add behaviors to the legacy code. Instead, the test redirected standard output to a buffer that only the test could access directly.\n\nWhen You Find a Defect\n\nOften, while writing characterization tests, a team will spot a mistake in the code. It might be a known defect that has eluded them, something intermittent, something that users have been working around, or all three.\n\nThe team is often shocked. “How did this ever work?!” is a question I’ve heard many times.\n\nIf you find a defect, resist the temptation to fix it outright. Instead, capture it in a repeatable characterization test to assure the defective behavior remains intact. Give the test a name or description that clearly states the behavior’s defective nature. If you log the defect in your queue (for example, a backlog or tracking system), add the defect’s identifier into the description, the name, or a comment in the test.\n\nWhy not fix the defect then and there? Recall that characterization testing is meant to capture existing behavior and lock it down while you refactor. Fixing a defect alters behavior and may have serious ramifications in another part of the system. There could be an existing workaround for that defect. If you fix it, you could break the workaround in your system or in your clients’ systems.4\n\n4 “Impossible!” said one student when I used a “one-off error” to explain this. (A one-off error occurs when someone familiar with Pascal or Fortran, for example, momentarily forgets that modern programmatic arrays start at index zero.) One of his teammates immediately interrupted, “Are you kidding? That’s literally the problem we saw last week!”\n\nSo, even if a defect appears to require just a simple fix, it may be more complex, require detailed documentation, or simply not be a priority at the moment. Given these possibilities, it’s best to make the whole team— particularly your product advocate—aware of the defect. This gives the team the opportunity to prioritize the repair in relation to all other defects, requests, and enhancements.\n\nIntroduce Virtualizing Proxy\n\nMost legacy code is tightly coupled to its external dependencies. This shows up as multiple calls, strewn haphazardly throughout the code (for example, the same sequence of JDBC calls within every data access object).\n\nIn most cases, your mock framework will help you get that code covered. Most mock frameworks create a temporary subclass of the dependency as a “dynamic proxy” that intercepts each call and performs whatever actions it\n\nwas trained to do. That means that they can stub out only calls that are already virtual (that is, overridable).\n\nThen there are the “worst case” external dependencies: Java final classes, .Net sealed classes, pure-virtual C++ classes, and classes with static methods. Calls to these classes cannot be easily stubbed, so replacing them with compatible test doubles is a challenge.\n\nIntroduce Virtualizing Proxy is a surgical refactoring with the sole purpose of making non-virtual external methods appear virtual again (from the caller’s perspective). That allows you to create a test double of the proxy.\n\nTo illustrate this technique, we’ll continue with our simple Java example. Here’s the code we’ll be working with (the changes from the previous example are emphasized):\n\npublic void openOrderReport() {\n\nOrder[] openOrders = FinancialDatabase.QueryOrders( \"open\", account.accountNumber(), java.time.Instant. formatOpenOrderReport(openOrders, System.out);\n\n}\n\npublic void formatOpenOrderReport(\n\nOrder[] openOrders, PrintStream output) { if (openOrders.length == 0) { output.println(\"--- [You have no open orders]\"); } else { output.format(\"| %8s | %8s | %8s | %8s | %8s | %8s |%n\" \"Order\", \"Type\", \"Status\", \"Symbol\", \"Shares\", \"Pri for (Order order : openOrders) { output.format(\"| %8s | %8s | %8s | %8s | %8d | %8d order.id(), order.type(), order.status(), order.symbol(), order.shares(), order.appropriateDisplayPrice()); } }\n\n}\n\nNote that an Introduce Virtualizing Proxy refactoring isn’t necessary to replace a PrintStream (it’s not a final class). Even so, it can still help us\n\ndecouple the business behaviors from the external dependencies.\n\nFigure 7.3 shows the structure of the relationship between ReportGenerator and its PrintStream before the Introduce Virtualizing Proxy refactoring.\n\nFigure 7.3 UML showing the original relationship between the legacy code and its external dependency.\n\nFigure 7.4 shows the results of the refactoring, plus test code that is easier to write once the refactoring is complete. Note that the new OutputProxy class is not a test double, but rather is part of the deployable implementation.\n\nFigure 7.4 UML showing the addition of the proxy and the test code.\n\nThe sections that follow describe the steps to this refactoring.\n\nStep 1: Create a Virtualizing Proxy\n\nCreate a new class within your implementation project (that is, not in your test project). For each method that the legacy code calls, add a method with the same method signature and return the external dependency’s interface exactly and make the methods virtual. Do not replicate any methods your system is not yet calling; you can always add them later if needed.\n\ntype. Match\n\nIn Java:\n\nimport java.io.PrintStream;\n\npublic class OutputProxy {\n\nprivate final PrintStream originalPrintStream;\n\npublic OutputProxy(PrintStream original) { this.originalPrintStream = original;\n\n}\n\npublic void println(String s) { originalPrintStream.println(s); }\n\npublic void format(String format, Object... arguments) { originalPrintStream.format(format, arguments); }\n\npublic boolean checkError() { return originalPrintStream.checkError(); }\n\n}\n\nHere’s a similar proxy for .Net’s Console.Out TextWriter. Note the use of the virtual keyword in each method.\n\nIn C#:\n\npublic class OutputProxy {\n\nprivate readonly TextWriter originalTextWriter;\n\npublic OutputProxy(TextWriter original) { originalTextWriter = original; }\n\npublic virtual void WriteLine(string s) { originalTextWriter.WriteLine(s); }\n\npublic virtual void WriteLine( string format, params object?[] arguments) { originalTextWriter.WriteLine(format, arguments); }\n\n}\n\nStep 2: Replace the Original with the Proxy\n\nIn your legacy implementation, replace the original with a proxy that wraps the original. This must happen before the call to the method containing the\n\nbehaviors to be tested; otherwise, you will not be able to inject a test double. If the external dependency is passed into the extracted method, then this change is very straightforward. All changes (there should be only two!) are emphasized here:\n\npublic void openOrderReport() {\n\nOrder[] openOrders = FinancialDatabase.QueryOrders( \"open\", account.accountNumber(), java.time.Instant. formatOpenOrderReport(openOrders, new OutputProxy(System.ou\n\n}\n\npublic void formatOpenOrderReport(\n\nOrder[] openOrders, OutputProxy output) { if (openOrders.length == 0) { output.println(\"--- [You have no open orders]\"); } else { output.format(\"| %8s | %8s | %8s | %8s | %8s | %8s |%n\" \"Order\", \"Type\", \"Status\", \"Symbol\", \"Shares\", \"Pri for (Order order : openOrders) { output.format(\"| %8s | %8s | %8s | %8s | %8d | %8d order.id(), order.type(), order.status(), order.symbol(), order.shares(), order.appropriateDisplayPrice()); } }\n\n}\n\nThe proxy’s interface looks like the original, but they are not of the same type. By changing the parameter type, we’ve effectively changed the entire method to call the new proxy without having to change anything in the method body.\n\nWill you need this surgical refactoring? Consider these points:\n\nThis refactoring is particularly valuable if your legacy code calls non-virtual methods on an external dependency. Otherwise, you can use your mock framework to test all relevant behaviors, and then safely perform any refactorings you need to clean up the design.\n\nThis refactoring is helpful when you are testing long blocks of legacy code that repeatedly access the same dependency. If you follow all steps carefully, you can minimize the amount of code that needs to change before you can add tests.\n\nIt helps test legacy code by consolidating challenging calls made to the external dependency’s type. This refactoring introduces a new class—the proxy class—into your implementation. This class resists unit testing for all the same reasons that the external dependency made the original legacy code difficult to test. Avoid adding any other behaviors to your thin new proxy class besides delegating to the external dependency and returning whatever it returns. You can rely on the compiler to check that all parameter types and return types match those of the external dependency.\n\nYou might not have another choice. Many teams, for various reasons, cannot safely upgrade their environment. If your team does not have access to the latest mock framework and/or latest version of the programming language, you might not have another way to stub non-virtual methods. Besides, wrapping an external dependency to decouple it from your business domain is a positive step toward a cleaner and more testable architecture.\n\nUsing a Test Double of the Proxy in a Test\n\nWith the refactoring complete, you can write characterization tests more easily. Note that whether you hand-craft a stub or use your mock framework, you must not inject any “original” external dependencies into these tests. Instead, pass null into the test double’s constructor. If you forget to override one of the proxy’s methods with your test double, your test will remind you by failing due to a null-pointer/null-reference exception or an error from your mock framework.\n\nHere’s an example OutputProxy spy (Java) and test:\n\npublic class OutputProxySpy extends OutputProxy {\n\nprivate StringBuilder outputBuffer = new StringBuilder(); public OutputProxySpy() { super(null); }\n\n@Override public void println(String s) { outputBuffer.append(s); }\n\n@Override public void format(String format, Object... arguments) { println(String.format(format, arguments)); }\n\n// for spying purposes only public String readAllOutput() { return outputBuffer.toString(); }\n\n}\n\n@Test void openOrderReport_WithRepresentativeSample() {\n\nAccount unusedTestAccount = null; ReportGenerator generator = new ReportGenerator(unusedTestA\n\nOrder[] variousOpenOrders = new Order[] { new Order(\"B3AAE\", \"MARKET\", \"OPEN\", \"LXEGF\", 1000, new Order(\"F5A62\", \"STOP\", \"OPEN\", \"LXODP\", 200, 22 new Order(\"6BE11\", \"LIMIT\", \"OPEN\", \"LXSPL\", 4, 186 }; OutputProxySpy spy = new OutputProxySpy(); generator.formatOpenOrderReport(variousOpenOrders, spy); assertThat(spy.readAllOutput()).isEqualTo( \"| Order | Type | Status | Symbol | Shares |\n\n+ \"| B3AAE | MARKET | OPEN | LXEGF | 1000 | + \"| F5A62 | STOP | OPEN | LXODP | 200 | 2 + \"| 6BE11 | LIMIT | OPEN | LXSPL | 4 | 186\n\n);\n\n}\n\nSummary: The Complete Toolbox\n\nThe techniques described in this chapter give developers the ability to “patch” their safety net wherever coverage is insufficient. Once behavioral coverage has been improved for a region of code requiring change, refactoring and TDD can proceed.\n\nWith TDD, refactoring, test doubles, and characterization testing all in your toolbox, you can now test, build, reshape, and enhance your high-quality software with complete confidence.\n\nChapter 8, “The Black Swans,” presents my first-person, longer-term experiences with these practices, which ultimately led to surprising value being delivered through our diligent, test-driven efforts. It’s a good chapter to show to your concerned managers or the skeptical architect.\n\nI hope that you, too, get to experience the satisfaction of delivering surprising levels of value and the pride in your team’s software design. Happy coding!\n\nOceanofPDF.com\n\nPart III: Return on Investment\n\nOceanofPDF.com\n\nChapter 8. The Black Swans\n\nFor a Test-Driven Development practice to be successful, two things are required. First, the team must be willing to give it their best effort for about a month. Second, they must have leadership support.\n\nBy “support,” I don’t mean just verbal permission. With any new practice, the team needs time to learn the best ways to follow the practice, time to experiment with the practice, and time to see some of the short-term benefits.\n\nWithout leadership support, TDD will not take root. Without some form of iteratively and team building test-driven discipline, a incrementally will rapidly lose the ability to refactor confidently, and over time will have to work harder to produce less valuable functionality.\n\nsoftware\n\nThat is what I call the Agilist’s Dilemma: A team attempting to use an incremental methodology to keep their product open to fluctuating business requirements, but without using developer practices designed to alter code confidently and continuously, will experience deteriorating quality and decreasing delivery of value. In this scenario, all of the benefits the organization expected to gain by building iteratively and incrementally are lost.\n\nOne way to garner better leadership support is to present case studies. Before I was an instructor of TDD, I was a practitioner, and this chapter documents my own experiences on test-driven teams. My hope is that these stories will help illuminate the business value of TDD and encourage teams and their leadership to take the time needed to assimilate a solid TDD discipline.\n\nThe Agilist’s Dilemma\n\nTo the best of my recollection, the brief conversation went something like this:\n\nDeveloper: “I’ll need to make one small change.”\n\nTester: “What will I need to test after that?”\n\nDeveloper: “It’s not a behavioral change, just a refactoring, so nothing,\n\nreally.”\n\nTester: “This is a change to how we access the database?”\n\nDeveloper: “Yep!”\n\nTester: “How many features could be impacted if something goes\n\nwrong?”\n\nDeveloper: “Well, it’s a fundamental part of the architecture, so all of\n\nthem?”\n\nTester: “So, I should probably test everything.”\n\nDeveloper: “Uh, yes.”\n\nFrom “No worries, test nothing” to “We’ll need to regression test everything” in less than 5 minutes! This dialog demonstrates the Agilist’s Dilemma.\n\nThe Vanilla Agile Team\n\nTeams building software iteratively and incrementally are doing just enough architecture, analysis, design, coding, and testing to build what is needed now. In other words, they focus on building what is likely the most important stuff.\n\nHowever, I’ve coached or trained dozens of teams that were not yet using any test-driven practice. Their description of what happened always follows a similar pattern.\n\nAt first, everything went fine. Developers built just enough, and testers tested those behaviors. No problem.\n\nAfter roughly a half-dozen iterations (that is, Scrum’s “sprints”), developers found themselves having to constantly modify existing code. On a newer product, the most accessed and most critical code often changes most frequently. That can be scary.\n\nNo professional developer wants to introduce defects! So, they slow down, becoming increasingly cautious about changing existing code. Or they copy/paste/modify similar existing code and introduce a branching statement to select either the existing code path or their new code path. This approach can seem much safer, but it degrades the quality of the design— that is, the ability to adapt to changes later.\n\nBecause the developer is becoming increasingly cautious, and the design is becoming less maintainable, requests for new features take even longer. Developer stress levels increase commensurately.\n\nTesters feel a similar increase in stress levels. Not only do they need to test the latest iteration’s new features, but also all behaviors implemented since the inception of the product. Eventually, the whole team is working nights and weekends to match their original pace. Or they cut corners— consciously or otherwise—which invariably introduces defects.\n\nWorking Without a Safety Net\n\nWhen a safety net of tests isn’t built alongside the software, the internal software design becomes brittle, requests take longer, and the product receives far less regression testing than it deserves. Despite the team’s best efforts, defects start popping up everywhere. The team is experiencing the Agilist’s Dilemma.\n\nOf course, the solution to the Agilist’s Dilemma is not to go back to a phased approach that would leave testing to the end, when it’s far too late. Rather, the best approach is to use the iterative and incremental practices described in this book to build and maintain a safety net of tests. And whereas most “agile” teams consider a week or two to be short timeboxes,",
      "page_number": 209
    },
    {
      "number": 8,
      "title": "The Black Swans",
      "start_page": 237,
      "end_page": 260,
      "detection_method": "regex_chapter_title",
      "content": "they must embrace skills that will let them develop working software with feedback cycles measured in seconds or minutes.1\n\n1 The exception being characterization testing. Occasionally, I’ve witnessed a team take hours to write the first of a few good characterization tests—but then the second one takes only a few minutes.\n\nThe Three Levels of Value\n\nThree interrelated and overlapping tiers of value can be realized when developers embrace a disciplined approach to TDD:\n\nLevel 1: A dramatic reduction in defects and rework\n\nLevel 2: Reduced feature time-to-market\n\nLevel 3: Implementing the unexpected\n\nThe third level—the long-term power of that safety net of regression tests —usually arrives as a pleasant surprise.\n\nLevel 1: A Dramatic Reduction in Defects and Rework\n\nThis is the most familiar of the three levels, and the most immediate. Because the team thinks in terms of concrete examples and records their expectations as automated tests, they prevent defects from getting into the software by instantly catching common mistakes.\n\nFor example, the Password Checker Lab in the Appendix has a rule that a password must be eight characters or more. If developers first try coding without tests, often they will write the following (in pseudocode):\n\nif password.length() > 8\n\nreturn true\n\nelse\n\nreturn false\n\nThat’s a defect, because 8 is not greater than 8. The true behavioral boundary exists between 7 and 8, not between 8 and 9.\n\nTDD avoids this problem by giving the developers a way to record the sample inputs and expected results as they think of them, before concerning themselves with the code to make each example pass.\n\ncheck length \"12345678\" → true check length \"1234567\" → false\n\nMistakes can still happen with TDD. But that’s the point: Most mistakes are caught within minutes, usually before code is committed to the repository. If the developer is taking small, rapid steps, then the developer knows what caused the error and who created it: They did!\n\nLocating and fixing the defect immediately is far less wasteful than waiting months before a tester or customer discovers it, logs it, tries to figure out who is to “blame,” and assigns the defect to a developer.\n\nI have met with many teams before they’ve embraced TDD who estimate that they spend half their time debugging, finding, and fixing defects. Half! That’s a horrible waste of time and professional talent.\n\nThe team must also consider the potential damage a single defect can cause to a customer’s business and the development organization’s reputation. Defects are too costly to leave to random chance.\n\nLevel 2: Reduced Feature Time-to-Market\n\nWhen teams are diligent about following TDD practices, the development of features will not slow down over time.\n\nAt Level 1, the team catches mistakes immediately through each unit test they add to the suite of tests. Once it is passing, the unit test assures the preservation of that single unit of behavior. Each time the full test suite runs (dozens or hundreds of times per day), it assures the preservation of all existing behaviors.\n\nWith that safety net of fast and comprehensive tests always in place, the team can confidently reshape the internal software design to make room for innovative new features, knowing they will not break anything. Because their confidence does not degrade over time, a request’s cycle time (the time it takes to complete the request from start to finish) won’t change\n\nsignificantly, whether it’s the first request the team ever builds or it’s built two years later.\n\nThat ability to refactor the design is essential to software development and is the only way teams can avoid the Agilist’s Dilemma.\n\nLevel 3: Implementing the Unexpected\n\nEventually (perhaps in six months, perhaps in two years), all of the team’s efforts may suddenly pay off, with interest. There are no guarantees, of course, but in my experience, the “unexpected feature request” will arrive.\n\nThe unexpected feature is frequently an innovative pivot, a wildly insightful customer request, or a clever repurposing. If implemented successfully, the feature opens up a whole new market segment, improves the flow of value in another area of the organization, or greatly aids in retaining critically important customers.\n\nWhether you will welcome the unexpected or whether it will topple your enterprise will depend on the state of your software design when it arrives.\n\nWhat follows are stories from my own experience as a software developer and XP coach. For every team I’ve been on that stuck diligently to TDD for six months or more, the third level of value was unlocked.\n\nThe Black Swan User Stories\n\nThe phrase “Black Swan” is used in Nassim Nicholas Taleb’s book The Black Swan (Random House, 2010) to describe an event that is rare, unforeseen, unpredictable, and very disruptive (in either a positive or negative way). Though they are unplanned and unpredictable, according to Taleb Black Swans are—given an appropriate length of time—inevitable.\n\nFukushima Recent earthquake/tsunami/nuclear-meltdown triple-gut punch and the COVID-19 pandemic. However, Black Swan events aren’t always so unpleasant: The discovery of penicillin, the smallpox vaccine, and simple hand soap have all had a dramatic positive effect on our lives.\n\nwell-known\n\nexamples\n\ninclude\n\nthe\n\nSo, a Black Swan user story (my term, combining Taleb’s term with Kent Beck’s “user story”) is a request for a change to a software product that is unplanned, unexpected, potentially “game-changing,” possibly very expensive to implement, and potentially extremely valuable. Depending on the state of the system that encounters a Black Swan event, it can be extremely costly, extremely valuable, or both.\n\nTaleb describes systems that improve when clobbered with such an event as “antifragile.” Contrast that term with “resilience,” which describes the condition in which the attacked system rebounds to a relatively steady state. An antifragile system goes one step further than a resilient one, and actually improves by learning and adapting to its new reality.\n\nWhat would it mean for software to be “antifragile”? If the Black Swan user story is properly implemented, the code will become even more flexible and potentially able to handle future requests more easily. It will have a similar effect on a stable team: The Black Swan will strengthen their confidence in their own abilities and will create trust among teammates, leaders, and customers.\n\nThe following tales are from my own first-person experience on long-term XP teams doing continuous, disciplined TDD. The shortest time I spent with these clients was six months. Our days, weeks, and months were filled with as close to full-time TDD as one can get, while still allowing us to take vacations and holidays.\n\nAfter months of disciplined TDD and diligent refactoring, these teams encountered a user story placed at the top of their queue that would have taken the average development team months to build (if at all). Our team completed it in far less time, usually within a single two-week iteration.\n\nThe following tales are all true, and the details are accurate to the best of my recollection. The dialog, however, has been improvised for brevity.\n\nA Major Architectural Change\n\nIn brief: Internationalize an entire suite of servlet applications. Done in three days, and about a dozen lines of code.\n\nIn 2001, James Shore and I were working on Novell’s Developer Portal. It was a series of applications deployed through a few Java servlets. When I got there, Jim and his team had been using XP practices for a while, and I was very impressed with the results.\n\nIn fact, as I told him then, it was the easiest code I had ever had the pleasure to work in, so far. Before joining Jim’s team, I had been subjected to EJBs, JSP pages, and numerous other failed attempts to streamline web application development. The brilliance of Jim Shore was that he never took a framework at face value. He would often come up with a simpler, home-brewed way to accomplish what those frameworks failed to deliver.\n\nAt the time, Jim and I were the whole Portland portion of the team; much of the team was located in the Salt Lake City area. We would fly to Salt Lake City every two weeks for planning. I clearly recall the day this Black Swan user story flew into the team room.\n\n“I know we agreed we’d only support languages represented by the Latin American character sets,” said the Product Advocate, “but we have an opportunity to make inroads into Asian markets, if only we could support Unicode character sets. So ?”\n\nAll static text in our app was already internationalized. The browser would tell the app the desired character set, and the app would serve up a page from the appropriate folder. Static text was translated by professional translators.\n\nThe challenge exposed by this user story was particularly noticeable in our “survey” app. This app allowed the user to build survey questions and multiple-choice answers. But Asian markets were limited to writing surveys in English (or French, or Spanish ).\n\nBeing new to the team, I felt my heart sink. “Oh, no!” I thought to myself, “What have I gotten myself into?! Internationalization! A major architectural change, that is!”\n\nThe team took a brief look at the code. Jim suggested an estimate of three days. I nearly fell out of my chair.\n\n“Are you kidding?!” I started, “To retrofit internationalization into all our servlets and all our pages and ???” I had flashbacks of poorly architected JSP-heavy dot-com apps, and needing to change every single text box and every line of dynamic text.\n\nJim volunteered to take that Black Swan back to Portland with us. The rest of the small team seemed happy to not have to deal with this user story. At first, I felt as they did: “Why didn’t we just turn this one down? This is going to be a disaster!” But Jim’s unexplained confidence was contagious, and by the time I was back in Portland, I was happy to tackle the challenge.\n\nWe did it in three days! In fact, we spent about 2.5 of those days doing research.\n\nIn 2001, there weren’t many web apps that could do what we were hoping to do. We experimented by registering with various well-known sites using some random international phrase (for example, “(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)”), and the app would respond with “Thank you for registering, ☐☐☐☐☐☐☐☐☐☐!”\n\nWe found only one site that responded correctly,2 which proved that it was possible. Then we set out to figure out exactly how to do it in Java. All of our web searches resulted in nothing. Even though Java was touted as “all Unicode, all the time,” apparently no one at that time had publicly documented the way to preserve the whole Unicode character from browser to database and back to browser again.\n\n2 Remember BabelFish.com? It’s still out there, though it looks and feels a bit dated.\n\nWe read carefully through the massive Java J2EE documentation regarding input and output streams and how to chain them together. Within minutes, we had our first passing unit test. We then thoroughly tested that code to make sure it worked with all types of character sets, old and new, 8-bit and 16-bit.\n\nThree days. Oh, we had some luck on our side, too: It turned out to be relatively easy to convert an Oracle database from 8-bit to 16-bit. You shut it down, alter a single configuration parameter, and restore all the data from your backups. So, three days of development time, plus a brief maintenance outage over the weekend, and we were up and running the next week.\n\nHow did we change all the code, pages, and servlets in half a day? It took only about a dozen lines of code, in one or two classes (Figure 8.1).\n\nFigure 8.1 My notes from 2001 on how to convert UTF8 to Unicode in Java.\n\nPart of doing TDD well is diligent, thoughtful refactoring. Long before I showed up, the team had continuously refactored away duplication and other code smells as needed, rather than waiting until “we have the time.” The design was nearly ideal for what the system was asked to do up until that point.\n\nAt some point, refactorings had led the team to isolate all user input and all generated output through one or two classes and two methods. So, when it came time to support Unicode user input and output, that code was already isolated in those two methods. This isolation had not been an upfront architectural decision. Instead, the design had been altered over time as the team worked to make maintenance easier and unit testing more expressive. It was an elegant example of “emergent design”: when simple guidelines and efficient practices, repeated with discipline, result in better outcomes than any upfront design technique could create.\n\nRoughly a dozen lines of code in three days doesn’t sound all that productive, does it? Yet this is one of the clearest examples of how lines-of- code is an entirely inappropriate measure of value delivered. Our software was now much more valuable to customers in China, Japan, South Korea, and others.\n\nAfterwards, I clearly recall thinking, “That was neat! But I doubt I will ever see something like that happen again in my career.” Thankfully, I was wrong. Black Swans may be rare, unpredictable, and expensive, but they’re also inevitable.\n\nOTIS 2\n\nIn brief: Add a functioning “Display in PDF” button to every one of dozens of HTML reports after having committed 6 months to HTML-only development. Done in three days.\n\nMenlo Innovations assigned me to a small team at the University of Michigan that was working on a product called OTIS2. We were tasked with a rewrite of the University of Michigan Organ Transplant Center’s Organ Transplant Information System (OTIS). OTIS version 1 (OTIS1) was written in FoxPro and had reached many limitations of that technology.\n\nFoxPro is not a self-documenting language, and OTIS1 was reaching its 10th year in production. Extracting all the business rules (better described as “critical patient-survival rules”) was going to be a challenge. Menlo and the U of M wanted to rewrite OTIS in Java partly to train the U of M developers in Java, object-oriented design, and Extreme Programming (XP). I was one of two XP developer/coach/architects sent from Menlo.\n\nTo thoroughly test OTIS2, we would run reports in parallel on both systems and compare the report output, letter for letter. In that way, as we developed OTIS2, good old trusted OTIS1 would itself provide detailed functional test scenarios for OTIS2.\n\nAbout six months into the program, after the first early release, the first Black Swan user story arrived, courtesy of the transplant surgeons themselves. They really liked what we were doing with OTIS2. As with OTIS1, however, the reports were all in HTML. Many of these reports would be printed and placed in patient records.\n\nThink back to the World Wide Web in 2001 (or if you’re not old enough to remember, just imagine it!). Whenever most printers printed a web page, that page would come out on two separate sheets of 8.5 × 11-inch paper: one page for the left part of the screen, and another for whatever remained on the right side of the screen. Apparently, the idea of fitting the full screen width into 8.5 inches, or simply printing it in landscape orientation, was not an interesting feature to the browser developers of the day.\n\nSo, when the report was printed, a nursing assistant would take the two sheets (or more, if it was a long report), staple or tape them together where the left and right sides of the report met. They would then fold the report back into a size that would fit into the patient record folder, and place it in the folder.\n\nThe surgeons made a simple request: “We really like OTIS2, but there’s just one small change we’d like: On each report, we’d like you to add a button that says ‘Print me in PDF format.’ We realize that this will impact the release plan, but it’s that important to us.” In other words, it was more important than all other user stories in the release plan.\n\nA whole new output format in addition to HTML, for dozens of unique reports! For most web apps built in the very early 2000s, this would have meant “a major architectural change.”\n\nAs the XP coach (and de facto architect), I knew that we had already done the “unthinkable” and hard-coded HTML tags into our Java presentation layer. Early in the project, we had realized that our reports would differ by columns. Patient data would appear as rows from a database query, but the columns were specific to which organ, or organ combination, was being transplanted: Heart. Heart–lung. Both lungs. Heart-plus-both-lungs. Liver. Liver–kidney. Brain.3\n\n3 A bad joke that we told each other far too often. Just seeing if you were paying attention!\n\nThere were other dimensions as well. The reports would satisfy a particular general query. Who is likely destined to receive the next transplant? Who survived the transplant for more than 10 years? And, presumably, there was enough data in these large reports to help create a hypothesis as to which of the thousands of variables were important to a patient’s survival. Eventually, surgeons could customize the reports by adding special sets of extra columns.\n\nHTML naturally holds cells within rows. But the domain asked us to orient cells within columns. Our business model already had this notion of a Column that would reference the correct Cell of data from each record in the query results. When the app generated HTML output, Report code would walk through the collection of Columns and ask them for the Cell corresponding to the correct row number.\n\nTo summarize what was in place prior to the Black Swan: We took database rows, organized them into columns, and then formatted them into rows. Insane, right? If you’re a fan of code optimization, you may be squirming in your seat as you read this. It was inefficient, but it was all happening in memory. No one ever complained about response time. The system’s bottleneck was always printer speed.\n\nBack to the Black Swan: Our team asked to have an hour or so to do some research. We quickly learned two very important things: (1) Whereas HTML is row-centric, PDF is column-centric, like the business model. (2)\n\nThere was a Java library available that could generate PDF output programmatically.\n\n“Three days,” I said. The stakeholders were both shocked and delighted.\n\nWe spent the first day refactoring. We took all the HTML formatting code and extracted it away from the Report code into something called the HTMLEmitter. We ran all the tests and pushed our changes to the build server.\n\nOn day two, we used TDD to create a PDFEmitter. Also using TDD, we allowed the browser to switch emitters based on something as simple as “? format=PDF” on the URL. Each Report was then initialized with the correct Emitter (an abstract class). It worked almost perfectly at the end of that second day.\n\nOn the morning of day three, we tweaked PDFEmitter (using TDD, of course) to get the cell padding and fonts just right. In two and a half days, we had completed what the surgeons had declared as their most important new feature. It was integrated, exploratory-tested, user-acceptance-tested, audited, and deployed into production by the end of that week. It was in use by the surgeons the very next week.\n\nThe Loan Securitization Pipeline\n\nIn brief: Convert our custom data format to the format used by our primary bank partner.\n\nIn 2003, I worked with a small team that was responsible for maintaining the sole mission-critical software application for our company. This organization was dependent upon one lone revenue stream: We took commercial mortgages and securitized them. We took batches of loans, helped rate them, and then bundled them together into bonds to be resold to investors. We took a fraction of a percentage point on each transaction, and the size and volume of those transactions kept the company quite profitable, at least until the real estate bubble popped in 2008. Everything was managed for the brokers by an internal web application.\n\nThis company had started as a small private startup, and the software was originally built by a single developer, who—I later learned—taught himself\n\nJava while writing the system. It was a mess. He had learned all the clever syntax of Java, and apparently not one iota of real object-oriented programming.\n\nThe system had a few end-to-end tests covering multiple broad scenarios. Helpful, yes—but also cumbersome, fragile, nearly incomprehensible, and terribly slow.\n\nOur first big challenge was to add a new type of loan. The application had been designed top-to-bottom to securitize just one type of loan into one type of security—a commercial mortgage-backed security (CMBS). Despite this fact (or maybe because of it), there was no Loan class in the system. Instead, the data and behaviors for a loan were spread out over numerous classes.\n\nGradually, painfully, and mostly out of necessity, we refactored the Java abomination and its tests. After about six months, the code was starting to look better, and we had a lot more faith in our safety net.\n\nAnd then the Black Swan user story arrived: The bank that handled the actual loans for our company saw an opportunity to streamline the whole process, if only we could exchange loan information in its own XML format. It would mean less tedious data entry for the brokers and more time in the field evaluating actual real estate.\n\nWe were already using XML to represent loans for the application’s report- generation features. But our XML format (or “schema”) was very different than that of the bank.\n\nAt the time, I didn’t see what—perhaps even then—was an obvious solution. Our team estimated that it would take months to alter the Java code to create and use a different schema.\n\n“Well, we gotta do it, so get started!” the manager sighed. “Oh, and you’ll need to train this new hire—just a year out of college!—while you’re at it.”\n\n“Okay,” I thought, “it’ll be trial-by-fire for this poor bloke.”\n\nSo, I sat down to pair with the new guy on his first day. I explained that we needed to rewrite a lot of our XML-emitting Java code, and why, blah blah blah .\n\nHe stopped me and said, bluntly, and with mild incredulity, “Why not just use XSLT?”\n\nI answered, “Well, because I have no idea what that is.”\n\nIn my defense, it was 2003—when XSLT was the hot new tool—and my time was fully occupied being the stressed-out XP developer/coach for a mission-critical but poorly designed financial application. The new developer explained that XSLT could transform text from one format into another (for example, XML into HTML). He was suggesting that we continue to use our format internally and build a translation layer between our system and the bank’s system.\n\nThe changes to the Java code turned out to be the easiest part of the project. We found a very simple XSLT-processing library available for Java. Thanks to all the previous refactoring we had done to add new loan types, we had a single convenient place in our code to pipe XML through the XSLT processor.\n\nThen came the hard part, or so it seemed. XSLT is extremely fussy: You could fix one problem and another would pop up—its effects painfully obvious, but its cause hidden within a huge pile of steaming XML. I fearfully imagined erroneous values placed into incorrect fields, resulting in false bond ratings, lost reputation and revenues, and crushing lawsuits.\n\nOnce the newbie developer taught me a little XSLT syntax, it occurred to me that we could apply TDD to the XSLT code. In production, the Java translator would read the XSLT file and process the stream of XML created by our existing system. In each unit test, we asked the translator to process a small string of carefully crafted XML written right into each unit test. The test would assert that the output string matched our (also carefully written) desired expectations. So, given this XML input, when processed by the full production XSLT file, then the resulting XML should match the expected XML.\n\nBecause XSLT is such a fussy declarative language that does exactly what you tell it to do, I cannot imagine how we could have built it without TDD. Had we tried to write those tests afterward, we would have encountered numerous defects that, when fixed, would have spawned other defects.\n\nOnly in the aggregate of a growing suite of tiny tests could the XSLT be compelled to do all the things we needed it to do, without doing something wrong or adding something “extra” (also very bad for us, and far more difficult to spot manually).\n\nUltimately, we turned that estimated 6-month painful rewrite into a 5- or 6- week educational and fun experience. Our brokers were happy to have the new system deployed almost 20 weeks earlier than estimated, and to start doing more of what they enjoyed doing. And we promoted that “junior” developer to senior developer at his 6-month anniversary. Everybody won!\n\nTaming the Black Swans\n\nThe arrival of a Black Swan user story is likely inevitable for any software program that continues for six months or more. That’s not a guarantee, but that’s been my experience.\n\nIf your team and your code aren’t ready to handle it, the cost of implementing the Black Swan will likely be prohibitive, and it will repeatedly get pushed lower on the queue and never make it into customers’ hands. If we had told the University of Michigan transplant surgeons, “A button? That’ll take six months,” that change would never have been scheduled. Today, medical interns might still be paying for our negligence with papercuts and stapler accidents.\n\nThe Black Swan user stories I’ve encountered have these attributes in common:\n\nEach Black Swan was a surprise to everyone, including the product advocate who thought it up. Surprising, disruptive, potentially quite valuable, and potentially quite expensive.\n\nThe Black Swan resulted from the product advocate’s knowledge of (1) the software’s existing capabilities, (2) the team’s talents and reliability, and (3) the shifting needs of the market.\n\nAt first, it seemed nearly impossible to implement the user story in any reasonable amount of time. Team members saw the Black Swan\n\nas a major architectural change, based on their years of experience prior to test-driven practices.\n\nUpon reflecting on root causes, these teams determined that success was attributed to the following:\n\nThere was never an effort to prepare for some unknown future eventuality. You cannot design in readiness for a particular Black Swan. To continue the metaphor, you can only “keep the lake waters clean.”\n\nThrough diligent refactoring, the software’s design was always appropriate for all existing behaviors, and the team had already consolidated almost all duplicated code into a clear and malleable design.\n\nDisciplined, long-term TDD had helped the team grow and maintain their safety net of fast and comprehensive tests. Every developer acknowledged that the suite would not have been as robust, or as fast, if unit tests had been neglected or retrofitted rather than test- driven.\n\nReal and continuous peer collaboration quickly led to technical insights or discoveries that were key to overcoming the team’s dread at facing the disruptive Black Swan.\n\nSummary\n\nThough we expect Black Swan user stories to be rare, in my career, one has arrived once or twice per year, on average. The arrival time and temperament of each Black Swan cannot be anticipated, and the successful deployment of the user story cannot be guaranteed.\n\nUnfortunately, the opposite is a near certainty. During my own 13 years of experience prior to encountering test-driven practices, and when observing client teams that were either reticent about or discouraged from embracing TDD, I’ve seen this sad outcome all too often: Without a sturdy safety net of tests, the team’s Black Swans were either repeatedly rejected as too\n\nexpensive or risky, or took many months to complete, occasionally missing a critical competitive window.\n\nMediocrity comes with its own punishments. A large organization might absorb the failure, but a small company can be obliterated by it.\n\nTDD and diligent refactoring will keep the code both clean enough for its current purpose and ready to change when necessary. The Black Swans can then arrive at your sparkling lake, land softly, and enrich your life.\n\nThe world of software development is changing rapidly. But whenever I look at the advancements of artificial intelligence/large language models or quantum computing, I still see the need for a human to translate customer needs into concrete, detailed examples. Otherwise, the resulting software remains a mystery, and the misunderstanding reveals itself to a user. The defect—the misunderstanding—could be a mere annoyance, but sometimes it might be life-threatening.\n\nTest-driven thinking, I’m sure, is here to stay.\n\nOceanofPDF.com\n\nAppendix Exercises\n\nThis appendix contains the recommended exercises discussed in the book. The exercises are designed to give you the opportunity to practice Test- Driven Development within a domain that is relatively simple and possibly familiar.\n\nEach exercise has a set of beginning tasks, followed by some extended options. The goal is not to build an entire application, but rather to get familiar with TDD and to notice any resistance to testing you encounter, either in the code or in yourself. These exercises also give you ample opportunities to practice refactoring to overcome the resistance.\n\nPassword Strength Checker\n\nThis is a good TDD exercise for exploring your design preferences and tolerance for code smells. Read the requests carefully and complete them all, even if some might seem a bit silly.\n\nPassword Strength Checker: Part 1\n\nUse TDD to develop a simple, in-process, easy-to-use API to check password strength. The caller shouldn’t need to know which checks are being done on the password string. A simple Boolean return value of true (strong enough) or false (too weak) is wanted.\n\nAs you write this code, notice whether a previous test fails when you make another test pass, and which steps you take to alleviate that issue.\n\nTo be an acceptably strong password, a string must:\n\nHave a length greater than 7 characters\n\nContain at least one alphabetic character\n\nContain at least one digit\n\nOnce you’re finished with this exercise, consider this question: If you were asked to alter this API, what sorts of changes would now be easy to test and develop, and which kinds would be more difficult to test and develop?\n\nImportant! Please do not turn the page and proceed to Part 2 until you have completed Part 1.\n\nPassword Strength Checker: Part 2\n\nThe client needs enhancements to your existing API, but they are hoping the new API will remain backward compatible with sites they’ve already built. Two enhancements have been requested:\n\nSome clients want a way to obtain a list of all the reasons why a password was not strong enough.\n\nSome clients want to be able to pass a Boolean “Admin” flag to the API. If this flag is set to true, the password must also meet the following criteria:\n\nBe > 10 characters long\n\nContain a special character\n\nHave a special character or digit as the last character\n\nOnce you’re finished with this exercise, consider the following questions:\n\nWhat did you do to remain backward compatible with Part 1? Were there other options that you considered, or that you would try if you had to start over?\n\nDepending on your choice of solutions, did you consider using test doubles? Did they help—or would they have helped—get all behaviors thoroughly tested?\n\nSalvo\n\nSpend as little or as much time as you want on this exercise. The point isn’t to finish, but rather to practice TDD.\n\nUsing TDD, build your own Salvo1 game. Start with a fresh code repository. Be sure to commit whenever all tests are passing, and push to the repo at least whenever you complete a task.\n\n1 https://en.wikipedia.org/wiki/Battleship_(game)\n\nThe basic rules of the game are as follows. Please read through all instructions before starting on the exercise:\n\nThere are two players.\n\nEach player has a 10 × 10 board representing their region of the sea.\n\nEach player’s ships are arranged horizontally or vertically (and secretly).\n\nA player’s ships cannot contact or overlap each other (see Figure A.1).\n\nShips vary in size.\n\nPlayers enter salvo coordinates to try to hit the other player’s ships.\n\nWhen all squares of a ship are hit, the ship is sunk.\n\nBoth players are notified if a salvo is a hit or a miss, and whenever a ship has sunk.\n\nThe player who sinks all of the other player’s ships wins.\n\nEach player starts with the same-sized fleet of ships. For example, these five:\n\nCarrier, size 5\n\nBattleship, size 4\n\nCruiser, size 3\n\nDestroyer, size 2\n\nSubmarine, size 1\n\nFigure A.1 A player’s board representing each ship as a gray rectangle and each previous strike location as an X.\n\nHere’s a suggested approach:\n\n1. Start with the domain model (no UI, no persistence). Do a little paired whiteboard UML or create CRC cards. Set a timer for 10–15 minutes —just long enough to identify some clear behaviors. Think about which behaviors you want, rather than which data structures you might use to implement them.\n\n2. Get the basics tested. Test behaviors and let objects and data structures emerge to support those behaviors.\n\n3. Choose from some of the extended options.\n\nThe extended options (in no particular order) are as follows:\n\nHave the game randomly place the ships on the boards for the players. Prevent a player’s ships from overlapping or being adjacent to each other (see Figure A.1 for an example).\n\n“Autopilot”: Have the software be one of the players. This should not require an AI or fancy decision tree: Walk through your own strategies and have the game mimic those. Note that the game needs\n\nto know what the human player’s board looks like, but “Autopilot” should not.\n\nLet players choose a “short” game (fewer ships) or a “longer” game (more, smaller ships, or a larger board).\n\nLet the players pause and save the game for later.\n\nCreate a console interface, or a graphical interface using JavaScript or a interface and architecture that will allow you to unit-test UI behaviors, preferably without painting phantom widgets on your screen while the tests are running.\n\nlightweight GUI framework. Choose an\n\nWill you allow for two human players? If so, how will you prevent them from viewing each other’s boards? Does your environment provide lightweight interprocess communication? Will you deploy to a cloud service that handles multiuser access?\n\nBe your own product advocate and add your own fun variations.\n\nOceanofPDF.com",
      "page_number": 237
    }
  ],
  "pages": [
    {
      "page_number": 4,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Essential Test-Driven Development\n\nRob Myers\n\nA NOTE FOR EARLY RELEASE READERS\n\nWith Early Release eBooks, you get books in their earliest form— the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.\n\nIf you have comments about how we might improve the content and/or examples in this book, or if you notice missing material within at PearsonITAcademics@pearson.com\n\nthis\n\ntitle,\n\nplease\n\nreach\n\nout\n\nto\n\nPearson\n\nOceanofPDF.com",
      "content_length": 523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Contents\n\nPreface Acknowledgments About the Author\n\nPart I: Core Techniques\n\nChapter 1: Thinking Test-Driven Chapter 2: Basic Moves Chapter 3: Build Upon Existing Behavior Chapter 4: Exceptional Behaviors Chapter 5: Sustaining a Test-Driven Practice\n\nPart II: Ancillary Practices Chapter 6: Test Doubles Chapter 7: Testing Legacy Code\n\nPart III: Return on Investment Chapter 8: The Black Swans Appendix: Exercises\n\nOceanofPDF.com",
      "content_length": 429,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Table of Contents\n\nPreface\n\nWhy This Book?\n\nTest-Driven Development in Brief\n\nTerms, Themes, and Conventions\n\nWho Is This Book For?\n\nBook Organization\n\nForeword\n\nDisillusionment\n\nThis Isn’t New\n\nThe Java Factory at Interface Systems\n\nWhat’s in a Name?\n\nThe Effect\n\nAcknowledgments About the Author\n\nPart I: Core Techniques\n\nChapter 1: Thinking Test-Driven\n\nThe Safety Net\n\nUnits of Behavior\n\nBehavioral Boundaries\n\nA Taste of TDD\n\nThe Future of Test-Driven Development\n\nSummary",
      "content_length": 477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Chapter 2: Basic Moves\n\nThe TDD Flowchart\n\nThinking in Tests\n\nRequests, Tasks, and a To-Do List\n\nExercise Walk-Through\n\nSummary\n\nChapter 3: Build Upon Existing Behavior\n\nNext Steps\n\nImproving the Tests\n\nRefactoring the Tests\n\nRemoving the Symptomless Defect\n\nTriangulating Away Side Effects\n\nA Picture Is Worth a Thousand Words\n\nSummary\n\nChapter 4: Exceptional Behaviors\n\nRaising/Throwing\n\nA Test That Passes When the Code “Fails”\n\nExternal Exceptions\n\nSummary\n\nChapter 5: Sustaining a Test-Driven Practice\n\nAttributes of a Good Unit Test\n\nWhere to Start\n\nWhat to Test\n\nWhen Code Is Difficult to Test\n\nTest Smells and Refactorings\n\nOther Common Testing Challenges\n\nSummary",
      "content_length": 672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Part II: Ancillary Practices\n\nChapter 6: Test Doubles\n\nThe Trouble with Dependencies\n\nA Test Double Taxonomy\n\nAdditional Recommendations\n\nSummary: Use with Care\n\nChapter 7: Testing Legacy Code\n\nCharacterization Testing\n\nOther Considerations\n\nIntroduce Virtualizing Proxy\n\nSummary: The Complete Toolbox\n\nPart III: Return on Investment\n\nChapter 8: The Black Swans\n\nThe Agilist’s Dilemma\n\nThe Three Levels of Value\n\nThe Black Swan User Stories\n\nTaming the Black Swans\n\nSummary\n\nAppendix: Exercises\n\nPassword Strength Checker\n\nSalvo\n\nOceanofPDF.com",
      "content_length": 544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Preface\n\nTest-Driven Development (TDD) is a straightforward practice that provides software developers, teams, and organizations with numerous short-term and long-term benefits. It is the development practice of writing an automated test or “specification” for a software behavior just before writing the code that implements that behavior and makes that test pass.\n\nTDD is often described as merely a way to produce software with fewer defects. What people often miss is that each existing test continues to play an essential role throughout the life cycle of the software. TDD not only catches most implementation mistakes instantly, but also leaves behind a representative guardian for each behavior. This safety net of existing tests allows developers to reshape the internal structure of the code without worrying about breaking another part of the system written months earlier. It gives them the confidence to make room for unexpected and innovative features. This ability to modify existing code without breaking anything has always been key to maintaining and enhancing software.\n\nTDD is the combination of two earlier Extreme Programming (XP) practices. In 1996, Kent Beck1 formulated XP, which provided a more tenable and sane approach to software development than many of the leading methods of the time. Two of the earliest practices in XP were (1) “test-first”—writing a unit test before writing the implementation that would make it pass; and (2) “refactoring”—reshaping the internal design of the resulting implementation. Beck saw value in combining and describing those two practices independent of XP, and in 2000 published Test-Driven Development by Example.\n\n1 https://en.wikipedia.org/wiki/Kent_Beck\n\nIt’s now 25 years later. Sadly, most software development teams still struggle with wholeheartedly embracing TDD—and still suffer the consequences of avoiding a test-driven approach.",
      "content_length": 1905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Why This Book?\n\nIn 1997, I had been writing software professionally for about a dozen years, and I was sick of it. No matter how much upfront design or testing we did, the customer would eventually create or uncover a scenario that no one had anticipated. The code would have to change, often dramatically, often wrecking our careful design considerations or creating a new and hidden defect.\n\nSo, in 1997, I was seriously pondering a career change. “Perhaps real estate?”\n\nBut in 1998, I had my first encounter with most of the practices described in this book. Suddenly programming made sense to me again! For the next six years, I had great fun writing the most well-designed and most defect- free software of my career, even when—especially when—the customers asked for something unexpected.\n\nMy personal mission in writing this book is a simple one: to help people craft quality software products and enjoy their careers. I wrote this book so the modern software developer can experience the ease and joy I’ve found using TDD.\n\nThe purpose of this book is to give software developers a set of simple— and essential!—practices they can use to build and maintain their code. At its core, TDD is truly simple. It gives you a clear set of steps through which you can channel all your creativity, talent, experience, and enjoyment of this craft. With these simple techniques, you can create, enhance, and maintain the most complex software imaginable.\n\nTest-Driven Development in Brief\n\nTest-Driven Development (TDD) is the practice of first considering what it is we want the software to do before deciding how we want the software to do it. We record each unique description of inputs and expected outcomes in an executable and repeatable format (a test) before writing the code that we presume will make the test pass.",
      "content_length": 1821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "TDD requires us to notice the thinking that must already occur before we can write any code and to preserve those thoughts in a readable and repeatable test suite. Its process is summarized in the following steps:\n\nStep 1. Write a failing test. Write one test for a desired nonexistent behavior, run it, and ensure it fails in a predictable manner.\n\nStep 2. Write just enough code to make that test pass. Write a minimal implementation that will pass the test without writing any more than is needed for this one test, and without breaking any previous tests.\n\nStep 3. Refactor. Look for opportunities to clarify and simplify the structure of the code and the tests, and make those changes in such a way that all tests continue to pass. This step is not optional. Often, refactoring helps make the next test easier to write.\n\nYou then return to step 1, gradually growing the behaviors, the structural design, and the suite of tests that protect those behaviors during all future changes.\n\nThis cycle is often abbreviated as “Red–Green –Refactor” or “Red–Green – Clean,” as illustrated in Figure P.1.",
      "content_length": 1099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Figure P.1 The simplest TDD flowchart, Red–Green–Clean\n\nTDD can be summarized as this simple set of mechanical steps. However, when developers apply their problem-solving skills, creativity, and experience to TDD practices, they can build high-quality software with speed born of confidence. With each iteration through the TDD cycle, they produce high-quality and well-designed software, and their growing suite of fast and comprehensive tests protects all prior investments in the solution.\n\nFor the team using TDD, the tests in a team’s test suite are the result of all previously completed requests. The test suite serves as a detailed, up-to- date, and runnable engineering specification, describing precisely what the software currently does. A good engineering specification is clear, readable, and easily understood by both current and future developers.\n\nRather than having later requests become increasingly challenging, consume ever more time, and lead to building irritation among team",
      "content_length": 997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "members, with TDD each request can be met with the same confident enthusiasm the team would have given it had it been their first request on the very first day of a project. Moreover, because each new request can be completed without having to tiptoe over existing functionality, time-to- market is significantly reduced.\n\nTerms, Themes, and Conventions\n\nImplementation: The code that delivers valuable software behaviors. In contrast to test code, which tests those behaviors.\n\nRefactoring: Any change to the software system that preserves existing overt behaviors and improves maintainability.\n\nOvert behavior: Behavior that is expressly described in the code, through loops, branches, method calls, and so on. A counterexample is speed of execution: Speed is an implicit behavior based on many complicated factors; no programming language has a faster keyword.\n\nMaintainability: A measure of how easily the software can be enhanced or repaired. It is a factor of both the code’s design and the thoroughness of the team’s test suite.\n\nDesign: The internal structure of the software’s code; that is, how developers structure the implementation and how they choose to name the parts of that structure. Good software design clarifies and communicates how behaviors are implemented. A good design allows new functionality to be added without excessive rework.\n\nI avoid using the term “design” when defining “refactoring” because both are often misunderstood to be topics that impact only developers, such that only developers would care about them. Maintainability, in contrast, is understood as how software grows in quality and value.\n\nRequest: Something the software development team is being asked to build that will incrementally increase the value of the software delivered. Agile methods have popularized the term “user story,”",
      "content_length": 1832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "meaning a very brief narrative describing an example of a user’s interaction with the software to accomplish that user’s immediate goal. A user story is typically much shorter than a “feature request,” but I use the shortened “request” herein as an umbrella term for either.\n\nTest: An automated script that defines and executes a behavior and checks that the results of that behavior are correct.\n\nIn this book, I will occasionally use “specification” or “scenario” instead of “test” whenever there is need to emphasize the test’s role as specification (i.e., documentation) or as a scenario (i.e., an example).\n\nScenario: A single example of a behavior that includes what needs to be true before the behavior occurs and what the results should be once it is complete. Many scenarios make up a specification.\n\nSpecification: Documentation describing all detailed requests. For a complex product, there are often two specifications: the product specification and the engineering specification. There can be overlap between the two, but the engineering specification often covers technical details that do not need to be visible to a user, business analyst, or product manager.\n\nUnit test: The term “unit test” has been around for decades, and people often assume that everyone knows what it means. Many modern developers dislike the term. Developers are often told “you must unit- test your code” as a requirement, and view it as a chore. Many earnest attempts to comply by testing individual methods or lines of code are likely to result in frustration and discouragement.\n\nThe industry has tried to solve the problem by changing the name. “Microtest,” “spec,” and “developer test” are sometimes used for this purpose, and they all express something true about these tests. For example, “spec” reminds us that we are writing a bit of engineering specification, and suggests that we may want to write it out before writing the implementation. “Developer test” conveys that these tests are written by developers, are read by developers, and exist primarily to support an ongoing development process.",
      "content_length": 2097,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "The terminology isn’t the problem; instead, the definition of “unit test” is the problem. It lacks clarity: What exactly is a unit of software?2 A class, a function or method, a branch or loop, a line of code? All of those are the content and structure of the code, not what the code does.\n\n2 “Microtest” suffers the same vagueness as “unit”: How small is “micro,” and what are we comparing it against?\n\nI use “unit” throughout this book3 to mean this: A unit of software is that can be described the smallest bit of runtime behavior independently. The implementation of a unit of behavior, as you’ll see later, can span methods or can hide within part of a Boolean conditional clause.\n\n3 I did run an informal poll on LinkedIn, and “unit test” won by a large margin, but the comments explaining my colleagues’ various opinions were even more helpful.\n\nTwo important themes are baked into this book: essentialism and emergence. Rarely are they mentioned by name, but they serve as guiding principles.\n\nRecommended Reading\n\nMcKeown, Greg. Essentialism: The Disciplined Pursuit of Less. New York: Crown Business, 2014.\n\nEssentialism\n\nThe choice of the word “essential” in the title of this book is intentional. The practices herein are essential, meaning they are necessary. Essential also implies that there is nothing extra. In his book Essentialism, McKeown quotes Dieter Rams, the original industrial designer at Braun: “Less, but better.”\n\nHow can you build valuable, quality software without causing your team to burn out? By using a practice that is, at its core, so simple that it stays out of the way, yet so powerful that it cuts away confusion, mistakes, and rework, replacing those with flexibility and confidence.",
      "content_length": 1724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Emergence\n\n“a process whereby larger entities, patterns, and regularities arise through interactions among smaller or simpler entities that themselves do not exhibit such properties.”4\n\n4 http://en.wikipedia.org/wiki/Emergence\n\nMost of the exercises and examples in this book come from the technical courses I teach, and every exercise is done using a test-driven approach and working in pairs. There could be 20 people in a class (that is, 10 pairs working separately on the same exercise), and I would see 10 perfectly acceptable but unique designs.\n\nThere is never one “right” design. Software design is highly contextual: Who is on the team, the experiences and education they have had, the technologies that are being used, and many other factors—all intertwine to make every software design unique.\n\nA metaphor I use to describe what is called “emergent design” is the pruning and shaping of a Japanese maple tree. One cannot entirely decide upfront what the tree will look like. You start with a little sapling, and the sapling, the soil, and the weather all have a say in how the little tree will grow up. You get feedback from the tree and work gradually with the tree, making small changes each season to reach a mutually acceptable aesthetic result. The discipline is a creative one that requires knowledge, and gets easier with experience.\n\nthe Test-driven software development implementation offer continuous feedback to each other. The developers who listen for that feedback fare better than those who try to force their preconceptions upon the software.\n\nis\n\nlike\n\nthat: The\n\ntests and\n\nWho Is This Book For?\n\nThis book is designed to equip developers with the essential skills and techniques that can be applied to diverse business domains, programming languages, frameworks, and tools that they will encounter throughout their",
      "content_length": 1844,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "software development careers, including—and perhaps most urgently— modern AI agentic tools.\n\nPractices and techniques tend to greatly outlive specific technologies. TDD —particularly the test-driven mindset and practices for maintaining a team’s suite of clear, fast tests—is applicable whether you will be writing unit tests, Gherkin scenarios, or prompts for an AI agent.\n\nI’ve worked with teams who have used the practices described in this book to build and maintain AWS Lambda functions; JavaScript using Promises, Angular, or React; node.js applications; Ruby on Rails apps; ASP.Net/ADO.Net applications; PL-SQL; and even assembler code.\n\nI have endeavored to write examples that are understandable even when written in a programming language unfamiliar to the reader. Examples are in a variety of programming languages (mostly Java, C#, and Ruby) to convey (1) that TDD is an effective practice regardless of language and (2) developers can comprehend code in unfamiliar languages if the tests and implementation are written clearly. I often chose the language that makes the example a bit clearer (e.g., C#’s override keyword makes an override obvious).\n\nBook Organization\n\nThis book is arranged into three parts: Core Techniques, Ancillary Practices, and Return on Investment.\n\nPart 1: Core Techniques\n\nChapter 1, “Thinking Test-Driven,” provides an overview of the TDD discipline and an investigation into TDD’s utility in a rapidly changing industry.\n\nChapter 2, “Basic Moves,” walks you through an example, demonstrating both the mechanics and the mindset that make up an effective TDD discipline. Part of that mindset is treating TDD as a cooperative win-win game that includes the development computer as one of the players.",
      "content_length": 1738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Chapter 3, “Build upon Existing Behavior,” continues the example from Chapter 2 and goes deeper into the test-driven mindset and how to approach your implementation from a test-driven perspective.\n\nChapter 4, “Exceptional Behaviors,” briefly covers how to use TDD to design behaviors that throw exceptions or raise errors.\n\nChapter 5, “Sustaining a Test-Driven Practice,” describes common pitfalls and provides additional techniques for maintaining your test suite and implementation in the long term. A discipline that loses its effectiveness over time isn’t worth starting, so I draw from the experiences of TDD teams that worked successfully together for six months or more. A catalog of common “test smells” is provided to assist you in maintaining your own test suites.\n\nPart 2: Ancillary Practices\n\nChapter 6, “Test Doubles,” covers a variety of techniques to substitute for troublesome external dependencies and greatly reduce the amount of time the test suite runs. Without some way to swap out external dependencies within your tests, your test suite will slow to a crawl, become an impractical burden, and eventually be abandoned.\n\nChapter 7, “Testing Legacy Code,” covers techniques to test existing untested code whenever it requires maintenance or enhancement. Testing legacy code does not have to be a horrible or futile effort. Practicing these techniques will also encourage and strengthen your TDD skills and will provide clarity on why TDD is the smarter “path of least resistance.”\n\nPart 3: Return on Investment\n\nChapter 8, “The Black Swans,” provides the business case for using TDD. Three tiers of value are described: (1) fewer defects and much less time spent on debugging, (2) shortened “cycle time” (that is, time spent building a feature), and (3) greater adaptability to valuable, innovative, and unforeseen feature requests. This is a good chapter to share with a team’s skeptics, including product managers, functional managers, architects, testers, and developers, regardless of their years of experience.",
      "content_length": 2035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Foreword\n\nDisillusionment\n\nEarly on in the pages that follow, author Rob Myers describes his mental state in 1997 when he was considering switching careers. He contemplated leaving software and going for his real estate license. He was falling out of love with a career he thought would carry him for a lifetime.\n\nI know that feeling. I’m sure many of us in the software industry do. It’s that moment where you look around and conclude there has to be a better way.\n\nLike Rob, I developed a fascination with programming computers when I was just a kid. I chased that feeling through high school and college, and discovered I not only enjoyed programming but was pretty good at it. I later learned I would also be good at leading teams of people to write code together and create more complex systems. That’s when the real trouble started.\n\nI noticed some important patterns that would later haunt me and the teams that I led.\n\nThe first pattern was that most code could only be maintained by the person who originally wrote it. This silo at once became job security, yet also a prison from which there was no escape.\n\nThe second pattern was that programmers, in general, were terrible project managers, and even worse at organizing processes to keep teams of programmers working together cohesively for a long period of time. The biggest project management challenge was the inability to guess how long things were going to take, to the point that the management team paying for the work eventually had to sound the deadline siren, and call for round-the- clock coding to hit the release date. This crunch time inevitably led to the",
      "content_length": 1632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "biggest nightmare a software team faces: the QA team finding a critical late-stage bug that had to be fixed before release. Yet, it could not be found, sometimes for months.\n\nMy team at Interface Systems in Ann Arbor, Michigan, faced such a nightmare scenario in the mid-1990s. It went on for several weeks as the programmers scrambled to find that elusive bug that absolutely could not be worked around. As the director of that team, I was called into regular update meetings with my bosses, who were frustrated and demanded a solution.\n\nFinally, one morning, one of my long-time lead developers told me, as I walked by his office, that he had found and solved the problem. Eureka! I asked him how he did it. He mentioned that he was perusing some directories of files containing the code and stumbled upon a test bed he had written months before; he dusted it off and ran it. He said it exposed the problem immediately and took him right to the problem.\n\nWhile I was thrilled to have this nightmare behind me, I knew it was only a temporary state of bliss, as there would inevitably be another monster bug hiding in the dark recesses of the code waiting to pounce at the most inconvenient time.\n\nI, too, began plotting my escape. My version was starting a canoe camp in the boundary waters of Minnesota.\n\nThere has to be a better way, I thought.\n\nThis Isn’t New\n\nRob writes that even 25-plus years after Kent Beck and others started espousing Test-Driven Development, unit testing, and automated unit test frameworks in the late 1990s, most still don’t truly practice it. Even those who do still are in need of good coaching and discipline to bring it into the flow of standard coding activity.\n\nMy experience tells me it’s been far longer since we actually knew what to do. I was at the University of Michigan pursuing degrees in computer science and engineering from 1978 to 1982. In the summer of 1980, I got a",
      "content_length": 1915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "phone call from Professor John Saylor, who asked if I’d be interested in joining a cohort of Michigan computer science majors for a summer internship at a high-flying entrepreneurial town called Manufacturing Data Systems, Inc. (MDSI). I jumped at the chance.\n\nfirm\n\nin\n\nFor the first four weeks of the internship, the entire group of eager students was put through an extensive and exciting boot camp to learn MDSI’s approach to writing Pascal code. One of the key constructs Dr. Saylor taught us was that we would write tests before we wrote the code; we would catalog the tests, then write the code, and run the tests to determine how well we did with our coding. We would then pay as much attention to our test code as we would our production code. I honestly thought I had been given a key to a portal for learning truly modern ways of coding. I couldn’t believe how lucky I was that I would be able to spend the summer refining this skill.\n\nAfter this four-week session was completed, we interns were assigned to our various departments and bosses for the summer. I was assigned to Vince B. to work on the CODE and CAPP projects. As soon as I settled in to my new cube, Vince called me into his office, closed the door, and told me that I wasn’t to use any of the techniques we had spent the last four weeks learning. He had a delivery style that made it quite clear I wasn’t to argue a case to try and change his mind. His basic message was “We don’t have time for this bullsh*t; we’ve got deadlines.”\n\nThe experience of those four weeks never left me—nor did the frustration of being denied the chance to use that experience. Later on in my tenure at MDSI, I had one chance to use these modern techniques with one other programmer. It resulted in the best code I had ever written professionally to that point. It was the only chance I’d had to apply Dr. Saylor’s approach.\n\nThe Java Factory at Interface Systems\n\nIn 1997, I was promoted to VP of R&D at Interface Systems. Now, I believed, I had the perch from which I could start pursuing that better way I had always believed was possible. For the first two years, though, things improved only here and there by applying the standard approach: working harder.",
      "content_length": 2218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "In 1999, some important coincidences began to occur. I started working with a couple of consultants named James Goebel and Tom Meloche, who worked for a company that had consultants placed at Chrysler. Those consultants had the good fortune to brush up against another consultant who was bringing radical new ideas to the Chrysler Comprehensive Compensation system, after that system went through its own disastrous history of implementation failure. They needed a fresh, new approach. Kent Beck was apparently quite persuasive in convincing the management team there to try something new. The result was that Tom and James began teaching me about Extreme Programming, which included the concepts of writing tests before you write the code and then automating those tests so that they could be run over and over again as the coding progressed. I was reminded of my time with Dr. Saylor and my own experience.\n\nI never looked back. The rest is history for me, and that history is well captured in my own book, Joy, Inc.: How We Built a Workplace People Love.\n\nI eventually warmed to all the other ideas of Extreme Programming, especially pair programming. But it was TDD and automated unit testing that truly drew the honeybee to the flower. It eventually became the basis for launching my own company, with Tom and James, called Menlo Innovations.\n\nWhat’s in a Name?\n\nThe dot-com bubble burst in early 2001, and the dream at Interface Systems ended with a pop when the California company that had purchased it in September 2000 had to shutter every remote office it had, including mine in Ann Arbor. James, Tom, and I knew we had built a great engine room. Sadly, the first time we did it, we built it inside the Titanic. The fate of the Titanic was not caused by the engine room. We knew how to build a great engine room and we could do it again. Looking back on the experience now, we had the great fortune of having been able to build the prototype of Menlo Innovations inside of Interface Systems.\n\nWe started the company as most begin—in the basement of my home. The company’s name was derived from my inspiration of seeing a replica of the",
      "content_length": 2145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Menlo Park, New Jersey, lab of Thomas Edison in Greenfield Village in Dearborn, where Henry Ford honored his friend Edison by reconstructing the lab.\n\nJames built the original website but did not include the address of the company in the website (my wife didn’t really think it was appropriate to invite physical traffic to our home in a cozy Ann Arbor neighborhood). We did, however, include our cell phone numbers so that people could contact us.\n\nJust a couple of weeks after launching the website, I received a call from someone who was passionately interested in what we were doing, especially since we were espousing ties to Kent Beck’s thinking and Extreme Programming. He wanted to come work for us. He was in northern California. I explained that we were not in hiring mode quite yet, but promised him I’d be in touch. I couldn’t imagine how someone in northern California would come work for us. (Remember, this was 2001.)\n\nAbout a month later, he called again and was looking for an update. I explained that things were starting slowly, but I promised I would be in touch. He pressed harder and said: “Look, I’ll just come down for lunch. I want to meet you guys.”\n\nPerplexed, I said, “Come down for lunch? What do you mean?”\n\n“Menlo Park isn’t that far for me.”\n\nI couldn’t help but laugh out loud. I said, “Rob, we are in Ann Arbor, Michigan!”\n\nHe didn’t hesitate. He said he would get on a plane. And he did.\n\nRob Myers commuted from California every two weeks for two years as he helped us rebuild the Organ Transplant Information System (OTIS) at the University of Michigan Health System. He joined our efforts to rebuild a critical piece of technology infrastructure at one of the leading organ transplant centers in the United States. When Menlo completed its portion of the work, there were almost 18,000 unit tests after 3 years of development. We had trained the Michigan team how to write tests and maintain existing ones. At the height of its operation, there were almost",
      "content_length": 1994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "30,000 unit tests faithfully being run each time the code was changed. That system stayed in operation for more than 15 years.\n\nIf we hadn’t named our company Menlo Innovations, I might never have met Rob. We cemented our relationship in the years between 2001 and 2003, and we’ve stayed in touched ever since.\n\nThe Effect\n\nMenlo started with all of these practices in place. We’ve never faltered. We worked on several systems (like OTIS) that quite literally hold the lives of people in the data and the algorithms—for example, a flow cytometer that is FDA-validated for monitoring the treatment of people with blood-borne cancers.\n\nIn all of our nearly 25 years to date, we have experienced exactly two software emergencies. Both were resolved within 24 hours. My life used to be firefighting every day.\n\nI still don’t see these techniques being systematically taught in university curricula. They’re touched on, but not taught, as John Saylor taught me in 1980.\n\nI first touched a computer to program it in 1971. I can say without a doubt that what you are about to learn—from one of the best teachers in this discipline—is the most revolutionary concept in computer programming ever discovered. Hands down.\n\nIn my early days, I believed there was a better way. There is, and you are about to learn all about it.\n\nIt is safe to say that for me, it saved my career and brought me back to the joy of the profession that I had yearned for from my earliest days.\n\nThank you, Rob Myers, for taking a chance on a company that you thought was just down the road from you but turned out to be across the country. My life was enriched for having had you be such an important influence in our earliest days. I am forever grateful.",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "—Richard Sheridan CEO, Co-founder, and Chief Storyteller, Menlo Innovations; author, Joy, Inc.: How We Built a Workplace People Love (2013), and Chief Joy Officer: How Great Leaders Elevate Human Energy and Eliminate Fear (2018)\n\nOceanofPDF.com",
      "content_length": 244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Acknowledgments\n\nThis content is currently in development.\n\nOceanofPDF.com",
      "content_length": 74,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "About the Author\n\nThis content is currently in development.\n\nOceanofPDF.com",
      "content_length": 75,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Part I: Core Techniques\n\nOceanofPDF.com",
      "content_length": 39,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Chapter 1. Thinking Test-Driven\n\nThose who want really reliable software will discover that they must find means of avoiding the majority of bugs to start with, and as a result the programming process will become cheaper. If you want more effective programmers, you will discover that they should not waste their time debugging, they should not introduce the bugs to start with.\n\n—Edsger W. Dijkstra, The Humble Programmer (1972)\n\nThe mechanics of Test-Driven Development (TDD) are simple, but TDD is not an invitation to disengage your intellect, nor does it require you to forget your own previous experiences and education.\n\nPeople naturally talk about software behaviors using examples, though they aren’t always aware that they’re doing so. While working with a team, I will often hear someone offer a simple example in a verbal sentence or two. By capturing that example in a small bit of automated, executable specification (a test), the team can be sure of the following:\n\nThe example doesn’t get lost in translation later.\n\nOnce complete, the implementation works correctly.\n\nThe behavior captured by that example is never mistakenly broken during the entire life of the product.\n\nFor developers, many examples will occur to them while they are writing code, but that dialog is often internal: “What do I want my next few lines of code to do?” Before a developer writes even the tiniest bit of logic, they already know what they expect that code to do. TDD asks the developer to record that thought by turning it into a concrete description of the example and its expected results.",
      "content_length": 1590,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "This book encourages a deeper dialog: “What if a caller does this? What do I want the software to do? How can I be sure I got it right, and that I don’t break something else?”\n\nWriting that thought in a familiar language and having it instantly added to an automated test suite costs the developer very little extra time and could easily save them hours upon hours of debugging later.\n\nThe Safety Net\n\nThe team’s collection of fast, automated, and comprehensive tests that grow from a diligent TDD practice is much more than a typical suite of tests. For the test-driven team, the test suite is a powerful safety net.\n\nAlthough each tiny individual unit test might seem quite simple by itself, in the aggregate that safety net protects the team’s entire previous efforts and the business’s investment in the software produced. Whereas individual the tests comprehensive safety net also protects against mistakes during refactoring.\n\ninstantly catch common mistakes\n\nin\n\nimplementation,\n\nUnits of Behavior\n\nThe following specification (in Ruby’s rSpec) describes software that emulates a simple box. We can add items to the box, remove them, and verify whether the box contains a specific item. In each scenario, the tested behavior is emphasized.\n\nit 'knows what has been added' do\n\n@box = Box.new @box.add(\"red pen\") expect(@box.has(\"red pen\")).to be true\n\nend\n\nit 'knows what it does not have' do\n\n@box = Box.new @box.add(\"red pen\") expect(@box.has(\"purple smartphone\")).to be false\n\nend",
      "content_length": 1489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "it 'can remove something it has' do\n\n@box = Box.new @box.add(\"red pen\") @box.remove(\"red pen\") expect(@box.has(\"red pen\")).to be false\n\nend\n\nNotice that add() cannot be tested without has(), and vice versa. That is, the behavior is spread out over two or three method calls on the same object. The implementations of these behaviors are encapsulated by the Box object, and the specification of each unit of behavior is clearly described by a single scenario. In other words, behavior is most clearly described in the tests.\n\nThe next example demonstrates how a single line of code can be part of multiple distinct behaviors. Imagine we are designing a retro-style volume control dial for a smartphone (Figure 1.1). The control can be set to only integer values, and our user interface (UI) developer tells us the dial will graphically “snap” to the next value as the user rotates the dial, perhaps with haptic feedback to emulate that memorable “click.”",
      "content_length": 953,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Figure 1.1 A stereo system’s volume dial circa 1980. (Photo: atm2003/123rf)\n\nIn a brief whiteboard discussion, we decide we need a volume-control application programming interface (API) that accepts an integer from 0 to 10. Zero is effectively “off” and 10 is the maximum volume (not 11!1).\n\n1 https://en.wikipedia.org/wiki/This_Is_Spinal_Tap\n\nThe UI developer assures us that we will not receive an out-of-range integer. However, we agree that if this somehow happens, our code will throw an IllegalArgumentException. In Java, this could look like the following:\n\nif (volumeZeroToTen > 10 || volumeZeroToTen < 0)\n\nthrow new IllegalArgumentException(\"Volume cannot be \" + valueZeroToTen);\n\naudioManager.setStreamVolume(volumeZeroToTen);\n\nThere are three scenarios:",
      "content_length": 764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "1. When volumeZeroToTen is greater than 10\n\n2. When volumeZeroToTen is less than 0\n\n3. When volumeZeroToTen is within the acceptable range\n\nEach of those three scenarios is different because of the first if conditional, and each deserves its own distinct unit test. The behavior is more clearly described by expressly writing tests for those three examples.\n\nBehavioral Boundaries\n\nA behavioral boundary is the conceptual boundary separating two (or more) related scenarios. In practice, it is often the boundary between two similar input values that give different results.\n\nLet’s return to the volume dial example. Suppose that rather than having the dial’s API set the actual phone volume (as previously shown), we decide to take an event-driven approach instead. An observer of a Dial object will need to query the setting whenever the graphical dial is turned. What the observer wants to know is not which value was set on the dial, but what percent of the maximum volume the setting represents.\n\nA developer might be tempted to aim for the “happy path” (the scenario that typically avoids all edge cases and error conditions), write a simple implementation (for example, return currentSettingFromZeroToTen * 10; ), and be done with it:\n\n@Test public void levelAsPercent_HappyPath_RightDownTheMiddle() {\n\nDial dial = new Dial(\"Volume\"); dial.setTo(5); assertThat(dial.levelAsPercent()).isEqualTo(50);\n\n}\n\nThis is a good start, but there is another way to approach this kind of problem, particularly when you are working with something more sophisticated than a volume dial.",
      "content_length": 1578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "TDD is first and foremost a practice that facilitates and supports a developer’s thinking and coding. Writing one test scenario often leads to an awareness of other related scenarios. Writing tests for those related scenarios is known as the “triangulation” technique; it is covered in detail in later chapters. In its most refined form, triangulation helps the developer identify behavioral boundaries by asking, “What value(s) will cause us to write smarter code?”\n\nLet’s review what we know about Dial:\n\nNegative numbers should throw an exception.\n\nNumbers greater than 10 should throw the same exception with a different message.\n\nZero is effectively “off.”\n\nAll other values represent a percentage of the maximum volume.\n\nThere is a behavioral boundary between –1 (an error) and 0 (off), and another behavioral boundary between 10 (maximum) and 11 (another error). However, there are no behavioral boundaries between any two of the integers between 1 and 10. The behavior—the complete code path—is the same for all: 1, 2, 5, 6, 9, and 10 must all exhibit the same behavior (in this case, a simple calculation) resulting in different answers.\n\nBecause 0 (off) could also be represented by 0 percent, is there a behavioral boundary between a dial setting of 0 and 1? It’s impossible to say without learning more about the product specification. If we are asked to have the software turn on a little green light whenever the sound system is turned on, that would call for a behavioral boundary between 0 and 1 (though not necessarily implemented by Dial). Until that behavior is requested, however, the test-driven developer would not build it.\n\nWhen you spot a behavioral boundary, you can write your tests for the values that represent the “edges” of that boundary:\n\n@BeforeEach public void initializeVolumeDial() {\n\ndial = new Dial(\"Volume\");\n\n}",
      "content_length": 1850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "@Test public void dialThrowsExceptionForValueUnderMin() {\n\nassertThatIllegalArgumentException().isThrownBy(() -> { dial.setTo(-1); });\n\n}\n\n@Test public void levelAsPercent_WhenOff() {\n\ndial.setTo(0); assertThat(dial.levelAsPercent()).isEqualTo(0);\n\n}\n\n@Test public void levelAsPercent_AtMaximum() {\n\ndial.setTo(10); assertThat(dial.levelAsPercent()).isEqualTo(100);\n\n}\n\n@Test public void dialThrowsExceptionForValueOverMax() {\n\nassertThatIllegalArgumentException().isThrownBy(() -> { dial.setTo(11); });\n\n}\n\nWhat if you hadn’t noticed the boundaries before writing your tests, and you had written that earlier happy path test for the dial setting of 5? You could either leave the original test within the suite or delete it. If the happy path test provides even the tiniest additional clarity in the specification, leave it. It would take perhaps one additional microsecond for each run. Alternatively, you could adjust the happy path test to be right at the edge of the boundary by changing the 5 to a 0 or a 10 and by changing the expected outcome to 0 or 100, respectively.\n\nIf you don’t test the edges of the behavioral boundaries, your team’s safety net will not be as clear and thorough as possible. For example, if we use 42 instead of 11 as the erroneous input greater than 10, and later another developer mistakenly increases the maximum value to 30, our test suite wouldn’t catch this problem. Our team might not learn about the defect until some unlucky consumer receives a very unpleasant (and very loud) surprise.",
      "content_length": 1526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Because thinking test-driven requires human thought, mistakes will still happen, but far less frequently. When TDD is used diligently, every software defect is either a missing test or a vaguely specified test (that is, a misunderstanding). In either case, the mistake is a gap in the team’s safety net. It often takes just one more test to close the gap.\n\nA Taste of TDD\n\nLet’s return to the Box software and give it the ability to indicate whether it is empty or not.\n\nThis example is written in JavaScript using Jasmine. In case you want to follow along, you won’t need anything except a text editor, a browser, and a download of the Jasmine library.\n\n1. Write a single test for a tiny bit of behavior:\n\ndescribe(\"box\", function() { it (\"starts out empty\", function() { var box = new Box(); expect(box.isEmpty()).toBeTruthy(); }); });\n\n2. Write just enough code so your test fails due to an assertion (also known as an expectation):\n\nBox = function() { };\n\nBox.prototype = { isEmpty: function() { return false; },\n\n};\n\n3. Run all of the tests—that is, develop the habit of running the whole suite each time. Make sure the new test fails with an informative message, as illustrated in Figure 1.2.",
      "content_length": 1198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Figure 1.2 Jasmine’s SpecRunner.html page showing the clean test failure.\n\nNote that Jasmine highlights the name of the test that has failed. The test (“spec”) name and failure message together describe the expected behavior. Seeing the test fail and reading the failure message is how you “test the test”: You know you’ve asked for behavior that has not yet been implemented.\n\n4. Write just enough code to get the test to pass as quickly as possible:\n\nisEmpty: function() { return true; },\n\nAll we did was change false to true. This technique, called Fake It, will be covered in detail in Chapter 2, “Basic Moves.”\n\n5. Run all of the tests again. They should all pass now, as shown in Figure 1.3.",
      "content_length": 697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Figure 1.3 Jasmine’s test-results browser page showing the tests passing.\n\nWe’re not done with this behavior, but this seemingly trivial code is sufficient to pass all existing tests.\n\n6. Refactor diligently!\n\nBecause this is our very first test, there’s nothing to refactor here. Nevertheless, you should always take a moment to review both the tests and the implementation to see if you reduce any duplication or improve the clarity.\n\nBack to step 1!\n\n1. Write a single test\n\nWhere there’s a true, there’s usually a false—for example, when a Box isn’t empty. We’ll give Box an add() method:\n\ndescribe(\"box\", function() { it (\"isn’t empty after adding\", function() { var box = new Box(); box.add(\"red pen\"); expect(box.isEmpty()).toBeFalsy(); });\n\nit (\"starts out empty\", function() { var box = new Box(); expect(box.isEmpty()).toBeTruthy(); }); });\n\n2. Write just enough code to make your test fail cleanly. It won’t fail cleanly until it has a stub for add(). Resist the temptation to write an implementation for add() before you see the test fail: Box = function() { };\n\nBox.prototype = { add: function(item) { }, isEmpty: function() { return true;",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "}, };\n\n3. Run all of the tests.\n\nOf course, the new test fails, as illustrated in Figure 1.4.\n\nFigure 1.4 A new failing test.\n\n4. Write just enough code to get both tests to pass:\n\nBox = function() { this.items = []; };\n\nBox.prototype = { add: function(item) { this.items.push(item); }, isEmpty: function() { return this.items.length === 0; }, };\n\n5. Run all of the tests to see them pass, as shown in Figure 1.5.",
      "content_length": 413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Figure 1.5 Both tests passing.\n\n6. Refactor diligently!\n\nRefactoring the tests is as important as refactoring the implementation. Doing so often helps you write later tests.\n\nThere is a tiny bit of duplication between the two tests, and that’s enough. The duplicated code is highlighted in the following code:\n\ndescribe(\"box\", function() { it (\"isn’t empty after adding\", function() { var box = new Box(); box.add(\"red pen\"); expect(box.isEmpty()).toBeFalsy(); });\n\nit (\"starts out empty\", function() { var box = new Box(); expect(box.isEmpty()).toBeTruthy(); }); });\n\nAll unit-testing frameworks have a way to perform common setup for each test in the suite. In Jasmine, it’s called beforeEach(), and it runs once before each test within the containing describe block:\n\ndescribe(\"box\", function() { var box; beforeEach(function() { box = new Box(); });\n\nit (\"isn’t empty after adding\", function() { box.add(\"red pen\");",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "expect(box.isEmpty()).toBeFalsy(); });\n\nit (\"starts out empty\", function() { expect(box.isEmpty()).toBeTruthy(); }); });\n\nAnother run of the tests confirms that the changes made did not break any tests (Figure 1.6).\n\nFigure 1.6 Checking the refactoring.\n\nHere’s what we’ve accomplished with this tiny example:\n\nWe have some tested, production-quality code.\n\nWe have written part of an engineering specification.\n\nWe have written two distinct behaviors that this object provides.\n\nWe’ve reduced duplication in the tests.\n\nIn Chapter 2, we’ll walk through a richer example and explore more test- driven thinking and design choices.\n\nThe Future of Test-Driven Development\n\nOne of my favorite personal mottoes is “Never an absolutist.”2 Do I believe TDD is a practice that will never change and will never disappear?",
      "content_length": 812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "2 For the record, I was saying this long before we heard Obiwan Kenobi’s equally silly paradoxical utterance, “Only a Sith deals in absolutes.”\n\nOf course not: It’s already changing. What follows are some of those changes, along with their implications for the future of TDD. In each case, the TDD game is slightly modified, but the thought processes, techniques, and human activities remain essentially the same.\n\nAs far as TDD disappearing, well, that won’t happen as long as humans are needed to explain to a computer what it is that we want it to do for us.\n\nTDD and Behavior-Driven Development\n\nBehavior-Driven Development (BDD) is like TDD in many ways. It is the whole-team practice of building software by writing brief, human-readable scenarios, and getting them to work one at a time. BDD is defined as the practice of “exploring desired system behaviors with examples in conversations and formalizing those examples into automated tests to guide development.”3\n\n3 As defined in Behavior-Driven Development with Cucumber by Richard Lawrence and Paul Rayner (Addison-Wesley, 2019). distilled from the ideas of Liz Keogh and Daniel Terhorst- North.\n\nBDD isn’t merely a renaming of TDD. As Matt Wynne wrote in The Cucumber Book (Pragmatic Bookshelf, 2017), “BDD builds upon TDD by formalizing the good habits of the best TDD practitioners.”\n\nIn practice, BDD differs from TDD in the following ways:\n\n1. The tests are “business-facing.” That is, they express business rules and requests. The suite of tests (“scenarios” or “examples”) represents a runnable product specification, whereas the emphasis of TDD is often on a runnable engineering specification. To keep the scenarios readable to the entire team—whether technically inclined or otherwise—BDD examples are written in a ubiquitous domain language and use a handful of keywords.\n\n2. BDD is a whole-team activity and is best implemented with nearly continuous collaboration between the product and development specialists. Although this approach might seem burdensome on the",
      "content_length": 2038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "surface, BDD can reduce the need for many traditional meetings and hand-offs. The team works closely together to plan, specify, build, test, and demonstrate working software within hours or even minutes.\n\n3. TDD is included as a practice within the BDD cycle. There are often situations where finer-grained, “infrastructural” testing is required to complete a BDD scenario without cluttering the product specification with engineering details. Ergo, there’s no need to choose between BDD or TDD. They go together nicely, like chocolate and peanut butter.\n\nMany of the test-driven techniques described in this book apply equally to a team’s BDD scenarios: They favor developing more, smaller tests with fewer assertions; setting up just enough to test the business rule; reducing (for example, using Cucumber’s Gherkin Background duplication keyword); making replacing challenging dependencies with test doubles; and taking small, quick, and safe steps toward the team’s goal.\n\nthe scenarios\n\nreadable; strategically\n\nRecommended Reading\n\nTo learn more about BDD, check out any one of the following highly rated books:\n\nThe Cucumber Book by Matt Wynne, Aslak Hellesøy, and Steve Tooke (Pragmatic Bookshelf, 2017).\n\nBehavior-Driven Development with Cucumber by Richard Lawrence and Paul Rayner (Addison-Wesley, 2019).\n\nBDD in Action by John Ferguson Smart (Manning, 2023).\n\nTDD and Functional Programming\n\nWhen using Functional Programming (FP), whenever you create a type with a rigorous contract, you are effectively testing a lot of assumptions at compile-time. But there is also behavioral (runtime) code, and where there’s behavior, there’s the opportunity to get it wrong. Ergo, unit-testing FP behaviors is still beneficial, and building those behaviors by using a test- driven approach is as critical to quality FP as it is to object-oriented development.",
      "content_length": 1861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "FP unit tests contain all the attributes of good tests, as described in Chapter 5, “Sustaining a Test-Driven Practice.” The following example gives a taste of unit testing in F# (a .Net FP language):\n\n[<Test>] let ``intersection of two sets should only contain common eleme\n\nlet left = initialSet |> add \"Rob\" |> add \"Awesome\"\n\nlet right = initialSet |> add \"Jason\" |> add \"Awesome\"\n\nlet result = left |> intersection right\n\nresult |> contains \"Awesome\" |> expectsToBeTrue\n\nresult |> contains \"Rob\" |> expectsToBeFalse\n\nresult |> contains \"Jason\" |> expectsToBeFalse\n\nHere is the passing implementation of intersection as another taste of FP syntax:\n\nlet intersection right left =\n\nleft |> List.filter(fun element -> right |> List.contains element )",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Daydreams of TDD, Quantum Computers, and No Implementation\n\nWe developers used to fear the day when computers could write their own code. We were worried not only because such computers might build Terminator robots (or worse, mountains of paperclips4) and take over the world, but also because we’d be out of a job.\n\n4 about https://en.wikipedia.org/wiki/Instrumental_convergence.\n\nRead\n\nthe\n\n“Paperclip\n\nProblem”\n\nFor many years, I’ve speculated that we would eventually see a computing breakthrough the entire implementation. At the time, I envisioned a quantum computer (QC) that could take a team’s specifications and search a “solution space” for a set of machine instructions that would pass all the tests.\n\nthat would allow\n\nthe computer\n\nto write\n\nIf the computer were fast enough, it could perhaps rewrite the entire implementation each time the team added a new test scenario. In other words, neither the team nor the development computer would ever need to read or refactor the implementation.\n\nTeams could then focus entirely on writing, refactoring, and maintaining the test suite. Future high-level programming languages could be limited to the syntax and structure needed for good test-writing (for example, no more loops or branching statements). Developers would write engineering specifications as unit tests, or co-author product scenarios while working side-by-side with product designers. Or perhaps the team roles of product designer, developer, and tester would blend and merge into something new. A test-driven approach would be the de facto standard for building and maintaining software.\n\nWhile I was daydreaming about the impacts of this hypothetical QC, a different computing breakthrough was happening. Noting the surging power and popularity of artificial intelligence5 (AI) and large language models (LLMs), I began to wonder whether these new tools would someday make my predictions a reality.\n\n5 Not to be confused with the “artificial intelligence” from the pre-LLM science fiction tales, which has been relabeled artificial general intelligence (AGI), and most likely does not exist at\n\nin",
      "content_length": 2125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "the time of this writing.\n\nThe Reality of TDD, Artificial Intelligence, and “Vibe Coding”\n\nUsing a rudimentary OpenAI script that contained a simple prompt including all my tests (I could easily add tests with each run of the script), I explored whether OpenAI could generate code to pass all my tests. And it did so, repeatedly and successfully.\n\nWith each new run of the script, my AI agent did not have access to any previous prompt, dialog, or existing implementation. There was nothing for me or the AI to refactor, because it always started from scratch and overwrote all the code with each run.\n\nMy experiment was only that: a simple proof-of-concept. I had the script build simple bits and pieces of various classroom exercises (for example, the Salvo game described in the Exercises Appendix at the end of this book).\n\nThere are several ways that AI tools are currently assisting real developers. Some offer AI “autocomplete” options that are quite good at predicting what the developer intended to write next, and the complete code appears much faster than the developer could have typed it. Others read all the existing code and will suggest refactorings. The developer merely needs to glance over the proffered code and press a single key to accept the changes.\n\nintegrated\n\ndevelopment\n\nenvironments\n\n(IDEs)\n\nAI agents can even write tests for you. However, the generated tests that I saw merely confirmed that the code did what it did, not necessarily that it did what was wanted. This is the gap in the workflow where humans are still needed: Someone needs to come up with descriptive and detailed examples of what they want the software to do.\n\nRight now, various methods for incorporating AI into the full development workflow are vying for our attention. One of those approaches is called “vibe coding,” which was described to me as allowing the AI agent to write the implementation without any review. The person receiving the code then tests it and gives the AI feedback. This is happening in a few different ways:",
      "content_length": 2034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "The human manually tests the application and then explains to the AI where it failed, often using an example. They do this repeatedly until the human is satisfied with the results. At least one person who used this method expressed occasional frustration with the AI agent’s tendency to incorrectly anticipate unspoken needs. to seemingly re-interpret older requirements or\n\nThe human lets the AI write its own tests, and the human reviews and runs those tests.\n\nThe human gives the AI examples, either one at a time or in a batch, and adds to those examples if either the human realizes there is a gap in the examples (a common and natural occurrence with a test-driven approach) or the AI delivers a solution that is in some way too generalized (also common when pairs or ensembles develop code using a test-driven approach). This style most closely resembles my early experiment with OpenAI.\n\nSome things I noted about these reported experiences:\n\nIn every case, clear, specific examples—either written by or approved by the person making the requests—were necessary at some point in the workflow.\n\nExcept for the time saved by having the AI write code (which was certainly a significant savings), little time was saved during interactions with the AI. Put another way, the person interacting with the AI still spent about the same amount of overall time explaining what they wanted.\n\nNone of the applications described to me could directly impact a user’s health, finances, or safety. I asked AI guru Scott Werner if he would use vibe coding for financial, medical, or safety applications, and he responded, “No, probably not anything life critical.”\n\nComputers can’t read our minds, and they don’t do well with ambiguous instructions. When it comes to our safety, health, finances, and other critical domains, we will need to describe—to the computer and to each other—all",
      "content_length": 1877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "desired outcomes using complete, descriptive, and detailed examples. That is exactly the practice of TDD.\n\nSummary\n\nThe real power of TDD comes from thinking about what the software needs to do and coming up with examples to represent that behavior. The red– green–clean steps are not meant to be mere mechanics, but rather a scaffolding to allow developers to explore, guide, and preserve those thoughts.\n\nThe next two chapters will use an example application to explore the thinking process in detail. Approach this material as you would approach the rules of a new game. These chapters provide the rules and basic strategies by walking you through a sample “game.” The example app is a simple one, and the resulting implementation will be trivial, but you will come away with an understanding of the power of test-driven thinking.\n\nOceanofPDF.com",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Chapter 2. Basic Moves\n\nTest-Driven Development (TDD) is, in many ways, a cooperative game that you play with the computer. This isn’t meant to imply that developers are “goofing around” when they use TDD. Rather, it means that there are purposeful rules to the game (for example, no additional implementation code can be added unless a single test is failing), a prescribed order to how events unfold (as illustrated in Figure 2.1), and some tried-and-true winning strategies.\n\nThis chapter uses a simplified product request to provide a walk-through of the basic steps of TDD, while also exploring the thought process that TDD enables to create genuine, valuable software. First, though, you need to become familiar with a few of the “game pieces” and the “rules.”\n\nThe TDD Flowchart\n\nThe simple red–green–clean diagram in the Preface serves as a reminder of how straightforward TDD is as a practice, but there are a few detours that you can make along the way. The flowchart in Figure 2.1 identifies some of the most common detours.",
      "content_length": 1035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Figure 2.1 TDD flowchart with more detail.\n\nFollow this flowchart until it becomes a habit, and then you will know what to do at any point. Realize, however, that having a flowchart or detailed recipe does not limit your intellect or creativity.",
      "content_length": 245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Even a simple flowchart can include details that may not be applicable to your situation. For example, consider the first diamond-shaped decision point, “Does everything compile?” At this point in the flow, the method you are testing might not yet exist, or you might have added a parameter. Regardless of whether you have a compiled language, at this point you write just enough implementation so that the test fails due to an assertion (note the second decision diamond), and not because something is undefined or throws a null-reference exception. In this way, you are “testing the test” to ensure that it accurately represents new behavior and that it can detect any mistakes in the future.\n\nDefinitions\n\nProduct advocate: This person collaborates closely with the rest of the development team to assure that customers’ most valuable software requests are delivered first and with impeccable quality. On Scrum teams, this is the “Product Owner”; on XP teams, it is the “Onsite Customer.”\n\nAssertion: A simple test function that compares an expected value or outcome with the actual value or outcome. Basic assertions are often included with unit-test frameworks. Assertions are also known as expectations. Here are some examples:\n\nUsing JUnit:\n\nassertEquals(expectedValue, actualValue);\n\nUsing rSpec:\n\nexpect(actualValue).to be(expectedValue)\n\nThinking in Tests\n\nTDD takes what is usually an unconscious skill and makes it conscious and repeatable. Developers, testers, and product advocates already converse in examples: “When this happens, that should happen” and “When this happens, that should not happen.” We take those examples and record them",
      "content_length": 1653,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "as repeatable automated tests, thereby preserving the effort we’ve invested in that behavior.\n\nEach test is a single scenario describing—to yourself and future developers —how to use this behavior, and what you expect the new behavior to do for the caller.\n\nYou write this test, or specification, just a moment before you write that implementation. If you follow this practice regularly, eventually you will be able to type in a new test as rapidly as you can think of the new scenario. Writing the test and refining the scenario often occur together. In this way, the amount of “extra” time required to write the test before you write the the implementation approaches zero. After all, you cannot write implementation code without already knowing what it is supposed to do.\n\nGiven that the word “test” carries some cultural baggage, you might want to think of each test as being an example, specification, or scenario. Besides, it isn’t really a test until it passes for the first time. From then on, that test checks whether the behavior you’ve developed remains intact indefinitely.\n\nThe structure of each test is determined by the testing framework used. Nevertheless, each test must contain three parts:1\n\n1 You may know these components as “arrange,” “act,” and “assert.” I prefer given/when/then because those terms are linguistically natural and require little explanation. Also, alliterations can accidentally achieve alternative arrangements.\n\n1. “Given”: Set up what needs to be true in this scenario before the tested behavior can be exercised.\n\n2. “When”: Invoke the behavior you want to test.\n\n3. “Then”: Once the behavior has completed, check your expected outcomes.\n\nHere’s what these parts mean for unit testing in particular:\n\nGiven: The test creates the state necessary for this distinct scenario. For example, it might create an instance of the object under test and",
      "content_length": 1886,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "set the state on that object so it will have the resources it needs. That state makes this scenario unique.\n\nWhen: Call the method or function under development.\n\nThen: Use the test framework’s assertions to confirm that everything happened as expected. What does the tested code do? Does it return a value? Does it change its own state? Does it delegate to a dependency? The test verifies that the implementation performed those tasks correctly. Tool Tip\n\nThere is at least one Junit-style or rSpec-style testing framework for most programming languages. I have used the following and recommend them: JUnit for Java, MSTest and NUnit for .Net languages, rSpec for Ruby, Python’s built-in unittest framework, and Jasmine and Mocha for JavaScript.\n\nSome test frameworks, like Cucumber, explicitly delineate these three testing sections. In other frameworks, you could add given/when/then comments, or you could separate the three sections with a line of whitespace.\n\nCan you identify the given, when, and then in the following test?\n\nimport org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.*;\n\npublic class TrackerGroupTests {\n\n@Test void isEmptyWhenConstructed() { TrackerGroup trackers = new TrackerGroup(); assertThat(trackers.isEmpty()).isTrue(); }\n\n}\n\nThis test is so small that the given might elude the casual reader: It’s just the new TrackerGroup(). The when is the call to trackers.isEmpty(). The rest of that line is all then.",
      "content_length": 1461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "You could break the last line of the preceding test into two lines and use a temporary variable to hold the results. Alternatively, you could put the whole test on one line, as shown here:\n\nassertThat(new TrackerGroup().isEmpty()).isTrue;\n\nWhat you choose will depend on what you and your teammates consider to be clear and readable test code (a topic addressed in Chapter 5, “Sustaining a Test-Driven Practice”).\n\nOf the three parts of a test, the most often neglected or abused is then. To write a test, you must know what the outcomes should be; otherwise, you are doing nothing more than exercising the code and discarding the results, which doesn’t test anything. Even worse, a collection of such tests would give your team a false sense of security by artificially increasing code coverage.\n\nOne way to avoid this pitfall is to write the assertion first. What do you expect to be the results of this new behavior? If you don’t know that, you aren’t ready to write a test. You might need to use a calculator or a slide rule, search the Internet, or ask someone who knows what is expected, such as your product advocate.\n\nKey Lesson\n\nYou cannot write a test for a behavior until you know what the outcome(s) should be. You must know the answer!\n\nOnce you know what the outcomes should be, you will have a better idea of how the object should be called to invoke that behavior. In other words, writing the test guides your design of the object’s interface from the perspective of code that calls your code.\n\nDefinition\n\nInterface: The services an object offers to other objects, and the data types necessary for those services to operate correctly. This includes publicly accessible methods, functions, and constructors, as well as the order and type of parameters passed into these methods. In many programming languages, an object’s interface",
      "content_length": 1847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "is primarily the method signatures marked with a public keyword.2\n\n2 There are exceptions: In Java, an object within the same package can access “package protected” methods of another object. Also, C# has redefined public as those methods accessible to code outside the DLL and has the internal keyword for parts of the interface that are accessible to other objects within the DLL.\n\nNot to be confused with the Java and C# interface keyword. Whenever I do refer to the interface keyword, I will make that distinction clear.\n\nThree Basic Techniques\n\nThe three primary “moves” (originally described by Kent Beck3) you play in the TDD “game” are as follows:\n\n3 Kent Beck, Test-Driven Development by Example (Addison-Wesley, 2003), p. 13.\n\nFake It (’Til You Make It): Adding implementation code that uses a constant or replaceable value just to get the test to pass with little effort, so you can move on to the next related test.\n\nTriangulation: Writing a test for a new part of the behavior you are adding, knowing that you haven’t covered enough scenarios to justify a complete implementation.\n\nObvious Implementation: Writing an implementation that works for all existing tests and is simple enough to write without mistakes. This is not necessarily the “final” implementation.\n\nTDD isn’t a thought-free practice, but it can look that way to an outside observer, because each move is small and quick. Your experience, intelligence, and creativity are all funneled through TDD. If you follow the rules of the game diligently, you will eventually stop thinking about the rules, and you will consistently write valuable, high-quality, maintainable code.",
      "content_length": 1651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Requests, Tasks, and a To-Do List\n\nTDD is a team effort. The team typically first breaks a request up into clear, easily digestible tasks.\n\nDefinition\n\nTask: A brief description of a short-term team activity or goal. Tasks differ from requests. Each task is a reminder for the team and contains a unique step required to complete the associated requests. By themselves, tasks don’t provide value to a user; therefore, tracking hourly estimates or actuals is unnecessary.\n\nTool Tip\n\nMy teams typically use large index cards (either 4 × 6 inches or 5 × 7 inches) for requests, but a request can also be a simplified “ticket” in your tracking software. We often use color-coding to differentiate between new requests and defects to repair. We use smaller (3 × 5 inches) index cards for tasks.\n\nFigure 2.2 illustrates an example of a request.\n\nFigure 2.2 Sample request.",
      "content_length": 866,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Figure 2.3 illustrates a related task.\n\nFigure 2.3 Sample team task.\n\nMy teams try to keep tasks down to a half-day or less, so we\n\nmight integrate the changes from one task before lunch and another before the end of the day. Pairs of developers volunteer for one task at a time, rather than having tasks be preassigned. If a task takes longer than a day, we inform the rest of the team, describe our lessons learned, and ask for help if necessary.\n\nOther examples of team tasks:\n\n“Refactor the duplicated adapter code so new adapters are easier to test”\n\n“Add an Accounts_Addresses join-table to the database schema and migrate existing relationships”\n\n“Set up a dev system for the new developer, including the IDE (do we need to purchase additional licenses?)”\n\nA team task is anything the team decides they must do soon\n\nthat isn’t already part of their process. In other words, “write tests” does not need to appear on a task because it is a routine part of how a test-driven team builds software.",
      "content_length": 1001,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "The Modest To-Do List\n\nOne important TDD “game piece” that often gets overlooked is a short “to- do” list to capture future moves you don’t want to forget.4 This list holds test scenarios and refactorings for the very near future that relate to the task you are currently working on, and no other. These to-do items can occur to you before you start the task, or they can arise while you are working on a test and its implementation.\n\n4 Kent Beck referred to this as a “test list,” but it often identifies tests, refactorings, and other task-specific steps (e.g., a quick Internet search.\n\nCapture the creative thoughts that occur once you see what you’ve written and how the design is emerging. To stay focused and energized, always keep this list extremely short—maybe a dozen or fewer items.\n\nGetting Started\n\nBefore starting a task, take a moment to consider what you are about to build. Briefly describe a few small scenarios or behaviors and write them on your list.\n\nStart out with a few simple, quick wins. What sort of default (but correct) answers will this new method or function return right away? What’s the initial state of an object? How do the constructor’s parameters affect that state? For example, for an online shopping cart, no sales tax should be applied if there are no items in the cart.\n\nAnother source of quick wins is often unhappy paths, or error cases. For example, what happens when the shopper tries to add an unavailable item to the shopping cart?\n\nDefinitions\n\nPair programming: The practice of working on a development task with one other person, at one computer. Developers work as peers to complete a single task together. They can switch between driver and navigator roles whenever they choose.\n\nDriver: The person currently at the keyboard. Their responsibilities include writing clear tests, implementing those",
      "content_length": 1849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "behaviors in the most expedient way possible, noticing when a design issue is making either of those processes more difficult, and mentioning possible next refactorings or tests.\n\nNavigator: The person not at the keyboard. Their responsibilities include keeping the pair focused on the real goal of the current task, recording new ideas for tests and refactorings on the to-do list, suggesting which item to tackle next, and making sure the code’s design is headed in a reasonable direction based on what the task requires.\n\nRed: The state of having at least one test failing. “In the red” means you have caused a test to fail, either intentionally or accidentally.\n\nGreen: The state of having all tests passing, including your latest test. “Get to green” means to do something expediently to reach the “green” state. “In the green” means you are within that state, and you can safely discuss next steps, refactor, commit your changes, or write the next test.\n\nTool Tip: Managing Your To-Do List\n\nIt’s nearly impossible to derive all useful to-do items beforehand. Brainstorming scenarios as a pair or a team is a powerful technique, but you should be sensitive to the point when you have identified enough scenarios. When you feel confident enough to jump in and write that first test, it’s a good time to stop pondering and start coding. Allow more detailed scenarios and edge cases to arise as you write the code and add them to the to-do list as you think of them.\n\nIf you’re using pair programming, the navigator can write down ideas (mostly potential tests and refactorings) while the driver is writing some bit of code or test. The driver can also ask the navigator to add something they’ve noticed. Postpone lengthy discussions while there is currently a failing test (that is, while you are “in the red”). Wait until you are “in the green” to discuss next steps.",
      "content_length": 1871,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "The to-do list can be maintained in a digital text file or on a piece of paper. Small index cards (3 × 5) work well. Whenever you think of something to add, draw an empty check box. Once the item is done, check the box. If you decide a to-do item is no longer relevant, X it out.\n\nBecause an index card has limited space, you won’t be tempted to include more items than you can accomplish in less than an hour. A long to-do list often contains some items you won’t really need.\n\nOnce your index card is full and all the check boxes are\n\nchecked or X-ed out, you can get another blank card, if necessary. If you’re working remotely with other developers, any simple, shared, synchronized format will work fine. A simple cloud-based text document works extremely well.\n\nMany integrated development environments (IDEs) have a\n\nway to keep track of to-do items by letting you add a “to-do comment.” Here’s an example:\n\n// TODO: refactor away the following duplicated code\n\nThat could be helpful, especially if you are coding solo. But please, never ever push a to-do comment to your\n\nrepository. To-do comments that are not done and are stored away in the repository will build up in your code base, much like ignored compiler warnings. Your latest insightful to-do reminder will become buried among the hordes of others, which defeats the purpose of having a to-do list in the first place. No one will know who is supposed to do that item, or when. In fact, it most likely will never get done. To avoid this outcome, decide whether the item really needs to be done, and then delete from the to-do list before you push to the repo.\n\nIf you are programming closely with others in a pair or\n\nensemble, don’t use to-do comments at all. As the driver, you can simply ask the navigator to record any to-do items that pop into your head, and you can discuss the merit of this new item once you’re back “in the green.”",
      "content_length": 1907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Exercise Walk-Through\n\nImagine you’re building an application that tracks geotags attached to endangered mammalian populations (elephants, whales, tigers, ). The team has determined that they will need to be able to manipulate groups of tracker tags, each having a unique identification string. A group of tags represents some segment of a population—for example, “all right whales detected in the North Atlantic,” or “all right whales detected last winter.” The scientists using this application aim to manipulate these groups, seeking trends and behavioral patterns.\n\nHere is one developer task for this application:\n\nBuild a TrackerGroup class that can be combined with other TrackerGroup collections and can tell us if one group contains, or is contained by, another.\n\nThe following are a developer’s discussion notes from the team’s conversation with the product advocate:\n\n1. We need to create TrackerGroups from arrays of tag IDs (which will come from database queries).\n\n2. We want the TrackerGroup to be able to tell us if it’s empty.\n\n3. We need to be able to merge two groups (like an OR operator) or determine how they overlap (AND). Examples:\n\na. { A, B, C, D, E } AND { X, Y, Z, A, B, C } returns { A, B, C }.\n\nb. { A, B, C, D, E } OR { X, Y, Z, A, B, C } returns { A, B, C, D, E,\n\nX, Y, Z }.\n\nOperations should return a new TrackerGroup object containing those tag IDs.\n\n4. We need to know if one group contains all the IDs of another group.\n\n5. We need to know when two groups are the same—that is, when they contain all the same IDs.",
      "content_length": 1550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Although these requests could all fit on a single index card, the walk- through will break them down further across two chapters. First, we will start with these items:\n\n[cb] The TrackerGroup knows if it is empty.\n\n[cb] The TrackerGroup knows if a tracker ID is in the group.\n\nNote that our first to-do item comes directly from note #2, and the second to-do item is needed to support the behaviors outlined in notes #3 and #4. These are quick and easy wins that we can build upon.\n\nDefinitions\n\nImmutable object: An object whose state cannot be changed once it is created. In Java and C#, dates, strings, and integers are good examples of immutable objects. Immutables have many advantages (such as thread-safety) and very few drawbacks. Consider using immutables in your business model whenever applicable. For example, a postal Address object could have the ability to change; alternatively, you could make the object immutable, with a change of address then being represented by a fresh new object containing the new data.\n\nSprinting Baby: The term “baby steps” is already in common usage as a descriptor for taking very small steps, and all big software changes are really made up of many very small changes anyway. But “baby steps” can also imply going slowly.\n\n“Sprinting Baby” is a metaphor suggesting how small, careful steps can be done quickly, resulting in a highly productive “flow” state. Small, quick wins can also keep you motivated.\n\nAs you build confidence in your test-writing skills and in the\n\nsafety net of tests, you will find yourself adding behaviors and refactoring more quickly, taking small, quick, clear, and safe steps toward your goal. So, you’re taking baby steps, but fast. That’s the “Sprinting Baby.”",
      "content_length": 1734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Step 1: Write a Test for a New Behavior\n\nThe first step, whenever you’re doing TDD, is always the same: Write a test.\n\nTDD requires a mental shift from immediately writing the inner workings of a class or function to first crafting a precise, repeatable example of how that behavior is meant to work.\n\nThis approach might seem backwards at first. After all, in the preceding example, the list of TrackerGroup behaviors has already been decomposed into simple behaviors. You can probably guess at the implementation of each one. So, why test it before you’ve written it?\n\nMany developers have been trained to write the implementation first, then write unit tests if there’s time remaining—and there’s almost never enough time remaining. That’s one of the more obvious benefits of TDD: The implementation has tests, guaranteed.\n\nBut there’s an even more important reason to write the test first: Developers naturally think in terms of examples, and they often use examples when discussing solutions with colleagues or stakeholders. What’s new here is taking those thoughts and recording them as executable specifications— that is, as tests.\n\nIf you don’t feel comfortable writing a test first, practice writing down simple examples on a scrap of paper. For example, you might write: “When I create a TrackerGroup with an empty array, it knows it’s empty.” Then turn that example into test code without making it any more elaborate or comprehensive than what’s on the scrap of paper:\n\nimport org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.*;\n\npublic class TrackerGroupTests {\n\n@Test void isEmptyWhenGivenAnEmptyArray() { TrackerGroup emptyGroup = new TrackerGroup(new String[] assertThat(emptyGroup.isEmpty()).isTrue(); }\n\n}",
      "content_length": 1748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Why the String array? This bit of code could have also been written as follows:\n\nTrackerGroup emptyGroup = new TrackerGroup();\n\nWith TDD, the way you write a test should match your expectations for objects and methods that will be useful in production. Early on, each new test will help you design the interface of the object. Be prepared to try out different styles of interfaces early, when altering the interface will not be challenging.\n\nIn this case, the list of developer notes presented earlier says that these objects will be created from the results of database queries. Tracker IDs could be added in the constructor either all at once or by using an add() method and requiring the caller to loop through the query results and add each one individually. When given that kind of choice, favor giving your object as much of its known state as possible in the constructor, and favor “immutable” objects—that is, once created, the objects do not change.\n\nBecause this is the very first test, it won’t even compile. The next step is to write enough code so the test compiles, runs, and fails “cleanly”—that is, it fails due to the assertion.\n\nStep 2: Make It Fail Cleanly\n\nWhat you need is just enough real code to make the test fail cleanly—that is, due to an assertion in the test, rather than because of a compile error or raised exception. This achievement is what we mean by “getting to red.”\n\nWriting a test for code that doesn’t exist yet might feel unnatural, particularly in a compiled language like Java. At this point, there’s no autocomplete to help you, and your IDE will flag a lot of what you type as a compiler error.\n\nThe following code makes the previous test fail cleanly:\n\npackage crittermaps; public class TrackerGroup {\n\npublic TrackerGroup(String[] trackerIDs) { }",
      "content_length": 1791,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "public boolean isEmpty() { return false; }\n\n}\n\nModern IDEs will do a lot of the boilerplate coding for you. For example, IntelliJ wrote the skeletal TrackerGroup class and a stub for the method isEmpty(). Having the IDE do the rote work will save you time and help you stay focused on writing the interesting runtime behavior.\n\nIf you work in a language that has less syntactic overhead than Java or C#, you may find it just as easy to write all the code yourself.\n\nAlways run all tests, rather than just the new one. Read the failure message to ensure that the new test is, indeed, the test that’s failing.\n\nTool Tip\n\nRun all the tests. There’s usually a hotkey (or menu option) to run all tests in the project. Make a habit of using that hotkey. Run the tests whenever you feel the need for feedback from them. Perhaps run them twice before pushing your changes to the repository. If a single test fails, don’t commit your changes.\n\nMany modern IDEs will determine which tests are new and which tests are affected by any addition or refactoring, and will run them instantly whenever you save a change. This can provide you with continuous automated test feedback.\n\nStep 3: Make It Pass\n\nOnce you have confirmed that the test fails cleanly, you will want to make it pass as simply as possible. You want to “get to green”; that is, you want all tests to pass.\n\nReaching the point where all tests pass is a safe resting point. When you are “in the green,” you can have discussions about better implementations, refactorings, and next steps.\n\nWhen you are “in the red” (that is, you have one failing test), you need to act. If you don’t know how to get your latest test to pass, then delete it and",
      "content_length": 1695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "write a smaller or simpler one—a test that you know how to implement.\n\nWe risk losing our way whenever we leave things in an uncertain state for too long. So, strive to take small, quick, simple, and safe steps.\n\nFor the first test describing part of a new behavior, a simple technique that Kent Beck called Fake It ‘Til You Make It will often be your best move.\n\nTechnique: Fake It\n\nThis technique for “getting to green” can seem counterintuitive, not because the implementation is difficult, but because it is absurdly easy. If the test represents the tiniest request you can make of your code, then you need only add a tiny bit of code to get it to pass.\n\nHere is the simplest possible implementation to get the test to pass. Anything more would include a small amount of untested—and therefore as-yet-unjustified—behavior:\n\npackage crittermaps; public class TrackerGroup {\n\npublic TrackerGroup(String[] arrayOfIDs) { }\n\npublic boolean isEmpty() { return true; }\n\n}\n\nMost developers, until they discover for themselves the advantages of TDD, just want to “write the code!”—meaning “all the code.” Resist that temptation with every fiber of your being!\n\nFake code is simple code. Typically, fake code is a single line of code, even just the return of a constant. If your fake code starts to look like an actual implementation, ask yourself: “Have I really tested all that behavior?”\n\nHere are a few notes about the Fake It technique:\n\n“Fake It” Until You’ve Justified It\n\nTo check whether the TrackerGroup is empty, you need to know the number of trackerIDs and compare that number to zero. You",
      "content_length": 1596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "could write that simple line of code in your sleep, right?\n\nBut we haven’t written enough test code to justify a complete implementation. Instead of writing more code than is currently needed, write down a few other examples on the to-do list, and wait to create a test from one of them until this test passes.\n\nImplementing everything now, without tests, would leave a tiny gap in the safety net. That gap might not seem like a big deal now, but most defects occur because a test is missing. Have you ever spotted a mistake in the code and thought, “How did we ever think that would work?” Save yourself hours of future debugging: Always write that test first.\n\nFake Code Is Real Code (But Temporary)\n\nThe term “fake” is misleading. It’s expedient code that gets the latest test to pass. It gets you to “green” rapidly, so you can do some refactoring, merge your changes, check your to-do list, write the next test, or take a lunch break.\n\nIn real-world development, fake code rarely survives more than a few seconds, but it does do something, and it isn’t broken.\n\nThe “Sprinting Baby” approach asks you to take smaller steps rapidly and to always stay in the realm of releasable quality, if not yet releasable scope.\n\nOnce you are safely “in the green,” you are ready for refactoring.\n\nStep 4: Refactor\n\nSo far in this walk-through, there’s nothing to refactor. However, you should get into the habit of taking a second or two to look around for something to improve, even if only slightly. If you neglect this step, the code smells will gradually accumulate, and a minor, simple refactoring will become a large, unpleasant refactoring.\n\nDefinition\n\nCode smell: A common pattern visible in the structure of your code that indicates the internal design needs to be improved.",
      "content_length": 1776,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Many code smells have names, such as Long Method. You should rely on your team’s experience and preferences regarding the identification of code smells (for example, determining how long of a method is too long) and which code smells should be addressed first.\n\nThe reason why refactoring is an efficient way to design code is that it’s always easier to be a critic than an artist. Developers can quickly spot something they don’t like about a bit of code, even if they don’t immediately know what else can be done. Without the safety net of tests to protect existing behaviors, they risk breaking a behavior or making the design worse.\n\nTDD asks teams to “clean as you go” by identifying code smells and refactoring them away.\n\nRecommended Reading\n\nFor a more comprehensive study of refactoring and code smells, read Martin Fowler’s Refactoring: Changing the Design of Existing Code (Addison-Wesley Professional, 2018). The first edition contained timeless examples in Java, and the second edition expands upon those using JavaScript.\n\nRepeat the Cycle\n\nAfter refactoring, return to step 1 and write another test. Typically, the previous test suggests the next logical test. If you haven’t completed any small part of the functionality, you will want to continue working on it until it is done. Advancing the behavior you have started writing is known as the Triangulation technique.\n\nTechnique: Triangulation\n\nTriangulation is the technique you use to push a behavior further, encouraging your implementation to handle interesting edge cases or behavior that is beyond the default. This technique involves writing another test related to the same behavior as the previous test. If the previous",
      "content_length": 1695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "implementation was fake, this will cause it to be replaced with a more sophisticated implementation.\n\nDefinition\n\nBehavioral boundary: The imaginary dotted line separating two or more very closely related behaviors. Here are some common examples of such boundaries:\n\nDefault behavior and behavior that is altered by state\n\nHaving none of something and having at least one\n\nInvalid data and valid data\n\nMore will be said about behavioral boundaries in Chapter 5,\n\n“Sustaining a Test-Driven Practice.”\n\nWhen triangulating, you want to address the other “side” of the behavioral boundary you have just exposed. For example:\n\nAn empty TrackerGroup implies that a non-empty TrackerGroup could exist.\n\nA TrackerGroup that does not contain a tracker ID implies that a TrackerGroup could contain that tracker ID.\n\nAlways update your to-do list as you think of new tests to write:\n\n[cm] The TrackerGroup knows if it is empty.\n\n[cb] When *not* empty.\n\n[cb] The TrackerGroup knows if a tracker ID is *NOT* in the group.\n\n[cb] When the tracker ID *IS* in the group.\n\nTo triangulate, create a non-empty TrackerGroup:\n\n@Test void isNotEmptyWhenOneItemIncludedInConstructor() {\n\nString[] trackerIDs = { \"any ID\" }; TrackerGroup trackers = new TrackerGroup(trackerIDs); assertThat(trackers.isEmpty()).isFalse();\n\n}",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Note that the test already fails cleanly with the existing implementation. Don’t add anything that isn’t needed to get a clean failure; for example, don’t create a field for the tracker IDs:\n\npublic class TrackerGroup {\n\npublic TrackerGroup(String[] trackerIDs) { }\n\npublic boolean isEmpty() { return true; }\n\n}\n\nTo get this test to pass, you can now add an “Obvious Implementation.”\n\nTechnique: Obvious Implementation\n\nAn Obvious Implementation is the simplest implementation that is sufficient to pass the failing test without faking anything within that implementation.\n\nTo get this latest test to pass without breaking others, add some simple logic. Here is one possible Obvious Implementation:\n\npublic class TrackerGroup {\n\nprivate final String[] trackerIDs;\n\npublic TrackerGroup(String[] trackerIDs) { this.trackerIDs = trackerIDs; }\n\npublic boolean isEmpty() { return trackerIDs.length == 0; }\n\n}\n\nAn Obvious Implementation is often the code that you knew you needed all along. So, why not write it first? First, you might overlook a simple yet significant behavioral boundary, which could result in a defect during a future modification. Second—and this happens more frequently than you might think—you could make a nearly invisible mistake and spend an excessive amount of time trying to fix what you thought was “obvious.” For those reasons, take the rapid baby steps leading up to “obvious.”",
      "content_length": 1402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "This change included a replacement of the fake code. If you forget to swap out the fake code, the failing test will continue to remind you.\n\nAlways take a tiny step, run the tests, and repeat. No matter how large and complex your software becomes, small steps taken within the safety net of tests will help you complete the task faster and with fewer errors.\n\nIf you can resist taking bigger steps—either broader tests or extra implementation—eventually the TDD steps will become an embodied habit, and every step you take will be quick and safe.\n\nNext Moves\n\nA couple of items remain on the “quick wins” to-do list. We will wrap up that list and introduce some interface design trade-offs simultaneously.\n\n[cm] The TrackerGroup knows if it is empty.\n\n[cm] When *not* empty.\n\n[cb] The TrackerGroup knows if a tracker ID is *NOT* in the group.\n\n[cb] When the tracker ID *IS* in the group.\n\nYou want a method to determine whether a TrackerGroup contains a particular tracker ID. When you write the first test for this entirely new method, give some thought to the name of that method.\n\nYou could give your new method a long and descriptive name. With autocomplete built into almost every IDE and editor nowadays, you may never need to type the full name more than once. For example:\n\nboolean containsThisParticularTrackerID(String trackerID)\n\nOr, your team might prefer Java’s naming conventions, where methods that return a Boolean value usually start with “is”:\n\nboolean isTrackerAMember(String trackerID)\n\nOr you could use something short, yet clear:\n\nboolean has(String trackerID)",
      "content_length": 1582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "The key guideline is to make the method’s purpose obvious in the caller’s code. Because the first caller of a new method is a unit test, this is the best opportunity to try out different names. For example:\n\nassertThat(endangeredAfricanElephants.has(“Dumbo”)).isTrue();\n\nYou do not have to complete your to-do list in top-to-bottom order. As an example, we will implement the last item on our to-do list next:\n\n@Test void containsAnID() {\n\nString idToFind = \"the ID of interest\"; String[] trackerIDs = { idToFind }; TrackerGroup trackers = new TrackerGroup(trackerIDs);\n\nassertThat(trackers.contains(idToFind)).isTrue();\n\n}\n\nWhy \"the ID of interest\"? Unless the test data is being used to test parsing or validation of the data, favor using data that is obviously nonsensical. Unrealistic data is shorthand for “this value doesn’t affect the outcome of this test.”\n\nAlso, idToFind is used instead of duplicating the string \"the ID of interest\" in the assertion. This helps in the following ways:\n\nIt prevents typos from causing the test to fail for the wrong reasons.\n\nThe name of idToFind helps clarify how the value is being used in the test. You could use just id, but why abbreviate? It’s not any ID; it’s the idToFind.\n\nIf you let the IDE write the skeletal method implementation for you, most IDEs will use the name of the test’s local variable as the name of the method’s parameter.\n\nHere is the code to make that test fail cleanly:\n\npublic class TrackerGroup { // ...Previous code is still here; not shown for brevity...\n\npublic boolean contains(String idToFind) { return false;",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "}\n\n}\n\nAnd here is the code to make it pass, using the Fake It technique:\n\npublic boolean contains(String idToFind) {\n\nreturn true;\n\n}\n\nChoosing Triangulate, Fake It, or Obvious Implementation\n\nOnce again, you probably know exactly how to implement contains() in your preferred programming language. Even so, let yourself complete the Triangulations around each behavioral boundary before implementing an Obvious Implementation. Each failing test earns you the chance to incrementally grow the code that you were already sure you needed.\n\nYou might wonder if you could replace the fake code with the Obvious Implementation as a “refactoring.” That would be appropriate only if it does not add anything to existing behavior. Later, you might forget to write the tests that would have earned you that implementation. Most of the (quite rare) defects found in test-driven code can be traced to a missed specification.\n\nWhen you play the TDD game by the rules, you will begin to feel more confident in your implementation. Once you’ve locked down part of a behavior with one or more passing tests, you won’t have to waste time wondering “Do I have it right? What happens if ?” Replace every “What happens if ?” with a Triangulation test.\n\nHere is the Triangulation test for contains():\n\n@Test void doesNotContainAnID() {\n\nTrackerGroup trackers = new TrackerGroup(new String[]{ \"not\n\nassertThat(trackers.contains(\"cannot be found\")).isFalse();\n\n}",
      "content_length": 1440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "Could you still get this test to pass with a single, simple line of code? More to the point, is there an unspoken assumption written into the existing tests?\n\nThere will be times when the lines between the Fake It and Obvious Implementation techniques are blurry. What you always want to do is write the simplest code that will pass the tests, then find and write the triangulating test that helps you get closer to a completely implemented behavior.\n\nAs the TDD game unfolds, you will often notice that extremely brief and simple tests will get you most of the way home, but a richer and more specific test might be needed to force the code to handle the most general cases. For example, to get the currently failing triangulating test to pass, try this:\n\npublic boolean contains(String idToFind) { return trackerIDs[0].equals(idToFind); }\n\nThat makes the test pass, but it’s silly, right? It seems too complex to be fake and too wrong to be obvious. Yet it is the simplest code that makes the existing tests pass. This indicates that there is a small hole in your safety net: The tests so far have included only one tracker ID. When you spot something like this, immediately add it to your to-do list:\n\n[cm] The TrackerGroup knows if it is empty.\n\n[cm] When *not* empty.\n\n[cm] The TrackerGroup knows if a tracker ID is *NOT* in the group.\n\n[cm] When the tracker ID *IS* in the group.\n\n[cb] When the ID to be found by contains() is included among other\n\nIDs\n\nChoosing to Add or Modify Tests\n\nTo extend the testing, you could write a new test with a TrackerGroup that has more than one ID, or you could add another item to the initialization array in an existing test.",
      "content_length": 1668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "The “correct” move is always contextual. In this example, the cost of being wrong is extremely small. When you’re faced with similar decisions in your own work, here are some things to keep in mind:\n\nMore, smaller tests are typically easier to read and understand than fewer, larger tests.\n\nFavor what is most readable and maintainable to your current teammates.\n\nYou want a set of tests that will be descriptive enough to prevent future misunderstandings.\n\nIf you choose a path that leads to a poor design, how much refactoring will it take to make the design clearer?\n\nFavor clarity, expressiveness, and comprehensiveness in the collection of tests you choose to leave behind.\n\nYou could write this as a new test or modify containsAnID():\n\n@Test void containsAnIDWhenMoreThanOneToSearchThrough() {\n\nString idToFind = \"id of interest\"; TrackerGroup trackers = new TrackerGroup(new String[]{ \"some uninteresting id\", idToFind });\n\nassertThat(trackers.contains(idToFind)).isTrue();\n\n}\n\nNow to “get to green,” here is one possible Obvious Implementation:\n\npublic boolean contains(String idToFind) { for (String trackerID : trackerIDs) { if (trackerID.equals(idToFind)) return true; } return false;\n\n}\n\nIt’s time for some closure on this to-do list:\n\n[cm] The TrackerGroup knows if it is empty.",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "[cm] When *not* empty.\n\n[cm] The TrackerGroup knows if a tracker ID is *NOT* in the group.\n\n[cm] When the tracker ID *IS* in the group.\n\n[cm] When the ID to be found by contains() is included among other\n\nIDs\n\nNever Skip Refactoring\n\nAs an example, let’s replace all occurrences of idToFind with trackerToFind. This would require a few straightforward “Rename Variable” or “Change Signature” refactorings. For example:\n\npublic boolean contains(String trackerToFind) {\n\nfor (String trackerID : trackerIDs) { if (trackerID.equals(trackerToFind)) return true; } return false;\n\n}\n\nRun your tests after each refactoring, even if the IDE did all the work for you or when it seems that “nothing could possibly break.” Stay “in the green” (that is, keep all tests passing) while the stakes are low. As the system becomes more complex, the more comprehensive your safety net is, the more confident you will be in all refactorings.\n\nSave Your Changes\n\nWhenever you feel like you’ve completed a small, but significant bit of functionality, run all your tests one more time to be sure they pass. Commit and push your tests and implementation to a repository (for example, with git). Then celebrate!\n\nSummary\n\nIn this chapter, you encountered many of the “essentials” for TDD. Later chapters will build upon these skills and will introduce useful ancillary",
      "content_length": 1343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "techniques. Here are some key takeaways from this chapter.\n\nDefining Test-Driven Development:\n\nPeople naturally think about and discuss software behaviors in terms of “given, when, then.” By writing each of those scenarios into a test, you will clarify your thoughts now, and those tests will protect those behaviors from unintended alterations in the future.\n\nThe “unit” you are testing with each unit test is a discrete behavior. You cannot always match a unit of behavior to consecutive lines of implementation code. Therefore, the only place the behavior is clearly described is in the test. Each test is a step-by-step recipe for one solitary path through the code.\n\nPlaying the TDD “Game”:\n\nYou are playing to win (to avoid defects) now (by writing the test) and in the future (whenever you need to refactor). With TDD, defects are usually due to missing tests, not incorrect tests.\n\nTake baby steps, but quickly. The full “turn” of the TDD cycle should take no more than 5 minutes. Notice whenever it takes longer, and give yourself time to step back and adjust your strategy for completing your current task. Write down some smaller steps on the to-do list, then tackle each individually.\n\nDo all thinking, planning, discussing, designing, and research while “in the green”—that is, when all tests are passing.\n\nWhen “in the red” (a test is failing), get to green (all tests passing) as quickly as possible. Do not linger in the danger zone. If you can’t get from red to green, consider reverting to the last point when all tests were passing.\n\nRefactor only when “in the green.” If a test fails during a refactoring, revert to the most recent known good state, then try taking a smaller step forward.\n\nWriting Clear Tests:\n\nEvery test has three parts: given (arranging what must be true before the behavior is invoked), when (invoking the behavior), and",
      "content_length": 1862,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "then (asserting what should be true after the behavior has been invoked).\n\nYou will likely write more lines of test code than lines of implementation. Each test is a description of a single scenario. It will read like a script, with step-by-step code, and without branches or loops.\n\nDon’t guess: To write a complete test, you must know the precise intended outcomes of the behavior.\n\nNotice behavioral boundaries and remember to test all “sides” of a boundary.\n\nEach unit test will remain as a detailed engineering specification describing one scenario.\n\nFavor more tests with fewer assertions, so any future failures pinpoint and describe the behavior that broke.\n\nChapter 3, “Build upon Existing Behavior,” continues the TrackerGroup walk-through with a focus on using the behaviors you’ve tested and built so far to write further tests and implementation. Thinking test-driven helps developers avoid continuously rebuilding the same behaviors in different parts of the system, or duplicating those behaviors in the tests.\n\nThere is also a second meaning of the title of Chapter 3: We’ll be delving into what it means to think test-driven by learning from simple but common coding mistakes.\n\nOceanofPDF.com",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Chapter 3. Build upon Existing Behavior\n\nIn this chapter, we continue the TrackerGroup walk-through from Chapter 2, “Basic Moves.” With the simplest but necessary behaviors built, it will be easier to build more complex behaviors.\n\nNext Steps\n\nEach time you preserve what you wrote (both tests and implementation) in a repository, you can step back and look at the business requests and modify or re-create your to-do list. Here is our original list of developer notes:\n\n1. We need to create TrackerGroups from arrays of tag IDs (which will come from database queries).\n\n2. We want the TrackerGroup to be able to tell us if it’s empty.\n\n3. We need to be able to merge two groups or determine how they overlap (AND). Examples: { A, B, C, D, E } AND { X, Y, Z, A, B, C } returns { A, B, C }. { A, B, C, D, E } OR { X, Y, Z, A, B, C } returns { A, B, C, D, E, X, Y, Z }. Operations return a new TrackerGroup object containing those tag IDs.\n\n4. We need to know if one group contains all the IDs of another group.\n\n5. We need to know when two groups are the same—that is, they contain all the same IDs.\n\nHere’s a fresh to-do list:",
      "content_length": 1126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "[cb] Merge two groups\n\n[cb] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nGroup Tests by Behavior\n\nThere is no need to limit yourself to one test class per production class. Grouping tests into separate classes is a helpful way to categorize different responsibilities of a production class.\n\nDepending on your unit-testing framework, you might also have available a way to nest groups of behaviors within a single test file. For example, Ruby’s rSpec and JavaScript’s Jasmine and Mocha all have a describe clause that allows grouping and nesting of distinct behaviors. Java’s JUnit5 also introduced nesting.1\n\n1 I found Junit5’s nesting syntax overly noisy and unhelpful. Your mileage may vary.\n\nIn this walk-through, we’ll stick to using separate test classes.\n\npublic class CombiningTrackerGroups {\n\n@Test void combinedWithEmpty_HasSameIDs() { String foo = \"foo\"; String bar = \"bar\"; TrackerGroup someGroup = new TrackerGroup(new String[] TrackerGroup empty = new TrackerGroup(new String[]{});\n\nTrackerGroup combo = someGroup.combinedWith(empty);\n\nassertThat(combo.contains(foo)).isTrue(); assertThat(combo.contains(bar)).isTrue(); }\n\n}\n\nThis new test uses contains(), which was built in Chapter 2. Once you’ve thoroughly tested a behavior, you can use it in another test without again asserting that it works, because the tests for contains() also run every time you run all tests.",
      "content_length": 1460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Typically, you want to have only one assertion per test, but this test has multiple assertions. Including multiple assertions is a mild test smell. However, when they’re checking similar results of the same behavior, and it’s clear why you’re putting the assertions together, then you can leave them together.\n\nA stronger unit-testing guideline regarding multiple assertions is this: A unit test must not have assertions separated by more behavioral code. Put another way, a test should not have more than one when step. Multiple assertions with more than one given or when are a sure sign that multiple behaviors are being tested (see the “Test Smells and Refactorings” section in Chapter 5, “Sustaining a Test-Driven Practice.”)\n\nFake It\n\nThe previous test was written to be an easy win. Here’s the simplest passing implementation:\n\npublic TrackerGroup combinedWith(TrackerGroup otherTrackerGroup\n\nreturn this;\n\n}\n\nTriangulate\n\nFake It often reveals the other side of a behavioral boundary and suggests a simple Triangulation scenario. In this case, you could write a new test by swapping the object being called with the object being passed as a parameter, and—voila!—you have a failing test:\n\n@Test void emptyCombinedWithNonemptyGivesAllTagsFromNonemptyGroup() {\n\nString foo = \"foo\"; String bar = \"bar\"; TrackerGroup someGroup = new TrackerGroup(new String[] { foo, bar }); TrackerGroup empty = new TrackerGroup(new String[]{});\n\nTrackerGroup combo = empty.combinedWith(someGroup);\n\nassertThat(combo.contains(foo)).isTrue();",
      "content_length": 1528,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "assertThat(combo.contains(bar)).isTrue();\n\n}\n\nThese tests have a lot of duplication. We’ll add reducing the duplication to the to-do list for now, because there’s now a failing test. Always wait until all tests pass before refactoring either the tests or the implementation.\n\nThis test now justifies a minimal passing implementation.\n\nObvious Implementation\n\nYou could still “fake it”; that is, you could return the group that isn’t empty. On the one hand, a fake that has logic, or that has as many lines of code as the Obvious Implementation, is overkill. On the other hand, that a fake could still work suggests that perhaps the Triangulation test could have been more robust. But we’re “in the red” right now, so let’s “get to green” and consider our alternatives once all tests are passing.\n\nThe code to make the latest test pass might look like this:\n\npublic TrackerGroup combinedWith(TrackerGroup otherTrackerGroup\n\nList<String> allIDs = new ArrayList<String>(); allIDs.addAll(Arrays.asList(this.trackerIDs)); allIDs.addAll(Arrays.asList(otherTrackerGroup.trackerIDs)); return new TrackerGroup(allIDs.toArray(new String[0]));\n\n}\n\nIf all the internal shuffling between Java’s arrays and Lists bothers you, you’re not alone.2 Now that the tests are passing, let’s refactor the implementation.\n\n2 Merging two arrays is simpler in some languages: In Ruby, the + operator concatenates two arrays into a new one. In Python, however, + performs matrix algebra on the two arrays. That’s not what I expected! Good thing I wrote a test first.\n\nRefactoring from Obvious to Better\n\nWith the passing tests in place, replacing one simple but awkward implementation with a cleaner one is a matter of refactoring. If the refactoring occurs to you while you are writing your minimal passing implementation, add the refactoring step to your to-do list.",
      "content_length": 1841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "There are now two new items on our to-do list. I chose not to mark “merge two groups” as complete, because the tests are missing some richness:\n\n[cb] Refactor String[] to List<String> internally\n\n[cb] Merge two groups with non-empty groups!\n\n[cb] Clean up the combinedWith tests\n\n[cb] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nAdding examples and refactorings to the list allows you to weigh your options. You might choose to get more tests passing first and see where that leads you. Or, if writing more tests will be easier due to a refactoring—a very common outcome—you would want to refactor first.\n\nHere is a refactored TrackerGroup class that uses Lists internally but preserves the public constructor that takes an array:\n\nimport java.util.ArrayList; import java.util.Collections; import java.util.List;\n\npublic class TrackerGroup {\n\nprivate final List<String> trackerIDs;\n\nprivate TrackerGroup(List<String> listOfIDs) { this.trackerIDs = listOfIDs; }\n\npublic TrackerGroup(String[] trackerIDs) { this(new ArrayList<String>()); Collections.addAll(this.trackerIDs, trackerIDs); }\n\npublic TrackerGroup combinedWith(TrackerGroup otherTrackerG List<String> allIDs = new ArrayList<String>(); allIDs.addAll(this.trackerIDs); allIDs.addAll(otherTrackerGroup.trackerIDs); return new TrackerGroup(allIDs); }",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "public boolean isEmpty() { return trackerIDs.isEmpty(); }\n\npublic boolean contains(String trackerToFind) { return trackerIDs.contains(trackerToFind); }\n\n}\n\nThis refactoring of TrackerGroup left the public interface unchanged and the tests untouched. That won’t always be true. If, for example, you use a Rename Method refactoring on contains() to call it has(), you will change the tests as well.\n\nYour IDE’s built-in refactorings will take care of all details that will allow the code to compile and the tests to continue passing. For example, the Rename Class refactoring in a Java IDE will also rename the source file, so an automated refactoring is generally much safer than using a global find- and-replace operation.\n\nIf you think you are finished with a refactoring, but one or more tests fail, it’s a sign that the refactoring step was a larger one than you were prepared to take. Whenever that happens, revert to a state where tests were passing and take a smaller step toward your design goal.\n\nAlways give your to-do list a quick glance. Check the boxes you’ve completed, and X out any that no longer seem necessary. Checking a box on the to-do list is a micro-celebration. I wouldn’t be surprised to learn that there’s a tiny release of endorphins each time I check a to-do box.\n\n[cm] Refactor String[] to List<String> internally\n\n[cb] Merge two groups with non-empty groups!\n\n[cb] Clean up the combinedWith tests\n\n[cb] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?",
      "content_length": 1551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Let’s work on the next item in the list: “Merge two groups with non-empty groups!”\n\nImproving the Tests\n\nAn empty TrackerGroup was used for the following reasons:\n\nIt was easiest to create.\n\nThe initial implementation was easy to fake (but only once).\n\nIt might have seemed like a behavioral boundary.\n\nBut is there a behavioral boundary between an empty TrackerGroup and a non-empty TrackerGroup? Not really: When you triangulate around a true behavioral boundary, you will need to add or change behavioral code. We did not need to add extra code to handle the empty/non-empty boundary. Put another way, empty/not-empty is a test-data boundary, but not a behavioral boundary.\n\nThe implementation already handles the general case, but the tests do not reflect that general case. You have two options: write a new test with richer test data (one that does not use an empty TrackerGroup) or alter the existing tests.\n\nInvoke the Jetlagged Intern\n\nCan you imagine a scenario where someone on your team might try to simplify or optimize the code, but instead mistakenly breaks it?\n\nThe combinedWith(), and a pretty good Fake It move:\n\nfollowing\n\nis also a minimal passing\n\nimplementation\n\npublic TrackerGroup combinedWith(TrackerGroup otherTrackerGroup if (this.trackerIDs.size() > otherTrackerGroup.trackerIDs.s return this; else return otherTrackerGroup;\n\n}\n\nfor",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "It’s also wrong, but it passes all the existing tests. There’s a “gap” in our safety net.\n\nIf you spot this gap earlier, follow the standard TDD steps and Triangulate further. But take comfort: Even the best developers and testers will occasionally miss a good triangulating example until they examine their implementation.\n\nStep back on occasion, while “in the green,” and imagine yourself as an earnest but jetlagged intern or a new hire. See if there’s a way you could accidentally break the implementation without the tests detecting the error. Then fix the tests so that those potential mistakes cannot ever reach production.\n\nDefinition\n\nJetlagged Intern Mind: An ancillary TDD practice where the developers look over the existing tests and implementation and ask, “Did we miss anything? Do our tests truly represent the behaviors clearly, or did we take a shortcut that leaves us vulnerable to misinterpretation?”\n\nFirst, put yourself in the role of the “jetlagged intern”— someone new to the team who has just arrived from an overseas business trip and is attempting to be immediately useful by simplifying or optimizing parts of your code. Could the jetlagged intern make a reasonable but invalid assumption about the existing implementation?\n\nThen, ask yourself if the safety net would stop this person\n\nfrom introducing a defect. Do you have at least one fast automated test that will keep that new defect from reaching production? If not, there’s a gap in the safety net.\n\nFor each gap, write a new test or improve an existing test to represent the example that will protect you against that defect. Presumably, this new test will pass right away.\n\nStrengthening the safety net isn’t meant to defend against only\n\na jetlagged intern. It also protects the team from inadvertently introducing that same mistaken optimization or simplification during future critical refactorings. A detailed safety net keeps the",
      "content_length": 1921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "team from having to know every detail of every line of code in the code base.\n\nInvoking Jetlagged Intern Mind whenever your intuition warns you that there’s a gap in the safety net will also improve your TDD game overall by strengthening your ability to create realistic and well-targeted examples.\n\nYou could either add the following test or use it to replace or modify one of the previous combinedWith() tests. Before you run it, though, ask yourself if it will fail:\n\n@Test void combiningTwoGroups_GivesNewGroupContainingAllIDs() {\n\nString one = \"one\"; String dup = \"duplicate\"; String three = \"three\"; TrackerGroup a = new TrackerGroup(new String[] { one, dup } TrackerGroup b = new TrackerGroup(new String[] { dup, three TrackerGroup combo = b.combinedWith(a); assertThat(combo.contains(one)).isTrue(); assertThat(combo.contains(dup)).isTrue(); assertThat(combo.contains(three)).isTrue();\n\n}\n\nThis test already passes because earlier we wrote a minimal passing implementation. Invoking Jetlagged Intern Mind isn’t part of the original TDD game, but it is an important practice that allows you to notice your blind spots and will improve your test-driven skills over time.\n\nWhenever you have a nagging feeling that you’ve written more code than your existing tests require, take a moment to review and strengthen the safety net around the additional behavior.\n\nRefactoring the Tests\n\nIf you cannot readily tell the difference between two tests and what they’re testing, refactor them to make them clearly distinct. Doing so will make the tests easier and quicker for new team members to digest, and will often make future tests easier to write.",
      "content_length": 1648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Refactor based on code smells: Choose clear names for tests and test data, remove duplication, and ensure your test data avoids special domain values unless those values are intended to alter behavior.\n\nHere is the result of a few refactorings of this chapter’s test class:\n\npackage tests;\n\nimport crittermaps.TrackerGroup;\n\nimport org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test;\n\nimport static org.assertj.core.api.Assertions.*;\n\npublic class CombiningTrackerGroups { private TrackerGroup emptyGroup; private TrackerGroup groupA; private TrackerGroup groupB; private static final String IN_GROUP_A = \"one\"; private static final String IN_A_AND_B = \"duplicate\"; private static final String IN_GROUP_B = \"three\";\n\n@BeforeEach void createSampleGroups() { emptyGroup = new TrackerGroup(new String[]{}); groupA = new TrackerGroup( new String[] {IN_GROUP_A, IN_A_AND_B}); groupB = new TrackerGroup( new String[] {IN_A_AND_B, IN_GROUP_B}); }\n\nprivate void assertGroupContainsAll(TrackerGroup result, String[] expectedElemen for (String nextElement : expectedElements) { assertThat(result.contains(nextElement)).isTrue(); } }\n\n@Test void combiningTwoGroups_GivesNewGroupContainingAllIDs() { TrackerGroup result = groupB.combinedWith(groupA); assertGroupContainsAll(result, new String[] {IN_GROUP_A, IN_A_AND_B, IN_GROUP_ }",
      "content_length": 1336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "@Test void emptyCombinedWithNonempty_HasAllFromNonempty() { TrackerGroup result = emptyGroup.combinedWith(groupA); assertGroupContainsAll(result, new String[] {IN_GROUP_A, IN_A_AND_B}); }\n\n}\n\nThis is what changed:\n\n1. Created constants for frequently used test values. This reduces duplication, improves readability, and avoids failures due to typos within a string—for example, \"RANDOM\" versus \"RAND0M\".3 3 Not a contrived example! Four developers all stared at a failure message that said “Expected ‘X3O4B’ but was ‘X304B’” for about 20 minutes, before someone figured it out. That’s one risk of using real airline confirmation numbers in unit-test data.\n\n2. Reduced duplication by creating a custom assertion via Extract Method.\n\n3. Removed one of the two tests using an empty group. It was redundant and could mislead a developer into thinking empty groups were special in a behavioral way.\n\nChapter 5 covers good test-writing practices, “test smells,” and other test refactorings in more detail.\n\nRemoving the Symptomless Defect\n\nTrackerGroup’s combinedWith() implementation has a potential flaw. The code copies every element of both TrackerGroups into the combo. What if an ID appears in both TrackerGroups? With the current implementation, duplicate entries would occur. That might not cause any visible problems with the requested behaviors now, but it could cause trouble later. Let’s add “prevent duplicate IDs” to the list and take care of it now:\n\n[cm] Refactor String[] to List<String> internally\n\n[cm] Merge two groups with non-empty groups!",
      "content_length": 1556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "[cm] Clean up the combinedWith tests\n\n[cb] Prevent duplicate IDs\n\n[cb] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nLet’s ask TrackerGroup for a count of IDs. Our product advocate might not have a need for that functionality yet, but we need it to fix a defect. Avoid adding getters, setters, or behaviors just because “we might need it someday,” but recognize that sometimes you may need to add behavior to make testing simpler. Occasionally, if it’s useful for the tests, that behavior will prove useful for the product.\n\nFirst, here’s a quick test to build the simple behavior of counting the number of IDs:\n\n@Test void canCountNumberOfIDs() {\n\nTrackerGroup group = new TrackerGroup(new String[] {\"one\", assertThat(group.idCount()).isEqualTo(3);\n\n}\n\nAnd here’s the code to make it pass:\n\npublic int idCount() {\n\nreturn trackerIDs.size();\n\n}\n\nNow we add a new test that checks for accidental duplicate IDs:\n\n@Test void combiningTwoGroups_DoesNotAddDuplicates() {\n\nTrackerGroup result = groupB.combinedWith(groupA); assertThat(result.idCount()).isEqualTo(3);\n\n}\n\nUnsurprisingly, this results in the following failure:\n\norg.opentest4j.AssertionFailedError: Expecting:\n\n<4>\n\nto be equal to:",
      "content_length": 1265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "<3>\n\nbut was not. Expected :3 Actual :4\n\nThere are many ways to fix this defect. One quick and simple way in Java is to replace ArrayList<String> with HashSet<String>. That approach will require a couple of refactorings, so at this point we would temporarily disable the combiningTwoGroups_DoesNotAddDuplicates() test.\n\nWe’ll first refactor every occurrence of “new ArrayList<String>()” into a private static factory method, freshStoreForIDs(), and change every other reference to the type of the tag ID storage to a Collection (the common ancestor of List and Set). That way the fix to avoid duplicates involves a one-line change to freshStoreForIDs().\n\nTypically, whether you do your refactoring before writing the test or after it passes is up to you. Always be sure you refactor only when all tests are passing. In this case, the easier path is to refactor first, then reactivate the test, then write the idCount() code to make it pass:\n\npackage crittermaps;\n\nimport java.util.*;\n\npublic class TrackerGroup {\n\nprivate static Collection<String> freshStoreForIDs() { return new HashSet<String>(); }\n\nprivate final Collection<String> trackerIDs;\n\nprivate TrackerGroup(Collection<String> listOfIDs) { this.trackerIDs = listOfIDs; }\n\npublic TrackerGroup(String[] trackerIDs) { this(freshStoreForIDs()); Collections.addAll(this.trackerIDs, trackerIDs); }\n\npublic TrackerGroup combinedWith(TrackerGroup otherTrackerG Collection<String> allIDs = freshStoreForIDs(); allIDs.addAll(this.trackerIDs);",
      "content_length": 1493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "allIDs.addAll(otherTrackerGroup.trackerIDs); return new TrackerGroup(allIDs); }\n\npublic boolean isEmpty() { return trackerIDs.isEmpty(); }\n\npublic boolean contains(String trackerToFind) { return trackerIDs.contains(trackerToFind); }\n\npublic int idCount() { return trackerIDs.size(); }\n\n}\n\nTriangulating Away Side Effects\n\nImagine that every time you turned up the volume on your music player, it also advanced to the next song. That would be really irritating, right?\n\nThat particular side effect may be unlikely, but some classes of side effects can certainly creep into your system if you’re not thinking test-driven. The next request on the list, “Determine how two groups overlap,” demonstrates this possibility.\n\n[cm] Refactor String[] to List<String> internally\n\n[cm] Merge two groups with non-empty groups!\n\n[cm] Clean up the combinedWith tests\n\n[cm] Prevent duplicate IDs\n\n[cb] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nWe write a test:",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "@Test void overlapContainsIDWhenInBothGroups() { TrackerGroup result = groupA.overlap(groupB); assertThat(result.contains(IN_A_AND_B)).isTrue(); }\n\nHere is some new TrackerGroup code to get the test to fail cleanly:\n\npublic TrackerGroup overlap(TrackerGroup otherTrackerGroup) {\n\nreturn new TrackerGroup(new String[] {});\n\n}\n\nAnd here’s some simple Fake It code to make the test pass:\n\npublic TrackerGroup overlap(TrackerGroup otherTrackerGroup) {\n\nreturn this;\n\n}\n\nDo you see the problem with this fake implementation? The result does, indeed, contain the expected IDs, but it also includes an undesirable extra ID.\n\nThe Jetlagged Intern Mind practice can help you avoid unexpected and undesirable side effects. You can’t test away all possible mistakes, but if you notice a likely erroneous side effect, write a test that assures this mistake never occurs.\n\nHere we Triangulate to make sure the “side effect” of including other IDs doesn’t happen:\n\n@Test void overlapDoesNOTContainIDsThatAreNOTInBothGroups() {\n\nTrackerGroup result = groupA.overlap(groupB); assertThat(result.contains(IN_GROUP_B)).isFalse(); assertThat(result.contains(IN_GROUP_A)).isFalse();\n\n}\n\nThen we write an Obvious Implementation (preferably obvious to all developers on your team):\n\npublic TrackerGroup overlap(TrackerGroup otherTrackerGroup) {\n\nCollection<String> foundInBoth = freshStoreForIDs(); otherTrackerGroup.trackerIDs.forEach( nextID -> { if (contains(nextID))",
      "content_length": 1447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "foundInBoth.add(nextID); }); return new TrackerGroup(foundInBoth);\n\n}\n\nDesign Detour: Let Design Choices Propagate Naturally\n\nA good design clearly communicates its intent to the team and is easy to maintain. “Good design” is contextual: It depends on the combined experience of team members with objects, functional programming, your programming languages, various schools of design wisdom (I recommend as starting points Daniel Terhorst North’s “CUPID” and Kent Beck’s “Four Rules of Simple Design”), and the intuitive wisdom of code smells.\n\nFor example, you may want to avoid using lambdas until all developers on the team are comfortable using lambdas. The most efficient way to share design knowledge is to play the TDD game in pairs or as an ensemble. My Extreme Programming (XP) teams typically change pairs frequently, which helps propagate new design idioms and technical knowledge throughout the code and the team.\n\nAvoid going through your code all at once and replacing an old idiom with a new one. Older code can be refactored when it needs to do something new. But whenever it is time to enhance old code, take the time to refactor it to conform to your team’s latest design preferences.\n\nA Picture Is Worth a Thousand Words\n\nSometimes, the tests, code, and to-do list aren’t enough to clarify all that needs to be done.\n\nLet’s use the next to-do item—“Does one group contain all the IDs of another group?”—as an example. Whenever I deliver my TDD course to developers, this is the point where roughly four out of five pairs of developers write tests that are less robust than they need to be.",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "[cm] Refactor String[] to List<String> internally\n\n[cm] Merge two groups with non-empty groups!\n\n[cm] Clean up the combinedWith tests\n\n[cm] Prevent duplicate IDs\n\n[cm] Determine how two groups overlap\n\n[cb] Does one group contain all the IDs of another group?\n\n[cb] Are two groups equal?\n\nLet’s skip a few details and jump to the typical results. Here are the tests:\n\npackage tests;\n\nimport crittermaps.TrackerGroup;\n\nimport org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test;\n\nimport static org.assertj.core.api.Assertions.*;\n\npublic class ContainsAll {\n\nprivate TrackerGroup smallGroup; private TrackerGroup biggerGroup;\n\n@BeforeEach void createGroups() { smallGroup = new TrackerGroup(new String[] { \"a\", \"b\", biggerGroup = new TrackerGroup(new String[] { \"a\", \"b\", }\n\n@Test void knowsWhenItDoesNOTContainAllIDs() { assertThat(smallGroup.containsAll(biggerGroup)).isFalse }\n\n@Test void knowsWhenItContainsAllIDs() { assertThat(biggerGroup.containsAll(smallGroup)).isTrue( }\n\n}",
      "content_length": 996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Here is a typical implementation:\n\npublic boolean containsAll(TrackerGroup otherTrackerGroup) {\n\nfor (String nextElement : otherTrackerGroup.trackerIDs) { if (!contains(nextElement)) return false; } return true;\n\n}\n\nBut wait! Do you see how those previous two tests do not fully represent the implemented behavior? We jumped to Obvious Implementation too soon.\n\nTo illustrate, let’s instead implement containsAll() with the following Fake It move:\n\nreturn trackerIDs.size() > otherTrackerGroup.tr\n\nDo all tests still pass? Yes. Is this a minimal passing implementation? Sure. But is it correct? No. The implementation does what we asked it to do, but the tests missed an important behavioral boundary.\n\nYour testing challenges will be much more complex than this example, but here are a few suggestions:\n\n1. Take a 10-minute break. Take a walk or grab a snack. Don’t spend the break worrying about the challenge. Instead, do something that takes your mind off the challenge. Perhaps the solution will present itself.\n\n2. Hold a CRC card session with a few other teammates. (Chapter 5, “Sustaining a Test-Driven Practice,” covers CRC cards.)\n\n3. Discuss the challenge with another developer, and draw some simple diagrams on a whiteboard: a UI mock-up, a UML class diagram, a sequence diagram, a state diagram, or—in this case—a Venn diagram.\n\nFigure 3.1 shows the two TrackerGroups used in the previous tests.",
      "content_length": 1409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Figure 3.1 Insufficient example data.\n\nWhat is missing is a robust counterexample, as illustrated in Figure 3.2.",
      "content_length": 112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Figure 3.2 Better example data.\n\nThe “other” TrackerGroup doesn’t have to be { x, y, z }. It could also be { a, b, x } or { a, b, c, x }. It needs at least one ID that does not appear in the “bigger” TrackerGroup.\n\nHere are the improved tests:\n\npublic class ContainsAll {\n\nprivate TrackerGroup smallGroup; private TrackerGroup biggerGroup; private TrackerGroup otherGroup;",
      "content_length": 372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "@BeforeEach void createGroups() { smallGroup = new TrackerGroup(new String[] { \"a\", \"b\", biggerGroup = new TrackerGroup(new String[] { \"a\", \"b\", otherGroup = new TrackerGroup(new String[] { \"x\", \"y\", }\n\n@Test void knowsWhenItDoesNOTContainAllIDs() { assertThat(biggerGroup.containsAll(otherGroup)).isFalse }\n\n@Test void knowsWhenItContainsAllIDs() { assertThat(biggerGroup.containsAll(smallGroup)).isTrue( }\n\n}\n\nThese tests will break the fake code and earn our original Obvious Implementation.\n\npublic boolean containsAll(TrackerGroup otherTrackerGroup) {\n\nfor (String nextElement : otherTrackerGroup.trackerIDs) { if (!contains(nextElement)) return false; } return true;\n\n}\n\nCombining Behaviors\n\nOne task remains. It involves an important design decision, and a delightfully simple conclusion:\n\n[cm] Refactor String[] to List<String> internally\n\n[cm] Merge two groups with non-empty groups!\n\n[cm] Clean up the combinedWith tests\n\n[cm] Prevent duplicate IDs\n\n[cm] Determine how two groups overlap\n\n[cm] Does one group contain all the IDs of another group?",
      "content_length": 1056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "[cb] Are two groups equal?\n\nYou are asked to provide a way to compare two TrackerGroups for equality. Should you override Java’s built-in equals() (Equals() in .Net) or create your own comparison method using your domain language?\n\nDesign Detour: Choosing equals(object other) or isEqualTo(TrackerGroup other)\n\nFor TrackerGroup—and most objects within a business application domain—if you need to know if two instances are equal, build a way to compare two instances without overriding any externally inherited methods. Use a name that is clear and a signature that requires a type within your business domain. For example:\n\npublic boolean isEqualTo(TrackerGroup other)\n\nWhen would you instead override Java’s equals() (.Net’s\n\nEquals())?\n\n1. When you are building a framework for use by developers, particularly when they are external to your team\n\n2. When instances could potentially be used within other common frameworks (for example, Collections)\n\n3. When the objects in your framework are considered “primitive”; that is, when they are used mostly as data- holders with limited behaviors or that rely upon familiar operators\n\nA classic example that meets all three criteria is if you were tasked with adding complex numbers (for example, 2.4 + 3.333i) to a programming language that doesn’t already support them (for example, Java).\n\nIf you do choose to override the built-in equals(), remember\n\nto test these edge cases:",
      "content_length": 1427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "1. group.equals(null) should return false, not throw an exception.\n\n2. group.equals(\"I am a string\") should return false. The correct method boolean equals(Object other) in Java, and public override bool Equals(object other) in C#. is\n\n3. Two objects that are equal (but are not necessarily the same instance) must return the same hash code (in Java and .Net).\n\n4. Test that equals() is false when a TrackerGroup is compared against an object of a different type that has the same hash code. For example:\n\n@Test void whenNotEqualDespiteEqualHashcodes() {\n\nassertThat(group.equals(new Integer(group.hashCode())\n\n}\n\nIf you skip this test, a jetlagged intern could replace your\n\nequals() implementation with this faulty one:\n\nreturn this.hashCode() == other.hashCode();\n\nTool Tip\n\nA unit-test framework is a good workbench for running quick experiments on an unfamiliar framework or syntax. For example, to assure myself that an Integer’s hash code was the integer value itself, I wrote this test:\n\n@Test void integerHashcodeIsObvious() {\n\nint aHashCode = 16309; assertThat(new Integer(aHashCode).hashCode()).isEqual\n\n}\n\nLet’s build our domain-specific isEqualTo() method using TDD, starting with a fresh test class for this behavior:",
      "content_length": 1231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "package tests;\n\nimport crittermaps.TrackerGroup; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.*;\n\npublic class Equality {\n\nprivate TrackerGroup group;\n\n@BeforeEach void createInterestingGroup() { group = new TrackerGroup(new String[] { \"this\", \"is\", \" }\n\n@Test void equalWhenTheyHaveTheSameIDs() { TrackerGroup sameIDsDifferentOrder = new TrackerGroup(new String[] { \"interesting\", assertThat(group.isEqualTo(sameIDsDifferentOrder)).isTr }\n\n}\n\nMake it fail cleanly with either of the following additions to TrackerGroup.\n\nEither:\n\npublic boolean isEqualTo(TrackerGroup other) {\n\nreturn false;\n\n}\n\nOr, if your IDE will automatically create the tested method for you:\n\npublic boolean isEqualTo(TrackerGroup other) {\n\nthrow new NotImplementedException();\n\n}\n\nTo make it pass:\n\npublic boolean isEqualTo(TrackerGroup other) {\n\nreturn true;\n\n}\n\nNow create one more test:",
      "content_length": 941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "@Test void whenNotEqual() {\n\nTrackerGroup notTheSameGroup = new TrackerGroup(new String[] {\"not\", \"as\", \"intere assertThat(group.isEqualTo(notTheSameGroup)).isFalse();\n\n}\n\nOur reward for saving this task until the end: a one-line minimal passing implementation!\n\npublic boolean isEqualTo(TrackerGroup other) { return this.containsAll(other) && other.containsAll(thi }\n\nIt’s a great feeling to see the list completed!\n\n[cm] Refactor String[] to List<String> internally\n\n[cm] Merge two groups with non-empty groups!\n\n[cm] Clean up the combinedWith tests\n\n[cm] Prevent duplicate IDs\n\n[cm] Determine how two groups overlap\n\n[cm] Does one group contain all the IDs of another group?\n\n[cm] Are two groups equal?\n\nAnd all of the tests pass (Figure 3.3)!",
      "content_length": 746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Figure 3.3 All tests passing!\n\nIt’s time to integrate your changes and celebrate.\n\nOptimize for the Right Reasons\n\nIn this example, the last method we wrote—isEqualTo()—could become a challenging performance bottleneck given enough data. isEqualTo() calls containsAll() twice, and containsAll() in turn calls contains() for each ID. Then contains() delegates to the enclosed HashSet, which is— thankfully—optimized for larger datasets.\n\nThere is likely an algorithm that would be more efficient for much larger datasets. Replacing the implementation with another, more efficient algorithm could be considered a reasonable refactoring. There are two important questions to ask yourself and your team before proceeding:\n\n1. Is it necessary?\n\nOptimizing parts of the code that are not the real bottleneck is unnecessary and could even make the real bottleneck worse. First, identify the actual bottleneck. Then, write a lightweight performance test isolating the area of code you plan to improve. This could be as simple as taking a timestamp at the beginning and end of a CPU-",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "intensive call, then asserting that the difference is shorter than an acceptable maximum (include a margin of error; otherwise, the test may later fail on rare occasions and halt your productivity). Adjust the maximum so that the test fails consistently, then replace the inefficient implementation with a better one (incrementally, if possible).\n\n2. Is it maintainable?\n\nTypically, less code is better. For example, the one-line implementation of isEqualTo() is easy to understand. You could incur a maintenance cost for a more complicated implementation: A developer’s time is currently the most expensive part of software development.\n\nIf you spend a lot of time making every bit of your code ultra-efficient— memory-wise or clock-cycle-wise (or both)—you are likely spending time that would be better spent building much-needed functionality.\n\nSummary\n\nIn this chapter, we expanded upon the basics of the TDD game as well as what it means to think “test-driven.”\n\nKey Takeaways\n\nThe TDD game is best played using a Sprinting Baby strategy:\n\n1. Ask, “What is the simplest test we can write to earn the next bit of implementation?” where “simplest” does not always mean easiest to write. A copy-and-paste of previous tests will often lead to gaps in behavioral coverage.\n\n2. Write the minimum passing implementation for that test without breaking any other tests. Take care not to jump to Obvious Implementation too soon.\n\nInvoke Jetlagged Intern Mind whenever you feel you might have added more behavior than you’ve tested.\n\nCombine Sprinting Baby and Jetlagged Intern Mind to craft the most “leak-proof” safety net possible.",
      "content_length": 1628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Any behavior that is fully tested can be used in the implementation and build isEqualTo(TrackerGroup other), you could optionally replace the implementation of the custom assertion assertGroupContainsAll(). in\n\nUse new language features only after assuring that they won’t negatively impact customers (i.e., do they need to be upgraded, too?) or your fellow developers.\n\nWith TDD, you can still introduce a defect if your test data is too simple. Ask yourself, “How will different test data alter the behavior?” This might lead to a new or better test.\n\nTests deserve refactoring as much as the implementation. Preserve the intent of each test (the expected behavior), while clarifying and simplifying how it tests that single behavior.\n\nWhat to Try Next\n\n1. Build Your Own TrackerGroup\n\nStart over and rebuild TrackerGroup your way, following the TDD steps rigorously. Use the to-do list specifications in this and the previous chapter, but ignore the book’s tests and implementations. Perhaps write down the to-do items, then close the book.\n\n2. Change Something and Build It Again\n\nStart over, completely from scratch, but change something.\n\nTry working as a pair or ensemble. Be sure everyone sticks to the rules of the TDD game.\n\nTry a different programming language. C# and Java are too similar. Perhaps try JavaScript, Ruby, or Python. Choose a language that you are comfortable with or one that you would like to learn. TDD is a great way to learn a new programming language.\n\nShuffle the order of the to-do items. You can build software in any order, if—as you go—you test all behaviors and refactor away code smells and test smells.",
      "content_length": 1642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "If you’ve built TrackerGroup twice, consider or discuss the following questions:\n\nWas the second time around faster or slower? Why?\n\nWas it easier or harder, and why?\n\nDoes the implementation look different?\n\nDo the tests look different?\n\nChapter 4, “Exceptional Behaviors,” is a brief “addendum” to the TrackerGroup example. It covers how to test scenarios that require an exception to be thrown, and how to test the behavior of custom exception classes.\n\nOceanofPDF.com",
      "content_length": 471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Chapter 4. Exceptional Behaviors\n\nAll the code you write is important and deserving of tests. Whenever your code throws or catches an exception or raises an error, that too is your code’s behavior and can be tested.\n\nKey Lesson\n\nTest your code’s behaviors. This includes interacting with an external dependency, throwing an exception, or formatting a simple output string.\n\nYou rarely need to test simple data accessors and mutators (“getters” and “setters”) or .Net “properties.” Your compiler in a strongly typed language will be your first line of defense for simple getters and setters.\n\nHowever, if the setter can throw an exception when the data is\n\ninvalid, test that. If the getter formats a string based on one or more fields, test that.\n\nRaising/Throwing\n\nThrowing an exception (or “raising an error” in many languages) is intended to let a developer, or DevOps person, know that something technical is broken, not that an end user made a typo. This includes developer mistakes (they will happen), unavailability of a service, and so on.\n\nAn exception is merely an object. Raising or throwing an error or exception creates an alternative return path. Nothing terribly mysterious is happening. The thrown exception will bubble up the call stack, ignoring whatever else",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "was going on (if statements, loops, and the like) until it’s caught by a catch clause or until it reaches the top of the call stack and halts the app.\n\nMost of the web apps I have worked on using TDD followed similar guidelines regarding exceptions:\n\n1. Catch Java’s checked exceptions from external dependencies (for example, database access) immediately. For example, catch Java’s JDBCException rather than declaring that your domain method “throws JDBCException.”\n\n2. Never ignore or merely log an exception! Wrap the external exception in your team’s customized domain exception and throw that. Wrapping the original exception preserves its stack trace.\n\n3. Capture any relevant local state in your domain exception’s constructor parameters and add that data to the exception message.\n\n4. Be extra careful to avoid including sensitive user data in any log or error message. For example, no one except the user ever needs to see the user’s password, even if it’s mistyped.\n\n5. Automatically deliver each domain exception to the development or support team. Log it or email it to a well-monitored inbox. In at least one case, my team was able to log and email the stack trace and message within the domain exception’s own constructor, after calling the parent class constructor. If the log file or inbox becomes unmanageably large, the team needs to ask themselves why the system is throwing so many exceptions. After all, exceptions are meant to be exceptional.\n\n6. Catch the exception only when something can be done about it—for example, perform a database rollback or notify the user that the attempted action did not occur. Retry loops often create more problems than they solve.\n\n7. Allow the exception to bubble up the stack until it reaches the point where the app must alert the user. Generally, the audience for an exception message is the development, support, or ops teams, and not",
      "content_length": 1895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "the end user. Consider providing the user with the following information:\n\na. A message that what they attempted failed to happen\n\nb. Why it happened, if known (a high-level explanation, not a stack\n\ntrace)\n\nc. What the system is going to do next (e.g., report the error to\n\nsupport)\n\nd. Next steps to retry or resolve the issue\n\ne. A unique support ticket number should they choose to call support\n\nfor more help\n\nIn this way, each thrown exception will guide you through the specific code path that generated the error, and the exception’s message will reveal the data that played a role. Eventually, these exceptions will become rare and will either indicate an unstable external resource or a missed test scenario (also known as a defect or bug).\n\nA Test That Passes When the Code “Fails”\n\nContinuing the TrackerGroup example from Chapters 2 and 3, imagine that legitimate tracker IDs come from either a database or a barcode reader, but on rare occasions, a null value from a defective barcode reader makes it into the list.\n\nYou want the test to pass when the correct exception type is thrown. The test should fail if no exception is thrown or if the wrong exception type is .Net’s example, (for thrown NullReferenceException):\n\nJava’s NullPointerException\n\nor\n\n@Test void throwsOnNullID() { assertThrows(InvalidIDException.class, () -> new TrackerGroup(new String[]{ \"good ID before bad data\", null})); }",
      "content_length": 1411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "All modern assertion frameworks have some type of assertThrows(). The assertion takes a block of code (as a lambda function) and executes it. If that code throws the expected exception, the assertion passes; otherwise, the assertion fails.\n\nThe test will pass when you add the code to check for null:\n\npublic TrackerGroup(String[] trackerIDs) { this(freshStoreForIDs()); for (String candidate : trackerIDs) { if (candidate == null) { throw new InvalidIDException(candidate); } } Collections.addAll(this.trackerIDs, trackerIDs); }\n\nAfter a simple Extract Method refactoring, the code takes this form:\n\npublic TrackerGroup(String[] trackerIDs) {\n\nthis(freshStoreForIDs()); checkIDs(trackerIDs); Collections.addAll(this.trackerIDs, trackerIDs);\n\n}\n\nprivate void checkIDs(String[] trackerIDs) {\n\nfor (String candidate : trackerIDs) { if (candidate == null) { throw new InvalidIDException(candidate); } }\n\n}\n\nAlong with asserting that the correct exception type is thrown under the right circumstances, assertThrows() returns the thrown exception so the same test can assert something about the message. My preference, though, is to test the domain exception’s message-formatting behaviors separately.\n\nDesign Detour\n\nDomain exceptions tend to go together with some grouping of system behaviors. You will find dependencies easier to manage if those exceptions are contained within the same Java package or",
      "content_length": 1400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "the same .Net namespace and dynamic link library (DLL) as the parts of your system that throw them.\n\nMany teams I’ve coached tried to put all their domain exceptions into one Java package or .Net DLL. But because specific knowledge of other domain types (for example, TrackerGroup) is often useful, combining domain exceptions together “because they’re exceptions” will cause unnecessary coupling among most of your DLLs or JAR files.\n\nHere are two example tests for InvalidIDException. What we want from this exception is for the invalid value to appear in the message, clearly delineated with square brackets, and with quotes around string values (so we can distinguish the difference between null and \"null\"):\n\n@Test void messageFormatsNullReference() { InvalidIDException myException = new InvalidIDException(null); assertThat(myException.getMessage()).contains(\"[null]\");\n\n}\n\n@Test void messageContainsTheInvalidValue() {\n\nInvalidIDException myException = new InvalidIDException(\"bleep!\"); assertThat(myException.getMessage()).contains(\"[\\\"bleep!\\\"]\n\n}\n\nHere is the entire InvalidIDException class:\n\npackage crittermaps; public class InvalidIDException extends RuntimeException {\n\npublic InvalidIDException(String badID) { super(String.format(\"Invalid tracker ID value [%s]\", badID != null ? String.format(\"\\\"%s\\\"\", badID) : badID)); }\n\n}",
      "content_length": 1343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "External Exceptions\n\nSimilarly, you can test what your code does with an exception thrown by an external dependency. Of course, to test that, you need to reliably cause the external dependency to throw the exception! Rather than trying something risky, slow, or expensive, replace the dependency with a “test double.” Chapter 6, “Test Doubles,” provides more details.\n\nSummary\n\nThe creation, throwing/raising, and handling of exceptions and errors are all significant behaviors deserving attention. Yet all too often, they are simply dismissed because they are not what we think of as “features.” Once we see that these behaviors are really not all that mysterious or special, they become easy to test and build:\n\nExceptions are not all that different from any other object, and can contain some simple behaviors.\n\nThrowing or “raising” will immediately break the code’s usual return path and—if not caught somewhere—will halt your software or at least the current transaction. That isn’t always a bad thing.\n\nCatching an exception is easy, but something besides merely logging the exception must happen. Otherwise, the software will continue on its normal path, pretending that nothing bad has happened. And that’s worse!\n\nWe now put the TrackerGroup example behind us. You now have all the “moves” of the TDD game.\n\nChapter 5, “Sustaining a Test-Driven Practice,” is packed with strategies for sustaining and winning the TDD “long game.” Keeping both the implementation and the test code flexible, changeable, and useful allows teams to extend and enhance their software indefinitely.\n\nOceanofPDF.com",
      "content_length": 1602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Chapter 5. Sustaining a Test-Driven Practice\n\nIn this chapter, we cover challenges that arise over time. Those challenges include knowing where to start, testing code that is difficult to test, resolving “test smells,” and having to alter multiple tests. Expecting these issues and knowing ways to resolve them will give you the confidence to face them head-on. Also, learning that they are not nearly as bad as the situations that teams face when they do not maintain their safety net will give you the courage to resolve them even when others might ask you to take a shortcut.\n\nAt the beginning of a test-driven project, you might spend a seemingly disproportionate amount of time on test maintenance. For example, you might need to continuously reshape your object interfaces or change the number of objects involved in a behavior until the design feels right. Don’t be discouraged. As you progress, you will uncover your team’s design preferences, tolerance for various smells, and preferred ways to test your domain that will set you up for success in the future.\n\nTest-suite design and implementation design are quite different, but they also tests help shape a clean implementation. Conversely, whenever a particular behavior resists testing, you will likely need to first refactor the implementation, or the tests, or both.\n\nreflect each other: Well-written\n\nKey Lesson\n\nIf software is difficult to test, it’s not necessarily that testing is difficult, but rather that the current design of the code is resistant to testing. Often, refactoring is necessary before you can add new behaviors test-driven.",
      "content_length": 1610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "On longer-term test-driven projects, you may encounter a counterintuitive truth: The team spends more time maintaining the software’s safety net than maintaining the deliverable implementation. This is mostly because the test-driven team can refactor and add new behaviors with much greater confidence and speed. They don’t have to fearfully belabor over each new line of code.\n\nThe other reason for this time imbalance is a bit more subtle: The way system behaviors are performed is defined in the implementation, but those behaviors themselves are defined in the tests. A truly test-driven team is more focused on the “what” and less focused on the “how.”\n\nLet’s first examine what you’re aiming for, then consider the various challenges and how to overcome them.\n\nAttributes of a Good Unit Test\n\nUnit tests are part of your code and well deserving your attention. Test code differs from implementation code in purpose, design, smells, and refactorings, but it needs to be maintained with at least as much diligence as the implementation.\n\nThe sections that follow describe the attributes of good unit tests, which overlap and support each other.\n\nFast\n\nLonger test runs lead to wasteful context-switching. Stay alert for when your full test run exceeds 40 seconds. Anecdotally, that’s about as long as the typical developer will wait before getting bored and switching to other tasks.\n\nWith just a few unit tests, you’ll notice a bit of startup overhead with each run (up to 5 seconds on some IDEs). Once your team reaches tens of thousands of tests (it might take a year), you won’t notice the overhead anymore. If you notice the feedback loop rising toward 40 seconds, take steps to speed up slow tests.",
      "content_length": 1708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "In his book Working Effectively with Legacy Code (Pearson, 2004), Michael Feathers defines a “unit test” as one that runs without touching a database, file system, web API, or network. The short version of this definition is that the test runs entirely “in process”—that is, in memory.\n\nYou can achieve this by isolating slow external dependencies and replacing them with test doubles (see Chapter 6, “Test Doubles”). I’ve witnessed clients reduce their regression testing cycle from weeks or even months to mere minutes by substituting externals with test doubles.1\n\n1 This doesn’t mean they didn’t still do some manual testing. No amount of automated testing can replace Exploratory Testing by a talented professional tester. But if testing finds a defect, that broken behavior can be expressed and corrected through a much faster, repeatable test.\n\nClear\n\nA good test clearly describes the behavior being tested. Strive to make each test excruciatingly obvious regarding which behavior it is testing, and how it differs from its neighboring tests.\n\nDefinition\n\nExcruciatingly obvious: My term for code that is so clear that its purpose and behavior are unambiguous, even to someone new to the team. There’s no need to overthink this: If two developers understand the code, you’re headed in the right direction.\n\nAlthough unit tests are expressed in a programming language, they should also reflect the team’s ubiquitous business language and avoid unfamiliar technical jargon. You want both you and your teammates to be able to read a dozen or two tests rapidly and get the overall idea of which behaviors are described and tested.\n\nYou want each test to be readable in isolation, and you want a collection of tests—within a test class or describe clause, for example—to tell a coherent story about the described behaviors. Keep your audience in mind.\n\nChoose test data values that are simple and distinct. If necessary, assign each value to a local variable to name it.",
      "content_length": 1973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "You want each test scenario to be brief but clear, so don’t skimp on keystrokes. Variable names such as “n” or “foo” do not improve readability.\n\nDon’t force yourself, or any future developer, to do math while reading the test, or to work out a complex Boolean conditional, or to guess at the meaning of a constant value (the Magic Number code smell).\n\nUnless a particular data value alters behavior, choose values that have no special meaning to the system. Instead of your CEO’s email address, use a example, fictitious “Zaphod.Beeblebrox@heartofgold.org.” If you need a date, perhaps choose a famous person’s birthdate:\n\ncharacter’s\n\naddress—for\n\nvar ladyGagasBirthday =\n\nnew SimpleDateFormat(\"dd MMM yyyy\").parse(\"28 Mar 1986\"\n\nValues that are unlikely to appear in your production environment will stand out clearly as test data much better than actual production data.\n\nConcise/Focused\n\nEach unit test should be as brief as reasonably possible and should test only one behavior. Nothing extra: No extra data, no extra calls, no unrelated assertions.\n\nA greater number of simple, precisely targeted tests offer you more information than fewer, broader (for example, end-to-end) tests. When a test fails, it should tell you precisely what is broken and under which scenario. If only one test is failing in an entire suite of concise tests, the tests that are passing tell you what is still working, which is also helpful.\n\nClear and concise tests also offer the team a detailed engineering specification. A developer can read through a group of related tests and glean the intended purpose of the code.\n\nTo check whether your tests are excruciatingly obvious, show them to someone without explaining the behaviors. Often when I’m delivering training, a developer will show me tests that were written earlier and try to explain what they are testing. I’ll usually ask them to stop. If the tests are not expressive, then explaining them while I’m reading them is distracting",
      "content_length": 1976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "and potentially misleading. If they’re clear and concise tests, even someone who is unfamiliar with those tests can absorb the details and the business rules faster than anyone could explain them.\n\nLet your tests speak for themselves. If they fail to communicate intent, those tests need refactoring.\n\nIndependent\n\nA unit test must not rely on other tests to run first, or at all. Each test must be able to run individually or as part of the complete safety net.\n\nToday, separate tests run in parallel. In addition, some IDEs detect which tests need to be re-executed due to a code change and will run them without waiting for you to activate them.\n\nPut simply, a unit test should stand alone: All the state should be expressed within the test, or in a nearby BeforeEach block (or equivalent). Avoid fetching state from hidden or shared sources. Avoid the temptation to create a test database or a separate massive file of data for the unit tests. If someone reading your test suite must open another text file or manually execute a SQL query to understand what is being tested, then you don’t have independent unit tests. This can often be fixed by using test doubles for external dependencies (see Chapter 6, “Test Doubles”).\n\nRepeatable\n\nGiven that the underlying implementation being tested hasn’t changed, a test must give the same result every time, regardless of time of day, which other tests were run previously, speed of execution, or which system it’s running on.\n\nKeep the attributes just presented in mind as you investigate the testing challenges and test smells (code smells that pertain to test code) in the rest of this chapter.",
      "content_length": 1645,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "Where to Start\n\nDevelopers naturally tend to think in tests, though they aren’t always aware they’re doing so.\n\nBefore you write a single programmatic statement, you already know why you need it. TDD turns those thoughts into concrete tests. You don’t think “I need a loop,” without first thinking “This needs to happen more than once.” You don’t think “I need a branch statement,” without first thinking “This only happens when that is true.”\n\nYou identify behavioral boundaries quite naturally. You think in tests.\n\nUnit tests are very concrete, low-level examples of how an object, method, or function is used. They are not product specifications. They are more akin to engineering specifications: You are describing and delineating the tolerances and responses of each part of the software’s behavior.\n\nWhen developers discuss behaviors with the business managers, these conversations are often at a level too high or abstract to begin thinking about behaviors at the “unit level.” Every team (not just TDD teams) must have a way to decompose requests before writing code. Fortunately, most people in the software industry—developers, but also product advocates, business analysts, and testers—will describe what the product should do, at some level, using examples.\n\nThere are excellent collaborative techniques beyond the scope of this book that can be used to effectively decompose requests to the point that they can be built in a test-driven manner. Two such techniques are Example Mapping and Behavior-Driven Development (BDD). BDD was discussed briefly in Chapter 1, “Thinking Test-Driven.”\n\nOnce the examples are sufficiently concrete and agreed upon, developers may need to break behaviors down further and distribute them across objects and functions. Where should the new behavior live? Which part of the whole system should you build first?\n\nFortunately, the inventor of TDD, Kent Beck, was already familiar with this issue. He and his frequent innovation partner, Ward Cunningham, gave us a",
      "content_length": 2007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "tool to do just enough upfront design: CRC cards.\n\nCRC Cards\n\nOften teams know what they need to build, but they’re not sure which classes should own which behaviors. One lightweight and collaborative software design practice that can be applied in such a situation is CRC cards. CRC cards encourage teams to focus first on services and relationships, and to defer details such as algorithms and implementation until the team feels ready to build the implementation in a test-driven way.\n\nTeams often use physical index cards as CRC cards, with each card class–responsibilities– representing one collaborators. Those are the three areas of text included on each physical index card (Figure 5.1):\n\nclass. CRC\n\nstands\n\nfor\n\nClass: This is usually the name of the class in an object-oriented programming language. It can also be a concept, category, or abstraction until the team works through a few quick iterations of the technique and identifies a useful class name.\n\nResponsibilities: These are the behaviors of the class, which are usually limited to the services this class offers to other classes in the system. They can be stated as public method names. Ideally, though, you should favor descriptive sentence fragments or service names, to avoid overdesign prior to writing the first unit test for that behavior.\n\nCollaborators: These are the other classes and objects with which this class will need to interact to accomplish its responsibilities. These are usually limited to other objects within the business domain. In other words, avoid including primitive objects like Date.",
      "content_length": 1585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Figure 5.1 CRC card format: Each card has an area for the Class name, Responsibilities, and Collaborators.\n\nTool Tip\n\nPrior to 2020, the primary format for a team’s CRC cards session was to sit around a table with a stack of blank index cards, pencils, and erasers. Since 2020, many teams have continued to use CRC cards, but done so within collaborative online environments such as Miro or Mural. The position of the areas of text on a rectangular widget is less important than the clarity of each region’s purpose. In Miro, we’ve used rectangular “sticky notes” and placed the text in class–responsibilities–collaborators order, separating the three regions with a blank line or a few dashes.\n\nTo get started, developers get together with a subset of teammates (roughly 2 to 6 people). The teammates then work through a few scenarios from the request and experiment with which object should take on which responsibilities. Whenever someone identifies an interesting class, category, abstraction, or concept, the team makes a new CRC card to represent it.",
      "content_length": 1056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "One teammate volunteers to write down the class name and the new responsibility. If an existing card delegates to the new class or is its responsibility, the new class is recorded as a collaborator. This continues until the team has worked through at least one scenario.\n\nThe team may choose to run through a few other likely scenarios, with each developer playing the “role” of the cards in their possession. If something about the scenario feels awkward, the team can create new classes, break down and move responsibilities, and perhaps toss out cards that end up with no discernible responsibilities. This is why each session includes some kind of eraser.\n\nTeams should not try to complete one card before moving on to another. Instead, they should talk through each interaction scenario and identify what each class is responsible for within that interaction, and to which other classes it delegates other responsibilities (the collaborators). The person holding a particular card records each new responsibility or collaborator.\n\nAs an example, here are some tiny requests from one of my extended exercises. These come from a single-player, turn-based, text-based game of my youth, which I’ve renamed Super Nostalgic Trek to avoid litigation.\n\nRaise shield!: To protect key subsystems, the command “shield up” raises the energy shield, and “shield transfer n” moves n units of energy from ship reserves to the shield. Shield strength min 0, max 10,000.\n\nShield can be depleted: Enemy fire is absorbed by the shield (if raised) until the shield is depleted. Any remaining energy damages a random subsystem (e.g., a weapon, the engines, life-support, sensors, or even the shield generator itself).\n\nRest: So that I can recover from a brutal space battle, the command “REST 3” consumes 3 standard game “days” to repair every damaged subsystem by that much. (Note: Damage is measured in the number of standard days needed to effect repairs based on the vague estimates of Chief Engineer Monty McGuffin.)",
      "content_length": 2005,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "In courses, I play the role of product advocate and share any missing context in conversations with teams. The sample results that follow contain knowledge gained from the written requests, from conversations with the product advocate, and from stepping through scenarios with each team member playing the role of one or more classes.\n\nHere are some scenarios that teams often come up with:\n\nWe’re attacked, and the shield withstands the attack.\n\nWe’re attacked, and the shield “buckles” because it is hit with more energy than it held.\n\nWe’re attacked, and the shield is down.\n\nWe rest, but only long enough to repair one of two damaged subsystems.\n\nWe rest needlessly, because no subsystems were damaged.\n\nFigure 5.2, Figure 5.3, and Figure 5.4 show what the resulting cards might look like.\n\nFigure 5.2 Sample CRC card for the game’s Shield class.",
      "content_length": 850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Figure 5.3 Sample CRC card for the game’s Ship class.\n\nFigure 5.4 Sample CRC card for the game’s Subsystem abstraction.\n\nThe angle brackets < and > around “Subsystem” indicate that it is an abstraction. “Subsystem” is a term that groups a set of objects into a convenient category, based on similar behaviors. When implemented, Subsystem might be an abstract class, with Shield as one of its concrete subclasses, but that isn’t the only possible outcome.\n\nAvoid attempting to lock in the “perfect” design with your CRC cards. The sample cards in these images are one possible outcome, written by a specific team. As a counterexample, another team argued that the responsibilities given to the Ship class in the sample card don’t belong",
      "content_length": 735,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "there. Instead, that team had an abstraction called “Turn,” which represented enemy attacks and player commands.\n\nNotice that each responsibility has a check box. Once you’re done with a CRC card session, you will have numerous reasonable places to start using TDD. Each check box represents at least one behavioral boundary. If a collaborator’s class or responsibility hasn’t been written yet, you can stub out enough to get started or create test doubles for those incomplete responsibilities (see Chapter 6).\n\nTool Tip\n\nOn our XP teams, pairs of developers would volunteer to pick up each resulting CRC card, treating it as a task card and building it separately from the others. Because we worked very closely together, a question like “Hey, what are you calling the subsystem repair method?” would get an immediate answer, thus reducing merge conflicts during integration.\n\nCRC cards are neither code nor documentation. Each CRC session should start with a fresh set of blank cards. Avoid writing code fragments on your cards. Also, don’t preserve them or try to keep them in sync with the code.\n\nDon’t spend more time in a CRC card session than you need to get started with TDD. Use CRC cards to explore possible designs and interactions of just a few classes, by running through a few scenarios until your team feels ready to write unit tests based on those class responsibilities. At that point, go build your code and refactor it. If a concern arises, regroup and start another CRC card session to work things out, given the newfound feedback that only real, working code can provide.\n\nWhat to Test\n\nWhen building a new group of object behaviors, you will often have too many ideas and scenarios in your head, rather than too few. Give yourself the time to write them down on your short to-do list. Avoid writing multiple tests that don’t pass. That would require you to fully design the interface",
      "content_length": 1906,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "(the way in which others will call your object or function) without the useful feedback from your own working implementation.\n\nFor your first test of each behavior, make an educated guess as to which “side” of a behavioral boundary will be easiest to implement, so you can strengthen your safety net with little effort. “Default” behavior, if any, is usually a quick win. Handling “failure” scenarios is also typically much easier than implementing success scenarios. Consider this example from TrackerGroup: Testing that a tracker ID is not found is easier than implementing a search within TrackerGroup.\n\nTest Behaviors, Not Implementation\n\nBehaviors expects. caller—the Implementation is the code that makes this behavior happen. Therefore, the real description of a behavior lives in the tests as executable specifications, not in the code that implements it.\n\nare what\n\nthe\n\nclient—sees\n\nand\n\nImagine that you’ve been asked to build a new type of collection, called a Box. You write this test (in Ruby):\n\ndescribe 'box' do\n\nit 'knows what has been added' do box = Box.new box.add(\"red pen\") expect(box.has(\"red pen\")).to be_truthy end\n\nend\n\nNotice that you need to make two calls to box to test one behavior. You’re not just testing the code within add() and has(). You’re testing that what is added to a Box can be found within that Box.\n\nSimilarly, what causes you to write the next test is not that you still need to test add() or has() independently. Rather, now that you’ve pinned down one side of the behavioral boundary, you need to pin down the other side:\n\nit 'knows what it does not have' do box = Box.new box.add(\"red pen\")",
      "content_length": 1639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "expect(box.has(\"purple smartphone\")).to be_falsy end\n\nNotice that the implementation of Box is not shown. What type of data structure Box uses is an implementation detail. When you write tests first, you will often have an implementation in mind. That’s okay, but when you approach development with a test-driven mindset—when you focus on how the behavior will be used—you will either prove your assumptions or (often) uncover a simpler implementation.\n\nTest Behaviors, Not Data Structures\n\nChoose behaviors that will drive the design toward a particular data structure, rather than asserting that the data structure exists.\n\nThere is a lab in the Appendix called Salvo. In it, you are asked to build a software version of a very old pen-and-paper game of the same name.2\n\n2 A precursor of the familiar Milton-Bradley board game, Battleship.\n\nThe initial rules for Salvo read like a recipe of data structures. One rule tells you that each player has a 10 × 10 board. If you read that rule and immediately start writing a test, you might assume that you need a two- dimensional array to represent the 10 × 10 board. How would you test-drive that into your code? What behavior does a 10 × 10 board have?\n\nOften, developers will start with the following kind of test:\n\nrequire 'salvo' describe 'board but poorly' do\n\nit 'is 10x10' do board = Board.new expect(board.height).to be(10) expect(board.width).to be(10) end\n\nend\n\nThere are two problems with this approach. First, height and width will likely remain simple accessors (“getters”).3 Typically, there’s no behavior in the implementation. Second, this test establishes an interface that might never be useful. Will there ever be a call to height or width besides the",
      "content_length": 1718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "calls in the test itself? If not, then the test adds unnecessary complexity to Board’s interface. If height and width are eventually needed (perhaps while displaying the board), they can be added later using tests for more interesting behaviors.\n\n3 In Ruby, if you don’t have a parameter, you don’t need empty parentheses to call a method.\n\nInstead, identify and test behaviors that will drive the use of a data structure. How will the game or a player interact with a Board? How could you justify a 10 × 10 array through that interface? Always find something the software is truly being asked to do for the caller or user.\n\nFor example, in the Salvo game, ships need to get placed on boards. So, we could attempt to place a tiny single-cell ship onto a board. Using this little ship, you can test many behaviors of the board without any consideration for the size or orientation (vertical/horizontal) of the ship. Here are two things a developer might start with:\n\nrequire 'salvo'\n\ndescribe 'when placing ships on boards' do\n\nbefore(:each) do @board = Board.new @ship = Ship.new(size=1) end\n\nit 'will place a ship where asked' do @board.place(@ship, row=0, column=0) expect(@board.whats_at(row=0, column=0)).to eq(@ship) end\n\nit 'will NOT place a ship off the board' do expect{ @board.place(@ship, row= -1, column=0) }. to raise_error(\"Invalid board position!\") end\n\nend\n\nWe still need more tests to be sure the board knows about all four of its edges. What are some other important “off the board” examples? You want to assert each erroneous condition separately, without ambiguity. For example, location (42, –1) would be ambiguous because either the row or column value could trigger the error. Instead, we identify that the four sets",
      "content_length": 1738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "of coordinates shown in Figure 5.5 hit the error-generating edge of each board boundary.\n\nFigure 5.5 Invalid Salvo ship-placement locations: (–1, 0), (0, 10), (10, 9), and (9, –1).\n\nDesign Detour\n\nDoes each new test need to be perfect and unchanging? Thankfully, no. Work iteratively until both your tests and your code are excruciatingly obvious.\n\nFor example, perhaps the “Invalid board position!” error message should list the offending coordinates. Would you add that now or later? At the very least, add it to your to-do list for later consideration.\n\nAnother example: The concept of ship orientation (vertical or\n\nhorizontal) could be added now or later, with an “orientation” parameter. If you think of it right away, add it to the interface and it won’t need to change later. The implementation could (should!) ignore the parameter until the Salvo tests include a larger ship.",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Alternatively, orientation would be easy to add later with an Add Parameter refactoring.4\n\n4 https://refactoring.guru/add-parameter\n\nLastly, should place() raise an error? In Chapter 4,\n\n“Exceptional Behaviors,” I recommended against raising errors to communicate invalid user input. So, does this example violate my recommendation?\n\nIn this case, imagine that this call to board.place() will be\n\ncalled after a check has been performed on the input values. More Salvo code appears later in this chapter, in the section, “Test Smells and Refactorings.” There, you’ll see why raising an error in board.place() could be appropriate.\n\nA test at (9, 9) would complete all edges of the board-size behavioral boundary. But would that be enough to justify adding a list or array?\n\nYou could get this far using the Fake It technique. Now consider these possible Triangulation tests to break the fake code:\n\nCheck that a successfully placed ship is not at a location where you didn’t place it.\n\nPlace two one-cell ships in two different locations and check that they are both where you placed them.\n\nWhat about testing other happy paths besides (0, 0) and (9,9)? Would you need to test position (5, 5)? Good question! Which leads to the next suggestion\n\nTest Behavioral Boundaries, Not Data\n\nIs there a need to create a unit test for location (5, 5), or for any other valid location other than (0, 0) and (9, 9)? Is there any behavioral difference between the tested locations and the other 98 valid board locations?\n\nRegarding the size of the Board, there is no behavioral difference in any of the other 98 valid board positions. Writing more unit tests that hit other",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "locations could clutter the engineering specification by having a cluster of tests that all test the same thing.\n\nWhenever this type of question arises, ask yourself whether you still need to (1) express some as-yet-unexpressed behavior or (2) prevent a reasonably intelligent but Jetlagged Intern from breaking something during a future refactoring attempt. If neither of those issues needs to be addressed, go on to your next to-do item. In Salvo, for example, the player can’t place two ships in the same spot on their board. That could certainly be your very next test, and it would justify using an appropriate data structure.\n\nTest Only What Is Known\n\nOn any software project where you are building something new, using new tools, or in a new environment, your job as a software developer will require extensive research. Unfortunately, you cannot determine all the things you will need to learn before you start coding. At the same time, you don’t want to have to go back and redesign your whole implementation.\n\nThe sections that follow describe ways to mitigate the risk associated with these unknowns.\n\nIsolate and Defer the Unknown\n\nWard Cunningham5 once said, “Code what you know.” The flip side of this statement is to avoid coding what you don’t yet know how to implement.\n\n5 https://en.wikipedia.org/wiki/Ward_Cunningham\n\nKeep the known and the unknown separated in the code. Objects and functions usually divide easily along the known/unknown boundary. If you guess incorrectly, a little refactoring will move behaviors into their proper homes.\n\nResearch the Unknown\n\nOften, the unknown is external to your system: hiding behind a third-party API, requiring a deeper discussion with the product advocate, or requiring the right search keywords to find a good example. In my experience, the",
      "content_length": 1805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "solution to the unknown is likely to be far simpler and narrower in scope than you had initially anticipated.\n\nAdd a research item to your to-do list. Ask the product advocate, an architect, or another teammate who is familiar with a new technology.\n\nAvoid collecting too many of these unknowns or deferring them for too long.\n\nExperiment with the Unknown\n\nPerhaps you will need to run a coding experiment. You can create a separate IDE project—that is, not part of your product or its tests—to perform these experiments. You can often use the unit-testing framework as your laboratory. “When I give this library these values, do I get back what I need?” When you have answers, apply what you’ve learned to the real product.\n\nZombies\n\nAnother lightweight approach to choosing what to test next is James Grenning’s “ZOMBIES.”6 ZOMBIES may be one of the best mnemonic acronyms in the software industry: It’s easy to remember, and all the letters are helpful. It is a reminder of very common test-writing heuristics that help you to stay on track and to unit-test all aspects of a behavior.\n\n6 https://blog.wingman-sw.com/tdd-guided-by-zombies\n\nAn old joke says there are only three numbers in computing: zero, one, and many. Unsurprisingly, those are the first three characters of ZOMBIES. Taken as a set, ZOM reminds us to start simple and add complexity incrementally.\n\nZero\n\nThe “zero” case might be a literal zero, or something that is empty, or something that does nothing by default. Or it could be any other initial state.",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "The “zero” case is often the quickest and easiest to build, and a great place to start. Even when you Fake It, you’re making progress on designing the interface. Plus, each passing test—no matter how trivial it might seem at first—“locks in” your winnings, protecting those basic behaviors while you tackle the more challenging scenarios. Grenning refers to this as “skillful procrastination.”\n\nHere’s an example: Imagine you’ve been tasked with building a library of various text filters. Perhaps one of these filters deletes suspicious scripting, another corrects your spelling, a third strips out all HTML tags, and so on. The “Zero” of ZOMBIES might prompt you to create a “pass-through” filter that lets everything through, or perhaps a “bit-bucket” type that filters everything, effectively throwing away all the text. Both would be easy to test and write.\n\nDesign Detour\n\nNull pointer/reference exceptions are evil!\n\nYou can avoid returning null (Ruby’s nil) and usually avoid checking for null values within your business domain. You will still need to check for null values returned from an external dependency or passed to you by an external caller. But within your own coding realm, every null return and every subsequent null check is avoidable.\n\nThe text filter example’s “pass-through” and “bit-bucket” filters could be “Null Objects.7” A Null Object is a concrete implementation of an abstraction (an abstract parent class or a Ruby module) that provides “default” behaviors—typically, doing nothing or returning a simple response to any request.\n\n7 https://en.wikipedia.org/wiki/Null_object_pattern\n\nReturning a Null Object is a better alternative than returning a\n\nnull pointer or reference. A method that responds with a null value pushes the responsibility to check for null onto the caller. If the caller instead tries to use that reference, a nasty null-reference exception will be thrown. Evil!",
      "content_length": 1916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "One\n\nNext is the case when there is one of something: one behavior, one object instance, one concrete class. Returning to the text filters example, that means it’s time to create a filter that does just one thing.\n\nMany (or More Complex)\n\nHere, your domain becomes more diverse and more useful. For example, perhaps you’ve already built a bit-bucket (which filters everything) and a pass-through (which filters nothing). Now, you will build something more complex. Perhaps that will be a filter that removes all HTML tags, highlights search keywords, compresses the text, or encrypts it. Or maybe it will be a filter that can combine any number of other filters.\n\nBoundaries and Behaviors\n\nAt this point, Grenning’s acronym ceases to be a stepwise succession and adds overarching reminders to the TDD thought process.\n\nSoftware behaviors exist because of boundary conditions and “edge cases.” The “B” in ZOMBIES reminds us to be vigilant about that fact, and not to neglect the other side of each behavioral boundary.\n\nInterface\n\nThe “I” component in ZOMBIES reminds us to shape our code from the interface inward. Each test is a client of the code, and each client is asking for something new. Design the interface (the service you’re providing) to suit the caller, not the implementation.\n\nExceptional Conditions\n\nThe “other side” of a behavioral boundary might be how your code deals with a programmatic error on the part of the caller, which requires that an exception be raised. Throwing that exception is the expected behavior in that scenario and should be tested.",
      "content_length": 1571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "Mixing the Interface concern with Exceptional conditions reminds us to give our exceptions meaningful names and to provide error messages that help other programmers correct the mistakes in their code.\n\nThe “E” in ZOMBIES isn’t only about throwing exceptions. Together with Boundaries, it reminds us to test our code’s responses to other exceptional conditions. For example, if your user’s full legal name is “III,” does your code handle this exceptional case gracefully?8\n\n8 An homage to the late great consultant and friend, III.\n\nSimple Scenarios and Simple Solutions\n\nSimple is never easy, so the “S” in ZOMBIES reminds us to take baby steps toward our solution. Each new test and each added behavior, when kept small and discrete, keeps both the code and the tests maintainable by us, the fallible human software developers.\n\nWhen Code Is Difficult to Test\n\nYou might find yourself wanting to write a test that is big or difficult to set up, or turns out awkward, complex, and difficult to understand. This dilemma is often a result of previous steps that were too large.\n\nKey Lesson\n\nThis key lesson from the beginning of this chapter deserves repeating: When code is difficult to test, it’s not because testing is difficult.\n\nInstead, your implementation is pushing back, telling you that\n\nyou have an opportunity to improve the design. Before writing that next test, step back and refactor the existing code or run a quick CRC card session.\n\nAny bit of isolated behavior does one of three things:\n\n1. Answers a query by returning a value (calculated or otherwise)",
      "content_length": 1571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "2. Alters the state of the object\n\n3. Delegates to another object or function\n\nThe first behavior is the easiest to test: When a method or function like ftoc(int fahrenheit) returns a value, you can easily assert that the return value is what you expect. But because this type of behavior is so easy to test, developers may add return values to methods that weren’t meant to return anything.\n\nThe second type of behavior, changing state, can be somewhat more difficult. The state-change must have a purpose, so the test changes the state and expects that purpose to be fulfilled. Otherwise, it isn’t a fully realized behavior.\n\nThe third behavior is perhaps the trickiest to test. When our object delegates to another object, questions inevitably arise: Where did this other object come from? How will we know it was called correctly? Is that object fully tested?\n\nWhenever you feel even the slightest push-back from your code, take a step back (if you’re in green). Ask, “Why is this code being so stubbornly untestable?”\n\nBefore writing the next test, you may need to refactor a little. So, follow your nose: Find the code smells that are preventing you from testing, and clean those up.\n\nTest Smells and Refactorings\n\nYou know, Homer, it’s very easy to criticize.\n\n—Marge Simpson\n\nFun, too!\n\n—Homer Simpson\n\nThe Simpsons, season 9, episode 14, “Bart Star”",
      "content_length": 1358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "tests slow down development? As with your Will maintaining implementation, if you make small changes and run them after each refactoring, your test-driven intuition and skills will continually improve, and maintenance of the tests will get easier. The process of running into a challenge and overcoming it will help you avoid that same challenge in the future.\n\nTeams that have been doing TDD for a while often report that they spend more time and creativity on their test suite than on their implementation— yet they do not see this as a problem. When a team takes the time to keep its specifications clear, simple, and passing, that well-crafted safety net will pay them back by making defect repairs and feature enhancements go much faster and more smoothly.\n\nOn a TDD project, the ratio of test code to implementation code is significant. My XP teams have generally had two to three times more lines of test code than implementation code. Again, this was not seen as a bad thing, for two reasons:\n\nThrough diligent, continuous refactoring, all duplication was wrung out of the code, leaving behind a smaller, clearer, and more maintainable implementation.\n\nGood test design and good object design differ: Good tests are “scripty” and self-contained. We want unit tests that are short, specific, and step-by-step scenarios, exercising only one isolated behavior. Contrast that to good object-oriented design, where objects often delegate anything outside of their primary purpose to another object, without being coupled to how that next object will accomplish its primary function.\n\nSo, good tests and good objects look very different, and their code smells are different, too. To clean up or avoid testing challenges, we need to be sensitive to test smells.\n\nWhat follows are some common test smells and tips on how to alleviate them. Many of these smells are similar, and there is considerable overlap among them. Read the section on each one independently and you’ll more readily notice the subtle differences.",
      "content_length": 2017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Design Detour\n\nSoftware design and test design influence each other.\n\nWhen you write the test first, you design a part of the public\n\ninterface to your implementation. When you refactor the implementation, you often make it easier to test.\n\nThese two design considerations work together. For example, if you break out a small object from a big one, you can also factor out parts of the existing tests to test the extracted behaviors directly on the new object. Both the implementation and the tests become simpler and clearer.\n\nThis process can even reduce the number of tests for\n\nresponsibilities that have become unnecessarily entangled. When you decouple behaviors, you often reduce the number of distinct permutations that need testing.\n\nDependencies that create permutations can be decoupled using\n\none of the many forms of test doubles (see Chapter 6, “Test Doubles”). The result is low-level unit tests for all discrete behaviors, including interactions and delegations across objects. Using unit tests, 100 percent of the behavior can be covered with fewer tests.\n\nWe’ll be using an informal taxonomy for the condition of the code samples that follow. “Smelly” means that something can be cleaned up to make the tests clearer. There are degrees of smelly, which are part opinion and part experience (for example, “mildly smelly” and “very smelly”). “Better” tends to imply that the code is now less smelly, but there may be further alterations that could get it to “good.” “Alternative” is used if there are multiple ways to clean up the smell depending on the real intent of the test.\n\nSmell: Unhelpful Test Names\n\nHow you name or describe each test and each test class is important. It helps make the suite of tests serve as a good “engineering specification,” and that organization provides useful information whenever a test fails.\n\nWhen the test first “fails” as part of the TDD cycle, read the failure report. Does it describe the failing behavior sufficiently to explain what isn’t",
      "content_length": 1997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "working and how that behavior fits into the overall system? That might sound like a lot, but if the test fails in the future, that description is just the starting point. The developers who read the failure report will begin thinking about the problem using the context provided by the failure report.\n\nSmelly (in Ruby’s rSpec):\n\nrequire 'salvo'\n\ndescribe \"board tests\" do\n\nit \"place fails\" do # ... scenario code goes here end it \"place succeeds\" do # ... scenario code goes here end\n\nend\n\nSmelly (in Java’s JUnit):\n\npackage salvo;\n\npublic class BoardTests {\n\n@Test void testPlaceFails() { // ... scenario code goes here }\n\n@Test void testPlaceSucceeds() { // ... scenario code goes here }\n\n}\n\nIn the first versions of JUnit, you had to name each test starting with the word “test.” Thankfully, that hasn’t been true for a long time. Even so, you should avoid using the same term in so many places that it becomes mere noise to the reader. What’s important is what unique scenario is being tested.\n\nIn JUnit and MSTest, you have a lot of flexibility when choosing test names, and you have a lot of characters available. In Jasmine, Mocha, and rSpec, you have a description string where you can write whole sentences",
      "content_length": 1216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "without using camel-case. Either way, you need to type your descriptive test name only once for each test—so make it a good one.\n\nYour team may want to experiment with different test-naming conventions. Recall that you do not have to limit yourself to one test class (or describe clause) per implementation class.\n\nInclude in each test name what makes it unique, even if that will be obvious in the body of the test. How does this test differ from its neighbors? Which side of a behavioral boundary does this test exercise?\n\nHere’s an example from the Salvo game (see the Appendix at the end of this book), written in Ruby’s rSpec:\n\nGood:\n\nrequire 'salvo'\n\ndescribe 'placing ships on the board' do it 'fails when two ships overlap' do # ... scenario code goes here end it 'works when there are empty cells around each ship' do # ... scenario code goes here end\n\nend\n\nIn Java’s JUnit, you might use the class name to describe the overall behavior, and the test-method names to describe each unique scenario.\n\nGood:\n\npackage salvo;\n\npublic class PlacingShipsOnTheBoard {\n\n@Test void fails_WhenTwoShipsOverlap() { // ... scenario code goes here }\n\n@Test void works_WhenThereAreEmptyCellsAroundEachShip() { // ... scenario code goes here",
      "content_length": 1233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "}\n\n}\n\nSmell: Unhelpful Instance Names\n\nName your object instances with variable names that add clarity to the test. You might not come up with the right names when you first write the test. Once the test is passing, be willing to read over your tests and see if a few simple Rename refactorings will enhance the clarity.\n\nOften, developers will start out with something that doesn’t seem to be a problem. For example:\n\n@ship = Ship.new(size=3, Orientation::VERTICAL)\n\nThis is fine until the number of items lacking intention-revealing names increases as you write more tests.\n\nSmelly:\n\n@ship1 = Ship.new(size=3, Orientation::VERTICAL) @ship2 = Ship.new(size=4, Orientation::HORIZONTAL)\n\nWhenever there’s a clear distinction to be made between objects, think of a name that will make the tests clearer—for example, boardToBeTested, theOtherBoard, emptyBoard, crowdedBoard, tinyShip, and biggestShip.\n\nGood:\n\n@vertical_ship = Ship.new(size=3, Orientation::VERTICAL) @horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL\n\nSmell: Copy/Paste/Modify\n\nTests that triangulate related behaviors will look very similar, but each one should be unique in some significant and obvious way.\n\nDevelopers often like to copy and paste an existing test, then modify it to match the next triangulating scenario. After a while, the resulting tests start to grow larger and more complex than necessary.",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Use your IDE’s hotkeys and autocomplete features until they become second nature. Frequently reread the handful of tests you’ve written most recently, then refactor away duplication and remove unnecessary given information. When you truly embrace test-driven thinking, you’ll find yourself using copy/paste/modify far less, instead favoring writing each test “from scratch.”\n\nSmell: Duplicate Given\n\nWhen building something new, you will often end up with a handful of tests that use the same lines of setup (given) code in each test.\n\nSmelly (in Ruby’s rSpec):\n\ndescribe 'placing ships on the board' do it 'fails when two ships overlap' do board = Board.new vertical_ship = Ship.new(size=3, Orientation::VERTICAL) horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL) # ... scenario code goes here end it 'works when there are empty cells around each ship' do board = Board.new vertical_ship = Ship.new(size=3, Orientation::VERTICAL) horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL) # ... scenario code goes here end\n\nend\n\nSmelly (in Java’s JUnit):\n\npublic class PlacingShipsOnTheBoard {\n\n@Test void fails_WhenTwoShipsOverlap() { Board board = new Board(); Ship verticalShip = new Ship(3, Orientation.VERTICAL); Ship horizontalShip = new Ship(4, Orientation.HORIZONTA // ... scenario code goes here }\n\n@Test void works_WhenThereAreEmptyCellsAroundEachShip() { Board board = new Board(); Ship verticalShip = new Ship(3, Orientation.VERTICAL);",
      "content_length": 1459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Ship horizontalShip = new Ship(4, Orientation.HORIZONTA // ... scenario code goes here }\n\n}\n\nEvery modern unit-testing framework has a “Before Each” mechanism for consolidating repeated given lines (henceforth referred to as BeforeEach). MSTest uses @Before, Jasmine uses beforeEach, rSpec uses before(:each), and JUnit uses @BeforeEach. A BeforeEach runs before each test within that test class or describe clause.\n\nFirst, find all duplicated given objects and assign them to fields with descriptive names. Then, move all duplicated setup into the BeforeEach block.\n\nBe sure you declare each object as a field in the test class and not as a method-scope local variable. Otherwise, the initialized objects will drop out of scope once the BeforeEach exits. Particularly when you are using JUnit or MSTest, be careful not to move both the declaration and the instantiation into @BeforeEach; otherwise, the local variables will shadow fields with the same name.\n\nGood (in Ruby’s rSpec):\n\ndescribe 'placing ships on the board' do\n\nbefore(:each) do @board = Board.new @vertical_ship = Ship.new(size=3, Orientation::VERTICAL) @horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL end it 'fails when two ships overlap' do # ... scenario code goes here end it 'works when there are empty cells around each ship' do # ... scenario code goes here end\n\nend\n\nGood (in Java’s JUnit):\n\npublic class PlacingShipsOnTheBoard {\n\nprivate Board board;",
      "content_length": 1437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "private Ship verticalShip; private Ship horizontalShip;\n\n@BeforeEach void setupBoardAndShips() { board = new Board(); verticalShip = new Ship(3, Orientation.VERTICAL); horizontalShip = new Ship(4, Orientation.HORIZONTAL); }\n\n@Test void fails_WhenTwoShipsOverlap() { // ... scenario code goes here }\n\n@Test void works_WhenThereAreEmptyCellsAroundEachShip() { // ... scenario code goes here }\n\n}\n\nIt’s best to wait to use the BeforeEach mechanism until you truly have some duplicated code. Make sure that what you move into this area is useful to most tests in the test class/describe clause. If some tests ignore some of the BeforeEach, that’s a mild test smell. However, if a test needs to overwrite or undo part of that BeforeEach, then you have the Rejected Given test smell (discussed later in this chapter).\n\nRemove duplication as it occurs. Don’t wait until it grows out of control. If you follow the “rule of two”—cleaning up duplication whenever you have two chunks of test code that look very similar—the next test you think of will be that much easier to write.\n\nReducing duplication also helps whenever the tests need refactoring. Often, changes can be made to the BeforeEach rather than to every test. For example, if you add a parameter to the tested object’s constructor, it will likely be initialized in the BeforeEach.\n\nYou might think it paradoxical that you want self-contained and readable tests, yet you are moving duplicated code out of those tests. Optimize for comprehension of a related group of tests; that is, keep the given code close to the tests that use it. If you must scroll up a few screen lengths to see it,",
      "content_length": 1640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "that’s probably fine. If you have hundreds of tests in the same file, consider splitting them up. See the discussion of the Rejected Given test smell later in this chapter for further guidance.\n\nSmell: Excessive Given\n\nThis smell arises when a test has a lot of things happening in the given section—either appearing in the test code or loaded from a test database. If the given contains the assembly of a complex object graph, or if the tested behavior needs to make calls to more than a few ancillary objects, you are likely trying to test too much in a single unit test.\n\nSmelly:\n\nit 'does everything, everywhere, all at once' do\n\nboard1 = Board.new player1 = Player.new(board1) board2 = Board.new player2 = Player.new(board2)\n\nsub1 = Ship.new(size=1) destroyer1 = Ship.new(size=3) battleship1 = Ship.new(size=4)\n\nsub2 = Ship.new(size=1) destroyer2 = Ship.new(size=3) battleship2 = Ship.new(size=4)\n\nboard1.place(sub1, row=0, column=0, Orientation::HORIZONTAL board1.place(destroyer1, row=0, column=2, Orientation::HORI board1.place(battleship1, row=0, column=9, Orientation::VER\n\n# ...and so on... end\n\nThis test smell provides an example of why, whenever it’s difficult to test a behavior, there’s likely a need to improve the design of the implementation.\n\nIf this smell emerges before there’s much implementation, first run through a CRC card session and identify each object’s isolated behaviors. Build those units of behavior first, and then see whether you still need to test some aggregate behavior.",
      "content_length": 1510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "If this smell pops up later in development, and many of these objects have significant the implementation. When numerous objects are needed to play a role in a unit- testing scenario, there is often some unintended coupling across some of the objects. Generally, look for long blocks (for example, in Java or C#, more than a dozen lines of code between curly braces). Often script-like, “procedural” code is used to “manage” an interaction. There is likely a place in your domain for such transactional scripts, but that behavior might not belong in any of the stateful objects you’ve created within your given part.\n\nimplementation, first\n\nlook for code smells within\n\nAs always, first get back to green, then follow your nose and refactor away those smells. After that, you can approach new behaviors using objects that have fewer and simpler responsibilities.\n\nIf it turns out that the scenario you’re testing truly requires several objects (e.g., a list of ships), consider using a “Builder”9 to assemble the required object graph and make the given more readable.\n\n9 https://en.wikipedia.org/wiki/Builder_pattern\n\nBetter:\n\nit 'does everything, everywhere, all at once' do\n\nplayer1 = PlayerBuilder.new .addShip(size=1, 0, 0, Orientation::HORIZONTAL .addShip(size=3, 0, 2, Orientation::HORIZONTAL .addShip(size=4, 0, 9, Orientation::VERTICAL) .build\n\nplayer2 = PlayerBuilder.new .addShip(size=1, 3, 8, Orientation::HORIZONTAL .addShip(size=3, 1, 1, Orientation::VERTICAL) .addShip(size=4, 5, 9, Orientation::VERTICAL) .build\n\n# ...and so on...\n\nend",
      "content_length": 1551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Smell: Rejected Given\n\nOften teams will pile a lot of given code into a single BeforeEach method, resulting in setup code that isn’t used in many tests, or—worse—needs to be undone or replaced in some tests.\n\nSmelly (a simplified example):\n\ndescribe 'board status' do\n\nbefore(:each) do @board = Board.new @board.place( Ship.new(size=2), Orientation::HORIZONTAL, row=0, column=0) end\n\nit 'reports all NOT sunk when one ship afloat' do expect(@board.all_sunk?).to be false end\n\nit 'reports all sunk when no ships afloat' do @board.attacked_at(0, 0) @board.attacked_at(0, 1) expect(@board.all_sunk?).to be true end\n\nit 'reports all sunk when empty even though this cannot happe @board = Board.new expect(@board.all_sunk?).to be true end\n\nend\n\nNote that the last spec is resetting @Board to test an unusual (impossible?10) case. The refactoring needed is to divide the tests into separate test classes or describe clauses, grouping them by what is common in the given.\n\n10 This “impossible” scenario should probably assert that a programmatic error is raised, but I needed the example to be as simple as possible.\n\nSmell: Forced Return Value\n\nA trap that developers often fall into when starting out with TDD is trying to force one method to do everything necessary, including returning an",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "otherwise meaningless status value.\n\nWhile it’s true that a test without an assertion (the then part) is worse than no test at all, we want assertions that make sense within the test scenario. Recall the three things that software can do: return a value, change state, or delegate. This smell happens when a developer tries to avoid testing the latter two possibilities.\n\nSmelly:\n\nit 'will place a ship where asked - with FORCED RETURN VALUE' d expect(@board.place(@ship, row=0, column=0) ).to eq(Board::SU\n\nend\n\nit 'will NOT place a ship off the board - with FORCED RETURN VA\n\nexpect(@board.place(@ship, row= -1, column=0) ).to eq(Board::FAILURE_INVALID_POSITION)\n\nend\n\nGiving place() a return value gives it two important jobs: placing the ship, but first determining whether it can be placed there.\n\nAlthough the place() method could do both,11 the trouble is that these return values force the client to deal with an error after it has occurred. Also, when a method’s name doesn’t describe what it’s returning, the calling code will also be less clear.\n\n11 There is a lot of precedent for doing this: Most UNIX system calls return a value indicating success or failure.\n\nOne short-term “fix” is to rename the method based on what it does and what it returns.\n\nBetter:\n\ndef place_ship_and_return_success_or_failure( ship, starting_row, starting_column) # significant logic removed for brevity... SUCCESS end\n\nThis Rename Method refactoring doesn’t solve the problem, but it does make the smell more obvious. By adding _and_ to a method name, you send",
      "content_length": 1553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "a very clear signal to your team that a method is doing more than one thing. If you cannot immediately come up with a way to resolve this smell, at least the smell will be easier to detect in the future.\n\nHow do you resolve this test smell for good? Split up the behaviors. What follows is one example of how to split up this Board behavior into two calls. One call is a query; the other is a request.\n\nGood:\n\ndescribe 'placing ships on the board' do\n\nbefore(:each) do @board = Board.new @ship = Ship.new(size=1) end\n\nit 'knows if tiny ship can be placed at empty square' do expect(@board.can_place_ship_at( @ship, row=0, column=0)).to be_truthy end\n\nit 'knows that tiny ship CANNOT be placed off the board' do expect(@board.can_place_ship_at( @ship, row= -1, column=0)).to be_falsy end\n\nit 'will place a ship where asked' do @board.place(@ship, row=0, column=0) expect(@board.whats_at(row=0, column=0)).to eq(@ship) end\n\nit 'will NOT place a ship off the board' do expect{ @board.place(@ship, row= -1, column=0) }. to raise_error(\"Invalid board position!\") end\n\nend\n\nThe client implementation will need to call the query first instead of checking for errors after the call. For example:\n\nif player_board.can_place_ship_at(player_ship,\n\nselected_row, selected_column)\n\nplayer_board.place(player_ship, selected_row, selected_column\n\nelse",
      "content_length": 1336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "notify_player(\"Sorry! You cannot place your\" + \" #{player_ship.ship_type}\" + \" at #{selected_row},#{selected_column}\")\n\nend\n\nThe complexity of responding to error codes has been avoided by using the lesser complexity of calling board twice. This code can also be written using a test-driven approach: Either the ship is on the board in the location requested, or the player is notified of their invalid selection.\n\nNote that the “Invalid board position!” error (or exception) would not be raised unless the logic has a defect. In other words, it is exceptional: It should never happen, and if it does, it’s because a developer has made a coding or refactoring mistake.\n\nThe implementation of place() may also call can_place_ship_at() in a guard clause to avoid attempting the impossible:\n\ndef place(ship, starting_row, starting_column)\n\nunless can_place_ship_at(ship, starting_row, starting_column) raise(\"Invalid board position!\") end\n\n# ...logic placing the ship on the board goes here...\n\nend\n\nDefinition\n\nGuard clause: A conditional statement that, when true, exits the method immediately. A guard clause stops the typical flow of the method, allowing the rest of the method to be written without additional indentation.\n\nDesign Detour\n\nThe example implementation of place() that calls can_place_ship_at() might bother some developers. Why invoke can_place_ship_at() twice for the same user action?\n\nThe concern is often one related to speed or efficiency. If can_place_ship_at() were expensive (for example, using a lot of",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "time or CPU cycles), then certainly limiting it to one call would be of benefit. Otherwise, computational efficiency is not a priority. Our software exists to make at least one human’s life fractionally more pleasant, often by helping make a menial or repetitive task go faster and be less prone to human error. Our product’s source code exists primarily to make maintaining and extending that software easier for the development team. There is seldom a need to make the computer’s task faster unless it helps with those two goals.\n\nThe two calls to can_place_ship_at() serve different purposes\n\nand result in different actions. On the one hand, when the caller checks and then transacts, the check fails if the player entered a choice that isn’t allowed by the rules of the game. At that point, the caller will—presumably—be asked to enter different coordinates. There is no programmatic error. If, on the other hand, the “Invalid board position!” error somehow gets raised, that event would indicate a true programmatic error.\n\nIn the case of a stand-alone app, a programmatic error should cause the entire application to halt. In the case of a transactional web application, it should roll back or avoid committing the latest transaction. In a few mission-critical web applications I worked on, an exception’s stack trace (and other relevant local state) would be sent automatically to the development team, and an on- call developer would be immediately notified.\n\nSmell: Coupled Assertions\n\nYou might find yourself using the same sequence of assertions repeatedly, either in different tests or in the same test. This smell suggests a relationship among the assertions in the pattern. It tells you that some form of conceptual coupling is prompting you to group the assertions.\n\nSmelly:\n\ndescribe 'ship status' do\n\nbefore(:each) do @board = Board.new @sub = Ship.new(size=1) @destroyer = Ship.new(size=3)",
      "content_length": 1908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "@battleship = Ship.new(size=4) @board.place(@sub, Orientation::HORIZONTAL, row=0, column=0 @board.place(@destroyer, Orientation::HORIZONTAL, row=0, co @board.place(@battleship, Orientation::VERTICAL, row=0, col end\n\nit 'reports undamaged if attack misses all ships' do @board.attacked_at(5, 4) expect(@battleship.damaged?).to be false expect(@battleship.sunk?).to be false\n\nexpect(@destroyer.damaged?).to be false expect(@destroyer.sunk?).to be false\n\nexpect(@sub.damaged?).to be false expect(@sub.sunk?).to be false end\n\nThis example repeatedly asserts against both the damaged? and sunk? methods. Thus, the notions of damaged and sunk are apparently related.\n\nAn aside: This example shows repetition in a single test, mostly to make the Coupled Assertions smell more obvious. You might also get a whiff of other test smells in this sample code. For example, assertions upon unaffected objects could be removed, leaving behind only those assertions for the object that changed (or that will change in the adjacent Triangulation test— for example, when the attack hits the @sub).\n\nThe refactoring depends on the reason for the relationship. If there’s a legitimate reason to check those same assertions in similar scenarios, then you could extract your own custom assertion.\n\nAlternative:\n\nit 'reports undamaged if attack misses all ships' do @board.attacked_at(5, 4) expect_ship_to_be_healthy(@battleship) expect_ship_to_be_healthy(@destroyer) expect_ship_to_be_healthy(@sub) end\n\ndef expect_ship_to_be_healthy(ship) expect(ship.damaged?).to be false",
      "content_length": 1551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "expect(ship.sunk?).to be false end\n\nYou can keep this custom assertion method local. Alternatively, if it’s useful in many test files (or describe clauses), then you could add it to your test framework.\n\nCoupled Assertions often indicate related behaviors that might be useful in the implementation. For example, perhaps one of Ship’s clients needs to know if a ship is both undamaged and still afloat:\n\nclass Ship\n\ndef undamaged? not ( damaged? or sunk? ) end # ...\n\nend\n\nIf so, then the tests could be altered to check that combined result.\n\nAlternative:\n\nit 'reports undamaged if attack misses all ships' do @board.attacked_at(5, 4) expect(@battleship.undamaged?).to be true expect(@destroyer.undamaged?).to be true expect(@sub.undamaged?).to be true end\n\nMost of the time, each assertion belongs in its own test. When you see two very different assertions that appear together, they are likely testing two different behaviors in the same test. If so, divide them into separate tests.\n\nGood:\n\nit 'reports undamaged if attack misses' do @board.attacked_at(5, 4) expect(@sub.damaged?).to be false end\n\nit 'reports not sunk if attack misses' do @board.attacked_at(5, 4) expect(@sub.sunk?).to be false end",
      "content_length": 1204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Design Detour\n\nIf you use a testing framework that allows for nested describe clauses, consider consolidating a duplicated when step into a BeforeEach, leaving only the assertions in the tests. In our simple Salvo example, this technique might look absurd, but anytime you need to assert against two very different outcomes, this refactoring could help clarify those differences.\n\ndescribe 'ship status' do\n\nbefore(:each) do @board = Board.new @sub = Ship.new(size=1) @board.place(@sub, Orientation::HORIZONTAL, row=0, co end\n\ndescribe 'when attack misses' do before(:each) do @board.attacked_at(5, 4) end\n\nit 'reports undamaged' do expect(@sub.damaged?).to be false end\n\nit 'reports not sunk' do expect(@sub.sunk?).to be false end end\n\nend\n\nThe rSpec- and Jasmine-style frameworks are well designed to\n\ncarry out this particular refactoring, because the BeforeEach that includes the tested behavior—the when of the test; in this case, @board.attacked_at(5, 4)—is very close to the two tests that use it. If there’s too much unrelated test code between the when and the then, the tests will be much less clear.\n\nTalk with your teammates before using this refactoring.\n\nDevelopers either love it or hate it, and it’s not always as readable in other test frameworks.",
      "content_length": 1264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Smell: Extended Narrative\n\nYou might be tempted to write a test that tests more than one scenario. Perhaps one scenario logically follows the other, or two scenarios share the same given setup. You test a little behavior, and—because you’re already set up for it—you test a little more.\n\nThe resulting tests typically follow a given–when–then–when–then or given–when–then–given–when–then pattern.\n\nSmelly:\n\nit 'reports sunk if all cells have been hit' do\n\ndestroyer = Ship.new(size=2) @board.place(destroyer, Orientation::HORIZONTAL, row=0, colum\n\n@board.attacked_at(0, 0) expect(destroyer.damaged?).to be true expect(destroyer.sunk?).to be false\n\n@board.attacked_at(0, 1) expect(destroyer.sunk?).to be true\n\nend\n\nThis is problematic because if one of the earlier assertions fails, the test halts, and the other behavior is not tested. Also, the test is less concise, and the description matches only one of the two scenarios.\n\nTool Tip\n\nThe first assertion/expectation that fails halts the test. (Other tests in the test run should continue to execute as expected.) Any further assertions within that test will not be checked. This is a valuable feature of test frameworks, because once a scenario is known to be in an undesirable state, any further information we try to gather will be unreliable and possibly misleading.\n\nThis is the reason for the oft-quoted “rule” of “One assertion per test.” (I’ve always treated it as a design guideline rather than a rule.) Notice how many of these test smells arise from having more than one assertion in a single test.",
      "content_length": 1562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "Particularly in frameworks that allow nested describe clauses,\n\neach with its own BeforeEach, it’s easy to create short, clear, concise tests with only one assertion.\n\nIdeally, each of your unit tests should fail for one and only one broken behavior. Then, whenever a test fails in the future, you’ll get more information from more tests with fewer assertions. The passing or failing of individual tests will help you triangulate the mistake.\n\nBetter:\n\nbefore(:each) do\n\n@board = Board.new @destroyer = Ship.new(size=2) @board.place(@destroyer, Orientation::HORIZONTAL, row=0, colu\n\nend\n\nit 'reports sunk if all cells have been hit' do\n\n@board.attacked_at(0, 0) @board.attacked_at(0, 1) expect(@destroyer.sunk?).to be true\n\nend\n\nit 'reports damaged if attack hits a cell' do\n\n@board.attacked_at(0, 0) expect(@destroyer.damaged?).to be true\n\nend\n\nit 'reports still afloat if NOT all cells hit' do\n\n@board.attacked_at(0, 0) expect(@destroyer.sunk?).to be false\n\nend\n\nSmell: Unrelated Assertions\n\nThis test smell is very similar to Coupled Assertions. In this case, though, you sense that the adjacent assertions are not at all related. Two flavors of this smell are most common: assertions upon different objects, and multiple assertions upon the state of the tested object.\n\nIf you assert upon more than one object in a test, consider testing those objects separately.",
      "content_length": 1367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Smelly:\n\nit 'will place a ship where asked' do\n\n@board.place(@sub, Orientation::HORIZONTAL, row=0, column=0) expect(@board.whats_at(row=0, column=0)).to eq(@sub) expect(@board.number_of_ships).to eq(1)\n\nend\n\nAre we testing that the board can tell us what has been placed on a particular location, or that it can count how many ships it contains, or both?\n\nPutting unrelated assertions information whenever one or both fail.\n\ninto separate\n\ntests will provide more\n\nBetter:\n\nit 'will place a ship where asked' do\n\n@board.place(@sub, Orientation::HORIZONTAL, row=0, column=0) expect(@board.whats_at(row=0, column=0)).to eq(@sub)\n\nend\n\nit 'knows how many ships it holds' do\n\n@board.place(@sub, Orientation::HORIZONTAL, row=0, column=0) expect(@board.number_of_ships).to eq(1)\n\nend\n\nSmell: Unfailing Assertion\n\nUnfailing Assertion is an assertion that is unrelated to the behavior being tested, usually testing something that couldn’t possibly fail. An unfailing assertion is misleading and confusing, and future readers will struggle to understand why it’s there.\n\nSmelly:\n\n@Test void theConstructorWorks() {\n\nShip shipToBeTested = new Ship(4); assertThat(shipToBeTested).isNotNull();\n\n}\n\nThe assertion that the reference pointer isn’t null cannot fail. Any condition that could conceivably leave shipToBeTested uninitialized would either",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "throw an exception (failing the test without reaching the assertion) or would crash the whole virtual machine.\n\nThe solution is to delete the Unfailing Assertion.\n\nSmell: Redundant Assertion\n\nThis test smell is a special case of Extended Narrative mixed with Unfailing Assertion. It arises whenever the same behavior and related assertion appears in different tests. This typically occurs when a developer writes a test without considering or trusting the existing tests and implementations. Perhaps the developer was uncertain of where to begin, or was not yet willing to rely on the safety net, or wrote the test using copy/paste/modify while thinking, “What’s the harm of an extra assertion?”\n\nIt’s acceptable—even commonplace—to use behaviors that are thoroughly tested elsewhere (within the same safety net) to set up a scenario (given) or to assert that something happened (then). But once you have tested a behavior elsewhere in your safety net, you do not need to assert that it is still working in those other scenarios.\n\nSmelly:\n\nit 'reports damaged if attack hits a cell' do\n\n@board.attacked_at(0, 0) expect(@destroyer.damaged?).to be true\n\nend\n\n# ...many other tests written here, perhaps over many days...\n\nit 'reports sunk if all cells have been hit' do\n\n@board.attacked_at(0, 0) expect(@destroyer.damaged?).to be true @board.attacked_at(0, 1) expect(@destroyer.sunk?).to be true\n\nend\n\nThe computer is not likely to stop computing correctly between the execution of one test and another. Yes, it is possible that a stray gamma particle will knock one of your system’s bits into the wrong state, and a test will fail randomly. But the odds are so slim that you’re better off running",
      "content_length": 1695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "the tests again before blaming gamma rays. If the same test fails twice, it was not caused by gamma rays.12\n\n12 https://radiolab.org/podcast/bit-flip\n\nThe solution: delete the smelly assertion. If you have doubts about the redundancy of the deleted assertion, run the tests with code-coverage afterward to confirm that the assertion was not testing some obscure bit of your implementation.\n\nSmell: Repeated Assertions\n\nWith this smell, you find multiple assertions in a test that are almost duplicates. This often occurs when arrays or collections are involved.\n\nMildly smelly:\n\nit 'finds the ship at all locations' do\n\nbattleship = Ship.new(size=4) @board.place(battleship, Orientation::HORIZONTAL, 3, 2) expect(@board.whats_at(3,2)).to eq(battleship) expect(@board.whats_at(3,3)).to eq(battleship) expect(@board.whats_at(3,4)).to eq(battleship) expect(@board.whats_at(3,5)).to eq(battleship)\n\nend\n\nThis is a mild smell because often the alternatives are worse. In this example, an alternative might be to loop through the coordinates and have the assertion occur within the loop. That smell, Branching Code (discussed later in this section), is worse.\n\nCreating a custom assertion could clarify the test or could further obfuscate the test. Try something; if it makes the test less clear, undo it and move on. Not every smell must be cleaned up.\n\nBetter:\n\nit 'finds the ship at all locations' do\n\nbattleship = Ship.new(size=4) @board.place(battleship, Orientation::HORIZONTAL, 3, 2) expectShipToOccupy(battleship, [[3,2],[3,3],[3,4],[3,5]])\n\nend\n\ndef expectShipToOccupy(ship, cells)",
      "content_length": 1584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "cells.each do |cell| expect(@board.whats_at(cell[0],cell[1])).to eq(ship), \"@[#{cell[0]},#{cell[1]}]\" end\n\nend\n\nA loop in a custom assertion doesn’t smell as bad as a loop in the test, because the custom assertion method gives the loop an intention-revealing name and declutters the flow of the test scenario. A simple custom assertion is often reusable, reducing duplication across tests.\n\nSmell: Branching Code\n\nThis smell occurs whenever there is a branch or loop in the test. It happens when a developer tries to get a single test to test too much.\n\nGood tests are written in a simple, step-by-step format that describes a scenario. As a result, they are more linear and “scripty” than a well- decomposed implementation.\n\nThis smell comes in various flavors:\n\nIf statements: The desire to add an if statement to a test is a clear indicator that it is testing too much. Instead, break the test into two tests with unique descriptions.\n\nLoops: Strong arguments can be made for looping over a whole table of data, and some of the unit-testing frameworks have utility syntax to provide that kind of testing. That’s great for testing in general. However, when you are developing code using TDD, you are testing the edges of behavioral boundaries. Is there a change in the code paths for each row of the table? If not, each row is testing the same logic and calculations. Again, it might not be a bad test, but it isn’t a “unit test” for the TDD game. It's better to write a single test for each edge of the behavioral boundary. The description (the name of the test method, or the text string that describes the purpose of the test) should then clarify how each test is unique.",
      "content_length": 1676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Try-catch statements: This is the most dangerous of all test smells in this catalog. If the catch catches something generic like Exception or Throwable, the test might not fail even when the code is broken. The resulting safety net might lie to you.\n\nIf your code throws an unexpected exception, the test framework will tell you. If throwing the exception is the expected behavior, then use the assertion designed to check for thrown exceptions. The syntax for an exception-thrown assertion typically takes a block, or “lambda function,” so it can be tricky. Familiarize yourself with the correct assertion syntax for your framework.\n\nSmell: Calculations\n\nLike Branching Code, this smell indicates you are asking your test (and your reader) to do too much.\n\nSmelly:\n\ndescribe 'converter' do\n\nit 'converts Fahrenheit to Celsius' do converter = Converter.new expect(converter.f_to_c(70)).to eq(70 - 32 * 5 / 9) end\n\nend\n\nThe calculation for the conversion appears in the test, but it’s difficult to understand at a glance. Here’s the implementation:\n\nclass Converter def f_to_c(f) f - 32 * 5 / 9 end\n\nend\n\nThis test passes. The trouble is that both the test and the implementation are wrong! This can happen when a developer copies the test code into the implementation, or vice versa.\n\nEven if the implementation and the calculations in the tests were correct, those calculations still make the tests harder to read. To understand the test, the reader’s brain must parse and execute the calculation.",
      "content_length": 1498,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "To resolve this test smell:\n\nAvoid mathematical operators in your tests. To write a good test, you must know the precise outcomes. Use a calculator, search the Internet, or ask a business analyst. If you’re working in a scientific field, ask your client scientists to get out their slide rules and tell you what the results should be.\n\nCreate a scenario with math so excruciatingly obvious that anyone who subsequently reads the test can do the math quickly without a calculator.\n\nAvoid “magic numbers” by assigning values to variables, constants, or fields with descriptive names.\n\nTest right at the edge of any behavioral boundaries. Since Fahrenheit-to-Celsius temperature conversion is a simple, linear, and well-known algorithm, it doesn’t really have any true behavioral boundaries. Even so, you can start with well-known values.\n\nBetter:\n\nit 'converts Fahrenheit to Celsius when water freezes' do converter = Converter.new expect(converter.f_to_c(32)).to eq(0) end\n\nit 'converts Fahrenheit to Celsius when water boils' do converter = Converter.new expect(converter.f_to_c(212)).to eq(100) end\n\nIn this case, two discrete values assure that the calculation is correct. They could be any two values.\n\nYou might prefer to use a temperature below freezing to confirm that your implementation allows for negative numbers. What follows is a truly nerdy example: absolute zero! It could be a behavioral boundary because it is the “lowest temperature possible.”13\n\n13 https://en.wikipedia.org/wiki/Absolute_zero",
      "content_length": 1510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "it 'converts Fahrenheit to Celsius at absolute zero (approximat\n\nexpect(converter.f_to_c(-459)).to eq(-273)\n\nend\n\nSmell: Custom Parent Test Classes\n\nThe very earliest versions of JUnit relied heavily on subclassing junit.TestCase rather than making use of delegation. In turn, our early TDD teams would often create a project-specific abstract subclass of TestCase, which would act as parent to all of our actual TestCase classes. That got messy very quickly.\n\nThere are often good reasons, and good ways, to add custom assertions or object builders to your test framework. Research the best way to do that, given your specific domain and tools.\n\nSmell: End-to-End Tests\n\nSome code smells aren’t always bad. Every TDD project I’ve worked on supplemented the unit-test safety net with a small percentage of end-to-end tests. The latter were called “integration tests,” “navigation tests,” or “acceptance tests.”\n\nThe trouble occurs whenever these tests slow down the feedback loop between making a change and knowing that you broke something. When actively playing the TDD game, 40 seconds seems to be the absolute maximum. If you’re integrating only once every couple of hours—for example, when a task is complete—then 15 minutes (the length of a coffee break) is still the absolute maximum. If any testing takes longer than that, the efficacy of the safety net will rapidly deteriorate.\n\nHere’s an example. Imagine you find a manual test case that looks something like this:\n\n1. Log in as Richie Rich.\n\n2. Check that you are on the home page.\n\n3. Click on the Performance menu.",
      "content_length": 1578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "4. Check that you are on the Performance page.\n\n5. Click on the Amazing Bookstore stock symbol.\n\n6. Check that you are on the personal performance page for Richie Rich’s Amazing Bookstore holdings.\n\n7. Assert that Richie Rich’s Amazing Bookstore performance is the correct percentage, given his various purchases of the stock over the years.\n\nThere are so many test smells here, and so much overhead just to get to the intended behavior under test—the personal investment performance calculation. Once this test is automated, there are at least a dozen reasons why it could fail. The system needs to authenticate the user, load all past purchases of the investment (from a database), obtain the current share price for the investment (from a third-party API), and perform the calculation correctly.\n\nLikely, all the code for loading assets from the database, calculating their current monetary value, and summing them up is located within one big smelly code block. The way to reduce this smell is to find discrete behaviors in the test and the implementation that can be tested in isolation —for example, the calculation of percent gain. You can then turn that part of the test into a small group of fast tests (see Chapter 7, “Testing Legacy Code”). Next, you should remove the related assertions from the integration test. Repeat as needed for authentication, navigation, database access, API access, and so on.\n\nWhat remains of your end-to-end test will be a test of the system’s integration points (presumably as deployed on a test server). You will benefit from testing the integration points of your system using small integration tests. But avoid forcing those tests to do double duty as tests of business logic.\n\nKey Lesson\n\nWhat you test separately, you can alter independently.\n\nOften, when I’ve seen teams develop and test with end-to-end\n\ntests, the implementation will contain large blocks of code that",
      "content_length": 1916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "reference multiple external resources. In the Richie Rich example, you can imagine a block of code that extracts investments from the database, then fetches the current price, performs the calculations, and writes the results to the UI framework. When all that happens within a single set of curly braces, it inadvertently couples all referenced dependencies. As a result, it becomes more difficult to change one without possibly breaking access to the others.\n\nBy testing behaviors separately through unit tests, you avoid\n\nunintended behavioral coupling within your implementation.\n\nTo minimize this test smell, always look for ways to unit-test a behavior. Here are some examples that I’ve encountered (and that sometimes surprised me):\n\nSite navigation is a behavior and can be unit-tested without deploying the site.\n\nPage rendering might be a behavior that can be isolated, depending on your architecture.\n\nOn at least two applications I encountered, database access could be integration-tested using the unit-testing framework, with a few simple Create–Read–Update–Delete tests. In one case, we ran these “object relational mapping” tests against a table named something like WORST_CASE_TABLE_FOR_TESTING_ONLY.\n\nFinally, here is the behavior that the original manual test was supposed to test, now with clearly stated inputs (given) and results (then):\n\nit 'gives percent gain for multiple purchases' do\n\nticker = \"AMZBK\" big_gain = Purchase.new(ticker, any_timestamp(), shares=10, p zero_gain = Purchase.new(ticker, any_timestamp(), 10, 25) small_loss = Purchase.new(ticker, any_timestamp(), 10, 30) purchases = [ big_gain, zero_gain, small_loss ]\n\ncurrent_price = 25 expected_percent_gain = 15 expect(account.percent_gain(purchases, current_price))",
      "content_length": 1757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": ".to eq(expected_percent_gain)\n\nend\n\nSmell: Tear Down\n\nTesting frameworks typically have an AfterEach mechanism that cleans up after each test (or an AfterAll to wrap up the full test run). It is most often used to reset a database or log out of an authenticated session.\n\nWhen doing TDD, this mechanism is rarely needed. Most tests are thoroughly cleaned up by the garbage collector.\n\nUsing AfterEach is a test smell that indicates you no longer have an independent unit test. It tells you a relatively expensive and independent resource needs to be released after the test runs.\n\nFor example, suppose your test populates records in a database, but then needs to erase them after the test runs. You might have written a very useful test, but it’s not isolated enough to assure that the test will pass quickly and consistently.\n\nWhenever this test smell arises, a test is likely accessing an external dependency that can be replaced with a test double (see Chapter 6).\n\nSmell: Frequent Changes to Given\n\nThis smell occurs when you find yourself returning to previous tests and changing the example data in the given part. Typically, this happens when you are developing a new behavior and permutations arise as you add more complexity.\n\nAnother indicator of this smell is example data that becomes less clear as it becomes less specific to each individual specification. In essence, you wind up with a jumbled, generic blob of test data for all related scenarios.\n\nThis smell often crops up during the Password Exercise (see the Appendix). That exercise asks you to build a single call to determine whether a new password is strong or weak based on a set of rules. The first rule is that the password must have a length greater than 7.",
      "content_length": 1734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "package password;\n\nimport org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.*;\n\npublic class PasswordCheckerTests {\n\nPasswordChecker checker;\n\n@BeforeEach void InitializeChecker() { checker = new PasswordChecker(); }\n\n@Test void StrongWhenLongEnough() { assertThat(checker.isStrong(\"12345678\") }\n\n@Test void WeakWhenTooShort() { assertThat(checker.isStrong(\"1234567\")) }\n\n}\n\nThis example is a bit contrived and simplistic, but it rapidly demonstrates what can happen when the given data (in this case, the password string) contains combinations that alter behavior.\n\nAccording to Part 1 of the two-part exercise, a strong password also needs to have an uppercase letter, a digit, a lowercase letter, and a special character. You might find yourself repeatedly going back to each existing test to make the “strong” password stronger.\n\nAt the same time, you should be conscientiously (one hopes) editing each weak password so that each exercises one, and only one, of the required rules. For example, having “foo” as the weak password in all tests is not at all clarifying, because it breaks all but one of the password rules in Part 1.\n\nFor example:\n\n@Test void StrongPassword() {\n\nassertThat(checker.isStrong(\"Pa$$w0rd\")).isTrue();",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "}\n\n@Test void WeakWhenTooShort() {\n\nassertThat(checker.isStrong(\"Pa$$w0r\")).isFalse();\n\n} @Test void WeakWhenMissingCapital() {\n\nassertThat(checker.isStrong(\"pa$$w0rd\")).isFalse();\n\n} @Test void WeakWhenMissingDigit() {\n\nassertThat(checker.isStrong(\"Pa$$word\")).isFalse();\n\n} @Test void WeakWhenMissingSpecialCharacter() {\n\nassertThat(checker.isStrong(\"Passw0rd\")).isFalse();\n\n}\n\n...\n\nThe Password Exercise is a contrived example designed to generate this very test smell quickly (and annoyingly). On a complex and realistic project, you might miss this smell until it becomes much more pungent. There’s a “tipping point” with code smells: Once they grow large enough, cleaning them up no longer seems cost-effective, and we then let them grow out of control.\n\nThis test smell always has a strong code smell mirrored in the implementation. Therefore, the solution involves refactoring both the tests and the implementation.\n\nPart 1 of the Password Exercise will let you get away with mildly smelly tests and implementation. Part 2 is designed to wreck everything. It simulates what happens when you are asked for unplanned enhancements that require invasive changes within your implementation. You’re asked not to read Part 2 until you’ve completed Part 1, to avoid spoilers! Once you have read Part 2’s requests, consider refactoring the existing tests and implementation before tackling those requests.\n\nWithout giving away too much of one possible “solution” to the Password Exercise, here are some things to consider when you encounter this test",
      "content_length": 1549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "smell while doing TDD in the real world:\n\nYour tested object is probably doing too much, which is causing you to churn through permutations in the test data.\n\nCould the called object delegate part of the overall behavior to a set of simpler objects? Look for which behaviors are changing most frequently across tests, extract those changing behaviors into their own isolated form using the most appropriate features of your programming language (for example, classes, functions, modules, or tuples), and test each discrete behavior in isolation. You will know you’re on the right track if you can test these new objects using simpler test data. For example, recall that the password length rule was originally tested with strong = “12345678” and weak = “1234567.” That wasn’t accidental: Those values tell you exactly where the behavioral boundary is for the length rule.\n\nTo uncover a name for those classes, functions, or objects, listen carefully in your conversations with teammates. When you talk about the business domain, does everyone keep using a noun that could represent a useful abstraction in your domain model? For example, the password strength checker applies multiple _______s (fill in the blank) to each password string.\n\nIf you used refactorings to remove excess behaviors from the original object and then wrote simpler unit tests for those new delegates, look at the remaining untested implementation of that object to determine what still needs testing. It won’t be trivial, but it won’t be subject to permutations.\n\nFinally, you can safely delete any redundant permutation tests.\n\nSmell: For Testing Only\n\nSometimes, a developer will add a method to the tested object to either inject test data or to have something to assert against. This is a noble effort, but if the method is never used elsewhere, then the test has expanded the object’s interface just to make it testable.\n\nSmelly:",
      "content_length": 1909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "describe \"board\" do it \"is 10x10\" do board = Board.new expect(board.height).to be(10) expect(board.width).to be(10) end\n\nend\n\nEven experienced TDD developers will occasionally create this smell. It can happen when building new objects, particularly if developers skip using CRC cards beforehand. Sometimes your code will need to get a little smellier before the right refactorings reveal themselves. So, if this test smell is short-lived, there’s no need to worry.\n\nHowever, if you’re about to knowingly commit a For Testing Only test smell to your team’s repository, first make the smell even more pungent, so that someone on your team will easily detect it later and perhaps find a better solution. For example, rename the questionable methods to something excruciatingly obvious and a little obnoxious.\n\nVery smelly (on purpose):\n\ndescribe \"board\" do it \"is 10x10\" do board = Board.new expect(board.height_FOR_TESTING_ONLY_please_fix_me).to be expect(board.width_FOR_TESTING_ONLY_please_fix_me).to be( end\n\nend\n\nThe For Testing Only smell will dissipate only after you find a cleaner design through refactoring, a richer behavior to be tested, or a real business need for the For Testing Only methods.\n\nSmell: Intermittent Failure\n\n“This must be Thursday,” said Arthur to himself, sinking low over his beer. “I never could get the hang of Thursdays.”\n\n—Douglas Adams, The Hitchhiker’s Guide to the Galaxy14",
      "content_length": 1409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "14 low\n\nwww.goodreads.com/quotes/13822-this-must-be-thursday-said-arthur-to-himself-sinking-\n\nTrue story: A client team had a test that would always fail on Thursdays. I asked them what they were doing about this test, and they said, “We wait until Friday and run it again.”\n\nThat level of unpredictability needs to be investigated. They had either a poorly conceived test, a race condition, or an unreliable data source.\n\nEvery Intermittent Failure represents a risky gap in your safety net.\n\nImagine you need to write a couple of stable unit tests for the following method (in C#):\n\npublic bool IsTodayThursday() { DateTime today = DateTime.Now; return today.DayOfWeek == DayOfWeek.Thursday; }\n\nThe trouble is caused by the external dependency DateTime.Now.\n\nFirst, perform the Extract Method refactoring on everything below the initialization of the external object(s). Make the newly extracted method internal so the tests can access it, but not public (unless and until a business need for the new method is found).\n\npublic bool IsTodayThursday() {\n\nDateTime today = DateTime.Now; return IsItThursday(today);\n\n}\n\ninternal bool IsItThursday(DateTime today) {\n\nreturn today.DayOfWeek == DayOfWeek.Thursday;\n\n}\n\nNow you can thoroughly test IsItThursday(DateTime today). Two tests are needed, one for each side of the behavioral boundary: one for Thursday and one for not-Thursday.",
      "content_length": 1382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Have you just traded Intermittent Failure for another test smell—for example, For Testing Only? Perhaps, but it’s a valuable trade-off: For Testing Only is a design issue, whereas Intermittent Failure is a quality issue.\n\nThat’s the nature of code smells. You clean up those that bother you the most, and sometimes you’re left with other smells that don’t demand your immediate attention.\n\nTool Tip\n\nDon’t neglect other forms of testing.\n\nTDD does a very nice job of covering very nearly 100 percent\n\nof your code. It will greatly reduce the need for other forms of testing, but it doesn’t eliminate them completely.\n\nThe Intermittent Failure example includes a line of non-unit-\n\ntestable code: the code in IsTodayThursday() that passes DateTime.Now to IsItThursday() and returns the results of that call. How would you test that?\n\nOne small, well-designed functional or integration test— written in any automated testing framework—could exercise quite a few external dependencies and test the code that calls them. For example, a single Cucumber scenario using the Playwright headless-browser library could run through just one full scenario and provide assurances that your application is deployed correctly (preferably to a test-only web server) and that all APIs, external libraries, and databases are accessible.\n\nSmell: Scattered Changes\n\nThe smell: Before writing a new test for a new behavior, you need to make the same change to a call or constructor in many existing tests. This often occurs when you are first developing an object’s interface. Perhaps you find a need to add a new parameter to a constructor or a method signature.\n\nScattered Changes is a very important smell, because it causes many developers to perceive test maintenance as more work than it’s worth. The key to overcoming this smell is to take baby steps.",
      "content_length": 1837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Definitions\n\nMethod signature: A method’s “signature” is what makes that method uniquely identifiable to the compiler or runtime environment. A method’s signature includes its name and parameter types, but not the return type.\n\nOverload method: An overload method is a method with the same name but with a parameter list that differs in number or types. Overloads allow you to give related methods the same name but a unique method signature.\n\nContrast overload with override, which is a method in a\n\nsubclass that has the same signature but a different implementation that “overrides” the parent class’s behavior. Unless the parent’s method is abstract (having no implementation), override methods are considered a very stinky code smell.\n\nNote that Ruby does not support overload methods, per se. Instead, it allows for optional parameters with default values.\n\nContinuing with the Salvo game example, suppose you initially decided to add the notion of orientation to each ship, but now you realize it would be better as a parameter to the board.place() method.\n\nSmelly:\n\ndescribe 'placing ships on the board' do\n\nbefore(:each) do @board = Board.new end\n\nit 'knows two ships cannot overlap' do vertical_ship = Ship.new(size=3, Orientation::VERTICAL) horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL) @board.place(vertical_ship, 0, 0) expect(@board.can_place_ship_at(horizontal_ship, 0, 0)).to end\n\nit 'will work when there are empty cells around each ship' do vertical_ship = Ship.new(size=3, Orientation::VERTICAL) horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL) @board.place(vertical_ship, 0, 0)",
      "content_length": 1621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "expect(@board.can_place_ship_at(horizontal_ship, 0, 2)).to end\n\nend\n\nRather than changing everything all at once, you can take several baby steps that will lead you to the desired outcome. Which baby steps you use, and in which order, will be very contextual. The overall strategy is to keep the tests passing with each small change.\n\nStart by looking for Duplicate Given and move the duplication into a BeforeEach.\n\nBetter:\n\ndescribe 'placing ships on the board' do\n\nbefore(:each) do @board = Board.new @vertical_ship = Ship.new(size=3, Orientation::VERTICAL) @horizontal_ship = Ship.new(size=4, Orientation::HORIZONTAL end\n\nit 'knows two ships cannot overlap' do @board.place(@vertical_ship, 0, 0) expect(@board.can_place_ship_at(@horizontal_ship, 0, 0)) .to be false end\n\nit 'will work when there are empty cells around each ship' do @board.place(@vertical_ship, 0, 0) expect(@board.can_place_ship_at(@horizontal_ship, 0, 2)).to end\n\nend\n\nYour next step depends on your language and tools. If your IDE has a built- in Introduce Parameter refactoring to add a parameter to a method call, consider using that to pass in an Orientation argument.\n\nIf your IDE doesn’t support Introduce Parameter, you can add the parameter to the method implementation yourself and let compiler errors guide you to all the places where you need to add the value. As a temporary measure, perhaps make it an optional parameter with a default value, if there is a sensical default. C# and Ruby support optional parameters, and if",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "the parameter isn’t given in a call, the default value is used. Note that optional parameters must follow required parameters. (In this example, neither HORIZONTAL nor VERTICAL makes much sense as a default: The caller really must provide the value.)\n\nAn alternative way to incrementally add a parameter to a method is to create an overload method. If your language does not support overloaded methods, then you could create a method with a slightly different name.\n\nIn our example, you can first perform an Extract Method refactoring on almost then pass ship.Orientation from within the old method as the new argument value. You can then make your new method public and change the tests to call that method. This technique is particularly useful if you need to preserve the existing interface of the object—for example, if it’s part of a deployed API that must remain backward compatible.\n\neverything within\n\ncan_place_ship_at()\n\nand\n\nHere’s an example using Extract Method in C# to create an overload of CanPlaceShipAt().\n\nBefore Extract Method (in C#):\n\npublic bool CanPlaceShipAt(\n\nShip ship, int startingRow, int startingColumn)\n\n{\n\norientation = ship.Orientation; // most of the logic removed for clarity return result;\n\n}\n\nAfter Extract Method (in C#):\n\npublic bool CanPlaceShipAt(\n\nShip ship, int startingRow, int startingColumn)\n\n{\n\norientation = ship.Orientation; return CanPlaceShipAt(ship, startingRow, startingColumn, orientation);\n\n}\n\npublic bool CanPlaceShipAt(\n\nShip ship, int startingRow, int startingColumn, Orientation orientation)\n\n{",
      "content_length": 1553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "// most of the logic removed for clarity return result;\n\n}\n\nThis refactoring keeps all previous calls intact. A possible next step could be to delete the original method, and then follow compiler errors (or test failures in Ruby) and add the orientation to each of those calls.\n\nGood:\n\ndescribe 'placing ships on the board' do\n\nbefore(:each) do @board = Board.new @vertical_ship = Ship.new(size=3) @horizontal_ship = Ship.new(size=4) end\n\nit 'knows two ships cannot overlap' do @board.place(@vertical_ship, 0, 0, Orientation::VERTICAL) expect(@board.can_place_ship_at(@horizontal_ship, 0, 0, Orientation::HORIZONTAL)) .to be false end\n\nit 'will work when there are empty cells around each ship' do @board.place(@vertical_ship, 0, 0, Orientation::VERTICAL) expect(@board.can_place_ship_at(@horizontal_ship, 0, 2, Orientation::HORIZONTAL)) .to be true end\n\nend\n\nIf you need to add a parameter to a constructor, the refactorings are similar. When one constructor calls another, they are referred to as “chained constructors.”\n\nSmell: Multiple Test Doubles\n\nTests that must train more than one test double are messy and hard to follow.15 The Multiple Test Doubles smell suggests that more than one “unit” of behavior is being tested. Related test smells are Excessive Given and End-to-End Tests.",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "15 Given that a few of the test smell descriptions suggest test doubles to resolve the smells, it’s only fair to warn you here about two test smells related to test doubles. You might want to revisit these two smells, Multiple Test Doubles and Exposed Implementation, after reading Chapter 6.\n\nThe most pungent form of this smell occurs when one test double must return another test double. For example, it would occur if a mock java.sql.Statement returns a mock java.sql.ResultSet to the tested object.\n\nThis smell occurs most frequently when tests are written after the implementation. If you find yourself “patching” your safety net around previously untested code, you might have to live with this test smell for a while. See Chapter 7, “Testing Legacy Code,” for helpful guidance on adding tests to untested code.\n\nSmell: Exposed Implementation\n\nSpies are a particular type of test double that record the interactions that occur between the tested object and the spy (representing a dependency). Because the test must train the spy to expect certain calls, the test usually contains (and tests) implementation details, rather than focusing on outcomes.\n\nThe Exposed Implementation smell can often be reduced by further identifying and breaking up responsibilities across objects or methods, either via CRC cards or refactorings. However, as with most code smells and test smells, there’s often a trade-off. Spies (described in more detail in Chapter 6) have an important role to play when you need to add tests to existing untested code (the topic of Chapter 7).\n\nOther Common Testing Challenges\n\nThe scenarios described in the sections that follow occur mostly when code is written before tests. They are described here because they reflect an important facet of test maintenance: These challenges can be avoided by writing each test first.",
      "content_length": 1846,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "How to Test a Private Method\n\nWhen playing the TDD game, private methods usually occur when some bit of behavior is extracted from the public behaviors. So, they’re fully tested due to your test-driven approach.\n\nIf you still feel that the private method could use some additional testing, your unease is a strong indication that the behavior lives in the wrong class. If it can be tested in isolation—that is, without a lot of the surrounding object state—then it is ripe for its own encapsulation. Look for opportunities to move the private method to a more appropriate home (with the Move Method refactoring) or create a new object (with the Extract Class refactoring).\n\nAfterward, you might want to clean up any Unrelated Assertions or Extended Narrative test smells that arose, as described earlier in this chapter.\n\nHow to Test an Abstract Class\n\nWith TDD, you typically wait until you have at least two similar objects before extracting an abstraction. Once you notice the duplication across objects, you can reduce it by moving the common behaviors to an abstract class (in Java or C#) or, in Ruby, perhaps a module.\n\nBut you don’t need to point the existing tests to the abstract class or module. Those behaviors are all tested through the concrete subclasses that you’ve already built using the test-driven approach.\n\nDesign Detour\n\nThere has long been a heuristic called the “Rule of Three.” When you have three of something, or when something occurs three times, it’s time to reduce the duplication across those three by some technique—for example, by creating a private method, an abstract parent class, or a Ruby module, or by delegating the behavior to a new dependency object.\n\nConsider being even more sensitive to duplication and playing\n\nthe TDD game by following the “Rule of Two.” It’s always",
      "content_length": 1813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "possible to separate behaviors that are common from those that are unique and to reduce duplication. It’s very rarely the wrong thing to do. Even if you make a slight misstep and must backtrack a little, your safety net always gives you the freedom to experiment with alternative designs.\n\nHow to Test a Method That Creates Its Own Dependencies\n\nThis challenge could occur even with TDD, though it’s mostly avoidable: Avoid both creating and calling an object within the same block of code.\n\nBefore you write the next test, follow these steps:\n\n1. Move all initializations up closer to the beginning of the method.\n\n2. Perform an Extract Method refactoring on everything below those initializations. All challenging dependencies will be passed into the new method as arguments.\n\n3. Make that new extracted method accessible to the tests.\n\n4. Test the new method by passing the dependencies created by the test. The most challenging dependencies can be replaced with test doubles.\n\nOther common refactorings could also help—for example, extracting a Factory Method or Builder to encapsulate the creation code. Test them separately. Your production system can use the factory or builder to assure that objects are created with all the necessary dependencies and absolutely no null pointers anywhere.\n\nA few such refactoring examples appeared earlier in this chapter. Look again at the code that answered the question, “Is it Thursday?” (“Smell: Intermittent Failure”), and the C# example where we extracted an overload of CanPlaceShipAt() (“Smell: Scattered Changes”).\n\nHow to Test or Refactor Existing Untested Code\n\nUntested code is often a huge impediment to starting and sustaining a healthy TDD practice. This problem is so big that it warrants its own chapter (see Chapter 7). That chapter will show you how to patch the safety net whenever you need to alter existing untested code.",
      "content_length": 1886,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Summary\n\nTeams that follow a test-driven approach with diligence will find that the safety net of tests need to be maintained if they want to continue receiving its benefits. Knowing what a good test looks like, how to write good tests, and how to tell when a single test or a collection of them could be made better are all part of the TDD long game.\n\nThe path to high-quality code is test-driven, and the path to an extensible design is refactoring—specifically, refactoring away code smells before they grow into mountains of technical debt. Frequently review your existing suite of tests and identify the most pungent or most common test smell currently wafting through the suite. Discuss it by name (if it’s in this chapter; otherwise, have your team choose their own name), and make an effort to refactor away the stink.\n\nThe chapters that follow describe ancillary but essential TDD practices that will help you with that maintenance. Chapter 6 will help you choose the right test doubles to keep the safety net running swiftly, providing the team with critical feedback within minutes instead of weeks. Chapter 7 addresses in detail the issues with existing untested code.\n\nOceanofPDF.com",
      "content_length": 1196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Part II: Ancillary Practices\n\nOceanofPDF.com",
      "content_length": 44,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Chapter 6. Test Doubles\n\nIn the late 1990s, when my team first started experimenting with a test- driven approach, we ran into trouble unit-testing any object that interacted with an external dependency (Figure 6.1). A hardware driver presented the most painful challenge because it didn’t even exist yet, and we had to talk to the hardware team in a different time zone to learn how it might behave. This type of challenge occurs whenever your code must interact with a dependency that is slow, unpredictable, complex, expensive, or risky.\n\nFigure 6.1 The problem: Testing object “a” with test t(a) involves external dependency x, which is relatively slow and could give",
      "content_length": 671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "unpredictable results.\n\nA test double is an object that represents the external dependency in a test scenario but is not the actual production dependency (Figure 6.2). It is created and controlled by the test, rather than by the tested object. Test doubles are most useful when an external dependency complicates ease of testing. A test double plays a supporting role in the test scenario.\n\nFigure 6.2 The external dependency x has been replaced with test double x′ (“x-prime”) under the control of test t(a).",
      "content_length": 509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "Key Lesson\n\nTest-Driven Development (TDD) is intended for creating, designing, and testing your code’s behaviors. To do this effectively, you will occasionally need to isolate your efforts from other people’s code—that is, from external dependencies.\n\nThere are different flavors of test doubles for different types of software behaviors. This chapter covers many types and uses of test doubles, with examples in Java.\n\nThe Trouble with Dependencies\n\nOften, software requires a dependency that exhibits one or more of the following testing obstacles:\n\nSlow: Anytime a test accesses a disk drive or network, either directly or indirectly, that test will take a lot longer to run than a similar test running entirely in memory. A team might not even notice this slowdown in the first few hundred tests. But once a full test run exceeds even a minute, you start losing the benefits of TDD’s rapid feedback loop.\n\nUnpredictable: Many external dependencies will respond differently over time, which makes writing an independent and repeatable test more challenging. The system clock, a random number generator, and even a “for testing only” database could give you an undesirable response during a test run. Some external dependencies are also unreliable (for example, the Wi-Fi in my house). You will want to test how your software responds to an outage without having to physically disconnect the dependency’s hardware at just the right millisecond.\n\nComplex: Your object-under-test might have dependencies that have their own dependencies, in a cascading tree of interdependence.\n\nExpensive: Some\n\nthe software’s external environment could be expensive in terms of real money. For\n\ninteractions with",
      "content_length": 1697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "example, when testing stock-trading software, you don’t want your tests buying and selling real stocks.\n\nRisky: When testing a rocket’s navigation software, you don’t want to have to launch a real rocket.\n\nA Test Double Taxonomy\n\nTest doubles can alleviate the difficulties in even the most challenging testing scenarios. In essence, the test double acts as a replacement for the usual dependency, yet is under the direction of the test. It can simulate any kind of behavior needed to test your code’s behavior. Additionally, once the code under test is called, the test double can optionally be queried to verify that all interactions went as planned.\n\nThere are several types of useful test doubles. Gerard Meszaros collected and delineated various terms that people were using and coined the term “test double” as an umbrella term for these constructs.1 The sections that follow introduce pertinent parts of his taxonomy, plus a new addition, the Nullable, which was named and defined by James Shore.\n\n1 https://martinfowler.com/bliki/TestDouble.html\n\nFakes\n\nA fake2 is a substitute dependency that has real—but not production-ready —behavior. It will behave sufficiently similar to the actual production dependency, but is limited, less expensive, or easier to install.\n\n2 Not to be confused with the TDD Fake It technique. You don’t need a fake to Fake It.\n\nExamples include (but are limited only by your creativity):\n\nA mobile-device emulator\n\nA lightweight, unjournaled in-memory database\n\nA localhost network connection",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "A for-testing-only web API account with a maximum budget of a few dollars\n\nA fast but obsolete encryption algorithm\n\nA virtual machine running a flavor of your operating system that is similar, but perhaps not the same as, the version used in production\n\nAll of these substitutes could be used to test your code’s behavior, but then you could choose not to use them when actually deploying your software.\n\nFakes can be either configured to be available to the test environment or constructed in each test and delivered to the system under test in the given part of the test.\n\nStubs\n\nA stub is a very simple replacement object that will return predetermined values. Stubs can return a constant value or values given to it by the test. They can also be told to throw an exception to simulate an outage or programmatic error.\n\nMock frameworks can create stubs for you, or you can “hand-craft” them in your test code. In strongly typed languages like Java and C#, a stub either implements an interface, subclasses an abstraction, or subclasses the dependency’s concrete class and overrides all required methods.\n\nStubs are typically “trained” to respond in the given portion of a test or group of tests. This is usually accomplished by passing the appropriate return values into the stub’s constructor, or with simple setters, or by using a stub Builder.3\n\n3 https://en.wikipedia.org/wiki/Builder_pattern\n\nConsider the Salvo game exercise found in the Appendix at the end of this book. There are two optional features: One allows the game to randomly place the player’s ships on the board, and another enables the game to play against the player. In either of those cases, the game needs a way to generate a random board position.",
      "content_length": 1726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "To test these features, your tests will need to have full control over a random number generator. The tests will replace the default random number generator with a simple stub. Each test “trains” the stub with the return values appropriate for that scenario.\n\nThe following example is a simple JUnit test using a hand-crafted stub. The stub definition that follows can be defined in its own file in the test project, or as an inner class within the JUnit test class.\n\n@Test void generatesRandomBoardPositions() {\n\n// Given: create the stub int expectedRow = 4; int expectedColumn = 2; java.util.Random randomNumbers = new MyRandomStub(new int[] {expectedRow, expectedCo // Given: introduce the stub to the tested object Player autopilot = new Player(randomNumbers);\n\n// When: int[] position = autopilot.selectBoardPosition();\n\n// Then: assertThat(position).isEqualTo( new int[] {expectedRow, expectedColumn});\n\n}\n\nAnd here is the stub, a hand-crafted subclass of java.util.Random:\n\npublic class MyRandomStub extends java.util.Random {\n\nprivate final int[] integersToReturn; private int indexToNextRandom = 0;\n\npublic MyRandomStub(int[] integersToReturn) { this.integersToReturn = integersToReturn; }\n\n@Override public int nextInt(int maximumExclusive) { return this.integersToReturn[indexToNextRandom++]; }\n\n}\n\nA few warnings about stubbing this way:",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Stubs are test code. Be sure they exist only in the test project.\n\nThe stub in the example extends Random instead of implementing the RandomGenerator interface. To implement the interface, you would need to stub every method declared on the interface, even if you don’t use them all. This is disappointing because using an external interface within your implementation is often preferable. Notably, there are many broad “kitchen sink” interfaces out in the wild; for example, java.sql.ResultSet has almost 200 method declarations. if you\n\nConsider what would happen\n\nto alter your implementation to call nextLong() instead of nextInt() as a refactoring (that is, without first adding or changing a test). Because the stub already inherited nextLong() from java.util.Random, if you forgot to alter the stub, then tests would start to fail intermittently (and randomly). Mock frameworks (described later) solve this problem by causing a failure whenever the tested object calls a method that the mock wasn’t expecting. Most mock frameworks will also provide in the failure messages a description of the calls that were received by mistake.\n\nYou could mistakenly train your test double to return values that the real production dependency would never produce. For example, MyRandomStub should never be trained with a stubbed return value of maximumExclusive or greater in any test, because the Java Class Library implementation of Random would never do that. Avoid testing impossible scenarios; otherwise, you might feel compelled to write defensive code for that scenario, either in your implementation or in the stub itself.\n\nStubs should be simple. If your stub does contain some logic—for example, MyRandomStub increments its index with each call—you may want to unit-test the stub, too. Just be sure never to deploy the stub into production!\n\nMocks\n\nA mock4 is a test double that is generated by a mock-object framework, used in a test, and then discarded once the test is done.",
      "content_length": 1980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "4 In conversation, many developers—me included—will use the term “mock” interchangeably with “test double” as the umbrella term. It’s simpler to say, it’s what we called these objects before a test double taxonomy or the mock frameworks existed, and it works as both noun and verb, as in “Let’s mock that with a mock.” The terminology that your team uses in conversation will depend on whether the team favors linguistic convenience or precision.\n\nMany of the interesting behavioral boundaries that occur when your code interacts with an external dependency are determined by all the myriad ways the dependency could respond. Could it throw an exception? Does it occasionally return a null pointer?\n\nMock-object frameworks are most helpful in such cases. A hand-crafted test double would become too complicated when used across multiple unique scenarios.\n\nThe mock object, its class definition (a temporary subclass of the dependency’s type), and its mock implementation are all created while the test is running. Each test “trains” its mock object, describing how it is to play its role in one unique scenario. At the end of the test, the mock object is discarded and its training forgotten.\n\nAn Example of a Mock as a Stub\n\nMocking involves four steps:\n\n1. Creating the mock object\n\n2. Training it\n\n3. Injecting it into the tested object\n\n4. Verifying that the tested object interacted with the mock as expected.\n\nYou can often skip the last step—verifying—and instead use assertions against other results.\n\nConsider the following example of how to test the Salvo board position- generator using Mockito (mock framework details are emphasized):\n\npackage salvo;\n\nimport org.junit.jupiter.api.Test; import static org.assertj.core.api.Assertions.*; import org.mockito.*;",
      "content_length": 1769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "public class PlayerAutopilot {\n\n@Test void generatesRandomBoardPositions() { // Given: create the mock java.util.random.RandomGenerator randomNumbers = Mockito.mock(java.util.random.RandomGenerator.c\n\n// Given: introduce the mock to the tested object Player autopilot = new Player(randomNumbers); int expectedRow = 4; int expectedColumn = 2;\n\n// Given: train the mock Mockito.when(randomNumbers.nextInt(10)) .thenReturn(expectedRow) .thenReturn(expectedColumn);\n\n// When: int[] position = autopilot.selectBoardPosition(); // Then: assertThat(position).isEqualTo( new int[] {expectedRow, expectedColumn}); }\n\n}\n\nAny inexpensive mock framework can mock out anything that’s “virtual.” If there’s an interface available for the external dependency, you can give your future self a little more design flexibility by mocking the interface instead example, java.util.random.RandomGenerator rather than java.util.Random.\n\nof\n\na\n\nspecific\n\nconcrete\n\nclass—for\n\nDefinitions\n\nState-based: A test that compares object states against expected values is a state-based test.\n\nInteraction-based: A test that compares expected interactions (that is, calls to dependencies) with what occurred during the test run is an interaction-based test. You want to test your code’s behaviors, and part of that might be making sure it calls external dependencies at the right time, in the right order, or with the right values.",
      "content_length": 1398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Spies\n\nA spy records each method call made to it and the parameters passed to those methods. Once the scenario is finished, the test can “debrief” the spy. That is, it can verify that the calls to the spy occurred as expected. With a spy, you can determine whether multiple calls happened in the correct order, the correct number of times, and with the correct argument values.\n\nSpies are useful for interaction-based test scenarios, but hand-crafting your own spy may require a lot of custom test-double code. A mock object framework can provide spies that can be trained to expect different calls in each unique scenario.\n\nUsing a spy with TDD may feel awkward: How do you write a test that expects that implementation? Spies can cause us to write tests that are interaction-based and may contain the Exposed Implementation test smell (described in Chapter 5, “Sustaining a Test-Driven Practice”).\n\na\n\nparticular\n\nimplementation without\n\npresuming\n\nDesign Detour\n\nState-based tests tend to be more readable than interaction-based tests, as well as more resilient during refactoring. State-based tests describe behaviors from the external perspective of the caller, whereas interaction-based tests describe encapsulated implementation details.\n\nWhenever possible, favor tests that check for valuable outcomes (state-based) rather than prescribed interactions (interaction-based). Carefully consider which assertions you need to be sure those outcomes are what you wanted. You might have to exercise several objects, preferably using test doubles to replace only external dependencies.\n\nFurthermore, whenever a behavior is difficult to test, it’s not because testing is difficult; it’s due to initial design assumptions. Get out the CRC cards or brainstorm at a whiteboard and find a way to test separate behaviors separately.\n\nSpies are still practical whenever your code calls an external dependency that does not provide an immediate testable result—for example, if your",
      "content_length": 1973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "call gives the dependency the address of a call-back function. Spies are also useful if you need to write a test for an existing untested implementation— that is, if the implementation was not written test-driven but now needs to tests. The ancillary practice called have fast and comprehensive characterization testing will help you write those tests; it is described in Chapter 7, “Testing Legacy Code.”\n\nAn Example of a Mock as a Spy\n\nWhenever there’s an interaction between two objects that needs testing, but there is no convenient result to assert against, you can spy on the interaction. Also, if an external call is slow or expensive, and you want to avoid a mistakenly duplicated call, you can debrief (verify) the spy to confirm the correct number of calls occurred. This is a quick and easy way to defend your team against an expensive mistake by a jetlagged intern.\n\nThe following code is identical to the previous example test except for the addition of the verify() call:\n\n@Test void generatesRandomBoardPositions() { // Given: create the mock java.util.random.RandomGenerator randomNumbers = Mockito.mock(java.util.random.RandomGenerator.c\n\n// Given: introduce the mock to the tested object Player autopilot = new Player(randomNumbers); int expectedRow = 4; int expectedColumn = 2;\n\n// Given: train the mock Mockito.when(randomNumbers.nextInt(10)) .thenReturn(expectedRow) .thenReturn(expectedColumn);\n\n// When: int[] position = autopilot.selectBoardPosition();\n\n// Then: assertThat(position).isEqualTo( new int[] {expectedRow, expectedColumn}); Mockito.verify(randomNumbers, Mockito.times(2))",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": ".nextInt(Mockito.anyInt()); }\n\nThe mock framework will cause the test to fail if there were either more or fewer calls than expected. In addition, most frameworks will tell you why. For example, consider the test results in Figure 6.3, where I mistakenly told the test to expect only one call to nextInt() using Mockito.times(1):\n\nFigure 6.3 Mockito tells you when expected and actual calls to the mock did not match.\n\nTable 6.1 summarizes the pros and cons of using mock frameworks.\n\nTable 6.1 Mock Framework Pros and Cons",
      "content_length": 523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "An alternative to spying with mocks that allows you to use state-based TDD was introduced by my old friend and colleague James Shore. He calls these test doubles Nullables.",
      "content_length": 172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Nullable\n\nA Nullable is a stub that exists within production code but is typically accessed only by tests. Or, as James Shore puts it in his abbreviated article on the topic:\n\nAt first glance, Nullables seem like test doubles, but they’re actually production code with an “off” switch.5\n\n5 www.jamesshore.com/v2/projects/nullables/a-light-introduction-to-nullables\n\nHere’s how Shore described Nullables to me in an email dated May 19, 2023:\n\nIt’s not just external dependencies that are Nullable; anything that has an external dependency anywhere in its dependency tree is also Nullable.\n\nA Nullable is a class with the ability to “turn off” its interaction with external systems. A Nullable that has been “turned off” has been Nulled. Nullables are production classes, so some people call them production doubles rather than test doubles.\n\nAny production class can be Nullable. Most Nullables delegate to Nullable dependencies, but at the lowest level, Nullables are implemented with the Null Object pattern.6 In statically-typed languages, this involves replacing the external dependency with an interface and two implementations: a wrapper around the actual external dependency, and a stub that “turns off” the external dependency. In normal operation, the external dependency is used, but when the class is Nulled, the stub is used.\n\n6 https://en.wikipedia.org/wiki/Null_object_pattern\n\nAlthough Nullables are mainly used for testing, they do have production uses. For example, imagine a command-line tool with a “--dry-run” option. When the option is used, the tool is supposed to run normally without actually doing anything. A Nullable could be used to implement that behavior.\n\nRecommended Reading\n\nA Light Introduction to Nullables by James Shore: www.jamesshore.com/v2/projects/nullables/a-light-introduction- to-nullables\n\nA Salvo sample test might look like the following. Note that the earlier sample MyRandomStub class could be used by the NulledBuilder once the stub class is moved into the production project and encapsulated within Player:",
      "content_length": 2056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "@Test void generatesRandomBoardPositions_NullableVersion() { int expectedRow = 4; int expectedColumn = 2; int unusedAvoidsFakingIt = 6; int[] numberSequence = new int[] {expectedRow, expectedColumn, unusedAvoid Player autopilot = Player.NulledBuilder().withNumbers(n .create();\n\nint[] firstPosition = autopilot.selectBoardPosition(); assertThat(firstPosition).isEqualTo( new int[] {expectedRow, expectedColumn}); }\n\nTo me, the use of Nullables feels like an architectural pattern: something that a team should consider—and experiment with—prior to crafting a new group of objects related to one or more external dependencies. The advantages of fully state-based testing could be appealing.\n\nAdditional Recommendations\n\nTest doubles can befuddle even the most experienced developers until they become second nature. The sections follow provide some considerations to keep in mind when using any test double.\n\nthat\n\nFavor Dependency Injection\n\nIn all of the examples in this chapter that involve mocks and spies, the mock object was created by the test and then introduced to the tested object via its constructor. This type of “dependency injection” is very common in object-oriented code and tends to keep objects decoupled from their dependencies’ constructors and often their concrete types.\n\nExcessively coupled:\n\nprivate String name; private UserType user;\n\npublic Player() { this.user = new ConsoleUser();",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "this.name = user.queryResponse(\"What is your name?\"); }\n\nThe Player class uses (is coupled to) not only the UserType interface and its queryResponse() method, but also the concrete ConsoleUser type and its default constructor.\n\nIf you avoid both building and calling the same object within the same block of code, you give your system much more flexibility and testability.\n\nLess coupling:\n\nprivate String name; private UserType user;\n\npublic Player(UserType userType) { this.user = userType; this.name = user.queryResponse(\"What is your name?\"); }\n\nSomewhere, the code must decide which UserType is to be used, and there are many patterns and techniques to choose from.\n\nSimple static factories:\n\nprotected Player(UserType userType) { this.user = userType; this.name = user.queryResponse(\"What is your name?\"); }\n\npublic static Player createHumanPlayer() { return new Player(new ConsoleUser()); }\n\npublic static Player createAutopilotAdversary(\n\nRandomGenerator randomNumbers) {\n\nreturn new Player(new Autopilot(randomNumbers)); }\n\nDesign Detour\n\nAvoid calling a production factory in a test to create the tested object. Instead, favor calling a constructor on the tested object—",
      "content_length": 1180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "one that takes all challenging dependencies as parameters. That way, you can more easily configure the object for various tests. Often, the given part in your earlier unit tests can serve as a model for how to best construct an instance of the tested class in production code.\n\nYou can also write unit tests for most factory code. You’ll want to do so if that code can return different subclasses under different circumstances, or if it configures the returned object in some way.\n\nTool Tip\n\nIn Java, a constructor can be made protected or package- protected to prevent excessive use outside the factories. When the tests have the same package name (even though they live in a separate project), they can still use a protected constructor to introduce test doubles.\n\nIn C#, internal constructors are made accessible to tests by\n\nadding the tests’ assembly name to the implementation’s assembly configuration using InternalsVisibleTo():\n\n[assembly: InternalsVisibleTo(\"SalvoTests\")] ... internal Player(UserType userType)\n\nWrap External Dependencies\n\nTest doubles are great choices as substitutes for dependencies that are slow, unpredictable, complex, expensive, or risky. Typically, those are external dependencies. Another good, but seemingly contradictory, approach is to substitute for only those objects your team has control over.\n\nHow could you substitute for only external dependencies, but also classes within your control? By creating wrapper objects at the periphery of your architecture. In other words, you can hide each external dependency behind a thin wrapper—so thin that it contains no testable behavior. Then your tests can replace the wrappers with test doubles.",
      "content_length": 1682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "This consideration is especially important when your external dependencies are inconveniently designed. For example, java.sql.ResultSet has nearly 200 unique method signatures that all declare that they can throw checked exception SQLException.\n\nIf you’re building an API, don’t require an external caller to pass in a ResultSet. Instead, create your own Java interface and offer that as a way for your clients to interact with your system. The caller will need to wrap its ResultSet in a wrapper that implements your interface (Figure 6.4). You can then test your API using test doubles that also implement your interface (Figure 6.5).\n\nFigure 6.4 A thin delegating wrapper b(x) exposes only the methods of x that are used. This decouples x from your business domain and can reduce the number of methods that need to be stubbed.",
      "content_length": 829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Figure 6.5 Wrapper b(x) is replaced with test double b′, greatly simplifying testing.\n\nIf, instead, you are calling an external API (for example, JDBC), consider wrapping the external services in simple delegating wrappers. Include only those method signatures that are used by your system, thereby shrinking both the interface and the cognitive dissonance associated with the bloated service. Your tests can then substitute a test double that extends your wrapper class.\n\nHere’s sample wrapper code for related classes from the java.sql package.7\n\n7 Important caveat: This example is meant to demonstrate the general technique, and not to suggest a generic solution for JDBC.",
      "content_length": 676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "ResultSet:\n\npublic class DatabaseResult {\n\nprivate final java.sql.ResultSet realResultSet;\n\npublic DatabaseResult(java.sql.ResultSet wrapped) { this.realResultSet = wrapped; }\n\npublic boolean getBoolean(String columnLabel) { try { return realResultSet.getBoolean(columnLabel); } catch (java.sql.SQLException sqle) { throw new DatabaseException(columnLabel, sqle); } }\n\npublic int getInt(String columnLabel) { try { return realResultSet.getInt(columnLabel); } catch (java.sql.SQLException sqle) { throw new DatabaseException(columnLabel, sqle); } }\n\n}\n\nSQLException:\n\nimport java.sql.SQLException;\n\npublic class DatabaseException extends RuntimeException {\n\npublic DatabaseException( String localData, SQLException sqlException) { super( String.format(\"Local data: [%s]\", localData), sqlException); }\n\n}\n\nNotes about wrappers:\n\nWrappers can also wrap their results if necessary. But more than two layers of wrapper? That’s starting to smell odd. When I’ve worked with JDBC, for example, we instead isolated complete actions behind façades8 (for example, “Transaction” and “Query”). A query",
      "content_length": 1088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "would result in a complete collection of business objects, and SQLException was wrapped in fewer than a half dozen places.\n\n8 https://en.wikipedia.org/wiki/Facade_pattern\n\nIn typed languages such as Java and C#, nearly all code in a thin wrapper is sufficiently “tested” by compiling.\n\nThe try/catch/wrap blocks in the sample DatabaseResult wrapper are fully unit-testable. Given that you’re building a wrapper for ResultSet, which is causing the testing challenges in the first place, you would need to mock ResultSet with a mock framework just for those behaviors within DatabaseResult. Even a top chef uses a microwave from time to time.\n\nChapter 7, “Testing Legacy Code,” contains an example of a particularly useful wrapper, the “virtualizing proxy.”\n\nUse Simple Objects as Their Own Test Doubles\n\nObjects that are easy to create, such as Date (in Java) and DateTime (in C#), can act as their own test doubles. For example, you can create a Java Date from a string:\n\nDate b5goesLive =\n\nnew SimpleDateFormat(\"dd-MMM-yyyy\").parse(\"09-Mar-2256\"\n\nDefinition\n\nSociable test: A test that allows the tested object to access real collaborators to accomplish the expected outcomes.9\n\n9 https://martinfowler.com/bliki/UnitTest.html\n\nObjects such as Date, String, and Integer are immutable. That assures you that their state will not change during the execution of a test, no matter what your code asks them to do.\n\nIf your tested object is easy to construct out of other easily constructed objects, then you can test each unique behavior with a representative object graph. As an example, imagine you need to test a Filesystem API. You",
      "content_length": 1630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "might be asked to test whether the Filesystem can report on capacity used and capacity remaining, and that it can build a full pathname of a particular file. These behaviors can be tested by building a simple object graph, as demonstrated in Figure 6.6. Because the behaviors of the tested object and its collaborators are all exercised by each test, such tests would be considered “sociable tests.”\n\nFigure 6.6 A Filesystem with a few folders and files of various sizes.\n\nWould sociable tests still be “unit tests”? Certainly, if those supporting objects do not themselves access a troublesome external dependency. Once you have fully tested a behavior, it can be used confidently in other tests and implementations. The behavior of all production classes in the sample object graph would be exercised in those activities. Meanwhile, if the relevant File and Folder behaviors have already been tested, they would simply play their part while new Filesystem behaviors are developed.",
      "content_length": 982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Summary: Use with Care\n\nreplacing challenging external for Test doubles are very useful dependencies. They can be used in suites of unit tests or Behavior-Driven Development’s scenarios, the suites sufficiently fast and repeatable.\n\nto make\n\nSometimes, though, they can be almost too powerful. Here are a few caveats.\n\nBe Sure to Deploy with Real Dependencies\n\nMake sure your deployed code receives a real dependency when it is created in production.\n\nFirst, be sure that no mock framework or stubbed class is referenced in your production-ready build. Keep these items in a separate test project that cannot get deployed beyond the build, integration, and test servers.\n\nSecond, avoid using null in all constructors, assignments, and parameter lists. Think of the null keyword as its own code smell—a very biting stench. Except for the need to check external values (for example, from an external caller or external API), endeavor to eliminate any need for the null keyword in your system.\n\nSupplement with Narrow Integration Tests\n\n“Narrow integration tests” (a term coined by James Shore) are not quite unit tests by this book’s definition, but they’re not whole end-to-end scenarios, either. They are tests of an external dependency itself, or the objects that directly interact with that dependency.\n\nIf there is some way to force the external dependency to give a predictable response, you can write tests for these narrow, isolated, external behaviors. As long as these tests remain few in number and tightly focused, you can keep your total build time below the “coffee break” standard, and you will feel more confident that all individual behaviors of your system have been fully and automatically tested.",
      "content_length": 1714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "Continuing with the JDBC example, a few of my teams were able to refactor JDBC access to isolate and reduce actual interactions to a handful of services: create, read, update, and delete (“C.R.U.D.” code). This allowed us to write about a half-dozen tests that truly interacted with JDBC.\n\nWe used a fake database (a test database freshly erased for every dev, test, or integration test run). Eventually, in-memory database instances became an option. Today, a local test database running on a solid-state drive can provide the same convenience.\n\nHaving a small number of narrow integration tests is a valuable addition to your safety net:\n\nUnlike a full suite of end-to-end tests, these tests are limited to integration points. They may be slower than unit tests, but each code path is tested only once, so the full integration test suite might add just a few seconds to the feedback loop.\n\nThey confirm that dependencies continue to behave as expected. Their narrowness helps pinpoint any undesirable changes whenever the team upgrades to a newer version of an API or programming language.\n\nThey test the minimal behaviors of architectural wrappers. By testing wrappers without test doubles, narrow integration tests provide confidence that a production deployment will not result in null- pointer/null-reference exceptions.\n\nUse Mock Frameworks Sparingly\n\nMock frameworks make it easy to test complicated interactions. They’re particularly useful when managing previously untested code (see Chapter 7). But that ease-of-testing can render the team less motivated to refactor a smelly design. Poor designs left lying around, despite being tested, can still cause confusion and slow further progress.\n\nAs in a sushi kitchen, the typical definition of “clean enough” is not clean enough. If you use a mock to expedite development, be sure to sniff out new code smells and be ready to refactor.",
      "content_length": 1893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Now that you have test doubles in your toolbox, you are fully prepared for Chapter 7, “Testing Legacy Code”!\n\nOceanofPDF.com",
      "content_length": 124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Chapter 7. Testing Legacy Code\n\nAfter a Test-Driven Development course at Microsoft (circa 2005), a developer came up to me and said, “Great class! Unfortunately, I probably won’t get to use TDD until I move to another team. You see, our team has about 400,000 lines of untested C# code to maintain.” That was the day I realized I needed to include techniques to wrangle legacy code.\n\nAt the time, there were concerns about the applicability of TDD for anything but greenfield projects. There were misguided suggestions to simply avoid or rewrite legacy code: “If it ain’t broke, don’t fix it!”\n\nThe reality of software development, however, is that adding new behaviors to an existing system often requires changes to older code.\n\nDefinitions\n\nLegacy code: In his book Working Effectively with Legacy Code (Pearson, 2004), Michael Feathers defines legacy code very simply: “Code without tests” (p. xvi). Feathers wasn’t trying to place blame; he was drawing a clearly defined boundary between two types of code: tested and untested.\n\nIn Art of Agile Development (O’Reilly Media, 2021), James Shore defines legacy code as “code you’re afraid to change” (p. 300).\n\nI consider Feathers’s and Shore’s definitions to be equivalent. Greenfield: Adjective denoting that a project or product is\n\nbeing built with all new code.\n\nLegacy code presents a dilemma: The team wants to enhance the system and deliver on received requests. To do so, they need to refactor, and that requires a good testing safety net. But untested code usually resists testing. To make it more testable, we often need to refactor it first.",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "A team with a poor safety net of tests, or a very slow test feedback loop, will be reluctant to change legacy code for fear of breaking something. Most teams still try to get around this risk by copying legacy code that is close to what they want, pasting it into another file, and modifying the pasted code to perform the new functionality. This copy/paste/modify technique does indeed preserve the behaviors of legacy code—usually “someone else’s code”—but results in more legacy code and increasingly unmaintainable designs.\n\nDesign Detour\n\nA cautionary tale about copy/paste/modify: I once met with a team that was asked to make a single change to a dynamic item on their website, and they unhappily realized they had to make the change to every one of their 100-plus ASP.Net pages. Then months later, the defect reports started to roll in: They had missed a few pages.\n\nThere is always a way to eliminate duplication, and it’s almost\n\nalways a good idea to do so.\n\nThe ASP.Net team had missed the opportunity to utilize an ASP.Net “User Control”—a separately tested custom widget that can appear on multiple pages yet provides a single point of maintenance. More generally, objects, functions, abstract classes (Java, C#), modules (Ruby), private methods, and many other programming language features are provided so that (1) the design can be understood by humans and (2) each change can occur with the least amount of risk.\n\nA fast, comprehensive safety net of tests prevents a team from breaking existing behavior while they are adding new behavior and refactoring the design. The safety net provides confidence, which in turn shortens average delivery time.\n\nLegacy code—code without fast tests—has huge holes in the safety net. Fortunately, there is a way to “patch” the holes: Characterization testing is an essential ancillary practice for your TDD toolbox. With a little imagination, it can be as enjoyable as using TDD to develop new features (Figure 7.1).",
      "content_length": 1970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Figure 7.1 Flowchart showing how TDD and characterization testing complement each other.\n\nOnce you’ve patched and strengthened your safety net with characterization tests—covering all behaviors in the areas of code that require change—you can confidently return to refactoring design and adding new functionality using TDD.",
      "content_length": 323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "Characterization Testing\n\nA characterization test is a test that characterizes—that is, captures and preserves—existing behaviors.\n\nimagine yourself as a “software When faced with paleontologist.” You are going to carefully sweep away the dust and dirt that has accreted over immeasurable time, then identify and preserve the fragile fossils of your system. Unlike in real paleontology, though, once you’ve identified and protected enough of the fossils, you can then turn a slow, plodding brontosaurus into a fast, intelligent velociraptor through refactoring and TDD.\n\nlegacy code,\n\nAs with TDD, the goal is to be able to confidently develop new and valuable features without damaging existing behavior. Unlike with TDD, however, you don’t decide which behavior to create by writing a failing test and then writing the correct code to make it pass. Instead, you write a test to explore and preserve what the existing code is already doing.\n\nThe characterization test recipe in brief is as follows:\n\n1. Ask the “Three Questions” (described in the next section).\n\n2. Start writing a test: Call a constructor and a method, or a static method. Pass in some test data and add at least one assertion. If this is impossible, use a “surgical refactoring” (also described later) to make testing possible.\n\n3. Run the test and learn from its failure: Did the tested object throw an exception? Did it delegate to an external dependency? Did it return an unexpected value?\n\n4. Update the test with what you learned in step 3. Add a mock object, update an assertion, and change the test’s expectations to meet what the object is doing with your test data.\n\n5. Repeat steps 3 and 4 until the test passes, and refactor the test until it is fast, independent, and repeatable.",
      "content_length": 1761,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "6. Return to step 2 for a different behavior. Continue until you have covered all code paths for the area you want covered.\n\nYou don’t need to cover all your legacy code all at once—just the code that is about to change. Apply characterization tests when and where you need them, rather than wasting precious time attempting to cover everything before you make a valuable change. Behavioral coverage will grow as needed, rather than looming over your team’s heads as one huge monolithic chore.\n\nThere are common legacy testing hurdles you can watch for and common ways to get over those hurdles. Still, you shouldn’t let the flowchart shown in Figure 7.1 and this brief recipe lure you into a sense of complacency. Characterization testing will require all your experience, intuition, and cleverness—so put on your paleontologist’s fedora and get digging.\n\nThe Three Questions\n\nFirst, examine your legacy code and roughly prioritize which areas you want to cover. You can use these questions as a guide, but they don’t have to be addressed as a sequential procedure. Instead, keep all of them in mind as you examine the code, and record answers to each as you notice something.\n\nQuestion 1. What does this class or method do? Write a brief description (not code!) of each behavior in a fresh to-do list. For now, write down one item for each separate path through the code. Characterization tests usually cover multiple behaviors, so do not become discouraged by the size of the list.\n\na. Look for loops. Will you need separate tests for zero, one, and\n\nmany times through the loop?\n\nb. Look for other forms of branching: if, if-else, and switch\n\nstatements.\n\nc. Be thorough. If the legacy code chooses an option, calculates a value, logs a message, throws an error, or calls an external dependency, write that down. You must assume that it does",
      "content_length": 1845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "everything for a reason. Plan to lock down that behavior, no matter how trivial it might seem.\n\nQuestion 2. What is preventing you from easily writing a test for those behaviors? Note anything that will create a challenge. For example:\n\na. An important method call that provides no meaningful return value.\n\nb. Numerous external dependencies, all accessed from the same\n\nmonolithic block of code.\n\nc. A dependency that cannot be easily substituted with a test double (for example, a sealed .Net class, final Java class, or nonvirtual C++ method).\n\nQuestion 3. What can be done about the challenges listed under Question 2? As you encounter testing challenges, write down any ideas you have that will make testing easier. Much of this chapter is dedicated to some of the most common techniques.\n\nRecommended Reading\n\nThere is an entire, massive book that offers numerous characterization testing techniques: Working Effectively with Legacy Code by Michael Feathers. Sections in his book are titled with a question or concern that you might have, and while covering one solution, the author sometimes guides you to another section to cover a related technique. The book is so well organized by topic that I’m not sure I’ve ever read the whole thing, because I usually end up skipping around.\n\nThe rest of this chapter covers the techniques that I’ve found\n\nto be most useful in a wide variety of circumstances. Those techniques may suffice for your purposes. Nevertheless, Feathers’s book is an excellent companion to this one.\n\nStart Writing the First Test\n\nJust jump in! Start writing a test that calls the object’s constructor or calls one static method. You can start by passing nonsensical parameter values:",
      "content_length": 1710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "nulls, minus one (–1), or perhaps “Inconceivable!” Follow the compiler’s and the test’s complaints to determine what else your test requires.\n\nIn the following example, we want to test openOrderReport(). However, it has a void return, so we may not know what to assert against. For now, we’re experimenting using the test framework:\n\n@Test void openOrderReport() {\n\nString fakeTestAccount = \"BOGUS\"; ReportGenerator generator = new ReportGenerator(fakeTestAcc generator.openOrderReport();\n\n}\n\nKey Lesson\n\nWhenever you’re using your test framework to experiment with a bit of legacy code or an external API, make sure your development/test system cannot reach a production version of any system beyond your local computer. You don’t want to buy 10,000 shares of a stock, or launch the $100 million rocket into space, or erase the production database. In Working Effectively with Legacy Code, Feathers suggests disconnecting the network while running the experiments. Sound advice, indeed!\n\nPerhaps unsurprisingly, most characterization tests do not pass right away:\n\njava.lang.RuntimeException: Account \"BOGUS\" not found\n\nat investors.FinancialDatabase.QueryAccount(Financ at legacy.ReportGenerator.<init>(ReportGenerator.j\n\nThe first test you write for a portion of legacy code tends to take the longest and requires the most mental effort, because most of the hurdles must be overcome in that first test. This is why teams often give up on characterization testing too soon: When the first test you write takes hours to complete, you might feel that it was wasted time. Not so!\n\nStep by step, you will encounter and overcome each hurdle preventing you from writing that first test. Afterward, the second test for the same block of",
      "content_length": 1731,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "code will likely take just a few minutes, because you’ve already found a way around those testing challenges.\n\nTool Tip\n\nWhen adding characterization tests, teams can use any preferred automated testing framework. I recommend using the unit-testing framework—at least at first—for these reasons:\n\nYou don’t have to switch back and forth between tools or languages. Everything is within the developers’ IDE and written in a single programming language.\n\nYou’re using the tool to explore what a certain part of the code is doing, so you are digging up and preserving behavior possibly the system— behavior that is not always clearly associated with user- visible features. located deep within\n\nA language like Cucumber’s Gherkin scenarios is great at describing desired behaviors to a broader audience (including product managers, business analysts, testers, auditors, and developers). In contrast, legacy behavior could be a mystery, an accident, or even a defect, or—at the very least—poorly documented from the start. Declaring legacy behavior as persistent and desirable by writing Gherkin scenarios could be misleading to others.\n\nDivide and Conquer\n\nOne of the most frequently used characterization testing tools in your toolbox will likely be the Extract Method refactoring.\n\nHere’s the sample untested code we’re working with:\n\npublic class ReportGenerator { private Account account; public ReportGenerator(String accountNumber) { account = FinancialDatabase.QueryAccount(accountNumber) } public void openOrderReport() { Order[] openOrders = FinancialDatabase.QueryOrders(",
      "content_length": 1578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "\"open\", account.accountNumber(), java.time.Instant.now()); if (openOrders.length == 0) { System.out.println(\"--- [You have no open orders]\") } else { System.out.format( \"| %8s | %8s | %8s | %8s | %8s | %8s |%n\", \"Order\", \"Type\", \"Status\", \"Symbol\", \"Shares\", \"Price\"); for (Order order : openOrders) { System.out.format( \"| %8s | %8s | %8s | %8s | %8d | %8d |%n\", order.id(), order.type(), order.status(), order.symbol(), order.shares(), order.appropriateDisplayPrice()); } } }\n\n}\n\nThe previous failure told us that this class is accessing FinancialDatabase (a test copy, we hope!) to fetch an Account.\n\nThe next step depends on what we intend to change once we have protected the existing behaviors with our improved safety net of tests. If we intend to allow this code to access a different database, we will need to wrap and substitute the database calls—a technique described later in this chapter. If we intend to change the format of this report (for example, adding a column or centering some of the displayed data), then we want to isolate and cover that behavior.\n\nGiven that the first attempt to test anything failed, we need to create what Feathers calls a “seam” in the code—that is, an entry point where we can test. That will require a refactoring.\n\nWe’ve already run into the most common dilemma: We want to refactor the code to make it testable, but we know it’s risky to change code that is not covered by tests. Without a sense of confidence and safety while refactoring legacy code, teams will often find ways to avoid changing that code,",
      "content_length": 1557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "typically by copying, pasting, and modifying it, and adding a branching statement that leads to their new code. Unfortunately, with this approach, the design continues to degrade, and maintenance costs continue to rise.\n\nInstead, when faced with this dilemma, you will need to apply surgical refactorings, just enough to make part of the code testable.\n\nDefinition\n\nSurgical refactoring: Any refactoring of legacy code that is relatively safe (behavior-preserving) despite a lack of tests around the code that is being changed.\n\nIf your IDE can perform a refactoring for you, you may consider that safe. If your programming language is strongly typed, some manual refactorings will cause a compiler error if you’ve made a mistake or forgotten to complete the refactoring (for example, Rename Method).\n\nHere are the results of a straightforward Extract Method that separates the behavior from the initialization:\n\npublic void openOrderReport() {\n\nOrder[] openOrders = FinancialDatabase.QueryOrders( \"open\", account.accountNumber(), java.time.Instant. formatOpenOrderReport(openOrders);\n\n}\n\nprivate void formatOpenOrderReport(Order[] openOrders) {\n\nif (openOrders.length == 0) { System.out.println(\"--- [You have no open orders]\"); } else { System.out.format( \"| %8s | %8s | %8s | %8s | %8s | %8s |%n\", \"Order\", \"Type\", \"Status\", \"Symbol\", \"Shares\", \"Price\"); for (Order order : openOrders) { System.out.format( \"| %8s | %8s | %8s | %8s | %8d | %8d |%n\", order.id(), order.type(), order.status(), order.symbol(), order.shares(),",
      "content_length": 1526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "order.appropriateDisplayPrice()); } }\n\n}\n\nBy default, Extract Method creates a private method. There are a few ways to make it accessible to tests.\n\nYou could make the new method’s access public:\n\npublic void formatOpenOrderReport(Order[] openOrders)\n\nIn Java, if the test case and the tested class are designated as being in the same package (even if they are in different projects or JAR files), tests can access package-private methods (no modifier): void formatOpenOrderReport(Order[] openOrders)\n\nIn C# (and other .Net languages), you can use internal, which makes the method accessible to everything within the same DLL file, but not accessible (by default) to calls beyond the DLL. Then you can add the test suite’s assembly as InternalsVisibleTo() in the assembly information: [assembly: InternalsVisibleTo(\"LegacyReportGeneratorTests\")] … internal void FormatOpenOrderReport(Order[] openOrders)\n\nIt’s also possible to use reflection in Java and C# to test the private method, although this is a very smelly practice. The test gets obfuscated with reflection syntax, and the name of the method shows up in quotes and might get missed during a future refactoring. Most significantly, the test is now testing what has been designated as an encapsulated implementation detail, not a service of the object. Design Detour: Should I test a private method?\n\nA private method presents a potent testing challenge. Moreover, like most testing challenges, it tells you that there is a better design right around the corner.",
      "content_length": 1520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Most environments have power tools that will let a developer\n\nset or check private values or call a private method within the tests. That might help you test legacy code in the short term, but such tests can complicate future refactorings. Most legacy code needs to be refactored anyway, so that it will be more expressive and flexible. Rather than take the quick-and-dirty path, consider which surgical refactorings would improve testability.\n\nDevelopers often have reasonable misgivings about arbitrarily\n\nmaking something public—that is, openly accessible to other parts of the system and external callers. But the desire to test a private method means that some important behavior is disguised as an implementation detail (the original programmatic meaning of private). This behavior likely belongs within a different class, possibly all by itself.\n\nOnce the behavior is moved to another class (for example, via\n\nan Extract Class1 or Move Method2 refactoring), it can then be called through a public method (optionally internal in C#) on the new class, and it can be fully tested in isolation.\n\n1 https://refactoring.guru/extract-class\n\n2 https://refactoring.guru/move-method\n\nBuild Tests Incrementally\n\nIn the first of the Three Questions, you identified what the legacy code does. Now, you need to find ways to assert that all the effects of that behavior continue to occur correctly whenever the test runs.\n\nStart with the simplest cases. You might cover less consequential code, but you might also learn the most about how to test this code. In our example, there is a “zero” case:\n\n@Test void openOrderReport_WhenNoOpenOrders() {\n\nAccount unusedTestAccount = null; ReportGenerator generator = new ReportGenerator(unusedTestA generator.formatOpenOrderReport(new Order[] {});",
      "content_length": 1782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "fail(\"We're not done yet!\");\n\n}\n\nThe call to org.assertj.core.api.Assertions.fail() reminds us that we have yet to set up a way to capture the output. Without this, the test will “pass” and a developer might be led into a false sense of confidence. Without a better assertion, there is nothing tested.\n\nNote that we’re passing in null, not as a String account number, but as an actual Account object. We want to be able to create a ReportGenerator while skipping the FinancialDatabase query. To get that change to compile, another straightforward surgical refactoring is needed that “chains” the constructors (that is, one constructor calls another). This is very much like Extract Method but focuses on a constructor.\n\npublic ReportGenerator(String accountNumber) { this( FinancialDatabase.QueryAccount(accountNumber) ); }\n\npublic ReportGenerator(Account account) { this.account = account; }\n\nWhen the test runs, the output \"--- [You have no open orders]\" goes to the console. We need a way to capture that output for the test, without adding new behaviors to the implementation. System.out is a PrintStream, which is fortunately not a final class. (If it were final, we would need to build a wrapper around it, as described later in the “Introduce Virtualizing Proxy” section).\n\nIn JUnit:\n\nprivate PrintStream originalConsole; private ByteArrayOutputStream actualOutputBuffer;\n\n@BeforeEach void redirectOutput() {\n\noriginalConsole = System.out; actualOutputBuffer = new ByteArrayOutputStream(); System.setOut(new PrintStream(actualOutputBuffer));\n\n} @AfterEach void restoreOutput() {",
      "content_length": 1585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "System.setOut(originalConsole);\n\n}\n\n@Test void openOrderReport_WhenNoOpenOrders() {\n\nAccount unusedTestAccount = null; ReportGenerator generator = new ReportGenerator(unusedTestA generator.formatOpenOrderReport(new Order[] {}); assertThat(actualOutputBuffer.toString()).isEqualTo( \"--- [You have no open orders]\\n\");\n\n}\n\nIn MSTest for C#, temporarily redirecting console output to a string looks very similar:\n\nprivate TextWriter originalConsole; private StringWriter actualOutputBuffer;\n\n[TestInitialize] public void RedirectOutput() {\n\noriginalConsole = Console.Out; actualOutputBuffer = new StringWriter(); Console.SetOut(actualOutputBuffer);\n\n}\n\n[TestCleanup] public void RestoreOutput() {\n\nConsole.SetOut(originalConsole);\n\n}\n\nUse BeforeEach and AfterEach for this sort of runtime bracketing in case the tested legacy code surprises you with an unexpected exception. An exception will cause the test to fail, but AfterEach will still put your environment back into a stable state. Note that this technique does introduce the Tear Down test smell described in Chapter 5, but it need linger only until you refactor the implementation to a better design.\n\nContinue to build tests until you have full behavioral coverage. This example needs just one more test to cover all behaviors. Rather than attempting to manually assemble the complex expected report output, let a test failure tell you exactly what is expected:",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "@Test void openOrderReport_WithRepresentativeSample() {\n\nAccount unusedTestAccount = null; ReportGenerator generator = new ReportGenerator(unusedTestA\n\nOrder[] variousOpenOrders = new Order[] { new Order(\"B3AAE\", \"MARKET\", \"OPEN\", \"LXEGF\", 1000, new Order(\"F5A62\", \"STOP\", \"OPEN\", \"LXODP\", 200, 22 new Order(\"6BE11\", \"LIMIT\", \"OPEN\", \"LXSPL\", 4, 186 }; generator.formatOpenOrderReport(variousOpenOrders); assertThat(actualOutputBuffer.toString()).isEqualTo( \"Inconceivable!\");\n\n}\n\nIt fails, of course. To do otherwise would be absolutely, totally, and in all other ways, “Inconceivable!”3 (Figure 7.2).\n\n3 https://en.wikipedia.org/wiki/The_Princess_Bride_(film)\n\nFigure 7.2 A failing characterization test reveals the actual results of the behavior.",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Copy and paste the actual results into the test and run it again. You might need to make minor formatting and whitespace adjustments so that it will pass:\n\n@Test void openOrderReport_WithRepresentativeSample() {\n\nAccount unusedTestAccount = null; ReportGenerator generator = new ReportGenerator(unusedTestA\n\nOrder[] variousOpenOrders = new Order[] { new Order(\"B3AAE\", \"MARKET\", \"OPEN\", \"LXEGF\", 1000, new Order(\"F5A62\", \"STOP\", \"OPEN\", \"LXODP\", 200, 22 new Order(\"6BE11\", \"LIMIT\", \"OPEN\", \"LXSPL\", 4, 186 }; generator.formatOpenOrderReport(variousOpenOrders); assertThat(actualOutputBuffer.toString()).isEqualTo(\n\n\"| Order | Type | Status | Symbol | Shares | Pr \"| B3AAE | MARKET | OPEN | LXEGF | 1000 | \"| F5A62 | STOP | OPEN | LXODP | 200 | 2 \"| 6BE11 | LIMIT | OPEN | LXSPL | 4 | 186\n\n);\n\n}\n\nOnce a test passes the first time, run it again. If a characterization test gives back different results each time, that indicates some dependency isn’t yet under the test’s control via a test double.\n\nof consistently, Both formatOpenOrderReport() are fully tested. It can now be refactored or altered using TDD. For example, if we wanted to center the “Type” column data, we could change the expected output in the test and then play the TDD game from that point forward.\n\ntests\n\nnow\n\npass\n\nand\n\nthe\n\nbehaviors\n\nOther Considerations\n\nWith characterization testing, you are patching your team’s safety net and shortening the length of the feedback loop between making a mistake and identifying that mistake, particularly if you were previously relying on manual test cases. Whenever you are writing characterization tests, keep the following considerations in mind.",
      "content_length": 1661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Test All Paths and All Results\n\nYour own legacy code will probably be much more complex than our simple example. Remember to iterate over all the behaviors you’ve identified with the first of the Three Questions, creating characterization tests that capture all the results of each behavior.\n\nMake a note of everything that happens within a single scenario. Add assertions for all results, including logging, state changes, events raised, console output, delegations, and so on.\n\nMaximize Coverage While Minimizing Effort\n\nIf you discover a way to increase behavioral coverage and shorten the test cycle with less of the team’s efforts, try it out.\n\nIt’s common for a characterization test to cover multiple behaviors. Suppose your code performs a complex calculation, chooses a code path based on those results (for example, a switch statement), writes the results to the console, and stores the results in a spreadsheet. That entire series of behaviors can be tested with as few tests as there are optional code paths (for example, the number of cases in a switch).\n\nYou can write each test to weave a different path through the code and cover different behavioral boundaries. Some behavioral boundaries might get tested more than once, and that’s fine.\n\nDepending on the complexity of the legacy code, you might not know which behavioral boundaries you are testing until the test passes. It’s not always easy to tell which branch of code will be taken based on the given parameters and the state of the tested object. Capture the actual results and use them as expected results. For your first test of a region of code, any path through the code is a good one.\n\nRefactor Characterization Tests, Too\n\nOnce one or two characterization tests are passing, look for test smells and refactor to eliminate them.",
      "content_length": 1807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Characterization tests are often longer than unit tests because they test multiple behaviors. Even so, you should not leave behind test smells that are easy to repair. For example:\n\nMake any arbitrary inputs or mocked interactions more explanatory.\n\nAdjust the inputs so they are right up against the behavioral boundaries. For example, if you tested code that includes if (x > 12) with x set to 4 and 42, instead change these values to 12 and 13.\n\nRename the test based on the scenario you’ve captured.\n\nRecognize That This Is Not Test-Driven Development\n\nCharacterization testing sometimes feels like TDD. After all, you write a test that fails, and then you make it pass. However, when you’re writing a the the characterization implementation’s behaviors, not the other way around.\n\ntest, you continue\n\nto fix\n\ntest\n\nto match\n\nWear One Hat at a Time\n\nConceptually, you have a “test-driven hat,” a “refactoring hat,” and now a “characterization testing hat.” You can fit only one hat atop your head at any moment.\n\nRecall that with TDD, you wait until the tests are passing before you tackle refactoring. Similarly, whenever you’re engaged in characterization testing, you should avoid trying to do two things at once: Do not change or add any behavior to your system while writing characterization tests.\n\nThat includes adding logging or output buffering to the implementation to facilitate the testing. In the ReportGenerator legacy example, we didn’t add behaviors to the legacy code. Instead, the test redirected standard output to a buffer that only the test could access directly.\n\nWhen You Find a Defect\n\nOften, while writing characterization tests, a team will spot a mistake in the code. It might be a known defect that has eluded them, something intermittent, something that users have been working around, or all three.",
      "content_length": 1832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "The team is often shocked. “How did this ever work?!” is a question I’ve heard many times.\n\nIf you find a defect, resist the temptation to fix it outright. Instead, capture it in a repeatable characterization test to assure the defective behavior remains intact. Give the test a name or description that clearly states the behavior’s defective nature. If you log the defect in your queue (for example, a backlog or tracking system), add the defect’s identifier into the description, the name, or a comment in the test.\n\nWhy not fix the defect then and there? Recall that characterization testing is meant to capture existing behavior and lock it down while you refactor. Fixing a defect alters behavior and may have serious ramifications in another part of the system. There could be an existing workaround for that defect. If you fix it, you could break the workaround in your system or in your clients’ systems.4\n\n4 “Impossible!” said one student when I used a “one-off error” to explain this. (A one-off error occurs when someone familiar with Pascal or Fortran, for example, momentarily forgets that modern programmatic arrays start at index zero.) One of his teammates immediately interrupted, “Are you kidding? That’s literally the problem we saw last week!”\n\nSo, even if a defect appears to require just a simple fix, it may be more complex, require detailed documentation, or simply not be a priority at the moment. Given these possibilities, it’s best to make the whole team— particularly your product advocate—aware of the defect. This gives the team the opportunity to prioritize the repair in relation to all other defects, requests, and enhancements.\n\nIntroduce Virtualizing Proxy\n\nMost legacy code is tightly coupled to its external dependencies. This shows up as multiple calls, strewn haphazardly throughout the code (for example, the same sequence of JDBC calls within every data access object).\n\nIn most cases, your mock framework will help you get that code covered. Most mock frameworks create a temporary subclass of the dependency as a “dynamic proxy” that intercepts each call and performs whatever actions it",
      "content_length": 2132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "was trained to do. That means that they can stub out only calls that are already virtual (that is, overridable).\n\nThen there are the “worst case” external dependencies: Java final classes, .Net sealed classes, pure-virtual C++ classes, and classes with static methods. Calls to these classes cannot be easily stubbed, so replacing them with compatible test doubles is a challenge.\n\nIntroduce Virtualizing Proxy is a surgical refactoring with the sole purpose of making non-virtual external methods appear virtual again (from the caller’s perspective). That allows you to create a test double of the proxy.\n\nTo illustrate this technique, we’ll continue with our simple Java example. Here’s the code we’ll be working with (the changes from the previous example are emphasized):\n\npublic void openOrderReport() {\n\nOrder[] openOrders = FinancialDatabase.QueryOrders( \"open\", account.accountNumber(), java.time.Instant. formatOpenOrderReport(openOrders, System.out);\n\n}\n\npublic void formatOpenOrderReport(\n\nOrder[] openOrders, PrintStream output) { if (openOrders.length == 0) { output.println(\"--- [You have no open orders]\"); } else { output.format(\"| %8s | %8s | %8s | %8s | %8s | %8s |%n\" \"Order\", \"Type\", \"Status\", \"Symbol\", \"Shares\", \"Pri for (Order order : openOrders) { output.format(\"| %8s | %8s | %8s | %8s | %8d | %8d order.id(), order.type(), order.status(), order.symbol(), order.shares(), order.appropriateDisplayPrice()); } }\n\n}\n\nNote that an Introduce Virtualizing Proxy refactoring isn’t necessary to replace a PrintStream (it’s not a final class). Even so, it can still help us",
      "content_length": 1589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "decouple the business behaviors from the external dependencies.\n\nFigure 7.3 shows the structure of the relationship between ReportGenerator and its PrintStream before the Introduce Virtualizing Proxy refactoring.\n\nFigure 7.3 UML showing the original relationship between the legacy code and its external dependency.\n\nFigure 7.4 shows the results of the refactoring, plus test code that is easier to write once the refactoring is complete. Note that the new OutputProxy class is not a test double, but rather is part of the deployable implementation.",
      "content_length": 549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Figure 7.4 UML showing the addition of the proxy and the test code.\n\nThe sections that follow describe the steps to this refactoring.\n\nStep 1: Create a Virtualizing Proxy\n\nCreate a new class within your implementation project (that is, not in your test project). For each method that the legacy code calls, add a method with the same method signature and return the external dependency’s interface exactly and make the methods virtual. Do not replicate any methods your system is not yet calling; you can always add them later if needed.\n\ntype. Match\n\nIn Java:\n\nimport java.io.PrintStream;\n\npublic class OutputProxy {\n\nprivate final PrintStream originalPrintStream;\n\npublic OutputProxy(PrintStream original) { this.originalPrintStream = original;",
      "content_length": 746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "}\n\npublic void println(String s) { originalPrintStream.println(s); }\n\npublic void format(String format, Object... arguments) { originalPrintStream.format(format, arguments); }\n\npublic boolean checkError() { return originalPrintStream.checkError(); }\n\n}\n\nHere’s a similar proxy for .Net’s Console.Out TextWriter. Note the use of the virtual keyword in each method.\n\nIn C#:\n\npublic class OutputProxy {\n\nprivate readonly TextWriter originalTextWriter;\n\npublic OutputProxy(TextWriter original) { originalTextWriter = original; }\n\npublic virtual void WriteLine(string s) { originalTextWriter.WriteLine(s); }\n\npublic virtual void WriteLine( string format, params object?[] arguments) { originalTextWriter.WriteLine(format, arguments); }\n\n}\n\nStep 2: Replace the Original with the Proxy\n\nIn your legacy implementation, replace the original with a proxy that wraps the original. This must happen before the call to the method containing the",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "behaviors to be tested; otherwise, you will not be able to inject a test double. If the external dependency is passed into the extracted method, then this change is very straightforward. All changes (there should be only two!) are emphasized here:\n\npublic void openOrderReport() {\n\nOrder[] openOrders = FinancialDatabase.QueryOrders( \"open\", account.accountNumber(), java.time.Instant. formatOpenOrderReport(openOrders, new OutputProxy(System.ou\n\n}\n\npublic void formatOpenOrderReport(\n\nOrder[] openOrders, OutputProxy output) { if (openOrders.length == 0) { output.println(\"--- [You have no open orders]\"); } else { output.format(\"| %8s | %8s | %8s | %8s | %8s | %8s |%n\" \"Order\", \"Type\", \"Status\", \"Symbol\", \"Shares\", \"Pri for (Order order : openOrders) { output.format(\"| %8s | %8s | %8s | %8s | %8d | %8d order.id(), order.type(), order.status(), order.symbol(), order.shares(), order.appropriateDisplayPrice()); } }\n\n}\n\nThe proxy’s interface looks like the original, but they are not of the same type. By changing the parameter type, we’ve effectively changed the entire method to call the new proxy without having to change anything in the method body.\n\nWill you need this surgical refactoring? Consider these points:\n\nThis refactoring is particularly valuable if your legacy code calls non-virtual methods on an external dependency. Otherwise, you can use your mock framework to test all relevant behaviors, and then safely perform any refactorings you need to clean up the design.",
      "content_length": 1487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "This refactoring is helpful when you are testing long blocks of legacy code that repeatedly access the same dependency. If you follow all steps carefully, you can minimize the amount of code that needs to change before you can add tests.\n\nIt helps test legacy code by consolidating challenging calls made to the external dependency’s type. This refactoring introduces a new class—the proxy class—into your implementation. This class resists unit testing for all the same reasons that the external dependency made the original legacy code difficult to test. Avoid adding any other behaviors to your thin new proxy class besides delegating to the external dependency and returning whatever it returns. You can rely on the compiler to check that all parameter types and return types match those of the external dependency.\n\nYou might not have another choice. Many teams, for various reasons, cannot safely upgrade their environment. If your team does not have access to the latest mock framework and/or latest version of the programming language, you might not have another way to stub non-virtual methods. Besides, wrapping an external dependency to decouple it from your business domain is a positive step toward a cleaner and more testable architecture.\n\nUsing a Test Double of the Proxy in a Test\n\nWith the refactoring complete, you can write characterization tests more easily. Note that whether you hand-craft a stub or use your mock framework, you must not inject any “original” external dependencies into these tests. Instead, pass null into the test double’s constructor. If you forget to override one of the proxy’s methods with your test double, your test will remind you by failing due to a null-pointer/null-reference exception or an error from your mock framework.\n\nHere’s an example OutputProxy spy (Java) and test:\n\npublic class OutputProxySpy extends OutputProxy {\n\nprivate StringBuilder outputBuffer = new StringBuilder(); public OutputProxySpy() { super(null); }",
      "content_length": 1978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "@Override public void println(String s) { outputBuffer.append(s); }\n\n@Override public void format(String format, Object... arguments) { println(String.format(format, arguments)); }\n\n// for spying purposes only public String readAllOutput() { return outputBuffer.toString(); }\n\n}\n\n@Test void openOrderReport_WithRepresentativeSample() {\n\nAccount unusedTestAccount = null; ReportGenerator generator = new ReportGenerator(unusedTestA\n\nOrder[] variousOpenOrders = new Order[] { new Order(\"B3AAE\", \"MARKET\", \"OPEN\", \"LXEGF\", 1000, new Order(\"F5A62\", \"STOP\", \"OPEN\", \"LXODP\", 200, 22 new Order(\"6BE11\", \"LIMIT\", \"OPEN\", \"LXSPL\", 4, 186 }; OutputProxySpy spy = new OutputProxySpy(); generator.formatOpenOrderReport(variousOpenOrders, spy); assertThat(spy.readAllOutput()).isEqualTo( \"| Order | Type | Status | Symbol | Shares |\n\n+ \"| B3AAE | MARKET | OPEN | LXEGF | 1000 | + \"| F5A62 | STOP | OPEN | LXODP | 200 | 2 + \"| 6BE11 | LIMIT | OPEN | LXSPL | 4 | 186\n\n);\n\n}",
      "content_length": 959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "Summary: The Complete Toolbox\n\nThe techniques described in this chapter give developers the ability to “patch” their safety net wherever coverage is insufficient. Once behavioral coverage has been improved for a region of code requiring change, refactoring and TDD can proceed.\n\nWith TDD, refactoring, test doubles, and characterization testing all in your toolbox, you can now test, build, reshape, and enhance your high-quality software with complete confidence.\n\nChapter 8, “The Black Swans,” presents my first-person, longer-term experiences with these practices, which ultimately led to surprising value being delivered through our diligent, test-driven efforts. It’s a good chapter to show to your concerned managers or the skeptical architect.\n\nI hope that you, too, get to experience the satisfaction of delivering surprising levels of value and the pride in your team’s software design. Happy coding!\n\nOceanofPDF.com",
      "content_length": 925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Part III: Return on Investment\n\nOceanofPDF.com",
      "content_length": 46,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Chapter 8. The Black Swans\n\nFor a Test-Driven Development practice to be successful, two things are required. First, the team must be willing to give it their best effort for about a month. Second, they must have leadership support.\n\nBy “support,” I don’t mean just verbal permission. With any new practice, the team needs time to learn the best ways to follow the practice, time to experiment with the practice, and time to see some of the short-term benefits.\n\nWithout leadership support, TDD will not take root. Without some form of iteratively and team building test-driven discipline, a incrementally will rapidly lose the ability to refactor confidently, and over time will have to work harder to produce less valuable functionality.\n\nsoftware\n\nThat is what I call the Agilist’s Dilemma: A team attempting to use an incremental methodology to keep their product open to fluctuating business requirements, but without using developer practices designed to alter code confidently and continuously, will experience deteriorating quality and decreasing delivery of value. In this scenario, all of the benefits the organization expected to gain by building iteratively and incrementally are lost.\n\nOne way to garner better leadership support is to present case studies. Before I was an instructor of TDD, I was a practitioner, and this chapter documents my own experiences on test-driven teams. My hope is that these stories will help illuminate the business value of TDD and encourage teams and their leadership to take the time needed to assimilate a solid TDD discipline.",
      "content_length": 1575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "The Agilist’s Dilemma\n\nTo the best of my recollection, the brief conversation went something like this:\n\nDeveloper: “I’ll need to make one small change.”\n\nTester: “What will I need to test after that?”\n\nDeveloper: “It’s not a behavioral change, just a refactoring, so nothing,\n\nreally.”\n\nTester: “This is a change to how we access the database?”\n\nDeveloper: “Yep!”\n\nTester: “How many features could be impacted if something goes\n\nwrong?”\n\nDeveloper: “Well, it’s a fundamental part of the architecture, so all of\n\nthem?”\n\nTester: “So, I should probably test everything.”\n\nDeveloper: “Uh, yes.”\n\nFrom “No worries, test nothing” to “We’ll need to regression test everything” in less than 5 minutes! This dialog demonstrates the Agilist’s Dilemma.\n\nThe Vanilla Agile Team\n\nTeams building software iteratively and incrementally are doing just enough architecture, analysis, design, coding, and testing to build what is needed now. In other words, they focus on building what is likely the most important stuff.\n\nHowever, I’ve coached or trained dozens of teams that were not yet using any test-driven practice. Their description of what happened always follows a similar pattern.",
      "content_length": 1174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "At first, everything went fine. Developers built just enough, and testers tested those behaviors. No problem.\n\nAfter roughly a half-dozen iterations (that is, Scrum’s “sprints”), developers found themselves having to constantly modify existing code. On a newer product, the most accessed and most critical code often changes most frequently. That can be scary.\n\nNo professional developer wants to introduce defects! So, they slow down, becoming increasingly cautious about changing existing code. Or they copy/paste/modify similar existing code and introduce a branching statement to select either the existing code path or their new code path. This approach can seem much safer, but it degrades the quality of the design— that is, the ability to adapt to changes later.\n\nBecause the developer is becoming increasingly cautious, and the design is becoming less maintainable, requests for new features take even longer. Developer stress levels increase commensurately.\n\nTesters feel a similar increase in stress levels. Not only do they need to test the latest iteration’s new features, but also all behaviors implemented since the inception of the product. Eventually, the whole team is working nights and weekends to match their original pace. Or they cut corners— consciously or otherwise—which invariably introduces defects.\n\nWorking Without a Safety Net\n\nWhen a safety net of tests isn’t built alongside the software, the internal software design becomes brittle, requests take longer, and the product receives far less regression testing than it deserves. Despite the team’s best efforts, defects start popping up everywhere. The team is experiencing the Agilist’s Dilemma.\n\nOf course, the solution to the Agilist’s Dilemma is not to go back to a phased approach that would leave testing to the end, when it’s far too late. Rather, the best approach is to use the iterative and incremental practices described in this book to build and maintain a safety net of tests. And whereas most “agile” teams consider a week or two to be short timeboxes,",
      "content_length": 2049,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "they must embrace skills that will let them develop working software with feedback cycles measured in seconds or minutes.1\n\n1 The exception being characterization testing. Occasionally, I’ve witnessed a team take hours to write the first of a few good characterization tests—but then the second one takes only a few minutes.\n\nThe Three Levels of Value\n\nThree interrelated and overlapping tiers of value can be realized when developers embrace a disciplined approach to TDD:\n\nLevel 1: A dramatic reduction in defects and rework\n\nLevel 2: Reduced feature time-to-market\n\nLevel 3: Implementing the unexpected\n\nThe third level—the long-term power of that safety net of regression tests —usually arrives as a pleasant surprise.\n\nLevel 1: A Dramatic Reduction in Defects and Rework\n\nThis is the most familiar of the three levels, and the most immediate. Because the team thinks in terms of concrete examples and records their expectations as automated tests, they prevent defects from getting into the software by instantly catching common mistakes.\n\nFor example, the Password Checker Lab in the Appendix has a rule that a password must be eight characters or more. If developers first try coding without tests, often they will write the following (in pseudocode):\n\nif password.length() > 8\n\nreturn true\n\nelse\n\nreturn false\n\nThat’s a defect, because 8 is not greater than 8. The true behavioral boundary exists between 7 and 8, not between 8 and 9.",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "TDD avoids this problem by giving the developers a way to record the sample inputs and expected results as they think of them, before concerning themselves with the code to make each example pass.\n\ncheck length \"12345678\" → true check length \"1234567\" → false\n\nMistakes can still happen with TDD. But that’s the point: Most mistakes are caught within minutes, usually before code is committed to the repository. If the developer is taking small, rapid steps, then the developer knows what caused the error and who created it: They did!\n\nLocating and fixing the defect immediately is far less wasteful than waiting months before a tester or customer discovers it, logs it, tries to figure out who is to “blame,” and assigns the defect to a developer.\n\nI have met with many teams before they’ve embraced TDD who estimate that they spend half their time debugging, finding, and fixing defects. Half! That’s a horrible waste of time and professional talent.\n\nThe team must also consider the potential damage a single defect can cause to a customer’s business and the development organization’s reputation. Defects are too costly to leave to random chance.\n\nLevel 2: Reduced Feature Time-to-Market\n\nWhen teams are diligent about following TDD practices, the development of features will not slow down over time.\n\nAt Level 1, the team catches mistakes immediately through each unit test they add to the suite of tests. Once it is passing, the unit test assures the preservation of that single unit of behavior. Each time the full test suite runs (dozens or hundreds of times per day), it assures the preservation of all existing behaviors.\n\nWith that safety net of fast and comprehensive tests always in place, the team can confidently reshape the internal software design to make room for innovative new features, knowing they will not break anything. Because their confidence does not degrade over time, a request’s cycle time (the time it takes to complete the request from start to finish) won’t change",
      "content_length": 2000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "significantly, whether it’s the first request the team ever builds or it’s built two years later.\n\nThat ability to refactor the design is essential to software development and is the only way teams can avoid the Agilist’s Dilemma.\n\nLevel 3: Implementing the Unexpected\n\nEventually (perhaps in six months, perhaps in two years), all of the team’s efforts may suddenly pay off, with interest. There are no guarantees, of course, but in my experience, the “unexpected feature request” will arrive.\n\nThe unexpected feature is frequently an innovative pivot, a wildly insightful customer request, or a clever repurposing. If implemented successfully, the feature opens up a whole new market segment, improves the flow of value in another area of the organization, or greatly aids in retaining critically important customers.\n\nWhether you will welcome the unexpected or whether it will topple your enterprise will depend on the state of your software design when it arrives.\n\nWhat follows are stories from my own experience as a software developer and XP coach. For every team I’ve been on that stuck diligently to TDD for six months or more, the third level of value was unlocked.\n\nThe Black Swan User Stories\n\nThe phrase “Black Swan” is used in Nassim Nicholas Taleb’s book The Black Swan (Random House, 2010) to describe an event that is rare, unforeseen, unpredictable, and very disruptive (in either a positive or negative way). Though they are unplanned and unpredictable, according to Taleb Black Swans are—given an appropriate length of time—inevitable.\n\nFukushima Recent earthquake/tsunami/nuclear-meltdown triple-gut punch and the COVID-19 pandemic. However, Black Swan events aren’t always so unpleasant: The discovery of penicillin, the smallpox vaccine, and simple hand soap have all had a dramatic positive effect on our lives.\n\nwell-known\n\nexamples\n\ninclude\n\nthe",
      "content_length": 1871,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "So, a Black Swan user story (my term, combining Taleb’s term with Kent Beck’s “user story”) is a request for a change to a software product that is unplanned, unexpected, potentially “game-changing,” possibly very expensive to implement, and potentially extremely valuable. Depending on the state of the system that encounters a Black Swan event, it can be extremely costly, extremely valuable, or both.\n\nTaleb describes systems that improve when clobbered with such an event as “antifragile.” Contrast that term with “resilience,” which describes the condition in which the attacked system rebounds to a relatively steady state. An antifragile system goes one step further than a resilient one, and actually improves by learning and adapting to its new reality.\n\nWhat would it mean for software to be “antifragile”? If the Black Swan user story is properly implemented, the code will become even more flexible and potentially able to handle future requests more easily. It will have a similar effect on a stable team: The Black Swan will strengthen their confidence in their own abilities and will create trust among teammates, leaders, and customers.\n\nThe following tales are from my own first-person experience on long-term XP teams doing continuous, disciplined TDD. The shortest time I spent with these clients was six months. Our days, weeks, and months were filled with as close to full-time TDD as one can get, while still allowing us to take vacations and holidays.\n\nAfter months of disciplined TDD and diligent refactoring, these teams encountered a user story placed at the top of their queue that would have taken the average development team months to build (if at all). Our team completed it in far less time, usually within a single two-week iteration.\n\nThe following tales are all true, and the details are accurate to the best of my recollection. The dialog, however, has been improvised for brevity.\n\nA Major Architectural Change\n\nIn brief: Internationalize an entire suite of servlet applications. Done in three days, and about a dozen lines of code.",
      "content_length": 2069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "In 2001, James Shore and I were working on Novell’s Developer Portal. It was a series of applications deployed through a few Java servlets. When I got there, Jim and his team had been using XP practices for a while, and I was very impressed with the results.\n\nIn fact, as I told him then, it was the easiest code I had ever had the pleasure to work in, so far. Before joining Jim’s team, I had been subjected to EJBs, JSP pages, and numerous other failed attempts to streamline web application development. The brilliance of Jim Shore was that he never took a framework at face value. He would often come up with a simpler, home-brewed way to accomplish what those frameworks failed to deliver.\n\nAt the time, Jim and I were the whole Portland portion of the team; much of the team was located in the Salt Lake City area. We would fly to Salt Lake City every two weeks for planning. I clearly recall the day this Black Swan user story flew into the team room.\n\n“I know we agreed we’d only support languages represented by the Latin American character sets,” said the Product Advocate, “but we have an opportunity to make inroads into Asian markets, if only we could support Unicode character sets. So ?”\n\nAll static text in our app was already internationalized. The browser would tell the app the desired character set, and the app would serve up a page from the appropriate folder. Static text was translated by professional translators.\n\nThe challenge exposed by this user story was particularly noticeable in our “survey” app. This app allowed the user to build survey questions and multiple-choice answers. But Asian markets were limited to writing surveys in English (or French, or Spanish ).\n\nBeing new to the team, I felt my heart sink. “Oh, no!” I thought to myself, “What have I gotten myself into?! Internationalization! A major architectural change, that is!”\n\nThe team took a brief look at the code. Jim suggested an estimate of three days. I nearly fell out of my chair.",
      "content_length": 1983,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "“Are you kidding?!” I started, “To retrofit internationalization into all our servlets and all our pages and ???” I had flashbacks of poorly architected JSP-heavy dot-com apps, and needing to change every single text box and every line of dynamic text.\n\nJim volunteered to take that Black Swan back to Portland with us. The rest of the small team seemed happy to not have to deal with this user story. At first, I felt as they did: “Why didn’t we just turn this one down? This is going to be a disaster!” But Jim’s unexplained confidence was contagious, and by the time I was back in Portland, I was happy to tackle the challenge.\n\nWe did it in three days! In fact, we spent about 2.5 of those days doing research.\n\nIn 2001, there weren’t many web apps that could do what we were hoping to do. We experimented by registering with various well-known sites using some random international phrase (for example, “(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)”), and the app would respond with “Thank you for registering, ☐☐☐☐☐☐☐☐☐☐!”\n\nWe found only one site that responded correctly,2 which proved that it was possible. Then we set out to figure out exactly how to do it in Java. All of our web searches resulted in nothing. Even though Java was touted as “all Unicode, all the time,” apparently no one at that time had publicly documented the way to preserve the whole Unicode character from browser to database and back to browser again.\n\n2 Remember BabelFish.com? It’s still out there, though it looks and feels a bit dated.\n\nWe read carefully through the massive Java J2EE documentation regarding input and output streams and how to chain them together. Within minutes, we had our first passing unit test. We then thoroughly tested that code to make sure it worked with all types of character sets, old and new, 8-bit and 16-bit.\n\nThree days. Oh, we had some luck on our side, too: It turned out to be relatively easy to convert an Oracle database from 8-bit to 16-bit. You shut it down, alter a single configuration parameter, and restore all the data from your backups. So, three days of development time, plus a brief maintenance outage over the weekend, and we were up and running the next week.",
      "content_length": 2189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "How did we change all the code, pages, and servlets in half a day? It took only about a dozen lines of code, in one or two classes (Figure 8.1).\n\nFigure 8.1 My notes from 2001 on how to convert UTF8 to Unicode in Java.",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "Part of doing TDD well is diligent, thoughtful refactoring. Long before I showed up, the team had continuously refactored away duplication and other code smells as needed, rather than waiting until “we have the time.” The design was nearly ideal for what the system was asked to do up until that point.\n\nAt some point, refactorings had led the team to isolate all user input and all generated output through one or two classes and two methods. So, when it came time to support Unicode user input and output, that code was already isolated in those two methods. This isolation had not been an upfront architectural decision. Instead, the design had been altered over time as the team worked to make maintenance easier and unit testing more expressive. It was an elegant example of “emergent design”: when simple guidelines and efficient practices, repeated with discipline, result in better outcomes than any upfront design technique could create.\n\nRoughly a dozen lines of code in three days doesn’t sound all that productive, does it? Yet this is one of the clearest examples of how lines-of- code is an entirely inappropriate measure of value delivered. Our software was now much more valuable to customers in China, Japan, South Korea, and others.\n\nAfterwards, I clearly recall thinking, “That was neat! But I doubt I will ever see something like that happen again in my career.” Thankfully, I was wrong. Black Swans may be rare, unpredictable, and expensive, but they’re also inevitable.\n\nOTIS 2\n\nIn brief: Add a functioning “Display in PDF” button to every one of dozens of HTML reports after having committed 6 months to HTML-only development. Done in three days.\n\nMenlo Innovations assigned me to a small team at the University of Michigan that was working on a product called OTIS2. We were tasked with a rewrite of the University of Michigan Organ Transplant Center’s Organ Transplant Information System (OTIS). OTIS version 1 (OTIS1) was written in FoxPro and had reached many limitations of that technology.",
      "content_length": 2018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "FoxPro is not a self-documenting language, and OTIS1 was reaching its 10th year in production. Extracting all the business rules (better described as “critical patient-survival rules”) was going to be a challenge. Menlo and the U of M wanted to rewrite OTIS in Java partly to train the U of M developers in Java, object-oriented design, and Extreme Programming (XP). I was one of two XP developer/coach/architects sent from Menlo.\n\nTo thoroughly test OTIS2, we would run reports in parallel on both systems and compare the report output, letter for letter. In that way, as we developed OTIS2, good old trusted OTIS1 would itself provide detailed functional test scenarios for OTIS2.\n\nAbout six months into the program, after the first early release, the first Black Swan user story arrived, courtesy of the transplant surgeons themselves. They really liked what we were doing with OTIS2. As with OTIS1, however, the reports were all in HTML. Many of these reports would be printed and placed in patient records.\n\nThink back to the World Wide Web in 2001 (or if you’re not old enough to remember, just imagine it!). Whenever most printers printed a web page, that page would come out on two separate sheets of 8.5 × 11-inch paper: one page for the left part of the screen, and another for whatever remained on the right side of the screen. Apparently, the idea of fitting the full screen width into 8.5 inches, or simply printing it in landscape orientation, was not an interesting feature to the browser developers of the day.\n\nSo, when the report was printed, a nursing assistant would take the two sheets (or more, if it was a long report), staple or tape them together where the left and right sides of the report met. They would then fold the report back into a size that would fit into the patient record folder, and place it in the folder.\n\nThe surgeons made a simple request: “We really like OTIS2, but there’s just one small change we’d like: On each report, we’d like you to add a button that says ‘Print me in PDF format.’ We realize that this will impact the release plan, but it’s that important to us.” In other words, it was more important than all other user stories in the release plan.",
      "content_length": 2202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "A whole new output format in addition to HTML, for dozens of unique reports! For most web apps built in the very early 2000s, this would have meant “a major architectural change.”\n\nAs the XP coach (and de facto architect), I knew that we had already done the “unthinkable” and hard-coded HTML tags into our Java presentation layer. Early in the project, we had realized that our reports would differ by columns. Patient data would appear as rows from a database query, but the columns were specific to which organ, or organ combination, was being transplanted: Heart. Heart–lung. Both lungs. Heart-plus-both-lungs. Liver. Liver–kidney. Brain.3\n\n3 A bad joke that we told each other far too often. Just seeing if you were paying attention!\n\nThere were other dimensions as well. The reports would satisfy a particular general query. Who is likely destined to receive the next transplant? Who survived the transplant for more than 10 years? And, presumably, there was enough data in these large reports to help create a hypothesis as to which of the thousands of variables were important to a patient’s survival. Eventually, surgeons could customize the reports by adding special sets of extra columns.\n\nHTML naturally holds cells within rows. But the domain asked us to orient cells within columns. Our business model already had this notion of a Column that would reference the correct Cell of data from each record in the query results. When the app generated HTML output, Report code would walk through the collection of Columns and ask them for the Cell corresponding to the correct row number.\n\nTo summarize what was in place prior to the Black Swan: We took database rows, organized them into columns, and then formatted them into rows. Insane, right? If you’re a fan of code optimization, you may be squirming in your seat as you read this. It was inefficient, but it was all happening in memory. No one ever complained about response time. The system’s bottleneck was always printer speed.\n\nBack to the Black Swan: Our team asked to have an hour or so to do some research. We quickly learned two very important things: (1) Whereas HTML is row-centric, PDF is column-centric, like the business model. (2)",
      "content_length": 2209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "There was a Java library available that could generate PDF output programmatically.\n\n“Three days,” I said. The stakeholders were both shocked and delighted.\n\nWe spent the first day refactoring. We took all the HTML formatting code and extracted it away from the Report code into something called the HTMLEmitter. We ran all the tests and pushed our changes to the build server.\n\nOn day two, we used TDD to create a PDFEmitter. Also using TDD, we allowed the browser to switch emitters based on something as simple as “? format=PDF” on the URL. Each Report was then initialized with the correct Emitter (an abstract class). It worked almost perfectly at the end of that second day.\n\nOn the morning of day three, we tweaked PDFEmitter (using TDD, of course) to get the cell padding and fonts just right. In two and a half days, we had completed what the surgeons had declared as their most important new feature. It was integrated, exploratory-tested, user-acceptance-tested, audited, and deployed into production by the end of that week. It was in use by the surgeons the very next week.\n\nThe Loan Securitization Pipeline\n\nIn brief: Convert our custom data format to the format used by our primary bank partner.\n\nIn 2003, I worked with a small team that was responsible for maintaining the sole mission-critical software application for our company. This organization was dependent upon one lone revenue stream: We took commercial mortgages and securitized them. We took batches of loans, helped rate them, and then bundled them together into bonds to be resold to investors. We took a fraction of a percentage point on each transaction, and the size and volume of those transactions kept the company quite profitable, at least until the real estate bubble popped in 2008. Everything was managed for the brokers by an internal web application.\n\nThis company had started as a small private startup, and the software was originally built by a single developer, who—I later learned—taught himself",
      "content_length": 1992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Java while writing the system. It was a mess. He had learned all the clever syntax of Java, and apparently not one iota of real object-oriented programming.\n\nThe system had a few end-to-end tests covering multiple broad scenarios. Helpful, yes—but also cumbersome, fragile, nearly incomprehensible, and terribly slow.\n\nOur first big challenge was to add a new type of loan. The application had been designed top-to-bottom to securitize just one type of loan into one type of security—a commercial mortgage-backed security (CMBS). Despite this fact (or maybe because of it), there was no Loan class in the system. Instead, the data and behaviors for a loan were spread out over numerous classes.\n\nGradually, painfully, and mostly out of necessity, we refactored the Java abomination and its tests. After about six months, the code was starting to look better, and we had a lot more faith in our safety net.\n\nAnd then the Black Swan user story arrived: The bank that handled the actual loans for our company saw an opportunity to streamline the whole process, if only we could exchange loan information in its own XML format. It would mean less tedious data entry for the brokers and more time in the field evaluating actual real estate.\n\nWe were already using XML to represent loans for the application’s report- generation features. But our XML format (or “schema”) was very different than that of the bank.\n\nAt the time, I didn’t see what—perhaps even then—was an obvious solution. Our team estimated that it would take months to alter the Java code to create and use a different schema.\n\n“Well, we gotta do it, so get started!” the manager sighed. “Oh, and you’ll need to train this new hire—just a year out of college!—while you’re at it.”\n\n“Okay,” I thought, “it’ll be trial-by-fire for this poor bloke.”\n\nSo, I sat down to pair with the new guy on his first day. I explained that we needed to rewrite a lot of our XML-emitting Java code, and why, blah blah blah .",
      "content_length": 1968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "He stopped me and said, bluntly, and with mild incredulity, “Why not just use XSLT?”\n\nI answered, “Well, because I have no idea what that is.”\n\nIn my defense, it was 2003—when XSLT was the hot new tool—and my time was fully occupied being the stressed-out XP developer/coach for a mission-critical but poorly designed financial application. The new developer explained that XSLT could transform text from one format into another (for example, XML into HTML). He was suggesting that we continue to use our format internally and build a translation layer between our system and the bank’s system.\n\nThe changes to the Java code turned out to be the easiest part of the project. We found a very simple XSLT-processing library available for Java. Thanks to all the previous refactoring we had done to add new loan types, we had a single convenient place in our code to pipe XML through the XSLT processor.\n\nThen came the hard part, or so it seemed. XSLT is extremely fussy: You could fix one problem and another would pop up—its effects painfully obvious, but its cause hidden within a huge pile of steaming XML. I fearfully imagined erroneous values placed into incorrect fields, resulting in false bond ratings, lost reputation and revenues, and crushing lawsuits.\n\nOnce the newbie developer taught me a little XSLT syntax, it occurred to me that we could apply TDD to the XSLT code. In production, the Java translator would read the XSLT file and process the stream of XML created by our existing system. In each unit test, we asked the translator to process a small string of carefully crafted XML written right into each unit test. The test would assert that the output string matched our (also carefully written) desired expectations. So, given this XML input, when processed by the full production XSLT file, then the resulting XML should match the expected XML.\n\nBecause XSLT is such a fussy declarative language that does exactly what you tell it to do, I cannot imagine how we could have built it without TDD. Had we tried to write those tests afterward, we would have encountered numerous defects that, when fixed, would have spawned other defects.",
      "content_length": 2154,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "Only in the aggregate of a growing suite of tiny tests could the XSLT be compelled to do all the things we needed it to do, without doing something wrong or adding something “extra” (also very bad for us, and far more difficult to spot manually).\n\nUltimately, we turned that estimated 6-month painful rewrite into a 5- or 6- week educational and fun experience. Our brokers were happy to have the new system deployed almost 20 weeks earlier than estimated, and to start doing more of what they enjoyed doing. And we promoted that “junior” developer to senior developer at his 6-month anniversary. Everybody won!\n\nTaming the Black Swans\n\nThe arrival of a Black Swan user story is likely inevitable for any software program that continues for six months or more. That’s not a guarantee, but that’s been my experience.\n\nIf your team and your code aren’t ready to handle it, the cost of implementing the Black Swan will likely be prohibitive, and it will repeatedly get pushed lower on the queue and never make it into customers’ hands. If we had told the University of Michigan transplant surgeons, “A button? That’ll take six months,” that change would never have been scheduled. Today, medical interns might still be paying for our negligence with papercuts and stapler accidents.\n\nThe Black Swan user stories I’ve encountered have these attributes in common:\n\nEach Black Swan was a surprise to everyone, including the product advocate who thought it up. Surprising, disruptive, potentially quite valuable, and potentially quite expensive.\n\nThe Black Swan resulted from the product advocate’s knowledge of (1) the software’s existing capabilities, (2) the team’s talents and reliability, and (3) the shifting needs of the market.\n\nAt first, it seemed nearly impossible to implement the user story in any reasonable amount of time. Team members saw the Black Swan",
      "content_length": 1861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "as a major architectural change, based on their years of experience prior to test-driven practices.\n\nUpon reflecting on root causes, these teams determined that success was attributed to the following:\n\nThere was never an effort to prepare for some unknown future eventuality. You cannot design in readiness for a particular Black Swan. To continue the metaphor, you can only “keep the lake waters clean.”\n\nThrough diligent refactoring, the software’s design was always appropriate for all existing behaviors, and the team had already consolidated almost all duplicated code into a clear and malleable design.\n\nDisciplined, long-term TDD had helped the team grow and maintain their safety net of fast and comprehensive tests. Every developer acknowledged that the suite would not have been as robust, or as fast, if unit tests had been neglected or retrofitted rather than test- driven.\n\nReal and continuous peer collaboration quickly led to technical insights or discoveries that were key to overcoming the team’s dread at facing the disruptive Black Swan.\n\nSummary\n\nThough we expect Black Swan user stories to be rare, in my career, one has arrived once or twice per year, on average. The arrival time and temperament of each Black Swan cannot be anticipated, and the successful deployment of the user story cannot be guaranteed.\n\nUnfortunately, the opposite is a near certainty. During my own 13 years of experience prior to encountering test-driven practices, and when observing client teams that were either reticent about or discouraged from embracing TDD, I’ve seen this sad outcome all too often: Without a sturdy safety net of tests, the team’s Black Swans were either repeatedly rejected as too",
      "content_length": 1704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "expensive or risky, or took many months to complete, occasionally missing a critical competitive window.\n\nMediocrity comes with its own punishments. A large organization might absorb the failure, but a small company can be obliterated by it.\n\nTDD and diligent refactoring will keep the code both clean enough for its current purpose and ready to change when necessary. The Black Swans can then arrive at your sparkling lake, land softly, and enrich your life.\n\nThe world of software development is changing rapidly. But whenever I look at the advancements of artificial intelligence/large language models or quantum computing, I still see the need for a human to translate customer needs into concrete, detailed examples. Otherwise, the resulting software remains a mystery, and the misunderstanding reveals itself to a user. The defect—the misunderstanding—could be a mere annoyance, but sometimes it might be life-threatening.\n\nTest-driven thinking, I’m sure, is here to stay.\n\nOceanofPDF.com",
      "content_length": 994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "Appendix Exercises\n\nThis appendix contains the recommended exercises discussed in the book. The exercises are designed to give you the opportunity to practice Test- Driven Development within a domain that is relatively simple and possibly familiar.\n\nEach exercise has a set of beginning tasks, followed by some extended options. The goal is not to build an entire application, but rather to get familiar with TDD and to notice any resistance to testing you encounter, either in the code or in yourself. These exercises also give you ample opportunities to practice refactoring to overcome the resistance.\n\nPassword Strength Checker\n\nThis is a good TDD exercise for exploring your design preferences and tolerance for code smells. Read the requests carefully and complete them all, even if some might seem a bit silly.\n\nPassword Strength Checker: Part 1\n\nUse TDD to develop a simple, in-process, easy-to-use API to check password strength. The caller shouldn’t need to know which checks are being done on the password string. A simple Boolean return value of true (strong enough) or false (too weak) is wanted.\n\nAs you write this code, notice whether a previous test fails when you make another test pass, and which steps you take to alleviate that issue.\n\nTo be an acceptably strong password, a string must:\n\nHave a length greater than 7 characters",
      "content_length": 1348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Contain at least one alphabetic character\n\nContain at least one digit\n\nOnce you’re finished with this exercise, consider this question: If you were asked to alter this API, what sorts of changes would now be easy to test and develop, and which kinds would be more difficult to test and develop?\n\nImportant! Please do not turn the page and proceed to Part 2 until you have completed Part 1.\n\nPassword Strength Checker: Part 2\n\nThe client needs enhancements to your existing API, but they are hoping the new API will remain backward compatible with sites they’ve already built. Two enhancements have been requested:\n\nSome clients want a way to obtain a list of all the reasons why a password was not strong enough.\n\nSome clients want to be able to pass a Boolean “Admin” flag to the API. If this flag is set to true, the password must also meet the following criteria:\n\nBe > 10 characters long\n\nContain a special character\n\nHave a special character or digit as the last character\n\nOnce you’re finished with this exercise, consider the following questions:\n\nWhat did you do to remain backward compatible with Part 1? Were there other options that you considered, or that you would try if you had to start over?\n\nDepending on your choice of solutions, did you consider using test doubles? Did they help—or would they have helped—get all behaviors thoroughly tested?",
      "content_length": 1361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "Salvo\n\nSpend as little or as much time as you want on this exercise. The point isn’t to finish, but rather to practice TDD.\n\nUsing TDD, build your own Salvo1 game. Start with a fresh code repository. Be sure to commit whenever all tests are passing, and push to the repo at least whenever you complete a task.\n\n1 https://en.wikipedia.org/wiki/Battleship_(game)\n\nThe basic rules of the game are as follows. Please read through all instructions before starting on the exercise:\n\nThere are two players.\n\nEach player has a 10 × 10 board representing their region of the sea.\n\nEach player’s ships are arranged horizontally or vertically (and secretly).\n\nA player’s ships cannot contact or overlap each other (see Figure A.1).\n\nShips vary in size.\n\nPlayers enter salvo coordinates to try to hit the other player’s ships.\n\nWhen all squares of a ship are hit, the ship is sunk.\n\nBoth players are notified if a salvo is a hit or a miss, and whenever a ship has sunk.\n\nThe player who sinks all of the other player’s ships wins.\n\nEach player starts with the same-sized fleet of ships. For example, these five:\n\nCarrier, size 5\n\nBattleship, size 4\n\nCruiser, size 3\n\nDestroyer, size 2",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Submarine, size 1\n\nFigure A.1 A player’s board representing each ship as a gray rectangle and each previous strike location as an X.\n\nHere’s a suggested approach:\n\n1. Start with the domain model (no UI, no persistence). Do a little paired whiteboard UML or create CRC cards. Set a timer for 10–15 minutes —just long enough to identify some clear behaviors. Think about which behaviors you want, rather than which data structures you might use to implement them.\n\n2. Get the basics tested. Test behaviors and let objects and data structures emerge to support those behaviors.\n\n3. Choose from some of the extended options.\n\nThe extended options (in no particular order) are as follows:\n\nHave the game randomly place the ships on the boards for the players. Prevent a player’s ships from overlapping or being adjacent to each other (see Figure A.1 for an example).\n\n“Autopilot”: Have the software be one of the players. This should not require an AI or fancy decision tree: Walk through your own strategies and have the game mimic those. Note that the game needs",
      "content_length": 1059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "to know what the human player’s board looks like, but “Autopilot” should not.\n\nLet players choose a “short” game (fewer ships) or a “longer” game (more, smaller ships, or a larger board).\n\nLet the players pause and save the game for later.\n\nCreate a console interface, or a graphical interface using JavaScript or a interface and architecture that will allow you to unit-test UI behaviors, preferably without painting phantom widgets on your screen while the tests are running.\n\nlightweight GUI framework. Choose an\n\nWill you allow for two human players? If so, how will you prevent them from viewing each other’s boards? Does your environment provide lightweight interprocess communication? Will you deploy to a cloud service that handles multiuser access?\n\nBe your own product advocate and add your own fun variations.\n\nOceanofPDF.com",
      "content_length": 836,
      "extraction_method": "Unstructured"
    }
  ]
}