{
  "metadata": {
    "title": "Investigating The Smells of LLM Generated Code",
    "author": "Debalina Ghosh Paul; Hong Zhu; Ian Bayley",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 17,
    "conversion_date": "2025-12-19T17:31:12.306610",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Investigating The Smells of LLM Generated Code.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-11)",
      "start_page": 1,
      "end_page": 11,
      "detection_method": "topic_boundary",
      "content": "5 2 0 2\n\nt c O 3\n\n] E S . s c [\n\n1 v 9 2 0 3 0 . 0 1 5 2 : v i X r a\n\nInvestigating The Smells of LLM Generated Code⋆\n\nDebalina Ghosh Paul, Hong Zhu∗, Ian Bayley\n\nSchool of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, OX3 0BP, UK\n\nAbstract\n\nContext:\n\nLarge Language Models (LLMs) are increasingly being used to generate program code. Much research has been reported on the\n\nfunctional correctness of generated code, but there is far less on code quality. Objectives:\n\nIn this study, we propose a scenario-based method of evaluating the quality of LLM-generated code to identify the weakest\n\nscenarios in which the quality of LLM generated code should be improved. Methods:\n\nThe method measures code smells, an important indicator of code quality, and compares them with a baseline formed from reference solutions of professionally written code. The test dataset is divided into various subsets according to the topics of the code and complexity of the coding tasks to represent different scenarios of using LLMs for code generation. We will also present an automated test system for this purpose and report experiments with the Java programs generated in response to prompts given to four state-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon. Results:\n\nWe find that LLM-generated code has a higher incidence of code smells compared to reference solutions. Falcon performed the least badly, with a smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%) and finally Codex (84.97%). The average smell increase across all LLMs was 63.34%, comprising 73.35% for implementation smells and 21.42% for design smells. We also found that the increase in code smells is greater for more complex coding tasks and for more advanced topics, such as those involving object-orientated concepts. Conclusion:\n\nIn terms of code smells, LLM’s performances on various coding task complexities and topics are highly correlated to the quality of human written code in the corresponding scenarios. However, the quality of LLM generated code is noticeably poorer than human written code.\n\nKeywords: Machine learning, Large language models, Performance evaluation, Code generation, Code quality, Usability, Code smell\n\n1. Introduction\n\n1.1. Motivation\n\nLarge language models (LLMs) are increasingly being used in practice to assist programmers with code generation. It is widely recognised that such machine learning (ML) models can significantly improve productivity [1], but there are con- cerns about the quality of the code generated. For example, the 2023 Stack Overflow Developer Survey conducted by Google in 2023 with over 90,000 respondents globally found that “39% of\n\ndevelopers say they don’t trust AI-generated code” [2]. More recently, based on a quantitative study of 211 million lines of code changes submitted to GitHub between January 2020 and December 2024, Harding et al. [3] identified multiple “signs of eroding code quality” such as duplicated code and various forms of technical debt. Moreover, they were able to link this rising defect rate with AI adoption. So this raises an impor- tant question: what specific quality defects are present in LLM generated code?\n\n⋆This paper is an extended and revised version of the conference paper en- titled \"Does LLM Generated Code Smell?\" to be published by IEEE in the Pro- ceeding of The 9th International Conference on Cloud, Big Data, and Com- munication Systems (ICCBDCS 2025). The results presented in the conference paper are preliminary and superseded by this paper.\n\n∗Corresponding author Email addresses: 19217422@brookes.ac.uk (Debalina Ghosh Paul), hzhu@brookes.ac.uk (Hong Zhu), ibayley@brookes.ac.uk (Ian Bayley)\n\nOur previous work [4] has looked at two of these attributes: correctness and complexity, and evaluated ChatGPT on the benchmark ScenEval that we constructed, where correctness is determined by passing all test cases automatically gener- ated from both ChatGPT generated code and the reference so- lution, and complexity was measured with cyclomatic com- It was found plexity, cognitive complexity, and line counts. that correctness was lower for more advanced coding topics and more complex coding tasks. ChatGPT-generated code was\n\nVersion 4.1; October 6, 2025.\n\nmore complex than human-written code. In addition, the com- plexity increases greater for more complex tasks than simpler ones. These suggest that LLMs are less useful for more com- plex and advanced tasks and that could explain why senior de- velopers use them less often [5], [6].\n\nHowever, Ziegler et al., in their study of GitHub Copilot, ob- served that the major driving force for the adoption of generated code is not its correctness but whether it is useful as a starting point for further development [7]. So, in this paper we will shift our attention to the quality attributes that are relevant to that. These include readability, testability, maintainability, ease of modification / evolution, reusability and so on.\n\n1.2. Challenges And Our Approach\n\nIt is difficult to measure LLM generated code on these qual- ity attributes since the context of their usage is unknown. Our solution is to use code smell detection techniques since they are well established in software engineering research for this pur- pose [8, 9, 10]. It is widely recognised that code smells are indi- cators of problems present in program code with maintenance, evolution and reuse [11].\n\nA problem with the concept of code smells, however, is that it is relatively subjective. Beck and Fowler define that bad code smells are “not precise criteria for flaws in program code” [8, 12]. They suggest that the presence of smells is “better to be judged based on informed human intuition” and research has found that “human agreement on smell detection is low” [13]. Our solution is to follow best practice in ML research: benchmark LLM performance on a dataset and compare with a baseline. The dataset we will use is ScenEval [4]. It consists of tasks collected from both textbooks and questions submitted to StackOverflow; the latter is a professional coding problem- solving website so the questions are real-world. Each task is accompanied by a reference solution either written by the text- book authors or supplied by professional programmers in IT in- dustry in response to the question on StackOverflow and scored highly by peers. These reference solutions provide a good base- line that reflects the current state-of-the-art in professionally written code so that we can decide whether the LLM-generated code is of comparable quality.\n\nTo achieve our research goal, it is insufficient to score each LLM with a single scalar value since they are used in many dif- ferent contexts for different purposes by different users. So we evaluate the LLMs on different scenarios. These include differ- ent problem topics and different complexities. The information we need for filtering on these scenarios is included as metadata with each coding task in the ScenEval benchmark [4]. Different subsets of the dataset can therefore be formed easily to repre- sent different scenarios.\n\nHowever, the existence of these subsets necessitates repeated experiments, so we automate both the execution of the experi- ments and the subsequent analysis of the large volume of data that is produced. We design and implement an automated test system following the datamorphic testing methodology [14] and execute the experiment with the Morphy test automation environment [15].\n\n2\n\nD. G. Paul, H. Zhu and I. Bayley\n\n1.3. Contributions\n\nOur main contributions are as follows.\n\n1. We propose a scenario-based method to investigate the quality of LLM generated code by detecting code smells, statistically analysing them and comparing them with a baseline of human-written programs. This method enables us to identify the quality weaknesses of LLM generated code specific to each scenario.\n\n2. We have designed and implemented a test system to auto- mate the experiments with LLMs and the analysis of the data obtained from the experiments.\n\n3. We have conducted a systematic and intensive experiment with four current state-of-the-art LLMs and compared the generated code with human-written reference solutions in the benchmark as the baseline. The experiment demon- strates the validity and feasibility of the proposed method, and the efficiency and effectiveness of the test system. 4. We have identified the weaknesses of LLM for code gen- eration with regards to the quality of the code generated. These results are the first of their kind in the literature as far as we know.\n\n1.4. Structure of the Paper\n\nThe paper is organised as follows. Section 2 reviews re- lated work on how to evaluate the quality of LLM-generated code and formulates the research questions. Section 3 gives a brief introduction to the notion of code smell and techniques for code smell detection. Our uses of these techniques are de- scribed. Section 4 presents the automated test system, which is designed and implemented based on the datamorphic software testing methodology. Section 5 presents the design of our ex- periment. Section 6 reports the results and presents the analysis of the data. Section 7 discusses the threats to experimental va- lidity. Finally, Section 8 summarises the findings and discusses directions for future work.\n\n2. Related Works And Open Problems\n\nEvaluation of the capability of LLMs in code generation has hitherto focussed on functional correctness, but far less on code quality and only recently; see [16] for a recent review. We now discuss the few works in the literature that are relevant, fol- lowed by the open research questions addressed in this paper.\n\n2.1. Manual Evaluation\n\nIn 2024, Miah and Zhu proposed a user-centric methodology to evaluate LLMs according to the quality of code generated [17]. The method consists of the following three components:\n\n1. A multi-attempt testing process model: the tester engages in an iterative process of interactions with a LLM by (a) formulating, revising and submitting a query to the LLM under test, (b) getting responses from the LLM, (c) assess- ing the LLM generated solution for usability, d) determin- ing whether a further attempt of querying the LLM should be made. This iterative process continues until either a\n\nVersion 4.1; October 6, 2025.\n\nsatisfactory solution is obtained or a threshold maximum number of allowed iterations is reached.\n\n2. A set of eight quality attributes related to how easily a hu- man user could use LLM generated code. These are accu- racy, completeness, conciseness, clarity of logic, readabil- ity, well-structured-ness, parameter coverage and depth of explanation.\n\n3. A set of three metrics that measure the user experience. The first is the average number of attempts, each of which is an iteration of the human-LLM interaction described above. The second is the average completion time for the task of coding using the LLM. The third is the success rate, where success means that useful code has been generated.\n\nThe authors illustrated the methodology using ChatGPT with 100 tasks in the programming language R. Usability was high: 3.8 out of 5, manually assessed on a Likert scale of 1 to 5 for each of the eight quality attributes. The average number of at- tempts was only 1.61 and the average completion time was 42 seconds.\n\nAlthough subjective manual evaluation is valuable for the user-centred process proposed in the paper [17], it is labour- intensive and error-prone. Therefore, objective automated methods are preferable.\n\n2.2. Automated Evaluation\n\nAs far as we know, the only way to use automation to evalu- ate code quality is via code smell detection. Siddiq et al. was perhaps the first to do this, in [18], where they used Pylint [19] to detect the code smells in three different training datasets: CodeXGlue [20], APPS [21], and Code Clippy [22]; Bandit [23] was also used in order to detect security code smells. To investigate the impact of code smell in training dataset, ten dif- ferent code models, each based on the GPT-Neo 125M model, were trained on these three datasets, then tested on the Hu- manEval dataset [24] and compared with GitHub Copilot.\n\nThey found that the most frequent smells in the training datasets were also the most frequent in the generated code. They concluded that smells in the training datasets leaked into to the code models, although there was no statistical analysis of the correlations between the two, nor any causality analysis. However, their work has raised concerns about code smells in training datasets.\n\nMoratis et al. applied code smell detection to the dataset De- vGPT of reported iterative conversations in GitHub between de- velopers and ChatGPT [25]. Two types of conversations were extracted:\n\n1. Write me this code, with text instructions as input to pro- duce a program code\n\n2. Improve this code, with code snippets as input to improve the quality of the input program code\n\nThe conversations were then fed into the code smell detec- tion tool PMD [26]. In the Write me this code category, there were 47 conversations and a total of 59 code smell violations in 144 code blocks. Half (50.8%) of the violations concerned the standard practices of code conventions, a third (37.3%) related\n\n3\n\nD. G. Paul, H. Zhu and I. Bayley\n\nto styles of coding that have an impact on code readability and the remainder (11.9%) were violations of coding rules that were more likely to lead to errors.\n\nIn the Improve this code category, there were 334 conversa- tions. In most cases, the output had fewer total violations and sometimes it was a lot fewer, suggesting that ChatGPT can be used for this purpose. Occasionally the output had more viola- tions typically this was only one or two violations and not of the type that would introduce errors. Most conversations required fewer than 5 attempts; where more were needed it was usually because multiple code snippets were supplied as input.\n\nMoratis et al. observed, however, that their findings were \"inherently optimistic, as it exclusively contains instances of successful interactions with ChatGPT” [25]. Moreover, it is unclear whether the code quality is better or worse than that of human developers.\n\nAnother attempt to apply code smell detection to measure the ability of LLMs to improve the quality of existing code is due to DePalma et al. [27], who developed prompts to ask ChatGPT to refactor Java code to improve quality on 8 different quality attributes. Once again, PMD was applied both to the original code and the refactored code.\n\nLiu et al.[28] took the idea of code smell measurement one step further to form a self-repairing mechanism. PMD was used once again for Java code but in conjunction with CheckStyle [29]. For Python code generation, the tools used were Pylint [19] and Flake8 [30].\n\nThey classified code quality issues into four categories: (a) Compilation and Runtime Errors, (b) Wrong Output (i.e. func- tional incorrectness of the generated code), (c) Code Style and Maintainability, and (d) Performance and Efficiency. For each of these categories, they identified the top 10 issues for Java and for Python. The dataset used was the LMDefect dataset [31] of 2033 coding tasks supplemented with coding tasks extracting from LeetCode. The experimental data shows great promise with a repair rate in the range 20% to 60%. However, fixes can often introduce new quality issues.\n\nTable 1 summarises the related works mentioned in this sub- section and contrasts them with the work reported in this pa- per. The column Aims gives the purpose of the research. The column Usage explains how code smell detection techniques achieve that purpose. The columns Tools, Dataset, Language, LLM and Smell Types give the code smell detection tool(s) used, the test dataset used to evaluate the LLM(s) with the size of the dataset in parentheses, the programming language in which the code is generated, the LLM(s) evaluated, and the types of code smell, respectively.\n\nIt is worth noting that the eight smells detected by DePalma et al. [27] are implementation smells. It is not explicitly stated what code smells were detected by Liu et al.’s work. However, we believe that architectural smells were not detected because there is no architectural level code generated by the test cases. For the same reason, architecture code smells were not detected in our work.\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\nTable 1: Summary of Related Works\n\nWork Siddiq, et al. 2022\n\nAims Investigating the impact of code smells in training datasets on the quality of generated code\n\nMoratis, et al. 2024\n\nAssessing the quality of code generated via iterative con- versations in the Write this code scenario Assessing the quality improvement of code generated via iterative conversations in the Improve this code scenario Evaluating LLM’s capability of code refactoring\n\nDePalma, et al. 2024 Liu, at el. 2024 This paper Evaluating LLMs on various types of code smells for var- ious types of code to generate and the complexities of coding task\n\nEvaluating LLM’s capability of fixing code quality issues\n\nTools Pylint\n\nBandit\n\nPMD\n\nPMD\n\nPMD, CheckStyle Pylint, Flake8 PMD, CheckStyle, DesigniteJava\n\nUsage Detect code smells in training datasets and generated codes Detect security smells in training datasets and generated codes Detect code smell and measure code quality DevGPT (47)\n\nDataset (Size) HumanEval (164)\n\nComparing code smells before and after refactoring Assessing the quality of the code before and after refactoring Assessing the quality of generated code\n\nAssessing the quality of code generated in various scenarios\n\nDevGPT (334)\n\nMa (40) LMDefect (2033) ScenEval (1000)\n\net\n\nal.[32]\n\n+\n\nLanguage Python\n\nJavaScript\n\nJava\n\nJava Python Java\n\nLLM GPT-Neo, GitHub Copilot\n\nChatGPT\n\nChatGPT\n\nChatGPT\n\nGemini ChatGPT, Codex, Falcon\n\nPro,\n\nSmell Types Implementation smells\n\nSecurity smells\n\nBest practice, code style, error prone\n\nBest practice, code style, design, documentation, error prone, multi-threading, performance, security Implementation and design smells\n\nImplementation and design smells\n\n2.3. Research Questions\n\n3. Code Smell Detection\n\nThe existing works discussed above give some picture of the prevalence of code smells in LLM-generated code but their con- text and research questions are different from ours and there is no comparison with the human-written alternative. To bridge this research gap, we will ask the following open research ques- tions:\n\nIn this section, we will review the notion of code smells as background and explain how they can be detected. We will also explain how the code smell detection tools PMD, Checkstyle and DesigniteJava will be employed in our investigation.\n\nRQ1. Does LLM-generated code have a quality compara- ble to that of human-written code?\n\nBy human-written code, we mean code written by textbook authors or professional programmers, since we believe that can fairly represent the current best practice. Since our approach, outlined in Section 1.2, is to measure code smells, we will ask how does the incidence of smells in LLM-generated code com- pare with that of human-written code.\n\nRQ2. On which programming topics is LLM-generated code is weaker or stronger in quality compared to human- written code?\n\nOnce again, this can be rephrased in terms of code smells. How do code smells of LLM-generated vary with the question topic? More importantly, on which topics are the smells most worsened or most improved compared to human-written code?\n\nRQ3. How does the quality of LLM-generated code vary with the complexity of the coding task?\n\n3.1. The Notion of Code Smell\n\nThe concept of a code smell originated in Fowler’s book on refactoring [12] having been coined by Beck [8]. It was de- fined as “indications that there is trouble that can be solved by a refactoring” and “certain structures in the code that suggest (sometimes they scream for) the possibility of refactoring”. The authors described a list of 22 code smells, and how in each case, refactoring methods can help to improve the quality of the pro- gram. Since then, the notion of code smell has been intensively studied (see, for example, [13, 33, 11] for systematic literature reviews) and generalised to software smells [34].\n\nBeck and Fowler noted two distinctive aspects of the notion of code smells. Firstly, they are indicative rather than “pre- cise criteria for flaws in program code”. The code may not be flawed and may function correctly, but there may be fu- ture problems with maintenance, evolution, and reuse[11]. Sec- ondly, they are subjective. It is better to judge smell based on “informed human intuition” and consequently, “human agree- ment on smell detection is low”, as has been proven by research [13].\n\nA closely related question would be is the difference in qual- ity compared to human-written code greater for complex coding tasks? Both of these questions can be transformed into corre- sponding questions on code smells as above.\n\nRQ4. On which quality attributes is LLM-generated code worse compared to human-written code, since there is where research efforts could be directed?\n\nWe can rephrase this to ask which code smells are most prevalent in LLM-generated code and whether each smell is more or less common than in human-written code.\n\nRQ5. How is the correctness of LLM-generated code re- lated to the usability of the code in terms of readability, modifiability, reusability and easiness to evolve?\n\nTo answer this question, we will separate the codes generated by LLMs according to their correctness, analyse their smells separately, and compare their code smells with the baseline.\n\nThe notion of code smell is linked to a number of other soft-\n\nware engineering concepts and techniques, as follows:\n\nSmells are indicators or symptoms of a deeper design problem in the program code, as discussed above.\n\nSmells are suboptimal or poor solutions to a coding prob- lem. Bad smells lead to a technical debt of needing to find better solutions later.\n\nSmells violate recommended best practice for the domain. These include coding conventions and/or software design principles. Therefore, smells can be detected by looking for the violations of best practices.\n\nSmells have a negative impact on the software quality at- tributes that are related to product revision and transition, such as modifiability, readability, testability, reusability, portability, etc. In this way, they make software difficult\n\n4\n\nVersion 4.1; October 6, 2025.\n\nto evolve, maintain, and reuse, and increase the likelihood of bugs, without themselves being bugs.\n\nAs Fowler suggested, smells should and can be elimi- nated or reduced, for example, by refactorings, which are meaning-preserving transformations on the software.\n\nSmells are recurring problems in program code. The pat- terns of such recurring problems bear similarity to the no- tion of anti-patterns.\n\n3.2. Types of Code Smells\n\nMany types of code smells have been defined and investi- gated in the literature. They can be classified according to a number of different criteria, such as the effect caused, design principles violated, location of the smell, its granularity, etc [34]. In this paper, we adopt the classification proposed by Suryanarayana, Samarthyam and Sharma [9, 10] which distin- guishes implementation smells from design smells (also known as micro-architectural smells) and architectural smells.\n\n3.2.1. Implementation Smells\n\nImplementation smells are concerned with suboptimal imple- mentation choices that make the code unnecessarily complex, difficult to maintain, and harder to understand. We detect and analyse the following:\n\nInconsistent Naming Convention. Deviations from the rec- ommended naming conventions.\n\nExcessive Complexity. An expression, statement or a method is difficult to understand due to lack of clarity caused by excessive complexity. For example, a state- ment could be excessively long, an expression could be excessively nested and/or have too many operations, and a method could have too many lines of code, and/or has an excessive list of parameters.\n\nIncompleteness. A piece of code is unfinished with, for example, \"TODO\" or \"FIXME\" tags, or a statement is in- complete. For example, a catch block may be missing han- dling logic, a conditional construct may be missing a ter- minating else clause, a switch or selector statement may be missing a default case, or more generally, a block of code within curly braces {} contains no executable state- ments, etc.\n\nRedundant Elements. The presence of duplicate parame- ters, methods, or code blocks. A method or attribute may have an unnecessary modifier, such as public where that visibility is already implied, or public static final where final would have been enough.\n\nImproper Alignment and Placement. Code is not properly aligned according to coding standards, and/or an entity in the code is misplaced; for example, attributes may not be given in the recommended order.\n\nMagic Number. A numeric literal is used directly in code without being defined as a constant.\n\n5\n\nD. G. Paul, H. Zhu and I. Bayley\n\nDead Code. Sections of code are no longer executed or provide no value for some other reason.\n\nResource Handling. Inefficiencies in the use of resources.\n\nDocumentation. Insufficient comments to explain the code properly.\n\n3.2.2. Design Smells\n\nDesign smells are concerned with design choices, as pre- sented in the program code, that violate fundamental design principles, such as poor use of object-orientation. They indicate the types of weaknesses that can lead to increased complexity, maintainability issues, and reduced code reusability. The fol- lowing are the types of design smells defined by Suryanarayana et al. [9, 10]; these are all detected and analysed in this paper.\n\nAbstraction Smell – Issues related to improper, missing, or unnecessary abstractions, affecting code clarity and reusability.\n\nEncapsulation Smell – Violations of encapsulation princi- ples, such as excessive exposure of internal details or in- adequate access restrictions.\n\nModularisation Smell – Poorly structured modules, in- cluding tightly coupled components, improper separation of concerns, and redundant dependencies.\n\nHierarchy Smell – Problems in class hierarchies, such as deep inheritance trees, improper sub-classing, or lack of adherence to object-oriented principles.\n\n3.2.3. Architectural Smells\n\nArchitectural smells are the weakness in the architectural de- sign of the system, as presented in the code, that often lead to reduced system flexibility, modification difficulties and main- tainability challenges. Typical examples include inappropriate layering and tight coupling between components and subsys- tems, etc. We will not consider these smells, however, because LLMs have limited capability for generating the entire system architecture and are not normally used for this purpose.\n\n3.3. Detecting Code Smells\n\nCode smell detection has been intensively studied in the soft- ware engineering literature; see, for example, [35, 36, 37] for systematic literature reviews. Fowler suggested that detection should be manual based on developer’s experience and intu- ition. However, this is not scalable and repeatable. So auto- mated tools should be used instead. Such tools can be classified into three types.\n\nStatic Code Analysis Approaches.\n\nTools for static analysis are usually based on either metrics or pattern-matching. Metrics on program code include Lines of Code (LOC), Number of Attributes per Class (NOA), Num- ber of Methods per Class (NOM), Number of Children Classes (NOC), Depth of Inheritance (DIT), etc. A metrics-based tool\n\nVersion 4.1; October 6, 2025.\n\ndetects code smells using a combination of these metrics. A draw-back of this approach is the arbitrary nature of the thresh- old values set for the metrics.\n\nA rule-based tool, in contrast, defines a set of detection rules based on the syntactic structure of the code. Often these rules are linked to coding conventions and design principles. Some- times the tool is configurable in that the smells can be speci- fied with editable rules. Usually, they can be seamlessly inte- grated into existing development workflows. Typical examples of these tools include PMD, Stylecheck, Pylint, DesigniteJava, etc. Recently, such tools have been applied to LLM-generated code; see Section 2.\n\nHistory-based Approaches.\n\nThe evolution history of the system can be used to analyse the symptoms caused by smells and hence identify the smell. However, only a small number of smells can be detected this way.\n\nML-based Approaches.\n\nThere are two approaches for using machine learning (ML)\n\nmodels [38, 39].\n\nThe first is to train a model with features based on metrics, like lines of code, cyclomatic complexity, coupling metrics, etc. This requires large, high-quality labelled datasets. However, these are rare for code smells. Consequently, such ML mod- els have not achieved the performance suitable for practical use [38, 39]. For example, the Naive Bayes model reported in [40] has low F1-scores for most smells and low precision in par- ticular, indicating a high number of false positives. Moreover, existing ML models for smell detection are binary classification models, i.e. each model only detects one type of smells.\n\nThe second approach is to use LLM models to detect code smells. However, a recent evaluation reported in [41] shows low F1 scores for both Llama variants and GPT-4, the latter below 0.04.\n\n3.4. Use of Smell Detection Tools\n\nIn this paper, we will use static code analysis tools to de- tect code smells in both LLM-generated code and the reference solutions. As with Liu et al. [28], we use PMD [42, 26] and Checkstyle [43, 29]. They are based on widely recognised cod- ing conventions: Google Java Style Guide 1 and the Sun Java Code Convention 2, respectively. Both of them are capable of detecting and reporting the violations of these rules. However, since both tools are relatively weak in detecting design level code smells, we also use DesigniteJava 3, which detects smells according the design principles violated.\n\nTables 2 and 3 show the smell detection rules provided by each tool used in our work. Columns Tool Used and Detection\n\n1https://google.github.io/styleguide/javaguide.html 2https://www.oracle.com/java/technologies/javase/codeconventions-\n\nintroduction.html\n\n3https://www.designite-tools.com/products-dj\n\n6\n\nD. G. Paul, H. Zhu and I. Bayley\n\nRules give the tool and the smell detection rule used. Readers are referred to the websites of the tools for the definitions of the rules.\n\nNote that none of the tools cover all smells so we need to combine them for maximum coverage. One smell type may be detected by several different rules, even by different tools. The number of violations for such a smell type is calculated by summing up the numbers of violations of different rules.\n\nWhere a rule is implemented by more than one tool, however, the violation is counted only once. We have found in such cases that both tools give the same number of violations for the rule on the same code extract. In Table 2, such cases are indicated by a footnote reference 4.\n\nSince empirical studies have found it difficult to set a limit on the number of violations for the code still to be of good quality, we will count the number and compare it with a baseline; this reflects current practice.\n\n4. Test System for Code Smell Analysis and Evaluation\n\nOur experiments with LLMs need to be automated and we do this by applying the methodology of datamorphic testing pro- posed by Zhu et al. [14], [15]. This treats software testing as a systems engineering problem and it encourages both efficient management of test resources and the evolution of the test fa- cilities alongside that of the software under test.\n\nAccording to the methodology, a test system comprises two types of artefacts: test entities and test morphisms. The former are objects and documents involved in testing, such as test data, test datasets, test results, etc. while the latter are operations that manipulate and/or generate these entities to perform testing tasks. This methodology is supported by the test automation environment Morphy [15].\n\nMorphy provides a Java framework in which a test system can be implemented as a Java class (more precisely, a hierar- chy of Java classes) that consists of a set of attributes repre- senting the test entities and a set of methods representing test morphisms. They are both annotated with metadata so that they can be recognised by Morphy, seamlessly integrated with Mor- phy’s testing tools, and applied to achieve test automation. The following types of test morphisms are recognised by Morphy.\n\nSeed Maker: Generates initial test cases from other enti- ties.\n\nDatamorphism: Transforms existing test cases into new ones.\n\nMetamorphism: Verifies the correctness of test cases and returns a Boolean result.\n\nTest Set Filter: Adds or removes test cases from a test set.\n\n4The result from the tool by applying this smell detection rule is ignored because the same rule is already checked by another tool where the rule may have a different name. Only the result from one tool on the same rule is taken into account.\n\nVersion 4.1; October 6, 2025.\n\nTable 2: Smell Detection Rules Used for Implementation Smells\n\nSmell Name\n\nInconsistent Naming Convention\n\nExcessive Complex\n\nRedundancy\n\nIncompleteness\n\nImproper Alignment and placement\n\nMagic Number\n\nDead Code\n\nResource Handling\n\nDetection Rule(s) Local Variable Naming Convention /Local Variable Name Formal Parameter Naming Convention PMD PMD Method Naming Convention /CheckStyle(4) /Method Name PMD Class Naming Convention PMD GenericsNaming CheckStyle AbbreviationAsWordInName CheckStyle AbstractClassName CheckStyle CatchParameterName CheckStyle ConstantName CheckStyle IllegalIdentifierName CheckStyle Simplify Boolean Expression PMD Simplify Conditional PMD/CheckStyle Simplify Boolean Return PMD Simplified Ternary CheckStyle Line Length CheckStyle Method Length /DesigniteJava(4) /Long Method PMD/DesigniteJava(4) Excessive Parameter List CheckStyle Redundant Import CheckStyle Redundant Modifier PMD Copy Paste Detector CheckStyle Missing Switch Default CheckStyle Todo Comment PMD Empty Control Statement PMD/CheckStyle(4) Empty Catch Block CheckStyle EmptyBlock CheckStyle Indentation CheckStyle FileTabCharacter CheckStyle NeedBraces PMD UselessParatheses CheckStyle LeftCurly CheckStyle RightCurly CheckStyle ParenPad MethodParamPad CheckStyle Variable Declaration Usage Distance CheckStyle CheckStyle Declaration Order CheckStyle Magic Number PMD Unused Formal Parameter PMD/CheckStyle(4) Unused Local Variables PMD Unused Private Fields PMD Unused Private Method CheckStyle Unused Imports PMD Close Resource PMD Avoid Instantiating Objects In Loops PMD Comment Required PMD Comment Size PMD Comment Content CheckStyle Javadoc Method CheckStyle Javadoc Type CheckStyle Missing Javadoc Package CheckStyle Javadoc Variable\n\nTool(s) Used PMD /CheckStyle(4)\n\nDocumentation\n\nTest Set Metric: Maps a test set to a real value, such as test adequacy.\n\nTest Case Filter: Maps a test case to a Boolean value. It can be used to determine whether the test case should be retained in the test set.\n\nTest Case Metric: Assigns a real-valued metric to individ- ual test cases (e.g., complexity).\n\nAnalyser: Examines the test set and produces a test report.\n\nExecuter: Runs the program under test using inputs from test cases and captures the outputs.\n\nGiven a test system implemented in Java, Morphy supports\n\ntest automation at the following three levels.\n\nAction: Executes a single test activity using test mor- phisms, built-in functions or tools.\n\n7\n\nD. G. Paul, H. Zhu and I. Bayley\n\nTable 3: Smell Detection Rules Used for Design Smells\n\nSmell Name\n\nModularity\n\nEncapsulation\n\nHierarchy\n\nTool Used Detection Rules PMD God Class PMD Data Class PMD Too Many Methods PMD Too Many Fields PMD Use Utility Class CheckStyle Hide Utility Class Constructor Broken Modularization DesigniteJava Cyclically-dependent Modularization DesigniteJava DesigniteJava Hub-like Modularization Insufficient Modularization DesigniteJava PMD Law of Demeter PMD Coupling Between Objects CheckStyle Class Fan Out Complexity CheckStyle Visibility Modifier PMD Excessive Public Count DesigniteJava Deficient Encapsulation CheckStyle Final Parameters CheckStyle Final Class CheckStyle Hidden Field DesigniteJava Unexploited Encapsulation DesigniteJava Broken Hierarchy DesigniteJava Cyclic Hierarchy DesigniteJava Deep Hierarchy DesigniteJava Missing Hierarchy DesigniteJava Multipath Hierarchy DesigniteJava Rebellious Hierarchy DesigniteJava Wide Hierarchy DesigniteJava Dependency Cycles btw Packages DesigniteJava Imperative Abstraction DesigniteJava Multifaceted Abstraction DesigniteJava Unnecessary Abstraction DesigniteJava Unutilized Abstraction\n\nAbstraction\n\nStrategy: Applies test strategies, which are algorithms with test morphisms and test entities as parameters.\n\nProcess: Runs test scripts of a high-level of abstraction. Such scripts can be obtained by recording interactive op- erations of Morphy, which can be edited, or even manually written, and replayed.\n\nOur test system extends that for a previous experiment, to analyse correctness and completeness of LLM-generated code [4] and uses the same benchmark, ScenEval. We define, how- ever, a new test entity code smell report and the following new test morphisms.\n\nTest Executors: LLM invokers. Four test executors for each of the four LLM models Gemini Pro, Codex, Falcon7B, and ChatGPT. The last of these has been inherited from the previous work [4] but the first three are new. Each executor submits the query to its respective LLM via an API call, and then extracts the Java code from the response text and saves it to a file for further analysis.\n\nAnalysers: Code Smell Detector Invokers. Three new test morphisms, PMD-analyser, Checkstyle-analyser and DesigniteJava-Analyser, which invoke the corresponding static code analysis tools PMD, Checkstyle and Designite- Java, and save the code smell reports into files.\n\nTest Set Metrics: Code Smell Statistical Analysers. Three analysers, Violations per Solution, Baseline Violations per Solution and Increase Rate to Baseline, which perform sta- tistical analysis on the code smell report files.\n\nTest System\n\nJava Compiler\n\nPMD (Code Smell Detector)\n\nDetected Code Smell\n\nJUnit Test Code\n\nGenerated Java Code\n\nTest Dataset\n\nTest Result Report\n\nPMD (Complexity Analyser)\n\nJava RTE + JUnit Executor\n\nCheckstyle (Code Smell Detector)\n\nSeed Makers\n\nManual Task Entry\n\nMorphy Automated Test Environment\n\nExtract W3 Resource\n\nChatGPT\n\nFalcon Invoker\n\nExtract Stack Overflow\n\nFalcon\n\nGemini Pro\n\nGemini Pro Invoker\n\nCodex Invoker\n\nChatGPT Invoker\n\nExecutors\n\nW3 Java Tutorial WebsiteScenEval Benchmark\n\nStack Overflow Website\n\nJava Programming Textbooks\n\nCodex\n\nTest Case Metrics\n\nExternal tools and data sources\n\nPMD Analyser\n\nComplexity Analyser\n\nAnalysers\n\nCompile Java Code\n\nExecute JUnit Test Code\n\nGenerate JUnit Test Code\n\nPurify Ref Test Code\n\nPurify Sol Test Code\n\nEvoSuite (JUnit Test Code Generator)\n\nTest Set Metrics/Analyser\n\nTopic Based Distribution\n\nYear Based Distribution\n\nCheckstyle Analyser\n\nViolations per Sample\n\nSmell Type Based Distribution\n\nExecutable Object Code\n\nComplexity Based Distribution\n\nTest Entities\n\nNew test morphism / entity for code smell analysisExisting test morphism / entity used for code smell analysisExisting test morphism / entity not used for code smell analysis\n\nDesigniteJava Analyser\n\nDesigniteJava (Code Smell Detector)\n\nAnalyse Correctness\n\nVersion 4.1; October 6, 2025.\n\nFigure 1: Structure of The Test System\n\nFig. 1 shows the structure of the test system. Test morphisms and entities inherited from, and explained in, our previous work [4] are shown in grey. New test morphisms and entities are shown in white. External tools invoked by them are shown in blue. Some of these are implemented in Python but invoked through a simple Python2Java interface.\n\n5. Design of the Experiments\n\nBefore discussing the experiment process, we will first re-\n\nview the LLMs, the benchmark and the platform.\n\n5.1. Subject LLMs\n\nTable 4 presents basic information about the four state-of- the-art LLMs we have studied: Gemini Pro, Falcon, ChatGPT, and Codex. All are commonly used for software development.\n\nTable 4: Large Language Models Evaluated\n\nName Gemini Pro Falcon ChatGPT Codex\n\nYear 2023 2023 2023 2021\n\nVersion Gemini Pro 1.0 Falcon-7B GPT-3.5-turbo GPT-3 (Codex)\n\nSize Unknown 7B Unknown 12B\n\n5.2. Benchmark\n\nThe benchmark ScenEval [4] contains more than 12,000 test cases of Java programming tasks. These test cases were cu- rated from textbooks, online tutorial websites and the profes- sional programming knowledge-sharing website Stack Over- flow. In contrast to other benchmarks for code generation (see e.g. [16]), ScenEval has two distinctive features which make it ideal for our purpose.\n\n1. Each test case is accompanied by the Java code for a refer- ence solution, typically textbook answers and highly-rated Stack Overflow posts. As discussed in 1.2, their code qual- ity represents the state of current practice so they enable us to establish a baseline of code smell for human-written Java code. 8\n\n1. Each test case is accompanied by the Java code for a refer- ence solution, typically textbook answers and highly-rated Stack Overflow posts. As discussed in 1.2, their code qual- ity represents the state of current practice so they enable us to establish a baseline of code smell for human-written Java code. 8\n\nD. G. Paul, H. Zhu and I. Bayley\n\n2. Each test case is also accompanied by metadata that speci- fies topic, complexity, source of the task, etc. This enables us to analyse the relationship between these concepts and code smell so that we can answer the research questions.\n\nOur test dataset, sampled at random from the ScenEval benchmark, contains equal quantities (500 each) from text- books and Stack Overflow. The other statistics are given in Table 5; Input Length and Complexity denote the number of words in the task description and the cyclomatic complexity of the reference solution.\n\nTable 5: Statistical Characteristics of the Test Dataset\n\nFeature # Topics # Tasks per Topic (average) # Tasks per Topic (max) # Tasks per Topic (min) Input Length (average) Input Length (max) Input Length (min) Complexity (average) Complexity (max) Complexity (min) Number of Coding Tasks\n\n#Textbook Tasks 25 20.00 79 8 18.55 35 11 3.448 6 1 500\n\n#Real Tasks 18 27.78 61 5 21.54 31 16 3.200 5 1 500\n\nLLM CodeGen Test System\n\nGenerated Java Code\n\nStyleCheck\n\nDesigniteJava\n\nLLM ChatGPT\n\nLLM Gemini Pro\n\nLLM Falcon\n\nLLM Codex\n\nOn CloudDesktop Computer•Dell Precision 3660•i7-13700•32GB Ram\n\nAPI invocation with coding task description as parameterGenerated solution of coding taskJava code generated by LLM\n\nCode smell reports\n\nDetected code smell\n\nCode Smell Detectors\n\nCoding task descriptionJava code of the reference solution\n\nMorphy (Automatic Test Environment)\n\nTest Dataset\n\nPMD\n\nGenerated solution\n\nInvocation of code smell detection tools with Java code as parameters\n\n5.3. Experiment Platform\n\nThe experiment was conducted with the automated test envi- ronment Morphy [14, 15] running on a desktop computer and is illustrated in Fig. 2. The LLM models under test were invoked through API calls as discussed in Section 4 and the smells of the LLM-generated code were analysed with PMD, Checkstyle and DesigniteJava as discussed in Section 3.\n\nFigure 2: The Experiment Setup\n\n5.4. Experiment Process\n\nVersion 4.1; October 6, 2025.\n\n1. Constructing Test Dataset: A test dataset containing 1000 coding tasks was constructed by random sampling of the ScenEval benchmark.\n\n2. Analysing Smells of Reference Solutions: For each cod- ing task in the test dataset, the Java code for the reference solution is analysed by invoking the code smell detection tools PMD, Checkstyle and DesigniteJava. The reports on the violations of smell detection rules are saved into sep- arate files for further statistically analysis to establish the baseline.\n\n3. Invoking LLMs: For each coding task in the test dataset, the LLMs under test are queried through API invocations and the solutions returned were collected. The Java code in the returned text is separated from the explanation text and saved into a .java file.\n\n4. Analysing Smells of LLM Generated Code: The Java codes generated by the LLMs are analysed via invocations of the code smell detection tools PMD, Checkstyle and Desig- niteJava. The violations of the detection rules are saved into code smell report files.\n\n5. Analysing Correctness of Generated Code: For each LLM generated Java code, the correctness of the code is deter- mined by running test data on both the LLM-generated code and the reference solution; see [4] for details.\n\n6. Analysing Complexity of Generated Code: The complexity of the LLM-generated code is measured the same way as in our previous work. [4].\n\n7. Statistical Analysis: The code smell report files are parsed, and statistical analysis is performed on various subsets of the test dataset to answer the research questions. Each sub- set represents a different scenario in the use of LLMs. The smell detection rules are partitioned into subsets according to their type, where needed to answer a research question. Given a subset T of the coding tasks and a subset S of smell detection rules, the following statistical data are cal- culated. lution, denoted by VS M LLM M using Equ. (1) .\n\n(b) The baseline for the test subset T w.r.t. smell detec- tion rules in S, i.e. the number of violations of smell detection rules per solution, denoted by VS B S (T), is calculated from the code smell reports of reference solutions using Equ. (2).\n\n(c) The increase rate of smells for LLM-generated code with respect to the baseline, denoted by InvM S (T), is calculated from VS M S (T) using Equ. (3).\n\nS (T) and VS B\n\nThese three equations are implemented as test set metrics and\n\nformally defined in the next subsection.\n\n5.5. Metrics of Performance\n\nLet t be a given coding task. We write M(t) to denote the program code generated by a LLM model M on coding task t and R(t) to denote the its reference solution in the benchmark.\n\n9\n\nD. G. Paul, H. Zhu and I. Bayley\n\nLet s be any given smell detection rule and c be a given Java code sample. We write Vs(c) to denote the set of violations of the rule s detected in the Java code c.\n\nLet T (cid:44) ∅ be a set of coding tasks, such as those for a specific\n\ntopic or complexity in the test dataset.\n\nLet S (cid:44) ∅ be a set of smell detection rules, such as those for\n\na particular type of code smell.\n\nDefinition 1. (LLM’s Smell Violations Per Solution (VS))\n\nWe write VS M\n\nS (T) to denote the smell violations of LLM M per solution w.r.t. a set S of smell detection rules and a set T of coding tasks, or simply violations per solution (VS). It is the average number of violations of the smell detection rules in S over the set of solutions generated by LLM M on coding tasks in T. Formally, we have that\n\nVS M\n\nS (T) =\n\n(cid:80)\n\nt∈T\n\n(cid:80)\n\ns∈S ∥Vs(M(t))∥\n\n∥T∥\n\nThe corresponding calculation for the baseline is as follows.\n\nDefinition 2. (Baseline’s Smell Violations per Solution)\n\nWe write VS B\n\nS (T) to denote the baseline’s smell violations per solution w.r.t. a set S of smell detection rules and a set T of coding tasks. This is the average number of violations of the smell detection rules in S over the reference solutions of the coding tasks in T. Formally, we have that\n\nVS B\n\nS (T) =\n\n(cid:80)\n\nt∈T\n\n(cid:80)\n\ns∈S ∥Vs(R(t))∥ ∥T∥\n\nSince the number of smell violations per solution is the only metric we are using to measure code smell, we will from now refer to it as the degree of code smell. Higher values mean poorer quality code. To compare the quality of LLM-generated code against a baseline, we will also measure the increase rate of code smells, as defined below.\n\nDefinition 3. (Increase Rate of Code Smells)\n\nThe increase rate of code smells for LLM model M with re- spect to the baseline on a set S of smell detection rules over a set T of coding tasks is denoted by IncM S (T), which is formally defined by the following equation.\n\nIncM\n\nS (T) =\n\nVS M\n\nS (T) − VS B VS B S (T)\n\nS (T)\n\nPositive values for this quantity mean that LLM-generated\n\ncode is of lower quality than the reference solution.\n\n6. The Results\n\nIn this section, we report the data collected from our experi- ments and answer each of the research questions with a statisti- cal analysis of the data.\n\n(1)\n\n□\n\n(2)\n\n□\n\n(3)\n\n□\n\n19.37827.57131.40631.78935.8440510152025303540ReferenceFalconGeminiProChatGPTCodexAverage Number of Smells Per Solution\n\n42.28%62.07%64.05%84.97%0%10%20%30%40%50%60%70%80%90%FalconGeminiProChatGPTCodexIncrease (%) of Code Smells w.r.t. Baseline\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\n19.37827.57131.40631.78935.8440510152025303540ReferenceFalconGeminiProChatGPTCodexAverage Number of Smells Per Solution\n\n42.28%62.07%64.05%84.97%0%10%20%30%40%50%60%70%80%90%FalconGeminiProChatGPTCodexIncrease (%) of Code Smells w.r.t. Baseline\n\n6.1. RQ1: Prevalence of Code Smells\n\nFirst, the experiment data shows that LLMs performed dif- ferently on the program topics. Although the strength of code smells for the baseline also vary over code topics, the LLMs have a higher standard deviation on VS values when compared to the baseline (1.37). The standard deviations for LLMs are 1.90 for Falcon, 1.96 for ChatGPT, 2.08 for Codex and 2.14 for Gemini Pro, respectively. On average over all LLMs, the standard deviation is 1.85, which is an increase of 31.86% com- pared to the baseline. This indicates that LLMs have a robust- ness problem in maintaining consistency in code quality. In this respect, Falcon is the best (1.90) and Gemini Pro the worst (2.14).\n\nResearch question RQ1 is concerned with the overall quality of the code generated by the LLMs. To answer this question, we calculated the VS on all smell detection rules over the whole test dataset for each LLM and compared it with the baseline. The results are shown in Fig. 3.\n\nObservation 3. Over different coding topics, the degree of code smells in LLM generated code varies significantly more than human written code.\n\n(a) Smell Violations per Solution\n\nSecond, not only does the degree of code smells vary over different code topics, there is a pattern to the variation. We analysed the Pearson correlation coefficients between the base- line VS values and those of LLM generated code over various code topics. We found that there is a strong correlation between them. The Pearson correlation coefficients are 0.6652 for Fal- con, 0.7249 for Gemini Pro, 0.7188 for ChatGPT and 0.7664 for Codex, respectively, and 0.7876 on average over all LLMs. Therefore, we have the following observation.\n\n(b) Increase Rate (%) of Code Smells\n\nObservation 4. The coding topics with strong code smells in human-written code are the same as the coding topics with strong code smells in LLM-written code.\n\nFigure 3: All Code Smells Detected on the Whole Test Dataset\n\nAs shown in Fig. 3(a), all four LLMs have stronger code smells than the baseline with Falcon performing the best (VS=27.571) and Codex the worst (VS=35.844). Moreover, the increase rate of code smells varies significantly in the range from 42.28% for Falcon to 84.97% for Codex. This is a strong evidence for the following observations.\n\nHowever, we found no strong correlation between the base- line VS values and the LLM’s increase rates of code smells. In fact, the Pearson correlation coefficients are negative, between -0.1592 and -0.3808.\n\nFinally, the data also shows that LLMs are more likely to worsen the code smells on more advanced coding topics. Table 7 shows the best and worst three topics as well as the most im- proved and worsened three topics by each LLM and on average over all studied LLMs. The best topics (i.e. the strength of code smells decreased) are Basic Exercise, String, DateTime; while the worst topics (i.e. the strength of code smells increased) are Searching and Sorting, Encapsulation, Inheritance, Polymor- phism, Interfaces, and Generics.\n\nObservation 1. LLM-generated code is of poorer quality in terms of the smell violations per solution when compared to the human-written code.\n\nObservation 2. The rate of increase in code smells of LLM generated code when compared to the human-written code varies significantly with the choice of LLM.\n\nThere are a few topics on which LLMs improved the code quality in terms of code smells. These include Regular Expres- sion by all LLMs and Enum improved by Gemini Pro, ChatGPT and Codex, Collections by Falcon and Gemini Pro, Interfaces and Lambda by Falcon and Methods by Codex, and DataType by ChatGPT. The highest improvement is 35.93% by Gemini Pro on the topic of Regular Expressions.\n\n6.2. RQ2: Variation of Code Smells by Topic\n\nResearch question RQ2 aims to identify the types of tasks for which LLM-generated code is of poor quality so that improve- ment can be directed to these tasks. To answer this question, we divide up the test dataset according to the topic of the code to be generated and then calculate the VS on subsets of these coding tasks with all smell detection rules.\n\nOn average over all LLMs studied, the most worsened topics are Encapsulation by 138.53%, Array by 101.88%, and OOP by 101.88%. The largest increase rate of code smell is 165.38% by ChatGPT on the topic of Encapsulation. Thus, we have the following observations.\n\nTable 6 presents the VS values for various programming top-\n\nics and the increase rates for each LLM model.\n\nFrom the experiment data, the following observations can be\n\nmade.\n\n10\n\nTopicBaselineFalconGeminiProChatGPTCodexAverageFalconGeminiProChatGPTCodexAverageBasic Exercise1.961.992.912.772.352.511.5348.4741.3319.9027.81DateTime2.182.592.853.623.333.1018.8130.7366.0652.7542.09String2.232.92.733.113.323.0230.0422.4239.4648.8835.20Array2.393.783.995.885.654.8358.1666.95146.03136.40101.88Input Output2.662.693.633.786.354.111.1336.4742.11138.7254.61Lambda2.712.413.954.164.333.71-11.0745.7653.5159.7836.99Collections2.912.562.585.643.683.62-12.03-11.3493.8126.4624.23Recursive Methods2.952.992.785.017.234.501.36-5.7669.83145.0852.63Thread2.963.574.595.746.225.0320.6155.0793.92110.1469.93Math2.973.354.534.995.154.5112.7952.5368.0173.4051.68DataType3.193.844.263.554.714.0920.3833.5411.2947.6528.21Methods3.333.943.784.773.94.1018.3213.5143.2417.1223.05OOP3.426.737.924.887.996.8896.78131.5842.69133.63101.17Exception Handling3.453.225.445.395.314.84-6.6757.6856.2353.9140.29Encapsulation3.649.917.959.667.218.68172.25118.41165.3898.08138.53Conditional3.733.483.846.364.984.67-6.702.9570.5133.5125.07Data Structure4.344.264.765.238.125.59-1.849.6820.5187.1028.86Enum4.564.53.884.384.664.36-1.32-14.91-3.952.19-4.50Generics5.465.739.427.958.357.864.9572.5345.6052.9344.00Interfaces5.574.276.758.719.217.24-23.3421.1856.3765.3529.89Regular Expression5.655.143.624.325.644.68-9.03-35.93-23.54-0.18-17.17Abstract Classes5.666.317.627.697.287.2311.4834.6335.8728.6227.65Searching & Sorting5.676.28.628.348.617.949.3552.0347.0951.8540.08Polymorphism5.726.916.748.448.487.6420.8017.8347.5548.2533.61Inheritance6.917.598.968.9810.489.009.8429.6729.9651.6630.28Average3.854.434.935.736.105.3015.2228.0148.9858.5337.69Std dev1.371.902.141.962.081.8539.5837.2439.5341.5931.86Average Number of Voilations per SolutionIncrease (%)\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\nTable 6: Code Smell by Code Topics\n\nTable 7: The Best/Worst Topics And The Most Improved/Worsened Topics\n\nModel\n\nBest Topics (VS) Basic Exercise (1.96) DateTime (2.18) String (2.23) Basic Exercise (1.99) Lambda (2.41) Collections (2.56) Collections (2.58) String (2.73) Recursive Methods (2.78) Basic Exercise (2.77) String (3.11) DataType (3.55) Basic Exercise (2.35) String (3.32) DateTime (3.33) Basic Exercise (2.51) String (3.02) DateTime (3.10)\n\nWorst Topics (VS) Searching & Sorting (5.67) Polymorphism (5.72) Inheritance (6.91) Polymorphism (6.91) Inheritance (7.59) Encapsulation (9.91) Searching & Sorting (8.62) Inheritance (8.96) Generics (9.42) Interfaces (8.71) Inheritance (8.98) Encapsulation (9.66) Searching & Sorting (8.61) Interfaces (9.21) Inheritance (10.48) Searching & Sorting (7.94) Encapsulation (8.68) Inheritance (9.00)\n\nMost Improved Toipics (Inc %) Most Worsened Topics (Inc %) N/A N/A N/A Interfaces (-23.34) Collections (-12.03) Lambda (-11.07) Regular Expression (-35.93) Enum (-14.91) Collections (-11.34) Regular Expression (-23.54) Enum (-3.95) DataType (11.29) Regular Expression (-0.18) Enum (2.19) Methods (17.12) Regular Expression (-17.17) Enum (-4.50) Methods (23.05)\n\nN/A N/A N/A Array (58.16) OOP (96.78) Encapsulation (172.25) Generics (72.53) Encapsulation (118.41) OOP (131.58) Thread (93.92) Array (146.03) Encapsulation (165.38) Array (136.40) Input Output (138.72) Recursive Methods (145.08) OOP (101.17) Array (101.88) Encapsulation (138.53)\n\nBaseline\n\nFalcon\n\nGemini Pro\n\nChatGPT\n\nCodex\n\nAverage\n\nThree different complexity metrics were used: cyclomatic com- plexity, cognitive complexity and lines of code. The results are presented in Fig. 4 graphically.\n\nObservation 5. The LLM-generated code has the best quality on basic coding topics, and the worst on advanced coping top- ics.\n\nFrom Fig. 4, it can be seen clearly that the VS tends to in- crease with each of three different metrics of complexity. This is confirmed by the Pearson Correlation coefficients between VS and complexity; see Table 8. For cyclomatic complexity, the Pearson correlation coefficients are all strongly positive (above 0.9) for each LLM as well as the baseline. For lines of code, the coefficients are a bit lower but still very high (in the range between 0.8685 and 0.9952 except ChatGPT (0.6347). For cog- nitive complexity, the results are mixed: very strong for Falcon (0.9754), but much lower for ChatGPT (0.3025), and around 0.6 for Gemini Pro and Codex, and 0.6894 on average over all LLMs.\n\nObservation 6. The LLM-generated code can have better quality in comparison with human written code on certain cod- ing topics, while it can also have significantly worse code qual- ity, especially on advanced coding topics.\n\n6.3. RQ3: Variation of Code Smells by Complexity\n\nResearch question RQ3 is concerned with how code quality varies with the complexity of the coding tasks. To answer this question, we calculated the VS on subsets of test cases that were formed according to the complexity of the coding tasks. Here, the complexity of a coding task is measured on the complexity of the reference solution provided by the benchmark ScenEval.\n\nTherefore, we have the following observations.\n\n11",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 12-17)",
      "start_page": 12,
      "end_page": 17,
      "detection_method": "topic_boundary",
      "content": "012345678901234567Code Smell (VS)Cyclomatic ComplexityCode Smell by Cyclomatic Complexity\n\nBaseline\n\nFalcon\n\nGeminiPro\n\nChatGPT\n\nCodex\n\n012345678901234567Code Smell (VS)Cognitive ComplexityCode Smell by Cognitive Complexity\n\n0123456789101-5051-100101-150151-200201-250251-300Code Smell (VS)Lines of CodeCode Smell by Lines of Code\n\n203080130180230123456Increase w.r.t. Baseline (%)Cognitive Complexity\n\n20020406080100120140160180123456Increase w.r.t. Baseline (%)Cyclomatic Complexity\n\nFalcon\n\nGemini Pro\n\nChatGPT\n\nCodex\n\n01020304050607080901-5051-100101-150151-200Increase w.r.t. Baseline (%)Lines of Code\n\n012345678901234567Code Smell (VS)Cyclomatic ComplexityCode Smell by Cyclomatic Complexity\n\nBaseline\n\nFalcon\n\nGeminiPro\n\nChatGPT\n\nCodex\n\n012345678901234567Code Smell (VS)Cognitive ComplexityCode Smell by Cognitive Complexity\n\n0123456789101-5051-100101-150151-200201-250251-300Code Smell (VS)Lines of CodeCode Smell by Lines of Code\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\nTable 8: Correlations Bwt VS and Coding Task Complexities\n\nComplexity Baseline Falcon GeminiPro ChatGPT Codex Average 0.9962 0.9653 0.9344 0.9555 Cyclomatic 0.6578 0.6894 Cognitive 0.8800 0.9754 0.8900 0.8685 Lines of Code 0.9952 0.8694\n\n0.9916 0.6312 0.9532\n\n0.9485 0.3025 0.6347\n\n012345678901234567Code Smell (VS)Cyclomatic ComplexityCode Smell by Cyclomatic Complexity\n\nBaseline\n\nFalcon\n\nGeminiPro\n\nChatGPT\n\nCodex\n\n012345678901234567Code Smell (VS)Cognitive ComplexityCode Smell by Cognitive Complexity\n\n0123456789101-5051-100101-150151-200201-250251-300Code Smell (VS)Lines of CodeCode Smell by Lines of Code\n\nFigure 5: Increase Rates of Code Smells by Complexity\n\nspecific type of code smell on the whole test dataset and com- pared with the baseline.\n\nTable 9 presents, in the form of a heat map, the VS for each specific type of smells as well as the increase rates for each LLM model in comparison with the baseline. The highest VS scores and the largest (i.e. worst) increase rate are highlighted in red, while the lowest VS scores and lowest increase rates are coloured in blue. Implementation smells are listed in the top half and design smells in the bottom half.\n\nFigure 4: Variation of Code Smells by Complexity\n\nObservation 7. For both human written code and LLM gener- ated code, the strength of code smells increases with the com- plexity of coding tasks.\n\nIt is worth noting that the baseline VS value also increases with cyclomatic complexity but it does so at a slower rate than any of the four LLMs. So, we can see that LLMs tend to strug- gle to produce good quality code when they are given highly complex tasks.\n\nFrom the experiment data, we observed the following phe-\n\nnomena.\n\nObservation 9. The least prevalent types of implementation smells for all LLMs as well as the reference solutions are Incompleteness, Inconsistent Naming Convention and Redun- dancy.\n\nHowever, by analysing the increase rate of code smells in the code generated by LLMs with respect to human written code, we found no obvious link between the complexity of coding tasks to the increase rate of code smells as shown in Fig. 5.\n\nObservation 10. The worst types of implementation smells for all LLMs and human written code are Magic Number, Docu- mentation and Improper Alignment and Placement.\n\nTherefore, we have the following observation.\n\nObservation 8. There is no clear evidence that the increase rate of code smells in LLM generated code is correlated to the complexity of coding task.\n\nIn general, there is a very strong correlation between LLM generated code and human written code on the VS scores on the types of code smells. As shown in Table 10, for implemen- tation types of code smells, the Pearson correlation coefficients between the baseline and the code generated by each LLM are all very close to 1. For design code smells, the Pearson correla- tion coefficients are in the range between 0.7263 for Falcon and\n\n6.4. RQ4: Variation of Code Smells by Smell Types\n\nResearch question RQ4 aims to identify the specific quality issues in LLM generated codes. We calculated the VS for each\n\n12\n\nChatGPTGeminiProFalconCodexAverageImpl Smells0.99870.99900.99610.99730.9980Design Smells0.87570.87660.72630.87480.8506\n\nSmell TypeReferenceChatGPTGeminiProFalconCodexAverageChatGPTGeminiProFalconCodexAverageIncompleteness0.00300.01400.01600.00800.01500.0133366.67433.33166.67400.00341.67Inconsistent Naming Convention0.00400.05000.05100.02600.05000.04431150.001175.00550.001150.001006.25Redundancy0.01800.07100.07800.04100.07100.0653294.44333.33127.78294.44262.50Dead Code0.08600.05800.06400.08600.05600.0660-32.56-25.580.00-34.88-23.26Resource Handling0.56500.58400.64200.56500.59500.59653.3613.630.005.315.58Excessive Complex0.65301.15101.35300.66101.27401.109876.26107.201.2395.1069.95Magic Number0.85101.37401.52300.85701.54601.325061.4678.970.7181.6755.70Documentation2.31403.25003.31702.31403.28103.040540.4543.340.0041.7931.40Improper Alignment / Placement11.150020.499019.560019.189024.185020.858383.8575.4372.10116.9187.07Average of Impl Smells15.644027.051026.604023.747031.073027.118872.9270.0651.8098.6373.35StDev of Impl Smells3.60496.64296.31996.24917.84846.7634370.61379.28179.61368.75324.12Hierarchy0.00000.00200.00200.00200.00200.0020N/AN/AN/AN/AN/AAbstraction0.00001.02701.03801.06301.03501.0408N/AN/AN/AN/AN/AModularity1.82401.91801.94601.54401.93801.83655.156.69-15.356.250.69Encapsulation1.91001.79101.81601.21501.79601.6545-6.23-4.92-36.39-5.97-13.38Average of Design Smells3.73404.73804.80203.82404.77104.533826.8928.602.4127.7721.42StDev of Design Smells1.07850.88110.89390.66690.88730.82768.058.2114.888.649.94Average of All Smells19.378031.789031.406027.571035.844031.652564.0562.0742.2884.9763.34StDev of All Smells3.01725.51175.24775.17686.51465.610982.6773.9271.57115.9185.96Avergae Numbers of Violations per Solution (VS)Increase in VS (%)\n\nChatGPTGeminiProFalconCodexChatGPTGemini Pro0.9985Falcon0.99310.9884Codex0.99920.99840.9926\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\nTable 9: Code Smells by Smell Type\n\nChatGPTGeminiProFalconCodexAverageImpl Smells-0.2257-0.2549-0.1518-0.2064-0.2188Design Smells-1-1-1-1-1\n\nTable 10: Pearson Correlation Coefficients btw Baseline and LLMs’ VS Scores on Various Smells Types\n\nall very close to 1; See Table 12. This implies the following observation.\n\nObservation 13. LLMs consistently increase the strength of code smells over various types of code smells.\n\nTable 12: Pearson Correlation Coefficients btw LLMs’ Increase Rates of Implementation Smells\n\n0.8766 for Gemini Pro. Therefore, we can confidently conclude that:\n\nObservation 11. The prevalence of code smells in LLM- generated code on various smell types is strongly correlated with that of human-written code.\n\nThe experiment data also demonstrated that the prevalence of a type of code smell in human written code does not imply that the smell increases in LLM generated code. As shown in Table 11, for implementation smells, the Pearson correlation coeffi- cients between LLMs’ smell increase rates and the VS scores of the baseline are all negative in the range between -0.2549 for Gemini Pro and -0.1518 for Falcon, where the average over all LLMs is -0.2188.\n\nFrom the experiment data, we can also have the following\n\nobservations.\n\nObservation 14. All LLMs performed well on the encapsula- tion type of design smells in comparison with the baseline.\n\nObservation 15. LLMs’ performance on design smells vary significantly with increase rates ranging from 2.41% for Fal- con to 28.60% for Gemini Pro, and the average increase rate over all LLMs is 21.42%. Falcon performed better on design smells than the other LLMs.\n\nTable 11: Pearson Correlation Coefficients btw Baseline VS Scores and LLMs’ Increase Rates on Various Smells Types\n\nFinally, by analysing the distributions of VS scores for each\n\ntypes of smells, we have the following observation.\n\nHowever, it is observable that the largest increase rates of code smells are on the types that are the least prevalent smell types of the baseline.\n\nObservation 16. For each type of smells, the violations of smell detection rules are concentrated in a small number of spe- cific smells.\n\nObservation 12. The largest increases of smells in LLM gen- erated code happened at the smell types of Incompleteness, In- consistent Naming Convention and Redundancy.\n\nTable 13 lists the most prevalent smells in each smell type, where column Top Smell(s) lists the most prevalent smell(s) of the smell type given in the column Smell Type. Column #Vio- lations gives the average numbers of violations of the specific smell rule over all LLMs. Column Weight gives the ratio of the\n\nFinally, the Pearson correlation coefficients between LLMs’ increase rates over various types of implementation smells are\n\n13\n\nSmell Type#VoilationsWeight (%)BasekineInconsistentNaming Convention44.25100.0041105.2599.596533.250.29059.591.19185.758.8102073299.3911119460.22039.750.19610.2577.3621.511.321Magic Number1325100.00851Dead Code66100.0086572.595.98539244.0226270588.972103224.257.3816698.53.2439917.7549.97912.00918.7550.03912.001490.2590.071730.00118.507.16114.00Hierarchy2.00100.000.001036.5099.590.003.750.360.00Imperative AbstractionModularityEncapsulationAbstractionUse Utility ClassFinal ParametersBroken HierarchyUnutilized AbstractionHide Utility Class ConstructorHidden FieldRedundancyImproper Alignment / PlacementIncompletenessResource HandlingDocumentationClose ResourceAvoid Instantiating Objects In LoopsComment RequiredJavadoc VariableComment SizeVariable Declaration Usage DistanceMissing Switch DefaultEmpty Catch BlockMagic NumberUnused ImportsRedundant ModifierRedundant ImportIndentationFileTabCharacterAbbreviation As Word In NameTop Smell(s)Simplify Boolean ExpressionLine LengthExcessive Complex\n\n7436786804700100200300400500600700800Gemini ProCodexChatGPTFalconNumber of Coding TasksNumber of Correct Solutions Generated by LLMs\n\nCorrect IncorrectCorrect IncorrectCorrect IncorrectChatGPT31.3332.9326.6328.114.704.82Gemini Pro30.6333.8825.8629.004.774.88Falcon32.0523.6827.8720.174.183.51Codex35.6436.4430.8731.664.774.78Average32.4131.7327.8127.244.604.50All SmellsImpl SmellsDesign SmellsLLM\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\nLLMAll SmellsImpl SmellsDesign SmellsChatGPT5.135.572.63Gemini Pro10.6212.162.29Falcon-26.11-27.63-15.93Codex2.232.550.17\n\nTable 13: The Most Prevalent Smells in Each Type\n\nFig. 6 shows the number of LLM generated programs that pass all tests. From the data shown in the figure, Gemini Pro performed the best with a success rate of 74.3% passing all tests, while Falcon is the worst with a success rate only 47.0%. Codex and ChatGPT performed very similar, both have a suc- cess rate around 68.0%.\n\nFigure 6: Numbers of LLM Generated Solutions Passed Test\n\nTo analyse how correctness is related to code smell, we split the test dataset into two subsets: one contains coding tasks that the LLM generated a correct code; the other contains tasks that LLM failed to generate a correct model. The smell violations per solution are calculated for each subset on all code smell detection rules. The results are shown in Table 14.\n\nviolations over all smells of the type. Column Baseline gives the number of violations in the reference solutions.\n\nTable 14: Smells of Correct and Incorrect Codes\n\nNote that there are fewer violations of design smells than im- plementation smells. We believe that there are two reasons for this. First, the test dataset contains very few coding tasks where the solutions require a large number of classes. In fact, only 68 (6.8% of the dataset) require more than one class. Design smells like Hierarchy smells do not present in code that has only one class. Second, there are less detection rules for design smells than those for implementation smells. However, fewer violations of design smells do not imply the better design qual- ity because design smells are at a higher level of abstraction and of greater granularity. Each violation of design smell could have a more serious impact than one violation of an implemen- It is not meaningful to compare the number of tation smell. violations of design smells to that of implementation smells.\n\nFrom Table 14, we can observe that correct code has less smells than incorrect code on average for three out of four LLMs: Gemini Pro, Codex and ChatGPT. However, Falcon is an exception. Its incorrect code has less smells than its correct code. Table 15 shows the rates of increase (%) in code smells from correct code to the incorrect code for different LLMs.\n\n6.5. RQ5: Variation of Code Smells by Correctness\n\nTable 15: Increases (%) in Smells from Correct to Incorrect Codes\n\nResearch question RQ5 aims to understand how the correct-\n\nness of LLM generated code relate to code smells.\n\nTo determine the correctness of a LLM generated solution, test cases were generated from both the reference solution and the LLM-generated code by employing the EvoSuite tool. These two sets of test cases are merged into one test suite. Both the reference solution and the generated code are tested on the test suite. If the reference solution fails on a test case that is generated from the LLM generated code, a commission error is detected. If the generated code fails on a test case that is generated from the reference solution, an omission error is de- tected. If neither a commission error nor an omission error are detected, i.e. it passes all of the tests, we regard the LLM gen- erated code is correct. Readers are referred to [4] for details about how this is conducted.\n\nFrom Table 15, we have the following observation.\n\nObservation 17. The degree of differences between the cor- rect and incorrect codes in terms of the strength of code smells varies significantly with the LLM models. For some LLMs, in- correct codes are more smelly; while for the others, the opposite is observable.\n\n14\n\nVersion 4.1; October 6, 2025.\n\n7. Discussion on Threats to Validity\n\nIn this section, we discuss the potential threats to the valid- ity of the experiment reported in the previous section and how these threats were addressed in the design and conduct of the experiment. We will also discuss how to further reduce the threats in future work. We will apply Wohlin et al.’s the frame- work [44] of classifying the threats to validity in software engi- neering experiments, as it is among the frameworks most used by the researchers in software engineering.\n\n7.1. Construct Validity\n\nConstruct validity is concerned with whether the data ob- tained by measurement and observation correctly and ade- quately represent the abstract concepts under study. In our con- text, it means whether the code smell detection rules correctly, adequately and fairly represent the quality aspects related to the readability, maintainability, ease of evolution, etc.\n\nOne primary threat to this validity lies in the reliance on the accuracy and coverage of the static analysis tools used in our study. Any limitations or inaccuracies in these tools could af- fect the precision of our smell detection. To mitigate this threat, we selected widely used and validated tools (i.e., PMD, Check- style, and DesigniteJava) that have demonstrated reliability in prior research and practice.\n\nAlso, we focused solely on code smells detectable by PMD, Checkstyle, and DesigniteJava. While this may exclude certain types of smells, the selected set of detection rules represents a well-established and widely adopted list of code smells. These have been well documented and widely used both in academic research and industry practice, lending credibility to their rel- evance and maturity. Moreover, we have combined the smell detection rules provided by these tools to maximised the cover- age of the smells.\n\n7.2. External Validity\n\nThe external validity is concerned with to what extent the re- sults of an experiment can be generalised. A potential threat to external validity involves the generalisability of our findings to other LLMs not studied in our experiments, coding in program- ming languages other than Java, and those types of coding tasks not covered by the test dataset.\n\nOur analysis is based exclusively on Java programs from the ScenEval dataset, which may limit the applicability of the re- sults to code generation tasks in other programming languages. Additionally, we evaluated outputs from some of the most widely used generative models GeminiPro, ChatGPT, Codex, and Falcon in Table 4. Other versions of these ML models and other ML models, such as CodeBERT [45] and CodeT5 [46], were not studied. Therefore, the results may not gener- alise across all generative coding models. However, there are observations that are consistent on all LLMs that we studied. These observations should be able to generalise to other ML models.\n\nOur dataset covers a wide range of topics. However, the ma- jority of coding tasks are of small scale in terms of complexity. Moreover, very few of the coding tasks require multiple classes.\n\n15\n\nD. G. Paul, H. Zhu and I. Bayley\n\nThe conclusions drawn from our experiment should be limited to the coding tasks well represented by our dataset. Any gen- eralisation of our conclusions to other kinds of coding tasks should be used with great care.\n\n7.3. Internal Validity\n\nInternal validity is concerned with the appropriateness of the design and conduct of the experiments. A typical example of the threats to internal validity is the existence of factors that influence the causal relationships under study but are not mea- sured and are not under our control.\n\nA potential threat to the internal validity of our experiment is that LLMs are inherently nondeterministic. For this reason, we have selected a large number of test cases (1000) at random to minimise the impact of LLM’s randomness. The scale of our experiment is much larger than most of the studies of LLMs’ capability in code generation. For future research, this threat to internal validity can be further reduced by using even more test cases and repeating the invocations of LLMs on each coding task.\n\nAnother potential threat to internal validity is that the qual- ity of program code in general and code smells in particular are very subjective as we discussed in Section 1 and 3. We have addressed this threat at the methodology level by excluding hu- man factors from the experiment by using a baseline formed by professionally written code and at the technology level by em- ploying the quantitative analysis of the experiment data using objective metrics.\n\nFinally, a potential threat to internal validity is that the im- plementation of the test system may contain bugs, thus the data collected may have errors. We have addressed this threat by careful testing and debugging of the test system. Moreover, to ensure the experimental reproducibility, the source code of the test system as well as the data are available to the public in the GitHub repository 5.\n\n7.4. Conclusion Validity\n\nConclusion validity is concerned with whether the conclu- sion drawn from the experiment is logically valid, such as whether correct statistical inference methods are used properly and whether the statistical inference power is strong enough.\n\nDue to the fact that the experiments with LLMs are time con- suming and resource demanding, the statistical inference power in this work is not ideal because the scale of our experiment is still limited. However, it is already much larger than other ex- isting similar works. We believe it is not a serious threat to conclusion validity. For future work, repeating the experiments with a larger test dataset will further reduce the threat.\n\n8. Conclusion and Future Work\n\nIn this paper, we proposed a scenario-based methodology to evaluate the usability of LLM-generated code on readability,\n\n5URL: https://github.com/hongzhu6129/EvaluateLLMCodeSmell\n\nVersion 4.1; October 6, 2025.\n\nmodifiability, reusability, ease of maintenance, ease of evolu- tion, etc., through assessing the code smells and comparing with a baseline obtained from code written by professional pro- grammers. An automated test system is designed and imple- mented in the datamorphic testing method. An intensive ex- periment with four prominent LLMs is conducted using the ScenEval benchmark for generating Java code.\n\nWe have found that the code smells, measured by the av- erage number of violations of code smell detection rules per solution, is significantly greater in LLM-generated code com- pared to human-written code. Across all LLMs, the average increase rates of implementation and design smells are 73.35% and 21.42%, respectively, while the average increase rate over all smells is 63.34%.\n\nThe performances of LLMs vary significantly over different topics of coding tasks and smell types. In general, the more complicated a coding task is, the stronger is the smell in LLM- generated code. The types of code smells that are the strongest in human written code are also the most prevalent in LLM gen- erated code. However, the increase rates of code smell types in LLM generated code show no correlation to the prevalence of the type of code smell in human written code. In general, the quality of generated code decreases with the complexity of the code task. This correlation is very clear when coding task com- plexity is measured by cyclomatic complexity and the lines of code, but is slightly less clear with cognitive complexity.\n\nFor future work, it is worth further expanding and repeating the experiments with more LLM models and using larger test dataset to reduce the risks of the potential threats to validity as discussed in Section 7.\n\nAs discussed in Section 3, it is difficult to set a threshold on the number of violations for the code to be of acceptable qual- ity. We could only compare with the baseline number, which reflects current practice. Hence, from our experiment data, we have difficulty to answer the question: is the quality of LLM generated code acceptable for use? We encourage further re- search in this area.\n\nMoreover, LLM-generated code has broader quality aspects of usability that are worth considering, for example, security and runtime efficiency of the generated code, which is ad- dressed by a recent work by Jonnala et al. [47]. This way we hope to provide a more holistic evaluation of LLM performance for code generation.\n\nFinally, it is worth investigating how code smell detection can be used to improve the quality of LLM generated code in a multi-attempt process proposed by Miah and Zhu [17].\n\nAcknowledgement\n\nThe research work reported in this paper is partly funded by Google’s PaliGemma Academic Program, which provided credits for using Google Cloud Platform resources.\n\nReferences\n\n[1] I. Shani, Survey reveals AI’s impact on the developer experience, GitHub Blog, https://github.blog/news-insights/research/\n\n16\n\nD. G. Paul, H. Zhu and I. Bayley\n\nsurvey-reveals-ais-impact-on-the-developer-experience/ (2024).\n\n[2] D. DeBellis, K. M. Storer, A. Lewis, B. Good, D. Villalba, E. Maxwell, K. Castillo, M. Irvine, N. Harvey, Accelerate state of DevOps, DORA 2024 Report, Google Cloud. https://cloud.google.com/blog/ products/devops-sre/announcing-the-2024-dora-report (2024).\n\n[3] W. Harding, AI copilot code quality: Evaluating 2024’s increased defect rate via code quality metrics, Tech. Rep., Alloy.dev Research, 211m Lines of Analyzed Code + Projections for 2025, (Feb. 2025).\n\n[4] D. G. Paul, H. Zhu, I. Bayley, ScenEval: A benchmark for scenario-based evaluation of code generation, in: Proc. of 2024 IEEE Int’l Conf. on Ar- tificial Intelligence Testing (AITest 2024), July 2024, pp. 55–63.\n\n[5] A. Zdravkovic, AMD takes holistic approach to AI coding copilots: The chipmaker is using ai throughout the software-development life cycle, IEEE Spectrum 62 (6), pp26–31, (2025)\n\n[6] B. K. Deniz, C. Gnanasambandam, M. Harrysson, A. Hussin, S. Srivas- tava, Unleashing developer productivity with generative AI, McKinsey Digital, 7 (2023).\n\n[7] A. Ziegler, E. Kalliamvakou, X. A. Li, A. Rice, D. Rifkin, S. Simister, G. Sittampalam, E. Aftandilian, Measuring github copilot’s impact on productivity, Communications of the ACM, 67 (3) pp.54–63, (2024).\n\n[8] K. Beck, M. Fowler, Bad smells in code, Ch. 3, in: [12], pp. 75–88.\n\n[9] G. Suryanarayana, G. Samarthyam, T. Sharma, Refactoring for Software Design Smells: Managing Technical Debt, Elsevier Science & Technol- ogy, 2014.\n\n[10] T. Sharma, M. Fragkoulis, D. Spinellis, Does your configuration code smell?, in: Proc. of 2016 IEEE/ACM 13th Working Conference on Min- ing Software Repositories (MSR 2016), IEEE, Athens, Greece, 2016, pp. 189–200.\n\n[11] G. Lacerda, F. Petrillo, M. Pimenta, Y. G. Guéhéneuc, Code smells and refactoring: A tertiary systematic review of challenges and observations, Journal of Systems and Software, 167, 110610, (2020)\n\n[12] M. Fowler, Refactoring: improving the design of existing code, Addison-\n\nWesley Professional, 2018.\n\n[13] J. A. M. Santos, J. B. Rocha-Junior, L. C. L. Prates, R. S. Do Nascimento, M. F. Freitas, M. G. De Mendonça, A systematic review on the code smell effect, Journal of Systems and Software, 144 pp450–477, (2018)\n\n[14] H. Zhu, D. Liu, I. Bayley, R. Harrison, F. Cuzzolin, Datamorphic testing: A method for testing intelligent applications, in: 2019 IEEE International Conference On Artificial Intelligence Testing (AITest 2019), IEEE, 2019, pp. 149–156.\n\n[15] H. Zhu, I. Bayley, D. Liu, X. Zheng, Automation of datamorphic test- ing, in: Proc. of 2020 IEEE Int’l Conf. On Artificial Intelligence Testing (AITest 2020), pp. 64–72, (2020)\n\n[16] D. G. Paul, H. Zhu, I. Bayley, Benchmarks and metrics for evaluations of code generation: A critical review, in: Proc. of 2024 IEEE International Conference on Artificial Intelligence Testing (AITest 2024), pp. 87–94 (2024).\n\n[17] T. Miah, H. Zhu, User centric evaluation of code generation tools, in: 2024 IEEE International Conference on Artificial Intelligence Testing (AITest 2024), pp. 109–119, IEEE, (2024)\n\n[18] M. L. Siddiq, S. H. Majumder, M. R. Mim, S. Jajodia, J. C. S. Santos, An empirical study of code smells in transformer-based code generation techniques, in: Proc. of 2022 IEEE 22nd Int’l Working Conf. on Source Code Analysis and Manipulation (SCAM 2022), IEEE, USA, 2022.\n\nVersion 4.1; October 6, 2025.\n\n[19] S. Thénault, et al., Pylint: Code analysis for Python. Accessed: 2025-04-\n\n11 (2001).\n\n[20] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang, et al., Codexglue: A machine learning benchmark dataset for code understanding and generation, arXiv preprint arXiv:2102.04664 (2021).\n\n[21] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, et al., Measuring coding challenge competence with apps, arXiv preprint arXiv:2105.09938 (2021).\n\n[22] N. Coooper, A. Arutiunian, S. Hincapié-Potes, B. Trevett, A. Raja, E. Hossami, M. Mathur, et al., Code clippy data: A large dataset of code data from GitHub for research into code language models (Oct. 2021). URL :https://github.com/CodedotAl/gpt-code-clippy/wiki/ Dataset\n\n[23] Bandit, Bandit: Security linter for Python code, https://bandit.\n\nreadthedocs.io/, accessed: 2025-04-11.\n\n[24] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Ed- wards, Y. Burda, N. Joseph, G. Brockman, et al., Evaluating large lan- guage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\n\n[25] K. Moratis, T. Diamantopoulos, D.-N. Nastos, A. Symeonidis, Write me this code: An analysis of chatgpt quality for producing source code, in: Proc. of the 21st IEEE/ACM Int’l Conf. on Mining Software Repositories (MSR 2024), Thessaloniki, Greece, IEEE/ACM, 2024.\n\n[26] T. Copeland, PMD Applied, Vol. 10, Centennial Books, San Francisco,\n\n2005.\n\n[27] K. DePalma, I. Miminoshvili, C. Henselder, K. Moss, E. A. Al Omar, Exploring ChatGPT’s code refactoring capabilities: An empirical study, Expert Systems with Applications, 249 (Part B) 123602, (2024)\n\n[28] Y. Liu, T. Le-Cong, R. Widyasari, C. Tantithamthavorn, L. Li, X. D. Le, D. Lo, Refining ChatGPT-generated code: Characterizing and mitigat- ing code quality issues, ACM Transactions on Software Engineering and Methodology, 33 (5) pp.1–26, (2024)\n\n[29] O. Burn, Checkstyle, http://checkstyle.sourceforge.net/, ac-\n\ncessed: 2025-04-11 (2003).\n\n[30] I. Cordasco, T. Ziade, Flake8: Your tool for style guide enforcement,\n\nPrograma de computador, accessed: 2025-04-11 (2010).\n\n[31] Z. Fan, X. Gao, M. Mirchev, A. Roychoudhury, S. H. Tan, Automated repair of programs from large language models, in: Proc. of 2023 IEEE/ACM 45th Int’l Conf. on Software Engineering (ICSE 2023), pp. 1469–1481, IEEE, 2023.\n\n[32] W. Ma, S. Liu, Z. Lin, W. Wang, Q. Hu, Y. Liu, C. Zhang, L. Nie, L. Li, Y. Liu, LLMs: Understanding code syntax and semantics for code analy- sis, arXiv preprint arXiv:2305.12138 (2023).\n\n[33] E. V. de Paulo Sobrinho, A. De Lucia, M. de Almeida Maia, A systematic literature review on bad smells–5 w’s: which, when, what, who, where, IEEE Transactions on Software Engineering, 47 (1) pp.17–66, (2018)\n\n[34] T. Sharma, D. Spinellis, A survey on software smells, Journal of Systems\n\nand Software, 138 , pp.158–173, (2018)\n\n[35] E. Fernandes, J. Oliveira, G. Vale, T. Paiva, E. Figueiredo, A review-based comparative study of bad smell detection tools, in: Proc. of the 20th int’l conf. on evaluation and assessment in software engineering, 2016, pp. 1–12.\n\n[36] M. S. Haque, J. Carver, T. Atkison, Causes, impacts, and detection ap- proaches of code smell: a survey, in: Proc. of the 2018 ACM Southeast Conference, 2018, pp. 1–8.\n\n17\n\nD. G. Paul, H. Zhu and I. Bayley\n\n[37] R. S. Menshawy, A. H. Yousef, A. Salem, Code smells and detection tech- niques: a survey, in: Proc. of 2021 int’l mobile, intelligent, and ubiquitous computing conference (MIUCC 2021), IEEE, 2021, pp. 78–83.\n\n[38] A. Al-Shaaby, H. Aljamaan, M. Alshayeb, Bad smell detection using ma- chine learning techniques: a systematic literature review, Arabian Journal for Science and Engineering, 45 (4) pp.2341–2369, (2020)\n\n[39] A. Alazba, H. Aljamaan, M. Alshayeb, Deep learning approaches for bad smell detection: a systematic literature review, Empirical Software Engi- neering, 28 (3), 77, (2023)\n\n[40] F. Pecorelli, F. Palomba, D. D. Nucci, A. D. Lucia, Comparing heuristic and machine learning approaches for metric-based code smell detection, in: Proc. of the 2019 IEEE/ACM 27th Int’l Conf. on Program Compre- hension (ICPC 2019), Montreal, Canada, 2019, pp. 1–11, IEEE.\n\n[41] D. Mesbah, N. E. Madhoun, K. A. Agha, H. Chalouati, Leveraging prompt-based large language models for code smell detection: A com- parative study on the mlcq dataset, in: Proc, of The 13th Int’l Conf. on Emerging Internet, Data & Web Technologies (EIDWT-2025), Springer, Matsue, Japan, 2025, pp. 444–454.\n\n[42] PMD, PMD: A source code analyzer, https://pmd.github.io/, ac-\n\ncessed: 2025-02-26.\n\n[43] Checkstyle, Checkstyle: A development tool to help you write Java code that follows a coding standard, https://checkstyle.sourceforge. io/, accessed: 2025-02-26.\n\n[44] C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, B. Regnell, A. Wesslén,\n\net al., Experimentation in software engineering, Springer, 2012.\n\n[45] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, et al., CodeBert: A pre-trained model for programming and natural languages, arXiv preprint arXiv:2002.08155 (2020).\n\n[46] Y. Wang, W. Wang, S. Joty, S. C. Hoi, CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and genera- tion, arXiv preprint arXiv:2109.00859 (2021).\n\n[47] R. Jonnala, J. Yang, Y. Lee, G. Liang, Z. Cao, Measuring and improving the efficiency of Python code generated by LLMs using CoT prompting and fine-tuning, IEEE Access (2025).",
      "page_number": 12
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "5 2 0 2\n\nt c O 3\n\n] E S . s c [\n\n1 v 9 2 0 3 0 . 0 1 5 2 : v i X r a\n\nInvestigating The Smells of LLM Generated Code⋆\n\nDebalina Ghosh Paul, Hong Zhu∗, Ian Bayley\n\nSchool of Engineering, Computing and Mathematics, Oxford Brookes University, Oxford, OX3 0BP, UK\n\nAbstract\n\nContext:\n\nLarge Language Models (LLMs) are increasingly being used to generate program code. Much research has been reported on the\n\nfunctional correctness of generated code, but there is far less on code quality. Objectives:\n\nIn this study, we propose a scenario-based method of evaluating the quality of LLM-generated code to identify the weakest\n\nscenarios in which the quality of LLM generated code should be improved. Methods:\n\nThe method measures code smells, an important indicator of code quality, and compares them with a baseline formed from reference solutions of professionally written code. The test dataset is divided into various subsets according to the topics of the code and complexity of the coding tasks to represent different scenarios of using LLMs for code generation. We will also present an automated test system for this purpose and report experiments with the Java programs generated in response to prompts given to four state-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon. Results:\n\nWe find that LLM-generated code has a higher incidence of code smells compared to reference solutions. Falcon performed the least badly, with a smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%) and finally Codex (84.97%). The average smell increase across all LLMs was 63.34%, comprising 73.35% for implementation smells and 21.42% for design smells. We also found that the increase in code smells is greater for more complex coding tasks and for more advanced topics, such as those involving object-orientated concepts. Conclusion:\n\nIn terms of code smells, LLM’s performances on various coding task complexities and topics are highly correlated to the quality of human written code in the corresponding scenarios. However, the quality of LLM generated code is noticeably poorer than human written code.\n\nKeywords: Machine learning, Large language models, Performance evaluation, Code generation, Code quality, Usability, Code smell\n\n1. Introduction\n\n1.1. Motivation\n\nLarge language models (LLMs) are increasingly being used in practice to assist programmers with code generation. It is widely recognised that such machine learning (ML) models can significantly improve productivity [1], but there are con- cerns about the quality of the code generated. For example, the 2023 Stack Overflow Developer Survey conducted by Google in 2023 with over 90,000 respondents globally found that “39% of\n\ndevelopers say they don’t trust AI-generated code” [2]. More recently, based on a quantitative study of 211 million lines of code changes submitted to GitHub between January 2020 and December 2024, Harding et al. [3] identified multiple “signs of eroding code quality” such as duplicated code and various forms of technical debt. Moreover, they were able to link this rising defect rate with AI adoption. So this raises an impor- tant question: what specific quality defects are present in LLM generated code?\n\n⋆This paper is an extended and revised version of the conference paper en- titled \"Does LLM Generated Code Smell?\" to be published by IEEE in the Pro- ceeding of The 9th International Conference on Cloud, Big Data, and Com- munication Systems (ICCBDCS 2025). The results presented in the conference paper are preliminary and superseded by this paper.\n\n∗Corresponding author Email addresses: 19217422@brookes.ac.uk (Debalina Ghosh Paul), hzhu@brookes.ac.uk (Hong Zhu), ibayley@brookes.ac.uk (Ian Bayley)\n\nOur previous work [4] has looked at two of these attributes: correctness and complexity, and evaluated ChatGPT on the benchmark ScenEval that we constructed, where correctness is determined by passing all test cases automatically gener- ated from both ChatGPT generated code and the reference so- lution, and complexity was measured with cyclomatic com- It was found plexity, cognitive complexity, and line counts. that correctness was lower for more advanced coding topics and more complex coding tasks. ChatGPT-generated code was",
      "content_length": 4259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "Version 4.1; October 6, 2025.\n\nmore complex than human-written code. In addition, the com- plexity increases greater for more complex tasks than simpler ones. These suggest that LLMs are less useful for more com- plex and advanced tasks and that could explain why senior de- velopers use them less often [5], [6].\n\nHowever, Ziegler et al., in their study of GitHub Copilot, ob- served that the major driving force for the adoption of generated code is not its correctness but whether it is useful as a starting point for further development [7]. So, in this paper we will shift our attention to the quality attributes that are relevant to that. These include readability, testability, maintainability, ease of modification / evolution, reusability and so on.\n\n1.2. Challenges And Our Approach\n\nIt is difficult to measure LLM generated code on these qual- ity attributes since the context of their usage is unknown. Our solution is to use code smell detection techniques since they are well established in software engineering research for this pur- pose [8, 9, 10]. It is widely recognised that code smells are indi- cators of problems present in program code with maintenance, evolution and reuse [11].\n\nA problem with the concept of code smells, however, is that it is relatively subjective. Beck and Fowler define that bad code smells are “not precise criteria for flaws in program code” [8, 12]. They suggest that the presence of smells is “better to be judged based on informed human intuition” and research has found that “human agreement on smell detection is low” [13]. Our solution is to follow best practice in ML research: benchmark LLM performance on a dataset and compare with a baseline. The dataset we will use is ScenEval [4]. It consists of tasks collected from both textbooks and questions submitted to StackOverflow; the latter is a professional coding problem- solving website so the questions are real-world. Each task is accompanied by a reference solution either written by the text- book authors or supplied by professional programmers in IT in- dustry in response to the question on StackOverflow and scored highly by peers. These reference solutions provide a good base- line that reflects the current state-of-the-art in professionally written code so that we can decide whether the LLM-generated code is of comparable quality.\n\nTo achieve our research goal, it is insufficient to score each LLM with a single scalar value since they are used in many dif- ferent contexts for different purposes by different users. So we evaluate the LLMs on different scenarios. These include differ- ent problem topics and different complexities. The information we need for filtering on these scenarios is included as metadata with each coding task in the ScenEval benchmark [4]. Different subsets of the dataset can therefore be formed easily to repre- sent different scenarios.\n\nHowever, the existence of these subsets necessitates repeated experiments, so we automate both the execution of the experi- ments and the subsequent analysis of the large volume of data that is produced. We design and implement an automated test system following the datamorphic testing methodology [14] and execute the experiment with the Morphy test automation environment [15].\n\n2\n\nD. G. Paul, H. Zhu and I. Bayley\n\n1.3. Contributions\n\nOur main contributions are as follows.\n\n1. We propose a scenario-based method to investigate the quality of LLM generated code by detecting code smells, statistically analysing them and comparing them with a baseline of human-written programs. This method enables us to identify the quality weaknesses of LLM generated code specific to each scenario.\n\n2. We have designed and implemented a test system to auto- mate the experiments with LLMs and the analysis of the data obtained from the experiments.\n\n3. We have conducted a systematic and intensive experiment with four current state-of-the-art LLMs and compared the generated code with human-written reference solutions in the benchmark as the baseline. The experiment demon- strates the validity and feasibility of the proposed method, and the efficiency and effectiveness of the test system. 4. We have identified the weaknesses of LLM for code gen- eration with regards to the quality of the code generated. These results are the first of their kind in the literature as far as we know.\n\n1.4. Structure of the Paper\n\nThe paper is organised as follows. Section 2 reviews re- lated work on how to evaluate the quality of LLM-generated code and formulates the research questions. Section 3 gives a brief introduction to the notion of code smell and techniques for code smell detection. Our uses of these techniques are de- scribed. Section 4 presents the automated test system, which is designed and implemented based on the datamorphic software testing methodology. Section 5 presents the design of our ex- periment. Section 6 reports the results and presents the analysis of the data. Section 7 discusses the threats to experimental va- lidity. Finally, Section 8 summarises the findings and discusses directions for future work.\n\n2. Related Works And Open Problems\n\nEvaluation of the capability of LLMs in code generation has hitherto focussed on functional correctness, but far less on code quality and only recently; see [16] for a recent review. We now discuss the few works in the literature that are relevant, fol- lowed by the open research questions addressed in this paper.\n\n2.1. Manual Evaluation\n\nIn 2024, Miah and Zhu proposed a user-centric methodology to evaluate LLMs according to the quality of code generated [17]. The method consists of the following three components:\n\n1. A multi-attempt testing process model: the tester engages in an iterative process of interactions with a LLM by (a) formulating, revising and submitting a query to the LLM under test, (b) getting responses from the LLM, (c) assess- ing the LLM generated solution for usability, d) determin- ing whether a further attempt of querying the LLM should be made. This iterative process continues until either a",
      "content_length": 6083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Version 4.1; October 6, 2025.\n\nsatisfactory solution is obtained or a threshold maximum number of allowed iterations is reached.\n\n2. A set of eight quality attributes related to how easily a hu- man user could use LLM generated code. These are accu- racy, completeness, conciseness, clarity of logic, readabil- ity, well-structured-ness, parameter coverage and depth of explanation.\n\n3. A set of three metrics that measure the user experience. The first is the average number of attempts, each of which is an iteration of the human-LLM interaction described above. The second is the average completion time for the task of coding using the LLM. The third is the success rate, where success means that useful code has been generated.\n\nThe authors illustrated the methodology using ChatGPT with 100 tasks in the programming language R. Usability was high: 3.8 out of 5, manually assessed on a Likert scale of 1 to 5 for each of the eight quality attributes. The average number of at- tempts was only 1.61 and the average completion time was 42 seconds.\n\nAlthough subjective manual evaluation is valuable for the user-centred process proposed in the paper [17], it is labour- intensive and error-prone. Therefore, objective automated methods are preferable.\n\n2.2. Automated Evaluation\n\nAs far as we know, the only way to use automation to evalu- ate code quality is via code smell detection. Siddiq et al. was perhaps the first to do this, in [18], where they used Pylint [19] to detect the code smells in three different training datasets: CodeXGlue [20], APPS [21], and Code Clippy [22]; Bandit [23] was also used in order to detect security code smells. To investigate the impact of code smell in training dataset, ten dif- ferent code models, each based on the GPT-Neo 125M model, were trained on these three datasets, then tested on the Hu- manEval dataset [24] and compared with GitHub Copilot.\n\nThey found that the most frequent smells in the training datasets were also the most frequent in the generated code. They concluded that smells in the training datasets leaked into to the code models, although there was no statistical analysis of the correlations between the two, nor any causality analysis. However, their work has raised concerns about code smells in training datasets.\n\nMoratis et al. applied code smell detection to the dataset De- vGPT of reported iterative conversations in GitHub between de- velopers and ChatGPT [25]. Two types of conversations were extracted:\n\n1. Write me this code, with text instructions as input to pro- duce a program code\n\n2. Improve this code, with code snippets as input to improve the quality of the input program code\n\nThe conversations were then fed into the code smell detec- tion tool PMD [26]. In the Write me this code category, there were 47 conversations and a total of 59 code smell violations in 144 code blocks. Half (50.8%) of the violations concerned the standard practices of code conventions, a third (37.3%) related\n\n3\n\nD. G. Paul, H. Zhu and I. Bayley\n\nto styles of coding that have an impact on code readability and the remainder (11.9%) were violations of coding rules that were more likely to lead to errors.\n\nIn the Improve this code category, there were 334 conversa- tions. In most cases, the output had fewer total violations and sometimes it was a lot fewer, suggesting that ChatGPT can be used for this purpose. Occasionally the output had more viola- tions typically this was only one or two violations and not of the type that would introduce errors. Most conversations required fewer than 5 attempts; where more were needed it was usually because multiple code snippets were supplied as input.\n\nMoratis et al. observed, however, that their findings were \"inherently optimistic, as it exclusively contains instances of successful interactions with ChatGPT” [25]. Moreover, it is unclear whether the code quality is better or worse than that of human developers.\n\nAnother attempt to apply code smell detection to measure the ability of LLMs to improve the quality of existing code is due to DePalma et al. [27], who developed prompts to ask ChatGPT to refactor Java code to improve quality on 8 different quality attributes. Once again, PMD was applied both to the original code and the refactored code.\n\nLiu et al.[28] took the idea of code smell measurement one step further to form a self-repairing mechanism. PMD was used once again for Java code but in conjunction with CheckStyle [29]. For Python code generation, the tools used were Pylint [19] and Flake8 [30].\n\nThey classified code quality issues into four categories: (a) Compilation and Runtime Errors, (b) Wrong Output (i.e. func- tional incorrectness of the generated code), (c) Code Style and Maintainability, and (d) Performance and Efficiency. For each of these categories, they identified the top 10 issues for Java and for Python. The dataset used was the LMDefect dataset [31] of 2033 coding tasks supplemented with coding tasks extracting from LeetCode. The experimental data shows great promise with a repair rate in the range 20% to 60%. However, fixes can often introduce new quality issues.\n\nTable 1 summarises the related works mentioned in this sub- section and contrasts them with the work reported in this pa- per. The column Aims gives the purpose of the research. The column Usage explains how code smell detection techniques achieve that purpose. The columns Tools, Dataset, Language, LLM and Smell Types give the code smell detection tool(s) used, the test dataset used to evaluate the LLM(s) with the size of the dataset in parentheses, the programming language in which the code is generated, the LLM(s) evaluated, and the types of code smell, respectively.\n\nIt is worth noting that the eight smells detected by DePalma et al. [27] are implementation smells. It is not explicitly stated what code smells were detected by Liu et al.’s work. However, we believe that architectural smells were not detected because there is no architectural level code generated by the test cases. For the same reason, architecture code smells were not detected in our work.",
      "content_length": 6115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Version 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\nTable 1: Summary of Related Works\n\nWork Siddiq, et al. 2022\n\nAims Investigating the impact of code smells in training datasets on the quality of generated code\n\nMoratis, et al. 2024\n\nAssessing the quality of code generated via iterative con- versations in the Write this code scenario Assessing the quality improvement of code generated via iterative conversations in the Improve this code scenario Evaluating LLM’s capability of code refactoring\n\nDePalma, et al. 2024 Liu, at el. 2024 This paper Evaluating LLMs on various types of code smells for var- ious types of code to generate and the complexities of coding task\n\nEvaluating LLM’s capability of fixing code quality issues\n\nTools Pylint\n\nBandit\n\nPMD\n\nPMD\n\nPMD, CheckStyle Pylint, Flake8 PMD, CheckStyle, DesigniteJava\n\nUsage Detect code smells in training datasets and generated codes Detect security smells in training datasets and generated codes Detect code smell and measure code quality DevGPT (47)\n\nDataset (Size) HumanEval (164)\n\nComparing code smells before and after refactoring Assessing the quality of the code before and after refactoring Assessing the quality of generated code\n\nAssessing the quality of code generated in various scenarios\n\nDevGPT (334)\n\nMa (40) LMDefect (2033) ScenEval (1000)\n\net\n\nal.[32]\n\n+\n\nLanguage Python\n\nJavaScript\n\nJava\n\nJava Python Java\n\nLLM GPT-Neo, GitHub Copilot\n\nChatGPT\n\nChatGPT\n\nChatGPT\n\nGemini ChatGPT, Codex, Falcon\n\nPro,\n\nSmell Types Implementation smells\n\nSecurity smells\n\nBest practice, code style, error prone\n\nBest practice, code style, design, documentation, error prone, multi-threading, performance, security Implementation and design smells\n\nImplementation and design smells\n\n2.3. Research Questions\n\n3. Code Smell Detection\n\nThe existing works discussed above give some picture of the prevalence of code smells in LLM-generated code but their con- text and research questions are different from ours and there is no comparison with the human-written alternative. To bridge this research gap, we will ask the following open research ques- tions:\n\nIn this section, we will review the notion of code smells as background and explain how they can be detected. We will also explain how the code smell detection tools PMD, Checkstyle and DesigniteJava will be employed in our investigation.\n\nRQ1. Does LLM-generated code have a quality compara- ble to that of human-written code?\n\nBy human-written code, we mean code written by textbook authors or professional programmers, since we believe that can fairly represent the current best practice. Since our approach, outlined in Section 1.2, is to measure code smells, we will ask how does the incidence of smells in LLM-generated code com- pare with that of human-written code.\n\nRQ2. On which programming topics is LLM-generated code is weaker or stronger in quality compared to human- written code?\n\nOnce again, this can be rephrased in terms of code smells. How do code smells of LLM-generated vary with the question topic? More importantly, on which topics are the smells most worsened or most improved compared to human-written code?\n\nRQ3. How does the quality of LLM-generated code vary with the complexity of the coding task?\n\n3.1. The Notion of Code Smell\n\nThe concept of a code smell originated in Fowler’s book on refactoring [12] having been coined by Beck [8]. It was de- fined as “indications that there is trouble that can be solved by a refactoring” and “certain structures in the code that suggest (sometimes they scream for) the possibility of refactoring”. The authors described a list of 22 code smells, and how in each case, refactoring methods can help to improve the quality of the pro- gram. Since then, the notion of code smell has been intensively studied (see, for example, [13, 33, 11] for systematic literature reviews) and generalised to software smells [34].\n\nBeck and Fowler noted two distinctive aspects of the notion of code smells. Firstly, they are indicative rather than “pre- cise criteria for flaws in program code”. The code may not be flawed and may function correctly, but there may be fu- ture problems with maintenance, evolution, and reuse[11]. Sec- ondly, they are subjective. It is better to judge smell based on “informed human intuition” and consequently, “human agree- ment on smell detection is low”, as has been proven by research [13].\n\nA closely related question would be is the difference in qual- ity compared to human-written code greater for complex coding tasks? Both of these questions can be transformed into corre- sponding questions on code smells as above.\n\nRQ4. On which quality attributes is LLM-generated code worse compared to human-written code, since there is where research efforts could be directed?\n\nWe can rephrase this to ask which code smells are most prevalent in LLM-generated code and whether each smell is more or less common than in human-written code.\n\nRQ5. How is the correctness of LLM-generated code re- lated to the usability of the code in terms of readability, modifiability, reusability and easiness to evolve?\n\nTo answer this question, we will separate the codes generated by LLMs according to their correctness, analyse their smells separately, and compare their code smells with the baseline.\n\nThe notion of code smell is linked to a number of other soft-\n\nware engineering concepts and techniques, as follows:\n\nSmells are indicators or symptoms of a deeper design problem in the program code, as discussed above.\n\nSmells are suboptimal or poor solutions to a coding prob- lem. Bad smells lead to a technical debt of needing to find better solutions later.\n\nSmells violate recommended best practice for the domain. These include coding conventions and/or software design principles. Therefore, smells can be detected by looking for the violations of best practices.\n\nSmells have a negative impact on the software quality at- tributes that are related to product revision and transition, such as modifiability, readability, testability, reusability, portability, etc. In this way, they make software difficult\n\n4",
      "content_length": 6123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Version 4.1; October 6, 2025.\n\nto evolve, maintain, and reuse, and increase the likelihood of bugs, without themselves being bugs.\n\nAs Fowler suggested, smells should and can be elimi- nated or reduced, for example, by refactorings, which are meaning-preserving transformations on the software.\n\nSmells are recurring problems in program code. The pat- terns of such recurring problems bear similarity to the no- tion of anti-patterns.\n\n3.2. Types of Code Smells\n\nMany types of code smells have been defined and investi- gated in the literature. They can be classified according to a number of different criteria, such as the effect caused, design principles violated, location of the smell, its granularity, etc [34]. In this paper, we adopt the classification proposed by Suryanarayana, Samarthyam and Sharma [9, 10] which distin- guishes implementation smells from design smells (also known as micro-architectural smells) and architectural smells.\n\n3.2.1. Implementation Smells\n\nImplementation smells are concerned with suboptimal imple- mentation choices that make the code unnecessarily complex, difficult to maintain, and harder to understand. We detect and analyse the following:\n\nInconsistent Naming Convention. Deviations from the rec- ommended naming conventions.\n\nExcessive Complexity. An expression, statement or a method is difficult to understand due to lack of clarity caused by excessive complexity. For example, a state- ment could be excessively long, an expression could be excessively nested and/or have too many operations, and a method could have too many lines of code, and/or has an excessive list of parameters.\n\nIncompleteness. A piece of code is unfinished with, for example, \"TODO\" or \"FIXME\" tags, or a statement is in- complete. For example, a catch block may be missing han- dling logic, a conditional construct may be missing a ter- minating else clause, a switch or selector statement may be missing a default case, or more generally, a block of code within curly braces {} contains no executable state- ments, etc.\n\nRedundant Elements. The presence of duplicate parame- ters, methods, or code blocks. A method or attribute may have an unnecessary modifier, such as public where that visibility is already implied, or public static final where final would have been enough.\n\nImproper Alignment and Placement. Code is not properly aligned according to coding standards, and/or an entity in the code is misplaced; for example, attributes may not be given in the recommended order.\n\nMagic Number. A numeric literal is used directly in code without being defined as a constant.\n\n5\n\nD. G. Paul, H. Zhu and I. Bayley\n\nDead Code. Sections of code are no longer executed or provide no value for some other reason.\n\nResource Handling. Inefficiencies in the use of resources.\n\nDocumentation. Insufficient comments to explain the code properly.\n\n3.2.2. Design Smells\n\nDesign smells are concerned with design choices, as pre- sented in the program code, that violate fundamental design principles, such as poor use of object-orientation. They indicate the types of weaknesses that can lead to increased complexity, maintainability issues, and reduced code reusability. The fol- lowing are the types of design smells defined by Suryanarayana et al. [9, 10]; these are all detected and analysed in this paper.\n\nAbstraction Smell – Issues related to improper, missing, or unnecessary abstractions, affecting code clarity and reusability.\n\nEncapsulation Smell – Violations of encapsulation princi- ples, such as excessive exposure of internal details or in- adequate access restrictions.\n\nModularisation Smell – Poorly structured modules, in- cluding tightly coupled components, improper separation of concerns, and redundant dependencies.\n\nHierarchy Smell – Problems in class hierarchies, such as deep inheritance trees, improper sub-classing, or lack of adherence to object-oriented principles.\n\n3.2.3. Architectural Smells\n\nArchitectural smells are the weakness in the architectural de- sign of the system, as presented in the code, that often lead to reduced system flexibility, modification difficulties and main- tainability challenges. Typical examples include inappropriate layering and tight coupling between components and subsys- tems, etc. We will not consider these smells, however, because LLMs have limited capability for generating the entire system architecture and are not normally used for this purpose.\n\n3.3. Detecting Code Smells\n\nCode smell detection has been intensively studied in the soft- ware engineering literature; see, for example, [35, 36, 37] for systematic literature reviews. Fowler suggested that detection should be manual based on developer’s experience and intu- ition. However, this is not scalable and repeatable. So auto- mated tools should be used instead. Such tools can be classified into three types.\n\nStatic Code Analysis Approaches.\n\nTools for static analysis are usually based on either metrics or pattern-matching. Metrics on program code include Lines of Code (LOC), Number of Attributes per Class (NOA), Num- ber of Methods per Class (NOM), Number of Children Classes (NOC), Depth of Inheritance (DIT), etc. A metrics-based tool",
      "content_length": 5195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Version 4.1; October 6, 2025.\n\ndetects code smells using a combination of these metrics. A draw-back of this approach is the arbitrary nature of the thresh- old values set for the metrics.\n\nA rule-based tool, in contrast, defines a set of detection rules based on the syntactic structure of the code. Often these rules are linked to coding conventions and design principles. Some- times the tool is configurable in that the smells can be speci- fied with editable rules. Usually, they can be seamlessly inte- grated into existing development workflows. Typical examples of these tools include PMD, Stylecheck, Pylint, DesigniteJava, etc. Recently, such tools have been applied to LLM-generated code; see Section 2.\n\nHistory-based Approaches.\n\nThe evolution history of the system can be used to analyse the symptoms caused by smells and hence identify the smell. However, only a small number of smells can be detected this way.\n\nML-based Approaches.\n\nThere are two approaches for using machine learning (ML)\n\nmodels [38, 39].\n\nThe first is to train a model with features based on metrics, like lines of code, cyclomatic complexity, coupling metrics, etc. This requires large, high-quality labelled datasets. However, these are rare for code smells. Consequently, such ML mod- els have not achieved the performance suitable for practical use [38, 39]. For example, the Naive Bayes model reported in [40] has low F1-scores for most smells and low precision in par- ticular, indicating a high number of false positives. Moreover, existing ML models for smell detection are binary classification models, i.e. each model only detects one type of smells.\n\nThe second approach is to use LLM models to detect code smells. However, a recent evaluation reported in [41] shows low F1 scores for both Llama variants and GPT-4, the latter below 0.04.\n\n3.4. Use of Smell Detection Tools\n\nIn this paper, we will use static code analysis tools to de- tect code smells in both LLM-generated code and the reference solutions. As with Liu et al. [28], we use PMD [42, 26] and Checkstyle [43, 29]. They are based on widely recognised cod- ing conventions: Google Java Style Guide 1 and the Sun Java Code Convention 2, respectively. Both of them are capable of detecting and reporting the violations of these rules. However, since both tools are relatively weak in detecting design level code smells, we also use DesigniteJava 3, which detects smells according the design principles violated.\n\nTables 2 and 3 show the smell detection rules provided by each tool used in our work. Columns Tool Used and Detection\n\n1https://google.github.io/styleguide/javaguide.html 2https://www.oracle.com/java/technologies/javase/codeconventions-\n\nintroduction.html\n\n3https://www.designite-tools.com/products-dj\n\n6\n\nD. G. Paul, H. Zhu and I. Bayley\n\nRules give the tool and the smell detection rule used. Readers are referred to the websites of the tools for the definitions of the rules.\n\nNote that none of the tools cover all smells so we need to combine them for maximum coverage. One smell type may be detected by several different rules, even by different tools. The number of violations for such a smell type is calculated by summing up the numbers of violations of different rules.\n\nWhere a rule is implemented by more than one tool, however, the violation is counted only once. We have found in such cases that both tools give the same number of violations for the rule on the same code extract. In Table 2, such cases are indicated by a footnote reference 4.\n\nSince empirical studies have found it difficult to set a limit on the number of violations for the code still to be of good quality, we will count the number and compare it with a baseline; this reflects current practice.\n\n4. Test System for Code Smell Analysis and Evaluation\n\nOur experiments with LLMs need to be automated and we do this by applying the methodology of datamorphic testing pro- posed by Zhu et al. [14], [15]. This treats software testing as a systems engineering problem and it encourages both efficient management of test resources and the evolution of the test fa- cilities alongside that of the software under test.\n\nAccording to the methodology, a test system comprises two types of artefacts: test entities and test morphisms. The former are objects and documents involved in testing, such as test data, test datasets, test results, etc. while the latter are operations that manipulate and/or generate these entities to perform testing tasks. This methodology is supported by the test automation environment Morphy [15].\n\nMorphy provides a Java framework in which a test system can be implemented as a Java class (more precisely, a hierar- chy of Java classes) that consists of a set of attributes repre- senting the test entities and a set of methods representing test morphisms. They are both annotated with metadata so that they can be recognised by Morphy, seamlessly integrated with Mor- phy’s testing tools, and applied to achieve test automation. The following types of test morphisms are recognised by Morphy.\n\nSeed Maker: Generates initial test cases from other enti- ties.\n\nDatamorphism: Transforms existing test cases into new ones.\n\nMetamorphism: Verifies the correctness of test cases and returns a Boolean result.\n\nTest Set Filter: Adds or removes test cases from a test set.\n\n4The result from the tool by applying this smell detection rule is ignored because the same rule is already checked by another tool where the rule may have a different name. Only the result from one tool on the same rule is taken into account.",
      "content_length": 5589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Version 4.1; October 6, 2025.\n\nTable 2: Smell Detection Rules Used for Implementation Smells\n\nSmell Name\n\nInconsistent Naming Convention\n\nExcessive Complex\n\nRedundancy\n\nIncompleteness\n\nImproper Alignment and placement\n\nMagic Number\n\nDead Code\n\nResource Handling\n\nDetection Rule(s) Local Variable Naming Convention /Local Variable Name Formal Parameter Naming Convention PMD PMD Method Naming Convention /CheckStyle(4) /Method Name PMD Class Naming Convention PMD GenericsNaming CheckStyle AbbreviationAsWordInName CheckStyle AbstractClassName CheckStyle CatchParameterName CheckStyle ConstantName CheckStyle IllegalIdentifierName CheckStyle Simplify Boolean Expression PMD Simplify Conditional PMD/CheckStyle Simplify Boolean Return PMD Simplified Ternary CheckStyle Line Length CheckStyle Method Length /DesigniteJava(4) /Long Method PMD/DesigniteJava(4) Excessive Parameter List CheckStyle Redundant Import CheckStyle Redundant Modifier PMD Copy Paste Detector CheckStyle Missing Switch Default CheckStyle Todo Comment PMD Empty Control Statement PMD/CheckStyle(4) Empty Catch Block CheckStyle EmptyBlock CheckStyle Indentation CheckStyle FileTabCharacter CheckStyle NeedBraces PMD UselessParatheses CheckStyle LeftCurly CheckStyle RightCurly CheckStyle ParenPad MethodParamPad CheckStyle Variable Declaration Usage Distance CheckStyle CheckStyle Declaration Order CheckStyle Magic Number PMD Unused Formal Parameter PMD/CheckStyle(4) Unused Local Variables PMD Unused Private Fields PMD Unused Private Method CheckStyle Unused Imports PMD Close Resource PMD Avoid Instantiating Objects In Loops PMD Comment Required PMD Comment Size PMD Comment Content CheckStyle Javadoc Method CheckStyle Javadoc Type CheckStyle Missing Javadoc Package CheckStyle Javadoc Variable\n\nTool(s) Used PMD /CheckStyle(4)\n\nDocumentation\n\nTest Set Metric: Maps a test set to a real value, such as test adequacy.\n\nTest Case Filter: Maps a test case to a Boolean value. It can be used to determine whether the test case should be retained in the test set.\n\nTest Case Metric: Assigns a real-valued metric to individ- ual test cases (e.g., complexity).\n\nAnalyser: Examines the test set and produces a test report.\n\nExecuter: Runs the program under test using inputs from test cases and captures the outputs.\n\nGiven a test system implemented in Java, Morphy supports\n\ntest automation at the following three levels.\n\nAction: Executes a single test activity using test mor- phisms, built-in functions or tools.\n\n7\n\nD. G. Paul, H. Zhu and I. Bayley\n\nTable 3: Smell Detection Rules Used for Design Smells\n\nSmell Name\n\nModularity\n\nEncapsulation\n\nHierarchy\n\nTool Used Detection Rules PMD God Class PMD Data Class PMD Too Many Methods PMD Too Many Fields PMD Use Utility Class CheckStyle Hide Utility Class Constructor Broken Modularization DesigniteJava Cyclically-dependent Modularization DesigniteJava DesigniteJava Hub-like Modularization Insufficient Modularization DesigniteJava PMD Law of Demeter PMD Coupling Between Objects CheckStyle Class Fan Out Complexity CheckStyle Visibility Modifier PMD Excessive Public Count DesigniteJava Deficient Encapsulation CheckStyle Final Parameters CheckStyle Final Class CheckStyle Hidden Field DesigniteJava Unexploited Encapsulation DesigniteJava Broken Hierarchy DesigniteJava Cyclic Hierarchy DesigniteJava Deep Hierarchy DesigniteJava Missing Hierarchy DesigniteJava Multipath Hierarchy DesigniteJava Rebellious Hierarchy DesigniteJava Wide Hierarchy DesigniteJava Dependency Cycles btw Packages DesigniteJava Imperative Abstraction DesigniteJava Multifaceted Abstraction DesigniteJava Unnecessary Abstraction DesigniteJava Unutilized Abstraction\n\nAbstraction\n\nStrategy: Applies test strategies, which are algorithms with test morphisms and test entities as parameters.\n\nProcess: Runs test scripts of a high-level of abstraction. Such scripts can be obtained by recording interactive op- erations of Morphy, which can be edited, or even manually written, and replayed.\n\nOur test system extends that for a previous experiment, to analyse correctness and completeness of LLM-generated code [4] and uses the same benchmark, ScenEval. We define, how- ever, a new test entity code smell report and the following new test morphisms.\n\nTest Executors: LLM invokers. Four test executors for each of the four LLM models Gemini Pro, Codex, Falcon7B, and ChatGPT. The last of these has been inherited from the previous work [4] but the first three are new. Each executor submits the query to its respective LLM via an API call, and then extracts the Java code from the response text and saves it to a file for further analysis.\n\nAnalysers: Code Smell Detector Invokers. Three new test morphisms, PMD-analyser, Checkstyle-analyser and DesigniteJava-Analyser, which invoke the corresponding static code analysis tools PMD, Checkstyle and Designite- Java, and save the code smell reports into files.\n\nTest Set Metrics: Code Smell Statistical Analysers. Three analysers, Violations per Solution, Baseline Violations per Solution and Increase Rate to Baseline, which perform sta- tistical analysis on the code smell report files.",
      "content_length": 5129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Test System\n\nJava Compiler\n\nPMD (Code Smell Detector)\n\nDetected Code Smell\n\nJUnit Test Code\n\nGenerated Java Code\n\nTest Dataset\n\nTest Result Report\n\nPMD (Complexity Analyser)\n\nJava RTE + JUnit Executor\n\nCheckstyle (Code Smell Detector)\n\nSeed Makers\n\nManual Task Entry\n\nMorphy Automated Test Environment\n\nExtract W3 Resource\n\nChatGPT\n\nFalcon Invoker\n\nExtract Stack Overflow\n\nFalcon\n\nGemini Pro\n\nGemini Pro Invoker\n\nCodex Invoker\n\nChatGPT Invoker\n\nExecutors\n\nW3 Java Tutorial WebsiteScenEval Benchmark\n\nStack Overflow Website\n\nJava Programming Textbooks\n\nCodex\n\nTest Case Metrics\n\nExternal tools and data sources\n\nPMD Analyser\n\nComplexity Analyser\n\nAnalysers\n\nCompile Java Code\n\nExecute JUnit Test Code\n\nGenerate JUnit Test Code\n\nPurify Ref Test Code\n\nPurify Sol Test Code\n\nEvoSuite (JUnit Test Code Generator)\n\nTest Set Metrics/Analyser\n\nTopic Based Distribution\n\nYear Based Distribution\n\nCheckstyle Analyser\n\nViolations per Sample\n\nSmell Type Based Distribution\n\nExecutable Object Code\n\nComplexity Based Distribution\n\nTest Entities\n\nNew test morphism / entity for code smell analysisExisting test morphism / entity used for code smell analysisExisting test morphism / entity not used for code smell analysis\n\nDesigniteJava Analyser\n\nDesigniteJava (Code Smell Detector)\n\nAnalyse Correctness\n\nVersion 4.1; October 6, 2025.\n\nFigure 1: Structure of The Test System\n\nFig. 1 shows the structure of the test system. Test morphisms and entities inherited from, and explained in, our previous work [4] are shown in grey. New test morphisms and entities are shown in white. External tools invoked by them are shown in blue. Some of these are implemented in Python but invoked through a simple Python2Java interface.\n\n5. Design of the Experiments\n\nBefore discussing the experiment process, we will first re-\n\nview the LLMs, the benchmark and the platform.\n\n5.1. Subject LLMs\n\nTable 4 presents basic information about the four state-of- the-art LLMs we have studied: Gemini Pro, Falcon, ChatGPT, and Codex. All are commonly used for software development.\n\nTable 4: Large Language Models Evaluated\n\nName Gemini Pro Falcon ChatGPT Codex\n\nYear 2023 2023 2023 2021\n\nVersion Gemini Pro 1.0 Falcon-7B GPT-3.5-turbo GPT-3 (Codex)\n\nSize Unknown 7B Unknown 12B\n\n5.2. Benchmark\n\nThe benchmark ScenEval [4] contains more than 12,000 test cases of Java programming tasks. These test cases were cu- rated from textbooks, online tutorial websites and the profes- sional programming knowledge-sharing website Stack Over- flow. In contrast to other benchmarks for code generation (see e.g. [16]), ScenEval has two distinctive features which make it ideal for our purpose.\n\n1. Each test case is accompanied by the Java code for a refer- ence solution, typically textbook answers and highly-rated Stack Overflow posts. As discussed in 1.2, their code qual- ity represents the state of current practice so they enable us to establish a baseline of code smell for human-written Java code. 8\n\n1. Each test case is accompanied by the Java code for a refer- ence solution, typically textbook answers and highly-rated Stack Overflow posts. As discussed in 1.2, their code qual- ity represents the state of current practice so they enable us to establish a baseline of code smell for human-written Java code. 8\n\nD. G. Paul, H. Zhu and I. Bayley\n\n2. Each test case is also accompanied by metadata that speci- fies topic, complexity, source of the task, etc. This enables us to analyse the relationship between these concepts and code smell so that we can answer the research questions.\n\nOur test dataset, sampled at random from the ScenEval benchmark, contains equal quantities (500 each) from text- books and Stack Overflow. The other statistics are given in Table 5; Input Length and Complexity denote the number of words in the task description and the cyclomatic complexity of the reference solution.\n\nTable 5: Statistical Characteristics of the Test Dataset\n\nFeature # Topics # Tasks per Topic (average) # Tasks per Topic (max) # Tasks per Topic (min) Input Length (average) Input Length (max) Input Length (min) Complexity (average) Complexity (max) Complexity (min) Number of Coding Tasks\n\n#Textbook Tasks 25 20.00 79 8 18.55 35 11 3.448 6 1 500\n\n#Real Tasks 18 27.78 61 5 21.54 31 16 3.200 5 1 500\n\nLLM CodeGen Test System\n\nGenerated Java Code\n\nStyleCheck\n\nDesigniteJava\n\nLLM ChatGPT\n\nLLM Gemini Pro\n\nLLM Falcon\n\nLLM Codex\n\nOn CloudDesktop Computer•Dell Precision 3660•i7-13700•32GB Ram\n\nAPI invocation with coding task description as parameterGenerated solution of coding taskJava code generated by LLM\n\nCode smell reports\n\nDetected code smell\n\nCode Smell Detectors\n\nCoding task descriptionJava code of the reference solution\n\nMorphy (Automatic Test Environment)\n\nTest Dataset\n\nPMD\n\nGenerated solution\n\nInvocation of code smell detection tools with Java code as parameters\n\n5.3. Experiment Platform\n\nThe experiment was conducted with the automated test envi- ronment Morphy [14, 15] running on a desktop computer and is illustrated in Fig. 2. The LLM models under test were invoked through API calls as discussed in Section 4 and the smells of the LLM-generated code were analysed with PMD, Checkstyle and DesigniteJava as discussed in Section 3.\n\nFigure 2: The Experiment Setup\n\n5.4. Experiment Process",
      "content_length": 5273,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Version 4.1; October 6, 2025.\n\n1. Constructing Test Dataset: A test dataset containing 1000 coding tasks was constructed by random sampling of the ScenEval benchmark.\n\n2. Analysing Smells of Reference Solutions: For each cod- ing task in the test dataset, the Java code for the reference solution is analysed by invoking the code smell detection tools PMD, Checkstyle and DesigniteJava. The reports on the violations of smell detection rules are saved into sep- arate files for further statistically analysis to establish the baseline.\n\n3. Invoking LLMs: For each coding task in the test dataset, the LLMs under test are queried through API invocations and the solutions returned were collected. The Java code in the returned text is separated from the explanation text and saved into a .java file.\n\n4. Analysing Smells of LLM Generated Code: The Java codes generated by the LLMs are analysed via invocations of the code smell detection tools PMD, Checkstyle and Desig- niteJava. The violations of the detection rules are saved into code smell report files.\n\n5. Analysing Correctness of Generated Code: For each LLM generated Java code, the correctness of the code is deter- mined by running test data on both the LLM-generated code and the reference solution; see [4] for details.\n\n6. Analysing Complexity of Generated Code: The complexity of the LLM-generated code is measured the same way as in our previous work. [4].\n\n7. Statistical Analysis: The code smell report files are parsed, and statistical analysis is performed on various subsets of the test dataset to answer the research questions. Each sub- set represents a different scenario in the use of LLMs. The smell detection rules are partitioned into subsets according to their type, where needed to answer a research question. Given a subset T of the coding tasks and a subset S of smell detection rules, the following statistical data are cal- culated. lution, denoted by VS M LLM M using Equ. (1) .\n\n(b) The baseline for the test subset T w.r.t. smell detec- tion rules in S, i.e. the number of violations of smell detection rules per solution, denoted by VS B S (T), is calculated from the code smell reports of reference solutions using Equ. (2).\n\n(c) The increase rate of smells for LLM-generated code with respect to the baseline, denoted by InvM S (T), is calculated from VS M S (T) using Equ. (3).\n\nS (T) and VS B\n\nThese three equations are implemented as test set metrics and\n\nformally defined in the next subsection.\n\n5.5. Metrics of Performance\n\nLet t be a given coding task. We write M(t) to denote the program code generated by a LLM model M on coding task t and R(t) to denote the its reference solution in the benchmark.\n\n9\n\nD. G. Paul, H. Zhu and I. Bayley\n\nLet s be any given smell detection rule and c be a given Java code sample. We write Vs(c) to denote the set of violations of the rule s detected in the Java code c.\n\nLet T (cid:44) ∅ be a set of coding tasks, such as those for a specific\n\ntopic or complexity in the test dataset.\n\nLet S (cid:44) ∅ be a set of smell detection rules, such as those for\n\na particular type of code smell.\n\nDefinition 1. (LLM’s Smell Violations Per Solution (VS))\n\nWe write VS M\n\nS (T) to denote the smell violations of LLM M per solution w.r.t. a set S of smell detection rules and a set T of coding tasks, or simply violations per solution (VS). It is the average number of violations of the smell detection rules in S over the set of solutions generated by LLM M on coding tasks in T. Formally, we have that\n\nVS M\n\nS (T) =\n\n(cid:80)\n\nt∈T\n\n(cid:80)\n\ns∈S ∥Vs(M(t))∥\n\n∥T∥\n\nThe corresponding calculation for the baseline is as follows.\n\nDefinition 2. (Baseline’s Smell Violations per Solution)\n\nWe write VS B\n\nS (T) to denote the baseline’s smell violations per solution w.r.t. a set S of smell detection rules and a set T of coding tasks. This is the average number of violations of the smell detection rules in S over the reference solutions of the coding tasks in T. Formally, we have that\n\nVS B\n\nS (T) =\n\n(cid:80)\n\nt∈T\n\n(cid:80)\n\ns∈S ∥Vs(R(t))∥ ∥T∥\n\nSince the number of smell violations per solution is the only metric we are using to measure code smell, we will from now refer to it as the degree of code smell. Higher values mean poorer quality code. To compare the quality of LLM-generated code against a baseline, we will also measure the increase rate of code smells, as defined below.\n\nDefinition 3. (Increase Rate of Code Smells)\n\nThe increase rate of code smells for LLM model M with re- spect to the baseline on a set S of smell detection rules over a set T of coding tasks is denoted by IncM S (T), which is formally defined by the following equation.\n\nIncM\n\nS (T) =\n\nVS M\n\nS (T) − VS B VS B S (T)\n\nS (T)\n\nPositive values for this quantity mean that LLM-generated\n\ncode is of lower quality than the reference solution.\n\n6. The Results\n\nIn this section, we report the data collected from our experi- ments and answer each of the research questions with a statisti- cal analysis of the data.\n\n(1)\n\n□\n\n(2)\n\n□\n\n(3)\n\n□",
      "content_length": 5041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "19.37827.57131.40631.78935.8440510152025303540ReferenceFalconGeminiProChatGPTCodexAverage Number of Smells Per Solution\n\n42.28%62.07%64.05%84.97%0%10%20%30%40%50%60%70%80%90%FalconGeminiProChatGPTCodexIncrease (%) of Code Smells w.r.t. Baseline\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\n19.37827.57131.40631.78935.8440510152025303540ReferenceFalconGeminiProChatGPTCodexAverage Number of Smells Per Solution\n\n42.28%62.07%64.05%84.97%0%10%20%30%40%50%60%70%80%90%FalconGeminiProChatGPTCodexIncrease (%) of Code Smells w.r.t. Baseline\n\n6.1. RQ1: Prevalence of Code Smells\n\nFirst, the experiment data shows that LLMs performed dif- ferently on the program topics. Although the strength of code smells for the baseline also vary over code topics, the LLMs have a higher standard deviation on VS values when compared to the baseline (1.37). The standard deviations for LLMs are 1.90 for Falcon, 1.96 for ChatGPT, 2.08 for Codex and 2.14 for Gemini Pro, respectively. On average over all LLMs, the standard deviation is 1.85, which is an increase of 31.86% com- pared to the baseline. This indicates that LLMs have a robust- ness problem in maintaining consistency in code quality. In this respect, Falcon is the best (1.90) and Gemini Pro the worst (2.14).\n\nResearch question RQ1 is concerned with the overall quality of the code generated by the LLMs. To answer this question, we calculated the VS on all smell detection rules over the whole test dataset for each LLM and compared it with the baseline. The results are shown in Fig. 3.\n\nObservation 3. Over different coding topics, the degree of code smells in LLM generated code varies significantly more than human written code.\n\n(a) Smell Violations per Solution\n\nSecond, not only does the degree of code smells vary over different code topics, there is a pattern to the variation. We analysed the Pearson correlation coefficients between the base- line VS values and those of LLM generated code over various code topics. We found that there is a strong correlation between them. The Pearson correlation coefficients are 0.6652 for Fal- con, 0.7249 for Gemini Pro, 0.7188 for ChatGPT and 0.7664 for Codex, respectively, and 0.7876 on average over all LLMs. Therefore, we have the following observation.\n\n(b) Increase Rate (%) of Code Smells\n\nObservation 4. The coding topics with strong code smells in human-written code are the same as the coding topics with strong code smells in LLM-written code.\n\nFigure 3: All Code Smells Detected on the Whole Test Dataset\n\nAs shown in Fig. 3(a), all four LLMs have stronger code smells than the baseline with Falcon performing the best (VS=27.571) and Codex the worst (VS=35.844). Moreover, the increase rate of code smells varies significantly in the range from 42.28% for Falcon to 84.97% for Codex. This is a strong evidence for the following observations.\n\nHowever, we found no strong correlation between the base- line VS values and the LLM’s increase rates of code smells. In fact, the Pearson correlation coefficients are negative, between -0.1592 and -0.3808.\n\nFinally, the data also shows that LLMs are more likely to worsen the code smells on more advanced coding topics. Table 7 shows the best and worst three topics as well as the most im- proved and worsened three topics by each LLM and on average over all studied LLMs. The best topics (i.e. the strength of code smells decreased) are Basic Exercise, String, DateTime; while the worst topics (i.e. the strength of code smells increased) are Searching and Sorting, Encapsulation, Inheritance, Polymor- phism, Interfaces, and Generics.\n\nObservation 1. LLM-generated code is of poorer quality in terms of the smell violations per solution when compared to the human-written code.\n\nObservation 2. The rate of increase in code smells of LLM generated code when compared to the human-written code varies significantly with the choice of LLM.\n\nThere are a few topics on which LLMs improved the code quality in terms of code smells. These include Regular Expres- sion by all LLMs and Enum improved by Gemini Pro, ChatGPT and Codex, Collections by Falcon and Gemini Pro, Interfaces and Lambda by Falcon and Methods by Codex, and DataType by ChatGPT. The highest improvement is 35.93% by Gemini Pro on the topic of Regular Expressions.\n\n6.2. RQ2: Variation of Code Smells by Topic\n\nResearch question RQ2 aims to identify the types of tasks for which LLM-generated code is of poor quality so that improve- ment can be directed to these tasks. To answer this question, we divide up the test dataset according to the topic of the code to be generated and then calculate the VS on subsets of these coding tasks with all smell detection rules.\n\nOn average over all LLMs studied, the most worsened topics are Encapsulation by 138.53%, Array by 101.88%, and OOP by 101.88%. The largest increase rate of code smell is 165.38% by ChatGPT on the topic of Encapsulation. Thus, we have the following observations.\n\nTable 6 presents the VS values for various programming top-\n\nics and the increase rates for each LLM model.\n\nFrom the experiment data, the following observations can be\n\nmade.\n\n10",
      "content_length": 5149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "TopicBaselineFalconGeminiProChatGPTCodexAverageFalconGeminiProChatGPTCodexAverageBasic Exercise1.961.992.912.772.352.511.5348.4741.3319.9027.81DateTime2.182.592.853.623.333.1018.8130.7366.0652.7542.09String2.232.92.733.113.323.0230.0422.4239.4648.8835.20Array2.393.783.995.885.654.8358.1666.95146.03136.40101.88Input Output2.662.693.633.786.354.111.1336.4742.11138.7254.61Lambda2.712.413.954.164.333.71-11.0745.7653.5159.7836.99Collections2.912.562.585.643.683.62-12.03-11.3493.8126.4624.23Recursive Methods2.952.992.785.017.234.501.36-5.7669.83145.0852.63Thread2.963.574.595.746.225.0320.6155.0793.92110.1469.93Math2.973.354.534.995.154.5112.7952.5368.0173.4051.68DataType3.193.844.263.554.714.0920.3833.5411.2947.6528.21Methods3.333.943.784.773.94.1018.3213.5143.2417.1223.05OOP3.426.737.924.887.996.8896.78131.5842.69133.63101.17Exception Handling3.453.225.445.395.314.84-6.6757.6856.2353.9140.29Encapsulation3.649.917.959.667.218.68172.25118.41165.3898.08138.53Conditional3.733.483.846.364.984.67-6.702.9570.5133.5125.07Data Structure4.344.264.765.238.125.59-1.849.6820.5187.1028.86Enum4.564.53.884.384.664.36-1.32-14.91-3.952.19-4.50Generics5.465.739.427.958.357.864.9572.5345.6052.9344.00Interfaces5.574.276.758.719.217.24-23.3421.1856.3765.3529.89Regular Expression5.655.143.624.325.644.68-9.03-35.93-23.54-0.18-17.17Abstract Classes5.666.317.627.697.287.2311.4834.6335.8728.6227.65Searching & Sorting5.676.28.628.348.617.949.3552.0347.0951.8540.08Polymorphism5.726.916.748.448.487.6420.8017.8347.5548.2533.61Inheritance6.917.598.968.9810.489.009.8429.6729.9651.6630.28Average3.854.434.935.736.105.3015.2228.0148.9858.5337.69Std dev1.371.902.141.962.081.8539.5837.2439.5341.5931.86Average Number of Voilations per SolutionIncrease (%)\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\nTable 6: Code Smell by Code Topics\n\nTable 7: The Best/Worst Topics And The Most Improved/Worsened Topics\n\nModel\n\nBest Topics (VS) Basic Exercise (1.96) DateTime (2.18) String (2.23) Basic Exercise (1.99) Lambda (2.41) Collections (2.56) Collections (2.58) String (2.73) Recursive Methods (2.78) Basic Exercise (2.77) String (3.11) DataType (3.55) Basic Exercise (2.35) String (3.32) DateTime (3.33) Basic Exercise (2.51) String (3.02) DateTime (3.10)\n\nWorst Topics (VS) Searching & Sorting (5.67) Polymorphism (5.72) Inheritance (6.91) Polymorphism (6.91) Inheritance (7.59) Encapsulation (9.91) Searching & Sorting (8.62) Inheritance (8.96) Generics (9.42) Interfaces (8.71) Inheritance (8.98) Encapsulation (9.66) Searching & Sorting (8.61) Interfaces (9.21) Inheritance (10.48) Searching & Sorting (7.94) Encapsulation (8.68) Inheritance (9.00)\n\nMost Improved Toipics (Inc %) Most Worsened Topics (Inc %) N/A N/A N/A Interfaces (-23.34) Collections (-12.03) Lambda (-11.07) Regular Expression (-35.93) Enum (-14.91) Collections (-11.34) Regular Expression (-23.54) Enum (-3.95) DataType (11.29) Regular Expression (-0.18) Enum (2.19) Methods (17.12) Regular Expression (-17.17) Enum (-4.50) Methods (23.05)\n\nN/A N/A N/A Array (58.16) OOP (96.78) Encapsulation (172.25) Generics (72.53) Encapsulation (118.41) OOP (131.58) Thread (93.92) Array (146.03) Encapsulation (165.38) Array (136.40) Input Output (138.72) Recursive Methods (145.08) OOP (101.17) Array (101.88) Encapsulation (138.53)\n\nBaseline\n\nFalcon\n\nGemini Pro\n\nChatGPT\n\nCodex\n\nAverage\n\nThree different complexity metrics were used: cyclomatic com- plexity, cognitive complexity and lines of code. The results are presented in Fig. 4 graphically.\n\nObservation 5. The LLM-generated code has the best quality on basic coding topics, and the worst on advanced coping top- ics.\n\nFrom Fig. 4, it can be seen clearly that the VS tends to in- crease with each of three different metrics of complexity. This is confirmed by the Pearson Correlation coefficients between VS and complexity; see Table 8. For cyclomatic complexity, the Pearson correlation coefficients are all strongly positive (above 0.9) for each LLM as well as the baseline. For lines of code, the coefficients are a bit lower but still very high (in the range between 0.8685 and 0.9952 except ChatGPT (0.6347). For cog- nitive complexity, the results are mixed: very strong for Falcon (0.9754), but much lower for ChatGPT (0.3025), and around 0.6 for Gemini Pro and Codex, and 0.6894 on average over all LLMs.\n\nObservation 6. The LLM-generated code can have better quality in comparison with human written code on certain cod- ing topics, while it can also have significantly worse code qual- ity, especially on advanced coding topics.\n\n6.3. RQ3: Variation of Code Smells by Complexity\n\nResearch question RQ3 is concerned with how code quality varies with the complexity of the coding tasks. To answer this question, we calculated the VS on subsets of test cases that were formed according to the complexity of the coding tasks. Here, the complexity of a coding task is measured on the complexity of the reference solution provided by the benchmark ScenEval.\n\nTherefore, we have the following observations.\n\n11",
      "content_length": 5039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "012345678901234567Code Smell (VS)Cyclomatic ComplexityCode Smell by Cyclomatic Complexity\n\nBaseline\n\nFalcon\n\nGeminiPro\n\nChatGPT\n\nCodex\n\n012345678901234567Code Smell (VS)Cognitive ComplexityCode Smell by Cognitive Complexity\n\n0123456789101-5051-100101-150151-200201-250251-300Code Smell (VS)Lines of CodeCode Smell by Lines of Code\n\n203080130180230123456Increase w.r.t. Baseline (%)Cognitive Complexity\n\n20020406080100120140160180123456Increase w.r.t. Baseline (%)Cyclomatic Complexity\n\nFalcon\n\nGemini Pro\n\nChatGPT\n\nCodex\n\n01020304050607080901-5051-100101-150151-200Increase w.r.t. Baseline (%)Lines of Code\n\n012345678901234567Code Smell (VS)Cyclomatic ComplexityCode Smell by Cyclomatic Complexity\n\nBaseline\n\nFalcon\n\nGeminiPro\n\nChatGPT\n\nCodex\n\n012345678901234567Code Smell (VS)Cognitive ComplexityCode Smell by Cognitive Complexity\n\n0123456789101-5051-100101-150151-200201-250251-300Code Smell (VS)Lines of CodeCode Smell by Lines of Code\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\nTable 8: Correlations Bwt VS and Coding Task Complexities\n\nComplexity Baseline Falcon GeminiPro ChatGPT Codex Average 0.9962 0.9653 0.9344 0.9555 Cyclomatic 0.6578 0.6894 Cognitive 0.8800 0.9754 0.8900 0.8685 Lines of Code 0.9952 0.8694\n\n0.9916 0.6312 0.9532\n\n0.9485 0.3025 0.6347\n\n012345678901234567Code Smell (VS)Cyclomatic ComplexityCode Smell by Cyclomatic Complexity\n\nBaseline\n\nFalcon\n\nGeminiPro\n\nChatGPT\n\nCodex\n\n012345678901234567Code Smell (VS)Cognitive ComplexityCode Smell by Cognitive Complexity\n\n0123456789101-5051-100101-150151-200201-250251-300Code Smell (VS)Lines of CodeCode Smell by Lines of Code\n\nFigure 5: Increase Rates of Code Smells by Complexity\n\nspecific type of code smell on the whole test dataset and com- pared with the baseline.\n\nTable 9 presents, in the form of a heat map, the VS for each specific type of smells as well as the increase rates for each LLM model in comparison with the baseline. The highest VS scores and the largest (i.e. worst) increase rate are highlighted in red, while the lowest VS scores and lowest increase rates are coloured in blue. Implementation smells are listed in the top half and design smells in the bottom half.\n\nFigure 4: Variation of Code Smells by Complexity\n\nObservation 7. For both human written code and LLM gener- ated code, the strength of code smells increases with the com- plexity of coding tasks.\n\nIt is worth noting that the baseline VS value also increases with cyclomatic complexity but it does so at a slower rate than any of the four LLMs. So, we can see that LLMs tend to strug- gle to produce good quality code when they are given highly complex tasks.\n\nFrom the experiment data, we observed the following phe-\n\nnomena.\n\nObservation 9. The least prevalent types of implementation smells for all LLMs as well as the reference solutions are Incompleteness, Inconsistent Naming Convention and Redun- dancy.\n\nHowever, by analysing the increase rate of code smells in the code generated by LLMs with respect to human written code, we found no obvious link between the complexity of coding tasks to the increase rate of code smells as shown in Fig. 5.\n\nObservation 10. The worst types of implementation smells for all LLMs and human written code are Magic Number, Docu- mentation and Improper Alignment and Placement.\n\nTherefore, we have the following observation.\n\nObservation 8. There is no clear evidence that the increase rate of code smells in LLM generated code is correlated to the complexity of coding task.\n\nIn general, there is a very strong correlation between LLM generated code and human written code on the VS scores on the types of code smells. As shown in Table 10, for implemen- tation types of code smells, the Pearson correlation coefficients between the baseline and the code generated by each LLM are all very close to 1. For design code smells, the Pearson correla- tion coefficients are in the range between 0.7263 for Falcon and\n\n6.4. RQ4: Variation of Code Smells by Smell Types\n\nResearch question RQ4 aims to identify the specific quality issues in LLM generated codes. We calculated the VS for each\n\n12",
      "content_length": 4111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "ChatGPTGeminiProFalconCodexAverageImpl Smells0.99870.99900.99610.99730.9980Design Smells0.87570.87660.72630.87480.8506\n\nSmell TypeReferenceChatGPTGeminiProFalconCodexAverageChatGPTGeminiProFalconCodexAverageIncompleteness0.00300.01400.01600.00800.01500.0133366.67433.33166.67400.00341.67Inconsistent Naming Convention0.00400.05000.05100.02600.05000.04431150.001175.00550.001150.001006.25Redundancy0.01800.07100.07800.04100.07100.0653294.44333.33127.78294.44262.50Dead Code0.08600.05800.06400.08600.05600.0660-32.56-25.580.00-34.88-23.26Resource Handling0.56500.58400.64200.56500.59500.59653.3613.630.005.315.58Excessive Complex0.65301.15101.35300.66101.27401.109876.26107.201.2395.1069.95Magic Number0.85101.37401.52300.85701.54601.325061.4678.970.7181.6755.70Documentation2.31403.25003.31702.31403.28103.040540.4543.340.0041.7931.40Improper Alignment / Placement11.150020.499019.560019.189024.185020.858383.8575.4372.10116.9187.07Average of Impl Smells15.644027.051026.604023.747031.073027.118872.9270.0651.8098.6373.35StDev of Impl Smells3.60496.64296.31996.24917.84846.7634370.61379.28179.61368.75324.12Hierarchy0.00000.00200.00200.00200.00200.0020N/AN/AN/AN/AN/AAbstraction0.00001.02701.03801.06301.03501.0408N/AN/AN/AN/AN/AModularity1.82401.91801.94601.54401.93801.83655.156.69-15.356.250.69Encapsulation1.91001.79101.81601.21501.79601.6545-6.23-4.92-36.39-5.97-13.38Average of Design Smells3.73404.73804.80203.82404.77104.533826.8928.602.4127.7721.42StDev of Design Smells1.07850.88110.89390.66690.88730.82768.058.2114.888.649.94Average of All Smells19.378031.789031.406027.571035.844031.652564.0562.0742.2884.9763.34StDev of All Smells3.01725.51175.24775.17686.51465.610982.6773.9271.57115.9185.96Avergae Numbers of Violations per Solution (VS)Increase in VS (%)\n\nChatGPTGeminiProFalconCodexChatGPTGemini Pro0.9985Falcon0.99310.9884Codex0.99920.99840.9926\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\nTable 9: Code Smells by Smell Type\n\nChatGPTGeminiProFalconCodexAverageImpl Smells-0.2257-0.2549-0.1518-0.2064-0.2188Design Smells-1-1-1-1-1\n\nTable 10: Pearson Correlation Coefficients btw Baseline and LLMs’ VS Scores on Various Smells Types\n\nall very close to 1; See Table 12. This implies the following observation.\n\nObservation 13. LLMs consistently increase the strength of code smells over various types of code smells.\n\nTable 12: Pearson Correlation Coefficients btw LLMs’ Increase Rates of Implementation Smells\n\n0.8766 for Gemini Pro. Therefore, we can confidently conclude that:\n\nObservation 11. The prevalence of code smells in LLM- generated code on various smell types is strongly correlated with that of human-written code.\n\nThe experiment data also demonstrated that the prevalence of a type of code smell in human written code does not imply that the smell increases in LLM generated code. As shown in Table 11, for implementation smells, the Pearson correlation coeffi- cients between LLMs’ smell increase rates and the VS scores of the baseline are all negative in the range between -0.2549 for Gemini Pro and -0.1518 for Falcon, where the average over all LLMs is -0.2188.\n\nFrom the experiment data, we can also have the following\n\nobservations.\n\nObservation 14. All LLMs performed well on the encapsula- tion type of design smells in comparison with the baseline.\n\nObservation 15. LLMs’ performance on design smells vary significantly with increase rates ranging from 2.41% for Fal- con to 28.60% for Gemini Pro, and the average increase rate over all LLMs is 21.42%. Falcon performed better on design smells than the other LLMs.\n\nTable 11: Pearson Correlation Coefficients btw Baseline VS Scores and LLMs’ Increase Rates on Various Smells Types\n\nFinally, by analysing the distributions of VS scores for each\n\ntypes of smells, we have the following observation.\n\nHowever, it is observable that the largest increase rates of code smells are on the types that are the least prevalent smell types of the baseline.\n\nObservation 16. For each type of smells, the violations of smell detection rules are concentrated in a small number of spe- cific smells.\n\nObservation 12. The largest increases of smells in LLM gen- erated code happened at the smell types of Incompleteness, In- consistent Naming Convention and Redundancy.\n\nTable 13 lists the most prevalent smells in each smell type, where column Top Smell(s) lists the most prevalent smell(s) of the smell type given in the column Smell Type. Column #Vio- lations gives the average numbers of violations of the specific smell rule over all LLMs. Column Weight gives the ratio of the\n\nFinally, the Pearson correlation coefficients between LLMs’ increase rates over various types of implementation smells are\n\n13",
      "content_length": 4705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Smell Type#VoilationsWeight (%)BasekineInconsistentNaming Convention44.25100.0041105.2599.596533.250.29059.591.19185.758.8102073299.3911119460.22039.750.19610.2577.3621.511.321Magic Number1325100.00851Dead Code66100.0086572.595.98539244.0226270588.972103224.257.3816698.53.2439917.7549.97912.00918.7550.03912.001490.2590.071730.00118.507.16114.00Hierarchy2.00100.000.001036.5099.590.003.750.360.00Imperative AbstractionModularityEncapsulationAbstractionUse Utility ClassFinal ParametersBroken HierarchyUnutilized AbstractionHide Utility Class ConstructorHidden FieldRedundancyImproper Alignment / PlacementIncompletenessResource HandlingDocumentationClose ResourceAvoid Instantiating Objects In LoopsComment RequiredJavadoc VariableComment SizeVariable Declaration Usage DistanceMissing Switch DefaultEmpty Catch BlockMagic NumberUnused ImportsRedundant ModifierRedundant ImportIndentationFileTabCharacterAbbreviation As Word In NameTop Smell(s)Simplify Boolean ExpressionLine LengthExcessive Complex\n\n7436786804700100200300400500600700800Gemini ProCodexChatGPTFalconNumber of Coding TasksNumber of Correct Solutions Generated by LLMs\n\nCorrect IncorrectCorrect IncorrectCorrect IncorrectChatGPT31.3332.9326.6328.114.704.82Gemini Pro30.6333.8825.8629.004.774.88Falcon32.0523.6827.8720.174.183.51Codex35.6436.4430.8731.664.774.78Average32.4131.7327.8127.244.604.50All SmellsImpl SmellsDesign SmellsLLM\n\nVersion 4.1; October 6, 2025.\n\nD. G. Paul, H. Zhu and I. Bayley\n\nLLMAll SmellsImpl SmellsDesign SmellsChatGPT5.135.572.63Gemini Pro10.6212.162.29Falcon-26.11-27.63-15.93Codex2.232.550.17\n\nTable 13: The Most Prevalent Smells in Each Type\n\nFig. 6 shows the number of LLM generated programs that pass all tests. From the data shown in the figure, Gemini Pro performed the best with a success rate of 74.3% passing all tests, while Falcon is the worst with a success rate only 47.0%. Codex and ChatGPT performed very similar, both have a suc- cess rate around 68.0%.\n\nFigure 6: Numbers of LLM Generated Solutions Passed Test\n\nTo analyse how correctness is related to code smell, we split the test dataset into two subsets: one contains coding tasks that the LLM generated a correct code; the other contains tasks that LLM failed to generate a correct model. The smell violations per solution are calculated for each subset on all code smell detection rules. The results are shown in Table 14.\n\nviolations over all smells of the type. Column Baseline gives the number of violations in the reference solutions.\n\nTable 14: Smells of Correct and Incorrect Codes\n\nNote that there are fewer violations of design smells than im- plementation smells. We believe that there are two reasons for this. First, the test dataset contains very few coding tasks where the solutions require a large number of classes. In fact, only 68 (6.8% of the dataset) require more than one class. Design smells like Hierarchy smells do not present in code that has only one class. Second, there are less detection rules for design smells than those for implementation smells. However, fewer violations of design smells do not imply the better design qual- ity because design smells are at a higher level of abstraction and of greater granularity. Each violation of design smell could have a more serious impact than one violation of an implemen- It is not meaningful to compare the number of tation smell. violations of design smells to that of implementation smells.\n\nFrom Table 14, we can observe that correct code has less smells than incorrect code on average for three out of four LLMs: Gemini Pro, Codex and ChatGPT. However, Falcon is an exception. Its incorrect code has less smells than its correct code. Table 15 shows the rates of increase (%) in code smells from correct code to the incorrect code for different LLMs.\n\n6.5. RQ5: Variation of Code Smells by Correctness\n\nTable 15: Increases (%) in Smells from Correct to Incorrect Codes\n\nResearch question RQ5 aims to understand how the correct-\n\nness of LLM generated code relate to code smells.\n\nTo determine the correctness of a LLM generated solution, test cases were generated from both the reference solution and the LLM-generated code by employing the EvoSuite tool. These two sets of test cases are merged into one test suite. Both the reference solution and the generated code are tested on the test suite. If the reference solution fails on a test case that is generated from the LLM generated code, a commission error is detected. If the generated code fails on a test case that is generated from the reference solution, an omission error is de- tected. If neither a commission error nor an omission error are detected, i.e. it passes all of the tests, we regard the LLM gen- erated code is correct. Readers are referred to [4] for details about how this is conducted.\n\nFrom Table 15, we have the following observation.\n\nObservation 17. The degree of differences between the cor- rect and incorrect codes in terms of the strength of code smells varies significantly with the LLM models. For some LLMs, in- correct codes are more smelly; while for the others, the opposite is observable.\n\n14",
      "content_length": 5131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Version 4.1; October 6, 2025.\n\n7. Discussion on Threats to Validity\n\nIn this section, we discuss the potential threats to the valid- ity of the experiment reported in the previous section and how these threats were addressed in the design and conduct of the experiment. We will also discuss how to further reduce the threats in future work. We will apply Wohlin et al.’s the frame- work [44] of classifying the threats to validity in software engi- neering experiments, as it is among the frameworks most used by the researchers in software engineering.\n\n7.1. Construct Validity\n\nConstruct validity is concerned with whether the data ob- tained by measurement and observation correctly and ade- quately represent the abstract concepts under study. In our con- text, it means whether the code smell detection rules correctly, adequately and fairly represent the quality aspects related to the readability, maintainability, ease of evolution, etc.\n\nOne primary threat to this validity lies in the reliance on the accuracy and coverage of the static analysis tools used in our study. Any limitations or inaccuracies in these tools could af- fect the precision of our smell detection. To mitigate this threat, we selected widely used and validated tools (i.e., PMD, Check- style, and DesigniteJava) that have demonstrated reliability in prior research and practice.\n\nAlso, we focused solely on code smells detectable by PMD, Checkstyle, and DesigniteJava. While this may exclude certain types of smells, the selected set of detection rules represents a well-established and widely adopted list of code smells. These have been well documented and widely used both in academic research and industry practice, lending credibility to their rel- evance and maturity. Moreover, we have combined the smell detection rules provided by these tools to maximised the cover- age of the smells.\n\n7.2. External Validity\n\nThe external validity is concerned with to what extent the re- sults of an experiment can be generalised. A potential threat to external validity involves the generalisability of our findings to other LLMs not studied in our experiments, coding in program- ming languages other than Java, and those types of coding tasks not covered by the test dataset.\n\nOur analysis is based exclusively on Java programs from the ScenEval dataset, which may limit the applicability of the re- sults to code generation tasks in other programming languages. Additionally, we evaluated outputs from some of the most widely used generative models GeminiPro, ChatGPT, Codex, and Falcon in Table 4. Other versions of these ML models and other ML models, such as CodeBERT [45] and CodeT5 [46], were not studied. Therefore, the results may not gener- alise across all generative coding models. However, there are observations that are consistent on all LLMs that we studied. These observations should be able to generalise to other ML models.\n\nOur dataset covers a wide range of topics. However, the ma- jority of coding tasks are of small scale in terms of complexity. Moreover, very few of the coding tasks require multiple classes.\n\n15\n\nD. G. Paul, H. Zhu and I. Bayley\n\nThe conclusions drawn from our experiment should be limited to the coding tasks well represented by our dataset. Any gen- eralisation of our conclusions to other kinds of coding tasks should be used with great care.\n\n7.3. Internal Validity\n\nInternal validity is concerned with the appropriateness of the design and conduct of the experiments. A typical example of the threats to internal validity is the existence of factors that influence the causal relationships under study but are not mea- sured and are not under our control.\n\nA potential threat to the internal validity of our experiment is that LLMs are inherently nondeterministic. For this reason, we have selected a large number of test cases (1000) at random to minimise the impact of LLM’s randomness. The scale of our experiment is much larger than most of the studies of LLMs’ capability in code generation. For future research, this threat to internal validity can be further reduced by using even more test cases and repeating the invocations of LLMs on each coding task.\n\nAnother potential threat to internal validity is that the qual- ity of program code in general and code smells in particular are very subjective as we discussed in Section 1 and 3. We have addressed this threat at the methodology level by excluding hu- man factors from the experiment by using a baseline formed by professionally written code and at the technology level by em- ploying the quantitative analysis of the experiment data using objective metrics.\n\nFinally, a potential threat to internal validity is that the im- plementation of the test system may contain bugs, thus the data collected may have errors. We have addressed this threat by careful testing and debugging of the test system. Moreover, to ensure the experimental reproducibility, the source code of the test system as well as the data are available to the public in the GitHub repository 5.\n\n7.4. Conclusion Validity\n\nConclusion validity is concerned with whether the conclu- sion drawn from the experiment is logically valid, such as whether correct statistical inference methods are used properly and whether the statistical inference power is strong enough.\n\nDue to the fact that the experiments with LLMs are time con- suming and resource demanding, the statistical inference power in this work is not ideal because the scale of our experiment is still limited. However, it is already much larger than other ex- isting similar works. We believe it is not a serious threat to conclusion validity. For future work, repeating the experiments with a larger test dataset will further reduce the threat.\n\n8. Conclusion and Future Work\n\nIn this paper, we proposed a scenario-based methodology to evaluate the usability of LLM-generated code on readability,\n\n5URL: https://github.com/hongzhu6129/EvaluateLLMCodeSmell",
      "content_length": 5979,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Version 4.1; October 6, 2025.\n\nmodifiability, reusability, ease of maintenance, ease of evolu- tion, etc., through assessing the code smells and comparing with a baseline obtained from code written by professional pro- grammers. An automated test system is designed and imple- mented in the datamorphic testing method. An intensive ex- periment with four prominent LLMs is conducted using the ScenEval benchmark for generating Java code.\n\nWe have found that the code smells, measured by the av- erage number of violations of code smell detection rules per solution, is significantly greater in LLM-generated code com- pared to human-written code. Across all LLMs, the average increase rates of implementation and design smells are 73.35% and 21.42%, respectively, while the average increase rate over all smells is 63.34%.\n\nThe performances of LLMs vary significantly over different topics of coding tasks and smell types. In general, the more complicated a coding task is, the stronger is the smell in LLM- generated code. The types of code smells that are the strongest in human written code are also the most prevalent in LLM gen- erated code. However, the increase rates of code smell types in LLM generated code show no correlation to the prevalence of the type of code smell in human written code. In general, the quality of generated code decreases with the complexity of the code task. This correlation is very clear when coding task com- plexity is measured by cyclomatic complexity and the lines of code, but is slightly less clear with cognitive complexity.\n\nFor future work, it is worth further expanding and repeating the experiments with more LLM models and using larger test dataset to reduce the risks of the potential threats to validity as discussed in Section 7.\n\nAs discussed in Section 3, it is difficult to set a threshold on the number of violations for the code to be of acceptable qual- ity. We could only compare with the baseline number, which reflects current practice. Hence, from our experiment data, we have difficulty to answer the question: is the quality of LLM generated code acceptable for use? We encourage further re- search in this area.\n\nMoreover, LLM-generated code has broader quality aspects of usability that are worth considering, for example, security and runtime efficiency of the generated code, which is ad- dressed by a recent work by Jonnala et al. [47]. This way we hope to provide a more holistic evaluation of LLM performance for code generation.\n\nFinally, it is worth investigating how code smell detection can be used to improve the quality of LLM generated code in a multi-attempt process proposed by Miah and Zhu [17].\n\nAcknowledgement\n\nThe research work reported in this paper is partly funded by Google’s PaliGemma Academic Program, which provided credits for using Google Cloud Platform resources.\n\nReferences\n\n[1] I. Shani, Survey reveals AI’s impact on the developer experience, GitHub Blog, https://github.blog/news-insights/research/\n\n16\n\nD. G. Paul, H. Zhu and I. Bayley\n\nsurvey-reveals-ais-impact-on-the-developer-experience/ (2024).\n\n[2] D. DeBellis, K. M. Storer, A. Lewis, B. Good, D. Villalba, E. Maxwell, K. Castillo, M. Irvine, N. Harvey, Accelerate state of DevOps, DORA 2024 Report, Google Cloud. https://cloud.google.com/blog/ products/devops-sre/announcing-the-2024-dora-report (2024).\n\n[3] W. Harding, AI copilot code quality: Evaluating 2024’s increased defect rate via code quality metrics, Tech. Rep., Alloy.dev Research, 211m Lines of Analyzed Code + Projections for 2025, (Feb. 2025).\n\n[4] D. G. Paul, H. Zhu, I. Bayley, ScenEval: A benchmark for scenario-based evaluation of code generation, in: Proc. of 2024 IEEE Int’l Conf. on Ar- tificial Intelligence Testing (AITest 2024), July 2024, pp. 55–63.\n\n[5] A. Zdravkovic, AMD takes holistic approach to AI coding copilots: The chipmaker is using ai throughout the software-development life cycle, IEEE Spectrum 62 (6), pp26–31, (2025)\n\n[6] B. K. Deniz, C. Gnanasambandam, M. Harrysson, A. Hussin, S. Srivas- tava, Unleashing developer productivity with generative AI, McKinsey Digital, 7 (2023).\n\n[7] A. Ziegler, E. Kalliamvakou, X. A. Li, A. Rice, D. Rifkin, S. Simister, G. Sittampalam, E. Aftandilian, Measuring github copilot’s impact on productivity, Communications of the ACM, 67 (3) pp.54–63, (2024).\n\n[8] K. Beck, M. Fowler, Bad smells in code, Ch. 3, in: [12], pp. 75–88.\n\n[9] G. Suryanarayana, G. Samarthyam, T. Sharma, Refactoring for Software Design Smells: Managing Technical Debt, Elsevier Science & Technol- ogy, 2014.\n\n[10] T. Sharma, M. Fragkoulis, D. Spinellis, Does your configuration code smell?, in: Proc. of 2016 IEEE/ACM 13th Working Conference on Min- ing Software Repositories (MSR 2016), IEEE, Athens, Greece, 2016, pp. 189–200.\n\n[11] G. Lacerda, F. Petrillo, M. Pimenta, Y. G. Guéhéneuc, Code smells and refactoring: A tertiary systematic review of challenges and observations, Journal of Systems and Software, 167, 110610, (2020)\n\n[12] M. Fowler, Refactoring: improving the design of existing code, Addison-\n\nWesley Professional, 2018.\n\n[13] J. A. M. Santos, J. B. Rocha-Junior, L. C. L. Prates, R. S. Do Nascimento, M. F. Freitas, M. G. De Mendonça, A systematic review on the code smell effect, Journal of Systems and Software, 144 pp450–477, (2018)\n\n[14] H. Zhu, D. Liu, I. Bayley, R. Harrison, F. Cuzzolin, Datamorphic testing: A method for testing intelligent applications, in: 2019 IEEE International Conference On Artificial Intelligence Testing (AITest 2019), IEEE, 2019, pp. 149–156.\n\n[15] H. Zhu, I. Bayley, D. Liu, X. Zheng, Automation of datamorphic test- ing, in: Proc. of 2020 IEEE Int’l Conf. On Artificial Intelligence Testing (AITest 2020), pp. 64–72, (2020)\n\n[16] D. G. Paul, H. Zhu, I. Bayley, Benchmarks and metrics for evaluations of code generation: A critical review, in: Proc. of 2024 IEEE International Conference on Artificial Intelligence Testing (AITest 2024), pp. 87–94 (2024).\n\n[17] T. Miah, H. Zhu, User centric evaluation of code generation tools, in: 2024 IEEE International Conference on Artificial Intelligence Testing (AITest 2024), pp. 109–119, IEEE, (2024)\n\n[18] M. L. Siddiq, S. H. Majumder, M. R. Mim, S. Jajodia, J. C. S. Santos, An empirical study of code smells in transformer-based code generation techniques, in: Proc. of 2022 IEEE 22nd Int’l Working Conf. on Source Code Analysis and Manipulation (SCAM 2022), IEEE, USA, 2022.",
      "content_length": 6433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Version 4.1; October 6, 2025.\n\n[19] S. Thénault, et al., Pylint: Code analysis for Python. Accessed: 2025-04-\n\n11 (2001).\n\n[20] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang, et al., Codexglue: A machine learning benchmark dataset for code understanding and generation, arXiv preprint arXiv:2102.04664 (2021).\n\n[21] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, et al., Measuring coding challenge competence with apps, arXiv preprint arXiv:2105.09938 (2021).\n\n[22] N. Coooper, A. Arutiunian, S. Hincapié-Potes, B. Trevett, A. Raja, E. Hossami, M. Mathur, et al., Code clippy data: A large dataset of code data from GitHub for research into code language models (Oct. 2021). URL :https://github.com/CodedotAl/gpt-code-clippy/wiki/ Dataset\n\n[23] Bandit, Bandit: Security linter for Python code, https://bandit.\n\nreadthedocs.io/, accessed: 2025-04-11.\n\n[24] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Ed- wards, Y. Burda, N. Joseph, G. Brockman, et al., Evaluating large lan- guage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\n\n[25] K. Moratis, T. Diamantopoulos, D.-N. Nastos, A. Symeonidis, Write me this code: An analysis of chatgpt quality for producing source code, in: Proc. of the 21st IEEE/ACM Int’l Conf. on Mining Software Repositories (MSR 2024), Thessaloniki, Greece, IEEE/ACM, 2024.\n\n[26] T. Copeland, PMD Applied, Vol. 10, Centennial Books, San Francisco,\n\n2005.\n\n[27] K. DePalma, I. Miminoshvili, C. Henselder, K. Moss, E. A. Al Omar, Exploring ChatGPT’s code refactoring capabilities: An empirical study, Expert Systems with Applications, 249 (Part B) 123602, (2024)\n\n[28] Y. Liu, T. Le-Cong, R. Widyasari, C. Tantithamthavorn, L. Li, X. D. Le, D. Lo, Refining ChatGPT-generated code: Characterizing and mitigat- ing code quality issues, ACM Transactions on Software Engineering and Methodology, 33 (5) pp.1–26, (2024)\n\n[29] O. Burn, Checkstyle, http://checkstyle.sourceforge.net/, ac-\n\ncessed: 2025-04-11 (2003).\n\n[30] I. Cordasco, T. Ziade, Flake8: Your tool for style guide enforcement,\n\nPrograma de computador, accessed: 2025-04-11 (2010).\n\n[31] Z. Fan, X. Gao, M. Mirchev, A. Roychoudhury, S. H. Tan, Automated repair of programs from large language models, in: Proc. of 2023 IEEE/ACM 45th Int’l Conf. on Software Engineering (ICSE 2023), pp. 1469–1481, IEEE, 2023.\n\n[32] W. Ma, S. Liu, Z. Lin, W. Wang, Q. Hu, Y. Liu, C. Zhang, L. Nie, L. Li, Y. Liu, LLMs: Understanding code syntax and semantics for code analy- sis, arXiv preprint arXiv:2305.12138 (2023).\n\n[33] E. V. de Paulo Sobrinho, A. De Lucia, M. de Almeida Maia, A systematic literature review on bad smells–5 w’s: which, when, what, who, where, IEEE Transactions on Software Engineering, 47 (1) pp.17–66, (2018)\n\n[34] T. Sharma, D. Spinellis, A survey on software smells, Journal of Systems\n\nand Software, 138 , pp.158–173, (2018)\n\n[35] E. Fernandes, J. Oliveira, G. Vale, T. Paiva, E. Figueiredo, A review-based comparative study of bad smell detection tools, in: Proc. of the 20th int’l conf. on evaluation and assessment in software engineering, 2016, pp. 1–12.\n\n[36] M. S. Haque, J. Carver, T. Atkison, Causes, impacts, and detection ap- proaches of code smell: a survey, in: Proc. of the 2018 ACM Southeast Conference, 2018, pp. 1–8.\n\n17\n\nD. G. Paul, H. Zhu and I. Bayley\n\n[37] R. S. Menshawy, A. H. Yousef, A. Salem, Code smells and detection tech- niques: a survey, in: Proc. of 2021 int’l mobile, intelligent, and ubiquitous computing conference (MIUCC 2021), IEEE, 2021, pp. 78–83.\n\n[38] A. Al-Shaaby, H. Aljamaan, M. Alshayeb, Bad smell detection using ma- chine learning techniques: a systematic literature review, Arabian Journal for Science and Engineering, 45 (4) pp.2341–2369, (2020)\n\n[39] A. Alazba, H. Aljamaan, M. Alshayeb, Deep learning approaches for bad smell detection: a systematic literature review, Empirical Software Engi- neering, 28 (3), 77, (2023)\n\n[40] F. Pecorelli, F. Palomba, D. D. Nucci, A. D. Lucia, Comparing heuristic and machine learning approaches for metric-based code smell detection, in: Proc. of the 2019 IEEE/ACM 27th Int’l Conf. on Program Compre- hension (ICPC 2019), Montreal, Canada, 2019, pp. 1–11, IEEE.\n\n[41] D. Mesbah, N. E. Madhoun, K. A. Agha, H. Chalouati, Leveraging prompt-based large language models for code smell detection: A com- parative study on the mlcq dataset, in: Proc, of The 13th Int’l Conf. on Emerging Internet, Data & Web Technologies (EIDWT-2025), Springer, Matsue, Japan, 2025, pp. 444–454.\n\n[42] PMD, PMD: A source code analyzer, https://pmd.github.io/, ac-\n\ncessed: 2025-02-26.\n\n[43] Checkstyle, Checkstyle: A development tool to help you write Java code that follows a coding standard, https://checkstyle.sourceforge. io/, accessed: 2025-02-26.\n\n[44] C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, B. Regnell, A. Wesslén,\n\net al., Experimentation in software engineering, Springer, 2012.\n\n[45] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, et al., CodeBert: A pre-trained model for programming and natural languages, arXiv preprint arXiv:2002.08155 (2020).\n\n[46] Y. Wang, W. Wang, S. Joty, S. C. Hoi, CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and genera- tion, arXiv preprint arXiv:2109.00859 (2021).\n\n[47] R. Jonnala, J. Yang, Y. Lee, G. Liang, Z. Cao, Measuring and improving the efficiency of Python code generated by LLMs using CoT prompting and fine-tuning, IEEE Access (2025).",
      "content_length": 5590,
      "extraction_method": "Unstructured"
    }
  ]
}