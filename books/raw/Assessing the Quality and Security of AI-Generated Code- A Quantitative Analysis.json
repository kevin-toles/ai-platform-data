{
  "metadata": {
    "title": "Assessing the Quality and Security of AI-Generated Code- A Quantitative Analysis",
    "author": "Abbas Sabra; Olivier Schmitt; Joseph Tyler",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 27,
    "conversion_date": "2025-12-19T17:19:38.459711",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Assessing the Quality and Security of AI-Generated Code- A Quantitative Analysis.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-10)",
      "start_page": 1,
      "end_page": 10,
      "detection_method": "topic_boundary",
      "content": "5 2 0 2\n\ng u A 0 2\n\n] E S . s c [\n\n1 v 7 2 7 4 1 . 8 0 5 2 : v i X r a\n\nAssessing the Quality and Security of AI-Generated Code: A Quantitative Analysis\n\nAbbas Sabra, Olivier Schmitt, Joseph Tyler\n\nSonar\n\nAugust 21, 2025\n\nAbstract\n\nThis study presents a quantitative evaluation of the code quality and security of five promi- nent Large Language Models (LLMs): Claude Sonnet 4, Claude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior research has assessed the functional performance of LLM- generated code [1], this research tested LLM output from 4,442 Java coding assignments through comprehensive static analysis using SonarQube [2]. The findings suggest that although LLMs can generate functional code, they also introduce a range of software defects, including bugs, security vulnerabilities, and code smells. These defects do not appear to be isolated; rather, they may represent shared weaknesses stemming from systemic limitations within current LLM code generation methods. In particular, critically severe issues, such as hard-coded passwords [3, 4] and path traversal vulnerabilities [5, 6], were observed across multiple models. These results in- dicate that LLM-generated code requires verification in order to be considered production-ready. This study found no direct correlation between a model’s functional performance (measured by Pass@1 rate of unit tests) and the overall quality and security of its generated code, measured by the number of SonarQube issues in benchmark solutions that passed the functional tests. This suggests that functional benchmark performance score is not a good indicator of overall code quality and security. The goal of this study is not to rank LLM performance but to high- light that all evaluated models appear to share certain weaknesses. Consequently, these findings support the view that static analysis can be a valuable instrument for detecting latent defects and an important safeguard for organizations that deploy AI in software development.\n\n1\n\nContents\n\n1 Introduction\n\n2 Research Objectives\n\n3 Experiment Setup\n\n3.1 Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Models Under Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Methodology: Code Generation and Static Analysis . . . . . . . . . . . . . . . . . . .\n\n4 Quality Overview of LLM-Generated Code\n\n4.1 Volumetric and structural characteristics . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Functional Performance and Overall Code Quality . . . . . . . . . . . . . . . . . . . 4.3 Overall issue density and distribution by type and severity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4\n\nIssue severity distribution (% of Total Issues)\n\n5 Analysis of Code Smells in LLM-Generated Java\n\n6 Analysis of Bugs in LLM-Generated Java\n\n7 Analysis of Security Vulnerabilities in LLM-Generated Java\n\n8 Discussion: Examples of Common SonarQube Detected Issues\n\n8.1 Theme: Deficient Error Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Theme: Resource Management Lapses . . . . . . . . . . . . . . . . . . . . . . . . . . 8.3 Theme: Critical Security Oversights . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.4 Theme: Excessive Code Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.5 Theme: Maintainability & Best Practice Violations (Redundant Code) . . . . . . . .\n\n9 Case Study: Evolution of Model Performance and Defect Characteristics\n\n9.1 Performance Benchmarks and Defect Severity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.2 Persistence and Evolution of Underlying Issues\n\n10 The Role of Static Analysis in Addressing Potential LLM-Generated Issues\n\n11 Conclusion\n\nAppendix\n\nA.1 Bugs categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Code smells categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Vulnerability categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2\n\n3\n\n4\n\n5 5 5 5\n\n7 7 7 8 9\n\n11\n\n13\n\n15\n\n17 17 17 17 18 18\n\n19 19 19\n\n20\n\n21\n\n23 23 24 25\n\n1\n\nIntroduction\n\nState of the art LLMs from providers such as Anthropic [7], OpenAI [8], and Meta [9] are increasingly utilized in software development for various tasks, including code generation, code autocompletion, and bug remediation. The potential for these tools to enhance developer productivity and accelerate development drives their widespread adoption. However, accelerated development must be balanced with the foundational principles of software quality. Core attributes like reliability, security, and maintainability are crucial for the long-term viability of any software project. Code that fails to meet these standards, whether authored by humans or generated by AI, can accrue technical debt [10, 11], introduce security risks, and degrade system reliability.\n\nThe prevailing discourse on LLMs has largely emphasized their generating capabilities, such as measuring functional correctness [1], sometimes overshadowing the critical need to evaluate the broader quality and security of their output [12, 13]. This concern is amplified by the rapid adoption of LLMs in software development, with reports indicating AI assistants write an average of 46% of developer code [14], while research continues to highlight that the generated code can introduce significant security vulnerabilities and bugs [12, 13]. Consequently, there is an established risk of developers overlooking necessary verification, as studies have found that programmers using AI assistants can produce less secure code while simultaneously showing greater confidence in its security [12].\n\n3\n\n2 Research Objectives\n\nThis investigation seeks to address the aforementioned quality gap. The study is guided by the following central research questions (RQs), which focus on identifying common patterns in the output of leading LLMs rather than on providing a comparative ranking:\n\nRQ1: What is the typical characteristic quality profile of Java code generated by contemporary,\n\nstate-of-the-art LLMs?\n\nRQ2: What are the most common categories and severity levels of issues observed in the code\n\ngenerated by these models?\n\nRQ3: How effectively can static analysis identify these defects and serve as a protective mechanism?\n\nRQ4: Does an improvement in a model’s functional performance correlate with an improvement in\n\nthe quality and security of its generated code?\n\nThis research investigates the gap between the output of LLMs and the quality standards for production-ready software. We posit that LLM-generated code is not immediately fit for produc- tion and requires rigorous verification, making functional metrics alone insufficient for evaluation. This study argues that the common error patterns in LLM-generated code make static analysis an essential tool for ensuring its quality and security.\n\n4\n\n3 Experiment Setup\n\nThis section outlines the experiment design used to evaluate the code quality and security of several leading LLMs using a set of Java programming challenges. The structure facilitates a cross-model comparison and the application of static analysis to identify issue patterns in the generated code.\n\n3.1 Benchmarks\n\nThe evaluation was conducted using Java as the target programming language. The benchmark dataset comprised 4,442 distinct coding problems from three recognized and publicly available sources: MultiPL-E-mbpp-java, MultiPL-E-humaneval-java [15], and ComplexCodeEval [16]. While code was generated for static analysis from all three sources, functional performance (i.e., test pass rates) was evaluated using only the two MultiPL-E benchmarks. These benchmarks were selected to represent a spectrum of Java programming challenges with varying complexity and domain relevance, forming a robust testbed for assessing the LLMs’ code generation capabilities.\n\n3.2 Models Under Test\n\nThe study assessed five prominent LLMs, representing diverse architectures, training methods, and accessibility:\n\nClaude Sonnet 4 and Claude 3.7 Sonnet (Anthropic): Included to examine how code generation quality evolves between successive model generations from the same provider [7].\n\nGPT-4o (OpenAI): Selected for its widespread adoption, as it frequently serves as the underlying technology for popular AI coding tools such as GitHub Copilot [8].\n\nLlama 3.2 90B (Meta): Chosen as a leading open-weight model representative of state-of- the-art, openly available LLMs [9].\n\nOpenCoder-8B (Llama-3.1-8B architecture): A smaller, open-source model included to determine if common code quality issues persist regardless of model size, which could reveal challenges inherent in the LLM methodology itself [17].\n\nFor consistency and reproducibility, identical prompts were provided to each model, and the tem- perature was set to zero.\n\n3.3 Methodology: Code Generation and Static Analysis\n\nOur methodology involved a three-phase process:\n\nCode Generation: Each of the five LLMs was prompted to generate compilable Java solu- tions for all 4,442 benchmark problems. The primary objective was to obtain functional and compilable code to serve as the input for the subsequent quality assessment.\n\nFunctional Performance Evaluation: The code generated for problems from the two MultiPL-E benchmarks was executed against the provided test suites to measure functional performance. This performance was quantified by Pass@1 rate of unit tests.\n\nCross-Model Static Analysis: Each generated Java file was then analyzed using Sonar- Qube [2]. The full default SonarWay Java ruleset, consisting of approximately 550 rules designed to be broadly suitable for most projects, was applied to detect various categories of software issues, including bugs, security vulnerabilities, and code smells. This allowed us to\n\n5\n\ncompare the distribution and prevalence of issues across models, identify shared weaknesses, and characterize the quality of AI-generated code.\n\nThis design enabled a detailed examination of not only whether LLMs could solve coding problems but also how well they did so from a quality and security perspective.\n\n6\n\n4 Quality Overview of LLM-Generated Code\n\nThis section addresses the characteristic quality profile of LLM-generated Java (RQ1) by providing a quantitative overview of the code generated for the 4,442 tasks. The analysis focuses on overarching characteristics and aggregated quality metrics.\n\n4.1 Volumetric and structural characteristics\n\nThe five LLMs exhibited notable differences in the volume and complexity of the Java code they generated. These metrics, detailed in Table 1, provide context for the subsequent analysis of code quality.\n\nTable 1: Comparative code generation metrics across LLMs (4,442 Tasks)\n\nLLM Model\n\nLOC Statements Functions Classes Files Comment Lines Comments (%) Cyclomatic Complexity Cognitive Complexity\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n370,816 288,126 209,994 196,927 120,288\n\n148,932 116,433 83,466 75,368 41,510\n\n46,235 27,496 24,309 22,694 8,338\n\n12,832 10,649 10,475 8,996 5,530\n\n4,442 4,442 4,442 4,442 4,442\n\n20,051 56,459 9,692 15,514 13,165\n\n5.10% 16.40% 4.40% 7.30% 9.90%\n\n81,667 55,485 44,387 37,948 18,850\n\n47,649 42,220 26,450 20,811 13,965\n\nThese metrics reveal distinct generative tendencies. Claude Sonnet 4 produced the most code (370,816 LOC) with the highest cumulative Cyclomatic Complexity (81,667) [18] and Cognitive Complexity (47,649) [19]. In contrast, OpenCoder-8B generated the most concise code (120,288 LOC) with the lowest complexity scores (18,850 Cyclomatic Complexity and 13,965 Cognitive Com- plexity). Comment density also varied widely, from Claude 3.7 Sonnet’s high of 16.4% to GPT-4o’s 4.4% and Claude Sonnet 4’s 5.1%.\n\nThe variance in structural metrics suggests that the choice of an LLM has significant consequences for a project’s long-term maintainability, irrespective of the model’s functional performance. This finding indicates that different architectural philosophies and training data result in varied structural outputs, rather than a consistent progression toward more concise or less complex code in newer models. This diversity underscores a key conclusion: LLM-generated code is not a monolithic entity, and development teams should adopt model selection and code review strategies that account for these structural differences.\n\n4.2 Functional Performance and Overall Code Quality\n\nBeyond structural characteristics, a central question of this study is whether a model’s functional performance—defined here as the success rate of generated code passing its intended tests—correlates with the quality of its generated code (RQ4). This study measured functional performance using the test pass rates from the MultiPL-E benchmarks, while static analysis was conducted on the code generated for all 4,442 tasks. Table 2 summarizes these metrics to provide an initial overview of code quality.\n\n7\n\nTable 2: LLM Performance: Test Pass Rates and SonarQube Issue Rates (4,442 Tasks)\n\nLLM Model\n\nPassing tests % SonarQube Discovered Issues\n\nIssues per passing task\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n77.04 72.46 69.67 61.47 60.43\n\n7,225 6,576 5,476 5,159 3,903\n\n2.11 2.04 1.77 1.89 1.45\n\nClaude Sonnet 4 demonstrated the highest test pass rate at 77.04%, while OpenCoder-8B exhibited the lowest at 60.43%. The \"Issues per Passing Task\" metric, which normalizes total issues against the number of functionally successful outputs, offers additional insight. OpenCoder-8B, despite its lower pass rate, presented the lowest number of issues per passing task (1.45). In contrast, Claude Sonnet 4, the top performer on pass rate, averaged 2.11 issues per passing task.\n\nA key observation is that even when LLM-generated code passes functional performance bench- marks, it is not free of underlying quality defects. For every task that OpenCoder-8B completed successfully, it still averaged 1.45 static analysis issues; similarly, GPT-4o averaged 1.77 issues per passing task. This pattern supports the premise that static code analysis is valuable for assessing the quality of functionally \"passing\" code. These underlying issues in successful tasks represent latent factors that could impact maintainability and reliability over time.\n\nThe consistent presence of quality issues in functionally correct code suggests that relying solely on functional performance benchmarks to evaluate LLM-generated code is insufficient and may introduce hidden risks. This highlights a potential paradox: as models become more capable, they may generate more sophisticated solutions that, while functionally robust, introduce a larger surface area for defects, leading to a greater number of static analysis findings. This dynamic is examined more closely in the following sections.\n\n4.3 Overall issue density and distribution by type and severity\n\nThis section examines the types and severities of issues identified in the Java code generated by the LLMs across the 4,442 tasks. Table 3 provides an overview of the resulting defect metrics.\n\nTable 3: Overall code quality defect metrics per LLM (4,442 Tasks)\n\nLLM Model\n\nSonarQube Discovered Issues\n\nLOC Issues per KLOC Issues per Passing Task\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n7,225 6,576 5,476 5,159 3,903\n\n370,816 288,126 209,994 196,927 120,288\n\n19.48 22.82 26.08 26.20 32.45\n\n2.11 2.04 1.77 1.89 1.45\n\nIssue density, measured as issues per thousand lines of code (KLOC), ranged from 19.48 for Claude Sonnet 4 to 32.45 for OpenCoder-8B. These differing densities appear to illustrate distinct profiles of issue generation rather than a simple quality ranking. A model that generates a larger volume of code might exhibit a higher total number of issues, even if its per-line quality is comparatively reasonable. The distribution of issues by type, shown in Table 4, reveals similarity across models.\n\n8\n\nTable 4: Distribution of issue types by LLM (absolute counts and percentage of total issues per model)\n\nLLM Model\n\nTotal Bugs % Bugs Total Vulnerabilities % Vulnerabilities Total Code Smells % Code Smells\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n423 352 406 398 247\n\n5.85% 5.35% 7.41% 7.71% 6.33%\n\n141 116 112 123 67\n\n1.95% 1.76% 2.05% 2.38% 1.72%\n\n6,661 6,108 4,958 4,638 3,589\n\n92.19% 92.88% 90.54% 89.90% 91.95%\n\nA key observation from the data is the striking similarity in the distribution of issue types across all evaluated models. Despite their varied architectures, each model produced a comparable mix of defects: approximately 90-93% code smells, 5-8% bugs, and around 2% security vulnerabilities (as shown in Table 4). This consistency across different models suggests a systemic pattern in the code generation process of current LLMs. While code smells are the most frequent, the consistent introduction of bugs and, more critically, security vulnerabilities is particularly noteworthy. A defect rate that includes a 5-8% chance of being a bug and a ≈2% chance of being a security vulnerability is significant. This finding underscores that LLM-generated code, even when it passes functional performance tests, is not immediately suitable for production environments. It highlights the critical need for rigorous static analysis and expert human review to identify and remediate these underlying quality and security issues before deployment, thereby preventing the accumulation of technical debt and security risks. Table 5 details the issue density by type for each model.\n\nTable 5: Issue density by type and LLM (per KLOC)\n\nLLM Model\n\nBug Density (Bugs/KLOC) Vulnerability Density (Vuln./KLOC) Code Smell Density (Smells/KLOC)\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n1.14 1.22 1.93 2.02 2.05\n\n0.38 0.40 0.53 0.62 0.56\n\n17.96 21.20 23.61 23.55 29.84\n\nDespite variations in density, all evaluated models produced all three types of issues: bugs, vulner- abilities, and code smells.\n\n4.4\n\nIssue severity distribution (% of Total Issues)\n\nThis section analyzes the proportional distribution of severity levels (Blocker, Critical, Major, Mi- nor) within each issue type to provide insight into the potential impact of the defects generated by the models. Table 6 shows the severity distribution for bugs.\n\nTable 6: Bug distribution (% of total bugs per model)\n\nLLM Model\n\nBLOCKER CRITICAL MAJOR MINOR\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n13.71 7.10 7.14 13.82 9.24\n\n4.49 3.98 3.45 4.77 12.05\n\n54.14 61.93 74.63 56.78 49.00\n\n27.66 26.99 14.78 24.62 29.72\n\nIn the bug category, GPT-4o exhibited a notable tendency, with nearly 75% of its bugs categorized\n\n9\n\nas ‘MAJOR’, indicating a propensity to produce significant functional defects. In contrast, Claude Sonnet 4 and Llama 3.2 90B presented the highest proportion of ‘BLOCKER’ bugs (approximately 14%), which represent defects that can prevent application functionality. OpenCoder-8B showed a high percentage of ‘CRITICAL’ bugs (12%) compared to other models, which generally ranged between 3% and 5%. For code smells, the severity distribution is presented in Table 7.\n\nTable 7: Code smell distribution (% of total code smells per model)\n\nLLM Model\n\nBLOCKER CRITICAL MAJOR MINOR\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n0.25 0.30 0.22 0.33 0.31\n\n8.89 12.96 5.79 5.36 5.49\n\n51.22 45.65 43.45 39.64 37.28\n\n39.65 41.09 50.54 54.67 56.93\n\nFor code smells, which relate primarily to maintainability, most issues were distributed across the ‘MAJOR’ and ‘MINOR’ categories for all models. Claude 3.7 Sonnet generated a markedly higher proportion of ‘CRITICAL’ code smells (13%) compared to the other models. Conversely, Llama 3.2 90B and OpenCoder-8B exhibited the highest percentages of ‘MINOR’ code smells, suggesting their maintainability issues were, on average, of less severe immediate impact. The severity distribution for vulnerabilities, shown in Table 8, highlights a more critical trend.\n\nTable 8: Vulnerability distribution (% of total vulnerabilities per model)\n\nLLM Model\n\nBLOCKER CRITICAL MAJOR MINOR\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n59.57 56.03 62.50 70.73 64.18\n\n28.37 28.45 23.21 22.76 26.87\n\n5.67 5.17 5.36 1.63 1.49\n\n6.38 10.34 8.93 4.88 7.46\n\nAn important finding from the analysis of vulnerabilities is that all models produced a high percent- age of ‘BLOCKER’ and ‘CRITICAL’ vulnerabilities. This observation underscores the importance of thorough security scanning for AI-generated code. For example, Llama 3.2 90B generated a high proportion of these issues, with over 70% of its identified vulnerabilities classified as ‘BLOCKER’. Similarly, results for OpenCoder-8B and GPT-4o indicated that nearly two-thirds of their detected vulnerabilities were of the highest severity levels.\n\n10",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 11-18)",
      "start_page": 11,
      "end_page": 18,
      "detection_method": "topic_boundary",
      "content": "5 Analysis of Code Smells in LLM-Generated Java\n\nIn addressing the most common categories of issues (RQ2), this section provides a detailed analysis of code smells. Code smells can serve as indicators of deeper structural problems within source code. While not representing direct functional errors, they can impede maintainability, comprehensibility, and evolvability, often contributing to the accumulation of technical debt [10, 11] or the introduction of bugs over time. As noted in the previous section, code smells [20] were the most frequent issue type identified across all evaluated LLMs. This section provides an analysis of the specific sub- categories of code smells observed, as detailed in Table 9, and explores potential factors that may contribute to these challenges for LLMs. For additional details regarding the categories, please refer to the Appendix.\n\nTable 9: Sub-categories of code smells and their origins (% of total code smells for model)\n\nCategory\n\nClaude Sonnet 4 (%)\n\nClaude 3.7 Sonnet (%)\n\nGPT-4o (%)\n\nLlama 3.2 90B (%)\n\nOpenCoder-8B (%)\n\nPotential Contributing Factors\n\nDead / Unused / Redundant code Design / Framework best-practices Assignment / Field / Scope visibility Collection / Generics / Param / Type Regex / Pattern / String / Format Cognitive / Computational complexity Control / Conditional-logic smell Deprecated / Obsolete APIs Naming / Style / Documentation Exception-handling smell Other\n\n14.83 22.26 11.96 13.94 13.70 4.25 4.67 2.01 2.69 0.05 9.64\n\n17.43 18.58 15.35 11.23 11.80 8.43 3.91 2.34 2.50 0.08 8.33\n\n26.3 20.81 13.21 9.92 7.36 3.73 4.03 2.08 2.84 0.06 9.64\n\n34.82 18.84 11.32 9.03 6.81 2.67 3.02 2.89 2.16 0.02 8.41\n\n42.74 12.45 11.95 7.89 5.29 2.79 2.20 4.01 1.89 0.06 8.72\n\nRequires non-local, project-wide reference analysis. Lacks context of project-specific framework conventions. Requires class-wide or non-local scope resolution. Requires deep semantic understanding of the API. Logical flaws may only be apparent at runtime. Complexity is a non-local property of the code. Difficulty in balancing correctness and readability. Requires knowledge of library deprecation roadmaps. Difficulty generalizing from project-specific conventions. Requires analysis of the cross-package dependency graph.\n\nThe following analysis delves into the most prominent categories from Table 9, exploring potential factors that contribute to them.\n\nDead / Unused / Redundant Code: This category was prevalent in the output of models such as Llama 3.2 90B (34.82% of its code smells) and OpenCoder-8B (42.74%). LLMs may struggle in this area, potentially because identifying such code can require a whole-project reference analysis. Since LLMs often operate with a limited context window, it can be challenging for them to determine if a generated element is utilized elsewhere in a larger application. This limitation could lead to the generation of syntactically plausible but unreferenced code, contributing to codebase bloat.\n\nDesign / Framework Best Practices: Claude Sonnet 4 showed a higher percentage of issues in this category (22.26%), which may reflect the model’s tendency to generate more thorough code by attempting to handle numerous edge cases. This approach, while sometimes leading to more robust error management, can also create unnecessary logical complexity if the model cannot infer the specific context. However, all LLMs may face a common challenge in this area, as adherence to specific design patterns or framework conventions can depend on knowledge of organizational rules or proprietary mechanisms that may not be well-represented in general training data.\n\nAssignment / Field / Scope Visibility: This issue was consistently observed, typically ranging from 11% to 15% of total code smells per model. This type of issue may arise because correctly determining scope can require a comprehensive, class-wide context. Defining the narrowest necessary scope for a variable might depend on an understanding of all interactions within a class, which could be challenging for a model to infer from localized generation.\n\nCollection / Generics / Parameter / Type Issues: Issues in this category may be common because the correct use of generics and parameterized types often requires a deeper understanding of API semantics. While LLMs may employ these features accurately in simple cases, ensuring\n\n11\n\ntype safety across complex interactions could require a level of semantic comprehension that may be challenging for current generative models.\n\nRegex / Pattern / String / Format: LLMs may generate regular expressions or string format- ting operations that are syntactically valid but contain subtle logical flaws. This type of issue may occur because such flaws are often revealed only through execution or deeper semantic analysis, which is typically outside the scope of token-by-token generation.\n\nCognitive / Computational Complexity: High cognitive complexity was a notable issue, par- ticularly for Claude 3.7 Sonnet (8.43% of its code smells). LLMs may face challenges with this metric, as complexity is often a non-local property of code. Since LLMs tend to optimize for lo- cal token probability, a sequence of individually plausible segments might cumulatively result in a method that is globally complex, as the model may not have an explicit mechanism to optimize for an overall complexity score.\n\nControl / Conditional-Logic Smell: LLMs may generate convoluted conditional logic. This issue may arise because capturing the nuance between functional correctness and code readability can be difficult. A model might not prioritize simpler or more idiomatic control flow structures if a more complex alternative also appears to satisfy the immediate generation objective.\n\nDeprecated / Obsolete APIs: The use of deprecated APIs and outdated dependencies poses a significant security risk, moving beyond a simple code smell. An LLM’s tendency to favor older API versions—a behavior likely influenced by its training data’s knowledge cut-off or the prevalence of older code examples—can inadvertently lead to the re-introduction of libraries with known vulner- abilities (CVEs). This specific blind spot highlights the necessity of complementing static analysis with Software Composition Analysis (SCA). While static analysis inspects the generated code itself, SCA provides the crucial, additional security layer of scanning its dependencies for known exploits, directly addressing a risk vector that is particularly pronounced in how LLMs currently operate.\n\nNaming / Style / Documentation: While LLMs may adhere to common coding conventions, they may not capture team- or project-specific naming styles. Such conventions vary widely across contexts, making them difficult for a model to generalize from disparate training data.\n\nException-Handling Smell: The use of generic exceptions appears to be a frequent code smell across the evaluated models. Formulating specific exception handling can require a detailed analysis of dependencies across a codebase, which is a challenge for common use of LLMs that operate primarily within a localized context.\n\n12\n\n6 Analysis of Bugs in LLM-Generated Java\n\nContinuing the investigation into the most common categories and severity levels of defects (RQ2), this section examines the bugs identified in the LLM-generated Java code. Bugs represent functional defects in code that can lead to incorrect behavior, application crashes, or unexpected outcomes. Static analysis detects bugs that can potentially compromise application stability and correctness. While numerically less frequent than code smells, their potential impact is often more immediate and severe. This section explores common bug categories observed in the LLM-generated Java code, which are summarized in Table 10, and discusses potential factors contributing to their introduction.\n\nTable 10: Sub-categories of bugs and their origins (% of total bugs for model)\n\nCategory\n\nClaude Sonnet 4 (%)\n\nClaude 3.7 Sonnet (%)\n\nGPT-4o (%)\n\nLlama 3.2 90B (%)\n\nOpenCoder-8B (%)\n\nPotential Contributing Factors\n\nControl-flow mistake API contract violation Exception handling Resource management / Leak Type-safety / Casts Concurrency / Threading Null / Data-value issues Performance / Structure Pattern / Regex Data-structure bug Serialization / Serializable Other\n\n14.83 10.29 16.75 15.07 11.24 9.81 7.89 4.31 2.63 1.44 0.00 5.74\n\n23.62 14.12 16.71 8.36 12.97 1.44 7.49 6.34 1.15 1.15 0.58 6.05\n\n48.15 8.64 11.60 7.41 7.90 1.73 8.89 3.95 0.74 0.00 0.00 0.99\n\n31.06 14.90 14.39 12.88 6.82 1.26 5.81 2.78 0.25 1.01 0.76 8.08\n\n21.37 19.35 14.52 9.68 7.66 2.82 6.85 5.24 2.42 1.61 1.61 6.85\n\nRequires deep, non-local path reasoning beyond pattern matching. Requires analysis of error propagation across multiple code branches. Dependent on understanding library intent and return semantics. Resource lifecycle management is a non-local problem. Requires precise tracking of static-type provenance. Concurrency concepts (e.g., atomicity) are underrepresented in corpora. Difficulty tracking nullability across complex data-flow paths. Potential generation of inefficient or suboptimal algorithms. Regex logical errors are often only evident at runtime. Proper collection usage is contingent on semantic intent. Requires knowledge of the framework’s object-graph semantics.\n\nA closer look at these categories, presented in Table 10, reveals common challenges LLMs face in ensuring the semantic correctness and runtime integrity of generated code.\n\nControl-Flow Mistakes: This category of bugs was particularly prominent in code from GPT-4o (48.15% of its total bugs) and was also significantly represented in output from Llama 3.2 90B (31.06%) and OpenCoder-8B (21.37%). These issues may be prevalent because ensuring correct control flow can require deep path reasoning. While models can generate plausible conditional statements or loops based on learned patterns, they may face challenges with the multi-step logic needed to ensure correctness across all execution paths, especially with intricate branching or edge cases.\n\nAPI Contract Violations: Models from the Llama family, specifically OpenCoder-8B (19.35%) and Llama 3.2 90B (14.90%), showed a higher proportion of issues in this area. Correctly using Application Programming Interfaces (APIs) can require analyzing error propagation and under- standing return value semantics. LLMs might overlook these nuances, potentially leading to bugs where API return values are ignored or misinterpreted. This could suggest a challenge for the models in comprehending sequential, stateful operations.\n\nException Handling (Bugs): Distinct from exception handling smells, this category pertains to functional defects. Addressing them can require knowledge of library intent. LLMs might gen- erate code that catches overly broad exceptions without reacting appropriately or that disregards checked exceptions, potentially due to challenges in inferring the specific purpose of library-thrown exceptions from training data.\n\nResource Management / Leaks: Claude Sonnet 4 demonstrated a higher proportion of issues in this category (15.07%). Resource leaks may be a persistent issue, possibly because resource lifecycles (e.g., opening and closing streams) can span multiple calls, which may be difficult to track within a local context window. This could lead to failures in ensuring that resources are properly closed across all paths, potentially introducing risks for long-running applications.\n\n13\n\nType-Safety / Casts: Issues such as illegal type casts may occur because ensuring cast safety can require precise tracking of a variable’s static-type history. A model might generate code involving type casting without being able to fully track the type history through complex data flows, which could result in runtime ClassCastException errors.\n\nConcurrency / Threading: Claude Sonnet 4 also showed a comparatively higher share of bugs in this category (9.81%). Correct concurrent programming is known to be difficult, and LLMs may face challenges in this area because concepts like thread state and atomicity can be complex to learn. Models may not always generate code that sufficiently covers complex threading scenarios, which could lead to potential race conditions or deadlocks.\n\nNull / Data-Value Issues: This was a common problem, accounting for 5-9% of identified bugs across the evaluated models. NullPointerExceptions may arise if a model faces difficulties tracking nullability across complex data flows. As a result, it might not consistently perform necessary null checks before dereferencing an object.\n\nPerformance / Structure (Bugs): LLMs may generate code that is functionally correct but ex- hibits poor runtime performance (e.g., inefficient loops). Such issues might occur because LLMs tend to optimize for local token generation probability rather than for global performance characteristics.\n\nPattern / Regex (Bugs): This category involves regular expressions that are functionally incor- rect. These issues may arise because regex edge cases often surface only during execution or deeper semantic analysis, which may extend beyond the focus on syntactic validity during generation.\n\nData-Structure Bugs: The misuse of data structures, such as attempting to access an element be- yond array bounds, may occur because the correct application of collections can be tied to semantic intent, a nuance that might be challenging for an LLM to capture fully.\n\nSerialization / Serializable: Bugs such as a class failing to implement Serializable when required may stem from the non-local, framework-dependent nature of such requirements, which can be chal- lenging for context-limited generation. The nature of these observed bugs suggests that LLMs may face challenges with the semantic correctness and runtime implications of the code they generate, potentially prioritizing syntactic plausibility over comprehensive functional integrity.\n\n14\n\n7 Analysis of Security Vulnerabilities in LLM-Generated Java\n\nAs a critical component of understanding the common issue categories (RQ2), this section ana- lyzes the security vulnerabilities found in the generated code. Security vulnerabilities represent exploitable flaws in code that can lead to data compromise, service disruption, or unauthorized ac- cess. While comprising a smaller percentage of total issues (approximately 2% across models), the 67 distinct types of vulnerabilities identified in this analysis are of significant concern due to their potential impact. Recent studies have highlighted these risks, identifying vulnerabilities in code generated by a range of models [12], which aligns with general industry awareness of key security risks [21]. A potential contributing factor to the prevalence of these vulnerabilities is that LLMs optimize for token likelihood based on training data, which may include insecure or outdated code snippets. Table 11 provides a detailed breakdown of the vulnerability sub-categories identified in this study.\n\nTable 11: Sub-categories of security vulnerabilities and their origins (% of total vulnerabilities for model)\n\nCategory\n\nClaude Sonnet 4 (%)\n\nClaude 3.7 Sonnet (%)\n\nGPT-4o (%)\n\nLlama 3.2 90B (%)\n\nOpenCoder-8B (%)\n\nPotential Contributing Factors\n\nPath-traversal & Injection Hard-coded credentials Cryptography misconfiguration XML External Entity (XXE) Inadequate I/O error-handling Certificate-validation omissions JSON-injection risk JWT signature not verified Other\n\n34.04 14.18 24.82 10.64 4.96 2.84 0.71 0.00 7.80\n\n31.03 10.34 23.28 15.52 7.76 4.31 0.00 0.00 7.76\n\n33.93 17.86 19.64 13.39 7.14 2.68 0.89 0.00 4.46\n\n26.83 23.58 22.76 19.51 4.88 0.00 0.81 0.00 1.63\n\n28.36 29.85 22.39 5.97 7.46 2.99 1.49 1.49 0.00\n\nRequires non-local taint analysis from a data source to a sink. Security-sensitive intent of constants is semantically unclear. Requires current knowledge of secure cryptographic algorithms and modes. Secure parser configuration is a non-local, multi-file problem. Semantic distinction between critical and benign errors is subtle. Secure, library-specific SSL/TLS usage patterns are rare in corpora. Trust boundaries are often implicit in data builders and serializers. Requires current, library-specific security token best practices.\n\nAnalysis of the vulnerabilities detailed in Table 11 points to systemic weaknesses related to non-local taint analysis and the semantic understanding of sensitive data.\n\nPath-Traversal & Injection: This category was a dominant type of vulnerability across all models, observed, for instance, in 34.04% of Claude Sonnet 4’s vulnerabilities and 33.93% of GPT- 4o’s. Preventing such flaws can require taint-tracking from an input source to a sensitive sink, a form of non-local analysis. The models may generate code that performs a function correctly but does not fully account for how unvalidated user input could manipulate file paths or inject commands. This challenge in performing comprehensive data flow analysis is a known security concern [5, 6].\n\nHard-Coded Credentials: This critical vulnerability was particularly prevalent in Llama-family models, appearing in 29.85% of OpenCoder-8B’s vulnerabilities and 23.58% of Llama 3.2 90B’s. This may arise because constant strings can appear benign, and their security-sensitive intent may not be apparent from common patterns in training data. A password string literal, for example, might not be treated as semantically distinct from any other string, particularly if similar insecure practices are present in the training corpus [3, 4].\n\nCryptography Misconfiguration: This was another significant area of weakness, exemplified by Claude Sonnet 4 at 24.82% of its vulnerabilities. Secure cryptography often requires precise knowledge of secure versus weak cipher algorithms and modes. LLMs may reproduce patterns involving deprecated or weak cryptographic primitives, possibly because their training data contains such examples and may not reflect the most up-to-date standards.\n\nXML External Entity (XXE): Vulnerabilities related to XXE may occur because correct config- urations of XML parsers can span multiple files. Lacking full application context of how the parser is configured, an LLM might generate XML parsing code with insecure default settings [22, 23].\n\n15\n\nInadequate I/O Error-Handling (Security): Failures to properly handle I/O errors can lead to security issues. LLMs may face challenges in this area, as distinguishing between security-critical and routine operational errors can be a subtle distinction that requires deeper contextual understanding.\n\nCertificate-Validation Omissions: Secure SSL/TLS communication typically requires proper certificate validation. LLMs may omit these steps, possibly because comprehensive, library-specific SSL/TLS usage patterns are underrepresented or oversimplified in general training data.\n\nJSON-Injection Risk: LLMs may fail to respect the trust boundaries implicit in JSON builders or serializers, resulting in insecure constructions when unsanitized user-controlled data is incorporated.\n\nJWT Signature Not Verified: Secure handling of JSON Web Tokens (JWT) generally requires verification of the token’s signature. An LLM might omit this step, possibly because doing so requires adherence to up-to-date, library-specific best practices that may not be current or compre- hensively represented in its training data.\n\nThe consistency of these vulnerabilities across different LLMs points to their training data as the most plausible origin. By learning from and replicating insecure code, the models demonstrate an inability to distinguish secure patterns from insecure ones, revealing a critical limitation in the current code generation paradigm. Therefore, while prompt engineering is useful, it is not a sufficient safeguard on its own. To mitigate these risks, applying rigorous security-focused static analysis (SAST) and expert human review to LLM-generated code is essential, particularly when it handles sensitive data or operations.\n\n16\n\n8 Discussion: Examples of Common SonarQube Detected Issues\n\nThis section provides concrete examples of common SonarQube detections, building upon the de- tailed categorization of Code Smells, Bugs, and Vulnerabilities presented in Sections 5, 6, and 7. These examples are intended to illustrate potential shared weaknesses in LLM-generated code. The focus is on how specific detections may relate to the broader issue categories discussed in those sections, rather than on model-specific prevalence.\n\n8.1 Theme: Deficient Error Handling\n\nIllustrative Rule: java:S112 – Define and throw a dedicated exception instead of using a generic one.\n\nObservation: A common pattern observed across the evaluated LLMs was the use of generic excep- tions (e.g., throw new Exception()) rather than specific, custom exceptions. This was frequently flagged by SonarQube rule java:S112 and was among the top issues for models like Claude Sonnet 4, GPT-4o, and Llama 3.2 90B.\n\nInterpretation: This observation may align with the challenge related to exception handling, as discussed in Section 5, which can require a \"cross-package dependency graph analysis.\" Generating specific exceptions often involves a broader understanding of an application’s error handling strategy and class hierarchy—a level of non-local context that may be challenging for an LLM to infer. This behavior is also symptomatic of a tendency of LLMs to lack specificity[add ref?], perhaps in order to avoid hallucinations; the generated code will avoid execution errors, but at the cost of understanding the nuance of the scenario the exception is designed to handle.\n\n8.2 Theme: Resource Management Lapses\n\nIllustrative Rule: java:S2095 – Use try-with-resources or close this resource in a \"finally\" clause.\n\nObservation: Failure to properly close resources, such as streams or network connections, was a recurring Blocker-level bug. Rule java:S2095 identified numerous instances of this across different models, including 54 instances for Claude Sonnet 4, 25 for GPT-4o, and 50 for Llama 3.2 90B.\n\nInterpretation: This bug category may be related to the challenge where a resource life-cycle spans many calls, making it difficult to manage. An LLM might lack the reasoning capability to consistently plan and track which resources are opened, and therefore fail to ensure its closure on all possible execution paths.\n\n8.3 Theme: Critical Security Oversights\n\nIllustrative Rule: java:S6437 – Revoke and change this password, as it is compromised - i.e., hardcoded password [3].\n\nObservation: The Blocker vulnerability of hardcoded credentials was detected in code generated by all five evaluated LLMs. For instance, 20 instances were found for Claude Sonnet 4, 20 for GPT-4o, and 29 for Llama 3.2 90B.\n\nInterpretation: This vulnerability arises from the model’s indiscriminate handling of string literals. The LLM does not differentiate between security-sensitive constants, such as passwords or API keys, and benign string values. Consequently, it embeds sensitive data directly into the source code as hardcoded constants. This behavior is consistent with a model designed to replicate plausible\n\n17\n\nstatistical patterns observed in its training data, which often includes insecure examples, rather than performing security-specific semantic analysis.\n\n8.4 Theme: Excessive Code Complexity\n\nIllustrative Rule: java:S3776 – Refactor this method to reduce its Cognitive Complexity.\n\nObservation: Many LLMs, notably Claude 3.7 Sonnet (422 instances) and GPT-4o (112 instances), generated methods with high Cognitive Complexity, which was flagged as a Critical issue by Sonar- Qube.\n\nInterpretation: Autoregressive LLMs generate tokens one at a time and are optimized to ensure generated code is locally coherent. Code architecture is of secondary importance during training, and at inference time the accumulating complexity of a given method is not tracked, therefore it is not surprising that LLMs generate code with high complexity, especially for tasks which require more sophisticated architecture.\n\n8.5 Theme: Maintainability & Best Practice Violations (Redundant Code)\n\nIllustrative Rule: java:S2094 – Remove this empty class, write its code or make it an \"interface\".\n\nObservation: The generation of empty classes or methods was a frequent Minor code smell across multiple models, including GPT-4o (531 instances) and OpenCoder-8B (661 instances).\n\nInterpretation: This appears to be a manifestation of the \"Dead / Unused / Redundant code\" smell category. This may be linked to the challenge of performing a \"whole-project reference analy- sis, not snippet.\" An LLM might generate such placeholder structures as part of a broader pattern but not subsequently populate them or identify their redundancy. These examples collectively suggest that common SonarQube detections can be indicative of potential systemic weaknesses in current LLM code generation approaches. The consistency of these anti-patterns across diverse LLMs may suggest that they are not merely random errors, but result from patterns learned from training data, or from challenges in translating high-level requirements into robust and maintainable code. These thematic groupings suggest that LLMs may face challenges with aspects of software engineering that involve foresight, strategic planning, and an understanding of non-local conse- quences—qualities often associated with the engineering discipline in software development. As we saw for error handing, this could be another example of LLMs avoiding hallucinations by lacking specificity, in this case generating pseudo-implementations.\n\n18",
      "page_number": 11
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 19-27)",
      "start_page": 19,
      "end_page": 27,
      "detection_method": "topic_boundary",
      "content": "9 Case Study: Evolution of Model Performance and Defect Char-\n\nacteristics\n\nTo directly address whether an improvement in a model’s functional performance correlates with an improvement in code quality (RQ4), this section presents a case study comparing two models from different generations. The analysis between Claude 3.7 Sonnet and its successor, Claude Sonnet 4, indicates that while benchmark performance can improve, the nature and severity of certain flaws might increase.\n\n9.1 Performance Benchmarks and Defect Severity\n\nImproved Functional Performance: The newer model, Claude Sonnet 4, achieved a higher benchmark score by passing 77.04% of tasks, compared to the older model’s 72.46%. This suggests progress in the model’s primary function of generating code that passes given tests.\n\nIncreased Bug Severity: An analysis of the generated bugs indicates a notable trend. The proportion of ‘BLOCKER’ bugs nearly doubled in Claude Sonnet 4, increasing to 13.71% from Claude 3.7 Sonnet’s 7.1%.\n\nIncreased Vulnerability Severity: A similar pattern was observed for security vulnerabilities. The proportion of ‘BLOCKER’ vulnerabilities rose from 56.03% in the older model to 59.57% in the newer model, suggesting that when the newer model introduced a vulnerability, it had a higher probability of being of the highest severity.\n\n9.2 Persistence and Evolution of Underlying Issues\n\nPersistent Code Quality Issues: The general profile of issues in the generated code appears largely consistent across model versions. For both models, code smells constituted the majority of flaws (over 92%). Despite its higher benchmark score, the newer Claude 4 still generated 2.11 issues for every test it passed, which may indicate that improved functional performance does not necessarily equate to a corresponding improvement in all code quality attributes.\n\nEvolution of the Defect Profile: Comparing the model versions reveals an evolving defect profile, not a simple reduction of flaws. Although the general defect categories persisted, their distribution shifted—for instance, the newer Claude 4 model produced a higher proportion of ‘Concurrency / Threading’ bugs. This suggests that changes in training strategies cause fundamental code genera- tion challenges to evolve rather than be resolved.\n\nThis case study suggests that progress in AI models is not uniform across all quality attributes. An increase in benchmark scores may paradoxically accompany more severe bugs and vulnerabilities, which underscores the critical role of rigorous static code analysis when applying LLMs to software development.\n\n19\n\n10 The Role of Static Analysis in Addressing Potential LLM-Generated\n\nIssues\n\nThe findings from the preceding sections regarding systemic issues in LLM generated code lead to the questions of detection and mitigation. To that end, this section addresses how effectively static analysis can serve as a protective mechanism for development teams using AI assisted tools (RQ3). In this capacity, static analysis, as exemplified by SonarQube in this study, offers a valuable safeguard by providing an automated and consistent mechanism for flagging known anti-patterns, including resource management lapses and critical security vulnerabilities, before they enter a code- base.\n\nThe value of a tool like SonarQube can be seen in its capacity to detect specific, and often critical, flaws of the types observed in the LLM-generated code. For instance, the security vulnerability of hardcoded credentials (SonarQube rule java:S6437) [3] was consistently observed in this study. As discussed in Section 7, this appears to be a common pitfall for LLMs, possibly related to challenges in discerning the sensitive nature of certain constants. Similarly, resource management lapses, such as unclosed streams (flagged by java:S2095), were identified as recurring Blocker-level bugs. This finding may align with the previously noted challenge for LLMs in managing resource lifecycles that extend beyond a local context view.\n\nFurthermore, the code smell of high Cognitive Complexity (flagged by java:S3776) was common in the output of several LLMs, including Claude 3.7 Sonnet (422 instances) and GPT-4o (112 instances). This may reflect a tendency for LLMs to optimize for local token generation, potentially without accounting for global complexity metrics. Such complex code can impede maintainability and testability.\n\nIn these examples, static analysis tools like SonarQube can provide an automated and consistent mechanism for flagging known anti-patterns before they creep into a codebase. The comprehensive rule sets of such tools are often designed to cover a wide spectrum of issues—including bugs, vul- nerabilities, and code smells—which aligns well with the types of potential flaws identified in this study. Automated detection may be particularly valuable because manual review, while important, might not consistently identify all such issues, especially when managing large volumes of code.\n\nStatic analysis tools may become particularly important within an LLM-driven development paradigm, as they can provide a consistent, objective baseline for quality and security that may not be an inher- ent feature of probabilistic generative models. As LLMs can introduce variability, the deterministic and rule-based nature of static analysis may offer a valuable safeguard.\n\nAs LLMs become more integral to software development, the function of static analysis tools could evolve from a quality and security assurance measure to a component of responsible AI adoption. Static analysis may help bridge the gap between the output of LLMs and the quality and security standards often required in professional software engineering. The integration of such tools into Continuous Integration/Continuous Deployment (CI/CD) pipelines can further enhance the value of static analysis by allowing for continuous validation of code contributions.\n\n20\n\n11 Conclusion\n\nThis quantitative analysis of five prominent LLMs over 4,442 Java tasks provides clear insights into the quality and security of AI-generated code. The findings address our four primary research questions, leading to a central conclusion: LLMs are powerful but imperfect coding assistants, and their output must be rigorously verified.\n\nFirst, the study confirms that all evaluated models produce issues. None generated consistently defect-free code, instead introducing a diverse spectrum of defects (RQ1). We classify these defects into the three analyzed types: code smells that degrade maintainability (e.g., excessive complexity), bugs that impact runtime reliability (e.g., resource leaks), and critical security vulnerabilities (RQ2). Security vulnerabilities include severe flaws such as hardcoded passwords [3, 4], path traversal vulnerabilities [5, 6], and XML External Entity (XXE) injection flaws [22, 23], demonstrating that static analysis is an effective mechanism for identifying these latent risks (RQ3).\n\nSecond, the research shows no correlation between a model’s functional performance and the quality of its code (RQ4). This leads to several critical insights for model selection. We found that bigger is not necessarily better, as a model’s scale or novelty did not guarantee higher-quality output. Critically, smaller can be just as good or better, with smaller models sometimes producing cleaner code for the tasks they successfully passed. This means that understanding your model is vital; teams must look beyond benchmark scores and evaluate a model’s unique defect profile to make informed choices.\n\nThird, these defects are best understood as features of the current technology, not bugs. They appear to be inherent consequences of a methodology that relies on replicating statistical patterns rather than performing semantic analysis. A prime example of this inherent risk is the use of outdated dependencies. Because LLMs are trained on older code, they frequently generate solutions with deprecated APIs or libraries containing known vulnerabilities (CVEs). This underscores the necessity of complementing static analysis with Software Composition Analysis (SCA) to manage a risk vector intrinsic to AI-assisted development.\n\nIn conclusion, the integration of LLMs into software development is transformative; however, lever- aging this capability effectively calls for informed vigilance. By understanding the potential pitfalls of LLM-generated code and by employing automated analysis tools, the software development com- munity can better navigate this new frontier, taking full advantage of AI while upholding established principles of high-quality, secure, and maintainable software.\n\nFuture Work. This study suggests several potential avenues for future investigation:\n\nThe impact of various prompt engineering and fine-tuning strategies aimed at mitigating the identified weaknesses in LLM-generated code.\n\nLongitudinal studies tracking the maintainability and technical debt accumulation of software systems with significant LLM contributions.\n\nThe effectiveness of LLMs in autonomously refactoring issues identified by static analysis tools, potentially creating a feedback loop for automated code improvement.\n\nComparative analyses of LLM performance on different programming languages to investigate whether these weaknesses are universal or vary by language.\n\nResearch into LLM architectures or training methodologies that could more directly address the challenges LLMs appear to face in generating robust and secure code.\n\n21\n\nSuch advancements could contribute to a new generation of LLMs that function as more reliable software engineering assistants.\n\n22\n\nAppendix\n\nA.1 Bugs categories\n\nTable 12: Bugs Categories and Sonar Rules\n\nBugs Categories\n\nDescription / Rule Examples\n\nSonar Rules Assigned\n\nAPI contract violation\n\nReturns, parameters, types, result ignored, equals/hashcode mismatch, contract breaking, incompatible\n\njava:S1206 java:S1221 java:S2109 java:S2159 java:S2177 java:S2225 java:S4143 java:S6863 java:S7184 java:S899\n\nControl-flow mistake\n\nAlways true/false, conditional blocks, unreachable, logic error, infinite/incorrect branching\n\njava:S2189 java:S2583 java:S3923 javabugs:S2190\n\nException handling\n\nSwallowing, suppressing, not propagating, mishandling exceptions and finally\n\njava:S1143 java:S1163 java:S2142 java:S3551\n\nResource mgmt/leak\n\nUnclosed resources, leaks, missed close, incorrect freeing\n\njava:S2095 java:S2116 java:S2886 java:S5164\n\nConcurrency/threading\n\nBroken synchronization, volatile misuse, thread-unsafe ops, double-checked locking, param sync\n\njava:S2168 java:S2222 java:S2445 java:S3077 java:S3078\n\nType-safety/casts\n\nBroken/illegal/unsafe casts, illegal type use, invalid class cast, use-after-cast, wrong generic\n\njava:S1872 java:S2175 java:S2184 java:S2677 javabugs:S6320\n\nPattern/regex\n\nBroken patterns, regex syntax or misuse, matches empty, ambiguous/redundant expressions\n\njava:S2639 java:S5842 java:S5850 java:S5855 java:S5856\n\nNull/data-value\n\nNPE, returns null, invalid/misused Optionals, misused null checks, redundant null-substitute\n\njava:S2259 java:S2789 java:S3655\n\nSerialization/Serializable Missing implements Serializable, writing unserializable object, incompatible object serialization\n\njava:S2118 java:S2441\n\nPerformance/structure\n\nStack overflows, repeated computation, non-terminating recursion, infinite loop\n\njava:S1751 java:S2119 java:S5998\n\nData structure\n\nWrong collection usage, wrong element access, improper generic usage, index-out-of-bounds, assignment\n\njavabugs:S6466\n\nOther\n\nAny bug not above\n\n23\n\nA.2 Code smells categories\n\nTable 13: Code Smells Categories and Sonar Rules\n\nCode Smells Categories\n\nDescription / Rule Examples\n\nRules Assigned\n\nNaming/style/and documentation\n\nField/class/interface naming/convention/miss- ing/incorrect/deprecated docs/visibility/badly named constants\n\njava:S101 java:S1123 java:S1124 java:S1133 java:S114 java:S115 java:S116 java:S1170 java:S119 java:S1214 java:S1215 java:S1444 java:S2176 java:S3008 java:S6126\n\nDead/unused/redundant code\n\nUnused variable/method/class/empty class/unused or redundant constants/dead logic/duplicate field/method\n\njava:S1126 java:S1130 java:S1144 java:S135 java:S1481 java:S1488 java:S1602 java:S1948 java:S2094 java:S2440 java:S2447 java:S2737 java:S2975 java:S3358 java:S3400 java:S3626 java:S3878 java:S3985 java:S4087 java:S4165 java:S4276 java:S5854\n\nCognitive/computational complexity Too complex/method/class\n\nsize/cyclomatic/brain method/excessive breaks/too many params\n\njava:S107 java:S3516 java:S3776 java:S6541\n\nStructure/architecture/layer cycle\n\nViolating package/schemat- ic/dependency/architecture rules or package cycles\n\njavaarchitecture:S7027, java:S6809, java:S6833\n\nCollection/generics/param/type\n\nUse generics/types/param types/rawtypes/method ref/lambda/field hiding/diamond op/refactor generics\n\njava:S1104 java:S1161 java:S1905 java:S2293 java:S2326 java:S3252 java:S3740 java:S4977 java:S6201 java:S6204\n\nAssignment/field/scope/visibility\n\nUse local not field/make field final/field hiding/static ref/move variable/logic/redundant assignment/visibility\n\njava:S127 java:S1854 java:S1450 java:S6213 java:S1117 java:S2209 java:S2201 java:S6837 java:S1994 java:S1193 java:S1611 java:S1141 java:S5993 java:S1165 java:S3305 java:S2147\n\nControl/conditional logic smell\n\nAlways true/false/merge if/redundant switch/repeated/complex ternary/duplicate branches\n\njava:S1066 java:S1125 java:S1301 java:S1871 java:S2589 java:S4144 java:S5411 java:S6208\n\nRegex/pattern/string/format\n\nInefficient/bad regex/string concatenation/builder/- toString/charsets/format- ting/name/misuse of equalsIgnoreCase\n\njava:S1149 java:S1153 java:S1192 java:S1210 java:S2629 java:S3457 java:S4635 java:S4719 java:S4738 java:S4968 java:S4973 java:S5413 java:S5843 java:S5866 java:S5869 java:S6019 java:S6035\n\nDeprecated/obsolete APIs\n\nUse of deprecated API/field/missing removal/overdue deprecated code\n\njava:S1874 java:S5738 java:S6355\n\nDesign & Framework Best-Practices\n\nSingleton checks/@Autowired advice/codegen/anti- pattern/miscellaneous maintainability\n\njava:S112 java:S1155 java:S1168 java:S1612 java:S2139 java:S6548 java:S6829 java:S6833\n\nOther\n\nAny Code Smell not above\n\n24\n\nA.3 Vulnerability categories\n\nTable 14: Vulnerability Categories and Sonar Rules\n\nVulnerabilities Categories\n\nDescription / Rule Examples\n\nSonar Rules Assigned\n\nHard-coded credentials\n\nHardcoded passwords/keys/secrets found in code\n\njava:S6437\n\nPath-traversal & injection\n\nPath traversal, unsafe archive/file/URL constructs, user-controlled path/file/cookie/URL/JSON\n\njava:S6377 javasecurity:S2083 javasecurity:S5144 javasecurity:S6096 javasecurity:S6287 javasecurity:S6549\n\nCryptography misconfiguration Weak/incorrect cipher/IV,\n\ninsecure encryption, insecure hash, improper cipher mode/padding\n\njava:S2053 java:S3329 java:S5445 java:S5542 java:S5547\n\nCertificate-validation omissions Missing server\n\njava:S4830 java:S5527\n\nhostname/certificate validation in SSL/TLS\n\nJWT signature not verified\n\nJWT signature is not checked/verified before being used\n\njava:S5659\n\nXML External Entity (XXE)\n\nExternal entity expansion, XML parser not protected from external entities\n\njava:S2755\n\nJSON-injection risk\n\nDirect construction of JSON from untrusted/user-controlled data\n\njavasecurity:S6398\n\nInadequate error handling (I/O) Failing to catch, propagate,\n\njava:S1989\n\nor properly handle critical exceptions from untrusted sources\n\nOther\n\nAny vulnerability not above\n\n25\n\nReferences\n\n[1] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavar- ian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Rad- ford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, “Evaluating large language models trained on code.” https://arxiv.org/abs/2107.03374, 2021.\n\n[2] SonarSource SA,\n\n“SonarQube Cloud Documentation.” https://docs.sonarsource.com/\n\nsonarqube-cloud/, 2024.\n\n[3] The MITRE Corporation, “CWE-798: Use of Hard-coded Credentials.” https://cwe.mitre.\n\norg/data/definitions/798.html, 2024.\n\n[4] OWASP, “A07:2021-Identification and Authentication Failures.” https://owasp.org/Top10/\n\nA07_2021-Identification_and_Authentication_Failures/, 2021.\n\n[5] The MITRE Corporation, “CWE-22:\n\nImproper Limitation of a Pathname to a Restricted\n\nDirectory (‘Path Traversal’).” https://cwe.mitre.org/data/definitions/22.html, 2024.\n\n[6] OWASP, “A01:2021-Broken Access Control.” https://owasp.org/Top10/A01_2021-Broken_\n\nAccess_Control/, 2021.\n\n[7] Anthropic,\n\n“Claude 3.7 sonnet and claude code.” https://www.anthropic.com/news/\n\nclaude-3-7-sonnet, Feb 2025.\n\n[8] J. Abrams, A. Ahuja, S. Akkalyoncu, and et al., “GPT-4o System Card,” arXiv preprint\n\narXiv:2405.07124, May 2024.\n\n[9] Meta,\n\n“Llama\n\n3.2 model\n\ncard.”\n\nhttps://huggingface.co/meta-llama/Llama-3.\n\n2-90B-Vision-Instruct, Sep 2024.\n\n[10] A. Martini and J. Bosch, “Towards a definition of technical debt,” in Proceedings of the 8th\n\nInternational Workshop on Technical Debt (TechDebt ’15), pp. 1–8, IEEE, 2015.\n\n[11] W. N. Behutiye, P. Rodriguez, M. Oivo, and A. Tosun, “Analyzing the concept of technical debt in the context of agile software development: A systematic literature review,” Journal of Systems and Software, vol. 217, p. 112166, 2024.\n\n[12] S. Dora, D. Lunkad, N. Aslam, S. Venkatesan, and S. K. Shukla, “The hidden risks of llm- generated web application code: A security-centric evaluation of code generation capabilities in large language models,” 2025.\n\n[13] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, and J. M. Zhang, “Large\n\nlanguage models for software engineering: Survey and open problems,” 2023.\n\n[14] S. Zhao, “Github copilot now has a better ai model and new capabilities.” https://github. blog/2023-02-14-github-copilot-now-has-a-better-ai-model-and-new-capabilities/, Feb 2023. Accessed on August 7, 2025.\n\n26\n\n[15] F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, C. J. Anderson, M. Q. Feldman, A. Guha, M. Greenberg, and A. Jangda, “MultiPL-E: A Scalable and Polyglot Approach to Benchmark- ing Neural Code Generation,” IEEE Transactions on Software Engineering, vol. 49, no. 11, pp. 4836–4855, 2023.\n\n[16] J. Feng, J. Liu, C. Gao, C. Y. Chong, C. Wang, S. Gao, and X. Xia, “Complexcodeeval: A benchmark for evaluating large code models on more complex code,” in Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, ASE ’24, p. 1895–1906, ACM, Oct. 2024.\n\n[17] S. Huang, T. Cheng, J. K. Liu, J. Hao, L. Song, Y. Xu, J. Yang, J. Liu, C. Zhang, L. Chai, R. Yuan, Z. Zhang, J. Fu, Q. Liu, G. Zhang, Z. Wang, Y. Qi, Y. Xu, and W. Chu, “Opencoder: The open cookbook for top-tier code large language models,” 2025.\n\n[18] T. J. McCabe, “A complexity measure,” IEEE Transactions on Software Engineering, vol. SE-2,\n\nno. 4, pp. 308–320, 1976.\n\n[19] G. A. Campbell, “Cognitive complexity: A new way of measuring understandability.” https:\n\n//www.sonarsource.com/docs/CognitiveComplexity.pdf, 2018.\n\n[20] M. Fowler, Refactoring: Improving the Design of Existing Code. Addison-Wesley Professional,\n\n1999.\n\n[21] OWASP, “OWASP Top 10:2021.” https://owasp.org/Top10/, 2021.\n\n[22] The MITRE Corporation, “CWE-611: Improper Restriction of XML External Entity Refer-\n\nence.” https://cwe.mitre.org/data/definitions/611.html, 2024.\n\n[23] OWASP,\n\n“A05:2021-Security\n\nMisconfiguration.”\n\nhttps://owasp.org/Top10/A05_\n\n2021-Security_Misconfiguration/, 2021.\n\n27",
      "page_number": 19
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "5 2 0 2\n\ng u A 0 2\n\n] E S . s c [\n\n1 v 7 2 7 4 1 . 8 0 5 2 : v i X r a\n\nAssessing the Quality and Security of AI-Generated Code: A Quantitative Analysis\n\nAbbas Sabra, Olivier Schmitt, Joseph Tyler\n\nSonar\n\nAugust 21, 2025\n\nAbstract\n\nThis study presents a quantitative evaluation of the code quality and security of five promi- nent Large Language Models (LLMs): Claude Sonnet 4, Claude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior research has assessed the functional performance of LLM- generated code [1], this research tested LLM output from 4,442 Java coding assignments through comprehensive static analysis using SonarQube [2]. The findings suggest that although LLMs can generate functional code, they also introduce a range of software defects, including bugs, security vulnerabilities, and code smells. These defects do not appear to be isolated; rather, they may represent shared weaknesses stemming from systemic limitations within current LLM code generation methods. In particular, critically severe issues, such as hard-coded passwords [3, 4] and path traversal vulnerabilities [5, 6], were observed across multiple models. These results in- dicate that LLM-generated code requires verification in order to be considered production-ready. This study found no direct correlation between a model’s functional performance (measured by Pass@1 rate of unit tests) and the overall quality and security of its generated code, measured by the number of SonarQube issues in benchmark solutions that passed the functional tests. This suggests that functional benchmark performance score is not a good indicator of overall code quality and security. The goal of this study is not to rank LLM performance but to high- light that all evaluated models appear to share certain weaknesses. Consequently, these findings support the view that static analysis can be a valuable instrument for detecting latent defects and an important safeguard for organizations that deploy AI in software development.\n\n1",
      "content_length": 2019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "Contents\n\n1 Introduction\n\n2 Research Objectives\n\n3 Experiment Setup\n\n3.1 Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Models Under Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Methodology: Code Generation and Static Analysis . . . . . . . . . . . . . . . . . . .\n\n4 Quality Overview of LLM-Generated Code\n\n4.1 Volumetric and structural characteristics . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Functional Performance and Overall Code Quality . . . . . . . . . . . . . . . . . . . 4.3 Overall issue density and distribution by type and severity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4\n\nIssue severity distribution (% of Total Issues)\n\n5 Analysis of Code Smells in LLM-Generated Java\n\n6 Analysis of Bugs in LLM-Generated Java\n\n7 Analysis of Security Vulnerabilities in LLM-Generated Java\n\n8 Discussion: Examples of Common SonarQube Detected Issues\n\n8.1 Theme: Deficient Error Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Theme: Resource Management Lapses . . . . . . . . . . . . . . . . . . . . . . . . . . 8.3 Theme: Critical Security Oversights . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.4 Theme: Excessive Code Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.5 Theme: Maintainability & Best Practice Violations (Redundant Code) . . . . . . . .\n\n9 Case Study: Evolution of Model Performance and Defect Characteristics\n\n9.1 Performance Benchmarks and Defect Severity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.2 Persistence and Evolution of Underlying Issues\n\n10 The Role of Static Analysis in Addressing Potential LLM-Generated Issues\n\n11 Conclusion\n\nAppendix\n\nA.1 Bugs categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Code smells categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Vulnerability categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2\n\n3\n\n4\n\n5 5 5 5\n\n7 7 7 8 9\n\n11\n\n13\n\n15\n\n17 17 17 17 18 18\n\n19 19 19\n\n20\n\n21\n\n23 23 24 25",
      "content_length": 2192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "1\n\nIntroduction\n\nState of the art LLMs from providers such as Anthropic [7], OpenAI [8], and Meta [9] are increasingly utilized in software development for various tasks, including code generation, code autocompletion, and bug remediation. The potential for these tools to enhance developer productivity and accelerate development drives their widespread adoption. However, accelerated development must be balanced with the foundational principles of software quality. Core attributes like reliability, security, and maintainability are crucial for the long-term viability of any software project. Code that fails to meet these standards, whether authored by humans or generated by AI, can accrue technical debt [10, 11], introduce security risks, and degrade system reliability.\n\nThe prevailing discourse on LLMs has largely emphasized their generating capabilities, such as measuring functional correctness [1], sometimes overshadowing the critical need to evaluate the broader quality and security of their output [12, 13]. This concern is amplified by the rapid adoption of LLMs in software development, with reports indicating AI assistants write an average of 46% of developer code [14], while research continues to highlight that the generated code can introduce significant security vulnerabilities and bugs [12, 13]. Consequently, there is an established risk of developers overlooking necessary verification, as studies have found that programmers using AI assistants can produce less secure code while simultaneously showing greater confidence in its security [12].\n\n3",
      "content_length": 1579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "2 Research Objectives\n\nThis investigation seeks to address the aforementioned quality gap. The study is guided by the following central research questions (RQs), which focus on identifying common patterns in the output of leading LLMs rather than on providing a comparative ranking:\n\nRQ1: What is the typical characteristic quality profile of Java code generated by contemporary,\n\nstate-of-the-art LLMs?\n\nRQ2: What are the most common categories and severity levels of issues observed in the code\n\ngenerated by these models?\n\nRQ3: How effectively can static analysis identify these defects and serve as a protective mechanism?\n\nRQ4: Does an improvement in a model’s functional performance correlate with an improvement in\n\nthe quality and security of its generated code?\n\nThis research investigates the gap between the output of LLMs and the quality standards for production-ready software. We posit that LLM-generated code is not immediately fit for produc- tion and requires rigorous verification, making functional metrics alone insufficient for evaluation. This study argues that the common error patterns in LLM-generated code make static analysis an essential tool for ensuring its quality and security.\n\n4",
      "content_length": 1212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "3 Experiment Setup\n\nThis section outlines the experiment design used to evaluate the code quality and security of several leading LLMs using a set of Java programming challenges. The structure facilitates a cross-model comparison and the application of static analysis to identify issue patterns in the generated code.\n\n3.1 Benchmarks\n\nThe evaluation was conducted using Java as the target programming language. The benchmark dataset comprised 4,442 distinct coding problems from three recognized and publicly available sources: MultiPL-E-mbpp-java, MultiPL-E-humaneval-java [15], and ComplexCodeEval [16]. While code was generated for static analysis from all three sources, functional performance (i.e., test pass rates) was evaluated using only the two MultiPL-E benchmarks. These benchmarks were selected to represent a spectrum of Java programming challenges with varying complexity and domain relevance, forming a robust testbed for assessing the LLMs’ code generation capabilities.\n\n3.2 Models Under Test\n\nThe study assessed five prominent LLMs, representing diverse architectures, training methods, and accessibility:\n\nClaude Sonnet 4 and Claude 3.7 Sonnet (Anthropic): Included to examine how code generation quality evolves between successive model generations from the same provider [7].\n\nGPT-4o (OpenAI): Selected for its widespread adoption, as it frequently serves as the underlying technology for popular AI coding tools such as GitHub Copilot [8].\n\nLlama 3.2 90B (Meta): Chosen as a leading open-weight model representative of state-of- the-art, openly available LLMs [9].\n\nOpenCoder-8B (Llama-3.1-8B architecture): A smaller, open-source model included to determine if common code quality issues persist regardless of model size, which could reveal challenges inherent in the LLM methodology itself [17].\n\nFor consistency and reproducibility, identical prompts were provided to each model, and the tem- perature was set to zero.\n\n3.3 Methodology: Code Generation and Static Analysis\n\nOur methodology involved a three-phase process:\n\nCode Generation: Each of the five LLMs was prompted to generate compilable Java solu- tions for all 4,442 benchmark problems. The primary objective was to obtain functional and compilable code to serve as the input for the subsequent quality assessment.\n\nFunctional Performance Evaluation: The code generated for problems from the two MultiPL-E benchmarks was executed against the provided test suites to measure functional performance. This performance was quantified by Pass@1 rate of unit tests.\n\nCross-Model Static Analysis: Each generated Java file was then analyzed using Sonar- Qube [2]. The full default SonarWay Java ruleset, consisting of approximately 550 rules designed to be broadly suitable for most projects, was applied to detect various categories of software issues, including bugs, security vulnerabilities, and code smells. This allowed us to\n\n5",
      "content_length": 2915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "compare the distribution and prevalence of issues across models, identify shared weaknesses, and characterize the quality of AI-generated code.\n\nThis design enabled a detailed examination of not only whether LLMs could solve coding problems but also how well they did so from a quality and security perspective.\n\n6",
      "content_length": 314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "4 Quality Overview of LLM-Generated Code\n\nThis section addresses the characteristic quality profile of LLM-generated Java (RQ1) by providing a quantitative overview of the code generated for the 4,442 tasks. The analysis focuses on overarching characteristics and aggregated quality metrics.\n\n4.1 Volumetric and structural characteristics\n\nThe five LLMs exhibited notable differences in the volume and complexity of the Java code they generated. These metrics, detailed in Table 1, provide context for the subsequent analysis of code quality.\n\nTable 1: Comparative code generation metrics across LLMs (4,442 Tasks)\n\nLLM Model\n\nLOC Statements Functions Classes Files Comment Lines Comments (%) Cyclomatic Complexity Cognitive Complexity\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n370,816 288,126 209,994 196,927 120,288\n\n148,932 116,433 83,466 75,368 41,510\n\n46,235 27,496 24,309 22,694 8,338\n\n12,832 10,649 10,475 8,996 5,530\n\n4,442 4,442 4,442 4,442 4,442\n\n20,051 56,459 9,692 15,514 13,165\n\n5.10% 16.40% 4.40% 7.30% 9.90%\n\n81,667 55,485 44,387 37,948 18,850\n\n47,649 42,220 26,450 20,811 13,965\n\nThese metrics reveal distinct generative tendencies. Claude Sonnet 4 produced the most code (370,816 LOC) with the highest cumulative Cyclomatic Complexity (81,667) [18] and Cognitive Complexity (47,649) [19]. In contrast, OpenCoder-8B generated the most concise code (120,288 LOC) with the lowest complexity scores (18,850 Cyclomatic Complexity and 13,965 Cognitive Com- plexity). Comment density also varied widely, from Claude 3.7 Sonnet’s high of 16.4% to GPT-4o’s 4.4% and Claude Sonnet 4’s 5.1%.\n\nThe variance in structural metrics suggests that the choice of an LLM has significant consequences for a project’s long-term maintainability, irrespective of the model’s functional performance. This finding indicates that different architectural philosophies and training data result in varied structural outputs, rather than a consistent progression toward more concise or less complex code in newer models. This diversity underscores a key conclusion: LLM-generated code is not a monolithic entity, and development teams should adopt model selection and code review strategies that account for these structural differences.\n\n4.2 Functional Performance and Overall Code Quality\n\nBeyond structural characteristics, a central question of this study is whether a model’s functional performance—defined here as the success rate of generated code passing its intended tests—correlates with the quality of its generated code (RQ4). This study measured functional performance using the test pass rates from the MultiPL-E benchmarks, while static analysis was conducted on the code generated for all 4,442 tasks. Table 2 summarizes these metrics to provide an initial overview of code quality.\n\n7",
      "content_length": 2816,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Table 2: LLM Performance: Test Pass Rates and SonarQube Issue Rates (4,442 Tasks)\n\nLLM Model\n\nPassing tests % SonarQube Discovered Issues\n\nIssues per passing task\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n77.04 72.46 69.67 61.47 60.43\n\n7,225 6,576 5,476 5,159 3,903\n\n2.11 2.04 1.77 1.89 1.45\n\nClaude Sonnet 4 demonstrated the highest test pass rate at 77.04%, while OpenCoder-8B exhibited the lowest at 60.43%. The \"Issues per Passing Task\" metric, which normalizes total issues against the number of functionally successful outputs, offers additional insight. OpenCoder-8B, despite its lower pass rate, presented the lowest number of issues per passing task (1.45). In contrast, Claude Sonnet 4, the top performer on pass rate, averaged 2.11 issues per passing task.\n\nA key observation is that even when LLM-generated code passes functional performance bench- marks, it is not free of underlying quality defects. For every task that OpenCoder-8B completed successfully, it still averaged 1.45 static analysis issues; similarly, GPT-4o averaged 1.77 issues per passing task. This pattern supports the premise that static code analysis is valuable for assessing the quality of functionally \"passing\" code. These underlying issues in successful tasks represent latent factors that could impact maintainability and reliability over time.\n\nThe consistent presence of quality issues in functionally correct code suggests that relying solely on functional performance benchmarks to evaluate LLM-generated code is insufficient and may introduce hidden risks. This highlights a potential paradox: as models become more capable, they may generate more sophisticated solutions that, while functionally robust, introduce a larger surface area for defects, leading to a greater number of static analysis findings. This dynamic is examined more closely in the following sections.\n\n4.3 Overall issue density and distribution by type and severity\n\nThis section examines the types and severities of issues identified in the Java code generated by the LLMs across the 4,442 tasks. Table 3 provides an overview of the resulting defect metrics.\n\nTable 3: Overall code quality defect metrics per LLM (4,442 Tasks)\n\nLLM Model\n\nSonarQube Discovered Issues\n\nLOC Issues per KLOC Issues per Passing Task\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n7,225 6,576 5,476 5,159 3,903\n\n370,816 288,126 209,994 196,927 120,288\n\n19.48 22.82 26.08 26.20 32.45\n\n2.11 2.04 1.77 1.89 1.45\n\nIssue density, measured as issues per thousand lines of code (KLOC), ranged from 19.48 for Claude Sonnet 4 to 32.45 for OpenCoder-8B. These differing densities appear to illustrate distinct profiles of issue generation rather than a simple quality ranking. A model that generates a larger volume of code might exhibit a higher total number of issues, even if its per-line quality is comparatively reasonable. The distribution of issues by type, shown in Table 4, reveals similarity across models.\n\n8",
      "content_length": 3005,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Table 4: Distribution of issue types by LLM (absolute counts and percentage of total issues per model)\n\nLLM Model\n\nTotal Bugs % Bugs Total Vulnerabilities % Vulnerabilities Total Code Smells % Code Smells\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n423 352 406 398 247\n\n5.85% 5.35% 7.41% 7.71% 6.33%\n\n141 116 112 123 67\n\n1.95% 1.76% 2.05% 2.38% 1.72%\n\n6,661 6,108 4,958 4,638 3,589\n\n92.19% 92.88% 90.54% 89.90% 91.95%\n\nA key observation from the data is the striking similarity in the distribution of issue types across all evaluated models. Despite their varied architectures, each model produced a comparable mix of defects: approximately 90-93% code smells, 5-8% bugs, and around 2% security vulnerabilities (as shown in Table 4). This consistency across different models suggests a systemic pattern in the code generation process of current LLMs. While code smells are the most frequent, the consistent introduction of bugs and, more critically, security vulnerabilities is particularly noteworthy. A defect rate that includes a 5-8% chance of being a bug and a ≈2% chance of being a security vulnerability is significant. This finding underscores that LLM-generated code, even when it passes functional performance tests, is not immediately suitable for production environments. It highlights the critical need for rigorous static analysis and expert human review to identify and remediate these underlying quality and security issues before deployment, thereby preventing the accumulation of technical debt and security risks. Table 5 details the issue density by type for each model.\n\nTable 5: Issue density by type and LLM (per KLOC)\n\nLLM Model\n\nBug Density (Bugs/KLOC) Vulnerability Density (Vuln./KLOC) Code Smell Density (Smells/KLOC)\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n1.14 1.22 1.93 2.02 2.05\n\n0.38 0.40 0.53 0.62 0.56\n\n17.96 21.20 23.61 23.55 29.84\n\nDespite variations in density, all evaluated models produced all three types of issues: bugs, vulner- abilities, and code smells.\n\n4.4\n\nIssue severity distribution (% of Total Issues)\n\nThis section analyzes the proportional distribution of severity levels (Blocker, Critical, Major, Mi- nor) within each issue type to provide insight into the potential impact of the defects generated by the models. Table 6 shows the severity distribution for bugs.\n\nTable 6: Bug distribution (% of total bugs per model)\n\nLLM Model\n\nBLOCKER CRITICAL MAJOR MINOR\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n13.71 7.10 7.14 13.82 9.24\n\n4.49 3.98 3.45 4.77 12.05\n\n54.14 61.93 74.63 56.78 49.00\n\n27.66 26.99 14.78 24.62 29.72\n\nIn the bug category, GPT-4o exhibited a notable tendency, with nearly 75% of its bugs categorized\n\n9",
      "content_length": 2758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "as ‘MAJOR’, indicating a propensity to produce significant functional defects. In contrast, Claude Sonnet 4 and Llama 3.2 90B presented the highest proportion of ‘BLOCKER’ bugs (approximately 14%), which represent defects that can prevent application functionality. OpenCoder-8B showed a high percentage of ‘CRITICAL’ bugs (12%) compared to other models, which generally ranged between 3% and 5%. For code smells, the severity distribution is presented in Table 7.\n\nTable 7: Code smell distribution (% of total code smells per model)\n\nLLM Model\n\nBLOCKER CRITICAL MAJOR MINOR\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n0.25 0.30 0.22 0.33 0.31\n\n8.89 12.96 5.79 5.36 5.49\n\n51.22 45.65 43.45 39.64 37.28\n\n39.65 41.09 50.54 54.67 56.93\n\nFor code smells, which relate primarily to maintainability, most issues were distributed across the ‘MAJOR’ and ‘MINOR’ categories for all models. Claude 3.7 Sonnet generated a markedly higher proportion of ‘CRITICAL’ code smells (13%) compared to the other models. Conversely, Llama 3.2 90B and OpenCoder-8B exhibited the highest percentages of ‘MINOR’ code smells, suggesting their maintainability issues were, on average, of less severe immediate impact. The severity distribution for vulnerabilities, shown in Table 8, highlights a more critical trend.\n\nTable 8: Vulnerability distribution (% of total vulnerabilities per model)\n\nLLM Model\n\nBLOCKER CRITICAL MAJOR MINOR\n\nClaude Sonnet 4 Claude 3.7 Sonnet GPT-4o Llama 3.2 90B OpenCoder-8B\n\n59.57 56.03 62.50 70.73 64.18\n\n28.37 28.45 23.21 22.76 26.87\n\n5.67 5.17 5.36 1.63 1.49\n\n6.38 10.34 8.93 4.88 7.46\n\nAn important finding from the analysis of vulnerabilities is that all models produced a high percent- age of ‘BLOCKER’ and ‘CRITICAL’ vulnerabilities. This observation underscores the importance of thorough security scanning for AI-generated code. For example, Llama 3.2 90B generated a high proportion of these issues, with over 70% of its identified vulnerabilities classified as ‘BLOCKER’. Similarly, results for OpenCoder-8B and GPT-4o indicated that nearly two-thirds of their detected vulnerabilities were of the highest severity levels.\n\n10",
      "content_length": 2166,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "5 Analysis of Code Smells in LLM-Generated Java\n\nIn addressing the most common categories of issues (RQ2), this section provides a detailed analysis of code smells. Code smells can serve as indicators of deeper structural problems within source code. While not representing direct functional errors, they can impede maintainability, comprehensibility, and evolvability, often contributing to the accumulation of technical debt [10, 11] or the introduction of bugs over time. As noted in the previous section, code smells [20] were the most frequent issue type identified across all evaluated LLMs. This section provides an analysis of the specific sub- categories of code smells observed, as detailed in Table 9, and explores potential factors that may contribute to these challenges for LLMs. For additional details regarding the categories, please refer to the Appendix.\n\nTable 9: Sub-categories of code smells and their origins (% of total code smells for model)\n\nCategory\n\nClaude Sonnet 4 (%)\n\nClaude 3.7 Sonnet (%)\n\nGPT-4o (%)\n\nLlama 3.2 90B (%)\n\nOpenCoder-8B (%)\n\nPotential Contributing Factors\n\nDead / Unused / Redundant code Design / Framework best-practices Assignment / Field / Scope visibility Collection / Generics / Param / Type Regex / Pattern / String / Format Cognitive / Computational complexity Control / Conditional-logic smell Deprecated / Obsolete APIs Naming / Style / Documentation Exception-handling smell Other\n\n14.83 22.26 11.96 13.94 13.70 4.25 4.67 2.01 2.69 0.05 9.64\n\n17.43 18.58 15.35 11.23 11.80 8.43 3.91 2.34 2.50 0.08 8.33\n\n26.3 20.81 13.21 9.92 7.36 3.73 4.03 2.08 2.84 0.06 9.64\n\n34.82 18.84 11.32 9.03 6.81 2.67 3.02 2.89 2.16 0.02 8.41\n\n42.74 12.45 11.95 7.89 5.29 2.79 2.20 4.01 1.89 0.06 8.72\n\nRequires non-local, project-wide reference analysis. Lacks context of project-specific framework conventions. Requires class-wide or non-local scope resolution. Requires deep semantic understanding of the API. Logical flaws may only be apparent at runtime. Complexity is a non-local property of the code. Difficulty in balancing correctness and readability. Requires knowledge of library deprecation roadmaps. Difficulty generalizing from project-specific conventions. Requires analysis of the cross-package dependency graph.\n\nThe following analysis delves into the most prominent categories from Table 9, exploring potential factors that contribute to them.\n\nDead / Unused / Redundant Code: This category was prevalent in the output of models such as Llama 3.2 90B (34.82% of its code smells) and OpenCoder-8B (42.74%). LLMs may struggle in this area, potentially because identifying such code can require a whole-project reference analysis. Since LLMs often operate with a limited context window, it can be challenging for them to determine if a generated element is utilized elsewhere in a larger application. This limitation could lead to the generation of syntactically plausible but unreferenced code, contributing to codebase bloat.\n\nDesign / Framework Best Practices: Claude Sonnet 4 showed a higher percentage of issues in this category (22.26%), which may reflect the model’s tendency to generate more thorough code by attempting to handle numerous edge cases. This approach, while sometimes leading to more robust error management, can also create unnecessary logical complexity if the model cannot infer the specific context. However, all LLMs may face a common challenge in this area, as adherence to specific design patterns or framework conventions can depend on knowledge of organizational rules or proprietary mechanisms that may not be well-represented in general training data.\n\nAssignment / Field / Scope Visibility: This issue was consistently observed, typically ranging from 11% to 15% of total code smells per model. This type of issue may arise because correctly determining scope can require a comprehensive, class-wide context. Defining the narrowest necessary scope for a variable might depend on an understanding of all interactions within a class, which could be challenging for a model to infer from localized generation.\n\nCollection / Generics / Parameter / Type Issues: Issues in this category may be common because the correct use of generics and parameterized types often requires a deeper understanding of API semantics. While LLMs may employ these features accurately in simple cases, ensuring\n\n11",
      "content_length": 4369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "type safety across complex interactions could require a level of semantic comprehension that may be challenging for current generative models.\n\nRegex / Pattern / String / Format: LLMs may generate regular expressions or string format- ting operations that are syntactically valid but contain subtle logical flaws. This type of issue may occur because such flaws are often revealed only through execution or deeper semantic analysis, which is typically outside the scope of token-by-token generation.\n\nCognitive / Computational Complexity: High cognitive complexity was a notable issue, par- ticularly for Claude 3.7 Sonnet (8.43% of its code smells). LLMs may face challenges with this metric, as complexity is often a non-local property of code. Since LLMs tend to optimize for lo- cal token probability, a sequence of individually plausible segments might cumulatively result in a method that is globally complex, as the model may not have an explicit mechanism to optimize for an overall complexity score.\n\nControl / Conditional-Logic Smell: LLMs may generate convoluted conditional logic. This issue may arise because capturing the nuance between functional correctness and code readability can be difficult. A model might not prioritize simpler or more idiomatic control flow structures if a more complex alternative also appears to satisfy the immediate generation objective.\n\nDeprecated / Obsolete APIs: The use of deprecated APIs and outdated dependencies poses a significant security risk, moving beyond a simple code smell. An LLM’s tendency to favor older API versions—a behavior likely influenced by its training data’s knowledge cut-off or the prevalence of older code examples—can inadvertently lead to the re-introduction of libraries with known vulner- abilities (CVEs). This specific blind spot highlights the necessity of complementing static analysis with Software Composition Analysis (SCA). While static analysis inspects the generated code itself, SCA provides the crucial, additional security layer of scanning its dependencies for known exploits, directly addressing a risk vector that is particularly pronounced in how LLMs currently operate.\n\nNaming / Style / Documentation: While LLMs may adhere to common coding conventions, they may not capture team- or project-specific naming styles. Such conventions vary widely across contexts, making them difficult for a model to generalize from disparate training data.\n\nException-Handling Smell: The use of generic exceptions appears to be a frequent code smell across the evaluated models. Formulating specific exception handling can require a detailed analysis of dependencies across a codebase, which is a challenge for common use of LLMs that operate primarily within a localized context.\n\n12",
      "content_length": 2766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "6 Analysis of Bugs in LLM-Generated Java\n\nContinuing the investigation into the most common categories and severity levels of defects (RQ2), this section examines the bugs identified in the LLM-generated Java code. Bugs represent functional defects in code that can lead to incorrect behavior, application crashes, or unexpected outcomes. Static analysis detects bugs that can potentially compromise application stability and correctness. While numerically less frequent than code smells, their potential impact is often more immediate and severe. This section explores common bug categories observed in the LLM-generated Java code, which are summarized in Table 10, and discusses potential factors contributing to their introduction.\n\nTable 10: Sub-categories of bugs and their origins (% of total bugs for model)\n\nCategory\n\nClaude Sonnet 4 (%)\n\nClaude 3.7 Sonnet (%)\n\nGPT-4o (%)\n\nLlama 3.2 90B (%)\n\nOpenCoder-8B (%)\n\nPotential Contributing Factors\n\nControl-flow mistake API contract violation Exception handling Resource management / Leak Type-safety / Casts Concurrency / Threading Null / Data-value issues Performance / Structure Pattern / Regex Data-structure bug Serialization / Serializable Other\n\n14.83 10.29 16.75 15.07 11.24 9.81 7.89 4.31 2.63 1.44 0.00 5.74\n\n23.62 14.12 16.71 8.36 12.97 1.44 7.49 6.34 1.15 1.15 0.58 6.05\n\n48.15 8.64 11.60 7.41 7.90 1.73 8.89 3.95 0.74 0.00 0.00 0.99\n\n31.06 14.90 14.39 12.88 6.82 1.26 5.81 2.78 0.25 1.01 0.76 8.08\n\n21.37 19.35 14.52 9.68 7.66 2.82 6.85 5.24 2.42 1.61 1.61 6.85\n\nRequires deep, non-local path reasoning beyond pattern matching. Requires analysis of error propagation across multiple code branches. Dependent on understanding library intent and return semantics. Resource lifecycle management is a non-local problem. Requires precise tracking of static-type provenance. Concurrency concepts (e.g., atomicity) are underrepresented in corpora. Difficulty tracking nullability across complex data-flow paths. Potential generation of inefficient or suboptimal algorithms. Regex logical errors are often only evident at runtime. Proper collection usage is contingent on semantic intent. Requires knowledge of the framework’s object-graph semantics.\n\nA closer look at these categories, presented in Table 10, reveals common challenges LLMs face in ensuring the semantic correctness and runtime integrity of generated code.\n\nControl-Flow Mistakes: This category of bugs was particularly prominent in code from GPT-4o (48.15% of its total bugs) and was also significantly represented in output from Llama 3.2 90B (31.06%) and OpenCoder-8B (21.37%). These issues may be prevalent because ensuring correct control flow can require deep path reasoning. While models can generate plausible conditional statements or loops based on learned patterns, they may face challenges with the multi-step logic needed to ensure correctness across all execution paths, especially with intricate branching or edge cases.\n\nAPI Contract Violations: Models from the Llama family, specifically OpenCoder-8B (19.35%) and Llama 3.2 90B (14.90%), showed a higher proportion of issues in this area. Correctly using Application Programming Interfaces (APIs) can require analyzing error propagation and under- standing return value semantics. LLMs might overlook these nuances, potentially leading to bugs where API return values are ignored or misinterpreted. This could suggest a challenge for the models in comprehending sequential, stateful operations.\n\nException Handling (Bugs): Distinct from exception handling smells, this category pertains to functional defects. Addressing them can require knowledge of library intent. LLMs might gen- erate code that catches overly broad exceptions without reacting appropriately or that disregards checked exceptions, potentially due to challenges in inferring the specific purpose of library-thrown exceptions from training data.\n\nResource Management / Leaks: Claude Sonnet 4 demonstrated a higher proportion of issues in this category (15.07%). Resource leaks may be a persistent issue, possibly because resource lifecycles (e.g., opening and closing streams) can span multiple calls, which may be difficult to track within a local context window. This could lead to failures in ensuring that resources are properly closed across all paths, potentially introducing risks for long-running applications.\n\n13",
      "content_length": 4381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Type-Safety / Casts: Issues such as illegal type casts may occur because ensuring cast safety can require precise tracking of a variable’s static-type history. A model might generate code involving type casting without being able to fully track the type history through complex data flows, which could result in runtime ClassCastException errors.\n\nConcurrency / Threading: Claude Sonnet 4 also showed a comparatively higher share of bugs in this category (9.81%). Correct concurrent programming is known to be difficult, and LLMs may face challenges in this area because concepts like thread state and atomicity can be complex to learn. Models may not always generate code that sufficiently covers complex threading scenarios, which could lead to potential race conditions or deadlocks.\n\nNull / Data-Value Issues: This was a common problem, accounting for 5-9% of identified bugs across the evaluated models. NullPointerExceptions may arise if a model faces difficulties tracking nullability across complex data flows. As a result, it might not consistently perform necessary null checks before dereferencing an object.\n\nPerformance / Structure (Bugs): LLMs may generate code that is functionally correct but ex- hibits poor runtime performance (e.g., inefficient loops). Such issues might occur because LLMs tend to optimize for local token generation probability rather than for global performance characteristics.\n\nPattern / Regex (Bugs): This category involves regular expressions that are functionally incor- rect. These issues may arise because regex edge cases often surface only during execution or deeper semantic analysis, which may extend beyond the focus on syntactic validity during generation.\n\nData-Structure Bugs: The misuse of data structures, such as attempting to access an element be- yond array bounds, may occur because the correct application of collections can be tied to semantic intent, a nuance that might be challenging for an LLM to capture fully.\n\nSerialization / Serializable: Bugs such as a class failing to implement Serializable when required may stem from the non-local, framework-dependent nature of such requirements, which can be chal- lenging for context-limited generation. The nature of these observed bugs suggests that LLMs may face challenges with the semantic correctness and runtime implications of the code they generate, potentially prioritizing syntactic plausibility over comprehensive functional integrity.\n\n14",
      "content_length": 2461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "7 Analysis of Security Vulnerabilities in LLM-Generated Java\n\nAs a critical component of understanding the common issue categories (RQ2), this section ana- lyzes the security vulnerabilities found in the generated code. Security vulnerabilities represent exploitable flaws in code that can lead to data compromise, service disruption, or unauthorized ac- cess. While comprising a smaller percentage of total issues (approximately 2% across models), the 67 distinct types of vulnerabilities identified in this analysis are of significant concern due to their potential impact. Recent studies have highlighted these risks, identifying vulnerabilities in code generated by a range of models [12], which aligns with general industry awareness of key security risks [21]. A potential contributing factor to the prevalence of these vulnerabilities is that LLMs optimize for token likelihood based on training data, which may include insecure or outdated code snippets. Table 11 provides a detailed breakdown of the vulnerability sub-categories identified in this study.\n\nTable 11: Sub-categories of security vulnerabilities and their origins (% of total vulnerabilities for model)\n\nCategory\n\nClaude Sonnet 4 (%)\n\nClaude 3.7 Sonnet (%)\n\nGPT-4o (%)\n\nLlama 3.2 90B (%)\n\nOpenCoder-8B (%)\n\nPotential Contributing Factors\n\nPath-traversal & Injection Hard-coded credentials Cryptography misconfiguration XML External Entity (XXE) Inadequate I/O error-handling Certificate-validation omissions JSON-injection risk JWT signature not verified Other\n\n34.04 14.18 24.82 10.64 4.96 2.84 0.71 0.00 7.80\n\n31.03 10.34 23.28 15.52 7.76 4.31 0.00 0.00 7.76\n\n33.93 17.86 19.64 13.39 7.14 2.68 0.89 0.00 4.46\n\n26.83 23.58 22.76 19.51 4.88 0.00 0.81 0.00 1.63\n\n28.36 29.85 22.39 5.97 7.46 2.99 1.49 1.49 0.00\n\nRequires non-local taint analysis from a data source to a sink. Security-sensitive intent of constants is semantically unclear. Requires current knowledge of secure cryptographic algorithms and modes. Secure parser configuration is a non-local, multi-file problem. Semantic distinction between critical and benign errors is subtle. Secure, library-specific SSL/TLS usage patterns are rare in corpora. Trust boundaries are often implicit in data builders and serializers. Requires current, library-specific security token best practices.\n\nAnalysis of the vulnerabilities detailed in Table 11 points to systemic weaknesses related to non-local taint analysis and the semantic understanding of sensitive data.\n\nPath-Traversal & Injection: This category was a dominant type of vulnerability across all models, observed, for instance, in 34.04% of Claude Sonnet 4’s vulnerabilities and 33.93% of GPT- 4o’s. Preventing such flaws can require taint-tracking from an input source to a sensitive sink, a form of non-local analysis. The models may generate code that performs a function correctly but does not fully account for how unvalidated user input could manipulate file paths or inject commands. This challenge in performing comprehensive data flow analysis is a known security concern [5, 6].\n\nHard-Coded Credentials: This critical vulnerability was particularly prevalent in Llama-family models, appearing in 29.85% of OpenCoder-8B’s vulnerabilities and 23.58% of Llama 3.2 90B’s. This may arise because constant strings can appear benign, and their security-sensitive intent may not be apparent from common patterns in training data. A password string literal, for example, might not be treated as semantically distinct from any other string, particularly if similar insecure practices are present in the training corpus [3, 4].\n\nCryptography Misconfiguration: This was another significant area of weakness, exemplified by Claude Sonnet 4 at 24.82% of its vulnerabilities. Secure cryptography often requires precise knowledge of secure versus weak cipher algorithms and modes. LLMs may reproduce patterns involving deprecated or weak cryptographic primitives, possibly because their training data contains such examples and may not reflect the most up-to-date standards.\n\nXML External Entity (XXE): Vulnerabilities related to XXE may occur because correct config- urations of XML parsers can span multiple files. Lacking full application context of how the parser is configured, an LLM might generate XML parsing code with insecure default settings [22, 23].\n\n15",
      "content_length": 4345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Inadequate I/O Error-Handling (Security): Failures to properly handle I/O errors can lead to security issues. LLMs may face challenges in this area, as distinguishing between security-critical and routine operational errors can be a subtle distinction that requires deeper contextual understanding.\n\nCertificate-Validation Omissions: Secure SSL/TLS communication typically requires proper certificate validation. LLMs may omit these steps, possibly because comprehensive, library-specific SSL/TLS usage patterns are underrepresented or oversimplified in general training data.\n\nJSON-Injection Risk: LLMs may fail to respect the trust boundaries implicit in JSON builders or serializers, resulting in insecure constructions when unsanitized user-controlled data is incorporated.\n\nJWT Signature Not Verified: Secure handling of JSON Web Tokens (JWT) generally requires verification of the token’s signature. An LLM might omit this step, possibly because doing so requires adherence to up-to-date, library-specific best practices that may not be current or compre- hensively represented in its training data.\n\nThe consistency of these vulnerabilities across different LLMs points to their training data as the most plausible origin. By learning from and replicating insecure code, the models demonstrate an inability to distinguish secure patterns from insecure ones, revealing a critical limitation in the current code generation paradigm. Therefore, while prompt engineering is useful, it is not a sufficient safeguard on its own. To mitigate these risks, applying rigorous security-focused static analysis (SAST) and expert human review to LLM-generated code is essential, particularly when it handles sensitive data or operations.\n\n16",
      "content_length": 1735,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "8 Discussion: Examples of Common SonarQube Detected Issues\n\nThis section provides concrete examples of common SonarQube detections, building upon the de- tailed categorization of Code Smells, Bugs, and Vulnerabilities presented in Sections 5, 6, and 7. These examples are intended to illustrate potential shared weaknesses in LLM-generated code. The focus is on how specific detections may relate to the broader issue categories discussed in those sections, rather than on model-specific prevalence.\n\n8.1 Theme: Deficient Error Handling\n\nIllustrative Rule: java:S112 – Define and throw a dedicated exception instead of using a generic one.\n\nObservation: A common pattern observed across the evaluated LLMs was the use of generic excep- tions (e.g., throw new Exception()) rather than specific, custom exceptions. This was frequently flagged by SonarQube rule java:S112 and was among the top issues for models like Claude Sonnet 4, GPT-4o, and Llama 3.2 90B.\n\nInterpretation: This observation may align with the challenge related to exception handling, as discussed in Section 5, which can require a \"cross-package dependency graph analysis.\" Generating specific exceptions often involves a broader understanding of an application’s error handling strategy and class hierarchy—a level of non-local context that may be challenging for an LLM to infer. This behavior is also symptomatic of a tendency of LLMs to lack specificity[add ref?], perhaps in order to avoid hallucinations; the generated code will avoid execution errors, but at the cost of understanding the nuance of the scenario the exception is designed to handle.\n\n8.2 Theme: Resource Management Lapses\n\nIllustrative Rule: java:S2095 – Use try-with-resources or close this resource in a \"finally\" clause.\n\nObservation: Failure to properly close resources, such as streams or network connections, was a recurring Blocker-level bug. Rule java:S2095 identified numerous instances of this across different models, including 54 instances for Claude Sonnet 4, 25 for GPT-4o, and 50 for Llama 3.2 90B.\n\nInterpretation: This bug category may be related to the challenge where a resource life-cycle spans many calls, making it difficult to manage. An LLM might lack the reasoning capability to consistently plan and track which resources are opened, and therefore fail to ensure its closure on all possible execution paths.\n\n8.3 Theme: Critical Security Oversights\n\nIllustrative Rule: java:S6437 – Revoke and change this password, as it is compromised - i.e., hardcoded password [3].\n\nObservation: The Blocker vulnerability of hardcoded credentials was detected in code generated by all five evaluated LLMs. For instance, 20 instances were found for Claude Sonnet 4, 20 for GPT-4o, and 29 for Llama 3.2 90B.\n\nInterpretation: This vulnerability arises from the model’s indiscriminate handling of string literals. The LLM does not differentiate between security-sensitive constants, such as passwords or API keys, and benign string values. Consequently, it embeds sensitive data directly into the source code as hardcoded constants. This behavior is consistent with a model designed to replicate plausible\n\n17",
      "content_length": 3158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "statistical patterns observed in its training data, which often includes insecure examples, rather than performing security-specific semantic analysis.\n\n8.4 Theme: Excessive Code Complexity\n\nIllustrative Rule: java:S3776 – Refactor this method to reduce its Cognitive Complexity.\n\nObservation: Many LLMs, notably Claude 3.7 Sonnet (422 instances) and GPT-4o (112 instances), generated methods with high Cognitive Complexity, which was flagged as a Critical issue by Sonar- Qube.\n\nInterpretation: Autoregressive LLMs generate tokens one at a time and are optimized to ensure generated code is locally coherent. Code architecture is of secondary importance during training, and at inference time the accumulating complexity of a given method is not tracked, therefore it is not surprising that LLMs generate code with high complexity, especially for tasks which require more sophisticated architecture.\n\n8.5 Theme: Maintainability & Best Practice Violations (Redundant Code)\n\nIllustrative Rule: java:S2094 – Remove this empty class, write its code or make it an \"interface\".\n\nObservation: The generation of empty classes or methods was a frequent Minor code smell across multiple models, including GPT-4o (531 instances) and OpenCoder-8B (661 instances).\n\nInterpretation: This appears to be a manifestation of the \"Dead / Unused / Redundant code\" smell category. This may be linked to the challenge of performing a \"whole-project reference analy- sis, not snippet.\" An LLM might generate such placeholder structures as part of a broader pattern but not subsequently populate them or identify their redundancy. These examples collectively suggest that common SonarQube detections can be indicative of potential systemic weaknesses in current LLM code generation approaches. The consistency of these anti-patterns across diverse LLMs may suggest that they are not merely random errors, but result from patterns learned from training data, or from challenges in translating high-level requirements into robust and maintainable code. These thematic groupings suggest that LLMs may face challenges with aspects of software engineering that involve foresight, strategic planning, and an understanding of non-local conse- quences—qualities often associated with the engineering discipline in software development. As we saw for error handing, this could be another example of LLMs avoiding hallucinations by lacking specificity, in this case generating pseudo-implementations.\n\n18",
      "content_length": 2471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "9 Case Study: Evolution of Model Performance and Defect Char-\n\nacteristics\n\nTo directly address whether an improvement in a model’s functional performance correlates with an improvement in code quality (RQ4), this section presents a case study comparing two models from different generations. The analysis between Claude 3.7 Sonnet and its successor, Claude Sonnet 4, indicates that while benchmark performance can improve, the nature and severity of certain flaws might increase.\n\n9.1 Performance Benchmarks and Defect Severity\n\nImproved Functional Performance: The newer model, Claude Sonnet 4, achieved a higher benchmark score by passing 77.04% of tasks, compared to the older model’s 72.46%. This suggests progress in the model’s primary function of generating code that passes given tests.\n\nIncreased Bug Severity: An analysis of the generated bugs indicates a notable trend. The proportion of ‘BLOCKER’ bugs nearly doubled in Claude Sonnet 4, increasing to 13.71% from Claude 3.7 Sonnet’s 7.1%.\n\nIncreased Vulnerability Severity: A similar pattern was observed for security vulnerabilities. The proportion of ‘BLOCKER’ vulnerabilities rose from 56.03% in the older model to 59.57% in the newer model, suggesting that when the newer model introduced a vulnerability, it had a higher probability of being of the highest severity.\n\n9.2 Persistence and Evolution of Underlying Issues\n\nPersistent Code Quality Issues: The general profile of issues in the generated code appears largely consistent across model versions. For both models, code smells constituted the majority of flaws (over 92%). Despite its higher benchmark score, the newer Claude 4 still generated 2.11 issues for every test it passed, which may indicate that improved functional performance does not necessarily equate to a corresponding improvement in all code quality attributes.\n\nEvolution of the Defect Profile: Comparing the model versions reveals an evolving defect profile, not a simple reduction of flaws. Although the general defect categories persisted, their distribution shifted—for instance, the newer Claude 4 model produced a higher proportion of ‘Concurrency / Threading’ bugs. This suggests that changes in training strategies cause fundamental code genera- tion challenges to evolve rather than be resolved.\n\nThis case study suggests that progress in AI models is not uniform across all quality attributes. An increase in benchmark scores may paradoxically accompany more severe bugs and vulnerabilities, which underscores the critical role of rigorous static code analysis when applying LLMs to software development.\n\n19",
      "content_length": 2610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "10 The Role of Static Analysis in Addressing Potential LLM-Generated\n\nIssues\n\nThe findings from the preceding sections regarding systemic issues in LLM generated code lead to the questions of detection and mitigation. To that end, this section addresses how effectively static analysis can serve as a protective mechanism for development teams using AI assisted tools (RQ3). In this capacity, static analysis, as exemplified by SonarQube in this study, offers a valuable safeguard by providing an automated and consistent mechanism for flagging known anti-patterns, including resource management lapses and critical security vulnerabilities, before they enter a code- base.\n\nThe value of a tool like SonarQube can be seen in its capacity to detect specific, and often critical, flaws of the types observed in the LLM-generated code. For instance, the security vulnerability of hardcoded credentials (SonarQube rule java:S6437) [3] was consistently observed in this study. As discussed in Section 7, this appears to be a common pitfall for LLMs, possibly related to challenges in discerning the sensitive nature of certain constants. Similarly, resource management lapses, such as unclosed streams (flagged by java:S2095), were identified as recurring Blocker-level bugs. This finding may align with the previously noted challenge for LLMs in managing resource lifecycles that extend beyond a local context view.\n\nFurthermore, the code smell of high Cognitive Complexity (flagged by java:S3776) was common in the output of several LLMs, including Claude 3.7 Sonnet (422 instances) and GPT-4o (112 instances). This may reflect a tendency for LLMs to optimize for local token generation, potentially without accounting for global complexity metrics. Such complex code can impede maintainability and testability.\n\nIn these examples, static analysis tools like SonarQube can provide an automated and consistent mechanism for flagging known anti-patterns before they creep into a codebase. The comprehensive rule sets of such tools are often designed to cover a wide spectrum of issues—including bugs, vul- nerabilities, and code smells—which aligns well with the types of potential flaws identified in this study. Automated detection may be particularly valuable because manual review, while important, might not consistently identify all such issues, especially when managing large volumes of code.\n\nStatic analysis tools may become particularly important within an LLM-driven development paradigm, as they can provide a consistent, objective baseline for quality and security that may not be an inher- ent feature of probabilistic generative models. As LLMs can introduce variability, the deterministic and rule-based nature of static analysis may offer a valuable safeguard.\n\nAs LLMs become more integral to software development, the function of static analysis tools could evolve from a quality and security assurance measure to a component of responsible AI adoption. Static analysis may help bridge the gap between the output of LLMs and the quality and security standards often required in professional software engineering. The integration of such tools into Continuous Integration/Continuous Deployment (CI/CD) pipelines can further enhance the value of static analysis by allowing for continuous validation of code contributions.\n\n20",
      "content_length": 3338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "11 Conclusion\n\nThis quantitative analysis of five prominent LLMs over 4,442 Java tasks provides clear insights into the quality and security of AI-generated code. The findings address our four primary research questions, leading to a central conclusion: LLMs are powerful but imperfect coding assistants, and their output must be rigorously verified.\n\nFirst, the study confirms that all evaluated models produce issues. None generated consistently defect-free code, instead introducing a diverse spectrum of defects (RQ1). We classify these defects into the three analyzed types: code smells that degrade maintainability (e.g., excessive complexity), bugs that impact runtime reliability (e.g., resource leaks), and critical security vulnerabilities (RQ2). Security vulnerabilities include severe flaws such as hardcoded passwords [3, 4], path traversal vulnerabilities [5, 6], and XML External Entity (XXE) injection flaws [22, 23], demonstrating that static analysis is an effective mechanism for identifying these latent risks (RQ3).\n\nSecond, the research shows no correlation between a model’s functional performance and the quality of its code (RQ4). This leads to several critical insights for model selection. We found that bigger is not necessarily better, as a model’s scale or novelty did not guarantee higher-quality output. Critically, smaller can be just as good or better, with smaller models sometimes producing cleaner code for the tasks they successfully passed. This means that understanding your model is vital; teams must look beyond benchmark scores and evaluate a model’s unique defect profile to make informed choices.\n\nThird, these defects are best understood as features of the current technology, not bugs. They appear to be inherent consequences of a methodology that relies on replicating statistical patterns rather than performing semantic analysis. A prime example of this inherent risk is the use of outdated dependencies. Because LLMs are trained on older code, they frequently generate solutions with deprecated APIs or libraries containing known vulnerabilities (CVEs). This underscores the necessity of complementing static analysis with Software Composition Analysis (SCA) to manage a risk vector intrinsic to AI-assisted development.\n\nIn conclusion, the integration of LLMs into software development is transformative; however, lever- aging this capability effectively calls for informed vigilance. By understanding the potential pitfalls of LLM-generated code and by employing automated analysis tools, the software development com- munity can better navigate this new frontier, taking full advantage of AI while upholding established principles of high-quality, secure, and maintainable software.\n\nFuture Work. This study suggests several potential avenues for future investigation:\n\nThe impact of various prompt engineering and fine-tuning strategies aimed at mitigating the identified weaknesses in LLM-generated code.\n\nLongitudinal studies tracking the maintainability and technical debt accumulation of software systems with significant LLM contributions.\n\nThe effectiveness of LLMs in autonomously refactoring issues identified by static analysis tools, potentially creating a feedback loop for automated code improvement.\n\nComparative analyses of LLM performance on different programming languages to investigate whether these weaknesses are universal or vary by language.\n\nResearch into LLM architectures or training methodologies that could more directly address the challenges LLMs appear to face in generating robust and secure code.\n\n21",
      "content_length": 3587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Such advancements could contribute to a new generation of LLMs that function as more reliable software engineering assistants.\n\n22",
      "content_length": 130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Appendix\n\nA.1 Bugs categories\n\nTable 12: Bugs Categories and Sonar Rules\n\nBugs Categories\n\nDescription / Rule Examples\n\nSonar Rules Assigned\n\nAPI contract violation\n\nReturns, parameters, types, result ignored, equals/hashcode mismatch, contract breaking, incompatible\n\njava:S1206 java:S1221 java:S2109 java:S2159 java:S2177 java:S2225 java:S4143 java:S6863 java:S7184 java:S899\n\nControl-flow mistake\n\nAlways true/false, conditional blocks, unreachable, logic error, infinite/incorrect branching\n\njava:S2189 java:S2583 java:S3923 javabugs:S2190\n\nException handling\n\nSwallowing, suppressing, not propagating, mishandling exceptions and finally\n\njava:S1143 java:S1163 java:S2142 java:S3551\n\nResource mgmt/leak\n\nUnclosed resources, leaks, missed close, incorrect freeing\n\njava:S2095 java:S2116 java:S2886 java:S5164\n\nConcurrency/threading\n\nBroken synchronization, volatile misuse, thread-unsafe ops, double-checked locking, param sync\n\njava:S2168 java:S2222 java:S2445 java:S3077 java:S3078\n\nType-safety/casts\n\nBroken/illegal/unsafe casts, illegal type use, invalid class cast, use-after-cast, wrong generic\n\njava:S1872 java:S2175 java:S2184 java:S2677 javabugs:S6320\n\nPattern/regex\n\nBroken patterns, regex syntax or misuse, matches empty, ambiguous/redundant expressions\n\njava:S2639 java:S5842 java:S5850 java:S5855 java:S5856\n\nNull/data-value\n\nNPE, returns null, invalid/misused Optionals, misused null checks, redundant null-substitute\n\njava:S2259 java:S2789 java:S3655\n\nSerialization/Serializable Missing implements Serializable, writing unserializable object, incompatible object serialization\n\njava:S2118 java:S2441\n\nPerformance/structure\n\nStack overflows, repeated computation, non-terminating recursion, infinite loop\n\njava:S1751 java:S2119 java:S5998\n\nData structure\n\nWrong collection usage, wrong element access, improper generic usage, index-out-of-bounds, assignment\n\njavabugs:S6466\n\nOther\n\nAny bug not above\n\n23",
      "content_length": 1920,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "A.2 Code smells categories\n\nTable 13: Code Smells Categories and Sonar Rules\n\nCode Smells Categories\n\nDescription / Rule Examples\n\nRules Assigned\n\nNaming/style/and documentation\n\nField/class/interface naming/convention/miss- ing/incorrect/deprecated docs/visibility/badly named constants\n\njava:S101 java:S1123 java:S1124 java:S1133 java:S114 java:S115 java:S116 java:S1170 java:S119 java:S1214 java:S1215 java:S1444 java:S2176 java:S3008 java:S6126\n\nDead/unused/redundant code\n\nUnused variable/method/class/empty class/unused or redundant constants/dead logic/duplicate field/method\n\njava:S1126 java:S1130 java:S1144 java:S135 java:S1481 java:S1488 java:S1602 java:S1948 java:S2094 java:S2440 java:S2447 java:S2737 java:S2975 java:S3358 java:S3400 java:S3626 java:S3878 java:S3985 java:S4087 java:S4165 java:S4276 java:S5854\n\nCognitive/computational complexity Too complex/method/class\n\nsize/cyclomatic/brain method/excessive breaks/too many params\n\njava:S107 java:S3516 java:S3776 java:S6541\n\nStructure/architecture/layer cycle\n\nViolating package/schemat- ic/dependency/architecture rules or package cycles\n\njavaarchitecture:S7027, java:S6809, java:S6833\n\nCollection/generics/param/type\n\nUse generics/types/param types/rawtypes/method ref/lambda/field hiding/diamond op/refactor generics\n\njava:S1104 java:S1161 java:S1905 java:S2293 java:S2326 java:S3252 java:S3740 java:S4977 java:S6201 java:S6204\n\nAssignment/field/scope/visibility\n\nUse local not field/make field final/field hiding/static ref/move variable/logic/redundant assignment/visibility\n\njava:S127 java:S1854 java:S1450 java:S6213 java:S1117 java:S2209 java:S2201 java:S6837 java:S1994 java:S1193 java:S1611 java:S1141 java:S5993 java:S1165 java:S3305 java:S2147\n\nControl/conditional logic smell\n\nAlways true/false/merge if/redundant switch/repeated/complex ternary/duplicate branches\n\njava:S1066 java:S1125 java:S1301 java:S1871 java:S2589 java:S4144 java:S5411 java:S6208\n\nRegex/pattern/string/format\n\nInefficient/bad regex/string concatenation/builder/- toString/charsets/format- ting/name/misuse of equalsIgnoreCase\n\njava:S1149 java:S1153 java:S1192 java:S1210 java:S2629 java:S3457 java:S4635 java:S4719 java:S4738 java:S4968 java:S4973 java:S5413 java:S5843 java:S5866 java:S5869 java:S6019 java:S6035\n\nDeprecated/obsolete APIs\n\nUse of deprecated API/field/missing removal/overdue deprecated code\n\njava:S1874 java:S5738 java:S6355\n\nDesign & Framework Best-Practices\n\nSingleton checks/@Autowired advice/codegen/anti- pattern/miscellaneous maintainability\n\njava:S112 java:S1155 java:S1168 java:S1612 java:S2139 java:S6548 java:S6829 java:S6833\n\nOther\n\nAny Code Smell not above\n\n24",
      "content_length": 2646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "A.3 Vulnerability categories\n\nTable 14: Vulnerability Categories and Sonar Rules\n\nVulnerabilities Categories\n\nDescription / Rule Examples\n\nSonar Rules Assigned\n\nHard-coded credentials\n\nHardcoded passwords/keys/secrets found in code\n\njava:S6437\n\nPath-traversal & injection\n\nPath traversal, unsafe archive/file/URL constructs, user-controlled path/file/cookie/URL/JSON\n\njava:S6377 javasecurity:S2083 javasecurity:S5144 javasecurity:S6096 javasecurity:S6287 javasecurity:S6549\n\nCryptography misconfiguration Weak/incorrect cipher/IV,\n\ninsecure encryption, insecure hash, improper cipher mode/padding\n\njava:S2053 java:S3329 java:S5445 java:S5542 java:S5547\n\nCertificate-validation omissions Missing server\n\njava:S4830 java:S5527\n\nhostname/certificate validation in SSL/TLS\n\nJWT signature not verified\n\nJWT signature is not checked/verified before being used\n\njava:S5659\n\nXML External Entity (XXE)\n\nExternal entity expansion, XML parser not protected from external entities\n\njava:S2755\n\nJSON-injection risk\n\nDirect construction of JSON from untrusted/user-controlled data\n\njavasecurity:S6398\n\nInadequate error handling (I/O) Failing to catch, propagate,\n\njava:S1989\n\nor properly handle critical exceptions from untrusted sources\n\nOther\n\nAny vulnerability not above\n\n25",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "References\n\n[1] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavar- ian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Rad- ford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, “Evaluating large language models trained on code.” https://arxiv.org/abs/2107.03374, 2021.\n\n[2] SonarSource SA,\n\n“SonarQube Cloud Documentation.” https://docs.sonarsource.com/\n\nsonarqube-cloud/, 2024.\n\n[3] The MITRE Corporation, “CWE-798: Use of Hard-coded Credentials.” https://cwe.mitre.\n\norg/data/definitions/798.html, 2024.\n\n[4] OWASP, “A07:2021-Identification and Authentication Failures.” https://owasp.org/Top10/\n\nA07_2021-Identification_and_Authentication_Failures/, 2021.\n\n[5] The MITRE Corporation, “CWE-22:\n\nImproper Limitation of a Pathname to a Restricted\n\nDirectory (‘Path Traversal’).” https://cwe.mitre.org/data/definitions/22.html, 2024.\n\n[6] OWASP, “A01:2021-Broken Access Control.” https://owasp.org/Top10/A01_2021-Broken_\n\nAccess_Control/, 2021.\n\n[7] Anthropic,\n\n“Claude 3.7 sonnet and claude code.” https://www.anthropic.com/news/\n\nclaude-3-7-sonnet, Feb 2025.\n\n[8] J. Abrams, A. Ahuja, S. Akkalyoncu, and et al., “GPT-4o System Card,” arXiv preprint\n\narXiv:2405.07124, May 2024.\n\n[9] Meta,\n\n“Llama\n\n3.2 model\n\ncard.”\n\nhttps://huggingface.co/meta-llama/Llama-3.\n\n2-90B-Vision-Instruct, Sep 2024.\n\n[10] A. Martini and J. Bosch, “Towards a definition of technical debt,” in Proceedings of the 8th\n\nInternational Workshop on Technical Debt (TechDebt ’15), pp. 1–8, IEEE, 2015.\n\n[11] W. N. Behutiye, P. Rodriguez, M. Oivo, and A. Tosun, “Analyzing the concept of technical debt in the context of agile software development: A systematic literature review,” Journal of Systems and Software, vol. 217, p. 112166, 2024.\n\n[12] S. Dora, D. Lunkad, N. Aslam, S. Venkatesan, and S. K. Shukla, “The hidden risks of llm- generated web application code: A security-centric evaluation of code generation capabilities in large language models,” 2025.\n\n[13] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, and J. M. Zhang, “Large\n\nlanguage models for software engineering: Survey and open problems,” 2023.\n\n[14] S. Zhao, “Github copilot now has a better ai model and new capabilities.” https://github. blog/2023-02-14-github-copilot-now-has-a-better-ai-model-and-new-capabilities/, Feb 2023. Accessed on August 7, 2025.\n\n26",
      "content_length": 2837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "[15] F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, C. J. Anderson, M. Q. Feldman, A. Guha, M. Greenberg, and A. Jangda, “MultiPL-E: A Scalable and Polyglot Approach to Benchmark- ing Neural Code Generation,” IEEE Transactions on Software Engineering, vol. 49, no. 11, pp. 4836–4855, 2023.\n\n[16] J. Feng, J. Liu, C. Gao, C. Y. Chong, C. Wang, S. Gao, and X. Xia, “Complexcodeeval: A benchmark for evaluating large code models on more complex code,” in Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, ASE ’24, p. 1895–1906, ACM, Oct. 2024.\n\n[17] S. Huang, T. Cheng, J. K. Liu, J. Hao, L. Song, Y. Xu, J. Yang, J. Liu, C. Zhang, L. Chai, R. Yuan, Z. Zhang, J. Fu, Q. Liu, G. Zhang, Z. Wang, Y. Qi, Y. Xu, and W. Chu, “Opencoder: The open cookbook for top-tier code large language models,” 2025.\n\n[18] T. J. McCabe, “A complexity measure,” IEEE Transactions on Software Engineering, vol. SE-2,\n\nno. 4, pp. 308–320, 1976.\n\n[19] G. A. Campbell, “Cognitive complexity: A new way of measuring understandability.” https:\n\n//www.sonarsource.com/docs/CognitiveComplexity.pdf, 2018.\n\n[20] M. Fowler, Refactoring: Improving the Design of Existing Code. Addison-Wesley Professional,\n\n1999.\n\n[21] OWASP, “OWASP Top 10:2021.” https://owasp.org/Top10/, 2021.\n\n[22] The MITRE Corporation, “CWE-611: Improper Restriction of XML External Entity Refer-\n\nence.” https://cwe.mitre.org/data/definitions/611.html, 2024.\n\n[23] OWASP,\n\n“A05:2021-Security\n\nMisconfiguration.”\n\nhttps://owasp.org/Top10/A05_\n\n2021-Security_Misconfiguration/, 2021.\n\n27",
      "content_length": 1565,
      "extraction_method": "Unstructured"
    }
  ]
}