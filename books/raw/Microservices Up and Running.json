{
  "metadata": {
    "title": "Microservices Up and Running",
    "author": "Ronnie Mitra, Irakli Nadareishvili",
    "publisher": "O'Reilly Media",
    "edition": "1st Edition",
    "isbn": "978-1492075455",
    "total_pages": 319,
    "conversion_date": "2025-11-08T12:43:07.798146",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Microservices - Up and Running.pdf"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary"
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary"
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-25)",
      "start_page": 17,
      "end_page": 25,
      "detection_method": "topic_boundary"
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 26-50)",
      "start_page": 26,
      "end_page": 50,
      "detection_method": "topic_boundary"
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 51-58)",
      "start_page": 51,
      "end_page": 58,
      "detection_method": "topic_boundary"
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 59-67)",
      "start_page": 59,
      "end_page": 67,
      "detection_method": "topic_boundary"
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 68-75)",
      "start_page": 68,
      "end_page": 75,
      "detection_method": "topic_boundary"
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 76-85)",
      "start_page": 76,
      "end_page": 85,
      "detection_method": "topic_boundary"
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 86-94)",
      "start_page": 86,
      "end_page": 94,
      "detection_method": "topic_boundary"
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 95-107)",
      "start_page": 95,
      "end_page": 107,
      "detection_method": "topic_boundary"
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 108-118)",
      "start_page": 108,
      "end_page": 118,
      "detection_method": "topic_boundary"
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 119-130)",
      "start_page": 119,
      "end_page": 130,
      "detection_method": "topic_boundary"
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 131-139)",
      "start_page": 131,
      "end_page": 139,
      "detection_method": "topic_boundary"
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 140-147)",
      "start_page": 140,
      "end_page": 147,
      "detection_method": "topic_boundary"
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 148-156)",
      "start_page": 148,
      "end_page": 156,
      "detection_method": "topic_boundary"
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 157-170)",
      "start_page": 157,
      "end_page": 170,
      "detection_method": "topic_boundary"
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 171-183)",
      "start_page": 171,
      "end_page": 183,
      "detection_method": "topic_boundary"
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 184-192)",
      "start_page": 184,
      "end_page": 192,
      "detection_method": "topic_boundary"
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 193-200)",
      "start_page": 193,
      "end_page": 200,
      "detection_method": "topic_boundary"
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 201-210)",
      "start_page": 201,
      "end_page": 210,
      "detection_method": "topic_boundary"
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 211-221)",
      "start_page": 211,
      "end_page": 221,
      "detection_method": "topic_boundary"
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 222-229)",
      "start_page": 222,
      "end_page": 229,
      "detection_method": "topic_boundary"
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 230-238)",
      "start_page": 230,
      "end_page": 238,
      "detection_method": "topic_boundary"
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 239-246)",
      "start_page": 239,
      "end_page": 246,
      "detection_method": "topic_boundary"
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 247-254)",
      "start_page": 247,
      "end_page": 254,
      "detection_method": "topic_boundary"
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 255-262)",
      "start_page": 255,
      "end_page": 262,
      "detection_method": "topic_boundary"
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 263-270)",
      "start_page": 263,
      "end_page": 270,
      "detection_method": "topic_boundary"
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 271-278)",
      "start_page": 271,
      "end_page": 278,
      "detection_method": "topic_boundary"
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 279-295)",
      "start_page": 279,
      "end_page": 295,
      "detection_method": "topic_boundary"
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 296-304)",
      "start_page": 296,
      "end_page": 304,
      "detection_method": "topic_boundary"
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 305-313)",
      "start_page": 305,
      "end_page": 313,
      "detection_method": "topic_boundary"
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 314-319)",
      "start_page": 314,
      "end_page": 319,
      "detection_method": "topic_boundary"
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "Ronnie Mitra &\nIrakli Nadareishvili\nMicroservices \n Up & Running\nA Step-by-Step Guide to Building a \nMicroservices Architecture",
      "content_length": 127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "O'REILLY*\n\nMicroservices: Up and Running\n\nMicroservices architectures offer faster change speeds,\nbetterscalabiity. and cleaner. evolable system designs. But\nImplementing your first micoservces architecture is if.\n\now do yournake myriad choices. educate your team ona the\ntechnical details, and navigate the organization toa successful\n‘execution tomaxamiz your chance of success With this book,\nauthors Ronnie Mra and ira Nadaeishvil provide step by:step\n‘uidance for building an elective microservces architect.\nArchitects and engineers willow an implementation journey\nbased an techniques and architectures that have proven to work\nfor miroservices systems. You'llbulld an operating model. a\nmicroservices design, an infrastructure foundation, and two\nworking mcroserices, then put those pieces together asasingle\nimplementation For anyone tasked with building microservices\nloramicroservces arcutecture, hs guide is valuable\n\n+ Leaman effecive and expt end-toend microservces\nsystem design\n\n+ Define teams, their responsibilities, and guidelines for\nWorking together\n\n+ Understand how tose abig aplication into acolection\nlof meroservices\n\n+ Examine howto isolate andembed data into corresponding\nmicoseraices\n\n+ Buildasimple yet powerful Cl/CO pipeline fr infrastructure\nchanges\n‘+ Wit code for sample microserices\n\n+ Deploy a working microservices pplkation on Amazon\nWeb Services\n\n“This book provides\nclear and direct\n‘guidance for turning\nmicroservice principles\n‘and practices into\n‘actual working code.\n'mrecommending this\nbook to everyone who\n‘wants to get up and\nrunning quick!\n\n“Mike Amundsen\n‘aatorandsoveot\n\nRonnie mirais an autho strategist\nSndconaltatwithover25yearsah\n‘experience working with web ard\n‘onnectvy technologies He's the\ncathr of crosenceActectare\nSd Continuous AP Management.\nboth from oly\n\n‘eal Radars the ve\npresident of ore insovaton|\n\nSt Captal One leading the\n\nteams responsible for butding2\n‘microservices based core banking\n‘ator Prevousy. he as\nfounder and CTO of heathtech\nSartupReferel and held\ntechnology leadership roles at CA\nTechnloglesand NPR, You can\n‘low raion Titer at @inacare\n\nusu consason\n\nW000 aii\n\nTwitter @oceilymedia\nfacebook com/orelly",
      "content_length": 2192,
      "extraction_method": "OCR"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "Ronnie Mitra and Irakli Nadareishvili\nMicroservices: Up and Running\nA Step-by-Step Guide to Building\na Microservices Architecture\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing",
      "content_length": 209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "978-1-492-07545-5\n[LSI]\nMicroservices: Up and Running\nby Ronnie Mitra and Irakli Nadareishvili\nCopyright © 2021 Mitra Pandey Consulting, Ltd. and Irakli Nadareishvili. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com.\nAcquisitions Editor: Melissa Duffield\nDevelopment Editor: Melissa Potter\nProduction Editor: Deborah Baker\nCopyeditor: Charles Roumeliotis\nProofreader: Piper Editorial, LLC\nIndexer: nSight, Inc.\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Kate Dullea\nDecember 2020:\n First Edition\nRevision History for the First Edition\n2020-11-25: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492075455 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Microservices: Up and Running, the\ncover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "To every person who took the time to chronicle and share their experiences. And to\nKairav, who didn’t help me write this dedication.\n—Ronnie Mitra \nTo Lucas, who was born shortly after we started working on this book and whose smiles\ngave me the strength to complete this book in the middle of a global pandemic; to my\nwife Ana, for her support; and to my amazing students at Temple University, in Phila‐\ndelphia, who kindly “test drove” early versions of a lot of the content in this book.\n—Irakli",
      "content_length": 498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "Table of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\n1. Toward a Microservices Architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nWhat Are Microservices?                                                                                                  2\nReducing Coordination Costs                                                                                          4\nThe Coordination Cost Problem                                                                                 4\nThe Hard Parts                                                                                                                6\nLearning by Doing                                                                                                             7\nThe “Up and Running” Microservices Model                                                            8\nDecisions, Decisions…                                                                                                    10\nWriting a Lightweight Architectural Decision Record                                           11\nSummary                                                                                                                           13\n2. Designing a Microservices Operating Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  15\nWhy Teams and People Matter                                                                                      16\nTeam Size                                                                                                                       17\nTeam Skills                                                                                                                     18\nInterteam Coordination                                                                                              19\nIntroducing Team Topologies                                                                                        21\nTeam Types                                                                                                                    21\nInteraction Modes                                                                                                        23\nDesigning a Microservices Team Topology                                                                 24\nEstablish a System Design Team                                                                                24\nBuilding a Microservices Team Template                                                                 26\nPlatform Teams                                                                                                             29\nEnabling and Complicated-Subsystem Teams                                                         31\nConsumer Teams                                                                                                          32\nv",
      "content_length": 3028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "Summary                                                                                                                       34\n3. Designing Microservices: The SEED(S) Process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  35\nIntroducing the Seven Essential Evolutions of Design for Services: The\nSEED(S) Method                                                                                                          36\nIdentifying Actors                                                                                                            37\nExample Actors in Our Sample Project                                                                    39\nIdentifying Jobs That Actors Have to Do                                                                     39\nUsing Job Story Format to Capture JTBDs                                                               41\nExample JTBDs in Our Sample Project                                                                    42\nDiscovering Interaction Patterns with Sequence Diagrams                                      43\nDeriving Actions and Queries from JTBDs                                                                 45\nExample Queries and Actions for Our Sample Project                                          47\nDescribing Each Query and Action as a Specification with an Open Standard     48\nExample OAS for an Action in Our Sample Project                                               49\nGetting Feedback on the API Specification                                                                 53\nImplementing Microservices                                                                                         53\nMicroservices Versus APIs                                                                                             54\nSummary                                                                                                                           56\n4. Rightsizing Your Microservices: Finding Service Boundaries. . . . . . . . . . . . . . . . . . . . . . .  57\nWhy Boundaries Matter, When They Matter, and How to Find Them                   57\nDomain-Driven Design and Microservice Boundaries                                             59\nContext Mapping                                                                                                          62\nSynchronous Versus Asynchronous Integrations                                                   65\nA DDD Aggregate                                                                                                        66\nIntroduction to Event Storming                                                                                    66\nThe Event-Storming Process                                                                                      68\nIntroducing the Universal Sizing Formula                                                                   73\nThe Universal Sizing Formula                                                                                    74\nSummary                                                                                                                           74\n5. Dealing with the Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  75\nIndependent Deployability and Data Sharing                                                             75\nMicroservices Embed Their Data                                                                                  77\nEmbedding Data Should Not Lead to an Explosion in the Number of\nDatabase Clusters                                                                                                      78\nData Embedding and the Data Delegate Pattern                                                     79\nUsing Data Duplication to Solve for Independence                                                81\nDistributed Transactions and Surviving Failures                                                    82\nEvent Sourcing and CQRS                                                                                              85\nvi \n| \nTable of Contents",
      "content_length": 4173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "Event Sourcing                                                                                                              85\nImproving Performance with Rolling Snapshots                                                    91\nEvent Store                                                                                                                    92\nCommand Query Responsibility Segregation                                                          93\nEvent Sourcing and CQRS Beyond Microservices                                                      94\nSummary                                                                                                                           95\n6. Building an Infrastructure Pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  97\nDevOps Principles and Practices                                                                                   98\nImmutable Infrastructure                                                                                           99\nInfrastructure as Code                                                                                               100\nContinuous Integration and Continuous Delivery                                               102\nSetting Up the IaC Environment                                                                                 104\nSet Up GitHub                                                                                                            104\nInstall Terraform                                                                                                        105\nConfiguring Amazon Web Services                                                                            106\nSetting Up an AWS Operations Account                                                                106\nConfigure the AWS CLI                                                                                            110\nSetting Up AWS Permissions                                                                                    112\nCreating an S3 Backend for Terraform                                                                   115\nBuilding an IaC Pipeline                                                                                               116\nCreating the Sandbox Repository                                                                            117\nUnderstanding Terraform                                                                                         119\nWriting the Code for the Sandbox Environment                                                  120\nBuilding the Pipeline                                                                                                 123\nTesting the Pipeline                                                                                                    133\nSummary                                                                                                                         135\n7. Building a Microservices Infrastructure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  137\nInfrastructure Components                                                                                          137\nThe Network                                                                                                               138\nThe Kubernetes Service                                                                                             139\nThe GitOps Deployment Server                                                                              140\nImplementing the Infrastructure                                                                                 142\nInstalling kubectl                                                                                                        142\nSetting Up the Module Repositories                                                                        142\nThe Network Module                                                                                                145\nThe Kubernetes Module                                                                                            160\nSetting Up Argo CD                                                                                                   171\nTesting the Environment                                                                                           175\nCleaning Up the Infrastructure                                                                                177\nTable of Contents \n| \nvii",
      "content_length": 4582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "Summary                                                                                                                         178\n8. Developer Workspace. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  179\nCoding Standards and the Developer’s Setup                                                            180\n10 Workspace Guidelines for a Superior Developer Experience                         181\nSetting Up a Containerized Environment Locally                                                    187\nInstalling Multipass                                                                                                    188\nEntering the Container and Mapping Folders                                                       189\nInstalling Docker                                                                                                           190\nTesting Docker                                                                                                            191\nAdvanced Local Docker Usage: Installing Cassandra                                              192\nInstalling Kubernetes                                                                                                     193\nSummary                                                                                                                         195\n9. Developing Microservices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  197\nDesigning Microservice Endpoints                                                                             197\nFlights Microservice                                                                                                   201\nReservations Microservice                                                                                        201\nDesigning an OpenAPI Specification                                                                      202\nImplementing the Data for a Microservice                                                                208\nRedis for the Reservations Data Model                                                                   209\nMySQL Data Model for the Flights Microservice                                                  211\nImplementing Code for a Microservice                                                                     212\nThe Code Behind the Flights Microservice                                                            213\nHealth Checks                                                                                                             218\nIntroducing a Second Microservice to the Project                                                   220\nHooking Services Up with an Umbrella Project                                                        226\nSummary                                                                                                                         229\n10. Releasing Microservices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  231\nSetting Up the Staging Environment                                                                          232\nThe Ingress Module                                                                                                   233\nThe Database Module                                                                                                234\nForking the Staging Infrastructure Project                                                             234\nConfiguring the Staging Workflow                                                                          235\nEditing the Staging Infrastructure Code                                                                 237\nShipping the Flight Information Container                                                               241\nIntroducing Docker Hub                                                                                          241\nConfiguring Docker Hub                                                                                          242\nConfiguring the Pipeline                                                                                           242\nDeploying the Flights Service Container                                                                    246\nviii \n| \nTable of Contents",
      "content_length": 4396,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "Understanding Kubernetes Deployments                                                              247\nCreating a Helm Chart                                                                                              248\nCreating the Microservices Deployment Repository                                            249\nArgo CD for GitOps Deployment                                                                           255\nClean Up                                                                                                                          260\nSummary                                                                                                                         261\n11. Managing Change. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  263\nChanges in a Microservices System                                                                            263\nBe Data-Oriented                                                                                                       264\nThe Impact of Changes                                                                                             265\nThree Deployment Patterns                                                                                      266\nConsiderations for Our Architecture                                                                         268\nInfrastructure Changes                                                                                              269\nMicroservices Changes                                                                                              273\nData Changes                                                                                                              277\nSummary                                                                                                                         279\n12. A Journey’s End (and a New Beginning). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  281\nOn Complexity and Simplification Using Microservices                                        282\nMicroservices Quadrant                                                                                            283\nMeasuring the Progress of a Microservices Transformation                                   285\nSummary                                                                                                                         288\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  291\nTable of Contents \n| \nix",
      "content_length": 2648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "Preface\nTen years ago a group of software architects gathered together and coined the term\nmicroservices to define a style of software architecture that had evolved. Since that\ntime, there’s been an explosion of classes, videos, and written works for the microser‐\nvices style. In fact, in 2016 we coauthored Microservice Architecture, a book that\noffered an introductory guide to the principles of a microservices system.\nSince the publication of that book, we and many others have had a chance to live with\nthe microservices systems we’ve built. Our own experiences, as well as conversations\nwith other practitioners, have led to a better understanding of the practical problems\nthat implementers face. A lot of that understanding comes from success, but some of\nthe most useful insights have come from mistakes.\nWe’ve endeavored to package up the experiences of practitioners into a highly opin‐\nionated guide. We live in an age with an abundance of practitioner advice available.\nBut, it can be difficult to navigate this sea of information and put it together in a way\nthat works. This book offers a practical, prescriptive model that spans team design,\ndomain design, infrastructure, engineering, and release. Our goal is to give you a uni‐\nfied view of a microservices implementation and a strong first step in your journey to\nadoption.\nWho Should Read This Book\nWe’ve written this book for microservices implementers. While we touch on some of\nthe principles and patterns of a microservices system, the focus of the book is on\npractical design and engineering. If you are an architect or engineer tasked with\nbuilding microservices or a microservices architecture, this is the book for you.\nBut this book is also a useful guide for readers who simply want to get “up close and\npersonal” with a microservices implementation. No matter what your role is, if you’re\ninterested in understanding the work that goes into building a microservices system,\nyou’ll find this book enlightening.\nxi",
      "content_length": 1994,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "What You’ll Need\nSince the scope of microservices is quite large, we use a number of different tools and\nmethods. If you want to follow along with all of the examples, you’ll need to install or\nsubscribe to use the folllowing tools and platforms:\n• Docker\n• Redis\n• MySQL\n• GitHub\n• GitHub Actions\n• Terraform\n• Amazon Web Services\n• kubectl\n• Helm\n• Argo CD\nWe provide instructions on where and how to access these tools in their relevant\nsections.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program ele‐\nments such as variable or function names, databases, data types, environment\nvariables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter‐\nmined by context.\nxii \n| \nPreface",
      "content_length": 1065,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "This element signifies a tip or suggestion.\nThis element signifies a general note.\nThis element indicates a warning or caution.\nUsing Code Examples\nSupplemental material (code examples, exercises, etc.) is available for download at\nhttps://oreil.ly/MicroservicesUpandRunning.\nIf you have a technical question or a problem using the code examples, please email\nbookquestions@oreilly.com.\nThis book is here to help you get your job done. In general, if example code is offered\nwith this book, you may use it in your programs and documentation. You do not\nneed to contact us for permission unless you’re reproducing a significant portion of\nthe code. For example, writing a program that uses several chunks of code from this\nbook does not require permission. Selling or distributing examples from O’Reilly\nbooks does require permission. Answering a question by citing this book and quoting\nexample code does not require permission. Incorporating a significant amount of\nexample code from this book into your product’s documentation does require\npermission.\nWe appreciate, but generally do not require, attribution. An attribution usually\nincludes the title, author, publisher, and ISBN. For example: “Microservices: Up and\nRunning by Ronnie Mitra and Irakli Nadareishvili (O’Reilly). Copyright 2021 Mitra\nPandey Consulting, Ltd. and Irakli Nadareishvili, 978-1-492-07545-5.”\nIf you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com.\nPreface \n| \nxiii",
      "content_length": 1532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "O’Reilly Online Learning\nFor more than 40 years, O’Reilly Media has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nOur unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nYou can access the web page for this book, where we list errata, examples, and any\nadditional information at https://oreil.ly/Microservices_Up_and_Running.\nEmail bookquestions@oreilly.com to comment or ask technical questions about this\nbook.\nFor news and information about our books and courses, visit http://oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://www.youtube.com/oreillymedia\nAcknowledgments\nWe’d like to thank our editors Melissa Potter and Deborah Baker, and the team at\nO’Reilly, without whom we’d never have finished this book. We’d also like to thank\nPete Hodgson, Chris O’Dell, Lorinda Brandon, JP Morgenthal, Mike Amundsen, and\nDavid Butland for the incredible insight, feedback, and observations they provided.\nFinally, we’d like to thank Capital One and Publicis Sapient for the support they pro‐\nvided in allowing us to bring this book to life.\nxiv \n| \nPreface",
      "content_length": 1827,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "CHAPTER 1\nToward a Microservices Architecture\nThe goal of this book is to help you build a working microservices architecture. In\npages you’ll find opinionated and prescriptive advice for building software. That\nadvice comes from real practitioner experiences that we’ve gathered, both from suc‐\ncessful implementations and the ones that could have gone better. We’ve refined these\nlessons into a model that we hope will get you up and running faster with your own\nsystem.\nRecently, the microservices style of building software has exploded in popularity. In\nthe early 2010s, the term microservices emerged as a way to describe a new style of\nsoftware architecture. Applications built in this newly named style are built with\nsmall, independent components that work together. Since then, adoption rates for the\nmicroservices style have skyrocketed. Startups, enterprise companies, and everyone\nin between have been learning and implementing microservices-style architectures.\nThe growing ecosystem of tools, services, and solutions in this space is testament to\nits widespread popularity. At the time of this writing, Allied Market Research has pre‐\ndicted that the global market for microservices architectures will grow to $8.07 billion\nUSD in 2026, from the current $2.07 billion USD. These kinds of numbers indicate a\nlot of interest, a lot of adoption, and lots and lots of microservices work.\nFor many, building software in the microservices way has turned out to be a chal‐\nlenge. The truth is that implementing a microservices system isn’t easy. Making lots of\nindependent parts work together is harder to do than it might sound. Management,\nmaintenance, support, and testing costs add up in the system. At scale, those costs\ncan become prohibitive. If you aren’t careful, the pain of managing the system can\nmake microservices seem like a bad idea.\nBut the benefits of building microservices make the risks worthwhile. Microservices\ndone well enable you to make software changes faster and safer at scale. Faster and\n1",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "safer change means more agility for your business. That agility translates to better\noutcomes for your business and your organization.\nThe trick to unlocking all that value is to have the right architecture in place to sup‐\nport the services. It needs to reduce system costs, without diminishing the value of\nindependent services. To build that architecture, you’ll need to make important deci‐\nsions early. Those decisions will span methods, processes, teams, technologies, and\ntools. They’ll also need to work together to form an emergent, optimized whole.\nA good way to build a system like this is through evolution. You could start with a few\nsmall decisions and learn and grow as you go. In fact, most early adopters ended up\nwith microservices through iterative experimentation. They didn’t set out with a goal\nof building a microservices-based application. Instead, they ended up with them\nthrough a continuous process of optimization and improvement.\nStarting from scratch and iterating takes time. But the good news is that you can use\nthe experiences of these practitioners to help you build your system faster. Begin your\nbuild with a foundation of patterns, methods, and tools that have been used together\nsuccessfully. Then optimize the system to meet the unique goals and constraints of\nyour organization.\nIn this book, we’ve documented the decisions that form a strong microservices foun‐\ndation. Before we can dive into the details of the model, let’s address an important\nquestion. What exactly do we mean by “microservices”?\nWhat Are Microservices?\nThere isn’t one official, canonical definition for microservices. A good starting place is\nJames Lewis and Martin Fowler’s seminal article on microservices from 2014. In that\npiece, they describe microservices as:\nan approach to developing a single application as a suite of small services, each run‐\nning in its own process and communicating with lightweight mechanisms. […] built\naround business capabilities and independently deployable by fully automated deploy‐\nment machinery.\nThe real heart of Lewis and Fowler’s article is the set of nine characteristics that\nmicroservices possess. Their list starts with the core microservice characteristic of\ncomponentization via services, which means breaking an application into smaller serv‐\nices. From there they go on to cover a wide breadth of capabilities. They document\nthe need for organizational and management design with the characteristics of orga‐\nnization around business capabilities and decentralized governance. They hint at\nDevOps and Agile delivery practices when they introduce infrastructure automation\nand products not projects. They also identify a few key architecture principles, such as\nsmart endpoints and dumb pipes, design for failure, and evolutionary design.\n2 \n| \nChapter 1: Toward a Microservices Architecture",
      "content_length": 2855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "Each of these characteristics is worth understanding, and we encourage you to read\ntheir article if you haven’t already. Together, these characteristics form a holistic solu‐\ntion with a very large set of concerns. It includes technology, infrastructure, engineer‐\ning, operationalization, governance, team structure, and culture.\nFor contrast, here is another definition for microservices from the book Microservice\nArchitecture by Irakli Nadareishvili, Ronnie Mitra, Matt McLarty, and Mike Amund‐\nsen (O’Reilly):\nA microservice is an independently deployable component of bounded scope that sup‐\nports interoperability through message-based communication. Microservice architec‐\nture is a style of engineering highly automated, evolvable software systems made up of\ncapability-aligned microservices.\nThis definition is similar to Lewis and Fowler’s, but it pays special attention to boun‐\nded scopes, interoperability, and message-based communication. It also makes a dis‐\ntinction between microservices and the architecture that enables them.\nThese are just two examples from a sea of microservices definitions. As with these\nexamples, most definitions are broadly similar, but each of them differs slightly in\ntheir focus. But they’re usually different enough that it becomes hard to gauge if\nyou’ve built a textbook microservices system.\nIn the world of technology, names are important because they give us a simple way of\ncommunicating complex concepts. In this case, the “microservices” label allows us to\ndescribe a style of software architecture that has three general design traits:\n1. The application architecture is primarily composed of machine-invocable “serv‐\nices” that are made available on a network.\n2. The sizes (or boundaries) of services are an important design factor. These\nboundaries include runtime, design-time, and people factors.\n3. The software system, organization, and way of working are holistically optimized\nto achieve a goal.\nThis is a pretty general set of design traits. For example, it doesn’t document organi‐\nzational styles, specific tools, or architectural principles that should be used. There\nalso aren’t any formal patterns or practices defined. Instead, this gives us just enough\ncharacteristics to be able to identify a microservices system when we see one.\nThe truth is, you can get away with calling almost any API-based system a microser‐\nvices architecture if you try hard enough. But the real focus should be on the goal of\nyour system. We think that question of why you’d build microservices is much more\nenlightening than the question of what they are. While there are lots of potential ben‐\nefits to microservices, we believe the best reason to build software this way is to\nreduce your coordination costs.\nWhat Are Microservices? \n| \n3",
      "content_length": 2792,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "Reducing Coordination Costs\nCompanies around the world have had success implementing microservices architec‐\ntures. Almost universally, the practitioners we’ve talked to have reported an increase\nin speed of software delivery. We believe that improvement comes from the funda‐\nmental benefit of the microservices style: a reduction in coordination costs.\nIt should be pointed out that there are many ways to increase speed in software engi‐\nneering. Building software the microservices way is just one option. For example, you\ncould build a system quickly by cutting corners and incurring “technical debt” that\nyou’ll deal with later. Or, you could focus less on stability and safety and just get your\nproduct out the door. In some situations and for some businesses these are reason‐\nable approaches.\nBut systems developed for the financial, healthcare, and government sectors, among\nothers, are not allowed to compromise on safety for the sake of speed. And yet, com‐\npetitive and market forces demand higher speed from these industries just like any\nother. This is where a microservices system can shine. It provides an architectural\napproach that allows you to increase speed without compromising safety. And it lets\nyou do that at scale.\nThe Coordination Cost Problem\nBuilding complex software is hard work. In films and on TV, a brilliant programmer\ncan heroically engineer a world-changing product over the course of a sleepless week‐\nend. In real life it takes lots of people and a whole lot of time to produce a quality\nresult. Multiple teams working on a complex project are typically implementing dif‐\nferent parts of said system, following independent roadmaps, at independent paces.\nPeriodically, these parts need to be integrated to resolve dependencies, at which point\nthe mostly autonomous teams need to coordinate their work (see Figure 1-1).\nImagine that Jane is the team lead in charge of the Accounting workstream. Her team\njust finished a sprint and has a dependency on a component being developed by the\nteam in charge of the Shipment module, led by Tyrone. Since roadmaps are inde‐\npendent, it could be that Tyrone’s team is not actually done with their implementation\nof the needed component, in the Shipment workstream. At this point Jane has one of\ntwo choices: she can either wait for the component to be delivered (prioritizing safety\nbut sacrificing speed by putting her team on halt) and do a proper integration test, or\nshe can rely on an agreed interface contract between her team and Tyrone’s, assuming\nthat his team will deliver the component exactly as planned. In the latter case, Jane\nwould proceed without interruption, increasing her team’s speed, but potentially\ncompromising the overall safety of the system since integration testing didn’t occur at\nthe earliest possible stage and a “happy path” assumption was made.\n4 \n| \nChapter 1: Toward a Microservices Architecture",
      "content_length": 2911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "Figure 1-1. Sample timeline of a complex project with coordination touchpoints\nAny team lead in a complex, multiteam environment regularly faces this choice\nbetween ignoring coordination costs and keeping momentum versus acknowledging\nthe need for coordination and slowing down. Typically we choose one or the other\nusing our intuition on risk versus benefits, but overall, in a sufficiently complex sys‐\ntem, when these choices occur frequently enough there is a very pronounced tension\nbetween speed and safety.\nThe tension is real; however, it is not related to our primal instincts and there is a way\nto fix it. Since coordination costs cause the tension, what if we had a system specifi‐\ncally designed in a way to minimize those coordination costs? What if instead of\nchoosing one way or the other, teams did not even face the choice most of the time?\nYou can have such a design, emphasizing the minimization of coordination, if you\nhave autonomous teams working on small batches of isolated work. And that is\nexactly what microservices architecture is all about, in its essence.\nUnderstanding that the fundamental force of building successful microservices archi‐\ntectures is aiming for the minimization of coordination is extremely useful. It gives us\na universal litmus test. Building complex distributed systems such as a microservices\narchitecture isn’t easy, and when in doubt we should always ask ourselves, “Is this\ndecision I am facing going to reduce coordination costs for my teams or not?” The\nright answer will be much more obvious when we view decisions from the perspec‐\ntive of coordination costs.\nUltimately, microservices have become popular because they help businesses succeed.\nModern organizations are under incredible pressure to adapt, change, and improve\nmore often and more quickly. Investing in a technology architecture that is purpose‐\nfully designed to change speed and change safety at the scale of a large organization\nReducing Coordination Costs \n| \n5",
      "content_length": 1989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "makes a lot of sense. The microservices style enables companies operating in complex\ndomains to have the agility of a simpler, smaller company while continuing to harness\nthe power and reach of their actual size. It’s incredibly appealing and the growth in\nadoption proves that—however, the benefits don’t come for free. It takes a lot of up-\nfront work, focus, and decision making to build a microservices architecture that can\nunlock that value.\nThe Hard Parts\nOne of the biggest hurdles that first-time microservices adopters face is dealing with\nthe enormous scope and breadth of a microservices system. You might start by focus‐\ning on creating smaller, bounded services. But very soon you’ll find yourself having to\ncome up with the right infrastructure, data models, frameworks, team models, and\nprocesses to support them. It’s a lot of ground to cover and dealing with all of that\nscope can lead to some unique challenges. Here are the three big design problems\nthat microservices architects and engineers usually face:\nLong feedback loops\nOne big challenge is that impactful decisions in a microservices system aren’t\neasy to measure. From the decisions you make today problems may emerge, but\nthey may not show up until much later. For example, when you start out you\nmight decide to use a shared communication library to make it easier for your\nservices to talk to each other. Over time it may become clear that keeping that\nlibrary up to date across all of your microservices and teams turns out to be a\nhuge problem. The crux of the problem here is that it’s difficult to understand the\nimpact of the decision you’re making until problems arise, which makes it diffi‐\ncult to evaluate options and choose among them.\nToo many moving parts\nAt its heart, a microservices system is a complex adaptive system. This means\nthat each part of the system impacts the other parts in some way. When all those\nparts come together an emergent system behavior is produced. If you’ve ever\nintroduced a new tool or a new process into an organization, you’ve probably\nseen this firsthand. Some teams take to new stimuli and change immediately, oth‐\ners need help and support to adapt, but no matter what, you almost always end\nup with consequences as to the way people work and the decisions that are made.\nFor example, technology teams who introduce Docker containerization tooling\ninevitably end up adapting their development and release life cycle as a conse‐\nquence of their adopting the container deployment model. Sometimes these con‐\nsequences are planned, but often we need to deal with the unintended\nconsequences of the changes that are introduced. This complexity is what makes\nmicroservices system design difficult. It’s difficult to predict the specific impacts\nof the changes that are introduced, leading to a risk that we’ll do more harm than\ngood with a new architecture model.\n6 \n| \nChapter 1: Toward a Microservices Architecture",
      "content_length": 2941,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "Analysis paralysis\nWhen we compound the problem of long feedback loops for our decision with\nthe complex system we need to design, it’s easy to see why microservices architec‐\nture is a challenge. The decisions you need to make are both highly impactful and\ndifficult to measure. This can lead to endless speculation, discussion, and evalua‐\ntion of architectural decisions because of the fear of making the wrong kind of\nsystem. Instead of building a system that can achieve business outcomes, we end\nup in a state of indecision, trying to model the endless permutations of our\nchoices. This condition is commonly known as analysis paralysis. It doesn’t help\nthat the web is full of horror stories, “bumper sticker” advice, and contradictory\nbest practices for building a microservices architecture.\nUltimately, the real challenge of building a microservices architecture is that of deal‐\ning with a big, complicated system that spans a huge scope. The good news is that this\nis not a unique problem to solve. In this book, we’ll be bringing together and using a\nset of practices and patterns that have evolved for this type of domain. We’ll also be\nintroducing and implementing tools that embody these ways of working and make\nthe work that happens in a microservices system easier, safer, cheaper, and faster.\nLearning by Doing\nSo far, we’ve established that the microservices style can help you deliver software\nfaster without compromising on safety. But we’ve also identified that the path to a\ngood microservices architecture is difficult and fraught with challenging and complex\ndecisions. Many of the successful microservices implementers we’ve talked to have\nbuilt their systems through continual iteration and improvement. Frequently, they’ve\nhad to build architectures that failed before they unlocked an understanding of how\nto build a system that works.\nIf you had unlimited time, you could build a great microservices architecture solely\nthrough experimentation. You could adopt endless organizational models, try every\nmethodology, and build microservices of various sizes. As long as you could measure\nyour results, you’d continue to improve the system. With enough trials, you’d end up\nwith a system that works for you as well as a lot of experience building microservice\nsystems.\nChances are, though, that you don’t have the luxury of unlimited time. So, how do\nyou build the expertise you need to build better microservices?\nTo help address this challenge, we’ve developed a prescriptive microservices model.\nWe’ve made decisions about team design, process, architecture, infrastructure, and\neven tools and technologies. We’ll cover a large scope of topic areas while building a\nsolution that brings those areas together. Our decisions are built on opinions based in\nexperiences building microservices systems for large organizations. If you follow our\nLearning by Doing \n| \n7",
      "content_length": 2895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "instructions, by the end of the book you’ll have built a simple, operational microser‐\nvices system in a cloud-based architecture.\nTo help bring our microservices examples to life, we’ll be using the\nbackdrop of a fictional airline reservations system. It will be a vastly\nsimplified version of what a real reservations system would look\nlike. Our very basic airline reservations system will include two\nfunctions: a read-only flight information service and a seat reserva‐\ntion service.\nOur goal is to guide you in building your first microservices implementation as\nquickly as possible. In our experience, the act of building a real system is the best way\nto gain a true understanding of the work involved and the key decisions. We don’t\nexpect you to agree with all of our decisions. In fact, questioning the decisions we’ve\nmade for you is a big part of the learning journey! We hope that the model we build\ntogether is only the first of many microservices systems that you’ll build.\nThe Dreyfus Model of Skill Acquisition\nStarting a learning journey by following instructions is a tried-and-\ntrue path to gaining expertise. In Stuart and Hubert Dreyfus’s Five-\nStage Model of Adult Skill Acquisition, the first stage involves\nfollowing prescriptive guidance before proficiency and expertise\nare established.\nThe “Up and Running” Microservices Model\nThe scope of a microservices architecture is quite large. Unfortunately, we can’t cover\nthe entire scope in this single book. However, we’ve made an effort to cover the topic\nareas that are the most relevant to a microservices system and have the biggest impact\non success. Let’s take a quick look at what we’ll be covering in our “up and running”\nmicroservices model.\nTeam design\nWe’ll kick off our build in Chapter 2 by tackling the people side of a microservi‐\nces system. We’ll uncover the challenges of effective team design and the funda‐\nmental factors that influence microservice coordination. We’ll also introduce the\nteams we’ll be using within our example system along with a tool called Team\nTopologies to help design them.\n8 \n| \nChapter 1: Toward a Microservices Architecture",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "Microservice design\nAfter designing the teams, we’ll introduce the SEED(S) process in Chapter 3. This\nis a design process that will help us create microservices that fulfill the needs of\nusers and consumers with actionable interfaces and behaviors. Then, in Chap‐\nter 4, we’ll take on the problem of designing the right boundaries for our example\nmicroservices. We’ll also introduce some important Domain-Driven Design con‐\ncepts and use a process called Event Storming to “rightsize” our services.\nData design\nData is one of the most difficult aspects of a microservices design. In Chapter 5,\nwe’ll take a look at the data factors you’ll need to consider in a microservices sys‐\ntem. We’ll introduce the concept of data independence and lay the groundwork\nfor the data architecture in our example project.\nCloud platform\nOur microservices implementation will be built on top of a cloud-based infra‐\nstructure. In Chapter 6, we’ll introduce and implement the principles of immuta‐\nble infrastructure and infrastructure as code (IaC) as the foundation for our\nmicroservices infrastructure. We’ll also introduce AWS as our cloud platform and\nbuild a GitHub Actions–based CI/CD pipeline. Then, in Chapter 7, we’ll use that\npipeline to design and develop an AWS-based microservices infrastructure that\nwill include networking, a Kubernetes cluster, and a GitOps deployment tool.\nMicroservices development\nWith our infrastructure platform in place, we’ll dive into the work of engineering\nthe microservices. We’ll start by covering the principles and tools you’ll need to\nsucceed in Chapter 8. Then in Chapter 9, we’ll implement two independent, het‐\nerogeneous microservices for our example application.\nRelease and change\nWe’ll bring the whole solution together in Chapter 10, where we’ll deploy one of\nthe example microservices we’ve engineered onto the cloud-based platform we’ve\ndeveloped. To do this, we’ll use a set of technologies including DockerHub,\nKubernetes, Helm, and Argo CD. Finally, after release, we’ll take a retrospective\nlook at the system in Chapter 11.\nThe model we’ve developed is built on a set of five guiding princi‐\nples, including the twelve-factor app pattern. If you’re interested,\nyou can read about our model’s guiding principles at this book’s\nGitHub repository.\nLearning by Doing \n| \n9",
      "content_length": 2315,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "Hopefully this short overview gives you an idea of the scope of our model and exam‐\nple application. By the end of the book we’ll have implemented a full-fledged system.\nTo get there, we’ll need to make a lot of decisions. So, the first tool we’ll need is a way\nof keeping track of the really important ones.\nDecisions, Decisions…\nWhen it comes to building software, decisions are a big deal. Professional software\nengineers and architects get paid a lot for the decisions that they make and the prob‐\nlems they solve. The quality of the software and the business outcomes they drive\ndepend on the quality of those decisions.\nBut decisions aren’t always easy to make. They also aren’t always correct. We make the\nbest decisions we can given the information, experience, and talent that we have.\nWhen any of those variables change, our decisions should change too. Some deci‐\nsions are correct at the time, but become outdated when technology, people, or situa‐\ntions change. Some decisions were never good ones in the first place. In either case,\nwe need a way of capturing the decisions that matter so we can re-evaluate and\nimprove on them over time.\nTo address that need, we’re going to use a tool called an architecture decision record\n(or ADR). We’re not sure who invented the term ADR or when it was first used, but\nthe idea of documenting design decisions has been around for a long time. The real\nproblem is that most people don’t take the time to do it. In our experience, ADRs are\nan extremely useful tool and a good way of getting clarity on the decisions that need\nto be made.\nA good decision record needs to capture four important elements:\nContext\nWhat is the challenge? What is the problem that we are trying to solve? What are\nthe constraints? A decision record should give us a summary of these contextual\nelements. That way we can understand the rationale for a decision and why it\nmay need to be updated.\nAlternatives\nA decision isn’t a decision unless there is a choice to be made. A good decision\nrecord should help us to understand what the choices are. This helps us to better\nunderstand the context and the “selection space” at the time the decision was\nmade.\nChoice\nAt the heart of a decision is the choice. Every decision record needs to document\nthe choice that was made.\n10 \n| \nChapter 1: Toward a Microservices Architecture",
      "content_length": 2352,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "Impact\nDecisions have consequences and a decision record should document the impor‐\ntant ones. What are the trade-offs? How will our decision choice impact the way\nwe work or other decisions that need to be made?\nYou can create decision records however you like. You can write them up as text files,\nuse a project management tool, or even track them in a spreadsheet. The format and\ntooling is less important than the content. As long as you capture the areas we’ve\ndescribed you’ll have a good decision record.\nFor our example project, we’ll use an existing format called a lightweight architectural\ndecision record (LADR). The LADR format was created by Michael Nygard, and is a\nnice concise way of documenting a decision record. Let’s get to know LADR by build‐\ning one together.\nIf you want to use something other than LADR, Joel Parker Hen‐\nderson maintains a great list of ADR formats and templates.\nWriting a Lightweight Architectural Decision Record\nThe first key decision we’ll record is the decision to keep a record of decisions. Put\nmore simply, we’ll create an ADR that says we intend to keep track of our decisions.\nAs we’ve mentioned, we’ll be using the LADR format. The nice thing about LADR is\nthat it’s designed to be lightweight. It lets us keep track of decisions in simple text files\nthat we can write quickly. Since we’re dealing with text files, we can even manage our\ndecision records in the same way we manage source code.\nLADRs are written using a text format called Markdown, which provides an elegant\nand simple way of writing documentation. What’s great about Markdown is that it’s\neasy for humans to read in its raw form and most popular tools know how to render\nit. For example, Confluence, GitLab, GitHub, and SharePoint can all process Mark‐\ndown and present it as a formatted, human-readable document.\nTo create our first Markdown-based LADR, open your favorite text editor and start\nworking on a new document. The first thing we’ll do is lay out the structure.\nDecisions, Decisions… \n| \n11",
      "content_length": 2024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "Add the following text to your LADR file:\n# OPM1: Use ADRS for decision tracking\n## Status\nAccepted\n## Context\n## Decision\n## Consequences\nThese are the key elements of our decision record. The # characters preceding the\nlines are Markdown tokens that will let the parser know that these lines are meant to\nbe headings. Notice that we’ve given this decision a title that corresponds to the deci‐\nsion we’re making. We’ve also given the decision the slightly cryptic title: “OPM1.”\nThis is just a short form code that will help us label and understand which part of the\nsystem the decision relates to. In this case, “OPM1” indicates that this is the first deci‐\nsion we’re recording related to the operating model.\nThe Status header of our record lets us know what life-cycle stage this decision is in.\nFor example, if you’re drafting a new decision that you need to get agreement on, you\nmight start with a status of Proposed. Or, if you’re considering changing an existing\ndecision, you might change its status to Under Review. In our case, we’ve already\nmade the decision for you, so we’ve set the status to Accepted.\nThe Context section describes the problem, constraints, and background for the deci‐\nsion being made. In our case, we want to capture the need to log important decisions\nand why that’s important. Add the following text (or your own variation of it) to the\nContext section of your record:\n## Context\nA microservices architecture is complex and we'll need to make many\ndecisions. We'll need a way to keep track of the important decision\nwe make, so that we can revisit and re-evalute them in the future.\nWe'd prefer to use a lightweight, text-based solution so that we\ndon't have to install any new software tools.\nWith the context in place, we can move on to recording the actual decision we’ve\nmade. We can list some of the alternatives considered as well as our choice to use\nLADR. Add the following to the Decision section to document this fact:\n## Decision\nWe've decided to use Michael Nygard's lightweight architectural\ndecision record (LADR) format. LADR is text based and is\nlightweight enough to meet our needs. We'll keep each LADR record\nin its own text file and manage the files like code.\nWe also considered the following alternative solutions:\n12 \n| \nChapter 1: Toward a Microservices Architecture",
      "content_length": 2329,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "* Project management tooling (not selected, because we didn't want\n  to install tools)\n* Informal or \"word of mouth\" record keeping (not reliable)\nAll that’s left is to document the consequences. In our case, one of the key conse‐\nquences is that we’ll need to spend time actually documenting our decisions and\nmanaging the records. Let’s capture that as follows:\n## Consequences\n* We'll need to write decision records for key decisions\n* We'll need a source code management solution to manage decision record files\nThat’s all it takes to write an LADR. This is an incredibly useful way of capturing your\nthinking and has the added benefit of forcing you to make rational, thoughtful deci‐\nsions in the first place. As we build our example flights application, we’ll be keeping a\nlog of the key decisions we make. To save time, we won’t write out the entire decision\nrecord. Instead we’ll highlight that a key decision has been made as in the following\nnote.\nKey Decision: Use ADRs for Decision Tracking\nUse ADRs to log the key decisions we’ve made in our system design and build.\nYou’ll be able to find a detailed version of each decision record at this book’s GitHub\nrepository.\nSummary\nIn this chapter we introduced some foundational concepts for this book. We provided\na loose definition of a microservices system, including a set of three key traits. We\nidentified the reduction of coordination costs as the key microservices benefit. We\nalso explored how complexity and analysis paralysis present challenges to microservi‐\nces adopters.\nTo help address these challenges, we introduced the “up and running” microservices\nmodel—an opinionated, prescriptive implementation that will accelerate the learning\nprocess for implementers. We covered the aspects of the model and the topics we’ll\ndiscuss. Finally, we introduced the concept of the architectural decision record\n(ADR) that we plan to use throughout the rest of the book.\nWith the overview out of the way, all that’s left is to build the system. We’ll kick things\noff in Chapter 2 by tackling how microservices work is done with a special focus on\nteam coordination.\nSummary \n| \n13",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "CHAPTER 2\nDesigning a Microservices Operating Model\nIn this book, we’ll be building a microservices-based application. To do that, we’ll\ndesign and build microservices as well as the infrastructure and tools you need to\nsupport them. However, the truth is that success with microservices takes more than\nwriting code and deploying it. To really succeed, you need to have the right people,\nthe right ways of working, and the right principles in place to make the whole system\nwork. That’s why we want to start our journey by designing a general operating model\nfor our application.\nAn operating model is the set of people, processes, and tools that underlies your sys‐\ntem. It shapes all the decision making and work that you do when you build software.\nFor example, an operating model can define the responsibilities of teams. It can also\ndefine governance over decision making and work.\nYou can think of the operating model as the “operating system” for your solution. All\nthe work needed to build microservices happens on top of the team structures, pro‐\ncesses, and boundaries you define. In practice, operating models can have a big scope\nand can be very detailed. But for our build, we’ll reduce the scope and focus on the\nmost important parts of a microservices system—how the teams are designed and\nhow they work together.\nThat’s what we’ll be covering in this chapter: the relationship between teams and\nmicroservices implementations. We’ll introduce a tool called Team Topologies and by\nthe end of the chapter we’ll have a team-based design that we can use as the founda‐\ntion for the rest of our build.\nYou don’t need to actually assemble the people and teams we’ve\ndefined in order to follow along with our “up and running” micro‐\nservices build.\n15",
      "content_length": 1760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "Let’s get started by taking a look at why teams and team design are so important in\nthe first place.\nWhy Teams and People Matter\nThe model we’re using in this book is mostly concerned with technology and tool\ndecisions. But technology alone won’t give you the value you need from a microservi‐\nces system. Technology is important. Good technology choices make it easier for you\nto do things that may have been prohibitively difficult. At its best, technology opens\ndoors and unlocks new opportunities. However, it’s useless on its own.\nYou can have the world’s best tools and platforms, but you’ll fail if you don’t have the\nright culture and organization in which to use them. The goal we’re trying to reach in\nour model is to put good technology in the hands of independent, high-functioning\nteams. So we’ll need to start by considering the types of teams and structure that will\nwork best for the model we’re going to develop.\nIn a microservices system, culture and team design matters. In our research for this\nbook and in our own implementation experiences we’ve learned an important truth:\npeople and process are critical success factors. A microservices implementation is val‐\nuable when it gives you the freedom to make changes easily and quickly. In practice,\nhowever, change is a byproduct of your organization’s decision-making capability. If\nyou can’t make quality decisions quickly, you’ll have a difficult time getting value\nfrom your microservices. It’d be like building a racing car with a very poor engine. No\nmatter how well the car is built, it’s never going to run the way it should.\nThe idea that team design and culture is important isn’t a new one. Mel Conway cap‐\ntured the impact of team structure on system design eloquently in his now-famous\narticle, “How Do Committees Invent?” Mel Conway’s insightful observations\nspawned an even more famous paraphrasing of his thesis, called “Conway’s Law”:\nAny organization that designs a system (defined broadly) will produce a design whose\nstructure is a copy of the organization’s communication structure.\n—Attributed to Fred Brooks\nConway tells us that the output of an organization reflects the way its people and\nteams communicate. For example, consider a microservices team that must consult a\ncentralized team of database experts whenever they need to change a data model.\nChances are that the data model and data implementation will also be centralized in\nthe system that gets produced. The system ends up matching the organization and\ncoordination model.\nThe takeaway from all this is that the people in a microservices system matter. The\nway that they make decisions, do their work, and communicate with each other has a\nbig impact on the system that gets produced. Generally speaking, there are three\n16 \n| \nChapter 2: Designing a Microservices Operating Model",
      "content_length": 2837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "people factors that have the biggest impact on a microservices system: team size, team\nskills, and interteam coordination. Let’s take a closer look at each of them, starting\nwith size.\nTeam Size\nThe “micro” in microservices implies that size matters and smaller is best. To be hon‐\nest, that’s a bit of an oversimplification. But the truth remains: buliding smaller\ndeployable services is an important part of succeeding with microservices. It also\nturns out that the size of the teams building those services matters a lot too.\nIf you have too many people on a team, they’ll need to spend more time communicat‐\ning with each other. That internal coordination will end up slowing the team down,\nresulting in slower delivery of changes. If you have too few people, you won’t have\nenough minds and hands to get the work done. “Rightsizing” teams is an important\npart of your system design. While there isn’t a specific size that works for everyone in\nall situations, a body of experience and studies on team sizes has evolved into\naccepted practice.\nBill Gore, cofounder of the Gore-Tex company, W. L. Gore, limited the size of com‐\npany teams to keep them effective. To make that happen, he instituted a built-in size\nlimit: everyone on a team must have a personal relationship with one another. When\na team gets so big that its members don’t know each other, the unit has grown too\nlarge.\nAnthropologist Robert Dunbar, in his studies of the social behavior of chimpanzees,\nobserved that the group sizes of chimpanzees correlated to their brain size. By extrap‐\nolating these findings to his understanding of the human brain, he established a set of\ngroup sizes for people. The Dunbar number states that we can only comfortably\nmaintain 150 stable relationships, based on the size of our brains. Dunbar also deter‐\nmined that humans could keep about 5 intimate, familial relationships and only\nabout 15 trusted friends.\nPerhaps most famously, Amazon CEO Jeff Bezos gave us the “two pizza rule”. It states\nthat an Amazon team should be small enough that it can be fed with two pizzas.\nAlthough the specific details about the size of the pizzas and the appetite of the team\nmembers are unclear, a two-pizza team is probably going to land somewhere in the 5\nto 15 person range that Dunbar describes and stands a good chance of maintaining\nthe personal relationship heuristic that Gore describes.\nAll of these stories point to a size limit based on the ability for people to communi‐\ncate effectively. Our experiences and our research align with this intuitive concept. To\nkeep the rate of change high, we’ll need to limit the size of the teams in our system. In\nour microservices model, we’re going to keep the size of teams to somewhere between\nfive to eight people.\nWhy Teams and People Matter \n| \n17",
      "content_length": 2798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "Key Decision: Team Size Should Be Limited\nThe teams that perform work in our system should have no more than eight people\neach.\nKeeping the team size down will help us limit the internal interaction needed. But it\nwill have a knock-on effect. Smaller team sizes usually mean more teams. So, we’ll\nneed to be careful in how we design the rest of the system. It’s no good to have small\nteams if they have to spend all their time coordinating with each other. To avoid that,\nwe’ll need to enable independent and autonomous work as much as safely possible.\nAnother side effect of making our teams smaller is that it limits the number of spe‐\ncialists we can have. With less people on the team, we’ll need to make sure we have\nenough talent collectively to deliver a quality output. That’s why we’ll need to con‐\nsider how we populate our teams from a skills perspective.\nTeam Skills\nA team can only be as good as its members. If we want high-performing teams, we’ll\nneed to pay special attention to the way we decide who gets to be on a team. For\nexample, which roles and specializations will our teams need? How talented and\nexperienced should individual team members be? What is the right mix of skills and\nexperience?\nThe truth is that these are difficult questions for us to answer universally. That’s\nbecause people and culture are often the most unique thing about the place where\nyou work. For example, a handful of companies spend a lot of money to have the top\n1% of technology talent from around the world working for them. Another company\nmight mostly hire local talent with a focus on career growth and learning on the job\nfrom a small number of experts. Good team design in these two companies will prob‐\nably look quite different.\nWe want this book to be focused on building a microservices implementation. So, we\nwon’t go very deep into organizational and culture design. The good news is that\nthere is a general principle we can adopt that seems to help microservices implement‐\ners universally. That’s the principle of the cross-functional team.\nIn a cross-functional team, people with different types of expertise (or functions)\nwork together toward the same goal. That expertise can span both technology and\nbusiness domains. For example, a cross-functional team could contain UX designers,\napplication developers, product owners, and business analysts.\n18 \n| \nChapter 2: Designing a Microservices Operating Model",
      "content_length": 2428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "Cross-functional teams have been around for a long time, dating\nback to at least the 1950s at the Northwestern Mutual Life Insur‐\nance Company.\nA big advantage of building a team this way is that you can make better decisions\nfaster. We’ve already established an upper limit on team size, by limiting membership\nto eight people. A “rightsized” team with the right people on board can move at high\nvelocity with authority.\nBut who are the right people? When it came to team size, we had anecdotes, experi‐\nence, and academic studies to draw on. But for team profiles, it’s much more difficult\nto find consistent stories. For example, when we’ve seen a large cloud vendor work on\nmicroservices, they’ve used four to five experts with cross-domain knowledge, cou‐\npled with a single testing expert. Conversely, we’ve seen consulting companies use a\nlarge mix of specialized engineers, product owners, project managers, and testing\nexperts on each team. The talent, experience, and culture of your organization will\ninform the precise mix of people.\nSo, rather than dictate exactly which roles you’ll need on your teams, we’ll make two\ngeneral decisions for our model. First, teams should be cross-functional. Our experi‐\nence shows that microservices work better when teams can make good decisions on\ntheir own. Cross-functional teams enable that. Second, teams should be comprised of\nmembers who directly influence the output. In this way, we’ll pick people who we\nknow can add value to the team. We don’t need observers on the team or people who\nare only tangentially related to the work and decisions that are being made.\nKey Decision: Principles for Team Membership Should Be Defined\nTeams should be cross-functional and consist only of members who can add value to\nthe team’s deliverable, service, or product.\nWith the right size and the right people, we should be able to build effective teams\nthat can get things done. As the number of teams grow, we’ll also need to consider\nhow teams coordinate with each other. That’s the last team property we need to\naddress.\nInterteam Coordination\nBuilding a team with the right size and filling it with the right people will help us cre‐\nate high-performing teams. But it’s the communication among teams, rather than\ninside them, that can really bog down a microservices system. We highlighted the\nproblem of coordination costs in “The Coordination Cost Problem” on page 4. If we\nWhy Teams and People Matter \n| \n19",
      "content_length": 2458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "can reduce the amount of coordination that takes place between teams, our microser‐\nvices teams will be able to deliver changes faster.\nIt would be nice if our microservices teams could act completely autonomously and\nindependently. If teams were free to make their own design, development, testing,\nand deployment decisions, there would be no “organizational friction” to slow things\ndown. In our experience this isn’t a practical method of operation.\nThat’s because coordination and collaboration are important for the success of an\norganization. We might want our microservices teams to act independently, but we\nalso want them to create services that are valuable to customers, users, and the orga‐\nnization. This means communication is required to establish shared goals, communi‐\ncate change requests, deliver feedback, and resolve problems.\nOn top of this, when teams operate completely independently, there’s less opportunity\nto share. Microservices teams working independently can pick the right tools for the\nright job and build highly efficient systems. But that efficiency is localized to the\nteam. Sometimes, that means we lose out on system-level efficiency. For example, if all\nour teams design and build their own cloud-based network architectures, we’ve lost\nan opportunity to do that work once and share it.\nIt’s possible to build an organization that enables efficient team\nindependence and autonomy through self-organization. For exam‐\nple, microservices pioneer Fred George has described a method he\ncalls Programmer Anarchy, in which technology teams have full\nautonomy (and responsibility) to form teams, choose work, and\ndesign their own solutions. But in our experience most enterprise\norganizations would have difficulty pulling this off consistently.\nIf we go too far towards team independence and autonomy, we’ll introduce system-\nlevel inefficiencies and misalignment with organizational goals. If we introduce too\nmuch coordination, we risk bogging the whole system down and losing the benefits\nof highly changeable microservices. The challenge is to strike the right balance\nbetween independent work and coordinated efforts. That takes some experimentation\nand continuous tuning of your team design.\nMost importantly, optimizing team coordination requires an active design effort. One\nof the mistakes we’ve seen practitioners make is to focus solely on the technical archi‐\ntecture. When that happens, the team design forms around the technology that’s been\ncreated. It’s only then that the problems with the coordination model become obvi‐\nous. By that point, it’s often too costly or too difficult to make changes.\n20 \n| \nChapter 2: Designing a Microservices Operating Model",
      "content_length": 2707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "To avoid this problem, we’ll address team coordination and team design as the first\nstep of our system design process. Some people call this an “inverse Conway maneu‐\nver,” because the communication structure we design will end up informing the sys‐\ntem that gets created. Whatever you want to call it, we’ve found that starting with a\nfocus on team design and coordination can really help you succeed with your micro‐\nservices design. In fact, this point is so important that we’ll log it as a decision.\nKey Decision: When to Design Teams and Coordination Models\nTeam and coordination design should start before the design of the system architec‐\nture or microservices. The team and coordination models must continually be upda‐\nted and improved for the life of the system.\nWe’ll cover this in the rest of this chapter. First, we’ll introduce a useful tool for\ndesigning microservices team models called Team Topologies.\nIntroducing Team Topologies\nSince we’re going to start our design work with a focus on teams, we’ll need a way of\ncataloging and communicating our decisions. There are plenty of ways of document‐\ning team designs. For our model, we’ll use a design tool called Team Topologies.\nTeam Topologies is a design approach invented by Matthew Skelton and Manuel Pais.\nWe like using it because it provides a formal language for talking about team design,\nwith a special focus on the way teams work with each other.\nWe won’t be using every aspect of the Team Topologies approach in our design work.\nInstead, we’ll be drawing on three elements: team types, team interaction modes, and\ndiagramming. With these parts, we’ll be able to build a simple, working design for our\nmicroservices teams.\nNext, we’ll look at different parts of the Team Topology approach, starting with the\ntypes of teams we can define.\nTeam Types\nOne of the core concepts of Team Topologies is team types. These are archetypes or\ncategories that describe the basic nature of a team, from the perspective of its com‐\nmunication with the rest of an organization. There are four team types defined in\nTeam Topologies: stream-aligned, enabling, complicated-subsystem, and platform.\nLet’s take a quick look at each of them:\nIntroducing Team Topologies \n| \n21",
      "content_length": 2235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "Stream-Aligned\nA stream-aligned team owns and runs a deliverable piece of work. The key char‐\nacteristic of this team is a continual delivery of something relevant to the busi‐\nness organization. The stream-aligned team embodies Amazon CTO Werner\nVogel’s comment on the responsibilities of Amazon teams: “You build it, you run\nit.” Stream-aligned teams don’t disband after a release. Instead, they continue to\nown and implement a “stream” of changes, improvements, and fixes to their busi‐\nness deliverable. For example, microservices teams are usually stream-aligned as\nthey continually release features to the services they own.\nEnabling\nAn enabling team supports the work of other teams with a consulting engage‐\nment model. These teams are usually composed of specialists and subject matter\nexperts who can bridge gaps in expertise or capability. But they can also help\nindividual teams understand the bigger picture of the organization or industry\nthey are operating in. For example, an enabling architecture team can help\nmicroservices teams understand emerging technical standards and conventions\nin the organization.\nComplicated-Subsystem\nThis type of team works on a domain or on subject matter that is difficult to\nunderstand. Or at least, it’s difficult enough that there is a lack of available\nresources in the organization. Some problem areas don’t scale well and can’t be\nembedded in every team. For example, tuning software for cryptographic secu‐\nrity requires a special kind of expertise and experience. Rather than trying to\nscale that skill across all teams, most organizations create a complicated-\nsubsystem security team who can engage with individual teams as needed.\nPlatform\nLike enabling teams, the platform team provides support to the rest of the orga‐\nnization, with one important difference—platform teams deliver a self-service\nenablement experience to their users. While the enabling and complicated-\nsubsystem teams are limited by the bandwidth of their people, a platform team\ninvests in building supporting tools and processes that can scale easily. This\nrequires more up-front investment and continual maintenance and support. The\nplatform becomes a product, whose users are the rest of the teams in the organi‐\nzation. For example, operations teams can become platform teams when they\noffer build and release tools to development teams for them to use.\nWith an understanding of these four team types, we can start communicating how we\nwant our teams to operate. To really communicate our team design, we will need one\nmore part of the model: the ways in which teams interact with each other, which we’ll\ncover next.\n22 \n| \nChapter 2: Designing a Microservices Operating Model",
      "content_length": 2712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "Interaction Modes\nOur goal in designing teams for the microservices build is to reduce the amount of\ncoordination that needs to happen for work to get done. The Team Topology team\ntypes help us identify the basic characteristics of a team. To really understand how\nand where we can reduce coordination costs, we’ll need to articulate the way our\nteams are coordinating with each other. That’s where the Team Topology interaction\nmodes come in. In their book, Skelton and Pais discuss three interaction modes,\nwhich describe different levels of coordination:\nCollaboration\nThis interaction mode requires both teams to work closely together. Collabora‐\ntion provides opportunities for teams to learn, discover, and innovate. But it\nrequires high levels of coordination from each team and is difficult to scale. For\nexample, a security team might collaborate with a microservices team to develop\na more secure version of their software. The collaborative work might entail\ndesigning, writing, and testing code together.\nFacilitating\nA facilitating interaction is similar to a collaborative one, but it is unidirectional.\nInstead of teams working together to solve a shared problem, one team plays a\nsupport role to help the other team deliver their desired outcome. An example of\na facilitating interaction would be when an infrastructure team helps a microser‐\nvices team understand how to troubleshoot issues with the network architecture\nthey’ve been provided.\nX-as-a-service\nSometimes team collaboration takes on a consumer-provider flavor. In this type\nof interaction, one team provides a service to other teams in the organization\nwith minimal levels of coordination. This usually occurs when a team releases a\nshared process, document, library, API, or platform. X-as-a-service interactions\ntend to scale well because they require less coordination. They are also a natural\nfit for platform teams, but other team types may incorporate this mode as well.\nFor example, an enabling architecture team might document a list of recom‐\nmended software patterns and offer those to all microservices teams in a “pat‐\nterns as a service” model.\nThere’s a lot more to Team Topologies then we’ve outlined here. Taken together, this\ncategorization of team types and interactions gives us a great palette of terms we can\nuse to paint a picture of what our microservices teams should look like, with particu‐\nlar emphasis on when and how much our teams will need to coordinate. In the next\nsection, we’ll use the terms we’ve borrowed from Team Topology to design a micro‐\nservices team model.\nIntroducing Team Topologies \n| \n23",
      "content_length": 2616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "Designing a Microservices Team Topology\nThe Team Topology approach gives us a language for talking about team coordina‐\ntion. What makes it really special, is that it’s a language built for visual representa‐\ntions. In this section, we’re going to create a design for our microservices teams that\ncommunicates the teams we need and how they will work together. When we’re done,\nwe’ll have a diagram that highlights the main points of team coordination and\ninteraction.\nTo create a team design and Team Topology, we’ll follow this step-by-step approach:\n1. Establish a system design team.\n2. Create a microservices team template for future teams.\n3. Define platform teams.\n4. Add enabling and complicated-subsystem teams.\n5. Add key consumer teams.\nAs we go through each of the steps, we’ll be documenting our team design and build‐\ning our Team Topology. For each step we’ll identify one or more teams, create and\npopulate a team design document, and draw the key interactions for that team. Let’s\nget started by focusing on the system design team.\nThere isn’t a single Team Topology that is a good fit for everyone. It\nwould be impossible to account for your organization’s size, people,\nskills, and needs. The topology we’ve created here is a consolidated\nversion of large enterprise-scale implementations that we’ve seen\nwork well.\nEstablish a System Design Team\nA microservices system is a complex system with lots of parts and lots of people\ndoing work. The software that gets built emerges from the collective decision making\nand work of all those people together. In our experience, getting everything to work\ntogether the way you want isn’t easy. That’s why you’ll need to designate a group of\npeople who can shape the vision and behavior of the system. In our model, we’ll call\nthis group the system design team.\n24 \n| \nChapter 2: Designing a Microservices Operating Model",
      "content_length": 1881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "In our model, the system design team has three core reponsibilities:\nDesign team structures\nThe system design team is the first team we’re putting together. It’s also the team\nthat we expect to design the teams that will do the work of building the system.\nThat’s the work we’ll be doing in our subsequent team design steps. In effect,\nwe’re playing the role of the system design team together.\nEstablish standards, incentives, and “guardrails”\nIn addition to forming teams, the system design team should shape the decisions\nthat individual teams can make. This ensures that teams produce results that\nalign with our system goals. One way to do this is by enacting standards that dic‐\ntate what teams can and can’t do. That’s the prescriptive approach we’ve taken for\nmany of the decisions in this book. In practice, too much standardization is diffi‐\ncult to maintain and too restrictive for a healthy system. Good designers will\nintroduce incentives to get more of the behavior they want and “guardrails” that\nact as lighter recommendations and references rather than outright rules.\nContinually improve the system\nFinally, the system design team needs to continually improve all the team\ndesigns, standards, incentives, and guardrails that have been introduced. To do\nthat, they’ll need to establish a way of monitoring or measuring the system as a\nwhole so that they can make changes and introduce improvements.\nIt’s useful to document these team responsibilities so that we can clearly communicate\nwhat each team does. In fact, we should document all of the key properties of our\nteams to make it easier to understand and improve them as the system evolves. At a\nminimum, we should cover the Team Topology type, the size of the team, and the\nresponsibilities we’ve defined earlier.\nLet’s start by deciding on a Team Topology type. After the initial setup of team\ndesigns and standards, we expect the system design team to focus on helping other\nteams build microservices and supporting components. We expect most of their work\nto be consulting based, facilitating delivery teams and helping them navigate the sys‐\ntem. Although the system design team delivers a system, the work we want them to\ndo is characteristic of an enabling team type.\nWe also want the system design team to be small. It should consist of just a few senior\nleaders, architects, and system designers who can quickly make decisions together for\nthe system as a whole. To that end, we’ll limit the size of the team to between three to\nfive people—even less than our general team size that we decided on earlier.\nDesigning a Microservices Team Topology \n| \n25",
      "content_length": 2633,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "Let’s capture these decisions and team properties by creating a lightweight design\ndocument for the system design team. Using your favorite text or document editor,\ncreate a file named system-design-team.md and populate it with the following content:\n# System Design Team\n## Team Type\nEnabling\n## Team Size\n3-5 People\n## Responsibilities\n* Design team structures\n* Establish standards and \"guardrails\"\n* Continually improve the system\nThe nice thing about using a text file for our team documentation is that we can treat\nit like code. Because of this, we can store the documentation in a code repository and\nversion it whenever we need to make changes. Alternatively, you can use a wiki,\ndocument repository, or whatever works best in your company. We’ll leave it to you to\ndecide how you want to manage your team design files. You can find all of the exam‐\nples for our team designs in our GitHub repository.\nAt this point, we’d typically diagram the team visually and map out its interactions\nwith other teams in the system. This is the heart of our team design work and allows\nus to visualize how teams will work together. For example, we can expect the system\ndesign team to use a facilitating interaction model with microservices teams. But\nsince this is the first team we’ve defined, we don’t have anything to interact with. So\nwe’ll leave the diagramming work for later.\nWith the system design team document created, we can move on to documenting and\ndiagramming our microservices teams.\nBuilding a Microservices Team Template\nIn the “up and running” model, every microservice is owned by a team. This single\nteam owns the decisions and work of designing, building, delivering, and maintain‐\ning a microservice. In practice, a single team may own multiple microservices. This is\nfine, and avoids unnecessary growth of teams. The most important constraint is that\nthe responsibility for a microservice is not shared across multiple teams. Microser‐\nvice ownership will be limited to an accountable and responsible team.\n26 \n| \nChapter 2: Designing a Microservices Operating Model",
      "content_length": 2086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "Key Decision: Microservice Ownership\nEach microservice will be owned by a single team, who will design, build, and run it.\nThis team is responsible for the microservice for the lifetime of the service.\nAs your system matures, you’ll end up with lots of microservices. You’ll also likely\nend up with lots of microservices teams. Since we expect to have multiple microservi‐\nces teams operating in our system, we won’t design each of them individually. Instead,\nwe’ll define a microservices team template that can be applied to any new teams we\ncreate. Think of this as creating a cookie cutter that we can use to “punch out” some\nmicroservices teams later on when we need them. Or, if you have a programming\nbackground, you can think of this as defining a “class,” for which we’ll be creating\n“instances” later.\nTo get started, we’ll do the same thing we did for our system design team—define\nsome essential team properties. Just like before, we’ll document the team type, team\nsize, and responsibilities. As we mentioned before, our microservices teams are\nexpected to own one or more microservices independently. That ownership includes\nrunning the service and releasing a continuous stream of improvements, fixes, and\nchanges as needed.\nWith that characteristic, it makes sense to classify the microservices team as stream-\naligned. We’ll also stick to the team-sizing decision we made earlier in this chapter\nand keep the team size between five to eight people. Let’s document all of these prop‐\nerties like we did before. Create a file named microservice-team-template.md and pop‐\nulate it with the following content:\n# Microservices Team Template\n## Team Type\nStream-Aligned\n## Team Size\n5-8 People\n## Responsibilities\n* Designing and developing microservice(s)\n* Testing, building, and delivering the microservice(s)\n* Troubleshooting issues\nDesigning a Microservices Team Topology \n| \n27",
      "content_length": 1894,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "With the template definition documented, we can start diagramming our team inter‐\naction model. To do that, open a drawing or diagramming tool and draw a horizontal\nrectangle as shown in Figure 2-1. We have used yellow for this; each type of team\nshould have its own color.\nFigure 2-1. A stream-aligned microservices team\nIf you don’t have a favorite diagramming tool, diagrams.net and\nLucidchart are good browser-based options that are free to get\nstarted with. Of course, you’re also free to diagram the old-\nfashioned way, with a pen and a napkin!\nIn the previous section we defined our system design team. Now that we have our\nmicroservices team diagrammed, we can add the systems team into the picture. Draw\nthe system design team using a vertical rectangle, as shown in Figure 2-2.\nFigure 2-2. The enabling system design team\nUse a unique color (we have used violet) for the system design team to denote that it’s\nan enabling team. We’ve placed it vertically and to the left of the microservices team\nto show an interaction between the two teams. In this case, we expect the system\ndesign team to facilitate the microservices teams. To keep things simple, we aren’t\ngoing to model the specific details of the interaction mode. Highlighting that the\nteams will need to interact is enough now.\nOur color choices for the team types in this chapter are based on\nthe illustrations shown on the Team Topologies website.\nIn practice, as your system evolves, you’ll need to replace this generic “Microservices\nteam” box with the actual names of your teams and the services they are working on.\n28 \n| \nChapter 2: Designing a Microservices Operating Model",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "Over time, you may also need to capture the interactions that must take place\nbetween your microservices teams. For example, if one microservice needs to invoke\nanother service, chances are there will be some coordination work that is worth\ncapturing.\nWe use a particular color to denote that our microservices team is stream-aligned.\nWe’ll be updating this diagram as we go through the team design steps, so keep your\ndrawing tool handy for later on. You may also want to save the diagram so you don’t\nlose any work.\nNow that we have our first two teams modeled, let’s take a look at the cloud platform\nteam.\nPlatform Teams\nPlatform teams are an important part of a microservices system. Most of the micro‐\nservices work is done by independent, stream-aligned teams. Without support, how‐\never, they’ll need to figure out how to solve a lot of development, testing, and\nimplementation problems on their own. Our facilitating system design team can\nenable some of their decision making, but the microservices teams will still need to\ndeal with the complexities of an entire technology stack and architecture.\nThat’s where platform team types can help. There are a lot of common components in\na microservices system. A platform team can make those common components avail‐\nable for microservices to use “as a service.” The service model improves the scalability\nof platform components, reducing the coordination problems that usually occur\nwhen shared components are centralized.\nIn our model, we’ve decided to instantiate a cloud platform team that offers a net‐\nwork, application, and deployment infrastructure to the rest of the organization as a\nservice. We’ll get into the details of what this offering looks like in Chapter 7, when\nwe dive into infrastructure design. The key point for now is that the teams in our sys‐\ntem will be able to create new environments on demand using the infrastructure serv‐\nices that our platform team provides.\nWith those details understood, we can document our cloud platform team in a file\ncalled cloud-platform-team.md with the following team properties:\n# Cloud Platform Team\n## Team Type\nPlatform\n## Team Size\n5-8 People\n## Responsibilities\n* Design and develop a network infrastructure\nDesigning a Microservices Team Topology \n| \n29",
      "content_length": 2275,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "* Design and develop an application infrastructure\n* Provide tools for building a new environment\n* Update network and application infrastructure when required\nNotice that one of the responsibilities of our cloud platform team is to update the\ninfrastructure that is being offered. This is a key part of a platform team’s responsibil‐\nity. They need to treat the users of the platform as if they are customers. In this rela‐\ntionship, the platform offering needs to be continually improved to meet their\ncustomer’s requirements and expectations.\nAs we’ve done before, we’ll add the cloud platform team to the Team Topology dia‐\ngram that we’ve been working on. But this time we need to model a platform team. To\ndo that, draw a horizontal rectangle (again, using a unique color; we’ve used light\nblue) below the microservices teams and connected to the system design team, as\nshown in Figure 2-3.\nFigure 2-3. The cloud platform team offering a service\nNote that we’ve also drawn a small black arrow between the platform and microservi‐\nces teams. This is to show that the platform team is implementing the x-as-a-service\nmodel for its interaction with the microservices team. Our diagram also shows that\nthe system design team will be enabling the work of the platform team. This will\nensure that the platform fits the goals and vision for the overall system.\nFor our “up and running” model, we’ve only defined a single instance of a platform\nteam. But, in practice, you’ll probably need to roll out multiple platform teams to\nkeep the teams to a manageable size. When that happens, you’ll also need to be con‐\nsider how multiple platform teams will coordinate together to offer services to the\nrest of the organization.\n30 \n| \nChapter 2: Designing a Microservices Operating Model",
      "content_length": 1780,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "Enabling and Complicated-Subsystem Teams\nWith the three teams we’ve designed, we have enough people in place to be able to\ndeliver a microservices system. Beyond these core capabilities, there may be addi‐\ntional capabilities that we want a team to own. That may be because there is an\nimportant set of skills that we want to provide enablement for. Or, because there is a\ncomplicated system feature that requires a dedicated team.\nIn our microservices model, we’ve decided to create a specialized release team. This\nadditional team owns the responsibility of releasing (or deploying) microservices into\na production-like environment. While a microservices team could deploy its own\nservices directly into a production environment, in our experience this isn’t always\nwhat happens.\nThat’s because in most organizations there is usually an additional testing and accept‐\nance check that needs to happen before a service can go live. Instead of deploying\ndirectly into production, microservices teams deliver a built and tested container.\nThat container is then automatically deployed by a release team who coordinates the\nwork of tests, approvals, and deployment of the change.\nThe release team embodies the complicated-subsystem team type. It contains special‐\nist knowledge of the release, approval, and deployment process and collaborates with\nstream-aligned teams to make that work happen. To document the design of our\nrelease team, create a file called release-team.md with the following properties:\n# Release Team\n## Team Type\nComplicated-Subsystem\n## Team Size\n5-8 People\n## Responsibilities\n* Releasing microservices to production\n* Coordinating approvals for releases\nNext, we’ll add the release team to the developing picture of our Team Topology.\nComplicated-subsystem teams are modeled with yet another specific color.\nDesigning a Microservices Team Topology \n| \n31",
      "content_length": 1877,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "So, open your Team Topology diagram and add a square (we’ve colored it red) near\nthe end of the microservices team’s box, as shown in Figure 2-4.\nFigure 2-4. The release team\nAs we can see from our emerging topology, one of the trade-offs to the release team\napproach is the coordination costs it brings. At scale, this can become a big problem.\nFor example, if you want to perform daily releases across multiple microservices, the\nrelease team will struggle to coordinate all of that activity. If you find yourself in that\nsituation, you’ll need to change the team design and shift the responsibilities for\ndeployment to the individual microservices teams.\nThe release team is the final team at the core of our microservices model. But to fin‐\nish our design, we need to consider the teams that will have to use our microservices.\nWe’ll cover that next.\nConsumer Teams\nMicroservices are only useful if they are used. So, it’s worthwhile identifying the con‐\nsumers of our microservices and how they’ll interact with our system teams. In some\narchitectures, that could include mobile application development teams, web devel‐\nopment teams, or even third-party organizations. In our model, the main consumer\nof our microservices system is the API team.\nThe API team is responsible for exposing our microservices to other development\nteams as an application programming interface (API). For example, a mobile applica‐\ntion development team would interact with the API released by this team and never\ncall our microservices directly.\nWe’ll get into the details of the API and the architecture later in the book. For now, it’s\nworth detailing the properties of our API team and its responsibilities. We can do that\nby creating a file named api-team.md and populating it as follows:\n32 \n| \nChapter 2: Designing a Microservices Operating Model",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "# API Team\n## Team Type\nStream-Aligned\n## Team Size\n5-8 People\n## Responsibilities\n* Design, develop, and maintain APIs at the boundary of the system\n* Connect API to internal microservices\nJust like the microservices team, the API team is a stream-aligned team. That’s\nbecause it needs to continually deliver changes to the API that reflect business needs\nand consumer demands. A special nuance of the API team is that, because the API\nneeds to call microservices to function, it is dependent on the microservices team.\nWe can model these interaction properties in our Team Topology model by adding\nanother rectangle at the top of our diagram to represent the API team. It should be of\nthe same color as the microservices team (we’ve used yellow), as it is also a stream-\naligned team. To reflect the dependency between our microservices and API teams,\nwe’ll again use a black arrow to show an x-as-a-service engagement model. This indi‐\ncates that the microservices team will need to make sure their services are invocable\nand usable in a self-service fashion.\nWhen you’re finished, the diagram should look something like Figure 2-5.\nFigure 2-5. The finished Team Topology with the API team\nWith this final team defined an in the picture, our topology looks a lot like the fin‐\nished product we showed you at the start of this section. With the topology defined,\nwe can see where the main coordination points are where the work is being done.\nOverall, our model enables a fairly independent, autonomous way of working. How‐\never, we’ll need to invest some time and effort into building a cloud platform as a ser‐\nvice to make our model work.\nWith our basic Team Topology defined, we can see how this work ties back to the\ngoal of our chapter—building an operating model.\nDesigning a Microservices Team Topology \n| \n33",
      "content_length": 1819,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "Summary\nTaken together, the decisions, team definitions, and topologies we’ve just created\nform our microservices operating model. With it, we’ve defined the teams that need\nto be created, their characteristics and responsibilities, and the way we expect our\nteams to work together. It’s an important design step and will influence the rest of our\nmicroservices work. In fact, every decision we make from this point on will be heavily\ninfluenced by the operating model we’ve just established.\nIn truth, we didn’t go very deep with our operating model design. In practice, it’s\nworthwhile drawing out more than one Team Topology diagram to reflect different\ntypes of interaction modes. For example, the toubleshooting problems with our sys‐\ntem would likely require a different engagement model from the one we’ve shown.\nSimilarly, we haven’t diagrammed the interactions required to change the deliverables\nthat the cloud platform, system design, and release teams provide.\nIn addition to the initial design, the operating model design should be continually\nimproved. One of the nice things about capturing our team definitions and topolo‐\ngies as documents is that we can treat them like code. So we can version and manage\nchanges as the system evolves. You may even want to add additional design assets to\nyour collection. For example, you could define service-level agreements for platform\nteams and skill inventories for your stream-aligned teams.\nUltimately, our goal in this chapter was to create a foundation for the rest of our\ndesign and development work. Our lightweight approach to the Team Topology and\nteam designs does just that. With our operating model in hand, we can move on to\ndesigning the actual microservices. That’s what we’ll cover in Chapter 3.\n34 \n| \nChapter 2: Designing a Microservices Operating Model",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "CHAPTER 3\nDesigning Microservices:\nThe SEED(S) Process\nIf you recall, in Chapter 1, we stated that the main benefit of adopting microservices\narchitecture is the ability to increase development speed without compromising\nsafety of a system, at scale. This is an extremely important benefit for organizations\ntackling significantly complex problems. Note though that this certainly happens as a\nresult of a conscious design, not by accident. In all but the simplest cases, it is impos‐\nsible to iterate toward a successful microservices architecture without an effective and\nexplicit, end-to-end system design.\nIn this chapter, we introduce an evolutionary process for designing microservices.\nThis methodology was first formulated by one of the authors at a healthcare startup\nhe cofounded, and later successfully implemented on numerous projects at other\ncompanies. The flexible approach has proven equally as effective for smaller organi‐\nzations tackling complex problems; for eaxmple, a pioneering startup revolutionizing\nthe vast healthcare industry and a large organization with thousands of software engi‐\nneers across hundreds of teams.\nKey Decision: Use a Standard Process for Service Design\nUse a standard, repeatable process to achieve consistently high-quality, customer-\ncentric design for the services in your system.\nThe microservices design system described in this chapter is a top-down, multistep\nmethodology, and a collection of reusable processes, where each later step evolves\nfrom a previous one. Due to its evolutionary nature, we call the system Seven Essen‐\ntial Evolutions of Design for Services or SEED(S). We find the tongue-in-cheek name\n35",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "fitting, given that the analyses performed with this methodology often prove to be the\nessential seeds from which a beautiful, complex microservices system emerges. Just as\na beautiful, flourishing garden starts with planting of some key seeds, the SEED(S)\nanalysis and design process is an essential first step of your microservices implemen‐\ntation that facilitates the later, coding part.\nIntroducing the Seven Essential Evolutions of Design for\nServices: The SEED(S) Method\nAs James Lewis and Martin Fowler point out in their seminal article about microser‐\nvices, one of the main traits of the microservices architecture is componentization of\na system via services. By “services,” they mean software components that are inde‐\npendently deployable and accessible over standard network protocols, such as a web\nservice request or a remote procedure call. By exposing system components as serv‐\nices, among other things, we commit to defining explicit public interfaces for them.\nIncreasing the flexibility and usability of these interfaces through good design can\nhave a profound impact on the robustness of the system’s architecture and on devel‐\noper productivity.\nThe SEED(S) process provides a repeatable, reliable, and battle-tested methodology\nfor designing service interfaces that are user-friendly and robust.\nIt should also be noted that, as a generic approach, the SEED(S) methodology is use‐\nful beyond just microservices and can be effective in the design of any number of ser‐\nvice types, including RESTful and GraphQL APIs created for frontend UIs. This wide\nrange of applicability should not be surprising. After all, from a technical perspective,\na microservice is also a kind of API, just developed with a specific type of boundaries\nin mind—those that minimize coordination needs.\nWithout further ado, the seven steps of the SEED(S) process are:\n1. Identifying actors\n2. Identifying jobs that actors have to do\n3. Discovering interaction patterns with sequence diagrams\n4. Deriving high-level actions and queries based on jobs to be done (JTBDs) and\nthe interaction patterns\n5. Describing each query and action as a specification, with an open standard (such\nas the OpenAPI Specification [OAS] or GraphQL schemas)\n6. Getting feedback on the API specification\n7. Implementing microservices\n36 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 2373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "Let’s explore each of these steps in greater detail and see how we can master using\nthem for service design.\nIdentifying Actors\nIn addition to being an evolutionary methodology, SEED(S) takes a distinctly\ncustomer-centric approach, viewing as products the services it is used to design. By\nnow the “APIs are products” mantra is not particularly novel; we have been shouting\nit from all possible mountaintops for years. The good thing is that a product-oriented\nperspective on APIs and services allows us to reuse a wealth of techniques from the\nbusiness world, where it is nothing new; in fact, the science and art of product man‐\nagement significantly predates that of APIs and even the internet itself. Many people\ntrack product management as a field back to the 1930s with Procter & Gamble and\nNeil H. McElroy’s attempts to improve the sales of P&G’s Camay brand of soap. In the\nensuing decades product management has evolved significantly, and there are a lot of\nlessons learned that we can reuse in the much more nascent API/services manage‐\nment space. If APIs are products, we should be able to use similar techniques to\ndesign APIs as what we use in product management.\nWhen designing a product, and consequently an API or a service, we have to under‐\nstand the “customer”; who is the service designed for? Typically, in the API and ser‐\nvice management space, we don’t call these personas “customers” but rather the less\ncommerce-oriented denomination of “actor,” removing any accidental, unintended\nconnotation of a financial transaction or interest being present between the service\nconsumer and a publisher.\nUsage of “actors” in the first modeling step of SEED(S) methodology is inspired by\nthe interaction design’s heritage of using “user personas” for a similar need. The\nnotion of personas, as an interaction design tool, was introduced by Alan Cooper in\nhis 1998 book, The Inmates Are Running the Asylum (Sams Publishing), and has\ngained significant adoption since. To be completely transparent, at this point per‐\nsonas have also received their share of criticism (which methodology has not?), and\nsome product teams passionately advocate using real user data instead. Discussing\nthe pros and cons of personas in product management is far, far beyond the scope of\nthis book. Actors are inspired by, but are not identical to, user personas. The purpose\nof actors is to aid in the modeling exercise at the stage in the design process when\nactual user data is typically limited.\nThe main motivation for starting modeling with the definition of actors is to aid in\nscoping and prioritization. Typical plagues of API and service design in our industry\nare overabstraction and lack of clarity regarding user needs. Too many APIs are sim‐\nply exposures of some database tables over HTTP or an attempt to provide direct net‐\nworked access into application internals, via remote procedure calls (RPCs). Such\napproaches often struggle in delivering for customer needs and achieving business\nIdentifying Actors \n| \n37",
      "content_length": 3023,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "goals. It should not be surprising. If we don’t even ask, “Who will be using this API?”\nand “What are their needs?” how can we possibly design solutions that solve for their\nneeds? And yet too many APIs and services are designed exactly this way: using a ser‐\nvice publisher’s goals, rather than that of the consumer. SEED(S) addresses this\nupside-down problem from the very first step, by identifying the actors first.\nKey Decision: Scope Service Design Using Key Actors\nStart service design by identifying key actors in your domain, to achieve customer-\ncentric scoping of the capabilities represented by the services.\nThere are several fundamental rules for identifying the right set of actors for your\ngoals:\n1. Much like with Cooper’s user personas, each actor must be specific, more so than\nprecise. By this we mean that identifying the boundaries of what key traits differ‐\nentiate various actors of our design is more important than identifying an excru‐\nciating level of detail for who the actors are. We ought to always remember that\nwe are in the process of modeling and so any modeling exercise is by definition\nimprecise: it’s not that we cannot capture the details, rather that we don’t care\nabout every single detail and are trying to capture the prioritized view of the real‐\nity relevant to us.\n2. Overlapping or too-broad actor definitions are usually red flags. Actors also must\nbe defined in context. Having a company-wide “portfolio” of actors that are\nreused for each application design is more than an indication of trouble—it’s an\n“all alarms on, call 911” sure sign that the process has derailed and has been\ncompromised.\n3. As models, actor definitions first and foremost represent the needs, pain points,\nand behaviors inherent to each actor archetype. These needs and behaviors that\ndistinguish one actor type from another are relevant, and there should be very\nlimited overlap.\n4. Less is more—you should use as few distinct actors as possible to describe your\nproblem area, but no fewer than necessary. In most cases, if you have more than\nfive actors for a service, it may be either an indication of prioritization gone awry,\nor service boundaries that are too broad.\n38 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 2261,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "Example Actors in Our Sample Project\nFollowing are some of the possible actors relevant to the sample project we intro‐\nduced in Chapter 1; an airline’s online reservation system, or, more specifically, its\nflight reservation subsystem:\nFrequent flyer\nEmma travels for work, has elite loyalty status with the airline, manages her\ntravel through her work’s reservation system, and uses a number of connected\napps to stay on top of her busy schedule. Due to her loyalty status, she is eligible\nfor many perks. Often planning trips on short notice, when traveling with family,\nshe typically uses loyalty miles.\nFamily vacationer\nRiley and their spouse are mostly traveling for vacations with their kid(s). They\nusually plan trips well in advance, and travel infrequently.\nAirline customer service agent\nSean is an experienced customer service agent assisting travelers with booking,\nrebooking, and resolving issues during travel and after through phone and online\nchat.\nOnce we have identified actors for our design effort, we can analyze the jobs that they\nhave to do using our system. Let’s explore what we mean by this in the next section.\nIdentifying Jobs That Actors Have to Do\nOnce we identify a target class of customers (actors, in our case), we need to spend a\nsignificant amount of our time understanding the jobs they have to get done, and\nonly then create a solution that best addresses their needs. This is a critical point\noften misunderstood or ignored in the design of services and APIs, so let us try to\nexplain the rationale behind its importance.\nAny effective API or service design methodology, including SEED(S), is based on a\nfundamental premise we mentioned earlier: that APIs and microservices are types of\nproducts, and in their design we can successfully employ the rich product manage‐\nment toolset that has been developed over many decades. We already applied one\nsuch tool to our modeling process: the identification of actors, in the manner of user\npersonas from interaction design. In this second step we will dive even deeper into\nproduct design, so it may be worth reiterating: why do we believe APIs are products?\nAfter all, a technical capability that is exposed over a network, using standard proto‐\ncols, i.e., what we call an “API,” doesn’t necessarily have an obvious resemblance to\nhand soaps, winter jackets, smartphones, and other physical products that we are\nmore accustomed to.\nIdentifying Jobs That Actors Have to Do \n| \n39",
      "content_length": 2466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "1 Quoted in Clayton M. Christensen et al., “What Customers Want from Your Products,” Harvard Business\nSchool, 2016, https://oreil.ly/NKolz.\nWell, what is a general definition of a product, anyway? There is no one, true defini‐\ntion that we are aware of, so we might as well use the one Wikipedia references:\nWe define a product as anything that can be offered to a market for attention, acquisi‐\ntion, use or consumption that might satisfy a want or need. Products include more\nthan just tangible objects, such as cars, computers or mobile phones. Broadly defined,\n“products” also include services, events, persons, places, organizations and ideas, or\nmixes of these.\n—Kotler et al., Principles of Marketing, 7th edition (Pearson)\nServices, whether web APIs or microservices, do satisfy this definition: producers\noffer services to their respective consumer(s), they satisfy the needs of their consum‐\ners, and this supply/demand can create a “market.”\nConsumers of APIs are typically frontend (web, mobile) or third-party (partner)\napplications, while consumers of microservices are various parts of the system itself,\nbut that’s a distinction largely irrelevant to their design process. We will dive into\ndefining differences between APIs and microservices later in this chapter.\nSo if APIs and microservices are products, how do we create better ones? The identi‐\nfication of actors is the first step, but what comes next? They must solve a customer’s\nproblem. Alas, the unfortunate reality is that too many products are designed from\nthe perspective of a solution provider obsessing about what they have to offer rather\nthan concentrating on the problems customers need to solve. Probably the most suc‐\ncinct explanation of this problem comes from the famous words of Harvard Business\nSchool marketing professor Theodore Levitt: “People don’t want to buy a quarter-\ninch drill. They want a quarter-inch hole!”1 Indeed, if you are a product company\nproducing drills, you need to realize that the real job customers are trying to get done\nmay be hanging a picture on their walls, not shopping for the most perfect general-\npurpose drill. If you fail to realize this, continuing your pursuit of perfecting a drill,\nyou will eventually be outmaneuvered by an inventor who comes up with a simpler,\nalternative solution to getting quarter-inch holes in customer walls. It may be a chem‐\nical reaction of sorts or something else—we wouldn’t know—but it will happen.\nIf you look at the history of technological advancement, it’s the problems that are\ntimeless; solutions change and evolve all the time. Case in point—nobody uses mag‐\nnetic tapes or floppy disks to save data anymore, but the job of needing to save and\ntransport data has not gone anywhere, even if it is all cloud-based now. Innovators\nmust concentrate more on solving problems, and less on perfecting the tools that are\ntypically transient.\n40 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 2967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "Harvard Business School professor Clayton Christensen names this observation the\n“theory of jobs to be done,” explaining that:\n[Customers] often buy things because they find themselves with a problem they would\nlike to solve.\n—Clayton Christensen, The Innovator’s Solution (Harvard Business Review Press)\nIn the Harvard Business School article quoted previously, “What Customers Want\nfrom Your Products,” Christensen and his coauthors further explain that product\ndesigns are successful and customers find them desirable when “the job, not the cus‐\ntomer, is the fundamental unit of analysis.”\nKey Decision: Use Jobs as the Unit of Analysis\nUse jobs that key actors have to get done, in your domain, as the unit of analysis for\ncollecting requirements.\nUsing Job Story Format to Capture JTBDs\nFor each of the actors we identify, we need to discover top JTBDs for that actor. For\nthe sake of uniformity, as well as to make sure key data points are well-documented,\nwe capture JTBDs in a standard format. The SEED(S) process uses the Job Stories for‐\nmat as defined by Paul Adams: “when <a circumstance>, I want to <motivation>, so I\ncan <goal>” (see Figure 3-1).\nFigure 3-1. Structure of a Job Story format\nA Job Story centers around circumstances, the actor’s motivations for a job to be\ndone, and the goal that they are trying to achieve.\nPlease note that in Adams’s original format, Job Stories are written\nin first person. In SEED(S), as you will see further in this chapter,\nwe prefer to write Job Stories in third person, highlighting who the\nactual actor for the Job Story is.\nIf you are familiar with User Stories from Scrum or other Agile methodologies, you\nmay have noticed that the Job Story looks almost identical. However, as Alan Klement\nexplains in his blog post, “Replacing the User Story with the Job Story”, there are cru‐\ncial differences between the two. User Stories revolve around a user persona; they\nIdentifying Jobs That Actors Have to Do \n| \n41",
      "content_length": 1969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "start with “as a <persona>,” while Job Stories disregard the persona and instead\nemphasize the circumstance.\nThis is important and aligned with Christensen’s “the job, not the customer, is the\nfundamental unit of analysis.” It is also spot-on, because in the context of describing a\nspecific job, persona does not matter anymore. If I need to hang a painting on a wall,\nit doesn’t really matter whether I am a licensed contractor or a novice homeowner, I\nwill need a quarter-inch hole in the wall (or several). It is the context, the circum‐\nstance in which we have a motivation to achieve a goal that matters, not who we are.\nLong story short, we identify actors to scope the list of jobs, but at the point of\ndescribing each job for that actor, we need to identify circumstances and not just\nrepeat 10 times “as a frequent flyer…”\nKey Decision: Use the Standard Job Story Format\nUse a standard format for capturing JTBDs (known as Job Story) to uniformly cap‐\nture circumstances, motivations, and goals for all your jobs.\nExample JTBDs in Our Sample Project\nLet’s pick some JTBDs for a family vacationer actor:\n1. When Riley is planning a flight for their family vacation, they want to be able to\nfilter available flights by multiple criteria, including: four adjacent seats available\non the flight, the number of connections, connections that go through airports\nthat have facilities friendly to young children, etc., so that their family can fly with\nmaximum comfort.\n2. When Riley is planning a quick, unplanned family getaway for a long weekend,\nthey want to get suggestions for interesting available trips that are affordable and\na short flight so they can have a list of choices they can consider.\nAnd now, let’s look at some jobs for a frequent flyer actor:\n1. When Emma’s plans change and she is unable to travel on a previously booked\nflight, she wants to easily reschedule her flight, so she can get a flight that works\nfor her new plans.\n2. When Emma prefers an available seat other than the one she has been currently\nassigned, she wants to select the alternative seat, so she can enjoy her flight more.\n42 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 2180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "Finally, this is what JTBDs may look like for a customer service agent:\n1. When a customer calls Sean, he wants to have a servicing ticket open pre-filled\nwith customer information, so he can start tracking the progress towards the res‐\nolution of the customer need.\n2. When a customer is asking Sean to find them a convenient flight for their trip, he\nwants to be able to find a fitting flight using a flexible set of filtering criteria, so he\ncan meet the customer need and book a flight.\nWhen possible, it is always a good idea to derive the Job Stories from user research.\nThe simple, nontechnical, and consistent format is very helpful for capturing the\nresearch in a consistent way.\nJob Stories provide a great format for conversations with subject matter experts and\nactual customers, but they are not convenient for deriving actual technical require‐\nments. Rather, we need to translate them into a more developer-friendly format,\nwhich is what the next few sections of the SEED(S) process are all about.\nDiscovering Interaction Patterns with Sequence Diagrams\nJob Stories are typically written by product managers from the business-value per‐\nspective and rarely correspond to our target services in any direct way. To proceed\nwith a good design, we need to understand the service interaction patterns of our\nsubdomain, i.e., the one that these services belong to. For complex interactions, a lin‐\near list of Job Stories will not be able to sufficiently support the design effort. Instead,\nyou will want to draw an interaction diagram, explaining the sequence of events\nwithin your model.\nIn the spirit of reusing existing, familiar standards, SEED(S) recommends employing\nUnified Modeling Language (UML) sequence diagrams for this task. You can use any\nother diagramming approach to express your model, since the whole purpose here is\nto communicate the intent and the model. However, if you do wish to use UML\nsequence diagrams, then we highly recommend using one of the Markdown-based\ndiagramming formats, such as PlantUML.\nDiscovering Interaction Patterns with Sequence Diagrams \n| \n43",
      "content_length": 2099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "We recommend this kind of approach because modeling in a microservices team is a\nteam activity. Using a text-based format instead of a graphical file will allow team\nmembers to:\n• Keep modeling separate from everyone’s personal choice of editor. PlantUML\nand other similar formats can be edited in many different editors. As an example,\nPlantUML is supported in Atlassian’s Confluence, a knowledge-management\nsoftware used by many software teams. There are also free online editors you can \nuse, such as LiveUML and PlantText.\n• Easily and effectively version-control sources of the diagrams. Text files are easy\nto diff, merge, and review in pull requests, none of which would be true for a\nbinary graphic file.\n• Conveniently integrate modeling into the release process. The diagrams become\ncode and anything you can do with the code, you can now do with your diagrams\nas well; if you also version-control them in a system like Git, for example.\nKey Decision: Use PlantUML Sequence Diagrams\nto Discover Interaction Patterns\nTo discover interaction patterns in SEED(S) methodology, we choose to use UML\nsequence diagrams expressed in a textual (Markdown) format such as PlantUML.\nAt this point in our design, we are in the phase of coming up with a technical model\nfor the requirements gathered in the physical world. The Job Stories and actors repre‐\nsent the requirements of the physical world. They do not generally map to technical\ninteractions one-to-one. As such, in your interaction model the events do not neces‐\nsarily have to occur between the actors described in the first step of the SEED(S) pro‐\ncess. Neither do they have to correspond to the jobs directly. Rather, your interaction\ndiagrams may go a level deeper and show how the user-centric requirements translate\ninto interactions between services at a technical level.\nFor instance, a very simple diagram describing interactions related to the JTBDs we\nalready identified earlier in this chapter, may read something like the following:\n@startuml\nactor Agent\nparticipant \"Agent Servicing\" as AS\nparticipant \"Reservations API\" as rAPI\nparticipant \"reservationCRUD\" as rCRUD\nAS -> rAPI: checkRes(reservationId)\nrAPI -> rCRUD: reserve(data)\n44 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 2269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "2 Bertrand Meyer, Object-Oriented Software Construction, 2nd ed. (New York: Prentice Hall, 2000).\nrAPI -> rCRUD: cancel(reservationId)\n@enduml\nIn LiveUML, this would render as in Figure 3-2.\nFigure 3-2. A rendered PlantUML of the sample UML sequence\nHere, Agent Servicing is a user interface (web or mobile app) that agents can use\ndirectly, Reservations API is a REST or GraphQL API that the app invokes, and reser‐\nvationCRUD is one of the microservices that fuels said API.\nOnce we have the sequence diagrams of the interactions, we can capture the technical\nrequirements for a microservice, or an API, in the form of a set of actions and queries\ndescribed using a standard syntax. Let’s explore in the next section what those are and\nhow they look.\nDeriving Actions and Queries from JTBDs\nJob Stories provide a great format for having fluid conversations with subject matter\nexperts and acquiring insights into customers’ needs. They may, however, be trouble‐\nsome starting points for actually designing API specifications. Once we understand\nservice interaction patterns and have had a chance to visualize those, we can trans‐\nform jobs into more technically oriented interface contracts and greatly simplify our\ndesign process. Following Bertrand Meyer’s command query separation (CQS) prin‐\nciple,2 in SEED(S) we model a system’s interface contracts as collections of two dis‐\ntinct types of interactions: the actions (“commands” in CQS) and the queries.\nDeriving Actions and Queries from JTBDs \n| \n45",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "Key Decision: Separate Service Endpoints into Commands and Queries\nUse the CQS principle to model the action side of the services separate from the\nquery side, and document each with their own standard format.\nIn SEED(S), queries are lookups with defined inputs and outputs. They should be\nclearly formulated contracts between a client and a server: what input a client sends\nand what response they expect. They are distinctly different from actions, in that\nqueries do not modify the system state (they “have no side effects”).\nActions, in contrast, are requests that cause some sort of state modification—they not\nonly do have side effects, but their whole purpose is to cause side effects. Much like\nqueries, actions also have well-defined contracts—for inputs, expected outcomes, and\nexpected responses.\nSimilar to Job Stories, we recommend using a standard format for capturing queries\nand actions. The template for queries looks something like this:\n• An expressive description of a query\n— Input: list of input variables\n— Response: list of output data elements\nLikewise, the standardized format for actions would look like the following:\n• An expressive description of an action\n— Input: list of input variables\n— Expected outcome: description of the induced side effect\n— Response (optional): list of data elements in the response (if any)\nPlease note that Job Stories do not always produce exactly one query or action. A Job\nStory can be translated into multiple queries and actions, and a resulting query or\naction may combine multiple Job Stories as its source. SEED(S) is a process of model‐\ning, design, and discovery, not a robotic process that is ripe for removal of the human\njudgment factor.\n46 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "Example Queries and Actions for Our Sample Project\nLet’s see some examples of our existing Job Stories translated into a bunch of queries\nand actions.\nQueries\nOne of our Job Stories described a family vacationer actor (Riley) who wants to find a\nflight that matches the travel comfort requirements of their family, by indicating\ndetailed preferences such as: number of adjacent seats, maximum number of connec‐\ntions, etc. To satisfy the needs of such a job, we need a query contract that allows indi‐\ncation of all such preferences as inputs to the search query. Therefore, our query\ndefinition may look like the following:\nQuery 1: Flight Search\n• Input: departure_date, return_date, origin_airport, destination_airport,\nnumber_of_passengers, \nbaby_friendly_connections, \nadjacent_seats,\nmax_connections, \nminimum_connection_time, \nmax_connection_time,\norder_criteria [object], customer_id (optional; to check loyalty privileges)\n• Response: list of flights satisfying the criteria\nAnother one of our Job Stories described a circumstance in which a frequent travel‐\ner’s plans suddenly change, and they are unable to travel on a previously scheduled\ndate/flight. This actor needs to reschedule their existing booking. To achieve this task,\nwe can imagine that at minimum we will need to know:\n• The unique reservation identifier of the previous booking so that we can grab\norigin and destination airports, as well as any other preferences, so that we can\nautomatically set those for the new search without asking the traveler to re-enter\nthem\n• A new departure date and return date that works for the traveler\nOnce we run the search, we will need to receive a list of flights that matches the input\ncriteria for the new dates, so that we can present it to the customer and let them\nchoose which flight they would like to rebook their travel ton.\nBased on this analysis, we can conclude that the “rebooking” query specification\ncould look like the following:\nQuery 2: Lookup of Alternative Flights for a Date Change\n• Input: reservation_id, new_departure_date, new_return_date\n• Response: list of alternative flights\nDeriving Actions and Queries from JTBDs \n| \n47",
      "content_length": 2164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "Actions\nUsing an analysis similar to the one we used for deriving queries from Job Stories,\nyou can produce actions that are required for rebooking and seat change jobs:\nTravel Rebooking\n• Input: original_reservation_id, new_flight_id, seat_ids[]\n• Expected outcome: new flight booked or error returned; if new flight is success‐\nfully booked, old one is canceled\n• Response: success code or a detailed error object\nSeat Change\n• Input: reservation_id, customer_id, requested_seat_ids[]\n• Expected outcome: new seat reserved if the seat is available and the traveler is\nqualified; old seat canceled if the new seat ends up being successfully reserved\n• Response: success code, or a detailed error object\nIn some sophisticated cases, you may find that the actions and queries approach of\ndefining interface contracts may not be sufficient. In these cases, to capture the more\ncomplex requirements, we highly recommend using Matt McLarty’s Microservice\nDesign Canvas. The design canvas and “actions and queries analysis” are substitutable\ntechniques in the same phase of the SEED(S) process. The canvas is a more powerful\ntool that we do not cover in this book, but it is well worth getting acquainted with.\nOnce we have a set of actions and queries, or a Microservice Design Canvas, we can\ntranslate those into a formal interface specification.\nDescribing Each Query and Action as a Specification with\nan Open Standard\nAs a general rule, it is important to formally describe the interface contract of an API\nor a microservice before we start implementing it in code. Such codified contracts\nserve as a mutually agreed-upon understanding between a service producer and con‐\nsumers, or API client developers. The contracts are also easily convertible into user-\nfriendly documentation and interactive playgrounds. Contracts implemented using\nopen standards such as the Open API Spec and GraphQL are widely supported by a\nrich set of tooling that allows easy rendering of documentation, streamlined creation\nof developer portals, etc.\nIn this section we will take the definition of an action that we described in the previ‐\nous SEED(S) phase and design a RESTful endpoint for it, using the Open API Specifi‐\ncation (OAS).\n48 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 2280,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "The OAS describes RESTful APIs in a standard, tech stack–agnostic manner. It is gov‐\nerned by OpenAPI Initiative, a Linux Foundation Collaborative Project. At the time\nof writing, the latest version of OAS is version 3.0.2.\nMicroservices interconnections do not have to be RESTful APIs. Other popular\nchoices include GraphQL, gRPC, and asynchronous event communications. At the\ntime of writing, using JSON-, ProtoBuf-, or Avro-encoded messages on Kafka\nStreams seems to be a popular choice. It does not matter which communication style\nyou choose; any one of them will assume exchange of messages and the format of\nthose messages should be well-documented and part of the exchange “contract.” For\neach of those styles you can apply the SEED(S) methodology in a way appropriate for\nthe particular style. Since RESTful APIs are probably easiest and still the most ubiqui‐\ntous, we demonstrate the approach using a RESTful design, but the methodology\nworks with others, as well.\nYou can use any tooling to edit and author your OASs. If you are looking for sugges‐\ntions, however, an open source setup that is available on most platforms, and seems\nto work well is the VS Code editor with the Open API Designer plug-in. Once you\nhave the plug-in installed and a descriptor YAML file open inside the active tab, press\nCTRL+ALT+P on Windows or CMD+ALT+P on macOS and choose the appropriate\npreview command to see the rendering of the specification, as shown in Figure 3-3.\nFigure 3-3. Selecting OAS Preview in VS Code\nExample OAS for an Action in Our Sample Project\nA simple version of the OAS for the rebooking action we described earlier in this\nchapter may look something like the following:\nopenapi: 3.0.0\ninfo:\n  title: Airline Reservations Management API\n  description: |\n    API for Airline Management System\n  version: 1.0.1\nservers:\n  - url: http://api.example.com/v1\n    description: Production Server\npaths:\n  /reservations/{reservation_id}:\n    put:\nDescribing Each Query and Action as a Specification with an Open Standard \n| \n49",
      "content_length": 2036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "# @see https://swagger.io/docs/specification/describing-parameters\n      summary: Book or re-book a reservation\n      description: |\n        Example request:\n        ```\n          PUT http://api.example.com/v1/reservations/d2783fc5-0fee\n        ```\n      parameters:\n        - name: reservation_id\n          in: path\n          required: true\n          description: Unique identifier of the reservation being created or\n                       changed\n          schema:\n            type : string\n          example: d2783fc5-0fee\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                outbound:\n                  type: object\n                  properties:\n                    flight_num:\n                      type: string\n                      example: \"AA 253\"\n                    flight_date:\n                      type: string\n                      example: \"2019-12-31T08:01:00\"\n                    seats:\n                      type: array\n                      items:\n                        type: string\n                returning:\n                  type: object\n                  properties:\n                    flight_num:\n                      type: string\n                      example: \"AA 254\"\n                    flight_date:\n                      type: string\n                      example: \"2020-01-07T14:16:00\"\n                    seats:\n                      type: array\n                      items:\n                        type: string\n            example: [\n              {\n50 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "outbound: {\n                  flight_num: \"AA 253\",\n                  flight_date: \"2019-12-31T08:01:00\",\n                  seats: [\n                    \"9C\"\n                  ]\n                },\n                returning: {\n                  flight_num: \"AA 254\",\n                  flight_date: \"2020-01-07T14:16:00\",\n                  seats: [\n                    \"10A\"\n                  ]\n                }\n              }\n            ]\n      responses:\n        '200':    # success response\n          description: Successful Reservation\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  reservation_id:\n                    type: string\n                    description: some additional description\n        '403':\n          description: seat(s) unavailable. Booking failed.\n          content:\n            application/json:\n              schema:\n                type: string\n                description: detailed information\nThe rendered output with the VS Code plug-in should look something like\nFigure 3-4.\nDescribing Each Query and Action as a Specification with an Open Standard \n| \n51",
      "content_length": 1186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "Figure 3-4. Rendering of a sample OAS document\nProducing a formal API contract is a huge milestone for the design of APIs and\nmicroservices. Some may even consider it a job well done at this point. However,\ngood API designs cannot end at this stage. We wish things were that simple, but there\nis actually an additional, critical activity that still needs to be completed. The next \nstep in the SEED(S) process captures this activity.\n52 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "Getting Feedback on the API Specification\nThe initial version of the API and service design as captured by an OAS-based\ndescription, or some other standard, is an important milestone, but there is more\nmodeling work that is necessary for a well-designed API.\nWe need to show the draft design of the endpoints to the client developers who will\nbe asked to use these APIs and services, and collect their feedback. If the previous\nsteps involved active brainstorming and work, this is the stage of careful listening and\nreflection. It is an incredibly important step for designing APIs, if you care to design\nthe kind of APIs and microservices that will stand the test of time and which your\nclients will love to use.\nKey Decision: Collect Feedback on Your Service Designs\nService design is not done until it is presented to the target audience for the service\nand feedback is collected and applied to the initial designs.\nGenerally, you need to keep in mind two groups of customers when designing serv‐\nices and APIs:\n• End users of the system. Your APIs enable the user experiences for them.\n• Client developers who will code against your services (APIs or microservices). They\nbuild end users’ experiences, such as web or mobile applications.\nAt the beginning of the SEED(S) process, we interview the end users to collect and\nunderstand the Job Stories relevant to them. However, later in the process we start\nreceiving feedback from the client developers. This can happen as early as the interac‐\ntions design phase, and then again once the OAS is produced, before coding. This\nsecond group, API client developers, must be interviewed to test the usability of the\ndesigns, to avoid coding something that may end up being rejected by them due to\npoor usability.\nBoth of the research activities are critical. The first study makes sure we build the\nright thing. The latter one makes sure we build it the right way!\nImplementing Microservices\nThe last step in the SEED(S) methodology is actually implementing microservices. It \nis intentionally done at the very end of the process. Coding is one of the most expen‐\nsive activities any software engineering team can undertake. Recoding a functionality\nthat was initially designed based on wrong assumptions is a horrible, time-\nconsuming, and expensive task. This is why we engage in a carefully thought-out\nGetting Feedback on the API Specification \n| \n53",
      "content_length": 2403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "process such as SEED(S) before we jump into coding microservices. Overall, it saves\ntime and delivers better outcomes.\nBefore we wrap up this chapter, we need to clarify an important detail. Throughout\nthis chapter, we have been saying “APIs and microservices” and we started by men‐\ntioning that the SEED(S) methodology can be equally successfully applied to both the\ndesign process of APIs as well as that of microservices. This is in part true because\nAPIs and microservices have a lot of similarities. But how are they different? Are\nmicroservices just small APIs? In the next section, we will try to shed some light onto\nthis important question.\nMicroservices Versus APIs\nAPIs and microservices do indeed have a lot in common. Microservices are capabili‐\nties exposed via standard network protocols, most commonly HTTP. But capabilities\nexposed as HTTP endpoints had been known as web APIs, way before the coining of\nthe term microservices. So are the two essentially the same thing? Are microservices\njust a new flavor of APIs—smaller APIs? More importantly, do we even need conven‐\ntional APIs once we start writing microservices, or do the smaller APIs (microservi‐\nces) replace the bigger (conventional) APIs? We have often seen these questions cause\na lot of confusion on teams trying to adopt microservices architecture.\nWith some frequency we have encountered developers referring to any small, focused\nAPIs as “microservices.” In such an approach microservices have the same role as\nAPIs had before them, so they do indeed replace the APIs of old. In our experience,\nthis is not an ideal approach for successful microservices thinking, and we offer an\nalternative, albeit opinionated, definition of what separates microservices from legacy\nAPIs. Our approach builds on the experience of some notable experts in the space,\nand is rooted in our own experiences with successful microservices projects and\nteams.\nMicroservices Are Not Just Smaller APIs\nMicroservices are not just smaller replacements for the APIs of the\nold days. Microservices provide the implementation of your sys‐\ntem, while APIs should still be the outward-facing interface of a\nsystem.\nWe think that if microservices replace anything, the things they replace are the modu‐\nlar components you used to build your systems with. If before you would build a\nlarge system by linking (statically or dynamically) various submodules together, in a\nmicroservices architecture the building blocks are networked services we call “micro‐\nservices.” This approach is depicted in Figure 3-5.\n54 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 2621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "Figure 3-5. Relationship between microservices and APIs\nNote that a similar approach—of separating APIs into “internal ones, the ones you\nbuild with”; and “external ones, the ones that are optimized for consumption by fron‐\ntends”—has been described by Phil Calçado as the Backend for Frontend pattern\nwhen he was at SoundCloud, and by Daniel Jacobson during his time at Netflix. Dan‐\niel Jacobson explained how at Netflix they separated APIs into Experience (frontend)\nand Ephemeral (backend) APIs.\nKey Decision: Web APIs Are Layered on Top of Microservices\nDifferentiate between web APIs that represent the public interface of your subsystem\nand microservices that represent the implementation of the same system. Avoid\nthinking of microservices as “just small APIs.”\nThere is no one, true way of organizing microservices and connecting them up with\n“frontend” APIs. This is the part where we live up to the promise of providing\nunabashedly opinionated guidance in this book. Our opinions are rooted in what we\nhave witnessed to have worked well, but we also acknowledge that other strategies\nmay have worked for other practitioners.\nIn our experience, the ideal separation of duties happens when all of the business\nlogic (capabilities) is implemented by microservices, while APIs act as a thin layer of\norchestration in front of those microservices. Additionally, we recommend that teams\ntry to avoid microservices directly “invoking” each other. Instead, for the sake of\nloose coupling, it’s best if any orchestrating workflow is implemented in the API layer,\nin front of microservices, without microservices knowing anything about each other.\nMicroservices Versus APIs \n| \n55",
      "content_length": 1681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "Note that there is no 1:1 relationship between an API and the\nmicroservices that implement the corresponding capability. These\ntwo assets are parts of fundamentally different layers of your\narchitecture.\nWe believe that such a “microservices should be unaware of each other and be orches‐\ntrated externally” approach is where the Unix philosophy of building a system as a\ncollection of composable tools resonates well with microservices architecture princi‐\nples. One of the most powerful aspects of the Unix philosophy is that you can com‐\nbine Unix tools (e.g., GNU tools) in a variety of ways using input and output piping\non the command line or in shell scripts. However, in order to achieve this, it’s critical\nthat various Unix tools act the same way for any input—they should not care who\n“calls” them or where their output goes. Components cannot explicitly know about\neach other for them to become composable. Loose coupling is what makes the whole\nthing work, not just that the tools are small-ish and focused. The same holds true for\nmicroservices.\nKeep Microservices Unaware of Each Other\nAvoid microservices directly “knowing” about each other and\ndirectly calling each other via synchronous interfaces. Instead, try\nto orchestrate processes involving multiple microservices in the\nAPI layer. If this is not possible, consider using asynchronous inter‐\nfaces between microservices where an upstream microservice pub‐\nlishes data to an event log (e.g., Kafka) and a downstream\nmicroservice can subscribe to that event log without the upstream\nmicroservice having tight coupling with the subscriber(s).\nSummary\nIn this chapter, we set up a critical foundation for understanding the process of\ndesigning robust microservices. By establishing an effective and repeatable methodol‐\nogy, the SEED(S) method, we acquired a powerful understanding of many aspects of\nwhat traits make projects successful in their microservices journey and learned how\nto adapt these traits for our own circumstances.\nIn the following chapters we will leverage the insights gained from an understanding\nof SEED(S). In Chapters 4 and 5 we will dive deeper into the design process for\nmicroservices, and in Chapter 9 we go through the code implementing several micro‐\nservices of our sample project, reusing, demonstrating, and expanding efforts started\nin this chapter.\n56 \n| \nChapter 3: Designing Microservices: The SEED(S) Process",
      "content_length": 2417,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "CHAPTER 4\nRightsizing Your Microservices:\nFinding Service Boundaries\nOne of the most challenging aspects of building a successful microservices system is\nthe identification of proper microservice boundaries. It makes intuitive sense that\nbreaking up a large codebase into smaller, simpler, more loosely coupled parts\nimproves maintainability, but how do we decide where and how to split the code into\nparts to achieve those desired properties? What rules do we use to know where one\nservice ends and another one starts? Answering these fundamental questions is chal‐\nlenging. A lot of teams new to microservices stumble at them. Drawing the microser‐\nvice boundaries incorrectly can significantly diminish the benefits of using\nmicroservices, or in some cases even derail the entire effort. It is then not surprising\nthat the most frequent, most pressing question microservices practitioners ask is:\nhow can a bigger application be properly sliced into a collection of microservices?\nIn this chapter, we look deep into the leading methodology for the effective analysis,\nmodeling, and decomposition of large domains (Domain-Driven Design), explain the\nefficiency benefits of using Event Storming for domain analysis, and close by intro‐\nducing the Universal Sizing Formula, a unique guidance for the effective sizing of\nmicroservices.\nWhy Boundaries Matter, When They Matter, and How to\nFind Them\nRight in the title of the architectural pattern, we have the word micro—the architec‐\nture we are designing is that of “micro” services! But how “micro” should our services\nbe? We are obviously not measuring the physical length of something and assuming\nthat micro means one-millionth of a meter (i.e., of the base unit of length in the Inter‐\nnational System of Units). So what does micro mean for our purposes? How are we\n57",
      "content_length": 1823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "supposed to slice up our larger problem into smaller services to achieve the promised\nbenefits of “micro” services? Maybe we could print our source code on paper, glue\neverything together, and measure the literal length of that? Or jokes aside, should we\ngo by the number of lines in our source code—keeping that number small to ensure\neach of our microservices is also small enough? What is “enough,” however? Maybe\nwe just arbitrarily declare that each microservice must have no greater than 500 lines\nof code? We could also draw boundaries at the familiar, functional edges of our\nsource code and say that each granular capability represented by a function in the\nsource code of our system is a microservice. This way we could build our entire appli‐\ncation with, say, serverless functions, declaring each such function to be a microser‐\nvice. Clean and easy! Right? Maybe not.\nIn practice, each of these simplistic approaches has indeed been tried and they all\nhave significant drawbacks. While source lines of code (SLOC) has historically\nenjoyed some usage as a measure of effort/complexity, it has since been widely\nacknowledged to be a poor measurement for determining the complexity or the true\nsize of any code and one that can be easily manipulated. Therefore, even if our goal\nwere to create “small” services with the hope of keeping them simple, lines of code\nwould be a poor measurement.\nDrawing boundaries at functional edges is even more tempting. And it has become\neven more tempting with the increase in popularity of serverless functions such as\nAmazon Web Services’ Lambda functions. Building on top of the productivity and\nwide adoption of AWS Lambdas, many teams have rushed into declaring those func‐\ntions “microservices.” There are a number of significant problems if you go down this\nroad, the most important of which are:\nDrawing boundaries based on technical needs is an anti-pattern\nPer Lewis and Fowler, microservices should be “organized around business capa‐\nbilities,” not technical needs. Similarly, Parnas, in an article from 1972, recom‐\nmends decomposing systems based on modular encapsulation of design changes\nover time. Neither approach necessarily aligns strongly with the boundaries of\nserverless functions.\nToo much granularity, too soon\nAn explosive level of granularity early in the microservices project life cycle can\nintroduce crushing levels of complexity that will stop the microservices effort in\nits tracks, even before it has a chance to take off and succeed.\nIn Chapter 1 we stated the primary goal of a microservices architecture: it is primarily\nabout minimization of coordination costs, in a complex, multiteam environment, to\nachieve harmony between speed and safety, at scale. Therefore, services should be\ndesigned in a way that minimizes coordination needs between the teams working on\ndifferent microservices. However, if we break code up into functions in a way that\ndoes not necessarily lead to minimized coordination, we will end up with incorrectly\n58 \n| \nChapter 4: Rightsizing Your Microservices: Finding Service Boundaries",
      "content_length": 3091,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "sized microservices. Just assuming that any way of organizing code into serverless\nfunctions will reduce coordination is misguided.\nEarlier we stated that an important reason for avoiding a size-based or functions-\naligned approach when splitting an application into microservices is the danger of\npremature optimization—having too many services that are too small too early in\nyour microservices journey. Early adopters of microservices, such as Netflix, Sound‐\nCloud, Amazon, and others, eventually found themselves having a lot of microservi‐\nces! That, however, does not mean that these companies started with hundreds of\nvery granular microservices on day one. Rather, a large number of microservices is\nwhat they optimized for after years of development, after having achieved the opera‐\ntional maturity capable of handling the level of complexity associated with the high\ngranularity of microservices.\nAvoid Creating Too Many Microservices Too Early\nThe sizing of services in a microservices architecture is most cer‐\ntainly a journey that should unfold in time. A sure way to sabotage\nthe entire effort is to attempt designing an overly granular system\nearly in that journey.\nWhether you are working on a greenfield project or decomposing\nan existing monolith, the approach should absolutely be to start\nwith only a handful of services and slowly increase the number of\nmicroservices over time. If this leads to some of your microservices\ninitially being larger than in their target state, it is totally OK. You\ncan split them up later.\nEven if we are starting with just a few microservices, taking it slow, we need some\nreliable methodology to determine how to size microservices. Next, we will explore\nbest practices successfully used in the industry.\nDomain-Driven Design and Microservice Boundaries\nAt the onset of figuring out microservices design best practices, Sam Newman intro‐\nduced some foundational ground rules in his book Building Microservices (O’Reilly).\nHe suggested that when drawing service boundaries, we should strive for such a\ndesign that the resulting services are:\nDomain-Driven Design and Microservice Boundaries \n| \n59",
      "content_length": 2153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "Loosely coupled\nServices should be fairly unaware and independent of each other, so that a code\nmodification in one of them doesn’t result in ripple effects in others. We’ll also\nprobably want to limit the number of different types of runtime calls from one\nservice to another since, beyond the potential performance problem, chatty com‐\nmunications can also lead to a tight coupling of components. Taking our “coordi‐\nnation minimization” approach, the benefit of the loose coupling of the services\nis quite obvious.\nHighly cohesive\nFeatures present in a service should be highly related, while unrelated features\nshould be encapsulated elsewhere. This way, if you need to change a logical unit\nof functionality, you should be able to change it in one place, minimizing time to\nreleasing that change (an important metric). In contrast, if we had to change the\ncode in a number of services, we would have to release lots of different services at\nthe same time to deliver that change. That would require significant levels of\ncoordination, especially if those services are “owned” by multiple teams, and it\nwould directly compromise our goal of minimizing coordination costs.\nAligned with business capabilities\nSince most requests for the modification or extension of functionality are driven\nby business needs, if our boundaries are closely aligned with the boundaries of\nbusiness capabilities, it would naturally follow that the first and second design\nrequirements, above, are more easily satisfied. During the days of monolith\narchitectures, software engineers often tried to standardize on “canonical data\nmodels.” However, the practice demonstrated, over and over again, that detailed\ndata models for modeling reality do not last for long—they change quite often\nand standardizing on them leads to frequent rework. Instead, what is more dura‐\nble is a set of business capabilities that your subsystems provide. An accounting\nmodule will always be able to provide the desired set of capabilities to your larger\nsystem, regardless of how its inner workings may evolve over time.\nThese design principles have proven to be very useful and received wide adoption\namong microservices practitioners. However, they are fairly high-level, aspirational\nprinciples and arguably do not provide the specific service-sizing guidance needed by\nday-to-day practitioners. In search of a more practical methodology, many turned to\nDomain-Driven Design.\nThe software design methodology known as Domain-Driven Design (DDD) signifi‐\ncantly predates microservices architecture. It was introduced by Eric Evans in 2003,\nin his seminal book of the same name, Domain-Driven Design: Tackling Complexity in\nthe Heart of Software (Addison-Wesley). The main premise of the methodology is the\nassertion that, when analyzing complex systems, we should avoid seeking a single\n60 \n| \nChapter 4: Rightsizing Your Microservices: Finding Service Boundaries",
      "content_length": 2925,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "unified domain model representing the entire system. Rather, as Evans said in his\nbook:\nMultiple models coexist on big projects, and this works fine in many cases. Different\nmodels apply in different contexts.\nOnce Evans established that a complex system is fundamentally a collection of multi‐\nple domain models, he made the critical additional step of introducing the notion of\nbounded context. Specifically, he stated that:\nA Bounded Context defines the range of applicability of each model.\nBounded contexts allow implementation and runtime execution of different parts of\nthe larger system to occur without corrupting the independent domain models\npresent in that system. After defining bounded contexts, Eric went on to also help‐\nfully provide a formula for identifying the optimal edges of a bounded context by\nestablishing the concept of Ubiquitous Language.\nTo understand the meaning of Ubiquitous Language, it is important to observe that a\nwell-defined domain model first and foremost provides a common vocabulary of\ndefined terms and notions, a common language for describing the domain, that\nsubject-matter experts and engineers develop together in close collaboration, balanc‐\ning the business requirements and implementation considerations. This common\nlanguage, or shared vocabulary, is what in DDD we call Ubiquitous Language. The\nimportance of this observation lies in acknowledging that same words may carry dif‐\nferent meanings in different bounded contexts. A classic example of this is shown in\nFigure 4-1. The term account carries significantly different meaning in the identity\nand access management, customer management, and financial accounting contexts of\nan online reservation system.\nFigure 4-1. Depending on the domain where it appears, “account” can have different\nmeanings\nIndeed, for an identity and access management context, an account is a set of creden‐\ntials used for authentication and authorization. For a customer management-\nbounded context, an account is a set of demographic and contact attributes, while for\na financial accounting context, it’s probably payment information and a list of past\ntransactions. We can see that the same basic English word is used with significantly\nDomain-Driven Design and Microservice Boundaries \n| \n61",
      "content_length": 2279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "different meaning in different contexts, and it is OK because we only need to agree on\nthe ubiquitous meaning of the terms (the Ubiquitous Language) within the bounded\ncontext of a specific domain model. According to DDD, by observing edges across\nwhich terms change their meaning, we can identify the boundaries of the contexts.\nIn DDD, not all terms that come to mind when discussing a domain model make into\nthe corresponding Ubiquitous Language. Concepts in a bounded context that are\ncore to the context’s primary purpose are part of the team’s Ubiquitous Language, all\nothers should be left out. These core concepts can be discovered from the set of\nJTBDs that you create for the bounded context. As an example, let’s look at\nFigure 4-2.\nFigure 4-2. Using Job Story syntax to identify key terms of a Ubiquitous Language\nIn this example, we are using the Job Story format that we introduced in Chapter 3\nand applying it to a job from the identity and access control bounded context. We can\nsee that key nouns, highlighted in Figure 4-2, correspond to the terms in the related\nUbiquitous Language. We highly recommend the technique of using key nouns from\nwell-written Job Stories in the identification of the vocabulary terms relevant to your\nUbiquitous Language.\nNow that we have discussed some key concepts of DDD, let’s also look at something\nthat can be very useful in designing microservice interactions properly: context map‐\nping. We will explore key aspects of context mapping in the next section.\nContext Mapping\nIn DDD, we do not attempt to describe a complex system with a single domain\nmodel. Rather, we design multiple independent models that coexist in the system.\nThese subdomains typically communicate with each other using published interface\ndescriptions. The representation of various domains in a larger system and the way\nthey collaborate with each other is called a context map. Consequently, the act of\nidentifying and describing said collaborations is known as context mapping, as shown\nin Figure 4-3.\n62 \n| \nChapter 4: Rightsizing Your Microservices: Finding Service Boundaries",
      "content_length": 2107,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "Figure 4-3. Context mapping\nDDD identifies several major types of collaboration interactions when mapping\nbounded contexts. The most basic type is known as a shared kernel. It occurs when\ntwo domains are developed largely independently and, almost by accident, they end\nup overlapping on some subset of each other’s domains (see Figure 4-4). Two parties\nmay agree to collaborate on this shared kernel, which may also include shared code\nand data model, as well as the domain description.\nFigure 4-4. Shared kernel\nWhile tempting on the surface of things (after all, the desire for collaboration is one\nof the most human of instincts), the shared kernel is a problematic pattern, especially\nwhen used for microservices architectures. By definition, a shared kernel immediately\nrequires a high degree of coordination between two independent teams to even jump-\nstart the relationship, and keeps requiring coordination for any further modifications.\nSprinkling your microservices architecture with shared kernels will introduce many\npoints of tight coordination. In cases when you do have to use a shared kernel in a\nmicroservices ecosystem, it’s advised that one team is designated as the primary\nowner/curator, and everybody else is a contributor.\nAlternatively, two bounded contexts can engage in what DDD calls an Upstream–\nDownstream kind of relationship. In this type of relationship, the Upstream acts as\nthe provider of some capability, and the Downstream is the consumer of said capabil‐\nity. Since domain definitions and implementations do not overlap, this type of rela‐\ntionship is more loosely coupled than a shared kernel (see Figure 4-5).\nFigure 4-5. Upstream–Downstream relationship\nDomain-Driven Design and Microservice Boundaries \n| \n63",
      "content_length": 1751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "Depending on the type of coordination and coupling, an Upstream–Downstream\nmapping can be introduced in several forms:\nCustomer–Supplier\nIn a customer–supplier scenario, Upstream (supplier) provides functionality to\nthe Downstream (customer). As long as the provided functionality is valuable,\neverybody is happy; however, Upstream carries the overhead of backwards com‐\npatibility. When Upstream modifies their service, they need to ensure that they\ndo not break anything for the customer. More dramatically, the Downstream\n(customer) carries the risk of the Upstream intentionally or unintentionally\nbreaking something for it, or ignoring the customer’s future needs.\nConformist\nAn extreme case of the risks for a customer–supplier relationship is the conform‐\nist relationship. It’s a variation on Upstream–Downstream, when the Upstream\nexplicitly does not or cannot care about the needs of its Downstream. It’s a use-\nat-your-own-risk kind of relationship. The Upstream provides some valuable\ncapability that the Downstream is interested in using, but given that the\nUpstream will not cater to its needs, the Downstream needs to constantly con‐\nform to the changes in the Upstream.\nConformist relationships often occur in large organizations and systems when a\nmuch larger subsystem is used by a smaller one. Imagine developing a small, new\ncapability inside an airline reservation system and needing to use, say, an enterprise\npayments system. Such a large enterprise system is unlikely to give the time of day to\nsome small, new initiative, but you also cannot just reimplement a whole payments\nsystem on your own. Either you will have to become a conformist, or another viable\nsolution may be to separate ways. The latter doesn’t always mean that you will imple‐\nment similar functionality yourself. Something like a payments system is complex\nenough that no small team should implement it as a side job of another goal, but you\nmight be able go outside the confines of your enterprise and use a commercially\navailable payments vendor instead, if your company allows it.\nIn addition to becoming a conformist or going separate ways, the Downstream has a\nfew more DDD-sanctioned ways of protecting itself from the negligence of its\nUpstream: an anti-corruption layer and using Upstreams that provide open host\ninterfaces.\nAnti-corruption layer\nIn this scenario, the Downstream creates a translation layer called an anti-\ncorruption layer (ACL) between its and the Upstream’s Ubiquitous Languages, to\nguard itself from future breaking changes in the Upstream’s interface. Creating an\nACL is an effective, sometimes necessary, measure of protection, but teams\nshould keep in mind that in the long term this can be quite expensive for the\nDownstream to maintain (see Figure 4-6).\n64 \n| \nChapter 4: Rightsizing Your Microservices: Finding Service Boundaries",
      "content_length": 2858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "Figure 4-6. Anti-corruption layer\nOpen host service\nWhen the Upstream knows that multiple Downstreams may be using its capabili‐\nties, instead of trying to coordinate the needs of its many current and future con‐\nsumers, it should instead define and publish a standard interface, which all\nconsumers will need to adopt. in DDD, such Upstreams are known as open host\nservices. By providing an open, easy protocol for all authorized parties to inte‐\ngrate with, and maintaining said protocol’s backwards compatibility or providing\nclear and safe versioning for it, the open host can scale its operations without\nmuch drama. Practically all public services (APIs) use this approach. For exam‐\nple, when you are using the APIs of a public cloud provider (AWS, Google,\nAzure, etc.), they usually don’t know or cater to you specifically as they have mil‐\nlions of customers, but they are able to provide and evolve a useful service by\noperating as an open host (see Figure 4-7).\nFigure 4-7. Open host service\nIn addition to relation types between domains, context mappings can also differenti‐\nate based on the integration types used between bounded contexts.\nSynchronous Versus Asynchronous Integrations\nIntegration interfaces between bounded contexts can be synchronous or asynchro‐\nnous, as shown in Figure 4-8. None of the integration patterns fundamentally assume\none or the other style.\nCommon patterns for synchronous integrations between contexts are RESTful APIs\ndeployed over HTTP, gRPC services using binary formats such as protobuf, and more\nrecently services using GraphQL interfaces.\nOn the asynchronous side, publish–subscribe types of interactions lead the way. In\nthis interaction pattern, the Upstream can generate events, and Downstream services\nhave workers able and interested in processing those, as depicted in Figure 4-8.\nDomain-Driven Design and Microservice Boundaries \n| \n65",
      "content_length": 1895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "Figure 4-8. Synchronous and asynchronous integrations\nPublish–subscribe interactions are more complex to implement and debug, but they\ncan provide a superior level of scalability, resilience, and flexibility, in that: multiple\nreceivers, even if implemented with heterogeneous tech stack, can subscribe to the\nsame events using a uniform approach and implementation.\nTo wrap up the discussion of Domain-Driven Design’s key concepts, we should\nexplore the concept of an aggregate. We discuss it in the next section.\nA DDD Aggregate\nIn DDD, an aggregate is a collection of related domain objects that can be viewed as a\nsingle unit by external consumers. Those external consumers only reference a single\nentity in the aggregate, and that entity is known in DDD as an aggregate root. Aggre‐\ngates allow domains to hide internal complexities of a domain, and expose only infor‐\nmation and capabilities (interface) that are “interesting” to an external consumer. For\ninstance, in the Upstream–Downstream mappings that we discussed earlier, the\nDownstream does not have to, and typically will not want to, know about every single\ndomain object within the Upstream. Instead, it will view the Upstream as an aggre‐\ngate, or a collection of aggregates.\nWe will see the notion of an aggregate resurface, in the next section when we discuss\nEvent Storming—a powerful methodology that can greatly streamline the process of\ndomain-driven analysis and turn it into a much faster and more fun exercise.\nIntroduction to Event Storming\nDomain-Driven Design is a powerful methodology for analyzing both the whole-\nsystem-level (called “strategic” in DDD) as well as the in-depth (called “tactical”)\ncomposition of your large, complex systems. We have also seen that DDD analysis\ncan help us identify fairly autonomous subcomponents, loosely coupled across boun‐\nded contexts of their respective domains.\nIt’s very easy to jump to the conclusion that in order to fully learn how to properly\nsize microservices, we just need to become really good in domain-driven analysis; if\n66 \n| \nChapter 4: Rightsizing Your Microservices: Finding Service Boundaries",
      "content_length": 2133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "we make our entire company also learn and fall in love with it (because DDD is cer‐\ntainly a team sport), we’ll be on our way to success!\nIn the early days of microservices architectures, DDD was so universally proclaimed\nas the one true way to size microservices that the rise of microservices gave a huge\nboost to the practice of DDD, as well—or at least more people became aware of it,\nand referenced it. Suddenly, many speakers were talking about DDD at all kinds of\nsoftware conferences, and a lot of teams started claiming that they were employing it\nin their daily work. Alas, a close look easily uncovered that the reality was somewhat\ndifferent and that DDD had become one of those “much-talked-about-less-practiced”\nthings.\nDon’t get us wrong: there were people using DDD way before microservices, and\nthere are plenty using it now as well, but speaking specifically of using it as a tool for\nsizing microservices, it was more hype and vaporware than reality.\nThere are two primary reasons why more people talked about DDD than practiced it\nin earnest: it is complex and it is expensive. Practicing DDD requires quite a lot of\nknowledge and experience. Eric Evans’s original book on the subject is a hefty 520\npages long, and you would need to read at least a few more books to really get it, not\nto mention gain some experience actually implementing it on a number of projects.\nThere simply were not enough people with the skills and experience and the learning\ncurve was steep.\nTo exacerbate the problem, as we mentioned, DDD is a team sport, and a time-\nconsuming one at that. It’s not enough to have a handful of technologists well-versed\nin DDD; you also need to sell your business, product, design, etc., teams on partici‐\npating in long and intense domain-design sessions, not to mention explain to them at\nleast the basics of what you are trying to achieve. Now, in the grand scheme of things,\nis it worth it? Very likely, yes: especially for large, risky, expensive systems, DDD can\nhave many benefits. However, if you are just looking to move quickly and size some\nmicroservices, and you have already cashed in your political capital at work, selling\neverybody on the new thing called microservices—good luck also asking a whole\nbunch of busy people to give you enough time to size your services right! It was just\nnot happening—too expensive and too time-consuming.\nAnd then suddenly a fellow by the name of Alberto Brandolini, who had invested\ndecades in understanding better ways for teams to collaborate, found a shortcut! He\nproposed a fun, lightweight, and inexpensive process called Event Storming, which is\nheavily based and inspired by the concepts of DDD but can help you find bounded\ncontexts in a matter of hours instead of weeks or months. The introduction of Event\nStorming was a breakthrough for inexpensive applicability of DDD specifically for\nthe sake of service sizing. Of course, it’s not a full replacement, and it won’t give you\nall the benefits of formal DDD (otherwise it would be magic). But as far as the dis‐\ncovery of bounded contexts goes, with good approximation—it is indeed magical!\nIntroduction to Event Storming \n| \n67",
      "content_length": 3172,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "Event Storming is a highly efficient exercise that helps identify bounded contexts of a\ndomain in a streamlined, fun, and efficient manner, typically much faster than with\nmore traditional, full DDD. It is a pragmatic approach that lowers the cost of DDD\nanalysis enough to make it viable in situations in which DDD would not be afforda‐\nble otherwise. Let’s see how this “magic” of Event Storming is actually executed.\nKey Decision: Use Event Storming Instead of Formal DDD\nUse the more lightweight Event Storming process instead of formal DDD to discover\nthe main aggregates in your subdomain and identify edges of the various bounded\ncontexts present in your system.\nThe Event-Storming Process\nThe beauty of Event Storming is in its ingenious simplicity. In physical spaces (prefer‐\nred, when possible), all you need to hold a session of Event Storming is a very long\nwall (the longer the better), a bunch of supplies, mostly stickies and Sharpies, and\nfour to five hours of time from well-represented members of your team. For a suc‐\ncessful Event Storming session, it is critical that participants are not only engineers.\nBroad participation from such groups as product, design, and business stakeholders\nmakes a significant difference. You can also host virtual Event Storming sessions\nusing digital collaboration tools that can mimic the physical process described here.\nThe process of hosting physical Event Storming sessions starts by purchasing the sup‐\nplies. To make things easier, we’ve created an Amazon shopping list that we use for\nEvent Storming sessions (see Figure 4-9). It is comprised of:\n• A large number of stickies of different colors, most importantly, orange and blue,\nand then several other colors for various object types. You need a lot of those.\n(Stores never had enough for me, so I got in the habit of buying online.)\n• A roll of 1/2-inch white artist tape.\n• A long roll of paper (e.g., IKEA Mala Drawing Paper) that we are going to hang\non the wall using the artist tape. Go ahead and create multiple “lanes.”\n• At least as many Sharpies as the number of session participants. Everybody needs\nto have their own!\n• Did we already mention a long, unobstructed wall that we can tape the roll of\npaper to?\n68 \n| \nChapter 4: Rightsizing Your Microservices: Finding Service Boundaries",
      "content_length": 2312,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "Figure 4-9. Required supplies for an Event Storming session\nDuring Event Storming sessions, broad participation, e.g., from subject-matter\nexperts, product owners, and interaction designers, is very valuable. Event Storming\nsessions are short enough (just several hours rather than analysis requiring days or\nweeks) that, considering the value of their outcomes, the clarity they bring for all rep‐\nresented groups and the time they save in the long term, they are time well-invested\nfor all participants. An Event Storming session that is limited to just software engi‐\nneers is mostly useless, since it happens in a bubble and cannot lead to the cross-\nfunctional conversations necessary for desired outcomes.\nOnce we have the supplies, the large room with a wide-open wall with a roll of paper\nwe have taped to it, and all the required people, we (the facilitator) ask everybody to\ngrab a bunch of orange stickies and a personal Sharpie. Then we give them a simple\nassignment: to write the key events of the domain being analyzed as orange sticky\nnotes (one event per one note), expressed in a verb in the past tense, and place the\nnotes along a timeline on the paper taped to the wall to create a “lane” of time, as\nshown in Figure 4-10.\nIntroduction to Event Storming \n| \n69",
      "content_length": 1279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "Figure 4-10. An event timeline with sticky notes\nParticipants should not obsess about the exact sequence of events, and at this stage\nthere should be no coordination of events among participants. The only thing they\nare asked is to individually think of as many events as possible and put the events\nthey think occur earlier in time to the left, and put the later events more to the right.\nIt is not their job to weed out duplicates. At least, not yet. This phase of the assign‐\nment usually takes 30 minutes to an hour, depending on the size of the problem and\nthe number of participants. Usually, you want to see at least 100 event sticky notes\ngenerated before you can call it a success.\nIn the second phase of the exercise, the group is asked to look at the resulting set of\nnotes on the wall, and with the help of the facilitator, to start arranging them into a\nmore coherent timeline, identifying and removing duplicates. Given enough time, it\nis very helpful for the participants to start creating a “storyline,” walking through the\nevents in an order that creates something like a “user journey.” In this phase, the team\nmay have some questions or confusion; we don’t try to solve these issues, but rather\ncapture them as “hotspots”—differently colored sticky notes (typically purple) that\nhave the questions on them. Hotspots will need to be answered offline, in follow-ups.\nThis phase can likewise take 30 to 60 minutes.\nIn the third stage, we create what in Event Storming is known as a reverse narrative.\nBasically, we walk the timeline backward, from the end to the start, and identify com‐\nmands; things that caused the events. We use sticky notes of a different color (typi‐\ncally blue) for the commands. At this stage your storyboard may look something like\nFigure 4-11.\n70 \n| \nChapter 4: Rightsizing Your Microservices: Finding Service Boundaries",
      "content_length": 1863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "Figure 4-11. Introducing commands to the Event Storming timeline\nBe aware that a lot of commands will have one-to-one relationship with an event. It\nwill feel redundant, like the same thing worded in the past versus present. Indeed, if\nyou look at the previous figure, the first two commands are like that. It often confuses\npeople new to Event Storming. Just ignore it! We don’t pass judgment during Event\nStorming, and while some commands may be 1:1 with events, some will not be. For\nexample, the “Submit payment authorization” command triggers a whole bunch of\nevents. Just capture what you know/think happens in real life and don’t worry about\nmaking things “pretty” or “neat.” The real world you are modeling is also usually\nmessy.\nIn the next phase, we acknowledge that commands do not produce events directly.\nRather, special types of domain entities accept commands and produce events. In\nEvent Storming, these entities are called aggregates (yes, the name is inspired by the\nsimilar notion in DDD). What we do in this stage is rearrange our commands and\nevents, breaking the timeline when needed, such that the commands that go to the\nsame aggregate are grouped around that aggregate and the events “fired” by that\naggregate are also moved to it. You can see an example of this stage of Event Storming\nin Figure 4-12.\nIntroduction to Event Storming \n| \n71",
      "content_length": 1365,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "Figure 4-12. Aggregates on an Event Storming timeline\nThis phase of the exercise can take 15 to 25 minutes. Once we are done with it, you\nshould discover that our wall now looks less like a timeline of events and more like a\ncluster of events and commands grouped around aggregates.\nGuess what? These clusters are the bounded contexts we were looking for.\nThe only thing left is to classify various contexts by the level of their priority (similar\nto “root,” “supportive,” and “generic” in DDD). To do this, we create a matrix of boun‐\nded context/subdomains and rank them across two properties: difficulty and compet‐\nitive edge. In each category, we use T-shirt sizes <S, M, or L> to rank accordingly. In\nthe end, the decision making as to when to invest effort is based on the following\nguidelines:\n1. Large competitive advantage/large effort: these are the contexts to design and\nimplement in-house and spend most time on.\n2. Small advantage/large effort: buy!\n3. Small advantage/small effort: great assignments to trainees.\n4. Other combinations are a coin toss and require a judgment call.\nThis last phase, the “competitive analysis,” is not part of Brandoli‐\nni’s original Event Storming process, and was proposed by Greg\nYoung for prioritizing domains in DDD in general. We find it to be\na useful and fun exercise when done with an adequate level of\nhumor.\n72 \n| \nChapter 4: Rightsizing Your Microservices: Finding Service Boundaries",
      "content_length": 1441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "The entire process is very interactive, requires the involvement of all participants, and\nusually ends up being fun. It will require experienced facilitator to keep things mov‐\ning smoothly, but the good news is that being a good facilitator doesn’t take the same\neffort as becoming a rocket scientist (or DDD expert). After reading this book and\nfacilitating some mock sessions for practice, you can easily become a world-class\nEvent Storming facilitator!\nAs a facilitator, it is a good idea to watch the time and have a plan for your session.\nFor a four-hour session rough allocation of time would look like:\n• Phase 1 (~30 min): Discover domain events\n• Phase 2 (~45 min): Enforce the timeline\n• Phase 3 (~60 min): Reverse narrative and Command Identification\n• Phase 4 (~30 min): Identify aggregates/bounded contexts\n• Phase 5 (~15 min): Competitive analysis\nAnd if you noticed that these times do not add up to 4 hours, keep in mind that you\nwill want to give people some breaks in the middle, as well as leave yourself time to\nprepare the space and provide guidance in the beginning.\nIntroducing the Universal Sizing Formula\nBounded contexts are a fantastic starting point for rightsizing microservices. We have\nto be cautious, however, to not assume that microservice boundaries are synonymous\nwith the bounded contexts from DDD or Event Storming. They are not. As a matter\nof fact, microservice boundaries cannot be assumed to be constant over time. They\nevolve over time and tend to follow an increasing granularity of microservices as the\norganizations and applications they are part of mature. For example, Adrian Cockroft\nnoted that this was definitely a repeating trend that they had observed during his time\nat Netflix.\nNobody Gets Microservice Boundaries Perfectly at the Outset\nIn successful cases of microservices adoption, teams do not start\nwith hundreds of microservices. They start with a much smaller\nnumber, closely aligned with bounded contexts. As time goes by,\nteams split microservices when they run into coordination depen‐\ndencies that they need to eliminate. This also means that teams are\nnot expected to get service boundaries “right” out of the gate.\nInstead, boundaries evolve over time, with a general direction of\nincreased granularity.\nIntroducing the Universal Sizing Formula \n| \n73",
      "content_length": 2320,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "It is worth noting that it’s typically easier to split a service than to merge several serv‐\nices back together, or to move a capability from one service to another. This is\nanother reason why we recommend starting with a coarse-grained design and waiting\nuntil we learn more about the domain and have enough complexity before we split\nand increase service granularity.\nWe have found that there are three principles that work well together when thinking\nabout the granularity of microservices. We call these principles the Universal Sizing\nFormula for microservices.\nThe Universal Sizing Formula\nTo achieve a reasonable sizing of microservices, you should:\n• Start with just a few microservices, possibly using bounded contexts.\n• Keep splitting as your application and services grow, being guided by the needs of\ncoordination avoidance.\n• Be on the right trajectory for decreasing coordination. This is vastly more impor‐\ntant than the current state of how “perfectly” you get service sizing.\nSummary\nIn this chapter we addressed a critical question of how to properly size microservices\nhead-on. We looked at Domain-Driven Design, a popular methodology for modeling\ndecomposition in complex systems; explained the process of conducting a highly effi‐\ncient domain analysis with the Event Storming methodology, and introduced the\nUniversal Sizing Formula, which offers unique guidance for effective sizing of\nmicroservices.\nIn the following chapters we will go deeper into implementation, showing how to\nmanage data in a loosely coupled, componentized microservices environment. We\nalso will walk you through a sample implementation for our demo project: an online\nreservation system.\n74 \n| \nChapter 4: Rightsizing Your Microservices: Finding Service Boundaries",
      "content_length": 1762,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "CHAPTER 5\nDealing with the Data\nIn this chapter, we’ll cover why microservices need to “own their own data” and what\nit means for your architecture. We will discuss when and how to use the the most\nimportant patterns for microservices data management: delegates, data lakes, Sagas,\nEvent Sourcing, and command query responsibility segregation (CQRS). While dis‐\ncussing these important topics, we will also try to demonstrate them on practical\nexamples using our sample project.\nWhen it comes to practical microservices development, one of the early challenges\nthat almost everyone hits is dealing with the data. If it wasn’t for the many challenges\nof data management in this space, turning complex, monolithic implementations into\nloosely coupled, “bite-sized,” manageable microservices would be fairly easy.\nThe design considerations for logical and physical models in microservices imple‐\nmentation are not the same as for designing data tables for the conventional, N-tier,\nmonolithic applications. In this chapter we will see why the differences arise, which\npatterns the microservices practitioners commonly use, and what techniques should\nbe employed to tackle the additional complexities we face when implementing micro‐\nservices systems.\nIndependent Deployability and Data Sharing\nIn Chapter 4, we mentioned that Sam Newman suggests microservices should gener‐\nally be:\n• Loosely coupled from each other; this also means independently deployable\n• Highly cohesive vis-à-vis capabilities inside the microservices\n75",
      "content_length": 1524,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "When services are loosely coupled, a change to one service should not result in a\nchange to another one. You may remember that the main benefit of a microservices\narchitecture is increased speed, in harmony with safety and quality, at scale. And this\nbenefit is achieved by eliminating, or at least decreasing, coordination needs between\nmicroservices. One critical, specific aspect of this loose coupling is what we call inde‐\npendent deployability—being able to make a change to one microservice, and deploy\nit, without the need to change or deploy any other parts of the system, any other\nmicroservices. This is really important and becomes vividly obvious if we visualize\nwhat a typical deployment pipeline looks like in a microservices architecture. In\nFigure 5-1, you can see a simplified graphic representation of deployment pipelines\nfor multiple microservices going through several environments, on their way to\nproduction.\nFigure 5-1. Example multi-environment release pipeline for microservices\nThe process of releasing code through a deployment pipeline becomes significantly\nmore complex and fragile if a deployment of one microservice triggers ripple effects\nof having to also redeploy other parts of the application. Such interdependencies can\ncompromise both the speed and safety of the entire system. Alternatively, if we can\nensure that we can always deploy each microservice independently, without having to\nworry about the ripple effects, we can keep our deployments nimble and safe.\nThere can be a number of reasons why you may not be able to make a deployment of\nyour microservices independent, but in the context of data management, the most\ncommon offender is co-ownership of a data space by multiple microservices. Such\nco-ownership can compromise their loose coupling and our ability to independently\ndeploy code.\nWe will start exploring techniques for avoiding data co-ownership across microservi‐\nces by discussing the notion of microservice-embedded data in the following sections.\n76 \n| \nChapter 5: Dealing with the Data",
      "content_length": 2050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "Microservices Embed Their Data\nIn monolith architectures, sharing of data is a common practice. In typical legacy sys‐\ntems, even in the more modular, service-oriented architecture (SOA) ones, code com‐\nponents co-own data across multiple services as a regular practice. It is actually very\nmuch expected—shared data is a primary pattern of integrating various modularized\nparts of a larger system. Sometimes when we speak of a “monolith,” people imagine\nthat it has no modularization, that it is indeed just one big thing that is not divided\ninto any kind of components. That is not true. Developers have long known that\ndividing a large codebase into smaller ones is highly beneficial for code organization\nand manageability. But a key shortcoming, before microservices, was that the mod‐\nules that the monoliths were divided into were not independently deployable. That\nmade them not loosely coupled, in relation to coordination costs! Case in point: it\nwas primarily due to data coupling that SOA designs never achieved independent\ndeployability and consequently the ability to both go fast and go safely at greater\nscale.\nLet’s look at an example of how a problem may occur. Say that multiple microservices\nshare ownership of a customer table in a database, as depicted in Figure 5-2, in “Data\nEmbedding and the Data Delegate Pattern” on page 79. By “ownership” we mean that\ndifferent microservices read and modify data from the shared table.\nImagine that a flight-search microservice needs to change a field type of one of the\ncolumns in the shared table. If the developers of this microservice just go ahead and\ndo it, let’s say from integer to float or something like that, the change could break the\nreservations or flight-tracking microservices, since they also access the same table\nand may rely on that field to have values of a certain type. To avoid introducing bugs,\nwhen we change the data model because of the needs of the flight-search microser‐\nvice, we will also need to accordingly change the code of the reservations microser‐\nvice or potentially others as well. And we’ll have to redeploy all of the changed\nmicroservices in one concerted effort. Ripple effects due to a changing data layer are\nvery common when multiple components co-own data, and they can cause signifi‐\ncant coupling of various services, which would be a problem for independent\ndeployability.\nSharing data spaces is a primary killer of independent development and independent\ndeployability, in monoliths. By contrast, in a microservices architecture, independent\ndeployability is emphasized as a core value and consequently, data sharing is prohibi‐\nted—microservices are never allowed co-own responsibility for a data space in a data‐\nbase. It should be very clear which microservice owns any dataset in the database, or\nas we commonly state the principle: microservices must own (embed) their data.\nMicroservices Embed Their Data \n| \n77",
      "content_length": 2932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "While embedding their own data is a universal rule for microservices, there are some\nimportant nuances to this principle that are critical to understand clearly. In the next\nsection we will discuss one such consideration in greater detail.\nEmbedding Data Should Not Lead to an Explosion in the Number of\nDatabase Clusters\nWhen building complex applications, we can often end up with different kinds of\ndatabases. Datasets in those databases (e.g., “tables” for relational databases) should\nnever have multiple microservices as co-owners. When you build big systems, you\ncould eventually have hundreds of microservices. Does it then mean that we have to\ndeploy hundreds of distinct clusters of Cassandra, Postgres, Redis, or MySQL? Teams\nimplementing microservices need clarity on how far they should take the notion of\n“microservices must embed their data.” Databases are themselves complex software\nsystems; they’re not deployed on just one server, rather most databases are deployed\non multiple servers for redundancy, reliability, and scalability—possibly dozens of\nservers across different geographic regions. When we introduce the concept of data\nembedding, teams will wonder if they need to create massive database clusters for\neach microservice they create.\nThis could turn into a major problem. If a massive number of database clusters (one\nor more per microservice) was required by a microservices architecture, then it\nwould be the most expensive architectural style in our industry’s history (or close to\nit). Fortunately, this is absolutely not the case. Data independence doesn’t mean that\neach microservice has to deploy its own, distinct cluster of scalable, redundant, and\ncomplex database installations.\nKey Decision: Microservices Can Share Physical Database Clusters\nMicroservices can and should share physical installations of database clusters. As long\nas services never share the same logical table space and never modify the same data,\nsharing physical installations is OK, in practice.\nIndependence of data management is more about not crossing the streams than any‐\nthing else. It’s about the ability to take your microservices and deploy with another\ndatabase installation if you need to. But you don’t have to deploy each service with a\ndifferent database cluster out of the gate. Cost is an important consideration and so is\nsimplicity. As long as multiple microservices are not accessing (most importantly:\nmodifying) the same data space, the data independence requirement is intact.\n78 \n| \nChapter 5: Dealing with the Data",
      "content_length": 2552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "Data Embedding and the Data Delegate Pattern\nLet’s look at an example in the context of our online reservation system. In the begin‐\nning we will look at the case in which the system is built with a conventional mono‐\nlithic, N-tier architecture. Now, such an application would still be divided into\ndifferent, smaller modules. These modules could even be deployed as networked\nservices. And they could definitely be small enough to be called “micro.” That does\nnot necessarily mean they are microservices, however. They can only be considered\nmicroservices if these components were modularized with the elimination of coordi‐\nnation as the goal and, more specifically, if they are loosely coupled and independ‐\nently deployable. If services are split arbitrarily and not loosely coupled, we can’t call\nsuch a system an example of a microservices architecture.\nIn our scenario, depicted in Figure 5-2, we have three services all requiring data from\nthe “flights” table: flight search, reservations management, and flight tracking.\nFigure 5-2. Example of a monolithic data management, characterized by data sharing\nClearly, based on our earlier analysis in this chapter, this data design is problematic\nfor a microservices architecture, because three services are sharing the data space and\nthus are compromising independent deployability.\nHow can we fix this situation? This particular case is actually quite easy to resolve,\nand we can employ the simple technique of hiding shared data behind a delegate ser‐\nvice, visualized in Figure 5-3.\nMicroservices Embed Their Data \n| \n79",
      "content_length": 1579,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "Figure 5-3. A simple graphic representation of data hiding via a delegate\nEssentially, what we do here is declare the flight inventory service to be the authorita‐\ntive service for all things related to flight information. Further, any service that\nrequires information about flights or needs to update information about flights is\nrequired to invoke an appropriate endpoint in the flight inventory service. If we\nimplement a sufficiently flexible flight lookup API call in the flight inventory service,\nthe former flight search service just becomes part of the functionality of the new\nflight inventory service. More importantly, this allows us to stop accessing the flights\ntable directly from the reservations and flight tracking services. Any information they\nneed about a flight they can obtain through the flight inventory service, going\nforward.\nFor example, when the reservations system needs to know if there are enough seats\nleft on a flight, it will send a corresponding query to the flight inventory service\ninstead of querying the flights table directly in the database. Or when the flight track‐\ning service needs to know or update the location of the plane in a flight, it will again\ndo so via the flight inventory service, not by accessing and modifying the flights table\ndirectly. This way the flight inventory service can be the delegate that hides data\nbehind itself, encapsulates the data, and wraps around the data. This will stop multi‐\nple services from sharing the same data table.\nPlease note that in this pattern, when several services need access to the same data, we\ndon’t have to necessarily convert one of them into a delegate. In the previous solution\nwe converted the flight search service into an inventory service and made it encapsu‐\nlate the flights table. We could have instead introduced a new service. For example,\nwe could introduce a new service called flight inventory and have the flight search\nmicroservice refer to it, just like reservations and tracking services do.\nThe approach of introducing a delegate is very elegant and will work in many differ‐\nent cases. Unfortunately, not all data-sharing needs can be addressed this way. It\nwould be extremely naive to think that the pattern we just discussed works for all\n80 \n| \nChapter 5: Dealing with the Data",
      "content_length": 2303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "scenarios. There are use cases where the required functionality legitimately needs to\naccess or modify data across the boundaries of microservices. Examples of such needs\nare found in the analytics, data audit, and machine learning contexts, among others.\nTraditional approaches to database transactions also require locking in on shared\ndata.\nFortunately, there are reasonable solutions for those other use cases as well, solutions\nthat are also capable of avoiding data sharing. To understand the solutions in this\nspace, let’s first explore various data access patterns we commonly encounter.\nUsing Data Duplication to Solve for Independence\nWhen we need read-only access to distributed data with no modification require‐\nments, as in the contexts of enterprise analytics, machine learning, audits, etc., a com‐\nmon solution is to copy datasets from all concerned microservices into a shared\nspace. The shared space is usually called a data lake. Please note that we are copying\ndata, not moving it! Data lakes are read-only, query-able data sinks. Microservices\nstill remain the authoritative sources of the corresponding datasets and act as the pri‐\nmary owners of the data. They just stream relevant data into the data lake where it\naccumulates and becomes ready for querying. For the sake of data integrity and\nclarity of data lineage, it’s important that we never operationally update such data in\nan aggregate index like a data lake. Data lakes may never be treated as the databases of\nrecord. They are reference data stores. We can see a generic graphic representation of\nthis setup in Figure 5-4.\nFigure 5-4. Streaming data from microservices into data lakes\nOnce data is streamed from the system of record (SOR) data stores (such as microser‐\nvices into data sinks), the aggregate data is indexed in a way that is optimized for\nquery-ability. Streaming data from SORs into data lakes is usually done using a relia‐\nble messaging infrastructure. IBM MQ and RabbitMQ have been used for many years\nin this context; Kafka seems to be the current most popular solution, while Apache\nPulsar is probably the most prominent and interesting new entrant in the space.\nData lakes and shared data indexes can solve for many read-only use cases. But what\nshould we do when distributed data is not read-only? In the next section we explore a\nsolution for the cases when we need to modify data in a coordinated fashion across\nMicroservices Embed Their Data \n| \n81",
      "content_length": 2460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "the datasets owned by multiple microservices. We will discuss how to implement dis‐\ntributed transactions in a microservices ecosystem.\nDistributed Transactions and Surviving Failures\nLet’s consider an example from our online reservations sample project. Specifically,\nlet’s explore what happens when somebody books a seat on a flight and to fulfill the\nbooking we need to execute a distributed transaction. This is a coordinated update\nacross multiple microservices that are in charge of things like using loyalty miles for\npayment, securing a seat, and sending an itinerary to the customer’s email. Such\ntransactions span multiple microservices: payments (with loyalty points) processing,\nreservations, and notifications, to be specific. Most importantly, we would typically\nwant all three steps to happen or none of them to happen. For instance, let’s say we\nsuddenly find out that the requested seat is no longer available. Perhaps when we\nstarted the process of deducting the miles for the payment, the seat was available, but\nby the time we finished the process, somebody had already reserved that seat. Obvi‐\nously we can’t reserve this seat twice, so we must consider what to do in that situation.\nIn a busy-enough system, such race conditions and failures are inevitable, so when\nthey do occur, we need to roll back the entire process. We clearly need to refund the\nloyalty points, at the very least. Let’s understand how we would coordinate such a dis‐\ntributed transaction.\nIn conventional monolithic applications, a process like the one we described would\nbe safely managed using database transactions. More specifically, this can be done\nwith database transactions that are said to exhibit the ACID characteristics of safety,\neven in the event of failures. ACID stands for atomicity, consistency, isolation, and\ndurability. These are defined as follows:\nAtomicity\nThe steps in a transaction are “all or nothing”; either all of them get executed, or\nnone of them.\nConsistency\nAny transaction should bring the system from one valid state into another valid\nstate.\nIsolation\nParallel execution of various transactions should result in the same state as if the\ntransactions were executed sequentially.\nDurability\nOnce a transaction is committed (fully executed), data won’t be lost despite any\npossible failures.\n82 \n| \nChapter 5: Dealing with the Data",
      "content_length": 2361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "Microservices simplify building systems safely at scale. It is critical to clarify, however,\nthat this doesn’t mean you can somehow prevent a failure from ever occurring. Com‐\npletely avoiding failures, whether with microservices or by any other means, is an\nimpossible task. Failures will always be present in a sufficiently complex system. What\nwe need to do is account for them and create the means for auto-recovery. In conven‐\ntional data management, ACID transactions are a great example of such thinking.\nSystems implementing ACID transactions assume that failures of all kinds happen all\nthe time, so we design our data storage systems in a way to make them resilient to the\nfailures.\nUnfortunately, ACID transactions are not a viable solution for distributed systems in\nwhich functionality is spread across multiple microservices deployed across a net‐\nwork independently. ACID transactions typically rely on usage of exclusive locks.\nGiven that microservices embed their data and do not allow code to manipulate data\nin another microservice, such locks would be either impossible or very expensive for\na microservices system to implement. Instead, we need to use patterns that work bet‐\nter in distributed architectures. In the next section we introduce a popular solution of\nthis type called saga transactions.\nDistributed transactions with sagas\nSagas were first described by Hector Garcia-Molina in 1987, long before modern dis‐\ntributed systems were a thing, and were later popularized by Clement Vasters’s 2012\nblog post as an effective solution for distributed systems.\nWith sagas, every step of a transaction not only performs the required action for that\nstep, but it also defines a compensating action that should execute if we need to roll\nback the transaction due to a later failure. A pointer (e.g., discovery information on a\nqueue) to this compensating action is registered on a routing slip and passed along to\nthe next step. If one of the later steps fails, it kicks off execution of all compensating\nactions on the routing slip, thus “undoing” the modifications and bringing the system\nto a reasonably compensated state.\nSagas Are Not Directly Equivalent to ACID Transactions\nSagas do not promise that when a distributed transaction is rolled\nback, the system will necessarily get back to the initial state. Rather,\nthe system should get to a reasonable state that reflects an accepta‐\nble level of undoing of the partially completed transaction.\nTo understand what we mean by “reasonable state,” let’s look at our initial example of\na seat reservation, as depicted in Figure 5-5.\nMicroservices Embed Their Data \n| \n83",
      "content_length": 2645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "Figure 5-5. A transaction distributed across multiple microservices\nIf a reservation booking attempt fails, for whatever reason, it will be cancelled, but we\nwill also invoke the compensating action of the previous steps: notification and pay‐\nment. The compensating action of a payment refunds money to the customer.\nDepending on the type of payment, a refund may not get processed immediately; so\nthe system may not return to its initial state immediately, but eventually the customer\nwill get all their money. That said, it will be visible that two payment actions cancelled\n(compensated) each other, rather than the customer not noticing any trace of transac‐\ntion reversal at all, which would have been the goal with ACID transactions.\nIn case of a notification things can become even messier. We may not be able to liter‐\nally recall an email or a text message that was sent, so the compensating transaction\nmay involve sending a new message notifying the customer that the previous message\nshould be disregarded and booking was actually unsuccessful. In some circumstan‐\nces, it can be a reasonable solution (if not, we will see later in the chapter how to\navoid it with proper sequencing), but doesn’t bring the system back to the initial state:\na customer will see two messages, instead of not seeing any.\nThese two examples hopefully gave a clear understanding of some of the ways in\nwhich compensating transactions, in sagas, are different from the conventional ACID\ntransactions.\nThe Sequence of Events in a Saga Is Meaningful\nNote that the sequence of events in a saga does matter and should\nbe constructed carefully. It usually pays off to move steps that are\nharder to compensate for toward the end of the transaction. For\ninstance, if business rules allow it, moving a notification to the very\nend of the process may save us from having to send a lot of correc‐\ntion messages. This way, by the time the transaction gets to sending\nalerts, we will know that the previous steps have succeeded.\n84 \n| \nChapter 5: Dealing with the Data",
      "content_length": 2047,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "1 Greg Young, “CQRS and Event Sourcing,” Code on the Beach 2014, https://oreil.ly/5-d5u.\nDelegate services, data lakes, and sagas are powerful patterns. They can solve many\ndata isolation challenges in microservices architectures, but not all of them. In the\nnext section we will discuss a powerful duo of design patterns: Event Sourcing and\nCQRS. These can pretty much address everything else remaining, providing a com‐\nplete toolset for data management in a microservices environment.\nEvent Sourcing and CQRS\nUp to this point we have discussed some ways to avoid data sharing when using tradi‐\ntional, relational data modeling. We showed how you can solve some of the data-\nsharing challenges, but eventually, for advanced scenarios, we will run into cases\nwhere relational modeling itself falls short of allowing the desired levels of data isola‐\ntion and loose coupling. A very common example is when teams need to create a\n“join” across datasets owned by different microservices. At its core, relational data\nmodeling is rooted in such foundational principles as data normalization, data reuse,\nand cross-referencing common data elements; i.e., it is fundamentally biased toward\nfavoring data sharing.\nEvent Sourcing\nSometimes, rather than trying to go around the predisposition for relational model‐\ning, we should switch to a completely different way of modeling data. A data model‐\ning approach that allows avoidance of data sharing, and thus has become popular in\nmicroservices, is known as Event Sourcing.\nOne of the earliest known mentions of Event Sourcing is in Martin Fowler’s 2005 arti‐\ncle. In 2014, Greg Young gave a seminal conference talk1 about Event Sourcing that\njump-started a new, strong wave of popularity for the design pattern. Greg has been\nan important voice and one of the key advocates in this space. We owe him a lot of\ngratitude for the advancement of Event Sourcing (and its relationship with CQRS,\nanother important pattern that we will discuss later in this chapter). In Greg’s own\nwords, Event Sourcing is an approach to data modeling that is all about storing events\nrather than the states of the domain objects of a system:\nEvent Sourcing is all about storing facts and any time you have “state” (structural mod‐\nels)—they are first-level derivative off of your facts. And they are transient.\nEvent Sourcing and CQRS \n| \n85",
      "content_length": 2363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "In this context, by “facts,” Young means the representative values of event occur‐\nrences. An example could be “the price of an economy seat on the LAX–IAD flight\nincreased by $200.”\nEvent Sourcing in accounting and chess\nUnless you have prior experience with Event Sourcing, this may feel odd. Most people\nwho haven’t worked on systems dealing with high-frequency trading platforms or\nhaven’t had a ton of advanced experience with microservices probably do not have\nany experience with Event Sourcing. That said, we can easily find examples of Event-\nSourcing systems in real life. If you’ve ever seen an accounting journal, it is a classic\nevent store. Accountants record individual transactions, and the balance is a result of\nsumming up all transactions. Accountants are not allowed to record “state”; i.e., they\njust write down the resulting balance after each transaction, without capturing the\ntransactions themselves. Similarly, if you have played chess and have recorded a chess\ngame, you would not write down the position of each piece on the board after each\nmove. Instead you are recording moves individually, and after each move the state of\nthe board is a result of the sum of all moves that have happened.\nFor instance, consider the record of the first seven moves of the historical game 6\nbetween many-times world chess champion Gary Kasparov and the IBM supercom‐\nputer Deep Blue, in 1997. Represented in algebraic notation, looks like the following:\n1. e4 c6\n2. d4 d5\n3. Nc3 dxe4\n4. Nxe4 Nd7\n5. Ng5 Ngf6\n6. Bd3 e6\n7. N1f3 h6\nThe corresponding state after the initial seven moves is depicted in Figure 5-6.\nWe can completely re-create the state of a chess game, such as that between Kasparov\nand Deep Blue, if we have the event log of all moves. This is an analog equivalent of\nEvent Sourcing from real life.\n86 \n| \nChapter 5: Dealing with the Data",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "Figure 5-6. Deep Blue versus Kasparov (source: Wikipedia)\nEvent Sourcing versus relational modeling\nIn conventional data systems such as relational databases, or even the more contem‐\nporary NoSQL and document databases, we usually store a state of something; for\ninstance, the current price of an economy seat on a flight. In Event Sourcing, we oper‐\nate with a completely different approach. In Event Sourcing we do not store current\nstate, rather we store facts; the incremental changes of the data. The current state of\nthe system is a derivative, a value that is calculated from the sequence of changes\n(events).\nEvent Sourcing and CQRS \n| \n87",
      "content_length": 648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "Let’s look at an example. A relational data model describing the customer manage‐\nment system for a flight reservations system may look like the diagram in Figure 5-7.\nFigure 5-7. Example of a relational data model\nWe can see that the data model could consist of a table storing customers’ contact\ninformation, which has one-to-many relationships with customer accounts and pay‐\nment methods. In turn, each customer account (e.g., business versus personal\naccounts) record can point to multiple completed trips, open reservations, and pref‐\nerences related to the account. While the details may vary, this is the kind of data\nmodel most software engineers would design using conventional databases.\nUsing Event Sourcing, we can design the same data model as a sequence of events,\nshown in Figure 5-8. Here you can see a representation of events that led to the same\nstate of the system as described in the state-oriented model earlier: first, the customer\ncontact information was collected, then a personal account was opened, which was\nfollowed by entering a personal payment method. After several reservations, and\ncompleted trips, this customer apparently decided to also open a business account;\nthey added payment information and started booking trips with this new account.\nAlong the way several preferences were also added and updated, bringing the system\nto the same state as in Figure 5-7, except here we can see the exact sequence of “facts”\nthat led to the current state, as opposed to just looking at the result in the state-\noriented representation.\nSo the sequence of events on the diagram gives you the same state that we had in the\nrelational data model. It is equivalent to what we had there, except this looks very,\nvery different. For instance, you may notice it looks much more uniform. There are\nsignificantly fewer opinionated decisions to be made about the various entity types\nand their relationships with each other. Event Sourcing in some ways is much simpler\nin that you just have a variety of business events that happen and then you can calcu‐\nlate the current state as a derivative of these events.\n88 \n| \nChapter 5: Dealing with the Data",
      "content_length": 2168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "Figure 5-8. Example of an event-sourced data model\nNot only is Event Sourcing more straightforward andpredictable, in Event Sourcing\nthere are no referential relationships between various entities. If we wanted to do a\nbrute force data segregation here, all we would need to do is say each type of event is\nowned by a different microservice, and voila, we could do that and avoid data shar‐\ning. For instance, we could have a microservice that is a customer demographics\nmicroservice and “customer info entered” would be an event that very naturally\nbelongs to that system of record.\nWhat does an event look like?\nNow that we hopefully have a good intuition about Event Sourcing and how it works,\nlet’s dig a little bit deeper into what data modeling and data management looks like in\nEvent Sourcing. It’s an approach of capturing the sequence of events. The state is just\nsomething you calculate off of these events—a state is a function of events. OK, this\nsounds a bit mathematical, but what does an event even look like? Well, events are\nvery simple. If we look at the “shape” of an event data structure, all it needs to have\nare three parts.\nFirst, the event needs some kind of unique identifier. You could for instance use a\nuniversally unique identifier (UUID), since they are globally unique, and this unique‐\nness obviously helps in distributed systems. It also needs to have an event type, so we\ndon’t mistake different event types. And then there’s just data, whatever data is rele‐\nvant for that event type:\n{\n  \"eventId\" : \"afb2d89d-2789-451f-857d-80442c8cd9a1\",\n  \"eventType\" : \"priceIncreased\",\n  \"data\" : {\n    \"amount\" : 120.99,\n    \"currency\" : \"USD\"\n  }\n}\nEvent Sourcing and CQRS \n| \n89",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "Design decisions of a technical nature are fairly straightforward when working with\nevents. Most work goes into properly describing domain-relevant fields of the events,\nbased on the business logic. There’s much less of the kind of subjective, technical\ntable-shape and relationship crafting that we engage in with the relational approach.\nCalculating current state with projections\nWhat happens when we actually need to calculate the point-in-time (e.g., current)\nstate of something? We run what in Event Sourcing is called projections. Projections\ngive us state based on events, and they’re also fairly simple. To run a projection, we\nneed a projection function. A projection function takes the current state and a new\nevent to calculate the new state.\nFor instance, a priceUp projection function, for an airline ticket price, may look like\nthe following:\nfunction priceUp(state, event) {\n  state.increasePrice(event.amount)\n}\nIt would be equivalent to an UPDATE prices SET price=… SQL query in a relational\nmodel. If we also had a corresponding price decrease projection function and we\nwanted to calculate price (state) at some point, we could run a projection by calling\nthe projection functions for all relevant events, like the following:\nfunction priceUp(state, event) {\n  state.increasePrice(event.amount)\n}\nfunction priceDown(state, event) {\n  state.decreasePrice(event.amount)\n}\nlet price = priceUp(priceUp(priceDown(s,e),e),e);\nIf you have ever worked with functional programming, you may notice that the cur‐\nrent state is the left fold of the events that occurred until the current time. Note that\nthrough using Event Sourcing you can calculate not just the current state but the state\nas of any point in time. This capability opens up endless possibilities for sophisticated\nanalytics, where you can ask questions like, “OK, I know what the state of the entity is\nnow, but what was the state at a date in the past that I am interested in?” This flexibil‐\nity can become one of the powerful benefits of using Event Sourcing, if you frequently\nneed to answer such questions.\n90 \n| \nChapter 5: Dealing with the Data",
      "content_length": 2127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "Improving Performance with Rolling Snapshots\nOne thing to note here is that projections can be computationally expensive. If a\nvalue is a current state and it’s the result of a sequence of thousands of state changes,\nlike a bank account balance, whenever you need the value of a current balance, would\nyou want to calculate it from scratch? You could argue that such an approach is slow\nand it can waste time and computational resources. It also cannot be as instantaneous\nas just retrieving the current state. You would be correct in that; however, we can\noptimize for speed and it doesn’t necessarily require a change in the approach.\nInstead of recalculating everything from the beginning—for example, the opening of\na bank account—we can keep saving intermediary values, along the way, and later we\ncan quickly calculate the state from the last snapshot. That would significantly speed\nup calculations.\nDepending on the event store implementation, it is common to snapshot intermedi‐\nary values at various time points. How to choose the appropriate moment of snap‐\nshotting may depend on your application’s domain. For instance, in a banking system,\nyou may snapshot account balances on the last day of every month, so that if you\nneed the balance on January 15, 2020, you will already have it from December 31,\n2019, and will just need to calculate the projection for two weeks, rather than the\nentire life of the bank account.\nIn Event Sourcing, the saved projections are usually called rolling snapshots. The\nspecifics of how you implement rolling snapshots and projections may depend on the\ncontext of your application. For instance, when we used monthly rolling snapshots\nfor the banking application example earlier, it made a lot of sense because this\napproach closely aligns with what happens in real life anyway. Banks calculate various\ntypes of balances at the end of the month, quarter, and year; this is known as “closing\nthe books.” You should always try to find natural time points in your own domains\nand align your snapshots with them.\nLater in this chapter we will see that with a pattern called Command Query Responsi‐\nbility Segregation (CQRS), we can do much more than just cache states in rolling\nsnapshots.\nHaving acquired an understanding and appreciation of Event Sourcing, let’s learn\nmore about how to implement it. What would the event store itself look like? And\nhow would we go about implementing one? We will answer these questions in the\nnext section.\nEvent Sourcing and CQRS \n| \n91",
      "content_length": 2516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "Event Store\nEvent stores can be relatively simple systems. You can use a variety of data storage\nsystems to implement one. Simple files on the filesystem, Amazon Simple Storage\nService (S3) buckets, or any database storage that can reliably store a sequence of data\nentries can all do the job. The interface of an event store needs to support three basic\nfunctions:\n• The ability to store new events and assign the correct sequence so we can retrieve\nevents in the order they were saved\n• The capability to notify event subscribers who are building projections about\nnew events they care about and enable the Competing Consumers pattern\n• The ability to get N number of events after event X for a specific event type, for\nreconciliation flows; i.e., recalculation in case projection is lost, compromised, or\ndoubted\nSo, at its essence, the basic interface of an event store is comprised of just two\nfunctions:\nsave(x)\ngetNAfterX()\nIn addition, there is a kind of robust notification system that allows consumers to\nsubscribe to events. By “robust” we mean conformation to the Competing Consumers\npattern. This pattern is important because whatever system is building a projection\noff of your events will likely want to have multiple instances of a client “listening” to\nthe events, both for redundancy and scalability’s sake. Our notifier must reasonably\naccommodate only-once delivery to a single instance of a listener, to avoid accidental\nevent duplication leading to data corruption. There are two approaches you can\nemploy here:\n1. Use a message queue implementation that already provides such guarantees to its\nconsumers; e.g., Apache Kafka.\n2. Allow consumers to register HTTP endpoints as callbacks. Invoke the callback\nendpoint for each new event and let a load balancer on the consumer side handle\nthe distribution of work.\nNeither approach is inherently better. One is push-based and the other is pull-based,\nand depending on what you are doing, you may prefer one over the other.\n92 \n| \nChapter 5: Dealing with the Data",
      "content_length": 2031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "Check Out a Sample Implementation\nDuring the writing of this book, we published an opinionated ref‐\nerence implementation of a skeleton event store on GitHub that\nyou can check out, take for a test drive, or contribute to.\nTo implement robust projections, Event Sourcing systems often use a complementary\npattern known as CQRS. In the next section we will explore the ideas behind it and\ntry to understand its essence.\nCommand Query Responsibility Segregation\nProjections for advanced event-sourced systems are typically built using the Com‐\nmand Query Responsibility Segregation (CQRS) pattern. The idea of CQRS is that\nthe way we query systems and the way we store data do not have to be the same.\nWhen we were talking about the event store and how simple it can be, one thing we\nskipped over was that the simple interface of save(x) and getNAfterX() functions is\nnot going to allow us to run elaborate queries over that data. For instance, it won’t\nallow us to run queries asking for all reservations in which a passenger has updated\ntheir seat in the last 24 hours. Those kind of queries are not implemented against the\nevent store, to keep the event store simple and focused. Event Sourcing should only\nsolve the problem of authoritatively and reliably storing an event log. For advanced\nqueries, every time an event occurs, we let another system, subscribed to the event\nstore, know about it and that system can then start building the indices that are opti‐\nmized for querying the data any way they need. The idea behind CQRS is that you\nshould not try to solve data storage, data ownership, and data queryability issues with\nthe same system. These concerns should be solved for independently.\nThe big win with using Event Sourcing and CQRS is that they allow us to design very\ngranular, loosely coupled components. With Event Sourcing, we can create microser‐\nvices so tiny that they just manage one type of event or run a single report. Targeted\nuse of Event Sourcing and CQRS can take us to the next level of autonomous granu‐\nlarity in microservices architectures. As such, they play a crucial role in the architec‐\ntural style.\nEvent Sourcing and CQRS Should Not Be Abused as a Cure-All Solution\nBe careful not to overuse Event Sourcing and CQRS. You should\nonly use them when necessary, since they can complicate your\nimplementation. They should not be used as the one and only data\nmodeling approach for your entire system. There are still many use\ncases in which the conventional, relational model is much simpler\nand should be utilized.\nEvent Sourcing and CQRS \n| \n93",
      "content_length": 2583,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "2 Eric Brewer, “CAP Twelve Years Later,” InfoQ, 2012, https://oreil.ly/Pg1pO, and Coda Hale, “You Can’t Sacri‐\nfice Partition,” 2010, https://oreil.ly/nHBoN.\nEvent Sourcing and CQRS can help you avoid data sharing between microservices in\nsophisticated cases where you require data joins across service boundaries, but they\ncome with a cost of complexity. Always consider other, simpler approaches, such as\nthe delegate service we described in this chapter, before you resort to Event Sourcing,\nfor a particular microservice.\nNow that we have acquired a solid, foundational understanding of Event Sourcing\nand CQRS, let’s also address where else these patterns can and should be used,\nbeyond just helping with loose data coupling for the data-embedding needs of\nmicroservices.\nEvent Sourcing and CQRS Beyond Microservices\nEvent Sourcing and CQRS can certainly be invaluable in avoiding data sharing and\nachieving loose coupling of microservices. Their benefit is not limited to loose cou‐\npling or even microservices architectures, however. Event Sourcing and CQRS are\npowerful data modeling tools that can benefit a variety of systems.\nConsider Event Sourcing and CQRS in relation to the consistency, availability, and\npartition tolerance (CAP) theorem. This theorem was famously formulated as a con‐\njecture by Eric Brewer in his 2000 keynote at the Symposium on Principles of Dis‐\ntributed Computing. The theorem, in its original form, stated that any distributed\nshared-data system can only have two out of three desirable properties:\nConsistency\nHaving a single view of the latest state of the data\nAvailability\nAbility to always read or update data\nPartition tolerance\nGetting accurate data even in the face of network partitions\nOver time, it was clarified that not all combinations of CAP are valid.2 For a dis‐\ntributed system we have to account for partition tolerance because network partitions\ncannot be avoided, and the choice becomes a sacrifice between consistency or availa‐\nbility. But what do we do if we really need both? It sounds childish to insist on want‐\ning everything if a mathematically proven theorem (which CAP became eventually)\ntells you that you cannot have it all.\nBut there is a catch! The CAP theorem tells us that a single system, with data sharing\ncannot violate the theorem. However, what if, using CQRS, we employ multiple sys‐\n94 \n| \nChapter 5: Dealing with the Data",
      "content_length": 2406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "tems and minimize data sharing? In such a case, we can prioritize consistency in the\nevent store and prioritize availability in the query indices. Certainly, that means what‐\never system we use for query indices may get consistency wrong, but they are not\nauthoritative sources, so we can always re-index from the event store, if need be. In a\nway, this allows us to, indeed, have the best of both worlds.\nThe second major benefit of the Event Sourcing and CQRS approach is related to\nauditability. When we use a relational data model, we do in-place updates. For\ninstance, if we decide that the customer’s address or phone number is wrong we will\nupdate it in the corresponding table. But what happens if the customer later disputes\ntheir record? With a relational model, we may have lost the history and find ourselves\nhelpless. With Event Sourcing, we have a perfect history of every change safely pre‐\nserved and we can see what the value of customer data was at any time in the past, as\nwell as how and when it got updated.\nSome readers may point out that even when they use relational modeling, it doesn’t\nnecessarily mean that they lose the history of data. They may be logging every change\nin some file, or systems like Splunk or ELK. So, how is logging different from Event\nSourcing? Are we just talking about good old logging here, branding it with some\nnew buzz name? The answer is absolutely not. It all comes down to: which system is\nthe source of truth in our architecture? Who do I “trust” if my log disagrees with my\ncurrent state? In Event Sourcing, the “state” is calculated from the events, so the\nanswer is self-evident. For Splunk logs that is not the case, so your source of truth is\nmost likely your relational model, even if you occasionally double-check it from the\nlogs to hunt down some bugs. When your reliable log of events is your source of\ntruth, you are using Event Sourcing as your data modeling approach. Otherwise, you\nare not, no matter how many logs you may be generating.\nSummary\nIn this chapter we discussed a fundamental concept of microservices architectures:\ndata isolation and the principle of embedding data into corresponding microservices.\nWe also explored how this principle, while necessary for loose coupling and inde‐\npendent deployability, can lead us to significant data management challenges if we\napproach them with conventional data modeling solutions, the ones designed for\nmonolithic N-tier applications. Further, we looked into a complete toolset of solu‐\ntions to the described challenges in the form of powerful, tried-and-true patterns that\naddress those challenges head-on. Last but not least, we introduced a new approach\nto data modeling, which is distinctly different from conventional, relational\nmodeling. We explained the benefits and appropriate usage contexts of Event Sourc‐\ning and CQRS, even beyond microservices needs.\nArmed with this powerful, foundational knowledge, we can now dive deep into the\nimplementation of our sample project. We will first start by setting up an automated,\nSummary \n| \n95",
      "content_length": 3073,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "containerized infrastructure and deployment pipelines for the project. This step is\ncrucial for tackling the operational complexities of a microservices-based project.\nThen we will share detailed guidance on creating a productive and repeatable devel‐\noper workspace—a critical foundation for creating an enjoyable developer experience\nin a heterogeneous environment. Finally, we will try to implement code for a couple\nof microservices of our sample project, leveraging all of the insights we have learned\nso far.\n96 \n| \nChapter 5: Dealing with the Data",
      "content_length": 554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "CHAPTER 6\nBuilding an Infrastructure Pipeline\nIn this chapter, we’ll establish the foundation for our infrastructure work. We’ll start\nby setting up an Amazon Web Services (AWS) account. Following that, we’ll set up a\ntool called a continuous integration and continuous delivery (CI/CD) pipeline to\nautomate infrastructure changes. With these tools, we’ll be able to define and provi‐\nsion microservices infrastructures throughout the book.\nEarlier, in Chapter 2, we established a platform team responsible for delivering the\nmicroservices infrastructure. We decided that this team would offer the infrastructure\nas a service. That meant that other teams should be able to use the infrastructure in a\nself-service manner, without having to coordinate heavily with the platform team.\nEnabling the “as a service” model requires some up-front investment. That’s what the\ntools in this chapter will help address.\nIn order to reduce the work that our microservices teams need to do, we’ll need to\nmake it easy for teams to move their code from local workstations onto a hosted\ninfrastructure. So we’ll need to lower the barrier for teams to be able to provision\nenvironments and deploy their services into a hosted system. We’ll need to make it\ncheap and easy to create a new environment and provide the right kit to make relea‐\nses safe and easy.\nIn practice, achieving those goals is difficult if you don’t have a good way of improv‐\ning the way you make changes to the infrastructure itself. If we can reduce the effort\ncost of building and changing the infrastructure, we’ll be able to deliver new environ‐\nments more easily and put more focus on improving the infrastructure to meet our\nsystem goals.\nThankfully, we don’t have to invent a solution for infrastructure changes by ourselves.\nWe have the luxury of being able to draw on the principles and philosophies of\nDevOps. In particular, using the DevOps practices of infrastructure as code (IaC), CI,\n97",
      "content_length": 1957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "and CD will help us achieve our objectives. We’ll be able to make infrastructure\nchanges faster, cheaper, and safe, and scale the work of building environments across\nour microservices teams.\nDevOps and Microservices\nThe goal of DevOps is the pursuit of improvements to the way that\nsoftware is developed, released, and supported. Working towards\nthat goal can span the domains of organizational design, culture,\nprocess, and tooling. The microservices style of architecture shares\na similar overall goal, but adds the additional characteristic of\nbounded services and independent deployment and management.\nMicroservices and DevOps go hand in hand—in fact, it would be\nextremely challenging to build applications in the microservices\nstyle without adopting DevOps practices.\nAdopting DevOps practices means that we get to take advantage of a bountiful eco‐\nsystem of tools for code management, build management, and releases. Using these\ntools will greatly reduce the time it takes to get our infrastructure solution up and\nrunning. By the end of this chapter, we’ll have set up a cloud-based toolchain that we\ncan use to build a microservices infrastructure. We’ll have an IaC repository, a starter\nfile for the infrastructure code, a pipeline for testing and building (see Figure 6-1),\nand a cloud foundation that we can build environments within.\nFigure 6-1. The target pipeline\nBut before we start building the pipeline, let’s take a look at the DevOps-based princi‐\nples and practices we’ve used to inform the design.\nDevOps Principles and Practices\nBuilding software in the DevOps way helps you reduce the time it takes to make\nchanges to your applications, without introducing additional risk. When you do it\nright, it gives you both change speed and change safety at the same time.\nThat’s exactly the benefit we want to provide with our infrastructure toolchain. If we\ncan improve the speed and safety of infrastructure changes, we can do more of them.\nWe’ll also be able to make more improvements more often and offer a better platform\nservice to our microservices teams.\n98 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 2135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "To make this happen, we’ll use three concepts from the DevOps world in our infra‐\nstructure platform:\n• Immutable infrastructure\n• IaC\n• CI and CD\nLet’s take a look at each of these ideas in more detail to understand how they’ll help\nus, starting with the principle of immutable infrastructure.\nImmutable Infrastructure\nAn object is immutable if it can’t be changed after it’s created. The only way to update\nan immutable object is to destroy it and create a new one. Things that are immutable\ncontain behavior and structures that are easier to predict and reproduce because they\ndon’t change. For example, in programming, an immutable data type would let you\nassign a value when it’s created but never let you change it. If you created a data type\ncalled x with a value of 10, you could be sure that it will always be 10 forever more.\nThis predictability can make activities like testing and replication of these objects\neasier.\nThe immutable infrastructure principle is an application of this immutability prop‐\nerty on infrastructure components. Suppose we were to set up and install a network\nload balancer with a set of defined routes. If we apply the immutability principle, the\nnetwork routes we’ve defined can’t be changed without destroying the load balancer\nand making a new one.\nThe main advantage of applying immutability here is to create predictable and easily\nreproducible infrastructures. In traditional systems, human operators need to do a lot\nof manual work to get things running. They patch systems, alter configurations, and\nstop and start processes. Servers and devices are kept continually running and the\noperator shapes the environment so that the application can run. When there are\nmultiple environments and servers, the operator needs to shape them all.\nBut, over time, as more changes are applied (often inconsistently), the state of these\nsystems drift. It becomes increasingly difficult to keep all of the servers running in the\nsame state. Introducing new servers or making changes to environment states\nbecomes a problem because of this variability and unpredictability. This unpredicta‐\nbility means that more expertise and manual effort is required, which slows down\ndelivery and makes it difficult to offer the infrastructure platform in a self-service\ntool as we outlined in our operating model.\nThat’s where immutable infrastructure comes in. By adopting the principle of immut‐\nability, we can create an infrastructure that is highly predictable and easy to replicate.\nDevOps Principles and Practices \n| \n99",
      "content_length": 2546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "This lends itself very well to the model we’re targeting. So, let’s make that our first key\ndecision for our infrastructure foundation.\nKey Decision: Apply the Principle of Immutable Infrastructure\nInfrastructure components must not be changed after they’ve been created. Changes\nmust be made by re-creating the component (and any dependent components) with\nthe new or altered properties.\nThe decision we’ve just made comes with a trade-off: the cost of destroying and re-\ncreating configurations. So we’ll need to make some additional decisions to make this\nprocess easier. Otherwise, we’ll end up never making infrastructure changes because\nit will be too difficult and too costly. The first decision we’ll make is one we’ve alluded\nto earlier in the book. We’ll build our platform in the cloud.\nKey Decision: Implement the Infrastructure in the Cloud\nInfrastructure components will be deployed and managed in a cloud platform.\nThis decision to build our microservices infrastructure in the cloud is an important\nenabler for our immutable infrastructure. Without it, the cost of physical hardware\nacquisition, server management, and software procurement would bury us in com‐\nplexity and cost. But in the cloud, the infrastructure components are virtual. With\nvirtuality, we can treat the infrastructure the same way we treat software. It gives us\nthe freedom to create and destroy servers and devices in the same way as we might do\nwith a software component or an object in a object-oriented system.\nImmutable infrastructure will help us avoid server drift and improve our ability to\nreplicate and instantiate new environments with a production-like state. However,\nwe’ll still need a way of defining all of our infrastructure with a manageable set of\nconfigurations. That’s where the principle of IaC can help.\nInfrastructure as Code\nIaC is based on a single powerful constraint: all infrastructure changes must be repre‐\nsented as a set of machine-readable files (or code). Teams that apply this contraint can\npoint to a group of files that define the target state for their infrastructure, and can re-\ncreate an environment by reapplying the code that created it. Managing the infra‐\nstructure code becomes a way of managing the infrastructure state. Ultimately\nadopting the principle of IaC means we can manage changes to our environments by\nmanaging the way we write, test and deploy our infrastructure code.\n100 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 2471,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "IaC is also very important for enabling our immutable infrastructure. Immutability\nrequires us to manage definitions for objects so they can be changed through re-\ncreation. There are plenty of ways to do that, but IaC lets us treat infrastructure the\nway we treat applications. With IaC, creating and changing components is similar to\nrunning a program. We’ll get to apply our know-how from the application develop‐\nment world to the infrastructure.\nIaC is a good fit for the system we’re trying to build, so let’s formalize this decision\nwith an ADR.\nKey Decision: Adopt IaC\nAll infrastructure changes should be made in managed code files. Changes should not\nbe made manually by human operators outside of the code.\nTo get going with an IaC approach, we’ll need a tool that will allow us to define the\nchanges we want to make as machine-readable code files. That tool will also need to\ninterpret our IaC files and apply them to a target environment. Years ago, we might\nhave had to build this tooling ourselves, but now there are lots of tools available that\ncan do this work for us. For our example project, we’re going to use HashiCorp’s Ter‐\nraform to define our changes and apply them to our cloud-based environment.\nAn introduction to Terraform\nTerraform is a popular tool for teams that are employing IaC principles and manag‐\ning their infrastructure in an automated, repeatable way. We’ve had success using Ter‐\nraform in our own projects and our straw poll of practitioners showed that it’s a\npopular choice among other implementers as well. In this model, we’ve chosen to use\nTerraform as the tool for infrastructure changes, so let’s start by documenting that\ndecision.\nKey Decision: Use Terraform for Infrastructure Changes\nWe’ll use HashiCorp’s Terraform tool to manage and apply changes to the platform\ninfrastructure.\nTerraform isn’t the only tool that can help us with infrastructure changes and there\nare plenty of popular alternatives available for use. We’ve chosen to use Terraform\nbecause it applies a declarative approach to infrastructure management. So we get to\ndeclare a target state for the infrastructure and Terraform will do the hard work of\nmaking it happen. That’s quite different from traditional configuration management\napproaches where we need to instruct the tool with step-by-step imperatives.\nDevOps Principles and Practices \n| \n101",
      "content_length": 2373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "Terraform also embraces the principle of immutable infrastructure that we decided to\nadopt earlier. In practice, this enables us to write Terraform code that describes a\ndesired state for an infrastructure component. When we apply our code, Terraform\nwill do the hard work of destroying it and re-creating it in its new form. That includes\ndealing with any dependent objects and destroying and re-creating those as well.\nTo make that magic happen, Terraform needs to keep track of states. It needs to track\nthe current state of the environment so that it can come up with a plan to produce the\nend state that we’ve defined. That state needs to be managed carefully and needs to be\nshared by everyone using the tool. Effective management of a Terraform solution\nmeans managing the state, configuration files, and quality, safety, and maintainability\nof the entire solution (just like we would for a software application).\nIf you want to learn more about Terraform, a good place to start is\ntheir documentation.\nContinuous Integration and Continuous Delivery\nImmutability and IaC make our infrastructure changes more predictable. But, those\npredictable changes may not always be safe. For example, what happens if a small\nchange to a network inadvertently brings down a load balancer in production? Or\nwhat if a change intended only for the development environment accidentally makes\nits way to production and causes an outage?\nOne way to mitigate these risks is to do a lot of checking (and double-checking) for\nevery change. But the problem with this approach is that it slows down our rate of\nchange because of all the validation work we’d need to do. It can also lead to the late\ndiscovery of problems that we should have found a lot earlier in our infrastructure\ndesign and development work. We end up spending a lot of time in a testing phase\nwhere we need to fix a large batch of problems that could fundamentally alter our\ninfrastructure plan.\nA more efficient approach is to apply the DevOps software practices of continuous\nintegration and continuous delivery (CI/CD). Instead of scheduling a big testing\neffort right before making a production change, we’ll continuously integrate our\nchanges into our repository. We’ll also continuously test that our changes work and\nautomatically deliver them. The goal is to get into a rhythm of releasing small, test‐\nable changes instead of a big batch.\n102 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 2456,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "Understanding CI/CD\nIf you want to understand CI/CD and learn how to implement it\neffectively, we recommend the books Continuous Integration by\nPaul M. Duvall, and Continuous Delivery by Jez Humble and David\nFarley (both Addison-Wesley).\nCI/CD practices rely heavily on tooling. Tools allows teams to run higher volumes of\ntests against their code in a more efficient way. There’s usually lots of different tools\nrequired to automate the integrateion and testing of software and infrastructure.\nThat’s why we’ll be using a special kind of tool called a pipeline. A pipeline tool lets\nyou define and manage the steps of a CI/CD process. That way, any code changes that\nwe make can automatically be integrated and delivered in the same way, every time.\nLet’s formalize our decision to use a CI/CD pipeline in this project.\nKey Decision: Apply System Changes with a CI/CD Pipeline\nAll changes must be applied through an automated pipeline and/or tool. There should\nbe no changes introduced through instructions in command line or operator\nconsoles.\nWe’ll be using a pipeline for all changes—not just infrastructure ones. In this chapter,\nwe’ll focus on the pipeline for our infrastructure changes. Later, in Chapter 10, we’ll\ndefine a CI/CD pipeline for the microservices. There are plenty of pipeline tool\noptions available, so we have another decision to make on tool choice. For our model,\nwe’ve decided to use GitHub Actions as our CI/CD pipeline tool.\nKey Decision: Use GitHub Actions for CI/CD Pipelines\nTeams should use GitHub Actions to implement CI/CD pipelines for infrastructure\nand microservices.\nAt the time of this writing, GitHub Actions is a relatively new product and is not as\nfeature rich as more established options like Jenkins and GitLab. We chose GitHub\nActions because we plan to use GitHub to manage our code. Being able to use a single\ntool for code management and CI/CD is attractive. That’s doubly true for this book,\nwhere we’re constrained by the limits of the printed page.\nBy the end of this chapter, we will have built a CI/CD pipeline in GitHub Actions.\nWe’ll also configure the pipeline to handle Terraform code and make changes to a\ncloud-hosted environment. In Chapter 7, we will use our pipeline to provision a\nDevOps Principles and Practices \n| \n103",
      "content_length": 2285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "microservices infrastructure. But the first step is to install some tools and set up a \nworking environment.\nSetting Up the IaC Environment\nWhen you write application code, you need a development environment with tools\nthat let you write, manage, test, and run your code. The same is true for infrastruc‐\nture code. In this section, we’ll set up both a local environment and a cloud-hosted\nenvironment. We’ll be using these environments to write, test, and publish infrastruc‐\nture code.\nSet Up GitHub\nThe first thing we’ll need is a way to manage and release our code. We’ll be using Git\nfor code management and GitHub as a host. There are plenty of great options avail‐\nable for Git hosting, GitLab being one of the most popular alternatives. We’ve chosen\nto use GitHub for our model because it’s become a very popular place to share code.\nThat’s useful for our implementation, because we’ll be sharing a lot of code and con‐\nfiguration with you as we build our example application.\nKey Decision: Use GitHub for Code Management\nAll code will be managed using the Git version control system and hosted in GitHub.\nIn order to work with our examples, you’ll need to register for a GitHub account and\nyou’ll also need a local copy of a Git client. Git is an incredibly popular source control\ntool, so chances are that you already have it installed in your machine and you are\nfamiliar with how to use it. If you don’t already have the Git client installed, visit the\nGit downloads page and follow the instructions to download the appropriate version.\nIf Git is new to you, we recommend that you start with the “Git\nBasics” chapter in Scott Chacon and Ben Straub’s Pro Git, which\nthey’ve graciously made available for free online. You can also visit\nGitHub’s “Git Handbook” if you’re just looking for a quick over‐\nview of what Git is and why it’s useful.\nIn addition to the Git client, you’ll need a GitHub account so you can manage your\ncode and configure your own CI/CD pipelines. If you don’t have a GitHub account\nalready, you can register for a free account.\nWe’ll be using Git and GitHub to manage our microservices code. But, applying our\nprinciple of IaC, we’ll also be using these tools to manage infrastructure code. That\n104 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 2284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "code will be written in a special language called HCL that Terraform will be able to\nunderstand. Let’s move on to installing the Terraform client.\nInstall Terraform\nAs we mentioned earlier, we’ll be using Terraform to manage and apply our infra‐\nstructure code declaratively. Our plan is to automatically run the Terraform client in\nan automated CI/CD pipeline. Since that pipeline will be hosted in GitHub, you don’t\nactually need to install Terraform on your workstation. However, in our experience\nyou’ll need a local installation to test code before you commit it to the pipeline. So, it’s\nworthwhile installing Terraform in your local environment.\nAt the time of writing, Terraform is available to run on the following platforms:\n• OS/X\n• FreeBSD\n• Linux\n• OpenBSD\n• Solaris\n• Windows\nVisit the Terraform site to download the client of your choice and install it on your\nmachine. We used version 0.12.20 for all of the examples in this book. We’ll leave it to\nyou to follow the instructions for the platform you’ve chosen.\nOnce you have completed the installation, run the following command to make sure\nTerraform is set up correctly:\n$ terraform version\nYou should get back something that looks like the following, depending on the ver‐\nsion you’ve installed:\nTerraform v0.12.20\nWe’ll be using Terraform to manage infrastructure resources in a cloud platform. In\nour model, those resources will be hosted in AWS. Let’s take a look at how and why\nwe’ll be using AWS and what we need to do to get started with it.\nSetting Up the IaC Environment \n| \n105",
      "content_length": 1556,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "Configuring Amazon Web Services\nEarlier, we made a decision to use a cloud-hosted infrastructure. But we didn’t decide\non which cloud platform to use. Today, there are three cloud platforms that most\nmicroservices practitioners use: Microsoft Azure, Google Cloud Platform (GCP), and\nAWS. We’ve had success using all of these with our own implementations and have\neven worked with companies who’ve embraced all three.\nFor our model and sample application we’ve decided to build for a single cloud pro‐\nvider. That will make our implementation simpler and faster to implement. In that\nvein, we’ve decided to use AWS and its services for our examples, primarily because it\nhad the biggest userbase at the time of writing. However, all of the big three cloud\nvendors offer similar services, so you’ll be able to adapt our model to all of them with\na bit of work.\nKey Decision: Host Microservices in AWS\nWe’ll use AWS as the cloud platform for microservices.\nSince we’ve decided to use AWS, you will need to have an AWS account to follow\nalong with our examples. If you don’t have one already, you can register. Keep in\nmind that you’ll need a credit card to activate your account.\nKeep an Eye on Your Billing\nAlthough AWS offers a free account tier, the examples in this book\nuse resources that aren’t included in the scope of free services. We’ll\ngive you instructions for tearing down any resources that we create,\nbut it will be up to you to make sure those resources are destroyed.\nIn addition to setting up an initial account, you’ll also need to set up an “operator”\naccount so that the tools we’re setting up will have access to your AWS instance.\nSetting Up an AWS Operations Account\nBy the end of this chapter, we’ll have a pipeline that can deploy infrastructure into\nAWS automatically. Sticking to our principles of infeastructure as code and immuta‐\nbility, we should never have to manage our AWS infrastructure by making changes\ndirectly through the browser. But, to start off with, we’ll need to perform a few steps\nmanually to get our system up and running. The first step will be to configure a set of\ncredentials and permissions to allow our tools to work with our AWS objects.\nIn AWS, users, groups, and permissions are all managed within the Identity and\nAccess Management (IAM) service. We’ll need to create a special user that represents\n106 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 2409,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "our tooling and define a set of permissions for what our tools can do. We’ll use this\nuser identity whenever we are making calls from our CI/CD pipeline platform. As we\nmentioned earlier, we’ll be using Terraform as our primary IaC tool. Follow the steps\nin this section to create a Terraform user in AWS that will allow us to make the kinds\nof changes we’ll need for our microservices environment.\nLog in to your AWS management console with your root user credentials. Once\nyou’ve logged in, you should be presented with a list of AWS services. Find and select\nthe IAM service—it’s usually found in the Security, Identity & Compliance section\n(see Figure 6-2).\nFigure 6-2. Select IAM\nSelect the Users link from the IAM navigation menu on the lefthand side of the\nscreen. Click the Add user button to start the IAM user creation process, as shown in\nFigure 6-3.\nConfiguring Amazon Web Services \n| \n107",
      "content_length": 901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "Figure 6-3. Add user button\nEnter ops-account in the User name field. We also want to use this account to acccess\nthe CLI and API, so select “Programmatic access” as the AWS “Access type,” as in\nFigure 6-4.\nFigure 6-4. Enter user details\nWhen you’ve done that, click the Next: Permissions button.\nOur operator account will need a lot of permissions to do work in AWS on our\nbehalf. For now, however, we’re only going to attach a single set of permissions, pack‐\naged together in an AWS policy called IAMFullAccess.\n108 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "To add this policy, select “Attach existing policies directly” from the set of options at\nthe top. Search for a policy called IAMFullAccess and select it by ticking its checkbox,\nas shown in Figure 6-5.\nFigure 6-5. Attach the IAMFullAccess policy\nWhen that’s done, click the Next: Tags button. We won’t be creating any tags, so click\nthe Next: Review button to review our user’s details (see Figure 6-6).\nFigure 6-6. Review user details\nConfiguring Amazon Web Services \n| \n109",
      "content_length": 476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "If everything looks OK to you, click the Create user button. You should now see a\nscreen that looks something like Figure 6-7.\nFigure 6-7. User created\nBefore we do anything else, we’ll need to make a note of our new user’s keys. Click the\nShow link and copy and paste both the “Access key ID” and the “Secret access key”\ninto a temporary file. We’ll use both of these later in this section with our automated\npipeline. Be careful with this key material as it will give whoever has it an opportunity\nto create resources in your AWS environment—at your expense.\nMake sure you take note of the access key ID and the secret access\nkey that were generated before you leave this screen. You’ll need\nthem later in this chapter.\nWe have now created a user called ops-account with permission to make IAM\nchanges. That gives us all that we need to transition from using the browser-based\nconsole over to the AWS CLI application that we installed earlier. The first thing we’ll\nneed to do is configure the CLI to use the ops user we’ve just created.\nConfigure the AWS CLI\nThere are three ways to manage major cloud provider configurations: a web browser,\nweb-based APIs, and a CLI. We’ve already used a web browser to create our operator\naccount and later we’ll be using Terraform to configure changes via the AWS APIs.\nBut, we’ll need to make some more changes before Terraform can make AWS API\ncalls on our behalf. For that we’ll use the AWS CLI.\nUsing the CLI makes it a lot easier for us to describe the changes you need to make.\nIt’s also less prone to the changes that user interfaces (UIs) go through. But to use the\nCLI, the first thing we’ll need to do is install it into our local working environment.\n110 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 1756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "Navigate to the AWS CLI download page and follow the instructions there to install\nthe CLI onto your local system.\nOnce it’s ready, the first thing we’ll do is configure the CLI so it can access our\ninstance. Run the aws configure command as shown in Example 6-1. You can\nreplace the default region name with an AWS region that is closer to you; a full list of\nAWS regions is available at AWS.\nExample 6-1. Configure the AWS CLI\n$ aws configure\nAWS Access Key ID [****************AMCK]: AMIB3IIUDHKPENIBWUVGR\nAWS Secret Access Key [****************t+ND]: /xd5QWmsqRsM1Lj4ISUmKoqV7/...\nDefault region name [None]: eu-west-2\nDefault output format [None]: json\nYou can test that you’ve configured the CLI correctly by listing the user accounts that\nhave been created. Run the iam list-users command to test your setup:\n$ aws iam list-users\n{\n    \"Users\": [\n        {\n            \"Path\": \"/\",\n            \"UserName\": \"admin\",\n            \"UserId\": \"AYURIGDYE7PXW3QCYYEWM\",\n            \"Arn\": \"arn:aws:iam::842218941332:user/admin\",\n            \"CreateDate\": \"2019-03-21T14:01:03+00:00\"\n        },\n        {\n            \"Path\": \"/\",\n            \"UserName\": \"ops-account\",\n            \"UserId\": \"AYUR4IGBHKZTE3YVBO2OB\",\n            \"Arn\": \"arn:aws:iam::842218941332:user/ops-account\",\n            \"CreateDate\": \"2020-07-06T15:15:31+00:00\"\n        }\n    ]\n}\nIf you’ve done everything correctly, you should see a list of your AWS user accounts.\nThat indicates that AWS CLI is working properly and has access to your instance.\nNow, we can set up the permissions our operations account will need.\nConfiguring Amazon Web Services \n| \n111",
      "content_length": 1626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "Setting Up AWS Permissions\nWhen we created our ops-account user we attached an IAM policy to it that only\ngives it permission to modify IAM settings. But our ops account will need a lot more\npermissions than that to manage the AWS resources we’ll need for our infrastructure\nbuild. In this section, we’ll use the AWS command-line tool to create and attach addi‐\ntional permission policies to the ops account.\nThe first thing we’ll do is make the ops-account user part of a new group called Ops-\nAccounts. That way we’ll be able to assign new users to the group if we want them to\nhave the same permissions. Use the following command to create a new group called\nOps-Accounts:\n$ aws iam create-group --group-name Ops-Accounts\nIf this is successful, the AWS CLI will display the group that has been created:\n{\n    \"Group\": {\n        \"Path\": \"/\",\n        \"GroupName\": \"Ops-Accounts\",\n        \"GroupId\": \"AGPA4IGBHKZTGWGQWW67X\",\n        \"Arn\": \"arn:aws:iam::842218941332:group/Ops-Accounts\",\n        \"CreateDate\": \"2020-07-06T15:29:14+00:00\"\n    }\n}\nNow, we just need to add our user to the new group. Use the following command to\ndo that:\n$ aws iam add-user-to-group --user-name ops-account --group-name Ops-Accounts\nIf it works, you’ll get no response from the CLI. In this case, no news is good news.\nNext, we need to attach a set of permissions to our Ops-Account group. Those per‐\nmissions will automatically be applied to our operations users, since we’ve made it\npart of the group. The permissions we’ll be attaching will let our user create and\nchange AWS resources. In practice, you’d likely need to change the permissions for\nyour Ops user as you go through the process of designing your infrastructure. In this\nbook, we’ve already done the design work ahead of time, so we know exactly which\npolicies need to be attached.\nRun the following command to attach all the policies we’ll need to the Ops-Accounts\ngroup:\n$ aws iam attach-group-policy --group-name Ops-Accounts\\\n --policy-arn arn:aws:iam::aws:policy/IAMFullAccess &&\\\naws iam attach-group-policy --group-name Ops-Accounts\\\n --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess &&\\\naws iam attach-group-policy --group-name Ops-Accounts\\\n112 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 2259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "--policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess &&\\\naws iam attach-group-policy --group-name Ops-Accounts\\\n --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy &&\\\naws iam attach-group-policy --group-name Ops-Accounts\\\n --policy-arn arn:aws:iam::aws:policy/AmazonEKSServicePolicy &&\\\naws iam attach-group-policy --group-name Ops-Accounts\\\n --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess &&\\\naws iam attach-group-policy --group-name Ops-Accounts\\\n --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess &&\\\naws iam attach-group-policy --group-name Ops-Accounts\\\n --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\nA scripted copy of this command is available at this book’s GitHub\nsite.\nIn addition to the out-of-the-box policies that AWS provides, we’ll also need some\nspecial permissions to work with the AWS Elastic Kubernetes Service (EKS). We’ll be\nintroducing EKS properly in the next chapter, but for now we need to get the permis‐\nsions sorted out. There isn’t an existing policy that we can attach for the permissions\nwe need, so we’ll need to create our own custom policy and attach it to our user\ngroup.\nTo do this, create a file called custom-eks-policy.json and populate it with the code in\nExample 6-2. We have also made a copy of this JSON file available in this book’s Git‐\nHub repository.\nExample 6-2. Custom JSON policy for EKS\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"eks:DescribeNodegroup\",\n        \"eks:DeleteNodegroup\",\n        \"eks:ListClusters\",\n        \"eks:CreateCluster\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"eks:*\",\n      \"Resource\": \"arn:aws:eks:*:*:cluster/*\"\n    }\nConfiguring Amazon Web Services \n| \n113",
      "content_length": 1800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "]\n}\nNow, run the following command to create a new policy named EKS-Management\nbased on the JSON we’ve just created:\n$ aws iam create-policy --policy-name EKS-Management\\\n --policy-document file://custom-eks-policy.json\nIf the command was successful, you’ll see a JSON representation of the new policy:\n{\n    \"Policy\": {\n        \"PolicyName\": \"EKS-Management\",\n        \"PolicyId\": \"ANPA4IGBHKZTP3CFK4FAW\",\n        \"Arn\": \"arn:aws:iam::[some_number]:policy/EKS-Management\",\n        \"Path\": \"/\",\n        \"DefaultVersionId\": \"v1\",\n        \"AttachmentCount\": 0,\n        \"PermissionsBoundaryUsageCount\": 0,\n        \"IsAttachable\": true,\n        \"CreateDate\": \"2020-07-06T15:50:26+00:00\",\n        \"UpdateDate\": \"2020-07-06T15:50:26+00:00\"\n    }\n}\nIn AWS, every resource has a unique identifier called an Amazon\nResource Name (ARN). The string of digits in the ARN of the pol‐\nicy you’ve just created will be unique to you and your AWS\ninstance. You’ll need to make note of your policy’s ARN string so\nthat you can reference it in the following steps.\nWith the new policy created, all that’s left is to attach it to our user group. Run the\nfollowing command, replacing the token we’ve called {YOUR_POLICY_ARN} with the\nARN from your policy:\n$ aws iam attach-group-policy --group-name Ops-Accounts \\\n   --policy-arn {YOUR_POLICY_ARN}\nYou now have an ops-account user that has the permissions needed to automatically\ncreate AWS infrastructure resources for us. We’ll be using this user account when we\nwrite our Terraform code and when we configure the infrastructure pipeline. Make\nsure you keep the access key and secret somewhere handy (and safe) as we’ll need it\nlater.\nWe have one last bit of setup to take care of before we can get to work building the\npipeline: the creation of an AWS S3 storage bucket for Terraform to store state.\n114 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 1885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "Creating an S3 Backend for Terraform\nTerraform is powerful because it allows us to declare what an infrastructure should\nlook like, rather than defining the specific steps needed to reach that state. Terraform\nworks its magic by making the right changes to an environment to make it look the\nway we’ve described. But, in order to do that, Terraform needs to keep track of what\nthe environment looks like and the last operations it’s performed. Terraform keeps\ntrack of all that information in a JSON-based state file that is read and updated every\ntime it is run.\nBy default, Terraform will keep this state file in your local filesystem. In practice, stor‐\ning the state file locally is problematic. State often needs to be shared across machines\nand users so that an environment can be managed in multiple places. However, local\nstate files are difficult to share and you can easily find yourself dealing with state con‐\nflicts and synchronization issues.\nInstead, we’ll use the AWS S3 service to store the Terraform state file. Terraform\ncomes with out-of-the-box support for using S3 as a state backend. All we’ll need to\ndo is create a new “bucket” for the data and make sure we have the correct permis‐\nsions set for our ops user account.\nLike most cloud providers, AWS provides lots of different data\nstorage options. Amazon’s Simple Storage Service (S3) lets you cre‐\nate data objects that can be referenced by a key. The data objects are\njust blobs to Amazon and can be in any format you like. In this\ncase, Terraform will be storing environment state as JSON objects.\nTo create a bucket, you’ll need to give it a unique name and pick the region that it\nshould reside in. You should have already selected a default region when you config‐\nured the AWS CLI and we suggest that you use the same region for the S3 bucket. You\ncan find more information about S3 bucket regions in the AWS documentation.\nS3 Bucket Names Must Be Unique\nAmazon S3 buckets can be referenced by their names. So the name\nyou pick must be unique across the entire AWS region that you\nselect. There is a good chance you won’t be able to use a generic\nname like “test” or “microservices.” Instead, you’ll need to come up\nwith something unique. Usually, appending your name to the\nbucket name works. Throughout this book, whenever we refer to\nthis S3 bucket, we’ll use the token {YOUR_S3_BUCKET_NAME} and\nleave it to you to replace it with your bucket name.\nConfiguring Amazon Web Services \n| \n115",
      "content_length": 2474,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "If you are hosting your bucket in the us-east-1 region, use the following command:\nS3 requires special handling if you’re not in the us-east-1 region.\nWe’ve listed the default us-east-1 and nondefault versions of the\ncommand in the following examples. We’ve also split up the com‐\nmand lines using the bash multiline operator “\\” for readability.\n$ aws s3api create-bucket --bucket {YOUR_S3_BUCKET_NAME} \\\n> --region us-east-1\nIf you are hosting the s3 bucket in a region other than us-east-1, use the following\ncommand:\n$ aws s3api create-bucket --bucket {YOUR_S3_BUCKET_NAME} \\\n> --region {YOUR_AWS_REGION} --create-bucket-configuration \\\n> LocationConstraint={YOUR_AWS_REGION}\nIf everything has gone well, you should see a JSON object with the location of your\nbucket. It will look something like this example for a bucket named my-msur-test:\n{\n    \"Location\": \"http://my-msur-test.s3.amazonaws.com/\"\n}\nThis indicates that the bucket has been successfully created and has been assigned its\nown unique URL. By default, S3 buckets aren’t publicly accessible. That’s a good thing\nbecause we don’t want just anyone to be able to see and change our Terraform state\nfile. However, we’ve already given our ops account user full permissions to the S3 ser‐\nvice, so it is ready for use.\nWith this final step complete, we now have an AWS user called ops-account config‐\nured to create, edit, and delete resources in AWS. We’ve also given it permissions to\nstore objects in a special S3 bucket we’ve created just for managing Terraform state.\nThis should be the last time we make manual operator changes to our AWS instance;\nfrom here on out we’ll only make changes through code and with an automated\npipeline!\nBuilding an IaC Pipeline\nWith the accounts, permissions, and tools ready to go, we can now get on with the\nreal focus of this chapter. By the end of this section, we’ll have an IaC pipeline imple‐\nmented and ready to use. Remember, the infrastructure pipeline is incredibly impor‐\ntant because it gives us a safe and easy way to provision environments quickly.\nWithout a pipeline, we’d end up with lots of manual steps and microservices environ‐\nments that have drifted apart in the way they work.\n116 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 2255,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "Instead, we’ll have a stable declarative definition of the infrastructure for our services.\nOur development and operations teams will be able to use those definitions to create\ntheir own environments to perform testing, make changes, and release services into\nproduction. We won’t be implementing any of the actual AWS infrastructure in this\nchapter, but we will be implementing a foundation that we’ll use in the next chapter.\nIn this section we’ll build the following components:\n• A GitHub-hosted Git repository for a sandbox testing environment\n• A Terraform root module that defines the sandbox\n• A GitHub Actions CI/CD pipeline that can create a sandbox environment\nThe sandbox testing environment we’re building is just a test environment that will\ngive us a chance to try out our IaC modules and pipelines. We’ll build it out in the\nnext chapter and then throw it away when we’re happy that everything works. Later,\nwe’ll use all of these assets to build a test environment for the microservices that we’ll\nbe designing and building.\nBut our first step will be to establish a repository for the code and pipeline, so let’s get\nstarted with creating the repository.\nCreating the Sandbox Repository\nWe’ve already mentioned in the beginning of this chapter that we’ll be using Git and\nGitHub to manage our infrastructure code. If you’ve been following along, you’ll\nalready have a local copy of Git installed and a GitHub account ready to be used.\nWe’re going to use both of those tools to create a new repository for our sandbox\nenvironment.\nIn our model, we’ve decided to give each environment its own repository with the\ncode and pipeline bundled within it. We like this approach because it gives teams\nmore independence in how they manage the environments they want to create, while\nstill keeping the pipeline configuration and code together for easier management.\nKey Decision: One Repository per Environment\nEach environment’s code and pipeline will be managed independently in its own code\nrepository.\nWe’ll use GitHub’s browser-based interface to create the sandbox repository.\nAlthough there is a GitHub CLI application available, it will be quicker and easier to\nuse the web-based interface to create our new repository. Later, we’ll also be using\nGitHub’s browser interface to run and monitor the pipeline.\nBuilding an IaC Pipeline \n| \n117",
      "content_length": 2355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "Some practitioners like to keep all of the environment configura‐\ntions together in a single “monorepo.” This makes it easier to share\nlibraries, components, and actions between all the environments\nand helps keep things consistent. Most practitioners also use speci‐\nalized CI/CD tooling (Jenkins being one of the most popular),\nrather than building inside GitHub. This is an important decision,\nso you’ll need to consider the trade-offs when you build your next\nmicroservices architecture based on the observations you make\nfrom the one we’re building together.\nTo create the repository, open your browser and navigate to the GitHub sign-in page.\nIf you haven’t already logged in to your GitHub account, you’ll be prompted to enter\nyour login credentials. Once that is done, you’ll be presented with a form to create a\nnew repository. Give your new repository the name env-sandbox and select Private\nfrom the access options. You should also tick the Add .gitignore checkbox and choose\nTerraform from the drop-down, as shown in Figure 6-8.\nFigure 6-8. Create a GitHub sandbox repository\nIt’s important that we ask GitHub to add a .gitignore for Terraform to the module\nbecause it will make sure we don’t accidentally commit Terraform’s hidden working\nfiles to our module. If you’ve missed this step you can always add this file later by\ncopying the source from this GitHub site.\n118 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 1434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "It’s possible to write code using GitHub’s browser-based text editor, but it’s not very\npractical for doing real work. Instead, we’ll clone this repository into a local develop‐\nment environment so we can use our own tools. We’ll leave it to you to create a clone\nof your env-sandbox repository in your local development environment.\nIf you’ve never worked with Git and GitHub before, you can find\nhelpful instructions on how to clone a GitHub repository in the\nofficial GitHub documentation.\nThat’s all we need to do with GitHub for now. We’ll come back to the browser-based\nGitHub interface later when we work on the pipeline. But with the local clone created,\nwe can begin work on the Terraform code.\nUnderstanding Terraform\nWe mentioned earlier that we’ll be using Terraform as our tool of choice for declara‐\ntively coding our infrastructure foundation. Terraform does a lot of complicated work\nto make changes that match a declared state. That said, it’s surprisingly easy to get\nstarted with and the language it uses is fairly intuitive. That makes it a great fit for our\narchitecture and our goal of getting a system running as quickly as possible.\nTerraform files are configured in a data format called HCL, which was invented by\nHashiCorp (the company that created Terraform). HCL is similar to JSON, with a few\nadaptations and improvements. If you’re used to JSON, the biggest difference you’ll\nnotice is that HCL doesn’t use a \":\" delimiter between key and value pairs. Instead,\nkeys and values are just separated by a white space or an \"=\" depending on the con‐\ntext. There are some other minor improvements, such as comments and multiline\nstrings. In our experience, it’s an easy language with a very low learning curve if\nyou’ve used JSON or YAML in the past.\nIn addition to understanding HCL, it’s useful to understand four of the key Terraform\nconcepts—backends, providers, resources, and modules:\nBackends\nTerraform needs to maintain a state file so that it knows what kinds of changes to\nmake to the infrastructure environment. A backend is the location of that state\nfile. By default this is located in the local filesystem. We’ll be using an AWS S3\nbucket that we configured earlier.\nResources\nA resource is an object that represents a thing for which you are declaring a state.\nTerraform does the work of making the changes to bring the resource to that\nstate.\nBuilding an IaC Pipeline \n| \n119",
      "content_length": 2415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "Providers\nA Terraform provider is a packaged library of resources that you can use in your\ncode. We’ll be using Terraform’s AWS provider for most of our work. The nice\nthing about Terraform is that you can use it for lots of different cloud platforms\nand infrastructure environments—you just need to specify the provider you plan\nto use.\nModules\nTerraform modules are similar to functions or procedures in a regular program‐\nming language. They give you a nice way of encapsulating your HCL code in a\nreusable, modular way.\nThere’s a lot more to Terraform that what we’ve described here, but this is enough\nknowledge for us to get started with our environment build work. If you want to go\ndeeper, the Terraform documentation is a great place to start.\nOur next step is to write some Terraform code that will help us build a sandbox envi‐\nronment.\nWriting the Code for the Sandbox Environment\nOur goal in this chapter is to set up the tooling and infrastructure for our environ‐\nment build, so we won’t be writing a complete Terraform file that defines our infra‐\nstructure until the next chapter. For now, we’ll need to create a simple starter file to\ntest our Terraform-based tool chain.\nThe Terraform CLI tool works by looking for files it recognizes in the working direc‐\ntory where it’s run. In particular, it looks for a file called main.tf and will parse that\nfile and apply changes based on its contents. You can only have one main.tf file in a\nsingle directory, so we’ll need to have a directory dedicated to our sandbox environ‐\nment and we’ll need to create a Terraform main.tf file that will describe its target state.\nWe’ve already created a Git repository called env-sandox for the sandbox environ‐\nment, so that’s the directory we’ll use for the Terraform code. Let’s get started by cre‐\nating a new file called main.tf in the local sandbox Git repository. Populate it with the\nHCL code in Example 6-3.\nYou’ll need to replace the tokens {YOUR_S3_BUCKET_NAME} and\n{YOUR_AWS_REGION} with the S3 bucket name you created earlier\nand the AWS region you’ve been using.\n120 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 2132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "Example 6-3. env-sandbox/main.tf\nterraform {\n  backend \"s3\" {\n    bucket = \"{YOUR_S3_BUCKET_NAME}\"\n    key    = \"terraform/backend\"\n    region = \"{YOUR_AWS_REGION}\"\n  }\n}\nlocals {\n  env_name         = \"sandbox\"\n  aws_region       = \"{YOUR_AWS_REGION}\"\n  k8s_cluster_name = \"ms-cluster\"\n}\n# Network Configuration\n# EKS Configuration\n# GitOps Configuration\nThe S3 bucket name should just be the name of your bucket, not\nthe full URL (for example, my-bucket).\nThe HCL snippet you’ve just written lets Terraform know that we are using an S3\nbucket to store our backend state. It also defines a set of local variables using a Terra‐\nform construct called locals. Finally, it has a few Terraform comments at the end,\nindicating where we’ll be adding details for the infrastructure. We’ll be using the local\nvariables and filling in the rest of the configuration in the next chapter. For now, we\njust want to test the scaffolding of our Terraform file.\nWith our first Terraform code file written, we’re ready to try running some Terraform\ncommands to make sure it works as expected. The Terraform CLI tool includes a lot\nof helpful features to improve the quality and safety of your infrastructure code. You\ncan use it to format (or lint) the HCL that you’ve written, validate the syntax, and do a\ndry run of the changes that Terraform would run against your provider.\nIf you’ve followed the instructions earlier in this chapter, you should have a local copy\nof Terraform available in your working environment. Make sure you are in the same\nworking directory as your main.tf file and try running the fmt command to format\nyour code:\nenv-sandbox msur$ terraform fmt main.tf\nBuilding an IaC Pipeline \n| \n121",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "The fmt command is a formatter that will examine your HCL file and make changes\nto improve its consistency and readability. If any changes were made it will output the\nname of the file that it changed.\nNext, we’ll validate that the syntax of the HCL we’ve written is valid. But, before we\ndo that we’ll need to install the providers we’re using; otherwise Terraform will com‐\nplain that it can’t do the syntax check. Run the following command to install the\nproviders:\nenv-sandbox msur$ terraform init\nSuccessfully configured the backend \"s3\"! Terraform will automatically\nuse this backend unless the backend configuration changes.\nTerraform has been successfully initialized!\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\nIf you get an error related to AWS credentials, make sure you’ve\nfollowed the instructions at the beginning of this chapter to config‐\nure access to an AWS environment first.\nNow we can run a validate command to ensure that we haven’t introduced any syntax\nerrors:\nenv-sandbox msur$ terraform validate\nSuccess! The configuration is valid.\nFinally, we can run a command called plan to see what changes Terraform would\nmake to create the environment we’ve specified. This performs the same steps that\nwill be run when the code is applied, but it doesn’t actually make any changes. Think\nof it as a dry run that allows Terraform to show you its plan for getting the infrastruc‐\nture to the state you’ve asked for. Use the following command to run a plan:\n122 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "$ terraform plan\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\nNo changes. Infrastructure is up-to-date.\nThis means that Terraform did not detect any differences between your configuration\nand the real physical resources. As a result, no actions need to be performed.\nNotice that our plan result isn’t very interesting: “No changes.” That’s because we\nhaven’t actually defined any resources to create. The good news is that we now have a\nsyntactically valid Terraform file to start building our sandbox environment. This is a\ngood time to commit and push the file into the GitHub repository so that the file is\navailable for use:\n$ git add .\n$ git commit -m \"The sandbox starter file\"\n$ git push origin\nWith our Terraform file working and ready to be used, we can shift our focus over to\nthe pipeline that we’ll use to automatically apply it.\nBuilding the Pipeline\nIn this section we’ll set up an automated CI/CD pipeline that will automatically apply\nthe Terraform file that we’ve just created. To configure the pipeline activities, we’ll be\nusing GitHub’s built-in DevOps tool, GitHub Actions. The nice thing about using\nGitHub Actions is that we can put our pipeline configuration in the same place as our\ninfrastructure code.\nThe easiest way to use GitHub Actions is to configure it through the browser inter‐\nface. So go back to your browser and navigate to the sandbox repository you created\nearlier in GitHub.\nOur plan is to create resources in the AWS account that we created earlier in this\nchapter. Thus, we’ll need to make sure that GitHub is able to use the AWS access key\nand secret that we provisioned when we created the operator account. There are lots\nof ways to manage secrets in a microservices architecture, but for our DevOps tool‐\ning, we’ll just use GitHub’s built-in secrets storage function.\nSetting up secrets\nNavigate to the GitHub secrets storage area by selecting Settings from the top naviga‐\ntion of your repository. Select Secrets from the menu of settings options on the left‐\nhand side of the screen, as shown in Figure 6-9.\nBuilding an IaC Pipeline \n| \n123",
      "content_length": 2216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "Figure 6-9. GitHub secrets\nSelect “Add a new secret” and create a secret called AWS_ACCESS_KEY_ID. Enter the\naccess key ID that you tucked away earlier in this chapter when you created your\noperator user. Repeat the process and create a secret named AWS_SECRET_ACCESS_KEY\nwith the secret access key you generated earlier. When you are done, you should have\nsomething that looks like Figure 6-10.\nFigure 6-10. Add your AWS ID and key\nNow that the secrets have been added, we can get started on the workflow for the\npipeline.\n124 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 578,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "Creating the workflow\nA workflow is the set of steps that we want to run whenever a pipeline is triggered.\nFor our microservices infrastructure pipeline, we’ll want a workflow that validates\nTerraform files and then applies them to our sandbox environment. But in addition\nto testing and applying infrastructure changes, we’ll need to add a few steps before\nand after applying our Terraform files.\nThe workflow will need to start with a trigger that lets GitHub know when the work‐\nflow should start. GitHub Actions gives us a few different options for triggers, but\nwe’ll use Git’s tag mechanism as the trigger for our infrastructure builds. A tag is a\nway of giving a name to or labeling a particular point in a Git repository history.\nUsing tagging as a trigger gives us a nice versioning history for the changes we are\nmaking to the environment. It also gives us a way of committing files to the reposi‐\ntory without triggering a build.\nWhen our pipeline workflow is triggered it will need to operate on the Terraform files\nthat we’ve committed to the repository. But, we’ll need some setup steps to prepare\nthe build environment. First, we’ll install Terraform and AWS just like we did in our\nlocal environment. Although we are running this in GitHub Actions, the actual build\ntakes place in a virtual machine, so we’ll also need to grab a copy of the code from our\ncode repository.\nFinally, when the changes are applied to our sandbox environment, we’ll have a\nchance to do any cleanup or post-provisioning activities. In our case, we’ll be making\na special configuration file available for download so that we can connect to the AWS-\nbased microservices environment from a local machine. When it’s complete, the pipe‐\nline will look like Figure 6-11.\nFigure 6-11. Infrastructure pipeline steps\nBuilding an IaC Pipeline \n| \n125",
      "content_length": 1833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "We’ll be defining the steps of the pipeline using the YAML language and the set of\nworkflow commands in GitHub Actions. You can refer to the full GitHub Actions\ndocumentation. Let’s dig into the YAML configuration by navigating to the GitHub\nActions page for your repository. You should be able to do this by selecting Actions\nfrom the top navigation bar in your sandbox’s GitHub repository page. When you get\nthere, you should see a screen that looks similar to Figure 6-12.\nFigure 6-12. Create a GitHub Actions workflow\nGitHub Actions provides you with templates you can use to quickly get started with a\nworkflow. However, we’re going to ignore the templates and set up a workflow our‐\nselves from scratch. Click the Set up a workflow yourself button in the top-right cor‐\nner of the screen (or wherever it is in the latest version of the interface).\nYou’ll now find yourself editing a newly created YAML file for your workflow. Git‐\nHub keeps the Actions files in a hidden directory called /.github/workflows. When\nyou clone a GitHub repository, you can edit these files in whatever editor you like, or\ncreate new YAML files to define new GitHub Action workflows. But the advantage of\nediting Actions on the GitHub website is that you can search for plug-ins from the\nmarketplace. So we’ll stick to the browser-based editor for our initial workflow\nediting.\nThe first thing we’ll do is configure a trigger for the workflow and set up a container\nenvironment to do the infrastructure build.\nTo help you understand what is happening, we’re going to explore\nthe workflow file as individual parts. We’ll explain each part as we\ngo along, but the actual workflow is all contained in a single file.\nYou can see an example of the completed workflow file at this\nbook’s GitHub site.\n126 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 1833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "Configuring the trigger and setup\nOne of the most important steps in our workflow is the trigger step that initiates it. As\nwe mentioned earlier, we’ll use a simple trigger based on Git’s tagging mechanism.\nWe’ll configure our pipeline so that it runs whenever infrastructure is tagged with a\nlabel that starts with a v. That way we can keep a version history of the infrastructure\nthat we’ve built. For example, our first infrastructure build could be tagged with\n“v1.0.”\nReplace the YAML in your workflow editor with the code in Example 6-4 to get\nstarted.\nExample 6-4. Workflow trigger and job setup\nname: Sandbox Environment Build\non:\n  create:\n    tags:\n      - v*\njobs:\n  build:\n    runs-on: ubuntu-latest\n    env:\n        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n    steps:\n    - uses: actions/checkout@v2\n    # Install Dependencies\n# Install Dependencies in Example 6-4 is a comment. We’ll use\ncomments in the YAML to describe what is happening, but also to\nindicate where we’ll be adding additional YAML in later steps.\nIn the preceding snippet, on is a GitHub Actions command that specifies the trigger\nfor the workflow. We’ve configured our workflow to run when a new tag that matches\nthe pattern v* is created. In addition, we’ve added a jobs collection that specifies the\nwork that GitHub should do when it is triggered. Jobs need to be run in a machine or\na container. The runs-on property indicates that we want to run this build in an\nUbuntu Linux Virtual Machine. We’re also adding the AWS secrets that we config‐\nured earlier in the build environment.\nThe steps collection indicates the specific workflow steps that the workflow will per‐\nform within the environment we have set up. But before we do anything else we need\nBuilding an IaC Pipeline \n| \n127",
      "content_length": 1851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "to get the code. So the first step we’ve defined is to check out our Terraform code\nfrom Git using the GitHub actions/checkout@v2 action. This creates a copy of the\ncode inside of the Ubuntu build environment for the rest of the job steps to act upon.\nActions are modularized libraries of code that can be called from a\nGitHub Actions workflow. Actions are the heart of the GitHub\nActions system and give it a richness of features and integration.\nThere is a large catalog of actions available in the GitHub Actions\nmarketplace that you can use in your workflow files. But, be selec‐\ntive when choosing them as anyone can create and publish new\nactions, so support, security, and quality are not guaranteed.\nWe have enough in our workflow to be able to run it, but it would not be able to do\nanything useful beyond grabbing a copy of our code. What we really want to do is\nstart working with Terraform, but before we can do that we need to get the environ‐\nment set up so that our tooling can be run. That means we need to add some depend‐\nency installation instructions.\nInstalling dependencies\nWhen we set up our local infrastructure development environment, we needed to\ninstall the Git, AWS, and Terraform command-line tools. We’ll need to do something\nsimilar in our build environment, but since we know the specific operations we’ll be\nrunning, we can set up a slightly leaner set of dependencies.\nThe good news is that we get Git for free when we use GitHub Actions, so we won’t\nneed to worry about installing it. Also, HashiCorp provides a ready-to-go GitHub\nAction for Terraform, so we won’t need to worry about installing the Terraform cli‐\nent. The only thing that’s left to deal with is our AWS configuration.\nEarlier in this chapter we used the AWS CLI to make changes to our AWS account. In\nour pipeline environment, however, we want to use Terraform to make changes. In\nfact, we don’t want to make any changes to the environment beyond what we’ve speci‐\nfied in our Terraform code. So we won’t need to install the AWS CLI.\nAll of this tells us that we don’t need to install any dependencies to make a pipeline\nthat can create AWS resources for us. But as we start building out our infrastructure\nin the next chapter, we’ll find out that our infrastructure needs some special depen‐\ndencies to deal with some of the complexities of installing a Kubernetes-based micro‐\nservices architecture.\n128 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 2460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "Because this is a book, we’ve identified the dependencies you’ll\nneed for the pipeline before you know you need them. That’s\nbecause books are easier to read when they are linear, so we’ve done\nthe work to give you a linear set of instructions to follow. In prac‐\ntice, you’ll go through several iterations of editing your pipeline\nactions as you test, and learn and develop your infrastructure and\nmicroservices pipelines.\nSpecifically, we’ll be installing an AWS authenticator tool and an installer for the Istio\nservice mesh. The AWS authenticator is a command-line tool that other tools can use\nto authenticate and access an AWS environment. This will come in handy later, when\nwe are working with Kubernetes and need to configure access to an AWS-hosted\nKubernetes cluster. Istio is a service mesh tool. We’ll introduce Istio in the next chap‐\nter; for now we just need to make sure we’ve installed the CLI tool.\nAdd the code in Example 6-5 to your workflow file to set up those dependencies in\nthe build environment. These steps need to be added after the # Install Dependen\ncies comment we added earlier. Be careful with the indenting and make sure you are\nlined up with the -uses step from earlier as YAML is very particular about spacing.\nExample 6-5. Installing dependencies\n[...]\n    # Install Dependencies\n    - name: Install aws-iam-authenticator\n      run: |\n        echo Installing aws-iam-authenticator...\n        mkdir ~/aws\n        curl -o ~/aws/aws-iam-authenticator \\\n        \"https://amazon-eks.s3.us-west-2.amazonaws.com/\\\n        1.16.8/2020-04-16/bin/linux/amd64/aws-iam-authenticator\"\n        chmod +x ~/aws/aws-iam-authenticator\n        sudo cp ~/aws/aws-iam-authenticator /usr/local/bin/aws-iam-authenticator\n  # Apply Terraform\nThe run commands in the YAML you’ve just added will run shell commands in the\nUbuntu build environment. We’ve added instructions to install the AWS IAM\nAuthenticator based on the AWS documentation as well as the Istio CLI tool.\nThe virtual machine (VM) defined in our GitHub Actions work‐\nflow will be created at the start of every pipeline run and destroyed\nat completion. That means our tools will be installed every time we\ntrigger our pipeline job and no state will be retained between runs.\nBuilding an IaC Pipeline \n| \n129",
      "content_length": 2284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "The last part of our YAML code uses HashiCorp’s Terraform setup action. As you can\nsee, this is much cleaner and easier to read and understand than the command-line\ninstallations we included for the AWS authenticator and Istio. GitHub Actions is bet‐\nter when you have actions to use, so it’s a good idea to take advantage of them when\nthey fit your needs.\nWith our dependencies set up and Terraform ready to go, we can now add some Ter‐\nraform handling steps to our workflow.\nApplying Terraform files\nIn “Writing the Code for the Sandbox Environment” on page 120, we used Terraform\ncommands to format and validate the HCL code we wrote in main.tf. We want to do\nsomething similar in our pipeline, but we want these activities to happen automati‐\ncally. The goal is for the Terraform code to be automatically formatted, validated, and\nplanned. We’ll also add an automatic “apply” step that will apply the plan and imple‐\nment changes.\nAdd the YAML code in Example 6-6 to the end of your workflow YAML, after the #\nApply Terraform comment.\nExample 6-6. Terraform workflow\n  # Apply Terraform\n \n- uses: hashicorp/setup-terraform@v1\n      with:\n        terraform_version: 0.12.19\n    - name: Terraform fmt\n      run: terraform fmt\n    - name: Terraform Init\n      run: terraform init\n    - name: Terraform Validate\n      run: terraform validate -no-color\n    - name: Terraform Plan\n      run: terraform plan -no-color\n    - name: Terraform Apply\n      run: terraform apply -no-color -auto-approve\n    # Publish Assets\n130 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 1569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "As you can see from your YAML, we’re using the run action to call the Terraform CLI\nfrom the Ubuntu shell. This is largely the same as what you did in your local environ‐\nment with the addition of the apply step at the end that will make real changes in the\nAWS infrastructure. Notice that we’ve added the -auto-approve flag to the apply\ncommand so that there won’t be any need for human interaction.\nWe’re almost done with the pipeline. The final step is to publish any files that we want\nto keep from our run.\nPublishing assets and committing changes\nWhen a GitHub Actions workflow completes, the VM that we used for our build is\ndestroyed. But sometimes we want to keep some of the state, files, or results for later\nuse. To help with that, GitHub provides an upload-artifact action that gives us an\neasy way to make files available for us to download later.\nIn the next chapter, we’ll be setting up a Kubernetes cluster on AWS. When you work\nwith Kubernetes, it’s useful to connect to the cluster from your remote machine. To\ndo that, you need a lot of connection and authentication details, which we’ll make\neasier by introducing a final step that provisions a Kubernetes configuration file that\ncan be downloaded to connect to the cluster once it is created.\nAdd the code in Example 6-7 to the end of the workflow file to implement the final\nstep of our job.\nExample 6-7. Upload kubeconfig\n    # Publish Assets\n    - name: Upload kubeconfig file\n      uses: actions/upload-artifact@v2\n      with:\n        name: kubeconfig\n        path: kubeconfig\nThis action uploads a file called kubeconfig from the local working directory of the\nbuild environment to your GitHub Actions repository. It assumes that the file exists,\nso we’ll need to create that file in the next chapter when we get into the details of\nbuildling our sandbox infrastructure.\nWith this final addition, you now have a complete infrastructure pipeline for your\nsandbox environment. GitHub manages the workflow files the same way it manages\ncode. So, we’ll need to commit our changes to save them. Click the Start commit but‐\nton, give the commit a description, and click the Commit new file button to finish\ncommiting the change (see Figure 6-13).\nBuilding an IaC Pipeline \n| \n131",
      "content_length": 2250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "Figure 6-13. Committing a change in GitHub Actions\nTaking Your Pipeline Further\nWe couldn’t fit all of the things that a production CI/CD IaC pipe‐\nline would have in this chapter. In particular, we had to omit inte‐\ngration testing from our pipeline activities. But we highly\nrecommend that you investigate and implement an integration-test\nstep for your Terraform code. The Go-based tool Terratest from\nGruntworks.io is worth looking at when you start introducing this\nkind of functionality.\nAll that’s left is to try out our workflow to make sure that it runs correctly.\n132 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 628,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "Testing the Pipeline\nTo test the pipeline that we’ve created, we’ll need to fire the trigger for the job we\ndefined. In our case, we need to create a Git tag in our repository with a label that\nstarts with the letter v. We could do this in the browser-based UI by using GitHub’s\nReleases feature. But since we’ll be doing most of our work outside GitHub on our\nlocal workstation, we’ll create the tag there instead.\nThe first thing we need to do is get the local clone of the repository up to date with\nthe changes we’ve made. To do that, open a shell in your workstation and run the\ncommand git pull in your env-sandbox directory. You should get a result that looks\nsomething like Example 6-8 indicating that we’ve pulled the new .github/workflows/\nmain.yml file into the local repository.\nExample 6-8. Pull changes into the local repository\nenv-sandbox msur$ git pull\nremote: Enumerating objects: 6, done.\nremote: Counting objects: 100% (6/6), done.\nremote: Compressing objects: 100% (3/3), done.\nremote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (5/5), done.\nFrom https://github.com/msur/env-sandbox\n   a6b706f..9923863  master     -> origin/master\nUpdating a6b706f..9923863\nFast-forward\n .github/workflows/main.yml | 54 ++++++++++++++++++++++++++++++++++++++++++++++++++\n 1 file changed, 54 insertions(+)\n create mode 100644 .github/workflows/main.yml\nNow that we are up to date with the GitHub-hosted repository, we can create a tag.\nSince this is just a test, we’ll label our release “v0.1.” Use the git tag command as\nshown in Example 6-9 to create the new tag with a label.\nExample 6-9. Create a v0.1 tag\nenv-sandbox msur$ git tag -a v0.1 -m \"workflow test\"\nAlthough we’ve created a tag, it only exists locally in our workstation clone of the\nrepository. In order to trigger our workflow, we’ll need to push this tag to our\nGitHub-hosted repository. Use the git push command with the name of the tag to\ndo this, as shown in Example 6-10.\nBuilding an IaC Pipeline \n| \n133",
      "content_length": 2013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "Example 6-10. Push the tag to GitHub\nenv-sandbox testuser$ git push origin v0.1\nEnumerating objects: 1, done.\nCounting objects: 100% (1/1), done.\nWriting objects: 100% (1/1), 165 bytes | 165.00 KiB/s, done.\nTotal 1 (delta 0), reused 0 (delta 0)\nTo https://github.com/mitraman/env-sandbox.git\n * [new tag]         v0.1 -> v0.1\nWe’ll be doing this sequence of tagging and pushing a tag whenever we want the pipe‐\nline to run. Pushing the tag should have triggered the workflow we’ve created in Git‐\nHub Actions, so all we need to do now is check to make sure it has run successfully.\nTo see the status of the run, go back to the browser-based GitHub interface and navi‐\ngate to Actions just like we did before. You should see something like Figure 6-14,\nindicating that our workflow job has completed successfully.\nFigure 6-14. A successful run of the pipeline\nYou can also see more details of the job that has been run. This can be useful if your\njob hasn’t run as expected and you need to do some troubleshooting. To see job\ndetails, select the workflow you want more details on (ours is called Sandbox Environ\nment Build), then select the job (in our case the job is called build). In the detail\nscreen you’ll be able to see what happened at each step of the job when the pipeline\nran (see Figure 6-15).\nGitHub Actions is a relatively new product and GitHub changes the UI frequently, so\nthe exact steps you need to take to reach this screen may have changed by the time\nyou read this. If you are having trouble getting to the steps for your job, refer to the\nGitHub documentation.\n134 \n| \nChapter 6: Building an Infrastructure Pipeline",
      "content_length": 1637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "Figure 6-15. Details of a job\nWith our pipeline successfully tested, we’ve finished setting up the tooling we need to\ndeclaratively build our infrastructure.\nSummary\nIn this chapter we set up a simple but powerful IaC pipeline based on some important\nDevOps principles and practices. We installed and used Terraform as our tool for\nimplementing the principles of IaC and immutable infrastructure. We set up a\nGitHub-based code repository to manage that code. Finally, we created a GitHub\nActions workflow as a CI/CD pipeline with automated testing to improve the safety\nand speed of our infrastructure changes.\nWe didn’t actually create any infrastructure resources, but we did walk through the\nsteps of making an infrastructure change. We created and edited a Terraform file, tes‐\nted and ran it locally, committed it to the repository, and tagged it to kick off a build\nand apply pipeline process. This sequence of steps is going to be our method for\nimmutable infrastructure development and we’ll be using it often in the next chapter\nwhere we design and build our microservices infrastructure.\nSummary \n| \n135",
      "content_length": 1113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "CHAPTER 7\nBuilding a Microservices Infrastructure\nIn the previous chapter we built a CI/CD pipeline for infrastructure changes. The\ninfrastructure for our microservices system will be defined in code and we’ll be able\nto use the pipeline to automate the testing and implementation of that code. With our\nautomated pipeline in place, we can start writing the code that will define the infra‐\nstructure for our microservices-based application. That’s what we’ll focus on in this\nchapter.\nSetting up the right infrastructure is vital to getting the most out of your microservi‐\nces system. Microservices give us a nice way of breaking the parts of our application\ninto bite-sized pieces. But we’ll need a lot of supporting infrastructure to make all\nthose bite-sized services work together properly. Before we can tackle the challenges\nof designing and engineering the services themselves, we’ll need to spend some time\nestablishing a network architecture and a deployment architecture for the services to\nuse.\nBy the end of this chapter, you’ll have built a cloud-based infrastructure designed to\nhost the microservices we’ll be building in the next chapter. We’ll start by introducing\nthe infrastructure and its components.\nInfrastructure Components\nThe infrastructure is the set of components that will allow us to deploy, manage, and\nsupport a microservices-based application. An infrastructure can include a lot of\nparts: hardware, software, networks, and tools. So the scope of components we’ll need\nto set up is quite large and getting all of those parts up and running is a big task.\nThankfully, as microservices approaches have matured, there’s been an explosion in\ntools and services that make this work easier. In our model, we’ll use tools like these\nas much as we can. We’ll also focus on getting the infrastructure to work with a single\n137",
      "content_length": 1851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "cloud platform (AWS) rather than building a cloud-agnostic application that can be\n“lifted and shifted” to other hosts. These decisions will make it possible for us to\ndefine a feature-rich infrastructure in the small space of this single chapter.\nBut it’s still quite a challenge! We’ll be covering a lot of topics in a small number of\npages. That means we’ll need to make some trade-offs. For example, we won’t be able\nto cover security, operations controls, or event logging and support. Instead we’ll\nfocus on designing and writing Terraform code to create a working network, an AWS\nmanaged Kubernetes service, and a declarative GitOps server. These three compo‐\nnents will give us the foundation we need to deploy our example microservices.\nNetwork design and Kubernetes are deep and complex topics that\nrequire much more discussion than we can afford to give them.\nThe good news is, you don’t need to be a network or Kubernetes\nexpert to set up your first microservices environment. If these are\nnew domains for you, you can follow the instructions we’ve pro‐\nvided to get those parts of the system up and running as a first step\nto learning more about them.\nLet’s kick things off by taking a quick tour of our main components, starting with the\nnetwork.\nThe Network\nMicroservices need to be run on a network. So we’ll need to make sure we have a suit‐\nable one set up. Since we’ve made the decision to host our services on an AWS cloud,\nwe’ll need to tailor our network design accordingly, and create a virtual network\ninstead of a physical one. We won’t need to worry about the details of physical rout‐\ners, cables, or network devices. Instead, we’ll need to learn to use the language of\nAWS network resources and configure those accordingly.\nWe’re going to keep our network design as simple as we can. We’ll build just enough\nto get our system up and running. But, we’ll still need to build and configure a few\nbasic resources to support the running of our future microservices:\nA virtual private cloud\nIn AWS, a virtual private cloud (VPC) is the parent object for a virtual network.\nWe’ll be creating and configuring a VPC as part of our network design.\nSubnets\nA VPC can be partitioned into multiple smaller networks called subnets. Subnets\ngive us a way of organizing network traffic and controlling access to resources.\nWe’ll be creating a total of four subnets as part of our network configuration.\n138 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "Routing and security\nIn addition to creating the VPC and subnet objects, we’ll be defining objects that\ndictate how traffic can flow in and out of them. For example, we’ll be defining\ntwo “private” subnets that will only accept traffic from inside our VPC.\nAs you can see, our network has a bit of complexity that we’ll need to deal with,\nincluding managing four “subnets” and connecting them. The main driver for this\ncomplexity comes from the needs of the Kubernetes service running on top of it. So,\nlet’s take a look at that next.\nThe Kubernetes Service\nThroughout this book, we’ve emphasized that reducing coordination costs is an\nimportant success factor for the system. We’ve also mentioned containers and con‐\ntainerization a few times in earlier chapters. That’s because containers are a great way\nof helping our teams get more done with less coordination costs. Containers give us\nthe advantages of running applications in a predictable, isolated system configuration\nwithout the overhead and heavy lifting that comes with a VM deployment. Microser‐\nvices and containers are a natural fit.\nIf you need help understanding containers and containerization,\nDocker’s website has a nice introductory explanation of containers.\nContainers make it easy for us to build microservices that run predictably across\nenvironments as a self-contained unit. But, containers don’t know how to start them‐\nselves, scale themselves, or heal themselves when they break. Containers work great\nin isolation, but a lot of operations work is required to manage them in production-\nlike environments. That’s where Kubernetes comes in.\nKubernetes is a container orchestration tool developed by Google. It solves the prob‐\nlems of working with containers at scale. Kubernetes provides a tool-based solution\nfor deploying, scaling, observing, and managing container-based applications. It can\nhelp you roll out and roll back container deployments, automatically deploy or\ndestroy container deployments based on demand patterns, mount storage systems,\nmanage secrets, and help with load balancing and traffic management. Kubernetes\ncan do a lot of complicated and complex work, and has quickly become an essential\npart of a microservices infrastructure stack.\nKubernetes is also pretty complicated itself, however. Because of this, we won’t be div‐\ning into the details of how Kubernetes works in this book. But we will be able to put\ntogether a working Kubernetes infrastructure hosted on AWS.\nInfrastructure Components \n| \n139",
      "content_length": 2510,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "If you want to learn about Kubernetes, a great introduction is pro‐\nvided by Kubernetes: Up and Running by Brendan Burns, Joe Beda,\nand Kelsey Hightower (O’Reilly).\nIf you’re new to Kubernetes, it’s worth understanding the big moving parts in a\nKubernetes system, so you’ll be able to follow along as we set one up:\nKubernetes cluster\nThe cluster is the parent object in a Kubernetes system. When you install Kuber‐\nnetes, you are installing a cluster. A cluster contains a control plane and a set of\nnodes.\nControl plane\nIn Kubernetes, the control plane is the “brains” of the cluster. It manages the sys‐\ntem by making decisions about starting, stopping, and replicating containers.\nThe control plane also provides an API that we can use to administrate the\ncluster.\nNodes\nThe runtime work happens in the nodes. Each node is a physical or virtual\nmachine that runs the container-based workload. In Kubernetes, nodes run pods.\nEach pod contains one or more containers. Every cluster has at least one node.\nIn our implementation, we’ll be using an AWS managed service for Kubernetes called\nElastic Kubernetes Service (EKS). We’re using EKS because it handles for us a lot of\nKubernetes’ complexity. It will help us provision the cluster and give us a control\nplane to use as a managed service. All we’ll need to do is configure the number and\ntypes of nodes we want and provision a suitable network.\nKey Decision: Use a Managed Kubernetes Service\nWe will use AWS EKS as a managed service for our Kubernetes cluster.\nThe last piece of our infrastructure is the GitOps deployment server. Let’s find out\nmore about what that is and how it will help us.\nThe GitOps Deployment Server\nIn Chapter 2, we introduced the release team. This is the team that will be responsible\nfor deploying microservices into production. We expect our microservices teams to\nuse a CI/CD pipeline to integrate, test, build, and deliver their services. But in our\noperating model they don’t own the actual deployment of the service into the system.\nWe made that decision because in our experience, production deployments are fairly\n140 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "complex and require special attention. To facilitate the deployment work of the\nrelease team, we’re introducing a special tool in our platform service offering: a\nGitOps deployment server.\nContinuous Delivery Versus Continuous Deployment\nOne of the confusing things about CI/CD is that the “CD” part can\nmean either Continuous Delivery or Continuous Deployment,\ndepending on who you ask. In our model, Continuous Delivery\n(CD) of microservices happens when our teams are able to auto‐\nmatically and continually ship their finished microservices as con‐\ntainers. Deployment happens when these containers are released\ninto the production environment by our release team.\nThe name GitOps, created by a company called WeaveWorks, describes a way of\nworking that uses Git as a “source of truth.” That means that whatever is in Git should\nbe the target state for the system. Like Terraform, GitOps prescribes a declarative\napproach. GitOps tools need to do the work of synchronizing system configurations\nto look like the state described in the Git repository. They also need to alert opera‐\ntions teams if the real world has drifted from the state defined in Git.\nArgo CD is a GitOps tool that facilitates the work of deploying Kubernetes applica‐\ntions. We’ve decided to use Argo CD for our release process because we like the\ndeclarative GitOps approach. If we didn’t use Argo CD we’d need to automate a series\nof Kubernetes calls to deploy an application. Instead, with the GitOps approach, we\nonly need to point Argo CD at a Git source and let it do the work of keeping our\nenvironment up to date.\nFor example, once we have Argo CD set up, we’ll be able to have it watch a microser‐\nvice code repository. When new changes are comitted and tested, Argo CD will be\nable to automatically deploy the new version of the service into the environment. This\ndeclarative, continuous deployment capability makes Argo CD a great product for\nour release teams to use.\nKey Decision: Deploy Microservices Using a GitOps Deployment Tool\nOur release teams will use Argo CD to manage microservice deployment into pro‐\nduction and production-like environments.\nBy the end of this chapter, we’ll have created a sandbox environment with an Argo\nCD server installed on top of an AWS managed Kubernetes cluster, running on an\nAWS VPC network. It will take a lot of Terraform code to make that happen, so pre‐\npare to roll up your sleeves. We’ll dive into the build in the next section.\nInfrastructure Components \n| \n141",
      "content_length": 2496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "Implementing the Infrastructure\nIn Chapter 6 we established a decision to use Terraform to write the code that defines\nour infrastructure and GitHub Actions to test and apply our infrastructure changes.\nIn this section, we’ll break our infrastructure design into discrete Terraform modules\nand call them from the sandbox environment we started building in the previous\nchapter. We’ll start by setting up the tools you’ll need in your infrastructure develop‐\nment workspace.\nInstalling kubectl\nIf you’ve followed along with the instructions in Chapter 6, you’ll already have an\nenvironment ready for the infrastructure build. So you’ll have:\n• An AWS instance and a configured operator account\n• Git, Terraform, and AWS CLI tools installed in your workstation\n• A GitHub Actions pipeline for the infrastructure\nIf you haven’t yet set up your GitHub Actions pipeline, or you had trouble getting it\nto work the way we’ve described, you can create a fork of a basic sandbox environ‐\nment by following the instructions in this book’s GitHub repository.\nIn addition to the setup we’ve done in the previous chapter, you’ll need to do one\nmore installation step to get ready for this chapter: installing kubectl. When we’re\ninstalling the Kubernetes service, we’ll need a way to test and interact with the Kuber‐\nnetes system. To do that, we’ll use the command-line application kubectl to interact\nwith a Kubernetes server.\nFollow the instructions in the Kubernetes documentation to install kubectl in your\nlocal system. We’ll leave it to you to pick the flavor that’s appropriate to your operat‐\ning system.\nWith the workspace set up and ready to go, we can move on to writing the Terraform\nmodules that will define the infrastructure.\nSetting Up the Module Repositories\nWhen you write professional software, it’s important to write clean, professional\ncode. When code is too difficult to understand, to maintain, or to change, the project\nbecomes costly to operate and maintain. All of that is true for our infrastructure code\nas well.\nSince we’re taking the IaC approach, we’ll need to apply good code practices to our\ninfrastructure project. The good news is that we have lots of existing guidance in our\nindustry on how to write code that is easier to learn, understand, and extend. The bad\n142 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "news is that not every principle and practice from traditional software development is\ngoing to be easy to implement in the IaC domain. That’s partly because the tooling\nand languages for IaC are still evolving and partly because the context of changing a\nlive, physical device is a different model from the traditional software development\nmodel.\nBut with Terraform we’ll be able to apply three essential coding practices that will\nhelp us write clean, easier-to-maintain code:\nUse modules\nWriting small functions that do one thing well\nEncapsulate\nHiding internal data structures and implementation details\nAvoid repetition\nDon’t repeat yourself (DRY), implementing code once in only one location\nTerraform’s built-in support for modules of infrastructure code will help us in using\nall three of these practices. We’ll be able to maintain our infrastructure code as a set of\nreusable, encapsulated modules. We’ll build modules for each of the architecturally\nsignificant parts of our system: networks, the API gateway, and the managed Amazon\nKubernetes service (EKS). Once we have our reusable modules in place, we’ll be able\nto implement another set of Terraform files that use them. We’ll be able to have a dif‐\nferent Terraform file for each environment that we want to create without repeating\nthe same infrastructure declarations in each one (see Figure 7-1).\nFigure 7-1. Reusing a network module\nThis approach allows us to easily spin up new environments by creating new Terra‐\nform files that reuse the modules we’ve developed. It also lets us make changes in just\none place when we want to change an infrastructure configuration across all\nenvironments. We can start by creating a simple module that defines a basic network\nand an environment file that uses it.\nImplementing the Infrastructure \n| \n143",
      "content_length": 1811,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "The infrastructure code we are writing in this chapter uses Terraform’s module struc‐\nture. Each module will have its own directory and contain variables.tf, main.tf, and\noutput.tf files. The advantage of this approach is that you can define a module once\nand use it in a parameterized way to build multiple environments. You can learn\nmore about these modules in the Terraform documentation.\nWe’re going to create two modules for our microservices infrastructure. First, we’ll\ncreate an AWS networking module that contains a declarative configuration of our\nsoftware-defined network. We’ll also create a Kubernetes module that defines an\nAWS-based managed Kubernetes configuration for our environments. We’ll be able\nto use both of these modules to create our sandbox environment.\nDon’t Use Our Configuration Files in Your Production Environment!\nWe’ve done our best to design an infrastructure that mirrors pro‐\nduction environments that large organizations use for microservi‐\nces. But space constraints prevent us from giving you a\ncomprehensive set of configurations that will work for your specific\nenvironment, security needs, and constraints. You can use this\nchapter as a quick starter and guide to the tools you’ll need, but we\nadvise that you spend time designing your own production-grade\ninfrastructure, configuration, and architecture.\nIn Chapter 6, we created a GitHub code repository for the sandbox environment code\nand its CI/CD pipeline. We’ll be using that code repository in this chapter, but we’ll\nalso create a new repository for each module we write. Terraform has built-in support\nfor importing modules that are managed as GitHub repositories, so it will be easy to\npull them in when we want to use them.\nTo get started, let’s create the repositories for all the modules we’ll be writing in this\nchapter. Go ahead and create three new public GitHub-hosted repositories with the\nnames described in Table 7-1.\nTable 7-1. Infrastructure module names\nRepository name\nVisibility Description\nmodule-aws-network\nPublic\nA Terraform module that creates the network\nmodule-aws-kubernetes\nPublic\nA Terraform module that sets up EKS\nmodule-argo-cd\nPublic\nA Terraform module that installs Argo CD into a cluster\nIf you aren’t sure how to create a GitHub repository, you can follow\nthe GitHub instructions.\n144 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2376,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "We recommend that you make these repositories public so that they are easier to\nimport into your Terraform environment definition. You can use private repositories\nif you prefer—you’ll just have to add some authentication information to your import\ncommand so that Terraform can get to the files correctly. You should also add\na .gitignore file to these repositories so you don’t end up with a lot of Terraform work‐\ning files pushed to your GitHub server. You can do that by choosing a Terra‐\nform .gitignore in the GitHub web GUI, or save the contents as a .gitignore file in the\nroot directory of your code repository, as outlined on this GitHub site.\nWith our three GitHub module repositories created and ready to be populated, we\ncan dive into the work of actually writing the actual infrastructure definitions—start‐\ning with the network.\nThe Network Module\nThe virtual network is a foundational part of our infrastructure, so it makes sense for\nus to start by defining the network module. In this section, we’ll write an AWS net‐\nwork module that will support a specific Kubernetes and microservices architecture\nand workload. Because it’s a module, we’ll be writing input, main, and output code—\njust like we’d write inputs, logic, and return values for an application function. When\nwe’re done, we’ll be able to use this module to easily provision a network environ‐\nment by specifying just a few input values.\nWe’ll be writing the network infrastructure code in the module-aws-network GitHub\nrepository that you created earlier. We’ll be creating and editing Terraform files in the\nroot directory of this module. If you haven’t already done so, clone the repository into\nyour local environment and get your favorite text editor ready.\nA completed listing for this AWS network module is available in\nthis book’s GitHub repository.\nNetwork module outputs\nLet’s start by defining the resources that we expect the networking module to pro‐\nduce. We’ll do this by creating a Terraform file called output.tf in the root directory of\nmodule-aws-network, as in Example 7-1.\nImplementing the Infrastructure \n| \n145",
      "content_length": 2115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "Example 7-1. module-aws-network/output.tf\noutput \"vpc_id\" {\n  value = aws_vpc.main.id\n}\noutput \"subnet_ids\" {\n  value = [\n    aws_subnet.public-subnet-a.id,\n    aws_subnet.public-subnet-b.id,\n    aws_subnet.private-subnet-a.id,\n    aws_subnet.private-subnet-b.id]\n}\noutput \"public_subnet_ids\" {\n  value = [aws_subnet.public-subnet-a.id, aws_subnet.public-subnet-b.id]\n}\noutput \"private_subnet_ids\" {\n  value = [aws_subnet.private-subnet-a.id, aws_subnet.private-subnet-b.id]\n}\nBased on the Terraform module output file, we can see that the network module cre‐\nates a VPC resource that represents the software-defined network for our system.\nWithin that network, our module will also create four logical subnets—these are the\nbounded parts of our network (or subnetworks). Two of these subnets will be public,\nmeaning that they will be accessible over the internet. Later, we’ll use all four subnets\nfor our Kubernetes cluster setup and eventually we’ll deploy our microservices into\nthem.\nNetwork module main configuration\nWith the output of our module defined, we can start putting together the declarative\ncode that builds it and creates the outputs we are expecting. In a Terraform module,\nwe’ll be creating and editing a file named main.tf in the root directory of the module-\naws-network repository.\nGetting the Source Code\nTo help you understand the network implementation, we’ve broken\nthe main.tf source code file into smaller parts. You can find the\ncomplete source code listing for this module at this book’s GitHub\nsite.\nWe’ll start our module implementation by creating an AWS VPC resource. Terraform\nprovides us with a special resource for defining AWS VPCs, so we’ll just need to fill in\na few parameters to create our definition. When we create a resource in Terraform,\n146 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "we define the parameters and configuration details for it in the Terraform syntax.\nWhen we apply these changes, Terraform will make an AWS API call and create the\nresource if it doesn’t exist already.\nYou can find all the Terraform documentation for the AWS pro‐\nvider on the Terraform site. You can also consult this documenta‐\ntion if you’re building a similar implementation in GCP or Azure.\nCreate a file called main.tf in the root of your network module’s repository and add\nthe Terraform code in Example 7-2 to the main.tf file to define a new AWS VPC\nresource.\nExample 7-2. modules-aws-network/main.tf\nprovider \"aws\" {\n  region = var.aws_region\n}\nlocals {\n  vpc_name = \"${var.env_name} ${var.vpc_name}\"\n  cluster_name = \"${var.cluster_name}-${var.env_name}\"\n}\n## AWS VPC definition\nresource \"aws_vpc\" \"main\" {\n  cidr_block = var.main_vpc_cidr\n  tags = {\n    \"Name\"                                        = local.vpc_name,\n    \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\",\n  }\n}\nThe network module starts with a declaration that it is using the AWS provider. This\nis a special instruction that lets Terraform know that it needs to download and install\nthe libraries it will need in order to communicate with the AWS API and create\nresources on our behalf. When we validate or apply this file in Terraform, it will\nattempt to connect to the AWS API using the credentials we’ve configured in the sys‐\ntem as environment variables. We’re also specifying an AWS region here so that Ter‐\nraform knows which region it should be working in.\nWe’ve also specified two local variables using a Terraform locals block. These vari‐\nables define a naming standard that will help us differentiate environment resources\nin the AWS console. This is especially important if we plan to create multiple environ‐\nments in the same AWS account space as it will help us avoid naming collisions.\nImplementing the Infrastructure \n| \n147",
      "content_length": 1929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "After the local variable declaration, you’ll find the code for creating a new AWS VPC.\nAs you can see, there isn’t much to it, but it does define two important things: a CIDR\nblock and a set of descriptive tags.\nClassless inter-domain routing (CIDR) is a standard way of describing an IP address\nrange for the network. It’s a shorthand string that defines which IP addresses are\nallowed inside a network or a subnet. For example, a CIDR value of 10.0.0.0/16\nwould mean that you could bind to any IP address between 10.0.0.0 and 10.0.255.255\ninside the VPC. We’ll be defining a pretty standard CIDR range for you when we\nbuild the sandbox environment, but for more details on how CIDRs work and why\nthey exist, you can read about them in the RFC.\nWe’ve also added some tag values to the VPC. Resource tags are useful because they\ngive us a way of easily identifying groups of resources when we need to administrate\nthem. Tags are also useful for automated tasks and for identifying resources that\nshould be managed in specific ways. In our definition, we have defined a “Name” tag\nto make our VPC easier to identify. We’ve also defined a Kubernetes tag that identifies\nthis cluster as a target for our Kubernetes cluster (which we’ll define in “Defining the\nEKS cluster” on page 162).\nAlso, notice that in a few cases we’ve referenced a variable instead of an actual value\nin our configuration. For example, our CIDR block is defined as var.main_vpc_cidr\nand it has a Name tag with the value local.vpc_name. These are Terraform variables,\nand we’ll define their values later when we use this module as part of our sandbox\nenvironment. The variables are what makes the modules reusable—by changing the\nvariable values we can change the types of environments that we create.\nWith our main VPC defined, we can move on to configuring the subnets for the net‐\nwork. As we mentioned earlier in this chapter, we’ll be using Amazon’s managed\nKubernetes service (EKS) to run our workload. In order for EKS to function properly,\nwe’ll need to have subnets defined in two different “availability zones.” In AWS, an\navailability zone represents a separate physical data center. It’s a useful construct,\nbecause even though the AWS resources are virtual, they’re still running on a com‐\nputer plugged into an outlet somewhere. By using two availability zones for our\ndeployment, we ensure that our services will still work even if one of the data centers\ngoes down.\nIn addition to configuring two availability zones, Amazon also recommends a VPC\nconfiguration with both public and private subnets. So our network will have public\nsubnets that allow traffic from the internet and private subnets that will only allow\ntraffic from inside the VPC. When EKS is running, it will deploy load balancers in the\npublic subnet to manage inbound traffic, which will be routed to our containerized\nmicroservices deployed in the private subnets.\n148 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "To meet those requirements, we’ll define a total of four subnets. Two of them will be\ndesignated as public subnets, so they’ll be accessible over the web. The other two sub‐\nnets will be private. We’ll also split our public and private subnets up so that they are\ndeployed in separate availability zones. When we’re done, we’ll have a network that\nlooks like Figure 7-2.\nFigure 7-2. AWS subnet design\nWe’ve already specified a CIDR for the IP range in our VPC. Now we’ll need to split\nup those IP addresses for the subnets to use. Since the subnets are inside of the VPC,\nthey’ll need to have a CIDR that is within the boundaries of the VPC IP range. We\nwon’t actually be defining those IP addresses in our module though. Instead, we’ll use\nvariables just like we did for the VPC.\nIn addition to the CIDR blocks, we’ll specify the availability zones for our subnets as a\nparameter. Rather than hardcoding the name of the availability zone, we’ll use a spe‐\ncial Terraform type called data that will let us dynamically choose the zone name. In\nthis case, we’ll put public-subnet-a and private-subnet-a in data.aws.availabil\nity_zones.available.names[0] and public-subnet-b and private-subnet-b in\ndata.aws.availability_zones.available.names[1]. Using dynamic data like this\nmakes it easier for us to spin up this infrastructure in different regions.\nFinally, we’ll add a name tag so that we can easily find our network resources through\nthe admin and ops consoles. We’ll also need to add some EKS tags to the subnet\nresources so that our AWS Kubernetes service will know which subnets we are using\nand what they are for. We’ll tag our public subnets with an elb role so that EKS\nknows it can use these subnets to create and deploy an elastic load balancer. We’ll tag\nthe private subnets with an internal-elb role to indicate that our workloads will be\ndeployed into them and can be load balanced. For more details on how AWS EKS\nuses load balancer tags, consult the AWS documentation.\nAdd the Terraform code in Example 7-3 to the end of your main.tf file in order to\ndeclare the subnet configuration.\nImplementing the Infrastructure \n| \n149",
      "content_length": 2139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "Example 7-3. modules-aws-network/main.tf (subnets)\n# subnet definition\ndata \"aws_availability_zones\" \"available\" {\n  state = \"available\"\n}\nresource \"aws_subnet\" \"public-subnet-a\" {\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = var.public_subnet_a_cidr\n  availability_zone = data.aws_availability_zones.available.names[0]\n  tags = {\n    \"Name\"                                        = (\n      \"${local.vpc_name}-public-subnet-a\"\n    )\n    \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/elb\"                      = \"1\"\n  }\n}\nresource \"aws_subnet\" \"public-subnet-b\" {\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = var.public_subnet_b_cidr\n  availability_zone = data.aws_availability_zones.available.names[1]\n  tags = {\n    \"Name\"                                        = (\n      \"${local.vpc_name}-public-subnet-b\"\n    )\n    \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/elb\"                      = \"1\"\n  }\n}\nresource \"aws_subnet\" \"private-subnet-a\" {\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = var.private_subnet_a_cidr\n  availability_zone = data.aws_availability_zones.available.names[0]\n  tags = {\n    \"Name\"                                        = (\n      \"${local.vpc_name}-private-subnet-a\"\n    )\n    \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/internal-elb\"             = \"1\"\n  }\n}\nresource \"aws_subnet\" \"private-subnet-b\" {\n  vpc_id            = aws_vpc.main.id\n150 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 1577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "cidr_block        = var.private_subnet_b_cidr\n  availability_zone = data.aws_availability_zones.available.names[1]\n  tags = {\n    \"Name\"                                        = (\n      \"${local.vpc_name}-private-subnet-b\"\n    )\n    \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/internal-elb\"             = \"1\"\n  }\n}\nIn Terraform, a data element is a way of querying the provider for\ninformation. In the network module, we’re using the aws_availa\nbility_zones data element to ask AWS for availability zone IDs in\nthe region we’ve specified. This is a nice way to avoid hardcoding\nvalues into the module.\nAlthough we’ve configured four subnets and their IP ranges, we haven’t yet defined\nthe network rules that AWS will need to manage traffic through them. To finish our\nnetwork design, we’ll need to implement a set of routing tables that define what traffic\nsources we will allow into our subnets. For example, we’ll need to establish how traffic\nwill be routed through our public subnets and how each of the subnets will be\nallowed to communicate with each other.\nWe’ll start by defining the routing rules for our two public subnets: public-subnet-a\nand public-subnet-b. To make these subnets accessible on the internet, we’ll need to\nadd a special resource to our VPC called an internet gateway. This is an AWS network\ncomponent that connects our private cloud to the public internet. Terraform gives us\na resource definition for the gateway, so we’ll use that and tie it to our VPC with the\nvpc_id configuration parameter.\nOnce we’ve added the internet gateway, we’ll need to define routing rules that let AWS\nknow how to route traffic from the gateway into our subnets. To do that, we’ll create\nan aws_route_table resource that allows all traffic from the internet (which we’ll\nidentify with CIDR block 0.0.0/0) through the gateway. Then we just need to create\nassociations between our two public subnets and the table we’ve defined.\nAdd the Terraform code in Example 7-4 to main.tf to define routing instructions for\nour network.\nImplementing the Infrastructure \n| \n151",
      "content_length": 2110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "Example 7-4. modules-aws-network/main.tf (public routes)\n# Internet gateway and routing tables for public subnets\nresource \"aws_internet_gateway\" \"igw\" {\n  vpc_id = aws_vpc.main.id\n  tags = {\n    Name = \"${local.vpc_name}-igw\"\n  }\n}\nresource \"aws_route_table\" \"public-route\" {\n  vpc_id = aws_vpc.main.id\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.igw.id\n  }\n  tags = {\n    \"Name\" = \"${local.vpc_name}-public-route\"\n  }\n}\nresource \"aws_route_table_association\" \"public-a-association\" {\n  subnet_id      = aws_subnet.public-subnet-a.id\n  route_table_id = aws_route_table.public-route.id\n}\nresource \"aws_route_table_association\" \"public-b-association\" {\n  subnet_id      = aws_subnet.public-subnet-b.id\n  route_table_id = aws_route_table.public-route.id\n}\nWith the routes for our public subnets defined, we can dive into the setup for our two\nprivate subnets. The route configuration for the private subnets will be a bit more\ncomplicated than what we’ve done so far. That’s because we’ll need to define a route\nfrom our private subnet out to the internet to allow our Kubernetes Pods to talk to\nthe EKS service.\nFor that kind of route to work, we’ll need a way for nodes in our private subnet to talk\nto the internet gateway we’ve deployed in the public subnets. In AWS, we’ll need to\ncreate a network address translation (NAT) gateway resource that gives us a path out.\nWhen we create the NAT, we’ll also need to assign it a special kind of IP address\ncalled an elastic IP address (or EIP). Because this is an AWS construct, the IP is a real\ninternet-accessible network address, unlike all the other addresses in our network,\nwhich are virtual and exist inside AWS alone. Since real IP addresses aren’t unlimited,\nAWS limits the amount of these available. Unfortunately, we can’t create an NAT\nwithout one, so we’ll have to use two of them—one for each NAT we are creating.\n152 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 1965,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "Add the Terraform code in Example 7-5 to implement an NAT gateway in our\nnetwork.\nExample 7-5. modules-aws-network/main.tf (NAT gateway)\nresource \"aws_eip\" \"nat-a\" {\n  vpc = true\n  tags = {\n    \"Name\" = \"${local.vpc_name}-NAT-a\"\n  }\n}\nresource \"aws_eip\" \"nat-b\" {\n  vpc = true\n  tags = {\n    \"Name\" = \"${local.vpc_name}-NAT-b\"\n  }\n}\nresource \"aws_nat_gateway\" \"nat-gw-a\" {\n  allocation_id = aws_eip.nat-a.id\n  subnet_id     = aws_subnet.public-subnet-a.id\n  depends_on    = [aws_internet_gateway.igw]\n  tags = {\n    \"Name\" = \"${local.vpc_name}-NAT-gw-a\"\n  }\n}\nresource \"aws_nat_gateway\" \"nat-gw-b\" {\n  allocation_id = aws_eip.nat-b.id\n  subnet_id     = aws_subnet.public-subnet-b.id\n  depends_on    = [aws_internet_gateway.igw]\n  tags = {\n    \"Name\" = \"${local.vpc_name}-NAT-gw-b\"\n  }\n}\nIn addition to the NAT gateway we’ve created, we’ll need to define routes for our pri‐\nvate subnets. Add the Terraform code in Example 7-6 to main.tf to complete the defi‐\nnition of our network routes.\nExample 7-6. modules/network/main.tf (private routes)\nresource \"aws_route_table\" \"private-route-a\" {\n  vpc_id = aws_vpc.main.id\n  route {\n    cidr_block     = \"0.0.0.0/0\"\n    nat_gateway_id = aws_nat_gateway.nat-gw-a.id\nImplementing the Infrastructure \n| \n153",
      "content_length": 1248,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "}\n  tags = {\n    \"Name\" = \"${local.vpc_name}-private-route-a\"\n  }\n}\nresource \"aws_route_table\" \"private-route-b\" {\n  vpc_id = aws_vpc.main.id\n  route {\n    cidr_block     = \"0.0.0.0/0\"\n    nat_gateway_id = aws_nat_gateway.nat-gw-b.id\n  }\n  tags = {\n    \"Name\" = \"${local.vpc_name}-private-route-b\"\n  }\n}\nresource \"aws_route_table_association\" \"private-a-association\" {\n  subnet_id      = aws_subnet.private-subnet-a.id\n  route_table_id = aws_route_table.private-route-a.id\n}\nresource \"aws_route_table_association\" \"private-b-association\" {\n  subnet_id      = aws_subnet.private-subnet-b.id\n  route_table_id = aws_route_table.private-route-b.id\n}\nThat’s it for our main network definition. When we eventually run this Terraform file,\nwe’ll have an AWS software-defined network that is ready for Kubernetes and our\nmicroservices. But, before we can use it, we’ll need to define all of the input variables\nthat this module needs. Although we’ve referenced a lot of var values in our code,\nTerraform modules require us to identify all of the input variables we’ll be using in a\nspecific file called variables.tf. If we don’t do that, we won’t be able to pass variable\nvalues into our module.\nNetwork module variables\nCreate a file in the root folder of the network module called variables.tf. Add the Ter‐\nraform code in Example 7-7 to variables.tf to define the inputs for the module.\n154 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 1440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "Example 7-7. modules/network/variables.tf\nvariable \"env_name\" {\n  type = string\n}\nvariable \"aws_region\" {\n  type = string\n}\nvariable \"vpc_name\" {\n  type    = string\n  default = \"ms-up-running\"\n}\nvariable \"main_vpc_cidr\" {\n  type = string\n}\nvariable \"public_subnet_a_cidr\" {\n  type = string\n}\nvariable \"public_subnet_b_cidr\" {\n  type = string\n}\nvariable \"private_subnet_a_cidr\" {\n  type = string\n}\nvariable \"private_subnet_b_cidr\" {\n  type = string\n}\nvariable \"cluster_name\" {\n  type = string\n}\nAs you can see, the variable definitions are fairly self-explanatory. They describe a\nname, optional description, and type value. In our module we’re only using string val‐\nues. In some cases, we’ve also provided a default value so that those inputs don’t\nalways have to be defined for every environment. We’ll give the module values for\nthose variables when we use it to create an environment.\nImplementing the Infrastructure \n| \n155",
      "content_length": 928,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "It’s good practice to include a description attribute for every vari‐\nable in your Terraform module. This improves the maintainability\nand usability of your modules and becomes increasingly important\nover time. We’ve done this for the Terraform files we’ve published\nin GitHub, but we’ve removed the descriptions in all our examples\nto save space in the book.\nThe Terraform code for our network module is now complete. At this point, you\nshould have a list of files that looks something like this in your module directory:\ndrwxr-xr-x   3 msur  staff   96 14 Jun 09:57 ..\ndrwxr-xr-x   7 msur  staff  224 14 Jun 09:58 .\n-rw-r--r--   1 msur  staff   23 14 Jun 09:57 README.md\ndrwxr-xr-x  13 msur  staff  416 14 Jun 09:57 .git\n-rw-r--r--   1 msur  staff    0 14 Jun 09:58 main.tf\n-rw-r--r--   1 msur  staff  612 14 Jun 09:58 variables.tf\n-rw-r--r--   1 msur  staff   72 14 Jun 09:58 outputs.tf\nWith the code written, we’ll be testing the network module by creating a sandbox\nenvironment network, but before we use the module we should make sure we haven’t\nmade any syntax errors. The Terraform command-line application includes some\nhandy features to format and validate code. If you haven’t already installed the Terra‐\nform client in your local system, you can find a binary for your operating system on\nthe Terraform site.\nUse the following Terraform command while you are in your module’s working direc‐\ntory to format the module’s code:\nmodule-aws-network$ terraform fmt\nThe fmt command will lint, or format, all the Terraform code in the working direc‐\ntory and ensure that it conforms to a set of built-in style guidelines. It will automati‐\ncally make those changes for you and will list any files that have been updated.\nNext, run terraform init so that Terraform can install the AWS provider libraries.\nWe need to do this so that we can validate the code. Note that you’ll need to have\nAWS credentials defined for this to work. If you haven’t done that yet, follow the\ninstructions in the previous chapter:\nmodule-aws-network$ terraform init\nIf you run into any problems, try to fix those before you continue; the Terraform doc‐\numentation has a helpful section on troubleshooting. Finally, you can run the vali\ndate command to make sure that our module is syntactically correct:\nmodule-aws-network$ terraform validate\nSuccess! The configuration is valid.\n156 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "If you need to debug your Terraform code, you can set the environ‐\nment variable TF_LOG to INFO or DEBUG. That will instruct Terra‐\nform to emit logging info to standard output.\nWhen you are satisfied that the code is formatted and valid, you can commit your\nchanges to the GitHub repository. If you’ve been working in a local repository, you\ncan use the following command to push your changes to the main repository:\nmodule-aws-network$ git add .\nmodule-aws-network$ git commit -m \"network module created\"\n[master ddb7e41] network module created\n 3 files changed, 226 insertions(+)\nmodule-aws-network$ git push\nOur Terraform-based network module is now complete and available for use. It has a\nvariables.tf file that describes the required and optional input variables to use it. It has\na main.tf file that declaratively defines the resources for our network design. Finally, it\nhas an outputs.tf file that defines the significant resources that we’ve created in the\nmodule. Now we can use the module to create a real network in our sandbox\nenvironment.\nCreate a sandbox network\nThe nice thing about using Terraform modules is that we can create our environ‐\nments easily in a repeatable way. Outside of the specific values we’ve defined in the\nvariables.tf file, any environment that we create with the module we’ve defined will\noperate with a network infrastructure that we know and understand. That means we\ncan expect our microservices to work in a predictable way as we move them through\ntesting and release environments since we have reduced the level of variation.\nBut to apply the module we’ve defined and create a new environment, we’ll need to\ncall it from a Terraform file that defines values for the module’s variables. To do that,\nwe’ll create a sandbox environment that demonstrates a practical example of using a\nTerraform module. If you followed the steps in Chapter 6, you’ll already have a code\nrepository for your sandbox environment with a single main.tf file in it.\nIn order to use the network module that we’ve created, we’ll use a special Terraform\nresource called module. It allows us to reference a Terraform module that we’ve cre‐\nated and pass in values for the variables that we’ve defined. Terraform expects a prop‐\nerty called source to exist in the module that indicates where it can find the code.\nIn our case, we want Terraform to retrieve the network module from a GitHub repos‐\nitory. To do this, we need to use a source property that starts with the string \"git\nhub.com\" and contains the path of our repository. That lets Terraform know it needs\nto pull the source from GitHub.\nImplementing the Infrastructure \n| \n157",
      "content_length": 2654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "For example, a source value of \"github.com/implementing-microservices/module-\naws-network\" references our example network module. You can find the path for\nyour module’s repository by copying the path from its GitHub URL (see Table 7-2).\nTable 7-2. Sandbox environment network variable\nName\nDescription\nExample\nYOUR_NETWORK_MOD\nULE_REPO_PATH\nThe path to your module’s repository in GitHub\ngithub.com/implementing-\nmicroservices/module-aws-network\nWhen you have the path for your network module ready, open the main.tf file for the\nsandbox environment you created in Chapter 6. Add the Terraform code in\nExample 7-8 after the # Network Configuration comment. Don’t forget to replace\nthe source value with the path of your network module’s GitHub repository.\nExample 7-8. env-sandbox/main.tf (network)\n...\n# Network Configuration\nmodule \"aws-network\" {\n  source = \"github.com/{YOUR_NETWORK_MODULE_REPO_PATH}\"\n  env_name              = local.env_name\n  vpc_name              = \"msur-VPC\"\n  cluster_name          = local.k8s_cluster_name\n  aws_region            = local.aws_region\n  main_vpc_cidr         = \"10.10.0.0/16\"\n  public_subnet_a_cidr  = \"10.10.0.0/18\"\n  public_subnet_b_cidr  = \"10.10.64.0/18\"\n  private_subnet_a_cidr = \"10.10.128.0/18\"\n  private_subnet_b_cidr = \"10.10.192.0/18\"\n}\n# EKS Configuration\n# GitOps Configuration\nAmazon’s S3 bucket names must be globally unique, so you’ll need\nto change the value of bucket to something that is unique and\nmeaningful for you. Refer to “Creating an S3 Backend for Terra‐\nform” on page 115 for instructions on how to set up the backend. If\nyou want to do a quick and dirty test, omit the backend definition\nand Terraform will store state locally in your filesystem.\n158 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 1775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "Our infrastructure pipeline will apply Terraform changes, but before we kick it off we\nneed to check to make sure that the Terraform code we’ve written will work. A good\nfirst step is to format and validate the code locally:\n$ terraform fmt\n[...]\n$ terraform init\n[...]\n$ terraform validate\nSuccess! The configuration is valid.\nIf you need to debug the networking module and end up making\ncode changes, you may need to run the following command in\nyour sandbox environment directory:\n$ terraform get -update\nThis will instruct Terraform to pull the latest version of the net‐\nwork module from GitHub.\nIf the code is valid, we can get a plan to validate the changes that Terraform will make\nwhen they are applied. It’s always a good idea to do a dry run and examine the\nchanges that will be made before you actually change the environment, so make this\nstep a part of your workflow. To get the Terraform plan, run this command:\n$ terraform plan\nTerraform will provide you with a list of the resources that will be created, deleted,\nand updated. If Terraform and AWS are new to you, the plan might be difficult to\nevaluate and understand in detail. But, you should still be able to get a general sense\nof what is going to happen. Since this is the first update, the plan should list a lot of\nnew resources that Terraform will create. When you’re ready, you can push the code\nto the GitHub repository and tag it for release:\n$ git add .\n$ git commit -m \"initial network release\"\n$ git push origin\n$ git tag -a v1.0 -m \"network build\"\n$ git push origin v1.0\nThere are two git push commands that we need to run. The first\none pushes the code changes we’ve made and the second only\npushes the tag.\nWith the code tagged and pushed, our GitHub Actions pipeline should take over and\nstart building the network for our sandbox environment. You’ll need to log in to Git‐\nHub and check the Actions tab in your sandbox environment repository to make sure\nImplementing the Infrastructure \n| \n159",
      "content_length": 1981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "that everything goes according to plan. If you don’t remember how to do that, you’ll\nfind instructions in Chapter 6.\nYou can test that the VPC has been successfully created by making an AWS CLI call.\nRun the following command to list the VPCs with a CIDR block that matches the one\nthat we’ve defined:\n$ aws ec2 describe-vpcs --filters Name=cidr,Values=10.10.0.0/16\nYou should get a JSON body back describing the VPC that we created. If that has hap‐\npened, it indicates that you now have an AWS network running and ready to use. It’s\nnow time to start writing the module for the Kubernetes service.\nThe Kubernetes Module\nOne of the most important parts of our microservices infrastructure is the Kuber‐\nnetes layer that orchestrates our container-based services. If we set it up correctly,\nKubernetes will give us an automated solution for resiliency, scaling, and fault toler‐\nance. It will also give us a great foundation for deploying our services in a dependable\nway. On top of that, an Istio service mesh gives us a powerful way of managing traffic\nand improving the way our microservices communicate.\nTo build our Kubernetes module, we’ll follow the same steps that we did to build our\nnetwork module. We’ll start by defining a set of output variables that define what the\nmodule will produce, then we’ll write the code that declaratively defines the configu‐\nration that Terraform will create. Finally, we’ll define the inputs. As we mentioned\nearlier in this chapter, we are managing each of infrastructure modules in it’s own\nGitHub code repository. So make sure you start by creating a new GitHub repository\nfor our Kubernetes module if you haven’t done so already.\nImplementing Kubernetes can get very complicated. So, to get our system up and\nrunning as quickly as possible, we’ll use a managed service that will hide some of the\nsetup and management complexity for us. Since we are running on AWS in our\nexamples, we’ll use the EKS bundled in Amazon’s cloud offering.\nThe configuration for managed Kubernetes services tends to be\nvery vendor specific, so the examples we provide here will likely\ntake some reworking if you want to use them in Google Cloud,\nAzure, or another hosted service.\nAn EKS cluster contains two parts: a control plane that hosts the Kubernetes system\nsoftware and a node group that hosts the VMs that our microservices will run on. In\norder to configure EKS, we’ll need to provide parameters for both of these areas.\nWhen the module is finished running, we can return an EKS cluster identifier so that\nwe have the option of inspecting or adding to the cluster with other modules.\n160 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "With all that in mind, let’s dive into the code that will make it come to life. We’ll be\nworking in the module-aws-kubernetes GitHub repository that you created earlier,\nso make sure you start by cloning it to your local machine. When you’ve done that,\nwe can begin by editing the Terraform outputs file.\nA completed listing for this Kubernetes module is available in this\nbook’s GitHub repository.\nKubernetes module outputs\nWe’ll start by declaring the outputs that our module provides. Create a Terraform file\ncalled outputs.tf in the root directory of the module-aws-kubernets repository and\nadd to it the code in Example 7-9.\nExample 7-9. module-aws-kubernetes/outputs.tf\noutput \"eks_cluster_id\" {\n  value = aws_eks_cluster.ms-up-running.id\n}\noutput \"eks_cluster_name\" {\n  value = aws_eks_cluster.ms-up-running.name\n}\noutput \"eks_cluster_certificate_data\" {\n  value = aws_eks_cluster.ms-up-running.certificate_authority.0.data\n}\noutput \"eks_cluster_endpoint\" {\n  value = aws_eks_cluster.ms-up-running.endpoint\n}\noutput \"eks_cluster_nodegroup_id\" {\n  value = aws_eks_node_group.ms-node-group.id\n}\nThe main value we’re returning is the identifier for the EKS cluster that we’ll be creat‐\ning in this module. The rest of the values need to be returned so that we can access\nthe cluster from other modules once the cluster is ready and operational. For exam‐\nple, we’ll need the endpoint and certificate data when we install the Argo CD server\ninto this EKS cluster at the end of the chapter.\nWhile the output of our module is pretty simple, the work of getting our EKS-based\nKubernetes system set up is going to be a bit more complicated. Just like we did\nImplementing the Infrastructure \n| \n161",
      "content_length": 1696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "before, we’ll build the module’s main Terraform file in parts before we test it and\napply it.\nDefining the EKS cluster\nTo start, create a Terraform file called main.tf in the root directory of your Kubernetes\nmodule and add an AWS provider definition, as in Example 7-10.\nExample 7-10. module-aws-kubernetes/main.tf\nprovider \"aws\" {\n  region = var.aws_region\n}\nRemember that we’ll be using the Terraform naming convention of var to indicate\nvalues that can be replaced by variables when our module is invoked.\nAs we mentioned earlier, we’re going to use Amazon’s EKS to create and manage our\nKubernetes installation. But EKS will need to create and modify AWS resources on\nour behalf in order to run. So we’ll need to set up permissions in our AWS account so\nthat it can do the work it needs to do. We’ll need to define policies and security rules \nat the overall cluster level and also for the VMs or nodes that EKS will be spinning up\nfor us to run microservices on.\nWe’ll start by focusing on the rules and policies for the entire EKS cluster. Add the\nTerraform code in Example 7-11 to your main.tf file to define a new cluster access\nmanagement policy.\nExample 7-11. module-aws-kubernetes/main.tf (cluster access management)\nlocals {\n  cluster_name = \"${var.cluster_name}-${var.env_name}\"\n}\nresource \"aws_iam_role\" \"ms-cluster\" {\n  name = local.cluster_name\n  assume_role_policy = <<POLICY\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"eks.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n162 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 1637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "]\n}\nPOLICY\n}\nresource \"aws_iam_role_policy_attachment\" \"ms-cluster-AmazonEKSClusterPolicy\" {\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\"\n  role       = aws_iam_role.ms-cluster.name\n}\nThe snippet here establishes a trust policy that allows the AWS EKS service to act on\nyour behalf. It defines a new identity and access management role for our EKS service\nand attaches a policy called AmazonEKSClusterPolicy to it. This policy has been\ndefined by AWS for us and gives the EKS the permissions it needs to create VMs and\nmake network changes as part of its Kubernetes management work. Notice that we\nare also defining and using a local variable for the name of the cluster. We’ll use that\nvariable throughout the module.\nNow that the cluster service’s role and policy are defined, add the code in\nExample 7-12 to your module’s main.tf file to define a network security policy for the\ncluster.\nExample 7-12. module-aws-kubernetes/main.tf (network security policy)\nresource \"aws_security_group\" \"ms-cluster\" {\n  name        = local.cluster\n  vpc_id      = var.vpc_id\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  tags = {\n    Name = \"ms-up-running\"\n  }\n}\nA VPC security group restricts the kind of traffic that can go into and out of the net‐\nwork. The Terraform code we’ve just written defines an egress rule that allows unre‐\nstricted outbound traffic, but doesn’t allow any inbound traffic, because there is no\ningress rule defined. Notice that we are applying this security group to a VPC that\nwill be defined by an input variable. When we use this module, we can give it the ID\nof the VPC that our networking module has created.\nImplementing the Infrastructure \n| \n163",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "With these policies and a security group defined for the EKS cluster, we can now add\nthe declaration for the cluster itself to the main.tf Terraform file (see Example 7-13).\nExample 7-13. module-aws-kubernetes/main.tf (cluster definition)\nresource \"aws_eks_cluster\" \"ms-up-running\" {\n  name     = local.cluster_name\n  role_arn = aws_iam_role.ms-cluster.arn\n  vpc_config {\n    security_group_ids = [aws_security_group.ms-cluster.id]\n    subnet_ids         = var.cluster_subnet_ids\n  }\n  depends_on = [\n    aws_iam_role_policy_attachment.ms-cluster-AmazonEKSClusterPolicy\n  ]\n}\nThe EKS cluster definition we’ve just created is pretty simple. It simply references the\nname, role, policy, and security group values we defined earlier. It also references a set\nof subnets that the cluster will be managing. These subnets will be the ones that we\ncreated earlier in the networking module, and we’ll be able to pass them into this\nKubernetes module as a variable.\nWhen AWS creates an EKS cluster, it automatically sets up all of the management\ncomponents that we need to run our Kubernetes cluster. This is called the control\nplane because it’s the brain of our Kubernetes system. But in addition to the control\nplane, our microservices need a place where they can run. In Kubernetes, that means\nwe need to set up nodes—the physical or VMs that containerized workloads can run\non.\nOne of the advantages of using a managed Kubernetes service like EKS is that we can\noffload some of the work of managing the creation, removal, and updating of Kuber‐\nnetes nodes. For our configuration, we’ll define a managed EKS node group and let\nAWS provision resources and interact with the Kubernetes system for us. But to get a\nmanaged node group running, we’ll still need to define a few important configuration\nvalues.\nDefining the EKS node group\nJust like we did for our cluster, we’ll begin the node configuration by defining a role\nand some security policies. Add the node group IAM definitions in Example 7-14 to\nthe Kubernetes module’s main.tf file.\n164 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "Example 7-14. module-aws-kubernetes/main.tf (node group IAM)\n# Node Role\nresource \"aws_iam_role\" \"ms-node\" {\n  name = \"${local.cluster_name}.node\"\n  assume_role_policy = <<POLICY\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\nPOLICY\n}\n# Node Policy\nresource \"aws_iam_role_policy_attachment\" \"ms-node-AmazonEKSWorkerNodePolicy\" {\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\"\n  role       = aws_iam_role.ms-node.name\n}\nresource \"aws_iam_role_policy_attachment\" \"ms-node-AmazonEKS_CNI_Policy\" {\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\"\n  role       = aws_iam_role.ms-node.name\n}\n[...]\nresource \"aws_iam_role_policy_attachment\" \"ms-node-ContainerRegistryReadOnly\" {\n[...]\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"\n  role       = aws_iam_role.ms-node.name\n}\nThe role and policies in this Terraform snippet will allow any nodes that are created\nto communicate with Amazon’s container registries and VM services. We need these\npolicies because the nodes in our Kubernetes system will need to be able to provision\ncomputing resources and access containers in order to run services. For more details\non the IAM role for EKS worker nodes, check out the AWS EKS documentation.\nNow that we have our node’s role and policy resources defined, we can write the dec‐\nlaration for a node group that uses them. In EKS, a managed node group needs to\nspecify the types of compute and storage resources it will use along with some\ndefined limits for the number of individual nodes or VMs that can be created\nImplementing the Infrastructure \n| \n165",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "automatically. This is important because we are letting EKS automatically provision\nand scale our nodes. We don’t want to inadvertently consume massive amounts of\nAWS resources and end up with a correspondingly massive bill.\nWe could hardcode all of these parameters in our module, but instead we’ll use input\nvariables as values for the size limits, disk size, and CPU types. That way we’ll be able\nto use the same Kubernetes module to create different kinds of environments. For\nexample, a development environment can be set up to use minimal resources, while a\nproduction environment can be more robust.\nAdd the Terraform code in Example 7-15 to the end of the module’s main.tf file to\ndefine our EKS node group.\nExample 7-15. module-aws-kubernetes/main.tf (node group)\nresource \"aws_eks_node_group\" \"ms-node-group\" {\n  cluster_name    = aws_eks_cluster.ms-up-running.name\n  node_group_name = \"microservices\"\n  node_role_arn   = aws_iam_role.ms-node.arn\n  subnet_ids      = var.nodegroup_subnet_ids\n  scaling_config {\n    desired_size = var.nodegroup_desired_size\n    max_size     = var.nodegroup_max_size\n    min_size     = var.nodegroup_min_size\n  }\n  disk_size      = var.nodegroup_disk_size\n  instance_types = var.nodegroup_instance_types\n  depends_on = [\n    aws_iam_role_policy_attachment.ms-node-AmazonEKSWorkerNodePolicy,\n    aws_iam_role_policy_attachment.ms-node-AmazonEKS_CNI_Policy,\n    aws_iam_role_policy_attachment.ms-node-AmazonEC2ContainerRegistryReadOnly,\n  ]\n}\nThe node group declaration is the last part of our EKS configuration. We have\nenough here to be able to call this module from our sandbox environment and\ninstantiate a running Kubernetes cluster on the AWS EKS service. Our module’s out‐\nputs will return the values that are needed to connect to the node group once it’s run‐\nning. But it’s also useful to provide those connection details in a configuration file for\nthe kubectl CLI that most operators use for Kubernetes management.\nOur last step is to generate a kubeconfig file that we’ll be able to use to connect to the\ncluster. Append the code in Example 7-16 to your module’s main.tf file.\n166 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "Example 7-16. module-aws-kubernetes/main.tf (generate kubeconfig)\n# Create a kubeconfig file based on the cluster that has been created\nresource \"local_file\" \"kubeconfig\" {\n  content  = <<KUBECONFIG_END\napiVersion: v1\nclusters:\n- cluster:\n    \"certificate-authority-data: >\n   ${aws_eks_cluster.ms-up-running.certificate_authority.0.data}\"\n    server: ${aws_eks_cluster.ms-up-running.endpoint}\n  name: ${aws_eks_cluster.ms-up-running.arn}\ncontexts:\n- context:\n    cluster: ${aws_eks_cluster.ms-up-running.arn}\n    user: ${aws_eks_cluster.ms-up-running.arn}\n  name: ${aws_eks_cluster.ms-up-running.arn}\ncurrent-context: ${aws_eks_cluster.ms-up-running.arn}\nkind: Config\npreferences: {}\nusers:\n- name: ${aws_eks_cluster.ms-up-running.arn}\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1alpha1\n      command: aws-iam-authenticator\n      args:\n        - \"token\"\n        - \"-i\"\n        - \"${aws_eks_cluster.ms-up-running.name}\"\n    KUBECONFIG_END\n  filename = \"kubeconfig\"\n}\nThis code looks complicated, but it’s actually fairly simple. We are using a special Ter‐\nraform resource called local_file to create a file named kubeconfig. We are then\npopulating kubeconfig with YAML content that defines the connection parameters for\nour Kubernetes cluster. Notice that we are getting the values for the YAML file from\nthe EKS resources that we created in the module.\nWhen Terraform runs this block of code, it will create a kubeconfig file in a local\ndirectory. We’ll be able to use that file to connect to the Kubernetes environment\nfrom CLI tools. We made a special provision for this file when we built our pipeline\nin Chapter 6. When you run the infrastructure pipeline, you’ll be able to download\nthis populated configuration file and use it to connect to the cluster. This configura‐\ntion file will make it a lot easier for you to connect to the cluster from your machine.\nWe’re almost done writing our Kubernetes service module; all that’s left is to define\nthe variables.\nImplementing the Infrastructure \n| \n167",
      "content_length": 2030,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "Kubernetes module variables\nTo declare the variables for our Kubernetes module, create a file called variables.tf in\nyour module-aws-kubernetes repository and add the code in Example 7-17.\nExample 7-17. module-aws-kubernetes/variables.tf\nvariable \"aws_region\" {\n  type        = string\n  default     = \"eu-west-2\"\n}\nvariable \"env_name\" {\n  type = string\n}\nvariable \"cluster_name\" {\n  type = string\n}\nvariable \"ms_namespace\" {\n  type    = string\n  default = \"microservices\"\n}\nvariable \"vpc_id\" {\n  type = string\n}\nvariable \"cluster_subnet_ids\" {\n  type = list(string)\n}\nvariable \"nodegroup_subnet_ids\" {\n  type = list(string)\n}\nvariable \"nodegroup_desired_size\" {\n  type    = number\n  default = 1\n}\nvariable \"nodegroup_min_size\" {\n  type    = number\n  default = 1\n}\nvariable \"nodegroup_max_size\" {\n  type    = number\n  default = 5\n}\n168 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 889,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "variable \"nodegroup_disk_size\" {\n  type = string\n}\nvariable \"nodegroup_instance_types\" {\n  type = list(string)\n}\nOur AWS Kubernetes module is now fully written. As we did for our network mod‐\nule, we’ll take a moment to clean up the formatting and validate the syntax of the\ncode by running the following Terraform commands:\nmodule-aws-kubernetes$ terraform fmt\n[...]\nmodule-aws-kubernetes$ terraform init\n[...]\nmodule-aws-kubernetes$ terraform validate\nSuccess! The configuration is valid.\nWhen you are satisfied that the code is valid, commit your changes and push them to\nGitHub, so that we can use this module in the sandbox environment:\n$ git add .\n$ git commit -m \"kubernetes module complete\"\n$ git push origin\nWith the EKS module ready to go, we can go back to our sandbox Terraform file and\nuse it.\nCreate a sandbox Kubernetes cluster\nNow that our complex Kubernetes system is wrapped up in a simple module, the\nwork of setting it up in our sandbox environment is pretty simple. All we’ll need to do\nis call our module with the input parameters that we want. Remember that our sand‐\nbox environment is defined in its own code repository and has its own Terraform file\ncalled main.tf which we’ve used to set up the network. We’ll be editing that file again,\nbut this time we’ll add a call to the Terraform module.\nIf you recall, we gave some of our input variables default values. To keep things sim‐\nple, we’ll just use those default values in our sandbox environment. We’ll also need to\npass some of the output variables from our network module into this Kubernetes\nmodule so that it installs the cluster on the network we’ve just created. But beyond\nthose inputs, you’ll need to define the aws_region value for your installation. This\nshould be the same as the value you used for the network module and the backend\nconfiguration. You’ll also need to set the source parameter to point to your GitHub-\nhosted module.\nUpdate the main.tf file of your sandbox environment so that it uses the Kubernetes\nmodule you’ve just created. You can add the module reference immediately after the\nImplementing the Infrastructure \n| \n169",
      "content_length": 2130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "#EKS Configuration placeholder we put in the file earlier. You’ll also need to replace\nthe token {YOUR_EKS_MODULE_PATH} with the path to your module’s GitHub reposi‐\ntory (see Example 7-18).\nExample 7-18. env-sandbox/main.tf (Kubernetes)\n...\n# Network Configuration\n...\n# EKS Configuration\nmodule \"aws-eks\" {\n  source = \"*github.com/{YOUR_EKS_MODULE_PATH}*\"\n  ms_namespace       = \"microservices\"\n  env_name           = local.env_name\n  aws_region         = local.aws_region\n  cluster_name       = local.k8s_cluster_name\n  vpc_id             = module.aws-network.vpc_id\n  cluster_subnet_ids = module.aws-network.subnet_ids\n  nodegroup_subnet_ids     = module.aws-network.private_subnet_ids\n  nodegroup_disk_size      = \"20\"\n  nodegroup_instance_types = [\"t3.medium\"]\n  nodegroup_desired_size   = 1\n  nodegroup_min_size       = 1\n  nodegroup_max_size       = 3\n}\n# GitOps Configuration\nNow you can commit and push this file into your CI/CD infrastructure pipeline and\ncreate a working EKS cluster. Don’t forget that you’ll need to use a tag to get the build\nto kick off. For example, you can run the following commands to create a 1.1 version\nof the infrastructure:\n$ git add .\n$ git commit -m \"initial k8s release\"\n$ git push\n$ git tag -a v1.1 -m \"k8s build\"\n$ git push origin v1.1\nBe prepared to wait for a few minutes for a result as provisioning a new EKS cluster\ncan take up to 10 to 15 minutes. When it’s done, you’ll have a powerful container-\nbased infrastructure up and running, ready to run your microservices resiliently.\n170 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 1590,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "The AWS EKS cluster we’ve defined here will accrue charges even\nwhen it’s idle. We recommend that you destroy the environment\nwhen you are not using it. You’ll find instructions for doing that in\n“Cleaning Up the Infrastructure” on page 177.\nYou can test that the cluster has been provisioned by running the following AWS CLI\ncommand:\n$  aws eks list-clusters\nIf all has gone well, you’ll get the following response:\n{\n    \"clusters\": [\n        \"ms-cluster-sandbox\"\n    ]\n}\nOur final step is to install a GitOps deployment tool that will come in handy when it’s\ntime to release our services into our environment’s Kubernetes cluster.\nSetting Up Argo CD\nAs we mentioned earlier, we’re going to complete our infrastructure setup with a\nGitOps server that we’ll use later in the book. We’ll continue to follow the module\npattern by creating a Terraform module for Argo CD that we can call to bootstrap the\nserver in our sandbox environment. Unlike the other modules, we’ll be installing\nArgo CD on the Kubernetes system that we’ve just instantiated.\nTo do that, we’ll need to let Terraform know that we’re using a different host. Up until\nnow, we’ve been using the AWS provider, which lets Terraform communicate with\nAWS through its API. For our Argo CD installation we’ll use a Kubernetes provider;\nthis enables Terraform to issue Kubernetes commands and install the application to\nour new cluster. We’ll also use a package-management system called Helm to do the\ninstallation. We’ll introduce Helm a little bit later, but for now, we’ll need to set up\nTerraform to use it as a provider.\nWe’ll install this resource into the Kubernetes cluster rather than on the AWS\nplatform.\nThat means we won’t be using the AWS provider. Instead, we’ll use Terraform’s\nKubernetes and Helm providers.\nA completed version of this module is available in this book’s Git‐\nHub repository.\nImplementing the Infrastructure \n| \n171",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "Create a file called main.tf file in the root directory of the module-argo-cd Git reposi‐\ntory that you created earlier. Add the code in Example 7-19 to set up the providers we\nneed for the installation.\nExample 7-19. module-argo-cd/main.tf\nprovider \"kubernetes\" {\n  load_config_file       = false\n  cluster_ca_certificate = base64decode(var.kubernetes_cluster_cert_data)\n  host                   = var.kubernetes_cluster_endpoint\n  exec {\n    api_version = \"client.authentication.k8s.io/v1alpha1\"\n    command     = \"aws-iam-authenticator\"\n    args        = [\"token\", \"-i\", \"${var.kubernetes_cluster_name}\"]\n  }\n}\nprovider \"helm\" {\n  kubernetes {\n    load_config_file       = false\n    cluster_ca_certificate = base64decode(var.kubernetes_cluster_cert_data)\n    host                   = var.kubernetes_cluster_endpoint\n    exec {\n      api_version = \"client.authentication.k8s.io/v1alpha1\"\n      command     = \"aws-iam-authenticator\"\n      args        = [\"token\", \"-i\", \"${var.kubernetes_cluster_name}\"]\n    }\n  }\n}\nTo configure the Kubernetes provider, we’re using the properties of the EKS cluster\nthat we provisioned earlier. These properties let Terraform know it needs to use the\nAWS authenticator to connect to the cluster along with the certificate that we’ve\nprovided.\nAs we mentioned earlier, we’re also using a provider for Helm. Helm is a popular way\nof describing a Kubernetes deployment and for distributing Kubernetes applications\nas packages. It’s similar to other package-management tools, such as apt-get in the\nLinux world, and is designed to make installation of Kubernetes-based applications\nsimple and easy. To configure our Helm provider, we simply need to provide a few\nKubernetes connection parameters.\nA Helm deployment is called a chart. We’ll be using a Helm chart provided by the\nArgo CD community to install the Argo CD server. Add the code in Example 7-20 to\nthe main.tf file to complete the installation declaration.\n172 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2006,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "Example 7-20. module-argo-cd/main.tf (Helm)\nresource \"kubernetes_namespace\" \"example\" {\n  metadata {\n    name = \"argo\"\n  }\n}\nresource \"helm_release\" \"argocd\" {\n  name       = \"msur\"\n  chart      = \"argo-cd\"\n  repository = \"https://argoproj.github.io/argo-helm\"\n  namespace  = \"argo\"\n}\nThis code creates a namespace for the Argo CD installation and uses the Helm pro‐\nvider to perform the installation. All that’s left to complete the Argo CD module is to\ndefine some variables.\nVariables for Argo CD\nCreate a file called variables.tf in your Argo CD module repository and add the code\nin Example 7-21.\nExample 7-21. module-argo-cd/variables.tf\nvariable \"kubernetes_cluster_id\" {\n  type = string\n}\nvariable \"kubernetes_cluster_cert_data\" {\n  type = string\n}\nvariable \"kubernetes_cluster_endpoint\" {\n  type = string\n}\nvariable \"kubernetes_cluster_name\" {\n  type = string\n}\nvariable \"eks_nodegroup_id\" {\n  type = string\n}\nWe need to define these variables so that we can configure the Kubernetes and Helm\nproviders in our code. So we’ll need to grab them from the Kubernetes module’s out‐\nput when we call it in our sandbox’s Terraform file. Before we get to that step, let’s\nImplementing the Infrastructure \n| \n173",
      "content_length": 1212,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "format and validate the code we’ve written in the same way as we did for our other\nmodules:\nmodule-argocd$ terraform fmt\n[...]\nmodule-argocd$ terraform init\n[...]\nmodule-argocd$ terraform validate\nSuccess! The configuration is valid.\nWhen you are satisfied that the code is valid, commit your code changes and push\nthem to the GitHub repository so that we can use the module in our sandbox\nenvironment:\n$ git add .\n$ git commit -m \"ArgoCD module init\"\n$ git push origin\nNow, as we’ve done before, we just need to call this module from our sandbox\ndefinition.\nInstalling Argo CD in the sandbox\nWe want the Argo CD installation to happen as part of our sandbox environment\nbootstrapping, so we need to call the module from the Terraform definition in our\nsandbox environment. Add the code in Example 7-22 to the end of your sandbox\nmodule’s main.tf file to install Argo CD. Don’t forget to use your module’s GitHub\nrepository path in the source property of the module definition.\nExample 7-22. env-sandbox/main.tf (Argo CD)\n...\n# Network Configuration\n...\n# EKS Configuration\n...\n# GitOps Configuration\nmodule \"argo-cd-server\" {\n  source = \"*github.com/{YOUR_ARGOCD_MODULE_PATH}*\"\n  kubernetes_cluster_id        = module.aws-eks.eks_cluster_id\n  kubernetes_cluster_name      = module.aws-eks.eks_cluster_name\n  kubernetes_cluster_cert_data = module.aws-eks.eks_cluster_certificate_data\n  kubernetes_cluster_endpoint  = module.aws-eks.eks_cluster_endpoint\n  eks_nodegroup_id = module.aws-eks.eks_cluster_nodegroup_id\n}\n174 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 1574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "Now, you can tag, commit, and push the Terraform file into your CI/CD pipeline just\nlike you’ve done before. For example, the following command will push a v1.2 tag\ninto the repository and kick off the pipeline process:\nYou’ll need to wait for the EKS build to complete before tagging\nand committing these Argo CD sandbox changes. Otherwise, there\nwon’t be a Kubernetes cluster for Argo CD to be deployed to.\n$ git add .\n$ git commit -m \"initial ArgoCD release\"\n$ git push origin\n$ git tag -a v1.2 -m \"ArgoCD build\"\n$ git push origin v1.2\nWhen our pipeline is finished applying changes, you’ll have a GitOps server that will\nhelp deploy microservices easier and more reliably. With that step completed, we’ve\nfinished defining and provisioning the sandbox environment. All that’s left is to test it\nand see if it works.\nTesting the Environment\nBefore we finish with our infrastructure implementation, it’s a good idea to run a test\nand make sure that the environment has been provisioned as expected. We’ll do this\nby verifying that we can log in to the Argo CD web console. That will prove that the\nentire stack is running and operational. But in order to do that, we’ll need to set up \nour kubectl CLI application.\nEarlier in this chapter, when we were creating the Terraform code for our Kubernetes\nmodule, we added a local file resource to create a kubeconfig file. Now, we need to\ndownload that file so that we can connect to the EKS cluster using the kubectl\napplication.\nTo retrieve this file, navigate to your sandbox GitHub repository in your browser and\nclick on the Actions tab. You should see a list of builds with your latest run at the top\nof the screen. When you select the build that you just performed, you should see an\nartifact called “kubeconfig” that you can click and download.\nIf you’re having trouble finding the page to download the artifact,\ntry following the instructions in the GitHub documentation.\nImplementing the Infrastructure \n| \n175",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "GitHub will package the artifact as a ZIP file, so after downloading it you’ll need to\ndecompress the package. Inside the ZIP file you should find a file called kubeconfig.\nTo use it, you just need to set an environment variable called KUBECONFIG that points\nto it. This will let the Kubernetes command-line application know where to find it.\nFor example, if the kubeconfig file is in your ~/Downloads directory, use the following\nvalue:\n$ export KUBECONFIG=~/Downloads/kubeconfig\nIf you like, you can copy the kubeconfig file to ~/.kube/config and\navoid having to set an environment variable. Just make sure you\naren’t overwriting a Kubernetes configuration you’re already using.\nYou can test that everything runs as expected by issuing the following command:\n$ kubectl get svc\nYou should see something like the following in response:\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   172.20.0.1   <none>        443/TCP   2h\nThis shows us that our network and EKS services were provisioned and we were able\nto successfully connect to the cluster. To get this information, kubectl makes an API\ncall to the Kubernetes cluster we’ve just created. Getting this response back is proof\nthat our cluster is up and running. As a final test, we’ll check to make sure that Argo\nCD has been installed in the cluster. Run the following command to verify that the\nArgo CD pods are running:\n$ kubectl get pods -n \"argo\"\nNAME                                                 READY STATUS    RESTARTS\nmsur-argocd-application-controller-5bddfb78fc-9jpzj  1/1   Running            0\nmsur-argocd-dex-server-84cd5fc9b9-bjzrm              1/1   Running            0\nmsur-argocd-redis-dc867dd9c-rpgww                    1/1   Running            0\nmsur-argocd-repo-server-75474975cc-j7lws             1/1   Running            0\nmsur-argocd-server-5cc998b478-wvkrr                  1/1   Running            0\nA Kubernetes Pod represents a deployable unit, consisting of one\nor more container images.\nLater in the process, we’ll get a chance to use Argo CD, the Kubernetes cluster, and\nthe rest of the infrastructure we’ve designed. But now that we know our pipeline and\nconfigurations work, it’s time to tear it all down. Don’t worry, though: with our code \nwritten, it will be easy to create our environment again when we need it.\n176 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 2407,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "Cleaning Up the Infrastructure\nWe now have our infrastructure up and running. But, if you aren’t planning on using\nit right away, it’s a good idea to clean things up so you don’t incur any costs to have it\nrunning. In particular, the elastic IP addresses that we used for our network can be\ncostly if we leave them up. Since our environment is now completely defined in Ter‐\nraform declarative files, we can re-create it in the same way whenever we need it, so\ndestroying the existing environment is a low-risk activity.\nTerraform will automatically destroy resources in the correct order for us because it\nhas internally created a dependency graph. To destroy the sandbox environment, use\nthe following steps:\n1. Navigate to the working directory of your sandbox environment code on your\nmachine. This is the same directory you used in “Installing Argo CD in the sand‐\nbox” on page 174.\n2. Pull the latest version of the code from the repository:\nenv-sandbox$ git pull\nInstall the Terraform providers that our environment code uses (we’ll need these\nso we can destroy the resources):\nenv-sandbox$ terraform init\n3. After Terraform has finished downloading plug-ins, enter the following com‐\nmand to destroy the sandbox environment:\nenv-sandbox$ terraform destroy\n4. Terraform will display the resources that it will destroy. You’ll need to say yes to\ncontinue to the removal process. It will probably take about five minutes to com‐\nplete. When it’s done all of the AWS resources that we created will be gone.\nWe’re able to destroy these AWS resources from our local\nmachine because we have AWS access and secret keys stored in\na local credentials file. This shouldn’t be the case for a produc‐\ntion or secured environment.\n5. When it’s done, you’ll see a message that looks like this:\nDestroy complete! Resources: 29 destroyed.\n6. To verify that the EKS resources have been removed, you can run the following\nAWS CLI command to list EKS clusters:\n$ aws eks list-clusters\nImplementing the Infrastructure \n| \n177",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "You should get back a response indicating that there are no EKS clusters left in\nyour instance:\n{\n    \"clusters\": []\n}\nYou can also run the following commands to double-check that the other billable\nresources have been removed:\n$ aws ec2 describe-vpcs --filters Name=cidr,Values=10.10.0.0/16\n$ aws elbv2 describe-load-balancers\nIt’s not absolutely necessary to run these CLI commands if terra\nform destroy returns successfully. We have included them so you\ncan double-check that they are really gone, so you will not be billed\nunexpectedly.\nIf something has gone wrong, you’ll need to use the AWS console and remove the\nresources manually. Consult the AWS documentation if you have trouble deleting\nresources through the console.\nSummary\nWe did a lot in this chapter. We created a Terraform module for our software-defined\nnetwork that spanned two availability zones in a single region. Next, we created a\nmodule that instantiates an AWS EKS cluster for Kubernetes. We also implemented\nan Argo CD GitOps server into the cluster using a Helm package. Finally, we imple‐\nmented a sandbox environment as code that uses all of these modules in a declarative,\nimmutable way.\nWe went into a lot of detail with the Terraform code in this chapter. We did that so\nyou could get a feel for what it takes to define an environment using infrastructure as\ncode, immutability, and a CI/CD pipeline. We also wanted you to get hands on with\nthe Terraform module pattern and some of the design decisions you’ll need to make\nfor your infrastructure. As we learn more about the microservices we are deploying,\nwe may need additional infrastructure modules, but later in the book we’ll use pre‐\nwritten, hosted code instead of walking through it all line by line.\nIn Chapter 8, we’ll get back to our example microservices and start the work of devel‐\noping them. When we’re done, we’ll be able to release them into the infrastructure\nwe’ve just designed.\n178 \n| \nChapter 7: Building a Microservices Infrastructure",
      "content_length": 1993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "CHAPTER 8\nDeveloper Workspace\nIn Chapter 1 we discussed how a microservices architecture is typically most benefi‐\ncial when it is applied to complex systems, and explained some of the underlying rea‐\nsons supporting this observation.\nIn any reasonably complex system, the only sustainable way to ensure that well-\nintentioned participants behave in a way that leads to positive and predictable collab‐\noration is to make the right behaviors the absolute easiest and most intuitive ones. If\ndoing the “right thing” is hard, over time most people will choose the path of least\nresistance—which will steer them the wrong way. It is therefore essential to invest\nearly in setting up repeatable, predictable, standardized development processes that\navoid unnecessary complexity and create an intuitively comfortable structure for\nyour developers.\nInvesting in an exceptional developer experience that aims at a\nconsistent and intuitive approach for all developers to easily “do the\nright thing” is one of the most underappreciated prerequisites of\nfacilitating a successful microservices culture.\nThis is why developing robust continuous integration and continuous deployment\n(CI/CD) pipelines, for both your code as well as infrastructure, is a key enabler for\nyour microservices efforts. Because of the modular nature of the architecture and the\nemphasis on independent deployability of each microservice, you will end up with\nmany pipelines. One thing you should certainly avoid is every team creating a pipe‐\nline for their microservice in their own way, without any consistency with the code‐\nbases of other microservices. Creating a new microservice should be a quick and\npredictable process. Ideally, it should be a templated process in which the majority of\nthings are fully automated.\n179",
      "content_length": 1793,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "Robust CI/CD pipelines are crucial, but just as important is how the local develop‐\nment workspace is set up and what practices teams use for creating code. Most soft‐\nware engineers spend the majority of their time writing code on their laptops.\nTapping into this process early and ensuring that the right guidance and tooling are\nprovided at this stage can also have tremendous benefits later in the process. To be\nclear, we do not mean you should dictate every aspect of a developer’s workflow. For\ninstance, if you ever try to standardize the code editor that members of any sizable\nteam must use, you will quickly make a lot of passionate enemies and achieve noth‐\ning. However, without becoming an overbearing tyrant we can and should declare\nsome fundamental principles the team must stick with.\nIn this chapter, we will start by introducing a set of 10 highly opinionated rules and\nguidelines for a developer workspace setup that we have used with great success on\nsome of our past microservices projects. Next, we’ll walk you through setting up local\ncontainerized environments on multiple platforms. We will show you how to kick-\nstart both vanilla Docker as well as a lightweight Kubernetes locally. Finally, we will\nshow you an advanced example of containerization: how to install a local Cassandra\ndatabase in the newly minted Docker setup.\nBy the end of this chapter, you should have a fully functioning, containerized infra‐\nstructure that’s ready for writing some microservices code. More importantly, you will\ngain a solid understanding of the principles we use to set up projects for easy and\nintuitive development. We will use these principles in Chapter 9 to properly lay out\nour code, when we get into the development phase of our implementation.\nCoding Standards and the Developer’s Setup\nWhen trying to introduce any organizational standards, it’s useful to clarify and agree\non goals, so people can relate to the “why” of the process before they are presented\nwith the actual mechanics, the “how” and “what” of it.\nWe recommend following three high-level goals as a starting point:\nCode can be set up in a short time frame\nThere are few things more frustrating than joining a new team and spending a\nweek or more on being able to set up an environment that allows you to start\ncoding in the new codebase. Whether you are joining a new company, or just\njumping in to give a helping hand to a team next to you in your current job, there\nare few more certain ways of killing any excitement and momentum than getting\nstuck on “How do I even run it?” Alas, this happens way too often. Our goal is\nthat a new developer unfamiliar to the code should be able to set up a microser‐\nvice, or a collection of microservices forming a logical subsystem, in under an\nhour!\n180 \n| \nChapter 8: Developer Workspace",
      "content_length": 2821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "New microservices can be created quickly, easily, and predictably\nThere is a lot of boilerplate related to jump-starting a new service. You need\nproper code templates for each supported tech stack, such as Java, Go, Node,\nPython, etc.; an automated testing and data management setup; dependencies\nsuch as data storage configured; and a skeleton of a pipeline bootstrapped; to\nname just a few. The worst-case scenario would be for a developer starting a new\nmicroservice to have to figure out all of these aspects from scratch, every single\ntime.\nActually, an even worse scenario would be different developers varying these\naspects unnecessarily for similar microservices. When you have a large codebase,\none of the most impactful things you can do to avoid chaos is to establish consis‐\ntency and familiarity. Providing well-thought-out templates for each of the stan‐\ndard tech stacks is a powerful way of achieving such consistency and high quality,\nwhile also increasing development speed.\nQuality control must be automated\nEnforcement of a company’s software development quality standards must be\nautomated and not left to human error.\nBased on these goals, we can derive a set of fundamental guidelines for a developer\nworkspace setup.\n10 Workspace Guidelines for a Superior Developer Experience\nThe following guidelines are fairly opinionated and are based on the experience of the\nauthors of this book. We recommend that you use these guidelines as a starting point.\nAfter you have had experience building services with our guidelines, we expect that\nyou may consider modifying some of them to better fit your individual needs and\nexperiences:\n1. Make Docker the only dependency.\nThe “works for me” syndrome plagues many developer teams. It’s essential that\nanybody be able to easily create the same environment. As such, elaborate, man‐\nual setups should be banned.\nWe live in the era of containerization and teams should leverage it. To set up\ncode, we should only expect to see the Docker runtime and Docker Compose on\nthe host machine—nothing else! It should not matter if the machine is running\nWindows, macOS, or Linux and what libraries are present. Such assumptions are\nexactly what lead to broken setups. For instance, there should be no set expecta‐\ntions about a specific version of Python, Go, Java, etc., being present on the\ndeveloper’s machine. Setup instructions must be automated, not codified in\nREAD.ME files.\nCoding Standards and the Developer’s Setup \n| \n181",
      "content_length": 2486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "2. Remote or local should not matter.\nSetup should work regardless of whether a developer runs code on their own lap‐\ntop or on a cloud server via an IDE’s remote development/SFTP plug-ins. This\nshould hold true by default and if there is a case in which this cannot be done, a\ncause for an exception must be justified and documented.\n3. Ensure a heterogeneous-ready workspace.\nA good setup should accommodate multiple microservices written in multiple\nprogramming languages, using multiple data storage systems. A microservices\narchitecture assumes the ability to combine heterogeneous microservices; it\ndoesn’t mean just putting one codebase in one container or standardizing on one\ntechnology stack. Too often we see “[some-language] microservices framework”\nin marketing materials. Well, guess what—if 100% of your microservices are\nwritten in Java, there is something wrong with the setup, and no, you don’t get to\nchuckle if all your services are written in a “cool” language like Go.\nNow, for the record: this does not in any way mean that in a well-managed\nmicroservices environment you should see every team picking whatever language\nand databases they feel like and going for it. Quite the opposite: when uncertain,\ndefinitely try to exercise caution and go with two, at most three, stacks. The point\nhere is that you should be able to introduce a new stack if you genuinely needed\nit, so in your example setup, you have to show that you actually can, that is, by \nimplementing more than one stack.\nThe Rule of Twos\nWe have found proactively practicing heterogeneity in a\nmicroservices setup to be a great approach. For any critical\ncomponent in your system, make sure that you are using at\nleast two alternatives in production at the same time—even\nwhen you only need one. You should also make sure that you\nhave an infrastructure to support the two alternatives as easily\nas you would use a single one. We call this approach the “Rule\nof Twos”.\nSay that most of your APIs are written in Node.js—a truly wonderful, I/O-\noptimized stack for writing APIs. See if some of them could be implemented in\nGo, or Java, or Rust, etc., maybe because they do something more CPU-bound,\nwhich Node is not great at. While you practice heterogeneity, however, do make\nsure that you limit the selection of your programming languages and database\nsystems across the entire application to two or three. Otherwise, you can run a\nhigh risk of confusing your teams with too much choice and creating serious\nmaintenance overheads.\n182 \n| \nChapter 8: Developer Workspace",
      "content_length": 2558,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "4. Running a single microservice and/or a subsystem of several ones should be equally\neasy.\nLet’s say an airlines reservation system is implemented as three microservices. A\ndeveloper should be able to check out any particular microservice individually\nand work on it, or check out an entire subsystem of interacting microservices\n(the reservation system implementation) and work on that. Both of these tasks\nshould be very easy.\n5. Run databases locally, if possible.\nFor the sake of isolation, for any database system’s local, Docker-ized alternatives\nshould be provided, and it should be trivial to switch over to cloud (e.g., AWS)\nservices via a configuration change. As an example, MinIO can act locally as a\ndrop-in replacement for S3. Many AWS service alternatives can be installed via\nthis GitHub site.\n6. Implement containerization guidelines.\nNot all containerization approaches are equal. Anybody can haphazardly stick\ncode into a Docker container, but making a containerized coding environment\ndeveloper-friendly takes more effort. Following are some principles that we have\nfound essential:\na. Even though the code runtime is containerized, developers must be able to\nedit code on a host machine (e.g., their laptop, an EC2 dev server), with any\ncode editor. However, during execution a full run/test/debug should be exe‐\ncuted in a container.\nb. Since Docker Compose can generally do anything a Dockerfile can, they can\neasily be confused by developers. As such, it is important to establish the dif‐\nference between the two. We recommend the following formula:\nUse a Dockerfile for building a container image, and Docker Compose for\nrunning things locally, including complex integrations. An image built with a\nDockerfile should be directly runnable on Kubernetes, AWS ECR, Swarm, or\nany other production-grade runtime. Please note that just because it can be\ndoesn’t mean the local/dev image will always necessarily be the same as the\none running in production. Teams do often optimize the former for usability\nand the latter for security and performance. A good example of this approach\nis the usage of multistage builds.\nc. Multistage builds must be utilized in Dockerfiles to accommodate usage of\nslim images in production and usage of more full-featured images for local\ndevelopment.\nd. Developer user experience is critical. Implementing hot-reloading of the code\nand/or the ability to connect a debugger out of the box is an important\nfeature.\nCoding Standards and the Developer’s Setup \n| \n183",
      "content_length": 2516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "7. Establish rules for painless database migrations.\nIt is extremely important to manage databases and the data in them in a way that\nsupports and enhances team collaboration. Changes to data schemas must be\ncodified and applied without any manual steps. The following list of principles\nfacilitate painless data management in a microservices environment:\na. Any and all changes to a database schema must be codified in a series of\n“database migration” scripts. Migration files should be named and ordered by\ndate.\nb. Database migrations should support both schema changes as well as sample\ndata insertion.\nc. Running database migrations should be part of the project launch (via Make\nstart, see the next section) and must be enforced.\nd. Running database migrations must be automated and should be part of any\nbuild (integration, feature branch builds for PR, etc.).\ne. It should be possible to indicate which migrations run on which environments\n(or which ones can be skipped), so that migrations that deal with sample data\ncreation can be skipped in production, for example.\nf. These rules apply to all data storage systems: relational, columnar, NoSQL,\nand so forth.\ng. Some examples:\na. Flyway hosts this introduction to database migrations\nb. See this blog post by Daniel Miranda et al. about database migrations for\nCassandra\nc. Check out this example of using Node’s db-migrate-sql for a MySQL\ndatabase\n8. Determine a pragmatic automated testing practice.\nAutomated testing is a complex subject. We have certainly seen both extremes of\nthe spectrum: some teams giving up entirely on automated testing, and others\nbeing overzealous on test-driven development to the extent of it becoming a\nproblem. We advocate for a measured, pragmatic approach to automated testing,\none that balances developer experience with quality metrics and accommodates\nthe differing personal preferences of various developers on the team.\na. Test-first, test-as-you-code, or test-after-code should all be acceptable practi‐\nces as long as all code is covered with a reasonable amount of meaningful tests\nbefore it is merged with the main branch.\nb. Teams should use a testing approach and frameworks that is idiomatic for the\nplatform/stack in which code is being developed (e.g., JUnit for Java). Codeba‐\n184 \n| \nChapter 8: Developer Workspace",
      "content_length": 2327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "ses of the same stack (e.g., Go, Java, etc.) should use a uniform approach and\nvarious microservices in the same language should not be doing different\nthings based on who wrote them and when.\nc. Using external tools, especially for acceptance or performance testing, is fine\nwith proper justification, given an important caveat: these tools (e.g., Cucum‐\nber) must be fully integrated in the code/repository of the service itself and\nusing and running them must be as easy as a native solution. An average\ndeveloper of the service should not need to set anything up to get things going\nand should be able to easily run tests with a command like make test-all.\nd. Special attention and care should be given to automated tests that span the\nboundaries of individual microservices. They will have to be applied either at\na higher level (e.g., an API that invokes microservices, or a UI), or in some\ncases, a dedicated repository may need to be set up to house testing orchestra‐\ntion and automation for such tests.\ne. Code linting/static analysis tooling should be set up and a consistent configu‐\nration for the linter must be adapted for the organization’s style.\n9. Branching and merging.\nVirtually everyone these days uses some form of code version control system.\nWhile the basics of version control–driven development are well-understood, it’s\nworth reminding ourselves of some core principles of good branching hygiene\nthat all team members should observe for a happy collaboration:\na. All development should happen on feature and bug branches.\nb. Merging of a branch to the main branch should not be allowed without all\ntests (including integration tests in a temporary integration cluster spun up\nfor the branch) passing on that branch.\nc. The status of the test runs (after each commit/push) must be readily visible for\ncode reviewers during pull requests.\nd. Linting/static analysis errors should prevent code from being pushed to a\nbranch, and/or merged into the main branch.\n10. Common targets should be codified in a makefile.\nEvery code repository (and generally there should be one repository per micro‐\nservice) should have a makefile that makes it easy for anybody to work with the\ncode, regardless of the programming language stack used. This makefile should\nhave standard targets, so that no matter what codebase, in whatever language the\ndeveloper clones, they should know that by running make run they can bring that\ncodebase up, and by running make test they can run automated tests.\nWe recommend defining and implementing the following standard targets for\nyour microservice makefiles:\nCoding Standards and the Developer’s Setup \n| \n185",
      "content_length": 2658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "• start: Run the code.\n• stop: Stop the code.\n• build: Build the code (typically a container image).\n• clean: Clean all caches and run from scratch.\n• add-module\n• remove-module\n• dependencies: Ensure all modules declared in dependency management are\ninstalled.\n• test: Run all tests and produce a coverage report.\n• tests-unit: Run only unit tests.\n• tests-at: Run only acceptance tests.\n• lint: Run a linter to ensure conformance of coding style with defined\nstandards.\n• migrate: Run database migrations.\n• add-migration: Create a new database migration.\n• logs: Show logs (from within the container).\n• exec: Execute a custom command inside the code’s container.\nCheck out example microservices in Go and Node that follow the aforementioned\npattern, and a sample setup of a multimicroservice workspace that follows the recom‐\nmendations defined on this GitHub site. For your convenience, we have also pub‐\nlished this template on Github in Markdown format, so you can easily link and refer\nto it from your projects if you need to.\nLet’s review what we have learned so far. First, we recognized that the developer expe‐\nrience is paramount for building happy, highly productive, and autonomous teams.\nNext, we identified three core goals for achieving a superior developer experience.\nLast, but not least, we delved into 10 principles that, in our experience, fulfill the\npromises of these goals. The result is a repeatable blueprint for developing highly\nuser-friendly developer workspaces for teams, regardless of which technology stack\nor specific tools end up being chosen. This is a solid foundation that will help you\ndelight your teams and create an early team bond when you start building microser‐\nvices organization or when you reorganize your existing teams into a microservices\nstructure.\nOne of the principles that allows us to create repeatable, reliable, and comfortable\ndevelopment setups is code containerization with Docker. In the following section,\n186 \n| \nChapter 8: Developer Workspace",
      "content_length": 2010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "we will dive into how to set up a solid containerized environment on major platforms,\nsuch as Linux, macOS, and Windows.\nSetting Up a Containerized Environment Locally\nEarlier in this chapter, we mentioned that the presence of Docker (and possibly of\nmake for running makefiles) should be the only expectation for a developer environ‐\nment. Everything else should be easily installable off of that. Let’s see how we can get\na complete Docker toolset, or even single-node Kubernetes, if needed, on various\nplatforms.\nInstalling Docker on a Linux machine is fairly straightforward, but what are some\nways of getting it on your macOS or Windows machine?\nWhen Docker4Mac and Docker4Windows came out they were truly revolutionary:\nbringing the cutting-edge power of Docker to the everyday desktop environments\nmost people use. Eventually they started supporting Kubernetes as well, and it looked\nlike the world could not be more perfect for a backend web developer, especially\nthose moving into microservices.\nThe easiest way to get Docker and Kubernetes on your macOS or Windows is still\nDocker4Mac and Docker4Windows. There are, however, other choices that may be\nappealing to you.\nAn unfortunate reality is that the day-to-day experience of using Docker4Mac and\nDocker4Windows can be quite hit-and-miss. Even on fairly modern, powerful hard‐\nware we have experienced high CPU usage and battery drain. For some it may also be\na problem that Docker4Mac only allows you to install one Docker instance and one\nKubernetes. If you experiment a lot, you may want to have more freedom to break\nthings.\nThankfully, there are alternatives. The obvious one is to install your own VMs with\nVirtualBox or its commercial alternatives. My experience, however, has been that\nthese are even heavier than Docker4Mac/Win packages.\nOne of the more interesting alternatives that I have recently started experimenting\nwith, however, is Multipass, a slick tool from Canonical, the creators of Ubuntu, that\nallows you to very quickly launch Ubuntu-based Docker hosts on your macOS or\nWindows machine (or even Linux). Multipass supports a number of underlying VMs,\nbut most importantly it defaults to HyperKit on macOS, and Hyper-V on Windows\n(Windows Pro required!), which in our experience are more lightweight.\nSetting Up a Containerized Environment Locally \n| \n187",
      "content_length": 2341,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "Installing Multipass\nMultipass installers for various platforms can be downloaded from the website. Once\nyou have it installed, check out the following interesting things you can do on macOS\nor Windows Subshell for Linux.\nTo launch a new Ubuntu environment:\n→ multipass launch -n docker\nLaunched: docker\n→ multipass list\nName                    State             IPv4             Image\ndocker                  Running           192.168.64.3     Ubuntu 20.04 LTS\nBy default, Multipass allocates 1 GB RAM, 5 GB disk, and 1 CPU core to the new\nmachine. These may not be sufficient. In our experience, if you are using something\nlike Node.js or Python with MySQL, 1 GB may be OK, but if you start using heavy\nJava applications with Java-based database systems such as Cassandra, you’ll need\nmore memory. We can override the defaults at launch:\n→ multipass launch -m 4G -n dubuntu\nLaunched: dubuntu\n→ multipass list\nName                    State             IPv4             Image\ndocker                  Running           192.168.64.3     Ubuntu 20.04 LTS\ndubuntu                 Running           192.168.64.4     Ubuntu 20.04 LTS\n→ multipass exec dubuntu -- bash\nubuntu@dubuntu:~$ free -m\n              total        used        free      shared  buff/cache   available\nMem:           3945          79        3640           0         225        3653\nWhile Multipass does allow you to indicate more than one CPU\nwith, say, -c 2, for us this resulted in broken containers on macOS.\nWe assume it may have something to do with limitations on the\nHypervisor implementation, but proceed with caution. Increasing\nmemory has been no problem.\nYou could also increase the memory of an existing container without reinstalling\neverything you already have set up. You have to be careful, since this process can be\nfragile, but generally speaking, you need to stop a Multipass process via launchctl\n(otherwise it will overwrite your changes) and edit the configuration JSON, then\nrelaunch the Multipass process:\n→ sudo launchctl unload /Library/LaunchDaemons/com.canonical.multipassd.plist\n→ sudo vi \"/var/root/Library/Application\n  Support/multipassd/multipassd-vm-instances.json\"\n→ sudo launchctl load /Library/LaunchDaemons/com.canonical.multipassd.plist\n188 \n| \nChapter 8: Developer Workspace",
      "content_length": 2279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "The JSON file you will be editing (multipassd-vm-instances.json) should look some‐\nthing like this:\n{\n    \"dubuntu\": {\n        \"deleted\": false,\n        \"disk_space\": \"5368709120\",\n        \"mac_addr\": \"52:54:00:27:53:b4\",\n        \"mem_size\": \"4294967296\",\n        \"metadata\": {\n        },\n        \"mounts\": [\n        ],\n        \"num_cores\": 1,\n        \"ssh_username\": \"ubuntu\",\n        \"state\": 4\n    }\n}\nAs you might guess, mem_size is what you want to override (in bytes). To be on the\nsafer side, we recommend indicating a number that is properly divisible by 1 GB.\nSince 1 GB is 1024 * 1024 * 1024 = 1,073,741,824 bytes, you should indicate a number\nthat is a multiple of 1,073,741,824; e.g., for 8 GB enter 1073741824 * 8 =\n8589934592.\nEntering the Container and Mapping Folders\nYou can launch any command within your container with a command like multipass\nexec <containername> -- <command launched inside>. For instance, to see free\nmemory in the container or to launch a bash shell, use the following:\n→ multipass exec dubuntu -- free -m\n              total        used        free      shared  buff/cache   available\nMem:           3945          77        3640           0         226        3654\nSwap:             0           0           0\n→ multipass exec dubuntu -- bash\nubuntu@dubuntu:~$ ls -al\ntotal 36\ndrwxr-xr-x 5 ubuntu ubuntu 4096 .\ndrwxr-xr-x 3 root   root   4096 ..\n-rw------- 1 ubuntu ubuntu  107 .bash_history\n-rw-r--r-- 1 ubuntu ubuntu  220 .bash_logout\n-rw-r--r-- 1 ubuntu ubuntu 3771 .bashrc\ndrwx------ 2 ubuntu ubuntu 4096 .cache\ndrwx------ 3 ubuntu ubuntu 4096 .gnupg\n-rw-r--r-- 1 ubuntu ubuntu  807 .profile\ndrwx------ 2 ubuntu ubuntu 4096 .ssh\nubuntu@dubuntu:~$ exit\nSetting Up a Containerized Environment Locally \n| \n189",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "exit\n→\nLaunching a shell of the primary container can be made easier by indicating which of\nyour containers you want to set as primary. Then you can just type multipass shell:\n→ multipass set client.primary-name=dubuntu\n→ multipass shell\nubuntu@dubuntu:~$\nTo map your home folder (on macOS) to a folder in the container, you can run:\n→ multipass mount $HOME dubuntu:/home/ubuntu/mac\nEnabling support for mounting -\n→ multipass exec dubuntu -- ls -ald mac\ndrwxr-xr-x 1 ubuntu ubuntu 3936 mac\n→ multipass info dubuntu\nName:           dubuntu\nState:          Running\nIPv4:           192.168.64.4\nRelease:        Ubuntu 18.04.4 LTS\nImage hash:     2f6bc5e7d9ac (Ubuntu 18.04 LTS)\nLoad:           0.00 0.08 0.07\nDisk usage:     1.1G out of 4.7G\nMemory usage:   81.9M out of 3.9G\nMounts:         /Users/irakli => /home/ubuntu/mac\n→ multipass exec dubuntu -- ls -al mac\ntotal 240120\ndrwxr-xr-x 1 ubuntu ubuntu      3936 .\ndrwxr-xr-x 6 ubuntu ubuntu      4096 ..\n-rw-r--r-- 1 ubuntu ubuntu     10244 .DS_Store\ndrwx------ 1 ubuntu ubuntu        64 .Trash\ndrwxr-xr-x 1 ubuntu ubuntu       512 .atom\ndrwxr-xr-x 1 ubuntu ubuntu       128 .aws\nNow that we have a functioning virtualized Linux via Multipass, installing Docker (or\neven local Kubernetes) becomes quite easy. Let’s see in the next section how we would\ngo about it.\nInstalling Docker\nYou can install Docker inside a container by following the usual Docker installation\nprocess:\n→ multipass shell\nubuntu@dubuntu:~$ sudo apt-get update && sudo apt-get upgrade -y\nubuntu@dubuntu:~$ sudo apt-get install build-essential -y\n# Sanity Check\n190 \n| \nChapter 8: Developer Workspace",
      "content_length": 1622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "ubuntu@dubuntu:~$ sudo apt-get remove docker \\\n                  docker-ce-cli docker-engine docker.io containerd runc\n# Install Docker and Docker Compose\nubuntu@dubuntu:~$ sudo snap install docker\nubuntu@dubuntu:~$ echo 'export PATH=/snap/bin:$PATH' >> ~/.bashrc\nubuntu@dubuntu:~$ source ~/.bashrc\nAfter completing these steps, you should have a working Docker installation, but it\ncan only be run as root (via sudo), which is both insecure as well as inconvenient. To\nfix it you should grant the default, nonprivileged user (ubuntu for this installation)\ngroup access to Docker, as shown in the following code. Note that you must log out of\nUbuntu and log back in for this change to take effect:\nubuntu@dubuntu:~$ sudo groupadd docker\nubuntu@dubuntu:~$ sudo usermod -aG docker $USER\nubuntu@dubuntu:~$ exit\nlogout\n→ multipass restart\n→ multipass shell\nubuntu@dubuntu:~$ docker ps\nCONTAINER ID   STATUS   IMAGE   PORTS   NAMES\nubuntu@dubuntu:~$ docker version\nClient:\n Version:           19.03.11\n API version:       1.40\nubuntu@dubuntu:~$ $ docker-compose --version\ndocker-compose version 1.25.5, build unknown\nTo test our new Docker setup, let’s now use it for bringing up a MySQL database with\nDocker Compose.\nTesting Docker\nFirst, let’s create a mysql-stack.yml file with instructions for Docker Compose:\nversion: '3.1'\nservices:\n  db:\n    image: mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: rootPass\n    ports:\n      - 33060:3306\nWe should mention here that, by default, this file would be called docker-compose.yml,\nbut we can use a custom name as long as we indicate the name we used with a special\nInstalling Docker \n| \n191",
      "content_length": 1656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "-f flag when we try to execute Docker Compose. Let’s now launch MySQL in a\nDocker container with the following:\nubuntu@dubuntu:~$ docker-compose -f mysql-stack.yml up -d\nubuntu@dubuntu:~$ docker ps\nCONTAINER ID   STATUS         IMAGE   PORTS\ne08f6f072c89   Up 3 seconds   mysql   33060/tcp, 0.0.0.0:33060->3306/tcp\nAt this point you should have a working Docker and Docker Compose setup. In the\nnext section, we will show you how to use these tools to easily install advanced com‐\nponents, such as a local Cassandra database, should you need to do so.\nAdvanced Local Docker Usage: Installing Cassandra\nWe have already discussed how to use Docker Compose for running a containerized\nMySQL database, but let’s look now at a somewhat more complex example of running\na Cassandra database in a container. Considering the popularity and versatility of\nCassandra, it is probably something you will have to deal with at some point in your\ncloud native microservices development journey.\nCassandra requires more than the default 1 GB RAM, so make sure\nyour Multipass container has more memory (e.g., 6–8 GB).\nFirst, create a docker-compose.yml file with the following content anywhere in the\ncontainer:\nversion: '3'\nservices:\n  cassandra-seed:\n    container_name: cassandra-seed\n    image: cassandra:3.11\n    ports:\n      - \"9042:9042\"   # Native protocol clients\n      # - \"7199:7199\"   # JMX\n      # - \"9160:9160\"   # Thrift clients\n    volumes:\n      - local_cassandra_data_seed:/var/lib/cassandra\nvolumes:\n  local_cassandra_data_seed:\nThen run it and check that it worked:\nubuntu@dubuntu:~/cassandra$ docker-compose up -d\nCreating network \"cassandra_default\" with the default driver\n192 \n| \nChapter 8: Developer Workspace",
      "content_length": 1716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "Creating cassandra-seed ... done\nubuntu@dubuntu:~/cassandra$ docker-compose ps\nName                   Command               State             Ports\n-------------------------------------------------------------------------------\ncassandra-seed   docker-entrypoint.sh cassa ...   Up      7000/tcp, 7001/tcp\nubuntu@dubuntu:~/cassandra$ docker exec -it cassandra-seed cqlsh\nConnected to Test Cluster at 127.0.0.1:9042.\n[cqlsh 5.0.1 | Cassandra 3.11.6 | CQL spec 3.4.4 | Native protocol v4]\nUse HELP for help.\ncqlsh> DESCRIBE keyspaces;\nThe last command, DESCRIBE keyspaces, will show all of the existing keyspaces in\nthe newly minted Cassandra installation. At this point, you should have a fully work‐\ning, local Cassandra setup. Next we will show you how to get a local Kubernetes envi‐\nronment installed when you need one.\nInstalling Kubernetes\nFor most cases, Docker Compose provides ample capabilities in orchestrating various\ncomponents and microservices that make up your overall application. For more\nadvanced cases, Kubernetes is a popular solution, with a much wider spectrum of\nfunctionality. It does, however, come with a proportionally higher level of complexity.\nAvoid Using Kubernetes Locally Unless You Must\nGenerally, we do not recommend using local Kubernetes for every‐\nday coding. Docker and Docker Compose can complete most\ncontainerization-related tasks more easily and they have more\nstraightforward tooling for building container images. Kubernetes\nexcels in orchestrating a runtime fleet of containers, which is rarely\nneeded in a Dev environment, but which is crucial for environ‐\nments such as production, preproduction, staging, performance\ntesting, etc. However, in some circumstances you may want to use\nKubernetes locally, especially for targeted testing.\nYou cannot simply install the official Kubernetes distribution on a single machine.\nKubernetes is designed to be deployed on a cluster of servers. However, there are\nmultiple nice side projects that bypass the requirement and get a functioning Kuber‐\nnetes setup on a single machine. The default one, created by the same community\nthat maintains Kubernetes, is Minikube. It is not the only solution, however. Two of\nour other favorites, based on simplicity and reliability, are Rancher’s k3s and Canoni‐\ncal’s MicroK8s.\nTo install Kubernetes locally with k3s, use the following:\nInstalling Kubernetes \n| \n193",
      "content_length": 2391,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "ubuntu@dubuntu:~$ curl -sfL https://get.k3s.io | sh -\n[INFO]  Finding release for channel stable\n[INFO]  Using v1.17.4+k3s1 as release\n[INFO]  Downloading hash \\\nhttps://github.com/rancher/k3s/releases/download/v1.17.4+k3s1/...\n[INFO]  Downloading binary \\\nhttps://github.com/rancher/k3s/releases/download/v1.17.4+k3s1/k3s\n[INFO]  Verifying binary download\n[INFO]  Installing k3s to /usr/local/bin/k3s\n...\nubuntu@dubuntu:~$ sudo k3s kubectl get nodes\nNAME      STATUS   ROLES    AGE    VERSION\ndubuntu   Ready    master   104s   v1.17.4+k3s1\nWith MicroK8s, the steps are similar but we are also adding a current user to a spe‐\ncific group, and will need to log in again just like we did with Docker:\nubuntu@dubuntu:~$ sudo snap install microk8s --classic\nmicrok8s v1.18.1 from Canonical✓ installed\nubuntu@dubuntu:~$ sudo usermod -a -G microk8s $USER\nubuntu@dubuntu:~$ sudo chown -f -R $USER ~/.kube\nubuntu@dubuntu:~$ exit\nlogout\n→ multipass shell\nubuntu@dubuntu:~$ microk8s.kubectl get services --all-namespaces\nNAMESPACE  NAME        TYPE       CLUSTER-IP    PORT(S)  AGE\ndefault    kubernetes  ClusterIP  10.152.183.1  443/TCP  3m22s\nAnd that’s pretty much all you need to have a functioning Kubernetes setup on a\ndevelopment machine.\nAs we mentioned in the beginning of this section, an out-of-the box Kubernetes envi‐\nronment does not have tooling to build container images. It requires supplying a URI\nof a pre-built image from a registry. This shortcoming makes Kubernetes a cumber‐\nsome choice for active development, since there is no obvious solution for facilitating\na streamlined build-run-test cycle. Kubernetes is really more of a tool for sophistica‐\nted orchestration in nondevelopment (QA, staging, preprod, prod, etc.) environ‐\nments. That said, several years after Kubernetes was released, an open source toolset\ncalled Skaffold was developed to make building container images pluggable into\nKubernetes life cycles.\nWe will not use local Kubernetes in most coding examples in this book, but if you\nwould like to take a look at a sample open source project implementing such a setup,\ncheck out the Skaffold microservices repository that we created for demonstration\npurposes.\n194 \n| \nChapter 8: Developer Workspace",
      "content_length": 2231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "Summary\nIn this chapter, we outlined goals for designing a developer-friendly workspace to\nensure that the space in which most developers spend the majority of their time\n(except maybe for meetings) is comfortable, reliable, and effective. Based on those\ngoals, we introduced 10 principles for building efficient developer workspaces and\ndemonstrated some of the steps for laying the containerized foundation for a variety\nof major operating systems: macOS, Windows, and Ubuntu Linux.\nThese concepts and skills will allow you to create delightful collaborative environ‐\nments for your development teams, make on-boarding of new developers a pleasant\nexperience, and facilitate good coding practices. In Chapter 9, we will come back to\nthe goals and principles outlined here as we demonstrate the finer details of how we\nput together the code and the underlying project.\nSummary \n| \n195",
      "content_length": 885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "CHAPTER 9\nDeveloping Microservices\nLet’s put to work some of the techniques we’ve been discussing and implement a sam‐\nple multimicroservices project. The implementation of the microservices in this sam‐\nple project will be greatly simplified. We will show just enough code to suffice for\ndemonstration purposes, but the steps and approaches we’ll discuss can be directly\napplied on much larger, real projects.\nWe will start by identifying fitting candidates for microservices based on a bounded\ncontexts analysis using Event Storming, similar to the process described in Chapter 4.\nNext we’ll go through the seven steps of the SEED(S) design methodology that we\ndiscussed in Chapter 3, culminating in writing the code for both of the sample micro‐\nservices. In the implementation of these services we will employ the data-modeling\nguidance from Chapter 5. And last, but not least, we will show how a user-friendly\ndevelopment environment for the microservices is properly set up and configured,\napplying many of the recommendations from Chapter 8, including the creation of an\numbrella project—a way to execute multiple microservices together in a developer\nworkspace.\nDesigning Microservice Endpoints\nLet’s assume that an Event Storming session that you conducted for a flight manage‐\nment software product identified two major bounded contexts:\n• Flights management\n• Reservations management\nAs we discussed in Chapter 4, in the initial stages it pays off to design microservices\nin a coarse-grained way. Specifically, we often align them with bounded contexts; i.e.,\nour first two microservices can be ms-flights and ms-reservations!\n197",
      "content_length": 1641,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "Now that we have the target microservices identified, we need to use the SEED(S)\ndesign process (introduced in Chapter 3) for them. According to the first step of the\nSEED(S) methodology, we need to identify various actors. For our purposes, we’ll\nassume the following actors:\n• The customer trying to book the flight\n• The airline’s consumer app (web, mobile, etc.)\n• The web APIs that the app interacts with (In Chapter 3, we mentioned that some\ncall these “backends for frontends” or BFF APIs.)\n• The flights management microservice: ms-flights\n• The reservations management microservice: ms-reservations\nLet’s look at some sample JTBDs that our product team may have collected from cus‐\ntomer interviews and business analysis research.\n1. When a customer interacts with the UI, the app needs to render a seating chart\nshowing occupied and available seats, so the customer can choose a seat.\n2. When a customer is finalizing a booking, the web app needs to reserve a seat for\nthe customer, so the app can avoid accidental seat reservation conflicts.\nRecall from Chapter 3 that we recommended BFF APIs be a thin layer with no busi‐\nness logic implementation. They mostly just orchestrate microservices. So there are\nusually jobs for which a BFF API needs microservices. The following list of jobs, the\nmore technical JTBDs, describes the needs between the BFF APIs and microservices:\n1. When the API is asked to provide a seating chart, the API needs ms-flights to pro‐\nvide a seating setup of the flight, so the API can retrieve availabilities and render\nthe final result.\n2. When the API needs to render a seating chart, the API needs ms-reservations to\nprovide a list of already reserved seats so the API can add that data to the seating\nsetup and return the seating chart.\n3. When the API is asked to reserve a seat, the API needs ms-reservations to fulfill\nthe reservation, so the API can reserve the seat.\nKey Decision: Avoid Microservices Calling Each Other Directly\nNote that we don’t let ms-flights call ms-reservations to assemble the seating chart,\nand instead have the BFF API handle the interaction. This refers back to the recom‐\nmendation in Chapter 3 that direct microservice-to-microservice calls be avoided.\n198 \n| \nChapter 9: Developing Microservices",
      "content_length": 2271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "Following the SEED(S) methodology, next we describe the interactions represented\nby various jobs, using UML sequence diagrams in PlantUML format:\n@startuml\nactor Customer as cust\nparticipant \"Web App\" as app\nparticipant \"BFF API\" as api\nparticipant \"ms-flights\" as msf\nparticipant \"ms-reservations\" as msr\ncust -[#blue]-> app ++: \"Flight Seats Page\"\napp -[#blue]-> api ++ : flight.getSeatingSituation()\napi -[#blue]-> api: auth\napi -> msf ++ : getFlightId()\nmsf --> api: flight_id\napi -> msf: getFlightSeating()\nreturn []flightSeating\napi -> msr ++ : getReservedSeats()\nreturn []reservedSeats\nreturn []SeatingSituation\nreturn \"Seats Selection Page\"\n|||\ncust -[#blue]->app ++: \"Choose a seat & checkout\"\napp-[#blue]->app: \"checkout workflow\"\napp-[#blue]->api ++: \"book the seat\"\napi -[#blue]->api: auth\napi->msr ++: \"reserveSeat()\"\nreturn \"success\"\nreturn \"success\"\nreturn \"Success Page\"\n@enduml\nThis can be rendered (e.g., using LiveUML) into the UML diagram shown in\nFigure 9-1.\nAs you can clearly see from this diagram, the first JTBD is to present a customer with\na “seats on the flight” page. To fulfill this job, an app (or a website) will need to call a\nfrontend (BFF) API that returns the seating “situation”: a list of seats with indicators\nfor which ones are occupied or vacant. The API will first authenticate the call to\nensure the app is authorized to ask such questions. If the auth passes, it will first try\nto get a flight_id from the ms-flights microservice. This is necessary because cus‐\ntomers usually just enter the nonunique flight number (identifying a route more than\na specific flight on a specific date) and flight date. With the unique flight_id\nreturned, the API will then get the list of seats from ms-flights. To make sure we can\nshow occupied seats, it will separately query ms-reservations for existing reservations\non the flight.\nDesigning Microservice Endpoints \n| \n199",
      "content_length": 1902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "Figure 9-1. Sequence diagram representing interactions of various JTBDs\nOf particular importance here is how we’re practicing the principle described in\nChapter 3 regarding microservices not calling each other directly and instead being\norchestrated by a thin API layer. This is entirely why ms-flights is not querying the\nlist of reserved seats from ms-reservations directly. Once the API collects all of the\nrequired information it can return the rich data to the app/website so the latter can\nrender the desired screen for the customer.\nIn the second part of Figure 9-1, we describe the second JTBD for the customer: once\nthey see the current seating situation, they want to pick a specific (available) seat and\nreserve it. To fulfill this task, API will again need to auth and then call a microservice,\nms-reservations, returning the status, success, or failure to the app, based on the\nresult of the booking attempt. This allows the app to let the customer know whether\ntheir request could be completed or not.\n200 \n| \nChapter 9: Developing Microservices",
      "content_length": 1059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "1 For demonstration purposes we are using the Seat Map object structure from Sabre’s Seat Map RESTful API, a\ngold standard of the industry.\nOnce we have the JTBDs, and understand the interactions, we can translate them into\nqueries and actions. We will do this for both ms-flights and ms-reservations. In\nChapter 3, we explained that you should also design actions and queries for the BFF\nAPI, not just microservices, but we will leave that task as an exercise to the reader.\nFlights Microservice\nTo compile actions and queries for ms-flights:\nGet flight details\n• Input: flight_no, departure_local_date_time (ISO8601 format and in the local\ntime zone)\n• Response: A unique flight_id identifying a specific flight on a specific date. In\npractice, this endpoint will very likely return other flight-related fields, but those\nare irrelevant for our context, so we are skipping over them.\nGet flight seating (the diagram of seats on a flight)\n• Input: flight_id\n• Response: Seat Map object in JSON format1\nReservations Microservice\nTo compile actions and queries for ms-reservations:\nQuery already reserved seats on a flight\n• Input: flight_id\n• Response: A list of already-taken seat numbers, each seat number in a format like\n“2A”\nReserve a seat on a flight\n• Input: flight_id, customer_id, seat_num\n• Expected outcome: A seat is reserved and unavailable to others, or an error fired if\nthe seat was unavailable\n• Response: Success (200 Success) or failure (403 Forbidden)\nAs discussed in Chapter 3, the beauty of writing down actions and queries is that they\nbring us much closer to being able to create the technical specifications of the services\nthan when jobs are presented in their business-oriented, jobs (JTBD) format.\nDesigning Microservice Endpoints \n| \n201",
      "content_length": 1766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "Now that we have the actions and queries for our microservices, we can proceed with\ndescribing the microservices we intend to build in a standard format. In our case, we\nwill build RESTful microservices and describe them with an OAS. In the next section,\nwe’ll see what this specification for our two microservices could look like.\nDesigning an OpenAPI Specification\nBased on the query and commands specification we just designed, translation into an\nOpenAPI Specification (OAS) becomes fairly straightforward. The top part of the\nspecification is usually some meta information:\n  openapi: 3.0.0\n  info:\n    title: Flights Management Microservice API\n    description: |\n      API Spec for Flight Management System\n    version: 1.0.1\n  servers:\n    - url: http://api.example.com/v1\n      description: Production Server\nFor the /flights endpoint you will want to provide flight_no and depar\nture_date_time input parameters in the query string of the request. The schema\nshould also describe the response JSON’s structure, containing flight_id, the origin\nairport’s identifier code, the destination airport’s code, and the HTTP code (200) for a\nsuccessful response. This part in OpenAPI format may look like:\n  paths:\n    /flights:\n      get:\n        summary: Look Up Flight Details with Flight No and Departure Date\n        description: |\n          Look up flight details, such as: the unique flight_id used by the\n          rest of the Flights management endpoints, flight departure and\n          arrival airports.\n          Example request:\n          ```\n            GET http://api.example.com/v1/flights?\n                flight_no=AA2532&departure_date_time=2020-05-17T13:20\n          ```\n        parameters:\n          - name: flight_no\n            in: query\n            required: true\n            description: Flight Number.\n            schema:\n              type : string\n            example: AA2532\n          - name: departure_date_time\n202 \n| \nChapter 9: Developing Microservices",
      "content_length": 1984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "in: query\n            required: true\n            description: Date and time (in ISO8601)\n            schema:\n              type : string\n            example: 2020-05-17T13:20\n        responses:\n          '200':    # success response\n            description: Successful Response\n            content:\n              application/json:\n                schema:\n                  type: array\n                  items:\n                    type: object\n                    properties:\n                      flight_id:\n                        type: string\n                        example: \"edcc03a4-7f4e-40d1-898d-bf84a266f1b9\"\n                      origin_code:\n                        type: string\n                        example: \"LAX\"\n                      destination_code:\n                        type: string\n                        example: \"DCA\"\n                  example:\n                    flight_id: \"edcc03a4-7f4e-40d1-898d-bf84a266f1b9\"\n                    origin_code: \"LAX\"\nWhen you design the specification for the /flights/{flight_no}/seat_map end‐\npoint, it can take the flight_no input parameter in the URL path itself, instead of in\nthe query part of the URL. In the response object, for demonstration purposes, we are\nusing a SeatMap object structure that mimics that of the industry gold standard, Sab‐\nre’s Seat Map API. If you were really building a commercial API, you would need to\ndesign your own implementation or acquire permission for reuse from the design’s\noriginal author:\n    /flights/{flight_no}/seat_map:\n      get:\n        summary: Get a seat map for a flight\n        description: |\n          Example request:\n          ```\n            GET http://api.example.com/\n                v1/flights/AA2532/datetime/2020-05-17T13:20/seats/12C\n          ```\n        parameters:\n          - name: flight_no\nDesigning Microservice Endpoints \n| \n203",
      "content_length": 1864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "in: path\n            required: true\n            description: Unique Flight Identifier\n            schema:\n              type : string\n            example: \"edcc03a4-7f4e-40d1-898d-bf84a266f1b9\"\n        responses:\n          '200':    # success response\n            description: Successful Response\n            content:\n              application/json:\n                schema:\n                  type: object\n                  properties:\n                    Cabin:\n                      type: array\n                      items:\n                        type: object\n                        properties:\n                          firstRow:\n                            type: number\n                            example: 8\n                          lastRow:\n                            type: number\n                            example: 23\n                          Wing:\n                            type: object\n                            properties:\n                              firstRow:\n                                type: number\n                                example: 14\n                              lastRow:\n                                type: number\n                                example: 22\n                          CabinClass:\n                            type: object\n                            properties:\n                              CabinType:\n                                type: string\n                                example: Economy\n                          Column:\n                            type: array\n                            items:\n                              type: object\n                              properties:\n                                Column:\n                                  type: string\n                                  example: A\n                                Characteristics:\n                                  type: array\n                                  example:\n204 \n| \nChapter 9: Developing Microservices",
      "content_length": 1961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "- Window\n                                  items:\n                                    type: string\n                          Row:\n                            type: array\n                            items:\n                              type: object\n                              properties:\n                                RowNumber:\n                                  type: number\n                                  example: 8\n                                Seat:\n                                  type: array\n                                  items:\n                                    type: object\n                                    properties:\n                                      premiumInd:\n                                        type: boolean\n                                        example: false\n                                      exitRowInd:\n                                        type: boolean\n                                        example: false\n                                      restrictedReclineInd:\n                                        type: boolean\n                                        example: false\n                                      noInfantInd:\n                                        type: boolean\n                                        example: false\n                                      Number:\n                                        type: string\n                                        example: A\n                                      Facilities:\n                                        type: array\n                                        items:\n                                          type: object\n                                          properties:\n                                            Detail:\n                                              type: object\n                                              properties:\n                                                content:\n                                                  type: string\n                                                  example: LegSpaceSeat\nYou can view the full source of the specification at this book’s GitHub site.\nThe OAS can be rendered with a number of editors; for instance, Swagger Editor.\nRendering the preceding specification produces a result that looks like Figure 9-2.\nDesigning Microservice Endpoints \n| \n205",
      "content_length": 2345,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "Figure 9-2. OAS for ms-flights rendered with Swagger Editor\nSimilarly to the OAS of the flights microservice, the designs for the endpoints of the\nreservation system would be something along the lines of:\n  openapi: 3.0.0\n  info:\n    title: Seat Reservation System API\n    description: |\n      API Spec for Fight Management System\n    version: 1.0.1\n  servers:\n    - url: http://api.example.com/v1\n      description: Production Server\n  paths:\n    /reservations:\n      get:\n206 \n| \nChapter 9: Developing Microservices",
      "content_length": 517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "summary: Get Reservations for a flight\n        description: |\n          Get all reservations for a specific flight\n        parameters:\n          - name: flight_id\n            in: query\n            required: true\n            schema:\n              type: string\n        responses:\n          '200':    # success response\n            description: Successful Response\n            content:\n              application/json:\n                schema:\n                  type: array\n                  items:\n                    type: object\n                    properties:\n                      seat_no:\n                        type: string\n                        example: \"18F\"\n                  example:\n                    - { seat_no: \"18F\" }\n                    - { seat_no: \"18D\" }\n                    - { seat_no: \"15A\" }\n                    - { seat_no: \"15B\" }\n                    - { seat_no: \"7A\" }\n      put:\n        summary: Reserve or cancel a seat\n        description: |\n          Reserves a seat or removes a seat reservation\n        requestBody:\n          required: true\n          content:\n            application/json:\n                schema:\n                  type: object\n                  properties:\n                    flight_id:\n                      description: Flight's Unique Identifier.\n                      type : string\n                      example: \"edcc03a4-7f4e-40d1-898d-bf84a266f1b9\"\n                    customer_id:\n                      description: Registered Customer's Unique Identifier\n                      type : string\n                      example: \"2e850e2f-f81d-44fd-bef8-3bb5e90791ff\"\n                    seat_num:\n                      description: seat number\n                      type: string\n                example:\n                  flight_id: \"edcc03a4-7f4e-40d1-898d-bf84a266f1b9\"\nDesigning Microservice Endpoints \n| \n207",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "customer_id: \"2e850e2f-f81d-44fd-bef8-3bb5e90791ff\"\n                  seat_num: \"8D\"\n        responses:\n          '200':\n            description: |\n              Success.\n            content:\n              application/json:\n                schema:\n                  type: object\n                  properties:\n                    status:\n                      type: string\n                      enum: [\"success\", \"error\"]\n                      example:\n                        \"success\"\n          '403':\n            description: seat(s) unavailable. Booking failed.\n            content:\n              application/json:\n                schema:\n                  type: object\n                  properties:\n                    error:\n                      type: string\n                    description:\n                      type: string\n                  example:\n                    error: \"Could not complete reservation\"\n                    description: \"Seat already reserved. Cannot double-book\"\nNow that we have our service designs and the corresponding OASs, it’s time to pro‐\nceed to the last step in the SEED(S) process: writing the code for the microservices.\nAs we implement the flights and reservations microservices, we will practice the prin‐\nciples discussed earlier in this book. Specifically, we will use different tech stacks for\nthese services, so we can demonstrate their ability to support heterogeneous imple‐\nmentation. The reservations microservice will be implemented in Python and Flask,\nwhile the flights microservice will be implemented in Node/Express.js.\nImplementing the Data for a Microservice\nTo emphasize the need for data independence that we discussed at length in Chap‐\nter 5, not only will we ensure the two microservices do not share any data space, but\nwe will intentionally implement them using entirely different backend data systems:\nRedis for the reservations and MySQL for flights. We will also explain how each of\nthese microservices benefits from the choice of data storage mechanisms. Let’s start\nwith the data for the reservations system microservice.\n208 \n| \nChapter 9: Developing Microservices",
      "content_length": 2140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "Redis for the Reservations Data Model\nIn the reservations system, we need to be able to capture a set of seat reservations for\na flight, and reserve a seat if it is not already booked. Redis has a perfect, simple data\nstructure that fits our use case very well: hashes.\nRedis hashes are optimized for storing mappings of lists of key/value pairs, where\nboth keys and values are of the string type. They are often used to store flat objects\nsuch as a user with first name, last name, email, etc. For us, it can serve as robust stor‐\nage for seat reservations information. We can have a hash object saved for each\nflight_id (specific flight) where keys of the hash are the seat numbers on the flight\nand the value is the customer_id for the customer that the seat is already reserved for.\nRedis has commands to set a new value in a hash and to get all set values (for when\nwe need to know all reserved seats), and, very conveniently, a command that allows\nus to set a value only if the value for the same key (seat) is not already set. That’s per‐\nfect for us, since we typically do not want to allow double-booking a seat on a flight.\nKey Decision: Use Redis to Implement the Reservations Database\nUse Redis as the data store for reservations to leverage its unique simplicity and flexi‐\nbility, characteristics fitting for the implementation of this microservice.\nLet’s see an example of reserving several seats on a flight uniquely identified with the\nflight_id of 40d1-898d-bf84a266f1b9. If you have a working Redis installation, or\nuse the Redis CLI from the reservations microservice’s workspace by invoking make\nredis after you check out that GitHub repository, you should be able to run the fol‐\nlowing commands:\n> HSETNX flight:40d1-898d-bf84a266f1b9 12B b4cdf96e-a24a-a09a-87fb1c47567c\n(integer) 1\n> HSETNX flight:40d1-898d-bf84a266f1b9 12C e0392920-a24a-b6e3-8b4ebcbe7d5c\n(integer) 1\n> HSETNX flight:40d1-898d-bf84a266f1b9 11A f4892d9e-a24a-8ed1-2397df0ddba7\n(integer) 1\n> HSETNX flight:40d1-898d-bf84a266f1b9 3A 017d40c6-a24b-b6d7-4bb15d04a10b\n(integer) 1\n> HSETNX flight:40d1-898d-bf84a266f1b9 3B 0c27f7c8-a24b-9556-fb37c840de89\n(integer) 1\n> HSETNX flight:40d1-898d-bf84a266f1b9 22A 0c27f7c8-a24b-9556-fb37c840de89\n(integer) 1\n> HSETNX flight:40d1-898d-bf84a266f1b9 22B 24ae6f02-a24b-a149-53d7a72f10c0\n(integer) 1\nThe HSETNX command we use here sets the value of the HSET key, we indicate, to the\nspecified value only if the key doesn’t already have a value. This way we avoid reserv‐\ning seats that are already assigned.\nImplementing the Data for a Microservice \n| \n209",
      "content_length": 2582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "Let’s see how we would get all of the occupied seats on a specific flight:\n> HKEYS flight:40d1-898d-bf84a266f1b9\n1) \"12B\"\n2) \"12C\"\n3) \"11A\"\n4) \"3A\"\n5) \"3B\"\n6) \"22A\"\n7) \"22B\"\nIf we wanted to get both keys and values, we can also do this:\n> HGETALL flight:40d1-898d-bf84a266f1b9\n 1) \"12B\"\n 2) \"b4cdf96e-a24a-a09a-87fb1c47567c\"\n 3) \"12C\"\n 4) \"e0392920-a24a-b6e3-8b4ebcbe7d5c\"\n 5) \"11A\"\n 6) \"f4892d9e-a24a-8ed1-2397df0ddba7\"\n 7) \"3A\"\n 8) \"017d40c6-a24b-b6d7-4bb15d04a10b\"\n 9) \"3B\"\n10) \"0c27f7c8-a24b-9556-fb37c840de89\"\n11) \"22A\"\n12) \"0c27f7c8-a24b-9556-fb37c840de89\"\n13) \"22B\"\n14) \"24ae6f02-a24b-a149-53d7a72f10c0\"\nLet’s now see what happens if we try to double-book an already reserved seat, such as\n12C:\n> HSETNX flight:40d1-898d-bf84a266f1b9 12C 083a6fc2-a24d-889b-6fc480858a38\n(integer) 0\nNotice how the response to this command is (integer) 0 instead of the (integer)\n1 we got for earlier HSETNX commands. This indicates that 0 fields were actually upda‐\nted and that is because 12C had already been reserved.\nAs you can see, choosing Redis as the data store for ms-reservations has made the\nimplementation easy and natural. We were able to use well-fitting data structures,\nsuch as HSET, that effortlessly and elegantly met our needs. The HSETNX command\nallowed us to avoid accidental double-bookings in a reliable and straightforward way.\nRedis is a fantastic key/value store that can be used in a wide variety of use cases,\nwhich is why it has a huge fan base among programmers. However, it is not going to\nbe the perfect database for every single use case. Sometimes we may have data needs \nthat are better met by other popular databases.\nTo demonstrate this, in the next section, we will implement the data for the ms-flights\nmicroservice using a traditional SQL database.\n210 \n| \nChapter 9: Developing Microservices",
      "content_length": 1822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "MySQL Data Model for the Flights Microservice\nThe first data model we need here should contain seat maps. As we saw in the OAS\nfor the flights microservice, the seat map is a complex JSON object. MySQL can be a\nbetter storage for such data than standard Redis. As of MySQL 5.7.8, MySQL has\nrobust, native support for JSON data types. This support has expanded and improved\nin the latest 8.x version of MySQL. It now also supports in-place, atomic updates of\nJSON values and JSON Merge Patch syntax. In comparison, Redis only supports\nJSON with a RedisJSON module that doesn’t come pre-built with the standard Redis\ndistribution.\nA well-implemented JSON data type provides tangible advantages over storing JSON\ndata in a string column: validation of JSON documents during inserts, internally opti‐\nmized binary storage, ability to look up subobjects and nested values directly by a key,\nand so on.\nAdditionally, in the lookup endpoint we need to query data by two fields: flight_no\nand datetime. A relational database is a more natural structure for such queries. In\nRedis, we would probably need to create a compound field to achieve the same. All in\nall, while we could technically implement this service with Redis as well, there are\nreasons to choose MySQL for doing this, among them that MySQL also helps us\ndemonstrate usage of different databases for different services. Real-life situations will\nobviously be more complex, with more aspects to consider.\nLet’s look at the seat_maps table:\nCREATE TABLE `seat_maps`  (\n  `flight_no` varchar(10) NULL,\n  `seat_map` json NULL,\n  `origin_code` varchar(10) NULL,\n  `destination_code` varchar(10) NULL,\n  PRIMARY KEY(`flight_no`)\n);\nAnother table we need is the mapping of flight_ids with flight_nos and date\ntimes. The creation script for this table may look something like the following:\nCREATE TABLE `flights`  (\n  `flight_id` varchar(36) NOT NULL,\n  `flight_no` varchar(10) NULL,\n  `flight_date` datetime(0) NULL,\n  PRIMARY KEY (`flight_id`),\n  INDEX `idx_flight_date`(`flight_no`, `flight_date`)\n  FOREIGN KEY(flight_no)\n    REFERENCES seat_maps(flight_no)\n);\nImplementing the Data for a Microservice \n| \n211",
      "content_length": 2163,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "Let’s insert our first sample seat map:\nINSERT INTO `seat_maps`(`flight_no`, `seat_map`, `origin_code`, /\n`destination_code`) VALUES ('AA2532', '{\\\"Cabin\\\": [{\\\"Row\\\": [{\\\"Seat\\\": /\n[{\\\"Number\\\": \\\"A\\\", \\\"Facilities\\\": [{\\\"Detail\\\": {\\\"content\\\": /\n\\\"LegSpaceSeat\\\"}}], \\\"exitRowInd\\\": false, \\\"premiumInd\\\": false, /\n\\\"noInfantInd\\\": false, \\\"restrictedReclineInd\\\": false}], \\\"RowNumber\\\": /\n8}], \\\"Wing\\\": {\\\"lastRow\\\": 22, \\\"firstRow\\\": 14}, \\\"Column\\\": /\n[{\\\"Column\\\": \\\"A\\\", \\\"Characteristics\\\": [\\\"Window\\\"]}], \\\"lastRow\\\": 23, /\n\\\"firstRow\\\": 8, \\\"CabinClass\\\": {\\\"CabinType\\\": \\\"Economy\\\"}}]}', /\n'LAX', 'DCA');\nOnce we have the proper JSON value in the database, we can easily select specific val‐\nues in it or filter by specific values. For instance:\nselect seat_map->>\"$.Cabin[0].firstRow\" from seat_maps\nNow that we have a working data model for both of our microservices, we can dive\ndeeper into the implementation of code for them.\nImplementing Code for a Microservice\nNow we’ll work toward the second goal that is the foundation of the “10 Workspace\nGuidelines for a Superior Developer Experience” on page 181 and start new microser‐\nvices quickly, using well-tested templates for each relevant tech stack. For the\nNode.js-implemented flights microservice we’ll use a popular bootstrapper, Node‐\nBootstrap. For the Python-based reservation microservice we’re going to use a Git‐\nHub template repository that contains most of the boilerplate code that we’ll need.\nKey Decision: Start Microservices with Reusable Templates\nUse code templates to jump-start a microservice development in each programming\nlanguage supported in your ecosystem. Using templates helps with speed of develop‐\nment without sacrificing quality, and keeps various microservices uniform in their\nkey traits.\nBased on the 10 guidelines, using any templates assumes that you have a working\nDocker installation and the GNU Make, since we use both of them. There are no\nother expectations, however. In Chapter 8, we showed how to set up Docker on mul‐\ntiple platforms. GNU Make usually comes preinstalled on macOS and Linux/Unix\nsystems. There are multiple ways to get GNU Make on Windows. The one we\nrecommend is Windows Subsystem for Linux.\n212 \n| \nChapter 9: Developing Microservices",
      "content_length": 2269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "Edit Code on a Host Run Inside Containers\nNote that in all further examples in this chapter, we assume that\nyou perform work on your host machine, without needing to log in\nto Docker containers.\nAs you work on a containerized project, your favorite code editor would be installed\non your macOS, Windows, or Linux machine and you will be executing various make\ncommands on that machine. Docker should be installed/available and most results of\nthe commands you issue will execute inside the containers, but there’s rarely a need\nfor you to explicitly shell into the containers, unless you are debugging something \nlow level.\nThe Code Behind the Flights Microservice\nTo use NodeBootstrap for jump-starting a Node/Express microservice, either install\nits bootstrapper with node install -g nodebootstrap (if you already have Node\navailable on your system), or clone this GitHub template repository.\nWhile the former may be somewhat easier, we will do the latter since we do not want\nto assume that you had to set up Node on your system. Go ahead and click “Use this\ntemplate” on the nodebootstrap-microservice’s main repository page, as shown in\nFigure 9-3.\nFigure 9-3. The nodebootstrap-microservice’s main repository page\nOnce you have created a new repository for the ms-flights microservice, at the desti‐\nnation of your choosing, let’s check it out on your developer machine and start modi‐\nfying things by writing code.\nImplementing Code for a Microservice \n| \n213",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "A nice thing about the NodeBootstrap template is that it comes with full support for\nan OAS of the microservices. Let’s take the specification we designed earlier and put it\ninto the docs/api.yml file, replacing the sample specification we already find there.\nMake sure you are in the docs subfolder and then run make start:\n→ make start\ndocker run -d --rm --name ms-nb-docs -p 3939:80 -v \\\nms-flights/docs/api.yml:/usr/share/nginx/html/swagger.yaml \\\n-e SPEC_URL=swagger.yaml redocly/redoc:v2.0.0-rc.8-1\n49e0986e318288c8bf6934e3d50ba93537ddf3711453ba6333ced1425576ecdf\nserver started at: http://0.0.0.0:3939\nThis will render the specification to a beautiful HTML template and make it available\nat http://0.0.0.0:3939. The rendering will probably look like Figure 9-4.\nFigure 9-4. Rendered OAS of the ms-flights microservice\nThe Nodebootstrap microservice comes with a sample “users” module, located under\nthe lib/users folder. Since we don’t need a user management module and do need a\nflights management one, let’s rename that folder flights and delete another default\nmodule, lib/homedoc, as we won’t need that one, either. When you remove the lib/\nhomedoc folder you need to also remove its plug from appConfig.js in the root folder.\nThis appears around line 24, and reads something like:\n  app.use('/',      require('homedoc')); // Attach to root route\nLikewise, change the hookup for the flights module in the same file, so that the line\nreads as follows:\napp.use('/flights', require('flights')); // Attach to sub-route\n214 \n| \nChapter 9: Developing Microservices",
      "content_length": 1569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "Once you are done making these modifications, edit lib/flights/controllers/mappings.js\nto introduce some input validation and indicate functions from the actions module of\nthe microservice that will be invoked for each of your two API endpoint routes:\nconst {spieler, check, matchedData, sanitize} = require('spieler')();\nconst router      = require('express').Router({ mergeParams: true });\nconst actions     = require('./actions');\nconst log = require(\"metalogger\")();\nconst flightNoValidation = check('flight_no',\n  'flight_no must be at least 3 chars long and contain letters and numbers')\n  .exists()\n  .isLength({ min: 3 })\n  .matches(/[a-zA-Z]{1,4}\\d+/)\nconst dateTimeValidation = check('departure_date_time',\n  'departure_date_time must be in YYYY-MM-ddThh:mm format')\n  .exists()\n  .matches(/\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}/)\nconst flightsValidator = spieler([\n  flightNoValidation,\n  dateTimeValidation\n]);\nconst seatmapsValidator = spieler([\n  flightNoValidation\n]);\nrouter.get('/', flightsValidator, actions.getFlightInfo);\nrouter.get('/:flight_no/seat_map', seatmapsValidator, actions.getSeatMap);\nmodule.exports = router;\nAs you can see, in this file we are setting up routes for our two main endpoints and\nvalidators that ensure that our input parameters are present, as well as properly for‐\nmatted. When they are not, NodeBootstrap also has standard error messaging to let\nthe client know.\nLet’s now implement some logic. First we need to create MySQL tables and some\nsample data. As you may guess, Nodebootstrap provides an easy solution for this as\nwell, in the form of database migrations: scripts that codify database modifications\nand allow you to apply them in any environment later.\nWe can create several database migrations with some make commands, as follows:\n→ make migration-create name=seat-maps\ndocker-compose -p msupandrunning up -d\nms-flights-db is up-to-date\nStarting ms-flights ... done\ndocker-compose -p msupandrunning exec ms-flights\nImplementing Code for a Microservice \n| \n215",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "./node_modules/db-migrate/bin/db-migrate create seat-maps --sql-file\n[INFO] Created migration at /opt/app/migrations/20200602055112-seat-maps.js\n[INFO] Created migration up sql file at\n  /opt/app/migrations/sqls/20200602055112-seat-maps-up.sql\n[INFO] Created migration down sql file at\n  /opt/app/migrations/sqls/20200602055112-seat-maps-down.sql\nsudo chown -R $USER ./migrations/sqls/\n[sudo] password for irakli:\n→ make migration-create name=flights\ndocker-compose -p msupandrunning up -d\nms-flights-db is up-to-date\nms-flights is up-to-date\ndocker-compose -p msupandrunning exec ms-flights\n  ./node_modules/db-migrate/bin/db-migrate create flights --sql-file\n[INFO] Created migration at /opt/app/migrations/20200602055121-flights.js\n[INFO] Created migration up sql file\n  at /opt/app/migrations/sqls/20200602055121-flights-up.sql\n[INFO] Created migration down sql file\n  at /opt/app/migrations/sqls/20200602055121-flights-down.sql\nsudo chown -R $USER ./migrations/sqls/\n→ make migration-create name=sample-data\ndocker-compose -p msupandrunning up -d\nms-flights-db is up-to-date\nms-flights is up-to-date\ndocker-compose -p msupandrunning exec ms-flights\n  ./node_modules/db-migrate/bin/db-migrate create sample-data --sql-file\n[INFO] Created migration at\n  /opt/app/migrations/20200602055127-sample-data.js\n[INFO] Created migration up sql file at\n  /opt/app/migrations/sqls/20200602055127-sample-data-up.sql\n[INFO] Created migration down sql file at\n  /opt/app/migrations/sqls/20200602055127-sample-data-down.sql\nsudo chown -R $USER ./migrations/sqls/\nAfter this, we should open the corresponding SQL files and insert the content in\nExamples 9-1, 9-2, and 9-3 into each one of them.\nExample 9-1. /migrations/sqls/[date]-seat-maps-up.sql\nCREATE TABLE `seat_maps` (\n  `flight_no` varchar(10) NOT NULL,\n  `seat_map` json NOT NULL,\n  `origin_code` varchar(10) NOT NULL,\n  `destination_code` varchar(10) NOT NULL,\n  PRIMARY KEY (`flight_no`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n216 \n| \nChapter 9: Developing Microservices",
      "content_length": 2018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "Example 9-2. /migrations/sqls/[date]-flights-up.sql\nCREATE TABLE `flights`  (\n  `flight_id` varchar(36) NOT NULL,\n  `flight_no` varchar(10) NOT NULL,\n  `flight_date` datetime(0) NULL,\n  PRIMARY KEY (`flight_id`),\n  FOREIGN KEY(`flight_no`)\n        REFERENCES seat_maps(`flight_no`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\nExample 9-3. /migrations/sqls/[date]-sample-data-up.sql\nINSERT INTO `seat_maps`\nVALUES ('AA2532', '{\\\"Cabin\\\": [{\\\"Row\\\": [{\\\"Seat\\\": [{\\\"Number\\\": \\\"A\\\",\n        \\\"Facilities\\\": [{\\\"Detail\\\": {\\\"content\\\": \\\"LegSpaceSeat\\\"}}],\n        \\\"exitRowInd\\\": false, \\\"premiumInd\\\": false, \\\"noInfantInd\\\": false,\n        \\\"restrictedReclineInd\\\": false}], \\\"RowNumber\\\": 8}],\n        \\\"Wing\\\": {\\\"lastRow\\\": 22, \\\"firstRow\\\": 14},\n        \\\"Column\\\": [{\\\"Column\\\": \\\"A\\\", \\\"Characteristics\\\": [\\\"Window\\\"]}],\n        \\\"lastRow\\\": 23, \\\"firstRow\\\": 8,\n        \\\"CabinClass\\\": {\\\"CabinType\\\": \\\"Economy\\\"}}]}', 'LAX', 'DCA');\nOnce you have these files, you can either just restart the project with make restart so\nthat the migrations will be automatically applied (the new ones get applied at every\nproject start to keep various installations consistent), or you can explicitly run a task\nto apply migrations with make migrate.\nFor the rest of the modifications, you will want to:\n1. Change ms-nodebootstrap-example to ms-flights in a variety of files, if you\ndidn’t install the project with the nodebootstrap utility, and just clone the reposi‐\ntory (the former approach does renaming for you).\n2. Modify the rest of the source code to implement the flights and seat_maps\nendpoints and hook them up with the database.\nms-flights Full Source Code\nYou can see a working version of the sample ms-flights code on this\nbook’s GitHub repository.\nWhen everything is working, you should be able to access your /flights endpoint\nlocally at a URL like the following:\nImplementing Code for a Microservice \n| \n217",
      "content_length": 1913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "http://0.0.0.0:5501/flights?flight_no=AA34&departure_date_time=2020-05-17T13:20\nThe seat_maps endpoint should appear in a URL:\nhttp://0.0.0.0:5501/flights/AA2532/seat_map\nBe sure to check out all of the makefile targets. Try testing one to get a sense of the\nuser experience provided by the template project and what kind of facilities you\nshould strive to provide to your developers with your templates. For the make test to\nwork, additional modifications are required, related to us deleting functionality from\nthe sample project. We aren’t covering those changes in detail here, so it’s best to just\ncheck out this book’s /ms-flights repository, which has every modification required.\nFeel free to submit bug requests if you run into any problems.\nHealth Checks\nTo manage the life cycle of the containers that the app will be deployed into, most\ncontainer-management solutions (e.g., Kubernetes, which we will use later in this\nbook) need a service to expose a health endpoint. In the case of Kubernetes, you\nshould generally provide liveness and readiness endpoints.\nKey Decision: Starting Microservices from Reusable Templates\nTo implement a health-check endpoint, we are going to use the draft RFC and a\nNode.js implementation of it.\nThe NodeBootstrap template already has a sample implementation for it, we just need\nto modify it for the ms-flights codebase.\nLet’s start by replacing lines 13–17 in appConfig.js with code like the following:\n// For Liveness Probe, defaults may be all you need.\nconst livenessCheck = healthcheck({\"path\" : \"/ping\"});\napp.use(livenessCheck.express());\n// For readiness check, let's also test the DB\nconst check = healthcheck();\nconst AdvancedHealthcheckers = require('healthchecks-advanced');\nconst advCheckers = new AdvancedHealthcheckers();\n// Database health check is cached for 10000ms = 10 seconds!\ncheck.addCheck('db', 'dbQuery', advCheckers.dbCheck,\n  {minCacheMs: 10000});\napp.use(check.express());\n218 \n| \nChapter 9: Developing Microservices",
      "content_length": 1989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "This will create a simple “Am I live?” check at /ping (known as a liveness probe in\nKubernetes) and a more advanced “Is the database also ready? Can I actually do\nuseful things?” check (known as the readiness probe in Kubernetes) at /health. Using\ntwo probes for overall health is very convenient since a microservice being up doesn’t\nalways mean that it is fully functional. If its dependency, such as a database, is not up\nyet or is down, it won’t be actually ready for useful work.\nThe fourth argument {minCacheMs: 10000} in the .addCheck() call sets minimal\ncache duration on the server side, indicated in milliseconds. This means you can tell\nthe health-check middleware (the module we use) to only run an expensive, database-\nquerying health-check probe against MySQL every 10 seconds (10,000 milliseconds),\nat most!\nEven if your health-probing infrastructure (e.g., Kubernetes) calls your health-check\nendpoint very frequently, the middleware will only trigger the calls you deemed light\nenough. For more heavy calls (e.g., database calls like the one to MySQL), the middle‐\nware (Maikai module) will serve cached values, avoiding stress on downstream sys‐\ntems like the database. To complete the setup, also edit the libs/healthchecks-\nadvanced/index.js file and rename the function to dbCheck. Then update the SQL\nquery so that lines 7–10 read:\nasync dbCheck() {\n  const start = new Date();\n  const conn = await db.conn();\n  const query = 'select count(1) from seat_maps';\nIf everything was done correctly and the microservice is up and running in a healthy\nway, if you now run curl http://0.0.0.0:5501/health, you should get a health\nendpoint output that looks like the following:\n{\n  \"details\": {\n    \"db:dbQuery\": {\n      \"status\": \"pass\",\n      \"metricValue\": 15,\n      \"metricUnit\": \"ms\",\n      \"time\": \"2020-06-28T22:32:46.167Z\"\n    }\n  },\n  \"status\": \"pass\"\n}\nIf you run curl http://0.0.0.0:5501/ping instead, you should get a simpler\noutput:\n{ \"status\": \"pass\" }\nImplementing Code for a Microservice \n| \n219",
      "content_length": 2024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "If you run into any issues while modifying the code yourself, you can see the full\nmicroservice implementation on this book’s GitHub repository.\nNow that we have a fully functioning ms-flights microservice implemented with\nNode.js and MySQL, let’s switch to the code behind the ms-reservations microservice.\nIntroducing a Second Microservice to the Project\nWe are going to implement a second microservice (ms-reservations) in Python and\nFlask using the Redis data store. Again following the second goal from “10 Work‐\nspace Guidelines for a Superior Developer Experience” on page 181, we will use a\ntemplate GitHub repository for a Python/Flask stack.\nAs you can see, this template has a lot of the same characteristics as the NodeBoot‐\nstrap one we just used for ms-flights: it only requires working with Docker and make,\nhas all of the make targets to support a smooth development experience, just like\nNodeBootstrap, and has a working setup for common tasks such as testing, linting,\netc. One thing it lacks, however, is support for database migrations.\nUnlike MySQL, Redis doesn’t really use database schemas, so there’s no burning need\nto codify various data definitions for “table” creations. You could still use migrations\nto create test data in various environments, but we will leave that task to the reader to\nfigure out and have fun with. It is one way this template is different from the ones you\nwould see that do use SQL databases.\nJust like with ms-flights, we’ll start our code modifications by placing the proper OAS\nwe developed earlier in this chapter into the docs/api.yml of the new ms-reservations\nrepository. After running make start in the docs folder (note: this is a separate make‐\nfile from the main one!), you should see the API specification for reservations ren‐\ndered at http://0.0.0.0:3939, appearing as shown in Figure 9-5.\n220 \n| \nChapter 9: Developing Microservices",
      "content_length": 1900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "Figure 9-5. Rendered OAS of the ms-reservations microservice\nWe will start modifying our template microservice by implementing the reservation\ncreation endpoint.\nOpen service.py and replace the mapping for the update_user POST /users endpoint\nwith the one for PUT /reservations, like this:\n@app.route('/reservations', methods=['PUT'])\ndef reserve():\n    \"\"\"Endpoint that reserves a seat for a customer\"\"\"\n    json_body = request.get_json(force=True)\n    resp = handlers.reserve(json_body)\n    if (resp.get(\"status\") == \"success\"):\n        return jsonify(resp)\n    else:\n        return Response(\n            json.dumps(resp),\n            status=403,\n            mimetype='application/json'\n        )\nAs you can see, based on the result of the reservation, we’ll output a success or an\nerror and provide a corresponding HTTP error code.\nTo fully implement this endpoint, we also need to create a handler for the mapping\n(usually tasked with error validation, but for brevity we’ll skip it) in src/handlers.py.\nWe’ll do this by replacing the save_user user creation handler with the following:\nIntroducing a Second Microservice to the Project \n| \n221",
      "content_length": 1147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "def reserve(json_body):\n    \"\"\"Save reservation callback\"\"\"\n    return model.save_reservation(json_body)\nMost importantly, we need to implement the actual save to the database in src/\nmodels.py by replacing the save_user function with something like the following:\ndef save_reservation(reservation):\n    \"\"\"Saves reservation into Redis database\"\"\"\n    seat_num = reservation['seat_num']\n    try:\n        result = this.redis_conn.hsetnx(\n            this.tblprefix + reservation['flight_id'],\n            seat_num,\n            reservation['customer_id'])\n    except redis.RedisError:\n        response = {\n            \"error\" : f\"Unexpected error reserving {seat_num}\"\n        }\n        log.error(f\"Unexpected error reserving {seat_num}\", exc_info=True)\n    else:\n        if result == 1:\n            response = {\n                \"status\": \"success\",\n            }\n        else:\n            response = {\n                \"error\" : f\"Could not complete reservation for {seat_num}\",\n                \"description\" : \"Seat already reserved. Cannot double-book\"\n            }\n    return response\nThis code implements the same hsetnx command in Python that we manually exe‐\ncuted earlier in the Redis CLI, when we were demonstrating the benefits of using\nRedis for the data model of the reservations microservice. Redis’s hsetnx method\nonly sets the value if one is not already set. This is how we reliably avoid accidental\ndouble-booking. When hsetnx is rejected due to an already set key, it returns 0 (as in:\n“0 records modified”); otherwise it returns 1, letting us know whether a conflict\noccurred.\nYou should also declare the table-level prefix for reservations in the module scope by\nadding the following code around line 19 of src/models.py, right after the this =\nsys.modules[__name__] declaration:\nthis = sys.modules[__name__] # Existing line\nthis.tblprefix = \"flights:\" # New line\nThe microservice template we used readily contains all of the code required to grab\nthe relevant credentials and configuration from the environment and connect to a\n222 \n| \nChapter 9: Developing Microservices",
      "content_length": 2090,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "Redis database. This is implemented in accordance with the popular guidance out‐\nlined in the manifesto for building better cloud native applications, known as the\nTwelve-Factor App. Specifically, the template aligns with the third factor of the docu‐\nment, which addresses preferred ways of configuration management. The fact that\nthe template we used already had this best practice fully implemented demonstrates\nonce again the significant benefits of leveraging code templates for microservices\ndevelopment.\nOnce you make all the required changes, the endpoint should work. You should be\nable to run make from the top level of the source code, which will build and run the\nproject at 0.0.0.0:7701.\nIf you encounter issues at any point or would like to check out the application logs for\nsome reason, you can do this using the logs-app make target:\n→ make logs-app\ndocker-compose -p ms-workspace-demo logs -f ms-template-microservice\nAttaching to ms-template-microservice\nms-template-microservice    | [INFO] Starting gunicorn 20.0.4\nms-template-microservice    | [INFO] Listening at: http://0.0.0.0:5000 (1)\nms-template-microservice    | [INFO] Using worker: sync\nms-template-microservice    | [INFO] Booting worker with pid: 15\nYou may notice that the logs say the service is running on port 5000, but that is true\ninside the Docker container; it’s not port 5000 on the host machine! We map the stan‐\ndard Flask port 5000 to 7701 on the host machine (your machine). You can view the\ncombined app and database logs by running make logs, or just the database logs by\nrunning make logs-db.\nNow let’s run several curl commands to insert a couple of reservations:\ncurl --header \"Content-Type: application/json\" \\\n  --request PUT \\\n  --data '{\"seat_num\":\"12B\",\"flight_id\":\"werty\", \"customer_id\": \"dfgh\"}' \\\n  http://0.0.0.0:7701/reservations\ncurl --header \"Content-Type: application/json\" \\\n  --request PUT \\\n  --data '{\"seat_num\":\"12C\",\"flight_id\":\"werty\", \"customer_id\": \"jkfl\"}' \\\n  http://0.0.0.0:7701/reservations\nWe can also test that our protection against accidental double-bookings works. Let’s\nverify this by attempting to reserve an already reserved seat (e.g., 12C):\ncurl -v --header \"Content-Type: application/json\" \\\n  --request PUT \\\n  --data '{\"seat_num\":\"12C\",\"flight_id\":\"werty\", \"customer_id\": \"another\"}' \\\n  http://0.0.0.0:7701/reservations\nIt will respond with HTTP 403 and an error message:\nIntroducing a Second Microservice to the Project \n| \n223",
      "content_length": 2468,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "→ curl -v --header \"Content-Type: application/json\" \\\n>   --request PUT \\\n>   --data '{\"seat_num\":\"12C\",\"flight_id\":\"werty\", \"customer_id\": \"another\"}' \\\n>   http://0.0.0.0:7701/reservations\n*   Trying 0.0.0.0:7701...\n* TCP_NODELAY set\n* Connected to 0.0.0.0 (127.0.0.1) port 7701 (#0)\n> PUT /reservations HTTP/1.1\n> Host: 0.0.0.0:7701\n> User-Agent: curl/7.68.0\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 64\n>\n< HTTP/1.1 403 FORBIDDEN\n< Server: gunicorn/20.0.4\n< Connection: close\n< Content-Type: application/json\n< Content-Length: 111\n<\n* Closing connection 0\n{\"error\": \"Could not complete reservation for 12C\",\n\"description\": \"Seat already reserved. Cannot double-book\"}\nPerfect!\nSince we now have some data in the Redis store, we can proceed to implementing the\nreservation retrieval endpoint as well. Again, we will start with the mapping defini‐\ntion in service.py, replacing the default /hello/<name> greeter endpoint with the\nfollowing:\n@app.route('/reservations', methods=['GET'])\ndef reservations():\n    \"\"\" Get Reservations Endpoint\"\"\"\n    flight_id = request.args.get('flight_id')\n    resp = handlers.get_reservations(flight_id)\n    return jsonify(resp)\nThe implementation of the handler in src/handlers.py will again be simple since we\nare skipping input validation, for the sake of brevity:\ndef get_reservations(flight_id):\n    \"\"\"Get reservations callback\"\"\"\n    return model.get_reservations(flight_id)\nThe model code will look like the following:\ndef get_reservations (flight_id):\n    \"\"\"List of reservations for a flight, from Redis database\"\"\"\n    try:\n        key = this.tblprefix + flight_id\n        reservations = this.redis_conn.hgetall(key)\n    except redis.RedisError:\n224 \n| \nChapter 9: Developing Microservices",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "response = {\n            \"error\" : \"Cannot retrieve reservations\"\n        }\n        log.error(\"Error retrieving reservations from Redis\",\n            exc_info=True)\n    else:\n        response = reservations\n    return response\nTo test this endpoint, we can issue a curl command and verify that we receive the \nexpected JSON response:\n→ curl -v  http://0.0.0.0:7701/reservations?flight_id=werty\n*   Trying 0.0.0.0:7701...\n* TCP_NODELAY set\n> GET /reservations?flight_id=werty HTTP/1.1\n> Host: 0.0.0.0:7701\n> Accept: */*\n>\n< HTTP/1.1 200 OK\n< Server: gunicorn/20.0.4\n< Connection: close\n< Content-Type: application/json\n< Content-Length: 90\n<\n{\n  \"12B\": \"dfgh\",\n  \"12C\": \"jkfl\",\n}\n* Closing connection 0\nms-reservations Full Source Code\nYou can see a working version of the sample ms-reservations code\non this book’s GitHub site.\nPlease take a look and try to use various make targets available in the repository to\nget a better feel for what you get from the template this code was bootstrapped from.\nYou should also use this opportunity to take a break and pat yourself on the back—\nyou just created and executed two perfectly sized, impeccably implemented, and\nbeautifully separate-stack microservices! Hooray!\nNow what we need to do is figure out a way to execute these two microservices (and\nany additional future components you may create) as a single unit. For this, we will\nintroduce the notion of an “umbrella project” and explain how to develop one.\nIntroducing a Second Microservice to the Project \n| \n225",
      "content_length": 1514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "Hooking Services Up with an Umbrella Project\nDeveloping individual microservices is how teams should be spending most of their\ntime. It’s essential for providing team autonomy, which leads to the ever-important\ncoordination minimizations, and most of our system design work in the microservi‐\nces style should indeed target the minimization of coordination needs. That said, at\nsome point we do need to try the entire project—all microservices working together.\nEven if this need is relatively rare, it is very important to make doing so easy, which is\nwhy principle four of the “10 Workspace Guidelines for a Superior Developer Experi‐\nence” on page 181 states: “Running a single microservice and/or a subsystem of sev‐\neral ones should be equally easy.”\nWe need an easy-to-use umbrella project, one that can launch all of our microservice-\nspecific subprojects in one simple command and make them all work together nicely,\nuntil such time as we decided to shut down the umbrella project with all of its com‐\nponents. This obviously should also be very easy to do. Everything we want our\ndevelopers to do without mistakes should be easy!\nTo deploy an easy-to-use umbrella project, we’ll use the microservices workspace\ntemplate available at this GitHub site and start a workspace for us at this book’s Git‐\nHub repository instead.\nKey Decision: Use Faux Git Submodules\nTo check out repositories of individual microservices under the umbrella repository,\nwe use the open source project Faux Git Submodules. The idea is to make it easy to\ndescend into a subfolder of your workspace repository containing a microservice and\ntreat it as a fully functioning repository, which you can update, commit code in, and\npush to. The basic intent is identical to that of regular Git submodules, except anyone\nwho has used them knows that the actual submodules can behave unpredictably and\ntend to be major pains in the neck. Faux submodules, in our opinion, are much sim‐\npler and work more predictably.\nWe’ll start by indicating the two repos we’ve just created as the components of the\nnew workspace, by editing the fgs.json file to look something like the following:\n{\n  \"ms-flights\" : {\n    \"url\"  : \"https://github.com/implementing-microservices/ms-flights\"\n  },\n  \"ms-reservations\" : {\n    \"url\" : \"https://github.com/implementing-microservices/ms-reservations\"\n  }\n}\n226 \n| \nChapter 9: Developing Microservices",
      "content_length": 2404,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "In the last configuration we indicated ms-flights and ms-reservations using the read-\nonly “http://” protocol. This was done so that you can follow the example. In real\nprojects, you would want to pull your repositories with the read/write “git://” proto‐\ncol so you can modify them.\nNow that we have configured repos.json, let’s pull the ms-flights and ms-reservations\nmicroservices into the workspace:\n→ make update\ngit clone -b master \\\n  https://github.com/implementing-microservices/ms-flights ms-flights\nCloning into 'ms-flights'...\ngit clone -b master \\\n  https://github.com/implementing-microservices/ms-reservations ms-reservations\nCloning into 'ms-reservations'...\nThis operation also helpfully adds the checked-out repositories to\nthe .gitignore of the parent folder, to prevent the parent repository\ntrying to double-commit them into the wrong place.\nWe also need to edit the bin/start.sh and bin/stop.sh scripts to make changes from the\ndefault. We’ll edit bin/start.sh as shown in Example 9-4.\nExample 9-4. bin/start.sh\n#!/usr/bin/env bash\nset -eu\nexport COMPOSE_PROJECT_NAME=msupandrunning\npushd ms-flights && make start\npopd\npushd ms-reservations && make start\npopd\nmake proxystarts\nEdit .bin/stop.sh as in Example 9-5.\nExample 9-5. bin/stop.sh\n#!/usr/bin/env bash\nset -eu\nexport COMPOSE_PROJECT_NAME=msupandrunning\nHooking Services Up with an Umbrella Project \n| \n227",
      "content_length": 1384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "pushd ms-flights && make stop\npopd\npushd ms-reservations && make stop\npopd\nmake proxystop\nTo keep things simple yet powerfully automated, our workspace setup is using the\nTraefik edge router for seamless routing to the microservices. It gets installed by our\ndocker-compose.yml file. Also, we will need to add Traefik-related labels to the docker-\ncompose.yml files of both microservices to ensure proper routing of those services, as\nshown in Examples 9-6 and 9-7.\nExample 9-6. ms-flights/docker-compose.yaml\nservices:\n  ms-flights:\n    container_name: ms-flights\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.ms-flights.rule=PathPrefix(`/reservations`)\"\nExample 9-7. ms-reservations/docker-compose.yaml\nservices:\n  ms-reservations:\n    container_name: ms-reservations\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.ms-reservations.rule=PathPrefix(`/reservations`)\"\nWe also need to update the umbrella project’s name (which serves as the namespace\nand network name for all services) in the workspace’s makefile, so that instead of\nproject:=ms-workspace-demo, it says:\nproject:=msupandrunning\nOnce you bring up the workspace by running make start at the workspace level, you\nwill be able to access both microservices in their attached-to-workspace form. We\nmounted Traefik to local port 9080, making http://0.0.0.0:9080/ our base URI. There‐\nfore, the following two commands are querying the reservations and flights systems:\n> curl http://0.0.0.0:9080/reservations?flight_id=qwerty\n> curl \\\n  http://0.0.0.0:9080/flights?\\\n  flight_no=AA34&departure_date_time=2020-05-17T13:20\nYou can see the full source of the umbrella project at this book’s GitHub site.\n228 \n| \nChapter 9: Developing Microservices",
      "content_length": 1752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "Summary\nIn this chapter we brought together a lot of system design and code implementation\nguidance that we had been teasing out to provide an end-to-end implementation of a\ncouple of powerful microservices, together with an umbrella workspace that allows us\nto work on these services either individually or as a unified project. We did this\nthrough a step-by-step implementation of the powerful SEED(S) methodology and\nthe design of individual data models, and learned how to quickly jump-start code\nimplementations from robust template projects.\nThe ability to put together well-modularized components quickly and efficiently can\nmake a material difference in your ability to execute microservice projects success‐\nfully. There’s a big difference between what you were able to achieve in this chapter,\nand somebody spending weeks figuring out the basic boilerplate or going down the\nrabbit hole of wrong decisions. This difference can be that of the success or failure of\nthe entire initiative.\nSummary \n| \n229",
      "content_length": 1012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "CHAPTER 10\nReleasing Microservices\nWe’re getting to an exciting part of our microservices build—the point where we\nactually bring everything together. So far, we’ve built an operating model, a microser‐\nvice design, an infrastructure foundation, and two working microservices. Now, we’ll\ntake all those pieces and put them together in a single implementation.\nWe’ll be covering a lot of ground in this chapter. We’ll build a new infrastructure envi‐\nronment called staging. Next, we’ll augment our code repository with a container\ndelivery process. With a container ready to go, we’ll implement a deployment process\nusing the Argo CD GitOps tool. When we’re done, we’ll have an architecture that\nlooks like Figure 10-1.\nFigure 10-1. Staging deployment\nBecause of the scope of what we need to cover, we’ll only deploy the\nflight information microservice. However, you can use all the\nmechanisms we describe here to deploy the reservations service as\nwell.\n231",
      "content_length": 958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "To make all this work, we’ll be using three different GitHub repositories with their\nown pipelines and assets (as shown in Figure 10-2). One of the reasons we’ve done it\nthis way is that it matches up well with the operating model we defined in Chapter 2\nand gives each of our teams their own responsibilities and domains to work in.\nFigure 10-2. Three code repositories for deployment\nThere’s a lot to cover, so let’s dive in with our first step: provisioning the AWS-based\nstaging environment.\nSetting Up the Staging Environment\nUp until now, we’ve been deploying microservices into a local developer environ‐\nment. Now we’ll take the same services we’ve built and tested locally and deploy them\ninto an AWS-based cloud infrastructure. In this section, we’ll build the staging infra‐\nstructure using the process shown in Figure 10-3.\nFigure 10-3. Building a staging environment\nWe started this work when we built the sandbox environment in Chapter 7. Now,\nwe’ll need to update that infrastructure code to reflect the needs of the flight informa‐\ntion and flight reservation microservices. We’ll be adding three new components to\nour Terraform code to support our microservices:\n• An ingress controller and edge router that sends requests to microservices in\nKubernetes\n• An AWS-based MySQL database instance for the flights microservice’s data\n232 \n| \nChapter 10: Releasing Microservices",
      "content_length": 1389,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "• An AWS-based Redis database instance for the reservations microservice’s data\nThis kind of nontrivial change could be risky. But this is where our immutable infra‐\nstructure and infrastructure as code (IaC) approach really starts to pay off! We know\nexactly what our current environment build looks like, because all of it is in our Ter‐\nraform code. All we need to do now is create modules for each of these new compo‐\nnents, update an environment definition, and run the build through our CI/CD\npipeline.\nIn Chapter 7, we walked through the process of writing each Terraform module\ntogether. But since we’ve already covered that in detail, this time we’ll use code and\nconfiguration assets that we’ve already written for you. You’ll just need to customize\nthem a bit to fit your needs.\nLet’s start by taking a quick tour of the new modules we’ll be using to provision our\nnew components, starting with the ingress gateway module.\nThe Ingress Module\nIn Chapter 9, we used an edge router called Traefik to route messages to our\ncontainer-based microservices. We’re going to implement a similar architecture in\nour AWS-based infrastructure. There are plenty of tools available to perform ingress\nrouting. For example, many practitioners use the Nginx ingress controller. Traefik is\nalso a fine choice: since we’ve already started using it in the development stage, we’ll\nmake the decision to implement it as our controller in the AWS environment as well.\nKey Decision: Implement a Traefik Ingress Controller\nWe’ll use Traefik to route messages from the load balancer to microservices deployed\nin Kubernetes.\nTo save time, we’ve already written a Terraform module that will install the Traefik\ningress controller into the environment. We’ll be able to use this module in our Terra‐\nform environment code, in the same manner as the network, EKS, and Argo CD\nmodules we used in Chapter 6. The code for the Traefik module is available at this\nGitHub site.\nWe won’t have time to implement a “backend for frontend” (BFF) API as part of our\nexample build. But the ingress we’re setting up lends itself well to being extended for\nthat purpose in the future. For example, you can provision an AWS API Gateway in\nfront of the ingress controller to compose services into a single API. In fact, we’ve\nimplemented Traefik using an AWS Network Load Balancer, which makes that kind\nof connectivity easier to implement.\nSetting Up the Staging Environment \n| \n233",
      "content_length": 2447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "We’ll get a chance to use this ingress module in “Forking the Staging Infrastructure\nProject” on page 234 when we build the staging environment. For now, let’s see how\nwe’ll support our database needs.\nThe Database Module\nEach of our microservices use different databases, so we’ll need to provision two dif‐\nferent databases in the infrastructure environment. We’ll need both a MySQL and a\nRedis database to support the needs of our microservices teams. For our build, we’ve\ndecided to use AWS managed versions of these database products. That way, our plat‐\nform team can offer two databases in an x-as-a-service manner to the microservices\nteams that need them.\nKey Decision: Use Shared and Managed Database Services\nThe platform team will create Terraform-based modules to provision AWS hosted\nand managed database services for each environment.\nIn our database module, we’ll use the AWS ElastiCache service to provision a Redis\ndata store and the AWS Relational Database (RDS) to provision a MySQL instance.\nWe’ve already written a module that does this, which you can find in this book’s Git‐\nHub repository. The module provisions both types of databases as well as the net‐\nwork configuration and access policies that the database service needs for operation.\nWhen the module is applied to the staging environment, we’ll have both a Redis and a\nMySQL database instance running and ready for use. All that’s left now is to use our\nmodules in a Terraform code file and provision an environment. That’s what we’ll\ncover in the next section.\nForking the Staging Infrastructure Project\nThe staging environment we need for releasing our microservices will be very similar\nto the sandbox environment we created in Chapter 7. We’ll continue to use the same\nmethods and principles we applied earlier. We’ll use Terraform to define the environ‐\nment in code and we’ll use the modules we wrote for the network, Kubernetes cluster,\nand Argo CD. We’ll complement those modules with the new database and ingress\ncontroller modules we’ve just described. Finally, we’ll use a GitHub Actions pipeline\nto provision the environment, just like we did for our sandbox environment.\nWe explained how to create a GitHub Actions pipeline in Chapter 6, and walked\nthrough the process of writing and using Terraform code in Chapter 7. So there’s no\nneed to do all of that again. Instead, we’ll use a staging environment skeleton project\nthat we’ve already created for you (see Figure 10-4). We’ll need to make a few small\n234 \n| \nChapter 10: Releasing Microservices",
      "content_length": 2545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "changes to the code so that it will work in your AWS environment. To do that, we’ll\nfork the repository so you can have your own copy that you can change as you like.\nFigure 10-4. Starting with the staging environment repository\nIn GitHub, a fork lets you make a copy of someone else’s code project in your own\naccount. To fork the staging environment repository, follow these steps:\n1. Open your browser and sign in to your GitHub account.\n2. Navigate to this book’s GitHub repository.\n3. Click the Fork button in the top-right corner of the screen.\nYou may want to duplicate this repository instead of forking it.\nThis will allow you to change the access mode of the repository to\nprivate instead of public. Instructions on duplicating a GitHub\nrepository are available in the GitHub documentation on the topic.\nOnce the operation is complete you’ll have your own forked copy of infra-staging-\nenv. But it’s not yet configured to use your AWS account or resources. The first thing\nwe’ll need to update is the GitHub Actions workflow.\nConfiguring the Staging Workflow\nThe forked CI/CD workflow we’ve just created won’t be able to access your AWS\naccount without credentials. So we’ll need to add AWS access management creden‐\ntials and a MySQL password to the repository’s secrets. You should have your AWS\noperator account credentials from the pipeline setup work we did in Chapter 6. If you\ndon’t have those keys anymore, you can open the AWS management console in a\nbrowser and create a new set of credentials for your operations user.\nWhen you have your credentials in hand, navigate to the Settings pane of your forked\nGitHub repository and choose Secrets from the lefthand navigation menu. Add the\nsecrets in Table 10-1 by clicking the New secret button.\nSetting Up the Staging Environment \n| \n235",
      "content_length": 1804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "Table 10-1. Infrastructure secrets\nKey\nValue\nAWS_ACCESS_KEY_ID\nThe access key ID for your AWS operator user\nAWS_SECRET_ACCESS_KEY The secret key for your operator user\nMYSQL_PASSWORD\nmicroservices\nMake sure you type these key names exactly as described in Table 10-1. If you don’t,\nthe pipeline won’t be able to access your AWS instance and create resources. Use the\nvalue microservices for the MYSQL_PASSWORD secret. This password will be used\nwhen we provision the AWS RDS database.\nWhen we forked the infra-staging-env repository, GitHub made a copy of the Actions\nworkflow that defines the CI/CD pipeline. But, for security reasons, GitHub doesn’t \nautomatically enable the GitHub Actions feature when you fork a repository (see\nFigure 10-5). So, you’ll need to get it running by doing the following:\n• Click the Actions tab in the management console for your forked repository.\n• If challenged, instruct GitHub to enable the workflow that we’ve forked.\nFigure 10-5. Enable GitHub Actions\nGitHub changes its user experience quite often, so the specific steps\nand screens you encounter might be different.\nOur forked infrastructure pipeline is now activated and ready to be triggered. Now\nwe just need to make a few adjustments to the Terraform code that creates the staging\nenvironment.\n236 \n| \nChapter 10: Releasing Microservices",
      "content_length": 1334,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "The staging workflow we’ve built for you will automatically gener‐\nate a kubeconfig file as part of the provisioning process. This file\ncontains connection information so you can connect to the Kuber‐\nnetes cluster that we’ll create on EKS. Since this code repository is\npublic, that file will be available to anyone who visits your reposi‐\ntory. In theory, this shouldn’t be a problem. Our EKS cluster\nrequires AWS credentials to authenticate and connect. That means\neven with the kubeconfig file an attacker shouldn’t be able to con‐\nnect to your cluster, unless they also have your AWS credentials.\nEditing the Staging Infrastructure Code\nThe Terraform code that we’ve written for you will provision a staging environment.\nBut, it won’t work properly until we set some local variable values that will be specific\nto your AWS account and environment. To do that, we’ll work on the files in your\nlocal environment. Also, you’ll need to create a clone of your forked infra-staging-env\nrepository. We’ll leave it to you to do that.\nIf you aren’t sure how to clone your repository, follow the instruc‐\ntions for your OS in the GitHub documentation.\nWe’ll be editing the main.tf file that defines the staging environment. You’ll need to\nchange the values of a few local variables that we’ve defined in Table 10-2.\nTable 10-2. Staging environment values in main.tf\nResource\nProperty name\nDescription\nterraform\nbucket\nThe name of the S3 bucket for your Terraform backend\nterraform\nkey\nThe identifier to use for your backend data in S3\nterraform\nregion\nYour AWS region\nlocals\naws_region\nYour AWS region\nThese values will be identical to the configuration we created in\nChapter 7, so if you have that code handy, you can copy and paste\nfrom there.\nTo make these changes, edit main.tf in your favorite text editor and update it with the\nappropriate values. All the values you need to replace are in the terraform and\nlocals sections, at the top of the file. You can also use this step to review the\nSetting Up the Staging Environment \n| \n237",
      "content_length": 2033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "Terraform file and learn more about what it does. When you’re done, the top of your\nfile should look similar to Example 10-1.\nExample 10-1. An updated main.tf for the staging environment\nterraform {\nbackend \"s3\" {\n  bucket = \"rm-terraform-backend\"\n  key = \"terraform-env\"\n  region = \"eu-west-2\"\n  }\n}\nlocals {\n  env_name = \"staging\"\n  aws_region = \"eu-west-2\"\n  k8s_cluster_name = \"ms-cluster\"\n}\nOur staging Terraform code is now ready to be applied. But it won’t work if we try to\nuse it. That’s because our AWS operator account doesn’t have the right privileges.\nWe’ve added some new database modules, but the operator account we’re using isn’t\nallowed to create or work with those AWS resources. If we tried to run our Terraform\nright now, we’d get access errors from AWS.\nTo solve that problem, we’ll need to give our AWS operator a few more permissions.\nWe’ll do this by creating a new IAM group for database work. When the group is set\nup, we’ll add our operator account to the group so it inherits those permissions.\nRun the following AWS CLI command to create a new group called DB-Ops:\n$ aws iam create-group --group-name DB-Ops\nNext, we can run the following command to attach access policies for RDS and Elas‐\ntiCache to the group:\n$ aws iam attach-group-policy --group-name DB-Ops\\\n --policy-arn arn:aws:iam::aws:policy/AmazonRDSFullAccess &&\\\naws iam attach-group-policy --group-name DB-Ops\\\n --policy-arn arn:aws:iam::aws:policy/AmazonElastiCacheFullAccess\nFinally, use a CLI command to add our Ops account to the group we’ve just created:\n$ aws iam add-user-to-group --user-name ops-account --group-name DB-Ops\nWith those permissions set, we’re just about ready to go. But before we commit, let’s\ndo a quick test to make sure our updated infrastructure code works. Run the follow‐\ning Terraform commands to format and validate our updated code:\ninfra-staging-env$ terraform fmt\n[...]\n238 \n| \nChapter 10: Releasing Microservices",
      "content_length": 1942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "infra-staging-env$ terraform init\n[...]\ninfra-staging-env$ terraform validate\n[...]\ninfra-staging-env$ terraform plan\n[...]\nNow we’re ready to commit the infrastructure code and kick off the CI/CD pipeline.\nLet’s start by committing our updated Terraform code to your forked repository:\n$ git add .\n$ git commit -m \"Staging environment with databases\"\n$ git push origin\nIf you recall in Chapter 6, our workflow gets triggered when we push a release tag\nthat starts with a v. Use the following Git commands to create a new v1.0 tag and\npush it to your forked repository:\n$ git tag -a v1.0 -m \"Initial staging environment build\"\n$ git push origin v1.0\nWith that, your staging provisioning process should be kicked off. You can validate\nthe status of your pipeline run in the browser-based GitHub console. If the pipeline\njob has succeeded, you now have a staging environment with a Kubernetes cluster\nand MySQL and Redis databases running and ready to use. We’ll need that Kuber‐\nnetes cluster for our microservices deployment. So our next step will be to validate\nthat it is up and running.\nTesting access to the Kubernetes cluster\nIn order to communicate with the staging Kubernetes cluster we’ll need the configu‐\nration details for the kubectl application. To get those details we’ll use the same pro‐\ncess we used in “Testing the Environment” on page 175—we’ll download a\nconfiguration file and update our local environment settings.\nMake sure the CI/CD pipeline has completed successfully before\ntrying to connect to the Kubernetes cluster.\nSet up your Kubernetes client environment by downloading the kubeconfig file that\nour GitHub Actions staging pipeline generated. Then set the KUBECONFIG environ‐\nment variable to point to the configuration that you’ve just downloaded:\n$ export KUBECONFIG=~/Downloads/kubeconfig\nSetting Up the Staging Environment \n| \n239",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "When the environment is set up, run kubectl get svc --all-namespaces to con‐\nfirm that our staging cluster is running and the Kubernetes objects have been\ndeployed. You should see a result that looks similar to Example 10-2.\nExample 10-2. get svc result\n$ kubectl get svc --all-namespaces\nNAMESPACE   NAME                               TYPE         CLUSTER-IP\nargocd      msur-argocd-application-controller ClusterIP    172.20.133.240\nargocd      msur-argocd-dex-server             ClusterIP    172.20.74.68\ndefault     ms-ingress-nginx-ingress           LoadBalancer 172.20.239.114\n[... lots more services ...]\nIn the result you should see a list of all the Kubernetes services that we’ve deployed.\nThat should include services for the Argo CD application and the Nginx ingress ser‐\nvice. That means that our cluster is up and running and the services we need have\nbeen successfully provisioned.\nCreate a Kubernetes secret\nThe last step we need to take care of is setting up a Kubernetes secret. When our flight\ninformation microservices connects to MySQL, it will need a password. To avoid\nstoring that password in plain text, we’re going to store it in a special Kubernetes\nobject that keeps it hidden from unauthorized viewers.\nRun the following command to create and populate the Kubernetes secret for the\nMySQL password:\n$ kubectl create secret generic \\\nmysql --from-literal=password=microservices -n microservices\nThe built-in secrets functions of Kubernetes are useful, but we rec‐\nommend that you use something more feature rich for a proper\nimplementation. There are lots of options available in this area,\nincluding HashiCorp Vault.\nWe now have a staging environment with an infrastructure that fits the needs of the\nmicroservices we’ve developed. The next step will be to publish those microservices\nas containers so that we can deploy them into the environment.\n240 \n| \nChapter 10: Releasing Microservices",
      "content_length": 1919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "Shipping the Flight Information Container\nIn Chapter 9, we used make to test, build, and run microservices locally in a develop‐\nment environment. But in order to build and deploy our services into testing, staging,\nand beyond, we’ll want a more repeatable and automated process.\nWe’ve already built a containerized version of the flights microser‐\nvice for you to use. So, if you aren’t interested in building a Docker\nHub deployment workflow yourself, you can skip ahead to\n“Deploying the Flights Service Container” on page 246.\nUsing automation and DevOps techniques to build our services improves the predict‐\nability, quality, and speed of our microservice deployments. This is the same principle\nwe applied to our infrastructure build. In this section, we’ll build a continuous inte‐\ngration and continuous delivery (CI/CD) pipeline to build and publish the flights\nmicroservice to a container registry, as in Figure 10-6.\nFigure 10-6. Microservices CI/CD\nLet’s start by taking a look at Docker Hub, the container registry we’ll be using to host\nmicroservice containers.\nIntroducing Docker Hub\nIn Chapter 9, our make-based build process used both docker-compose and docker to \nproduce containers for testing and release. In order to get those containers into our\nstaging environment, we’ll need a way to move them, or ship them over. Containers\nare a lot like binary applications, so we could just upload them into the filesystem of\nthe target environment. But this would get messy when the number of containers we\nneed to manage grows.\nInstead, we’ll ship our containers into a container registry. A registry is a software sys‐\ntem that stores containers. A good registry keeps containers safe, and makes them\neasy to discover and to update and change. Docker even defines an API for registry\noperations and the Docker engine has built-in support for using it.\nShipping the Flight Information Container \n| \n241",
      "content_length": 1917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "There are plenty of registry hosting options available that support the Docker registry\nAPI. All the major cloud providers can host a secure, private registry for you. You can\nalso host your own registry server using Docker’s open source implementation. For\nthis book, we’ll use Docker’s publicly hosted registry called Docker Hub. We’ve\nchosen Docker Hub because it’s free to use, it’s popular, and it has good integration\noptions with GitHub Actions.\nKey Decision: Use Docker Hub as a Container Registry\nWe’ll be shipping our microservice container into a Docker Hub container registry.\nConfiguring Docker Hub\nSetting up a new Docker Hub registry is pretty easy. All you’ll need to do is log in to\nDocker Hub and create one. You can create a repository for our flight application\nexample by following these steps:\n1. Go to the Docker Hub home page in your browser.\n2. Log in to Docker Hub.\n3. Click the Create Repository button.\n4. Give the repository the name “flights.”\n5. Click the Create button.\nIn order to use Docker Hub, you’ll need to have a Docker account.\nIf you installed Docker when you set up your developer environ‐\nment, you’ll have a Docker ID already. If you don’t have an ID yet,\nvisit https://hub.docker.com and create one.\nIf you run into problems during this process, visit the Docker documentation site.\nWith a Docker account and a container repository, we’re ready to build and push con‐\ntainers into it with a CI/CD pipeline.\nConfiguring the Pipeline\nSo far, we’ve been using GitHub Actions as our pipeline tool for all of our IaC-based\nprovisioning work. It does the job well enough for our needs, so for the sake of con‐\nsistency we’ll use Actions again as the pipeline for our microservices container builds.\nAs an added bonus, we’ll be able to take advantage of actions that Docker has pub‐\nlished that will make our workflow easier to integrate with Docker Hub.\n242 \n| \nChapter 10: Releasing Microservices",
      "content_length": 1936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "In Chapter 9, we walked through the process of creating a flight information micro‐\nservice. If you followed along with those steps, you should have a GitHub repository\nthat contains the code and makefiles (see Figure 10-7). We’ll create our GitHub\nActions workflow inside the repository so that our CI pipeline can live right along‐\nside the code. If you don’t have your own flights service repository yet, you can create\na fork of this book’s example repository.\nFigure 10-7. Build a container in the flight service repository\nJust as we’ve done before, we’ll start our pipeline configuration by adding credentials\nto the GitHub repository.\nConfiguring Docker Hub secrets\nOur workflow will need to communicate with Docker Hub in order to publish a con‐\ntainer. So we’ll need to add our Docker Hub access information as secrets in the\nflights GitHub repository. Specifically, we’ll need to create and populate two secret\nkeys as defined in Table 10-3. These credentials are the same ones you would have\nused to log in to Docker Hub.\nTable 10-3. GitHub secrets for Docker Hub\nKey\nDescription\nDOCKER_USERNAME Your Docker account identity\nDOCKER_PASSWORD\nYour Docker account password\nWe’ve gone through the details of setting up secrets a few times already, but just as a\nreminder, you’ll need to do the following:\n1. Using your browser, navigate to the settings page of your forked ms-flights repos‐\nitory in GitHub.\n2. Select Secrets from the side navigation.\n3. Add the secret you want to define by clicking New Secret.\nShipping the Flight Information Container \n| \n243",
      "content_length": 1570,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "You may have noticed that we aren’t adding any AWS account secrets to this reposi‐\ntory. That’s because we won’t be deploying into an AWS instance in this pipeline. This\nworkflow will only focus on pushing containers into the Docker Hub registry—not\nthe deployment of the containers into our staging environment.\nThis is a useful separation to create because we want our microservice containers to\nbe portable and environment agnostic (this means we won’t add any environment-\nspecific logic or values into the build). Using the same built container in all our test\nand release environments should improve the reliability of our system overall.\nAll we need to do now is create the workflow that does the work of building, testing,\nand shipping the container.\nShipping the flight service container\nIf you’ve forked the ms-flights repository, you’ll find that we’ve already written a Git‐\nHub Actions workflow for you that builds and ships the container. All you need to do\nto is enable the workflow by navigating to the Actions tab in your forked repository,\nwhere you’ll be prompted to enable the workflow. If you’ve built your own ms-flights\nrepository, you can copy the workflow code into your own workflows directory.\nThe GitHub Actions workflow we’ve defined is triggered by a release tag, and has the\nfollowing steps:\n1. Runs unit tests on the code\n2. Builds a containerized version of the microservices\n3. Pushes the container to the Docker Hub registry\nWe’ve already added Docker Hub credentials to the repository, so the workflow is\nready to run. All we’ll need to do is push a tag called v1.0 into the release to trigger\nthe CI/CD workflow. We’ve done this a few times before using the command line and\na local copy of the repository. But to save time we’ll trigger this build using the Git‐\nHub browser-based UI.\nIn your browser, navigate to the Code tab of your forked ms-flights GitHub reposi‐\ntory. On the righthand side of the screen, click the “Create a new release” link in the\nReleases section, as shown in Figure 10-8.\n244 \n| \nChapter 10: Releasing Microservices",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "Figure 10-8. Create a new release\nNext, enter the value v1.0 in the tag version field, as shown in Figure 10-9. Then click\nthe Publish Release button at the bottom of the screen.\nFigure 10-9. Setting the tag version\nPublishing a GitHub release with a tag of v1.0 is the same as pushing the tag with the\nGit CLI. The end result should be that our GitHub Actions workflow will have kicked\noff. You can navigate to the Actions tab in your repository and verify that a workflow\nnamed CICD has started. It will take a few minutes to run the makefile and package up\nthe container. When it’s done, the flights service container will be pushed and ready\nto use in the Docker Hub registry.\nTo validate that the container has been shipped, access your Docker Hub account in\nthe browser and take a look at your repositories. You should see an entry for the\nflights container that was just pushed. It will look something like Figure 10-10.\nShipping the Flight Information Container \n| \n245",
      "content_length": 977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "Figure 10-10. ms-flights container pushed\nWe now have a containerized ms-flights microservice ready to be deployed into our\nstaging environment. With our microservices pushed into the container registry, we\ncan move on to the work of deploying the container into our staging environment.\nDeploying the Flights Service Container\nWe now have all the pieces in place to deploy the flights microservice. We’ve provi‐\nsioned a test environment using our infrastructure pipeline and we’ve created a\ndeployable containerized image for the service. To complete our deployment work,\nwe’ll use the Argo CD GitOps deployment tool we installed in our infrastructure\nstack in Chapter 7. When we’re finished with this section we’ll have a running version\nof the flight information microservice deployed and ready for use.\nTo make repeatable deployment easier, we’ll be creating a new deployment repository\nthat will contain Helm packages. We first introduced Helm in “Setting Up Argo CD”\non page 171, when we were building our first environment. The Helm packages we\nbuild will describe how a microservice should be deployed. When they’re ready and\npushed into the deployment repository, Argo CD will use them to deploy containers\ninto the staging environment (Figure 10-11).\nFigure 10-11. The Helm package will define the service deployment\n246 \n| \nChapter 10: Releasing Microservices",
      "content_length": 1371,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "All of this deployment work will happen within the world of Kubernetes, so let’s get a\nbasic understanding of what that means.\nUnderstanding Kubernetes Deployments\nIn Chapter 7, we introduced and installed Kubernetes to help with operating and run‐\nning container-based microservices. Kubernetes is popular because it handles a lot of\nthe work that needs to be done to start containers, check on their health, find serv‐\nices, replicate them, and start them again when they fail. This gives our system the\nresilience and self-healing qualities that will help us meet our guiding principles.\nBut a Kubernetes cluster still needs to be told what to do. It can’t deploy your micro‐\nservices without knowing where to find the container image. It can’t check on the\nhealth of a microservice without knowing which API to call. We also need to provide\nKubernetes with some limits for the number of container instances we want it to cre‐\nate and how those services should be accessed over the network.\nKubernetes sees the world as a set of declarative configuration objects. To configure a\nmicroservices deployment, you will need to describe the optimal state for a running\nversion of your container. Provided that you have described your running configura‐\ntion correctly, Kubernetes will do the work behind the scenes to bring your service to\nlife—and keep it that way.\nThis declarative approach is similar to the way we used Terraform to describe our\ninfrastructure resources. In Kubernetes, we’ll define a set of special deployment\nobjects using the YAML format. The truth is that Kubernetes is incredibly compli‐\ncated, so we won’t be able to go into a lot of detail in this book. But it’s helpful to\ncover a few of the core objects so that we can understand how our microservices will\nbe deployed.\nUnderstanding Kubernetes objects and controllers\nThere are many objects to learn about if you want to properly understand how to run\na Kubernetes platform. But for our purposes, we’ll just need a surface-level under‐\nstanding of five key objects in order to create a simple deployment package for our \nflights microservice: Pods, ReplicaSets, Deployments, Services, and Ingress.\nPod\nA Pod is an object that describes a basic workload unit. It defines one or more\nDocker containers that need to be started and managed together.\nReplicaSet\nReplicaSets let Kubernetes know how many instances of a specific Pod it should\nstart up and run at the same time. You usually won’t need to work with Replica\nSets directly.\nDeploying the Flights Service Container \n| \n247",
      "content_length": 2554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "Deployment\nThe Deployment controller declares a desired state for a Pod and associated Repli\ncaSets. This is the main object you need to work with to create a Kubernetes\nDeployment.\nService\nA Service defines how applications in the Kubernetes cluster can access this Pod\nover the network—even when there are multiple replicas running at the same\ntime. The Service object lets you define a single IP and port for accessing a\ngroup of replicated Pods. You’ll almost always want to define a service for a\nmicroservice deployment.\nIngress\nThe Ingress object allows you to identify an ingress route to your Service for\napplications outside the cluster. The Ingress declaration can include routing\nrules so that an ingress controller can route messages to the right Services.\nIn order to deploy our microservice, we’ll need to write declarative configurations for\nthe Ingress, Service, and Deployment objects. Although we won’t be writing config‐\nurations for Pods and ReplicaSets as files in their own right, we’ll be including their\ndetails in the Deployment object configuration. As we mentioned earlier, we’ll be\nusing Helm to package all of these files up.\nCreating a Helm Chart\nA Kubernetes Deployment can require a lot of communication with the cluster. You\nneed to make multiple calls to the Kubernetes API, letting it know how, when, and\nwhere you want to deploy your containers. To help manage some of that complexity,\nwe’ll use the Helm packaging tool.\nHelm is a package manager for Kubernetes. It gives us an easier way to manage the\ninstallation and deployment of application into a Kubernetes cluster. We used Helm\nearlier in the book to install off-the-shelf packages like Argo CD. Now we’ll write our\nown Helm package so we can install our microservices just as easily.\nTo use Helm, we’ll first need to understand the three important concepts of charts,\ntemplates, and values:\nCharts\nA chart is a bundle of files that describe a Kubernetes resource or deployment.\nThe chart is the core unit of deployment in Helm. We used pre-made charts ear‐\nlier in the book when we deployed Kubernetes-based applications like Argo CD.\n248 \n| \nChapter 10: Releasing Microservices",
      "content_length": 2174,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "Templates\nTemplates are files in a chart that describe a specific Kubernetes resource. They’re\ncalled templates because they contain special instructions that Helm uses to\nreplace values in the file. For example, you can create a Service template for a\nmicroservice and make the port number of the Service a templated value.\nValues\nEvery chart has a values file that defines the values that should be used to popu‐\nlate a template. Value files are a useful way of managing the differences between\nenvironments. Values can also be overridden when the Helm chart is installed.\nTo create a flights Helm package, we’ll need to create a Helm chart. Within that chart,\nwe’ll define a set of template files that declare how the flights service should be\ndeployed. Our template will have some parameterized values that will make it usable\nfor different types of environments. Finally we’ll create a values file for the staging\nenvironment that populates our templates.\nAs we described in “Deploying the Flights Service Container” on page 246, we want\nour Helm charts to be available for Argo CD to retrieve and use. So the first thing\nwe’ll need to do is create a microservices deployment repository to store and manage\nthem.\nCreating the Microservices Deployment Repository\nWe’ll be keeping our Helm charts in a single “monorepo” of microservice deploy‐\nments. This fits well with our operating model and allows the release team to manage\nthe actual release of services in a holistic fashion. The microservices teams can still\nown their own Helm deployment charts and deploy into the deployment repository\nindependently (see Figure 10-12).\nFigure 10-12. Create a deployment package in the deployment repository\nTo get started, create a new GitHub repository called ms-deploy. Once it’s ready, cre‐\nate a local clone of the repository in your development environment. We’ve done this\na few times already, so we won’t go through all the details again. If you need help\nDeploying the Flights Service Container \n| \n249",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "remembering the process for repository creation, the GitHub Quickstart documenta‐\ntion is a good place to start.\nThe deployment repository you’re creating now will become the\n“source of truth” for the Argo CD GitOps deployment tool that\nwe’ll set up later in the chapter.\nYou should now have an empty Git repository ready to be populated with Helm\npackages.\nThe easiest way to start working with Helm files is to use the Helm CLI application.\nHelm’s CLI allows you to create, install, and inspect Helm charts by using the Kuber‐\nnetes API. In our examples, we’ll be using Helm version 3.2.4, which you can find at\nthis GitHub site.\nIf you don’t have the Helm CLI already, download and install it on your local machine\nnow. When you’ve done that, you’ll be ready to create the ms-flights Helm chart.\nCreate a Helm chart\nOne of the nice things about the Helm CLI is that it provides a handy function for\nquickly bootstrapping a new chart. To create our skeleton chart, make sure you are in\nthe root directory of your ms-deploy repository and run the following command:\nms-deploy $ helm create ms-flights\nWhen it’s done, Helm will have created a basic package that contains a chart.yaml file\nwhich describes our chart, a values.yaml file we can use to customize chart values,\nand a templates directory that contains a whole set of Kubernetes YAML templates for\na basic deployment.\nThe great thing about using Helm is that most of the boilerplate code that we’d need\nto write for a basic microservices Kubernetes deployment has been handled for us\nalready. We’ll only need to make a few small changes to the templates that Helm has\ngenerated for us to have a working, deployable package.\nIn particular, we’ll need to update the templates/deployment.yaml file just a little bit to\nmake it more specific to the container that we want to deploy.\nUpdate the flights deployment template\nThe /ms-flights/templates/deployment.yaml file is a Kubernetes object description file\nthat declares the target deployment state for a Pod. We’ve already mentioned that\nKubernetes objects can get pretty complicated. The good news is that the file that\nHelm generated for us includes a lot of placeholder values that we can leave as is. But\n250 \n| \nChapter 10: Releasing Microservices",
      "content_length": 2261,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "we’ll need to make a few small changes for this deployment to work for our flights\nmicroservice.\nLet’s start by getting a basic understanding of some key YAML properties in the\ndeployment object:\napiVersion\nEvery Kubernetes YAML file specifies the version of a named Kubernetes API\nthat this file uses.\nkind\nThis identifies the type of Kubernetes object. In this case, the Kubernetes object is\nDeployment.\nspec\nThe specification for the Kubernetes object—this is the heart of the description.\nspec.replicas\nSpecifies the number of replicas we want for this deployment. Kubernetes will\ncreate ReplicaSets for us based on this value.\nspec.template\nThe template property of the Deployment specification is the template for the\nPod that we are planning to deploy. Kubernetes uses this template to provision\nthe pods we are deploying.\nspec.template.containers\nThe containers property of a Pod template identifies the container image and\nenvironment values that Kubernetes should use when it creates a replica of a Pod.\nFor our simple deployment, we’re going to use the default values that Helm has gener‐\nated for most of the Deployment object’s properties. But we’ll need to update\nspec.template.containers so that it works for the ms-flights container that we’ve\nbuilt.\nUpdate the YAML for the containers property so that it contains the env, ports,\nlivenessProbe, and readinessProbe values shown in Example 10-3.\nExample 10-3. ms-flights template specification\nspec:\n[...]\n  template:\n  [...]\n    spec:\n    [...]\n      containers:\nDeploying the Flights Service Container \n| \n251",
      "content_length": 1576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "- name: {{ .Chart.Name }}\n          [...]\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          env:\n            - name: MYSQL_HOST\n              value: {{ .Values.MYSQL_HOST | quote }}\n            - name: MYSQL_USER\n              value: {{ .Values.MYSQL_USER | quote }}\n            - name : MYSQL_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: {{ .Values.MYSQLSecretName }}\n                  key: {{ .Values.MYSQLSecretKey }}\n            - name: MYSQL_DATABASE\n              value: {{ .Values.MYSQL_DATABASE | quote }}\n          ports:\n            - name: http\n              containerPort: 5501\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /ping\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: http\n          [...]\nA completed example of the ms-flights Helm chart is available at\nthis book’s GitHub site.\nThe update we’ve made to the containers section includes the following:\n• A templated set of environment variables for connecting to a MySQL database\n(we’ll set the actual values later)\n• The TCP port that the flights microservice will bind to and our container exposes\n• Liveness and readiness endpoints that Kubernetes will use to check if the Pod is\nstill alive (as defined in Chapter 9)\nThat’s all we need to customize to make the generated Helm templates work for us.\nWith the deployment template we’ve created, we have a parameterized Kubernetes\nDeployment object defined. We’ll only need to define some values to use in the\ntemplate.\n252 \n| \nChapter 10: Releasing Microservices",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "Set package values\nOne of the nice things about using a Helm package for deployment is that we can\nreuse the same template for lots of different environments by changing a few values.\nOne way to set those values is through the Helm client at the time of installation. We\ndid this earlier in the book when we installed the Helm package for Argo CD.\nAnother option is to create a file that serializes all of the values you want to use in a\nsingle place. This is the approach we’ll take for our deployment package. This gives us\nthe advantage of being able to manage our deployment value files as code. We’ll use\nthe values.yaml file that Helm has generated for us already. You’ll find that file in the\nroot directory of the ms-flights chart.\nFirst, we’ll need to update the details for the Docker image. Open the values.yaml file\nin your favorite text editor and find the image key at the beginning of the YAML file.\nUpdate image with the details in Example 10-4.\nExample 10-4. Image example\nreplicaCount: 1\nimage:\n  repository: \"msupandrunning/flights\"\n  pullPolicy: IfNotPresent\n  tag: \"v1.0\"\nThis example uses the container we’ve already built for you. If you\nwant to use your own, you’ll need to change the values of reposi\ntory and tag.\nNext, we’ll add MySQL connection values so the microservice can connect to the\nstaging environment’s database services. Add the following YAML to your values file\n(you can add it immediately after the tag property):\nimage:\n[..]\nMYSQL_HOST: rds.staging.msur-vpc.com\nMYSQL_USER: microservices\nMYSQL_DATABASE: microservices_db\nMYSQLSecretName: mysql\nMYSQLSecretKey: password\nFinally, find the ingress property near the end of the YAML file and update it with\nthe following text:\nDeploying the Flights Service Container \n| \n253",
      "content_length": 1763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "ingress:\n  enabled: true\n  annotations:\n    kubernetes.io/ingress.class: traefik\n  hosts:\n    - host: flightsvc.com\n      paths: [\"/flights\"]\nThis definition lets our Ingress service know that it should route any messages sent to\nthe host flightsvc.com with a URI of /flights to the flight information microservice. We\nwon’t need to actually host the service at the flightsvc.com domain, we’ll just need to\nmake sure that HTTP requests have those values if we want them to reach our\nservice.\nFor a production environment, we’d probably have more values and template changes\nwe’d want to make. But to get up and running, this is more than enough.\nTest and commit the package\nThe last thing we’ll need to do is a quick dry-run test to ensure that we haven’t made\nany syntax errors. You’ll need to have connectivity to your Kubernetes cluster, so\nmake sure you still have that environment accessible. Run the following command to\nmake sure that Helm will be able to build a package:\nms-flights$ helm install --debug --dry-run flight-info .\nIf it works, Helm will return a lot of YAML that shows the objects that it would gener‐\nate. It should end with something that looks like this:\n[... lots of YAML...]\n  backend:\n              serviceName: flight-info-ms-flights\n              servicePort: 80\nNOTES:\n1. Get the application URL by running these commands:\n  http://flightsvc.com/flights\nIf you’re having trouble getting your Helm package to work, check\nout a reference example for the flights service package at this book’s\nexample repository.\nIf everything looks good, commit the finished Helm files to the GitHub repository:\nms-flights$ git add .\nms-flights$ git commit -m \"initial commit\"\nms-flights$ git push origin\nNow that the package files are available in the deployment monorepo, we’re ready to\nuse them with the Argo CD GitOps deployment tool.\n254 \n| \nChapter 10: Releasing Microservices",
      "content_length": 1896,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "Argo CD for GitOps Deployment\nSo far, we’ve created a Helm chart that gives us a more consumable way of deploying\nmicroservices into the Kubernetes cluster. Helm comes with the capability of per‐\nforming deployments into Kubernetes clusters, so we’ve already done enough to be\nable to deploy the flight information service into the staging environment.\nBut with what we have now, this would be a very manual operation and we’d need to\nuse the Helm CLI for every deployment. We’d also need to somehow keep track of the\ncurrent state and version of deployed services so that we’d know if a new deployment\nis necessary when our deployment repository is updated.\nInstead, we can do something better. Earlier in Chapter 7, we introduced Argo CD as\nour continuous deployment tool. Now is our opportunity to use it and improve the\nway we deploy services into our environments.\nArgo CD is a GitOps deployment tool, designed to use a Git repository as the source\nfor the desired deployment state for our workloads and services. When it checks a\nrepository that we’ve specified, it determines whether the target state we defined\nmatches the running state in the environment. If it doesn’t, Argo CD can “synchron‐\nize” the deployment to match what we declared in our Helm charts.\nThis declarative approach fits well with our principles and the other tools that we’ve\nadopted, like Terraform. To make all this magic happen, we just need to log in to the\nArgo CD instance that we’ve installed in staging, point to our ms-deploy repository,\nand set up a synchronized deployment.\nMake sure you’ve added the MySQL password Kubernetes Secret as\ndescribed in “Create a Kubernetes secret” on page 240. Otherwise,\nthe flight information service won’t be able to start up.\nLog in to Argo CD\nBefore we can log in to Argo CD, we’ll need to get the password for the Argo admin‐\nistrative user. Argo CD does something clever and makes the default password the\nsame as the name of the Kubernetes object that it runs on. Run the following kubectl\ncommand to find the Argo CD Pod:\n$ kubectl get pods -n \"argocd\" | grep argocd-server\nNAME                                READY     STATUS    RESTARTS   AGE\nmsur-argocd-server-c6d4ffcf-9z4c2   1/1       Running   0          51s\nCopy the name of the Pod somewhere as that will be the password we’ll use to log in.\nFor example, in the result shown, the password would be msur-argocd-server-\nc6d4ffcf-9z4c2. In order to access the login screen and use our credentials, we’ll\nDeploying the Flights Service Container \n| \n255",
      "content_length": 2538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "need to set up a port-forwarding rule. That’s because we haven’t properly defined a\nway to access our Kubernetes cluster from the internet. But thankfully kubectl pro‐\nvides a handy built-in tool for forwarding requests from your local machine into the\ncluster. Use the following to get it running:\n$ kubectl port-forward svc/msur-argocd-server 8443:443 -n \"argocd\"\nForwarding from 127.0.0.1:8443 -> 8080\nForwarding from [::1]:8443 -> 8080\nNow you should be able to navigate to localhost:8443 in your browser. You’ll almost\ndefinitely get a warning indicating that the site can’t be trusted. That’s OK and is\nexpected at this point. Let your browser know that it is OK to proceed and you should\nthen see a login screen that looks like the one shown in Figure 10-13.\nFigure 10-13. Argo CD login screen\nEnter admin as your user ID and use the password you noted earlier and log in. If you\ncan log in successfully, you’ll see a dashboard screen. Now we can move on to creat‐\ning a reference to our flight information service deployment.\n256 \n| \nChapter 10: Releasing Microservices",
      "content_length": 1077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "Sync and deploy a microservice\nIn Argo CD, a microservice or workload that needs to be deployed is called an appli‐\ncation. To deploy the flight-information microservice we’ll need to create a new\n“application” and configure it with values that reference the Helm package in the Git\nrepository that we created earlier.\nStart by clicking the Create Application or New App button on the dashboard screen.\nWhen you click it, a web form will slide in from the righthand side of the screen that\nyou’ll need to populate. This is where you define the metadata for the application and\nthe location of the Helm package. In our case, we’ll want Argo CD to pull that from\nthe deployments monorepo and the ms-flights directory within it.\nUse the values in Table 10-4 to set up your flight-information microservice deploy‐\nment. Make sure you replace the value YOUR_DEPLOYMENTS_REPOSITORY_URL with the\nURL of the deployment repository from “Creating the Microservices Deployment\nRepository” on page 249 so that Argo CD can access your Helm packages.\nTable 10-4. Flight-information service values\nSection\nKey\nValue\nGENERAL\nApplication name\nflight-info\nGENERAL\nProject\ndefault\nGENERAL\nSync policy\nmanual\nSOURCE\nRepository URL\nYOUR_DEPLOYMENTS_REPOSITORY_URL\nSOURCE\nPath\nms-flights\nDESTINATION\nCluster\nin-cluster (https://kubernetes.default.svc)\nDESTINATION Namespace\nmicroservices\nWhen you are done filling in the form, click the Create button.\nIf you run into any trouble, consult the Argo CD documentation\nfor instructions on setting up an application.\nIf you’ve created the application successfully, Argo CD will list the flight-info applica‐\ntion in the dashboard, as shown in Figure 10-14.\nDeploying the Flights Service Container \n| \n257",
      "content_length": 1727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "Figure 10-14. Flight-information application created\nHowever, while the application has been created, it’s not yet synchronized with the\nDeployment declaration, and the flight-info application in our cluster doesn’t match\nthe description in our package. That’s because Argo CD hasn’t actually done the\nDeployment yet. To make that happen, click the flight-info application that we’ve just\ncreated, click the Sync button, and then click the Synchronize button in the window\nthat slides in, as shown in Figure 10-15.\nFigure 10-15. Synchronize the flight-information application\nWhen you click Synchronize, Argo CD will do the work it needs to do to make your\napplication match the state you’ve described in the Helm package. If everything goes\nsmoothly, you’ll have a healthy, synchronized, and deployed microservice, as shown\nin Figure 10-16.\n258 \n| \nChapter 10: Releasing Microservices",
      "content_length": 885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "Figure 10-16. Deployed flight service\nIf your deployment status isn’t “healthy,” try clicking the pod (the\nlast node on the far right of the tree). You’ll be able to view events\nand log messages that can help you troubleshoot the problem.\nOur container has been deployed in the Kubernetes cluster, its health checks and liv‐\neness checks have passed, and it is ready to receive requests. This is a big milestone!\nNow, let’s try testing the flights service with a simple request.\nTest the flights service\nOur flights microservice is now up and running in our AWS-hosted staging environ‐\nment. In order to test the service with a request message, we’ll need to access Traefik’s\nload balancer, which will route our request to the containerized service. The first\nthing we’ll need is the load balancer’s network address. Since we didn’t set up a DNS\nentry, AWS will have given us a random address automatically. To get that address,\nrun the following kubectl command:\n$ kubectl get svc ms-traefik-ingress\nYou should get back something that looks like this:\nNAME                 TYPE           CLUSTER-IP       EXTERNAL-IP\nms-traefik-ingress   LoadBalancer   172.20.149.191   ab.elb.amazonaws.com\nThe EXTERNAL-IP is the address of the Traefik load balancer. Make a note of it for our\ntest request.\nWe’ll be using curl to send a request message to the flights microservice. If you don’t\nhave a local copy of curl, you can get it from this site. If you’ve never used it before,\ncurl is a powerful command-line tool for sending messages to URL-based addresses.\nWe’re using curl because it has a lot of useful options, including the ability to set a\nhost header in the HTTP request. That’s helpful for us because we need to set a host\nof flightsvc.com for our ingress routing rule to work.\nDeploying the Flights Service Container \n| \n259",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "Run the following curl command to send a test request message to the flights service\n(replacing {TRAEFIK-EXTERNAL-IP} with the address for your load balancer):\ncurl --header \"Host: flightsvc.com\" \\\n {TRAEFIK-EXTERNAL-IP}/flights?flight_no=AA2532\nIf all has gone well, you’ll get the details of that flight as a JSON-formatted response.\nYou can use a dedicated API testing tool such as Postman or\nSoapUI to get a more user-friendly formatted version of the\nresponse message.\nThe HTTP request we’ve just made calls the ingress service, which in turn routes the\nmessage to the flights microservice based on the ingress rule we defined earlier in this\nchapter. The flights microservice retrieves data from the database service we provi‐\nsioned and returns a result to us through the load balancer. With that request, we’ve\nbeen able to bring together all the parts of our architecture deployment and test an\nend-to-end microservices architecture!\nAll that’s left is to clean up, so we don’t end up paying for AWS resources that we\naren’t using.\nAWS bills you for EKS resources even when they aren’t handling\ntraffic, so make sure you tear down your infrastructure if you aren’t\nusing it.\nClean Up\nAs we’ve done before, we’ll use a local Terraform client to bring down the infrastruc‐\nture. Make sure you’re in the directory where your staging Terraform files are and run\nthe following command:\ninfra-staging-env $ terraform destroy\nWhen it’s successfully completed, our Kubernetes-based staging environment will be\ndestroyed. You can check to make sure that the resources have been destroyed by\nusing the AWS CLI or the AWS browser-based console. We gave you some examples\nof CLI commands you can run in Chapter 6.\n260 \n| \nChapter 10: Releasing Microservices",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "Summary\nAt the beginning of this chapter we warned you that we’d be doing a lot of work. All\nthat work paid off as we ended up with the end-to-end deployment of the microservi‐\nces architecture that we’ve been building throughout this book. We also got to reuse\nsome of the tools and practices we established earlier to get more done in less time.\nIn this chapter we updated our infrastructure template to support the dependencies\nfrom our microservices teams. We implemented a build and integration pipeline in\nour microservice code repositories, and we built a new deployment repository and\ntool-based process to get services deployed.\nHopefully, you’ve been able to see how the decisions we made at deployment time\nhave been heavily influenced by the earlier decisions we made on principles, operat‐\ning models, infrastructure, and design. They’ve all come together to form an end state\nthat allowed us to build a running implementation.\nBut the real test of a microservices system is how it handles change. That’s what we’ll\ncover in our next chapter.\nSummary \n| \n261",
      "content_length": 1071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "CHAPTER 11\nManaging Change\nWe have now built a microservices system that is optimized to reduce change costs.\nWe’ve done it quite quickly and with a great variety of tools, technologies, and reposi‐\ntories. In this chapter, we’ll take a step back and consider the system we’ve built from\nthe perspective of change. We’ll explore what change looks like for the system we’ve\nbuilt. We’ll take a look at the typical kinds of change you’ll need to do and the pat‐\nterns and methods that work well to support them.\nChange is an important factor because of the impact it has. Poorly designed software\ncan end up costing organizations a lot of pain. As we highlighted in Chapter 1, one of\nthe benefits of a microservices system is that it makes change faster and safer.\nAlso, change will always have a cost. In a software system, that cost is a combination\nof time, money, and impact to people. To get the most out of our microservices sys‐\ntem, we need to minimize change cost and make changes that have the greatest\nimpact. Reducing the cost of change gives all of our teams more freedom to experi‐\nment, optimize, and improve. Focusing change activities gives us better results from a\nfinite change budget.\nLet’s start by getting a better understanding of the kinds of changes we can expect in a\nmicroservices system and the best way to make decisions about change.\nChanges in a Microservices System\nIn a microservices system, change should be a feature, not a problem or a bug. That\nmeans you should be able to change the system to make it better and get more value\nfrom the software you’ve built. When people think about software change, they often\nthink about extrinsic drivers—the things that come from business or user input. For\nexample, here are some common reasons to make changes in your system:\n263",
      "content_length": 1804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "• Supporting a new product launch\n• Resolving a logic bug that is degrading the user experience\n• Integrating with a new partner\nThese are all important reasons to change and our architecture should facilitate these\nkinds of changes to make them as cost-effective as possible. But it’s important to\nunderstand that the microservices style is an optimization technique. That means we\nshould consider intrinsic drivers as well. The following changes come from our\nobservation of the system itself:\n• Splitting a microservice to reduce code complexity\n• Redeploying infrastructure to avoid drifting from the infrastructure code\n• Optimizing the CI/CD pipeline to deliver changes faster\nThere’s no doubt that you’ll need to support extrinsic change. But to get the best value\nfrom your system you’ll need to plan for and execute intrinsic change as well. A good\nway to adopt this continual improvement mindset is to use data and measurements to\nguide your decisions.\nBe Data-Oriented\nA classic problem in software development is overengineering and premature optimi‐\nzation. This happens when we design software or architecture to resolve a problem\nthat hardly ever occurs. Or when our solution to a predicted problem is more costly\nthan the problem itself will ever be.\nThis can be a danger for a microservices system as well. That’s why it’s a good idea to\nuse data and measurements to guide your decision making about when to make\nchanges—especially the intrinsic improvement ones. Without data, you’ll be guessing\nand you’ll probably end up working hard to improve parts of the system that actually\ndon’t need any help. Meanwhile, other pressing problem areas may go undetected.\nWith finite resources, you can’t afford to work that way.\nProduct teams use data to make better informed decisions about the changes they\nwant to make. Businesses use objective and key results (OKRs), key performance\nindicators (KPIs), net promoter scores, satisfaction surveys, and revenue numbers to\nhelp shape their strategic decision making and their backlog of changes.\nYou’ll need something similar to inform your improvement and optimization plans.\nFor example, consider collecting the following project, design, and runtime metrics to\nget a better understanding of your improvement opportunities:\n264 \n| \nChapter 11: Managing Change",
      "content_length": 2319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "• Change time per microservice\n• Frequency of changes per microservice\n• Number of microservices changed per change request\n• Lines of code in a microservice (as a datapoint, not a constraint!)\n• Runtime latency per microservice\n• Dependencies between microservices\nWe didn’t implement observability or reporting in our microservi‐\nces architecture. That’s because we had limited space and wanted to\nfocus on some of the more foundational elements. But the good\nnews is that all the hooks are there for you to extend the system to\ngive you some of the metrics we’ve been describing.\nCollectively, these kinds of analytic metrics can give you a better, holistic picture of\nwhere improvements can be made. Then you can make a decision about where to\nbest spend your efforts. Of course, you’ll also need to balance the improvements you\ncan make against the impact that a change will have.\nThe Impact of Changes\nThere are many potential impacts that come from software change, but four in partic‐\nular seem to cause the most strife for modern organizations: implementation time,\ncoordination time, downtime, and consumer impact. When we review change costs\nin a microservices system, it’s a good idea to consider these focus areas. Let’s take a\nquick look at each of them:\nImplementation time\nA core part of any change cost is the time it takes to actually make the change.\nThis includes the time required to understand the current state, make the desired\nchanges, test changes, and update the production environment. A big factor for\nimplementation time is the readability, learnability, and maintainability of the\ncomponents to be changed.\nCoordination time\nIn order to implement a change, there will almost always be some form of com‐\nmunication between teams. Coordination time is a subset of implementation\ntime, but it’s worth calling out on its own. In fact, it’s so important, we’ve men‐\ntioned it a few times in this book. Coordination time can include the amount of\ntime spent getting access to resources and gaining permission and agreement on\nchange activities and the general “organizational friction” that comes from\nChanges in a Microservices System \n| \n265",
      "content_length": 2168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "working in a large organization. Coordination time is often a factor of organiza‐\ntional design and structure.\nDowntime\nDowntime is a measurement of how long the system or a system component\nremains unavailable while a change is being implemented. Years ago, downtime\nwas an accepted part of the software change process. But times and expectations\nhave changed. Now there is increasing pressure on technology teams to minimize\nthe downtime required for changes. In fact, in a microservices system it’s com‐\nmon to strive for a “zero-downtime” change model in which the system remains\nconstantly available.\nConsumer impact\nAn often forgotten impact is the cost that a change has on the users of the system.\nDowntime captures one form of consumer impact, but even in a “zero-\ndowntime” model there can be costly impacts that could have been avoided. For\nexample, a change to an infrastructure module may have wide-reaching impact\non microservice developer teams. Similarly, a change to an interface can break\nthe code of every component that uses it.\nSoftware architecture has a big role to play in the costs and impacts of change across\nall four of the lenses we’ve described. But another part of the story is the way that\nchanges are applied. Microservices architectures, cloud infrastructures, and DevOps\npractices have enabled practices that are a huge leap forward. Let’s take a look at two\nmodern deployment patterns as well as an older one that has managed to stick\naround.\nThree Deployment Patterns\nThere are lots of different ways to apply changes and deploy software components.\nBefore we dive into the changeability of the architecture we’ve built, it’s worth review‐\ning three deployment patterns that we’ll use when we make changes in our system:\nblue-green, canary, and multiple versions. We’ll start by looking at blue-green\ndeployments.\nBlue-green deployment\nIn a blue-green deployment, there are two parallel environments maintained. One is\nlive and accepts traffic while the other is idle. Change is applied to the idle environ‐\nment and when ready, traffic is routed to the changed environment. The two environ‐\nments now switch roles with idle becoming live and live becoming idle, ready for the\nnext change.\nThis is a useful deployment pattern because it allows you to make changes in a pro‐\nduction environment safely. Switching the traffic over means that you don’t have to\n266 \n| \nChapter 11: Managing Change",
      "content_length": 2430,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "worry about repeating the change in a live system. The actual colors of the environ‐\nments are unimportant—the key to this pattern is that the two environments inter‐\nchange roles between live and idle.\nA benefit of this pattern is that it can vastly reduce downtime, all the way down to a\nzero-downtime model. However, maintaining two environments requires careful\nhandling of persistent systems like databases. Persistent, changing data needs to be\nsynchronized, replicated, or maintained entirely outside of the blue-green model.\nCanary deployment\nA canary deployment is similar to a blue-green deployment, but instead of maintain‐\ning two complete environments, you release two components in parallel. The “can‐\nary” in this pattern is the version that acts as a “canary in a coal mine”, alerting you to\ndanger early. For example, to perform a canary deployment of a web application,\nyou’d release a new canary version of the web application alongside the original web\napplication that continues to run.\nJust like the blue-green pattern, canary deployments require traffic management and\nrouting logic in order to work. After deploying the new version of an application,\nsome traffic is routed to the new version. The traffic that hits the canary version\ncould be a percent of the total load or could be based on a unique header or special\nidentifier. However it’s done, over time more traffic is routed to the canary version\nuntil it eventually gets promoted to full-fledged production state.\nAlthough the canary pattern is similar to blue-green, it has the added advantage of\nbeing finer-grained. Instead of maintaining an entire duplicate environment, we can\nfocus on a smaller, bounded change and make that change within a running system.\nThis can cause problems if the canary we are deploying impacts other parts of our\nsystem. For example, if our canary deployment alters a shared system resource in a\nnew way, even handling 1% of traffic in the canary could have catastrophic effects.\nBut in a system that’s designed for independent deployment, the canary pattern can\nwork quite well. When changes are made to components that are well bounded and\nown their own resources, the blast radius of damage is limited. So it’s a good pattern\nto have in your tool belt if you are working with the right type of architecture. As\nwe’ll see, the canary pattern turns out to be a good fit for the microservices architec‐\nture we’ve built in this book.\nMultiple versions\nThe last pattern to cover is one that considers users and clients as part of the change\nprocess—running multiple versions in parallel. The blue-green and canary deploy‐\nment patterns we’ve covered already use a mechanism of temporarily running parallel\ninstances (sometimes called the expand and contract pattern). But in both of those\ncases, you’d typically run your new and old instances privately, not sharing details of\nChanges in a Microservices System \n| \n267",
      "content_length": 2933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "the new function until it’s safe to use. The routing decision is implicit and hidden\nfrom users of the system.\nThe multiple versions pattern makes changes more transparent to the users and cli‐\nents of the system. In this deployment pattern we explictly version a component or\ninterface and allow clients to choose which version of the component they want to\nuse. In this way, we can support the use of multiple versions at the same time.\nThe main reason to employ this technique is if we’re making a change that will\nrequire a dependent system to make a change as well. We use this pattern when we\nknow people we don’t coordinate with will need to do work for the change to be com‐\npleted. A classic example of this situation is when you want to change an API in a way\nthat will break client code. In this scenario, managing migration for all parties would\nrequire significant coordination effort. Instead, we can keep older versions running\nso that we don’t need to wait for every client to change.\nThere are some significant challenges to using this approach. Every version of a com‐\nponent we introduce brings added maintenance and complexity costs for our system.\nVersions need to be able to run safely together and parallel versions need to be con‐\ntinually maintained, supported, documented, and kept secure. That overhead can\nbecome an operational headache and can slow down changeability of the system over\ntime. Eventually, you’ll need to migrate users of old versions and do some contraction\nof versions.\nThere are some systems that almost never contract their versions.\nFor example, at the time of this writing, the Salesforce SaaS API is\non version 49 and supporting 19 previous versions in parallel!\nWe now have a decent framework for assessing the impact of change and a set of typi‐\ncal deployment patterns we can use to describe how change might be handled. Now\nwe can dive into an evaluation of the architecture we’ve built from a change perspec‐\ntive across infrastructure, microservices, and data. We’ll start by examining the\nchangeability of our infrastructure platform.\nConsiderations for Our Architecture\nIf you followed along with the instructions in this book, you’ll have built a pretty\nadvanced microservices architecture in a fairly short amount of time. That speed of\nengineering is a testament to the incredible tools, services, and software that are avail‐\nable for us to use. But building something fast is no good if it doesn’t do the job it’s\nsupposed to do. For us, this means the architecture we built should hold its own\nwhen it comes to changeability.\n268 \n| \nChapter 11: Managing Change",
      "content_length": 2626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "In this section, we’ll take a tour of the system and get a closer look at how the deci‐\nsions we’ve made have impacted the changeability of the architecture. We’ll look at\nchange through the factors of implementation costs, coordination time, downtime,\nand consumer impacts that we introduced earlier in this chapter. To make things eas‐\nier, we’ll split the architecture into three subsystems: infrastructure, microservices,\nand data, considering each in turn. Let’s start by examining our infrastructure.\nInfrastructure Changes\nIn Chapter 7, we developed a Terraform-based platform for our microservices that\nincluded networking, Kubernetes, and a GitOps deployment tool. Later, we added\nMySQL and Redis databases to the stack based on the emerging needs of the micro‐\nservices teams. It’s realistic to expect the infrastructure platform to continue to\nchange as the needs of users and teams evolve, and demand patterns and business\ngoals change.\nFor our infrastructure, we can divide change into two categories: changes that extend\nthe platform with new resources and changes that alter existing platform resources.\nCreating new resources is a form of extension that has little impact on the running\nsystem, while changes to existing resources need to be managed more carefully.\nExamples of adding new resources to our architecture would be:\n• Implementing a new event-streaming infrastructure using AWS SNS for new\nmicroservices that are being developed\n• Provisioning an Elastic Container Service (ECS) instance and VPC for the instal‐\nlation of a third-party application\n• Adding a new operator account to the IAM system\nHere are some examples of changes that would alter an existing resource:\n• Changing the network design of the VPC our EKS service is deployed within\n• Upgrading the MySQL version of our RDS instance\n• Modifying the configuration of the Kubernetes cluster\nWe’ll need to consider both types of changes when we assess the changeability of our\ninfrastructure. Let’s start by looking at implementation costs.\nConsiderations for Our Architecture \n| \n269",
      "content_length": 2074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "Infrastructure change: Implementation costs\nThe implementation cost of making infrastructure changes is a function of how diffi‐\ncult a change is to both understand and execute. This is where the investment we\nmade in our infrastructure design helps. Our decisions to embrace the principle of\nimmutable infrastructure, build a CI/CD pipeline, and write IaC combine to greatly\nreduce the cost of making changes.\nWhen it comes time for you to make an infrastructure change, you can employ a\nchange process that looks something like this, thanks to the tools we’ve implemented:\n1. Decide on the infrastructure change you want to make.\n2. Identify the infrastructure code you need to change (e.g., do you need to create a\nnew Terraform module? Are you just updating an environment definition?).\n3. Test the infrastructure change in an infrastructure development environment.\n4. Try to deploy applications and microservices to the updated infrastructure.\n5. Run tests and release (e.g., integration tests, performance tests, and end-to-end\ntests).\nBy adopting the principle of IaC, the changeability of our infrastructure design has\nimproved significantly. We only ever make changes through the infrastructure pipe‐\nline, so we know that if it’s not in the code, it’s not in the infrastructure.\nWe use the same code modules in every environment, so we know that if your infra‐\nstructure changes work in a development environment, they should work in the pro‐\nduction environment as well. Finally, our automated pipeline ensures that our\ninfrastructure code and tests will be run in the same way consistently and repeatedly.\nWhat we’ve done in our system is to drive variation out of the change process. With\nless uncertainty for us to worry about, we can focus more on making the change\nitself. Writing IaC requires a bit more up-front effort, but the payoff when it comes to\nchanges makes it a worthwhile investment.\nOverall, the infrastructure implementation costs should be lower with our architec‐\nture than they would be if we had just made changes directly using the AWS console.\nInfrastructure change: Coordination costs\nWhen we developed our operating model in Chapter 2, we made an important deci‐\nsion about how infrastructure work would be done. We decided that a single team\ncalled the platform team would be responsible for designing, maintaining, and run‐\nning our cloud-based infrastructure. Centralizing the infrastructure design within\nthis team means that we’ll pay a relatively low coordination cost for decision making.\nThat’s because we won’t need to gain consensus among all of the parties in your sys‐\ntem whenever an infrastructure change is needed. Instead the platform team has\n270 \n| \nChapter 11: Managing Change",
      "content_length": 2734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "independent authority and autonomy (and responsibility!) over infrastructure\nchanges.\nIn practice, it’s difficult to offer an infrastructure platform in a pure x-as-a-service\nmanner. Enablement, engagement, and agreement are bound to be needed for micro‐\nservices teams to use the platform for delivery. The centralized nature of the platform\nteam is also a potential problem. What happens when teams require conflicting\nchanges? How are new changes tested across all the teams?\nThe platform model only works if there are tools and processes to properly enable a\nself-service, low-coordination mode of interaction. That requires a lot of up-front\nand continual work and shouldn’t be underestimated. For example, a Terraform-\nbased environment shouldn’t be offered to the microservices teams without appropri‐\nate documentation, issue tracking, and a reasonable level of support.\nIn a full-fledged microservices system for any reasonably sized\norganization, infrastructure changes almost always incur additional\ncoordination costs from gating processes. The potential impact of a\npoor infrastructure decision is high, so it’s common to require\nsecurity, business, and risk checks before an infrastructure change\ncan be deployed. One practical way to reduce coordination costs is\nto treat these groups as consumers of the platform and design the\nsolution accordingly.\nInfrastructure change: Downtime\nIt’s difficult to make infrastructure changes without introducing a little bit of down‐\ntime. That’s because infrastructure is such a foundational part of a software system.\nFor example, how do you upgrade a Kubernetes server or make a major network\nchange without bringing the system down temporarily?\nThe infrastructure system we’ve built can handle extensions and additions pretty\neasily. All that’s needed is some Terraform code that will run through the pipeline.\nHowever, our system isn’t great at handling changes to existing parts without at least a\nsmall outage.\nA big challenge for us is the immutable nature of our infrastructure. If we want to\nmake even a small change to a component, we need to first destroy it. That can be a\nproblem if we’re hoping to handle workload and traffic at the same time.\nTo make these kinds of in situ infrastructure changes, we could adopt the blue-green\ndeployment pattern (see “Blue-green deployment” on page 266). In fact, we’d even\ntake it a step further and use a phoenix deployment pattern. This pattern is similar to\nblue-green, but instead of having an environment idling, we’ll create new environ‐\nments as needed using our IaC pipeline.\nConsiderations for Our Architecture \n| \n271",
      "content_length": 2631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "This means we could spin up a new environment with our changes. After some test‐\ning we could deploy all our microservices into it and if everything looked good, we\ncould switch our live traffic over to the new environment. For example, an API gate‐\nway or load balancer would give us the traffic-routing features we’d need to facilitate\nthis kind of maneuver.\nBut our big problem is data. We don’t have a clean separation between our data\ninstance and our application instances. For the sake of brevity and simplicity, we’ve\nthrown all of our databases in the same network as the microservice instances. That\nmeans we don’t have an easy way of spinning up a new environment without doing\nsome heavy data replication work. That’s going to add a lot of complexity to the\nchange process.\nIf zero downtime is an important principle for you, you’ll need to reconsider the\ninfrastructure design from the data perspective.\nInfrastructure change: Consumer impact\nConsumers of our applications won’t interact with the infrastructure directly. How‐\never, since we’ve made the decision to offer the infrastructure “as a service” within our\noperating model, we need to consider the impact of changes on our microservices\nteams.\nWhen you change any part of the infrastructure, you’ll need to consider how that\nchange might impact all of the microservices teams consuming and using the plat‐\nform as a service. This can turn out to be a big coordination activity as the number of\nmicroservices in your system grows.\nIn truth, the architecture we’ve built doesn’t do a lot to address this problem. If you\nuse the architecture as is, you’ll need to do some work to make sure that infrastruc‐\nture changes won’t break existing microservices. There will need to be some testing\ninvolved whenever changes are made.\nIn order to keep coordination costs light, the platform and microservices teams need\nto establish a method of communicating changes, keeping automated tests up to date,\nand sharing the responsibility for overall reliability and quality in the system. As\nalways, that requires a mix of Team Topologies, architecture, and good tools and\ntechnologies.\nOne thing we can expect for certain is that the number of microservices will grow\nbeyond the two services we used in our flight system example. So there will be lots of\nchanges for the microservices teams to handle. Let’s dive into those changes next.\n272 \n| \nChapter 11: Managing Change",
      "content_length": 2433,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "Microservices Changes\nMost of the changes you’ll need to make in the system will be to the microservices\nthemselves. When you want to offer new products, change the way that a user experi‐\nence works, or just fine-tune the system, chances are you’ll be making changes to the\nmicroservices subsystem. That may mean creating a new microservice, updating the\nlogic of an existing service or even retiring, splitting, or combining services.\nIn our up and running architecture, it’s easy to imagine that we may want to add\nmore features to our travel system. For example, we might want to add train bookings\nto our search and reservation system. In that case, it’s easy to imagine that we’d be\ncreating a new cluster of microservices and updating the API in the gateway to sup‐\nport those new, extended features.\nAs we’ve seen across all of our domains, adding something new is usually the easiest\nkind of change we can make. Things get more complicated when we need to change a\nservice that is already running. Consider the complications that might arise from\nthese kinds of changes:\n• Splitting the flight-information microservice into services for domestic and inter‐\nnational flights\n• Updating the flight-reservation service with a new “tentative” booking state\n• Merging the flight-information and flight-reservation microservices together\nIn all of these cases, change management gets more complicated because these serv‐\nices are in use. Thankfully, the architecture we’ve built together does a great job of\nminimizing these impacts. Let’s take a look at microservices changes through the lens\nof our four key change impacts.\nMicroservices: Implementation costs\nWhen it comes to changing a microservice, the main implementation costs come\nfrom being able to understand, maintain, and test the code. In our architecture, we’ve\nmade some important decisions to bring down the cost of implementation:\nUsed Event Storming to rightsize microservices\nEvent Storming helped us define boundaries for our microservices that were\ninternally consistent and addressed specific parts of our domain. The net effect is\nthat code comprehension should be improved and changes can be implemented\nin smaller batches with speed.\nConsiderations for Our Architecture \n| \n273",
      "content_length": 2255,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "All microservices use microservice-bootstrap\nThe microservice-bootstrap framework gives our teams a consistent way of\ndocumenting and testing the microservices they develop. By making this frame‐\nwork mandatory we’ve reduced some of the burden of making and testing\nchanges across the organizations. Developers can quickly become familiar with\nthe tool, and the work of testing and building services can become a common\ncompetency across teams.\nUsed CI/CD for microservices\nUsing a CI/CD pipeline means that all of our code changes are tested, linted, and\nvalidated. The net effect of this is a greater chance that the code is in a usable,\nmaintainable state by the time it comes to make a new change.\nOverall, the rightsizing of the service and the DevOps tooling we’ve put in place\nshould greatly reduce the costs of making code changes to microservices in our\narchitecture.\nMicroservices: Coordination costs\nCoordination costs can be a big problem for making software changes. Over time, a\nsimple piece of application code can grow to contain a mess of interdependencies\nwith other libraries, components, and systems. Those interdependencies make it diffi‐\ncult to make changes quickly because of the organizational friction that comes from\nhaving to work with many other people and teams to understand if a change can be\nmade safely.\nIn our architecture, we’ve made a few decisions that should help reduce this cost. In\n“What Are Microservices?” on page 2, we described a definition of microservice that\nhighlighted characteristics of independence for our microservice engineering and\nrelease work. This line of thinking led us to make decisions that could increase the\nindependence of a microservices team:\n• Every microservice is owned by only one team.\n• Every microservice has its own repository and CI/CD pipeline.\nTaken together, these decisions increase the autonomy of teams that are making\nmicroservices code changes.\nIn addition to reducing interteam coordination, our decision to “rightsize” our ser‐\nvice boundaries and keep team sizes constrained ensures that the coordination costs\nwithin our team should stay relatively low as well. It’s fair to say that reducing coordi‐\nnation costs for microservices code changes has been a primary driver for the archi‐\ntecture we’ve built.\n274 \n| \nChapter 11: Managing Change",
      "content_length": 2332,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "But there are two areas where coordination costs are difficult to avoid in our up and\nrunning architecture: life-cycle events and interface changes.\nIn Chapter 2, we introduced the system team that owns responsibility for the health\nand value of the system as a whole. The kinds of changes that come from the system\nteam can result in high levels of coordination. For example, what happens when the\nsystem team decides that two microservices should be merged into one? Worse, what\nhappens if those microservices are owned by two different teams? In our architecture,\nthese types of changes will require much more negotiation, planning, and communi‐\ncation than code changes to an individual microservice.\nWe’ve deemed this to be an acceptable cost trade-off. In our experience, life-cycle and\nsystem-grooming changes are relatively rare compared to changing code to reflect\nnew business or technology requirements. It makes sense to optimize the change\nmodel for the types of change that we expect to happen more frequently.\nIt’s one thing to change the code of a microservice, but you’ll often find yourself\nneeding to change the interface of a microservice as well. In these cases, there may be\nadditional coordination effort required due to the contractual nature of an API\nbetween consumer and provider. We’ll touch on this change factor in more detail in\n“Microservices: Consumer impact” on page 276.\nFinally, in Chapter 2, we made a decision to have a single release team that would\nown responsibility for updating the production environment. This decision has the\nmost potential to go wrong! We established a release team to give special attention to\nchanges and the coordination costs that often come with them. We’ve also tried to\narm the release team with deployment tools to minimize any impacts to velocity. But\nultimately if the release team becomes a bottleneck to change, the system design must\nbe revisited. We’ll reevaluate the topology and the tools that enable the release cycle.\nOverall, the coordination costs of microservices change within our architecture are\nlow. This is thanks to the operating model, tools, and design decisions we’ve made\nthroughout this book.\nMicroservices: Downtime\nAnother change area that we’ve optimized for is in minimizing the downtime\nrequired when an individual microservice is changed. That’s because of the tooling\nand infrastructure we introduced at the platform level. The key to bringing this cost\ndown is our ability to use the canary deployment pattern (“Canary deployment” on\npage 267) for microservices releases. When it comes time to release a new version of a\nmicroservice you can use the tooling we’ve installed to perform the following change\nprocess:\nConsiderations for Our Architecture \n| \n275",
      "content_length": 2761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "1. Deploy the new version of the microservice as a canary, alongside the existing\nversion.\n2. Implement a traffic routing rule to send a small percentage of traffic to the new\nversion.\n3. Observe the health of the new version and verify that the results are as expected.\n4. Promote the canary microservice by routing all traffic to the new version.\n5. Drain and delete the older version of the microservice.\nThis pattern will work for most of the changes you need to make and you’ll be able to\nuse Argo CD to orchestrate the canary activities. However, be careful using this pat‐\ntern when a new version of the microservice will make a change that could impact the\nolder version. For example, if a new version changes data in a shared database, make\nsure that change is compatible with previous running versions.\nMicroservices: Consumer impact\nSo far, we’ve mostly focused on changes to microservices code. The logic, validation,\nand behavior of the service is reflected in code, so that’s where a high frequency of\nour changes will be found. But sometimes you’ll need to make changes to the inter‐\nface (or API) of a microservice and that can cause some big problems.\nChanging the interface of a microservice is almost inevitable. You’ll eventually want\nto change the parameters of an operation or change the data that comes back from a\ncall. The problem is that as other services and components start to depend on the\ninterface, even small changes can result in a lot of work for everyone involved.\nWe haven’t really built anything into our architecture to reduce the consumer impact\nof making changes. The best way to reduce the consumer impact of an API change is\nto adhere to some good design practices: don’t change what you’ve already released,\nwrite client code that tolerates new data, and don’t make new input parameters\nmandatory.\nOur favorite source for API design advice is Mike Amundsen. If\nyou’re interested in building evolvable APIs, we recommend learn‐\ning from the API change patterns in his book Design and Build\nGreat Web APIs (Pragmatic Bookshelf, 2020).\nIn addition to these kinds of design principles, some microservices practitioners have\nadopted contract testing as a way to minimize coordination costs between teams\nwhen changing interfaces. In contract testing, consumers and providers share a con‐\ntract that describes how the interface will be used. This allows providers to run\n276 \n| \nChapter 11: Managing Change",
      "content_length": 2444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "contract tests independently and validate that their changes will not impact existing\nclients of an API.\nIn order to get our system up and running as quickly as reasonably\npossible, we didn’t include a contract testing component in our\narchitecture. But many practitioners have had success using Pact\nfor consumer-driven contract testing. Tools like Pact allow your\nconsumers and providers to continually share and test changes that\nare made to their interfaces.\nBut even with contract testing, chances are that you’ll eventually need to introduce a\nchange that will break someone’s code. In that case, you’ll need to implement some\nform of the multiple versions pattern (see “Multiple versions” on page 267) and main‐\ntain an old microservice until the client team can make the changes they need to.\nUltimately, our architecture doesn’t do a whole lot to reduce the cost of consumer\nimpact changes. API change is hard, and it will take good design thinking and good\nplanning to make those changes affordable. Another area of danger is data and that’s\nwhat we’ll cover next.\nData Changes\nOne of the most difficult aspects of maintaining a microservices architecture is deal‐\ning with the data. Data models are notoriously difficult to change. A persistence layer\nis a much needed part of any software system, but when it comes time to change the\nstructure of data, things can get complicated. Software components grow to be\ndependent on the data systems that they use and changing them can have a big cost\nand impact to the system.\nWe’ve tried to make decisions that improve this situation and lessen the cost of data\nmodel changes. Let’s take a look at the data architecture we’ve built through our four\nlenses of change.\nData: Implementation costs\nAt its most basic level, the cost of changing a data model is a function of how complex\nthe structure, formats, and relationships are and the tooling or language that’s needed\nto make the change. The complexity of a model can increase when there are compli‐\ncated values and many different data types, unique keys, or complicated values. The\ncost really comes from having to understand the model itself, so that changes can be\nmade safely.\nWe haven’t done much in our architecture explicitly to prevent a data model from\ngetting too complicated. But we did make a decision that microservices should own\nConsiderations for Our Architecture \n| \n277",
      "content_length": 2396,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "their own data. This decision alone should help constrain the scope and size of a\nmodel, in the same way it should help us reduce the cost of a code change.\nSo, just as with code changes, you should get a great deal of implementation cost ben‐\nefit from the decision to prioritize independence. But, just like with code, you’ll need\nto keep measuring the implementation costs to ensure that the service and its data\nmodel don’t grow to a size that negates the benefits of a strong boundary.\nData: Coordination costs\nAn even bigger benefit of prioritizing independence is the reduction in coordination\ncosts. By deciding that microservices own their own data, we’re free to make changes\nto our data structures without having to consult with other teams or system owners.\nThis is in stark contrast to more traditional models where multiple teams may be\nusing a shared data service and changes need to be coordinated carefully across all\ndata users.\nHowever, you should beware: there’s a hidden cost to the independent data approach.\nWe’ve optimized our architecture for high-speed, autonomous local changes. This\nmade system-wide changes more costly. For example, if you need to globally change\nthe definition of an airline identifier code, you’ll need to coordinate across all of the\nteams who have implemented a data model that uses it. In our architecture, that could\nbe more costly than if we had just used a shared database.\nA good resource for understanding distributed data patterns is\nMartin \nKleppmann’s \nDesigning \nData-Intensive \nApplications\n(O’Reilly, 2017).\nWe decided to optimize for local changes because in our experience there is a higher\nchange frequency of local changes. But you’ll need to change that decision if the sys‐\ntem you’re building is likely to have sweeping, global changes.\nIf you find that you’re often making changes to multiple data mod‐\nels in the system at the same time, it could be a sign that the\nboundaries of your microservices need to be reevaluated.\nData: Downtime\nOur independent data model gives us some big advantages when it comes to coordi‐\nnation. But it isn’t built for zero-downtime data model changes. That’s especially true\nfor the MySQL database that our flight-information microservice uses.\n278 \n| \nChapter 11: Managing Change",
      "content_length": 2283,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "The root of our limitation is that we’re using a shared database instance to serve mul‐\ntiple replicas of a microservice. When it comes time to make a change to a data\nmodel, it’s difficult to do that without impacting existing microservices that are cur‐\nrently running. This may be easier to do with the Redis store that our reservations\nservice uses, but we’ll still need to be wary of making changes that will break existing\nversions.\nIn cases where we need to make an intrusive data model change, the simplest option\nmay be to destroy the existing microservice versions and replace them with new\ninstances that can implement the data change. In a Kubernetes environment, that can\nbe performed with minimal impact to service. But if any and all downtime is out of\nthe question, a more elaborate blue-green deployment would be needed.\nData: Consumer impact\nBecause we’ve made the decision that microservices should own their own data, the\nimpact of a data model change is restricted to the service itself. So we can freely make\nchanges without impacting the consumer of a service directly. Since the data model is\nencapsulated within the microservice, your microservice teams will have more\nautonomy to make changes to their model, although as we highlighted above, these\nchanges may require a small amount of downtime.\nIn practice, a data model change is likely to require code and even interface changes.\nBut we’re free to separate or stagger these modifications so that a data model change\ncan be made first, before we implement changes that will impact consumers directly.\nSummary\nOverall, we’ve seen that the architecture we’ve built is designed to make changes eas‐\nier and cost-efficient. Change can come from extrinsic or intrinsic sources, but the\nkey is to reduce the costs and impacts so that your teams have more freedom to\nimprove the system and the products and experience that the system powers.\nWe’ve looked at our architecture across infrastructure, code, APIs, and data from the\nperspective of change. As you’ve seen in this chapter, the decisions we made through‐\nout this book have combined to create a profile of changeability for this system. Some\nof our decisions were trade-offs that were made to optimize for certain types of\nchange. Other decisions were trade-offs based on the constraints of the medium of a\ndesign in a book!\nRegardless of the reasons, we’ve now been able to both build a microservices architec‐\nture and evaluate its usefulness and suitability. The only thing left to do with our\narchitecture is to make it even better. That’s what we’ll cover in our final chapter.\nSummary \n| \n279",
      "content_length": 2629,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "CHAPTER 12\nA Journey’s End (and a New Beginning)\nCongratulations, you’ve made it to the final chapter! While you may be reaching the\nend of your journey with us, we hope it is only the beginning of a long and fruitful\njourney in successfully implementing microservices on real projects. We make no\nexcuses for the fact that we are admirers of the microservices architecture and of the\nbenefits it can bring when deployed in the right context, with the right intentions and\nskill. It is by no means the only choice and should never be implemented without\nunderstanding all the implications, but it can certainly be a very powerful choice in\nyour arsenal of architectural tools.\nWe have witnessed many successful microservices projects. There is also no shortage\nof failed attempts to adopting microservices. Our main motivation in writing this\nbook was to increase a reader’s chances of success, if they so choose to implement\ntheir system in the microservices style. We tried to do it by providing step-by-step,\npragmatic guidelines on when, why, and how to deploy microservices, explaining\ncore concepts and demonstrating the implementation of those concepts, using simple\nexamples. We hope we were successful in achieving the goal of turning abstract con‐\ncepts into a more approachable step-by-step explanation, but most importantly, we\nhope you enjoyed reading this book, even if it provided only a handful of key ideas\nyou think you can use when implementing your own systems.\nBefore we part ways, we wanted to share some final thoughts that summarize our\nunderstanding of the architectural decisions in microservices and an approach we\nrecommend for continually measuring the progress of a transformation, if you do\ndecide to embark on one.\n281",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "On Complexity and Simplification Using Microservices\nThroughout this book we have asserted that microservices are most applicable when\nutilized to implement large, complex, continuously changing systems. Intuitively, this\nstatement makes sense: a microservices architecture itself is not simple, so embarking\non that journey has to be worth it—maybe when it helps solve something even more\ncomplex. But what is the nature of complexity and how exactly do microservices\ndecrease complexity, if at all?\nA seminal work on software complexity is Fred Brooks’s 1986 article, “No Silver Bul‐\nlet”, where he aptly notes:\nThere is no single development, in either technology or management technique, which\nby itself promises even one order of magnitude improvement in productivity, in relia‐\nbility, in simplicity.\nBrooks continues to elaborate, explaining that the reason for this phenomena is the\npresence of essential complexity in software systems. While in any codebase there is\nalways some accidental complexity (the complexity related to our own implementa‐\ntion choices), the majority of the complexity that we deal with in software systems is\nnot accidental, it’s related to the very essence of the complexity of modeling the prob‐\nlem domain itself. It is “essential complexity,” by which Brooks means the sophistica‐\nted datasets, relationships among data items, algorithms, and invocation flows that\nrepresent the model a system is attempting to represent. If we tried to simplify a sys‐\ntem beyond its essential complexity, we would be taking away from its core model,\nand it would no longer be the same system.\nWhen dealing with microservices, early adopters are often attracted by the promise of\nmicroservices making building complex systems…wait for it…simpler! Most people\nwould rather microservices make it easier for them to get their jobs done than make\ntheir lives unnecessarily difficult, so a promise of “it will be easier” is unsurprisingly a\npowerful motivator. Quick, off-the-cuff explanations are readily provided: by imple‐\nmenting a “larger” system as a collection of many simple microservices, we are mak‐\ning the whole process simpler! Skeptics may immediately note that while each\nmicroservice may be small and simple, orchestrating a large number of them into a\ncoherent, complex system cannot be expected to be an easy task. And they are cor‐\nrect; but more importantly, for those of us who have read Brooks’s “No Silver Bullet,”\none additional, troublesome question is: have microservices broken Brooks’s conjec‐\nture that there is no way to take away essential complexity? Or is a microservices\narchitecture purely addressing the accidental part of system complexity, and does\nsuch an amazing job that we can still feel the improvement?\nThe truth is that neither is the case. The microservices concept is not just about acci‐\ndental complexity and not a methodology for better coding hygiene, it is an essen‐\ntially different approach. And no—it does not invalidate Brooks’s observation. Rather,\n282 \n| \nChapter 12: A Journey’s End (and a New Beginning)",
      "content_length": 3081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "it achieves its goals in accord with it. You see, you cannot eliminate essential com‐\nplexity, but you can shift it: you can move essential complexity from one part of the\nsystem into another. This would not seem like a big deal, unless different parts of the\nsystem required different levels of effort.\nSimply speaking, when building any software system there’s the implementation part\nof it (the code) and the operational part of it (the deployment and orchestration). We\ncan make the code simpler by breaking it up into many small microservices. Such\nchange will make your operations equally harder. It would seem that we haven’t\ngained much, since by simplifying one part we have made another one more difficult,\nbut in reality this type of complexity shift can be quite beneficial if you can automate\nthe part that you are making “harder” but not the one that you have made simpler.\nThe increased complexity of operations matters much less if they can be automated.\nAnd that is indeed the gain.\nIn the last decade or so we’ve gotten very good at automating software operations. A\nvast arsenal of operation automation tooling such as Ansible, Puppet, Chef, Terra‐\nform, Docker, and Kubernetes, together with serverless functions and a wide variety\nof cloud services provided to us without us having to even think about it have made\nbuilding complex operations materially simpler, beyond anything Brooks could have\nimagined in 1986. Actually designing and writing code, however, is more or less as\ndifficult as it was in the 1980s. Don’t get us wrong: there have been some advance‐\nments, sure, but nothing material. Therefore, if we shift complexity from coding into\noperations we can make things easier, in nontrivial ways.\nMicroservices Can Provide Simplification\nA microservices architecture can be materially simpler than its\nalternatives when implementing complex systems. This does not\nviolate Brooks’s “No Silver Bullet” principle because microservices\ndo not eliminate essential complexity. Rather, this architectural\nstyle is about shifting complexity from the area we cannot, yet,\nautomate, design and code, into the area we have gotten very good\nat automating; operations. The net gain can be substantial.\nMicroservices Quadrant\nLet’s dig deeper into the subject of complexity. Systems Theory distinguishes the defi‐\nnition of a complicated system and a complex system. This was further expanded\nupon and popularized for decision making in the Cynefin framework. A complicated\nsystem can be very sophisticated and hard to understand, but in its essence is predict‐\nable and based on a finite number of well-defined rules. In contrast, a complex sys‐\ntem is by essence nondeterministic, composed of many components that interact at a\nhigh degree of freedom, and can consequently produce emergent behaviors. If we\nOn Complexity and Simplification Using Microservices \n| \n283",
      "content_length": 2887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "were to classify monoliths and microservices in these terms, monoliths would be con‐\nsidered complicated, whereas microservices would be much more aligned with the\ndefinition of complex systems.\nAnother interesting classification is the notion of “easy” versus “simple.” As most\ndesigners would passionately attest, these seemingly synonymous adjectives could not\nbe any more different, in the context of design. Simple things are notoriously hard to\ndesign (think Apple’s original iPod and iMac, or a simple invention such as the com‐\nputer mouse), whereas easy designs are not necessarily simple to use.\nCombining these two perspectives across the axis of architecture and implementa‐\ntion, a couple of years ago we created the “microservices quadrant” that you can see\nin Figure 12-1.\nFigure 12-1. The microservices quadrant (source: https://oreil.ly/IO5t8)\nThis tongue-in-cheek quadrant visualization (beloved by business publications and\nMBA graduates) states that when we think of the overall complexity–simplicity con‐\ntinuum, we can align different solution types across four quadrants:\n• Microservices would be a complex implementation, but a simple design\n(architecture).\n• Monoliths would be a complicated implementation, with an easy (but not neces‐\nsarily simple) architectural design.\nAs for the other two quadrants:\n284 \n| \nChapter 12: A Journey’s End (and a New Beginning)",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "• Many software engineers would like to have a solution that has a simple architec‐\nture, and at the same time, a predictable, even if complicated, implementation.\nThese would probably be the “dreamland”—nonmicroservices implementations\nthat are elegant and successful, so you may not call them “monoliths” to avoid the\nnow-established negative connotation. Candidly, these are quite rare; if your sys‐\ntem is continuously and rapidly changing, achieving a solution in this quadrant is\ntantamount to achieving a dream.\n• In the lower right quadrant, we have a situation where we got away with an easy\ndesign (think minimal effort), and ended up with a complex implementation that\nsomehow still functions despite the easy architecture. Well, that would be a uni‐\ncorn in more ways than one, including the need for the so-called “10x develop‐\ners” to support and maintain it. But we are sure such things also exist.\nThe microservices quadrant gives a shorthand representation of where microservices\nand monolith solutions land vis-à-vis architectural simplicity compared to imple‐\nmentation complexity.\nHaving discussed the nature of microservices architectures through the lens of com‐\nplexity, we would like to give the reader another important perspective: how to think\nabout a microservice transformation over time.\nIn Chapter 11, we discussed both the role microservices architecture plays in helping\nteams tackle change in complex systems and techniques to manage change when\nimplementing microservices. There is another important aspect of change in regards\nto microservices: the transformation that an organization as a whole needs to go\nthrough when transitioning from a nonmicroservices culture and adopting this novel\norganizational–technical structure. In the next section we will discuss how to be suc‐\ncessful with a microservices transformation by taking a holistic look at one and\navoiding the trap of technology-only blinders.\nMeasuring the Progress of a Microservices Transformation\nWhen we discuss the migration to microservices, it’s important to remember that we\nare talking about a style that encompasses complex technology and highly disruptive\ncultural transformations of an organization. If not carefully managed, the odds of get‐\nting it wrong are naturally much higher than those of accidentally getting lucky with\nit. If you have followed the various critical posts published about microservices in the\npast several years, you could observe a fairly discernible pattern. First, a company\nadopted a microservices architecture and wrote a cheerful blog post about its benefits\nand promise; this was followed by a blog post years later, complaining about micro‐\nservices’ complexity and praising a switch back to a monolith. While for some teams,\nprojects, or companies, microservices may indeed be the wrong choice, the reality is\nMeasuring the Progress of a Microservices Transformation \n| \n285",
      "content_length": 2919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "that genuine poor fit is not always the cause of the failure. More often than not,\nflawed execution is the root cause of the disappointing results.\nThere is no turnkey software that teams can just buy from a vendor, or install with an\nopen source license, that can magically make us “microservices” overnight. Moreover,\nthere is not even some strict set of policies and guidelines that guarantee success. In\nreality, many traits of microservices architectures are aspirational and not directly\nquantifiable: independent deployability, decentralized governance, infrastructure\nautomation, and evolutionary architecture, among others, are not things that any\nteam can excel at right out of the gate or easily measure their progress toward! They\ntake a long time and a fair amount of patience to mature, and are seldom perfect. It\nshould not be a goal to perfect them in the early days of a transformation effort.\nOne of the most damaging things an organization can do at the\nearly stages of microservices adoption is to establish a “microservi‐\nces police” that will strictly govern adherence to all of the microser‐\nvices principles and traits. Migrating to microservices is a long\nprocess; it’s a journey that requires patience and measurement.\nThe thinking that teams need to adopt when considering their level of maturity vis-à-\nvis microservices traits is largely similar to the philosophy we described in Chapter 4\nwhen discussing rightsizing microservices: the size and granularity of a microservice\nevolves over time organically and forcing an attempt to start with a target granularity\nearly on is detrimental. Similarly there are significant risks in prematurely insisting\non a “perfect” microservices implementation as it relates to traits like independent\ndeployability and automation too early in the transformation process. Instead, it is\nimperative for teams to remain pragmatic and ask themselves questions such as:\n• While Kubernetes is undoubtedly the leading container orchestration solution,\ndo we currently have the skills and developer capacity to support it? Even if it is\nprovided by our cloud hosting solution? Or should we start with something\nmuch simpler (e.g., AWS ECS)?\n• How automated does our infrastructure need to be in the early days? What level\nof self-healing is absolutely necessary in the early days?\n• What systems can we delegate to a cloud provider to manage (e.g., databases,\nevent streaming, etc.) even if eventually we may bring those back in-house? Do\nwe absolutely need to start with a new, shiny database system, or can we instead\ninitially use a less powerful, but cloud-provided database to cut down on mainte‐\nnance overhead?\nIn most cases, the right thing to do in regard to these, and similar, questions is to cut\nyourselves a fair amount of slack in the early days. It may be wise to stick to “boring”\ntech, and avoid upgrading MySQL to Cassandra or replacing Java with Golang at the\n286 \n| \nChapter 12: A Journey’s End (and a New Beginning)",
      "content_length": 2993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "same time you’re also trying to adopt microservices, especially if your teams are unfa‐\nmiliar with these new technologies. Instead, teams must concentrate on things that\nmake business difference, and avoid getting bogged down by endless cycles of infra‐\nstructure setup, tech stack upgrades, and experimentation with cool new tools, criti‐\ncally delaying delivery of business value. Such delays can easily lead to stakeholders\nshutting down a transformation effort before it even gets properly started.\nIt is extremely important to remember that microservices architec‐\nture is a journey, not just a destination. In this journey, the trajec‐\ntory of the progress means everything, and surprising as it may\nsound, current state is of much less significance. This is especially\ntrue in the early days of the transformation efforts.\nWe discussed in Chapter 1 that minimizing coordination costs is a core technique of a\nmicroservices architecture. It is so fundamental that teams that can demonstrate\nmovement towards the diminishing levels of coordination needs are going to do well\nregardless of how many principles from Newman, Lewis, Fowler, or Mitra/Nadareish‐\nvili they adhere to initially. As long as they move in the right direction, the trajectory\nwill win in the long run—every single time. The approach here is similar to the con‐\ncept of Fitness Functions, as described in Building Evolutionary Architectures by Neal\nFord, Rebecca Parsons, and Patrick Kua (O’Reilly).\nHow do we know if we are on the right trajectory? Sure, understanding that coordi‐\nnation costs are our main enemy is helpful, but we cannot directly measure “coordi‐\nnation cost” as a value. Some teams try measuring “speed” or “safety,” but that is\nequally problematic, as these values are derivatives and measurements are indefensi‐\nble. You will almost certainly notice a perceived increase in speed and safety, but to\nclaim causality, what are you going to compare the new speed to? Nobody builds the\nsame exact system once as a monolith and then as a microservices architecture. Any\nincrease in speed will be intuitively rewarding but unscientific. The same idea applies\nto attempts of measuring increases in safety as well.\nInstead, we propose measuring three values, two of which are directly related to the\ntrajectory of increasing team autonomy, and the third that best indicates the overall\nefficiency of software teams (as described in Accelerate by N. Forsgren, J. Humble,\nand G. Kim (IT Revolution Press)):\n• The average size of an autonomous team, across all teams\n• The average length of time an autonomous team can work without getting halted\non waiting for another team (waiting usually being caused by a critical\ndependency)\n• The frequency of successful deployments\nMeasuring the Progress of a Microservices Transformation \n| \n287",
      "content_length": 2826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "In a healthy microservices transformation that is on the right trajectory, you should\nsee a gradual decrease in the size of autonomous teams and an increase in the amount\nof time that teams can work independently. For instance, you may observe that aver‐\nage autonomous team size in your organization used to be 15 to 20 members and\nafter implementing microservices it starts to gradually decrease to 10, 8, 6…\nLikewise, you should observe a decrease in frequency of coordination-related dead‐\nlocks. A coordination deadlock is a stoppage during which an autonomous team is\nwaiting on another team for a shared capability to be made available for them; e.g., an\ninfrastructure team provisioning a highly available Kafka or Cassandra cluster, or a\nsecurity review team completing a code audit. Another common example of a team\ngetting halted is when they need to wait for the outcome of a coordination meeting in\nwhich various stakeholders are making a critical decision.\nScheduling such meetings can take time due to varying priorities of the stakeholders.\nTracking the number of dependencies that a team needs to clear before a code release\nto production is also a quantity worth measuring. Another important example of the\ntype of event to track is whether teams need to often wait for other teams to make\ncode changes, caused by the change in a shared data model. The triggers and duration\nof stoppages will vary depending on an organization and the business contexts. It’s\nimportant to track both trigger types as well as stoppage duration, so that meaningful,\nactionable lessons learned can be derived and improvements can be made.\nThe third metric, deployment frequency, does not directly measure coordination\ncosts, but is a general metric that has been scientifically proven by Forsgren et al. to\nbe a powerful indicator of team agility. When applied to independently deployable\nmicroservices, in our experience it can also indicate the health of a microservices\ntransformation trajectory.\nBy consistently measuring the three metrics and ensuring the transformation is on\nthe right track, teams can free themselves from the anxiety of achieving perfection in\nevery single microservices trait, freeing themselves for long-term success.\nSummary\nIn this last, closing chapter of the book we shared with you how we think of micro‐\nservices. Microservices can make your complex systems simpler, but it is no “silver\nbullet” and it is important to understand that the final effect is achieved by shifting\ncomplexity, not necessarily magically eliminating it. When we make such assertions,\nit also helps to be clear what we mean by “complexity,” and it is different from the\nnotion of “complicated” systems, and what role “easy” versus “simple” architectural\napproaches play in classifying various system delivery approaches.\nWe then shared our perspective on the importance of patience and a long-term out‐\nlook during a microservices transformation. It is a journey and a marathon, not a\n288 \n| \nChapter 12: A Journey’s End (and a New Beginning)",
      "content_length": 3048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "sprint, and teams intending to be successful need to be equipped with proper tools\nand concentrate on the trajectory of transformation much more than the current\nstate. Make sure you get in the habit of measuring some reliable metrics to ensure you\nare still on the right track and that your trajectory is healthy.\nWe hope you have enjoyed reading this book. We hope it provided more hands-on\nand practical guidance than what has been previously available, and that you had fun\ngoing through the code and examples.\nWe wish you much success on your own microservices transformation journey and\nwould love to hear from you—what you learn when you embark on implementing\nmicroservices in your own work.\nAll the best.\nSummary \n| \n289",
      "content_length": 729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "Index\nA\nACID\natomicity, 82\nconsistency, 82\ndurability, 82\nisolation, 82\nsagas and, 84\nACL (anti-corruption layer), 64\nopen host service, 65\nactions\nJTBDs, 45-47, 48\nOAS (OpenAPI Specification), 49\nactors, SEED(S)\nidentifying, 37-39\nidentifying jobs, 39-41\nJTBDs (jobs to be done)\njob stories, 41-43\nADR (architecture decision record), 10-11\nalternatives, 10\nchoice, 10\ncontext, 10\nimpact, 11\nanalysis paralysis, 7\nAPI (application programming interface)\nAWS (Amazon Web Services)\nTerraform and, 110, 171\nBFF, 198\nconsumer teams, 32-33\nconsumers, 40\ncustomers, 37\nmicroservices and, 3\nOAS (OpenAPI Specification), 48-52\nfeedback, 53\noverabstraction, 37\nproduct-oriented perspective, 37, 39\nReservations API, 45\nversus microservices, 54-56\narchitecture, 3\n(see also microservices)\nchange and, 266\ndata isolation, 85\ndesign for failure, 2\ndesign traits, 3\nECS (Elastic Container Service), 269\nevolutionary design, 2\nmonolith architectures, 60\nresources, adding, 269\nsmart endpoints and dumb pipes, 2\nsoftware architecture, 3\ntechnology, 5\nArgo CD, 141\ndeployment, applications and, 257-259\nGitOps and, 255\nHelm charts, 249\nHelm package, 253-254\nKubernetes module, 171-173\nlog in, 255-256\nsandbox repository, 174-175\nvariables, 173-174\nARN (Amazon Resource Name), 114\nasset publishing, 131-132\nasynchronous integration, DDD (domain-\ndriven design), 65\natomicity (ACID), 82\nautomated testing, workspace, 184\nAWS (Amazon Web Services)\nAPIs, Terraform and, 110\nARN (Amazon Resource Name), 114\n291",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "authenticator, 129\nCLI (command-line interface), configuring,\n110-111\nconfiguring, 106\nEKS (Elastic Kubernetes Service), 113, 140\ncluster, 162-164\nnode group, 164-167\nIAM (Identity and Access Management),\n106\nLambdas, 58\noperations account, 106-110\npermissions setup, 112-114\nS3 storage\nnames, 158\nTerraform, 115-116\nB\nBezos, Jeff, 17\nBFF APIs, 198\nblue-green deployment, 266\nboundaries, 57\nDDD (domain-driven design), 59-62\ncollaboration, 63\ndependencies, 73\ngranularity, 58\nLambdas and, 58\ntechnical needs, 58\nbounded contexts, 61\nUbiquitous Language, 61\nbranching, version control, 185\nC\ncanary deployment, 267\nCAP theorem, 94-95\nCassandra, 192-193\nchange, 263\narchitecture and, 266\nas feature, 263\nconsumer impact, 266\ncoordination time and, 265\ndata, 264-265\nconsumer impact, 279\ncoordination costs, 278\ndowntime, 278\nimplementation costs, 277\ndeployment patterns, 266\nblue-green deployment, 266\ncanary deployment, 267\nexpand and contract, 267\nmultiple-versions, 267\ndowntime and, 266\nimplementation time and, 265\ninfrastructure, 269\nchange process, 270\nconsumer impact, 272\ncoordination costs, 270-271\ndowntime, 271-272\nimplementation costs, 270\nmeasurements and, 264-265\nmicroservices, 273\nCI/CD pipeline, 274\nconsumer impact, 276-277\ncoordination costs, 274-275\ndowntime, 275\nevent storming, 273\nimplementation costs, 273-274\nreasons for, 263\ncharts, 248\nHelm charts, creating, 250\ntemplates, 249\nvalues, 249\nCI/CD (Continuous Integration/Continuous\nDelivery), 102-104\ntooling, 103\nCI/CD pipeline, 97, 103, 123, 179\nconfiguring, 242\nGitHub Actions and, 103\nmicroservices change, 274\nCIDR (Classless Inter-Domain Routing), 148\ncloud platform, 9\ncloud platform teams, 29\ncode\nimplementing, 212-213\nsandbox repository, 120-123\ncoding standards, 180\ncollaboration interaction mode, Team Topol‐\nogy, 23\ncollaboration, DDD (domain-driven design),\n63\ncomplicated-subsystem teams, 22\nenabling, 31-32\ncomponentization via services, 2\nconformist relationship, 64\nconsistency (ACID), 82\nconsumer impact\ndata changes, 279\ninfrastructure cost and, 272\nmicroservice change, 276-277\n292 \n| \nIndex",
      "content_length": 2089,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "consumer teams, 32-33\nAPIs, 32-33\nconsumers, APIs, 40\ncontainer registry, 241\ncontainerization, 183, 187\ncontainers\ncommands, launching, 189-190\nDocker, installing, 190-191\nflights microservice, 246\nshipping, 241\nDocker Hub, 241\nDocker Hub secrets, 243\nflights microservice, 244-246\npipeline configuration, 242\ncontext mapping, 62-65\nACL (anti-corruption layer), 64\nopen host service, 65\nconformist relationship, 64\nUpstream-Downstream mapping, 63\nConway, Mel, 16\ncoordination cost\ndata changes, 278\ninfrastructure cost and, 270-271\nissues, 4-6\nmicroservice change, 274-275\nCQRS (Command Query Responsibility Segre‐\ngation), 91, 93-94\nCAP theorem and, 94\ncross-functional teams, 18\nD\ndata\nchange and\nconsumer impact, 279\ncoordination costs, 278\ndowntime, 278\nimplementation costs, 277\nembedding, 77\ndata delegate pattern and, 79-81\ndatabases, number, 78\nSOR (system of record) and, 81\ndata delegate pattern, 79-81\ndata design, 9\ndata duplication, independence and, 81-82\ndata implementation, 208\nMySQL data model, 211-212\nRedis, 209-210\ndata lakes, 81\ndata management, 75\nACID transactions, 83\ndata space co-ownership, 76\nEvent Sourcing and, 89\nindependence and, 78\nmonolithic, 79\ndata modeling, Event Sourcing and, 89\ndata sharing, 77\nindexes, 81\ndatabase clusters, 78\ndatabase module, staging environment, 234\ndatabases\nACID and, 82\ndata lakes and, 81\ndataset ownership, 77\nembedded data and, 78\nlocal, 183\nmigrations, 184\nmake commands, 215\nrelational, 87\nshared data, 81\nDDD (domain-driven design)\naggregate, 66\nasynchronous integration, 65\nboundaries and, 59-62\ncollaboration, 63\ncontext mapping, 62-65\nACL (anti-corruption layer), 64, 65\nconformist relationship, 64\nUpstream-Downstream mapping, 63\nevent storming, 66-73\ngranularity, 58\nsynchronous integration, 65\ndecentralized governance, 2\ndecision records, 10-11\ndependencies, 4\nboundaries and, 73\nDocker as, 181\ninstalling, 128-130\ndeployment, 76\n(see also independent deployability)\nArgo CD applications, 257-259\nblue-green deployment, 266\ncanary deployment, 267\nchange and, 266-268\ncontainers, flights microservice, 246\nexpand and contract pattern, 267\nGitOps, Argo CD, 255\nHelm package\nArgo CD, 253-254\nIndex \n| \n293",
      "content_length": 2179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "testing, 254\nKubernetes, 247\nHelm charts, 248-249\nmultiple-versions, 267\nrepository, 249-250\nDeployment controller, Kubernetes, 248\ndeployment template, 250\ndesign\ndata design, 9\nendpoints, 197\nJTBDs, 198-201\nSEED(S), 198\nfor failure, 2\nmicroservice design, 9\nOpenAPI spec, 202-208\nteam design, 8\nTeam Topologies, 24\nDevOps, 98\nCI/CD (Continuous Integration/Continu‐\nous Delivery), 102-104\nIaC (infrastructure as code), 100-101\nenvironment setup, 104\nTerraform, 101, 105\nimmutable infrastructure, 99-100\nIaC (infrastructure as code) and, 101\nprinciples and practices, 98\ndiagramming tools, 28\ndistributed transactions\nfailures and, 82-83\nSagas, 83-85\nDocker\nas dependency, 181\nCassandra, 192-193\ncontainers in, 190-191\nLinux installation, 187\ntesting, 191-192\nDocker Compose, 183\nDocker Hub, 241\nconfiguring, 242\ncontainer registry, 241\nregistry set up, 242\nsecrets, configuring, 243\nDocker4Mac, 187\nDocker4Windows, 187\nDockerfiles, images, 183\ndowntime\ndata changes, 278\ninfrastructure cost and, 271-272\nmicroservice change, 275\nDreyfus Model of Skill Acquisition, 8\nDunbar, Robert, 17\nduplicating events, 92\ndurability (ACID), 82\nE\nECS (Elastic Container Service), 269\nEKS (Elastic Kubernetes Service), 113, 140\nAmazonEKSClusterPolicy, 163\ncluster, 160, 162-164\ndefinition, 164\ncustom-eks-policy.json file, 113\nload balancers and, 148\nnode group, 164-167\nsubnets, 148\ntags, 149\nelastic IP addresses, 152\nembedding data, 77\ndata delegate pattern and, 79-81\ndatabases, number, 78\nenabling teams, 22\nendpoint design, 197\nJTBDs, 198-201\nSEED(S), 198\nenvironment variables, MySQL databases, 252\nevent duplication, 92\nEvent Sourcing, 85-86\naccounting, 86\nCAP theorem and, 94\nchess, 86\ndata management and, 89\ndata modeling and, 89\nprojections, 90\nrolling snapshots, 91\nversus relational modeling, 87-89\nevent stores, 92-93\nevent storming\nDDD (domain-driven design), 66-73\nmicroservices change, 273\nevolutionary design, 2\nexpand-and-contract pattern, 267\nF\nfacilitating interaction mode, Team Topology,\n23\nfeedback loops, 6\nFlask, 220-225\nflights microservice\ncontainer, shipping, 241\n294 \n| \nIndex",
      "content_length": 2093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "deployment package, 247\ndeployment template, 250\nDocker Hub secrets, 243\nHelm charts, 250\nHelm package, 249\ntesting, 254\nMySQL and, 240\nrepository, 242\nshipping, 244-246\ntesting, 259-260\nfolders, mapping, 189-190\nforking staging infrastructure, 234\nG\nGCP (Google Cloud Platform), AWS (Amazon\nWeb Services) and, 106\nGeorge, Fred, 20\nGit, 104\nGitHub, 104\naccount setup, 104\nforking repository, 235\ngitignore file, 145\nsandbox repository, 117\nsecrets storage area, 123\nset up, 104\nZIP files, 176\nGitHub Actions, CI/CD pipeline, 103\nGitOps, 140-141\nArgo CD, 141\ndeployment, 255\nGore, Bill, 17\nH\nHCL\nencapsulation, 120\nfmt command, 122\nJSON and, 119\nsandbox repository, 120\nTerraform and, 119\nvalidation, 122\nhealth checks, 218-220\nHelm (Kubernetes), 171, 248-249\nchart creation, 250\nHelm package\ntesting, 254\nvalues, 253-254\nheterogeneous-ready workspace, 182\nI\nIaC (infrastructure as code), DevOps, 100-101\nenvironment setup, 104\nGitHub and, 104-104\npipeline, building, 116-132\nasset publishing, 131-132\ndependencies, 128-130\nGitHub secrets, 123-124\nsandbox repository, 117-119\nTerraform, 119-123\nTerraform files, 130-131\ntesting, 133-135\ntrigger, 127-128\nworkflow creation, 125-126\nTerraform, 101\ninstallation, 105\nIAM (Identity and Access Management), AWS,\n106\nIBM MQ, 81\nimmutable infrastructure (DevOps), 99-100\nIaC (infrastructure as code) and, 101\nimplementation cost\ndata changes, 277\ninfrastructure cost and, 270\nmicroservice change, 273-274\nindependent deployability, 76\ndata duplication and, 81-82\nindexes, shared data, 81\ninfrastructure, 97\n(see also CI/CD pipeline)\nchange and, 269\nchange process, 270\nconsumer impact, 272\ncoordination costs, 270-271\ndowntime, 271-272\nimplementation costs, 270\nCI/CD (Continuous Integration/Continu‐\nous Delivery), 102-104\nclean up, 177\ndependencies, installing, 128-130\nenvironment, testing, 175-176\nGitOps, 140-141\nIaC (infrastructure as code), DevOps,\n100-101\nimmutable infrastructure (DevOps), 99-100\nkubectl, installing, 142\nKubernetes, 139\ncluster, 140\ncontrol plane, 140\nIndex \n| \n295",
      "content_length": 2034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "nodes, 140\nKubernetes module, 160\nArgo CD setup, 171-173\nArgo CD variables, 173-174\noutputs, 161\nsandbox repository, 169-171\nvariables, 168-169\nnetwork module, 145\nconfiguration, 146-154\noutputs, 146\nnetworks, 138\nrouting, 139\nsecurity, 139\nsubnets, 138\nVPC (virtual private cloud), 138\nstaging\ncode editing, 237-239\nforking, 234\ningress module, 233-234\nsetup, 232-233\nworkflow configuration, 235\nTerraform module\nAWS provider, 147\ninternet gateway, 151\nlocal variables, 147\nmain configuration, 146-154\noutputs, 145\nrepositories, 142-145\nsandbox network, 157-160\nvariables, 154-157\nVPC, 148\ninfrastructure automation, 2\ningress module, staging environment, 233-234\nIngress object, Kubernetes, 248\ninteraction patterns, sequence diagrams, 43-45\ninternet gateway, 151\nIP addresses\nCIDR, 148\nelastic, 152\nisolation (ACID), 82\nJ\njob stories, 41-43\nJSON\ncurl command and, 225\ncustom-eks-policy.json file, 113\nEKS-management policy, 114\nMultipass and, 188\nmultipassd-vm-instances.json, 189\nMySQL and, 211\nRedisJSON module, 211\nseat map, 211\nJTBDs (jobs to be done)\nactions, 45-47, 48\nendpoint design, 198-201\njob stories, 41-43\nqueries, 45-47\nK\nkubeconfig file\ngenerating, 166\nTerraform and, 167\nworkflow staging, 237\nYAML content, 167\nkubectl\nconfiguration details, 239\ninstalling, 142\nrequest forwarding, 256\nkubectl CLI, 166\napplication setup, 175\nKubernetes, 139\nclient environment, 239\ncluster, 140\naccess testing, 239-240\nsandbox network, 169-171\ncontrol plane, 140\ndeployment, 247\nHelm charts, 248-249\nHelm, 171\nHelm charts, creating, 250\ninstalling, 193-194\nmodule, 160\nArgo CD setup, 171-173\nArgo CD variables, 173-174\noutputs, 161\nvariables, 168-169\nnodes, 140\nobjects\nDeployment controller, 248\ndescription file, 250\nIngress, 248\nPods, 247\nReplicaSets, 247\nServices, 248\nYAML templates, 250\n296 \n| \nIndex",
      "content_length": 1809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "L\nLADR (lightweight architectural decision\nrecord), 11-13\n(see also ADR)\nLambdas, 58\nLinux\nDocker installation, 187\nOAS (OpenAPI Specification), 49\nLiveUML, 44\nload balancing\nEKS (Elastic Kubernetes Service), 148\nimmutable infrastructure, 99\nTraefik, 259\nlocal variables, Terraform modules, 147\nlocal workspace, 182\nM\nmakefiles, targets, 185\nmapping\ncontext mapping, DDD (domain-driven\ndesign), 62-65\nfolders, 189-190\nMarkdown, 11-13\n# character, 12\nContext section, 12\nStatus header, 12\nmerging, version control, 185\nmessaging\nIBM MQ, 81\nRabbitMQ, 81\nmicroservice architecture, 3\n(see also microservices)\nmicroservice design, 9\nmicroservice-bootstrap, 274\nmicroservices\nAPIs (application programming interfaces)\nand, 3\nchange and\nCI/CD pipeline, 274\nconsumer impact, 276-277\ncoordination costs, 274-275\ndowntime, 275\nevent storming, 273\nimplementation costs, 273-274\nchanges, 273\ncharacteristics, 2\ncomplex, 282-283\ndefinition, 2, 3\ndevelopment, 9\ngoals, 58\nimplementing, 53\noverview, 2-3\nownership, 27\nscope, 8\nsecrets management, 123\nsimplification, 282-283\nsizing, 59\nuniversal sizing, 73-74\nsoftware architecture, 3\ntransformation, measuring, 285-288\nversus APIs, 54-56\nmicroservices quadrant, 283\nmicroservices system\nculture, 16\nteam design and, 16\nMicrosoft Azure, 106\nmigration, databases, 184\nmake commands, 215\nmodules\nKubernetes, 160\nArgo CD setup, 171-173\nArgo CD variables, 173-174\noutputs, 161\nvariables, 168-169\nrepositories, 142-145\nTerraform, 142-145\nAWS provider, 147\nlocal variables, 147\nmain configuration, 146-154\noutputs, 145\nsandbox network, 157-160\nvariables, 154-157\nVPC, 148\nmonolith architectures, 60\nmonolithic data management, 79\nmonorepo, 118\nMultipass installation, 188-189\nMySQL, 211-212\nconnection values, 253\nenvironment variables, 252\nhealth checks and, 219\nJSON data type, 211\nKubernetes secrets, 240\nRDS (Relational Database) and, 234\nstaging environment, 234\ntables, 215\nworkflow staging, 235\nIndex \n| \n297",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "N\nNAT (Network Address Translation) gateway,\n152\nnetwork module, 145\nconfiguration, 146-154\noutputs, 146\nnetworks\nrouting, 139\nsecurity, 139\nsubnets, 138, 148\nVPC (virtual private cloud), 138\nNodebootstrap microservice, 213\nnodes\nEKS (Elastic Kubernetes Service), 160, 164\nEKS cluster, 164\nKubernetes cluster, 140\nO\nOAS (OpenAPI Specification), 48-52\nactions, 49\ndesign, 202-208\nfeedback, 53\nRESTful APIs, 49\nYAML file, 49\nobjects\nKubernetes\nDeployment controller, 248\ndescription file, 250\nIngress, 248\nPods, 247\nReplicaSets, 247\nServices, 248\nopen host service, 65\noperating model\noverview, 15\npeople and, 16\nteams and, 16\norganized around business capabilities, 2\nP\npermissions, AWS (Amazon Web Services),\n112-114\nPlantUML, 43\nUML sequence diagrams, 199\nplatform teams, 22, 29-30, 270\ncloud platform, 29\npods\nArgo CD, 176\nKubernetes, 140, 247\nproducts not projects, 2\nproducts, definition, 40\nProgrammer Anarchy, 20\nprojections (Event Sourcing), 90\nPython, 220-225\nQ\nqueries, 93\n(see also CQRS)\nJTBDs, 45-47\nR\nRabbitMQ, 81\nRedis, 209-210\nRedisJSON module, 211\nregistries\ncontainer registry, 241\nhosting, 242\nrelational modeling, Event Sourcing and, 87\nrelease, 9\nremote workspace, 182\nReplicaSets, Kubernetes, 247\nrepositories\nmicroservice deployment, 249-250\nmonorepo, 118\nsandbox, 117-119\nstaging environment, 234\nReservations API, 45\nRESTful APIs, 36\nsynchronous integration, 65\nrolling snapshots, 91\nrouting, 139\nRule of Twos, 182\nS\nS3 (Simple Storage Service)\nJSON objects, 116\nnames, 158\nTerraform and, 115-116\nSagas, 83-85\nsagas\nACID and, 84\ntransactions, 83\nsandbox repository\nArgo CD, 174-175\nbuilding, 117-119\ncode, 120-123\n298 \n| \nIndex",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "GitHub interface, 117\nHCL code, 120\nKubernetes cluster, 169-171\nTerraform CLI tool, 120\nTerraform module, 157-160\nsecrets\nDocker Hub, 243\nKubernetes\nbuilt-in functions, 240\nMySQL, 240\nrepositories, 235\nsetup, 243\nsecurity\ncollaboration and, 23\nEKS (Elastic Kubernetes Service), 162, 164\nGitHub Actions, 236\nnetworks, 139\nsubsystem and, 22\nVPC (virtual private cloud), 163\nSEED(S), 35\nendpoint design, 198\nGraphQL APIs, 36\nidentifying actors, 37-39\nidentifying jobs for actors, 39-41\nJTBDs (jobs to be done)\nactions, 45-47, 48\njob stories, 41-43\nqueries, 45-47\nsequence diagrams, 43-45\nUML, 43\nsequence diagrams\ninteraction patterns, 43-45\nUML, 43\nServices, Kubernetes, 248\nSeven Essential Evolutions of Design for Serv‐\nices (see SEED(S))\nShared_Kernel, 63\nsharing data (see data sharing)\nshipping container, 241\nDocker Hub, 241\nsecrets, 243\nflights microservice, 244-246\npipeline configuration, 242\nskills\nDreyfus Model of Skill Acquisition, 8\nteams, 18-19\nsmart endpoints and dumb pipes, 2\nsoftware architecture, 3\nSOR (system of record), 81\nstaging environment\ncode editing, 237-239\nforking, 234\nKubernetes cluster, testing access, 239-240\nmain.tf file, 237\nrepositories, 234\nsetup, 232-233\ndatabase module, 234\ningress module, 233-234\nworkflow\nconfiguring, 235\nkubeconfig file, 237\nstream-aligned teams, 22, 27\nsubnets, 138, 148\nEKS (Elastic Kubernetes Service), 148\ntags, 149\nprivate, 152\nrouting rules, 151\nsynchronous integration, DDD (domain-driven\ndesign), 65\nsyncing microservices, 257-259\nsystem design team, 24\nguardrails, 25\nimprovements, 25\nincentives, 25\nstandards, 25\nstructures, 25\nSystems Theory, 283\nT\nteam design, 8\nTeam Topologies, 21\ncomplicated-subsystem, 22\nenabling, 31-32\nconsumer teams, 32-33\ndesigning, 24\nenabling, 22\ninteraction modes\ncollaboration, 23\nfacilitating, 23\nX-as-a-service, 23\nplatform, 22, 29-30\ncloud platform, 29\nstream-aligned, 22, 27\nteams\ncoordination and collaboration, 19-21\ncross-functional, 18\nplatform, 270\nsize, 17-18\nIndex \n| \n299",
      "content_length": 1985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "skills, 18-19\nsystem design, 24\nguardrails, 25\nimprovements, 25\nincentives, 25\nstandards, 25\nstructures, 25\ntemplate, 26-29\ntechnology architecture, 5\ntemplates\ncharts, 249\ndeployment, 250\nteams, 26-29\nYAML, 250\nTerraform, 101\nAWS S3 bucket backend, 115-116\ndata element, 151\nfiles, applying, 130\nHCL and, 119-120\ninstallation, 105\nJSON-based state file, 115\nmodules, 142-145\nAWS provider, 147\nlocal variables, 147\nmain configuration, 146\noutputs, 145\nsandbox network, 157-160\nvariables, 154-157\nVPC, 148\nsandbox repository and, 120\ntesting\nautomated, 184\nDocker, 191-192\nflight microservice, 259-260\nIaC (infrastructure as code) pipeline,\n133-135\nTraefik, 259\ntransactions\ndistributed\nfailures and, 82-83\nSagas, 83-85\nsagas, 83\ntrigger, workflow, 127-128\nU\nUbiquitous Language, 61\numbrella projects, 226-228\nUML\nLiveUML, 44\nPlantUML, 43\nsequence diagrams, 43\nPlantUML format, 199\nuniversal sizing, 73-74\nup and running microservices model\nchange, 9\ncloud platform, 9\ndata design, 9\nmicroservice design, 9\nmicroservice development, 9\nrelease, 9\nteam design, 8\nUpstream-Downstream mapping, 63\nV\nvalues, charts, 249\nvariables\nKubernetes module, 168-169\nTerraform modules, 147, 154-157\nversion control\nbranching, 185\nmerging, 185\nVPC (virtual private cloud), 138, 148\nW\nworkflow, 125-126\nstaging environment\nconfiguring, 235\nkubeconfig file, 237\ntrigger, 127-128\nworkspace\ncontainerization, 183\ndatabases\nlocal, 183\nmigrations, 184\nDocker\nas dependency, 181\nLinux, 187\nheterogeneous-ready, 182\nlocal, 182\nmakefiles, targets, 185\nmicroservices\nsingle, 183\nsubsystem, 183\nMultipass, 188-189\nremote, 182\nRule of Twos, 182\ntesting, automated, 184\nversion control\n300 \n| \nIndex",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "branching, 185\nmerging, 185\nX\nX-as-a-service interaction mode, Team Topol‐\nogy, 23\nY\nYAML, 126, 130\ndeployment object, 247, 251\nkubeconfig file, 167\nspacing, 129\ntemplates, 250\nIndex \n| \n301",
      "content_length": 190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "About the Authors\nRonnie Mitra is an author, strategist, and consultant with over 25 years of experience\nworking with web and connectivity technologies. He is the coauthor of Microservice\nArchitecture and Continuous API Management (both O’Reilly).\nIrakli Nadareishvili is the vice president of Core Innovation at Capital One Financial\nCorporation, leading the teams responsible for building Capital One’s modern, cloud\nnative, microservices-based core banking platform. Before Capital One, Irakli was\ncofounder and CTO of ReferWell, a successful New York City–based health technol‐\nogy startup, and held technology leadership roles at CA Technologies and NPR. Irakli\nis the coauthor of Microservice Architecture (O’Reilly). You can follow Irakli on Twit‐\nter at @inadarei.\nColophon\nThe animal on the cover of Microservices: Up and Running is the sparkling violetear\nhummingbird (Colibri coruscans). This hummingbird lives in a range that runs along\nthe northwestern coast of South America, in higher-elevation habitats among the\nAndes mountains. Known in the Quechua language as Siwar q’inti, these humming‐\nbirds have a place in local folklore as a sign of good luck.\nSparkling violetears are iridescent green with purple markings on the head and chest.\nThe longer purple feathers at their ears extend outward from their heads during dis‐\nplay. Large for hummingbirds, they average about five to six inches long, and weigh\nabout a quarter ounce. Females lay two eggs in a nest of their own making, and incu‐\nbate the eggs. The chicks fledge from the nest at three weeks.\nBecause they live at higher, colder altitudes, sparkling violetears are among the spe‐\ncies of hummingbirds that enter a deep torpor each night to sleep. In this\nhibernation-like state of reduced body functions and a near-acclimation to surround‐\ning cold temperatures, which it then reverses at dawn, the bird is able to survive long,\ncold nights without the food it would otherwise need to stay warm. The mechanisms\nby which they accomplish this complicated feat are the subject of ongoing scientific\nstudies.\nThe sparkling violetear is common across its range, and is rated by the IUCN to be of\nLeast Concern. Many of the animals on O’Reilly covers are endangered; all of them\nare important to the world.\nThe color illustration on the cover is by Karen Montgomery, based on a black-and-\nwhite engraving from Wood’s Natural History. The cover fonts are Gilroy Semibold\nand Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe\nMyriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 2588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "There’s much more  \nwhere this came from.\nExperience books, videos, live online  \ntraining courses, and more from O’Reilly  \nand our 200+ partners—all in one place.\nLearn more at oreilly.com/online-learning\n©2019 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175",
      "content_length": 298,
      "extraction_method": "Direct"
    }
  ]
}