{
  "metadata": {
    "title": "GitOps Cookbook - Red Hat",
    "author": "Natale Vinto and Alex Soto Bueno",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 245,
    "conversion_date": "2025-12-19T17:29:02.180758",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "GitOps Cookbook - Red Hat.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-9)",
      "start_page": 1,
      "end_page": 9,
      "detection_method": "topic_boundary",
      "content": "GitOps Cookbook\n\nKubernetes Automation in Practice\n\nCompliments of\n\nV\n\ni\n\nn\n\nS\n\nt\n\no\n\no\n\nt\n\n&\n\no\n\nNatale Vinto & Alex Soto Bueno\n\nGitOps Cookbook Why are so many companies adopting GitOps for their DevOps and cloud native strategy? This reliable framework is quickly becoming the standard method for deploying apps to Kubernetes. With this practical, developer-oriented book, DevOps engineers, developers, IT architects, and SREs will learn the most useful recipes and examples for following GitOps practices.\n\nThrough their years of experience in application modernization, CI/CD, and automation, authors Alex Soto Bueno and Natale Vinto from Red Hat take you through all the steps necessary for successful hands-on application development and deployment with GitOps. Once you start using the recipes in this book, you’ll have a head start in development cycles on Kubernetes following the GitOps approach.\n\nYou’ll learn how to:\n\nDevelop and deploy applications on Kubernetes\n\nUnderstand the basics of CI/CD and automation on Kubernetes and apply GitOps practices to implement development cycles on the platform\n\nPrepare the app for deployment in multiple environments or multiple Kubernetes clusters\n\nDeploy apps for Kubernetes clusters or for multiple environments using GitOps and Argo CD\n\nCreate Kubernetes-native pipelines with Tekton\n\nProvide and extend DevOps skills for the team working on Kubernetes\n\nGITOPS / KUBERNETES\n\nUS $79.99 ISBN: 978-1-098-13517-1 ISBN: 978-1-492-109747-1\n\nCAN $99.99\n\n“For any IT professional, it can be challenging to stay on top of the newest technologies and best practices in the ever-changing space of software delivery. In this book, Alex and Natale share practical hands-on examples from working across many different organizations on implementing GitOps and CI/CD in a cloud native environment. Pick your favorite recipe and get cooking!”\n\n—Sasha Rosenbaum Principal at Ergonautic\n\nNatale Vinto is a developer advocate at Red Hat, helping customers with their Kubernetes and cloud native strategy.\n\nAlex Soto Bueno is director of developer experience at Red Hat and coauthor of Quarkus Cookbook.\n\nTwitter: @oreillymedia linkedin.com/company/oreilly-media youtube.com/oreillymedia\n\nS\n\no\n\nt\n\no\n\nV\n\ni\n\nn\n\nt\n\no\n\n&\n\nLaunch your Developer Sandbox for Red Hat OpenShift today\n\nred.ht/sandb0x\n\nGitOps Cookbook Kubernetes Automation in Practice\n\nNatale Vinto and Alex Soto Bueno\n\nGitOps Cookbook by Natale Vinto and Alex Soto Bueno\n\nCopyright © 2023 Natale Vinto and Alex Soto Bueno. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nAcquisitions Editor: John Devins Development Editor: Shira Evans Production Editor: Kate Galloway Copyeditor: Kim Cofer Proofreader: Liz Wheeler\n\nIndexer: nSight, Inc. Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Kate Dullea\n\nJanuary 2023:\n\nFirst Edition\n\nRevision History for the First Edition 2023-01-03: First Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492097471 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. GitOps Cookbook, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views. While the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\nThis work is part of a collaboration between O’Reilly and Red Hat. See our statement of editorial independence.\n\n978-1-098-14809-6\n\n[LSI]\n\nTo Alessia and Sofia, the most beautiful chapters of my life. —Natale\n\n[Ada i Alexandra] Sabeu que sou flipants, encara que sortiu del fang. —Alex\n\nTable of Contents\n\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi\n\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\n\n1.\n\nIntroduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1 What Is GitOps? 1 1.2 Why GitOps? 2 1.3 Kubernetes CI/CD 3 1.4 App Deployment with GitOps on Kubernetes 4 1.5 DevOps and Agility 5\n\n2. Requirements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1 Registering for a Container Registry 7 2.2 Registering for a Git Repository 9 2.3 Creating a Local Kubernetes Cluster 12\n\n3. Containers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.1 Building a Container Using Docker 18 3.2 Building a Container Using Dockerless Jib 23 3.3 Building a Container Using Buildah 27 3.4 Building a Container with Buildpacks 32 3.5 Building a Container Using Shipwright and kaniko in Kubernetes 35 3.6 Final Thoughts 42\n\n4. Kustomize. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.1 Using Kustomize to Deploy Kubernetes Resources 44 4.2 Updating the Container Image in Kustomize 50\n\nvii\n\n4.3 Updating Any Kubernetes Field in Kustomize 52 4.4 Deploying to Multiple Environments 57 4.5 Generating ConfigMaps in Kustomize 60 4.6 Final Thoughts 66\n\n5. Helm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 5.1 Creating a Helm Project 68 5.2 Reusing Statements Between Templates 75 5.3 Updating a Container Image in Helm 79 5.4 Packaging and Distributing a Helm Chart 82 5.5 Deploying a Chart from a Repository 84 5.6 Deploying a Chart with a Dependency 88 5.7 Triggering a Rolling Update Automatically 93 5.8 Final Thoughts 98\n\n6. Cloud Native CI/CD. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 6.1 Install Tekton 100 6.2 Create a Hello World Task 107 6.3 Create a Task to Compile and Package an App from Git 108 6.4 Create a Task to Compile and Package an App from Private Git 114 6.5 Containerize an Application Using a Tekton Task and Buildah 117 6.6 Deploy an Application to Kubernetes Using a Tekton Task 122 6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes 125 6.8 Using Tekton Triggers to Compile and Package an Application Git 139 6.10 Update a Kubernetes Resource Using Helm and Create a Pull Request 144 6.11 Use Drone to Create a Pipeline for Kubernetes 148 6.12 Use GitHub Actions for CI 150\n\n7. Argo CD. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 7.1 Deploy an Application Using Argo CD 156 7.2 Automatic Synchronization 162 7.3 Kustomize Integration 166 7.4 Helm Integration 168 7.5 Image Updater 171 7.6 Deploy from a Private Git Repository 178 7.7 Order Kubernetes Manifests 182 7.8 Define Synchronization Windows 187\n\nviii\n\n|\n\nTable of Contents\n\n8. Advanced Topics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 8.1 Encrypt Sensitive Data (Sealed Secrets) 192 8.2 Encrypt Secrets with ArgoCD (ArgoCD + HashiCorp Vault + External\n\nSecret) 195\n\n8.3 Trigger the Deployment of an Application Automatically (Argo CD\n\nWebhooks) 198 8.4 Deploy to Multiple Clusters 200 8.5 Deploy a Pull Request to a Cluster 206 8.6 Use Advanced Deployment Techniques 208\n\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n\nTable of Contents\n\n|\n\nix",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 10-19)",
      "start_page": 10,
      "end_page": 19,
      "detection_method": "topic_boundary",
      "content": "Foreword\n\nA few years ago, during a trip to Milan for a Red Hat event, I ran into a passionate colleague at the Red Hat office. We spoke at length about how customers in Italy adopt containers to speed up application development on OpenShift. While his name slipped my mind at the time, his enthusiasm about the subject didn’t, especially since he was also hospitable enough to take me to an espresso bar near the office to show me what real coffee tastes like. A while later, I was introduced to a developer advocate in a meeting who would speak at a conference about CI/CD using products like OpenShift Pipelines and OpenShift GitOps that my teams delivered at the time. At that moment, I instantly recognized Natale. Many who attended that talk thought it was insightful, given his firsthand grasp of challenges that customers experience when delivering applications and his hands-on approach to technology.\n\nApplication delivery is a complex process involving many systems and teams with numerous handoffs between these parties, often synonymous with delays and back- and-forth talks at each point. Automation has long been a key enabler for improving this process and has become particularly popular within the DevOps movement. Continuous integration, infrastructure as code, and numerous other practices became common in many organizations as they navigated their journey toward adopting DevOps.\n\nMore recently, and coinciding with the increased adoption of Kubernetes, GitOps as a blueprint for implementing a subset of DevOps practices has become an area I fre‐ quently get asked about. While neither the term nor the practices GitOps advocates are new, it does combine. It presents the existing knowledge in a workflow that is simple, easy to understand, and can be implemented in a standard way across many teams.\n\nxi\n\nAlthough the path to adopting the GitOps workflow is simple and concrete, many technical choices need to be made to fit within each organization’s security, compli‐ ance, operational, and other requirements. Therefore, I am particularly thrilled about the existence of this book and the practical guides it provides to assist these teams in making choices that are right for their applications, teams, and organizations.\n\n—Siamak Sadeghianfar Product Management, Red Hat\n\nxii\n\n|\n\nForeword\n\nPreface\n\nWe wrote this book for builders. Whether you are a developer, DevOps engineer, site reliability engineer (SRE), or platform engineer dealing with Kubernetes, you are building some good stuff. We would like to share our experience from what we have learned in the field and in the community about the latest Kubernetes automation insights for pipelines and CI/CD workloads. The book contains a comprehensive list of the most popular available software and tools in the Kubernetes and cloud native ecosystem for this purpose. We aim to provide a list of practical recipes that might help your daily job or are worth exploring further. We are not sticking to a particular technology or project for implementing Kubernetes automation. However, we are opinionated on some of our choices to deliver a concise GitOps pathway.\n\nThe book is organized in sequential chapters, from the basics to advanced topics in the Kubernetes ecosystem, following the GitOps principles. We hope you’ll find these recipes valuable and inspiring for your projects!\n\n• Chapter 1 is an introduction to GitOps principles and why they are continuously becoming more common and essential for any new IT project.\n\nChapter 2 covers the installation requirements to run these recipes in a Kuber‐ • netes cluster. Concepts and tools like Git, Container Registry, Container Run‐ time, and Kubernetes are necessary for this journey.\n\nChapter 3 walks you through a complete overview of containers and why they • are essential for application development and deployment today. Kubernetes is a container-orchestration platform; however, it doesn’t build containers out of the box. Therefore, we’ll provide a list of practical recipes for making container apps with the most popular tools available in the cloud native community.\n\nChapter 4 gives you an overview of Kustomize, a popular tool for managing • Kubernetes resources. Kustomize is interoperable, and you find it often used within CI/CD pipelines.\n\nxiii\n\nChapter 5 explores Helm, a trendy tool to package applications in Kubernetes. • Helm is also a templating system that you can use to deploy apps in CI/CD workloads.\n\n• Chapter 6 walks you through cloud native CI/CD systems for Kubernetes. It gives a comprehensive list of recipes for the continuous integration part with Tekton, the Kubernetes-native CI/CD system. Additionally, it also covers other tools such as Drone and GitHub Actions.\n\n• Chapter 7 kicks off the pure GitOps part of the book as it sticks to the Continu‐ ous Deployment phase with Argo CD, a popular GitOps tool for Kubernetes.\n\nChapter 8 goes into the advanced topics for GitOps with Argo CD, such as • secrets management, progressive application delivery, and multicluster deploy‐ ments. This concludes the most common use cases and architectures you will likely work with today and tomorrow following the GitOps approach.\n\nConventions Used in This Book The following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nConstant width italic\n\nShows text that should be replaced with user-supplied values or by values deter‐ mined by context.\n\nThis element signifies a tip or suggestion.\n\nThis element signifies a general note.\n\nxiv\n\n| Preface\n\nThis element indicates a warning or caution.\n\nUsing Code Examples Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/gitops-cookbook.\n\nIf you have a technical question or a problem using the code examples, please send email to bookquestions@oreilly.com.\n\nThis book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require permission.\n\nWe appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “GitOps Cookbook by Natale Vinto and Alex Soto Bueno (O’Reilly). Copyright 2023 Natale Vinto and Alex Soto Bueno, 978-1-492-09747-1.”\n\nIf you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.\n\nO’Reilly Online Learning\n\nFor more than 40 years, O’Reilly Media has provided technol‐ ogy and business training, knowledge, and insight to help companies succeed.\n\nOur unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\n\nPreface\n\n|\n\nxv\n\nHow to Contact Us Please address comments and questions concerning this book to the publisher:\n\nO’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax)\n\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/gitops-cookbook.\n\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\n\nFor news and information about our books and courses, visit https://oreilly.com.\n\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media.\n\nFollow us on Twitter: https://twitter.com/oreillymedia.\n\nWatch us on YouTube: https://youtube.com/oreillymedia.\n\nAcknowledgments We both want to thank our tech reviewers Peter Miron and Andy Block for their accurate review that helped us improve the reading experience with this book. Thanks also to the people at O’Reilly who helped us during the whole writing cycle. Many thanks to our colleagues Aubrey Muhlac and Colleen Lobner for the great support with publishing this book. Thanks to Kamesh Sampath and all the people who helped us during the early release phases with comments and suggestions that we added to the book—your input is much appreciated!\n\nAlex Soto During these challenging times, I’d like to acknowledge Santa (aquest any sí), Uri (don’t stop the music), Guiri (un ciclista), Gavina, Gabi (thanks for the support), and Edgar and Ester (life is good especially on Friday); my friends Edson, Sebi (the best fellow traveler), Burr (I learned a lot from you), Kamesh, and all the Red Hat developers team, we are the best.\n\nJonathan Vila, Abel Salgado, and Jordi Sola for the fantastic conversations about Java and Kubernetes.\n\nxvi\n\n| Preface\n\nLast but certainly not least, I’d like to acknowledge Anna for being here; my parents Mili and Ramon for buying my first computer; my daughters Ada and Alexandra, “sou les ninetes dels meus ulls.”\n\nNatale Vinto Special thanks to Alessia for the patience and motivation that helped me while writ‐ ing this book. And to my parents for everything they made for me, grazie mamma e papà, you are the best!\n\nPreface\n\n|\n\nxvii\n\nCHAPTER 1 Introduction\n\nWith the advent of practices such as infrastructure as code (IaC), software develop‐ ment has pushed the boundaries of platforms where you can run applications. This becomes more frequent with programmable, API-driven platforms such as public clouds and open source infrastructure solutions. While some years ago developers were only focusing on application source code, today they also have the opportunity to code the infrastructure where their application will run. This gives control and enables automation, which significantly reduces lead time.\n\nA good example is with Kubernetes, a popular open source container workload orchestration platform and the de facto standard for running production applica‐ tions, either on public or private clouds. The openness and extensibility of the platform enables automation, which reduces risks of delivery and increases service quality. Furthermore, this powerful paradigm is extended by another increasingly popular approach called GitOps.\n\n1.1 What Is GitOps? GitOps is a methodology and practice that uses Git repositories as a single source of truth to deliver infrastructure as code. It takes the pillars and approaches from DevOps culture and provides a framework to start realizing the results. The relation‐ ship between DevOps and GitOps is close, as GitOps has become the popular choice to implement and enhance DevOps, platform engineering, and SRE.\n\nGitOps is an agnostic approach, and a GitOps framework can be built with tools such as Git, Kubernetes, and CI/CD solutions. The three main pillars of GitOps are:\n\n• Git is the single source of truth\n\nTreat everything as code •\n\n1\n\n• Operations are performed through Git workflows\n\nThere is an active community around GitOps, and the GitOps Working Group defines a set of GitOps Principles (currently in version 1.0.0) available at OpenGitOps:\n\nDeclarative\n\nA system managed by GitOps must have its desired state expressed declaratively.\n\nVersioned and immutable\n\nThe desired state is stored in a way that enforces immutability and versioning and retains a complete version history.\n\nPulled automatically\n\nSoftware agents automatically pull the desired state declarations from the source.\n\nContinuously reconciled\n\nSoftware agents continuously observe the actual system state and attempt to apply the desired state.\n\n1.2 Why GitOps? Using the common Git-based workflows that developers are familiar with, GitOps expands upon existing processes from application development to deployment, app lifecycle management, and infrastructure configuration.\n\nEvery change throughout the application lifecycle is traced in the Git repository and is auditable. This approach is beneficial for both developers and operations teams as it enhances the ability to trace and reproduce issues quickly, improving overall security. One key point is to reduce the risk of unwanted changes (drift) and correct them before they go into production.\n\nHere is a summary of the benefits of the GitOps adoption in four key aspects:\n\nStandard workflow\n\nUse familiar tools and Git workflows from application development teams\n\nEnhanced security\n\nReview changes beforehand, detect configuration drifts, and take action\n\nVisibility and audit\n\nCapture and trace any change to clusters through Git history\n\nMulticluster consistency\n\nReliably and consistently configure multiple environments and multiple Kuber‐ netes clusters and deployment\n\n2\n\n|\n\nChapter 1: Introduction\n\n1.3 Kubernetes CI/CD Continuous integration (CI) and continuous delivery (CD) are methods used to fre‐ quently deliver apps by introducing automation into the stages of app development. CI/CD pipelines are one of the most common use cases for GitOps.\n\nIn a typical CI/CD pipeline, submitted code checks the CI process while the CD process checks and applies requirements for things like security, infrastructure as code, or any other boundaries set for the application framework. All code changes are tracked, making updates easy while also providing version control should a rollback be needed. CD is the GitOps domain and it works together with the CI part to deploy apps in multiple environments, as you can see in Figure 1-1.\n\nFigure 1-1. Continuous integration and continuous delivery\n\nWith Kubernetes, it’s easy to implement an in-cluster CI/CD pipeline. You can have CI software create the container image representing your application and store it in a container image registry. Afterward, a Git workflow such as a pull request can change the Kubernetes manifests illustrating the deployment of your apps and start a CD sync loop, as shown in Figure 1-2.\n\nFigure 1-2. Application deployment model\n\nThis cookbook will show practical recipes for implementing this model on Kuber‐ netes acting as a CI/CD and GitOps platform.\n\n1.3 Kubernetes CI/CD\n\n|\n\n3",
      "page_number": 10
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 20-30)",
      "start_page": 20,
      "end_page": 30,
      "detection_method": "topic_boundary",
      "content": "1.4 App Deployment with GitOps on Kubernetes As GitOps is an agnostic, platform-independent approach, the application deploy‐ ment model on Kubernetes can be either in-cluster or multicluster. An external GitOps tool can use Kubernetes just as a target platform for deploying apps. At the same time, in-cluster approaches run a GitOps engine inside Kubernetes to deploy apps and sync manifests in one or more Kubernetes clusters.\n\nThe GitOps engine takes care of the CD part of the CI/CD pipeline and accomplishes a GitOps loop, which is composed of four main actions as shown in Figure 1-3:\n\nDeploy\n\nDeploy the manifests from Git.\n\nMonitor\n\nMonitor either the Git repo or the cluster state.\n\nDetect drift\n\nDetect any change from what is described in Git and what is present in the cluster.\n\nTake action\n\nPerform an action that reflects what is on Git (rollback or three-way diff). Git is the source of truth, and any change is performed via a Git workflow.\n\nFigure 1-3. GitOps loop\n\nIn Kubernetes, application deployment using the GitOps approach makes use of at least two Git repositories: one for the app source code, and one for the Kubernetes manifests describing the app’s deployment (Deployment, Service, etc.).\n\nFigure 1-4 illustrates the structure of a GitOps project on Kubernetes.\n\n4\n\n|\n\nChapter 1: Introduction\n\nFigure 1-4. Kubernetes GitOps loop\n\nThe following list outlines the items in the workflow:\n\n1. 1. App source code repository\n\n2. 2. CI pipeline creating a container image\n\n3. 3. Container image registry\n\n4. 4. Kubernetes manifests repository\n\n5. 5. GitOps engine syncing manifests to one or more clusters and detecting drifts\n\n1.5 DevOps and Agility GitOps is a developer-centric approach to continuous delivery and infrastructure operations, and a developer workflow through Git for automating processes. As DevOps is complementary to Agile software development, GitOps is complementary to DevOps for infrastructure automation and application lifecycle management. As you can see in Figure 1-5, it’s a developer workflow for automating operations.\n\nOne of the most critical aspects of the Agile methodology is to reduce the lead time, which is described more abstractly as the time elapsed between identifying a requirement and its fulfillment.\n\n1.5 DevOps and Agility\n\n|\n\n5\n\nFigure 1-5. GitOps development cycle\n\nReducing this time is fundamental and requires a cultural change in IT organizations. Seeing applications live provides developers with a feedback loop to redesign and improve their code and make their projects thrive. Similarly to DevOps, GitOps also requires a cultural adoption in business processes. Every operation, such as applica‐ tion deployment or infrastructure change, is only possible through Git workflows. And sometimes, this means a cultural shift.\n\nThe “Teaching Elephants to Dance (and Fly!)” speech from Burr Sutter gives a clear idea of the context. The elephant is where your organization is today. There are phases of change between traditional and modern environments powered by GitOps tools. Some organizations have the luxury of starting from scratch, but for many businesses, the challenge is teaching their lumbering elephant to dance like a graceful ballerina.\n\n6\n\n|\n\nChapter 1: Introduction\n\nCHAPTER 2 Requirements\n\nThis book is about GitOps and Kubernetes, and as such, you’ll need a container registry to publish the containers built throughout the book (see Recipe 2.1).\n\nAlso, a Git service is required to implement GitOps methodologies; you’ll learn how to register to public Git services like GitHub or GitLab (see Recipe 2.2).\n\nFinally, it would be best to have a Kubernetes cluster to run the book examples. Although we’ll show you how to install Minikube as a Kubernetes cluster (see Recipe 2.3), and the book is tested with Minikube, any Kubernetes installation should work as well.\n\nLet’s prepare your laptop to execute the recipes provided in this book.\n\n2.1 Registering for a Container Registry\n\nProblem You want to create an account for a container registry service so you can store generated containers.\n\nSolution You may need to publish some containers into a public container registry as you work through this book. Use Docker Hub (docker.io) to publish containers.\n\nIf you already have an account with docker.io, you can skip the following steps. Otherwise, keep reading to learn how to sign up for an account.\n\n7\n\nDiscussion Visit DockerHub to sign up for an account. The page should be similar to Figure 2-1.\n\nFigure 2-1. DockerHub registration page\n\nWhen the page is loaded, fill in the form by setting a Docker ID, Email, and Pass‐ word, and click the Sign Up button.\n\nWhen you are registered and your account confirmed, you’ll be ready to publish containers under the previous step’s Docker ID.\n\nSee Also Another popular container registry service is quay.io. It can be used on the cloud (like docker.io) or installed on-premises.\n\nVisit the website to get more information about Quay. The page should be similar to Figure 2-2.\n\n8\n\n|\n\nChapter 2: Requirements\n\nFigure 2-2. Quay registration page\n\n2.2 Registering for a Git Repository\n\nProblem You want to create an account for a Git service so you can store source code in a repository.\n\nSolution You may need to publish some source code into a public Git service in this book. Use GitHub as a Git service to create and fork Git repositories.\n\nIf you already have an account with GitHub, you can skip the following steps, otherwise keep reading to learn how to sign up for an account.\n\nDiscussion Visit the GitHub web page to sign up for an account. The page should be similar to Figure 2-3.\n\n2.2 Registering for a Git Repository\n\n|\n\n9\n\nFigure 2-3. GitHub welcome page to register\n\nWhen the page is loaded, click the Sign up for GitHub button (see Figure 2-3) and follow the instructions. The Sign in page should be similar to Figure 2-4.\n\nFigure 2-4. Sign In GitHub page\n\nWhen you are registered and your account confirmed, you’ll be ready to start creating or forking Git repositories into your GitHub account.\n\n10\n\n|\n\nChapter 2: Requirements\n\nAlso, you’ll need to fork the book source code repository into your account. Click the Fork button shown in Figure 2-5.\n\nFigure 2-5. Fork button\n\nThen select your account in the Owner section, if not selected yet, and click the button “Create fork” button as shown in Figure 2-6.\n\nFigure 2-6. Create fork button\n\n2.2 Registering for a Git Repository\n\n|\n\n11\n\nTo follow along with the example in the following chapters, you can clone this book’s repositories locally. When not mentioned explicitly, we will refer to the examples available in the chapters repo:\n\ngit clone https://github.com/gitops-cookbook/chapters\n\nSee Also Another popular Git service is GitLab. It can be used on the cloud or installed on-premises.\n\nVisit GitLab for more information.\n\n2.3 Creating a Local Kubernetes Cluster\n\nProblem You want to spin up a Kubernetes cluster locally.\n\nSolution In this book, you may need a Kubernetes cluster to run most recipes. Use Minikube to spin up a Kubernetes cluster in your local machine.\n\nDiscussion Minikube uses container/virtualization technology like Docker, Podman, Hyperkit, Hyper-V, KVM, or VirtualBox to boot up a Linux machine with a Kubernetes cluster installed inside.\n\nFor simplicity and to use an installation that will work in most of the platforms, we are going to use VirtualBox as a virtualization system.\n\nTo install VirtualBox (if you haven’t done it yet), visit the home page and click the Download link as shown in Figure 2-7.\n\nFor those using macOS, the following instructions have been tested on a Mac AMD64 with macOS Monterey and VirtualBox 6.1. At the time of writing this book, there were some incompatibilities when using the ARM version or macOS Ventura.\n\n12\n\n|\n\nChapter 2: Requirements\n\nFigure 2-7. VirtualBox home page\n\nSelect the package based on the operating system, download it, and install it on your computer. After installing VirtualBox (we used the 6.1.x version), the next step is to download and spin up a cluster using Minikube.\n\nVisit the GitHub repo, unfold the Assets section, and download the Minikube file that matches your platform specification. For example, in the case of an AMD Mac, you should select minikube-darwin-amd64 as shown in Figure 2-8.\n\nUncompress the file (if necessary) and copy it with the name minikube in a directory accessible by the PATH environment variable such as (/usr/local/bin) in Linux or macOS.\n\nWith VirtualBox and Minikube installed, we can spin up a Kubernetes cluster in the local machine. Let’s install Kubernetes version 1.23.0 as it was the latest version at the time of writing (although any other previous versions can be used as well).\n\n2.3 Creating a Local Kubernetes Cluster\n\n|\n\n13\n\nFigure 2-8. Minikube release page\n\nRun the following command in a terminal window to spin up the Kubernetes cluster with 8 GB of memory assigned:\n\nminikube start --kubernetes-version='v1.23.0' / --driver='virtualbox' --memory=8196 -p gitops\n\nCreates a Kubernetes cluster with version 1.23.0\n\nUses VirtualBox as virtualization tool\n\nCreates a profile name (gitops) to the cluster to refer to it later\n\n14\n\n|\n\nChapter 2: Requirements",
      "page_number": 20
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 31-40)",
      "start_page": 31,
      "end_page": 40,
      "detection_method": "topic_boundary",
      "content": "The output lines should be similar to:\n\n[gitops] Minikube v1.24.0 on Darwin 12.0.1 Using the virtualbox driver based on user configuration Starting control plane node gitops in cluster gitops Creating virtualbox VM (CPUs=2, Memory=8196MB, Disk=20000MB) ... > kubeadm.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s > kubelet.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s > kubectl.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s > kubeadm: 43.11 MiB / 43.11 MiB [---------------] 100.00% 3.46 MiB p/s 13s > kubectl: 44.42 MiB / 44.42 MiB [---------------] 100.00% 3.60 MiB p/s 13s > kubelet: 118.73 MiB / 118.73 MiB [-------------] 100.00% 6.32 MiB p/s 19s\n\n▪ Generating certificates and keys ... ▪ Booting up control plane ... ▪ Configuring RBAC rules ... ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5 ...\n\nVerifying Kubernetes components... Enabled addons: storage-provisioner, default-storageclass\n\n/usr/local/bin/kubectl is version 1.21.0, which may have incompatibilites with Kubernetes 1.23.0. ▪ Want kubectl v1.23.0? Try 'minikube kubectl -- get pods -A' Done! kubectl is now configured to use \"gitops\" cluster and \"default\" namespace by default\n\nStarts the gitops cluster\n\nBoots up the Kubernetes cluster control plane\n\nDetects that we have an old kubectl tool\n\nCluster is up and running\n\nTo align the Kubernetes cluster and Kubernetes CLI tool version, you can download the kubectl 1.23.0 version running from https://dl.k8s.io/release/v1.23.0/bin/darwin/ amd64/kubectl.\n\nYou need to change darwin/amd64 to your specific architecture. For example, in Windows it might be windows/amd64/kubectl.exe.\n\nCopy the kubectl CLI tool in a directory accessible by the PATH environment variable such as (/usr/local/bin) in Linux or macOS.\n\n2.3 Creating a Local Kubernetes Cluster\n\n|\n\n15\n\nSee Also There are other ways to run Kubernetes in a local machine.\n\nOne that is very popular is kind.\n\nAlthough the examples in this book should work in any Kubernetes implementation as only standard resources are used, we’ve only tested with Minikube.\n\n16\n\n|\n\nChapter 2: Requirements\n\nCHAPTER 3 Containers\n\nContainers are a popular and standard format for packaging applications. The format is an open standard promoted by the Open Container Initiative (OCI), an open gov‐ ernance structure for the express purpose of creating open industry standards around container formats and runtimes. The openness of this format ensures portability and interoperability across different operating systems, vendors, platforms, or clouds. Kubernetes runs containerized apps, so before going into the GitOps approach to managing apps on Kubernetes, we provide a list of recipes useful for understanding how to package your application as a container image.\n\nThe first step for creating images is to use a container engine for packaging your application by building a layered structure containing a base OS and additional layers on top such as runtimes, libraries, and applications. Docker is a widespread open source implementation of a container engine and runtime, and it can generate a container image by specifying a manifest called a Dockerfile (see Recipe 3.1).\n\nSince the format is open, it’s possible to create container images with other tools. Docker, a popular container engine, requires the installation and the execution of a daemon that can handle all the operations with the container engine. Developers can use a software development kit (SDK) to interact with the Docker daemon or use dockerless solutions such as JiB to create container images (see Recipe 3.2).\n\nIf you don’t want to rely on a specific programming language or SDK to build container images, you can use another daemonless solution like Buildah (see Recipe 3.3) or Buildpacks (see Recipe 3.4). Those are other popular open source tools for building OCI container images. By avoiding dependencies from the OS, such tools make automation more manageable and portable (see Chapter 6).\n\n17\n\nKubernetes doesn’t provide a native mechanism for building container images. How‐ ever, its highly extensible architecture allows interoperability with external tools and the platform’s extensibility to create container images. Shipwright is an open source framework for building container images on Kubernetes, providing an abstraction that can use tools such as kaniko, Buildpacks, or Buildah (see Recipe 3.5) to create container images.\n\nAt the end of this chapter, you’ll learn how to create OCI-compliant container images from a Dockerfile, either from a host with Docker installed, or using tools such as Buildah and Buildpacks.\n\n3.1 Building a Container Using Docker\n\nProblem You want to create a container image for your application with Docker.\n\nSolution The first thing you need to do is install Docker.\n\nDocker is available for Mac, Windows, and Linux. Download the installer for your operating system and refer to the documentation to start the Docker service.\n\nDevelopers can create a container image by defining a Dockerfile. The best definition for a Dockerfile comes from the Docker documentation itself: “A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.”\n\nContainer images present a layered structure, as you can see in Figure 3-1. Each container image provides the foundation layer for a container, and any update is just an additional layer that can be committed on the foundation.\n\n18\n\n|\n\nChapter 3: Containers\n\nFigure 3-1. Container image layers\n\nYou can create a Dockerfile like the one shown here, which will generate a container image for Python apps. You can also find this example in this book’s repository.\n\nFROM registry.access.redhat.com/ubi8/python-39 ENV PORT 8080 EXPOSE 8080 WORKDIR /usr/src/app\n\nCOPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nENTRYPOINT [\"python\"] CMD [\"app.py\"]\n\nFROM: always start from a base image as a foundational layer. In this case we start from a Universal Base Image (UBI), publicly available based on RHEL 8 with Python 3.9 runtime.\n\nENV: set an environment variable for the app.\n\nEXPOSE: expose a port to the container network, in this case port TCP 8080.\n\nWORKDIR: set a directory inside the container to work with.\n\nCOPY: copy the assets from the source code files on your workstation to the container image layer, in this case, to the WORKDIR.\n\nRUN: run a command inside the container, using the tools already available within the base image. In this case, it runs the pip tool to install dependencies.\n\nENTRYPOINT: define the entry point for your app inside the container. It can be a binary or a script. In this case, it runs the Python interpreter.\n\n3.1 Building a Container Using Docker\n\n|\n\n19\n\nCMD: the command that is used when starting a container. In this case it uses the name of the Python app app.py.\n\nYou can now create your container image with the following command:\n\ndocker build -f Dockerfile -t quay.io/gitops-cookbook/pythonapp:latest\n\nChange the container image name with the your registry, user, and repo. Example: quay.io/youruser/yourrepo:latest. See Chap‐ ter 2 for how to create a new account on registries such as Quay.io.\n\nYour container image is building now. Docker will fetch existing layers from a public container registry (DockerHub, Quay, Red Hat Registry, etc.) and add a new layer with the content specified in the Dockerfile. Such layers could also be available locally, if already downloaded, in special storage called a container cache or Docker cache.\n\nSTEP 1: FROM registry.access.redhat.com/ubi8/python-39 Getting image source signatures Copying blob adffa6963146 done Copying blob 4125bdfaec5e done Copying blob 362566a15abb done Copying blob 0661f10c38cc done Copying blob 26f1167feaf7 done Copying config a531ae7675 done Writing manifest to image destination Storing signatures STEP 2: ENV PORT 8080 --> 6dbf4ac027e STEP 3: EXPOSE 8080 --> f78357fe402 STEP 4: WORKDIR /usr/src/app --> 547bf8ca5c5 STEP 5: COPY requirements.txt ./ --> 456cab38c97 STEP 6: RUN pip install --no-cache-dir -r requirements.txt Collecting Flask Downloading Flask-2.0.2-py3-none-any.whl (95 kB) |████████████████████████████████| 95 kB 10.6 MB/s Collecting itsdangerous>=2.0 Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB) Collecting Werkzeug>=2.0 Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB) |████████████████████████████████| 288 kB 1.7 MB/s Collecting click>=7.1.2 Downloading click-8.0.3-py3-none-any.whl (97 kB) |████████████████████████████████| 97 kB 31.9 MB/s Collecting Jinja2>=3.0 Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB) |████████████████████████████████| 133 kB 38.8 MB/s STEP 7: COPY . .\n\n20\n\n|\n\nChapter 3: Containers\n\n--> 3e6b73464eb STEP 8: ENTRYPOINT [\"python\"] --> acabca89260 STEP 9: CMD [\"app.py\"] STEP 10: COMMIT quay.io/gitops-cookbook/pythonapp:latest --> 52e134d39af 52e134d39af013a25f3e44d25133478dc20b46626782762f4e46b1ff6f0243bb\n\nYour container image is now available in your Docker cache and ready to be used. You can verify its presence with this command:\n\ndocker images\n\nYou should get the list of available container images from the cache in output. Those could be images you have built or downloaded with the docker pull command:\n\nREPOSITORY TAG IMAGE ID CREATED↳ SIZE quay.io/gitops-cookbook/pythonapp latest 52e134d39af0 6 minutes ago↳ 907 MB\n\nOnce your image is created, you can consume it locally or push it to a public container registry to be consumed elsewhere, like from a CI/CD pipeline.\n\nYou need to first log in to your public registry. In this example, we are using Quay:\n\ndocker login quay.io\n\nYou should get output similar to this:\n\nLogin Succeeded!\n\nThen you can push your container image to the registry:\n\ndocker push quay.io/gitops-cookbook/pythonapp:latest\n\nAs confirmed, you should get output similar to this:\n\nGetting image source signatures Copying blob e6e8a2c58ac5 done Copying blob 3ba8c926eef9 done Copying blob 558b534f4e1b done Copying blob 25f82e0f4ef5 done Copying blob 7b17276847a2 done Copying blob 352ba846236b done Copying blob 2de82c390049 done Copying blob 26525e00a8d8 done Copying config 52e134d39a done Writing manifest to image destination Copying config 52e134d39a [--------------------------------------] 0.0b / 5.4KiB Writing manifest to image destination Storing signatures\n\n3.1 Building a Container Using Docker\n\n|\n\n21\n\nDiscussion You can create container images in this way with Docker from your workstation or any host where the Docker service/daemon is running.\n\nAdditionally, you can use functionalities offered by a public regis‐ try such as Quay.io that can directly create the container image from a Dockerfile and store it to the registry.\n\nThe build requires access to all layers, thus an internet connection to the registries storing base layers is needed, or at least having them in the container cache. Docker has a layered structure where any change to your app is committed on top of the existing layers, so there’s no need to download all the layers each time since it will add only deltas for each new change.\n\nContainer images typically start from a base OS layer such as Fedora, CentOS, Ubuntu, Alpine, etc. However, they can also start from scratch, an empty layer for super-minimal images contain‐ ing only the app’s binary. See the scratch documentation for more info.\n\nIf you want to run your previously created container image, you can do so with this command:\n\ndocker run -p 8080:8080 -ti quay.io/gitops-cookbook/pythonapp:latest\n\ndocker run has many options to start your container. The most common are:\n\np\n\nBinds the port of the container with the port of the host running such container.\n\nt\n\nAttaches a TTY to the container.\n\ni\n\nGoes into an interactive mode.\n\nd\n\nGoes in the background, printing a hash that you can use to interact asynchro‐ nously with the running container.\n\n22\n\n|\n\nChapter 3: Containers\n\nThe preceding command will start your app in the Docker network and bind it to port 8080 of your workstation:\n\nServing Flask app 'app' (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on all addresses. WARNING: This is a development server. Do not use it in a production deployment. * Running on http://10.0.2.100:8080/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 103-809-567 curl http://localhost:8080\n\nServing Flask app 'app' (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on all addresses. WARNING: This is a development server. Do not use it in a production deployment. * Running on http://10.0.2.100:8080/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 103-809-567 curl http://localhost:8080\n\nYou should get output like this:\n\nHello, World!\n\nSee Also\n\n• Best practices for writing Dockerfiles\n\n• Manage Docker images\n\n3.2 Building a Container Using Dockerless Jib\n\nProblem You are a software developer, and you want to create a container image without installing Docker or any additional software on your workstation.\n\nSolution As discussed in Recipe 3.1, you need to install the Docker engine to create container images. Docker requires permissions to install a service running as a daemon, thus a privileged process in your operating system. Today, dockerless solutions are also available for developers; a popular one is Jib.\n\nJib is an open source framework for Java made by Google to build OCI-compliant container images, without the need for Docker or any container runtime. Jib comes as a library that Java developers can import in their Maven or Gradle projects. This means you can create a container image for your app without writing or maintaining any Dockerfiles, delegating this complexity to Jib.\n\n3.2 Building a Container Using Dockerless Jib\n\n|\n\n23\n\nWe see the benefits from this approach as the following:1\n\nPure Java\n\nNo Docker or Dockerfile knowledge is required. Simply add Jib as a plug-in, and it will generate the container image for you.\n\nSpeed\n\nThe application is divided into multiple layers, splitting dependencies from classes. There’s no need to rebuild the container image like for Dockerfiles; Jib takes care of modifying the layers that changed.\n\nReproducibility\n\nUnnecessary updates are not triggered because the same contents generate the same image.\n\nThe easiest way to kickstart a container image build with Jib on existing Maven is by adding the plug-in via the command line:\n\nmvn compile com.google.cloud.tools:jib-maven-plugin:3.2.0:build -Dimage=<MY IMAGE>\n\nAlternatively, you can do so by adding Jib as a plug-in into your pom.xml:\n\n<project> ... <build> <plugins> ... <plugin> <groupId>com.google.cloud.tools</groupId> <artifactId>jib-maven-plugin</artifactId> <version>3.2.0</version> <configuration> <to> <image>myimage</image> </to> </configuration> </plugin> ... </plugins> </build> ... </project>\n\nIn this way, you can also manage other settings such as authentication or parameters for the build.\n\nLet’s now add Jib to an existing Java application, a Hello World application in Spring Boot that you can find in the book’s repository.\n\n1 For a presentation about Jib, see Appu Goundan and Qingyang Chen’s presentation from Velocity San Jose\n\n2018.\n\n24\n\n|\n\nChapter 3: Containers",
      "page_number": 31
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 41-49)",
      "start_page": 41,
      "end_page": 49,
      "detection_method": "topic_boundary",
      "content": "Run the following command to create a container image without using Docker, and push it directly to a container registry. In this example, we use Quay.io, and we will store the container image at quay.io/gitops-cookbook/jib-example:latest, so you will need to provide your credentials for the registry:\n\nmvn compile com.google.cloud.tools:jib-maven-plugin:3.2.0:build \\ -Dimage=quay.io/gitops-cookbook/jib-example:latest \\ -Djib.to.auth.username=<USERNAME> \\ -Djib.to.auth.password=<PASSWORD>\n\nThe authentication here is handled with command-line options, but Jib can manage existing authentication with Docker CLI or read credentials from your settings.xml file.\n\nThe build takes a few moments, and the result is a Java-specific container image, based on the adoptOpenJDK base image, built locally and pushed directly to a regis‐ try. In this case, to Quay.io:\n\n[INFO] Scanning for projects... [INFO] [INFO] --------------------------< com.redhat:hello >-------------------------- [INFO] Building hello 0.0.1-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- ... [INFO] Containerizing application to quay.io/gitops-cookbook/jib-example... [INFO] Using credentials from <to><auth> for quay.io/gitops-cookbook/jib-example [INFO] The base image requires auth. Trying again for eclipse-temurin:11-jre... [INFO] Using base image with digest:↳ sha256:83d92ee225e443580cc3685ef9574582761cf975abc53850c2bc44ec47d7d943O] [INFO] [INFO] Container entrypoint set to [java, -cp, @/app/jib-classpath-file,↳ com.redhat.hello.HelloApplication]FO] [INFO] [INFO] Built and pushed image as quay.io/gitops-cookbook/jib-example [INFO] Executing tasks: [INFO] [==============================] 100,0% complete [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 41.366 s [INFO] Finished at: 2022-01-25T19:04:09+01:00 [INFO] ------------------------------------------------------------------------\n\nIf you have Docker and run the command docker images, you won’t see this image in your local cache!\n\n3.2 Building a Container Using Dockerless Jib\n\n|\n\n25\n\nDiscussion Your container image is not present in your local cache, as you don’t need any container runtime to build images with Jib. You won’t see it with the docker images command, but you can pull it from the public container registry afterward, and it will store it in your cache.\n\nThis approach is suitable for development velocity and automation, where the CI system doesn’t need to have Docker installed on the nodes where it runs. Jib can create the container image without any Dockerfiles. Additionally, it can push the image to a container registry.\n\nIf you also want to store it locally from the beginning, Jib can connect to Docker hosts and do it for you.\n\nYou can pull your container image from the registry to try it:\n\ndocker run -p 8080:8080 -ti quay.io/gitops-cookbook/jib-example\n\nTrying to pull quay.io/gitops-cookbook/jib-example:latest... Getting image source signatures Copying blob ea362f368469 done Copying blob d5cc550bb6a0 done Copying blob bcc17963ea24 done Copying blob 9b46d5d971fa done Copying blob 51f4f7c353f0 done Copying blob 43b2cdfa19bb done Copying blob fd142634d578 done Copying blob 78c393914c97 done Copying config 346462b8d3 done Writing manifest to image destination Storing signatures\n\n. ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.6.3)\n\n2022-01-25 18:36:24.762 INFO 1 --- [ main] com.redhat.hello.HelloApplication↳ : Starting HelloApplication using Java 11.0.13 on a719cf76f440 with PID 1↳ (/app/classes started by root in /) 2022-01-25 18:36:24.765 INFO 1 --- [ main] com.redhat.hello.HelloApplication↳ : No active profile set, falling back to default profiles: default 2022-01-25 18:36:25.700 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWeb- Server↳ : Tomcat initialized with port(s): 8080 (http) 2022-01-25 18:36:25.713 INFO 1 --- [ main] o.apache.catalina.core.StandardSer- vice↳ : Starting service [Tomcat] 2022-01-25 18:36:25.713 INFO 1 --- [ main] org.apache.catalina.core.StandardEn-\n\n26\n\n|\n\nChapter 3: Containers\n\ngine↳ : Starting Servlet engine: [Apache Tomcat/9.0.56] 2022-01-25 18:36:25.781 INFO 1 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/]↳ : Initializing Spring embedded WebApplicationContext 2022-01-25 18:36:25.781 INFO 1 --- [ main] w.s.c.ServletWebServerApplicationCon- text↳ : Root WebApplicationContext: initialization completed in 947 ms 2022-01-25 18:36:26.087 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWeb- Server↳ : Tomcat started on port(s): 8080 (http) with context path '' 2022-01-25 18:36:26.096 INFO 1 --- [ main] com.redhat.hello.HelloApplication↳ : Started HelloApplication in 1.778 seconds (JVM running for 2.177)\n\nGet the hello endpoint:\n\ncurl localhost:8080/hello\n\n{\"id\":1,\"content\":\"Hello, World!\"}\n\nSee Also\n\n• Using Jib with Quarkus projects\n\n3.3 Building a Container Using Buildah\n\nProblem Sometimes installing or managing Docker is not possible. Dockerless solutions for creating container images are useful in use cases such as local development or CI/CD systems.\n\nSolution The OCI specification is an open standard, and this favors multiple open source implementations for the container engine and the container image building mecha‐ nism. Two growing popular examples today are Podman and Buildah.\n\nWhile Docker uses a single monolithic application for creating, running, and shipping container images, the codebase for con‐ tainer management functionalities here has been split between different projects like Podman, Buildah, and Skopeo. Podman sup‐ port is already available on Mac and Windows, however Buildah is currently only available on Linux or Linux subsystems such as WSL2 for Windows. See the documentation to install it on your workstation.\n\nThose are two complementary open source projects and command-line tools that work on OCI containers and images; however, they differ in their specialization.\n\n3.3 Building a Container Using Buildah\n\n|\n\n27\n\nWhile Podman specializes in commands and functions that help you to maintain and modify container images, such as pulling, tagging, and pushing, Buildah specializes in building container images. Decoupling functions in different processes is done by design, as the authors wanted to move from the single privileged process Docker model to a lightweight, rootless, daemonless, and decoupled set of tools to improve agility and security.\n\nFollowing the same approach, you find Skopeo, a tool used to move container images; and CRI-O, a container engine complaint with the Kubernetes container runtime interface for running appli‐ cations.\n\nBuildah supports the Dockerfile format, but its goal is to provide a lower-level interface to build container images without requiring a Dockerfile. Buildah is a daemonless solution that can create images inside a container without mounting the Docker socket. This functionality improves security and portability since it’s easy to add Buildah builds on the fly to a CI/CD pipeline where the Linux or Kubernetes nodes do not require a Docker installation.\n\nAs we discussed, you can create a container image with or without a Dockerfile. Let’s now create a simple HTTPD container image without a Dockerfile.\n\nYou can start from any base image such as CentOS:\n\nbuildah from centos\n\nYou should get output similar to this:\n\nResolved short name \"centos\" to a recorded short-name alias↳ (origin: /etc/containers/registries.conf.d/shortnames.conf) Getting image source signatures Copying blob 926a85fb4806 done Copying config 2f3766df23 done Writing manifest to image destination Storing signatures centos-working-container\n\nSimilarly to Docker and docker images, you can run the com‐ mand buildah containers to get the list of available images from the container cache. If you also have installed Podman, this is similar to podman images.\n\nIn this case, the container image ID is centos-working-container, and you can refer to it for creating the other layers.\n\nNow let’s install the httpd package inside a new layer:\n\n28\n\n|\n\nChapter 3: Containers\n\nbuildah run centos-working-container yum install httpd -y\n\nYou should get output similar to this:\n\nCentOS Linux 8 - AppStream 9.0 MB/s | 8.4 MB 00:00 CentOS Linux 8 - BaseOS 436 kB/s | 4.6 MB 00:10 CentOS Linux 8 - Extras 23 kB/s | 10 kB 00:00 Dependencies resolved. =============================================================================== Package Arch Version Repository Size =============================================================================== Installing: httpd x86_64 2.4.37-43.module_el8.5.0+1022+b541f3b1 Installing dependencies: apr x86_64 1.6.3-12.el8 apr-util x86_64 1.6.1-6.el8 brotli x86_64 1.0.6-3.el8 centos-logos-httpd noarch 85.8-2.el8 httpd-filesystem noarch 2.4.37-43.module_el8.5.0+1022+b541f3b1 httpd-tools x86_64 2.4.37-43.module_el8.5.0+1022+b541f3b1 mailcap noarch 2.1.48-3.el8 mod_http2 x86_64 1.15.7-3.module_el8.4.0+778+c970deab Installing weak dependencies: apr-util-bdb x86_64 1.6.1-6.el8 apr-util-openssl x86_64 1.6.1-6.el8 Enabling module streams: ... Complete!\n\nNow let’s copy a welcome HTML page inside the container running HTTPD. You can find the source code in this book’s repo:\n\n<html> <head> <title>GitOps CookBook example</title> </head> <body> <h1>Hello, World!</h1> </body> </html>\n\nbuildah copy centos-working-container index.html /var/www/html/index.html\n\nFor each new layer added, you should get output with the new container image hash, similar to the following:\n\n78c6e1dcd6f819581b54094fd38a3fd8f170a2cb768101e533c964e04aacab2e\n\nbuildah config --entrypoint \"/usr/sbin/httpd -DFOREGROUND\" centos-working-container\n\nbuildah commit centos-working-container quay.io/gitops-cookbook/gitops-website\n\n3.3 Building a Container Using Buildah\n\n|\n\n29\n\nYou should get output similar to this:\n\nGetting image source signatures Copying blob 618ce6bf40a6 skipped: already exists Copying blob eb8c13ba832f done Copying config b825e91208 done Writing manifest to image destination Storing signatures b825e91208c33371e209cc327abe4f53ee501d5679c127cd71c4d10cd03e5370\n\nYour container image is now in the container cache, ready to run or push to another registry.\n\nAs mentioned before, Buildah can also create container images from a Dockerfile. Let’s make the same container image from the Dockerfile listed here:\n\nFROM centos:latest RUN yum -y install httpd COPY index.html /var/www/html/index.html EXPOSE 80 CMD [\"/usr/sbin/httpd\", \"-DFOREGROUND\"]\n\nbuildah bud -f Dockerfile -t quay.io/gitops-cookbook/gitops-website\n\nSTEP 1: FROM centos:latest Resolved short name \"centos\" to a recorded short-name alias↳ (origin: /etc/containers/registries.conf.d/shortnames.conf) Getting image source signatures Copying blob 926a85fb4806 done Copying config 2f3766df23 done Writing manifest to image destination Storing signatures STEP 2: RUN yum -y install httpd CentOS Linux 8 - AppStream 9.6 MB/s | 8.4 MB 00:00 CentOS Linux 8 - BaseOS 7.5 MB/s | 4.6 MB 00:00 CentOS Linux 8 - Extras 63 kB/s | 10 kB 00:00 Dependencies resolved. ... Complete! STEP 3: COPY index.html /var/www/html/index.html STEP 4: EXPOSE 80 STEP 5: CMD [\"/usr/sbin/httpd\", \"-DFOREGROUND\"] STEP 6: COMMIT quay.io/gitops-cookbook/gitops-website Getting image source signatures Copying blob 618ce6bf40a6 skipped: already exists Copying blob 1be523a47735 done Copying config 3128caf147 done Writing manifest to image destination Storing signatures --> 3128caf1475 3128caf147547e43b84c13c241585d23a32601f2c2db80b966185b03cb6a8025\n\nIf you have also installed Podman, you can run it this way:\n\npodman run -p 8080:80 -ti quay.io/gitops-cookbook/gitops-website\n\n30\n\n|\n\nChapter 3: Containers\n\nThen you can test it by opening the browser on http://localhost:8080.\n\nDiscussion With Buildah, you have the opportunity to create container images from scratch or starting from a Dockerfile. You don’t need to install Docker, and everything is designed around security: rootless mechanism, daemonless utilities, and more refined control of creating image layers.\n\nBuildah can also build images from scratch, thus it creates an empty layer similar to the FROM scratch Dockerfile statement. This aspect is useful for creating very lightweight images containing only the packages needed to run your application, as you can see in Figure 3-2.\n\nFigure 3-2. Buildah image shrink\n\nA good example use case for a scratch build is considering the development images versus staging or production images. During development, container images may require a compiler and other tools. However, in production, you may only need the runtime or your packages.\n\nSee Also\n\n• Running Buildah inside a container\n\n3.3 Building a Container Using Buildah\n\n|\n\n31\n\n3.4 Building a Container with Buildpacks\n\nProblem Creating container image by using Dockerfiles can be challenging at scale. You want a tool complementing Docker that can inspect your application source code to create container images without writing a Dockerfile.\n\nSolution Cloud Native Buildpacks is an open source project that provides a set of executables to inspect your app source code and to create a plan to build and run your applica‐ tion.\n\nBuildpacks can create OCI-compliant container images without a Dockerfile, starting from the app source code, as you can see in Figure 3-3.\n\nFigure 3-3. Buildpacks builds\n\nThis mechanism consists of two phases:\n\nDetection\n\nBuildpacks tooling will navigate your source code to discover which program‐ ming language or framework is used (e.g., POM, NPM files, Python require‐ ments, etc.) and assign a suitable buildpack for the build.\n\nBuilding\n\nOnce a buildpack is found, the source is compiled and Buildpacks creates a container image with the appropriate entry point and startup scripts.\n\nTo use Buildpacks, you have to download the pack CLI for your operating system (Mac, Windows, Linux), and also have Docker installed.\n\nOn macOS, pack is available through Homebrew as follows:\n\nbrew install buildpacks/tap/pack\n\n32\n\n|\n\nChapter 3: Containers\n\nNow let’s start creating our container image with Buildpacks from a sample Node.js app. You can find the app source code in this book’s repository:\n\ncd chapters/ch03/nodejs-app\n\nThe app directory structure contains a package.json file, a manifest listing Node.js packages required for this build, which helps Buildpacks understand which buildpack to use.\n\nYou can verify it with this command:\n\npack builder suggest\n\nYou should get output similar to this:\n\nSuggested builders: Google: gcr.io/buildpacks/builder:v1↳ Ubuntu 18 base image with buildpacks for .NET, Go, Java, Node.js,↳ and Python Heroku: heroku/buildpacks:18↳ Base builder for Heroku-18 stack, based on ubuntu:18.04 base↳ image Heroku: heroku/buildpacks:20↳ Base builder for Heroku-20 stack, based on ubuntu:20.04 base↳ image Paketo Buildpacks: paketobuildpacks/builder:base↳ Ubuntu bionic base image with buildpacks for Java, .NET Core,↳ Node.js, Go, Python, Ruby, NGINX and Procfile Paketo Buildpacks: paketobuildpacks/builder:full↳ Ubuntu bionic base image with buildpacks for Java, .NET Core,↳ Node.js, Go, Python, PHP, Ruby, Apache HTTPD, NGINX and Procfile Paketo Buildpacks: paketobuildpacks/builder:tiny↳ Tiny base image (bionic build image, distroless-like run image)↳ with buildpacks for Java, Java Native Image and Go\n\nNow you can decide to pick one of the suggested buildpacks. Let’s try the paketo buildpacks/builder:base, which also contains the Node.js runtime:\n\npack build nodejs-app --builder paketobuildpacks/builder:base\n\nRun pack builder inspect paketobuildpacks/builder:base to know the exact content of libraries and frameworks available in this buildpack.\n\nThe building process should start accordingly, and after a while, it should finish, and you should get output similar to this:\n\nbase: Pulling from paketobuildpacks/builder bf99a8b93828: Pulling fs layer ... Digest: sha256:7034e52388c11c5f7ee7ae8f2d7d794ba427cc2802f687dd9650d96a70ac0772\n\n3.4 Building a Container with Buildpacks\n\n|\n\n33",
      "page_number": 41
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 50-58)",
      "start_page": 50,
      "end_page": 58,
      "detection_method": "topic_boundary",
      "content": "Status: Downloaded newer image for paketobuildpacks/builder:base base-cnb: Pulling from paketobuildpacks/run bf99a8b93828: Already exists 9d58a4841c3f: Pull complete 77a4f59032ac: Pull complete 24e58505e5e0: Pull complete Digest: sha256:59aa1da9db6d979e21721e306b9ce99a7c4e3d1663c4c20f74f9b3876cce5192 Status: Downloaded newer image for paketobuildpacks/run:base-cnb ===> ANALYZING Previous image with name \"nodejs-app\" not found ===> DETECTING 5 of 10 buildpacks participating paketo-buildpacks/ca-certificates 3.0.1 paketo-buildpacks/node-engine 0.11.2 paketo-buildpacks/npm-install 0.6.2 paketo-buildpacks/node-module-bom 0.2.0 paketo-buildpacks/npm-start 0.6.1 ===> RESTORING ===> BUILDING ... Paketo NPM Start Buildpack 0.6.1 Assigning launch processes web: node server.js\n\n===> EXPORTING Adding layer 'paketo-buildpacks/ca-certificates:helper' Adding layer 'paketo-buildpacks/node-engine:node' Adding layer 'paketo-buildpacks/npm-install:modules' Adding layer 'launch.sbom' Adding 1/1 app layer(s) Adding layer 'launcher' Adding layer 'config' Adding layer 'process-types' Adding label 'io.buildpacks.lifecycle.metadata' Adding label 'io.buildpacks.build.metadata' Adding label 'io.buildpacks.project.metadata' Setting default process type 'web' Saving nodejs-app... *** Images (82b805699d6b): nodejs-app Adding cache layer 'paketo-buildpacks/node-engine:node' Adding cache layer 'paketo-buildpacks/npm-install:modules' Adding cache layer 'paketo-buildpacks/node-module-bom:cyclonedx-node-module' Successfully built image nodejs-app\n\nNow let’s run it with Docker:\n\ndocker run --rm -p 3000:3000 nodejs-app\n\nYou should get output similar to this:\n\nServer running at http://0.0.0.0:3000/\n\n34\n\n|\n\nChapter 3: Containers\n\nView the running application:\n\ncurl http://localhost:3000/\n\nYou should get output similar to this:\n\nHello Buildpacks!\n\nDiscussion Cloud Native Buildpacks is an incubating project in the Cloud Native Computing Foundation (CNCF), and it supports both Docker and Kubernetes. On Kubernetes, it can be used with Tekton, a Kubernetes-native CI/CD system that can run Buildpacks as a Tekton Task to create container images. It recently adopted the Boson Project to provide a functions-as-a-service (FaaS) experience on Kubernetes with Knative, by enabling the build of functions via buildpacks.\n\nSee Also\n\n• Using Buildpacks with Tekton Pipelines\n\n• FaaS Knative Boson project’s buildpacks\n\n3.5 Building a Container Using Shipwright and kaniko in Kubernetes\n\nProblem You need to create a container image, and you want to do it with Kubernetes.\n\nSolution Kubernetes is well known as a container orchestration platform to deploy and manage apps. However, it doesn’t include support for building container images out-of-the-box. Indeed, according to Kubernetes documentation: “(Kubernetes) Does not deploy source code and does not build your application. Continuous Integration, Delivery, and Deployment (CI/CD) workflows are determined by organization cul‐ tures and preferences as well as technical requirements.”\n\nAs mentioned, one standard option is to rely on CI/CD systems for this purpose, like Tekton (see Chapter 6). Another option is to use a framework to manage builds with many underlying tools, such as the one we discussed in the previous recipes. One example is Shipwright.\n\n3.5 Building a Container Using Shipwright and kaniko in Kubernetes\n\n|\n\n35\n\nShipwright is an extensible framework for building container images on Kubernetes. It supports popular tools such as Buildah, Cloud Native Buildpacks, and kaniko. It uses Kubernetes-style APIs, and it runs workloads using Tekton.\n\nThe benefit for developers is a simplified approach for building container images, by defining a minimal YAML file that does not require any previous knowledge of containers or container engines. This approach makes this solution agnostic and highly integrated with the Kubernetes API ecosystem.\n\nThe first thing to do is to install Shipwright to your Kubernetes cluster, say kind or Minikube (see Chapter 2), following the documentation or from OperatorHub.io.\n\nUsing Operators and Operator Lifecycle Manager (OLM) gives consistency for installing/uninstalling software on Kubernetes, along with dependency management and lifecycle control. For instance, the Tekton Operator dependency is automatically resolved and installed if you install Shipwright via the Operator. Check the OLM documentation for details with this approach.\n\nLet’s follow the standard procedure from the documentation. First you need to install the Tekton dependency. At the time of writing this book, it is version 0.30.0:\n\nkubectl apply -f \\ https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.30.0/ release.yaml\n\nThen you install Shipwright. At the time of writing this book, it is version 0.7.0:\n\nkubectl apply -f \\ https://github.com/shipwright-io/build/releases/download/v0.7.0/release.yaml\n\nFinally, you install Shipwright build strategies:\n\nkubectl apply -f \\ https://github.com/shipwright-io/build/releases/download/v0.7.0/sample- strategies.yaml\n\nOnce you have installed Shipwright, you can start creating your container image build using one of these tools:\n\nkaniko •\n\nCloud Native Buildpacks •\n\n• BuildKit\n\n• Buildah\n\nLet’s explore kaniko.\n\n36\n\n|\n\nChapter 3: Containers\n\nkaniko is another dockerless solution to build container images from a Dockerfile inside a container or Kubernetes cluster. Shipwright brings additional APIs to Kuber‐ netes to use tools such as kaniko to create container images, acting as an abstract layer that can be considered an extensible building system for Kubernetes.\n\nLet’s explore the APIs that are defined from Cluster Resource Definitions (CRDs):\n\nClusterBuildStrategy\n\nRepresents the type of build to execute.\n\nBuild\n\nRepresents the build. It includes the specification of one ClusterBuildStrategy object.\n\nBuildRun\n\nRepresents a running build. The build starts when this object is created.\n\nRun the following command to check all available ClusterBuildStrategy (CBS) objects:\n\nkubectl get cbs\n\nYou should get a list of available CBSs to consume:\n\nNAME AGE buildah 26s buildkit 26s buildpacks-v3 26s buildpacks-v3-heroku 26s kaniko 26s kaniko-trivy 26s ko 26s source-to-image 26s source-to-image-redhat 26s\n\nThis CRD is cluster-wide, available for all namespaces. If you don’t see any items, please install the Shipwright build strategies as dis‐ cussed previously.\n\nShipwright will generate a container image on the Kubernetes nodes container cache, and then it can push it to a container registry.\n\nYou need to provide the credentials to push the image to the registry in the form of a Kubernetes Secret. For example, if you use Quay you can create one like the following:\n\nREGISTRY_SERVER=quay.io REGISTRY_USER=<your_registry_user> REGISTRY_PASSWORD=<your_registry_password>\n\n3.5 Building a Container Using Shipwright and kaniko in Kubernetes\n\n|\n\n37\n\nEMAIL=<your_email> kubectl create secret docker-registry push-secret \\ --docker-server=$REGISTRY_SERVER \\ --docker-username=$REGISTRY_USER \\ --docker-password=$REGISTRY_PASSWORD \\ --docker-email=$EMAIL\n\nWith Quay, you can use an encrypted password instead of using your account password. See the documentation for more details.\n\nNow let’s create a build-kaniko.yaml file containing the Build object that will use kaniko to containerize a Node.js sample app. You can find the source code in this book’s repository:\n\napiVersion: shipwright.io/v1alpha1 kind: Build metadata: name: buildpack-nodejs-build spec: source: url: https://github.com/shipwright-io/sample-nodejs contextDir: docker-build strategy: name: kaniko kind: ClusterBuildStrategy output: image: quay.io/gitops-cookbook/sample-nodejs:latest credentials: name: push-secret\n\nRepository to grab the source code from.\n\nThe directory where the source code is present.\n\nThe ClusterBuildStrategy to use.\n\nThe destination of the resulting container image. Change this with your con‐ tainer registry repo.\n\nThe secret to use to authenticate to the container registry and push the image.\n\n38\n\n|\n\nChapter 3: Containers\n\nNow, let’s create the Build object:\n\nkubectl create -f build-kaniko.yaml\n\nYou should get output similar to this:\n\nbuild.shipwright.io/kaniko-nodejs-build created\n\nLet’s list the available builds:\n\nkubectl get builds\n\nYou should get output similar to the following:\n\nNAME REGISTERED REASON BUILDSTRATEGYKIND↳ BUILDSTRATEGYNAME CREATIONTIME kaniko-nodejs-build True Succeeded ClusterBuildStrategy↳ kaniko 13s\n\nAt this point, your Build is REGISTERED, but it’s not started yet. Let’s create the following object in order to start it:\n\napiVersion: shipwright.io/v1alpha1 kind: BuildRun metadata: generateName: kaniko-nodejs-buildrun- spec: buildRef: name: kaniko-nodejs-build\n\nkubectl create -f buildrun.yaml\n\nIf you check the list of running pods, you should see one being created:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS↳ AGE kaniko-nodejs-buildrun-b9mmb-qbrgl-pod-dk7xt 0/3 PodInitializing 0↳ 19s\n\nWhen the STATUS changes, the build will start, and you can track the progress by checking the logs from the containers used by this pod to run the build in multiple steps:\n\nstep-source-default\n\nThe first step, used to get the source code\n\nstep-build-and-push\n\nThe step to run the build, either from source code or from a Dockerfile like in this case with kaniko\n\nstep-results\n\nThe result of the build\n\n3.5 Building a Container Using Shipwright and kaniko in Kubernetes\n\n|\n\n39\n\nLet’s check the logs of the building phase:\n\nkubectl logs -f kaniko-nodejs-buildrun-b9mmb-qbrgl-pod-dk7xt -c step-build-and-push\n\nINFO[0001] Retrieving image manifest ghcr.io/shipwright-io/shipwright-samples/ node:12 INFO[0001] Retrieving image ghcr.io/shipwright-io/shipwright-samples/node:12↳ from registry ghcr.io INFO[0002] Built cross stage deps: map[] INFO[0002] Retrieving image manifest ghcr.io/shipwright-io/shipwright-samples/ node:12 INFO[0002] Returning cached image manifest INFO[0002] Executing 0 build triggers INFO[0002] Unpacking rootfs as cmd COPY . /app requires it. INFO[0042] COPY . /app INFO[0042] Taking snapshot of files... INFO[0042] WORKDIR /app INFO[0042] cmd: workdir INFO[0042] Changed working directory to /app INFO[0042] No files changed in this command, skipping snapshotting. INFO[0042] RUN pwd && ls -l && npm install &&↳ npm run print-http-server-version INFO[0042] Taking snapshot of full filesystem... INFO[0052] cmd: /bin/sh INFO[0052] args: [-c pwd && ls -l && npm install &&↳ npm run print-http-server-version] INFO[0052] Running: [/bin/sh -c pwd && ls -l && npm install &&↳ npm run print-http-server-version] /app total 44 -rw-r--r-- 1 node node 261 Jan 27 14:29 Dockerfile -rw-r--r-- 1 node node 30000 Jan 27 14:29 package-lock.json -rw-r--r-- 1 node node 267 Jan 27 14:29 package.json drwxr-xr-x 2 node node 4096 Jan 27 14:29 public npm WARN npm-simple-renamed@0.0.1 No repository field. npm WARN npm-simple-renamed@0.0.1 No license field.\n\nadded 90 packages from 40 contributors and audited 90 packages in 6.405s\n\n10 packages are looking for funding run `npm fund` for details\n\nfound 0 vulnerabilities\n\n> npm-simple-renamed@0.0.1 print-http-server-version /app > serve -v\n\n13.0.2 INFO[0060] Taking snapshot of full filesystem... INFO[0062] EXPOSE 8080 INFO[0062] cmd: EXPOSE INFO[0062] Adding exposed port: 8080/tcp INFO[0062] CMD [\"npm\", \"start\"]\n\n40\n\n|\n\nChapter 3: Containers\n\nINFO[0070] Pushing image to quay.io/gitops-cookbook/sample-nodejs:latest INFO[0393] Pushed image to 1 destinations\n\nThe image is built and pushed to the registry, and you can check the result from this command as well:\n\nkubectl get buildruns\n\nAnd on your registry, as shown in Figure 3-4.\n\nFigure 3-4. Image pushed to Quay\n\nDiscussion Shipwright provides a convenient way to create container images on Kubernetes, and its agnostic approach makes it robust and interoperable. The project aims at being the Build API for Kubernetes, providing an easier path for developers to automate on Kubernetes. As Tekton runs under the hood creating builds, Shipwright also makes transitioning from micropipeline to extended pipeline workflows on Kubernetes easier.\n\nAs a reference, if you would like to create a build with Buildah instead of kaniko, it’s just a ClusterBuildStrategy change in your Build object:\n\napiVersion: shipwright.io/v1alpha1 kind: Build metadata: name: buildpack-nodejs-build spec: source: url: https://github.com/shipwright-io/sample-nodejs contextDir: source-build strategy: name: buildah kind: ClusterBuildStrategy\n\n3.5 Building a Container Using Shipwright and kaniko in Kubernetes\n\n|\n\n41\n\noutput: image: quay.io/gitops-cookbook/sample-nodejs:latest credentials: name: push-secret\n\nAs we discussed previously in Recipe 3.3, Buildah can create the container image from the source code. It doesn’t need a Dockerfile.\n\nSelecting Buildah as the ClusterBuildStrategy.\n\n3.6 Final Thoughts The container format is the de facto standard for packaging applications, and today many tools help create container images. Developers can create images with Docker or with other tools and frameworks and then use the same with any CI/CD system to deploy their apps to Kubernetes.\n\nWhile Kubernetes per se doesn’t build container images, some tools interact with the Kubernetes API ecosystem to add this functionality. This aspect improves devel‐ opment velocity and consistency across environments, delegating this complexity to the platform.\n\nIn the following chapters, you will see how to control the deployment of your con‐ tainers running on Kubernetes with tools such as Kustomize or Helm, and then how to add automation to support highly scalable workloads with CI/CD and GitOps.\n\n42\n\n|\n\nChapter 3: Containers",
      "page_number": 50
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 59-67)",
      "start_page": 59,
      "end_page": 67,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 4 Kustomize\n\nDeploying to a Kubernetes cluster is, in summary, applying some YAML files and checking the result.\n\nThe hard part is developing the initial YAML files version; after that, usually, they suffer only small changes such as updating the container image tag version, the num‐ ber of replicas, or a new configuration value. One option is to make these changes directly in the YAML files—it works, but any error in this version (modification of the wrong line, deleting something by mistake, putting in the wrong whitespace) might be catastrophic.\n\nFor this reason, some tools let you define base Kubernetes manifests (which change infrequently) and specific files (maybe one for each environment) for setting the parameters that change more frequently. One of these tools is Kustomize.\n\nIn this chapter, you’ll learn how to use Kustomize to manage Kubernetes resource files in a template-free way without using any DSL.\n\nThe first step is to create a Kustomize project and deploy it to a Kubernetes cluster (see Recipe 4.1).\n\nAfter the first deployment, the application is automatically updated with a new container image, a new configuration value, or any other field, such as the replica number (see Recipes 4.2 and 4.3).\n\nIf you’ve got several running environments (i.e., staging, production, etc.), you need to manage them similarly. Still, with its particularities, Kustomize lets you define a set of custom values per environment (see Recipe 4.4).\n\nApplication configuration values are properties usually mapped as a Kubernetes ConfigMap. Any change (and its consequent update on the cluster) on a ConfigMap\n\n43\n\ndoesn’t trigger a rolling update of the application, which means that the application will run with the previous version until you manually restart it.\n\nKustomize provides some functions to automatically execute a rolling update when the ConfigMap of an application changes (see Recipe 4.5).\n\n4.1 Using Kustomize to Deploy Kubernetes Resources\n\nProblem You want to deploy several Kubernetes resources at once.\n\nSolution Use Kustomize to configure which resources to deploy.\n\nDeploying an application to a Kubernetes cluster isn’t as trivial as just applying one YAML/JSON file containing a Kubernetes Deployment object. Usually, other Kuber‐ netes objects must be defined like Service, Ingress, ConfigMaps, etc., which makes things a bit more complicated in terms of managing and updating these resources (the more resources to maintain, the more chance to update the wrong one) as well as applying them to a cluster (should we run multiple kubectl commands?).\n\nKustomize is a CLI tool, integrated within the kubectl tool to manage, customize, and apply Kubernetes resources in a template-less way.\n\nWith Kustomize, you need to set a base directory with standard Kubernetes resource files (no placeholders are required) and create a kustomization.yaml file where resources and customizations are declared, as you can see in Figure 4-1.\n\nFigure 4-1. Kustomize layout\n\nLet’s deploy a simple web page with HTML, JavaScript, and CSS files.\n\nFirst, open a terminal window and create a directory named pacman, then create three Kubernetes resource files to create a Namespace, a Deployment, and a Service with the following content.\n\n44\n\n|\n\nChapter 4: Kustomize\n\nThe namespace at pacman/namespace.yaml:\n\napiVersion: v1 kind: Namespace metadata: name: pacman\n\nThe deployment file at pacman/deployment.yaml:\n\napiVersion: apps/v1 kind: Deployment metadata: name: pacman-kikd namespace: pacman labels: app.kubernetes.io/name: pacman-kikd spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd ports: - containerPort: 8080 name: http protocol: TCP\n\nThe service file at pacman/service.yaml:\n\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: ports: - name: http port: 8080 targetPort: 8080 selector: app.kubernetes.io/name: pacman-kikd\n\nNotice that these files are Kubernetes files that you could apply to a Kubernetes cluster without any problem as no special characters or placeholders are used.\n\n4.1 Using Kustomize to Deploy Kubernetes Resources\n\n|\n\n45\n\nThe second thing is to create the kustomization.yaml file in the pacman directory containing the list of resources that belongs to the application and are applied when running Kustomize:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./namespace.yaml - ./deployment.yaml - ./service.yaml\n\nKustomization file\n\nResources belonging to the application processed in depth-first order\n\nAt this point, we can apply the kustomization file into a running cluster by running the following command:\n\nkubectl apply --dry-run=client -o yaml \\ -k ./\n\nPrints the result of the kustomization run, without sending the result to the cluster\n\nWith -k option sets kubectl to use the kustomization file\n\nDirectory with parent kustomization.yaml file\n\nWe assume you’ve already started a Minikube cluster as shown in Recipe 2.3.\n\nThe output is the YAML file that would be sent to the server if the dry-run option was not used:\n\napiVersion: v1 items: - apiVersion: v1 kind: Namespace metadata: name: pacman - apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman\n\n46\n\n|\n\nChapter 4: Kustomize\n\nspec: ports: - name: http port: 8080 targetPort: 8080 selector: app.kubernetes.io/name: pacman-kikd - apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd ports: - containerPort: 8080 name: http protocol: TCP kind: List metadata: {}\n\nList of all Kubernetes objects defined in kustomization.yaml to apply\n\nThe namespace document\n\nThe service document\n\nThe deployment document\n\nDiscussion The resources section supports different inputs in addition to directly setting the YAML files.\n\nFor example, you can set a base directory with its own kustomization.yaml and Kubernetes resources files and refer it from another kustomization.yaml file placed in another directory.\n\n4.1 Using Kustomize to Deploy Kubernetes Resources\n\n|\n\n47\n\nGiven the following directory layout:\n\n. ├── base │ ├── kustomization.yaml │ └── deployment.yaml ├── kustomization.yaml ├── configmap.yaml\n\nAnd the Kustomization definitions in the base directory:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml\n\nYou’ll see that the root directory has a link to the base directory and a ConfigMap definition:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./base - ./configmap.yaml\n\nSo, applying the root kustomization file will automatically apply the resources defined in the base kustomization file.\n\nAlso, resources can reference external assets from a URL following the HashiCorp URL format. For example, we refer to a GitHub repository by setting the URL:\n\nresources: - github.com/lordofthejars/mysql - github.com/lordofthejars/mysql?ref=test\n\nRepository with a root-level kustomization.yaml file\n\nRepository with a root-level kustomization.yaml file on branch test\n\nYou’ve seen the application of a Kustomize file using kubectl, but Kustomize also comes with its own CLI tool offering a set of commands to interact with Kustomize resources.\n\nThe equivalent command to build Kustomize resources using kustomize instead of kubectl is:\n\nkustomize build\n\n48\n\n|\n\nChapter 4: Kustomize\n\nAnd the output is:\n\napiVersion: v1 kind: Namespace metadata: name: pacman --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: ports: - name: http port: 8080 targetPort: 8080 selector: app.kubernetes.io/name: pacman-kikd --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd ports: - containerPort: 8080 name: http protocol: TCP\n\nIf you want to apply this output generated by kustomize to the cluster, run the following command:\n\nkustomize build . | kubectl apply -f -\n\n4.1 Using Kustomize to Deploy Kubernetes Resources\n\n|\n\n49\n\nSee Also\n\n• Kustomize\n\n• kustomize/v4.4.1 on GitHub\n\n• HashiCorp URL format\n\n4.2 Updating the Container Image in Kustomize\n\nProblem You want to update the container image from a deployment file using Kustomize.\n\nSolution Use the images section to update the container image.\n\nOne of the most important and most-used operations in software development is updating the application to a newer version either with a bug fix or with a new feature. In Kubernetes, this means that you need to create a new container image, and name it accordingly using the tag section (<registry>/<username>/ <project>:<tag>).\n\nGiven the following partial deployment file:\n\nspec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd\n\nService 1.0.0 is deployed\n\nWe can update the version tag to 1.0.1 by using the images section in the kustomiza‐ tion.yaml file:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./namespace.yaml - ./deployment.yaml - ./service.yaml images: - name: lordofthejars/pacman-kikd newTag: 1.0.1\n\n50\n\n|\n\nChapter 4: Kustomize\n\nimages section\n\nSets the name of the image to update\n\nSets the new tag value for the image\n\nFinally, use kubectl in dry-run or kustomize to validate that the output of the deployment file contains the new tag version. In a terminal window, run the follow‐ ing command:\n\nkustomize build\n\nThe output of the preceding command is:\n\n... apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.1 imagePullPolicy: Always name: pacman-kikd ports: - containerPort: 8080 name: http protocol: TCP\n\nVersion set in the kustomize.yaml file\n\nKustomize is not intrusive, which means that the original deploy‐ ment.yaml file still contains the original tag (1.0.0).\n\n4.2 Updating the Container Image in Kustomize\n\n|\n\n51",
      "page_number": 59
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 68-75)",
      "start_page": 68,
      "end_page": 75,
      "detection_method": "topic_boundary",
      "content": "Discussion One way to update the newTag field is by editing the kustomization.yaml file, but you can also use the kustomize tool for this purpose.\n\nRun the following command in the same directory as the kustomization.yaml file:\n\nkustomize edit set image lordofthejars/pacman-kikd:1.0.2\n\nCheck the content of the kustomization.yaml file to see that the newTag field has been updated:\n\n... images: - name: lordofthejars/pacman-kikd newTag: 1.0.2\n\n4.3 Updating Any Kubernetes Field in Kustomize\n\nProblem You want to update a field (i.e., number of replicas) using Kustomize.\n\nSolution Use the patches section to specify a change using the JSON Patch specification.\n\nIn the previous recipe, you saw how to update the container image tag, but sometimes you might change other parameters like the number of replicas or add annotations, labels, limits, etc.\n\nTo cover these scenarios, Kustomize supports the use of JSON Patch to modify any Kubernetes resource defined as a Kustomize resource. To use it, you need to specify the JSON Patch expression to apply and which resource to apply the patch to.\n\nFor example, we can modify the number of replicas in the following partial deploy‐ ment file from one to three:\n\napiVersion: apps/v1 kind: Deployment metadata: name: pacman-kikd namespace: pacman labels: app.kubernetes.io/name: pacman-kikd spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template:\n\n52\n\n|\n\nChapter 4: Kustomize\n\nmetadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: ...\n\nFirst, let’s update the kustomization.yaml file to modify the number of replicas defined in the deployment file:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml patches: - target: version: v1 group: apps kind: Deployment name: pacman-kikd namespace: pacman patch: |- - op: replace path: /spec/replicas value: 3\n\nPatch resource.\n\ntarget section sets which Kubernetes object needs to be changed. These values match the deployment file created previously.\n\nPatch expression.\n\nModification of a value.\n\nPath to the field to modify.\n\nNew value.\n\nFinally, use kubectl in dry-run or kustomize to validate that the output of the deployment file contains the new tag version. In a terminal window, run the follow‐ ing command:\n\nkustomize build\n\nThe output of the preceding command is:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd\n\n4.3 Updating Any Kubernetes Field in Kustomize\n\n|\n\n53\n\nname: pacman-kikd namespace: pacman spec: replicas: 3 selector: matchLabels: app.kubernetes.io/name: pacman-kikd ...\n\nThe replicas value can also be updated using the replicas field in the kustomization.yaml file. The equivalent Kustomize file using the replicas field is shown in the following snippet:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization replicas: - name: pacman-kikd count: 3 resources: - deployment.yaml\n\nDeployment to update the replicas\n\nNew replicas value\n\nKustomize lets you add (or delete) values, in addition to modifying a value. Let’s see how to add a new label:\n\n... patches: - target: version: v1 group: apps kind: Deployment name: pacman-kikd namespace: pacman patch: |- - op: replace path: /spec/replicas value: 3 - op: add path: /metadata/labels/testkey value: testvalue\n\nAdds a new field with value\n\nPath with the field to add\n\nThe value to set\n\n54\n\n|\n\nChapter 4: Kustomize\n\nThe result of applying the file is:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd testkey: testvalue name: pacman-kikd namespace: pacman spec: replicas: 3 selector: ...\n\nAdded label\n\nDiscussion Instead of embedding a JSON Patch expression, you can create a YAML file with a Patch expression and refer to it using the path field instead of patch.\n\nCreate an external patch file named external_patch containing the JSON Patch expression:\n\nop: replace path: /spec/replicas value: 3 - op: add path: /metadata/labels/testkey value: testvalue\n\nAnd change the patch field to path pointing to the patch file:\n\n... patches: - target: version: v1 group: apps kind: Deployment name: pacman-kikd namespace: pacman path: external_patch.yaml\n\nPath to external patch file\n\nIn addition to the JSON Patch expression, Kustomize also supports Strategic Merge Patch to modify Kubernetes resources. In summary, a Strategic Merge Patch (or SMP) is an incomplete YAML file that is merged against a completed YAML file.\n\nOnly a minimal deployment file with container name information is required to update a container image:\n\n4.3 Updating Any Kubernetes Field in Kustomize\n\n|\n\n55\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml patches: - target: labelSelector: \"app.kubernetes.io/name=pacman-kikd\" patch: |- apiVersion: apps/v1 kind: Deployment metadata: name: pacman-kikd spec: template: spec: containers: - name: pacman-kikd image: lordofthejars/pacman-kikd:1.2.0\n\nTarget is selected using label\n\nPatch is smart enough to detect if it is an SMP or JSON Patch\n\nThis is a minimal deployment file\n\nSets only the field to change, the rest is left as is\n\nThe generated output is the original deployment.yaml file but with the new container image:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.2.0 imagePullPolicy: Always ...\n\n56\n\n|\n\nChapter 4: Kustomize\n\npath is supported as well.\n\nSee Also\n\n• RFC 6902: JavaScript Object Notation (JSON) Patch\n\nStrategic Merge Patch •\n\n4.4 Deploying to Multiple Environments\n\nProblem You want to deploy the same application in different namespaces using Kustomize.\n\nSolution Use the namespace field to set the target namespace.\n\nIn some circumstances, it’s good to have the application deployed in different name‐ spaces; for example, one namespace can be used as a staging environment, and another one as the production namespace. In both cases, the base Kubernetes files are the same, with minimal changes like the namespace deployed, some configuration parameters, or container version, to mention a few. Figure 4-2 shows an example.\n\nFigure 4-2. Kustomize layout\n\nkustomize lets you define multiple changes with a different namespace, as overlays on a common base using the namespace field. For this example, all base Kubernetes resources are put in the base directory and a new directory is created for customiza‐ tions of each environment:\n\n. ├── base │ ├── deployment.yaml\n\n4.4 Deploying to Multiple Environments\n\n|\n\n57\n\n│ └── kustomization.yaml ├── production │ └── kustomization.yaml └── staging └── kustomization.yaml\n\nBase files\n\nChanges specific to production environment\n\nChanges specific to staging environment\n\nThe base kustomization file contains a reference to its resources:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml\n\nThere is a kustomization file with some parameters set for each environment direc‐ tory. These reference the base directory, the namespace to inject into Kubernetes resources, and finally, the image to deploy, which in production is 1.1.0 but in staging is 1.2.0-beta.\n\nFor the staging environment, kustomization.yaml content is shown in the following listing:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../base namespace: staging images: - name: lordofthejars/pacman-kikd newTag: 1.2.0-beta\n\nReferences to base directory\n\nSets namespace to staging\n\nSets the container tag for the staging environment\n\nThe kustomization file for production is similar to the staging one, but changes the namespace and the tag:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../base namespace: prod images:\n\n58\n\n|\n\nChapter 4: Kustomize\n\nname: lordofthejars/pacman-kikd newTag: 1.1.0\n\nSets namespace for production\n\nSets the container tag for the production environment\n\nRunning kustomize produces different output depending on the directory where it is run; for example, running kustomize build in the staging directory produces:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: staging spec: replicas: 1 ... template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.2.0-beta ...\n\nNamespace value is injected\n\nContainer tag for the staging environment is injected\n\nBut if you run it in the production directory, the output is adapted to the production configuration:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: prod spec: replicas: 1 ... spec: containers: - image: lordofthejars/pacman-kikd:1.1.0 ...\n\nInjects the production namespace\n\n4.4 Deploying to Multiple Environments\n\n|\n\n59",
      "page_number": 68
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 76-89)",
      "start_page": 76,
      "end_page": 89,
      "detection_method": "topic_boundary",
      "content": "Container tag for the production environment\n\nDiscussion Kustomize can preappend/append a value to the names of all resources and refer‐ ences. This is useful when a different name in the resource is required depending on the environment, or to set the version deployed in the name:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../base namespace: staging namePrefix: staging- nameSuffix: -v1-2-0 images: - name: lordofthejars/pacman-kikd newTag: 1.2.0-beta\n\nPrefix to preappend\n\nSuffix to append\n\nAnd the resulting output is as follows:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: staging-pacman-kikd-v1-2-0 namespace: staging spec: ...\n\nNew name of the deployment file\n\n4.5 Generating ConfigMaps in Kustomize\n\nProblem You want to generate Kubernetes ConfigMaps using Kustomize.\n\nSolution Use the ConfigMapGenerator feature field to generate a Kubernetes ConfigMap resource on the fly.\n\n60\n\n|\n\nChapter 4: Kustomize\n\nKustomize provides two ways of adding a ConfigMap as a Kustomize resource: either by declaring a ConfigMap as any other resource or declaring a ConfigMap from a ConfigMapGenerator.\n\nWhile using ConfigMap as a resource offers no other advantage than populating Kubernetes resources as any other resource, ConfigMapGenerator automatically appends a hash to the ConfigMap metadata name and also modifies the deployment file with the new hash. This minimal change has a deep impact on the application’s lifecycle, as we’ll see soon in the example.\n\nLet’s consider an application running in Kubernetes and configured using a Config Map—for example, a database timeout connection parameter. We decided to increase this number at some point, so the ConfigMap file is changed to this new value, and we deploy the application again. Since the ConfigMap is the only changed file, no rolling update of the application is done. A manual rolling update of the application needs to be triggered to propagate the change to the application. Figure 4-3 shows what is changed when a ConfigMap object is updated.\n\nFigure 4-3. Change of a ConfigMap\n\nBut, if ConfigMapGenerator manages the ConfigMap, any change on the configura‐ tion file also changes the deployment Kubernetes resource. Since the deployment file has changed too, an automatic rolling update is triggered when the resources are applied, as shown in Figure 4-4.\n\nMoreover, when using ConfigMapGenerator, multiple configuration datafiles can be combined into a single ConfigMap, making a perfect use case when every environ‐ ment has different configuration files.\n\n4.5 Generating ConfigMaps in Kustomize\n\n|\n\n61\n\nFigure 4-4. Change of a ConfigMap using ConfigMapGenerator\n\nLet’s start with a simple example, adding the ConfigMapGenerator section in the kustomization.yaml file.\n\nThe deployment file is similar to the one used in previous sections of this chapter but includes the volumes section:\n\napiVersion: apps/v1 kind: Deployment metadata: name: pacman-kikd spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd volumeMounts: - name: config mountPath: /config volumes: - name: config configMap: name: pacman-configmap\n\nConfigMap name is used in the kustomization.yaml file\n\n62\n\n|\n\nChapter 4: Kustomize\n\nThe configuration properties are embedded within the kustomization.yaml file. Notice that the ConfigMap object is created on the fly when the kustomization file is built:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml configMapGenerator: - name: pacman-configmap literals: - db-timeout=2000 - db-username=Ada\n\nName of the ConfigMap set in the deployment file\n\nEmbeds configuration values in the file\n\nSets a key/value pair for the properties\n\nFinally, use kubectl in dry-run or kustomize to validate that the output of the deployment file contains the new tag version. In a terminal window, run the follow‐ ing command:\n\nkustomize build\n\nThe output of the preceding command is a new ConfigMap with the configuration values set in kustomization.yaml. Moreover, the name of the ConfigMap is updated by appending a hash in both the generated ConfigMap and deployment:\n\napiVersion: v1 data: db-timeout: \"2000\" db-username: Ada kind: ConfigMap metadata: name: pacman-configmap-96kb69b6t4 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels:\n\n4.5 Generating ConfigMaps in Kustomize\n\n|\n\n63\n\napp.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd volumeMounts: - mountPath: /config name: config volumes: - configMap: name: pacman-configmap-96kb69b6t4 name: config\n\nConfigMap with properties\n\nName with hash\n\nName field is updated to the one with the hash triggering a rolling update\n\nSince the hash is calculated for any change in the configuration properties, a change on them provokes a change on the output triggering a rolling update of the applica‐ tion. Open the kustomization.yaml file and update the db-timeout literal from 2000 to 1000 and run kustomize build again. Notice the change in the ConfigMap name using a new hashed value:\n\napiVersion: v1 data: db-timeout: \"1000\" db-username: Ada kind: ConfigMap metadata: name: pacman-configmap-6952t58tb4 --- apiVersion: apps/v1 kind: Deployment ... volumes: - configMap: name: pacman-configmap-6952t58tb4 name: config\n\nNew hashed value\n\nDiscussion ConfigMapGenerator also supports merging configuration properties from different sources.\n\n64\n\n|\n\nChapter 4: Kustomize\n\nCreate a new kustomization.yaml file in the dev_literals directory, setting it as the previous directory and overriding the db-username value:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../literals configMapGenerator: - name: pacman-configmap behavior: merge literals: - db-username=Alexandra\n\nBase directory\n\nMerge properties (can be create or replace too)\n\nOverridden value\n\nRunning the kustomize build command produces a ConfigMap containing a merge of both configuration properties:\n\napiVersion: v1 data: db-timeout: \"1000\" db-username: Alexandra kind: ConfigMap metadata: name: pacman-configmap-ttfdfdk5t8 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd ...\n\nInherits from base\n\nOverrides value\n\nIn addition to setting configuration properties as literals, Kustomize supports defin‐ ing them as .properties files.\n\nCreate a connection.properties file with two properties inside:\n\ndb-url=prod:4321/db db-username=ada\n\nThe kustomization.yaml file uses the files field instead of literals:\n\n4.5 Generating ConfigMaps in Kustomize\n\n|\n\n65\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml configMapGenerator: - name: pacman-configmap files: - ./connection.properties\n\nSets a list of files to read\n\nPath to the properties file\n\nRunning the kustomize build command produces a ConfigMap containing the name of the file as a key, and the value as the content of the file:\n\napiVersion: v1 data: connection.properties: |- db-url=prod:4321/db db-username=ada kind: ConfigMap metadata: name: pacman-configmap-g9dm2gtt77 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd ...\n\nSee Also Kustomize offers a similar way to deal with Kubernetes Secrets. But as we’ll see in Chapter 8, the best way to deal with Kubernetes Secrets is using Sealed Secrets.\n\n4.6 Final Thoughts Kustomize is a simple tool, using template-less technology that allows you to define plain YAML files and override values either using a merge strategy or using JSON Patch expressions. The structure of a project is free as you define the directory layout you feel most comfortable with; the only requirement is the presence of a kustomization.yaml file.\n\nBut there is another well-known tool to manage Kubernetes resources files, that in our opinion, is a bit more complicated but more powerful, especially when the appli‐ cation/service to deploy has several dependencies such as databases, mail servers, caches, etc. This tool is Helm, and we’ll cover it in Chapter 5.\n\n66\n\n|\n\nChapter 4: Kustomize\n\nCHAPTER 5 Helm\n\nIn Chapter 4, you learned about Kustomize, a simple yet powerful tool to manage Kubernetes resources. But another popular tool aims to simplify the Kubernetes resources management too: Helm.\n\nHelm works similarly to Kustomize, but it’s a template solution and acts more like a package manager, producing artifacts that are versionable, sharable, or deployable.\n\nIn this chapter, we’ll introduce Helm, a package manager for Kubernetes that helps install and manage Kubernetes applications using the Go template language in YAML files.\n\nThe first step is to create a Helm project and deploy it to a Kubernetes cluster (see Recipes 5.1 and 5.2). After the first deployment, the application is updated with a new container image, a new configuration value, or any other field, such as the replica number (see Recipe 5.3).\n\nOne of the differences between Kustomize and Helm is the concept of a Chart. A Chart is a packaged artifact that can be shared and contains multiple elements like dependencies on other Charts (see Recipes 5.4, 5.5, and 5.6).\n\nApplication configuration values are properties usually mapped as a Kubernetes ConfigMap. Any change (and its consequent update on the cluster) on a ConfigMap doesn’t trigger a rolling update of the application, which means that the application will run with the previous version until you manually restart it.\n\nHelm provides some functions to automatically execute a rolling update when the ConfigMap of an application changes (see Recipe 5.7).\n\n67\n\n5.1 Creating a Helm Project\n\nProblem You want to create a simple Helm project.\n\nSolution Use the Helm CLI tool to create a new project.\n\nIn contrast to Kustomize, which can be used either within the kubectl command or as a standalone CLI tool, Helm needs to be downloaded and installed in your local machine.\n\nHelm is a packager for Kubernetes that bundles related manifest files and packages them into a single logical deployment unit: a Chart. Thus simplified, for many engineers, Helm makes it easy to start using Kubernetes with real applications.\n\nHelm Charts are useful for addressing the installation complexities and simple upgrades of applications.\n\nFor this book, we use Helm 3.7.2, which you can download from GitHub and install in your PATH directory.\n\nOpen a terminal and run the following commands to create a Helm Chart directory layout:\n\nmkdir pacman mkdir pacman/templates\n\ncd pacman\n\nThen create three files: one that defines the Chart, another representing the deploy‐ ment template using the Go template language and template functions from the Sprig library, and finally a file containing the default values for the Chart.\n\nA Chart.yaml file declares the Chart with information such as version or name. Create the file in the root directory:\n\napiVersion: v2 name: pacman description: A Helm chart for Pacman\n\ntype: application\n\nversion: 0.1.0\n\nappVersion: \"1.0.0\"\n\n68\n\n|\n\nChapter 5: Helm\n\nVersion of the Chart. This is updated when something in the Chart definition is changed.\n\nVersion of the application.\n\nLet’s create a deployment.yaml and a service.yaml template file to deploy the application.\n\nThe deployment.yaml file templatizes the deployment’s name, the application version, the replica count, the container image and tag, the pull policy, the security context, and the port:\n\napiVersion: apps/v1 kind: Deployment metadata: name: {{ .Chart.Name}} labels: app.kubernetes.io/name: {{ .Chart.Name}} {{- if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{- end }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} spec: containers: - image: \"{{ .Values.image.repository }}: {{ .Values.image.tag | default .Chart.AppVersion}}\" imagePullPolicy: {{ .Values.image.pullPolicy }} securityContext: {{- toYaml .Values.securityContext | nindent 14 }} name: {{ .Chart.Name}} ports: - containerPort: {{ .Values.image.containerPort }} name: http protocol: TCP\n\nSets the name from the Chart.yaml file\n\nConditionally sets the version based on the presence of the appVersion in the Chart.yaml file\n\nSets the appVersion value but quoting the property\n\n5.1 Creating a Helm Project\n\n|\n\n69\n\nPlaceholder for the replicaCount property\n\nPlaceholder for the container image\n\nPlaceholder for the image tag if present and if not, defaults to the Chart.yaml property\n\nSets the securityContext value as a YAML object and not as a string, indenting it 14 spaces\n\nThe service.yaml file templatizes the service name and the container port:\n\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: {{ .Chart.Name }} name: {{ .Chart.Name }} spec: ports: - name: http port: {{ .Values.image.containerPort }} targetPort: {{ .Values.image.containerPort }} selector: app.kubernetes.io/name: {{ .Chart.Name }}\n\nThe values.yaml file contains the default values for the Chart. These values can be overridden at runtime, but they provide good initial values.\n\nCreate the file in the root directory with some default values:\n\nimage: repository: quay.io/gitops-cookbook/pacman-kikd tag: \"1.0.0\" pullPolicy: Always containerPort: 8080\n\nreplicaCount: 1 securityContext: {}\n\nDefines the image section\n\nSets the repository property\n\nEmpty securityContext\n\nBuilt-in properties are capitalized; for this reason, properties defined in the Chart.yaml file start with an uppercase letter.\n\n70\n\n|\n\nChapter 5: Helm\n\nSince the toYaml function is used for the securityContext value, the expected value for the securityContext property in values.yaml should be a YAML object. For example:\n\nsecurityContext: capabilities: drop: - ALL readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000\n\nThe relationship between all elements is shown in Figure 5-1.\n\nFigure 5-1. Relationship between Helm elements\n\n5.1 Creating a Helm Project\n\n|\n\n71\n\nAt this point the Helm directory layout is created and should be similar to this:\n\npacman ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── service.yaml └── values.yaml\n\nThe Chart.yaml file is the Chart descriptor and contains metadata related to the Chart.\n\nThe templates directory contains all template files used for installing a Chart.\n\nThese files are Helm template files used to deploy the application.\n\nThe values.yaml file contains the default values for a Chart.\n\nTo render the Helm Chart locally to YAML, run the following command in a terminal window:\n\nhelm template .\n\nThe output is:\n\n--- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: pacman name: pacman spec: ports: - name: http port: 8080 targetPort: 8080 selector: app.kubernetes.io/name: pacman --- apiVersion: apps/v1 kind: Deployment metadata: name: pacman labels: app.kubernetes.io/name: pacman app.kubernetes.io/version: \"1.0.0\" spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman template:\n\n72\n\n|\n\nChapter 5: Helm\n\nmetadata: labels: app.kubernetes.io/name: pacman spec: containers: - image: \"quay.io/gitops-cookbook/pacman-kikd:1.0.0\" imagePullPolicy: Always securityContext: {} name: pacman ports: - containerPort: 8080 name: http protocol: TCP\n\nName is injected from Chart.yaml\n\nPort is set in values.yaml\n\nVersion is taken from Chart version\n\nConcatenates content from two attributes\n\nEmpty security context\n\nYou can override any default value by using the --set parameter in Helm. Let’s override the replicaCount value from one (defined in values.yaml) to three:\n\nhelm template --set replicaCount=3 .\n\nAnd the replicas value is updated:\n\napiVersion: apps/v1 kind: Deployment metadata: name: pacman labels: app.kubernetes.io/name: pacman app.kubernetes.io/version: \"1.0.0\" spec: replicas: 3 ...\n\nDiscussion Helm is a package manager for Kubernetes, and as such, it helps you with the task of versioning, sharing, and upgrading Kubernetes applications.\n\nLet’s see how to install the Helm Chart to a Kubernetes cluster and upgrade the application.\n\n5.1 Creating a Helm Project\n\n|\n\n73",
      "page_number": 76
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 90-106)",
      "start_page": 90,
      "end_page": 106,
      "detection_method": "topic_boundary",
      "content": "With Minikube up and running, execute the following command in a terminal window, and run the install command to deploy the application to the cluster:\n\nhelm install pacman .\n\nThe Chart is installed in the running Kubernetes instance:\n\nLAST DEPLOYED: Sat Jan 22 15:13:50 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None\n\nGet the list of current deployed pods, Deployments, and Services to validate that the Helm Chart is deployed in the Kubernetes cluster:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE pacman-7947b988-kzjbc 1/1 Running 0 60s\n\nkubectl get deployment\n\nNAME READY UP-TO-DATE AVAILABLE AGE pacman 1/1 1 1 4m50s\n\nkubectl get services\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE pacman ClusterIP 172.30.129.123 <none> 8080/TCP 9m55s\n\nAlso, it’s possible to get history information about the deployed Helm Chart using the history command:\n\nhelm history pacman\n\nREVISION APP VERSION DESCRIPTION 1 1.0.0 Install complete\n\nUPDATED\n\nSat Jan 22 15:23:22 2022\n\nSTATUS\n\ndeployed\n\nCHART ↳\n\npacman-0.1.0↳\n\nTo uninstall a Chart from the cluster, run uninstall command:\n\nhelm uninstall pacman\n\nrelease \"pacman\" uninstalled\n\nHelm is a package manager that lets you share the Chart (package) to other Charts as a dependency. For example, you can have a Chart defining the deployment of the application and another Chart as a dependency setting a database deployment. In this way, the installation process installs the application and the database Chart automatically.\n\nWe’ll learn about the packaging process and adding dependencies in a later section.\n\n74\n\n|\n\nChapter 5: Helm\n\nYou can use the helm create <name> command to let the Helm tool skaffold the project.\n\nSee Also\n\n• Helm\n\nGo template package •\n\nSprig Function Documentation •\n\n5.2 Reusing Statements Between Templates\n\nProblem You want to reuse template statements across several files.\n\nSolution Use _helpers.tpl to define reusable statements.\n\nWe deployed a simple application to Kubernetes using Helm in the previous recipe. This simple application was composed of a Kubernetes Deployment file and a Kuber‐ netes Service file where the selector field was defined with the same value.\n\nAs a reminder:\n\n... spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} ...\n\nservice.yaml ---- ... selector: app.kubernetes.io/name: {{ .Chart.Name }} ----\n\nIf you need to update this field—for example, adding a new label as a selector—you would need to update in three places, as shown in the previous snippets.\n\n5.2 Reusing Statements Between Templates\n\n|\n\n75\n\nHelm lets you create a _helpers.tpl file in the templates directory defining statements that can be called in templates to avoid this problem.\n\nLet’s refactor the previous example to use the _helpers.tpl file to define the selector Labels.\n\nCreate the _helpers.tpl file in the templates directory with the following content:\n\n{{- define \"pacman.selectorLabels\" -}} app.kubernetes.io/name: {{ .Chart.Name}} {{- end }}\n\nDefines the statement name\n\nDefines the logic of the statement\n\nThen replace the template placeholders shown in previous snippets with a call to the podman.selectorLabels helper statement using the include keyword:\n\nspec: replicas: {{ .Values.replicaCount }} selector: matchLabels: {{- include \"pacman.selectorLabels\" . | nindent 6 }} template: metadata: labels: {{- include \"pacman.selectorLabels\" . | nindent 8 }} spec: containers:\n\nCalls pacman.selectorLabels with indentation\n\nCalls pacman.selectorLabels with indentation\n\nTo render the Helm Chart locally to YAML, run the following command in a terminal window:\n\nhelm template .\n\nThe output is:\n\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: pacman name: pacman spec: ports: - name: http port: 8080 targetPort: 8080\n\n76\n\n|\n\nChapter 5: Helm\n\nselector: app.kubernetes.io/name: pacman --- apiVersion: apps/v1 kind: Deployment metadata: name: pacman labels: app.kubernetes.io/name: pacman app.kubernetes.io/version: \"1.0.0\" spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman template: metadata: labels: app.kubernetes.io/name: pacman spec: containers: - image: \"quay.io/gitops-cookbook/pacman-kikd:1.0.0\" imagePullPolicy: Always securityContext: {} name: pacman ports: - containerPort: 8080 name: http protocol: TCP\n\nSelector is updated with value set in _helpers.tpl\n\nSelector is updated with value set in _helpers.tpl\n\nSelector is updated with value set in _helpers.tpl\n\nDiscussion If you want to update the selector labels, the only change you need to do is an update to the _helpers.tpl file:\n\n{{- define \"pacman.selectorLabels\" -}} app.kubernetes.io/name: {{ .Chart.Name}} app.kubernetes.io/version: {{ .Chart.AppVersion}} {{- end }}\n\nAdds a new attribute\n\n5.2 Reusing Statements Between Templates\n\n|\n\n77\n\nTo render the Helm Chart locally to YAML, run the following command in a terminal window:\n\nhelm template .\n\nThe output is:\n\n--- # Source: pacman/templates/service.yaml apiVersion: v1 kind: Service metadata: ... selector: app.kubernetes.io/name: pacman app.kubernetes.io/version: 1.0.0 --- apiVersion: apps/v1 kind: Deployment metadata: name: pacman labels: app.kubernetes.io/name: pacman app.kubernetes.io/version: \"1.0.0\" spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman app.kubernetes.io/version: 1.0.0 template: metadata: labels: app.kubernetes.io/name: pacman app.kubernetes.io/version: 1.0.0 spec: ...\n\nLabel is added\n\nLabel is added\n\nLabel is added\n\nAlthough it’s common to use __helpers.tpl as the filename to define functions, you can name any file starting with __, and Helm will read the functions too.\n\n78\n\n|\n\nChapter 5: Helm\n\n5.3 Updating a Container Image in Helm\n\nProblem You want to update the container image from a deployment file using Helm and upgrade the running instance.\n\nSolution Use the upgrade command.\n\nWith Minikube up and running, deploy version 1.0.0 of the pacman application:\n\nhelm install pacman .\n\nWith the first revision deployed, let’s update the container image to a new version and deploy it.\n\nYou can check revision number by running the following command:\n\nhelm history pacman\n\nREVISION UPDATED STATUS CHART APP VERSION↳ DESCRIPTION 1 Sun Jan 23 16:00:09 2022 deployed pacman-0.1.0 1.0.0↳ Install complete\n\nTo update the version, open values.yaml and update the image.tag field to the newer container image tag:\n\nimage: repository: quay.io/gitops-cookbook/pacman-kikd tag: \"1.1.0\" pullPolicy: Always containerPort: 8080\n\nreplicaCount: 1 securityContext: {}\n\nUpdates to container tag to 1.1.0\n\nThen update the appVersion field of the Chart.yaml file:\n\napiVersion: v2 name: pacman description: A Helm chart for Pacman\n\ntype: application version: 0.1.0 appVersion: \"1.1.0\"\n\nVersion is updated accordingly\n\n5.3 Updating a Container Image in Helm\n\n|\n\n79\n\nYou can use appVersion as the tag instead of having two separate fields. Using two fields or one might depend on your use case, versioning strategy, and lifecycle of your software.\n\nAfter these changes, upgrade the deployment by running the following command:\n\nhelm upgrade pacman .\n\nThe output reflects that a new revision has been deployed:\n\nRelease \"pacman\" has been upgraded. Happy Helming! NAME: pacman LAST DEPLOYED: Mon Jan 24 11:39:28 2022 NAMESPACE: asotobue-dev STATUS: deployed REVISION: 2 TEST SUITE: None\n\nNew revision\n\nThe history command shows all changes between all versions:\n\nhelm history pacman\n\nREVISION UPDATED STATUS DESCRIPTION 1 Mon Jan 24 10:22:06 2022 superseded Install complete 2 Mon Jan 24 11:39:28 2022 deployed Upgrade complete\n\nCHART\n\npacman-0.1.0\n\npacman-0.1.0\n\nAPP VERSION↳\n\n1.0.0↳\n\n1.1.0↳\n\nappVersion is the application version, so every time you change the application version, you should update that field too. On the other side, version is the Chart version and should be updated when the definition of the Chart (i.e., templates) changes, so both fields are independent.\n\nDiscussion Not only you can install or upgrade a version with Helm, but you can also roll back to a previous revision.\n\nIn the terminal window, run the following command:\n\nhelm rollback pacman 1\n\nRollback was a success! Happy Helming!\n\n80\n\n|\n\nChapter 5: Helm\n\nRunning the history command reflects this change too:\n\nhelm history pacman\n\nREVISION UPDATED STATUS DESCRIPTION 1 Mon Jan 24 10:22:06 2022 superseded Install complete 2 Mon Jan 24 11:39:28 2022 superseded Upgrade complete 3 Mon Jan 24 12:31:58 2022 deployed Rollback to\n\nCHART\n\npacman-0.1.0\n\npacman-0.1.0\n\npacman-0.1.0\n\nAPP VERSION↳\n\n1.0.0↳\n\n1.1.0↳\n\n1.0.0↳\n\nFinally, Helm offers a way to override values, not only using the --set argument as shown in Recipe 5.1, but by providing a YAML file.\n\nCreate a new YAML file named newvalues.yaml in the root directory with the follow‐ ing content:\n\nimage: tag: \"1.2.0\"\n\nThen run the template command, setting the new file as an override of values.yaml:\n\nhelm template pacman -f newvalues.yaml .\n\nThe resulting YAML document is using the values set in values.yaml but overriding the images.tag set in newvalues.yaml:\n\napiVersion: apps/v1 kind: Deployment metadata: name: pacman ... spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman template: metadata: labels: app.kubernetes.io/name: pacman spec: containers: - image: \"quay.io/gitops-cookbook/pacman-kikd:1.2.0\" imagePullPolicy: Always ...\n\n5.3 Updating a Container Image in Helm\n\n|\n\n81\n\n5.4 Packaging and Distributing a Helm Chart\n\nProblem You want to package and distribute a Helm Chart so it can be reused by others.\n\nSolution Use the package command.\n\nHelm is a package manager for Kubernetes. As we’ve seen in this chapter, the basic unit in Helm is a Chart containing the Kubernetes files required to deploy the application, the default values for the templates, etc.\n\nBut we’ve not yet seen how to package Helm Charts and distribute them to be available to other Charts as dependencies or deployed by other users.\n\nLet’s package the pacman Chart into a .tgz file. In the pacman directory, run the following command:\n\nhelm package .\n\nAnd you’ll get a message informing you where the archive is stored:\n\nSuccessfully packaged chart and saved it to:↳ gitops-cookbook/code/05_helm/04_package/pacman/pacman-0.1.0.tgz\n\nA Chart then needs to be published into a Chart repository. A Chart repository is an HTTP server with an index.yaml file containing metadata information regarding Charts and .tgz Charts.\n\nTo publish them, update the index.yaml file with the new metadata information, and upload the artifact.\n\nThe directory layout of a repository might look like this:\n\nrepo ├── index.yaml ├── pacman-0.1.0.tgz\n\nThe index.yaml file with information about each Chart present in the repository looks like:\n\napiVersion: v1 entries: pacman: - apiVersion: v2 appVersion: 1.0.0 created: \"2022-01-24T16:42:54.080959+01:00\" description: A Helm chart for Pacman digest: aa3cce809ffcca86172fc793d7804d1c61f157b9b247680a67d5b16b18a0798d name: pacman\n\n82\n\n|\n\nChapter 5: Helm\n\ntype: application urls: - pacman-0.1.0.tgz version: 0.1.0 generated: \"2022-01-24T16:42:54.080485+01:00\"\n\nYou can run helm repo index in the root directory, where pack‐ aged Charts are stored, to generate the index file automatically.\n\nDiscussion In addition to packaging a Helm Chart, Helm can generate a signature file for the packaged Chart to verify its correctness later.\n\nIn this way, you can be sure it has not been modified, and it’s the correct Chart.\n\nTo sign/verify the package, you need a pair of GPG keys in the machine; we’re assuming you already have one pair created.\n\nNow you need to call the package command but set the -sign argument with the required parameters to generate a signature file:\n\nhelm package --sign --key 'me@example.com' \\ --keyring /home/me/.gnupg/secring.gpg .\n\nNow, two files are created—the packaged Helm Chart (.tgz) and the signature file (.tgz.prov):\n\n. ├── Chart.yaml ├── pacman-0.1.0.tgz ├── pacman-0.1.0.tgz.prov ├── templates │ ├── deployment.yaml │ └── service.yaml └── values.yaml\n\nChart package\n\nSignature file\n\nRemember to upload both files in the Chart repository.\n\n5.4 Packaging and Distributing a Helm Chart\n\n|\n\n83\n\nTo verify that a Chart is valid and has not been manipulated, use the verify command:\n\nhelm verify pacman-0.1.0.tgz\n\nSigned by: alexs (book) <asotobu@example.com> Using Key With Fingerprint: 57C4511D738BC0B288FAF9D69B40EB787040F3CF Chart Hash Verified:↳ sha256:d8b2e0c5e12a8425df2ea3a903807b93aabe4a6ff8277511a7865c847de3c0bf\n\nIt’s valid\n\nSee Also\n\n• The Chart Repository Guide\n\n• Helm Provenance and Integrity\n\n5.5 Deploying a Chart from a Repository\n\nProblem You want to deploy a Helm Chart stored in Chart repository.\n\nSolution Use the repo add command to add the remote repository and the install command to deploy it.\n\nPublic Helm Chart repositories like Bitnami are available for this purpose.\n\nTo install Charts from a repository (either public or private), you need to register it using its URL:\n\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\nURL of Helm Chart repository where index.yaml is placed\n\nList the registered repositories:\n\nhelm repo list\n\nNAME stable bitnami\n\nURL https://charts.helm.sh/stable https://charts.bitnami.com/bitnami\n\nBitnami repo is registered\n\n84\n\n|\n\nChapter 5: Helm\n\nRun helm repo update to get the latest list of Charts for each repo.\n\nAfter registering a repository, you might want to find which Charts are available.\n\nIf you want to deploy a PostgreSQL instance in the cluster, use the search command to search all repositories for a Chart that matches the name:\n\nhelm search repo postgresql\n\nThe outputs are the list of Charts that matches the name, the version of the Chart and PostgreSQL, and a description. Notice the name of the Chart is composed of the repository name and the Chart name, i.e., bitnami/postgresql:\n\nNAME\n\nCHART VERSION\n\nAPP VERSION↳\n\nDESCRIPTION\n\nbitnami/postgresql\n\n10.16.2\n\n11.14.0↳\n\nChart for PostgreSQL, an object-relational data...\n\nbitnami/postgresql-ha\n\n8.2.6\n\n11.14.0↳\n\nChart for PostgreSQL with HA architecture (usin...\n\nstable/postgresql\n\n8.6.4\n\n11.7.0↳\n\nDEPRECATED Chart for PostgreSQL, an object-rela...\n\nstable/pgadmin\n\n1.2.2\n\n4.18.0↳\n\npgAdmin is a web based administration tool for ...\n\nstable/stolon\n\n1.6.5\n\n0.16.0↳\n\nDEPRECATED - Stolon - PostgreSQL cloud native H...\n\nstable/gcloud-sqlproxy DEPRECATED Google Cloud SQL Proxy stable/prometheus-postgres-exporter\n\n0.6.1\n\n1.3.1\n\n1.11↳\n\n0.8.0↳\n\nDEPRECATED A Helm chart for prometheus postgres...\n\nTo deploy the PostgreSQL Chart, run the install command but change the location of the Helm Chart from a local directory to the full name of the Chart (<repo>/ <chart>):\n\nhelm install my-db \\ --set postgresql.postgresqlUsername=my-default,postgresql.↳ postgresqlPassword=postgres,postgresql.postgresqlDatabase=mydb,↳ postgresql.persistence.enabled=false \\ bitnami/postgresql\n\nSets the name of the deployment\n\nOverrides default values to the ones set in the command line\n\nSets the PostgreSQL Chart stored in the Bitnami repo\n\n5.5 Deploying a Chart from a Repository\n\n|\n\n85\n\nAnd a detailed output is shown in the console:\n\nNAME: my-db LAST DEPLOYED: Mon Jan 24 22:33:56 2022 NAMESPACE: asotobue-dev STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: CHART NAME: postgresql CHART VERSION: 10.16.2 APP VERSION: 11.14.0\n\n** Please be patient while the chart is being deployed **\n\nPostgreSQL can be accessed via port 5432 on the following DNS names↳ from within your cluster:\n\nmy-db-postgresql.asotobue-dev.svc.cluster.local - Read/Write connection\n\nTo get the(((\"passwords\", \"Helm Charts\")))(((\"Helm\", \"Charts\", \"pass- words\")))(((\"Charts\", \"passwords\"))) password for \"postgres\" run:\n\nexport POSTGRES_ADMIN_PASSWORD=$(kubectl get secret↳ --namespace asotobue-dev my-db-postgresql -o↳ jsonpath=\"{.data.postgresql-postgres-password}\" | base64 --decode)\n\nTo get the password for \"my-default\" run:\n\nexport POSTGRES_PASSWORD=$(kubectl get secret↳ --namespace asotobue-dev my-db-postgresql -o↳ jsonpath=\"{.data.postgresql-password}\" | base64 --decode)\n\nTo connect to your database run the following command:\n\nkubectl run my-db-postgresql-client --rm --tty -i --restart='Never'↳ --namespace asotobue-dev↳ --image docker.io/bitnami/postgresql:11.14.0-debian-10-r28↳ --env=\"PGPASSWORD=$POSTGRES_PASSWORD\"↳ --command -- psql --host my-db-postgresql -U my-default -d mydb↳ -p 5432\n\nTo connect to your (((\"Helm\", \"Charts\", \"connecting to databases\")))(((\"Charts\", \"databases\", \"connecting to\")))(((\"databases\", \"connecting to\", \"Helm Charts\")))database from outside the cluster execute the following commands:\n\nkubectl port-forward --namespace asotobue-dev svc/my-db-postgresql 5432:5432 & PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U my-default -d mydb -p 5432\n\nInspect the installation by listing pods, Services, StatefulSets, or Secrets:\n\n86\n\n|\n\nChapter 5: Helm\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE my-db-postgresql-0 1/1 Running 0 23s\n\nkubectl get services\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-db-postgresql ClusterIP 172.30.35.1 <none> 5432/TCP 3m33s my-db-postgresql-headless ClusterIP None <none> 5432/TCP 3m33s\n\nkubectl get statefulset\n\nNAME READY AGE my-db-postgresql 1/1 4m24s\n\nkubectl get secrets\n\nNAME TYPE DATA AGE my-db-postgresql Opaque 2 5m23s sh.helm.release.v1.my-db.v1 helm.sh/release.v1 1 5m24s\n\nDiscussion When a third party creates a Chart, there is no direct access to default values or the list of parameters to override. Helm provides a show command to check these values:\n\nhelm show values bitnami/postgresql\n\nAnd shows all the possible values:\n\n## @section Global parameters ## Global Docker image parameters ## Please, note that this will override the image parameters, including dependen cies ## configured to use the global value ## Current available global Docker image parameters: imageRegistry, imagePullSe crets ## and storageClass ##\n\n## @param global.imageRegistry Global Docker image registry ## @param global.imagePullSecrets Global Docker registry secret names as an array ## @param global.storageClass Global StorageClass for Persistent Volume(s) ## global: imageRegistry: \"\" ## E.g. ## imagePullSecrets: ## - myRegistryKeySecretName ## imagePullSecrets: [] ...\n\n5.5 Deploying a Chart from a Repository\n\n|\n\n87\n\n5.6 Deploying a Chart with a Dependency\n\nProblem You want to deploy a Helm Chart that is a dependency of another Chart.\n\nSolution Use the dependencies section in the Chart.yaml file to register other Charts. So far, we’ve seen how to deploy simple services to the cluster, but usually a service might have other dependencies like a database, mail server, distributed cache, etc.\n\nIn the previous section, we saw how to deploy a PostgreSQL server in a Kubernetes cluster. In this section, we’ll see how to deploy a service composed of a Java service returning a list of songs stored in a PostgreSQL database. The application is summar‐ ized in Figure 5-2.\n\nFigure 5-2. Music application overview\n\nLet’s start creating the Chart layout shown in Recipe 5.1:\n\nmkdir music mkdir music/templates\n\ncd music\n\nThen create two template files to deploy the music service.\n\n88\n\n|\n\nChapter 5: Helm\n\nThe templates/deployment.yaml file contains the Kubernetes Deployment definition:\n\napiVersion: apps/v1 kind: Deployment metadata: name: {{ .Chart.Name}} labels: app.kubernetes.io/name: {{ .Chart.Name}} {{- if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{- end }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} spec: containers: - image: \"{{ .Values.image.repository }}:↳ {{ .Values.image.tag | default .Chart.AppVersion}}\" imagePullPolicy: {{ .Values.image.pullPolicy }} name: {{ .Chart.Name}} ports: - containerPort: {{ .Values.image.containerPort }} name: http protocol: TCP env: - name: QUARKUS_DATASOURCE_JDBC_URL value: {{ .Values.postgresql.server | ↳ default (printf \"%s-postgresql\" ( .Release.Name )) | quote }} - name: QUARKUS_DATASOURCE_USERNAME value: {{ .Values.postgresql.postgresqlUsername | ↳ default (printf \"postgres\" ) | quote }} - name: QUARKUS_DATASOURCE_PASSWORD valueFrom: secretKeyRef: name: {{ .Values.postgresql.secretName | ↳ default (printf \"%s-postgresql\" ( .Release.Name )) | quote }} key: {{ .Values.postgresql.secretKey }}\n\nThe templates/service.yaml file contains the Kubernetes Service definition:\n\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: {{ .Chart.Name }} name: {{ .Chart.Name }} spec: ports: - name: http\n\n5.6 Deploying a Chart with a Dependency\n\n|\n\n89\n\nport: {{ .Values.image.containerPort }} targetPort: {{ .Values.image.containerPort }} selector: app.kubernetes.io/name: {{ .Chart.Name }}\n\nAfter the creation of the templates, it’s time for the Chart metadata Chart.yaml file. In this case, we need to define the dependencies of this Chart too. Since the music service uses a PostgreSQL database, we can add the Chart used in Recipe 5.5 as a dependency:\n\napiVersion: v2 name: music description: A Helm chart for Music service\n\ntype: application version: 0.1.0 appVersion: \"1.0.0\"\n\ndependencies: - name: postgresql version: 10.16.2 repository: \"https://charts.bitnami.com/bitnami\"\n\nDependencies section\n\nName of the Chart to add as dependency\n\nChart version\n\nRepository\n\nThe final file is Values.yaml with default configuration values. In this case, a new sec‐ tion is added to configure music deployment with PostgreSQL instance parameters:\n\nimage: repository: quay.io/gitops-cookbook/music tag: \"1.0.0\" pullPolicy: Always containerPort: 8080\n\nreplicaCount: 1\n\npostgresql: server: jdbc:postgresql://music-db-postgresql:5432/mydb postgresqlUsername: my-default secretName: music-db-postgresql secretKey: postgresql-password\n\nPostgreSQL section\n\n90\n\n|\n\nChapter 5: Helm",
      "page_number": 90
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 107-117)",
      "start_page": 107,
      "end_page": 117,
      "detection_method": "topic_boundary",
      "content": "With the Chart in place, the next thing to do is download the dependency Chart and store it in the charts directory. This process is automatically done by running the dependency update command:\n\nhelm dependency update\n\nThe command output shows that one Chart has been downloaded and saved:\n\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"stable\" chart repository ...Successfully got an update from the \"bitnami\" chart repository Update Complete. ⎈Happy Helming!⎈ Saving 1 charts Downloading postgresql from repo https://charts.bitnami.com/bitnami Deleting outdated charts\n\nThe directory layout looks like this:\n\nmusic ├── Chart.lock ├── Chart.yaml ├── charts │ └── postgresql-10.16.2.tgz ├── templates │ ├── deployment.yaml │ └── service.yaml └── values.yaml\n\nPostgreSQL Chart is placed in the correct directory\n\nFinally, we deploy the Chart, setting configuration PostgreSQL deployment values from the command line:\n\nhelm install music-db --set postgresql.postgresqlPassword=postgres postgresql.post- gresqlDatabase=mydb,postgresql.persistence.enabled=false .\n\nThe installation process shows information about the deployment:\n\nNAME: music-db LAST DEPLOYED: Tue Jan 25 17:53:17 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None\n\nInspect the installation by listing pods, Services, StatefulSets, or Secrets:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE music-67dbf986b7-5xkqm 1/1 Running 1 (32s ago) 39s music-db-postgresql-0 1/1 Running 0 39s\n\nkubectl get statefulset\n\n5.6 Deploying a Chart with a Dependency\n\n|\n\n91\n\nNAME READY AGE music-db-postgresql 1/1 53s\n\nkubectl get services\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 40d music ClusterIP 10.104.110.34 <none> 8080/TCP 82s music-db-postgresql ClusterIP 10.110.71.13 <none> 5432/TCP 82s music-db-postgresql-headless ClusterIP None <none> 5432/TCP 82s\n\nWe can validate the access to the music service by using port forwarding to the Kubernetes Service.\n\nOpen a new terminal window and run the following command:\n\nkubectl port-forward service/music 8080:8080\n\nForwarding from 127.0.0.1:8080 -> 8080 Forwarding from [::1]:8080 -> 8080\n\nThe terminal is blocked and it’s normal until you stop the kubectl port-forward process. Thanks to port forwarding, we can access the music service using the local host address and port 8080.\n\nIn another terminal, curl the service:\n\ncurl localhost:8080/song\n\nThe request is sent to the music service deployed in Kubernetes and returns a list of songs:\n\n[ { \"id\": 1, \"artist\": \"DT\", \"name\": \"Quiero Munchies\" }, { \"id\": 2, \"artist\": \"Lin-Manuel Miranda\", \"name\": \"We Don't Talk About Bruno\" }, { \"id\": 3, \"artist\": \"Imagination\", \"name\": \"Just An Illusion\" }, { \"id\": 4, \"artist\": \"Txarango\", \"name\": \"Tanca Els Ulls\" }, { \"id\": 5,\n\n92\n\n|\n\nChapter 5: Helm\n\n\"artist\": \"Halsey\", \"name\": \"Could Have Been Me\" } ]\n\n5.7 Triggering a Rolling Update Automatically\n\nProblem You want to trigger a rolling update of deployment when a ConfigMap object is changed.\n\nSolution Use the sha256sum template function to generate a change on the deployment file.\n\nIn Recipe 4.5, we saw that Kustomize has a ConfigMapGenerator that automatically appends a hash to the ConfigMap metadata name and modifies the deployment file with the new hash when used. Any change on the ConfigMap triggers a rolling update of the deployment.\n\nHelm doesn’t provide a direct way like Kustomize does to update a deployment file when the ConfigMap changes, but there is a template function to calculate a SHA-256 hash of any file and embed the result in the template.\n\nSuppose we’ve got a Node.js application that returns a greeting message. An environ‐ ment variable configures this greeting message, and in the Kubernetes Deployment, this variable is injected from a Kubernetes ConfigMap.\n\nFigure 5-3 shows an overview of the application.\n\nFigure 5-3. Greetings application overview\n\nLet’s create the Helm Chart for the Greetings application; note that we’re not covering the entire process of creating a Chart, but just the essential parts. You can refer to Recipe 5.1 to get started.\n\n5.7 Triggering a Rolling Update Automatically\n\n|\n\n93\n\nCreate a deployment template that injects a ConfigMap as an environment variable. The following listing shows the file:\n\napiVersion: apps/v1 kind: Deployment metadata: name: {{ .Chart.Name}} labels: app.kubernetes.io/name: {{ .Chart.Name}} {{- if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{- end }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} spec: containers: - image: \"{{ .Values.image.repository }}:↳ {{ .Values.image.tag | default .Chart.AppVersion}}\" imagePullPolicy: {{ .Values.image.pullPolicy }} name: {{ .Chart.Name}} ports: - containerPort: {{ .Values.image.containerPort }} name: http protocol: TCP env: - name: GREETING valueFrom: configMapKeyRef: name: {{ .Values.configmap.name}} key: greeting\n\nConfigMap name\n\nProperty key of the ConfigMap\n\nThe initial ConfigMap file is shown in the following listing:\n\napiVersion: v1 kind: ConfigMap metadata: name: greeting-config data: greeting: Aloha\n\n94\n\n|\n\nChapter 5: Helm\n\nSets ConfigMap name\n\nKey/value\n\nCreate a Kubernetes Service template to access the service:\n\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: {{ .Chart.Name }} name: {{ .Chart.Name }} spec: ports: - name: http port: {{ .Values.image.containerPort }} targetPort: {{ .Values.image.containerPort }} selector: app.kubernetes.io/name: {{ .Chart.Name }}\n\nUpdate the values.yaml file with the template configmap parameters:\n\nimage: repository: quay.io/gitops-cookbook/greetings tag: \"1.0.0\" pullPolicy: Always containerPort: 8080\n\nreplicaCount: 1\n\nconfigmap: name: greeting-config\n\nRefers to ConfigMap name\n\nFinally, install the Chart using the install command:\n\nhelm install greetings .\n\nWhen the Chart is deployed, use the kubectl port-forward command in one terminal to get access to the service:\n\nkubectl port-forward service/greetings 8080:8080\n\nAnd curl the service in another terminal window:\n\ncurl localhost:8080 Aloha Ada\n\nConfigured greeting is used\n\n5.7 Triggering a Rolling Update Automatically\n\n|\n\n95\n\nNow, let’s update the ConfigMap file to a new greeting message:\n\napiVersion: v1 kind: ConfigMap metadata: name: greeting-config data: greeting: Hola\n\nNew greeting message\n\nUpdate the appVersion field from the Chart.yaml file to 1.0.1 and upgrade the Chart by running the following command:\n\nhelm upgrade greetings .\n\nRestart the kubectl port-forward process and curl the service again:\n\ncurl localhost:8080 Aloha Alexandra\n\nGreeting message isn’t updated\n\nThe ConfigMap object is updated during the upgrade, but since there are no changes in the Deployment object, there is no restart of the pod; hence the environment variable is not set to the new value. Listing the pods shows no execution of the rolling update:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE greetings-64ddfcb649-m5pml 1/1 Running 0 2m21s\n\nAge value shows no rolling update\n\nFigure 5-4 summarizes the change.\n\nFigure 5-4. Greetings application with new configuration value\n\nLet’s use the sha256sum function to calculate an SHA-256 value of the configmap.yaml file content and set it as a pod annotation, which effectively triggers a rolling update as the pod definition has changed:\n\n96\n\n|\n\nChapter 5: Helm\n\nspec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} annotations: checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\")↳ . | sha256sum }}\n\nIncludes the configmap.yaml file, calculates the SHA-256 value, and sets it as an annotation\n\nUpdate the ConfigMap again with a new value:\n\napiVersion: v1 kind: ConfigMap metadata: name: greeting-config data: greeting: Namaste\n\nNew greeting message\n\nUpdate the appVersion field from Chart.yaml to 1.0.1 and upgrade the Chart by running the following command:\n\nhelm upgrade greetings .\n\nRestart the kubectl port-forward process and curl the service again:\n\ncurl localhost:8080 Namaste Alexandra\n\nGreeting message is the new one\n\nList the pods deployed in the cluster again, and you’ll notice that a rolling update is happening:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE greetings-5c6b86dbbd-2p9bd 0/1 ContainerCreating 0 3s greetings-64ddfcb649-m5pml 1/1 Running 0 2m21s\n\nA rolling update is happening\n\nDescribe the pod to validate that the annotation with the SHA-256 value is present:\n\nkubectl describe pod greetings-5c6b86dbbd-s4n7b\n\n5.7 Triggering a Rolling Update Automatically\n\n|\n\n97\n\nThe output shows all pod parameters. The important one is the annotations placed at the top of the output showing the checksum/config annotation containing the calculated SHA-256 value:\n\nName: greetings-5c6b86dbbd-s4n7b Namespace: asotobue-dev Priority: -3 Priority Class Name: sandbox-users-pods Node: ip-10-0-186-34.ec2.internal/10.0.186.34 Start Time: Thu, 27 Jan 2022 11:55:02 +0100 Labels: app.kubernetes.io/name=greetings pod-template-hash=5c6b86dbbd Annotations: checksum/config:↳ 59e9100616a11d65b691a914cd429dc6011a34e02465173f5f53584b4aa7cba8\n\nCalculated value\n\nFigure 5-5 summarizes the elements that changed when the application was updated.\n\nFigure 5-5. Final overview of the Greetings application\n\n5.8 Final Thoughts In the previous chapter, we saw Kustomize; in this chapter, we’ve seen another tool to help deploy Kubernetes applications.\n\nWhen you need to choose between Kustomize or Helm, you might have questions on which one to use.\n\nIn our experience, the best way to proceed is with Kustomize for simple projects, where only simple changes might be required between new deployments.\n\nIf the project is complex with external dependencies, and several deployment param‐ eters, then Helm is a better option.\n\n98\n\n|\n\nChapter 5: Helm\n\nCHAPTER 6 Cloud Native CI/CD\n\nIn the previous chapter you learned about Helm, a popular templating system for Kubernetes. All the recipes from previous chapters represent a common tooling for creating and managing containers for Kubernetes, and now it’s time to think about the automation on Kubernetes using such tools. Let’s move our focus to the cloud native continuous integration/continuous deployment (CI/CD).\n\nContinuous integration is an automated process that takes new code created by a developer and builds, tests, and runs that code. The cloud native CI refers to the model where cloud computing and cloud services are involved in this process. The benefits from this model are many, such as portable and reproducible workloads across clouds for highly scalable and on-demand use cases. And it also represents the building blocks for GitOps workflows as it enables automation through actions performed via Git.\n\nTekton is a popular open source implementation of a cloud native CI/CD system on top of Kubernetes. In fact, Tekton installs and runs as an extension on a Kubernetes cluster and comprises a set of Kubernetes Custom Resources that define the building blocks you can create and reuse for your pipelines.1 (See Recipe 6.1.)\n\nThe Tekton engine lives inside a Kubernetes cluster and through its API objects represents a declarative way to define the actions to perform. The core components such as Tasks and Pipelines can be used to create a pipeline to generate artifacts and/or containers from a Git repository (see Recipes 6.2, 6.3, and 6.4).\n\nTekton also supports a mechanism for automating the start of a Pipeline with Trig‐ gers. These allow you to detect and extract information from events from a variety of\n\n1 See the Tekton documentation.\n\n99\n\nsources, such as a webhook, and to start Tasks or Pipelines accordingly (see Recipe 6.8).\n\nWorking with private Git repositories is a common use case that Tekton supports nicely (see Recipe 6.4), and building artifacts and creating containers can be done in many ways such as with Buildah (see Recipe 6.5) or Shipwright, which we discussed in Chapter 3. It is also possible to integrate Kustomize (see Recipe 6.9) and Helm (see Recipe 6.10) in order to make the CI part dynamic and take benefit of the rich ecosystem of Kubernetes tools.\n\nTekton is Kubernetes-native solution, thus it’s universal; however, it’s not the only cloud native CI/CD citizen in the market. Other good examples for GitOps-ready workloads are Drone (Recipe 6.11) and GitHub Actions (Recipe 6.12).\n\n6.1 Install Tekton\n\nProblem You want to install Tekton in order to have cloud native CI/CD on your Kubernetes cluster.\n\nSolution Tekton is a Kubernetes-native CI/CD solution that can be installed on top of any Kubernetes cluster. The installation brings you a set of Kubernetes Custom Resources (CRDs) that you can use to compose your Pipelines, as shown in Figure 6-1:\n\nTask\n\nA reusable, loosely coupled number of steps that perform a specific function (e.g., building a container image). Tasks get executed as Kubernetes pods, while steps in a Task map onto containers.\n\nPipeline\n\nA list Tasks needed to build and/or deploy your apps.\n\nTaskRun\n\nThe execution and result of running an instance of a Task.\n\nPipelineRun\n\nThe execution and result of running an instance of a Pipeline, which includes a number of TaskRuns.\n\nTrigger\n\nDetects an event and connects to other CRDs to specify what happens when such an event occurs.\n\n100\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nFigure 6-1. Tekton Pipelines\n\nTo install Tekton, you just need kubectl CLI and a Kubernetes cluster such as Minikube (see Chapter 2).\n\nTekton has a modular structure. You can install all components separately or all at once (e.g., with an Operator):\n\nTekton Pipelines\n\nContains Tasks and Pipelines\n\nTekton Triggers\n\nContains Triggers and EventListeners\n\nTekton Dashboard\n\nA convenient dashboard to visualize Pipelines and logs\n\nTekton CLI\n\nA CLI to manage Tekton objects (start/stop Pipelines and Tasks, check logs)\n\n6.1 Install Tekton\n\n|\n\n101",
      "page_number": 107
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 118-125)",
      "start_page": 118,
      "end_page": 125,
      "detection_method": "topic_boundary",
      "content": "You can also use a Kubernetes Operator to install and manage Tekton components on your cluster. See more details on how from OperatorHub.\n\nFirst you need to install the Tekton Pipelines component. At the time of writing this book, we are using version 0.37.0:\n\nkubectl apply \\ -f https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.37.0/ release.yaml\n\nThe installation will create a new Kubernetes namespace called tekton-pipelines and you should see output similar to the following:\n\nnamespace/tekton-pipelines created podsecuritypolicy.policy/tekton-pipelines created clusterrole.rbac.authorization.k8s.io/tekton-pipelines-controller-cluster-access created clusterrole.rbac.authorization.k8s.io/tekton-pipelines-controller-tenant-access created clusterrole.rbac.authorization.k8s.io/tekton-pipelines-webhook-cluster-access cre- ated role.rbac.authorization.k8s.io/tekton-pipelines-controller created role.rbac.authorization.k8s.io/tekton-pipelines-webhook created role.rbac.authorization.k8s.io/tekton-pipelines-leader-election created role.rbac.authorization.k8s.io/tekton-pipelines-info created serviceaccount/tekton-pipelines-controller created serviceaccount/tekton-pipelines-webhook created clusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-cluster- access created clusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-tenant- access created clusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook-cluster- access created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-leaderelection created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook-leaderelection cre- ated rolebinding.rbac.authorization.k8s.io/tekton-pipelines-info created customresourcedefinition.apiextensions.k8s.io/clustertasks.tekton.dev created customresourcedefinition.apiextensions.k8s.io/pipelines.tekton.dev created customresourcedefinition.apiextensions.k8s.io/pipelineruns.tekton.dev created customresourcedefinition.apiextensions.k8s.io/resolutionrequests.resolution.tek- ton.dev created customresourcedefinition.apiextensions.k8s.io/pipelineresources.tekton.dev created customresourcedefinition.apiextensions.k8s.io/runs.tekton.dev created customresourcedefinition.apiextensions.k8s.io/tasks.tekton.dev created customresourcedefinition.apiextensions.k8s.io/taskruns.tekton.dev created secret/webhook-certs created\n\n102\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/validation.web- hook.pipeline.tekton.dev created mutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.pipeline.tek- ton.dev created validatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.pipe- line.tekton.dev created clusterrole.rbac.authorization.k8s.io/tekton-aggregate-edit created clusterrole.rbac.authorization.k8s.io/tekton-aggregate-view created configmap/config-artifact-bucket created configmap/config-artifact-pvc created configmap/config-defaults created configmap/feature-flags created configmap/pipelines-info created configmap/config-leader-election created configmap/config-logging created configmap/config-observability created configmap/config-registry-cert created deployment.apps/tekton-pipelines-controller created service/tekton-pipelines-controller created horizontalpodautoscaler.autoscaling/tekton-pipelines-webhook created deployment.apps/tekton-pipelines-webhook created service/tekton-pipelines-webhook created\n\nYou can monitor and verify the installation with the following command:\n\nkubectl get pods -w -n tekton-pipelines\n\nYou should see output like this:\n\nNAME READY STATUS RESTARTS AGE tekton-pipelines-controller-5fd68749f5-tz8dv 1/1 Running 0 3m4s tekton-pipelines-webhook-58dcdbfd9b-hswpk 1/1 Running 0 3m4s\n\nThe preceding command goes in watch mode, thus it remains appended. Press Ctrl+C in order to stop it when you see the con‐ troller and webhook pods in Running status.\n\nThen you can install Tekton Triggers. At the time of writing this book, we are using version 0.20.1:\n\nkubectl apply \\ -f https://storage.googleapis.com/tekton-releases/triggers/previous/v0.20.1/ release.yaml\n\nYou should see the following output:\n\npodsecuritypolicy.policy/tekton-triggers created clusterrole.rbac.authorization.k8s.io/tekton-triggers-admin created clusterrole.rbac.authorization.k8s.io/tekton-triggers-core-interceptors created clusterrole.rbac.authorization.k8s.io/tekton-triggers-core-interceptors-secrets created clusterrole.rbac.authorization.k8s.io/tekton-triggers-eventlistener-roles created\n\n6.1 Install Tekton\n\n|\n\n103\n\n104\n\nclusterrole.rbac.authorization.k8s.io/tekton-triggers-eventlistener-clusterroles created role.rbac.authorization.k8s.io/tekton-triggers-admin created role.rbac.authorization.k8s.io/tekton-triggers-admin-webhook created role.rbac.authorization.k8s.io/tekton-triggers-core-interceptors created role.rbac.authorization.k8s.io/tekton-triggers-info created serviceaccount/tekton-triggers-controller created serviceaccount/tekton-triggers-webhook created serviceaccount/tekton-triggers-core-interceptors created clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-controller-admin cre- ated clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-webhook-admin created clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-core-interceptors cre- ated clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-core-interceptors- secrets created rolebinding.rbac.authorization.k8s.io/tekton-triggers-controller-admin created rolebinding.rbac.authorization.k8s.io/tekton-triggers-webhook-admin created rolebinding.rbac.authorization.k8s.io/tekton-triggers-core-interceptors created rolebinding.rbac.authorization.k8s.io/tekton-triggers-info created customresourcedefinition.apiextensions.k8s.io/clusterinterceptors.triggers.tek- ton.dev created customresourcedefinition.apiextensions.k8s.io/clustertriggerbindings.triggers.tek- ton.dev created customresourcedefinition.apiextensions.k8s.io/eventlisteners.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/triggers.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/triggerbindings.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/triggertemplates.triggers.tekton.dev created secret/triggers-webhook-certs created validatingwebhookconfiguration.admissionregistration.k8s.io/validation.web- hook.triggers.tekton.dev created mutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.triggers.tek- ton.dev created validatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.trig- gers.tekton.dev created clusterrole.rbac.authorization.k8s.io/tekton-triggers-aggregate-edit created clusterrole.rbac.authorization.k8s.io/tekton-triggers-aggregate-view created configmap/config-defaults-triggers created configmap/feature-flags-triggers created configmap/triggers-info created configmap/config-logging-triggers created configmap/config-observability-triggers created service/tekton-triggers-controller created deployment.apps/tekton-triggers-controller created service/tekton-triggers-webhook created deployment.apps/tekton-triggers-webhook created deployment.apps/tekton-triggers-core-interceptors created service/tekton-triggers-core-interceptors created clusterinterceptor.triggers.tekton.dev/cel created clusterinterceptor.triggers.tekton.dev/bitbucket created clusterinterceptor.triggers.tekton.dev/github created\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nclusterinterceptor.triggers.tekton.dev/gitlab created secret/tekton-triggers-core-interceptors-certs created\n\nYou can monitor and verify the installation with the following command:\n\nkubectl get pods -w -n tekton-pipelines\n\nYou should see three new pods created and running—tekton-triggers-controller, tekton-triggers-core-interceptors, and tekton-triggers-webhook:\n\nNAME READY STATUS RESTARTS AGE tekton-pipelines-controller-5fd68749f5-tz8dv 1/1 Running 0 27m tekton-pipelines-webhook-58dcdbfd9b-hswpk 1/1 Running 0 27m tekton-triggers-controller-854d44fd5d-8jf9q 1/1 Running 0 105s tekton-triggers-core-interceptors-5454f8785f-dhsrb 1/1 Running 0 104s tekton-triggers-webhook-86d75f875-zmjf4 1/1 Running 0 105s\n\nAfter this you have a fully working Tekton installation on top of your Kubernetes cluster, supporting Pipelines and automation via event Triggers. In addition to that, you could install the Tekton Dashboard in order to visualize Tasks, Pipelines, and logs via a nice UI. At the time of writing this book, we are using version 0.28.0:\n\nkubectl apply \\ -f https://storage.googleapis.com/tekton-releases/dashboard/previous/v0.28.0/ tekton-dashboard-release.yaml\n\nYou should have output similar to the following:\n\ncustomresourcedefinition.apiextensions.k8s.io/extensions.dashboard.tekton.dev cre- ated serviceaccount/tekton-dashboard created role.rbac.authorization.k8s.io/tekton-dashboard-info created clusterrole.rbac.authorization.k8s.io/tekton-dashboard-backend created clusterrole.rbac.authorization.k8s.io/tekton-dashboard-tenant created rolebinding.rbac.authorization.k8s.io/tekton-dashboard-info created clusterrolebinding.rbac.authorization.k8s.io/tekton-dashboard-backend created configmap/dashboard-info created service/tekton-dashboard created deployment.apps/tekton-dashboard created clusterrolebinding.rbac.authorization.k8s.io/tekton-dashboard-tenant created\n\nYou can monitor and verify the installation with the following command:\n\nkubectl get pods -w -n tekton-pipelines\n\nYou should see a new pod created and running—tekton-dashboard:\n\nNAME READY STATUS RESTARTS AGE tekton-dashboard-786b6b5579-sscgz 1/1 Running 0\n\n6.1 Install Tekton\n\n|\n\n105\n\n2m25s tekton-pipelines-controller-5fd68749f5-tz8dv 1/1 Running 1 (7m16s ago) 5d7h tekton-pipelines-webhook-58dcdbfd9b-hswpk 1/1 Running 1 (7m6s ago) 5d7h tekton-triggers-controller-854d44fd5d-8jf9q 1/1 Running 2 (7m9s ago) 5d7h tekton-triggers-core-interceptors-5454f8785f-dhsrb 1/1 Running 1 (7m7s ago) 5d7h tekton-triggers-webhook-86d75f875-zmjf4 1/1 Running 2 (7m9s ago) 5d7h\n\nBy default, the Dashboard is not exposed outside the Kubernetes cluster. You can access it by using the following command:\n\nkubectl port-forward svc/tekton-dashboard 9097:9097 -n tekton-pipelines\n\nThere are several ways to expose internal services in Kubernetes; you could also create an Ingress for that as shown in the Tekton Dashboard documentation.\n\nYou can now browse to http://localhost:9097 to access your Dashboard, as shown in Figure 6-2.\n\nYou can download and install the Tekton CLI for your OS to start creating Tasks and Pipelines from the command line. At the time of writing this book, we are using version 0.25.0.\n\nFigure 6-2. Tekton Dashboard\n\nFinally, verify that tkn and Tekton are configured correctly:\n\n106\n\n|\n\nChapter 6: Cloud Native CI/CD\n\ntkn version\n\nYou should get the following output:\n\nClient version: 0.25.0 Pipeline version: v0.37.0 Triggers version: v0.20.1 Dashboard version: v0.28.0\n\nSee Also\n\n• Tekton Getting Started\n\n6.2 Create a Hello World Task\n\nProblem You want to start using Tekton by exploring Tasks and creating a sample one.\n\nSolution In Tekton, a Task defines a series of steps that run sequentially to perform logic that the Task requires. Every Task runs as a pod on your Kubernetes cluster, with each step running in its own container. While steps within a Task are sequential, Tasks can be executed inside a Pipeline in parallel. Therefore, Tasks are the building blocks for running Pipelines with Tekton.\n\nLet’s create a Hello World Task:\n\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: hello spec: steps: - name: say-hello image: registry.access.redhat.com/ubi8/ubi command: - /bin/bash args: ['-c', 'echo Hello GitOps Cookbook reader!']\n\nThe API as an object of kind Task\n\nThe name of the Task\n\nThe list of steps contained within this Task, in this case just one\n\nThe name of the step\n\n6.2 Create a Hello World Task\n\n|\n\n107\n\nThe container image where the step starts\n\nFirst you need to create this resource in Kubernetes:\n\nkubectl create -f helloworld-task.yaml\n\nYou should get the following output:\n\ntask.tekton.dev/hello created\n\nYou can verify that the object has been created in your current Kubernetes namespace:\n\nkubectl get tasks\n\nYou should get output similar to the following:\n\nNAME AGE hello 90s\n\nNow you can start your Tekton Task with tkn CLI:\n\ntkn task start --showlog hello\n\nYou should get output similar to the following:\n\nTaskRun started: hello-run-8bmzz Waiting for logs to be available... [say-hello] Hello World\n\nA TaskRun is the API representation of a running Task. See Recipe 6.3 for more details.\n\nSee Also\n\n• Tekton Task documentation\n\n6.3 Create a Task to Compile and Package an App from Git\n\nProblem You want to automate compiling and packaging an app from Git on Kubernetes with Tekton.\n\nSolution As seen in Recipe 6.2, Tekton Tasks have a flexible mechanism to add a list of sequential steps to automate actions. The idea is to create a list of Tasks with a chain\n\n108\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nof input/output that can be used to compose Pipelines. Therefore a Task can contain a series of optional fields for a better control over the resource:\n\ninputs\n\nThe resources ingested by the Task.\n\noutputs\n\nThe resources produced by the Task.\n\nparams\n\nThe parameters that will be used in the Task steps. Each parameter has:\n\nname\n\nThe name of the parameter.\n\ndescription\n\nThe description of the parameter.\n\ndefault\n\nThe default value of the parameter.\n\nresults\n\nThe names under which Tasks write execution results.\n\nworkspaces\n\nThe paths to volumes needed by the Task.\n\nvolumes\n\nThe Task can also mount external volumes using the volumes attribute.\n\nThe following example, as illustrated in Figure 6-3, shows a Task named build-app that clones the sources using the git command and lists the source code in output.\n\nFigure 6-3. build-app Task\n\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: build-app spec: workspaces: - name: source description: The git repo will be cloned onto the volume backing this work\n\n6.3 Create a Task to Compile and Package an App from Git\n\n|\n\n109",
      "page_number": 118
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 126-135)",
      "start_page": 126,
      "end_page": 135,
      "detection_method": "topic_boundary",
      "content": "110\n\nspace params: - name: contextDir description: the context dir within source default: quarkus - name: tlsVerify description: tls verify type: string default: \"false\" - name: url default: https://github.com/gitops-cookbook/tekton-tutorial-greeter.git - name: revision default: master - name: subdirectory default: \"\" - name: sslVerify description: defines if http.sslVerify should be set to true or false in the global git config type: string default: \"false\" steps: - image: 'gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git- init:v0.21.0' name: clone resources: {} script: | CHECKOUT_DIR=\"$(workspaces.source.path)/$(params.subdirectory)\" cleandir() { # Delete any existing contents of the repo directory if it exists. # # We don't just \"rm -rf $CHECKOUT_DIR\" because $CHECKOUT_DIR might be \"/\" # or the root of a mounted volume. if [[ -d \"$CHECKOUT_DIR\" ]] ; then # Delete non-hidden files and directories rm -rf \"$CHECKOUT_DIR\"/* # Delete files and directories starting with . but excluding .. rm -rf \"$CHECKOUT_DIR\"/.[!.]* # Delete files and directories starting with .. plus any other charac ter rm -rf \"$CHECKOUT_DIR\"/..?* fi } /ko-app/git-init \\ -url \"$(params.url)\" \\ -revision \"$(params.revision)\" \\ -path \"$CHECKOUT_DIR\" \\ -sslVerify=\"$(params.sslVerify)\" cd \"$CHECKOUT_DIR\" RESULT_SHA=\"$(git rev-parse HEAD)\" - name: build-sources image: gcr.io/cloud-builders/mvn command: - mvn args:\n\n|\n\nChapter 6: Cloud Native CI/CD\n\n-DskipTests - clean - install env: - name: user.home value: /home/tekton workingDir: \"/workspace/source/$(params.contextDir)\"\n\nA Task step and Pipeline Task can share a common filesystem via a Tekton workspace. The workspace could be either backed by something like Persistent‐ VolumeClaim (PVC) and a ConfigMap, or just ephemeral (emptyDir).\n\nA Task can have parameters; this feature makes the execution dynamic.\n\nLet’s create the Task with the following command:\n\nkubectl create -f build-app-task.yaml\n\nYou should get output similar to the following:\n\ntask.tekton.dev/build-app created\n\nYou can verify that the object has been created in your current Kubernetes namespace:\n\nkubectl get tasks\n\nYou should get output similar to the following:\n\nNAME AGE build-app 3s\n\nYou can also list the Task with the tkn CLI:\n\ntkn task ls\n\nYou should get output similar to the following:\n\nNAME DESCRIPTION AGE build-app 10 seconds ago\n\nWhen you start a Task, a new TaskRun object is created. TaskRuns are the API representation of a running Task; thus you can create it with the tkn CLI using the following command:\n\ntkn task start build-app \\ --param contextDir='quarkus' \\ --workspace name=source,emptyDir=\"\" \\ --showlog\n\n6.3 Create a Task to Compile and Package an App from Git\n\n|\n\n111\n\nWhen parameters are used inside a Task or Pipeline, you will be prompted to add new values or confirm default ones, if any. In order to use the default values from the Task defintion without prompting for values, you can use the --use-param-defaults option.\n\nYou should get output similar to the following:\n\n? Value for param `tlsVerify` of type `string`? (Default is `false`) false ? Value for param `url` of type `string`? (Default is `https:// github.com/gitops-cookbook/tekton-tutorial-greeter.git`) https://github.com/gitops- cookbook/tekton-tutorial-greeter.git ? Value for param `revision` of type `string`? (Default is `master`) master ? Value for param `subdirectory` of type `string`? (Default is ``) ? Value for param `sslVerify` of type `string`? (Default is `false`) false TaskRun started: build-app-run-rzcd8 Waiting for logs to be available... [clone] {\"level\":\"info\",\"ts\":1659278019.0018234,\"caller\":\"git/ git.go:169\",\"msg\":\"Successfully cloned https://github.com/gitops-cookbook/tekton- tutorial-greeter.git @ d9291c456db1ce29177b77ffeaa9b71ad80a50e6 (grafted, HEAD, ori gin/master) in path /workspace/source/\"} [clone] {\"level\":\"info\",\"ts\":1659278019.0227938,\"caller\":\"git/ git.go:207\",\"msg\":\"Successfully initialized and updated submodules in path /work space/source/\"}\n\n[build-sources] [INFO] Scanning for projects... [build-sources] Downloading from central: https://repo.maven.apache.org/maven2/io/ quarkus/quarkus-universe-bom/1.6.1.Final/quarkus-universe-bom-1.6.1.Final.pom Downloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus- universe-bom/1.6.1.Final/quarkus-universe-bom-1.6.1.Final.pom (412 kB at 118 kB/s) [build-sources] [INFO] ... [build-sources] [INFO] Installing /workspace/source/quarkus/target/tekton-quarkus- greeter.jar to /root/.m2/repository/com/redhat/developers/tekton-quarkus-greeter/ 1.0.0-SNAPSHOT/tekton-quarkus-greeter-1.0.0-SNAPSHOT.jar [build-sources] [INFO] Installing /workspace/source/quar- kus/pom.xml to /root/.m2/repository/com/redhat/developers/tekton-quarkus-greeter/ 1.0.0-SNAPSHOT/tekton-quarkus-greeter-1.0.0-SNAPSHOT.pom [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] BUILD SUCCESS [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] Total time: 04:41 min [build-sources] [INFO] Finished at: 2022-07-31T14:38:22Z [build-sources] [INFO] ------------------------------------------------------------------------\n\nOr, you can create a TaskRun object manually like this:\n\napiVersion: tekton.dev/v1beta1 kind: TaskRun metadata:\n\n112\n\n|\n\nChapter 6: Cloud Native CI/CD\n\ngenerateName: build-app-run- labels: app.kubernetes.io/managed-by: tekton-pipelines tekton.dev/task: build-app spec: params: - name: contextDir value: quarkus - name: revision value: master - name: sslVerify value: \"false\" - name: subdirectory value: \"\" - name: tlsVerify value: \"false\" - name: url value: https://github.com/gitops-cookbook/tekton-tutorial-greeter.git taskRef: kind: Task name: build-app workspaces: - emptyDir: {} name: source\n\nIf you don’t want to specify a name for each TaskRun, you can use the generate Name attribute to let Tekton pick a random one from the string you defined.\n\nHere you list the Task that the TaskRun is referring to.\n\nAnd start it in this way:\n\nkubectl create -f build-app-taskrun.yaml\n\nYou should get output similar to the following:\n\ntaskrun.tekton.dev/build-app-run-65vmh created\n\nYou can also verify it with the tkn CLI:\n\ntkn taskrun ls\n\nYou should get output similar to the following:\n\nNAME STARTED DURATION STATUS build-app-run-65vmh 1 minutes ago 2m37s Succeeded build-app-run-rzcd8 2 minutes ago 3m58s Succeeded\n\nYou can get the logs from the TaskRun by specifying the name of the TaskRun:\n\ntkn taskrun logs build-app-run-65vmh -f\n\n6.3 Create a Task to Compile and Package an App from Git\n\n|\n\n113\n\nSee Also Debugging a TaskRun\n\n6.4 Create a Task to Compile and Package an App from Private Git\n\nProblem You want to use a private Git repository to automate compiling and packaging of an app on Kubernetes with Tekton.\n\nSolution In Recipe 6.3 you saw how to compile and package a sample Java application using a public Git repository, but most of the time people deal with private repos at work, so how do you integrate them? Tekton supports the following authentication schemes for use with Git:\n\n• Basic-auth\n\nSSH•\n\nWith both options you can use a Kubernetes Secret to store your credentials and attach them to the ServiceAccount running your Tekton Tasks or Pipelines.\n\nTekton uses a default service account, however you can override it following the documentation here.\n\nLet’s start with a common example of basic authentication and a popular Git service such as GitHub.\n\nGitHub uses personal access tokens (PATs) as an alternative to using passwords for authentication. You can use a PAT instead of a clear-text password to enhance security.\n\n114\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nFirst you need to create a Secret. You can do this by creating the following YAML file:\n\napiVersion: v1 kind: Secret metadata: name: github-secret annotations: tekton.dev/git-0: https://github.com type: kubernetes.io/basic-auth stringData: username: YOUR_USERNAME password: YOUR_PASSWORD\n\nHere you specify the URL for which Tekton will use this Secret, in this case GitHub\n\nThis is the type of Secret, in this case a basic authentication one\n\nYour Git user, in this case your GitHub user\n\nYou Git password, in this case your GitHub personal access token\n\nYou can now create the Secret with the following command:\n\nkubectl create -f git-secret.yaml\n\nYou should get the following output:\n\nsecret/git-secret created\n\nYou can also avoid writing YAML and do everything with kubectl as follows:\n\nkubectl create secret generic git-secret \\ --type=kubernetes.io/basic-auth \\ --from-literal=username=YOUR_USERNAME \\ --from-literal=password=YOUR_PASSWORD\n\nAnd then you just annotate the Secret as follows:\n\nkubectl annotate secret git-secret \"tekton.dev/git-0=https://github.com\"\n\nOnce you have created and annotated your Secret, you have to attach it to the ServiceAccount running your Tekton Tasks or Pipelines.\n\nLet’s create a new ServiceAccount for this purpose:\n\napiVersion: v1 kind: ServiceAccount metadata: name: tekton-bot-sa secrets: - name: git-secret\n\nList of Secrets attached to this ServiceAccount\n\n6.4 Create a Task to Compile and Package an App from Private Git\n\n|\n\n115\n\nkubectl create -f tekton-bot-sa.yaml\n\nYou should get the following output:\n\nserviceaccount/tekton-bot-sa created\n\nYou can create the ServiceAccount directly with kubectl as follows:\n\nkubectl create serviceaccount tekton-bot-sa\n\nand then patch it to add the secret reference:\n\nkubectl patch serviceaccount tekton-bot-sa -p '{\"secrets\": [{\"name\": \"git-secret\"}]}'\n\nOnce credentials are set up and linked to the ServiceAccount running Tasks or Pipe‐ lines, you can just add the --serviceaccount=<NAME> option to your tkn command, using the Recipe 6.3 example:\n\ntkn task start build-app \\ --serviceaccount='tekton-bot-sa' \\ --param url='https://github.com/gitops-cookbook/tekton-greeter-private.git' \\ --param contextDir='quarkus' \\ --workspace name=source,emptyDir=\"\" \\ --showlog\n\nHere you specify the ServiceAccount to use; this will override the default one at runtime.\n\nHere you can override the default repository with one of your choice. In this example there’s a private repository that you cannot access, but you can create a private repository on your own and test it like this.\n\nYou should get output similar to the following:\n\n... [clone] {\"level\":\"info\",\"ts\":1659354692.1365478,\"caller\":\"git/ git.go:169\",\"msg\":\"Successfully cloned https://github.com/gitops-cookbook/tekton- greeter-private.git @ 5250e1fa185805373e620d1c04a0c48129efd2ee (grafted, HEAD, ori gin/master) in path /workspace/source/\"} [clone] {\"level\":\"info\",\"ts\":1659354692.1546066,\"caller\":\"git/ git.go:207\",\"msg\":\"Successfully initialized and updated submodules in path /work space/source/\"} ... [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] BUILD SUCCESS [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] Total time: 04:30 min [build-sources] [INFO] Finished at: 2022-07-31T15:30:01Z\n\n116\n\n|\n\nChapter 6: Cloud Native CI/CD\n\n[build-sources] [INFO] ------------------------------------------------------------------------\n\nSee Also\n\n• Tekton Authentication\n\n6.5 Containerize an Application Using a Tekton Task and Buildah\n\nProblem You want to compile, package, and containerize your app with a Tekton Task on Kubernetes.\n\nSolution Automation is essential when adopting the cloud native approach, and if you decide to use Kubernetes for your CI/CD workloads, you need to provide a way to package and deploy your applications.\n\nIn fact, Kubernetes per se doesn’t have a built-in mechanism to build containers; it just relies on add-ons such as Tekton or external services for this purpose. That’s why in Chapter 3 we did an overview on how to create containers for packaging applications with various open source tools. In Recipe 3.3 we used Buildah to create a container from a Dockerfile.\n\nThanks to Tekton’s extensible model, you can reuse the same Task defined in Recipe 6.3 to add a step to create a container using the outcomes from the previous steps, as shown in Figure 6-4.\n\nFigure 6-4. Build Push app\n\nThe container can be pushed to a public container registry such as DockerHub or Quay.io, or to a private container registry. Similar to what we have seen in Recipe 6.4 for private Git repositories, pushing a container image to a container registry needs authentication. A Secret needs to be attached to the ServiceAccount running the Task as follows. See Chapter 2 for how to register and use a public registry.\n\n6.5 Containerize an Application Using a Tekton Task and Buildah\n\n|\n\n117\n\nkubectl create secret docker-registry container-registry-secret \\ --docker-server='YOUR_REGISTRY_SERVER' \\ --docker-username='YOUR_REGISTRY_USER' \\ --docker-password='YOUR_REGISTRY_PASS'\n\nsecret/container-registry-secret created\n\nVerify it is present and check that the Secret is of type kubernetes.io/dockercon figjson:\n\nkubectl get secrets\n\nYou should get the following output:\n\nNAME TYPE DATA AGE container-registry-secret kubernetes.io/dockerconfigjson 1 1s\n\nLet’s create a ServiceAccount for this Task:\n\nkubectl create serviceaccount tekton-registry-sa\n\nThen let’s add the previously generated Secret to this ServiceAccount:\n\nkubectl patch serviceaccount tekton-registry-sa \\ -p '{\"secrets\": [{\"name\": \"container-registry-secret\"}]}'\n\nYou should get the following output:\n\nserviceaccount/tekton-registry-sa patched\n\nLet’s add a new step to create a container image and push it to a container registry. In the following example we use the book’s organization repos at Quay.io—quay.io/ gitops-cookbook/tekton-greeter:latest:\n\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: build-push-app spec: workspaces: - name: source description: The git repo will be cloned onto the volume backing this work space params: - name: contextDir description: the context dir within source default: quarkus - name: tlsVerify description: tls verify type: string default: \"false\" - name: url default: https://github.com/gitops-cookbook/tekton-tutorial-greeter.git - name: revision default: master - name: subdirectory\n\n118\n\n|\n\nChapter 6: Cloud Native CI/CD\n\ndefault: \"\" - name: sslVerify description: defines if http.sslVerify should be set to true or false in the global git config type: string default: \"false\" - name: storageDriver type: string description: Storage driver default: vfs - name: destinationImage description: the fully qualified image name default: \"\" steps: - image: 'gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git- init:v0.21.0' name: clone resources: {} script: | CHECKOUT_DIR=\"$(workspaces.source.path)/$(params.subdirectory)\" cleandir() { # Delete any existing contents of the repo directory if it exists. # # We don't just \"rm -rf $CHECKOUT_DIR\" because $CHECKOUT_DIR might be \"/\" # or the root of a mounted volume. if [[ -d \"$CHECKOUT_DIR\" ]] ; then # Delete non-hidden files and directories rm -rf \"$CHECKOUT_DIR\"/* # Delete files and directories starting with . but excluding .. rm -rf \"$CHECKOUT_DIR\"/.[!.]* # Delete files and directories starting with .. plus any other charac ter rm -rf \"$CHECKOUT_DIR\"/..?* fi } /ko-app/git-init \\ -url \"$(params.url)\" \\ -revision \"$(params.revision)\" \\ -path \"$CHECKOUT_DIR\" \\ -sslVerify=\"$(params.sslVerify)\" cd \"$CHECKOUT_DIR\" RESULT_SHA=\"$(git rev-parse HEAD)\" - name: build-sources image: gcr.io/cloud-builders/mvn command: - mvn args: - -DskipTests - clean - install env: - name: user.home value: /home/tekton workingDir: \"/workspace/source/$(params.contextDir)\"\n\n6.5 Containerize an Application Using a Tekton Task and Buildah\n\n|\n\n119",
      "page_number": 126
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 136-143)",
      "start_page": 136,
      "end_page": 143,
      "detection_method": "topic_boundary",
      "content": "name: build-and-push-image image: quay.io/buildah/stable script: | #!/usr/bin/env bash buildah --storage-driver=$STORAGE_DRIVER --tls-verify=$(params.tlsVerify) bud --layers -t $DESTINATION_IMAGE $CONTEXT_DIR buildah --storage-driver=$STORAGE_DRIVER --tls-verify=$(params.tlsVerify) push $DESTINATION_IMAGE docker://$DESTINATION_IMAGE env: - name: DESTINATION_IMAGE value: \"$(params.destinationImage)\" - name: CONTEXT_DIR value: \"/workspace/source/$(params.contextDir)\" - name: STORAGE_DRIVER value: \"$(params.storageDriver)\" workingDir: \"/workspace/source/$(params.contextDir)\" volumeMounts: - name: varlibc mountPath: /var/lib/containers volumes: - name: varlibc emptyDir: {}\n\nLet’s create this Task:\n\nkubectl create -f build-push-app.yaml\n\nYou should get the following output:\n\ntask.tekton.dev/build-push-app created\n\nNow let’s start the Task with the Buildah step creating a container image and with a new parameter destinationImage to specify where to push the resulting container image:\n\ntkn task start build-push-app \\ --serviceaccount='tekton-registry-sa' \\ --param url='https://github.com/gitops-cookbook/tekton-tutorial-greeter.git' \\ --param destinationImage='quay.io/gitops-cookbook/tekton-greeter:latest' \\ --param contextDir='quarkus' \\ --workspace name=source,emptyDir=\"\" \\ --use-param-defaults \\ --showlog\n\nHere you can place your registry; in this example we are using the book’s organi‐ zation repos at Quay.io.\n\nYou should get output similar to the following:\n\n... Downloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/ plexus-utils/3.0.5/plexus-utils-3.0.5.jar (230 kB at 301 kB/s) [build-sources] [INFO] Installing /workspace/source/quarkus/target/tekton-quarkus- greeter.jar to /root/.m2/repository/com/redhat/developers/tekton-quarkus-greeter/ 1.0.0-SNAPSHOT/tekton-quarkus-greeter-1.0.0-SNAPSHOT.jar\n\n120\n\n|\n\nChapter 6: Cloud Native CI/CD\n\n[build-sources] [INFO] Installing /workspace/source/quar- kus/pom.xml to /root/.m2/repository/com/redhat/developers/tekton-quarkus-greeter/ 1.0.0-SNAPSHOT/tekton-quarkus-greeter-1.0.0-SNAPSHOT.pom [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] BUILD SUCCESS [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] Total time: 02:59 min [build-sources] [INFO] Finished at: 2022-08-02T06:18:37Z [build-sources] [INFO] ------------------------------------------------------------------------ [build-and-push-image] STEP 1/2: FROM registry.access.redhat.com/ubi8/openjdk-11 [build-and-push-image] Trying to pull registry.access.redhat.com/ubi8/ openjdk-11:latest... [build-and-push-image] Getting image source signatures [build-and-push-image] Checking if image destination supports signatures [build-and-push-image] Copying blob sha256:1e09a5ee0038fbe06a18e7f355188bbabc387467144abcd435f7544fef395aa1 [build-and-push-image] Copying blob sha256:0d725b91398ed3db11249808d89e688e62e511bbd4a2e875ed8493ce1febdb2c [build-and-push-image] Copying blob sha256:1e09a5ee0038fbe06a18e7f355188bbabc387467144abcd435f7544fef395aa1 [build-and-push-image] Copying blob sha256:0d725b91398ed3db11249808d89e688e62e511bbd4a2e875ed8493ce1febdb2c [build-and-push-image] Copying blob sha256:e441d34134fac91baa79be3e2bb8fb3dba71ba5c1ea012cb5daeb7180a054687 [build-and-push-image] Copying blob sha256:e441d34134fac91baa79be3e2bb8fb3dba71ba5c1ea012cb5daeb7180a054687 [build-and-push-image] Copying config sha256:0c308464b19eaa9a01c3fdd6b63a043c160d4eea85e461bbbb7d01d168f6d993 [build-and-push-image] Writing manifest to image destination [build-and-push-image] Storing signatures [build-and-push-image] STEP 2/2: COPY target/quarkus-app /deployments/ [build-and-push-image] COMMIT quay.io/gitops-cookbook/tekton-greeter:latest [build-and-push-image] --> 42fe38b4346 [build-and-push-image] Successfully tagged quay.io/gitops-cookbook/tekton- greeter:latest [build-and-push-image] 42fe38b43468c3ca32262dbea6fd78919aba2bd35981cd4f71391e07786c9e21 [build-and-push-image] Getting image source signatures [build-and-push-image] Copying blob sha256:647a854c512bad44709221b6b0973e884f29bcb5a380ee32e95bfb0189b620e6 [build-and-push-image] Copying blob sha256:f2ee6b2834726167d0de06f3bbe65962aef79855c5ede0d2ba93b4408558d9c9 [build-and-push-image] Copying blob sha256:8e0e04b5c700a86f4a112f41e7e767a9d7c539fe3391611313bf76edb07eeab1 [build-and-push-image] Copying blob sha256:69c55192bed92cbb669c88eb3c36449b64ac93ae466abfff2a575273ce05a39e [build-and-push-image] Copying config sha256:42fe38b43468c3ca32262dbea6fd78919aba2bd35981cd4f71391e07786c9e21 [build-and-push-image] Writing manifest to image destination [build-and-push-image] Storing signatures\n\n6.5 Containerize an Application Using a Tekton Task and Buildah\n\n|\n\n121\n\nSee Also\n\n• Buildah\n\n• Docker Authentication for Tekton\n\n6.6 Deploy an Application to Kubernetes Using a Tekton Task\n\nProblem You want to deploy an application from a container image to Kubernetes with a Tekton Task.\n\nSolution While in Recipes 6.3, 6.4, and 6.5 we have listed a Tekton Task that is useful for continuous integration (CI), in this recipe we’ll start having a look at the Continous Deployment (CD) part by deploying an existing container image to Kubernetes.\n\nWe can reuse the container image we created and pushed in Recipe 6.5, available at quay.io/gitops-cookbook/tekton-greeter:latest:\n\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: kubectl spec: params: - name: SCRIPT description: The kubectl CLI arguments to run type: string default: \"kubectl help\" steps: - name: oc image: quay.io/openshift/origin-cli:latest script: | #!/usr/bin/env bash\n\n$(params.SCRIPT)\n\nFor this example we are using kubectl from this container image, which also contains OpenShift CLI and it has an smaller size compared to gcr.io/cloud- builders/kubectl.\n\nLet’s create this Task:\n\nkubectl create -f kubectl-task.yaml\n\n122\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nYou should get the following output:\n\ntask.tekton.dev/kubectl created\n\nAs discussed in Recipe 6.5, Tekton uses a default ServiceAccount for running Tasks and Pipelines, unless a specific one is defined at runtime or overridden at a global scope. The best practice is always to create a specific ServiceAccount for a particular action, so let’s create one named tekton-deployer-sa for this use case as follows:\n\nkubectl create serviceaccount tekton-deployer-sa\n\nYou should get the following output:\n\nserviceaccount/tekton-deployer-sa created\n\nA ServiceAccount needs permission to deploy an application to Kubernetes. Roles and RoleBindings are API objects used to map a certain permission to a user or a ServiceAccount.\n\nYou first define a Role named pipeline-role for the ServiceAccount running the Tekton Task with permissions to deploy apps:\n\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: task-role rules: - apiGroups: - \"\" resources: - pods - services - endpoints - configmaps - secrets verbs: - \"*\" - apiGroups: - apps resources: - deployments - replicasets verbs: - \"*\" - apiGroups: - \"\" resources: - pods verbs: - get - apiGroups: - apps resources: - replicasets\n\n6.6 Deploy an Application to Kubernetes Using a Tekton Task\n\n|\n\n123\n\nverbs: - get\n\nNow you need to bind the Role to the ServiceAccount:\n\napiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: task-role-binding roleRef: kind: Role name: task-role apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: tekton-deployer-sa\n\nNow you can create the two resources as follows:\n\nkubectl create -f task-role.yaml kubectl create -f task-role-binding.yaml\n\nYou should get the following output:\n\nrole.rbac.authorization.k8s.io/task-role created rolebinding.rbac.authorization.k8s.io/task-role-binding created\n\nFinally, you can define a TaskRun as follows:\n\napiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: kubectl-taskrun spec: serviceAccountName: tekton-deployer-sa taskRef: name: kubectl params: - name: SCRIPT value: | kubectl create deploy tekton-greeter --image=quay.io/gitops-cookbook/ tekton-greeter:latest\n\nAnd run it in this way:\n\nkubectl create -f kubectl-taskrun.yaml\n\nYou should get the following output:\n\ntaskrun.tekton.dev/kubectl-run created\n\nYou can check the logs to see the results:\n\ntkn taskrun logs kubectl-run -f\n\nYou should get output similar to the following:\n\n124\n\n|\n\nChapter 6: Cloud Native CI/CD\n\n? Select taskrun: kubectl-run started 9 seconds ago [oc] deployment.apps/tekton-greeter created\n\nAfter a few seconds you should see the Deployment in Ready state:\n\nkubectl get deploy\n\nNAME READY UP-TO-DATE AVAILABLE AGE tekton-greeter 1/1 1 0 30s\n\nThe first time might take a while due to the time it takes to pull the container image.\n\nCheck if the app is available, expose the Deployment, and forward Kubernetes traffic to your workstation to test it:\n\nkubectl expose deploy/tekton-greeter --port 8080 kubectl port-forward svc/tekton-greeter 8080:8080\n\nIn another terminal, run this command:\n\ncurl localhost:8080\n\nYou should see the following output:\n\nMeeow!! from Tekton ----\n\nSee Also\n\n• Tekton Task\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\nProblem You want to create a Pipeline to compile, package, and deploy an app on Kubernetes with Tekton.\n\nSolution In the previous recipes we have seen how to create Tasks to execute one or more steps sequentially to build apps. In this recipe we will discuss Tekton Pipelines, a collection of Tasks that you can define and compose in a specific order of execution, either sequentially or in parallel, as you can see in Figure 6-5.\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\n|\n\n125\n\nFigure 6-5. Tekton Pipelines flows\n\nTekton Pipelines supports parameters and a mechanism to exchange outcomes between different Tasks. For instance, using the examples shown in Recipes 6.5 and 6.6:\n\nkubectl patch serviceaccount tekton-deployer-sa \\ -p '{\"secrets\": [{\"name\": \"container-registry-secret\"}]}'\n\napiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: tekton-greeter-pipeline spec: params: - name: GIT_REPO type: string - name: GIT_REF type: string - name : DESTINATION_IMAGE type: string - name : SCRIPT type: string tasks: - name: build-push-app taskRef: name: build-push-app params: - name: url value: \"$(params.GIT_REPO)\" - name: revision value: \"$(params.GIT_REF)\" - name: destinationImage value: \"$(params.DESTINATION_IMAGE)\"\n\n126\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nworkspaces: - name: source - name: deploy-app taskRef: name: kubectl params: - name: SCRIPT value: \"$(params.SCRIPT)\" workspaces: - name: source runAfter: - build-push-app workspaces: - name: source\n\nPipeline parameters\n\nA list of Tasks for the Pipeline\n\nThe exact name of the Task to use\n\nYou can decide the order with the runAfter field to indicate that a Task must execute after one or more other Tasks\n\nOne or more common Workspaces used to share data between Tasks\n\nLet’s create the Pipeline as follows:\n\nkubectl create -f tekton-greeter-pipeline.yaml\n\nYou should get the following output:\n\npipeline.tekton.dev/tekton-greeter-pipeline created\n\nSimilarly to TaskRuns, you can run this Pipeline by creating a PipelineRun resource as follows:\n\napiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: tekton-greeter-pipeline-run- spec: params: - name: GIT_REPO value: https://github.com/gitops-cookbook/tekton-tutorial-greeter.git - name: GIT_REF value: \"master\" - name: DESTINATION_IMAGE value: \"quay.io/gitops-cookbook/tekton-greeter:latest\" - name: SCRIPT value: | kubectl create deploy tekton-greeter --image=quay.io/gitops-cookbook/ tekton-greeter:latest\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\n|\n\n127",
      "page_number": 136
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 144-153)",
      "start_page": 144,
      "end_page": 153,
      "detection_method": "topic_boundary",
      "content": "pipelineRef: name: tekton-greeter-pipeline workspaces: - name: source emptyDir: {}\n\nYou can run the Pipeline by creating this PipelineRun object as follows:\n\nkubectl create -f tekton-greeter-pipelinerun.yaml\n\nYou can check the status:\n\ntkn pipelinerun ls\n\nNAME STARTED DURATION STATUS tekton-greeter-pipeline-run-ntl5r 7 seconds ago --- Running\n\nNow that you have seen how to reuse existing Tasks within a Pipeline, it’s a good time to introduce the Tekton Hub, a web-based platform for developers to discover, share, and contribute Tasks and Pipelines for Tekton (see Figure 6-6).\n\nFigure 6-6. Tekton Hub\n\nYou can implement the same Pipeline with Tasks already available in the Hub. In our case, we have:\n\ngit-clone\n\nTask that clones a repo from the provided URL into the output Workspace.\n\nbuildah\n\nTask that builds source into a container image and can push it to a container registry.\n\n128\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nkubernetes-actions\n\nThe generic kubectl CLI task, which can be used to run all kinds of k8s commands.\n\nFirst let’s add them to our namespace as follows:\n\ntkn hub install task git-clone tkn hub install task maven tkn hub install task buildah tkn hub install task kubernetes-actions\n\nYou should get output similar to the following to confirm they are installed properly in your namespace:\n\nTask git-clone(0.7) installed in default namespace Task maven(0.2) installed in default namespace Task buildah(0.4) installed in default namespace Task kubernetes-actions(0.2) installed in default namespace\n\nYou can cross-check it with the following command:\n\nkubectl get tasks\n\nYou should get output similar to the following:\n\nNAME AGE ... buildah 50s git-clone 52s kubernetes-actions 49s maven 51s ...\n\nSome Tekton installations like the one made with the Operator for OpenShift Pipelines provide a common list of useful Tasks such as those just listed, provided as ClusterTasks. ClusterTasks are Tasks available for all namespaces within the Kubernetes cluster. Check if your installation already provides some with this command: kubectl get clustertasks.\n\nNow the Pipeline has four Tasks, as you can see in Figure 6-7.\n\nFigure 6-7. Pipeline\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\n|\n\n129\n\nIn this example you’ll see a PersistentVolumeClaim as a Workspace because here the data is shared among different Tasks so we need to persist it:\n\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: app-source-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi\n\nAs usual, you can create the resource with kubectl:\n\nkubectl create -f app-source-pvc.yaml\n\nYou should see the following output:\n\npersistentvolumeclaim/app-source-pvc created\n\nkubectl get pvc\n\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE app-source-pvc Bound pvc-e85ade46-aaca-4f3f-b644-d8ff99fd9d5e 1Gi RWO standard 61s\n\nIn Minikube you have a default StorageClass that provides dynamic storage for the cluster. If you are using another Kubernetes cluster, please make sure you have a dynamic storage support.\n\nThe Pipeline definition now is:\n\napiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: tekton-greeter-pipeline-hub spec: params: - default: https://github.com/gitops-cookbook/tekton-tutorial-greeter.git name: GIT_REPO type: string - default: master name: GIT_REF type: string - default: quay.io/gitops-cookbook/tekton-greeter:latest name: DESTINATION_IMAGE type: string - default: kubectl create deploy tekton-greeter --image=quay.io/gitops-cookbook/ tekton-greeter:latest\n\n130\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nname: SCRIPT type: string - default: ./Dockerfile name: CONTEXT_DIR type: string - default: . name: IMAGE_DOCKERFILE type: string - default: . name: IMAGE_CONTEXT_DIR type: string tasks: - name: fetch-repo params: - name: url value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REF) - name: deleteExisting value: \"true\" - name: verbose value: \"true\" taskRef: kind: Task name: git-clone workspaces: - name: output workspace: app-source - name: build-app params: - name: GOALS value: - -DskipTests - clean - package - name: CONTEXT_DIR value: $(params.CONTEXT_DIR) runAfter: - fetch-repo taskRef: kind: Task name: maven workspaces: - name: maven-settings workspace: maven-settings - name: source workspace: app-source - name: build-push-image params: - name: IMAGE value: $(params.DESTINATION_IMAGE) - name: DOCKERFILE value: $(params.IMAGE_DOCKERFILE) - name: CONTEXT\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\n|\n\n131\n\nvalue: $(params.IMAGE_CONTEXT_DIR) runAfter: - build-app taskRef: kind: Task name: buildah workspaces: - name: source workspace: app-source - name: deploy params: - name: script value: $(params.SCRIPT) runAfter: - build-push-image taskRef: kind: Task name: kubernetes-actions workspaces: - name: app-source - name: maven-settings\n\nLet’s create the resource:\n\nkubectl create -f tekton-greeter-pipeline-hub.yaml\n\nWe are using the same Secret and ServiceAccount defined in Recipe 6.5 to log in against Quay.io in order to push the container image.\n\nYou can now start the Pipeline as follows:\n\ntkn pipeline start tekton-greeter-pipeline-hub \\ --serviceaccount='tekton-deployer-sa' \\ --param GIT_REPO='https://github.com/gitops-cookbook/tekton-tutorial- greeter.git' \\ --param GIT_REF='master' \\ --param CONTEXT_DIR='quarkus' \\ --param DESTINATION_IMAGE='quay.io/gitops-cookbook/tekton-greeter:latest' \\ --param IMAGE_DOCKERFILE='quarkus/Dockerfile' \\ --param IMAGE_CONTEXT_DIR='quarkus' \\ --param SCRIPT='kubectl create deploy tekton-greeter --image=quay.io/gitops- cookbook/tekton-greeter:latest' \\ --workspace name=app-source,claimName=app-source-pvc \\ --workspace name=maven-settings,emptyDir=\"\" \\ --use-param-defaults \\ --showlog\n\n[fetch-repo : clone] + CHECKOUT_DIR=/workspace/output/ [fetch-repo : clone] + /ko-app/git-init '-url=https://github.com/gitops-cookbook/ tekton-tutorial-greeter.git' '-revision=master' '-refspec=' '-path=/workspace/out put/' '-sslVerify=true' '-submodules=true' '-depth=1' '-sparseCheckoutDirectories='\n\n132\n\n|\n\nChapter 6: Cloud Native CI/CD\n\n[fetch-repo : clone] {\"level\":\"info\",\"ts\":1660819038.5526028,\"caller\":\"git/ git.go:170\",\"msg\":\"Successfully cloned https://github.com/gitops-cookbook/tekton- tutorial-greeter.git @ d9291c456db1ce29177b77ffeaa9b71ad80a50e6 (grafted, HEAD, ori gin/master) in path /workspace/output/\"} [fetch-repo : clone] {\"level\":\"info\",\"ts\":1660819038.5722632,\"caller\":\"git/ git.go:208\",\"msg\":\"Successfully initialized and updated submodules in path /work space/output/\"} [fetch-repo : clone] + cd /workspace/output/ [fetch-repo : clone] + git rev-parse HEAD [fetch-repo : clone] + RESULT_SHA=d9291c456db1ce29177b77ffeaa9b71ad80a50e6 [fetch-repo : clone] + EXIT_CODE=0 [fetch-repo : clone] + '[' 0 '!=' 0 ] [fetch-repo : clone] + printf '%s' d9291c456db1ce29177b77ffeaa9b71ad80a50e6 [fetch-repo : clone] + printf '%s' https://github.com/gitops-cookbook/tekton- tutorial-greeter.git ... [build-app : mvn-goals] [INFO] [org.jboss.threads] JBoss Threads version 3.1.1.Final [build-app : mvn-goals] [INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 1296ms [build-app : mvn-goals] [INFO] ------------------------------------------------------------------------ [build-app : mvn-goals] [INFO] BUILD SUCCESS [build-app : mvn-goals] [INFO] ------------------------------------------------------------------------ [build-app : mvn-goals] [INFO] Total time: 03:18 min [build-app : mvn-goals] [INFO] Finished at: 2022-08-18T10:31:00Z [build-app : mvn-goals] [INFO] ------------------------------------------------------------------------ [build-push-image : build] STEP 1/2: FROM registry.access.redhat.com/ubi8/ openjdk-11 [build-push-image : build] Trying to pull registry.access.redhat.com/ubi8/ openjdk-11:latest... [build-push-image : build] Getting image source signatures [build-push-image : build] Checking if image destination supports signatures [build-push-image : build] Copying blob sha256:e441d34134fac91baa79be3e2bb8fb3dba71ba5c1ea012cb5daeb7180a054687 [build-push-image : build] Copying blob sha256:1e09a5ee0038fbe06a18e7f355188bbabc387467144abcd435f7544fef395aa1 [build-push-image : build] Copying blob sha256:0d725b91398ed3db11249808d89e688e62e511bbd4a2e875ed8493ce1febdb2c [build-push-image : build] Copying blob sha256:e441d34134fac91baa79be3e2bb8fb3dba71ba5c1ea012cb5daeb7180a054687 [build-push-image : build] Copying blob sha256:1e09a5ee0038fbe06a18e7f355188bbabc387467144abcd435f7544fef395aa1 [build-push-image : build] Copying blob sha256:0d725b91398ed3db11249808d89e688e62e511bbd4a2e875ed8493ce1febdb2c [build-push-image : build] Copying config sha256:0c308464b19eaa9a01c3fdd6b63a043c160d4eea85e461bbbb7d01d168f6d993 [build-push-image : build] Writing manifest to image destination [build-push-image : build] Storing signatures [build-push-image : build] STEP 2/2: COPY target/quarkus-app /deployments/ [build-push-image : build] COMMIT quay.io/gitops-cookbook/tekton-greeter:latest [build-push-image : build] --> c07e36a8e61\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\n|\n\n133\n\n[build-push-image : build] Successfully tagged quay.io/gitops-cookbook/tekton- greeter:latest [build-push-image : build] c07e36a8e6104d2e5c7d79a6cd34cd7b44eb093c39ef6c1487a37d7bd2305b8a [build-push-image : build] Getting image source signatures [build-push-image : build] Copying blob sha256:7853a7797845542e3825d4f305e4784ea7bf492cd4364fc93b9afba3ac0c9553 [build-push-image : build] Copying blob sha256:8e0e04b5c700a86f4a112f41e7e767a9d7c539fe3391611313bf76edb07eeab1 [build-push-image : build] Copying blob sha256:647a854c512bad44709221b6b0973e884f29bcb5a380ee32e95bfb0189b620e6 [build-push-image : build] Copying blob sha256:69c55192bed92cbb669c88eb3c36449b64ac93ae466abfff2a575273ce05a39e [build-push-image : build] Copying config sha256:c07e36a8e6104d2e5c7d79a6cd34cd7b44eb093c39ef6c1487a37d7bd2305b8a [build-push-image : build] Writing manifest to image destination [build-push-image : build] Storing signatures [build-push-image : build] sha256:12dd3deb6305b9e125309b68418d0bb81f805e0fe7ac93942dc94764aee9f492quay.io/ gitops-cookbook/tekton-greeter:latest [deploy : kubectl] deployment.apps/tekton-greeter created\n\nYou can use the Tekton Dashboard to create and visualize your running Tasks and Pipelines as shown in Figure 6-8.\n\nFigure 6-8. Tekton Dashboard TaskRuns\n\n134\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nSee Also\n\n• Tekton Catalog\n\n6.8 Using Tekton Triggers to Compile and Package an Application Automatically When a Change Occurs on Git\n\nProblem You want to automate your CI/CD Pipelines when a change on Git occurs.\n\nSolution Tekton Triggers is the Tekton component that brings automation for Tasks and Pipelines with Tekton. It is an interesting feature for a GitOps strategy for cloud native CI/CD as it supports external events from a large set of sources such as Git events (Git push or pull requests).\n\nMost Git repository servers support the concept of webhooks, calling to an external source via HTTP(S) when a change in the code repository happens. Tekton provides an API endpoint that supports receiving hooks from remote systems in order to trig‐ ger builds. By pointing the code repository’s hook at the Tekton resources, automated code/build/deploy pipelines can be achieved.\n\nThe installation of Tekton Triggers, which we discussed in Recipe 6.1, brings a set of CRDs to manage event handling for Tasks and Pipelines. In this recipe we will use the following, as illustrated also in Figure 6-9:\n\nFigure 6-9. Tekton Triggers\n\nTriggerTemplate\n\nA template for newly created resources. It supports parameters to create specific PipelineRuns.\n\nTriggerBinding\n\nValidates events and extracts payload fields.\n\n6.8 Using Tekton Triggers to Compile and Package an Application Automatically When a Change Occurs on Git\n\n|\n\n135\n\nEventListener\n\nConnects TriggerBindings and TriggerTemplates into an addressable endpoint (the event sink). It uses the extracted event parameters from each Trigger Binding (and any supplied static parameters) to create the resources specified in the corresponding TriggerTemplate. It also optionally allows an external service to preprocess the event payload via the interceptor field.\n\nBefore creating these resources, you need to set up permissions to let Tekton Triggers create Pipelines and Tasks. You can use the setup available from the book’s repository with the following command:\n\nkubectl apply \\ -f https://raw.githubusercontent.com/gitops-cookbook/chapters/main/chapters/ch06/ rbac.yaml\n\nThis will create a new ServiceAccount named tekton-triggers-sa that has the per‐ missions needed to interact with the Tekton Pipelines component. As confirmation, from the previous command you should get the following output:\n\nserviceaccount/tekton-triggers-sa created rolebinding.rbac.authorization.k8s.io/triggers-example-eventlistener-binding con- figured clusterrolebinding.rbac.authorization.k8s.io/triggers-example-eventlistener- clusterbinding configured\n\nYou can now add automation to your Pipelines like the one we defined in Recipe 6.7 creating these three resources:\n\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerTemplate metadata: name: tekton-greeter-triggertemplate spec: params: - name: git-revision - name: git-commit-message - name: git-repo-url - name: git-repo-name - name: content-type - name: pusher-name resourcetemplates: - apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: labels: tekton.dev/pipeline: tekton-greeter-pipeline-hub name: tekton-greeter-pipeline-webhook-$(uid) spec: params: - name: GIT_REPO value: $(tt.params.git-repo-url) - name: GIT_REF\n\n136\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nvalue: $(tt.params.git-revision) serviceAccountName: tekton-triggers-example-sa pipelineRef: name: tekton-greeter-pipeline-hub workspaces: - name: app-source persistentVolumeClaim: claimName: app-source-pvc - name: maven-settings emptyDir: {}\n\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: tekton-greeter-triggerbinding spec: params: - name: git-repo-url value: $(body.repository.clone_url) - name: git-revision value: $(body.after)\n\napiVersion: triggers.tekton.dev/v1alpha1 kind: EventListener metadata: name: tekton-greeter-eventlistener spec: serviceAccountName: tekton-triggers-example-sa triggers: - bindings: - ref: tekton-greeter-triggerbinding template: ref: tekton-greeter-triggertemplate\n\nYou can create the resources just listed as follows:\n\nkubectl create -f tekton-greeter-triggertemplate.yaml kubectl create -f tekton-greeter-triggerbinding.yaml kubectl create -f tekton-greeter-eventlistener.yaml\n\nYou should get the following output:\n\ntriggertemplate.triggers.tekton.dev/tekton-greeter-triggertemplate created triggerbinding.triggers.tekton.dev/tekton-greeter-triggerbinding created eventlistener.triggers.tekton.dev/tekton-greeter-eventlistener created\n\nContextually, a new pod is created representing the EventListener:\n\nkubectl get pods\n\nYou should get output similar to the following:\n\nNAME READY STATUS RESTARTS AGE el-tekton-greeter-eventlistener-5db7b9fcf9-6nrgx 1/1 Running 0 10s\n\nThe EventListener pod listens for events at a specified port, and it is bound to a Kubernetes Service:\n\n6.8 Using Tekton Triggers to Compile and Package an Application Automatically When a Change Occurs on Git\n\n|\n\n137",
      "page_number": 144
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 154-167)",
      "start_page": 154,
      "end_page": 167,
      "detection_method": "topic_boundary",
      "content": "kubectl get svc\n\nYou should get output similar to the following:\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP↳ PORT(S) AGE el-tekton-greeter-eventlistener ClusterIP 10.100.36.199 <none> ↳ 8080/TCP,9000/TCP 10s ...\n\nIf you are running your Git server outside the cluster (e.g., GitHub or GitLab), you need to expose the Service, for example, with an Ingress. Afterwards you can configure webhooks on your Git server using the EventListener URL associated to your Ingress.\n\nWith Minikube you can add support for Ingresses with this com‐ mand: minikube addons enable ingress. Then you need to map a hostname for the Ingress.\n\nFor the purpose of this book we can just simulate the webhook as it would come from the Git server.\n\nFirst you can map the EventListener Service to your local networking with the following command:\n\nkubectl port-forward svc/el-tekton-greeter-eventlistener 8080\n\nThen you can invoke the Trigger by making an HTTP request to http://localhost:8080 using curl. The HTTP request must be a POST request containing a JSON payload and it should contain the fields referenced via a TriggerBinding. In our case we mapped body.repository.clone_url and body.after.\n\nCheck the documentation of your Git server to get the list of parameters that a webhook can generate. In this example we are using the GitHub Webhooks reference.\n\nTo test Triggers, run this command:\n\ncurl -X POST \\ http://localhost:8080 \\ -H 'Content-Type: application/json' \\ -d '{ \"after\": \"d9291c456db1ce29177b77ffeaa9b71ad80a50e6\", \"repos itory\": { \"clone_url\" : \"https://github.com/gitops-cookbook/tekton-tutorial- greeter.git\" } }'\n\nYou should get output similar to the following:\n\n138\n\n|\n\nChapter 6: Cloud Native CI/CD\n\n{\"eventListener\":\"tekton-greeter-eventlistener\",\"namespace\":\"default\",\"eventListe nerUID\":\"c00567eb-d798-4c4a-946d-f1732fdfc313\",\"eventID\":\"17dd25bb-a1fe-4f84-8422- c3abc5f10066\"}\n\nA new Pipeline now is started and you can check it with the following command:\n\ntkn pipelinerun ls\n\nYou should see it in Running status as follows:\n\ntekton-greeter-pipeline-3244b67f-31d3-4597-af1c-3c1aa6693719 4 seconds ago --- Running\n\nSee Also\n\n• Tekton Triggers examples\n\n• Getting Started with Tekton Triggers\n\n• Securing webhooks with event listeners\n\n6.9 Update a Kubernetes Resource Using Kustomize and Push the Change to Git\n\nProblem You want to use Kustomize in your Tekton Pipelines in order to automate Kubernetes manifests updates.\n\nSolution As we discussed in Chapter 4, Kustomize is a powerful tool to manage Kubernetes manifests. Kustomize can add, remove, or patch configuration options without fork‐ ing. In Recipe 4.2 you saw how to update a Kubernetes Deployment with a new container image hash using the kustomize CLI.\n\nIn this recipe, you’ll see how to let Tekton update it using Kustomize. This is very useful for GitOps as it allows an automated update on Git to the manifests describing an application running on Kubernetes, favoring the interconnection with a GitOps tool such as Argo CD in order to sync resources (see Chapter 7).\n\nWhen adopting the GitOps approach, it’s common to have one or more repositories for the Kubernetes manifests and then one or more repositories for the apps as well.\n\nThus let’s introduce a Task that accepts the Kubernetes manifests repository as a parameter and can update the container image reference as seen in Recipe 4.2:\n\napiVersion: tekton.dev/v1beta1 kind: Task metadata:\n\n6.9 Update a Kubernetes Resource Using Kustomize and Push the Change to Git\n\n|\n\n139\n\n140\n\nannotations: tekton.dev/pipelines.minVersion: 0.12.1 tekton.dev/tags: git name: git-update-deployment labels: app.kubernetes.io/version: '0.2' operator.tekton.dev/provider-type: community spec: description: >- This Task can be used to update image digest in a Git repo using kustomize. It requires a secret with credentials for accessing the git repo. params: - name: GIT_REPOSITORY type: string - name: GIT_REF type: string - name: NEW_IMAGE type: string - name: NEW_DIGEST type: string - name: KUSTOMIZATION_PATH type: string results: - description: The commit SHA name: commit steps: - image: 'docker.io/alpine/git:v2.26.2' name: git-clone resources: {} script: > rm -rf git-update-digest-workdir\n\ngit clone $(params.GIT_REPOSITORY) -b $(params.GIT_REF) git-update-digest-workdir workingDir: $(workspaces.workspace.path) - image: 'quay.io/wpernath/kustomize-ubi:latest' name: update-digest resources: {} script: > cd git-update-digest-workdir/$(params.KUSTOMIZATION_PATH)\n\nkustomize edit set image $(params.NEW_IMAGE)@$(params.NEW_DIGEST)\n\necho \"##########################\"\n\necho \"### kustomization.yaml ###\"\n\necho \"##########################\"\n\ncat kustomization.yaml workingDir: $(workspaces.workspace.path) - image: 'docker.io/alpine/git:v2.26.2' name: git-commit\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nresources: {} script: | cd git-update-digest-workdir\n\ngit config user.email \"tektonbot@redhat.com\" git config user.name \"My Tekton Bot\"\n\ngit status git add $(params.KUSTOMIZATION_PATH)/kustomization.yaml git commit -m \"[ci] Image digest updated\"\n\ngit push\n\nRESULT_SHA=\"$(git rev-parse HEAD | tr -d '\\n')\" EXIT_CODE=\"$?\" if [ \"$EXIT_CODE\" != 0 ] then exit $EXIT_CODE fi # Make sure we don't add a trailing newline to the result! echo -n \"$RESULT_SHA\" > $(results.commit.path) workingDir: $(workspaces.workspace.path) workspaces: - description: The workspace consisting of maven project. name: workspace\n\nThis Task is composed of three steps:\n\ngit-clone\n\nClones the Kubernetes manifests repository\n\nupdate-digest\n\nRuns kustomize to update the Kubernetes Deployment with a container image hash given as a parameter\n\ngit-commit\n\nUpdates the Kubernetes manifest repo with the new container image hash\n\nYou can create the Task with the following command:\n\nkubectl create -f git-update-deployment-task.yaml\n\nYou should get the following output:\n\ntask.tekton.dev/git-update-deployment created\n\nYou can now add this Task to a Pipeline similar to the one you saw in Recipe 6.7 in order to automate the update of your manifests with Kustomize:\n\napiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: pacman-pipeline spec:\n\n6.9 Update a Kubernetes Resource Using Kustomize and Push the Change to Git\n\n|\n\n141\n\n142\n\nparams: - default: https://github.com/gitops-cookbook/pacman-kikd.git name: GIT_REPO type: string - default: master name: GIT_REVISION type: string - default: quay.io/gitops-cookbook/pacman-kikd name: DESTINATION_IMAGE type: string - default: . name: CONTEXT_DIR type: string - default: 'https://github.com/gitops-cookbook/pacman-kikd-manifests.git' name: CONFIG_GIT_REPO type: string - default: main name: CONFIG_GIT_REVISION type: string tasks: - name: fetch-repo params: - name: url value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REVISION) - name: deleteExisting value: \"true\" taskRef: name: git-clone workspaces: - name: output workspace: app-source - name: build-app taskRef: name: maven params: - name: GOALS value: - -DskipTests - clean - package - name: CONTEXT_DIR value: \"$(params.CONTEXT_DIR)\" workspaces: - name: maven-settings workspace: maven-settings - name: source workspace: app-source runAfter: - fetch-repo - name: build-push-image taskRef: name: buildah\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nparams: - name: IMAGE value: \"$(params.DESTINATION_IMAGE)\" workspaces: - name: source workspace: app-source runAfter: - build-app - name: git-update-deployment params: - name: GIT_REPOSITORY value: $(params.CONFIG_GIT_REPO) - name: NEW_IMAGE value: $(params.DESTINATION_IMAGE) - name: NEW_DIGEST value: $(tasks.build-push-image.results.IMAGE_DIGEST) - name: KUSTOMIZATION_PATH value: env/dev - name: GIT_REF value: $(params.CONFIG_GIT_REVISION) runAfter: - build-push-image taskRef: kind: Task name: git-update-deployment workspaces: - name: workspace workspace: app-source workspaces: - name: app-source - name: maven-settings\n\nAs you can see from this example, you can take a result of a previous Task as an input for the following one. In this case the hash of the container image generated by the build-push-image Task is used to update the manifests with Kustomize.\n\nYou can create the Pipeline with the following command:\n\nkubectl create -f pacman-pipeline.yaml\n\nYou should get the following output:\n\npipeline.tekton.dev/pacman-pipeline created\n\nThe git-commit step requires authentication to your Git server in order to push the updates to the repo. Since this example is on GitHub, we are using a GitHub Personal Access Token (see Recipe 6.4) attached to the ServiceAccount tekton-bot-sa.\n\n6.9 Update a Kubernetes Resource Using Kustomize and Push the Change to Git\n\n|\n\n143\n\nMake sure to add the repo and registry’s Kubernetes Secrets as described in Recipes 6.4 and 6.5:\n\nkubectl patch serviceaccount tekton-bot-sa -p '{\"secrets\": [{\"name\": \"git- secret\"}]}' kubectl patch serviceaccount tekton-bot-sa \\ -p '{\"secrets\": [{\"name\": \"containerregistry- secret\"}]}'\n\nMake sure you have created a PVC for the Pipeline as defined in Recipe 6.7.\n\nNow you can start the Pipeline as follows:\n\ntkn pipeline start pacman-pipeline \\ --serviceaccount='tekton-bot-sa' \\ --param GIT_REPO='https://github.com/gitops-cookbook/pacman-kikd.git' \\ --param GIT_REVISION='main' \\ --param DESTINATION_IMAGE='quay.io/gitops-cookbook/pacman-kikd:latest' \\ --param CONFIG_GIT_REPO='https://github.com/gitops-cookbook/pacman-kikd- manifests.git' \\ --param CONFIG_GIT_REVISION='main' \\ --workspace name=app-source,claimName=app-source-pvc \\ --workspace name=maven-settings,emptyDir=\"\" \\ --use-param-defaults \\ --showlog\n\n6.10 Update a Kubernetes Resource Using Helm and Create a Pull Request\n\nProblem You want to automate the deployment of Helm-packaged apps with a Tekton Pipeline.\n\nSolution In Chapter 5 we discussed Helm and how it can be used to manage applications on Kubernetes in a convenient way. In this recipe you’ll see how to automate Helm- powered deployments through a Pipeline in order to install or update an application running on Kubernetes.\n\nAs shown in Recipe 6.7, you can use Tekton Hub to find and install Tekton Tasks. In fact, you can use the helm-upgrade-from-repo Task to have Helm support for your Pipelines.\n\n144\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nTo install it, run this command:\n\ntkn hub install task helm-upgrade-from-repo\n\nThis Task can install a Helm Chart from a Helm repository. For this example, we pro‐ vide a Helm repository in this book’s repository that you can add with the following command:\n\nhelm repo add gitops-cookbook https://gitops-cookbook.github.io/helm-charts/\n\nYou should get the following output:\n\n\"gitops-cookbook\" has been added to your repositories\n\nYou can install the Helm Chart with the following command:\n\nhelm install pacman gitops-cookbook/pacman\n\nYou should get output similar to the following:\n\nNAME: pacman LAST DEPLOYED: Mon Aug 15 17:02:21 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None USER-SUPPLIED VALUES: {}\n\nThe app should be now deployed and running on Kubernetes:\n\nkubectl get pods -l=app.kubernetes.io/name=pacman\n\nYou should get the following output:\n\nNAME READY STATUS RESTARTS AGE pacman-6798d65d84-9mt8p 1/1 Running 0 30s\n\nNow let’s update the Deployment with a Tekton Task running a helm upgrade with the following TaskRun:\n\napiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: helm-pacman-run- spec: serviceAccountName: tekton-deployer-sa taskRef: name: helm-upgrade-from-repo params: - name: helm_repo value: https://gitops-cookbook.github.io/helm-charts/ - name: chart_name value: gitops-cookbook/pacman - name: release_version value: 0.1.0 - name: release_name\n\n6.10 Update a Kubernetes Resource Using Helm and Create a Pull Request\n\n|\n\n145\n\nvalue: pacman - name: overwrite_values value: replicaCount=2\n\nThe helm-upgrade-from-repo Task needs permission to list objects in the work‐ ing namespace, so you need a ServiceAccount with special permissions as seen in Recipe 6.6.\n\nYou can override values in the Chart’s values.yaml file by adding them in this param. Here we are setting up two replicas for the Pac-Man game.\n\nRun the Task with the following command:\n\nkubectl create -f helm-pacman-taskrun.yaml\n\nYou should get output similar to the following:\n\ntaskrun.tekton.dev/helm-pacman-run-qghx8 created\n\nCheck logs with tkn CLI and select the running Task:\n\ntkn taskrun logs -f\n\nYou should get output similar to the following, where you can see the Helm upgrade has been successfully performed:\n\n[upgrade-from-repo] current installed helm releases [upgrade-from-repo] NAME NAMESPACE REVISION UPDA- TED STATUS CHART APP VERSION [upgrade-from-repo] pacman default 1 2022-08-15 17:02:21.633934129 +0200 +0200 deployed pacman-0.1.0 1.0.0 [upgrade-from-repo] parsing helms repo name... [upgrade-from-repo] adding helm repo... [upgrade-from-repo] \"gitops-cookbook\" has been added to your repositories [upgrade-from-repo] adding updating repo... [upgrade-from-repo] Hang tight while we grab the latest from your chart reposito- ries... [upgrade-from-repo] ...Successfully got an update from the \"gitops-cookbook\" chart repository [upgrade-from-repo] Update Complete. ⎈Happy Helming!⎈ [upgrade-from-repo] installing helm chart... [upgrade-from-repo] history.go:56: [debug] getting history for release pacman [upgrade-from-repo] upgrade.go:123: [debug] preparing upgrade for pacman [upgrade-from-repo] upgrade.go:131: [debug] performing update for pacman [upgrade-from-repo] upgrade.go:303: [debug] creating upgraded release for pacman [upgrade-from-repo] client.go:203: [debug] checking 2 resources for changes [upgrade-from-repo] client.go:466: [debug] Looks like there are no changes for Service \"pacman\" [upgrade-from-repo] wait.go:47: [debug] beginning wait for 2 resources with time- out of 5m0s [upgrade-from-repo] ready.go:277: [debug] Deployment is not ready: default/pacman. 1 out of 2 expected pods are ready [upgrade-from-repo] ready.go:277: [debug] Deployment is not ready: default/pacman.\n\n146\n\n|\n\nChapter 6: Cloud Native CI/CD\n\n1 out of 2 expected pods are ready [upgrade-from-repo] ready.go:277: [debug] Deployment is not ready: default/pacman. 1 out of 2 expected pods are ready [upgrade-from-repo] upgrade.go:138: [debug] updating status for upgraded release for pacman [upgrade-from-repo] Release \"pacman\" has been upgraded. Happy Helming! [upgrade-from-repo] NAME: pacman [upgrade-from-repo] LAST DEPLOYED: Mon Aug 15 15:23:31 2022 [upgrade-from-repo] NAMESPACE: default [upgrade-from-repo] STATUS: deployed [upgrade-from-repo] REVISION: 2 [upgrade-from-repo] TEST SUITE: None [upgrade-from-repo] USER-SUPPLIED VALUES: [upgrade-from-repo] replicaCount: 2 [upgrade-from-repo] [upgrade-from-repo] COMPUTED VALUES: [upgrade-from-repo] image: [upgrade-from-repo] containerPort: 8080 [upgrade-from-repo] pullPolicy: Always [upgrade-from-repo] repository: quay.io/gitops-cookbook/pacman-kikd [upgrade-from-repo] tag: 1.0.0 [upgrade-from-repo] replicaCount: 2 [upgrade-from-repo] securityContext: {} [upgrade-from-repo] [upgrade-from-repo] HOOKS: [upgrade-from-repo] MANIFEST: [upgrade-from-repo] --- [upgrade-from-repo] # Source: pacman/templates/service.yaml [upgrade-from-repo] apiVersion: v1 [upgrade-from-repo] kind: Service [upgrade-from-repo] metadata: [upgrade-from-repo] labels: [upgrade-from-repo] app.kubernetes.io/name: pacman [upgrade-from-repo] name: pacman [upgrade-from-repo] spec: [upgrade-from-repo] ports: [upgrade-from-repo] - name: http [upgrade-from-repo] port: 8080 [upgrade-from-repo] targetPort: 8080 [upgrade-from-repo] selector: [upgrade-from-repo] app.kubernetes.io/name: pacman [upgrade-from-repo] --- [upgrade-from-repo] # Source: pacman/templates/deployment.yaml [upgrade-from-repo] apiVersion: apps/v1 [upgrade-from-repo] kind: Deployment [upgrade-from-repo] metadata: [upgrade-from-repo] name: pacman [upgrade-from-repo] labels: [upgrade-from-repo] app.kubernetes.io/name: pacman [upgrade-from-repo] app.kubernetes.io/version: \"1.0.0\" [upgrade-from-repo] spec: [upgrade-from-repo] replicas: 2 [upgrade-from-repo] selector: [upgrade-from-repo] matchLabels:\n\n6.10 Update a Kubernetes Resource Using Helm and Create a Pull Request\n\n|\n\n147\n\n[upgrade-from-repo] app.kubernetes.io/name: pacman [upgrade-from-repo] template: [upgrade-from-repo] metadata: [upgrade-from-repo] labels: [upgrade-from-repo] app.kubernetes.io/name: pacman [upgrade-from-repo] spec: [upgrade-from-repo] containers: [upgrade-from-repo] - image: \"quay.io/gitops-cookbook/pacman-kikd:1.0.0\" [upgrade-from-repo] imagePullPolicy: Always [upgrade-from-repo] securityContext: [upgrade-from-repo] {} [upgrade-from-repo] name: pacman [upgrade-from-repo] ports: [upgrade-from-repo] - containerPort: 8080 [upgrade-from-repo] name: http [upgrade-from-repo] protocol: TCP [upgrade-from-repo]\n\nkubectl get deploy -l=app.kubernetes.io/name=pacman\n\npacman 2/2 2 2 9s\n\n6.11 Use Drone to Create a Pipeline for Kubernetes\n\nProblem You want to create a CI/CD pipeline for Kubernetes with Drone.\n\nSolution Drone is an open source project for cloud native continuous integration (CI). It uses YAML build files to define and execute build pipelines inside containers.\n\nIt has two main components:\n\nServer\n\nIntegrates with popular SCMs such as GitHub, GitLab, or Gitea\n\nRunner\n\nActs as an agent running on a certain platform\n\nYou can install the Server of your choice following the documentation and install the Kubernetes Runner.\n\nIn this example you will create a Java Maven-based pipeline using the Pac-Man app. First, install the Drone CLI for your OS; you can get it from the official website here.\n\n148\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nOn macOS, drone is available through Homebrew as follows:\n\nbrew tap drone/drone && brew install drone\n\nThen configure Drone, copy the DRONE_TOKEN from your instance under the Drone Account settings page, then create/update the file called .envrc.local and add the variables to override:\n\nexport DRONE_TOKEN=\"<YOUR-TOKEN>\"\n\nEnsure the token is loaded:\n\ndrone info\n\nNow activate the repo in Drone:\n\ndrone repo enable https://github.com/gitops-cookbook/pacman-kikd.git\n\nSimilarly to Tekton, Drone’s pipeline will compile, test, and build the app. Then it will create and push the container image to a registry.\n\nAdd credentials to your container registry as follows (here, we’re using Quay.io):\n\ndrone secret add --name image_registry \\ --data quay.io https://github.com/gitops-cookbook/pacman-kikd.git\n\ndrone secret add --name image_registry_user \\ --data YOUR_REGISTRY_USER https://github.com/gitops-cookbook/pacman-kikd.git\n\ndrone secret add --name image_registry_password \\ --data YOUR_REGISTRY_PASS https://github.com/gitops-cookbook/pacman-kikd.git\n\ndrone secret add --name destination_image \\ --data quay.io/YOUR_REGISTRY_USER>/pacman-kikd.git https://github.com/gitops- cookbook/pacman-kikd.git\n\nCreate a file called .drone.yaml as follows:\n\nkind: pipeline type: docker name: java-pipeline platform: os: linux arch: arm64 trigger: branch: - main clone: disable: true steps: - name: clone sources image: alpine/git pull: if-not-exists\n\n6.11 Use Drone to Create a Pipeline for Kubernetes\n\n|\n\n149\n\ncommands: - git clone https://github.com/gitops-cookbook/pacman-kikd.git . - git checkout $DRONE_COMMIT - name: maven-build image: maven:3-jdk-11 commands: - mvn install -DskipTests=true -B - mvn test -B - name: publish image: plugins/docker:20.13 pull: if-not-exists settings: tags: \"latest\" dockerfile: Dockerfile insecure: true mtu: 1400 username: from_secret: image_registry_user password: from_secret: image_registry_password registry: from_secret: image_registry repo: from_secret: destination_image\n\nStart the pipeline:\n\ndrone exec --pipeline=java-pipeline\n\nYou can also trigger the pipeline to start by pushing to your Git repo.\n\nSee Also\n\nExample Maven Pipeline from Drone docs •\n\nComplete Quarkus pipeline example in Drone •\n\n6.12 Use GitHub Actions for CI\n\nProblem You want to use GitHub Actions for CI in order to compile and package an app as a container image ready to be deployed in CD.\n\n150\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nSolution GitHub Actions are event-driven automation tasks available for any GitHub reposi‐ tory. An event automatically triggers the workflow, which contains a job. The job then uses steps to control the order in which actions are run. These actions are the commands that automate software building, testing, and deployment.\n\nIn this recipe, you will add a GitHub Action for building the Pac-Man game container image, and pushing it to the GitHub Container Registry.\n\nAs GitHub Actions are connected to repositories, you can fork the Pac-Man repository from this book’s code repositories to add your GitHub Actions. See the documentation about forking repositories for more info on this topic.\n\nGitHub Actions workflows run into environments and they can reference an environ‐ ment to use the environment’s protection rules and secrets.\n\nWorkflows and jobs are defined with a YAML file containing all the needed steps. Inside your repository, you can create one with the path .github/workflows/ pacman-ci-action.yml:\n\n# This is a basic workflow to help you get started with Actions\n\nname: pacman-ci-action\n\nenv: IMAGE_REGISTRY: ghcr.io/${{ github.repository_owner }} REGISTRY_USER: ${{ github.actor }} REGISTRY_PASSWORD: ${{ github.token }} APP_NAME: pacman IMAGE_TAGS: 1.0.0 ${{ github.sha }}\n\n# Controls when the workflow will run on: # Triggers the workflow on push or pull request events but only for the # \"main\" branch push: branches: [ \"main\" ] pull_request: branches: [ \"main\" ]\n\n# Allows you to run this workflow manually from the Actions tab workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in # parallel jobs: # This workflow contains a single job called \"build-and-push\" build-and-push:\n\n6.12 Use GitHub Actions for CI\n\n|\n\n151",
      "page_number": 154
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 168-178)",
      "start_page": 168,
      "end_page": 178,
      "detection_method": "topic_boundary",
      "content": "152\n\n# The type of runner that the job will run on runs-on: ubuntu-latest\n\n# Steps represent a sequence of tasks that will be executed as part of the # job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can # access it - uses: actions/checkout@v3\n\nname: Set up JDK 11 uses: actions/setup-java@v3 with: java-version: '11' distribution: 'adopt' cache: maven\n\nname: Build with Maven run: mvn --batch-mode package\n\nname: Buildah Action id: build-image uses: redhat-actions/buildah-build@v2 with: image: ${{ env.IMAGE_REGISTRY }}/${{ env.APP_NAME }} tags: ${{ env.IMAGE_TAGS }} containerfiles: | ./Dockerfile - name: Push to Registry id: push-to-registry uses: redhat-actions/push-to-registry@v2 with: image: ${{ steps.build-image.outputs.image }} tags: ${{ steps.build-image.outputs.tags }} registry: ${{ env.IMAGE_REGISTRY }} username: ${{ env.REGISTRY_USER }} password: ${{ env.REGISTRY_PASSWORD }} Name of the Action.\n\nEnvironment variables to be used in the workflow. This includes default environ‐ ment variables and the Secret you added to the environment.\n\nHere’s where you define which type of trigger you want for this workflow. In this case, any change to the repository (Push) to the master branch will trigger the action to start. Check out the documentation for a full list of triggers that can be used.\n\nName of this Job.\n\nList of steps; each step contains an action for the pipeline.\n\n|\n\nChapter 6: Cloud Native CI/CD\n\nBuildah Build. This action builds container images using Buildah.\n\nPush to Registry. This action is used to push to the GitHub Registry using built-in credentials available for GitHub repository owners.\n\nAfter each Git push or pull request, a new run of the action is performed as shown in Figure 6-10.\n\nGitHub offers its own container registry available at ghcr.io, and container images are referenced as part of the GitHub Packages. By default the images are public. See this book’s repository as a reference.\n\nFigure 6-10. GitHub Actions Jobs\n\nSee Also\n\n• GitHub Actions Jobs\n\n• Red Hat Actions\n\nDeploy to Kubernetes cluster Action •\n\n6.12 Use GitHub Actions for CI\n\n|\n\n153\n\nCHAPTER 7 Argo CD\n\nIn the previous chapter, you learned about Tekton and other engines such as GitHub Actions to implement the continuous integration (CI) part of a project.\n\nAlthough CI is important because it’s where you build the application and check that nothing has been broken (running unit tests, component tests, etc.), there is still a missing part: how to deploy this application to an environment (a Kubernetes cluster) using the GitOps methodology and not creating a script running kubectl/ helm commands.\n\nAs Daniel Bryant puts it, “If you weren’t using SSH in the past to deploy your application in production, don’t use kubectl to do it in Kubernetes.”\n\nIn this chapter, we’ll introduce you to Argo CD, a declarative, GitOps continuous delivery (CD) tool for Kubernetes. In the first part of the section, we’ll see the deployment of an application using Argo CD (Recipes 7.1 and 7.2).\n\nArgo CD not only supports the deployment of plain Kubernetes manifests, but also the deployment of Kustomize projects (Recipe 7.3) and Helm projects (Recipe 7.4).\n\nA typical operation done in Kubernetes is a rolling update to a new version of the container, and Argo CD integrates with another tool to make this process smooth (Recipe 7.5).\n\nDelivering complex applications might require some orchestration on when and how the application must be deployed and released (Recipes 7.7 and 7.8).\n\nWe’ll see how to:\n\n• Install and deploy the first application.\n\n• Use automatic deployment and self-healing applications.\n\n155\n\n• Execute a rolling update when a new container is released.\n\n• Give an order on the execution.\n\nIn this chapter, we are using the https://github.com/gitops-cookbook/gitops- cookbook-sc.git GitHub repository as source directory. To run it successfully in this chapter, you should fork it and use it in the YAML files provided in the examples.\n\n7.1 Deploy an Application Using Argo CD\n\nProblem You want Argo CD to deploy an application defined in a Git repository.\n\nSolution Create an Application resource to set up Argo CD to deploy the application.\n\nTo install Argo CD, create the argocd namespace and apply the Argo CD installation manifest:\n\nkubectl apply -n argocd \\ -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.3.4/manifests/install.yaml\n\nOptional Steps It’s not mandatory to install the Argo CD CLI tool, or expose the Argo CD server service to access the Argo CD Dashboard. Still, in this book, we’ll use them in the recipes to show you the final result after applying the manifests. So, although not mandatory, we encourage you to follow the next steps to be aligned with the book.\n\nTo install the argocd CLI tool, go to the Argo CD CLI GitHub release page and in the Assets section, download the tool for your platform.\n\nAfter installing the argocd tool, the argocd-server Kubernetes Service needs to be exposed. You can use any technique such as Ingress or set the service as LoadBa lancer but we’ll use the kubectl port-forwarding to connect to the API server without exposing the service:\n\nkubectl port-forward svc/argocd-server -n argocd 9090:443\n\nAt this point, you can access the Argo CD server using http://localhost:9090.\n\nThe initial password for the admin account is generated automatically in a secret named argocd-initial-admin-secret in the argocd namespace:\n\nargoPass=$(kubectl -n argocd get secret argocd-initial-admin-secret -o json path=\"{.data.password}\" | base64 -d)\n\n156\n\n|\n\nChapter 7: Argo CD\n\nargoURL=localhost:9090\n\nargocd login --insecure --grpc-web $argoURL --username admin --password $argo Pass\n\n'admin:login' logged in successfully\n\nYou should use the same credentials to access the Argo CD UI.\n\nLet’s make Argo CD deploy a simple web application showing a box with a config‐ ured color. The application is composed of three Kubernetes manifest files, including a Namespace, a Deployment, and a Service definition.\n\nThe files are located in the ch07/bgd folder of the book’s repository.\n\nAll these files are known as an Application in Argo CD. Therefore, you must define it as such to apply these manifests in your cluster.\n\nLet’s check the Argo CD Application resource file used for deploying the application:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgd-app namespace: argocd spec: destination: namespace: bgd server: https://kubernetes.default.svc project: default source: repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git path: ch07/bgd targetRevision: main\n\nNamespace where Argo CD is installed\n\nTarget cluster and namespace\n\nInstalling the application in Argo CD’s default project\n\nThe manifest repo where the YAML resides\n\nThe path to look for manifests\n\nBranch to checkout\n\n7.1 Deploy an Application Using Argo CD\n\n|\n\n157\n\nIn the terminal window, run the following command to register the Argo CD application:\n\nkubectl apply -f manual-bgd-app.yaml\n\nAt this point, the application is registered as an Argo CD application.\n\nYou can check the status using either argocd or the UI; run the following command to list applications using the CLI too:\n\nargocd app list\n\nAnd the output is something like:\n\nNAME CLUSTER NAMESPACE PROJECT STATUS bgd-app https://kubernetes.default.svc bgd default OutOfSync\n\nThe important field here is STATUS. It’s OutOfSync, which means the application is registered, and there is a drift between the current status (in this case, no application deployed) and the content in the Git repository (the application deployment files).\n\nYou’ll notice that no pods are running if you get all the pods from the bgd namespace:\n\nkubectl get pods -n bgd\n\nNo resources found in bgd namespace.\n\nArgo CD doesn’t synchronize the application automatically by default. It just shows a divergence, and the user is free to fix it by triggering a synchronized operation.\n\nWith the CLI, you synchronize the application by running the following command in a terminal:\n\nargocd app sync bgd-app\n\nAnd the ouput of the command shows all the important information regarding the deployment:\n\nName: bgd-app Project: default Server: https://kubernetes.default.svc Namespace: bgd URL: https://openshift-gitops-server-openshift-gitops.apps.open- shift.sotogcp.com/applications/bgd-app Repo: https://github.com/lordofthejars/gitops-cookbook-sc.git Target: main Path: ch07/bgd SyncWindow: Sync Allowed Sync Policy: <none> Sync Status: Synced to main (384cd3d) Health Status: Progressing\n\nOperation: Sync Sync Revision: 384cd3d21c534e75cb6b1a6921a6768925b81244 Phase: Succeeded\n\n158\n\n|\n\nChapter 7: Argo CD\n\nStart: 2022-06-16 14:45:12 +0200 CEST Finished: 2022-06-16 14:45:13 +0200 CEST Duration: 1s Message: successfully synced (all tasks run)\n\nGROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Namespace bgd bgd Running Synced namespace/bgd cre- ated Service bgd bgd Synced Healthy service/bgd created apps Deployment bgd bgd Synced Progressing deploy- ment.apps/bgd created Namespace bgd Synced\n\nYou can synchronize the application from the UI as well, by clicking the SYNC button as shown in Figure 7-1.\n\nFigure 7-1. Argo CD web console\n\n7.1 Deploy an Application Using Argo CD\n\n|\n\n159\n\nIf you get all the pods from the bgd namespace, you’ll notice one pod running:\n\nkubectl get pods -n bgd\n\nNAME READY STATUS RESTARTS AGE bgd-788cb756f7-jll9n 1/1 Running 0 60s\n\nAnd the same for the Service:\n\nkubectl get services -n bgd\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) bgd ClusterIP 172.30.35.199 <none> 8080:32761/TCP\n\nExposed port is 32761\n\nIn the following sections, you’ll need to access the deployed service to validate that it’s deployed. There are several ways to access services deployed to Minikube; for the following chapters, we use the Minikube IP and the exposed port of the service.\n\nRun the following command in a terminal window to get the Minikube IP:\n\nminikube ip -p gitops 192.168.59.100\n\nOpen a browser window, set the previous IP followed by the exposed port (in this example 192.168.59.100:32761), and access the service to validate that the color of the circles in the box is blue, as shown in Figure 7-2.\n\nFigure 7-2. Deployed application\n\n160\n\n|\n\nChapter 7: Argo CD\n\nDiscussion Now it’s time to update the application deployment files. This time we will change the value of an environment variable defined in the bgd-deployment.yaml file.\n\nOpen ch07/bgd/bgd-deployment.yaml in your file editor and change the COLOR envi‐ ronment variable value from blue to green:\n\nspec: containers: - image: quay.io/redhatworkshops/bgd:latest name: bgd env: - name: COLOR value: \"green\"\n\nIn a terminal run the following commands to commit and push the file so the change is available for Argo CD:\n\ngit add . git commit -m \"Updates color\"\n\ngit push origin main\n\nWith the change pushed, check the status of the application again:\n\nargocd app list\n\nNAME CLUSTER NAMESPACE PROJECT STATUS bgd-app https://kubernetes.default.svc bgd default Sync\n\nWe see the application status is Sync. This happens because Argo CD uses a polling approach to detect divergences between what’s deployed and what’s defined in Git. After some time (by default, it’s 3 minutes), the application status will be OutOfSync:\n\nargocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH bgd-app https://kubernetes.default.svc bgd default OutOfSync Healthy\n\nTo synchronize the changes, run the sync subcommand:\n\nargocd app sync bgd-app\n\nAfter some seconds, access the service and validate that the circles are green, as shown in Figure 7-3.\n\n7.1 Deploy an Application Using Argo CD\n\n|\n\n161\n\nFigure 7-3. Deployed application\n\nTo remove the application, use the CLI tool or the UI:\n\nargocd app delete bgd-app\n\nAlso, revert the changes done in the Git repository to get the initial version of the application and push them:\n\ngit revert HEAD\n\ngit push origin main\n\n7.2 Automatic Synchronization\n\nProblem You want Argo CD to automatically update resources when there are changes.\n\nSolution Use the syncPolicy section with an automated policy.\n\nArgo CD can automatically synchronize an application when it detects differences between the manifests in Git and the Kubernetes cluster.\n\nA benefit of automatic sync is that there is no need to log in to the Argo CD API, with the security implications that involves (managing secrets, network, etc.), and the use of the argocd tool. Instead, when a manifest is changed and pushed to the Git repository with the changes to the tracking Git repo, the manifests are automatically applied.\n\nLet’s modify the previous Argo CD manifest file (Application), adding the sync Policy section, so changes are deployed automatically:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata:\n\n162\n\n|\n\nChapter 7: Argo CD\n\nname: bgd-app namespace: argocd spec: destination: namespace: bgd server: https://kubernetes.default.svc project: default source: path: ch07/bgd repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main syncPolicy: automated: {}\n\nStarts the synchronization policy configuration section\n\nArgo CD automatically syncs the repo\n\nAt this point, we can apply the Application file into a running cluster by running the following command:\n\nkubectl apply -f bgd/bgd-app.yaml\n\nNow, Argo CD deploys the application without executing any other command.\n\nRun the kubectl command or check in the Argo CD UI to validate that the deploy‐ ment is happening:\n\nkubectl get pods -n bgd\n\nNAME READY STATUS RESTARTS AGE bgd-788cb756f7-jll9n 1/1 Running 0 60s\n\nAccess the service and validate that the circles are blue, as shown in Figure 7-4.\n\nFigure 7-4. Deployed application\n\n7.2 Automatic Synchronization\n\n|\n\n163",
      "page_number": 168
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 179-187)",
      "start_page": 179,
      "end_page": 187,
      "detection_method": "topic_boundary",
      "content": "To remove the application, use the CLI tool or the UI:\n\nargocd app delete bgd-app\n\nDiscussion Although Argo CD deploys applications automatically, it uses some default conserva‐ tive strategies for safety reasons.\n\nTwo of these are the pruning of deleted resources and the self-healing of the applica‐ tion in case a change was made in the Kubernetes cluster directly instead of through Git.\n\nBy default, Argo CD will not delete (prune) any resource when it detects that it is no longer available in Git, and it will be in an OutOfSync status. If you want Argo CD to delete these resources, you can do it in two ways.\n\nThe first way is by manually invoking a sync with the -prune option:\n\nargocd app sync --prune\n\nThe second way is letting Argo CD delete pruned resources automatically by setting the prune attribute to true in the automated section:\n\nsyncPolicy: automated: prune: true\n\nEnables automatic pruning\n\nAnother important concept affecting how the application is automatically updated is self-healing.\n\nArgo CD is configured not to correct any drift made manually in the cluster. For example, Argo CD will let the execution of a kubectl patch directly in the cluster change any configuration parameter of the application.\n\nLet’s see it in action.\n\nThe color of the circle is set as an environment variable (COLOR).\n\nNow, let’s change the COLOR environment variable to green using the kubectl patch command.\n\nRun the following command in the terminal:\n\nkubectl -n bgd patch deploy/bgd \\ --type='json' -p='[{\"op\": \"replace\", \"path\": \"/ spec/template/spec/containers/0/env/0/value\", \"value\":\"green\"}]'\n\nWait for the rollout to happen:\n\nkubectl rollout status deploy/bgd -n bgd\n\n164\n\n|\n\nChapter 7: Argo CD\n\nIf you refresh the browser, you should see green circles now, as shown in Figure 7-5.\n\nFigure 7-5. Deployed application\n\nLooking over the Argo CD sync status, you’ll see that it’s OutOfSync as the application and the definition in the Git repository (COLOR: blue) diverges.\n\nArgo CD will not roll back to correct this drift as the selfHeal property default is set to false.\n\nLet’s remove the application and deploy a new one, but set selfHeal to true in this case:\n\nargocd app delete bgd-app\n\nLet’s enable the selfHealing property, and repeat the experiment:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgd-app namespace: argocd spec: destination: namespace: bgd server: https://kubernetes.default.svc project: default source: path: ch07/bgd repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main syncPolicy: automated: prune: true selfHeal: true\n\nselfHeal set to true to correct any drift\n\nAnd in the terminal apply the resource:\n\nkubectl apply -f bgd/heal-bgd-app.yaml\n\n7.2 Automatic Synchronization\n\n|\n\n165\n\nRepeat the previous steps:\n\n1. Open the browser to check that the circles are blue. 1. 2. Reexecute the kubectl -n bgd patch deploy/bgd ... command. 2.\n\n3. Refresh the browser and check that the circles are still blue. 3.\n\nArgo CD corrects the drift introduced by the patch command, synchronizing the application to the correct state defined in the Git repository.\n\nTo remove the application, use the CLI tool or the UI:\n\nargocd app delete bgd-app\n\nSee Also\n\n• Argo CD Automated Sync Policy\n\n• Argo CD Sync Options\n\n7.3 Kustomize Integration\n\nProblem You want to use Argo CD to deploy Kustomize manifests.\n\nSolution Argo CD supports several different ways in which Kubernetes manifests can be defined:\n\n• Kustomize\n\n• Helm\n\n• Ksonnet\n\nJsonnet •\n\nYou can also extend the supported ways to custom ones, but this is out of the scope of this book.\n\nArgo CD detects a Kustomize project if there are any of the following files: kustomiza‐ tion.yaml, kustomization.yml, or Kustomization.\n\nLet’s deploy the same BGD application, but in this case, deployed as Kustomize manifests.\n\nMoreover, we’ll set kustomize to override the COLOR environment variable to yellow.\n\n166\n\n|\n\nChapter 7: Argo CD\n\nThe Kustomize file defined in the repository looks like this:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: bgdk resources: - ../base - bgdk-ns.yaml patchesJson6902: - target: version: v1 group: apps kind: Deployment name: bgd namespace: bgdk patch: |- - op: replace path: /spec/template/spec/containers/0/env/0/value value: yellow\n\nDirectory with standard deployment files (blue circles)\n\nSpecific file for creating a namespace\n\nPatches standard deployment files\n\nPatches the deployment file\n\nOverrides the environment variable value to yellow\n\nYou don’t need to create this file as it’s already stored in the Git repository.\n\nCreate the following Application file to deploy the application:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgdk-app namespace: argocd spec: destination: namespace: bgdk server: https://kubernetes.default.svc project: default source: path: ch07/bgdk/bgdk repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git\n\n7.3 Kustomize Integration\n\n|\n\n167\n\ntargetRevision: main syncPolicy: automated: {}\n\nAt this point, we can apply the Application file to a running cluster by running the following command:\n\nkubectl apply -f bgdk/bgdk-app.yaml\n\nAccess the service and you’ll notice the circles are yellow instead of blue.\n\nTo remove the application, use the CLI tool or the UI:\n\nargocd app delete bgdk-app\n\nDiscussion We can explicitly specify which tool to use, overriding the default algorithm used by Argo CD in the Application file. For example, we can use a plain directory strategy regarding the presence of the kustomization.yaml file:\n\nsource: directory: recurse: true\n\nOverrides always use a plain directory strategy\n\nPossible strategies are: directory, chart, helm, kustomize, path, and plugin.\n\nEverything we’ve seen about Kustomize is valid when using Argo CD.\n\nSee Also\n\n• Chapter 4\n\n• argo-cd/application-crd.yaml on GitHub\n\nArgo CD Tool Detection •\n\n7.4 Helm Integration\n\nProblem You want to use Argo CD to deploy Helm manifests.\n\n168\n\n|\n\nChapter 7: Argo CD\n\nSolution Argo CD supports installing Helm Charts to the cluster when it detects a Helm project in the deployment directory (when the Chart.yaml file is present).\n\nLet’s deploy the same BGD application, but in this case, deployed as a Helm manifest.\n\nThe layout of the project is a simple Helm layout already created in the GitHub repository you’ve cloned previously:\n\n├── Chart.yaml ├── charts ├── templates │ ├── NOTES.txt │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── ns.yaml │ ├── service.yaml │ ├── serviceaccount.yaml │ └── tests │ └── test-connection.yaml └── values.yaml\n\nCreate a bgdh/bgdh-app.yaml file to define the Argo CD application:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgdh-app namespace: argocd spec: destination: namespace: bgdh server: https://kubernetes.default.svc project: default source: path: ch07/bgdh repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main syncPolicy: automated: {}\n\nAt this point, we can apply the Application file into a running cluster by running the following command:\n\nkubectl apply -f bgdh/bgdh-app.yaml\n\nValidate the pod is running in the bgdh namespace:\n\nkubectl get pods -n bgdh\n\nNAME READY STATUS RESTARTS AGE bgdh-app-556c46fcd6-ctfkf 1/1 Running 0 5m43s\n\n7.4 Helm Integration\n\n|\n\n169\n\nTo remove the application, use the CLI tool or the UI:\n\nargocd app delete bgdh-app\n\nDiscussion Argo CD populates build environment variables to Helm manifests (actually also Kustomize, Jsonnet, and custom tools support too).\n\nThe following variables are set:\n\nARGOCD_APP_NAME • • ARGOCD_APP_NAMESPACE • • ARGOCD_APP_REVISION • • ARGOCD_APP_SOURCE_PATH • • ARGOCD_APP_SOURCE_REPO_URL • • ARGOCD_APP_SOURCE_TARGET_REVISION • • KUBE_VERSION • • KUBE_API_VERSIONS • apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgdh-app namespace: openshift-gitops spec: destination: ... source: path: ch07/bgd ... helm: parameters: - name: app value: $ARGOCD_APP_NAME\n\nARGOCD_APP_NAME • • ARGOCD_APP_NAMESPACE • • ARGOCD_APP_REVISION • • ARGOCD_APP_SOURCE_PATH • • ARGOCD_APP_SOURCE_REPO_URL • • ARGOCD_APP_SOURCE_TARGET_REVISION • • KUBE_VERSION • • KUBE_API_VERSIONS • apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgdh-app namespace: openshift-gitops spec: destination: ... source: path: ch07/bgd ... helm: parameters: - name: app value: $ARGOCD_APP_NAME\n\nSpecific Helm section\n\nExtra parameters to set, same as setting them in values.yaml, but high preference\n\nThe name of the parameter\n\nThe value of the parameter, in this case from a Build Env var\n\n170\n\n|\n\nChapter 7: Argo CD\n\nArgo CD can use a different values.yaml file or set parameter values to override the ones defined in values.yaml:\n\nargocd app set bgdh-app --values new-values.yaml\n\nargocd app set bgdh-app -p service.type=LoadBalancer\n\nNote that values files must be in the same Git repository as the Helm Chart.\n\nArgo CD supports Helm hooks too.\n\nSee Also\n\n• Chapter 5\n\n• argo-cd/application-crd.yaml on GitHub\n\n7.5 Image Updater\n\nProblem You want Argo CD to automatically deploy a container image when it’s published.\n\nSolution Use Argo CD Image Updater to detect a change on the container registry and update the deployment files.\n\nOne of the most repetitive tasks during development is deploying a new version of a container image.\n\nWith a pure Argo CD solution, after the container image is published to a container registry, we need to update the Kubernetes/Kustomize/Helm manifest files pointing to the new container image and push the result to the Git repository.\n\nThis process implies:\n\n1. 1. Clone the repo\n\n2. 2. Parse the YAML files and update them accordingly\n\n3. Commit and Push the changes 3.\n\nThese boilerplate tasks should be defined for each repository during the continuous integration phase. Although this approach works, it could be automated so the cluster\n\n7.5 Image Updater\n\n|\n\n171\n\ncould detect a new image pushed to the container registry and update the current deployment file pointing to the newer version.\n\nThis is exactly what Argo CD Image Updater (ArgoCD IU) does. It’s a Kubernetes controller monitoring for a new container version and updating the manifests defined in the Argo CD Application file.\n\nThe Argo CD IU lifecycle and its relationship with Argo CD are shown in Figure 7-6.\n\nFigure 7-6. Argo CD Image Updater lifecycle\n\nAt this time, Argo CD IU only updates manifests of Kustomize or Helm. In the case of Helm, it needs to support specifying the image’s tag using a parameter (image.tag).\n\nLet’s install the controller in the same namespace as Argo CD:\n\nkubectl apply -f \\ https://raw.githubusercontent.com/argoproj-labs/argocd-imageupdater/v0.12.0/mani- fests/install.yaml -n argocd\n\nValidate the installation process, checking that the pod status of the controller is Running:\n\nkubectl get pods -n argocd\n\nNAME READY STATUS RESTARTS AGE argocd-image-updater-59c45cbc5c-kjjtp 1/1 Running 0 40h\n\n172\n\n|\n\nChapter 7: Argo CD",
      "page_number": 179
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 188-200)",
      "start_page": 188,
      "end_page": 200,
      "detection_method": "topic_boundary",
      "content": "Before using Argo CD IU, we create a Kubernetes Secret representing the Git creden‐ tials, so the updated manifests can be pushed to the repository. The secret must be at the Argo CD namespace and, in this case, we name it git-creds.\n\nkubectl -n argocd create secret generic git-creds \\ --from-literal=user name=<git_user> \\ --from-literal=password=<git_password_or_token>\n\nFinally, let’s annotate the Application manifest with some special annotations so the controller can start monitoring the registry:\n\nimage-list\n\nSpecify one or more images (comma-separated-value) considered for updates.\n\nwrite-back-method\n\nMethods to propagate new versions. There are git and argocd methods imple‐ mented to update to a newer image. The Git method commits the change to the Git repository. Argo CD uses the Kubernetes/ArgoCD API to update the resource.\n\nThere are more configuration options, but the previous ones are the most important to get started.\n\nLet’s create an Argo CD Application manifest annotated with Argo CD IU annotations:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgdk-app namespace: argocd annotations: argocd-image-updater.argoproj.io/image-list: myalias=quay.io/rhdevelopers/bgd\n\nargocd-image-updater.argoproj.io/write-back-method: git:secret:openshift- gitops/git-creds argocd-image-updater.argoproj.io/git-branch: main spec: destination: namespace: bgdk server: https://kubernetes.default.svc project: default source: path: ch07/bgdui/bgdk repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main syncPolicy: automated: {}\n\nAdds annotations section\n\n7.5 Image Updater\n\n|\n\n173\n\nSets the monitored image name\n\nConfigures to use Git as write-back-method, setting the location of the creden‐ tials (<namespace>/<secretname>)\n\nSets the branch to push changes\n\nNow apply the manifest to deploy the application’s first version and enable Argo CD IU to update the repository when a new image is pushed to the container registry:\n\nkubectl apply -f bgdui/bgdui-app.yaml\n\nAt this point, version 1.0.0 is up and running in the bgdk namespace, and you may access it as we’ve done before. Let’s generate a new container version to validate that the new image is in the repository.\n\nTo simplify the process, we’ll tag the container with version 1.1.0 as it was a new one.\n\nGo to the Quay repository created at the beginning of this chapter, go to the tags section, push the gear icon, and select Add New Tag to create a new container, as shown in Figure 7-7.\n\nFigure 7-7. Tag container\n\nSet the tag to 1.1.0 value as shown in the figure Figure 7-8.\n\nFigure 7-8. Tag container\n\n174\n\n|\n\nChapter 7: Argo CD\n\nAfter this step, you should have a new container created as shown in Figure 7-9.\n\nWait for around two minutes until the change is detected and the controller triggers the repo update.\n\nFigure 7-9. Final result\n\nTo validate the triggering process check the logs of the controller:\n\nkubectl logs argocd-image-updater-59c45cbc5c-kjjtp -f -n argocd\n\n... time=\"2022-06-20T21:19:05Z\" level=info msg=\"Setting new image to quay.io/rhdevel opers/bgd:1.1.0\" alias=myalias application=bgdk-app image_name=rhdevelopers/bgd image_tag=1.0.0 registry=quay.io time=\"2022-06-20T21:19:05Z\" level=info msg=\"Successfully updated image 'quay.io/ rhdevelopers/bgd:1.0.0' to 'quay.io/rhdevelopers/bgd:1.1.0', but pending spec update (dry run=false)\" alias=myalias application=bgdk-app image_name=rhdevelop ers/bgd image_tag=1.0.0 registry=quay.io time=\"2022-06-20T21:19:05Z\" level=info msg=\"Committing 1 parameter update(s) for application bgdk-app\" application=bgdk-app ...\n\nDetects the change and updates the image\n\nAfter that, if you inspect the repository, you’ll see a new Kustomize file named .argocd-source-bgdk-app.yaml, updating the image value to the new con‐ tainer, as shown in Figure 7-10.\n\n7.5 Image Updater\n\n|\n\n175\n\nFigure 7-10. New Kustomize file updating to the new container\n\nNow Argo CD can detect the change and update the cluster properly with the new image.\n\nTo remove the application, use the CLI tool or the UI:\n\nargocd app delete bgdk-app\n\nDiscussion An update strategy defines how Argo CD IU will find new versions. With no change, Argo CD IU uses a semantic version to detect the latest version.\n\nAn optional version constraint field may be added to restrict which versions are allowed to be automatically updated. To only update patch versions, we can change the image-list annotation as shown in the following snippet:\n\nargocd-image-updater.argoproj.io/image-list: myalias=quay.io/rhdevelopers/bgd:1.2.x\n\nArgo CD Image Updater can update to the image that has the most recent build date:\n\nargocd-image-updater.argoproj.io/myalias.update-strategy: latest argocd-image-updater.argoproj.io/myimage.allow-tags: regexp:^[0-9a-f]{7}$\n\nRestricts the tags considered for the update\n\nThe digest update strategy will use image digests to update your applications’ image tags:\n\nargocd-image-updater.argoproj.io/myalias.update-strategy: digest\n\nSo far, the container was stored in a public registry. If the repository is private, Argo CD Image Updater needs read access to the repo to detect any change.\n\nFirst of all, create a new secret representing the container registry credentials:\n\nkubectl create -n argocd secret docker-registry quayio --docker-server=quay.io -- docker-username=$QUAY_USERNAME --docker-password=$QUAY_PASSWORD\n\n176\n\n|\n\nChapter 7: Argo CD\n\nArgo CD Image Updater uses a ConfigMap as a configuration source, which is the place to register the private container registry. Create a new ConfigMap manifest setting the supported registries:\n\napiVersion: v1 kind: ConfigMap metadata: name: argocd-image-updater-config data: registries.conf: | registries: - name: RedHat Quay api_url: https://quay.io prefix: quay.io insecure: yes credentials: pullsecret:argocd/quayio\n\nName of the Argo CD IU ConfigMap\n\nPlace to register all registries\n\nA name to identify it\n\nURL of the service\n\nThe prefix used in the container images\n\nGets the credentials from the quayio secret stored at argocd namespace\n\nArgo CD Image Updater commits the update with a default message:\n\ncommit 3caf0af8b7a26de70a641c696446bbe1cd04cea8 (HEAD -> main, origin/main) Author: argocd-image-updater <noreply@argoproj.io> Date: Thu Jun 23 09:41:00 2022 +0000\n\nbuild: automatic update of bgdk-app\n\nupdates image rhdevelopers/bgd tag '1.0.0' to '1.1.0'\n\nWe can update the default commit message to one that fits your requirements. Configure the git.commit-message-template key in ArgoCD IU argocd-image- updater-config ConfigMap with the message:\n\napiVersion: v1 kind: ConfigMap metadata: name: argocd-image-updater-config data: git.user: alex git.email: alex@example.com git.commit-message-template: |\n\n7.5 Image Updater\n\n|\n\n177\n\nbuild: automatic update of {{ .AppName }}\n\n{{ range .AppChanges -}} updates image {{ .Image }} tag '{{ .OldTag }}' to '{{ .NewTag }}' {{ end -}}\n\nArgo CD IU ConfigMap\n\nCommit user\n\nCommmit email\n\nGolang text/template content\n\nThe name of the application\n\nList of changes performed by the update\n\nImage name\n\nPrevious container tag\n\nNew container tag\n\nRemember to restart the Argo CD UI controller when the Config Map is changed:\n\nkubectl rollout restart deployment argocd-image-updater -n argocd\n\nSee Also\n\n• Argo CD Image Updater\n\n7.6 Deploy from a Private Git Repository\n\nProblem You want Argo CD to deploy manifests.\n\nSolution Use Argo CD CLI/UI or YAML files to register the repositories’ credential informa‐ tion (username/password/token/key).\n\n178\n\n|\n\nChapter 7: Argo CD\n\nIn Argo CD, you have two ways to register a Git repository with its credentials. One way is using the Argo CD CLI/Argo CD UI tooling. To register a private repository in Argo CD, set the username and password by running the following command:\n\nargocd repo add https://github.com/argoproj/argocd-example-apps \\ --username <username> --password <password>\n\nAlternatively, we can use the Argo CD UI to register it too. Open Argo CD UI in a browser, and click the Settings/Repositories button (the one with gears) as shown in Figure 7-11.\n\nFigure 7-11. Settings menu\n\nThen click the “Connect Repo using HTTPS” button and fill the form with the required data as shown in Figure 7-12.\n\nFigure 7-12. Configuration of repository\n\n7.6 Deploy from a Private Git Repository\n\n|\n\n179\n\nFinally, click the Connect button to test that it’s possible to establish a connection and add the repository into Argo CD.\n\nThe other way is to create a Kubernetes Secret manifest file with that repository and credentials information:\n\napiVersion: v1 kind: Secret metadata: name: private-repo namespace: argocd labels: argocd.argoproj.io/secret-type: repository stringData: type: git url: https://github.com/argoproj/private-repo password: my-password username: my-username\n\nCreate a secret in the Argo CD namespace\n\nSets secret type as repository\n\nURL of the repository to register\n\nPassword to access\n\nUsername to access\n\nIf you apply this file, it will have the same effect as the manual approach.\n\nAt this point, every time we define a repoURL value in the Application resource with a repository URL registered for authentication, Argo CD will use the registered credentials to log in.\n\nDiscussion In addition to setting credentials such as username and password for accessing a private Git repo, Argo CD also supports other methods such as tokens, TLS client certificates, SSH private keys, or GitHub App credentials.\n\nLet’s see some examples using Argo CD CLI or Kubernetes Secrets.\n\nTo configure a TLS client certificate:\n\nargocd repo add https://repo.example.com/repo.git \\ --tls-client-cert-path ~/mycert.crt \\ --tls-client-cert-key-path ~/mycert.key\n\n180\n\n|\n\nChapter 7: Argo CD\n\nFor SSH, you just need to set the location of the SSH private key:\n\nargocd repo add git@github.com:argoproj/argocd-example-apps.git \\ --ssh-privatekey-path ~/.ssh/id_rsa\n\nOr using a Kubernetes Secret:\n\napiVersion: v1 kind: Secret metadata: name: private-repo namespace: argocd labels: argocd.argoproj.io/secret-type: repository stringData: type: git url: git@github.com:argoproj/my-private-repository sshPrivateKey: | -----BEGIN OPENSSH PRIVATE KEY----- ... -----END OPENSSH PRIVATE KEY-----\n\nSets the content of the SSH private key\n\nIf you are using the GitHub App method, you need to set the App ID, the App Installation ID, and the private key:\n\nargocd repo add https://github.com/argoproj/argocd-example-apps.git --github-app- id 1 --github-app-installation-id 2 --github-app-private-key-path test.private- key.pem\n\nOr using the declarative approach:\n\napiVersion: v1 kind: Secret metadata: name: github-repo namespace: argocd labels: argocd.argoproj.io/secret-type: repository stringData: type: git repo: https://ghe.example.com/argoproj/my-private-repository githubAppID: 1 githubAppInstallationID: 2 githubAppEnterpriseBaseUrl: https://ghe.example.com/api/v3 githubAppPrivateKeySecret: | -----BEGIN OPENSSH PRIVATE KEY----- ... -----END OPENSSH PRIVATE KEY-----\n\nSets GitHub App parameters\n\nOnly valid if GitHub App Enterprise is used\n\n7.6 Deploy from a Private Git Repository\n\n|\n\n181\n\nFor the access token, use the account name as the username and the token in the password field.\n\nChoosing which strategy to use will depend on your experience managing Kubernetes Secrets. Remember that a Secret in Kubernetes is not encrypted but encoded in Base64, so it is not secured by default.\n\nWe recommend using only the declarative approach when you’ve got a good strategy for securing the secrets.\n\nWe’ve not discussed the Sealed Secrets project yet (we’ll do so in the following chapter), but when using Sealed Secrets, the labels will be removed to avoid the SealedSecret object having a template section that encodes all the fields you want the controller to put in the unsealed Secret:\n\nspec: ... template: metadata: labels: \"argocd.argoproj.io/secret-type\": repository\n\n7.7 Order Kubernetes Manifests\n\nProblem You want to use Argo CD to deploy.\n\nSolution Use sync waves and resource hooks to modify the default order of applying manifests.\n\nArgo CD applies the Kubernetes manifests (plain, Helm, Kustomize) in a particular order using the following logic:\n\n1. By kind 1.\n\na. a. Namespaces\n\nb. NetworkPolicy b.\n\nc. c. Limit Range\n\nd. d. ServiceAccount\n\ne. e. Secret\n\nf. ConfigMap f.\n\ng. g. StorageClass\n\n182\n\n|\n\nChapter 7: Argo CD\n\nh. h. PersistentVolumes\n\ni. i. ClusterRole\n\nj. j. Role\n\nk. k. Service\n\nl. l. DaemonSet\n\nm. Pod m.\n\nn. n. ReplicaSet\n\no. o. Deployment\n\np. p. StatefulSet\n\nq. q. Job\n\nr. r. Ingress\n\n2. 2. In the same kind, then by name (alphabetical order)\n\nArgo CD has three phases when applying resources: the first phase is executed before applying the manifests (PreSync), the second phase is when the manifests are applied (Sync), and the third phase is executed after all manifests are applied and synchronized (PostSync).\n\nFigure 7-13 summarizes these phases.\n\nFigure 7-13. Hooks and sync waves\n\nResource hooks are scripts executed at a given phase, or if the Sync phase failed, you could run some rollback operations.\n\n7.7 Order Kubernetes Manifests\n\n|\n\n183\n\nTable 7-1 lists the available resource hooks.\n\nTable 7-1. Resource hooks\n\nHook\n\nPreSync\n\nDescription Executes prior to the application of the manifests\n\nUse case Database migrations\n\nSync\n\nExecutes at the same time as manifests\n\nComplex rolling update strategies like canary releases or dark launches\n\nPostSync Executes after all Sync hooks have completed and\n\nRun tests to validate deployment was correctly done\n\nwere successful (healthy)\n\nSyncFail Executes when the sync operation fails\n\nRollback operations in case of failure\n\nSkip\n\nSkip the application of the manifest\n\nWhen manual steps are required to deploy the application (i.e., releasing public traffic to new version)\n\nHooks are defined as an annotation named argocd.argoproj.io/hook to a Kuber‐ netes resource. In the following snippet, a PostSync manifest is defined:\n\napiVersion: batch/v1 kind: Job metadata: name: todo-insert annotations: argocd.argoproj.io/hook: PostSync\n\nJob’s name\n\nSets when the manifest is applied\n\nDeletion Policies A hook is not deleted when finished; for example, if you run a Kubernetes Job, it’ll remain Completed.\n\nThis might be the desired state, but we can specify to automatically delete these resources if annotated with argocd.argoproj.io/hook-delete-policy and the pol‐ icy value is set.\n\nSupported policies are:\n\nPolicy HookSucceeded\n\nDescription Deleted after the hook succeeded\n\nHookFailed\n\nDeleted after the hook failed\n\nBeforeHookCreation Deleted before the new one is created\n\n184\n\n|\n\nChapter 7: Argo CD\n\nA sync wave is a way to order how Argo CD applies the manifests stored in Git.\n\nAll manifests have zero waves by default, and the lower values go first. Use the argocd.argoproj.io/sync-wave annotation to set the wave number to a resource.\n\nFor example, you might want to deploy a database first and then create the database schema; for this case, you should set a sync-wave lower in the database deployment file than in the job for creating the database schema, as shown in the following snippet:\n\napiVersion: apps/v1 kind: Deployment metadata: name: postgresql namespace: todo annotations: argocd.argoproj.io/sync-wave: \"0\" ... apiVersion: batch/v1 kind: Job metadata: name: todo-table namespace: todo annotations: argocd.argoproj.io/sync-wave: \"1\"\n\nPostgreSQL deployment\n\nSync wave for PostgreSQL deployment is 0\n\nName of the Job\n\nJob executed when PostgreSQL is healthy\n\nDiscussion When Argo CD starts applying the manifests, it orders the resources in the following way:\n\n1. 1. Phase\n\n2. 2. Wave (lower precedence first)\n\n3. Kind 3.\n\n4. Name 4.\n\nLet’s deploy a more significant application with deployment files, sync waves, and hooks.\n\n7.7 Order Kubernetes Manifests\n\n|\n\n185",
      "page_number": 188
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 201-209)",
      "start_page": 201,
      "end_page": 209,
      "detection_method": "topic_boundary",
      "content": "The sample application deployed is a TODO application connected with a database (PostgreSQL) to store TODOs. To deploy the application, some particular order needs to be applied; for example, the database server must be running before creating the database schema. Also, when the whole application is deployed, we insert some default TODOs into the database to run a post-sync manifest.\n\nThe overall process is shown in Figure 7-14.\n\nFigure 7-14. Todo app\n\n186\n\n|\n\nChapter 7: Argo CD\n\nCreate an Application resource pointing out to the application:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: todo-app namespace: argocd spec: destination: namespace: todo server: https://kubernetes.default.svc project: default source: path: ch07/todo repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main syncPolicy: automated: prune: true selfHeal: false syncOptions: - CreateNamespace=true\n\nIn the terminal, apply the resource, and Argo CD will deploy all applications in the specified order.\n\nSee Also\n\n• gitops-engine/sync_tasks.go on GitHub\n\n7.8 Define Synchronization Windows\n\nProblem You want Argo CD to block or allow application synchronization depending on time.\n\nSolution Argo CD has the sync windows concept to configure time windows where application synchronizations (applying new resources that have been pushed to the repository) will either be blocked or allowed.\n\nTo define a sync window, create an AppProject manifest setting the kind (either allow or deny), a schedule in cron format to define the initial time, a duration of the window, and which resources the sync window is applied to (Application, namespaces, or clusters).\n\n7.8 Define Synchronization Windows\n\n|\n\n187\n\nAbout Cron Expressions\n\nA cron expression represents a time. It’s composed of the following fields:\n\n┌────────── minute (0 - 59) │ ┌────────── hour (0 - 23) │ │ ┌────────── day of the month (1 - 31) │ │ │ ┌────────── month (1 - 12) │ │ │ │ ┌────────── day of the week (0 - 6) * * * * *\n\nThe AppProject resource is responsible for defining these windows where synchroni‐ zations are permitted/blocked.\n\nCreate a new file to permit synchronizations only from 22:00 to 23:00 (just one hour) and for Argo CD Applications whose names end in -prod:\n\napiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: default spec: syncWindows: - kind: allow schedule: '0 22 * * *' duration: 1h applications: - '*-prod'\n\nList of windows\n\nAllow syncs\n\nOnly at 22:00\n\nFor 1 hour (23:00)\n\nSets the applications that affect this window\n\nRegular expression matching any application whose name ends with -prod\n\nDiscussion We cannot perform a sync of the application (neither automatic nor manual) when it’s not the time configured in the time window defined in the AppProject manifest. However, we can configure a window to allow manual syncs.\n\n188\n\n|\n\nChapter 7: Argo CD\n\nUsing the CLI tool:\n\nargocd proj windows enable-manual-sync <PROJECT ID>\n\nAlso, manual sync can be set in the YAML file. In the following example, we’re setting manual synchronization for the namespace default, denying synchronizations at 22:00 for one hour and allowing synchronizations in prod-cluster at 23:00 for one hour:\n\napiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: default namespace: argocd spec: syncWindows: - kind: deny schedule: '0 22 * * *' duration: 1h manualSync: true namespaces: - bgd - kind: allow schedule: '0 23 * * *' duration: 1h clusters: - prod-cluster\n\nBlock synchronizations\n\nEnable manual sync to default namespace\n\nConfigure namespaces to block\n\nConfigure clusters to allow syncs at 23:00\n\nWe can inspect the current windows from the UI by going to the Settings → Projects → default → windows tab or by using the argocd CLI tool:\n\nargocd proj windows list default\n\nID STATUS KIND SCHEDULE DURATION APPLICATIONS NAMESPACES CLUSTERS MANUALSYNC 0 Inactive deny 0 22 * * * 1h - bgd - Enabled 1 Inactive allow 0 23 * * * 1h - - prod-cluster Disabled\n\n7.8 Define Synchronization Windows\n\n|\n\n189\n\nCHAPTER 8 Advanced Topics\n\nIn the previous chapter, you had an overview of implementing GitOps workflows using Argo CD recipes. Argo CD is a famous and influential open source project that helps with both simple use cases and more advanced ones. In this chapter, we will discuss topics needed when you move forward in your GitOps journey, and you need to manage security, automation, and advanced deployment models for multicluster scenarios.\n\nSecurity is a critical aspect of automation and DevOps. DevSecOps is a new definition of an approach where security is a shared responsibility throughout the entire IT lifecycle. Furthermore, the DevSecOps Manifesto specifies security as code to operate and contribute value with less friction. And this goes in the same direction as GitOps principles, where everything is declarative.\n\nOn the other hand, this also poses the question of avoiding storing unencrypted plain-text credentials in Git. As stated in the book Path to GitOps by Christian Hernandez, Argo CD luckily currently provides two patterns to manage security in GitOps workflows:\n\n• Storing encrypted secrets in Git, such as with a Sealed Secret (see Recipe 8.1)\n\nStoring secrets in external services or vaults, then storing only the reference to • such secrets in Git (see Recipe 8.2)\n\nThe chapter then moves to advanced deployment techniques, showing how to manage webhooks with Argo CD (see Recipe 8.3) and with ApplicationSets (see Recipe 8.4). ApplicationSets is a component of Argo CD that allows management deployments of many applications, repositories, or clusters from a single Kubernetes resource. In essence, a templating system for the GitOps application is ready to be deployed and synced in multiple Kubernetes clusters (see Recipe 8.5).\n\n191\n\nLast but not least, the book ends with a recipe on Progressive Delivery for Kuber‐ netes with Argo Rollouts (Recipe 8.6), useful for deploying the application using an advanced deployment technique such as blue-green or canary.\n\n8.1 Encrypt Sensitive Data (Sealed Secrets)\n\nProblem You want to manage Kubernetes Secrets and encrypted objects in Git.\n\nSolution Sealed Secrets is an open source project by Bitnami used to encrypt a Kubernetes Secrets into a SealedSecret Kubernetes Custom Resource, representing an encrypted object safe to store in Git.\n\nSealed Secrets uses public-key cryptography and consists of two main components:\n\nA Kubernetes controller that has knowledge about the private and public key • used to decrypt and encrypt encrypted secrets and is responsible for reconcilia‐ tion. The controller also supports automatic secret rotation for the private key and key expiration management in order to enforce the re-encryption of secrets. • kubeseal, a CLI used by developers to encrypt their secrets before committing • them to a Git repository.\n\nThe SealedSecret object is encrypted and decrypted only by the SealedSecret controller running in the target Kubernetes cluster. This operation is exclusive only to this component, thus nobody else can decrypt the object. The kubeseal CLI allows the developer to take a normal Kubernetes Secret resource and convert it to a SealedSecret resource definition as shown in Figure 8-1.\n\nIn your Kubernetes cluster with Argo CD, you can install the kubeseal CLI for your operating system from the GitHub project’s releases. At the time of writing this book, we are using version 0.18.2.\n\nOn macOS, kubeseal is available through Homebrew as follows:\n\nbrew install kubeseal\n\n192\n\n|\n\nChapter 8: Advanced Topics\n\nFigure 8-1. Sealed Secrets with GitOps\n\nAfter you install the CLI, you can install the controller as follows:\n\nkubectl create \\ -f https://github.com/bitnami-labs/sealed-secrets/releases/download/0.18.2/control- ler.yaml\n\nYou should have output similar to the following:\n\nserviceaccount/sealed-secrets-controller created deployment.apps/sealed-secrets-controller created customresourcedefinition.apiextensions.k8s.io/sealedsecrets.bitnami.com created service/sealed-secrets-controller created rolebinding.rbac.authorization.k8s.io/sealed-secrets-controller created rolebinding.rbac.authorization.k8s.io/sealed-secrets-service-proxier created role.rbac.authorization.k8s.io/sealed-secrets-service-proxier created role.rbac.authorization.k8s.io/sealed-secrets-key-admin created clusterrolebinding.rbac.authorization.k8s.io/sealed-secrets-controller created clusterrole.rbac.authorization.k8s.io/secrets-unsealer created\n\nAs an example, let’s create a Secret for the Pac-Man game deployed in Chapter 5:\n\nkubectl create secret generic pacman-secret \\ --from-literal=user=pacman \\ --from-literal=pass=pacman\n\nYou should have the following output:\n\nsecret/pacman-secret created\n\nAnd here you can see the YAML representation:\n\nkubectl get secret pacman-secret -o yaml\n\n8.1 Encrypt Sensitive Data (Sealed Secrets)\n\n|\n\n193\n\napiVersion: v1 data: pass: cGFjbWFu user: cGFjbWFu kind: Secret metadata: name: pacman-secret namespace: default type: Opaque\n\nNow, you can convert the Secret into a SealedSecret in this way:\n\nkubectl get secret pacman-secret -o yaml \\ | kubeseal -o yaml > pacman-sealedsecret.yaml\n\napiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: creationTimestamp: null name: pacman-secret namespace: default spec: encryptedData: pass: AgBJR1AgZ5Gu5NOVsG1E8SKBcdB3QSDdzZka3RRYuWV7z8g7ccQ0dGc1suVOP8wX/ ZpPmIMp8+urPYG62k4EZRUjuu/Vg2E1nSbsGBh9eKu3NaO6tGSF3eGk6PzN6XtRhDeER4u7MG5pj/ +FXRAKcy8Z6RfzbVEGq/QJQ4z0ecSNdJmG07ERMm1Q+lPNGvph2Svx8aCgFLqRsdLhFyvwb TyB3XnmFHrPr+2DynxeN8XVMoMkRYXgVc6GAoxUK7CnC3Elpuy7lIdPwc5QBx9kUVfra83LX8/KxeaJ wyCqvscIGjtcxUtpTpF5jm1t1DSRRNbc4m+7pTwTmnRiUuaMVeujaBco4521yTkh5iEPjnj vUt+VzK01NVoeNunqIazp15rFwTvmiQ5PAtbiUXpT733zCr60QBgSxPg31vw98+u+RcIHvaMIoDCqaX xUdcn2JkUF+bZXtxNmIRTAiQVQ1vEPmrZxpvZcUh/PPC4L/RFWrQWnOzKRyqLq9wRoSLPbKyvMX naxH0v3USGIktmtJlGjlXoW/i+HIoSeMFS0mUAzOF5M5gweOhtxKGh3Y74ZDn5PbVA/ 9kbkuWgvPNGDZL924Dm6AyM5goHECr/RRTm1e22K9BfPASARZuGA6paqb9h1XEqyqesZgM0R8PLiy Luu+tpqydR0SiYLc5VltdjzpIyyy9Xmw6Aa3/4SB+4tSwXSUUrB5yc= user: AgBhYDZQzOwinetPceZL897aibTYp4QPGFvP6ZhDyuUAx OWXBQ7jBA3KPUqLvP8vBcxLAcS7HpKcDSgCdi47D2WhShdBR4jWJufwKmR3j+ayTdw72t3ALpQhTYI0iMY TiNdR0/o3vf0jeNMt/oWCRsifqBxZaIShE53rAFEjEA6D7CuCDXu8BHk1DpSr79d5Au4puzpH VODh+v1T+Yef3k7DUoSnbYEh3CvuRweiuq5lY8G0oob28j38wdyxm3GIrexa+M/ ZIdO1hxZ6jz4edv6ejdZfmQNdru3c6lmljWwcO+0Ue0MqFi4ZF/YNUsiojI+781n1m3K/ giKcyPLn0skD7DyeKPoukoN6W5P71OuFSkF+VgIeejDaxuA7bK3PEaUgv79KFC9aEEnBr/ 7op7HY7X6aMDahmLUc/+zDhfzQvwnC2wcj4B8M2OBFa2ic2PmGzrIWhlBbs1OgnpehtG SETq+YRDH0alWOdFBq1U8qn6QA8Iw6ewu8GTele3zlPLaADi5O6LrJbIZNlY0+PutWfjs9ScVVEJy+I9BGd yT6tiA/4v4cxH6ygG6NzWkqxSaYyNrWWXtLhOlqyCpTZ tUwHnF+OLB3gCpDZPx+NwTe2Kn0jY0c83LuLh5PJ090AsWWqZaRQyE LeL6y6mVekQFWHGfK6t57Vb7Z3+5XJCgQn+xFLkj3SIz0ME5D4+DSsUDS1fyL8uI= template: data: null metadata: creationTimestamp: null name: pacman-secret namespace: default type: Opaque\n\nHere you find the data encrypted by the Sealed Secrets controller.\n\n194\n\n|\n\nChapter 8: Advanced Topics\n\nNow you can safely push your SealedSecret to your Kubernetes manifests repo and create the Argo CD application. Here’s an example from this book’s repository:\n\nargocd app create pacman \\ --repo https://github.com/gitops-cookbook/pacman-kikd-manifests.git \\ --path 'k8s/sealedsecrets' \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace default \\ --sync-policy auto\n\nCheck if the app is running and healthy:\n\nargocd app list\n\nYou should get output similar to the following:\n\nNAME CLUSTER NAMESPACE PROJECT STATUS HEALTH ↳ SYNCPOLICY CONDITIONS REPO PATH TARGET pacman https://kubernetes.default.svc default default Synced Healthy↳ <none> <none> https://github.com/gitops-cookbook/pacman-kikd- manifests.git k8s/sealedsecrets\n\n8.2 Encrypt Secrets with ArgoCD (ArgoCD + HashiCorp Vault + External Secret)\n\nProblem You want to avoid storing credentials in Git and you want to manage them in external services or vaults.\n\nSolution In Recipe 8.1 you saw how to manage encrypted data in Git following the GitOps declarative way, but how do you avoid storing even encrypted credentials with GitOps?\n\nOne solution is External Secrets, an open source project initially created by GoDaddy, which aims at storing secrets in external services or vaults from different vendors, then storing only the reference to such secrets in Git.\n\nToday, External Secrets supports systems such as AWS Secrets Manager, HashiCorp Vault, Google Secrets Manager, Azure Key Vault, and more. The idea is to provide a user-friendly abstraction for the external API that stores and manages the lifecycles of the secrets.\n\nIn depth, ExternalSecrets is a Kubernetes controller that reconciles Secrets into the cluster from a Custom Resource that includes a reference to a secret in an external key management system. The Custom Resource SecretStore specifies the backend\n\n8.2 Encrypt Secrets with ArgoCD (ArgoCD + HashiCorp Vault + External Secret)\n\n|\n\n195",
      "page_number": 201
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 210-217)",
      "start_page": 210,
      "end_page": 217,
      "detection_method": "topic_boundary",
      "content": "containing the confidential data, and how it should be transformed into a Secret by defining a template, as you can see in Figure 8-2. The SecretStore has the configura‐ tion to connect to the external secret manager.\n\nThus, the ExternalSecrets objects can be safely stored in Git, as they do not contain any confidential information, but just the references to the external services manag‐ ing credentials.\n\nFigure 8-2. External Secrets with Argo CD\n\nYou can install External Secrets with a Helm Chart as follows. At the time of writing this book, we are using version 0.5.9:\n\nhelm repo add external-secrets https://charts.external-secrets.io\n\nhelm install external-secrets \\ external-secrets/external-secrets \\ -n external-secrets \\ --create-namespace\n\nYou should get output similar to the following:\n\nNAME: external-secrets LAST DEPLOYED: Fri Sep 2 13:09:53 2022 NAMESPACE: external-secrets STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: external-secrets has been deployed successfully!\n\n196\n\n|\n\nChapter 8: Advanced Topics\n\nIn order to begin using ExternalSecrets, you will need to set up a SecretStore or ClusterSecretStore resource (for example, by creating a vault SecretStore).\n\nMore information on the different types of SecretStores and how to configure them can be found in our GitHub page.\n\nYou can also install the External Secrets Operator with OLM from OperatorHub.io.\n\nAs an example with one of the providers supported, such as HashiCorp Vault, you can do the following.\n\nFirst download and install HashiCorp Vault for your operating system and get your Vault Token. Then create a Kubernetes Secret as follows:\n\nexport VAULT_TOKEN=<YOUR_TOKEN> kubectl create secret generic vault-token \\ --from-literal=token=$VAULT_TOKEN \\ -n external-secrets\n\nThen create a SecretStore as a reference to this external system:\n\napiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: vault-secretstore namespace: default spec: provider: vault: server: \"http://vault.local:8200\" path: \"secret\" version: \"v2\" auth: tokenSecretRef: name: \"vault-token\" key: \"token\" namespace: external-secrets\n\nHostname where your Vault is running\n\nName of the Kubernetes Secret containing the vault token\n\nKey to address the value in the Kubernetes Secret containing the vault token content:\n\nkubectl create -f vault-secretstore.yaml\n\n8.2 Encrypt Secrets with ArgoCD (ArgoCD + HashiCorp Vault + External Secret)\n\n|\n\n197\n\nNow you can create a Secret in your Vault as follows:\n\nvault kv put secret/pacman-secrets pass=pacman\n\nAnd then reference it from the ExternalSecret as follows:\n\napiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: pacman-externalsecrets namespace: default spec: refreshInterval: \"15s\" secretStoreRef: name: vault-secretstore kind: SecretStore target: name: pacman-externalsecrets data: - secretKey: token remoteRef: key: secret/pacman-secrets property: pass\n\nkubectl create -f pacman-externalsecrets.yaml\n\nNow you can deploy the Pac-Man game with Argo CD using External Secrets as follows:\n\nargocd app create pacman \\ --repo https://github.com/gitops-cookbook/pacman-kikd-manifests.git \\ --path 'k8s/externalsecrets' \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace default \\ --sync-policy auto\n\n8.3 Trigger the Deployment of an Application Automatically (Argo CD Webhooks)\n\nProblem You don’t want to wait for Argo CD syncs and you want to immediately deploy an application when a change occurs in Git.\n\nSolution While Argo CD polls Git repositories every three minutes to detect changes to the monitored Kubernetes manifests, it also supports an event-driven approach with webhooks notifications from popular Git servers such as GitHub, GitLab, or Bitbucket.\n\n198\n\n|\n\nChapter 8: Advanced Topics\n\nArgo CD Webhooks are enabled in your Argo CD installation and available at the endpoint /api/webhooks.\n\nTo test webhooks with Argo CD using Minikube you can use Helm to install a local Git server such as Gitea, an open source lightweight server written in Go, as follows:\n\nhelm repo add gitea-charts https://dl.gitea.io/charts/ helm install gitea gitea-charts/gitea\n\nYou should have output similar to the following:\n\nhelm install gitea gitea-charts/gitea \"gitea-charts\" has been added to your repositories NAME: gitea LAST DEPLOYED: Fri Sep 2 15:04:04 2022 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Get the application URL by running these commands: echo \"Visit http://127.0.0.1:3000 to use your application\" kubectl --namespace default port-forward svc/gitea-http 3000:3000\n\nLog in to the Gitea server with the default credentials you find the in the values.yaml file from the Helm Chart here or define new ones via overriding them.\n\nImport the Pac-Man manifests repo into Gitea.\n\nConfigure the Argo app:\n\nargocd app create pacman-webhook \\ --repo http://gitea-http.default.svc:3000/gitea_admin/pacman-kikd-manifests.git \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace default \\ --path k8s \\ --sync-policy auto\n\nTo add a webhook to Gitea, navigate to the top-right corner and click Settings. Select the Webhooks tab and configure it as shown in Figure 8-3:\n\nPayload URL: http://localhost:9090/api/webhooks • • Content type: application/json •\n\n8.3 Trigger the Deployment of an Application Automatically (Argo CD Webhooks)\n\n|\n\n199\n\nFigure 8-3. Gitea Webhooks\n\nYou can omit the Secret for this example; however, it’s best practice to configure secrets for your webhooks. Read more from the docs.\n\nSave it and push your change to the repo on Gitea. You will see a new sync from Argo CD immediately after your push.\n\n8.4 Deploy to Multiple Clusters\n\nProblem You want to deploy an application to different clusters.\n\nSolution Argo CD supports the ApplicationSet resource to “templetarize” an Argo CD Application resource. It covers different use cases, but the most important are:\n\n200\n\n|\n\nChapter 8: Advanced Topics\n\n• Use a Kubernetes manifest to target multiple Kubernetes clusters.\n\n• Deploy multiple applications from one or multiple Git repositories.\n\nSince the ApplicationSet is a template file with placeholders to substitute at run‐ time, we need to feed these with some values. For this purpose, ApplicationSet has the concept of generators.\n\nA generator is responsible for generating the parameters, which will finally be replaced in the template placeholders to generate a valid Argo CD Application.\n\nCreate the following ApplicationSet:\n\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: bgd-app namespace: argocd spec: generators: - list: elements: - cluster: staging url: https://kubernetes.default.svc location: default - cluster: prod url: https://kubernetes.default.svc location: app template: metadata: name: '{{cluster}}-app' spec: project: default source: repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main path: ch08/bgd-gen/{{cluster}} destination: server: '{{url}}' namespace: '{{location}}' syncPolicy: syncOptions: - CreateNamespace=true\n\nDefines a generator\n\nSets the value of the parameters\n\nDefines the Application resource as a template\n\ncluster placeholder\n\n8.4 Deploy to Multiple Clusters\n\n|\n\n201\n\nurl placeholder\n\nApply the previous file by running the following command:\n\nkubectl apply -f bgd-application-set.yaml\n\nWhen this ApplicationSet is applied to the cluster, Argo CD generates and automat‐ ically registers two Application resources. The first one is:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: staging-app spec: project: default source: path: ch08/bgd-gen/staging repoURL: https://github.com/example/app.git targetRevision: HEAD destination: namespace: default server: https://kubernetes.default.svc ...\n\nAnd the second one:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: prod-app spec: project: default source: path: ch08/bgd-gen/prod repoURL: https://github.com/example/app.git targetRevision: HEAD destination: namespace: app server: https://kubernetes.default.svc ...\n\nInspect the creation of both Application resources by running the following command:\n\n# Remember to login first argocd login --insecure --grpc-web $argoURL --username admin --password $argoPass\n\nargocd app list\n\nAnd the output should be similar to (trunked):\n\nNAME CLUSTER NAMESPACE prod-app https://kubernetes.default.svc app staging-app https://kubernetes.default.svc default\n\n202\n\n|\n\nChapter 8: Advanced Topics\n\nDelete both applications by deleting the ApplicationSet file:\n\nkubectl delete -f bgd-application-set.yaml\n\nDiscussion We’ve seen the simplest generator, but there are eight generators in total at the time of writing this book:\n\nList\n\nGenerates Application definitions through a fixed list of clusters. (It’s the one we’ve seen previously).\n\nCluster\n\nSimilar to List but based on the list of clusters defined in Argo CD.\n\nGit\n\nGenerates Application definitions based on a JSON/YAML properties file within a Git repository or based on the directory layout of the repository.\n\nSCM Provider\n\nGenerates Application definitions from repositories within an organization.\n\nPull Request\n\nGenerates Application definitions from open pull requests.\n\nCluster Decision Resource\n\nGenerates Application definitions using duck-typing.\n\nMatrix\n\nCombines values of two separate generators.\n\nMerge\n\nMerges values from two or more generators.\n\nIn the previous example, we created the Application objects from a fixed list of elements. This is fine when the number of configurable environments is small; in the example, two clusters refer to two Git folders (ch08/bgd-gen/staging and ch08/ bgd-gen/prod). In the case of multiple environments (which means various folders), we can dynamically use the Git generator to generate one Application per directory.\n\nLet’s migrate the previous example to use the Git generator. As a reminder, the Git directory layout used was:\n\nbgd-gen ├── staging │ ├── ...yaml └── prod ├── ...yaml\n\n8.4 Deploy to Multiple Clusters\n\n|\n\n203",
      "page_number": 210
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 218-227)",
      "start_page": 218,
      "end_page": 227,
      "detection_method": "topic_boundary",
      "content": "Create a new file of type ApplicationSet generating an Application for each direc‐ tory of the configured Git repo:\n\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: cluster-addons namespace: openshift-gitops spec: generators: - git: repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git revision: main directories: - path: ch08/bgd-gen/* template: metadata: name: '{{path[0]}}{{path[2]}}' spec: project: default source: repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main path: '{{path}}' destination: server: https://kubernetes.default.svc namespace: '{{path.basename}}'\n\nConfigures the Git repository to read layout\n\nInitial path to start scanning directories\n\nApplication definition\n\nThe directory paths within the Git repository matching the path wildcard (stag ing or prod)\n\nDirectory path (full path)\n\nThe rightmost pathname\n\nApply the resource:\n\nkubectl apply -f bgd-git-application-set.yaml\n\nArgo CD creates two applications as there are two directories:\n\nargocd app list\n\nNAME CLUSTER NAMESPACE ch08prod https://kubernetes.default.svc prod ch08staging https://kubernetes.default.svc staging\n\n204\n\n|\n\nChapter 8: Advanced Topics\n\nAlso, this generator is handy when your application is composed of different compo‐ nents (service, database, distributed cache, email server, etc.), and deployment files for each element are placed in other directories. Or, for example, a repository with all operators required to be installed in the cluster:\n\napp ├── tekton-operator │ ├── ...yaml ├── prometheus-operator │ ├── ...yaml └── istio-operator ├── ...yaml\n\nInstead of reacting to directories, Git generator can create Application objects with parameters specified in JSON/YAML files.\n\nThe following snippet shows an example JSON file:\n\n{ \"cluster\": { \"name\": \"staging\", \"address\": \"https://1.2.3.4\" } }\n\nThis is an excerpt of the ApplicationSet to react to these files:\n\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: guestbook spec: generators: - git: repoURL: https://github.com/example/app.git revision: HEAD files: - path: \"app/**/config.json\" template: metadata: name: '{{cluster.name}}-app' ....\n\nFinds all config.json files placed in all subdirectories of the app\n\nInjects the value set in config.json\n\nThis ApplicationSet will generate one Application for each config.json file in the folders matching the path expression.\n\n8.4 Deploy to Multiple Clusters\n\n|\n\n205\n\nSee Also\n\n• Argo CD Generators\n\n• Duck Types\n\n8.5 Deploy a Pull Request to a Cluster\n\nProblem You want to deploy a preview of the application when a pull request is created.\n\nSolution Use the pull request generator to automatically discover open pull requests within a repository and create an Application object.\n\nLet’s create an ApplicationSet reacting to any GitHub pull request annotated with the preview label created on the configured repository.\n\nCreate a new file named bgd-pr-application-set.yaml with the following content:\n\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: myapps namespace: openshift-gitops spec: generators: - pullRequest: github: owner: gitops-cookbook repo: gitops-cookbook-sc labels: - preview requeueAfterSeconds: 60 template: metadata: name: 'myapp-{{branch}}-{{number}}' spec: source: repoURL: 'https://github.com/gitops-cookbook/gitops-cookbook-sc.git' targetRevision: '{{head_sha}}' path: ch08/bgd-pr project: default destination: server: https://kubernetes.default.svc namespace: '{{branch}}-{{number}}'\n\nGitHub pull request generator\n\n206\n\n|\n\nChapter 8: Advanced Topics\n\nOrganization/user\n\nRepository\n\nSelect the target PRs\n\nPolling time in seconds to check if there is a new PR (60 seconds)\n\nSets the name with branch name and number\n\nSets the Git SHA number\n\nApply the previous file by running the following command:\n\nkubectl apply -f bgd-pr-application-set.yaml\n\nNow, if you list the Argo CD applications, you’ll see that none are registered. The reason is there is no pull request yet in the repository labeled with preview:\n\nargocd app list NAME CLUSTER NAMESPACE PROJECT STATUS\n\nCreate a pull request against the repository and label it with preview.\n\nIn GitHub, the pull request window should be similar to Figure 8-4.\n\nFigure 8-4. Pull request in GitHub\n\nWait for one minute until the ApplicationSet detects the change and creates the Application object.\n\nRun the following command to inspect that the change has been detected and registered:\n\n8.5 Deploy a Pull Request to a Cluster\n\n|\n\n207\n\nkubectl describe applicationset myapps -n argocd\n\n... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal created 23s applicationset-controller created Applica- tion \"myapp-lordofthejars-patch-1-1\" Normal unchanged 23s (x2 over 23s) applicationset-controller unchanged Appli- cation \"myapp-lordofthejars-patch-1-1\"\n\nCheck the registration of the Application to the pull request:\n\nargocd app list NAME CLUSTER NAMESPACE myapp-lordofthejars-patch-1-1 https://kubernetes.default.svc lordofthejars- patch-1-1\n\nThe Application object is automatically removed when the pull request is closed.\n\nDiscussion At the time of writing this book, the following pull request providers are supported:\n\n• GitHub\n\n• Bitbucket\n\n• Gitea\n\n• GitLab\n\nThe ApplicationSet controller polls every requeueAfterSeconds interval to detect changes but also supports using webhook events.\n\nTo configure it, follow Recipe 8.3, but also enable sending pull requests events too in the Git provider.\n\n8.6 Use Advanced Deployment Techniques\n\nProblem You want to deploy the application using an advanced deployment technique such as blue-green or canary.\n\nSolution Use the Argo Rollouts project to roll out updates to an application.\n\n208\n\n|\n\nChapter 8: Advanced Topics\n\nArgo Rollouts is a Kubernetes controller providing advanced deployment techniques such as blue-green, canary, mirroring, dark canaries, traffic analysis, etc. to Kuber‐ netes. It integrates with many Kubernetes projects like Ambassador, Istio, AWS Load Balancer Controller, NGNI, SMI, or Traefik for traffic management, and projects like Prometheus, Datadog, and New Relic to perform analysis to drive progressive delivery.\n\nTo install Argo Rollouts to the cluster, run the following command in a terminal window:\n\nkubectl create namespace argo-rollouts\n\nkubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/relea- ses/download/v1.2.2/install.yaml ... clusterrolebinding.rbac.authorization.k8s.io/argo-rollouts created secret/argo-rollouts-notification-secret created service/argo-rollouts-metrics created deployment.apps/argo-rollouts created\n\nAlthough it’s not mandatory, we recommend you install the Argo Rollouts Kubectl Plugin to visualize rollouts. Follow the instructions to install it. With everything in place, let’s deploy the initial version of the BGD application.\n\nArgo Rollouts doesn’t use the standard Kubernetes Deployment file, but a specific new Kubernetes resource named Rollout. It’s like a Deployment object, hence all its options are supported, but it adds some fields to configure the rolling update.\n\nLet’s deploy the first version of the application. We’ll define the canary release process when Kubernetes executes a rolling update, which in this case follows these steps:\n\n1. 1. Forward 20% of traffic to the new version.\n\n2. 2. Wait until a human decides to proceed with the process.\n\n3. Forward 40%, 60%, 80% of the traffic to the new version automatically, waiting 3. 30 seconds between every increase.\n\nCreate a new file named bgd-rollout.yaml with the following content:\n\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: bgd-rollouts spec: replicas: 5 strategy: canary: steps: - setWeight: 20 - pause: {}\n\n8.6 Use Advanced Deployment Techniques\n\n|\n\n209\n\nsetWeight: 40 - pause: {duration: 30s} - setWeight: 60 - pause: {duration: 30s} - setWeight: 80 - pause: {duration: 30s} revisionHistoryLimit: 2 selector: matchLabels: app: bgd-rollouts template: metadata: creationTimestamp: null labels: app: bgd-rollouts spec: containers: - image: quay.io/rhdevelopers/bgd:1.0.0 name: bgd env: - name: COLOR value: \"blue\" resources: {} Sets the ratio of canary\n\nRollout is paused\n\nPauses the rollout for 30 seconds\n\ntemplate Deployment definition\n\nApply the resource to deploy the application. Since there is no previous deployment, the canary part is ignored:\n\nkubectl apply -f bgd-rollout.yaml\n\nCurrently, there are five pods as specified in the replicas field:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE bgd-rollouts-679cdfcfd-6z2zf 1/1 Running 0 12m bgd-rollouts-679cdfcfd-8c6kl 1/1 Running 0 12m bgd-rollouts-679cdfcfd-8tb4v 1/1 Running 0 12m bgd-rollouts-679cdfcfd-f4p7f 1/1 Running 0 12m bgd-rollouts-679cdfcfd-tljfr 1/1 Running 0 12m\n\n210\n\n|\n\nChapter 8: Advanced Topics\n\nAnd using the Argo Rollout Kubectl Plugin:\n\nkubectl argo rollouts get rollout bgd-rollouts\n\nName: bgd-rollouts Namespace: default Status: ✔ Healthy Strategy: Canary Step: 8/8 SetWeight: 100 ActualWeight: 100 Images: quay.io/rhdevelopers/bgd:1.0.0 (stable) Replicas: Desired: 5 Current: 5 Updated: 5 Ready: 5 Available: 5\n\nNAME KIND STATUS AGE INFO ⟳ bgd-rollouts Rollout ✔ Healthy 13m └──# revision:1 └──⧉ bgd-rollouts-679cdfcfd ReplicaSet ✔ Healthy 13m stable ├──□ bgd-rollouts-679cdfcfd-6z2zf Pod ✔ Running 13m ready:1/1 ├──□ bgd-rollouts-679cdfcfd-8c6kl Pod ✔ Running 13m ready:1/1 ├──□ bgd-rollouts-679cdfcfd-8tb4v Pod ✔ Running 13m ready:1/1 ├──□ bgd-rollouts-679cdfcfd-f4p7f Pod ✔ Running 13m ready:1/1 └──□ bgd-rollouts-679cdfcfd-tljfr Pod ✔ Running 13m ready:1/1\n\nLet’s deploy a new version to trigger a canary rolling update. Create a new file named bgd-rollout-v2.yaml with exactly the same content as the previous one, but change the environment variable COLOR value to green:\n\n... name: bgd env: - name: COLOR value: \"green\" resources: {}\n\nApply the previous resource and check how Argo Rollouts executes the rolling update. List the pods again to check that 20% of the pods are new while the other 80% are the old version:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE bgd-rollouts-679cdfcfd-6z2zf 1/1 Running 0 27m bgd-rollouts-679cdfcfd-8c6kl 1/1 Running 0 27m bgd-rollouts-679cdfcfd-8tb4v 1/1 Running 0 27m bgd-rollouts-679cdfcfd-tljfr 1/1 Running 0 27m bgd-rollouts-c5495c6ff-zfgvn 1/1 Running 0 13s\n\n8.6 Use Advanced Deployment Techniques\n\n|\n\n211\n\nNew version pod\n\nAnd do the same using the Argo Rollout Kubectl Plugin:\n\nkubectl argo rollouts get rollout bgd-rollouts\n\n... NAME KIND STATUS AGE INFO ⟳ bgd-rollouts Rollout ॥ Paused 31m ├──# revision:2 │ └──⧉ bgd-rollouts-c5495c6ff ReplicaSet ✔ Healthy 3m21s canary │ └──□ bgd-rollouts-c5495c6ff-zfgvn Pod ✔ Running 3m21s ready:1/1 └──# revision:1 └──⧉ bgd-rollouts-679cdfcfd ReplicaSet ✔ Healthy 31m stable ├──□ bgd-rollouts-679cdfcfd-6z2zf Pod ✔ Running 31m ready:1/1 ├──□ bgd-rollouts-679cdfcfd-8c6kl Pod ✔ Running 31m ready:1/1 ├──□ bgd-rollouts-679cdfcfd-8tb4v Pod ✔ Running 31m ready:1/1 └──□ bgd-rollouts-679cdfcfd-tljfr Pod ✔ Running 31m ready:1/1\n\nRemember that the rolling update process is paused until the operator executes a manual step to let the process continue. In a terminal window, run the following command:\n\nkubectl argo rollouts promote bgd-rollouts\n\nThe rollout is promoted and continues with the following steps, which is substituting the old version pods with new versions every 30 seconds:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE bgd-rollouts-c5495c6ff-2g7r8 1/1 Running 0 89s bgd-rollouts-c5495c6ff-7mdch 1/1 Running 0 122s bgd-rollouts-c5495c6ff-d9828 1/1 Running 0 13s bgd-rollouts-c5495c6ff-h4t6f 1/1 Running 0 56s bgd-rollouts-c5495c6ff-zfgvn 1/1 Running 0 11m\n\nThe rolling update finishes with the new version progressively deployed to the cluster.\n\nDiscussion Kubernetes doesn’t implement advanced deployment techniques natively. For this reason, Argo Rollouts uses the number of deployed pods to implement the canary release.\n\nAs mentioned before, Argo Rollouts integrates with Kubernetes products that offer advanced traffic management capabilities like Istio.\n\nUsing Istio, the traffic splitting is done correctly at the infrastructure level instead of playing with replica numbers like in the first example. Argo Rollouts integrates with Istio to execute a canary release, automatically updating the Istio VirtualService object.\n\n212\n\n|\n\nChapter 8: Advanced Topics\n\nAssuming you already know Istio and have a Kubernetes cluster with Istio installed, you can perform integration between Argo Rollouts and Istio by setting the trafficRouting from Rollout resource to Istio.\n\nFirst, create a Rollout file with Istio configured:\n\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: bgdapp labels: app: bgdapp spec: strategy: canary: steps: - setWeight: 20 - pause: duration: \"1m\" - setWeight: 50 - pause: duration: \"2m\" canaryService: bgd-canary stableService: bgd trafficRouting: istio: virtualService: name: bgd routes: - primary replicas: 1 revisionHistoryLimit: 2 selector: matchLabels: app: bgdapp version: v1 template: metadata: labels: app: bgdapp version: v1 annotations: sidecar.istio.io/inject: \"true\" spec: containers: - image: quay.io/rhdevelopers/bgd:1.0.0 name: bgd env: - name: COLOR value: \"blue\" resources: {}\n\n8.6 Use Advanced Deployment Techniques\n\n|\n\n213",
      "page_number": 218
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 228-243)",
      "start_page": 228,
      "end_page": 243,
      "detection_method": "topic_boundary",
      "content": "Canary section\n\nReference to a Kubernetes Service pointing to the new service version\n\nReference to a Kubernetes Service pointing to the old service version\n\nConfigures Istio\n\nReference to the VirtualService where weight is updated\n\nName of the VirtualService\n\nRoute name within VirtualService\n\nDeploys the Istio sidecar container\n\nThen, we create two Kubernetes Services pointing to the same deployment used to redirect traffic to the old or the new one.\n\nThe following Kubernetes Service is used in the stableService field:\n\napiVersion: v1 kind: Service metadata: name: bgd labels: app: bgdapp spec: ports: - name: http port: 8080 selector: app: bgdapp\n\nAnd the Canary one is the same but with a different name. It’s the one used in the canaryService field:\n\napiVersion: v1 kind: Service metadata: name: bgd-canary labels: app: bgdapp spec: ports: - name: http port: 8080 selector: app: bgdapp\n\n214\n\n|\n\nChapter 8: Advanced Topics\n\nFinally, create the Istio Virtual Service to be updated by Argo Rollouts to update the canary traffic for each service:\n\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bgd spec: hosts: - bgd http: - route: - destination: host: bgd weight: 100 - destination: host: bgd-canary weight: 0 name: primary\n\nStable Kubernetes Service\n\nCanary Kubernetes Service\n\nRoute name\n\nAfter applying these resources, we’ll get the first version of the application up and running:\n\nkubectl apply -f bgd-virtual-service.yaml kubectl apply -f service.yaml kubectl apply -f service-canary.yaml kubectl apply -f bgd-isio-rollout.yaml\n\nWhen any update occurs on the Rollout object, the canary release will start as described in the Solution. Now, Argo Rollouts updates the bgd virtual service weights automatically instead of playing with pod numbers.\n\nSee Also\n\nArgo Rollouts - Kubernetes Progressive Delivery Controller •\n\nIstio - Argo Rollouts •\n\n• Istio\n\n• Istio Tutorial from Red Hat\n\n8.6 Use Advanced Deployment Techniques\n\n|\n\n215\n\nA accounts\n\ncontainer registry services, creating, 7-8 GitHub, creating, 9-10\n\nadvanced deployment techniques, 209 Agile, 5 application deployment model, 4-5 Application resource file (Argo CD), 157 applications Argo CD\n\nremoving, 164 self-healing, 164-166 updating deployment files, 161\n\ncompiling, 108-113\n\nGitHub Actions, 150-153 from private repositories, 114-116 Tekton Triggers, 135-139\n\nconfiguration values, 43 container images, 17\n\ncreating with Docker, 18-23\n\ncontainerizing using Tekton Tasks, 117-120 deployment, 4-5\n\nArgo CD, 156-162 Argo Rollouts, 208-215 automatic with webhooks, 198-200 to Kubernetes, 122-125 to multiple clusters, 200-205 to multiple namespaces, 57-60 Tekton Pipelines, 125-134\n\nHelm-packaged, updating with Tekton Pipe‐\n\nline, 144-146 Java, adding Jib, 24-25 listing with Argo CD, 158 packaging, 108-113\n\nIndex\n\nGitHub Actions, 150-153 from private repositories, 114-116 Tekton Triggers, 135-139\n\nPython, Dockerfile, 19 registering, Argo CD, 158 synchronization (Argo CD), 158-159 defining time windows, 187-189\n\nApplicationSets, 191, 201-203 appVersion tag, 80 Argo CD, 155\n\nadmin account, password, 156 application deployment\n\nautomatic with webhooks, 198-200 updating files, 161\n\napplication synchronization, defining time\n\nwindows, 187-189\n\napplications\n\ndeploying, 156-162 deploying to multiple clusters, 200-205 listing, 158 registering, 158 removing, 164 self-healing, 164-166 synchronizing, 158-159, 162-166\n\nApplicationSet resource, creating, 201-203 ApplicationSets, 191 automated policy, 162 container images, updating automatically,\n\n171-178\n\ncron expressions, 188 Git repositories, registering, 179-182 Helm hooks, 171 Helm manifests, deploying, 168-171 installing, 156-157\n\n217\n\nKubernetes manifests, setting deployment\n\norder, 182-187\n\nKustomize manifests, deploying, 166-168 manifests\n\nannotations, 173 deploying, 178-182 resources, pruning, 164 Sealed Secrets, installing, 192 secrets, encrypting, 195-198 security management, GitOps workflows,\n\n191\n\nSTATUS field, 158, 161 syncPolicy, 162\n\nArgo CD Image Updater, 172\n\nConfigMap, 177 configuration options, 173 default commit message, 177 installing, 172 repository read access, 176 version constraint field, 176\n\nArgo Rollouts, 192\n\napplications, deploying, 208-215 Kubectl Plugin, 209\n\nargocd CLI tool, installing, 156 authentication Jib, 25 Tekton schemes, 114-116 automated policy, Argo CD, 162\n\nB base directory files, Kustomize, 44 build logs, checking, 39 build-kaniko.yaml file, 38 Buildah, 17\n\ncontainer images\n\nbuilding from scratch, 31 creating, 28-31\n\nDockerfile, 28\n\nbuildah containers command, 28 Buildah, OS support, 27 Buildpacks, 17\n\ncontainer images, creating, 32-35 Homebrew, 32\n\nC canary release process, 209-215 CD (continuous deployment)\n\nArgo CD, 155 Kubernetes, Tekton Tasks, 122-125\n\n218\n\n|\n\nIndex\n\nCentos, container images, 28 Chart.yaml file creating, 68 dependencies section, registering Charts,\n\n88-92\n\nCharts, 67\n\navailable, 85 default values, checking, 87 deploying, 84-87\n\nwith dependencies, 88-92 External Secrets, installing, 196 Helm repositories, installing from, 145 history command, 74 packaging/distributing, 82-83 public repositories, 84 publishing, 82 registering, 84 repositories, 82 sharing to other Charts, 74 uninstalling, 74 validating, 84\n\nCI (continuous integration), 99, 155\n\nDrone, 148\n\nCI/CD (continuous integration/continuous\n\ndelivery) GitHub Actions\n\ncompiling applications, 150-153 packaging applications, 150-153\n\nKubernetes, 3\n\nCLI tool, 15 Cloud Native Buildpacks (see Buildpacks) cloud native CI, 99 cluster decision resource generator, 203 cluster generator, 203 Cluster Resource Definitions (CRDs) (see CRDs (Cluster Resource Definitions))\n\nclusters\n\nArgo Rollouts, installing, 209 local, creating, 12-15\n\nClusterSecretStore resource, 197-198 ClusterTasks, 129 ConfigMap, 43\n\nArgo CD Image Updater, 177 generating, Kustomize, 60-66 rolling updates, Helm, 67 ConfigMapGenerator, 61-65 configuration properties\n\nhashes, 64 merging, 64\n\nrolling updates, 67\n\ncontainer engines, Docker, 17 container images\n\nBuildah, creating, 28-31 Buildpacks, 32-35 Centos, 28 consuming, 21 creating, 17\n\ncommands, 20 pushing to registry, 118-120\n\nDocker\n\ncache, verifying, 21 creating, 18-23 HTTPD, creating, 28 Jib, creating, 23-27 Kubernetes, building with Shipwright, 35-41 layers, 20, 22\n\ninternet connections, 22\n\nlist of available, 21 naming, 20 OCI, building, 27-31 plugins, adding, 24 pushing to registry, 21 references, updating, 139-141 revision number, checking, 79 running, 22 structure, 18-19 updating\n\nautomatically with Argo CD, 171-178 Helm, 79-81 Kustomize, 50-52\n\nversion tag, updating, 50-52\n\ncontainer registry\n\ncredentials, adding, 149 GitHub, 153\n\ncontainer registry services, creating accounts,\n\n7-8\n\ncontainers, 17\n\naccessing, 23 publishing, 7\n\ncontinuous deployment (CD) (see CD (contin‐\n\nuous deployment))\n\ncontinuous integration (CI) (see CI (continuous\n\nintegration))\n\ncontinuous integration/continuous delivery\n\n(CI/CD) (see CI/CD (continuous integra‐ tion/continuous delivery))\n\nCRDs (Cluster Resource Definitions), event\n\nhandling, 135\n\nCRDs (Kubernetes Custom Resources), 100 credentials\n\ncontainer registries, adding, 149 managing in external resources, 195-198\n\nCRI-O, 28 cron expressions, 188\n\nD daemonless container images, 17 daemons, Docker, 17 default service account, Tekton, 114 dependencies section (Chart.yaml), registering\n\nCharts, 88-92\n\ndeployment\n\napplications\n\nArgo CD, 156-162 Argo Rollouts, 208-215 to multiple clusters, 200-205 automatic with webhooks, 198-200 Charts with dependencies, 88-92 Kubernetes, updating with Tekton using\n\nKustomize, 139-144\n\nrolling updates, triggering automatically,\n\n93-98 updating\n\ncontainer images, 50-51 deployment files, 161 with Tekton TaskRun, 145\n\ndeployment.yaml, Helm Charts, 69 development cycle, 5-6 DevOps, 5 DevSecOps, 191 Docker, 17\n\ncontainer images creating, 18-23 verifying, 21\n\ninstalling, 18\n\ndocker images command, 25 docker pull command, 21 docker run command, 22 Dockerfiles, 17 Buildah, 28 defining, 18-20\n\nDockerHub, accounts, 7-8 dockerless container images, 17, 23\n\n(see also Jib) creating, 23-27 kaniko, 37\n\nDrone\n\nIndex\n\n|\n\n219\n\ncomponents, 148 configuring, 149 installing, 148 Kubernetes pipelines, creating, 148-150 repositories, activating, 149\n\ndrone.yaml, creating, 149 dynamic storage support, Kubernetes, 130\n\nE environments, deploying to multiple, 57-60 event handling, CRDs, 135 EventListener, 136, 137 External Secrets, managing credentials, 195-198\n\nF forks, creating repositories, 11-12\n\nG generators (ApplicationSet resource), 201-203 Git\n\nmanaging Kubernetes Secrets, 192-195 repositories registration, 9-12, 179-182\n\ngit generator, 203 git-clone, 141 git-commit, 141 GitHub\n\naccounts, creating, 9-10 container registry, 153\n\nGitHub Actions applications\n\ncompiling, 150-153 packaging, 150-153\n\nworkflow, 151\n\nGitOps\n\nbenefits, 2 development cycle, 5-6 loops, 4 principles, 2 project structure, 4 security, 191 workflow, 5 Working Group, 2\n\nGradle, 23\n\nH hashes, configuration properties, 64 Helm, 67\n\n220\n\n|\n\nIndex\n\napplications, updating with Tekton Pipeline,\n\n144-146 Charts, 67, 68\n\nchecking default values, 87 creating directories, 68 deploying, 84-87 deploying with dependencies, 88-92 finding available, 85 history command, 74 installing External Secrets, 196 installing to Kubernetes clusters, 73 packaging/distributing, 82-83 public repositories, 84 publishing, 82 registering, 84 rendering, 72 sharing to other Charts, 74 uninstalling, 74 validating, 84\n\ncontainer images, updating, 79-81 default values, overriding, 73 elements, relationships of, 71 hooks, Argo CD support, 171 installed elements, listing, 74 manifests, deploying with Argo CD,\n\n168-171\n\nprojects, creating, 68-74 scaffolding projects, 75 template statements, reusable, 75-78 values, overriding, 81 Helm Chart repositories, 84 helm create <name> command, 75 helm rollback command, 80 helm template command, 72 helm-upgrade-from-repo Task, 144 _helpers.tpl, 75-78 history command, 80 Homebrew, Buildpacks, 32 HTTPD container image, creating, 28 httpd package, installing, 28\n\nI images, creating, 17 index.yaml, updating, 82 install command, 84, 85\n\nJ Java services, deploying, 88-92 Java, Jib, 24-25\n\nJib, 17\n\nbenefits, 24 container images creating, 23-27 storing in cache, 26\n\nDockerfiles, 23 Java applications, 24-25\n\nJSON Patch, updating container image fields,\n\n52-55\n\nK kaniko, 36-39 kind, 16 kubectl\n\napply command, 46 Kustomize, 44 Tekton, installing, 101\n\nKubectl Plugin (Argo Rollouts), 209 Kubernetes, 1\n\nadvanced deployment techniques, 209 application deployment model, 4-5 applications\n\ndeploying to, 122-125 Tekton Pipelines, 125-134\n\nCI/CD (continuous integration/continuous\n\ndelivery), 3\n\nCLI tool, 15 clusters, creating locally, 12-15 ConfigMap, 43 ConfigMap, generating with Kustomize,\n\n60-66\n\ncontainer images, building with Shipwright,\n\n35-41\n\ncontainerized applications, 17 dynamic storage support, 130 ExternalSecrets, 195 fields\n\nadding with Kustomize, 54 updating with Kustomize, 52-56\n\nGitOps loops, 4 GitOps project structure, 4 Helm Charts\n\ninstalling, 73 listing installed elements, 74\n\ninstalling, 13 manifests\n\nautomatic updates, 139-144 setting deployment order in Argo CD,\n\n182-187\n\npipelines, creating with Drone, 148-150 resource files, creating, 44-45 resources, deploying with Kustomize, 44-49\n\nKubernetes Operators\n\ninstalling Tekton components, 102\n\nKubernetes Secrets, 37 creating, 115-115 Git repositories, registering, 181 managing, 192 ServiceAccount, attaching, 117 Tekton, 114\n\nkustomization.yaml, 44\n\nConfigMapGenerator, 62 container images, updating, 50 creating, 46 referencing external assets, 48-49 referencing from another kustomiza‐\n\ntion.yaml file, 47\n\nKustomize, 43\n\nbase directory files, 44 ConfigMap, generating, 60-66 container images, updating, 50-52 deployment, multiple namespaces, 57-60 Kubernetes adding fields, 54 Kubernetes manifests, automatic updates,\n\n139-144\n\nKubernetes resources, deploying, 44-49 Kubernetes, updating fields, 52-56 manifests, deploying with Argo CD,\n\n166-168\n\npreappend/append values to resources, 60 web pages, deploying, 44-47 kustomize build command, 51 kustomize command, building resources, 48-49\n\nL list generator, 203 local clusters, creating, 12-15 loops, GitOps, 4\n\nM manifests\n\nArgo CD IU annotations, 173 deploying, Argo CD, 178-182 Dockerfiles, 17 Helm, deploying with Argo CD, 168-171 Kustomize, deploying with Argo CD,\n\n166-168\n\nNode.js packages, 33\n\nIndex\n\n|\n\n221\n\nsynchronizing, 4 updating automatically, 139-144\n\nmatrix generator, 203 Maven, 23\n\ncontainer images, building, 24\n\nmerge generator, 203 Minikube\n\napplication deployment, Argo CD web‐\n\nhooks, 199\n\ncontainer/virtualization technologies, 12 installing, 13 IP, accessing, 160 platform-specific files, 13\n\nN namespace field, deploying applications, 57-60 namespaces\n\ndeploying to multiple, 57-60 tekton-pipelines, 102-103\n\nnewTag field, updating, 52 Node.js, Buildpacks container images, 33\n\nO OCI containers, 17 building, 27-31\n\nOpen Container Initiative, 17 OpenShift Pipelines, 129\n\nP pack builder inspect paketobuild‐\n\npacks/builder:base command, 33\n\npack builder suggest command, 33 package command, 82 passwords, Argo CD admin account, 156 Patch expressions, modifying Kubernetes\n\nresources, 55\n\npatch files, creating, 55 pipeline, creating for Kubernetes with Drone,\n\n148-150\n\nPipelineRun object, 128 Pipelines (Tekton), 100-101\n\napplications, deploying to Kubernetes,\n\n125-134\n\nautomating, 135-139 creating, 127 flow, 125 Helm-packaged applications, updating,\n\n144-146\n\n222\n\n|\n\nIndex\n\nmanifests, automatic updates, 139-144 Tasks, adding, 141 Tekton Tasks, 107\n\nplugins, building container images with Jib, 24 Podman, OS support, 27 PostgreSQL servers, deploying, 85-87 projects Helm\n\ncreating, 68-74 scaffolding, 75\n\nstructure, 4 .properties files, 65 Public Helm Chart repositories, 84 pull request generator, 203, 206 pull requests\n\ncreating, 207 deploying to clusters, 206-208\n\nQ Quay\n\nKubernetes Secrets, 37 logging into, 21 registration, 8\n\nR registries\n\ncontainer images, pushing to, 21 logging into, 21 repo add command, 84 repo index command, 83 repo list command, 84 repo update command, 85 repositories\n\nactivating in Drone, 149 directory layout, 82 forks, creating, 11-12 Git\n\nregistering, 9-12 registering with Argo CD, 179-182\n\nGitHub Actions, 151 Helm Charts, 82\n\ndeploying, 84-87 public, 84\n\nprivate, compiling/packaging applications\n\nwith Tekton, 114-116\n\nresource files (Kubernetes), creating, 44-45 resource hooks\n\nmanifest deployment, 183-184\n\ndeletion policies, 184\n\nsync waves, 185\n\nmodifying manifest order, 182-187\n\nresources\n\nConfigMapGenerator, 61-65 deploying with Kustomize, 44-49 pruning by Argo CD, 164 Roles, ServiceAccounts, 123 rollback command, 80 Runner (Drone), 148\n\nS scm provider generator, 203 Sealed Secrets, 192 installing, 192 Secrets, creating, 193-194\n\nsearch command, 85 secrets\n\nDrone, adding to container registries, 149 encrypting with Argo CD, 195-198\n\nSecretStore resources, 197-198 security, GitOps workflows, 191 selfHeal property (Argo CD), 165 Server (Drone), 148 service.yaml (Helm Charts), 70 ServiceAccounts creating, 118 Roles, 123 secrets, attaching, 117 Tekton, 115-116 Tekton Tasks, creating, 123 Tekton Triggers, 136\n\nservices\n\nGitHub, 9-12 GitLab, 12\n\nsha256sum template function, 93, 96-98 Shipwright, building container images, 35-41 show command, 87 Skopeo, 28 SSH, registering Git repositories, 181 statements, reusable, 75-78 STATUS field (Argo CD), 158, 161 Strategic Merge Patch, 55 Sutter, Burr, \"Teaching Elephants to Dance (and\n\nFly!)\", 6\n\nsync waves, modifying manifest order, 182-187 syncPolicy, Argo CD, 162\n\nT TaskRun objects, 108\n\ncreating, 111-113 logs, 113 Tasks (Tekton) applications\n\ncontainerizing, 117-120 deploying to Kubernetes, 122-125\n\nbuild-app, 109-111\n\ncreating, 111-111\n\ncontainer image references, updating,\n\n139-141 creating, 107-108 displaying running, 134 Helm Charts, installing from Helm reposi‐\n\ntory, 145 parameters, 112 Pipelines\n\nadding to, 141 deploying applications to Kubernetes,\n\n125-134\n\nresults as input for succeeding Tasks, 143 ServiceAccounts, creating, 118, 123 Tekton Hub, 128 workspaces, persisting, 130\n\n\"Teaching Elephants to Dance (and Fly!)\", 6 Tekton, 36, 99-100 applications\n\ncompiling, 108-113 compiling from private repositories,\n\n114-116\n\ndeploying to Kubernetes, 122-125 packaging, 108-113 packaging from private repositories,\n\n114-116\n\nauthentication schemes, 114-116 configuration, verifying, 106 CRDs (Kubernetes Custom Resources), 100 default service account, 114 fields, 109-109 installing, 100-107 Kubernetes manifests, automatic updates,\n\n139-144\n\nKubernetes Secrets, 114 creating, 115-115\n\nmodules, 101 Pipelines, 100-101 creating, 127 deploying applications to Kubernetes,\n\n125-134\n\nflow, 125\n\nIndex\n\n|\n\n223\n\nupdating Helm-packaged applications,\n\n144-146 pods, 103-105 ServiceAccounts, 115-116 Tasks\n\nbuild-app, 109-111 containerizing applications, 117-120 creating, 107-108, 111-111 creating ServiceAccounts, 123 parameters, 112 Tekton Dashboard component\n\naccessing, 106 displaying running Tasks, 134 installing, 105-105\n\nTekton Hub, 128\n\nTasks, Helm support, 144 Tekton Pipelines component, 102 Tekton Triggers component\n\napplications\n\ncompiling automatically, 135-139 packaging automatically, 135-139\n\ninstalling, 103-105 ServiceAccounts, 136\n\ntemplate command, 81 template statements, reusable, 75-78 TLS client certificate, configuring, 180 TriggerBinding, 135 TriggerTemplate, 135\n\n224\n\n|\n\nIndex\n\nU update-digest, 141 upgrade command, 79-80\n\nV values.yaml (Helm Charts), 70 VirtualBox, installing, 12 virtualization systems, 12\n\nW web pages, deploying with Kustomize, 44-47 webhooks, 135\n\napplication deployment, 198-200 parameters, 138\n\nwelcome page, copying to container, 29 workflow, 5\n\ndevelopment cycle, 5\n\nWorking Group, 2 workspaces, persisting, 130\n\nY YAML, 43\n\nkustomization.yaml, 44 Patch expressions, 55\n\nAbout the Authors\n\nNatale Vinto is a software engineer with more than 10 years of expertise in IT and ICT technologies and a consolidated background in Telecommunications and Linux operating systems. As a solution architect with a Java development background, he spent some years as an EMEA Specialist Solution Architect for OpenShift at Red Hat. He is coauthor of Modernizing Enterprise Java for O’Reilly. Today Natale is lead developer advocate at Red Hat, helping people within communities and customers have success with their Kubernetes and cloud native strategy. You can follow more frequent updates on his Twitter feed and connect with him on LinkedIn.\n\nAlex Soto Bueno is a director of developer experience at Red Hat. He is passionate about the Java world, software automation, and he believes in the open source software model. Alex is the coauthor of Testing Java Microservices (Manning), Quar‐ kus Cookbook (O’Reilly), and the forthcoming Kubernetes Secrets Management (Man‐ ning), and is a contributor to several open source projects. A Java Champion since 2017, he is also an international speaker and teacher at Salle URL University. You can follow more frequent updates on his Twitter feed and connect with him on LinkedIn.\n\nColophon\n\nThe animal on the cover of GitOps Cookbook is a yellow mongoose (Cynictis penicil‐ lata). These small mammals are found in sub-Saharan Africa, primarily in forests, woodlands, grasslands, and scrub. They are sometimes referred to as red meerkats. Yellow mongoose are smaller than most other species, weighing only 16–29 ounces. There are 12 subspecies that vary in color, body size (9–13 inches), tail (7–10 inches), and length of coat: the northern subspecies found in Botswana are typically smaller with grizzled grayish coats while the southern populations in South Africa and Namibia are larger and tawny yellow. All subspecies have slender bodies with lighter fur on the chin and underbelly, small ears, pointed noses, and bushy tails.\n\nYellow mongoose are carnivores that mainly feed on insects, birds, frogs, lizards, eggs and small rodents. They are social species and live in colonies of up to 20 individuals in extensive, permanent burrows with many entrances, chambers, and tunnels. Most of their day is spent foraging or sunbathing outside the burrow. In the wild, they breed from July to September with most females giving birth to two or three offspring in October and November. The young are born in an underground chamber and stay there until they are weaned (about 10 weeks). Yellow mongoose are considered fully grown at 10 months old.\n\nYellow mongoose are classified as a species of least concern by the IUCN; their populations are stable and they don’t face any major threats. They do carry of strain of rabies in the wild and are seen as pests and hunted by farmers in parts of South Africa. Many of the animals on O’Reilly covers are endangered; all of them are important to the world.\n\nThe cover illustration is by Karen Montgomery, based on an antique line engraving from The Pictorial Museum of Animated Nature. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "page_number": 228
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 244-245)",
      "start_page": 244,
      "end_page": 245,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 244
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "GitOps Cookbook\n\nKubernetes Automation in Practice\n\nCompliments of\n\nV\n\ni\n\nn\n\nS\n\nt\n\no\n\no\n\nt\n\n&\n\no\n\nNatale Vinto & Alex Soto Bueno",
      "content_length": 128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "GitOps Cookbook Why are so many companies adopting GitOps for their DevOps and cloud native strategy? This reliable framework is quickly becoming the standard method for deploying apps to Kubernetes. With this practical, developer-oriented book, DevOps engineers, developers, IT architects, and SREs will learn the most useful recipes and examples for following GitOps practices.\n\nThrough their years of experience in application modernization, CI/CD, and automation, authors Alex Soto Bueno and Natale Vinto from Red Hat take you through all the steps necessary for successful hands-on application development and deployment with GitOps. Once you start using the recipes in this book, you’ll have a head start in development cycles on Kubernetes following the GitOps approach.\n\nYou’ll learn how to:\n\nDevelop and deploy applications on Kubernetes\n\nUnderstand the basics of CI/CD and automation on Kubernetes and apply GitOps practices to implement development cycles on the platform\n\nPrepare the app for deployment in multiple environments or multiple Kubernetes clusters\n\nDeploy apps for Kubernetes clusters or for multiple environments using GitOps and Argo CD\n\nCreate Kubernetes-native pipelines with Tekton\n\nProvide and extend DevOps skills for the team working on Kubernetes\n\nGITOPS / KUBERNETES\n\nUS $79.99 ISBN: 978-1-098-13517-1 ISBN: 978-1-492-109747-1\n\nCAN $99.99\n\n“For any IT professional, it can be challenging to stay on top of the newest technologies and best practices in the ever-changing space of software delivery. In this book, Alex and Natale share practical hands-on examples from working across many different organizations on implementing GitOps and CI/CD in a cloud native environment. Pick your favorite recipe and get cooking!”\n\n—Sasha Rosenbaum Principal at Ergonautic\n\nNatale Vinto is a developer advocate at Red Hat, helping customers with their Kubernetes and cloud native strategy.\n\nAlex Soto Bueno is director of developer experience at Red Hat and coauthor of Quarkus Cookbook.\n\nTwitter: @oreillymedia linkedin.com/company/oreilly-media youtube.com/oreillymedia\n\nS\n\no\n\nt\n\no\n\nV\n\ni\n\nn\n\nt\n\no\n\n&",
      "content_length": 2123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Launch your Developer Sandbox for Red Hat OpenShift today\n\nred.ht/sandb0x",
      "content_length": 73,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "GitOps Cookbook Kubernetes Automation in Practice\n\nNatale Vinto and Alex Soto Bueno",
      "content_length": 83,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "GitOps Cookbook by Natale Vinto and Alex Soto Bueno\n\nCopyright © 2023 Natale Vinto and Alex Soto Bueno. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nAcquisitions Editor: John Devins Development Editor: Shira Evans Production Editor: Kate Galloway Copyeditor: Kim Cofer Proofreader: Liz Wheeler\n\nIndexer: nSight, Inc. Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Kate Dullea\n\nJanuary 2023:\n\nFirst Edition\n\nRevision History for the First Edition 2023-01-03: First Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492097471 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. GitOps Cookbook, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views. While the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\nThis work is part of a collaboration between O’Reilly and Red Hat. See our statement of editorial independence.\n\n978-1-098-14809-6\n\n[LSI]",
      "content_length": 2057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "To Alessia and Sofia, the most beautiful chapters of my life. —Natale\n\n[Ada i Alexandra] Sabeu que sou flipants, encara que sortiu del fang. —Alex",
      "content_length": 146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Table of Contents\n\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi\n\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\n\n1.\n\nIntroduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1 What Is GitOps? 1 1.2 Why GitOps? 2 1.3 Kubernetes CI/CD 3 1.4 App Deployment with GitOps on Kubernetes 4 1.5 DevOps and Agility 5\n\n2. Requirements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1 Registering for a Container Registry 7 2.2 Registering for a Git Repository 9 2.3 Creating a Local Kubernetes Cluster 12\n\n3. Containers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.1 Building a Container Using Docker 18 3.2 Building a Container Using Dockerless Jib 23 3.3 Building a Container Using Buildah 27 3.4 Building a Container with Buildpacks 32 3.5 Building a Container Using Shipwright and kaniko in Kubernetes 35 3.6 Final Thoughts 42\n\n4. Kustomize. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.1 Using Kustomize to Deploy Kubernetes Resources 44 4.2 Updating the Container Image in Kustomize 50\n\nvii",
      "content_length": 1538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "4.3 Updating Any Kubernetes Field in Kustomize 52 4.4 Deploying to Multiple Environments 57 4.5 Generating ConfigMaps in Kustomize 60 4.6 Final Thoughts 66\n\n5. Helm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 5.1 Creating a Helm Project 68 5.2 Reusing Statements Between Templates 75 5.3 Updating a Container Image in Helm 79 5.4 Packaging and Distributing a Helm Chart 82 5.5 Deploying a Chart from a Repository 84 5.6 Deploying a Chart with a Dependency 88 5.7 Triggering a Rolling Update Automatically 93 5.8 Final Thoughts 98\n\n6. Cloud Native CI/CD. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 6.1 Install Tekton 100 6.2 Create a Hello World Task 107 6.3 Create a Task to Compile and Package an App from Git 108 6.4 Create a Task to Compile and Package an App from Private Git 114 6.5 Containerize an Application Using a Tekton Task and Buildah 117 6.6 Deploy an Application to Kubernetes Using a Tekton Task 122 6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes 125 6.8 Using Tekton Triggers to Compile and Package an Application Git 139 6.10 Update a Kubernetes Resource Using Helm and Create a Pull Request 144 6.11 Use Drone to Create a Pipeline for Kubernetes 148 6.12 Use GitHub Actions for CI 150\n\n7. Argo CD. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 7.1 Deploy an Application Using Argo CD 156 7.2 Automatic Synchronization 162 7.3 Kustomize Integration 166 7.4 Helm Integration 168 7.5 Image Updater 171 7.6 Deploy from a Private Git Repository 178 7.7 Order Kubernetes Manifests 182 7.8 Define Synchronization Windows 187\n\nviii\n\n|\n\nTable of Contents",
      "content_length": 1845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "8. Advanced Topics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 8.1 Encrypt Sensitive Data (Sealed Secrets) 192 8.2 Encrypt Secrets with ArgoCD (ArgoCD + HashiCorp Vault + External\n\nSecret) 195\n\n8.3 Trigger the Deployment of an Application Automatically (Argo CD\n\nWebhooks) 198 8.4 Deploy to Multiple Clusters 200 8.5 Deploy a Pull Request to a Cluster 206 8.6 Use Advanced Deployment Techniques 208\n\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n\nTable of Contents\n\n|\n\nix",
      "content_length": 651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Foreword\n\nA few years ago, during a trip to Milan for a Red Hat event, I ran into a passionate colleague at the Red Hat office. We spoke at length about how customers in Italy adopt containers to speed up application development on OpenShift. While his name slipped my mind at the time, his enthusiasm about the subject didn’t, especially since he was also hospitable enough to take me to an espresso bar near the office to show me what real coffee tastes like. A while later, I was introduced to a developer advocate in a meeting who would speak at a conference about CI/CD using products like OpenShift Pipelines and OpenShift GitOps that my teams delivered at the time. At that moment, I instantly recognized Natale. Many who attended that talk thought it was insightful, given his firsthand grasp of challenges that customers experience when delivering applications and his hands-on approach to technology.\n\nApplication delivery is a complex process involving many systems and teams with numerous handoffs between these parties, often synonymous with delays and back- and-forth talks at each point. Automation has long been a key enabler for improving this process and has become particularly popular within the DevOps movement. Continuous integration, infrastructure as code, and numerous other practices became common in many organizations as they navigated their journey toward adopting DevOps.\n\nMore recently, and coinciding with the increased adoption of Kubernetes, GitOps as a blueprint for implementing a subset of DevOps practices has become an area I fre‐ quently get asked about. While neither the term nor the practices GitOps advocates are new, it does combine. It presents the existing knowledge in a workflow that is simple, easy to understand, and can be implemented in a standard way across many teams.\n\nxi",
      "content_length": 1827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Although the path to adopting the GitOps workflow is simple and concrete, many technical choices need to be made to fit within each organization’s security, compli‐ ance, operational, and other requirements. Therefore, I am particularly thrilled about the existence of this book and the practical guides it provides to assist these teams in making choices that are right for their applications, teams, and organizations.\n\n—Siamak Sadeghianfar Product Management, Red Hat\n\nxii\n\n|\n\nForeword",
      "content_length": 488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Preface\n\nWe wrote this book for builders. Whether you are a developer, DevOps engineer, site reliability engineer (SRE), or platform engineer dealing with Kubernetes, you are building some good stuff. We would like to share our experience from what we have learned in the field and in the community about the latest Kubernetes automation insights for pipelines and CI/CD workloads. The book contains a comprehensive list of the most popular available software and tools in the Kubernetes and cloud native ecosystem for this purpose. We aim to provide a list of practical recipes that might help your daily job or are worth exploring further. We are not sticking to a particular technology or project for implementing Kubernetes automation. However, we are opinionated on some of our choices to deliver a concise GitOps pathway.\n\nThe book is organized in sequential chapters, from the basics to advanced topics in the Kubernetes ecosystem, following the GitOps principles. We hope you’ll find these recipes valuable and inspiring for your projects!\n\n• Chapter 1 is an introduction to GitOps principles and why they are continuously becoming more common and essential for any new IT project.\n\nChapter 2 covers the installation requirements to run these recipes in a Kuber‐ • netes cluster. Concepts and tools like Git, Container Registry, Container Run‐ time, and Kubernetes are necessary for this journey.\n\nChapter 3 walks you through a complete overview of containers and why they • are essential for application development and deployment today. Kubernetes is a container-orchestration platform; however, it doesn’t build containers out of the box. Therefore, we’ll provide a list of practical recipes for making container apps with the most popular tools available in the cloud native community.\n\nChapter 4 gives you an overview of Kustomize, a popular tool for managing • Kubernetes resources. Kustomize is interoperable, and you find it often used within CI/CD pipelines.\n\nxiii",
      "content_length": 1981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Chapter 5 explores Helm, a trendy tool to package applications in Kubernetes. • Helm is also a templating system that you can use to deploy apps in CI/CD workloads.\n\n• Chapter 6 walks you through cloud native CI/CD systems for Kubernetes. It gives a comprehensive list of recipes for the continuous integration part with Tekton, the Kubernetes-native CI/CD system. Additionally, it also covers other tools such as Drone and GitHub Actions.\n\n• Chapter 7 kicks off the pure GitOps part of the book as it sticks to the Continu‐ ous Deployment phase with Argo CD, a popular GitOps tool for Kubernetes.\n\nChapter 8 goes into the advanced topics for GitOps with Argo CD, such as • secrets management, progressive application delivery, and multicluster deploy‐ ments. This concludes the most common use cases and architectures you will likely work with today and tomorrow following the GitOps approach.\n\nConventions Used in This Book The following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nConstant width italic\n\nShows text that should be replaced with user-supplied values or by values deter‐ mined by context.\n\nThis element signifies a tip or suggestion.\n\nThis element signifies a general note.\n\nxiv\n\n| Preface",
      "content_length": 1602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "This element indicates a warning or caution.\n\nUsing Code Examples Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/gitops-cookbook.\n\nIf you have a technical question or a problem using the code examples, please send email to bookquestions@oreilly.com.\n\nThis book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require permission.\n\nWe appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “GitOps Cookbook by Natale Vinto and Alex Soto Bueno (O’Reilly). Copyright 2023 Natale Vinto and Alex Soto Bueno, 978-1-492-09747-1.”\n\nIf you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.\n\nO’Reilly Online Learning\n\nFor more than 40 years, O’Reilly Media has provided technol‐ ogy and business training, knowledge, and insight to help companies succeed.\n\nOur unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\n\nPreface\n\n|\n\nxv",
      "content_length": 1994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "How to Contact Us Please address comments and questions concerning this book to the publisher:\n\nO’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax)\n\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/gitops-cookbook.\n\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\n\nFor news and information about our books and courses, visit https://oreilly.com.\n\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media.\n\nFollow us on Twitter: https://twitter.com/oreillymedia.\n\nWatch us on YouTube: https://youtube.com/oreillymedia.\n\nAcknowledgments We both want to thank our tech reviewers Peter Miron and Andy Block for their accurate review that helped us improve the reading experience with this book. Thanks also to the people at O’Reilly who helped us during the whole writing cycle. Many thanks to our colleagues Aubrey Muhlac and Colleen Lobner for the great support with publishing this book. Thanks to Kamesh Sampath and all the people who helped us during the early release phases with comments and suggestions that we added to the book—your input is much appreciated!\n\nAlex Soto During these challenging times, I’d like to acknowledge Santa (aquest any sí), Uri (don’t stop the music), Guiri (un ciclista), Gavina, Gabi (thanks for the support), and Edgar and Ester (life is good especially on Friday); my friends Edson, Sebi (the best fellow traveler), Burr (I learned a lot from you), Kamesh, and all the Red Hat developers team, we are the best.\n\nJonathan Vila, Abel Salgado, and Jordi Sola for the fantastic conversations about Java and Kubernetes.\n\nxvi\n\n| Preface",
      "content_length": 1829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Last but certainly not least, I’d like to acknowledge Anna for being here; my parents Mili and Ramon for buying my first computer; my daughters Ada and Alexandra, “sou les ninetes dels meus ulls.”\n\nNatale Vinto Special thanks to Alessia for the patience and motivation that helped me while writ‐ ing this book. And to my parents for everything they made for me, grazie mamma e papà, you are the best!\n\nPreface\n\n|\n\nxvii",
      "content_length": 418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "CHAPTER 1 Introduction\n\nWith the advent of practices such as infrastructure as code (IaC), software develop‐ ment has pushed the boundaries of platforms where you can run applications. This becomes more frequent with programmable, API-driven platforms such as public clouds and open source infrastructure solutions. While some years ago developers were only focusing on application source code, today they also have the opportunity to code the infrastructure where their application will run. This gives control and enables automation, which significantly reduces lead time.\n\nA good example is with Kubernetes, a popular open source container workload orchestration platform and the de facto standard for running production applica‐ tions, either on public or private clouds. The openness and extensibility of the platform enables automation, which reduces risks of delivery and increases service quality. Furthermore, this powerful paradigm is extended by another increasingly popular approach called GitOps.\n\n1.1 What Is GitOps? GitOps is a methodology and practice that uses Git repositories as a single source of truth to deliver infrastructure as code. It takes the pillars and approaches from DevOps culture and provides a framework to start realizing the results. The relation‐ ship between DevOps and GitOps is close, as GitOps has become the popular choice to implement and enhance DevOps, platform engineering, and SRE.\n\nGitOps is an agnostic approach, and a GitOps framework can be built with tools such as Git, Kubernetes, and CI/CD solutions. The three main pillars of GitOps are:\n\n• Git is the single source of truth\n\nTreat everything as code •\n\n1",
      "content_length": 1661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "• Operations are performed through Git workflows\n\nThere is an active community around GitOps, and the GitOps Working Group defines a set of GitOps Principles (currently in version 1.0.0) available at OpenGitOps:\n\nDeclarative\n\nA system managed by GitOps must have its desired state expressed declaratively.\n\nVersioned and immutable\n\nThe desired state is stored in a way that enforces immutability and versioning and retains a complete version history.\n\nPulled automatically\n\nSoftware agents automatically pull the desired state declarations from the source.\n\nContinuously reconciled\n\nSoftware agents continuously observe the actual system state and attempt to apply the desired state.\n\n1.2 Why GitOps? Using the common Git-based workflows that developers are familiar with, GitOps expands upon existing processes from application development to deployment, app lifecycle management, and infrastructure configuration.\n\nEvery change throughout the application lifecycle is traced in the Git repository and is auditable. This approach is beneficial for both developers and operations teams as it enhances the ability to trace and reproduce issues quickly, improving overall security. One key point is to reduce the risk of unwanted changes (drift) and correct them before they go into production.\n\nHere is a summary of the benefits of the GitOps adoption in four key aspects:\n\nStandard workflow\n\nUse familiar tools and Git workflows from application development teams\n\nEnhanced security\n\nReview changes beforehand, detect configuration drifts, and take action\n\nVisibility and audit\n\nCapture and trace any change to clusters through Git history\n\nMulticluster consistency\n\nReliably and consistently configure multiple environments and multiple Kuber‐ netes clusters and deployment\n\n2\n\n|\n\nChapter 1: Introduction",
      "content_length": 1805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "1.3 Kubernetes CI/CD Continuous integration (CI) and continuous delivery (CD) are methods used to fre‐ quently deliver apps by introducing automation into the stages of app development. CI/CD pipelines are one of the most common use cases for GitOps.\n\nIn a typical CI/CD pipeline, submitted code checks the CI process while the CD process checks and applies requirements for things like security, infrastructure as code, or any other boundaries set for the application framework. All code changes are tracked, making updates easy while also providing version control should a rollback be needed. CD is the GitOps domain and it works together with the CI part to deploy apps in multiple environments, as you can see in Figure 1-1.\n\nFigure 1-1. Continuous integration and continuous delivery\n\nWith Kubernetes, it’s easy to implement an in-cluster CI/CD pipeline. You can have CI software create the container image representing your application and store it in a container image registry. Afterward, a Git workflow such as a pull request can change the Kubernetes manifests illustrating the deployment of your apps and start a CD sync loop, as shown in Figure 1-2.\n\nFigure 1-2. Application deployment model\n\nThis cookbook will show practical recipes for implementing this model on Kuber‐ netes acting as a CI/CD and GitOps platform.\n\n1.3 Kubernetes CI/CD\n\n|\n\n3",
      "content_length": 1358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "1.4 App Deployment with GitOps on Kubernetes As GitOps is an agnostic, platform-independent approach, the application deploy‐ ment model on Kubernetes can be either in-cluster or multicluster. An external GitOps tool can use Kubernetes just as a target platform for deploying apps. At the same time, in-cluster approaches run a GitOps engine inside Kubernetes to deploy apps and sync manifests in one or more Kubernetes clusters.\n\nThe GitOps engine takes care of the CD part of the CI/CD pipeline and accomplishes a GitOps loop, which is composed of four main actions as shown in Figure 1-3:\n\nDeploy\n\nDeploy the manifests from Git.\n\nMonitor\n\nMonitor either the Git repo or the cluster state.\n\nDetect drift\n\nDetect any change from what is described in Git and what is present in the cluster.\n\nTake action\n\nPerform an action that reflects what is on Git (rollback or three-way diff). Git is the source of truth, and any change is performed via a Git workflow.\n\nFigure 1-3. GitOps loop\n\nIn Kubernetes, application deployment using the GitOps approach makes use of at least two Git repositories: one for the app source code, and one for the Kubernetes manifests describing the app’s deployment (Deployment, Service, etc.).\n\nFigure 1-4 illustrates the structure of a GitOps project on Kubernetes.\n\n4\n\n|\n\nChapter 1: Introduction",
      "content_length": 1322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Figure 1-4. Kubernetes GitOps loop\n\nThe following list outlines the items in the workflow:\n\n1. 1. App source code repository\n\n2. 2. CI pipeline creating a container image\n\n3. 3. Container image registry\n\n4. 4. Kubernetes manifests repository\n\n5. 5. GitOps engine syncing manifests to one or more clusters and detecting drifts\n\n1.5 DevOps and Agility GitOps is a developer-centric approach to continuous delivery and infrastructure operations, and a developer workflow through Git for automating processes. As DevOps is complementary to Agile software development, GitOps is complementary to DevOps for infrastructure automation and application lifecycle management. As you can see in Figure 1-5, it’s a developer workflow for automating operations.\n\nOne of the most critical aspects of the Agile methodology is to reduce the lead time, which is described more abstractly as the time elapsed between identifying a requirement and its fulfillment.\n\n1.5 DevOps and Agility\n\n|\n\n5",
      "content_length": 975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Figure 1-5. GitOps development cycle\n\nReducing this time is fundamental and requires a cultural change in IT organizations. Seeing applications live provides developers with a feedback loop to redesign and improve their code and make their projects thrive. Similarly to DevOps, GitOps also requires a cultural adoption in business processes. Every operation, such as applica‐ tion deployment or infrastructure change, is only possible through Git workflows. And sometimes, this means a cultural shift.\n\nThe “Teaching Elephants to Dance (and Fly!)” speech from Burr Sutter gives a clear idea of the context. The elephant is where your organization is today. There are phases of change between traditional and modern environments powered by GitOps tools. Some organizations have the luxury of starting from scratch, but for many businesses, the challenge is teaching their lumbering elephant to dance like a graceful ballerina.\n\n6\n\n|\n\nChapter 1: Introduction",
      "content_length": 956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "CHAPTER 2 Requirements\n\nThis book is about GitOps and Kubernetes, and as such, you’ll need a container registry to publish the containers built throughout the book (see Recipe 2.1).\n\nAlso, a Git service is required to implement GitOps methodologies; you’ll learn how to register to public Git services like GitHub or GitLab (see Recipe 2.2).\n\nFinally, it would be best to have a Kubernetes cluster to run the book examples. Although we’ll show you how to install Minikube as a Kubernetes cluster (see Recipe 2.3), and the book is tested with Minikube, any Kubernetes installation should work as well.\n\nLet’s prepare your laptop to execute the recipes provided in this book.\n\n2.1 Registering for a Container Registry\n\nProblem You want to create an account for a container registry service so you can store generated containers.\n\nSolution You may need to publish some containers into a public container registry as you work through this book. Use Docker Hub (docker.io) to publish containers.\n\nIf you already have an account with docker.io, you can skip the following steps. Otherwise, keep reading to learn how to sign up for an account.\n\n7",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Discussion Visit DockerHub to sign up for an account. The page should be similar to Figure 2-1.\n\nFigure 2-1. DockerHub registration page\n\nWhen the page is loaded, fill in the form by setting a Docker ID, Email, and Pass‐ word, and click the Sign Up button.\n\nWhen you are registered and your account confirmed, you’ll be ready to publish containers under the previous step’s Docker ID.\n\nSee Also Another popular container registry service is quay.io. It can be used on the cloud (like docker.io) or installed on-premises.\n\nVisit the website to get more information about Quay. The page should be similar to Figure 2-2.\n\n8\n\n|\n\nChapter 2: Requirements",
      "content_length": 648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Figure 2-2. Quay registration page\n\n2.2 Registering for a Git Repository\n\nProblem You want to create an account for a Git service so you can store source code in a repository.\n\nSolution You may need to publish some source code into a public Git service in this book. Use GitHub as a Git service to create and fork Git repositories.\n\nIf you already have an account with GitHub, you can skip the following steps, otherwise keep reading to learn how to sign up for an account.\n\nDiscussion Visit the GitHub web page to sign up for an account. The page should be similar to Figure 2-3.\n\n2.2 Registering for a Git Repository\n\n|\n\n9",
      "content_length": 624,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Figure 2-3. GitHub welcome page to register\n\nWhen the page is loaded, click the Sign up for GitHub button (see Figure 2-3) and follow the instructions. The Sign in page should be similar to Figure 2-4.\n\nFigure 2-4. Sign In GitHub page\n\nWhen you are registered and your account confirmed, you’ll be ready to start creating or forking Git repositories into your GitHub account.\n\n10\n\n|\n\nChapter 2: Requirements",
      "content_length": 407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Also, you’ll need to fork the book source code repository into your account. Click the Fork button shown in Figure 2-5.\n\nFigure 2-5. Fork button\n\nThen select your account in the Owner section, if not selected yet, and click the button “Create fork” button as shown in Figure 2-6.\n\nFigure 2-6. Create fork button\n\n2.2 Registering for a Git Repository\n\n|\n\n11",
      "content_length": 356,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "To follow along with the example in the following chapters, you can clone this book’s repositories locally. When not mentioned explicitly, we will refer to the examples available in the chapters repo:\n\ngit clone https://github.com/gitops-cookbook/chapters\n\nSee Also Another popular Git service is GitLab. It can be used on the cloud or installed on-premises.\n\nVisit GitLab for more information.\n\n2.3 Creating a Local Kubernetes Cluster\n\nProblem You want to spin up a Kubernetes cluster locally.\n\nSolution In this book, you may need a Kubernetes cluster to run most recipes. Use Minikube to spin up a Kubernetes cluster in your local machine.\n\nDiscussion Minikube uses container/virtualization technology like Docker, Podman, Hyperkit, Hyper-V, KVM, or VirtualBox to boot up a Linux machine with a Kubernetes cluster installed inside.\n\nFor simplicity and to use an installation that will work in most of the platforms, we are going to use VirtualBox as a virtualization system.\n\nTo install VirtualBox (if you haven’t done it yet), visit the home page and click the Download link as shown in Figure 2-7.\n\nFor those using macOS, the following instructions have been tested on a Mac AMD64 with macOS Monterey and VirtualBox 6.1. At the time of writing this book, there were some incompatibilities when using the ARM version or macOS Ventura.\n\n12\n\n|\n\nChapter 2: Requirements",
      "content_length": 1369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Figure 2-7. VirtualBox home page\n\nSelect the package based on the operating system, download it, and install it on your computer. After installing VirtualBox (we used the 6.1.x version), the next step is to download and spin up a cluster using Minikube.\n\nVisit the GitHub repo, unfold the Assets section, and download the Minikube file that matches your platform specification. For example, in the case of an AMD Mac, you should select minikube-darwin-amd64 as shown in Figure 2-8.\n\nUncompress the file (if necessary) and copy it with the name minikube in a directory accessible by the PATH environment variable such as (/usr/local/bin) in Linux or macOS.\n\nWith VirtualBox and Minikube installed, we can spin up a Kubernetes cluster in the local machine. Let’s install Kubernetes version 1.23.0 as it was the latest version at the time of writing (although any other previous versions can be used as well).\n\n2.3 Creating a Local Kubernetes Cluster\n\n|\n\n13",
      "content_length": 954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Figure 2-8. Minikube release page\n\nRun the following command in a terminal window to spin up the Kubernetes cluster with 8 GB of memory assigned:\n\nminikube start --kubernetes-version='v1.23.0' / --driver='virtualbox' --memory=8196 -p gitops\n\nCreates a Kubernetes cluster with version 1.23.0\n\nUses VirtualBox as virtualization tool\n\nCreates a profile name (gitops) to the cluster to refer to it later\n\n14\n\n|\n\nChapter 2: Requirements",
      "content_length": 431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "The output lines should be similar to:\n\n[gitops] Minikube v1.24.0 on Darwin 12.0.1 Using the virtualbox driver based on user configuration Starting control plane node gitops in cluster gitops Creating virtualbox VM (CPUs=2, Memory=8196MB, Disk=20000MB) ... > kubeadm.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s > kubelet.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s > kubectl.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s > kubeadm: 43.11 MiB / 43.11 MiB [---------------] 100.00% 3.46 MiB p/s 13s > kubectl: 44.42 MiB / 44.42 MiB [---------------] 100.00% 3.60 MiB p/s 13s > kubelet: 118.73 MiB / 118.73 MiB [-------------] 100.00% 6.32 MiB p/s 19s\n\n▪ Generating certificates and keys ... ▪ Booting up control plane ... ▪ Configuring RBAC rules ... ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5 ...\n\nVerifying Kubernetes components... Enabled addons: storage-provisioner, default-storageclass\n\n/usr/local/bin/kubectl is version 1.21.0, which may have incompatibilites with Kubernetes 1.23.0. ▪ Want kubectl v1.23.0? Try 'minikube kubectl -- get pods -A' Done! kubectl is now configured to use \"gitops\" cluster and \"default\" namespace by default\n\nStarts the gitops cluster\n\nBoots up the Kubernetes cluster control plane\n\nDetects that we have an old kubectl tool\n\nCluster is up and running\n\nTo align the Kubernetes cluster and Kubernetes CLI tool version, you can download the kubectl 1.23.0 version running from https://dl.k8s.io/release/v1.23.0/bin/darwin/ amd64/kubectl.\n\nYou need to change darwin/amd64 to your specific architecture. For example, in Windows it might be windows/amd64/kubectl.exe.\n\nCopy the kubectl CLI tool in a directory accessible by the PATH environment variable such as (/usr/local/bin) in Linux or macOS.\n\n2.3 Creating a Local Kubernetes Cluster\n\n|\n\n15",
      "content_length": 1851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "See Also There are other ways to run Kubernetes in a local machine.\n\nOne that is very popular is kind.\n\nAlthough the examples in this book should work in any Kubernetes implementation as only standard resources are used, we’ve only tested with Minikube.\n\n16\n\n|\n\nChapter 2: Requirements",
      "content_length": 285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "CHAPTER 3 Containers\n\nContainers are a popular and standard format for packaging applications. The format is an open standard promoted by the Open Container Initiative (OCI), an open gov‐ ernance structure for the express purpose of creating open industry standards around container formats and runtimes. The openness of this format ensures portability and interoperability across different operating systems, vendors, platforms, or clouds. Kubernetes runs containerized apps, so before going into the GitOps approach to managing apps on Kubernetes, we provide a list of recipes useful for understanding how to package your application as a container image.\n\nThe first step for creating images is to use a container engine for packaging your application by building a layered structure containing a base OS and additional layers on top such as runtimes, libraries, and applications. Docker is a widespread open source implementation of a container engine and runtime, and it can generate a container image by specifying a manifest called a Dockerfile (see Recipe 3.1).\n\nSince the format is open, it’s possible to create container images with other tools. Docker, a popular container engine, requires the installation and the execution of a daemon that can handle all the operations with the container engine. Developers can use a software development kit (SDK) to interact with the Docker daemon or use dockerless solutions such as JiB to create container images (see Recipe 3.2).\n\nIf you don’t want to rely on a specific programming language or SDK to build container images, you can use another daemonless solution like Buildah (see Recipe 3.3) or Buildpacks (see Recipe 3.4). Those are other popular open source tools for building OCI container images. By avoiding dependencies from the OS, such tools make automation more manageable and portable (see Chapter 6).\n\n17",
      "content_length": 1870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Kubernetes doesn’t provide a native mechanism for building container images. How‐ ever, its highly extensible architecture allows interoperability with external tools and the platform’s extensibility to create container images. Shipwright is an open source framework for building container images on Kubernetes, providing an abstraction that can use tools such as kaniko, Buildpacks, or Buildah (see Recipe 3.5) to create container images.\n\nAt the end of this chapter, you’ll learn how to create OCI-compliant container images from a Dockerfile, either from a host with Docker installed, or using tools such as Buildah and Buildpacks.\n\n3.1 Building a Container Using Docker\n\nProblem You want to create a container image for your application with Docker.\n\nSolution The first thing you need to do is install Docker.\n\nDocker is available for Mac, Windows, and Linux. Download the installer for your operating system and refer to the documentation to start the Docker service.\n\nDevelopers can create a container image by defining a Dockerfile. The best definition for a Dockerfile comes from the Docker documentation itself: “A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.”\n\nContainer images present a layered structure, as you can see in Figure 3-1. Each container image provides the foundation layer for a container, and any update is just an additional layer that can be committed on the foundation.\n\n18\n\n|\n\nChapter 3: Containers",
      "content_length": 1504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Figure 3-1. Container image layers\n\nYou can create a Dockerfile like the one shown here, which will generate a container image for Python apps. You can also find this example in this book’s repository.\n\nFROM registry.access.redhat.com/ubi8/python-39 ENV PORT 8080 EXPOSE 8080 WORKDIR /usr/src/app\n\nCOPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nENTRYPOINT [\"python\"] CMD [\"app.py\"]\n\nFROM: always start from a base image as a foundational layer. In this case we start from a Universal Base Image (UBI), publicly available based on RHEL 8 with Python 3.9 runtime.\n\nENV: set an environment variable for the app.\n\nEXPOSE: expose a port to the container network, in this case port TCP 8080.\n\nWORKDIR: set a directory inside the container to work with.\n\nCOPY: copy the assets from the source code files on your workstation to the container image layer, in this case, to the WORKDIR.\n\nRUN: run a command inside the container, using the tools already available within the base image. In this case, it runs the pip tool to install dependencies.\n\nENTRYPOINT: define the entry point for your app inside the container. It can be a binary or a script. In this case, it runs the Python interpreter.\n\n3.1 Building a Container Using Docker\n\n|\n\n19",
      "content_length": 1270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "CMD: the command that is used when starting a container. In this case it uses the name of the Python app app.py.\n\nYou can now create your container image with the following command:\n\ndocker build -f Dockerfile -t quay.io/gitops-cookbook/pythonapp:latest\n\nChange the container image name with the your registry, user, and repo. Example: quay.io/youruser/yourrepo:latest. See Chap‐ ter 2 for how to create a new account on registries such as Quay.io.\n\nYour container image is building now. Docker will fetch existing layers from a public container registry (DockerHub, Quay, Red Hat Registry, etc.) and add a new layer with the content specified in the Dockerfile. Such layers could also be available locally, if already downloaded, in special storage called a container cache or Docker cache.\n\nSTEP 1: FROM registry.access.redhat.com/ubi8/python-39 Getting image source signatures Copying blob adffa6963146 done Copying blob 4125bdfaec5e done Copying blob 362566a15abb done Copying blob 0661f10c38cc done Copying blob 26f1167feaf7 done Copying config a531ae7675 done Writing manifest to image destination Storing signatures STEP 2: ENV PORT 8080 --> 6dbf4ac027e STEP 3: EXPOSE 8080 --> f78357fe402 STEP 4: WORKDIR /usr/src/app --> 547bf8ca5c5 STEP 5: COPY requirements.txt ./ --> 456cab38c97 STEP 6: RUN pip install --no-cache-dir -r requirements.txt Collecting Flask Downloading Flask-2.0.2-py3-none-any.whl (95 kB) |████████████████████████████████| 95 kB 10.6 MB/s Collecting itsdangerous>=2.0 Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB) Collecting Werkzeug>=2.0 Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB) |████████████████████████████████| 288 kB 1.7 MB/s Collecting click>=7.1.2 Downloading click-8.0.3-py3-none-any.whl (97 kB) |████████████████████████████████| 97 kB 31.9 MB/s Collecting Jinja2>=3.0 Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB) |████████████████████████████████| 133 kB 38.8 MB/s STEP 7: COPY . .\n\n20\n\n|\n\nChapter 3: Containers",
      "content_length": 1977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "--> 3e6b73464eb STEP 8: ENTRYPOINT [\"python\"] --> acabca89260 STEP 9: CMD [\"app.py\"] STEP 10: COMMIT quay.io/gitops-cookbook/pythonapp:latest --> 52e134d39af 52e134d39af013a25f3e44d25133478dc20b46626782762f4e46b1ff6f0243bb\n\nYour container image is now available in your Docker cache and ready to be used. You can verify its presence with this command:\n\ndocker images\n\nYou should get the list of available container images from the cache in output. Those could be images you have built or downloaded with the docker pull command:\n\nREPOSITORY TAG IMAGE ID CREATED↳ SIZE quay.io/gitops-cookbook/pythonapp latest 52e134d39af0 6 minutes ago↳ 907 MB\n\nOnce your image is created, you can consume it locally or push it to a public container registry to be consumed elsewhere, like from a CI/CD pipeline.\n\nYou need to first log in to your public registry. In this example, we are using Quay:\n\ndocker login quay.io\n\nYou should get output similar to this:\n\nLogin Succeeded!\n\nThen you can push your container image to the registry:\n\ndocker push quay.io/gitops-cookbook/pythonapp:latest\n\nAs confirmed, you should get output similar to this:\n\nGetting image source signatures Copying blob e6e8a2c58ac5 done Copying blob 3ba8c926eef9 done Copying blob 558b534f4e1b done Copying blob 25f82e0f4ef5 done Copying blob 7b17276847a2 done Copying blob 352ba846236b done Copying blob 2de82c390049 done Copying blob 26525e00a8d8 done Copying config 52e134d39a done Writing manifest to image destination Copying config 52e134d39a [--------------------------------------] 0.0b / 5.4KiB Writing manifest to image destination Storing signatures\n\n3.1 Building a Container Using Docker\n\n|\n\n21",
      "content_length": 1661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Discussion You can create container images in this way with Docker from your workstation or any host where the Docker service/daemon is running.\n\nAdditionally, you can use functionalities offered by a public regis‐ try such as Quay.io that can directly create the container image from a Dockerfile and store it to the registry.\n\nThe build requires access to all layers, thus an internet connection to the registries storing base layers is needed, or at least having them in the container cache. Docker has a layered structure where any change to your app is committed on top of the existing layers, so there’s no need to download all the layers each time since it will add only deltas for each new change.\n\nContainer images typically start from a base OS layer such as Fedora, CentOS, Ubuntu, Alpine, etc. However, they can also start from scratch, an empty layer for super-minimal images contain‐ ing only the app’s binary. See the scratch documentation for more info.\n\nIf you want to run your previously created container image, you can do so with this command:\n\ndocker run -p 8080:8080 -ti quay.io/gitops-cookbook/pythonapp:latest\n\ndocker run has many options to start your container. The most common are:\n\np\n\nBinds the port of the container with the port of the host running such container.\n\nt\n\nAttaches a TTY to the container.\n\ni\n\nGoes into an interactive mode.\n\nd\n\nGoes in the background, printing a hash that you can use to interact asynchro‐ nously with the running container.\n\n22\n\n|\n\nChapter 3: Containers",
      "content_length": 1514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "The preceding command will start your app in the Docker network and bind it to port 8080 of your workstation:\n\nServing Flask app 'app' (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on all addresses. WARNING: This is a development server. Do not use it in a production deployment. * Running on http://10.0.2.100:8080/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 103-809-567 curl http://localhost:8080\n\nServing Flask app 'app' (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on all addresses. WARNING: This is a development server. Do not use it in a production deployment. * Running on http://10.0.2.100:8080/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 103-809-567 curl http://localhost:8080\n\nYou should get output like this:\n\nHello, World!\n\nSee Also\n\n• Best practices for writing Dockerfiles\n\n• Manage Docker images\n\n3.2 Building a Container Using Dockerless Jib\n\nProblem You are a software developer, and you want to create a container image without installing Docker or any additional software on your workstation.\n\nSolution As discussed in Recipe 3.1, you need to install the Docker engine to create container images. Docker requires permissions to install a service running as a daemon, thus a privileged process in your operating system. Today, dockerless solutions are also available for developers; a popular one is Jib.\n\nJib is an open source framework for Java made by Google to build OCI-compliant container images, without the need for Docker or any container runtime. Jib comes as a library that Java developers can import in their Maven or Gradle projects. This means you can create a container image for your app without writing or maintaining any Dockerfiles, delegating this complexity to Jib.\n\n3.2 Building a Container Using Dockerless Jib\n\n|\n\n23",
      "content_length": 2124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "We see the benefits from this approach as the following:1\n\nPure Java\n\nNo Docker or Dockerfile knowledge is required. Simply add Jib as a plug-in, and it will generate the container image for you.\n\nSpeed\n\nThe application is divided into multiple layers, splitting dependencies from classes. There’s no need to rebuild the container image like for Dockerfiles; Jib takes care of modifying the layers that changed.\n\nReproducibility\n\nUnnecessary updates are not triggered because the same contents generate the same image.\n\nThe easiest way to kickstart a container image build with Jib on existing Maven is by adding the plug-in via the command line:\n\nmvn compile com.google.cloud.tools:jib-maven-plugin:3.2.0:build -Dimage=<MY IMAGE>\n\nAlternatively, you can do so by adding Jib as a plug-in into your pom.xml:\n\n<project> ... <build> <plugins> ... <plugin> <groupId>com.google.cloud.tools</groupId> <artifactId>jib-maven-plugin</artifactId> <version>3.2.0</version> <configuration> <to> <image>myimage</image> </to> </configuration> </plugin> ... </plugins> </build> ... </project>\n\nIn this way, you can also manage other settings such as authentication or parameters for the build.\n\nLet’s now add Jib to an existing Java application, a Hello World application in Spring Boot that you can find in the book’s repository.\n\n1 For a presentation about Jib, see Appu Goundan and Qingyang Chen’s presentation from Velocity San Jose\n\n2018.\n\n24\n\n|\n\nChapter 3: Containers",
      "content_length": 1458,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Run the following command to create a container image without using Docker, and push it directly to a container registry. In this example, we use Quay.io, and we will store the container image at quay.io/gitops-cookbook/jib-example:latest, so you will need to provide your credentials for the registry:\n\nmvn compile com.google.cloud.tools:jib-maven-plugin:3.2.0:build \\ -Dimage=quay.io/gitops-cookbook/jib-example:latest \\ -Djib.to.auth.username=<USERNAME> \\ -Djib.to.auth.password=<PASSWORD>\n\nThe authentication here is handled with command-line options, but Jib can manage existing authentication with Docker CLI or read credentials from your settings.xml file.\n\nThe build takes a few moments, and the result is a Java-specific container image, based on the adoptOpenJDK base image, built locally and pushed directly to a regis‐ try. In this case, to Quay.io:\n\n[INFO] Scanning for projects... [INFO] [INFO] --------------------------< com.redhat:hello >-------------------------- [INFO] Building hello 0.0.1-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- ... [INFO] Containerizing application to quay.io/gitops-cookbook/jib-example... [INFO] Using credentials from <to><auth> for quay.io/gitops-cookbook/jib-example [INFO] The base image requires auth. Trying again for eclipse-temurin:11-jre... [INFO] Using base image with digest:↳ sha256:83d92ee225e443580cc3685ef9574582761cf975abc53850c2bc44ec47d7d943O] [INFO] [INFO] Container entrypoint set to [java, -cp, @/app/jib-classpath-file,↳ com.redhat.hello.HelloApplication]FO] [INFO] [INFO] Built and pushed image as quay.io/gitops-cookbook/jib-example [INFO] Executing tasks: [INFO] [==============================] 100,0% complete [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 41.366 s [INFO] Finished at: 2022-01-25T19:04:09+01:00 [INFO] ------------------------------------------------------------------------\n\nIf you have Docker and run the command docker images, you won’t see this image in your local cache!\n\n3.2 Building a Container Using Dockerless Jib\n\n|\n\n25",
      "content_length": 2223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Discussion Your container image is not present in your local cache, as you don’t need any container runtime to build images with Jib. You won’t see it with the docker images command, but you can pull it from the public container registry afterward, and it will store it in your cache.\n\nThis approach is suitable for development velocity and automation, where the CI system doesn’t need to have Docker installed on the nodes where it runs. Jib can create the container image without any Dockerfiles. Additionally, it can push the image to a container registry.\n\nIf you also want to store it locally from the beginning, Jib can connect to Docker hosts and do it for you.\n\nYou can pull your container image from the registry to try it:\n\ndocker run -p 8080:8080 -ti quay.io/gitops-cookbook/jib-example\n\nTrying to pull quay.io/gitops-cookbook/jib-example:latest... Getting image source signatures Copying blob ea362f368469 done Copying blob d5cc550bb6a0 done Copying blob bcc17963ea24 done Copying blob 9b46d5d971fa done Copying blob 51f4f7c353f0 done Copying blob 43b2cdfa19bb done Copying blob fd142634d578 done Copying blob 78c393914c97 done Copying config 346462b8d3 done Writing manifest to image destination Storing signatures\n\n. ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.6.3)\n\n2022-01-25 18:36:24.762 INFO 1 --- [ main] com.redhat.hello.HelloApplication↳ : Starting HelloApplication using Java 11.0.13 on a719cf76f440 with PID 1↳ (/app/classes started by root in /) 2022-01-25 18:36:24.765 INFO 1 --- [ main] com.redhat.hello.HelloApplication↳ : No active profile set, falling back to default profiles: default 2022-01-25 18:36:25.700 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWeb- Server↳ : Tomcat initialized with port(s): 8080 (http) 2022-01-25 18:36:25.713 INFO 1 --- [ main] o.apache.catalina.core.StandardSer- vice↳ : Starting service [Tomcat] 2022-01-25 18:36:25.713 INFO 1 --- [ main] org.apache.catalina.core.StandardEn-\n\n26\n\n|\n\nChapter 3: Containers",
      "content_length": 2162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "gine↳ : Starting Servlet engine: [Apache Tomcat/9.0.56] 2022-01-25 18:36:25.781 INFO 1 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/]↳ : Initializing Spring embedded WebApplicationContext 2022-01-25 18:36:25.781 INFO 1 --- [ main] w.s.c.ServletWebServerApplicationCon- text↳ : Root WebApplicationContext: initialization completed in 947 ms 2022-01-25 18:36:26.087 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWeb- Server↳ : Tomcat started on port(s): 8080 (http) with context path '' 2022-01-25 18:36:26.096 INFO 1 --- [ main] com.redhat.hello.HelloApplication↳ : Started HelloApplication in 1.778 seconds (JVM running for 2.177)\n\nGet the hello endpoint:\n\ncurl localhost:8080/hello\n\n{\"id\":1,\"content\":\"Hello, World!\"}\n\nSee Also\n\n• Using Jib with Quarkus projects\n\n3.3 Building a Container Using Buildah\n\nProblem Sometimes installing or managing Docker is not possible. Dockerless solutions for creating container images are useful in use cases such as local development or CI/CD systems.\n\nSolution The OCI specification is an open standard, and this favors multiple open source implementations for the container engine and the container image building mecha‐ nism. Two growing popular examples today are Podman and Buildah.\n\nWhile Docker uses a single monolithic application for creating, running, and shipping container images, the codebase for con‐ tainer management functionalities here has been split between different projects like Podman, Buildah, and Skopeo. Podman sup‐ port is already available on Mac and Windows, however Buildah is currently only available on Linux or Linux subsystems such as WSL2 for Windows. See the documentation to install it on your workstation.\n\nThose are two complementary open source projects and command-line tools that work on OCI containers and images; however, they differ in their specialization.\n\n3.3 Building a Container Using Buildah\n\n|\n\n27",
      "content_length": 1889,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "While Podman specializes in commands and functions that help you to maintain and modify container images, such as pulling, tagging, and pushing, Buildah specializes in building container images. Decoupling functions in different processes is done by design, as the authors wanted to move from the single privileged process Docker model to a lightweight, rootless, daemonless, and decoupled set of tools to improve agility and security.\n\nFollowing the same approach, you find Skopeo, a tool used to move container images; and CRI-O, a container engine complaint with the Kubernetes container runtime interface for running appli‐ cations.\n\nBuildah supports the Dockerfile format, but its goal is to provide a lower-level interface to build container images without requiring a Dockerfile. Buildah is a daemonless solution that can create images inside a container without mounting the Docker socket. This functionality improves security and portability since it’s easy to add Buildah builds on the fly to a CI/CD pipeline where the Linux or Kubernetes nodes do not require a Docker installation.\n\nAs we discussed, you can create a container image with or without a Dockerfile. Let’s now create a simple HTTPD container image without a Dockerfile.\n\nYou can start from any base image such as CentOS:\n\nbuildah from centos\n\nYou should get output similar to this:\n\nResolved short name \"centos\" to a recorded short-name alias↳ (origin: /etc/containers/registries.conf.d/shortnames.conf) Getting image source signatures Copying blob 926a85fb4806 done Copying config 2f3766df23 done Writing manifest to image destination Storing signatures centos-working-container\n\nSimilarly to Docker and docker images, you can run the com‐ mand buildah containers to get the list of available images from the container cache. If you also have installed Podman, this is similar to podman images.\n\nIn this case, the container image ID is centos-working-container, and you can refer to it for creating the other layers.\n\nNow let’s install the httpd package inside a new layer:\n\n28\n\n|\n\nChapter 3: Containers",
      "content_length": 2079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "buildah run centos-working-container yum install httpd -y\n\nYou should get output similar to this:\n\nCentOS Linux 8 - AppStream 9.0 MB/s | 8.4 MB 00:00 CentOS Linux 8 - BaseOS 436 kB/s | 4.6 MB 00:10 CentOS Linux 8 - Extras 23 kB/s | 10 kB 00:00 Dependencies resolved. =============================================================================== Package Arch Version Repository Size =============================================================================== Installing: httpd x86_64 2.4.37-43.module_el8.5.0+1022+b541f3b1 Installing dependencies: apr x86_64 1.6.3-12.el8 apr-util x86_64 1.6.1-6.el8 brotli x86_64 1.0.6-3.el8 centos-logos-httpd noarch 85.8-2.el8 httpd-filesystem noarch 2.4.37-43.module_el8.5.0+1022+b541f3b1 httpd-tools x86_64 2.4.37-43.module_el8.5.0+1022+b541f3b1 mailcap noarch 2.1.48-3.el8 mod_http2 x86_64 1.15.7-3.module_el8.4.0+778+c970deab Installing weak dependencies: apr-util-bdb x86_64 1.6.1-6.el8 apr-util-openssl x86_64 1.6.1-6.el8 Enabling module streams: ... Complete!\n\nNow let’s copy a welcome HTML page inside the container running HTTPD. You can find the source code in this book’s repo:\n\n<html> <head> <title>GitOps CookBook example</title> </head> <body> <h1>Hello, World!</h1> </body> </html>\n\nbuildah copy centos-working-container index.html /var/www/html/index.html\n\nFor each new layer added, you should get output with the new container image hash, similar to the following:\n\n78c6e1dcd6f819581b54094fd38a3fd8f170a2cb768101e533c964e04aacab2e\n\nbuildah config --entrypoint \"/usr/sbin/httpd -DFOREGROUND\" centos-working-container\n\nbuildah commit centos-working-container quay.io/gitops-cookbook/gitops-website\n\n3.3 Building a Container Using Buildah\n\n|\n\n29",
      "content_length": 1700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "You should get output similar to this:\n\nGetting image source signatures Copying blob 618ce6bf40a6 skipped: already exists Copying blob eb8c13ba832f done Copying config b825e91208 done Writing manifest to image destination Storing signatures b825e91208c33371e209cc327abe4f53ee501d5679c127cd71c4d10cd03e5370\n\nYour container image is now in the container cache, ready to run or push to another registry.\n\nAs mentioned before, Buildah can also create container images from a Dockerfile. Let’s make the same container image from the Dockerfile listed here:\n\nFROM centos:latest RUN yum -y install httpd COPY index.html /var/www/html/index.html EXPOSE 80 CMD [\"/usr/sbin/httpd\", \"-DFOREGROUND\"]\n\nbuildah bud -f Dockerfile -t quay.io/gitops-cookbook/gitops-website\n\nSTEP 1: FROM centos:latest Resolved short name \"centos\" to a recorded short-name alias↳ (origin: /etc/containers/registries.conf.d/shortnames.conf) Getting image source signatures Copying blob 926a85fb4806 done Copying config 2f3766df23 done Writing manifest to image destination Storing signatures STEP 2: RUN yum -y install httpd CentOS Linux 8 - AppStream 9.6 MB/s | 8.4 MB 00:00 CentOS Linux 8 - BaseOS 7.5 MB/s | 4.6 MB 00:00 CentOS Linux 8 - Extras 63 kB/s | 10 kB 00:00 Dependencies resolved. ... Complete! STEP 3: COPY index.html /var/www/html/index.html STEP 4: EXPOSE 80 STEP 5: CMD [\"/usr/sbin/httpd\", \"-DFOREGROUND\"] STEP 6: COMMIT quay.io/gitops-cookbook/gitops-website Getting image source signatures Copying blob 618ce6bf40a6 skipped: already exists Copying blob 1be523a47735 done Copying config 3128caf147 done Writing manifest to image destination Storing signatures --> 3128caf1475 3128caf147547e43b84c13c241585d23a32601f2c2db80b966185b03cb6a8025\n\nIf you have also installed Podman, you can run it this way:\n\npodman run -p 8080:80 -ti quay.io/gitops-cookbook/gitops-website\n\n30\n\n|\n\nChapter 3: Containers",
      "content_length": 1879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Then you can test it by opening the browser on http://localhost:8080.\n\nDiscussion With Buildah, you have the opportunity to create container images from scratch or starting from a Dockerfile. You don’t need to install Docker, and everything is designed around security: rootless mechanism, daemonless utilities, and more refined control of creating image layers.\n\nBuildah can also build images from scratch, thus it creates an empty layer similar to the FROM scratch Dockerfile statement. This aspect is useful for creating very lightweight images containing only the packages needed to run your application, as you can see in Figure 3-2.\n\nFigure 3-2. Buildah image shrink\n\nA good example use case for a scratch build is considering the development images versus staging or production images. During development, container images may require a compiler and other tools. However, in production, you may only need the runtime or your packages.\n\nSee Also\n\n• Running Buildah inside a container\n\n3.3 Building a Container Using Buildah\n\n|\n\n31",
      "content_length": 1036,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "3.4 Building a Container with Buildpacks\n\nProblem Creating container image by using Dockerfiles can be challenging at scale. You want a tool complementing Docker that can inspect your application source code to create container images without writing a Dockerfile.\n\nSolution Cloud Native Buildpacks is an open source project that provides a set of executables to inspect your app source code and to create a plan to build and run your applica‐ tion.\n\nBuildpacks can create OCI-compliant container images without a Dockerfile, starting from the app source code, as you can see in Figure 3-3.\n\nFigure 3-3. Buildpacks builds\n\nThis mechanism consists of two phases:\n\nDetection\n\nBuildpacks tooling will navigate your source code to discover which program‐ ming language or framework is used (e.g., POM, NPM files, Python require‐ ments, etc.) and assign a suitable buildpack for the build.\n\nBuilding\n\nOnce a buildpack is found, the source is compiled and Buildpacks creates a container image with the appropriate entry point and startup scripts.\n\nTo use Buildpacks, you have to download the pack CLI for your operating system (Mac, Windows, Linux), and also have Docker installed.\n\nOn macOS, pack is available through Homebrew as follows:\n\nbrew install buildpacks/tap/pack\n\n32\n\n|\n\nChapter 3: Containers",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Now let’s start creating our container image with Buildpacks from a sample Node.js app. You can find the app source code in this book’s repository:\n\ncd chapters/ch03/nodejs-app\n\nThe app directory structure contains a package.json file, a manifest listing Node.js packages required for this build, which helps Buildpacks understand which buildpack to use.\n\nYou can verify it with this command:\n\npack builder suggest\n\nYou should get output similar to this:\n\nSuggested builders: Google: gcr.io/buildpacks/builder:v1↳ Ubuntu 18 base image with buildpacks for .NET, Go, Java, Node.js,↳ and Python Heroku: heroku/buildpacks:18↳ Base builder for Heroku-18 stack, based on ubuntu:18.04 base↳ image Heroku: heroku/buildpacks:20↳ Base builder for Heroku-20 stack, based on ubuntu:20.04 base↳ image Paketo Buildpacks: paketobuildpacks/builder:base↳ Ubuntu bionic base image with buildpacks for Java, .NET Core,↳ Node.js, Go, Python, Ruby, NGINX and Procfile Paketo Buildpacks: paketobuildpacks/builder:full↳ Ubuntu bionic base image with buildpacks for Java, .NET Core,↳ Node.js, Go, Python, PHP, Ruby, Apache HTTPD, NGINX and Procfile Paketo Buildpacks: paketobuildpacks/builder:tiny↳ Tiny base image (bionic build image, distroless-like run image)↳ with buildpacks for Java, Java Native Image and Go\n\nNow you can decide to pick one of the suggested buildpacks. Let’s try the paketo buildpacks/builder:base, which also contains the Node.js runtime:\n\npack build nodejs-app --builder paketobuildpacks/builder:base\n\nRun pack builder inspect paketobuildpacks/builder:base to know the exact content of libraries and frameworks available in this buildpack.\n\nThe building process should start accordingly, and after a while, it should finish, and you should get output similar to this:\n\nbase: Pulling from paketobuildpacks/builder bf99a8b93828: Pulling fs layer ... Digest: sha256:7034e52388c11c5f7ee7ae8f2d7d794ba427cc2802f687dd9650d96a70ac0772\n\n3.4 Building a Container with Buildpacks\n\n|\n\n33",
      "content_length": 1977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Status: Downloaded newer image for paketobuildpacks/builder:base base-cnb: Pulling from paketobuildpacks/run bf99a8b93828: Already exists 9d58a4841c3f: Pull complete 77a4f59032ac: Pull complete 24e58505e5e0: Pull complete Digest: sha256:59aa1da9db6d979e21721e306b9ce99a7c4e3d1663c4c20f74f9b3876cce5192 Status: Downloaded newer image for paketobuildpacks/run:base-cnb ===> ANALYZING Previous image with name \"nodejs-app\" not found ===> DETECTING 5 of 10 buildpacks participating paketo-buildpacks/ca-certificates 3.0.1 paketo-buildpacks/node-engine 0.11.2 paketo-buildpacks/npm-install 0.6.2 paketo-buildpacks/node-module-bom 0.2.0 paketo-buildpacks/npm-start 0.6.1 ===> RESTORING ===> BUILDING ... Paketo NPM Start Buildpack 0.6.1 Assigning launch processes web: node server.js\n\n===> EXPORTING Adding layer 'paketo-buildpacks/ca-certificates:helper' Adding layer 'paketo-buildpacks/node-engine:node' Adding layer 'paketo-buildpacks/npm-install:modules' Adding layer 'launch.sbom' Adding 1/1 app layer(s) Adding layer 'launcher' Adding layer 'config' Adding layer 'process-types' Adding label 'io.buildpacks.lifecycle.metadata' Adding label 'io.buildpacks.build.metadata' Adding label 'io.buildpacks.project.metadata' Setting default process type 'web' Saving nodejs-app... *** Images (82b805699d6b): nodejs-app Adding cache layer 'paketo-buildpacks/node-engine:node' Adding cache layer 'paketo-buildpacks/npm-install:modules' Adding cache layer 'paketo-buildpacks/node-module-bom:cyclonedx-node-module' Successfully built image nodejs-app\n\nNow let’s run it with Docker:\n\ndocker run --rm -p 3000:3000 nodejs-app\n\nYou should get output similar to this:\n\nServer running at http://0.0.0.0:3000/\n\n34\n\n|\n\nChapter 3: Containers",
      "content_length": 1720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "View the running application:\n\ncurl http://localhost:3000/\n\nYou should get output similar to this:\n\nHello Buildpacks!\n\nDiscussion Cloud Native Buildpacks is an incubating project in the Cloud Native Computing Foundation (CNCF), and it supports both Docker and Kubernetes. On Kubernetes, it can be used with Tekton, a Kubernetes-native CI/CD system that can run Buildpacks as a Tekton Task to create container images. It recently adopted the Boson Project to provide a functions-as-a-service (FaaS) experience on Kubernetes with Knative, by enabling the build of functions via buildpacks.\n\nSee Also\n\n• Using Buildpacks with Tekton Pipelines\n\n• FaaS Knative Boson project’s buildpacks\n\n3.5 Building a Container Using Shipwright and kaniko in Kubernetes\n\nProblem You need to create a container image, and you want to do it with Kubernetes.\n\nSolution Kubernetes is well known as a container orchestration platform to deploy and manage apps. However, it doesn’t include support for building container images out-of-the-box. Indeed, according to Kubernetes documentation: “(Kubernetes) Does not deploy source code and does not build your application. Continuous Integration, Delivery, and Deployment (CI/CD) workflows are determined by organization cul‐ tures and preferences as well as technical requirements.”\n\nAs mentioned, one standard option is to rely on CI/CD systems for this purpose, like Tekton (see Chapter 6). Another option is to use a framework to manage builds with many underlying tools, such as the one we discussed in the previous recipes. One example is Shipwright.\n\n3.5 Building a Container Using Shipwright and kaniko in Kubernetes\n\n|\n\n35",
      "content_length": 1653,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Shipwright is an extensible framework for building container images on Kubernetes. It supports popular tools such as Buildah, Cloud Native Buildpacks, and kaniko. It uses Kubernetes-style APIs, and it runs workloads using Tekton.\n\nThe benefit for developers is a simplified approach for building container images, by defining a minimal YAML file that does not require any previous knowledge of containers or container engines. This approach makes this solution agnostic and highly integrated with the Kubernetes API ecosystem.\n\nThe first thing to do is to install Shipwright to your Kubernetes cluster, say kind or Minikube (see Chapter 2), following the documentation or from OperatorHub.io.\n\nUsing Operators and Operator Lifecycle Manager (OLM) gives consistency for installing/uninstalling software on Kubernetes, along with dependency management and lifecycle control. For instance, the Tekton Operator dependency is automatically resolved and installed if you install Shipwright via the Operator. Check the OLM documentation for details with this approach.\n\nLet’s follow the standard procedure from the documentation. First you need to install the Tekton dependency. At the time of writing this book, it is version 0.30.0:\n\nkubectl apply -f \\ https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.30.0/ release.yaml\n\nThen you install Shipwright. At the time of writing this book, it is version 0.7.0:\n\nkubectl apply -f \\ https://github.com/shipwright-io/build/releases/download/v0.7.0/release.yaml\n\nFinally, you install Shipwright build strategies:\n\nkubectl apply -f \\ https://github.com/shipwright-io/build/releases/download/v0.7.0/sample- strategies.yaml\n\nOnce you have installed Shipwright, you can start creating your container image build using one of these tools:\n\nkaniko •\n\nCloud Native Buildpacks •\n\n• BuildKit\n\n• Buildah\n\nLet’s explore kaniko.\n\n36\n\n|\n\nChapter 3: Containers",
      "content_length": 1901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "kaniko is another dockerless solution to build container images from a Dockerfile inside a container or Kubernetes cluster. Shipwright brings additional APIs to Kuber‐ netes to use tools such as kaniko to create container images, acting as an abstract layer that can be considered an extensible building system for Kubernetes.\n\nLet’s explore the APIs that are defined from Cluster Resource Definitions (CRDs):\n\nClusterBuildStrategy\n\nRepresents the type of build to execute.\n\nBuild\n\nRepresents the build. It includes the specification of one ClusterBuildStrategy object.\n\nBuildRun\n\nRepresents a running build. The build starts when this object is created.\n\nRun the following command to check all available ClusterBuildStrategy (CBS) objects:\n\nkubectl get cbs\n\nYou should get a list of available CBSs to consume:\n\nNAME AGE buildah 26s buildkit 26s buildpacks-v3 26s buildpacks-v3-heroku 26s kaniko 26s kaniko-trivy 26s ko 26s source-to-image 26s source-to-image-redhat 26s\n\nThis CRD is cluster-wide, available for all namespaces. If you don’t see any items, please install the Shipwright build strategies as dis‐ cussed previously.\n\nShipwright will generate a container image on the Kubernetes nodes container cache, and then it can push it to a container registry.\n\nYou need to provide the credentials to push the image to the registry in the form of a Kubernetes Secret. For example, if you use Quay you can create one like the following:\n\nREGISTRY_SERVER=quay.io REGISTRY_USER=<your_registry_user> REGISTRY_PASSWORD=<your_registry_password>\n\n3.5 Building a Container Using Shipwright and kaniko in Kubernetes\n\n|\n\n37",
      "content_length": 1616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "EMAIL=<your_email> kubectl create secret docker-registry push-secret \\ --docker-server=$REGISTRY_SERVER \\ --docker-username=$REGISTRY_USER \\ --docker-password=$REGISTRY_PASSWORD \\ --docker-email=$EMAIL\n\nWith Quay, you can use an encrypted password instead of using your account password. See the documentation for more details.\n\nNow let’s create a build-kaniko.yaml file containing the Build object that will use kaniko to containerize a Node.js sample app. You can find the source code in this book’s repository:\n\napiVersion: shipwright.io/v1alpha1 kind: Build metadata: name: buildpack-nodejs-build spec: source: url: https://github.com/shipwright-io/sample-nodejs contextDir: docker-build strategy: name: kaniko kind: ClusterBuildStrategy output: image: quay.io/gitops-cookbook/sample-nodejs:latest credentials: name: push-secret\n\nRepository to grab the source code from.\n\nThe directory where the source code is present.\n\nThe ClusterBuildStrategy to use.\n\nThe destination of the resulting container image. Change this with your con‐ tainer registry repo.\n\nThe secret to use to authenticate to the container registry and push the image.\n\n38\n\n|\n\nChapter 3: Containers",
      "content_length": 1168,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Now, let’s create the Build object:\n\nkubectl create -f build-kaniko.yaml\n\nYou should get output similar to this:\n\nbuild.shipwright.io/kaniko-nodejs-build created\n\nLet’s list the available builds:\n\nkubectl get builds\n\nYou should get output similar to the following:\n\nNAME REGISTERED REASON BUILDSTRATEGYKIND↳ BUILDSTRATEGYNAME CREATIONTIME kaniko-nodejs-build True Succeeded ClusterBuildStrategy↳ kaniko 13s\n\nAt this point, your Build is REGISTERED, but it’s not started yet. Let’s create the following object in order to start it:\n\napiVersion: shipwright.io/v1alpha1 kind: BuildRun metadata: generateName: kaniko-nodejs-buildrun- spec: buildRef: name: kaniko-nodejs-build\n\nkubectl create -f buildrun.yaml\n\nIf you check the list of running pods, you should see one being created:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS↳ AGE kaniko-nodejs-buildrun-b9mmb-qbrgl-pod-dk7xt 0/3 PodInitializing 0↳ 19s\n\nWhen the STATUS changes, the build will start, and you can track the progress by checking the logs from the containers used by this pod to run the build in multiple steps:\n\nstep-source-default\n\nThe first step, used to get the source code\n\nstep-build-and-push\n\nThe step to run the build, either from source code or from a Dockerfile like in this case with kaniko\n\nstep-results\n\nThe result of the build\n\n3.5 Building a Container Using Shipwright and kaniko in Kubernetes\n\n|\n\n39",
      "content_length": 1378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Let’s check the logs of the building phase:\n\nkubectl logs -f kaniko-nodejs-buildrun-b9mmb-qbrgl-pod-dk7xt -c step-build-and-push\n\nINFO[0001] Retrieving image manifest ghcr.io/shipwright-io/shipwright-samples/ node:12 INFO[0001] Retrieving image ghcr.io/shipwright-io/shipwright-samples/node:12↳ from registry ghcr.io INFO[0002] Built cross stage deps: map[] INFO[0002] Retrieving image manifest ghcr.io/shipwright-io/shipwright-samples/ node:12 INFO[0002] Returning cached image manifest INFO[0002] Executing 0 build triggers INFO[0002] Unpacking rootfs as cmd COPY . /app requires it. INFO[0042] COPY . /app INFO[0042] Taking snapshot of files... INFO[0042] WORKDIR /app INFO[0042] cmd: workdir INFO[0042] Changed working directory to /app INFO[0042] No files changed in this command, skipping snapshotting. INFO[0042] RUN pwd && ls -l && npm install &&↳ npm run print-http-server-version INFO[0042] Taking snapshot of full filesystem... INFO[0052] cmd: /bin/sh INFO[0052] args: [-c pwd && ls -l && npm install &&↳ npm run print-http-server-version] INFO[0052] Running: [/bin/sh -c pwd && ls -l && npm install &&↳ npm run print-http-server-version] /app total 44 -rw-r--r-- 1 node node 261 Jan 27 14:29 Dockerfile -rw-r--r-- 1 node node 30000 Jan 27 14:29 package-lock.json -rw-r--r-- 1 node node 267 Jan 27 14:29 package.json drwxr-xr-x 2 node node 4096 Jan 27 14:29 public npm WARN npm-simple-renamed@0.0.1 No repository field. npm WARN npm-simple-renamed@0.0.1 No license field.\n\nadded 90 packages from 40 contributors and audited 90 packages in 6.405s\n\n10 packages are looking for funding run `npm fund` for details\n\nfound 0 vulnerabilities\n\n> npm-simple-renamed@0.0.1 print-http-server-version /app > serve -v\n\n13.0.2 INFO[0060] Taking snapshot of full filesystem... INFO[0062] EXPOSE 8080 INFO[0062] cmd: EXPOSE INFO[0062] Adding exposed port: 8080/tcp INFO[0062] CMD [\"npm\", \"start\"]\n\n40\n\n|\n\nChapter 3: Containers",
      "content_length": 1921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "INFO[0070] Pushing image to quay.io/gitops-cookbook/sample-nodejs:latest INFO[0393] Pushed image to 1 destinations\n\nThe image is built and pushed to the registry, and you can check the result from this command as well:\n\nkubectl get buildruns\n\nAnd on your registry, as shown in Figure 3-4.\n\nFigure 3-4. Image pushed to Quay\n\nDiscussion Shipwright provides a convenient way to create container images on Kubernetes, and its agnostic approach makes it robust and interoperable. The project aims at being the Build API for Kubernetes, providing an easier path for developers to automate on Kubernetes. As Tekton runs under the hood creating builds, Shipwright also makes transitioning from micropipeline to extended pipeline workflows on Kubernetes easier.\n\nAs a reference, if you would like to create a build with Buildah instead of kaniko, it’s just a ClusterBuildStrategy change in your Build object:\n\napiVersion: shipwright.io/v1alpha1 kind: Build metadata: name: buildpack-nodejs-build spec: source: url: https://github.com/shipwright-io/sample-nodejs contextDir: source-build strategy: name: buildah kind: ClusterBuildStrategy\n\n3.5 Building a Container Using Shipwright and kaniko in Kubernetes\n\n|\n\n41",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "output: image: quay.io/gitops-cookbook/sample-nodejs:latest credentials: name: push-secret\n\nAs we discussed previously in Recipe 3.3, Buildah can create the container image from the source code. It doesn’t need a Dockerfile.\n\nSelecting Buildah as the ClusterBuildStrategy.\n\n3.6 Final Thoughts The container format is the de facto standard for packaging applications, and today many tools help create container images. Developers can create images with Docker or with other tools and frameworks and then use the same with any CI/CD system to deploy their apps to Kubernetes.\n\nWhile Kubernetes per se doesn’t build container images, some tools interact with the Kubernetes API ecosystem to add this functionality. This aspect improves devel‐ opment velocity and consistency across environments, delegating this complexity to the platform.\n\nIn the following chapters, you will see how to control the deployment of your con‐ tainers running on Kubernetes with tools such as Kustomize or Helm, and then how to add automation to support highly scalable workloads with CI/CD and GitOps.\n\n42\n\n|\n\nChapter 3: Containers",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "CHAPTER 4 Kustomize\n\nDeploying to a Kubernetes cluster is, in summary, applying some YAML files and checking the result.\n\nThe hard part is developing the initial YAML files version; after that, usually, they suffer only small changes such as updating the container image tag version, the num‐ ber of replicas, or a new configuration value. One option is to make these changes directly in the YAML files—it works, but any error in this version (modification of the wrong line, deleting something by mistake, putting in the wrong whitespace) might be catastrophic.\n\nFor this reason, some tools let you define base Kubernetes manifests (which change infrequently) and specific files (maybe one for each environment) for setting the parameters that change more frequently. One of these tools is Kustomize.\n\nIn this chapter, you’ll learn how to use Kustomize to manage Kubernetes resource files in a template-free way without using any DSL.\n\nThe first step is to create a Kustomize project and deploy it to a Kubernetes cluster (see Recipe 4.1).\n\nAfter the first deployment, the application is automatically updated with a new container image, a new configuration value, or any other field, such as the replica number (see Recipes 4.2 and 4.3).\n\nIf you’ve got several running environments (i.e., staging, production, etc.), you need to manage them similarly. Still, with its particularities, Kustomize lets you define a set of custom values per environment (see Recipe 4.4).\n\nApplication configuration values are properties usually mapped as a Kubernetes ConfigMap. Any change (and its consequent update on the cluster) on a ConfigMap\n\n43",
      "content_length": 1633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "doesn’t trigger a rolling update of the application, which means that the application will run with the previous version until you manually restart it.\n\nKustomize provides some functions to automatically execute a rolling update when the ConfigMap of an application changes (see Recipe 4.5).\n\n4.1 Using Kustomize to Deploy Kubernetes Resources\n\nProblem You want to deploy several Kubernetes resources at once.\n\nSolution Use Kustomize to configure which resources to deploy.\n\nDeploying an application to a Kubernetes cluster isn’t as trivial as just applying one YAML/JSON file containing a Kubernetes Deployment object. Usually, other Kuber‐ netes objects must be defined like Service, Ingress, ConfigMaps, etc., which makes things a bit more complicated in terms of managing and updating these resources (the more resources to maintain, the more chance to update the wrong one) as well as applying them to a cluster (should we run multiple kubectl commands?).\n\nKustomize is a CLI tool, integrated within the kubectl tool to manage, customize, and apply Kubernetes resources in a template-less way.\n\nWith Kustomize, you need to set a base directory with standard Kubernetes resource files (no placeholders are required) and create a kustomization.yaml file where resources and customizations are declared, as you can see in Figure 4-1.\n\nFigure 4-1. Kustomize layout\n\nLet’s deploy a simple web page with HTML, JavaScript, and CSS files.\n\nFirst, open a terminal window and create a directory named pacman, then create three Kubernetes resource files to create a Namespace, a Deployment, and a Service with the following content.\n\n44\n\n|\n\nChapter 4: Kustomize",
      "content_length": 1655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "The namespace at pacman/namespace.yaml:\n\napiVersion: v1 kind: Namespace metadata: name: pacman\n\nThe deployment file at pacman/deployment.yaml:\n\napiVersion: apps/v1 kind: Deployment metadata: name: pacman-kikd namespace: pacman labels: app.kubernetes.io/name: pacman-kikd spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd ports: - containerPort: 8080 name: http protocol: TCP\n\nThe service file at pacman/service.yaml:\n\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: ports: - name: http port: 8080 targetPort: 8080 selector: app.kubernetes.io/name: pacman-kikd\n\nNotice that these files are Kubernetes files that you could apply to a Kubernetes cluster without any problem as no special characters or placeholders are used.\n\n4.1 Using Kustomize to Deploy Kubernetes Resources\n\n|\n\n45",
      "content_length": 1049,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "The second thing is to create the kustomization.yaml file in the pacman directory containing the list of resources that belongs to the application and are applied when running Kustomize:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./namespace.yaml - ./deployment.yaml - ./service.yaml\n\nKustomization file\n\nResources belonging to the application processed in depth-first order\n\nAt this point, we can apply the kustomization file into a running cluster by running the following command:\n\nkubectl apply --dry-run=client -o yaml \\ -k ./\n\nPrints the result of the kustomization run, without sending the result to the cluster\n\nWith -k option sets kubectl to use the kustomization file\n\nDirectory with parent kustomization.yaml file\n\nWe assume you’ve already started a Minikube cluster as shown in Recipe 2.3.\n\nThe output is the YAML file that would be sent to the server if the dry-run option was not used:\n\napiVersion: v1 items: - apiVersion: v1 kind: Namespace metadata: name: pacman - apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman\n\n46\n\n|\n\nChapter 4: Kustomize",
      "content_length": 1163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "spec: ports: - name: http port: 8080 targetPort: 8080 selector: app.kubernetes.io/name: pacman-kikd - apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd ports: - containerPort: 8080 name: http protocol: TCP kind: List metadata: {}\n\nList of all Kubernetes objects defined in kustomization.yaml to apply\n\nThe namespace document\n\nThe service document\n\nThe deployment document\n\nDiscussion The resources section supports different inputs in addition to directly setting the YAML files.\n\nFor example, you can set a base directory with its own kustomization.yaml and Kubernetes resources files and refer it from another kustomization.yaml file placed in another directory.\n\n4.1 Using Kustomize to Deploy Kubernetes Resources\n\n|\n\n47",
      "content_length": 1042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Given the following directory layout:\n\n. ├── base │ ├── kustomization.yaml │ └── deployment.yaml ├── kustomization.yaml ├── configmap.yaml\n\nAnd the Kustomization definitions in the base directory:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml\n\nYou’ll see that the root directory has a link to the base directory and a ConfigMap definition:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./base - ./configmap.yaml\n\nSo, applying the root kustomization file will automatically apply the resources defined in the base kustomization file.\n\nAlso, resources can reference external assets from a URL following the HashiCorp URL format. For example, we refer to a GitHub repository by setting the URL:\n\nresources: - github.com/lordofthejars/mysql - github.com/lordofthejars/mysql?ref=test\n\nRepository with a root-level kustomization.yaml file\n\nRepository with a root-level kustomization.yaml file on branch test\n\nYou’ve seen the application of a Kustomize file using kubectl, but Kustomize also comes with its own CLI tool offering a set of commands to interact with Kustomize resources.\n\nThe equivalent command to build Kustomize resources using kustomize instead of kubectl is:\n\nkustomize build\n\n48\n\n|\n\nChapter 4: Kustomize",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "And the output is:\n\napiVersion: v1 kind: Namespace metadata: name: pacman --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: ports: - name: http port: 8080 targetPort: 8080 selector: app.kubernetes.io/name: pacman-kikd --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd ports: - containerPort: 8080 name: http protocol: TCP\n\nIf you want to apply this output generated by kustomize to the cluster, run the following command:\n\nkustomize build . | kubectl apply -f -\n\n4.1 Using Kustomize to Deploy Kubernetes Resources\n\n|\n\n49",
      "content_length": 922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "See Also\n\n• Kustomize\n\n• kustomize/v4.4.1 on GitHub\n\n• HashiCorp URL format\n\n4.2 Updating the Container Image in Kustomize\n\nProblem You want to update the container image from a deployment file using Kustomize.\n\nSolution Use the images section to update the container image.\n\nOne of the most important and most-used operations in software development is updating the application to a newer version either with a bug fix or with a new feature. In Kubernetes, this means that you need to create a new container image, and name it accordingly using the tag section (<registry>/<username>/ <project>:<tag>).\n\nGiven the following partial deployment file:\n\nspec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd\n\nService 1.0.0 is deployed\n\nWe can update the version tag to 1.0.1 by using the images section in the kustomiza‐ tion.yaml file:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./namespace.yaml - ./deployment.yaml - ./service.yaml images: - name: lordofthejars/pacman-kikd newTag: 1.0.1\n\n50\n\n|\n\nChapter 4: Kustomize",
      "content_length": 1097,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "images section\n\nSets the name of the image to update\n\nSets the new tag value for the image\n\nFinally, use kubectl in dry-run or kustomize to validate that the output of the deployment file contains the new tag version. In a terminal window, run the follow‐ ing command:\n\nkustomize build\n\nThe output of the preceding command is:\n\n... apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.1 imagePullPolicy: Always name: pacman-kikd ports: - containerPort: 8080 name: http protocol: TCP\n\nVersion set in the kustomize.yaml file\n\nKustomize is not intrusive, which means that the original deploy‐ ment.yaml file still contains the original tag (1.0.0).\n\n4.2 Updating the Container Image in Kustomize\n\n|\n\n51",
      "content_length": 971,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Discussion One way to update the newTag field is by editing the kustomization.yaml file, but you can also use the kustomize tool for this purpose.\n\nRun the following command in the same directory as the kustomization.yaml file:\n\nkustomize edit set image lordofthejars/pacman-kikd:1.0.2\n\nCheck the content of the kustomization.yaml file to see that the newTag field has been updated:\n\n... images: - name: lordofthejars/pacman-kikd newTag: 1.0.2\n\n4.3 Updating Any Kubernetes Field in Kustomize\n\nProblem You want to update a field (i.e., number of replicas) using Kustomize.\n\nSolution Use the patches section to specify a change using the JSON Patch specification.\n\nIn the previous recipe, you saw how to update the container image tag, but sometimes you might change other parameters like the number of replicas or add annotations, labels, limits, etc.\n\nTo cover these scenarios, Kustomize supports the use of JSON Patch to modify any Kubernetes resource defined as a Kustomize resource. To use it, you need to specify the JSON Patch expression to apply and which resource to apply the patch to.\n\nFor example, we can modify the number of replicas in the following partial deploy‐ ment file from one to three:\n\napiVersion: apps/v1 kind: Deployment metadata: name: pacman-kikd namespace: pacman labels: app.kubernetes.io/name: pacman-kikd spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template:\n\n52\n\n|\n\nChapter 4: Kustomize",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: ...\n\nFirst, let’s update the kustomization.yaml file to modify the number of replicas defined in the deployment file:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml patches: - target: version: v1 group: apps kind: Deployment name: pacman-kikd namespace: pacman patch: |- - op: replace path: /spec/replicas value: 3\n\nPatch resource.\n\ntarget section sets which Kubernetes object needs to be changed. These values match the deployment file created previously.\n\nPatch expression.\n\nModification of a value.\n\nPath to the field to modify.\n\nNew value.\n\nFinally, use kubectl in dry-run or kustomize to validate that the output of the deployment file contains the new tag version. In a terminal window, run the follow‐ ing command:\n\nkustomize build\n\nThe output of the preceding command is:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd\n\n4.3 Updating Any Kubernetes Field in Kustomize\n\n|\n\n53",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "name: pacman-kikd namespace: pacman spec: replicas: 3 selector: matchLabels: app.kubernetes.io/name: pacman-kikd ...\n\nThe replicas value can also be updated using the replicas field in the kustomization.yaml file. The equivalent Kustomize file using the replicas field is shown in the following snippet:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization replicas: - name: pacman-kikd count: 3 resources: - deployment.yaml\n\nDeployment to update the replicas\n\nNew replicas value\n\nKustomize lets you add (or delete) values, in addition to modifying a value. Let’s see how to add a new label:\n\n... patches: - target: version: v1 group: apps kind: Deployment name: pacman-kikd namespace: pacman patch: |- - op: replace path: /spec/replicas value: 3 - op: add path: /metadata/labels/testkey value: testvalue\n\nAdds a new field with value\n\nPath with the field to add\n\nThe value to set\n\n54\n\n|\n\nChapter 4: Kustomize",
      "content_length": 920,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "The result of applying the file is:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd testkey: testvalue name: pacman-kikd namespace: pacman spec: replicas: 3 selector: ...\n\nAdded label\n\nDiscussion Instead of embedding a JSON Patch expression, you can create a YAML file with a Patch expression and refer to it using the path field instead of patch.\n\nCreate an external patch file named external_patch containing the JSON Patch expression:\n\nop: replace path: /spec/replicas value: 3 - op: add path: /metadata/labels/testkey value: testvalue\n\nAnd change the patch field to path pointing to the patch file:\n\n... patches: - target: version: v1 group: apps kind: Deployment name: pacman-kikd namespace: pacman path: external_patch.yaml\n\nPath to external patch file\n\nIn addition to the JSON Patch expression, Kustomize also supports Strategic Merge Patch to modify Kubernetes resources. In summary, a Strategic Merge Patch (or SMP) is an incomplete YAML file that is merged against a completed YAML file.\n\nOnly a minimal deployment file with container name information is required to update a container image:\n\n4.3 Updating Any Kubernetes Field in Kustomize\n\n|\n\n55",
      "content_length": 1201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml patches: - target: labelSelector: \"app.kubernetes.io/name=pacman-kikd\" patch: |- apiVersion: apps/v1 kind: Deployment metadata: name: pacman-kikd spec: template: spec: containers: - name: pacman-kikd image: lordofthejars/pacman-kikd:1.2.0\n\nTarget is selected using label\n\nPatch is smart enough to detect if it is an SMP or JSON Patch\n\nThis is a minimal deployment file\n\nSets only the field to change, the rest is left as is\n\nThe generated output is the original deployment.yaml file but with the new container image:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: pacman spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.2.0 imagePullPolicy: Always ...\n\n56\n\n|\n\nChapter 4: Kustomize",
      "content_length": 996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "path is supported as well.\n\nSee Also\n\n• RFC 6902: JavaScript Object Notation (JSON) Patch\n\nStrategic Merge Patch •\n\n4.4 Deploying to Multiple Environments\n\nProblem You want to deploy the same application in different namespaces using Kustomize.\n\nSolution Use the namespace field to set the target namespace.\n\nIn some circumstances, it’s good to have the application deployed in different name‐ spaces; for example, one namespace can be used as a staging environment, and another one as the production namespace. In both cases, the base Kubernetes files are the same, with minimal changes like the namespace deployed, some configuration parameters, or container version, to mention a few. Figure 4-2 shows an example.\n\nFigure 4-2. Kustomize layout\n\nkustomize lets you define multiple changes with a different namespace, as overlays on a common base using the namespace field. For this example, all base Kubernetes resources are put in the base directory and a new directory is created for customiza‐ tions of each environment:\n\n. ├── base │ ├── deployment.yaml\n\n4.4 Deploying to Multiple Environments\n\n|\n\n57",
      "content_length": 1106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "│ └── kustomization.yaml ├── production │ └── kustomization.yaml └── staging └── kustomization.yaml\n\nBase files\n\nChanges specific to production environment\n\nChanges specific to staging environment\n\nThe base kustomization file contains a reference to its resources:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml\n\nThere is a kustomization file with some parameters set for each environment direc‐ tory. These reference the base directory, the namespace to inject into Kubernetes resources, and finally, the image to deploy, which in production is 1.1.0 but in staging is 1.2.0-beta.\n\nFor the staging environment, kustomization.yaml content is shown in the following listing:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../base namespace: staging images: - name: lordofthejars/pacman-kikd newTag: 1.2.0-beta\n\nReferences to base directory\n\nSets namespace to staging\n\nSets the container tag for the staging environment\n\nThe kustomization file for production is similar to the staging one, but changes the namespace and the tag:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../base namespace: prod images:\n\n58\n\n|\n\nChapter 4: Kustomize",
      "content_length": 1245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "name: lordofthejars/pacman-kikd newTag: 1.1.0\n\nSets namespace for production\n\nSets the container tag for the production environment\n\nRunning kustomize produces different output depending on the directory where it is run; for example, running kustomize build in the staging directory produces:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: staging spec: replicas: 1 ... template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.2.0-beta ...\n\nNamespace value is injected\n\nContainer tag for the staging environment is injected\n\nBut if you run it in the production directory, the output is adapted to the production configuration:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd namespace: prod spec: replicas: 1 ... spec: containers: - image: lordofthejars/pacman-kikd:1.1.0 ...\n\nInjects the production namespace\n\n4.4 Deploying to Multiple Environments\n\n|\n\n59",
      "content_length": 1054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Container tag for the production environment\n\nDiscussion Kustomize can preappend/append a value to the names of all resources and refer‐ ences. This is useful when a different name in the resource is required depending on the environment, or to set the version deployed in the name:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../base namespace: staging namePrefix: staging- nameSuffix: -v1-2-0 images: - name: lordofthejars/pacman-kikd newTag: 1.2.0-beta\n\nPrefix to preappend\n\nSuffix to append\n\nAnd the resulting output is as follows:\n\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: staging-pacman-kikd-v1-2-0 namespace: staging spec: ...\n\nNew name of the deployment file\n\n4.5 Generating ConfigMaps in Kustomize\n\nProblem You want to generate Kubernetes ConfigMaps using Kustomize.\n\nSolution Use the ConfigMapGenerator feature field to generate a Kubernetes ConfigMap resource on the fly.\n\n60\n\n|\n\nChapter 4: Kustomize",
      "content_length": 1001,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Kustomize provides two ways of adding a ConfigMap as a Kustomize resource: either by declaring a ConfigMap as any other resource or declaring a ConfigMap from a ConfigMapGenerator.\n\nWhile using ConfigMap as a resource offers no other advantage than populating Kubernetes resources as any other resource, ConfigMapGenerator automatically appends a hash to the ConfigMap metadata name and also modifies the deployment file with the new hash. This minimal change has a deep impact on the application’s lifecycle, as we’ll see soon in the example.\n\nLet’s consider an application running in Kubernetes and configured using a Config Map—for example, a database timeout connection parameter. We decided to increase this number at some point, so the ConfigMap file is changed to this new value, and we deploy the application again. Since the ConfigMap is the only changed file, no rolling update of the application is done. A manual rolling update of the application needs to be triggered to propagate the change to the application. Figure 4-3 shows what is changed when a ConfigMap object is updated.\n\nFigure 4-3. Change of a ConfigMap\n\nBut, if ConfigMapGenerator manages the ConfigMap, any change on the configura‐ tion file also changes the deployment Kubernetes resource. Since the deployment file has changed too, an automatic rolling update is triggered when the resources are applied, as shown in Figure 4-4.\n\nMoreover, when using ConfigMapGenerator, multiple configuration datafiles can be combined into a single ConfigMap, making a perfect use case when every environ‐ ment has different configuration files.\n\n4.5 Generating ConfigMaps in Kustomize\n\n|\n\n61",
      "content_length": 1656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Figure 4-4. Change of a ConfigMap using ConfigMapGenerator\n\nLet’s start with a simple example, adding the ConfigMapGenerator section in the kustomization.yaml file.\n\nThe deployment file is similar to the one used in previous sections of this chapter but includes the volumes section:\n\napiVersion: apps/v1 kind: Deployment metadata: name: pacman-kikd spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels: app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd volumeMounts: - name: config mountPath: /config volumes: - name: config configMap: name: pacman-configmap\n\nConfigMap name is used in the kustomization.yaml file\n\n62\n\n|\n\nChapter 4: Kustomize",
      "content_length": 781,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "The configuration properties are embedded within the kustomization.yaml file. Notice that the ConfigMap object is created on the fly when the kustomization file is built:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml configMapGenerator: - name: pacman-configmap literals: - db-timeout=2000 - db-username=Ada\n\nName of the ConfigMap set in the deployment file\n\nEmbeds configuration values in the file\n\nSets a key/value pair for the properties\n\nFinally, use kubectl in dry-run or kustomize to validate that the output of the deployment file contains the new tag version. In a terminal window, run the follow‐ ing command:\n\nkustomize build\n\nThe output of the preceding command is a new ConfigMap with the configuration values set in kustomization.yaml. Moreover, the name of the ConfigMap is updated by appending a hash in both the generated ConfigMap and deployment:\n\napiVersion: v1 data: db-timeout: \"2000\" db-username: Ada kind: ConfigMap metadata: name: pacman-configmap-96kb69b6t4 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman-kikd template: metadata: labels:\n\n4.5 Generating ConfigMaps in Kustomize\n\n|\n\n63",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "app.kubernetes.io/name: pacman-kikd spec: containers: - image: lordofthejars/pacman-kikd:1.0.0 imagePullPolicy: Always name: pacman-kikd volumeMounts: - mountPath: /config name: config volumes: - configMap: name: pacman-configmap-96kb69b6t4 name: config\n\nConfigMap with properties\n\nName with hash\n\nName field is updated to the one with the hash triggering a rolling update\n\nSince the hash is calculated for any change in the configuration properties, a change on them provokes a change on the output triggering a rolling update of the applica‐ tion. Open the kustomization.yaml file and update the db-timeout literal from 2000 to 1000 and run kustomize build again. Notice the change in the ConfigMap name using a new hashed value:\n\napiVersion: v1 data: db-timeout: \"1000\" db-username: Ada kind: ConfigMap metadata: name: pacman-configmap-6952t58tb4 --- apiVersion: apps/v1 kind: Deployment ... volumes: - configMap: name: pacman-configmap-6952t58tb4 name: config\n\nNew hashed value\n\nDiscussion ConfigMapGenerator also supports merging configuration properties from different sources.\n\n64\n\n|\n\nChapter 4: Kustomize",
      "content_length": 1112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Create a new kustomization.yaml file in the dev_literals directory, setting it as the previous directory and overriding the db-username value:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../literals configMapGenerator: - name: pacman-configmap behavior: merge literals: - db-username=Alexandra\n\nBase directory\n\nMerge properties (can be create or replace too)\n\nOverridden value\n\nRunning the kustomize build command produces a ConfigMap containing a merge of both configuration properties:\n\napiVersion: v1 data: db-timeout: \"1000\" db-username: Alexandra kind: ConfigMap metadata: name: pacman-configmap-ttfdfdk5t8 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd ...\n\nInherits from base\n\nOverrides value\n\nIn addition to setting configuration properties as literals, Kustomize supports defin‐ ing them as .properties files.\n\nCreate a connection.properties file with two properties inside:\n\ndb-url=prod:4321/db db-username=ada\n\nThe kustomization.yaml file uses the files field instead of literals:\n\n4.5 Generating ConfigMaps in Kustomize\n\n|\n\n65",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ./deployment.yaml configMapGenerator: - name: pacman-configmap files: - ./connection.properties\n\nSets a list of files to read\n\nPath to the properties file\n\nRunning the kustomize build command produces a ConfigMap containing the name of the file as a key, and the value as the content of the file:\n\napiVersion: v1 data: connection.properties: |- db-url=prod:4321/db db-username=ada kind: ConfigMap metadata: name: pacman-configmap-g9dm2gtt77 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: pacman-kikd name: pacman-kikd ...\n\nSee Also Kustomize offers a similar way to deal with Kubernetes Secrets. But as we’ll see in Chapter 8, the best way to deal with Kubernetes Secrets is using Sealed Secrets.\n\n4.6 Final Thoughts Kustomize is a simple tool, using template-less technology that allows you to define plain YAML files and override values either using a merge strategy or using JSON Patch expressions. The structure of a project is free as you define the directory layout you feel most comfortable with; the only requirement is the presence of a kustomization.yaml file.\n\nBut there is another well-known tool to manage Kubernetes resources files, that in our opinion, is a bit more complicated but more powerful, especially when the appli‐ cation/service to deploy has several dependencies such as databases, mail servers, caches, etc. This tool is Helm, and we’ll cover it in Chapter 5.\n\n66\n\n|\n\nChapter 4: Kustomize",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "CHAPTER 5 Helm\n\nIn Chapter 4, you learned about Kustomize, a simple yet powerful tool to manage Kubernetes resources. But another popular tool aims to simplify the Kubernetes resources management too: Helm.\n\nHelm works similarly to Kustomize, but it’s a template solution and acts more like a package manager, producing artifacts that are versionable, sharable, or deployable.\n\nIn this chapter, we’ll introduce Helm, a package manager for Kubernetes that helps install and manage Kubernetes applications using the Go template language in YAML files.\n\nThe first step is to create a Helm project and deploy it to a Kubernetes cluster (see Recipes 5.1 and 5.2). After the first deployment, the application is updated with a new container image, a new configuration value, or any other field, such as the replica number (see Recipe 5.3).\n\nOne of the differences between Kustomize and Helm is the concept of a Chart. A Chart is a packaged artifact that can be shared and contains multiple elements like dependencies on other Charts (see Recipes 5.4, 5.5, and 5.6).\n\nApplication configuration values are properties usually mapped as a Kubernetes ConfigMap. Any change (and its consequent update on the cluster) on a ConfigMap doesn’t trigger a rolling update of the application, which means that the application will run with the previous version until you manually restart it.\n\nHelm provides some functions to automatically execute a rolling update when the ConfigMap of an application changes (see Recipe 5.7).\n\n67",
      "content_length": 1510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "5.1 Creating a Helm Project\n\nProblem You want to create a simple Helm project.\n\nSolution Use the Helm CLI tool to create a new project.\n\nIn contrast to Kustomize, which can be used either within the kubectl command or as a standalone CLI tool, Helm needs to be downloaded and installed in your local machine.\n\nHelm is a packager for Kubernetes that bundles related manifest files and packages them into a single logical deployment unit: a Chart. Thus simplified, for many engineers, Helm makes it easy to start using Kubernetes with real applications.\n\nHelm Charts are useful for addressing the installation complexities and simple upgrades of applications.\n\nFor this book, we use Helm 3.7.2, which you can download from GitHub and install in your PATH directory.\n\nOpen a terminal and run the following commands to create a Helm Chart directory layout:\n\nmkdir pacman mkdir pacman/templates\n\ncd pacman\n\nThen create three files: one that defines the Chart, another representing the deploy‐ ment template using the Go template language and template functions from the Sprig library, and finally a file containing the default values for the Chart.\n\nA Chart.yaml file declares the Chart with information such as version or name. Create the file in the root directory:\n\napiVersion: v2 name: pacman description: A Helm chart for Pacman\n\ntype: application\n\nversion: 0.1.0\n\nappVersion: \"1.0.0\"\n\n68\n\n|\n\nChapter 5: Helm",
      "content_length": 1408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Version of the Chart. This is updated when something in the Chart definition is changed.\n\nVersion of the application.\n\nLet’s create a deployment.yaml and a service.yaml template file to deploy the application.\n\nThe deployment.yaml file templatizes the deployment’s name, the application version, the replica count, the container image and tag, the pull policy, the security context, and the port:\n\napiVersion: apps/v1 kind: Deployment metadata: name: {{ .Chart.Name}} labels: app.kubernetes.io/name: {{ .Chart.Name}} {{- if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{- end }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} spec: containers: - image: \"{{ .Values.image.repository }}: {{ .Values.image.tag | default .Chart.AppVersion}}\" imagePullPolicy: {{ .Values.image.pullPolicy }} securityContext: {{- toYaml .Values.securityContext | nindent 14 }} name: {{ .Chart.Name}} ports: - containerPort: {{ .Values.image.containerPort }} name: http protocol: TCP\n\nSets the name from the Chart.yaml file\n\nConditionally sets the version based on the presence of the appVersion in the Chart.yaml file\n\nSets the appVersion value but quoting the property\n\n5.1 Creating a Helm Project\n\n|\n\n69",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Placeholder for the replicaCount property\n\nPlaceholder for the container image\n\nPlaceholder for the image tag if present and if not, defaults to the Chart.yaml property\n\nSets the securityContext value as a YAML object and not as a string, indenting it 14 spaces\n\nThe service.yaml file templatizes the service name and the container port:\n\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: {{ .Chart.Name }} name: {{ .Chart.Name }} spec: ports: - name: http port: {{ .Values.image.containerPort }} targetPort: {{ .Values.image.containerPort }} selector: app.kubernetes.io/name: {{ .Chart.Name }}\n\nThe values.yaml file contains the default values for the Chart. These values can be overridden at runtime, but they provide good initial values.\n\nCreate the file in the root directory with some default values:\n\nimage: repository: quay.io/gitops-cookbook/pacman-kikd tag: \"1.0.0\" pullPolicy: Always containerPort: 8080\n\nreplicaCount: 1 securityContext: {}\n\nDefines the image section\n\nSets the repository property\n\nEmpty securityContext\n\nBuilt-in properties are capitalized; for this reason, properties defined in the Chart.yaml file start with an uppercase letter.\n\n70\n\n|\n\nChapter 5: Helm",
      "content_length": 1204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Since the toYaml function is used for the securityContext value, the expected value for the securityContext property in values.yaml should be a YAML object. For example:\n\nsecurityContext: capabilities: drop: - ALL readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000\n\nThe relationship between all elements is shown in Figure 5-1.\n\nFigure 5-1. Relationship between Helm elements\n\n5.1 Creating a Helm Project\n\n|\n\n71",
      "content_length": 424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "At this point the Helm directory layout is created and should be similar to this:\n\npacman ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── service.yaml └── values.yaml\n\nThe Chart.yaml file is the Chart descriptor and contains metadata related to the Chart.\n\nThe templates directory contains all template files used for installing a Chart.\n\nThese files are Helm template files used to deploy the application.\n\nThe values.yaml file contains the default values for a Chart.\n\nTo render the Helm Chart locally to YAML, run the following command in a terminal window:\n\nhelm template .\n\nThe output is:\n\n--- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: pacman name: pacman spec: ports: - name: http port: 8080 targetPort: 8080 selector: app.kubernetes.io/name: pacman --- apiVersion: apps/v1 kind: Deployment metadata: name: pacman labels: app.kubernetes.io/name: pacman app.kubernetes.io/version: \"1.0.0\" spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman template:\n\n72\n\n|\n\nChapter 5: Helm",
      "content_length": 1037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "metadata: labels: app.kubernetes.io/name: pacman spec: containers: - image: \"quay.io/gitops-cookbook/pacman-kikd:1.0.0\" imagePullPolicy: Always securityContext: {} name: pacman ports: - containerPort: 8080 name: http protocol: TCP\n\nName is injected from Chart.yaml\n\nPort is set in values.yaml\n\nVersion is taken from Chart version\n\nConcatenates content from two attributes\n\nEmpty security context\n\nYou can override any default value by using the --set parameter in Helm. Let’s override the replicaCount value from one (defined in values.yaml) to three:\n\nhelm template --set replicaCount=3 .\n\nAnd the replicas value is updated:\n\napiVersion: apps/v1 kind: Deployment metadata: name: pacman labels: app.kubernetes.io/name: pacman app.kubernetes.io/version: \"1.0.0\" spec: replicas: 3 ...\n\nDiscussion Helm is a package manager for Kubernetes, and as such, it helps you with the task of versioning, sharing, and upgrading Kubernetes applications.\n\nLet’s see how to install the Helm Chart to a Kubernetes cluster and upgrade the application.\n\n5.1 Creating a Helm Project\n\n|\n\n73",
      "content_length": 1069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "With Minikube up and running, execute the following command in a terminal window, and run the install command to deploy the application to the cluster:\n\nhelm install pacman .\n\nThe Chart is installed in the running Kubernetes instance:\n\nLAST DEPLOYED: Sat Jan 22 15:13:50 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None\n\nGet the list of current deployed pods, Deployments, and Services to validate that the Helm Chart is deployed in the Kubernetes cluster:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE pacman-7947b988-kzjbc 1/1 Running 0 60s\n\nkubectl get deployment\n\nNAME READY UP-TO-DATE AVAILABLE AGE pacman 1/1 1 1 4m50s\n\nkubectl get services\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE pacman ClusterIP 172.30.129.123 <none> 8080/TCP 9m55s\n\nAlso, it’s possible to get history information about the deployed Helm Chart using the history command:\n\nhelm history pacman\n\nREVISION APP VERSION DESCRIPTION 1 1.0.0 Install complete\n\nUPDATED\n\nSat Jan 22 15:23:22 2022\n\nSTATUS\n\ndeployed\n\nCHART ↳\n\npacman-0.1.0↳\n\nTo uninstall a Chart from the cluster, run uninstall command:\n\nhelm uninstall pacman\n\nrelease \"pacman\" uninstalled\n\nHelm is a package manager that lets you share the Chart (package) to other Charts as a dependency. For example, you can have a Chart defining the deployment of the application and another Chart as a dependency setting a database deployment. In this way, the installation process installs the application and the database Chart automatically.\n\nWe’ll learn about the packaging process and adding dependencies in a later section.\n\n74\n\n|\n\nChapter 5: Helm",
      "content_length": 1601,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "You can use the helm create <name> command to let the Helm tool skaffold the project.\n\nSee Also\n\n• Helm\n\nGo template package •\n\nSprig Function Documentation •\n\n5.2 Reusing Statements Between Templates\n\nProblem You want to reuse template statements across several files.\n\nSolution Use _helpers.tpl to define reusable statements.\n\nWe deployed a simple application to Kubernetes using Helm in the previous recipe. This simple application was composed of a Kubernetes Deployment file and a Kuber‐ netes Service file where the selector field was defined with the same value.\n\nAs a reminder:\n\n... spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} ...\n\nservice.yaml ---- ... selector: app.kubernetes.io/name: {{ .Chart.Name }} ----\n\nIf you need to update this field—for example, adding a new label as a selector—you would need to update in three places, as shown in the previous snippets.\n\n5.2 Reusing Statements Between Templates\n\n|\n\n75",
      "content_length": 1056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Helm lets you create a _helpers.tpl file in the templates directory defining statements that can be called in templates to avoid this problem.\n\nLet’s refactor the previous example to use the _helpers.tpl file to define the selector Labels.\n\nCreate the _helpers.tpl file in the templates directory with the following content:\n\n{{- define \"pacman.selectorLabels\" -}} app.kubernetes.io/name: {{ .Chart.Name}} {{- end }}\n\nDefines the statement name\n\nDefines the logic of the statement\n\nThen replace the template placeholders shown in previous snippets with a call to the podman.selectorLabels helper statement using the include keyword:\n\nspec: replicas: {{ .Values.replicaCount }} selector: matchLabels: {{- include \"pacman.selectorLabels\" . | nindent 6 }} template: metadata: labels: {{- include \"pacman.selectorLabels\" . | nindent 8 }} spec: containers:\n\nCalls pacman.selectorLabels with indentation\n\nCalls pacman.selectorLabels with indentation\n\nTo render the Helm Chart locally to YAML, run the following command in a terminal window:\n\nhelm template .\n\nThe output is:\n\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: pacman name: pacman spec: ports: - name: http port: 8080 targetPort: 8080\n\n76\n\n|\n\nChapter 5: Helm",
      "content_length": 1237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "selector: app.kubernetes.io/name: pacman --- apiVersion: apps/v1 kind: Deployment metadata: name: pacman labels: app.kubernetes.io/name: pacman app.kubernetes.io/version: \"1.0.0\" spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman template: metadata: labels: app.kubernetes.io/name: pacman spec: containers: - image: \"quay.io/gitops-cookbook/pacman-kikd:1.0.0\" imagePullPolicy: Always securityContext: {} name: pacman ports: - containerPort: 8080 name: http protocol: TCP\n\nSelector is updated with value set in _helpers.tpl\n\nSelector is updated with value set in _helpers.tpl\n\nSelector is updated with value set in _helpers.tpl\n\nDiscussion If you want to update the selector labels, the only change you need to do is an update to the _helpers.tpl file:\n\n{{- define \"pacman.selectorLabels\" -}} app.kubernetes.io/name: {{ .Chart.Name}} app.kubernetes.io/version: {{ .Chart.AppVersion}} {{- end }}\n\nAdds a new attribute\n\n5.2 Reusing Statements Between Templates\n\n|\n\n77",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "To render the Helm Chart locally to YAML, run the following command in a terminal window:\n\nhelm template .\n\nThe output is:\n\n--- # Source: pacman/templates/service.yaml apiVersion: v1 kind: Service metadata: ... selector: app.kubernetes.io/name: pacman app.kubernetes.io/version: 1.0.0 --- apiVersion: apps/v1 kind: Deployment metadata: name: pacman labels: app.kubernetes.io/name: pacman app.kubernetes.io/version: \"1.0.0\" spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman app.kubernetes.io/version: 1.0.0 template: metadata: labels: app.kubernetes.io/name: pacman app.kubernetes.io/version: 1.0.0 spec: ...\n\nLabel is added\n\nLabel is added\n\nLabel is added\n\nAlthough it’s common to use __helpers.tpl as the filename to define functions, you can name any file starting with __, and Helm will read the functions too.\n\n78\n\n|\n\nChapter 5: Helm",
      "content_length": 859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "5.3 Updating a Container Image in Helm\n\nProblem You want to update the container image from a deployment file using Helm and upgrade the running instance.\n\nSolution Use the upgrade command.\n\nWith Minikube up and running, deploy version 1.0.0 of the pacman application:\n\nhelm install pacman .\n\nWith the first revision deployed, let’s update the container image to a new version and deploy it.\n\nYou can check revision number by running the following command:\n\nhelm history pacman\n\nREVISION UPDATED STATUS CHART APP VERSION↳ DESCRIPTION 1 Sun Jan 23 16:00:09 2022 deployed pacman-0.1.0 1.0.0↳ Install complete\n\nTo update the version, open values.yaml and update the image.tag field to the newer container image tag:\n\nimage: repository: quay.io/gitops-cookbook/pacman-kikd tag: \"1.1.0\" pullPolicy: Always containerPort: 8080\n\nreplicaCount: 1 securityContext: {}\n\nUpdates to container tag to 1.1.0\n\nThen update the appVersion field of the Chart.yaml file:\n\napiVersion: v2 name: pacman description: A Helm chart for Pacman\n\ntype: application version: 0.1.0 appVersion: \"1.1.0\"\n\nVersion is updated accordingly\n\n5.3 Updating a Container Image in Helm\n\n|\n\n79",
      "content_length": 1149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "You can use appVersion as the tag instead of having two separate fields. Using two fields or one might depend on your use case, versioning strategy, and lifecycle of your software.\n\nAfter these changes, upgrade the deployment by running the following command:\n\nhelm upgrade pacman .\n\nThe output reflects that a new revision has been deployed:\n\nRelease \"pacman\" has been upgraded. Happy Helming! NAME: pacman LAST DEPLOYED: Mon Jan 24 11:39:28 2022 NAMESPACE: asotobue-dev STATUS: deployed REVISION: 2 TEST SUITE: None\n\nNew revision\n\nThe history command shows all changes between all versions:\n\nhelm history pacman\n\nREVISION UPDATED STATUS DESCRIPTION 1 Mon Jan 24 10:22:06 2022 superseded Install complete 2 Mon Jan 24 11:39:28 2022 deployed Upgrade complete\n\nCHART\n\npacman-0.1.0\n\npacman-0.1.0\n\nAPP VERSION↳\n\n1.0.0↳\n\n1.1.0↳\n\nappVersion is the application version, so every time you change the application version, you should update that field too. On the other side, version is the Chart version and should be updated when the definition of the Chart (i.e., templates) changes, so both fields are independent.\n\nDiscussion Not only you can install or upgrade a version with Helm, but you can also roll back to a previous revision.\n\nIn the terminal window, run the following command:\n\nhelm rollback pacman 1\n\nRollback was a success! Happy Helming!\n\n80\n\n|\n\nChapter 5: Helm",
      "content_length": 1369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Running the history command reflects this change too:\n\nhelm history pacman\n\nREVISION UPDATED STATUS DESCRIPTION 1 Mon Jan 24 10:22:06 2022 superseded Install complete 2 Mon Jan 24 11:39:28 2022 superseded Upgrade complete 3 Mon Jan 24 12:31:58 2022 deployed Rollback to\n\nCHART\n\npacman-0.1.0\n\npacman-0.1.0\n\npacman-0.1.0\n\nAPP VERSION↳\n\n1.0.0↳\n\n1.1.0↳\n\n1.0.0↳\n\nFinally, Helm offers a way to override values, not only using the --set argument as shown in Recipe 5.1, but by providing a YAML file.\n\nCreate a new YAML file named newvalues.yaml in the root directory with the follow‐ ing content:\n\nimage: tag: \"1.2.0\"\n\nThen run the template command, setting the new file as an override of values.yaml:\n\nhelm template pacman -f newvalues.yaml .\n\nThe resulting YAML document is using the values set in values.yaml but overriding the images.tag set in newvalues.yaml:\n\napiVersion: apps/v1 kind: Deployment metadata: name: pacman ... spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: pacman template: metadata: labels: app.kubernetes.io/name: pacman spec: containers: - image: \"quay.io/gitops-cookbook/pacman-kikd:1.2.0\" imagePullPolicy: Always ...\n\n5.3 Updating a Container Image in Helm\n\n|\n\n81",
      "content_length": 1199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "5.4 Packaging and Distributing a Helm Chart\n\nProblem You want to package and distribute a Helm Chart so it can be reused by others.\n\nSolution Use the package command.\n\nHelm is a package manager for Kubernetes. As we’ve seen in this chapter, the basic unit in Helm is a Chart containing the Kubernetes files required to deploy the application, the default values for the templates, etc.\n\nBut we’ve not yet seen how to package Helm Charts and distribute them to be available to other Charts as dependencies or deployed by other users.\n\nLet’s package the pacman Chart into a .tgz file. In the pacman directory, run the following command:\n\nhelm package .\n\nAnd you’ll get a message informing you where the archive is stored:\n\nSuccessfully packaged chart and saved it to:↳ gitops-cookbook/code/05_helm/04_package/pacman/pacman-0.1.0.tgz\n\nA Chart then needs to be published into a Chart repository. A Chart repository is an HTTP server with an index.yaml file containing metadata information regarding Charts and .tgz Charts.\n\nTo publish them, update the index.yaml file with the new metadata information, and upload the artifact.\n\nThe directory layout of a repository might look like this:\n\nrepo ├── index.yaml ├── pacman-0.1.0.tgz\n\nThe index.yaml file with information about each Chart present in the repository looks like:\n\napiVersion: v1 entries: pacman: - apiVersion: v2 appVersion: 1.0.0 created: \"2022-01-24T16:42:54.080959+01:00\" description: A Helm chart for Pacman digest: aa3cce809ffcca86172fc793d7804d1c61f157b9b247680a67d5b16b18a0798d name: pacman\n\n82\n\n|\n\nChapter 5: Helm",
      "content_length": 1577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "type: application urls: - pacman-0.1.0.tgz version: 0.1.0 generated: \"2022-01-24T16:42:54.080485+01:00\"\n\nYou can run helm repo index in the root directory, where pack‐ aged Charts are stored, to generate the index file automatically.\n\nDiscussion In addition to packaging a Helm Chart, Helm can generate a signature file for the packaged Chart to verify its correctness later.\n\nIn this way, you can be sure it has not been modified, and it’s the correct Chart.\n\nTo sign/verify the package, you need a pair of GPG keys in the machine; we’re assuming you already have one pair created.\n\nNow you need to call the package command but set the -sign argument with the required parameters to generate a signature file:\n\nhelm package --sign --key 'me@example.com' \\ --keyring /home/me/.gnupg/secring.gpg .\n\nNow, two files are created—the packaged Helm Chart (.tgz) and the signature file (.tgz.prov):\n\n. ├── Chart.yaml ├── pacman-0.1.0.tgz ├── pacman-0.1.0.tgz.prov ├── templates │ ├── deployment.yaml │ └── service.yaml └── values.yaml\n\nChart package\n\nSignature file\n\nRemember to upload both files in the Chart repository.\n\n5.4 Packaging and Distributing a Helm Chart\n\n|\n\n83",
      "content_length": 1166,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "To verify that a Chart is valid and has not been manipulated, use the verify command:\n\nhelm verify pacman-0.1.0.tgz\n\nSigned by: alexs (book) <asotobu@example.com> Using Key With Fingerprint: 57C4511D738BC0B288FAF9D69B40EB787040F3CF Chart Hash Verified:↳ sha256:d8b2e0c5e12a8425df2ea3a903807b93aabe4a6ff8277511a7865c847de3c0bf\n\nIt’s valid\n\nSee Also\n\n• The Chart Repository Guide\n\n• Helm Provenance and Integrity\n\n5.5 Deploying a Chart from a Repository\n\nProblem You want to deploy a Helm Chart stored in Chart repository.\n\nSolution Use the repo add command to add the remote repository and the install command to deploy it.\n\nPublic Helm Chart repositories like Bitnami are available for this purpose.\n\nTo install Charts from a repository (either public or private), you need to register it using its URL:\n\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\nURL of Helm Chart repository where index.yaml is placed\n\nList the registered repositories:\n\nhelm repo list\n\nNAME stable bitnami\n\nURL https://charts.helm.sh/stable https://charts.bitnami.com/bitnami\n\nBitnami repo is registered\n\n84\n\n|\n\nChapter 5: Helm",
      "content_length": 1112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Run helm repo update to get the latest list of Charts for each repo.\n\nAfter registering a repository, you might want to find which Charts are available.\n\nIf you want to deploy a PostgreSQL instance in the cluster, use the search command to search all repositories for a Chart that matches the name:\n\nhelm search repo postgresql\n\nThe outputs are the list of Charts that matches the name, the version of the Chart and PostgreSQL, and a description. Notice the name of the Chart is composed of the repository name and the Chart name, i.e., bitnami/postgresql:\n\nNAME\n\nCHART VERSION\n\nAPP VERSION↳\n\nDESCRIPTION\n\nbitnami/postgresql\n\n10.16.2\n\n11.14.0↳\n\nChart for PostgreSQL, an object-relational data...\n\nbitnami/postgresql-ha\n\n8.2.6\n\n11.14.0↳\n\nChart for PostgreSQL with HA architecture (usin...\n\nstable/postgresql\n\n8.6.4\n\n11.7.0↳\n\nDEPRECATED Chart for PostgreSQL, an object-rela...\n\nstable/pgadmin\n\n1.2.2\n\n4.18.0↳\n\npgAdmin is a web based administration tool for ...\n\nstable/stolon\n\n1.6.5\n\n0.16.0↳\n\nDEPRECATED - Stolon - PostgreSQL cloud native H...\n\nstable/gcloud-sqlproxy DEPRECATED Google Cloud SQL Proxy stable/prometheus-postgres-exporter\n\n0.6.1\n\n1.3.1\n\n1.11↳\n\n0.8.0↳\n\nDEPRECATED A Helm chart for prometheus postgres...\n\nTo deploy the PostgreSQL Chart, run the install command but change the location of the Helm Chart from a local directory to the full name of the Chart (<repo>/ <chart>):\n\nhelm install my-db \\ --set postgresql.postgresqlUsername=my-default,postgresql.↳ postgresqlPassword=postgres,postgresql.postgresqlDatabase=mydb,↳ postgresql.persistence.enabled=false \\ bitnami/postgresql\n\nSets the name of the deployment\n\nOverrides default values to the ones set in the command line\n\nSets the PostgreSQL Chart stored in the Bitnami repo\n\n5.5 Deploying a Chart from a Repository\n\n|\n\n85",
      "content_length": 1789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "And a detailed output is shown in the console:\n\nNAME: my-db LAST DEPLOYED: Mon Jan 24 22:33:56 2022 NAMESPACE: asotobue-dev STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: CHART NAME: postgresql CHART VERSION: 10.16.2 APP VERSION: 11.14.0\n\n** Please be patient while the chart is being deployed **\n\nPostgreSQL can be accessed via port 5432 on the following DNS names↳ from within your cluster:\n\nmy-db-postgresql.asotobue-dev.svc.cluster.local - Read/Write connection\n\nTo get the(((\"passwords\", \"Helm Charts\")))(((\"Helm\", \"Charts\", \"pass- words\")))(((\"Charts\", \"passwords\"))) password for \"postgres\" run:\n\nexport POSTGRES_ADMIN_PASSWORD=$(kubectl get secret↳ --namespace asotobue-dev my-db-postgresql -o↳ jsonpath=\"{.data.postgresql-postgres-password}\" | base64 --decode)\n\nTo get the password for \"my-default\" run:\n\nexport POSTGRES_PASSWORD=$(kubectl get secret↳ --namespace asotobue-dev my-db-postgresql -o↳ jsonpath=\"{.data.postgresql-password}\" | base64 --decode)\n\nTo connect to your database run the following command:\n\nkubectl run my-db-postgresql-client --rm --tty -i --restart='Never'↳ --namespace asotobue-dev↳ --image docker.io/bitnami/postgresql:11.14.0-debian-10-r28↳ --env=\"PGPASSWORD=$POSTGRES_PASSWORD\"↳ --command -- psql --host my-db-postgresql -U my-default -d mydb↳ -p 5432\n\nTo connect to your (((\"Helm\", \"Charts\", \"connecting to databases\")))(((\"Charts\", \"databases\", \"connecting to\")))(((\"databases\", \"connecting to\", \"Helm Charts\")))database from outside the cluster execute the following commands:\n\nkubectl port-forward --namespace asotobue-dev svc/my-db-postgresql 5432:5432 & PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U my-default -d mydb -p 5432\n\nInspect the installation by listing pods, Services, StatefulSets, or Secrets:\n\n86\n\n|\n\nChapter 5: Helm",
      "content_length": 1789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "kubectl get pods\n\nNAME READY STATUS RESTARTS AGE my-db-postgresql-0 1/1 Running 0 23s\n\nkubectl get services\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-db-postgresql ClusterIP 172.30.35.1 <none> 5432/TCP 3m33s my-db-postgresql-headless ClusterIP None <none> 5432/TCP 3m33s\n\nkubectl get statefulset\n\nNAME READY AGE my-db-postgresql 1/1 4m24s\n\nkubectl get secrets\n\nNAME TYPE DATA AGE my-db-postgresql Opaque 2 5m23s sh.helm.release.v1.my-db.v1 helm.sh/release.v1 1 5m24s\n\nDiscussion When a third party creates a Chart, there is no direct access to default values or the list of parameters to override. Helm provides a show command to check these values:\n\nhelm show values bitnami/postgresql\n\nAnd shows all the possible values:\n\n## @section Global parameters ## Global Docker image parameters ## Please, note that this will override the image parameters, including dependen cies ## configured to use the global value ## Current available global Docker image parameters: imageRegistry, imagePullSe crets ## and storageClass ##\n\n## @param global.imageRegistry Global Docker image registry ## @param global.imagePullSecrets Global Docker registry secret names as an array ## @param global.storageClass Global StorageClass for Persistent Volume(s) ## global: imageRegistry: \"\" ## E.g. ## imagePullSecrets: ## - myRegistryKeySecretName ## imagePullSecrets: [] ...\n\n5.5 Deploying a Chart from a Repository\n\n|\n\n87",
      "content_length": 1408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "5.6 Deploying a Chart with a Dependency\n\nProblem You want to deploy a Helm Chart that is a dependency of another Chart.\n\nSolution Use the dependencies section in the Chart.yaml file to register other Charts. So far, we’ve seen how to deploy simple services to the cluster, but usually a service might have other dependencies like a database, mail server, distributed cache, etc.\n\nIn the previous section, we saw how to deploy a PostgreSQL server in a Kubernetes cluster. In this section, we’ll see how to deploy a service composed of a Java service returning a list of songs stored in a PostgreSQL database. The application is summar‐ ized in Figure 5-2.\n\nFigure 5-2. Music application overview\n\nLet’s start creating the Chart layout shown in Recipe 5.1:\n\nmkdir music mkdir music/templates\n\ncd music\n\nThen create two template files to deploy the music service.\n\n88\n\n|\n\nChapter 5: Helm",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "The templates/deployment.yaml file contains the Kubernetes Deployment definition:\n\napiVersion: apps/v1 kind: Deployment metadata: name: {{ .Chart.Name}} labels: app.kubernetes.io/name: {{ .Chart.Name}} {{- if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{- end }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} spec: containers: - image: \"{{ .Values.image.repository }}:↳ {{ .Values.image.tag | default .Chart.AppVersion}}\" imagePullPolicy: {{ .Values.image.pullPolicy }} name: {{ .Chart.Name}} ports: - containerPort: {{ .Values.image.containerPort }} name: http protocol: TCP env: - name: QUARKUS_DATASOURCE_JDBC_URL value: {{ .Values.postgresql.server | ↳ default (printf \"%s-postgresql\" ( .Release.Name )) | quote }} - name: QUARKUS_DATASOURCE_USERNAME value: {{ .Values.postgresql.postgresqlUsername | ↳ default (printf \"postgres\" ) | quote }} - name: QUARKUS_DATASOURCE_PASSWORD valueFrom: secretKeyRef: name: {{ .Values.postgresql.secretName | ↳ default (printf \"%s-postgresql\" ( .Release.Name )) | quote }} key: {{ .Values.postgresql.secretKey }}\n\nThe templates/service.yaml file contains the Kubernetes Service definition:\n\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: {{ .Chart.Name }} name: {{ .Chart.Name }} spec: ports: - name: http\n\n5.6 Deploying a Chart with a Dependency\n\n|\n\n89",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "port: {{ .Values.image.containerPort }} targetPort: {{ .Values.image.containerPort }} selector: app.kubernetes.io/name: {{ .Chart.Name }}\n\nAfter the creation of the templates, it’s time for the Chart metadata Chart.yaml file. In this case, we need to define the dependencies of this Chart too. Since the music service uses a PostgreSQL database, we can add the Chart used in Recipe 5.5 as a dependency:\n\napiVersion: v2 name: music description: A Helm chart for Music service\n\ntype: application version: 0.1.0 appVersion: \"1.0.0\"\n\ndependencies: - name: postgresql version: 10.16.2 repository: \"https://charts.bitnami.com/bitnami\"\n\nDependencies section\n\nName of the Chart to add as dependency\n\nChart version\n\nRepository\n\nThe final file is Values.yaml with default configuration values. In this case, a new sec‐ tion is added to configure music deployment with PostgreSQL instance parameters:\n\nimage: repository: quay.io/gitops-cookbook/music tag: \"1.0.0\" pullPolicy: Always containerPort: 8080\n\nreplicaCount: 1\n\npostgresql: server: jdbc:postgresql://music-db-postgresql:5432/mydb postgresqlUsername: my-default secretName: music-db-postgresql secretKey: postgresql-password\n\nPostgreSQL section\n\n90\n\n|\n\nChapter 5: Helm",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "With the Chart in place, the next thing to do is download the dependency Chart and store it in the charts directory. This process is automatically done by running the dependency update command:\n\nhelm dependency update\n\nThe command output shows that one Chart has been downloaded and saved:\n\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"stable\" chart repository ...Successfully got an update from the \"bitnami\" chart repository Update Complete. ⎈Happy Helming!⎈ Saving 1 charts Downloading postgresql from repo https://charts.bitnami.com/bitnami Deleting outdated charts\n\nThe directory layout looks like this:\n\nmusic ├── Chart.lock ├── Chart.yaml ├── charts │ └── postgresql-10.16.2.tgz ├── templates │ ├── deployment.yaml │ └── service.yaml └── values.yaml\n\nPostgreSQL Chart is placed in the correct directory\n\nFinally, we deploy the Chart, setting configuration PostgreSQL deployment values from the command line:\n\nhelm install music-db --set postgresql.postgresqlPassword=postgres postgresql.post- gresqlDatabase=mydb,postgresql.persistence.enabled=false .\n\nThe installation process shows information about the deployment:\n\nNAME: music-db LAST DEPLOYED: Tue Jan 25 17:53:17 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None\n\nInspect the installation by listing pods, Services, StatefulSets, or Secrets:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE music-67dbf986b7-5xkqm 1/1 Running 1 (32s ago) 39s music-db-postgresql-0 1/1 Running 0 39s\n\nkubectl get statefulset\n\n5.6 Deploying a Chart with a Dependency\n\n|\n\n91",
      "content_length": 1601,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "NAME READY AGE music-db-postgresql 1/1 53s\n\nkubectl get services\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 40d music ClusterIP 10.104.110.34 <none> 8080/TCP 82s music-db-postgresql ClusterIP 10.110.71.13 <none> 5432/TCP 82s music-db-postgresql-headless ClusterIP None <none> 5432/TCP 82s\n\nWe can validate the access to the music service by using port forwarding to the Kubernetes Service.\n\nOpen a new terminal window and run the following command:\n\nkubectl port-forward service/music 8080:8080\n\nForwarding from 127.0.0.1:8080 -> 8080 Forwarding from [::1]:8080 -> 8080\n\nThe terminal is blocked and it’s normal until you stop the kubectl port-forward process. Thanks to port forwarding, we can access the music service using the local host address and port 8080.\n\nIn another terminal, curl the service:\n\ncurl localhost:8080/song\n\nThe request is sent to the music service deployed in Kubernetes and returns a list of songs:\n\n[ { \"id\": 1, \"artist\": \"DT\", \"name\": \"Quiero Munchies\" }, { \"id\": 2, \"artist\": \"Lin-Manuel Miranda\", \"name\": \"We Don't Talk About Bruno\" }, { \"id\": 3, \"artist\": \"Imagination\", \"name\": \"Just An Illusion\" }, { \"id\": 4, \"artist\": \"Txarango\", \"name\": \"Tanca Els Ulls\" }, { \"id\": 5,\n\n92\n\n|\n\nChapter 5: Helm",
      "content_length": 1274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "\"artist\": \"Halsey\", \"name\": \"Could Have Been Me\" } ]\n\n5.7 Triggering a Rolling Update Automatically\n\nProblem You want to trigger a rolling update of deployment when a ConfigMap object is changed.\n\nSolution Use the sha256sum template function to generate a change on the deployment file.\n\nIn Recipe 4.5, we saw that Kustomize has a ConfigMapGenerator that automatically appends a hash to the ConfigMap metadata name and modifies the deployment file with the new hash when used. Any change on the ConfigMap triggers a rolling update of the deployment.\n\nHelm doesn’t provide a direct way like Kustomize does to update a deployment file when the ConfigMap changes, but there is a template function to calculate a SHA-256 hash of any file and embed the result in the template.\n\nSuppose we’ve got a Node.js application that returns a greeting message. An environ‐ ment variable configures this greeting message, and in the Kubernetes Deployment, this variable is injected from a Kubernetes ConfigMap.\n\nFigure 5-3 shows an overview of the application.\n\nFigure 5-3. Greetings application overview\n\nLet’s create the Helm Chart for the Greetings application; note that we’re not covering the entire process of creating a Chart, but just the essential parts. You can refer to Recipe 5.1 to get started.\n\n5.7 Triggering a Rolling Update Automatically\n\n|\n\n93",
      "content_length": 1345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Create a deployment template that injects a ConfigMap as an environment variable. The following listing shows the file:\n\napiVersion: apps/v1 kind: Deployment metadata: name: {{ .Chart.Name}} labels: app.kubernetes.io/name: {{ .Chart.Name}} {{- if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{- end }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} spec: containers: - image: \"{{ .Values.image.repository }}:↳ {{ .Values.image.tag | default .Chart.AppVersion}}\" imagePullPolicy: {{ .Values.image.pullPolicy }} name: {{ .Chart.Name}} ports: - containerPort: {{ .Values.image.containerPort }} name: http protocol: TCP env: - name: GREETING valueFrom: configMapKeyRef: name: {{ .Values.configmap.name}} key: greeting\n\nConfigMap name\n\nProperty key of the ConfigMap\n\nThe initial ConfigMap file is shown in the following listing:\n\napiVersion: v1 kind: ConfigMap metadata: name: greeting-config data: greeting: Aloha\n\n94\n\n|\n\nChapter 5: Helm",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Sets ConfigMap name\n\nKey/value\n\nCreate a Kubernetes Service template to access the service:\n\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: {{ .Chart.Name }} name: {{ .Chart.Name }} spec: ports: - name: http port: {{ .Values.image.containerPort }} targetPort: {{ .Values.image.containerPort }} selector: app.kubernetes.io/name: {{ .Chart.Name }}\n\nUpdate the values.yaml file with the template configmap parameters:\n\nimage: repository: quay.io/gitops-cookbook/greetings tag: \"1.0.0\" pullPolicy: Always containerPort: 8080\n\nreplicaCount: 1\n\nconfigmap: name: greeting-config\n\nRefers to ConfigMap name\n\nFinally, install the Chart using the install command:\n\nhelm install greetings .\n\nWhen the Chart is deployed, use the kubectl port-forward command in one terminal to get access to the service:\n\nkubectl port-forward service/greetings 8080:8080\n\nAnd curl the service in another terminal window:\n\ncurl localhost:8080 Aloha Ada\n\nConfigured greeting is used\n\n5.7 Triggering a Rolling Update Automatically\n\n|\n\n95",
      "content_length": 1028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Now, let’s update the ConfigMap file to a new greeting message:\n\napiVersion: v1 kind: ConfigMap metadata: name: greeting-config data: greeting: Hola\n\nNew greeting message\n\nUpdate the appVersion field from the Chart.yaml file to 1.0.1 and upgrade the Chart by running the following command:\n\nhelm upgrade greetings .\n\nRestart the kubectl port-forward process and curl the service again:\n\ncurl localhost:8080 Aloha Alexandra\n\nGreeting message isn’t updated\n\nThe ConfigMap object is updated during the upgrade, but since there are no changes in the Deployment object, there is no restart of the pod; hence the environment variable is not set to the new value. Listing the pods shows no execution of the rolling update:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE greetings-64ddfcb649-m5pml 1/1 Running 0 2m21s\n\nAge value shows no rolling update\n\nFigure 5-4 summarizes the change.\n\nFigure 5-4. Greetings application with new configuration value\n\nLet’s use the sha256sum function to calculate an SHA-256 value of the configmap.yaml file content and set it as a pod annotation, which effectively triggers a rolling update as the pod definition has changed:\n\n96\n\n|\n\nChapter 5: Helm",
      "content_length": 1180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} annotations: checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\")↳ . | sha256sum }}\n\nIncludes the configmap.yaml file, calculates the SHA-256 value, and sets it as an annotation\n\nUpdate the ConfigMap again with a new value:\n\napiVersion: v1 kind: ConfigMap metadata: name: greeting-config data: greeting: Namaste\n\nNew greeting message\n\nUpdate the appVersion field from Chart.yaml to 1.0.1 and upgrade the Chart by running the following command:\n\nhelm upgrade greetings .\n\nRestart the kubectl port-forward process and curl the service again:\n\ncurl localhost:8080 Namaste Alexandra\n\nGreeting message is the new one\n\nList the pods deployed in the cluster again, and you’ll notice that a rolling update is happening:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE greetings-5c6b86dbbd-2p9bd 0/1 ContainerCreating 0 3s greetings-64ddfcb649-m5pml 1/1 Running 0 2m21s\n\nA rolling update is happening\n\nDescribe the pod to validate that the annotation with the SHA-256 value is present:\n\nkubectl describe pod greetings-5c6b86dbbd-s4n7b\n\n5.7 Triggering a Rolling Update Automatically\n\n|\n\n97",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "The output shows all pod parameters. The important one is the annotations placed at the top of the output showing the checksum/config annotation containing the calculated SHA-256 value:\n\nName: greetings-5c6b86dbbd-s4n7b Namespace: asotobue-dev Priority: -3 Priority Class Name: sandbox-users-pods Node: ip-10-0-186-34.ec2.internal/10.0.186.34 Start Time: Thu, 27 Jan 2022 11:55:02 +0100 Labels: app.kubernetes.io/name=greetings pod-template-hash=5c6b86dbbd Annotations: checksum/config:↳ 59e9100616a11d65b691a914cd429dc6011a34e02465173f5f53584b4aa7cba8\n\nCalculated value\n\nFigure 5-5 summarizes the elements that changed when the application was updated.\n\nFigure 5-5. Final overview of the Greetings application\n\n5.8 Final Thoughts In the previous chapter, we saw Kustomize; in this chapter, we’ve seen another tool to help deploy Kubernetes applications.\n\nWhen you need to choose between Kustomize or Helm, you might have questions on which one to use.\n\nIn our experience, the best way to proceed is with Kustomize for simple projects, where only simple changes might be required between new deployments.\n\nIf the project is complex with external dependencies, and several deployment param‐ eters, then Helm is a better option.\n\n98\n\n|\n\nChapter 5: Helm",
      "content_length": 1250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "CHAPTER 6 Cloud Native CI/CD\n\nIn the previous chapter you learned about Helm, a popular templating system for Kubernetes. All the recipes from previous chapters represent a common tooling for creating and managing containers for Kubernetes, and now it’s time to think about the automation on Kubernetes using such tools. Let’s move our focus to the cloud native continuous integration/continuous deployment (CI/CD).\n\nContinuous integration is an automated process that takes new code created by a developer and builds, tests, and runs that code. The cloud native CI refers to the model where cloud computing and cloud services are involved in this process. The benefits from this model are many, such as portable and reproducible workloads across clouds for highly scalable and on-demand use cases. And it also represents the building blocks for GitOps workflows as it enables automation through actions performed via Git.\n\nTekton is a popular open source implementation of a cloud native CI/CD system on top of Kubernetes. In fact, Tekton installs and runs as an extension on a Kubernetes cluster and comprises a set of Kubernetes Custom Resources that define the building blocks you can create and reuse for your pipelines.1 (See Recipe 6.1.)\n\nThe Tekton engine lives inside a Kubernetes cluster and through its API objects represents a declarative way to define the actions to perform. The core components such as Tasks and Pipelines can be used to create a pipeline to generate artifacts and/or containers from a Git repository (see Recipes 6.2, 6.3, and 6.4).\n\nTekton also supports a mechanism for automating the start of a Pipeline with Trig‐ gers. These allow you to detect and extract information from events from a variety of\n\n1 See the Tekton documentation.\n\n99",
      "content_length": 1771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "sources, such as a webhook, and to start Tasks or Pipelines accordingly (see Recipe 6.8).\n\nWorking with private Git repositories is a common use case that Tekton supports nicely (see Recipe 6.4), and building artifacts and creating containers can be done in many ways such as with Buildah (see Recipe 6.5) or Shipwright, which we discussed in Chapter 3. It is also possible to integrate Kustomize (see Recipe 6.9) and Helm (see Recipe 6.10) in order to make the CI part dynamic and take benefit of the rich ecosystem of Kubernetes tools.\n\nTekton is Kubernetes-native solution, thus it’s universal; however, it’s not the only cloud native CI/CD citizen in the market. Other good examples for GitOps-ready workloads are Drone (Recipe 6.11) and GitHub Actions (Recipe 6.12).\n\n6.1 Install Tekton\n\nProblem You want to install Tekton in order to have cloud native CI/CD on your Kubernetes cluster.\n\nSolution Tekton is a Kubernetes-native CI/CD solution that can be installed on top of any Kubernetes cluster. The installation brings you a set of Kubernetes Custom Resources (CRDs) that you can use to compose your Pipelines, as shown in Figure 6-1:\n\nTask\n\nA reusable, loosely coupled number of steps that perform a specific function (e.g., building a container image). Tasks get executed as Kubernetes pods, while steps in a Task map onto containers.\n\nPipeline\n\nA list Tasks needed to build and/or deploy your apps.\n\nTaskRun\n\nThe execution and result of running an instance of a Task.\n\nPipelineRun\n\nThe execution and result of running an instance of a Pipeline, which includes a number of TaskRuns.\n\nTrigger\n\nDetects an event and connects to other CRDs to specify what happens when such an event occurs.\n\n100\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1736,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Figure 6-1. Tekton Pipelines\n\nTo install Tekton, you just need kubectl CLI and a Kubernetes cluster such as Minikube (see Chapter 2).\n\nTekton has a modular structure. You can install all components separately or all at once (e.g., with an Operator):\n\nTekton Pipelines\n\nContains Tasks and Pipelines\n\nTekton Triggers\n\nContains Triggers and EventListeners\n\nTekton Dashboard\n\nA convenient dashboard to visualize Pipelines and logs\n\nTekton CLI\n\nA CLI to manage Tekton objects (start/stop Pipelines and Tasks, check logs)\n\n6.1 Install Tekton\n\n|\n\n101",
      "content_length": 543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "You can also use a Kubernetes Operator to install and manage Tekton components on your cluster. See more details on how from OperatorHub.\n\nFirst you need to install the Tekton Pipelines component. At the time of writing this book, we are using version 0.37.0:\n\nkubectl apply \\ -f https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.37.0/ release.yaml\n\nThe installation will create a new Kubernetes namespace called tekton-pipelines and you should see output similar to the following:\n\nnamespace/tekton-pipelines created podsecuritypolicy.policy/tekton-pipelines created clusterrole.rbac.authorization.k8s.io/tekton-pipelines-controller-cluster-access created clusterrole.rbac.authorization.k8s.io/tekton-pipelines-controller-tenant-access created clusterrole.rbac.authorization.k8s.io/tekton-pipelines-webhook-cluster-access cre- ated role.rbac.authorization.k8s.io/tekton-pipelines-controller created role.rbac.authorization.k8s.io/tekton-pipelines-webhook created role.rbac.authorization.k8s.io/tekton-pipelines-leader-election created role.rbac.authorization.k8s.io/tekton-pipelines-info created serviceaccount/tekton-pipelines-controller created serviceaccount/tekton-pipelines-webhook created clusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-cluster- access created clusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-tenant- access created clusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook-cluster- access created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-leaderelection created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook-leaderelection cre- ated rolebinding.rbac.authorization.k8s.io/tekton-pipelines-info created customresourcedefinition.apiextensions.k8s.io/clustertasks.tekton.dev created customresourcedefinition.apiextensions.k8s.io/pipelines.tekton.dev created customresourcedefinition.apiextensions.k8s.io/pipelineruns.tekton.dev created customresourcedefinition.apiextensions.k8s.io/resolutionrequests.resolution.tek- ton.dev created customresourcedefinition.apiextensions.k8s.io/pipelineresources.tekton.dev created customresourcedefinition.apiextensions.k8s.io/runs.tekton.dev created customresourcedefinition.apiextensions.k8s.io/tasks.tekton.dev created customresourcedefinition.apiextensions.k8s.io/taskruns.tekton.dev created secret/webhook-certs created\n\n102\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 2584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "validatingwebhookconfiguration.admissionregistration.k8s.io/validation.web- hook.pipeline.tekton.dev created mutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.pipeline.tek- ton.dev created validatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.pipe- line.tekton.dev created clusterrole.rbac.authorization.k8s.io/tekton-aggregate-edit created clusterrole.rbac.authorization.k8s.io/tekton-aggregate-view created configmap/config-artifact-bucket created configmap/config-artifact-pvc created configmap/config-defaults created configmap/feature-flags created configmap/pipelines-info created configmap/config-leader-election created configmap/config-logging created configmap/config-observability created configmap/config-registry-cert created deployment.apps/tekton-pipelines-controller created service/tekton-pipelines-controller created horizontalpodautoscaler.autoscaling/tekton-pipelines-webhook created deployment.apps/tekton-pipelines-webhook created service/tekton-pipelines-webhook created\n\nYou can monitor and verify the installation with the following command:\n\nkubectl get pods -w -n tekton-pipelines\n\nYou should see output like this:\n\nNAME READY STATUS RESTARTS AGE tekton-pipelines-controller-5fd68749f5-tz8dv 1/1 Running 0 3m4s tekton-pipelines-webhook-58dcdbfd9b-hswpk 1/1 Running 0 3m4s\n\nThe preceding command goes in watch mode, thus it remains appended. Press Ctrl+C in order to stop it when you see the con‐ troller and webhook pods in Running status.\n\nThen you can install Tekton Triggers. At the time of writing this book, we are using version 0.20.1:\n\nkubectl apply \\ -f https://storage.googleapis.com/tekton-releases/triggers/previous/v0.20.1/ release.yaml\n\nYou should see the following output:\n\npodsecuritypolicy.policy/tekton-triggers created clusterrole.rbac.authorization.k8s.io/tekton-triggers-admin created clusterrole.rbac.authorization.k8s.io/tekton-triggers-core-interceptors created clusterrole.rbac.authorization.k8s.io/tekton-triggers-core-interceptors-secrets created clusterrole.rbac.authorization.k8s.io/tekton-triggers-eventlistener-roles created\n\n6.1 Install Tekton\n\n|\n\n103",
      "content_length": 2146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "104\n\nclusterrole.rbac.authorization.k8s.io/tekton-triggers-eventlistener-clusterroles created role.rbac.authorization.k8s.io/tekton-triggers-admin created role.rbac.authorization.k8s.io/tekton-triggers-admin-webhook created role.rbac.authorization.k8s.io/tekton-triggers-core-interceptors created role.rbac.authorization.k8s.io/tekton-triggers-info created serviceaccount/tekton-triggers-controller created serviceaccount/tekton-triggers-webhook created serviceaccount/tekton-triggers-core-interceptors created clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-controller-admin cre- ated clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-webhook-admin created clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-core-interceptors cre- ated clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-core-interceptors- secrets created rolebinding.rbac.authorization.k8s.io/tekton-triggers-controller-admin created rolebinding.rbac.authorization.k8s.io/tekton-triggers-webhook-admin created rolebinding.rbac.authorization.k8s.io/tekton-triggers-core-interceptors created rolebinding.rbac.authorization.k8s.io/tekton-triggers-info created customresourcedefinition.apiextensions.k8s.io/clusterinterceptors.triggers.tek- ton.dev created customresourcedefinition.apiextensions.k8s.io/clustertriggerbindings.triggers.tek- ton.dev created customresourcedefinition.apiextensions.k8s.io/eventlisteners.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/triggers.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/triggerbindings.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/triggertemplates.triggers.tekton.dev created secret/triggers-webhook-certs created validatingwebhookconfiguration.admissionregistration.k8s.io/validation.web- hook.triggers.tekton.dev created mutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.triggers.tek- ton.dev created validatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.trig- gers.tekton.dev created clusterrole.rbac.authorization.k8s.io/tekton-triggers-aggregate-edit created clusterrole.rbac.authorization.k8s.io/tekton-triggers-aggregate-view created configmap/config-defaults-triggers created configmap/feature-flags-triggers created configmap/triggers-info created configmap/config-logging-triggers created configmap/config-observability-triggers created service/tekton-triggers-controller created deployment.apps/tekton-triggers-controller created service/tekton-triggers-webhook created deployment.apps/tekton-triggers-webhook created deployment.apps/tekton-triggers-core-interceptors created service/tekton-triggers-core-interceptors created clusterinterceptor.triggers.tekton.dev/cel created clusterinterceptor.triggers.tekton.dev/bitbucket created clusterinterceptor.triggers.tekton.dev/github created\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 2910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "clusterinterceptor.triggers.tekton.dev/gitlab created secret/tekton-triggers-core-interceptors-certs created\n\nYou can monitor and verify the installation with the following command:\n\nkubectl get pods -w -n tekton-pipelines\n\nYou should see three new pods created and running—tekton-triggers-controller, tekton-triggers-core-interceptors, and tekton-triggers-webhook:\n\nNAME READY STATUS RESTARTS AGE tekton-pipelines-controller-5fd68749f5-tz8dv 1/1 Running 0 27m tekton-pipelines-webhook-58dcdbfd9b-hswpk 1/1 Running 0 27m tekton-triggers-controller-854d44fd5d-8jf9q 1/1 Running 0 105s tekton-triggers-core-interceptors-5454f8785f-dhsrb 1/1 Running 0 104s tekton-triggers-webhook-86d75f875-zmjf4 1/1 Running 0 105s\n\nAfter this you have a fully working Tekton installation on top of your Kubernetes cluster, supporting Pipelines and automation via event Triggers. In addition to that, you could install the Tekton Dashboard in order to visualize Tasks, Pipelines, and logs via a nice UI. At the time of writing this book, we are using version 0.28.0:\n\nkubectl apply \\ -f https://storage.googleapis.com/tekton-releases/dashboard/previous/v0.28.0/ tekton-dashboard-release.yaml\n\nYou should have output similar to the following:\n\ncustomresourcedefinition.apiextensions.k8s.io/extensions.dashboard.tekton.dev cre- ated serviceaccount/tekton-dashboard created role.rbac.authorization.k8s.io/tekton-dashboard-info created clusterrole.rbac.authorization.k8s.io/tekton-dashboard-backend created clusterrole.rbac.authorization.k8s.io/tekton-dashboard-tenant created rolebinding.rbac.authorization.k8s.io/tekton-dashboard-info created clusterrolebinding.rbac.authorization.k8s.io/tekton-dashboard-backend created configmap/dashboard-info created service/tekton-dashboard created deployment.apps/tekton-dashboard created clusterrolebinding.rbac.authorization.k8s.io/tekton-dashboard-tenant created\n\nYou can monitor and verify the installation with the following command:\n\nkubectl get pods -w -n tekton-pipelines\n\nYou should see a new pod created and running—tekton-dashboard:\n\nNAME READY STATUS RESTARTS AGE tekton-dashboard-786b6b5579-sscgz 1/1 Running 0\n\n6.1 Install Tekton\n\n|\n\n105",
      "content_length": 2169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "2m25s tekton-pipelines-controller-5fd68749f5-tz8dv 1/1 Running 1 (7m16s ago) 5d7h tekton-pipelines-webhook-58dcdbfd9b-hswpk 1/1 Running 1 (7m6s ago) 5d7h tekton-triggers-controller-854d44fd5d-8jf9q 1/1 Running 2 (7m9s ago) 5d7h tekton-triggers-core-interceptors-5454f8785f-dhsrb 1/1 Running 1 (7m7s ago) 5d7h tekton-triggers-webhook-86d75f875-zmjf4 1/1 Running 2 (7m9s ago) 5d7h\n\nBy default, the Dashboard is not exposed outside the Kubernetes cluster. You can access it by using the following command:\n\nkubectl port-forward svc/tekton-dashboard 9097:9097 -n tekton-pipelines\n\nThere are several ways to expose internal services in Kubernetes; you could also create an Ingress for that as shown in the Tekton Dashboard documentation.\n\nYou can now browse to http://localhost:9097 to access your Dashboard, as shown in Figure 6-2.\n\nYou can download and install the Tekton CLI for your OS to start creating Tasks and Pipelines from the command line. At the time of writing this book, we are using version 0.25.0.\n\nFigure 6-2. Tekton Dashboard\n\nFinally, verify that tkn and Tekton are configured correctly:\n\n106\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "tkn version\n\nYou should get the following output:\n\nClient version: 0.25.0 Pipeline version: v0.37.0 Triggers version: v0.20.1 Dashboard version: v0.28.0\n\nSee Also\n\n• Tekton Getting Started\n\n6.2 Create a Hello World Task\n\nProblem You want to start using Tekton by exploring Tasks and creating a sample one.\n\nSolution In Tekton, a Task defines a series of steps that run sequentially to perform logic that the Task requires. Every Task runs as a pod on your Kubernetes cluster, with each step running in its own container. While steps within a Task are sequential, Tasks can be executed inside a Pipeline in parallel. Therefore, Tasks are the building blocks for running Pipelines with Tekton.\n\nLet’s create a Hello World Task:\n\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: hello spec: steps: - name: say-hello image: registry.access.redhat.com/ubi8/ubi command: - /bin/bash args: ['-c', 'echo Hello GitOps Cookbook reader!']\n\nThe API as an object of kind Task\n\nThe name of the Task\n\nThe list of steps contained within this Task, in this case just one\n\nThe name of the step\n\n6.2 Create a Hello World Task\n\n|\n\n107",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "The container image where the step starts\n\nFirst you need to create this resource in Kubernetes:\n\nkubectl create -f helloworld-task.yaml\n\nYou should get the following output:\n\ntask.tekton.dev/hello created\n\nYou can verify that the object has been created in your current Kubernetes namespace:\n\nkubectl get tasks\n\nYou should get output similar to the following:\n\nNAME AGE hello 90s\n\nNow you can start your Tekton Task with tkn CLI:\n\ntkn task start --showlog hello\n\nYou should get output similar to the following:\n\nTaskRun started: hello-run-8bmzz Waiting for logs to be available... [say-hello] Hello World\n\nA TaskRun is the API representation of a running Task. See Recipe 6.3 for more details.\n\nSee Also\n\n• Tekton Task documentation\n\n6.3 Create a Task to Compile and Package an App from Git\n\nProblem You want to automate compiling and packaging an app from Git on Kubernetes with Tekton.\n\nSolution As seen in Recipe 6.2, Tekton Tasks have a flexible mechanism to add a list of sequential steps to automate actions. The idea is to create a list of Tasks with a chain\n\n108\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "of input/output that can be used to compose Pipelines. Therefore a Task can contain a series of optional fields for a better control over the resource:\n\ninputs\n\nThe resources ingested by the Task.\n\noutputs\n\nThe resources produced by the Task.\n\nparams\n\nThe parameters that will be used in the Task steps. Each parameter has:\n\nname\n\nThe name of the parameter.\n\ndescription\n\nThe description of the parameter.\n\ndefault\n\nThe default value of the parameter.\n\nresults\n\nThe names under which Tasks write execution results.\n\nworkspaces\n\nThe paths to volumes needed by the Task.\n\nvolumes\n\nThe Task can also mount external volumes using the volumes attribute.\n\nThe following example, as illustrated in Figure 6-3, shows a Task named build-app that clones the sources using the git command and lists the source code in output.\n\nFigure 6-3. build-app Task\n\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: build-app spec: workspaces: - name: source description: The git repo will be cloned onto the volume backing this work\n\n6.3 Create a Task to Compile and Package an App from Git\n\n|\n\n109",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "110\n\nspace params: - name: contextDir description: the context dir within source default: quarkus - name: tlsVerify description: tls verify type: string default: \"false\" - name: url default: https://github.com/gitops-cookbook/tekton-tutorial-greeter.git - name: revision default: master - name: subdirectory default: \"\" - name: sslVerify description: defines if http.sslVerify should be set to true or false in the global git config type: string default: \"false\" steps: - image: 'gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git- init:v0.21.0' name: clone resources: {} script: | CHECKOUT_DIR=\"$(workspaces.source.path)/$(params.subdirectory)\" cleandir() { # Delete any existing contents of the repo directory if it exists. # # We don't just \"rm -rf $CHECKOUT_DIR\" because $CHECKOUT_DIR might be \"/\" # or the root of a mounted volume. if [[ -d \"$CHECKOUT_DIR\" ]] ; then # Delete non-hidden files and directories rm -rf \"$CHECKOUT_DIR\"/* # Delete files and directories starting with . but excluding .. rm -rf \"$CHECKOUT_DIR\"/.[!.]* # Delete files and directories starting with .. plus any other charac ter rm -rf \"$CHECKOUT_DIR\"/..?* fi } /ko-app/git-init \\ -url \"$(params.url)\" \\ -revision \"$(params.revision)\" \\ -path \"$CHECKOUT_DIR\" \\ -sslVerify=\"$(params.sslVerify)\" cd \"$CHECKOUT_DIR\" RESULT_SHA=\"$(git rev-parse HEAD)\" - name: build-sources image: gcr.io/cloud-builders/mvn command: - mvn args:\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "-DskipTests - clean - install env: - name: user.home value: /home/tekton workingDir: \"/workspace/source/$(params.contextDir)\"\n\nA Task step and Pipeline Task can share a common filesystem via a Tekton workspace. The workspace could be either backed by something like Persistent‐ VolumeClaim (PVC) and a ConfigMap, or just ephemeral (emptyDir).\n\nA Task can have parameters; this feature makes the execution dynamic.\n\nLet’s create the Task with the following command:\n\nkubectl create -f build-app-task.yaml\n\nYou should get output similar to the following:\n\ntask.tekton.dev/build-app created\n\nYou can verify that the object has been created in your current Kubernetes namespace:\n\nkubectl get tasks\n\nYou should get output similar to the following:\n\nNAME AGE build-app 3s\n\nYou can also list the Task with the tkn CLI:\n\ntkn task ls\n\nYou should get output similar to the following:\n\nNAME DESCRIPTION AGE build-app 10 seconds ago\n\nWhen you start a Task, a new TaskRun object is created. TaskRuns are the API representation of a running Task; thus you can create it with the tkn CLI using the following command:\n\ntkn task start build-app \\ --param contextDir='quarkus' \\ --workspace name=source,emptyDir=\"\" \\ --showlog\n\n6.3 Create a Task to Compile and Package an App from Git\n\n|\n\n111",
      "content_length": 1274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "When parameters are used inside a Task or Pipeline, you will be prompted to add new values or confirm default ones, if any. In order to use the default values from the Task defintion without prompting for values, you can use the --use-param-defaults option.\n\nYou should get output similar to the following:\n\n? Value for param `tlsVerify` of type `string`? (Default is `false`) false ? Value for param `url` of type `string`? (Default is `https:// github.com/gitops-cookbook/tekton-tutorial-greeter.git`) https://github.com/gitops- cookbook/tekton-tutorial-greeter.git ? Value for param `revision` of type `string`? (Default is `master`) master ? Value for param `subdirectory` of type `string`? (Default is ``) ? Value for param `sslVerify` of type `string`? (Default is `false`) false TaskRun started: build-app-run-rzcd8 Waiting for logs to be available... [clone] {\"level\":\"info\",\"ts\":1659278019.0018234,\"caller\":\"git/ git.go:169\",\"msg\":\"Successfully cloned https://github.com/gitops-cookbook/tekton- tutorial-greeter.git @ d9291c456db1ce29177b77ffeaa9b71ad80a50e6 (grafted, HEAD, ori gin/master) in path /workspace/source/\"} [clone] {\"level\":\"info\",\"ts\":1659278019.0227938,\"caller\":\"git/ git.go:207\",\"msg\":\"Successfully initialized and updated submodules in path /work space/source/\"}\n\n[build-sources] [INFO] Scanning for projects... [build-sources] Downloading from central: https://repo.maven.apache.org/maven2/io/ quarkus/quarkus-universe-bom/1.6.1.Final/quarkus-universe-bom-1.6.1.Final.pom Downloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/quarkus- universe-bom/1.6.1.Final/quarkus-universe-bom-1.6.1.Final.pom (412 kB at 118 kB/s) [build-sources] [INFO] ... [build-sources] [INFO] Installing /workspace/source/quarkus/target/tekton-quarkus- greeter.jar to /root/.m2/repository/com/redhat/developers/tekton-quarkus-greeter/ 1.0.0-SNAPSHOT/tekton-quarkus-greeter-1.0.0-SNAPSHOT.jar [build-sources] [INFO] Installing /workspace/source/quar- kus/pom.xml to /root/.m2/repository/com/redhat/developers/tekton-quarkus-greeter/ 1.0.0-SNAPSHOT/tekton-quarkus-greeter-1.0.0-SNAPSHOT.pom [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] BUILD SUCCESS [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] Total time: 04:41 min [build-sources] [INFO] Finished at: 2022-07-31T14:38:22Z [build-sources] [INFO] ------------------------------------------------------------------------\n\nOr, you can create a TaskRun object manually like this:\n\napiVersion: tekton.dev/v1beta1 kind: TaskRun metadata:\n\n112\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 2688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "generateName: build-app-run- labels: app.kubernetes.io/managed-by: tekton-pipelines tekton.dev/task: build-app spec: params: - name: contextDir value: quarkus - name: revision value: master - name: sslVerify value: \"false\" - name: subdirectory value: \"\" - name: tlsVerify value: \"false\" - name: url value: https://github.com/gitops-cookbook/tekton-tutorial-greeter.git taskRef: kind: Task name: build-app workspaces: - emptyDir: {} name: source\n\nIf you don’t want to specify a name for each TaskRun, you can use the generate Name attribute to let Tekton pick a random one from the string you defined.\n\nHere you list the Task that the TaskRun is referring to.\n\nAnd start it in this way:\n\nkubectl create -f build-app-taskrun.yaml\n\nYou should get output similar to the following:\n\ntaskrun.tekton.dev/build-app-run-65vmh created\n\nYou can also verify it with the tkn CLI:\n\ntkn taskrun ls\n\nYou should get output similar to the following:\n\nNAME STARTED DURATION STATUS build-app-run-65vmh 1 minutes ago 2m37s Succeeded build-app-run-rzcd8 2 minutes ago 3m58s Succeeded\n\nYou can get the logs from the TaskRun by specifying the name of the TaskRun:\n\ntkn taskrun logs build-app-run-65vmh -f\n\n6.3 Create a Task to Compile and Package an App from Git\n\n|\n\n113",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "See Also Debugging a TaskRun\n\n6.4 Create a Task to Compile and Package an App from Private Git\n\nProblem You want to use a private Git repository to automate compiling and packaging of an app on Kubernetes with Tekton.\n\nSolution In Recipe 6.3 you saw how to compile and package a sample Java application using a public Git repository, but most of the time people deal with private repos at work, so how do you integrate them? Tekton supports the following authentication schemes for use with Git:\n\n• Basic-auth\n\nSSH•\n\nWith both options you can use a Kubernetes Secret to store your credentials and attach them to the ServiceAccount running your Tekton Tasks or Pipelines.\n\nTekton uses a default service account, however you can override it following the documentation here.\n\nLet’s start with a common example of basic authentication and a popular Git service such as GitHub.\n\nGitHub uses personal access tokens (PATs) as an alternative to using passwords for authentication. You can use a PAT instead of a clear-text password to enhance security.\n\n114\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "First you need to create a Secret. You can do this by creating the following YAML file:\n\napiVersion: v1 kind: Secret metadata: name: github-secret annotations: tekton.dev/git-0: https://github.com type: kubernetes.io/basic-auth stringData: username: YOUR_USERNAME password: YOUR_PASSWORD\n\nHere you specify the URL for which Tekton will use this Secret, in this case GitHub\n\nThis is the type of Secret, in this case a basic authentication one\n\nYour Git user, in this case your GitHub user\n\nYou Git password, in this case your GitHub personal access token\n\nYou can now create the Secret with the following command:\n\nkubectl create -f git-secret.yaml\n\nYou should get the following output:\n\nsecret/git-secret created\n\nYou can also avoid writing YAML and do everything with kubectl as follows:\n\nkubectl create secret generic git-secret \\ --type=kubernetes.io/basic-auth \\ --from-literal=username=YOUR_USERNAME \\ --from-literal=password=YOUR_PASSWORD\n\nAnd then you just annotate the Secret as follows:\n\nkubectl annotate secret git-secret \"tekton.dev/git-0=https://github.com\"\n\nOnce you have created and annotated your Secret, you have to attach it to the ServiceAccount running your Tekton Tasks or Pipelines.\n\nLet’s create a new ServiceAccount for this purpose:\n\napiVersion: v1 kind: ServiceAccount metadata: name: tekton-bot-sa secrets: - name: git-secret\n\nList of Secrets attached to this ServiceAccount\n\n6.4 Create a Task to Compile and Package an App from Private Git\n\n|\n\n115",
      "content_length": 1474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "kubectl create -f tekton-bot-sa.yaml\n\nYou should get the following output:\n\nserviceaccount/tekton-bot-sa created\n\nYou can create the ServiceAccount directly with kubectl as follows:\n\nkubectl create serviceaccount tekton-bot-sa\n\nand then patch it to add the secret reference:\n\nkubectl patch serviceaccount tekton-bot-sa -p '{\"secrets\": [{\"name\": \"git-secret\"}]}'\n\nOnce credentials are set up and linked to the ServiceAccount running Tasks or Pipe‐ lines, you can just add the --serviceaccount=<NAME> option to your tkn command, using the Recipe 6.3 example:\n\ntkn task start build-app \\ --serviceaccount='tekton-bot-sa' \\ --param url='https://github.com/gitops-cookbook/tekton-greeter-private.git' \\ --param contextDir='quarkus' \\ --workspace name=source,emptyDir=\"\" \\ --showlog\n\nHere you specify the ServiceAccount to use; this will override the default one at runtime.\n\nHere you can override the default repository with one of your choice. In this example there’s a private repository that you cannot access, but you can create a private repository on your own and test it like this.\n\nYou should get output similar to the following:\n\n... [clone] {\"level\":\"info\",\"ts\":1659354692.1365478,\"caller\":\"git/ git.go:169\",\"msg\":\"Successfully cloned https://github.com/gitops-cookbook/tekton- greeter-private.git @ 5250e1fa185805373e620d1c04a0c48129efd2ee (grafted, HEAD, ori gin/master) in path /workspace/source/\"} [clone] {\"level\":\"info\",\"ts\":1659354692.1546066,\"caller\":\"git/ git.go:207\",\"msg\":\"Successfully initialized and updated submodules in path /work space/source/\"} ... [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] BUILD SUCCESS [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] Total time: 04:30 min [build-sources] [INFO] Finished at: 2022-07-31T15:30:01Z\n\n116\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "[build-sources] [INFO] ------------------------------------------------------------------------\n\nSee Also\n\n• Tekton Authentication\n\n6.5 Containerize an Application Using a Tekton Task and Buildah\n\nProblem You want to compile, package, and containerize your app with a Tekton Task on Kubernetes.\n\nSolution Automation is essential when adopting the cloud native approach, and if you decide to use Kubernetes for your CI/CD workloads, you need to provide a way to package and deploy your applications.\n\nIn fact, Kubernetes per se doesn’t have a built-in mechanism to build containers; it just relies on add-ons such as Tekton or external services for this purpose. That’s why in Chapter 3 we did an overview on how to create containers for packaging applications with various open source tools. In Recipe 3.3 we used Buildah to create a container from a Dockerfile.\n\nThanks to Tekton’s extensible model, you can reuse the same Task defined in Recipe 6.3 to add a step to create a container using the outcomes from the previous steps, as shown in Figure 6-4.\n\nFigure 6-4. Build Push app\n\nThe container can be pushed to a public container registry such as DockerHub or Quay.io, or to a private container registry. Similar to what we have seen in Recipe 6.4 for private Git repositories, pushing a container image to a container registry needs authentication. A Secret needs to be attached to the ServiceAccount running the Task as follows. See Chapter 2 for how to register and use a public registry.\n\n6.5 Containerize an Application Using a Tekton Task and Buildah\n\n|\n\n117",
      "content_length": 1568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "kubectl create secret docker-registry container-registry-secret \\ --docker-server='YOUR_REGISTRY_SERVER' \\ --docker-username='YOUR_REGISTRY_USER' \\ --docker-password='YOUR_REGISTRY_PASS'\n\nsecret/container-registry-secret created\n\nVerify it is present and check that the Secret is of type kubernetes.io/dockercon figjson:\n\nkubectl get secrets\n\nYou should get the following output:\n\nNAME TYPE DATA AGE container-registry-secret kubernetes.io/dockerconfigjson 1 1s\n\nLet’s create a ServiceAccount for this Task:\n\nkubectl create serviceaccount tekton-registry-sa\n\nThen let’s add the previously generated Secret to this ServiceAccount:\n\nkubectl patch serviceaccount tekton-registry-sa \\ -p '{\"secrets\": [{\"name\": \"container-registry-secret\"}]}'\n\nYou should get the following output:\n\nserviceaccount/tekton-registry-sa patched\n\nLet’s add a new step to create a container image and push it to a container registry. In the following example we use the book’s organization repos at Quay.io—quay.io/ gitops-cookbook/tekton-greeter:latest:\n\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: build-push-app spec: workspaces: - name: source description: The git repo will be cloned onto the volume backing this work space params: - name: contextDir description: the context dir within source default: quarkus - name: tlsVerify description: tls verify type: string default: \"false\" - name: url default: https://github.com/gitops-cookbook/tekton-tutorial-greeter.git - name: revision default: master - name: subdirectory\n\n118\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "default: \"\" - name: sslVerify description: defines if http.sslVerify should be set to true or false in the global git config type: string default: \"false\" - name: storageDriver type: string description: Storage driver default: vfs - name: destinationImage description: the fully qualified image name default: \"\" steps: - image: 'gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git- init:v0.21.0' name: clone resources: {} script: | CHECKOUT_DIR=\"$(workspaces.source.path)/$(params.subdirectory)\" cleandir() { # Delete any existing contents of the repo directory if it exists. # # We don't just \"rm -rf $CHECKOUT_DIR\" because $CHECKOUT_DIR might be \"/\" # or the root of a mounted volume. if [[ -d \"$CHECKOUT_DIR\" ]] ; then # Delete non-hidden files and directories rm -rf \"$CHECKOUT_DIR\"/* # Delete files and directories starting with . but excluding .. rm -rf \"$CHECKOUT_DIR\"/.[!.]* # Delete files and directories starting with .. plus any other charac ter rm -rf \"$CHECKOUT_DIR\"/..?* fi } /ko-app/git-init \\ -url \"$(params.url)\" \\ -revision \"$(params.revision)\" \\ -path \"$CHECKOUT_DIR\" \\ -sslVerify=\"$(params.sslVerify)\" cd \"$CHECKOUT_DIR\" RESULT_SHA=\"$(git rev-parse HEAD)\" - name: build-sources image: gcr.io/cloud-builders/mvn command: - mvn args: - -DskipTests - clean - install env: - name: user.home value: /home/tekton workingDir: \"/workspace/source/$(params.contextDir)\"\n\n6.5 Containerize an Application Using a Tekton Task and Buildah\n\n|\n\n119",
      "content_length": 1460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "name: build-and-push-image image: quay.io/buildah/stable script: | #!/usr/bin/env bash buildah --storage-driver=$STORAGE_DRIVER --tls-verify=$(params.tlsVerify) bud --layers -t $DESTINATION_IMAGE $CONTEXT_DIR buildah --storage-driver=$STORAGE_DRIVER --tls-verify=$(params.tlsVerify) push $DESTINATION_IMAGE docker://$DESTINATION_IMAGE env: - name: DESTINATION_IMAGE value: \"$(params.destinationImage)\" - name: CONTEXT_DIR value: \"/workspace/source/$(params.contextDir)\" - name: STORAGE_DRIVER value: \"$(params.storageDriver)\" workingDir: \"/workspace/source/$(params.contextDir)\" volumeMounts: - name: varlibc mountPath: /var/lib/containers volumes: - name: varlibc emptyDir: {}\n\nLet’s create this Task:\n\nkubectl create -f build-push-app.yaml\n\nYou should get the following output:\n\ntask.tekton.dev/build-push-app created\n\nNow let’s start the Task with the Buildah step creating a container image and with a new parameter destinationImage to specify where to push the resulting container image:\n\ntkn task start build-push-app \\ --serviceaccount='tekton-registry-sa' \\ --param url='https://github.com/gitops-cookbook/tekton-tutorial-greeter.git' \\ --param destinationImage='quay.io/gitops-cookbook/tekton-greeter:latest' \\ --param contextDir='quarkus' \\ --workspace name=source,emptyDir=\"\" \\ --use-param-defaults \\ --showlog\n\nHere you can place your registry; in this example we are using the book’s organi‐ zation repos at Quay.io.\n\nYou should get output similar to the following:\n\n... Downloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/ plexus-utils/3.0.5/plexus-utils-3.0.5.jar (230 kB at 301 kB/s) [build-sources] [INFO] Installing /workspace/source/quarkus/target/tekton-quarkus- greeter.jar to /root/.m2/repository/com/redhat/developers/tekton-quarkus-greeter/ 1.0.0-SNAPSHOT/tekton-quarkus-greeter-1.0.0-SNAPSHOT.jar\n\n120\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1890,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "[build-sources] [INFO] Installing /workspace/source/quar- kus/pom.xml to /root/.m2/repository/com/redhat/developers/tekton-quarkus-greeter/ 1.0.0-SNAPSHOT/tekton-quarkus-greeter-1.0.0-SNAPSHOT.pom [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] BUILD SUCCESS [build-sources] [INFO] ------------------------------------------------------------------------ [build-sources] [INFO] Total time: 02:59 min [build-sources] [INFO] Finished at: 2022-08-02T06:18:37Z [build-sources] [INFO] ------------------------------------------------------------------------ [build-and-push-image] STEP 1/2: FROM registry.access.redhat.com/ubi8/openjdk-11 [build-and-push-image] Trying to pull registry.access.redhat.com/ubi8/ openjdk-11:latest... [build-and-push-image] Getting image source signatures [build-and-push-image] Checking if image destination supports signatures [build-and-push-image] Copying blob sha256:1e09a5ee0038fbe06a18e7f355188bbabc387467144abcd435f7544fef395aa1 [build-and-push-image] Copying blob sha256:0d725b91398ed3db11249808d89e688e62e511bbd4a2e875ed8493ce1febdb2c [build-and-push-image] Copying blob sha256:1e09a5ee0038fbe06a18e7f355188bbabc387467144abcd435f7544fef395aa1 [build-and-push-image] Copying blob sha256:0d725b91398ed3db11249808d89e688e62e511bbd4a2e875ed8493ce1febdb2c [build-and-push-image] Copying blob sha256:e441d34134fac91baa79be3e2bb8fb3dba71ba5c1ea012cb5daeb7180a054687 [build-and-push-image] Copying blob sha256:e441d34134fac91baa79be3e2bb8fb3dba71ba5c1ea012cb5daeb7180a054687 [build-and-push-image] Copying config sha256:0c308464b19eaa9a01c3fdd6b63a043c160d4eea85e461bbbb7d01d168f6d993 [build-and-push-image] Writing manifest to image destination [build-and-push-image] Storing signatures [build-and-push-image] STEP 2/2: COPY target/quarkus-app /deployments/ [build-and-push-image] COMMIT quay.io/gitops-cookbook/tekton-greeter:latest [build-and-push-image] --> 42fe38b4346 [build-and-push-image] Successfully tagged quay.io/gitops-cookbook/tekton- greeter:latest [build-and-push-image] 42fe38b43468c3ca32262dbea6fd78919aba2bd35981cd4f71391e07786c9e21 [build-and-push-image] Getting image source signatures [build-and-push-image] Copying blob sha256:647a854c512bad44709221b6b0973e884f29bcb5a380ee32e95bfb0189b620e6 [build-and-push-image] Copying blob sha256:f2ee6b2834726167d0de06f3bbe65962aef79855c5ede0d2ba93b4408558d9c9 [build-and-push-image] Copying blob sha256:8e0e04b5c700a86f4a112f41e7e767a9d7c539fe3391611313bf76edb07eeab1 [build-and-push-image] Copying blob sha256:69c55192bed92cbb669c88eb3c36449b64ac93ae466abfff2a575273ce05a39e [build-and-push-image] Copying config sha256:42fe38b43468c3ca32262dbea6fd78919aba2bd35981cd4f71391e07786c9e21 [build-and-push-image] Writing manifest to image destination [build-and-push-image] Storing signatures\n\n6.5 Containerize an Application Using a Tekton Task and Buildah\n\n|\n\n121",
      "content_length": 2922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "See Also\n\n• Buildah\n\n• Docker Authentication for Tekton\n\n6.6 Deploy an Application to Kubernetes Using a Tekton Task\n\nProblem You want to deploy an application from a container image to Kubernetes with a Tekton Task.\n\nSolution While in Recipes 6.3, 6.4, and 6.5 we have listed a Tekton Task that is useful for continuous integration (CI), in this recipe we’ll start having a look at the Continous Deployment (CD) part by deploying an existing container image to Kubernetes.\n\nWe can reuse the container image we created and pushed in Recipe 6.5, available at quay.io/gitops-cookbook/tekton-greeter:latest:\n\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: kubectl spec: params: - name: SCRIPT description: The kubectl CLI arguments to run type: string default: \"kubectl help\" steps: - name: oc image: quay.io/openshift/origin-cli:latest script: | #!/usr/bin/env bash\n\n$(params.SCRIPT)\n\nFor this example we are using kubectl from this container image, which also contains OpenShift CLI and it has an smaller size compared to gcr.io/cloud- builders/kubectl.\n\nLet’s create this Task:\n\nkubectl create -f kubectl-task.yaml\n\n122\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "You should get the following output:\n\ntask.tekton.dev/kubectl created\n\nAs discussed in Recipe 6.5, Tekton uses a default ServiceAccount for running Tasks and Pipelines, unless a specific one is defined at runtime or overridden at a global scope. The best practice is always to create a specific ServiceAccount for a particular action, so let’s create one named tekton-deployer-sa for this use case as follows:\n\nkubectl create serviceaccount tekton-deployer-sa\n\nYou should get the following output:\n\nserviceaccount/tekton-deployer-sa created\n\nA ServiceAccount needs permission to deploy an application to Kubernetes. Roles and RoleBindings are API objects used to map a certain permission to a user or a ServiceAccount.\n\nYou first define a Role named pipeline-role for the ServiceAccount running the Tekton Task with permissions to deploy apps:\n\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: task-role rules: - apiGroups: - \"\" resources: - pods - services - endpoints - configmaps - secrets verbs: - \"*\" - apiGroups: - apps resources: - deployments - replicasets verbs: - \"*\" - apiGroups: - \"\" resources: - pods verbs: - get - apiGroups: - apps resources: - replicasets\n\n6.6 Deploy an Application to Kubernetes Using a Tekton Task\n\n|\n\n123",
      "content_length": 1259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "verbs: - get\n\nNow you need to bind the Role to the ServiceAccount:\n\napiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: task-role-binding roleRef: kind: Role name: task-role apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: tekton-deployer-sa\n\nNow you can create the two resources as follows:\n\nkubectl create -f task-role.yaml kubectl create -f task-role-binding.yaml\n\nYou should get the following output:\n\nrole.rbac.authorization.k8s.io/task-role created rolebinding.rbac.authorization.k8s.io/task-role-binding created\n\nFinally, you can define a TaskRun as follows:\n\napiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: kubectl-taskrun spec: serviceAccountName: tekton-deployer-sa taskRef: name: kubectl params: - name: SCRIPT value: | kubectl create deploy tekton-greeter --image=quay.io/gitops-cookbook/ tekton-greeter:latest\n\nAnd run it in this way:\n\nkubectl create -f kubectl-taskrun.yaml\n\nYou should get the following output:\n\ntaskrun.tekton.dev/kubectl-run created\n\nYou can check the logs to see the results:\n\ntkn taskrun logs kubectl-run -f\n\nYou should get output similar to the following:\n\n124\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "? Select taskrun: kubectl-run started 9 seconds ago [oc] deployment.apps/tekton-greeter created\n\nAfter a few seconds you should see the Deployment in Ready state:\n\nkubectl get deploy\n\nNAME READY UP-TO-DATE AVAILABLE AGE tekton-greeter 1/1 1 0 30s\n\nThe first time might take a while due to the time it takes to pull the container image.\n\nCheck if the app is available, expose the Deployment, and forward Kubernetes traffic to your workstation to test it:\n\nkubectl expose deploy/tekton-greeter --port 8080 kubectl port-forward svc/tekton-greeter 8080:8080\n\nIn another terminal, run this command:\n\ncurl localhost:8080\n\nYou should see the following output:\n\nMeeow!! from Tekton ----\n\nSee Also\n\n• Tekton Task\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\nProblem You want to create a Pipeline to compile, package, and deploy an app on Kubernetes with Tekton.\n\nSolution In the previous recipes we have seen how to create Tasks to execute one or more steps sequentially to build apps. In this recipe we will discuss Tekton Pipelines, a collection of Tasks that you can define and compose in a specific order of execution, either sequentially or in parallel, as you can see in Figure 6-5.\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\n|\n\n125",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Figure 6-5. Tekton Pipelines flows\n\nTekton Pipelines supports parameters and a mechanism to exchange outcomes between different Tasks. For instance, using the examples shown in Recipes 6.5 and 6.6:\n\nkubectl patch serviceaccount tekton-deployer-sa \\ -p '{\"secrets\": [{\"name\": \"container-registry-secret\"}]}'\n\napiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: tekton-greeter-pipeline spec: params: - name: GIT_REPO type: string - name: GIT_REF type: string - name : DESTINATION_IMAGE type: string - name : SCRIPT type: string tasks: - name: build-push-app taskRef: name: build-push-app params: - name: url value: \"$(params.GIT_REPO)\" - name: revision value: \"$(params.GIT_REF)\" - name: destinationImage value: \"$(params.DESTINATION_IMAGE)\"\n\n126\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "workspaces: - name: source - name: deploy-app taskRef: name: kubectl params: - name: SCRIPT value: \"$(params.SCRIPT)\" workspaces: - name: source runAfter: - build-push-app workspaces: - name: source\n\nPipeline parameters\n\nA list of Tasks for the Pipeline\n\nThe exact name of the Task to use\n\nYou can decide the order with the runAfter field to indicate that a Task must execute after one or more other Tasks\n\nOne or more common Workspaces used to share data between Tasks\n\nLet’s create the Pipeline as follows:\n\nkubectl create -f tekton-greeter-pipeline.yaml\n\nYou should get the following output:\n\npipeline.tekton.dev/tekton-greeter-pipeline created\n\nSimilarly to TaskRuns, you can run this Pipeline by creating a PipelineRun resource as follows:\n\napiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: tekton-greeter-pipeline-run- spec: params: - name: GIT_REPO value: https://github.com/gitops-cookbook/tekton-tutorial-greeter.git - name: GIT_REF value: \"master\" - name: DESTINATION_IMAGE value: \"quay.io/gitops-cookbook/tekton-greeter:latest\" - name: SCRIPT value: | kubectl create deploy tekton-greeter --image=quay.io/gitops-cookbook/ tekton-greeter:latest\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\n|\n\n127",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "pipelineRef: name: tekton-greeter-pipeline workspaces: - name: source emptyDir: {}\n\nYou can run the Pipeline by creating this PipelineRun object as follows:\n\nkubectl create -f tekton-greeter-pipelinerun.yaml\n\nYou can check the status:\n\ntkn pipelinerun ls\n\nNAME STARTED DURATION STATUS tekton-greeter-pipeline-run-ntl5r 7 seconds ago --- Running\n\nNow that you have seen how to reuse existing Tasks within a Pipeline, it’s a good time to introduce the Tekton Hub, a web-based platform for developers to discover, share, and contribute Tasks and Pipelines for Tekton (see Figure 6-6).\n\nFigure 6-6. Tekton Hub\n\nYou can implement the same Pipeline with Tasks already available in the Hub. In our case, we have:\n\ngit-clone\n\nTask that clones a repo from the provided URL into the output Workspace.\n\nbuildah\n\nTask that builds source into a container image and can push it to a container registry.\n\n128\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "kubernetes-actions\n\nThe generic kubectl CLI task, which can be used to run all kinds of k8s commands.\n\nFirst let’s add them to our namespace as follows:\n\ntkn hub install task git-clone tkn hub install task maven tkn hub install task buildah tkn hub install task kubernetes-actions\n\nYou should get output similar to the following to confirm they are installed properly in your namespace:\n\nTask git-clone(0.7) installed in default namespace Task maven(0.2) installed in default namespace Task buildah(0.4) installed in default namespace Task kubernetes-actions(0.2) installed in default namespace\n\nYou can cross-check it with the following command:\n\nkubectl get tasks\n\nYou should get output similar to the following:\n\nNAME AGE ... buildah 50s git-clone 52s kubernetes-actions 49s maven 51s ...\n\nSome Tekton installations like the one made with the Operator for OpenShift Pipelines provide a common list of useful Tasks such as those just listed, provided as ClusterTasks. ClusterTasks are Tasks available for all namespaces within the Kubernetes cluster. Check if your installation already provides some with this command: kubectl get clustertasks.\n\nNow the Pipeline has four Tasks, as you can see in Figure 6-7.\n\nFigure 6-7. Pipeline\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\n|\n\n129",
      "content_length": 1311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "In this example you’ll see a PersistentVolumeClaim as a Workspace because here the data is shared among different Tasks so we need to persist it:\n\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: app-source-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi\n\nAs usual, you can create the resource with kubectl:\n\nkubectl create -f app-source-pvc.yaml\n\nYou should see the following output:\n\npersistentvolumeclaim/app-source-pvc created\n\nkubectl get pvc\n\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE app-source-pvc Bound pvc-e85ade46-aaca-4f3f-b644-d8ff99fd9d5e 1Gi RWO standard 61s\n\nIn Minikube you have a default StorageClass that provides dynamic storage for the cluster. If you are using another Kubernetes cluster, please make sure you have a dynamic storage support.\n\nThe Pipeline definition now is:\n\napiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: tekton-greeter-pipeline-hub spec: params: - default: https://github.com/gitops-cookbook/tekton-tutorial-greeter.git name: GIT_REPO type: string - default: master name: GIT_REF type: string - default: quay.io/gitops-cookbook/tekton-greeter:latest name: DESTINATION_IMAGE type: string - default: kubectl create deploy tekton-greeter --image=quay.io/gitops-cookbook/ tekton-greeter:latest\n\n130\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "name: SCRIPT type: string - default: ./Dockerfile name: CONTEXT_DIR type: string - default: . name: IMAGE_DOCKERFILE type: string - default: . name: IMAGE_CONTEXT_DIR type: string tasks: - name: fetch-repo params: - name: url value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REF) - name: deleteExisting value: \"true\" - name: verbose value: \"true\" taskRef: kind: Task name: git-clone workspaces: - name: output workspace: app-source - name: build-app params: - name: GOALS value: - -DskipTests - clean - package - name: CONTEXT_DIR value: $(params.CONTEXT_DIR) runAfter: - fetch-repo taskRef: kind: Task name: maven workspaces: - name: maven-settings workspace: maven-settings - name: source workspace: app-source - name: build-push-image params: - name: IMAGE value: $(params.DESTINATION_IMAGE) - name: DOCKERFILE value: $(params.IMAGE_DOCKERFILE) - name: CONTEXT\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\n|\n\n131",
      "content_length": 956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "value: $(params.IMAGE_CONTEXT_DIR) runAfter: - build-app taskRef: kind: Task name: buildah workspaces: - name: source workspace: app-source - name: deploy params: - name: script value: $(params.SCRIPT) runAfter: - build-push-image taskRef: kind: Task name: kubernetes-actions workspaces: - name: app-source - name: maven-settings\n\nLet’s create the resource:\n\nkubectl create -f tekton-greeter-pipeline-hub.yaml\n\nWe are using the same Secret and ServiceAccount defined in Recipe 6.5 to log in against Quay.io in order to push the container image.\n\nYou can now start the Pipeline as follows:\n\ntkn pipeline start tekton-greeter-pipeline-hub \\ --serviceaccount='tekton-deployer-sa' \\ --param GIT_REPO='https://github.com/gitops-cookbook/tekton-tutorial- greeter.git' \\ --param GIT_REF='master' \\ --param CONTEXT_DIR='quarkus' \\ --param DESTINATION_IMAGE='quay.io/gitops-cookbook/tekton-greeter:latest' \\ --param IMAGE_DOCKERFILE='quarkus/Dockerfile' \\ --param IMAGE_CONTEXT_DIR='quarkus' \\ --param SCRIPT='kubectl create deploy tekton-greeter --image=quay.io/gitops- cookbook/tekton-greeter:latest' \\ --workspace name=app-source,claimName=app-source-pvc \\ --workspace name=maven-settings,emptyDir=\"\" \\ --use-param-defaults \\ --showlog\n\n[fetch-repo : clone] + CHECKOUT_DIR=/workspace/output/ [fetch-repo : clone] + /ko-app/git-init '-url=https://github.com/gitops-cookbook/ tekton-tutorial-greeter.git' '-revision=master' '-refspec=' '-path=/workspace/out put/' '-sslVerify=true' '-submodules=true' '-depth=1' '-sparseCheckoutDirectories='\n\n132\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "[fetch-repo : clone] {\"level\":\"info\",\"ts\":1660819038.5526028,\"caller\":\"git/ git.go:170\",\"msg\":\"Successfully cloned https://github.com/gitops-cookbook/tekton- tutorial-greeter.git @ d9291c456db1ce29177b77ffeaa9b71ad80a50e6 (grafted, HEAD, ori gin/master) in path /workspace/output/\"} [fetch-repo : clone] {\"level\":\"info\",\"ts\":1660819038.5722632,\"caller\":\"git/ git.go:208\",\"msg\":\"Successfully initialized and updated submodules in path /work space/output/\"} [fetch-repo : clone] + cd /workspace/output/ [fetch-repo : clone] + git rev-parse HEAD [fetch-repo : clone] + RESULT_SHA=d9291c456db1ce29177b77ffeaa9b71ad80a50e6 [fetch-repo : clone] + EXIT_CODE=0 [fetch-repo : clone] + '[' 0 '!=' 0 ] [fetch-repo : clone] + printf '%s' d9291c456db1ce29177b77ffeaa9b71ad80a50e6 [fetch-repo : clone] + printf '%s' https://github.com/gitops-cookbook/tekton- tutorial-greeter.git ... [build-app : mvn-goals] [INFO] [org.jboss.threads] JBoss Threads version 3.1.1.Final [build-app : mvn-goals] [INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 1296ms [build-app : mvn-goals] [INFO] ------------------------------------------------------------------------ [build-app : mvn-goals] [INFO] BUILD SUCCESS [build-app : mvn-goals] [INFO] ------------------------------------------------------------------------ [build-app : mvn-goals] [INFO] Total time: 03:18 min [build-app : mvn-goals] [INFO] Finished at: 2022-08-18T10:31:00Z [build-app : mvn-goals] [INFO] ------------------------------------------------------------------------ [build-push-image : build] STEP 1/2: FROM registry.access.redhat.com/ubi8/ openjdk-11 [build-push-image : build] Trying to pull registry.access.redhat.com/ubi8/ openjdk-11:latest... [build-push-image : build] Getting image source signatures [build-push-image : build] Checking if image destination supports signatures [build-push-image : build] Copying blob sha256:e441d34134fac91baa79be3e2bb8fb3dba71ba5c1ea012cb5daeb7180a054687 [build-push-image : build] Copying blob sha256:1e09a5ee0038fbe06a18e7f355188bbabc387467144abcd435f7544fef395aa1 [build-push-image : build] Copying blob sha256:0d725b91398ed3db11249808d89e688e62e511bbd4a2e875ed8493ce1febdb2c [build-push-image : build] Copying blob sha256:e441d34134fac91baa79be3e2bb8fb3dba71ba5c1ea012cb5daeb7180a054687 [build-push-image : build] Copying blob sha256:1e09a5ee0038fbe06a18e7f355188bbabc387467144abcd435f7544fef395aa1 [build-push-image : build] Copying blob sha256:0d725b91398ed3db11249808d89e688e62e511bbd4a2e875ed8493ce1febdb2c [build-push-image : build] Copying config sha256:0c308464b19eaa9a01c3fdd6b63a043c160d4eea85e461bbbb7d01d168f6d993 [build-push-image : build] Writing manifest to image destination [build-push-image : build] Storing signatures [build-push-image : build] STEP 2/2: COPY target/quarkus-app /deployments/ [build-push-image : build] COMMIT quay.io/gitops-cookbook/tekton-greeter:latest [build-push-image : build] --> c07e36a8e61\n\n6.7 Create a Tekton Pipeline to Build and Deploy an App to Kubernetes\n\n|\n\n133",
      "content_length": 3034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "[build-push-image : build] Successfully tagged quay.io/gitops-cookbook/tekton- greeter:latest [build-push-image : build] c07e36a8e6104d2e5c7d79a6cd34cd7b44eb093c39ef6c1487a37d7bd2305b8a [build-push-image : build] Getting image source signatures [build-push-image : build] Copying blob sha256:7853a7797845542e3825d4f305e4784ea7bf492cd4364fc93b9afba3ac0c9553 [build-push-image : build] Copying blob sha256:8e0e04b5c700a86f4a112f41e7e767a9d7c539fe3391611313bf76edb07eeab1 [build-push-image : build] Copying blob sha256:647a854c512bad44709221b6b0973e884f29bcb5a380ee32e95bfb0189b620e6 [build-push-image : build] Copying blob sha256:69c55192bed92cbb669c88eb3c36449b64ac93ae466abfff2a575273ce05a39e [build-push-image : build] Copying config sha256:c07e36a8e6104d2e5c7d79a6cd34cd7b44eb093c39ef6c1487a37d7bd2305b8a [build-push-image : build] Writing manifest to image destination [build-push-image : build] Storing signatures [build-push-image : build] sha256:12dd3deb6305b9e125309b68418d0bb81f805e0fe7ac93942dc94764aee9f492quay.io/ gitops-cookbook/tekton-greeter:latest [deploy : kubectl] deployment.apps/tekton-greeter created\n\nYou can use the Tekton Dashboard to create and visualize your running Tasks and Pipelines as shown in Figure 6-8.\n\nFigure 6-8. Tekton Dashboard TaskRuns\n\n134\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "See Also\n\n• Tekton Catalog\n\n6.8 Using Tekton Triggers to Compile and Package an Application Automatically When a Change Occurs on Git\n\nProblem You want to automate your CI/CD Pipelines when a change on Git occurs.\n\nSolution Tekton Triggers is the Tekton component that brings automation for Tasks and Pipelines with Tekton. It is an interesting feature for a GitOps strategy for cloud native CI/CD as it supports external events from a large set of sources such as Git events (Git push or pull requests).\n\nMost Git repository servers support the concept of webhooks, calling to an external source via HTTP(S) when a change in the code repository happens. Tekton provides an API endpoint that supports receiving hooks from remote systems in order to trig‐ ger builds. By pointing the code repository’s hook at the Tekton resources, automated code/build/deploy pipelines can be achieved.\n\nThe installation of Tekton Triggers, which we discussed in Recipe 6.1, brings a set of CRDs to manage event handling for Tasks and Pipelines. In this recipe we will use the following, as illustrated also in Figure 6-9:\n\nFigure 6-9. Tekton Triggers\n\nTriggerTemplate\n\nA template for newly created resources. It supports parameters to create specific PipelineRuns.\n\nTriggerBinding\n\nValidates events and extracts payload fields.\n\n6.8 Using Tekton Triggers to Compile and Package an Application Automatically When a Change Occurs on Git\n\n|\n\n135",
      "content_length": 1426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "EventListener\n\nConnects TriggerBindings and TriggerTemplates into an addressable endpoint (the event sink). It uses the extracted event parameters from each Trigger Binding (and any supplied static parameters) to create the resources specified in the corresponding TriggerTemplate. It also optionally allows an external service to preprocess the event payload via the interceptor field.\n\nBefore creating these resources, you need to set up permissions to let Tekton Triggers create Pipelines and Tasks. You can use the setup available from the book’s repository with the following command:\n\nkubectl apply \\ -f https://raw.githubusercontent.com/gitops-cookbook/chapters/main/chapters/ch06/ rbac.yaml\n\nThis will create a new ServiceAccount named tekton-triggers-sa that has the per‐ missions needed to interact with the Tekton Pipelines component. As confirmation, from the previous command you should get the following output:\n\nserviceaccount/tekton-triggers-sa created rolebinding.rbac.authorization.k8s.io/triggers-example-eventlistener-binding con- figured clusterrolebinding.rbac.authorization.k8s.io/triggers-example-eventlistener- clusterbinding configured\n\nYou can now add automation to your Pipelines like the one we defined in Recipe 6.7 creating these three resources:\n\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerTemplate metadata: name: tekton-greeter-triggertemplate spec: params: - name: git-revision - name: git-commit-message - name: git-repo-url - name: git-repo-name - name: content-type - name: pusher-name resourcetemplates: - apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: labels: tekton.dev/pipeline: tekton-greeter-pipeline-hub name: tekton-greeter-pipeline-webhook-$(uid) spec: params: - name: GIT_REPO value: $(tt.params.git-repo-url) - name: GIT_REF\n\n136\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "value: $(tt.params.git-revision) serviceAccountName: tekton-triggers-example-sa pipelineRef: name: tekton-greeter-pipeline-hub workspaces: - name: app-source persistentVolumeClaim: claimName: app-source-pvc - name: maven-settings emptyDir: {}\n\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: tekton-greeter-triggerbinding spec: params: - name: git-repo-url value: $(body.repository.clone_url) - name: git-revision value: $(body.after)\n\napiVersion: triggers.tekton.dev/v1alpha1 kind: EventListener metadata: name: tekton-greeter-eventlistener spec: serviceAccountName: tekton-triggers-example-sa triggers: - bindings: - ref: tekton-greeter-triggerbinding template: ref: tekton-greeter-triggertemplate\n\nYou can create the resources just listed as follows:\n\nkubectl create -f tekton-greeter-triggertemplate.yaml kubectl create -f tekton-greeter-triggerbinding.yaml kubectl create -f tekton-greeter-eventlistener.yaml\n\nYou should get the following output:\n\ntriggertemplate.triggers.tekton.dev/tekton-greeter-triggertemplate created triggerbinding.triggers.tekton.dev/tekton-greeter-triggerbinding created eventlistener.triggers.tekton.dev/tekton-greeter-eventlistener created\n\nContextually, a new pod is created representing the EventListener:\n\nkubectl get pods\n\nYou should get output similar to the following:\n\nNAME READY STATUS RESTARTS AGE el-tekton-greeter-eventlistener-5db7b9fcf9-6nrgx 1/1 Running 0 10s\n\nThe EventListener pod listens for events at a specified port, and it is bound to a Kubernetes Service:\n\n6.8 Using Tekton Triggers to Compile and Package an Application Automatically When a Change Occurs on Git\n\n|\n\n137",
      "content_length": 1654,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "kubectl get svc\n\nYou should get output similar to the following:\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP↳ PORT(S) AGE el-tekton-greeter-eventlistener ClusterIP 10.100.36.199 <none> ↳ 8080/TCP,9000/TCP 10s ...\n\nIf you are running your Git server outside the cluster (e.g., GitHub or GitLab), you need to expose the Service, for example, with an Ingress. Afterwards you can configure webhooks on your Git server using the EventListener URL associated to your Ingress.\n\nWith Minikube you can add support for Ingresses with this com‐ mand: minikube addons enable ingress. Then you need to map a hostname for the Ingress.\n\nFor the purpose of this book we can just simulate the webhook as it would come from the Git server.\n\nFirst you can map the EventListener Service to your local networking with the following command:\n\nkubectl port-forward svc/el-tekton-greeter-eventlistener 8080\n\nThen you can invoke the Trigger by making an HTTP request to http://localhost:8080 using curl. The HTTP request must be a POST request containing a JSON payload and it should contain the fields referenced via a TriggerBinding. In our case we mapped body.repository.clone_url and body.after.\n\nCheck the documentation of your Git server to get the list of parameters that a webhook can generate. In this example we are using the GitHub Webhooks reference.\n\nTo test Triggers, run this command:\n\ncurl -X POST \\ http://localhost:8080 \\ -H 'Content-Type: application/json' \\ -d '{ \"after\": \"d9291c456db1ce29177b77ffeaa9b71ad80a50e6\", \"repos itory\": { \"clone_url\" : \"https://github.com/gitops-cookbook/tekton-tutorial- greeter.git\" } }'\n\nYou should get output similar to the following:\n\n138\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "{\"eventListener\":\"tekton-greeter-eventlistener\",\"namespace\":\"default\",\"eventListe nerUID\":\"c00567eb-d798-4c4a-946d-f1732fdfc313\",\"eventID\":\"17dd25bb-a1fe-4f84-8422- c3abc5f10066\"}\n\nA new Pipeline now is started and you can check it with the following command:\n\ntkn pipelinerun ls\n\nYou should see it in Running status as follows:\n\ntekton-greeter-pipeline-3244b67f-31d3-4597-af1c-3c1aa6693719 4 seconds ago --- Running\n\nSee Also\n\n• Tekton Triggers examples\n\n• Getting Started with Tekton Triggers\n\n• Securing webhooks with event listeners\n\n6.9 Update a Kubernetes Resource Using Kustomize and Push the Change to Git\n\nProblem You want to use Kustomize in your Tekton Pipelines in order to automate Kubernetes manifests updates.\n\nSolution As we discussed in Chapter 4, Kustomize is a powerful tool to manage Kubernetes manifests. Kustomize can add, remove, or patch configuration options without fork‐ ing. In Recipe 4.2 you saw how to update a Kubernetes Deployment with a new container image hash using the kustomize CLI.\n\nIn this recipe, you’ll see how to let Tekton update it using Kustomize. This is very useful for GitOps as it allows an automated update on Git to the manifests describing an application running on Kubernetes, favoring the interconnection with a GitOps tool such as Argo CD in order to sync resources (see Chapter 7).\n\nWhen adopting the GitOps approach, it’s common to have one or more repositories for the Kubernetes manifests and then one or more repositories for the apps as well.\n\nThus let’s introduce a Task that accepts the Kubernetes manifests repository as a parameter and can update the container image reference as seen in Recipe 4.2:\n\napiVersion: tekton.dev/v1beta1 kind: Task metadata:\n\n6.9 Update a Kubernetes Resource Using Kustomize and Push the Change to Git\n\n|\n\n139",
      "content_length": 1802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "140\n\nannotations: tekton.dev/pipelines.minVersion: 0.12.1 tekton.dev/tags: git name: git-update-deployment labels: app.kubernetes.io/version: '0.2' operator.tekton.dev/provider-type: community spec: description: >- This Task can be used to update image digest in a Git repo using kustomize. It requires a secret with credentials for accessing the git repo. params: - name: GIT_REPOSITORY type: string - name: GIT_REF type: string - name: NEW_IMAGE type: string - name: NEW_DIGEST type: string - name: KUSTOMIZATION_PATH type: string results: - description: The commit SHA name: commit steps: - image: 'docker.io/alpine/git:v2.26.2' name: git-clone resources: {} script: > rm -rf git-update-digest-workdir\n\ngit clone $(params.GIT_REPOSITORY) -b $(params.GIT_REF) git-update-digest-workdir workingDir: $(workspaces.workspace.path) - image: 'quay.io/wpernath/kustomize-ubi:latest' name: update-digest resources: {} script: > cd git-update-digest-workdir/$(params.KUSTOMIZATION_PATH)\n\nkustomize edit set image $(params.NEW_IMAGE)@$(params.NEW_DIGEST)\n\necho \"##########################\"\n\necho \"### kustomization.yaml ###\"\n\necho \"##########################\"\n\ncat kustomization.yaml workingDir: $(workspaces.workspace.path) - image: 'docker.io/alpine/git:v2.26.2' name: git-commit\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "resources: {} script: | cd git-update-digest-workdir\n\ngit config user.email \"tektonbot@redhat.com\" git config user.name \"My Tekton Bot\"\n\ngit status git add $(params.KUSTOMIZATION_PATH)/kustomization.yaml git commit -m \"[ci] Image digest updated\"\n\ngit push\n\nRESULT_SHA=\"$(git rev-parse HEAD | tr -d '\\n')\" EXIT_CODE=\"$?\" if [ \"$EXIT_CODE\" != 0 ] then exit $EXIT_CODE fi # Make sure we don't add a trailing newline to the result! echo -n \"$RESULT_SHA\" > $(results.commit.path) workingDir: $(workspaces.workspace.path) workspaces: - description: The workspace consisting of maven project. name: workspace\n\nThis Task is composed of three steps:\n\ngit-clone\n\nClones the Kubernetes manifests repository\n\nupdate-digest\n\nRuns kustomize to update the Kubernetes Deployment with a container image hash given as a parameter\n\ngit-commit\n\nUpdates the Kubernetes manifest repo with the new container image hash\n\nYou can create the Task with the following command:\n\nkubectl create -f git-update-deployment-task.yaml\n\nYou should get the following output:\n\ntask.tekton.dev/git-update-deployment created\n\nYou can now add this Task to a Pipeline similar to the one you saw in Recipe 6.7 in order to automate the update of your manifests with Kustomize:\n\napiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: pacman-pipeline spec:\n\n6.9 Update a Kubernetes Resource Using Kustomize and Push the Change to Git\n\n|\n\n141",
      "content_length": 1402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "142\n\nparams: - default: https://github.com/gitops-cookbook/pacman-kikd.git name: GIT_REPO type: string - default: master name: GIT_REVISION type: string - default: quay.io/gitops-cookbook/pacman-kikd name: DESTINATION_IMAGE type: string - default: . name: CONTEXT_DIR type: string - default: 'https://github.com/gitops-cookbook/pacman-kikd-manifests.git' name: CONFIG_GIT_REPO type: string - default: main name: CONFIG_GIT_REVISION type: string tasks: - name: fetch-repo params: - name: url value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REVISION) - name: deleteExisting value: \"true\" taskRef: name: git-clone workspaces: - name: output workspace: app-source - name: build-app taskRef: name: maven params: - name: GOALS value: - -DskipTests - clean - package - name: CONTEXT_DIR value: \"$(params.CONTEXT_DIR)\" workspaces: - name: maven-settings workspace: maven-settings - name: source workspace: app-source runAfter: - fetch-repo - name: build-push-image taskRef: name: buildah\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "params: - name: IMAGE value: \"$(params.DESTINATION_IMAGE)\" workspaces: - name: source workspace: app-source runAfter: - build-app - name: git-update-deployment params: - name: GIT_REPOSITORY value: $(params.CONFIG_GIT_REPO) - name: NEW_IMAGE value: $(params.DESTINATION_IMAGE) - name: NEW_DIGEST value: $(tasks.build-push-image.results.IMAGE_DIGEST) - name: KUSTOMIZATION_PATH value: env/dev - name: GIT_REF value: $(params.CONFIG_GIT_REVISION) runAfter: - build-push-image taskRef: kind: Task name: git-update-deployment workspaces: - name: workspace workspace: app-source workspaces: - name: app-source - name: maven-settings\n\nAs you can see from this example, you can take a result of a previous Task as an input for the following one. In this case the hash of the container image generated by the build-push-image Task is used to update the manifests with Kustomize.\n\nYou can create the Pipeline with the following command:\n\nkubectl create -f pacman-pipeline.yaml\n\nYou should get the following output:\n\npipeline.tekton.dev/pacman-pipeline created\n\nThe git-commit step requires authentication to your Git server in order to push the updates to the repo. Since this example is on GitHub, we are using a GitHub Personal Access Token (see Recipe 6.4) attached to the ServiceAccount tekton-bot-sa.\n\n6.9 Update a Kubernetes Resource Using Kustomize and Push the Change to Git\n\n|\n\n143",
      "content_length": 1381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Make sure to add the repo and registry’s Kubernetes Secrets as described in Recipes 6.4 and 6.5:\n\nkubectl patch serviceaccount tekton-bot-sa -p '{\"secrets\": [{\"name\": \"git- secret\"}]}' kubectl patch serviceaccount tekton-bot-sa \\ -p '{\"secrets\": [{\"name\": \"containerregistry- secret\"}]}'\n\nMake sure you have created a PVC for the Pipeline as defined in Recipe 6.7.\n\nNow you can start the Pipeline as follows:\n\ntkn pipeline start pacman-pipeline \\ --serviceaccount='tekton-bot-sa' \\ --param GIT_REPO='https://github.com/gitops-cookbook/pacman-kikd.git' \\ --param GIT_REVISION='main' \\ --param DESTINATION_IMAGE='quay.io/gitops-cookbook/pacman-kikd:latest' \\ --param CONFIG_GIT_REPO='https://github.com/gitops-cookbook/pacman-kikd- manifests.git' \\ --param CONFIG_GIT_REVISION='main' \\ --workspace name=app-source,claimName=app-source-pvc \\ --workspace name=maven-settings,emptyDir=\"\" \\ --use-param-defaults \\ --showlog\n\n6.10 Update a Kubernetes Resource Using Helm and Create a Pull Request\n\nProblem You want to automate the deployment of Helm-packaged apps with a Tekton Pipeline.\n\nSolution In Chapter 5 we discussed Helm and how it can be used to manage applications on Kubernetes in a convenient way. In this recipe you’ll see how to automate Helm- powered deployments through a Pipeline in order to install or update an application running on Kubernetes.\n\nAs shown in Recipe 6.7, you can use Tekton Hub to find and install Tekton Tasks. In fact, you can use the helm-upgrade-from-repo Task to have Helm support for your Pipelines.\n\n144\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "To install it, run this command:\n\ntkn hub install task helm-upgrade-from-repo\n\nThis Task can install a Helm Chart from a Helm repository. For this example, we pro‐ vide a Helm repository in this book’s repository that you can add with the following command:\n\nhelm repo add gitops-cookbook https://gitops-cookbook.github.io/helm-charts/\n\nYou should get the following output:\n\n\"gitops-cookbook\" has been added to your repositories\n\nYou can install the Helm Chart with the following command:\n\nhelm install pacman gitops-cookbook/pacman\n\nYou should get output similar to the following:\n\nNAME: pacman LAST DEPLOYED: Mon Aug 15 17:02:21 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None USER-SUPPLIED VALUES: {}\n\nThe app should be now deployed and running on Kubernetes:\n\nkubectl get pods -l=app.kubernetes.io/name=pacman\n\nYou should get the following output:\n\nNAME READY STATUS RESTARTS AGE pacman-6798d65d84-9mt8p 1/1 Running 0 30s\n\nNow let’s update the Deployment with a Tekton Task running a helm upgrade with the following TaskRun:\n\napiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: generateName: helm-pacman-run- spec: serviceAccountName: tekton-deployer-sa taskRef: name: helm-upgrade-from-repo params: - name: helm_repo value: https://gitops-cookbook.github.io/helm-charts/ - name: chart_name value: gitops-cookbook/pacman - name: release_version value: 0.1.0 - name: release_name\n\n6.10 Update a Kubernetes Resource Using Helm and Create a Pull Request\n\n|\n\n145",
      "content_length": 1487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "value: pacman - name: overwrite_values value: replicaCount=2\n\nThe helm-upgrade-from-repo Task needs permission to list objects in the work‐ ing namespace, so you need a ServiceAccount with special permissions as seen in Recipe 6.6.\n\nYou can override values in the Chart’s values.yaml file by adding them in this param. Here we are setting up two replicas for the Pac-Man game.\n\nRun the Task with the following command:\n\nkubectl create -f helm-pacman-taskrun.yaml\n\nYou should get output similar to the following:\n\ntaskrun.tekton.dev/helm-pacman-run-qghx8 created\n\nCheck logs with tkn CLI and select the running Task:\n\ntkn taskrun logs -f\n\nYou should get output similar to the following, where you can see the Helm upgrade has been successfully performed:\n\n[upgrade-from-repo] current installed helm releases [upgrade-from-repo] NAME NAMESPACE REVISION UPDA- TED STATUS CHART APP VERSION [upgrade-from-repo] pacman default 1 2022-08-15 17:02:21.633934129 +0200 +0200 deployed pacman-0.1.0 1.0.0 [upgrade-from-repo] parsing helms repo name... [upgrade-from-repo] adding helm repo... [upgrade-from-repo] \"gitops-cookbook\" has been added to your repositories [upgrade-from-repo] adding updating repo... [upgrade-from-repo] Hang tight while we grab the latest from your chart reposito- ries... [upgrade-from-repo] ...Successfully got an update from the \"gitops-cookbook\" chart repository [upgrade-from-repo] Update Complete. ⎈Happy Helming!⎈ [upgrade-from-repo] installing helm chart... [upgrade-from-repo] history.go:56: [debug] getting history for release pacman [upgrade-from-repo] upgrade.go:123: [debug] preparing upgrade for pacman [upgrade-from-repo] upgrade.go:131: [debug] performing update for pacman [upgrade-from-repo] upgrade.go:303: [debug] creating upgraded release for pacman [upgrade-from-repo] client.go:203: [debug] checking 2 resources for changes [upgrade-from-repo] client.go:466: [debug] Looks like there are no changes for Service \"pacman\" [upgrade-from-repo] wait.go:47: [debug] beginning wait for 2 resources with time- out of 5m0s [upgrade-from-repo] ready.go:277: [debug] Deployment is not ready: default/pacman. 1 out of 2 expected pods are ready [upgrade-from-repo] ready.go:277: [debug] Deployment is not ready: default/pacman.\n\n146\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 2291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "1 out of 2 expected pods are ready [upgrade-from-repo] ready.go:277: [debug] Deployment is not ready: default/pacman. 1 out of 2 expected pods are ready [upgrade-from-repo] upgrade.go:138: [debug] updating status for upgraded release for pacman [upgrade-from-repo] Release \"pacman\" has been upgraded. Happy Helming! [upgrade-from-repo] NAME: pacman [upgrade-from-repo] LAST DEPLOYED: Mon Aug 15 15:23:31 2022 [upgrade-from-repo] NAMESPACE: default [upgrade-from-repo] STATUS: deployed [upgrade-from-repo] REVISION: 2 [upgrade-from-repo] TEST SUITE: None [upgrade-from-repo] USER-SUPPLIED VALUES: [upgrade-from-repo] replicaCount: 2 [upgrade-from-repo] [upgrade-from-repo] COMPUTED VALUES: [upgrade-from-repo] image: [upgrade-from-repo] containerPort: 8080 [upgrade-from-repo] pullPolicy: Always [upgrade-from-repo] repository: quay.io/gitops-cookbook/pacman-kikd [upgrade-from-repo] tag: 1.0.0 [upgrade-from-repo] replicaCount: 2 [upgrade-from-repo] securityContext: {} [upgrade-from-repo] [upgrade-from-repo] HOOKS: [upgrade-from-repo] MANIFEST: [upgrade-from-repo] --- [upgrade-from-repo] # Source: pacman/templates/service.yaml [upgrade-from-repo] apiVersion: v1 [upgrade-from-repo] kind: Service [upgrade-from-repo] metadata: [upgrade-from-repo] labels: [upgrade-from-repo] app.kubernetes.io/name: pacman [upgrade-from-repo] name: pacman [upgrade-from-repo] spec: [upgrade-from-repo] ports: [upgrade-from-repo] - name: http [upgrade-from-repo] port: 8080 [upgrade-from-repo] targetPort: 8080 [upgrade-from-repo] selector: [upgrade-from-repo] app.kubernetes.io/name: pacman [upgrade-from-repo] --- [upgrade-from-repo] # Source: pacman/templates/deployment.yaml [upgrade-from-repo] apiVersion: apps/v1 [upgrade-from-repo] kind: Deployment [upgrade-from-repo] metadata: [upgrade-from-repo] name: pacman [upgrade-from-repo] labels: [upgrade-from-repo] app.kubernetes.io/name: pacman [upgrade-from-repo] app.kubernetes.io/version: \"1.0.0\" [upgrade-from-repo] spec: [upgrade-from-repo] replicas: 2 [upgrade-from-repo] selector: [upgrade-from-repo] matchLabels:\n\n6.10 Update a Kubernetes Resource Using Helm and Create a Pull Request\n\n|\n\n147",
      "content_length": 2138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "[upgrade-from-repo] app.kubernetes.io/name: pacman [upgrade-from-repo] template: [upgrade-from-repo] metadata: [upgrade-from-repo] labels: [upgrade-from-repo] app.kubernetes.io/name: pacman [upgrade-from-repo] spec: [upgrade-from-repo] containers: [upgrade-from-repo] - image: \"quay.io/gitops-cookbook/pacman-kikd:1.0.0\" [upgrade-from-repo] imagePullPolicy: Always [upgrade-from-repo] securityContext: [upgrade-from-repo] {} [upgrade-from-repo] name: pacman [upgrade-from-repo] ports: [upgrade-from-repo] - containerPort: 8080 [upgrade-from-repo] name: http [upgrade-from-repo] protocol: TCP [upgrade-from-repo]\n\nkubectl get deploy -l=app.kubernetes.io/name=pacman\n\npacman 2/2 2 2 9s\n\n6.11 Use Drone to Create a Pipeline for Kubernetes\n\nProblem You want to create a CI/CD pipeline for Kubernetes with Drone.\n\nSolution Drone is an open source project for cloud native continuous integration (CI). It uses YAML build files to define and execute build pipelines inside containers.\n\nIt has two main components:\n\nServer\n\nIntegrates with popular SCMs such as GitHub, GitLab, or Gitea\n\nRunner\n\nActs as an agent running on a certain platform\n\nYou can install the Server of your choice following the documentation and install the Kubernetes Runner.\n\nIn this example you will create a Java Maven-based pipeline using the Pac-Man app. First, install the Drone CLI for your OS; you can get it from the official website here.\n\n148\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "On macOS, drone is available through Homebrew as follows:\n\nbrew tap drone/drone && brew install drone\n\nThen configure Drone, copy the DRONE_TOKEN from your instance under the Drone Account settings page, then create/update the file called .envrc.local and add the variables to override:\n\nexport DRONE_TOKEN=\"<YOUR-TOKEN>\"\n\nEnsure the token is loaded:\n\ndrone info\n\nNow activate the repo in Drone:\n\ndrone repo enable https://github.com/gitops-cookbook/pacman-kikd.git\n\nSimilarly to Tekton, Drone’s pipeline will compile, test, and build the app. Then it will create and push the container image to a registry.\n\nAdd credentials to your container registry as follows (here, we’re using Quay.io):\n\ndrone secret add --name image_registry \\ --data quay.io https://github.com/gitops-cookbook/pacman-kikd.git\n\ndrone secret add --name image_registry_user \\ --data YOUR_REGISTRY_USER https://github.com/gitops-cookbook/pacman-kikd.git\n\ndrone secret add --name image_registry_password \\ --data YOUR_REGISTRY_PASS https://github.com/gitops-cookbook/pacman-kikd.git\n\ndrone secret add --name destination_image \\ --data quay.io/YOUR_REGISTRY_USER>/pacman-kikd.git https://github.com/gitops- cookbook/pacman-kikd.git\n\nCreate a file called .drone.yaml as follows:\n\nkind: pipeline type: docker name: java-pipeline platform: os: linux arch: arm64 trigger: branch: - main clone: disable: true steps: - name: clone sources image: alpine/git pull: if-not-exists\n\n6.11 Use Drone to Create a Pipeline for Kubernetes\n\n|\n\n149",
      "content_length": 1498,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "commands: - git clone https://github.com/gitops-cookbook/pacman-kikd.git . - git checkout $DRONE_COMMIT - name: maven-build image: maven:3-jdk-11 commands: - mvn install -DskipTests=true -B - mvn test -B - name: publish image: plugins/docker:20.13 pull: if-not-exists settings: tags: \"latest\" dockerfile: Dockerfile insecure: true mtu: 1400 username: from_secret: image_registry_user password: from_secret: image_registry_password registry: from_secret: image_registry repo: from_secret: destination_image\n\nStart the pipeline:\n\ndrone exec --pipeline=java-pipeline\n\nYou can also trigger the pipeline to start by pushing to your Git repo.\n\nSee Also\n\nExample Maven Pipeline from Drone docs •\n\nComplete Quarkus pipeline example in Drone •\n\n6.12 Use GitHub Actions for CI\n\nProblem You want to use GitHub Actions for CI in order to compile and package an app as a container image ready to be deployed in CD.\n\n150\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Solution GitHub Actions are event-driven automation tasks available for any GitHub reposi‐ tory. An event automatically triggers the workflow, which contains a job. The job then uses steps to control the order in which actions are run. These actions are the commands that automate software building, testing, and deployment.\n\nIn this recipe, you will add a GitHub Action for building the Pac-Man game container image, and pushing it to the GitHub Container Registry.\n\nAs GitHub Actions are connected to repositories, you can fork the Pac-Man repository from this book’s code repositories to add your GitHub Actions. See the documentation about forking repositories for more info on this topic.\n\nGitHub Actions workflows run into environments and they can reference an environ‐ ment to use the environment’s protection rules and secrets.\n\nWorkflows and jobs are defined with a YAML file containing all the needed steps. Inside your repository, you can create one with the path .github/workflows/ pacman-ci-action.yml:\n\n# This is a basic workflow to help you get started with Actions\n\nname: pacman-ci-action\n\nenv: IMAGE_REGISTRY: ghcr.io/${{ github.repository_owner }} REGISTRY_USER: ${{ github.actor }} REGISTRY_PASSWORD: ${{ github.token }} APP_NAME: pacman IMAGE_TAGS: 1.0.0 ${{ github.sha }}\n\n# Controls when the workflow will run on: # Triggers the workflow on push or pull request events but only for the # \"main\" branch push: branches: [ \"main\" ] pull_request: branches: [ \"main\" ]\n\n# Allows you to run this workflow manually from the Actions tab workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in # parallel jobs: # This workflow contains a single job called \"build-and-push\" build-and-push:\n\n6.12 Use GitHub Actions for CI\n\n|\n\n151",
      "content_length": 1786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "152\n\n# The type of runner that the job will run on runs-on: ubuntu-latest\n\n# Steps represent a sequence of tasks that will be executed as part of the # job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can # access it - uses: actions/checkout@v3\n\nname: Set up JDK 11 uses: actions/setup-java@v3 with: java-version: '11' distribution: 'adopt' cache: maven\n\nname: Build with Maven run: mvn --batch-mode package\n\nname: Buildah Action id: build-image uses: redhat-actions/buildah-build@v2 with: image: ${{ env.IMAGE_REGISTRY }}/${{ env.APP_NAME }} tags: ${{ env.IMAGE_TAGS }} containerfiles: | ./Dockerfile - name: Push to Registry id: push-to-registry uses: redhat-actions/push-to-registry@v2 with: image: ${{ steps.build-image.outputs.image }} tags: ${{ steps.build-image.outputs.tags }} registry: ${{ env.IMAGE_REGISTRY }} username: ${{ env.REGISTRY_USER }} password: ${{ env.REGISTRY_PASSWORD }} Name of the Action.\n\nEnvironment variables to be used in the workflow. This includes default environ‐ ment variables and the Secret you added to the environment.\n\nHere’s where you define which type of trigger you want for this workflow. In this case, any change to the repository (Push) to the master branch will trigger the action to start. Check out the documentation for a full list of triggers that can be used.\n\nName of this Job.\n\nList of steps; each step contains an action for the pipeline.\n\n|\n\nChapter 6: Cloud Native CI/CD",
      "content_length": 1454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Buildah Build. This action builds container images using Buildah.\n\nPush to Registry. This action is used to push to the GitHub Registry using built-in credentials available for GitHub repository owners.\n\nAfter each Git push or pull request, a new run of the action is performed as shown in Figure 6-10.\n\nGitHub offers its own container registry available at ghcr.io, and container images are referenced as part of the GitHub Packages. By default the images are public. See this book’s repository as a reference.\n\nFigure 6-10. GitHub Actions Jobs\n\nSee Also\n\n• GitHub Actions Jobs\n\n• Red Hat Actions\n\nDeploy to Kubernetes cluster Action •\n\n6.12 Use GitHub Actions for CI\n\n|\n\n153",
      "content_length": 676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "CHAPTER 7 Argo CD\n\nIn the previous chapter, you learned about Tekton and other engines such as GitHub Actions to implement the continuous integration (CI) part of a project.\n\nAlthough CI is important because it’s where you build the application and check that nothing has been broken (running unit tests, component tests, etc.), there is still a missing part: how to deploy this application to an environment (a Kubernetes cluster) using the GitOps methodology and not creating a script running kubectl/ helm commands.\n\nAs Daniel Bryant puts it, “If you weren’t using SSH in the past to deploy your application in production, don’t use kubectl to do it in Kubernetes.”\n\nIn this chapter, we’ll introduce you to Argo CD, a declarative, GitOps continuous delivery (CD) tool for Kubernetes. In the first part of the section, we’ll see the deployment of an application using Argo CD (Recipes 7.1 and 7.2).\n\nArgo CD not only supports the deployment of plain Kubernetes manifests, but also the deployment of Kustomize projects (Recipe 7.3) and Helm projects (Recipe 7.4).\n\nA typical operation done in Kubernetes is a rolling update to a new version of the container, and Argo CD integrates with another tool to make this process smooth (Recipe 7.5).\n\nDelivering complex applications might require some orchestration on when and how the application must be deployed and released (Recipes 7.7 and 7.8).\n\nWe’ll see how to:\n\n• Install and deploy the first application.\n\n• Use automatic deployment and self-healing applications.\n\n155",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "• Execute a rolling update when a new container is released.\n\n• Give an order on the execution.\n\nIn this chapter, we are using the https://github.com/gitops-cookbook/gitops- cookbook-sc.git GitHub repository as source directory. To run it successfully in this chapter, you should fork it and use it in the YAML files provided in the examples.\n\n7.1 Deploy an Application Using Argo CD\n\nProblem You want Argo CD to deploy an application defined in a Git repository.\n\nSolution Create an Application resource to set up Argo CD to deploy the application.\n\nTo install Argo CD, create the argocd namespace and apply the Argo CD installation manifest:\n\nkubectl apply -n argocd \\ -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.3.4/manifests/install.yaml\n\nOptional Steps It’s not mandatory to install the Argo CD CLI tool, or expose the Argo CD server service to access the Argo CD Dashboard. Still, in this book, we’ll use them in the recipes to show you the final result after applying the manifests. So, although not mandatory, we encourage you to follow the next steps to be aligned with the book.\n\nTo install the argocd CLI tool, go to the Argo CD CLI GitHub release page and in the Assets section, download the tool for your platform.\n\nAfter installing the argocd tool, the argocd-server Kubernetes Service needs to be exposed. You can use any technique such as Ingress or set the service as LoadBa lancer but we’ll use the kubectl port-forwarding to connect to the API server without exposing the service:\n\nkubectl port-forward svc/argocd-server -n argocd 9090:443\n\nAt this point, you can access the Argo CD server using http://localhost:9090.\n\nThe initial password for the admin account is generated automatically in a secret named argocd-initial-admin-secret in the argocd namespace:\n\nargoPass=$(kubectl -n argocd get secret argocd-initial-admin-secret -o json path=\"{.data.password}\" | base64 -d)\n\n156\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "argoURL=localhost:9090\n\nargocd login --insecure --grpc-web $argoURL --username admin --password $argo Pass\n\n'admin:login' logged in successfully\n\nYou should use the same credentials to access the Argo CD UI.\n\nLet’s make Argo CD deploy a simple web application showing a box with a config‐ ured color. The application is composed of three Kubernetes manifest files, including a Namespace, a Deployment, and a Service definition.\n\nThe files are located in the ch07/bgd folder of the book’s repository.\n\nAll these files are known as an Application in Argo CD. Therefore, you must define it as such to apply these manifests in your cluster.\n\nLet’s check the Argo CD Application resource file used for deploying the application:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgd-app namespace: argocd spec: destination: namespace: bgd server: https://kubernetes.default.svc project: default source: repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git path: ch07/bgd targetRevision: main\n\nNamespace where Argo CD is installed\n\nTarget cluster and namespace\n\nInstalling the application in Argo CD’s default project\n\nThe manifest repo where the YAML resides\n\nThe path to look for manifests\n\nBranch to checkout\n\n7.1 Deploy an Application Using Argo CD\n\n|\n\n157",
      "content_length": 1286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "In the terminal window, run the following command to register the Argo CD application:\n\nkubectl apply -f manual-bgd-app.yaml\n\nAt this point, the application is registered as an Argo CD application.\n\nYou can check the status using either argocd or the UI; run the following command to list applications using the CLI too:\n\nargocd app list\n\nAnd the output is something like:\n\nNAME CLUSTER NAMESPACE PROJECT STATUS bgd-app https://kubernetes.default.svc bgd default OutOfSync\n\nThe important field here is STATUS. It’s OutOfSync, which means the application is registered, and there is a drift between the current status (in this case, no application deployed) and the content in the Git repository (the application deployment files).\n\nYou’ll notice that no pods are running if you get all the pods from the bgd namespace:\n\nkubectl get pods -n bgd\n\nNo resources found in bgd namespace.\n\nArgo CD doesn’t synchronize the application automatically by default. It just shows a divergence, and the user is free to fix it by triggering a synchronized operation.\n\nWith the CLI, you synchronize the application by running the following command in a terminal:\n\nargocd app sync bgd-app\n\nAnd the ouput of the command shows all the important information regarding the deployment:\n\nName: bgd-app Project: default Server: https://kubernetes.default.svc Namespace: bgd URL: https://openshift-gitops-server-openshift-gitops.apps.open- shift.sotogcp.com/applications/bgd-app Repo: https://github.com/lordofthejars/gitops-cookbook-sc.git Target: main Path: ch07/bgd SyncWindow: Sync Allowed Sync Policy: <none> Sync Status: Synced to main (384cd3d) Health Status: Progressing\n\nOperation: Sync Sync Revision: 384cd3d21c534e75cb6b1a6921a6768925b81244 Phase: Succeeded\n\n158\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Start: 2022-06-16 14:45:12 +0200 CEST Finished: 2022-06-16 14:45:13 +0200 CEST Duration: 1s Message: successfully synced (all tasks run)\n\nGROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Namespace bgd bgd Running Synced namespace/bgd cre- ated Service bgd bgd Synced Healthy service/bgd created apps Deployment bgd bgd Synced Progressing deploy- ment.apps/bgd created Namespace bgd Synced\n\nYou can synchronize the application from the UI as well, by clicking the SYNC button as shown in Figure 7-1.\n\nFigure 7-1. Argo CD web console\n\n7.1 Deploy an Application Using Argo CD\n\n|\n\n159",
      "content_length": 584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "If you get all the pods from the bgd namespace, you’ll notice one pod running:\n\nkubectl get pods -n bgd\n\nNAME READY STATUS RESTARTS AGE bgd-788cb756f7-jll9n 1/1 Running 0 60s\n\nAnd the same for the Service:\n\nkubectl get services -n bgd\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) bgd ClusterIP 172.30.35.199 <none> 8080:32761/TCP\n\nExposed port is 32761\n\nIn the following sections, you’ll need to access the deployed service to validate that it’s deployed. There are several ways to access services deployed to Minikube; for the following chapters, we use the Minikube IP and the exposed port of the service.\n\nRun the following command in a terminal window to get the Minikube IP:\n\nminikube ip -p gitops 192.168.59.100\n\nOpen a browser window, set the previous IP followed by the exposed port (in this example 192.168.59.100:32761), and access the service to validate that the color of the circles in the box is blue, as shown in Figure 7-2.\n\nFigure 7-2. Deployed application\n\n160\n\n|\n\nChapter 7: Argo CD",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Discussion Now it’s time to update the application deployment files. This time we will change the value of an environment variable defined in the bgd-deployment.yaml file.\n\nOpen ch07/bgd/bgd-deployment.yaml in your file editor and change the COLOR envi‐ ronment variable value from blue to green:\n\nspec: containers: - image: quay.io/redhatworkshops/bgd:latest name: bgd env: - name: COLOR value: \"green\"\n\nIn a terminal run the following commands to commit and push the file so the change is available for Argo CD:\n\ngit add . git commit -m \"Updates color\"\n\ngit push origin main\n\nWith the change pushed, check the status of the application again:\n\nargocd app list\n\nNAME CLUSTER NAMESPACE PROJECT STATUS bgd-app https://kubernetes.default.svc bgd default Sync\n\nWe see the application status is Sync. This happens because Argo CD uses a polling approach to detect divergences between what’s deployed and what’s defined in Git. After some time (by default, it’s 3 minutes), the application status will be OutOfSync:\n\nargocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH bgd-app https://kubernetes.default.svc bgd default OutOfSync Healthy\n\nTo synchronize the changes, run the sync subcommand:\n\nargocd app sync bgd-app\n\nAfter some seconds, access the service and validate that the circles are green, as shown in Figure 7-3.\n\n7.1 Deploy an Application Using Argo CD\n\n|\n\n161",
      "content_length": 1374,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "Figure 7-3. Deployed application\n\nTo remove the application, use the CLI tool or the UI:\n\nargocd app delete bgd-app\n\nAlso, revert the changes done in the Git repository to get the initial version of the application and push them:\n\ngit revert HEAD\n\ngit push origin main\n\n7.2 Automatic Synchronization\n\nProblem You want Argo CD to automatically update resources when there are changes.\n\nSolution Use the syncPolicy section with an automated policy.\n\nArgo CD can automatically synchronize an application when it detects differences between the manifests in Git and the Kubernetes cluster.\n\nA benefit of automatic sync is that there is no need to log in to the Argo CD API, with the security implications that involves (managing secrets, network, etc.), and the use of the argocd tool. Instead, when a manifest is changed and pushed to the Git repository with the changes to the tracking Git repo, the manifests are automatically applied.\n\nLet’s modify the previous Argo CD manifest file (Application), adding the sync Policy section, so changes are deployed automatically:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata:\n\n162\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "name: bgd-app namespace: argocd spec: destination: namespace: bgd server: https://kubernetes.default.svc project: default source: path: ch07/bgd repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main syncPolicy: automated: {}\n\nStarts the synchronization policy configuration section\n\nArgo CD automatically syncs the repo\n\nAt this point, we can apply the Application file into a running cluster by running the following command:\n\nkubectl apply -f bgd/bgd-app.yaml\n\nNow, Argo CD deploys the application without executing any other command.\n\nRun the kubectl command or check in the Argo CD UI to validate that the deploy‐ ment is happening:\n\nkubectl get pods -n bgd\n\nNAME READY STATUS RESTARTS AGE bgd-788cb756f7-jll9n 1/1 Running 0 60s\n\nAccess the service and validate that the circles are blue, as shown in Figure 7-4.\n\nFigure 7-4. Deployed application\n\n7.2 Automatic Synchronization\n\n|\n\n163",
      "content_length": 923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "To remove the application, use the CLI tool or the UI:\n\nargocd app delete bgd-app\n\nDiscussion Although Argo CD deploys applications automatically, it uses some default conserva‐ tive strategies for safety reasons.\n\nTwo of these are the pruning of deleted resources and the self-healing of the applica‐ tion in case a change was made in the Kubernetes cluster directly instead of through Git.\n\nBy default, Argo CD will not delete (prune) any resource when it detects that it is no longer available in Git, and it will be in an OutOfSync status. If you want Argo CD to delete these resources, you can do it in two ways.\n\nThe first way is by manually invoking a sync with the -prune option:\n\nargocd app sync --prune\n\nThe second way is letting Argo CD delete pruned resources automatically by setting the prune attribute to true in the automated section:\n\nsyncPolicy: automated: prune: true\n\nEnables automatic pruning\n\nAnother important concept affecting how the application is automatically updated is self-healing.\n\nArgo CD is configured not to correct any drift made manually in the cluster. For example, Argo CD will let the execution of a kubectl patch directly in the cluster change any configuration parameter of the application.\n\nLet’s see it in action.\n\nThe color of the circle is set as an environment variable (COLOR).\n\nNow, let’s change the COLOR environment variable to green using the kubectl patch command.\n\nRun the following command in the terminal:\n\nkubectl -n bgd patch deploy/bgd \\ --type='json' -p='[{\"op\": \"replace\", \"path\": \"/ spec/template/spec/containers/0/env/0/value\", \"value\":\"green\"}]'\n\nWait for the rollout to happen:\n\nkubectl rollout status deploy/bgd -n bgd\n\n164\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1712,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "If you refresh the browser, you should see green circles now, as shown in Figure 7-5.\n\nFigure 7-5. Deployed application\n\nLooking over the Argo CD sync status, you’ll see that it’s OutOfSync as the application and the definition in the Git repository (COLOR: blue) diverges.\n\nArgo CD will not roll back to correct this drift as the selfHeal property default is set to false.\n\nLet’s remove the application and deploy a new one, but set selfHeal to true in this case:\n\nargocd app delete bgd-app\n\nLet’s enable the selfHealing property, and repeat the experiment:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgd-app namespace: argocd spec: destination: namespace: bgd server: https://kubernetes.default.svc project: default source: path: ch07/bgd repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main syncPolicy: automated: prune: true selfHeal: true\n\nselfHeal set to true to correct any drift\n\nAnd in the terminal apply the resource:\n\nkubectl apply -f bgd/heal-bgd-app.yaml\n\n7.2 Automatic Synchronization\n\n|\n\n165",
      "content_length": 1066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Repeat the previous steps:\n\n1. Open the browser to check that the circles are blue. 1. 2. Reexecute the kubectl -n bgd patch deploy/bgd ... command. 2.\n\n3. Refresh the browser and check that the circles are still blue. 3.\n\nArgo CD corrects the drift introduced by the patch command, synchronizing the application to the correct state defined in the Git repository.\n\nTo remove the application, use the CLI tool or the UI:\n\nargocd app delete bgd-app\n\nSee Also\n\n• Argo CD Automated Sync Policy\n\n• Argo CD Sync Options\n\n7.3 Kustomize Integration\n\nProblem You want to use Argo CD to deploy Kustomize manifests.\n\nSolution Argo CD supports several different ways in which Kubernetes manifests can be defined:\n\n• Kustomize\n\n• Helm\n\n• Ksonnet\n\nJsonnet •\n\nYou can also extend the supported ways to custom ones, but this is out of the scope of this book.\n\nArgo CD detects a Kustomize project if there are any of the following files: kustomiza‐ tion.yaml, kustomization.yml, or Kustomization.\n\nLet’s deploy the same BGD application, but in this case, deployed as Kustomize manifests.\n\nMoreover, we’ll set kustomize to override the COLOR environment variable to yellow.\n\n166\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "The Kustomize file defined in the repository looks like this:\n\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: bgdk resources: - ../base - bgdk-ns.yaml patchesJson6902: - target: version: v1 group: apps kind: Deployment name: bgd namespace: bgdk patch: |- - op: replace path: /spec/template/spec/containers/0/env/0/value value: yellow\n\nDirectory with standard deployment files (blue circles)\n\nSpecific file for creating a namespace\n\nPatches standard deployment files\n\nPatches the deployment file\n\nOverrides the environment variable value to yellow\n\nYou don’t need to create this file as it’s already stored in the Git repository.\n\nCreate the following Application file to deploy the application:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgdk-app namespace: argocd spec: destination: namespace: bgdk server: https://kubernetes.default.svc project: default source: path: ch07/bgdk/bgdk repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git\n\n7.3 Kustomize Integration\n\n|\n\n167",
      "content_length": 1039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "targetRevision: main syncPolicy: automated: {}\n\nAt this point, we can apply the Application file to a running cluster by running the following command:\n\nkubectl apply -f bgdk/bgdk-app.yaml\n\nAccess the service and you’ll notice the circles are yellow instead of blue.\n\nTo remove the application, use the CLI tool or the UI:\n\nargocd app delete bgdk-app\n\nDiscussion We can explicitly specify which tool to use, overriding the default algorithm used by Argo CD in the Application file. For example, we can use a plain directory strategy regarding the presence of the kustomization.yaml file:\n\nsource: directory: recurse: true\n\nOverrides always use a plain directory strategy\n\nPossible strategies are: directory, chart, helm, kustomize, path, and plugin.\n\nEverything we’ve seen about Kustomize is valid when using Argo CD.\n\nSee Also\n\n• Chapter 4\n\n• argo-cd/application-crd.yaml on GitHub\n\nArgo CD Tool Detection •\n\n7.4 Helm Integration\n\nProblem You want to use Argo CD to deploy Helm manifests.\n\n168\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Solution Argo CD supports installing Helm Charts to the cluster when it detects a Helm project in the deployment directory (when the Chart.yaml file is present).\n\nLet’s deploy the same BGD application, but in this case, deployed as a Helm manifest.\n\nThe layout of the project is a simple Helm layout already created in the GitHub repository you’ve cloned previously:\n\n├── Chart.yaml ├── charts ├── templates │ ├── NOTES.txt │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── ns.yaml │ ├── service.yaml │ ├── serviceaccount.yaml │ └── tests │ └── test-connection.yaml └── values.yaml\n\nCreate a bgdh/bgdh-app.yaml file to define the Argo CD application:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgdh-app namespace: argocd spec: destination: namespace: bgdh server: https://kubernetes.default.svc project: default source: path: ch07/bgdh repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main syncPolicy: automated: {}\n\nAt this point, we can apply the Application file into a running cluster by running the following command:\n\nkubectl apply -f bgdh/bgdh-app.yaml\n\nValidate the pod is running in the bgdh namespace:\n\nkubectl get pods -n bgdh\n\nNAME READY STATUS RESTARTS AGE bgdh-app-556c46fcd6-ctfkf 1/1 Running 0 5m43s\n\n7.4 Helm Integration\n\n|\n\n169",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "To remove the application, use the CLI tool or the UI:\n\nargocd app delete bgdh-app\n\nDiscussion Argo CD populates build environment variables to Helm manifests (actually also Kustomize, Jsonnet, and custom tools support too).\n\nThe following variables are set:\n\nARGOCD_APP_NAME • • ARGOCD_APP_NAMESPACE • • ARGOCD_APP_REVISION • • ARGOCD_APP_SOURCE_PATH • • ARGOCD_APP_SOURCE_REPO_URL • • ARGOCD_APP_SOURCE_TARGET_REVISION • • KUBE_VERSION • • KUBE_API_VERSIONS • apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgdh-app namespace: openshift-gitops spec: destination: ... source: path: ch07/bgd ... helm: parameters: - name: app value: $ARGOCD_APP_NAME\n\nARGOCD_APP_NAME • • ARGOCD_APP_NAMESPACE • • ARGOCD_APP_REVISION • • ARGOCD_APP_SOURCE_PATH • • ARGOCD_APP_SOURCE_REPO_URL • • ARGOCD_APP_SOURCE_TARGET_REVISION • • KUBE_VERSION • • KUBE_API_VERSIONS • apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgdh-app namespace: openshift-gitops spec: destination: ... source: path: ch07/bgd ... helm: parameters: - name: app value: $ARGOCD_APP_NAME\n\nSpecific Helm section\n\nExtra parameters to set, same as setting them in values.yaml, but high preference\n\nThe name of the parameter\n\nThe value of the parameter, in this case from a Build Env var\n\n170\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1304,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Argo CD can use a different values.yaml file or set parameter values to override the ones defined in values.yaml:\n\nargocd app set bgdh-app --values new-values.yaml\n\nargocd app set bgdh-app -p service.type=LoadBalancer\n\nNote that values files must be in the same Git repository as the Helm Chart.\n\nArgo CD supports Helm hooks too.\n\nSee Also\n\n• Chapter 5\n\n• argo-cd/application-crd.yaml on GitHub\n\n7.5 Image Updater\n\nProblem You want Argo CD to automatically deploy a container image when it’s published.\n\nSolution Use Argo CD Image Updater to detect a change on the container registry and update the deployment files.\n\nOne of the most repetitive tasks during development is deploying a new version of a container image.\n\nWith a pure Argo CD solution, after the container image is published to a container registry, we need to update the Kubernetes/Kustomize/Helm manifest files pointing to the new container image and push the result to the Git repository.\n\nThis process implies:\n\n1. 1. Clone the repo\n\n2. 2. Parse the YAML files and update them accordingly\n\n3. Commit and Push the changes 3.\n\nThese boilerplate tasks should be defined for each repository during the continuous integration phase. Although this approach works, it could be automated so the cluster\n\n7.5 Image Updater\n\n|\n\n171",
      "content_length": 1289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "could detect a new image pushed to the container registry and update the current deployment file pointing to the newer version.\n\nThis is exactly what Argo CD Image Updater (ArgoCD IU) does. It’s a Kubernetes controller monitoring for a new container version and updating the manifests defined in the Argo CD Application file.\n\nThe Argo CD IU lifecycle and its relationship with Argo CD are shown in Figure 7-6.\n\nFigure 7-6. Argo CD Image Updater lifecycle\n\nAt this time, Argo CD IU only updates manifests of Kustomize or Helm. In the case of Helm, it needs to support specifying the image’s tag using a parameter (image.tag).\n\nLet’s install the controller in the same namespace as Argo CD:\n\nkubectl apply -f \\ https://raw.githubusercontent.com/argoproj-labs/argocd-imageupdater/v0.12.0/mani- fests/install.yaml -n argocd\n\nValidate the installation process, checking that the pod status of the controller is Running:\n\nkubectl get pods -n argocd\n\nNAME READY STATUS RESTARTS AGE argocd-image-updater-59c45cbc5c-kjjtp 1/1 Running 0 40h\n\n172\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Before using Argo CD IU, we create a Kubernetes Secret representing the Git creden‐ tials, so the updated manifests can be pushed to the repository. The secret must be at the Argo CD namespace and, in this case, we name it git-creds.\n\nkubectl -n argocd create secret generic git-creds \\ --from-literal=user name=<git_user> \\ --from-literal=password=<git_password_or_token>\n\nFinally, let’s annotate the Application manifest with some special annotations so the controller can start monitoring the registry:\n\nimage-list\n\nSpecify one or more images (comma-separated-value) considered for updates.\n\nwrite-back-method\n\nMethods to propagate new versions. There are git and argocd methods imple‐ mented to update to a newer image. The Git method commits the change to the Git repository. Argo CD uses the Kubernetes/ArgoCD API to update the resource.\n\nThere are more configuration options, but the previous ones are the most important to get started.\n\nLet’s create an Argo CD Application manifest annotated with Argo CD IU annotations:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgdk-app namespace: argocd annotations: argocd-image-updater.argoproj.io/image-list: myalias=quay.io/rhdevelopers/bgd\n\nargocd-image-updater.argoproj.io/write-back-method: git:secret:openshift- gitops/git-creds argocd-image-updater.argoproj.io/git-branch: main spec: destination: namespace: bgdk server: https://kubernetes.default.svc project: default source: path: ch07/bgdui/bgdk repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main syncPolicy: automated: {}\n\nAdds annotations section\n\n7.5 Image Updater\n\n|\n\n173",
      "content_length": 1644,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Sets the monitored image name\n\nConfigures to use Git as write-back-method, setting the location of the creden‐ tials (<namespace>/<secretname>)\n\nSets the branch to push changes\n\nNow apply the manifest to deploy the application’s first version and enable Argo CD IU to update the repository when a new image is pushed to the container registry:\n\nkubectl apply -f bgdui/bgdui-app.yaml\n\nAt this point, version 1.0.0 is up and running in the bgdk namespace, and you may access it as we’ve done before. Let’s generate a new container version to validate that the new image is in the repository.\n\nTo simplify the process, we’ll tag the container with version 1.1.0 as it was a new one.\n\nGo to the Quay repository created at the beginning of this chapter, go to the tags section, push the gear icon, and select Add New Tag to create a new container, as shown in Figure 7-7.\n\nFigure 7-7. Tag container\n\nSet the tag to 1.1.0 value as shown in the figure Figure 7-8.\n\nFigure 7-8. Tag container\n\n174\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "After this step, you should have a new container created as shown in Figure 7-9.\n\nWait for around two minutes until the change is detected and the controller triggers the repo update.\n\nFigure 7-9. Final result\n\nTo validate the triggering process check the logs of the controller:\n\nkubectl logs argocd-image-updater-59c45cbc5c-kjjtp -f -n argocd\n\n... time=\"2022-06-20T21:19:05Z\" level=info msg=\"Setting new image to quay.io/rhdevel opers/bgd:1.1.0\" alias=myalias application=bgdk-app image_name=rhdevelopers/bgd image_tag=1.0.0 registry=quay.io time=\"2022-06-20T21:19:05Z\" level=info msg=\"Successfully updated image 'quay.io/ rhdevelopers/bgd:1.0.0' to 'quay.io/rhdevelopers/bgd:1.1.0', but pending spec update (dry run=false)\" alias=myalias application=bgdk-app image_name=rhdevelop ers/bgd image_tag=1.0.0 registry=quay.io time=\"2022-06-20T21:19:05Z\" level=info msg=\"Committing 1 parameter update(s) for application bgdk-app\" application=bgdk-app ...\n\nDetects the change and updates the image\n\nAfter that, if you inspect the repository, you’ll see a new Kustomize file named .argocd-source-bgdk-app.yaml, updating the image value to the new con‐ tainer, as shown in Figure 7-10.\n\n7.5 Image Updater\n\n|\n\n175",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "Figure 7-10. New Kustomize file updating to the new container\n\nNow Argo CD can detect the change and update the cluster properly with the new image.\n\nTo remove the application, use the CLI tool or the UI:\n\nargocd app delete bgdk-app\n\nDiscussion An update strategy defines how Argo CD IU will find new versions. With no change, Argo CD IU uses a semantic version to detect the latest version.\n\nAn optional version constraint field may be added to restrict which versions are allowed to be automatically updated. To only update patch versions, we can change the image-list annotation as shown in the following snippet:\n\nargocd-image-updater.argoproj.io/image-list: myalias=quay.io/rhdevelopers/bgd:1.2.x\n\nArgo CD Image Updater can update to the image that has the most recent build date:\n\nargocd-image-updater.argoproj.io/myalias.update-strategy: latest argocd-image-updater.argoproj.io/myimage.allow-tags: regexp:^[0-9a-f]{7}$\n\nRestricts the tags considered for the update\n\nThe digest update strategy will use image digests to update your applications’ image tags:\n\nargocd-image-updater.argoproj.io/myalias.update-strategy: digest\n\nSo far, the container was stored in a public registry. If the repository is private, Argo CD Image Updater needs read access to the repo to detect any change.\n\nFirst of all, create a new secret representing the container registry credentials:\n\nkubectl create -n argocd secret docker-registry quayio --docker-server=quay.io -- docker-username=$QUAY_USERNAME --docker-password=$QUAY_PASSWORD\n\n176\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Argo CD Image Updater uses a ConfigMap as a configuration source, which is the place to register the private container registry. Create a new ConfigMap manifest setting the supported registries:\n\napiVersion: v1 kind: ConfigMap metadata: name: argocd-image-updater-config data: registries.conf: | registries: - name: RedHat Quay api_url: https://quay.io prefix: quay.io insecure: yes credentials: pullsecret:argocd/quayio\n\nName of the Argo CD IU ConfigMap\n\nPlace to register all registries\n\nA name to identify it\n\nURL of the service\n\nThe prefix used in the container images\n\nGets the credentials from the quayio secret stored at argocd namespace\n\nArgo CD Image Updater commits the update with a default message:\n\ncommit 3caf0af8b7a26de70a641c696446bbe1cd04cea8 (HEAD -> main, origin/main) Author: argocd-image-updater <noreply@argoproj.io> Date: Thu Jun 23 09:41:00 2022 +0000\n\nbuild: automatic update of bgdk-app\n\nupdates image rhdevelopers/bgd tag '1.0.0' to '1.1.0'\n\nWe can update the default commit message to one that fits your requirements. Configure the git.commit-message-template key in ArgoCD IU argocd-image- updater-config ConfigMap with the message:\n\napiVersion: v1 kind: ConfigMap metadata: name: argocd-image-updater-config data: git.user: alex git.email: alex@example.com git.commit-message-template: |\n\n7.5 Image Updater\n\n|\n\n177",
      "content_length": 1344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "build: automatic update of {{ .AppName }}\n\n{{ range .AppChanges -}} updates image {{ .Image }} tag '{{ .OldTag }}' to '{{ .NewTag }}' {{ end -}}\n\nArgo CD IU ConfigMap\n\nCommit user\n\nCommmit email\n\nGolang text/template content\n\nThe name of the application\n\nList of changes performed by the update\n\nImage name\n\nPrevious container tag\n\nNew container tag\n\nRemember to restart the Argo CD UI controller when the Config Map is changed:\n\nkubectl rollout restart deployment argocd-image-updater -n argocd\n\nSee Also\n\n• Argo CD Image Updater\n\n7.6 Deploy from a Private Git Repository\n\nProblem You want Argo CD to deploy manifests.\n\nSolution Use Argo CD CLI/UI or YAML files to register the repositories’ credential informa‐ tion (username/password/token/key).\n\n178\n\n|\n\nChapter 7: Argo CD",
      "content_length": 776,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "In Argo CD, you have two ways to register a Git repository with its credentials. One way is using the Argo CD CLI/Argo CD UI tooling. To register a private repository in Argo CD, set the username and password by running the following command:\n\nargocd repo add https://github.com/argoproj/argocd-example-apps \\ --username <username> --password <password>\n\nAlternatively, we can use the Argo CD UI to register it too. Open Argo CD UI in a browser, and click the Settings/Repositories button (the one with gears) as shown in Figure 7-11.\n\nFigure 7-11. Settings menu\n\nThen click the “Connect Repo using HTTPS” button and fill the form with the required data as shown in Figure 7-12.\n\nFigure 7-12. Configuration of repository\n\n7.6 Deploy from a Private Git Repository\n\n|\n\n179",
      "content_length": 770,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Finally, click the Connect button to test that it’s possible to establish a connection and add the repository into Argo CD.\n\nThe other way is to create a Kubernetes Secret manifest file with that repository and credentials information:\n\napiVersion: v1 kind: Secret metadata: name: private-repo namespace: argocd labels: argocd.argoproj.io/secret-type: repository stringData: type: git url: https://github.com/argoproj/private-repo password: my-password username: my-username\n\nCreate a secret in the Argo CD namespace\n\nSets secret type as repository\n\nURL of the repository to register\n\nPassword to access\n\nUsername to access\n\nIf you apply this file, it will have the same effect as the manual approach.\n\nAt this point, every time we define a repoURL value in the Application resource with a repository URL registered for authentication, Argo CD will use the registered credentials to log in.\n\nDiscussion In addition to setting credentials such as username and password for accessing a private Git repo, Argo CD also supports other methods such as tokens, TLS client certificates, SSH private keys, or GitHub App credentials.\n\nLet’s see some examples using Argo CD CLI or Kubernetes Secrets.\n\nTo configure a TLS client certificate:\n\nargocd repo add https://repo.example.com/repo.git \\ --tls-client-cert-path ~/mycert.crt \\ --tls-client-cert-key-path ~/mycert.key\n\n180\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "For SSH, you just need to set the location of the SSH private key:\n\nargocd repo add git@github.com:argoproj/argocd-example-apps.git \\ --ssh-privatekey-path ~/.ssh/id_rsa\n\nOr using a Kubernetes Secret:\n\napiVersion: v1 kind: Secret metadata: name: private-repo namespace: argocd labels: argocd.argoproj.io/secret-type: repository stringData: type: git url: git@github.com:argoproj/my-private-repository sshPrivateKey: | -----BEGIN OPENSSH PRIVATE KEY----- ... -----END OPENSSH PRIVATE KEY-----\n\nSets the content of the SSH private key\n\nIf you are using the GitHub App method, you need to set the App ID, the App Installation ID, and the private key:\n\nargocd repo add https://github.com/argoproj/argocd-example-apps.git --github-app- id 1 --github-app-installation-id 2 --github-app-private-key-path test.private- key.pem\n\nOr using the declarative approach:\n\napiVersion: v1 kind: Secret metadata: name: github-repo namespace: argocd labels: argocd.argoproj.io/secret-type: repository stringData: type: git repo: https://ghe.example.com/argoproj/my-private-repository githubAppID: 1 githubAppInstallationID: 2 githubAppEnterpriseBaseUrl: https://ghe.example.com/api/v3 githubAppPrivateKeySecret: | -----BEGIN OPENSSH PRIVATE KEY----- ... -----END OPENSSH PRIVATE KEY-----\n\nSets GitHub App parameters\n\nOnly valid if GitHub App Enterprise is used\n\n7.6 Deploy from a Private Git Repository\n\n|\n\n181",
      "content_length": 1390,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "For the access token, use the account name as the username and the token in the password field.\n\nChoosing which strategy to use will depend on your experience managing Kubernetes Secrets. Remember that a Secret in Kubernetes is not encrypted but encoded in Base64, so it is not secured by default.\n\nWe recommend using only the declarative approach when you’ve got a good strategy for securing the secrets.\n\nWe’ve not discussed the Sealed Secrets project yet (we’ll do so in the following chapter), but when using Sealed Secrets, the labels will be removed to avoid the SealedSecret object having a template section that encodes all the fields you want the controller to put in the unsealed Secret:\n\nspec: ... template: metadata: labels: \"argocd.argoproj.io/secret-type\": repository\n\n7.7 Order Kubernetes Manifests\n\nProblem You want to use Argo CD to deploy.\n\nSolution Use sync waves and resource hooks to modify the default order of applying manifests.\n\nArgo CD applies the Kubernetes manifests (plain, Helm, Kustomize) in a particular order using the following logic:\n\n1. By kind 1.\n\na. a. Namespaces\n\nb. NetworkPolicy b.\n\nc. c. Limit Range\n\nd. d. ServiceAccount\n\ne. e. Secret\n\nf. ConfigMap f.\n\ng. g. StorageClass\n\n182\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "h. h. PersistentVolumes\n\ni. i. ClusterRole\n\nj. j. Role\n\nk. k. Service\n\nl. l. DaemonSet\n\nm. Pod m.\n\nn. n. ReplicaSet\n\no. o. Deployment\n\np. p. StatefulSet\n\nq. q. Job\n\nr. r. Ingress\n\n2. 2. In the same kind, then by name (alphabetical order)\n\nArgo CD has three phases when applying resources: the first phase is executed before applying the manifests (PreSync), the second phase is when the manifests are applied (Sync), and the third phase is executed after all manifests are applied and synchronized (PostSync).\n\nFigure 7-13 summarizes these phases.\n\nFigure 7-13. Hooks and sync waves\n\nResource hooks are scripts executed at a given phase, or if the Sync phase failed, you could run some rollback operations.\n\n7.7 Order Kubernetes Manifests\n\n|\n\n183",
      "content_length": 746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "Table 7-1 lists the available resource hooks.\n\nTable 7-1. Resource hooks\n\nHook\n\nPreSync\n\nDescription Executes prior to the application of the manifests\n\nUse case Database migrations\n\nSync\n\nExecutes at the same time as manifests\n\nComplex rolling update strategies like canary releases or dark launches\n\nPostSync Executes after all Sync hooks have completed and\n\nRun tests to validate deployment was correctly done\n\nwere successful (healthy)\n\nSyncFail Executes when the sync operation fails\n\nRollback operations in case of failure\n\nSkip\n\nSkip the application of the manifest\n\nWhen manual steps are required to deploy the application (i.e., releasing public traffic to new version)\n\nHooks are defined as an annotation named argocd.argoproj.io/hook to a Kuber‐ netes resource. In the following snippet, a PostSync manifest is defined:\n\napiVersion: batch/v1 kind: Job metadata: name: todo-insert annotations: argocd.argoproj.io/hook: PostSync\n\nJob’s name\n\nSets when the manifest is applied\n\nDeletion Policies A hook is not deleted when finished; for example, if you run a Kubernetes Job, it’ll remain Completed.\n\nThis might be the desired state, but we can specify to automatically delete these resources if annotated with argocd.argoproj.io/hook-delete-policy and the pol‐ icy value is set.\n\nSupported policies are:\n\nPolicy HookSucceeded\n\nDescription Deleted after the hook succeeded\n\nHookFailed\n\nDeleted after the hook failed\n\nBeforeHookCreation Deleted before the new one is created\n\n184\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "A sync wave is a way to order how Argo CD applies the manifests stored in Git.\n\nAll manifests have zero waves by default, and the lower values go first. Use the argocd.argoproj.io/sync-wave annotation to set the wave number to a resource.\n\nFor example, you might want to deploy a database first and then create the database schema; for this case, you should set a sync-wave lower in the database deployment file than in the job for creating the database schema, as shown in the following snippet:\n\napiVersion: apps/v1 kind: Deployment metadata: name: postgresql namespace: todo annotations: argocd.argoproj.io/sync-wave: \"0\" ... apiVersion: batch/v1 kind: Job metadata: name: todo-table namespace: todo annotations: argocd.argoproj.io/sync-wave: \"1\"\n\nPostgreSQL deployment\n\nSync wave for PostgreSQL deployment is 0\n\nName of the Job\n\nJob executed when PostgreSQL is healthy\n\nDiscussion When Argo CD starts applying the manifests, it orders the resources in the following way:\n\n1. 1. Phase\n\n2. 2. Wave (lower precedence first)\n\n3. Kind 3.\n\n4. Name 4.\n\nLet’s deploy a more significant application with deployment files, sync waves, and hooks.\n\n7.7 Order Kubernetes Manifests\n\n|\n\n185",
      "content_length": 1179,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "The sample application deployed is a TODO application connected with a database (PostgreSQL) to store TODOs. To deploy the application, some particular order needs to be applied; for example, the database server must be running before creating the database schema. Also, when the whole application is deployed, we insert some default TODOs into the database to run a post-sync manifest.\n\nThe overall process is shown in Figure 7-14.\n\nFigure 7-14. Todo app\n\n186\n\n|\n\nChapter 7: Argo CD",
      "content_length": 483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Create an Application resource pointing out to the application:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: todo-app namespace: argocd spec: destination: namespace: todo server: https://kubernetes.default.svc project: default source: path: ch07/todo repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main syncPolicy: automated: prune: true selfHeal: false syncOptions: - CreateNamespace=true\n\nIn the terminal, apply the resource, and Argo CD will deploy all applications in the specified order.\n\nSee Also\n\n• gitops-engine/sync_tasks.go on GitHub\n\n7.8 Define Synchronization Windows\n\nProblem You want Argo CD to block or allow application synchronization depending on time.\n\nSolution Argo CD has the sync windows concept to configure time windows where application synchronizations (applying new resources that have been pushed to the repository) will either be blocked or allowed.\n\nTo define a sync window, create an AppProject manifest setting the kind (either allow or deny), a schedule in cron format to define the initial time, a duration of the window, and which resources the sync window is applied to (Application, namespaces, or clusters).\n\n7.8 Define Synchronization Windows\n\n|\n\n187",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "About Cron Expressions\n\nA cron expression represents a time. It’s composed of the following fields:\n\n┌────────── minute (0 - 59) │ ┌────────── hour (0 - 23) │ │ ┌────────── day of the month (1 - 31) │ │ │ ┌────────── month (1 - 12) │ │ │ │ ┌────────── day of the week (0 - 6) * * * * *\n\nThe AppProject resource is responsible for defining these windows where synchroni‐ zations are permitted/blocked.\n\nCreate a new file to permit synchronizations only from 22:00 to 23:00 (just one hour) and for Argo CD Applications whose names end in -prod:\n\napiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: default spec: syncWindows: - kind: allow schedule: '0 22 * * *' duration: 1h applications: - '*-prod'\n\nList of windows\n\nAllow syncs\n\nOnly at 22:00\n\nFor 1 hour (23:00)\n\nSets the applications that affect this window\n\nRegular expression matching any application whose name ends with -prod\n\nDiscussion We cannot perform a sync of the application (neither automatic nor manual) when it’s not the time configured in the time window defined in the AppProject manifest. However, we can configure a window to allow manual syncs.\n\n188\n\n|\n\nChapter 7: Argo CD",
      "content_length": 1157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Using the CLI tool:\n\nargocd proj windows enable-manual-sync <PROJECT ID>\n\nAlso, manual sync can be set in the YAML file. In the following example, we’re setting manual synchronization for the namespace default, denying synchronizations at 22:00 for one hour and allowing synchronizations in prod-cluster at 23:00 for one hour:\n\napiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: default namespace: argocd spec: syncWindows: - kind: deny schedule: '0 22 * * *' duration: 1h manualSync: true namespaces: - bgd - kind: allow schedule: '0 23 * * *' duration: 1h clusters: - prod-cluster\n\nBlock synchronizations\n\nEnable manual sync to default namespace\n\nConfigure namespaces to block\n\nConfigure clusters to allow syncs at 23:00\n\nWe can inspect the current windows from the UI by going to the Settings → Projects → default → windows tab or by using the argocd CLI tool:\n\nargocd proj windows list default\n\nID STATUS KIND SCHEDULE DURATION APPLICATIONS NAMESPACES CLUSTERS MANUALSYNC 0 Inactive deny 0 22 * * * 1h - bgd - Enabled 1 Inactive allow 0 23 * * * 1h - - prod-cluster Disabled\n\n7.8 Define Synchronization Windows\n\n|\n\n189",
      "content_length": 1137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "CHAPTER 8 Advanced Topics\n\nIn the previous chapter, you had an overview of implementing GitOps workflows using Argo CD recipes. Argo CD is a famous and influential open source project that helps with both simple use cases and more advanced ones. In this chapter, we will discuss topics needed when you move forward in your GitOps journey, and you need to manage security, automation, and advanced deployment models for multicluster scenarios.\n\nSecurity is a critical aspect of automation and DevOps. DevSecOps is a new definition of an approach where security is a shared responsibility throughout the entire IT lifecycle. Furthermore, the DevSecOps Manifesto specifies security as code to operate and contribute value with less friction. And this goes in the same direction as GitOps principles, where everything is declarative.\n\nOn the other hand, this also poses the question of avoiding storing unencrypted plain-text credentials in Git. As stated in the book Path to GitOps by Christian Hernandez, Argo CD luckily currently provides two patterns to manage security in GitOps workflows:\n\n• Storing encrypted secrets in Git, such as with a Sealed Secret (see Recipe 8.1)\n\nStoring secrets in external services or vaults, then storing only the reference to • such secrets in Git (see Recipe 8.2)\n\nThe chapter then moves to advanced deployment techniques, showing how to manage webhooks with Argo CD (see Recipe 8.3) and with ApplicationSets (see Recipe 8.4). ApplicationSets is a component of Argo CD that allows management deployments of many applications, repositories, or clusters from a single Kubernetes resource. In essence, a templating system for the GitOps application is ready to be deployed and synced in multiple Kubernetes clusters (see Recipe 8.5).\n\n191",
      "content_length": 1768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Last but not least, the book ends with a recipe on Progressive Delivery for Kuber‐ netes with Argo Rollouts (Recipe 8.6), useful for deploying the application using an advanced deployment technique such as blue-green or canary.\n\n8.1 Encrypt Sensitive Data (Sealed Secrets)\n\nProblem You want to manage Kubernetes Secrets and encrypted objects in Git.\n\nSolution Sealed Secrets is an open source project by Bitnami used to encrypt a Kubernetes Secrets into a SealedSecret Kubernetes Custom Resource, representing an encrypted object safe to store in Git.\n\nSealed Secrets uses public-key cryptography and consists of two main components:\n\nA Kubernetes controller that has knowledge about the private and public key • used to decrypt and encrypt encrypted secrets and is responsible for reconcilia‐ tion. The controller also supports automatic secret rotation for the private key and key expiration management in order to enforce the re-encryption of secrets. • kubeseal, a CLI used by developers to encrypt their secrets before committing • them to a Git repository.\n\nThe SealedSecret object is encrypted and decrypted only by the SealedSecret controller running in the target Kubernetes cluster. This operation is exclusive only to this component, thus nobody else can decrypt the object. The kubeseal CLI allows the developer to take a normal Kubernetes Secret resource and convert it to a SealedSecret resource definition as shown in Figure 8-1.\n\nIn your Kubernetes cluster with Argo CD, you can install the kubeseal CLI for your operating system from the GitHub project’s releases. At the time of writing this book, we are using version 0.18.2.\n\nOn macOS, kubeseal is available through Homebrew as follows:\n\nbrew install kubeseal\n\n192\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 1765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "Figure 8-1. Sealed Secrets with GitOps\n\nAfter you install the CLI, you can install the controller as follows:\n\nkubectl create \\ -f https://github.com/bitnami-labs/sealed-secrets/releases/download/0.18.2/control- ler.yaml\n\nYou should have output similar to the following:\n\nserviceaccount/sealed-secrets-controller created deployment.apps/sealed-secrets-controller created customresourcedefinition.apiextensions.k8s.io/sealedsecrets.bitnami.com created service/sealed-secrets-controller created rolebinding.rbac.authorization.k8s.io/sealed-secrets-controller created rolebinding.rbac.authorization.k8s.io/sealed-secrets-service-proxier created role.rbac.authorization.k8s.io/sealed-secrets-service-proxier created role.rbac.authorization.k8s.io/sealed-secrets-key-admin created clusterrolebinding.rbac.authorization.k8s.io/sealed-secrets-controller created clusterrole.rbac.authorization.k8s.io/secrets-unsealer created\n\nAs an example, let’s create a Secret for the Pac-Man game deployed in Chapter 5:\n\nkubectl create secret generic pacman-secret \\ --from-literal=user=pacman \\ --from-literal=pass=pacman\n\nYou should have the following output:\n\nsecret/pacman-secret created\n\nAnd here you can see the YAML representation:\n\nkubectl get secret pacman-secret -o yaml\n\n8.1 Encrypt Sensitive Data (Sealed Secrets)\n\n|\n\n193",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "apiVersion: v1 data: pass: cGFjbWFu user: cGFjbWFu kind: Secret metadata: name: pacman-secret namespace: default type: Opaque\n\nNow, you can convert the Secret into a SealedSecret in this way:\n\nkubectl get secret pacman-secret -o yaml \\ | kubeseal -o yaml > pacman-sealedsecret.yaml\n\napiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: creationTimestamp: null name: pacman-secret namespace: default spec: encryptedData: pass: AgBJR1AgZ5Gu5NOVsG1E8SKBcdB3QSDdzZka3RRYuWV7z8g7ccQ0dGc1suVOP8wX/ ZpPmIMp8+urPYG62k4EZRUjuu/Vg2E1nSbsGBh9eKu3NaO6tGSF3eGk6PzN6XtRhDeER4u7MG5pj/ +FXRAKcy8Z6RfzbVEGq/QJQ4z0ecSNdJmG07ERMm1Q+lPNGvph2Svx8aCgFLqRsdLhFyvwb TyB3XnmFHrPr+2DynxeN8XVMoMkRYXgVc6GAoxUK7CnC3Elpuy7lIdPwc5QBx9kUVfra83LX8/KxeaJ wyCqvscIGjtcxUtpTpF5jm1t1DSRRNbc4m+7pTwTmnRiUuaMVeujaBco4521yTkh5iEPjnj vUt+VzK01NVoeNunqIazp15rFwTvmiQ5PAtbiUXpT733zCr60QBgSxPg31vw98+u+RcIHvaMIoDCqaX xUdcn2JkUF+bZXtxNmIRTAiQVQ1vEPmrZxpvZcUh/PPC4L/RFWrQWnOzKRyqLq9wRoSLPbKyvMX naxH0v3USGIktmtJlGjlXoW/i+HIoSeMFS0mUAzOF5M5gweOhtxKGh3Y74ZDn5PbVA/ 9kbkuWgvPNGDZL924Dm6AyM5goHECr/RRTm1e22K9BfPASARZuGA6paqb9h1XEqyqesZgM0R8PLiy Luu+tpqydR0SiYLc5VltdjzpIyyy9Xmw6Aa3/4SB+4tSwXSUUrB5yc= user: AgBhYDZQzOwinetPceZL897aibTYp4QPGFvP6ZhDyuUAx OWXBQ7jBA3KPUqLvP8vBcxLAcS7HpKcDSgCdi47D2WhShdBR4jWJufwKmR3j+ayTdw72t3ALpQhTYI0iMY TiNdR0/o3vf0jeNMt/oWCRsifqBxZaIShE53rAFEjEA6D7CuCDXu8BHk1DpSr79d5Au4puzpH VODh+v1T+Yef3k7DUoSnbYEh3CvuRweiuq5lY8G0oob28j38wdyxm3GIrexa+M/ ZIdO1hxZ6jz4edv6ejdZfmQNdru3c6lmljWwcO+0Ue0MqFi4ZF/YNUsiojI+781n1m3K/ giKcyPLn0skD7DyeKPoukoN6W5P71OuFSkF+VgIeejDaxuA7bK3PEaUgv79KFC9aEEnBr/ 7op7HY7X6aMDahmLUc/+zDhfzQvwnC2wcj4B8M2OBFa2ic2PmGzrIWhlBbs1OgnpehtG SETq+YRDH0alWOdFBq1U8qn6QA8Iw6ewu8GTele3zlPLaADi5O6LrJbIZNlY0+PutWfjs9ScVVEJy+I9BGd yT6tiA/4v4cxH6ygG6NzWkqxSaYyNrWWXtLhOlqyCpTZ tUwHnF+OLB3gCpDZPx+NwTe2Kn0jY0c83LuLh5PJ090AsWWqZaRQyE LeL6y6mVekQFWHGfK6t57Vb7Z3+5XJCgQn+xFLkj3SIz0ME5D4+DSsUDS1fyL8uI= template: data: null metadata: creationTimestamp: null name: pacman-secret namespace: default type: Opaque\n\nHere you find the data encrypted by the Sealed Secrets controller.\n\n194\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 2104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "Now you can safely push your SealedSecret to your Kubernetes manifests repo and create the Argo CD application. Here’s an example from this book’s repository:\n\nargocd app create pacman \\ --repo https://github.com/gitops-cookbook/pacman-kikd-manifests.git \\ --path 'k8s/sealedsecrets' \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace default \\ --sync-policy auto\n\nCheck if the app is running and healthy:\n\nargocd app list\n\nYou should get output similar to the following:\n\nNAME CLUSTER NAMESPACE PROJECT STATUS HEALTH ↳ SYNCPOLICY CONDITIONS REPO PATH TARGET pacman https://kubernetes.default.svc default default Synced Healthy↳ <none> <none> https://github.com/gitops-cookbook/pacman-kikd- manifests.git k8s/sealedsecrets\n\n8.2 Encrypt Secrets with ArgoCD (ArgoCD + HashiCorp Vault + External Secret)\n\nProblem You want to avoid storing credentials in Git and you want to manage them in external services or vaults.\n\nSolution In Recipe 8.1 you saw how to manage encrypted data in Git following the GitOps declarative way, but how do you avoid storing even encrypted credentials with GitOps?\n\nOne solution is External Secrets, an open source project initially created by GoDaddy, which aims at storing secrets in external services or vaults from different vendors, then storing only the reference to such secrets in Git.\n\nToday, External Secrets supports systems such as AWS Secrets Manager, HashiCorp Vault, Google Secrets Manager, Azure Key Vault, and more. The idea is to provide a user-friendly abstraction for the external API that stores and manages the lifecycles of the secrets.\n\nIn depth, ExternalSecrets is a Kubernetes controller that reconciles Secrets into the cluster from a Custom Resource that includes a reference to a secret in an external key management system. The Custom Resource SecretStore specifies the backend\n\n8.2 Encrypt Secrets with ArgoCD (ArgoCD + HashiCorp Vault + External Secret)\n\n|\n\n195",
      "content_length": 1933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "containing the confidential data, and how it should be transformed into a Secret by defining a template, as you can see in Figure 8-2. The SecretStore has the configura‐ tion to connect to the external secret manager.\n\nThus, the ExternalSecrets objects can be safely stored in Git, as they do not contain any confidential information, but just the references to the external services manag‐ ing credentials.\n\nFigure 8-2. External Secrets with Argo CD\n\nYou can install External Secrets with a Helm Chart as follows. At the time of writing this book, we are using version 0.5.9:\n\nhelm repo add external-secrets https://charts.external-secrets.io\n\nhelm install external-secrets \\ external-secrets/external-secrets \\ -n external-secrets \\ --create-namespace\n\nYou should get output similar to the following:\n\nNAME: external-secrets LAST DEPLOYED: Fri Sep 2 13:09:53 2022 NAMESPACE: external-secrets STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: external-secrets has been deployed successfully!\n\n196\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "In order to begin using ExternalSecrets, you will need to set up a SecretStore or ClusterSecretStore resource (for example, by creating a vault SecretStore).\n\nMore information on the different types of SecretStores and how to configure them can be found in our GitHub page.\n\nYou can also install the External Secrets Operator with OLM from OperatorHub.io.\n\nAs an example with one of the providers supported, such as HashiCorp Vault, you can do the following.\n\nFirst download and install HashiCorp Vault for your operating system and get your Vault Token. Then create a Kubernetes Secret as follows:\n\nexport VAULT_TOKEN=<YOUR_TOKEN> kubectl create secret generic vault-token \\ --from-literal=token=$VAULT_TOKEN \\ -n external-secrets\n\nThen create a SecretStore as a reference to this external system:\n\napiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: vault-secretstore namespace: default spec: provider: vault: server: \"http://vault.local:8200\" path: \"secret\" version: \"v2\" auth: tokenSecretRef: name: \"vault-token\" key: \"token\" namespace: external-secrets\n\nHostname where your Vault is running\n\nName of the Kubernetes Secret containing the vault token\n\nKey to address the value in the Kubernetes Secret containing the vault token content:\n\nkubectl create -f vault-secretstore.yaml\n\n8.2 Encrypt Secrets with ArgoCD (ArgoCD + HashiCorp Vault + External Secret)\n\n|\n\n197",
      "content_length": 1390,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "Now you can create a Secret in your Vault as follows:\n\nvault kv put secret/pacman-secrets pass=pacman\n\nAnd then reference it from the ExternalSecret as follows:\n\napiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: pacman-externalsecrets namespace: default spec: refreshInterval: \"15s\" secretStoreRef: name: vault-secretstore kind: SecretStore target: name: pacman-externalsecrets data: - secretKey: token remoteRef: key: secret/pacman-secrets property: pass\n\nkubectl create -f pacman-externalsecrets.yaml\n\nNow you can deploy the Pac-Man game with Argo CD using External Secrets as follows:\n\nargocd app create pacman \\ --repo https://github.com/gitops-cookbook/pacman-kikd-manifests.git \\ --path 'k8s/externalsecrets' \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace default \\ --sync-policy auto\n\n8.3 Trigger the Deployment of an Application Automatically (Argo CD Webhooks)\n\nProblem You don’t want to wait for Argo CD syncs and you want to immediately deploy an application when a change occurs in Git.\n\nSolution While Argo CD polls Git repositories every three minutes to detect changes to the monitored Kubernetes manifests, it also supports an event-driven approach with webhooks notifications from popular Git servers such as GitHub, GitLab, or Bitbucket.\n\n198\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 1337,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Argo CD Webhooks are enabled in your Argo CD installation and available at the endpoint /api/webhooks.\n\nTo test webhooks with Argo CD using Minikube you can use Helm to install a local Git server such as Gitea, an open source lightweight server written in Go, as follows:\n\nhelm repo add gitea-charts https://dl.gitea.io/charts/ helm install gitea gitea-charts/gitea\n\nYou should have output similar to the following:\n\nhelm install gitea gitea-charts/gitea \"gitea-charts\" has been added to your repositories NAME: gitea LAST DEPLOYED: Fri Sep 2 15:04:04 2022 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Get the application URL by running these commands: echo \"Visit http://127.0.0.1:3000 to use your application\" kubectl --namespace default port-forward svc/gitea-http 3000:3000\n\nLog in to the Gitea server with the default credentials you find the in the values.yaml file from the Helm Chart here or define new ones via overriding them.\n\nImport the Pac-Man manifests repo into Gitea.\n\nConfigure the Argo app:\n\nargocd app create pacman-webhook \\ --repo http://gitea-http.default.svc:3000/gitea_admin/pacman-kikd-manifests.git \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace default \\ --path k8s \\ --sync-policy auto\n\nTo add a webhook to Gitea, navigate to the top-right corner and click Settings. Select the Webhooks tab and configure it as shown in Figure 8-3:\n\nPayload URL: http://localhost:9090/api/webhooks • • Content type: application/json •\n\n8.3 Trigger the Deployment of an Application Automatically (Argo CD Webhooks)\n\n|\n\n199",
      "content_length": 1564,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "Figure 8-3. Gitea Webhooks\n\nYou can omit the Secret for this example; however, it’s best practice to configure secrets for your webhooks. Read more from the docs.\n\nSave it and push your change to the repo on Gitea. You will see a new sync from Argo CD immediately after your push.\n\n8.4 Deploy to Multiple Clusters\n\nProblem You want to deploy an application to different clusters.\n\nSolution Argo CD supports the ApplicationSet resource to “templetarize” an Argo CD Application resource. It covers different use cases, but the most important are:\n\n200\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "• Use a Kubernetes manifest to target multiple Kubernetes clusters.\n\n• Deploy multiple applications from one or multiple Git repositories.\n\nSince the ApplicationSet is a template file with placeholders to substitute at run‐ time, we need to feed these with some values. For this purpose, ApplicationSet has the concept of generators.\n\nA generator is responsible for generating the parameters, which will finally be replaced in the template placeholders to generate a valid Argo CD Application.\n\nCreate the following ApplicationSet:\n\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: bgd-app namespace: argocd spec: generators: - list: elements: - cluster: staging url: https://kubernetes.default.svc location: default - cluster: prod url: https://kubernetes.default.svc location: app template: metadata: name: '{{cluster}}-app' spec: project: default source: repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main path: ch08/bgd-gen/{{cluster}} destination: server: '{{url}}' namespace: '{{location}}' syncPolicy: syncOptions: - CreateNamespace=true\n\nDefines a generator\n\nSets the value of the parameters\n\nDefines the Application resource as a template\n\ncluster placeholder\n\n8.4 Deploy to Multiple Clusters\n\n|\n\n201",
      "content_length": 1267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "url placeholder\n\nApply the previous file by running the following command:\n\nkubectl apply -f bgd-application-set.yaml\n\nWhen this ApplicationSet is applied to the cluster, Argo CD generates and automat‐ ically registers two Application resources. The first one is:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: staging-app spec: project: default source: path: ch08/bgd-gen/staging repoURL: https://github.com/example/app.git targetRevision: HEAD destination: namespace: default server: https://kubernetes.default.svc ...\n\nAnd the second one:\n\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: prod-app spec: project: default source: path: ch08/bgd-gen/prod repoURL: https://github.com/example/app.git targetRevision: HEAD destination: namespace: app server: https://kubernetes.default.svc ...\n\nInspect the creation of both Application resources by running the following command:\n\n# Remember to login first argocd login --insecure --grpc-web $argoURL --username admin --password $argoPass\n\nargocd app list\n\nAnd the output should be similar to (trunked):\n\nNAME CLUSTER NAMESPACE prod-app https://kubernetes.default.svc app staging-app https://kubernetes.default.svc default\n\n202\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 1245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Delete both applications by deleting the ApplicationSet file:\n\nkubectl delete -f bgd-application-set.yaml\n\nDiscussion We’ve seen the simplest generator, but there are eight generators in total at the time of writing this book:\n\nList\n\nGenerates Application definitions through a fixed list of clusters. (It’s the one we’ve seen previously).\n\nCluster\n\nSimilar to List but based on the list of clusters defined in Argo CD.\n\nGit\n\nGenerates Application definitions based on a JSON/YAML properties file within a Git repository or based on the directory layout of the repository.\n\nSCM Provider\n\nGenerates Application definitions from repositories within an organization.\n\nPull Request\n\nGenerates Application definitions from open pull requests.\n\nCluster Decision Resource\n\nGenerates Application definitions using duck-typing.\n\nMatrix\n\nCombines values of two separate generators.\n\nMerge\n\nMerges values from two or more generators.\n\nIn the previous example, we created the Application objects from a fixed list of elements. This is fine when the number of configurable environments is small; in the example, two clusters refer to two Git folders (ch08/bgd-gen/staging and ch08/ bgd-gen/prod). In the case of multiple environments (which means various folders), we can dynamically use the Git generator to generate one Application per directory.\n\nLet’s migrate the previous example to use the Git generator. As a reminder, the Git directory layout used was:\n\nbgd-gen ├── staging │ ├── ...yaml └── prod ├── ...yaml\n\n8.4 Deploy to Multiple Clusters\n\n|\n\n203",
      "content_length": 1544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "Create a new file of type ApplicationSet generating an Application for each direc‐ tory of the configured Git repo:\n\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: cluster-addons namespace: openshift-gitops spec: generators: - git: repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git revision: main directories: - path: ch08/bgd-gen/* template: metadata: name: '{{path[0]}}{{path[2]}}' spec: project: default source: repoURL: https://github.com/gitops-cookbook/gitops-cookbook-sc.git targetRevision: main path: '{{path}}' destination: server: https://kubernetes.default.svc namespace: '{{path.basename}}'\n\nConfigures the Git repository to read layout\n\nInitial path to start scanning directories\n\nApplication definition\n\nThe directory paths within the Git repository matching the path wildcard (stag ing or prod)\n\nDirectory path (full path)\n\nThe rightmost pathname\n\nApply the resource:\n\nkubectl apply -f bgd-git-application-set.yaml\n\nArgo CD creates two applications as there are two directories:\n\nargocd app list\n\nNAME CLUSTER NAMESPACE ch08prod https://kubernetes.default.svc prod ch08staging https://kubernetes.default.svc staging\n\n204\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Also, this generator is handy when your application is composed of different compo‐ nents (service, database, distributed cache, email server, etc.), and deployment files for each element are placed in other directories. Or, for example, a repository with all operators required to be installed in the cluster:\n\napp ├── tekton-operator │ ├── ...yaml ├── prometheus-operator │ ├── ...yaml └── istio-operator ├── ...yaml\n\nInstead of reacting to directories, Git generator can create Application objects with parameters specified in JSON/YAML files.\n\nThe following snippet shows an example JSON file:\n\n{ \"cluster\": { \"name\": \"staging\", \"address\": \"https://1.2.3.4\" } }\n\nThis is an excerpt of the ApplicationSet to react to these files:\n\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: guestbook spec: generators: - git: repoURL: https://github.com/example/app.git revision: HEAD files: - path: \"app/**/config.json\" template: metadata: name: '{{cluster.name}}-app' ....\n\nFinds all config.json files placed in all subdirectories of the app\n\nInjects the value set in config.json\n\nThis ApplicationSet will generate one Application for each config.json file in the folders matching the path expression.\n\n8.4 Deploy to Multiple Clusters\n\n|\n\n205",
      "content_length": 1257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "See Also\n\n• Argo CD Generators\n\n• Duck Types\n\n8.5 Deploy a Pull Request to a Cluster\n\nProblem You want to deploy a preview of the application when a pull request is created.\n\nSolution Use the pull request generator to automatically discover open pull requests within a repository and create an Application object.\n\nLet’s create an ApplicationSet reacting to any GitHub pull request annotated with the preview label created on the configured repository.\n\nCreate a new file named bgd-pr-application-set.yaml with the following content:\n\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: myapps namespace: openshift-gitops spec: generators: - pullRequest: github: owner: gitops-cookbook repo: gitops-cookbook-sc labels: - preview requeueAfterSeconds: 60 template: metadata: name: 'myapp-{{branch}}-{{number}}' spec: source: repoURL: 'https://github.com/gitops-cookbook/gitops-cookbook-sc.git' targetRevision: '{{head_sha}}' path: ch08/bgd-pr project: default destination: server: https://kubernetes.default.svc namespace: '{{branch}}-{{number}}'\n\nGitHub pull request generator\n\n206\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Organization/user\n\nRepository\n\nSelect the target PRs\n\nPolling time in seconds to check if there is a new PR (60 seconds)\n\nSets the name with branch name and number\n\nSets the Git SHA number\n\nApply the previous file by running the following command:\n\nkubectl apply -f bgd-pr-application-set.yaml\n\nNow, if you list the Argo CD applications, you’ll see that none are registered. The reason is there is no pull request yet in the repository labeled with preview:\n\nargocd app list NAME CLUSTER NAMESPACE PROJECT STATUS\n\nCreate a pull request against the repository and label it with preview.\n\nIn GitHub, the pull request window should be similar to Figure 8-4.\n\nFigure 8-4. Pull request in GitHub\n\nWait for one minute until the ApplicationSet detects the change and creates the Application object.\n\nRun the following command to inspect that the change has been detected and registered:\n\n8.5 Deploy a Pull Request to a Cluster\n\n|\n\n207",
      "content_length": 927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "kubectl describe applicationset myapps -n argocd\n\n... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal created 23s applicationset-controller created Applica- tion \"myapp-lordofthejars-patch-1-1\" Normal unchanged 23s (x2 over 23s) applicationset-controller unchanged Appli- cation \"myapp-lordofthejars-patch-1-1\"\n\nCheck the registration of the Application to the pull request:\n\nargocd app list NAME CLUSTER NAMESPACE myapp-lordofthejars-patch-1-1 https://kubernetes.default.svc lordofthejars- patch-1-1\n\nThe Application object is automatically removed when the pull request is closed.\n\nDiscussion At the time of writing this book, the following pull request providers are supported:\n\n• GitHub\n\n• Bitbucket\n\n• Gitea\n\n• GitLab\n\nThe ApplicationSet controller polls every requeueAfterSeconds interval to detect changes but also supports using webhook events.\n\nTo configure it, follow Recipe 8.3, but also enable sending pull requests events too in the Git provider.\n\n8.6 Use Advanced Deployment Techniques\n\nProblem You want to deploy the application using an advanced deployment technique such as blue-green or canary.\n\nSolution Use the Argo Rollouts project to roll out updates to an application.\n\n208\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 1253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Argo Rollouts is a Kubernetes controller providing advanced deployment techniques such as blue-green, canary, mirroring, dark canaries, traffic analysis, etc. to Kuber‐ netes. It integrates with many Kubernetes projects like Ambassador, Istio, AWS Load Balancer Controller, NGNI, SMI, or Traefik for traffic management, and projects like Prometheus, Datadog, and New Relic to perform analysis to drive progressive delivery.\n\nTo install Argo Rollouts to the cluster, run the following command in a terminal window:\n\nkubectl create namespace argo-rollouts\n\nkubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/relea- ses/download/v1.2.2/install.yaml ... clusterrolebinding.rbac.authorization.k8s.io/argo-rollouts created secret/argo-rollouts-notification-secret created service/argo-rollouts-metrics created deployment.apps/argo-rollouts created\n\nAlthough it’s not mandatory, we recommend you install the Argo Rollouts Kubectl Plugin to visualize rollouts. Follow the instructions to install it. With everything in place, let’s deploy the initial version of the BGD application.\n\nArgo Rollouts doesn’t use the standard Kubernetes Deployment file, but a specific new Kubernetes resource named Rollout. It’s like a Deployment object, hence all its options are supported, but it adds some fields to configure the rolling update.\n\nLet’s deploy the first version of the application. We’ll define the canary release process when Kubernetes executes a rolling update, which in this case follows these steps:\n\n1. 1. Forward 20% of traffic to the new version.\n\n2. 2. Wait until a human decides to proceed with the process.\n\n3. Forward 40%, 60%, 80% of the traffic to the new version automatically, waiting 3. 30 seconds between every increase.\n\nCreate a new file named bgd-rollout.yaml with the following content:\n\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: bgd-rollouts spec: replicas: 5 strategy: canary: steps: - setWeight: 20 - pause: {}\n\n8.6 Use Advanced Deployment Techniques\n\n|\n\n209",
      "content_length": 2021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "setWeight: 40 - pause: {duration: 30s} - setWeight: 60 - pause: {duration: 30s} - setWeight: 80 - pause: {duration: 30s} revisionHistoryLimit: 2 selector: matchLabels: app: bgd-rollouts template: metadata: creationTimestamp: null labels: app: bgd-rollouts spec: containers: - image: quay.io/rhdevelopers/bgd:1.0.0 name: bgd env: - name: COLOR value: \"blue\" resources: {} Sets the ratio of canary\n\nRollout is paused\n\nPauses the rollout for 30 seconds\n\ntemplate Deployment definition\n\nApply the resource to deploy the application. Since there is no previous deployment, the canary part is ignored:\n\nkubectl apply -f bgd-rollout.yaml\n\nCurrently, there are five pods as specified in the replicas field:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE bgd-rollouts-679cdfcfd-6z2zf 1/1 Running 0 12m bgd-rollouts-679cdfcfd-8c6kl 1/1 Running 0 12m bgd-rollouts-679cdfcfd-8tb4v 1/1 Running 0 12m bgd-rollouts-679cdfcfd-f4p7f 1/1 Running 0 12m bgd-rollouts-679cdfcfd-tljfr 1/1 Running 0 12m\n\n210\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "And using the Argo Rollout Kubectl Plugin:\n\nkubectl argo rollouts get rollout bgd-rollouts\n\nName: bgd-rollouts Namespace: default Status: ✔ Healthy Strategy: Canary Step: 8/8 SetWeight: 100 ActualWeight: 100 Images: quay.io/rhdevelopers/bgd:1.0.0 (stable) Replicas: Desired: 5 Current: 5 Updated: 5 Ready: 5 Available: 5\n\nNAME KIND STATUS AGE INFO ⟳ bgd-rollouts Rollout ✔ Healthy 13m └──# revision:1 └──⧉ bgd-rollouts-679cdfcfd ReplicaSet ✔ Healthy 13m stable ├──□ bgd-rollouts-679cdfcfd-6z2zf Pod ✔ Running 13m ready:1/1 ├──□ bgd-rollouts-679cdfcfd-8c6kl Pod ✔ Running 13m ready:1/1 ├──□ bgd-rollouts-679cdfcfd-8tb4v Pod ✔ Running 13m ready:1/1 ├──□ bgd-rollouts-679cdfcfd-f4p7f Pod ✔ Running 13m ready:1/1 └──□ bgd-rollouts-679cdfcfd-tljfr Pod ✔ Running 13m ready:1/1\n\nLet’s deploy a new version to trigger a canary rolling update. Create a new file named bgd-rollout-v2.yaml with exactly the same content as the previous one, but change the environment variable COLOR value to green:\n\n... name: bgd env: - name: COLOR value: \"green\" resources: {}\n\nApply the previous resource and check how Argo Rollouts executes the rolling update. List the pods again to check that 20% of the pods are new while the other 80% are the old version:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE bgd-rollouts-679cdfcfd-6z2zf 1/1 Running 0 27m bgd-rollouts-679cdfcfd-8c6kl 1/1 Running 0 27m bgd-rollouts-679cdfcfd-8tb4v 1/1 Running 0 27m bgd-rollouts-679cdfcfd-tljfr 1/1 Running 0 27m bgd-rollouts-c5495c6ff-zfgvn 1/1 Running 0 13s\n\n8.6 Use Advanced Deployment Techniques\n\n|\n\n211",
      "content_length": 1568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "New version pod\n\nAnd do the same using the Argo Rollout Kubectl Plugin:\n\nkubectl argo rollouts get rollout bgd-rollouts\n\n... NAME KIND STATUS AGE INFO ⟳ bgd-rollouts Rollout ॥ Paused 31m ├──# revision:2 │ └──⧉ bgd-rollouts-c5495c6ff ReplicaSet ✔ Healthy 3m21s canary │ └──□ bgd-rollouts-c5495c6ff-zfgvn Pod ✔ Running 3m21s ready:1/1 └──# revision:1 └──⧉ bgd-rollouts-679cdfcfd ReplicaSet ✔ Healthy 31m stable ├──□ bgd-rollouts-679cdfcfd-6z2zf Pod ✔ Running 31m ready:1/1 ├──□ bgd-rollouts-679cdfcfd-8c6kl Pod ✔ Running 31m ready:1/1 ├──□ bgd-rollouts-679cdfcfd-8tb4v Pod ✔ Running 31m ready:1/1 └──□ bgd-rollouts-679cdfcfd-tljfr Pod ✔ Running 31m ready:1/1\n\nRemember that the rolling update process is paused until the operator executes a manual step to let the process continue. In a terminal window, run the following command:\n\nkubectl argo rollouts promote bgd-rollouts\n\nThe rollout is promoted and continues with the following steps, which is substituting the old version pods with new versions every 30 seconds:\n\nkubectl get pods\n\nNAME READY STATUS RESTARTS AGE bgd-rollouts-c5495c6ff-2g7r8 1/1 Running 0 89s bgd-rollouts-c5495c6ff-7mdch 1/1 Running 0 122s bgd-rollouts-c5495c6ff-d9828 1/1 Running 0 13s bgd-rollouts-c5495c6ff-h4t6f 1/1 Running 0 56s bgd-rollouts-c5495c6ff-zfgvn 1/1 Running 0 11m\n\nThe rolling update finishes with the new version progressively deployed to the cluster.\n\nDiscussion Kubernetes doesn’t implement advanced deployment techniques natively. For this reason, Argo Rollouts uses the number of deployed pods to implement the canary release.\n\nAs mentioned before, Argo Rollouts integrates with Kubernetes products that offer advanced traffic management capabilities like Istio.\n\nUsing Istio, the traffic splitting is done correctly at the infrastructure level instead of playing with replica numbers like in the first example. Argo Rollouts integrates with Istio to execute a canary release, automatically updating the Istio VirtualService object.\n\n212\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 2012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Assuming you already know Istio and have a Kubernetes cluster with Istio installed, you can perform integration between Argo Rollouts and Istio by setting the trafficRouting from Rollout resource to Istio.\n\nFirst, create a Rollout file with Istio configured:\n\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: bgdapp labels: app: bgdapp spec: strategy: canary: steps: - setWeight: 20 - pause: duration: \"1m\" - setWeight: 50 - pause: duration: \"2m\" canaryService: bgd-canary stableService: bgd trafficRouting: istio: virtualService: name: bgd routes: - primary replicas: 1 revisionHistoryLimit: 2 selector: matchLabels: app: bgdapp version: v1 template: metadata: labels: app: bgdapp version: v1 annotations: sidecar.istio.io/inject: \"true\" spec: containers: - image: quay.io/rhdevelopers/bgd:1.0.0 name: bgd env: - name: COLOR value: \"blue\" resources: {}\n\n8.6 Use Advanced Deployment Techniques\n\n|\n\n213",
      "content_length": 915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "Canary section\n\nReference to a Kubernetes Service pointing to the new service version\n\nReference to a Kubernetes Service pointing to the old service version\n\nConfigures Istio\n\nReference to the VirtualService where weight is updated\n\nName of the VirtualService\n\nRoute name within VirtualService\n\nDeploys the Istio sidecar container\n\nThen, we create two Kubernetes Services pointing to the same deployment used to redirect traffic to the old or the new one.\n\nThe following Kubernetes Service is used in the stableService field:\n\napiVersion: v1 kind: Service metadata: name: bgd labels: app: bgdapp spec: ports: - name: http port: 8080 selector: app: bgdapp\n\nAnd the Canary one is the same but with a different name. It’s the one used in the canaryService field:\n\napiVersion: v1 kind: Service metadata: name: bgd-canary labels: app: bgdapp spec: ports: - name: http port: 8080 selector: app: bgdapp\n\n214\n\n|\n\nChapter 8: Advanced Topics",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "Finally, create the Istio Virtual Service to be updated by Argo Rollouts to update the canary traffic for each service:\n\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bgd spec: hosts: - bgd http: - route: - destination: host: bgd weight: 100 - destination: host: bgd-canary weight: 0 name: primary\n\nStable Kubernetes Service\n\nCanary Kubernetes Service\n\nRoute name\n\nAfter applying these resources, we’ll get the first version of the application up and running:\n\nkubectl apply -f bgd-virtual-service.yaml kubectl apply -f service.yaml kubectl apply -f service-canary.yaml kubectl apply -f bgd-isio-rollout.yaml\n\nWhen any update occurs on the Rollout object, the canary release will start as described in the Solution. Now, Argo Rollouts updates the bgd virtual service weights automatically instead of playing with pod numbers.\n\nSee Also\n\nArgo Rollouts - Kubernetes Progressive Delivery Controller •\n\nIstio - Argo Rollouts •\n\n• Istio\n\n• Istio Tutorial from Red Hat\n\n8.6 Use Advanced Deployment Techniques\n\n|\n\n215",
      "content_length": 1042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "A accounts\n\ncontainer registry services, creating, 7-8 GitHub, creating, 9-10\n\nadvanced deployment techniques, 209 Agile, 5 application deployment model, 4-5 Application resource file (Argo CD), 157 applications Argo CD\n\nremoving, 164 self-healing, 164-166 updating deployment files, 161\n\ncompiling, 108-113\n\nGitHub Actions, 150-153 from private repositories, 114-116 Tekton Triggers, 135-139\n\nconfiguration values, 43 container images, 17\n\ncreating with Docker, 18-23\n\ncontainerizing using Tekton Tasks, 117-120 deployment, 4-5\n\nArgo CD, 156-162 Argo Rollouts, 208-215 automatic with webhooks, 198-200 to Kubernetes, 122-125 to multiple clusters, 200-205 to multiple namespaces, 57-60 Tekton Pipelines, 125-134\n\nHelm-packaged, updating with Tekton Pipe‐\n\nline, 144-146 Java, adding Jib, 24-25 listing with Argo CD, 158 packaging, 108-113\n\nIndex\n\nGitHub Actions, 150-153 from private repositories, 114-116 Tekton Triggers, 135-139\n\nPython, Dockerfile, 19 registering, Argo CD, 158 synchronization (Argo CD), 158-159 defining time windows, 187-189\n\nApplicationSets, 191, 201-203 appVersion tag, 80 Argo CD, 155\n\nadmin account, password, 156 application deployment\n\nautomatic with webhooks, 198-200 updating files, 161\n\napplication synchronization, defining time\n\nwindows, 187-189\n\napplications\n\ndeploying, 156-162 deploying to multiple clusters, 200-205 listing, 158 registering, 158 removing, 164 self-healing, 164-166 synchronizing, 158-159, 162-166\n\nApplicationSet resource, creating, 201-203 ApplicationSets, 191 automated policy, 162 container images, updating automatically,\n\n171-178\n\ncron expressions, 188 Git repositories, registering, 179-182 Helm hooks, 171 Helm manifests, deploying, 168-171 installing, 156-157\n\n217",
      "content_length": 1726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Kubernetes manifests, setting deployment\n\norder, 182-187\n\nKustomize manifests, deploying, 166-168 manifests\n\nannotations, 173 deploying, 178-182 resources, pruning, 164 Sealed Secrets, installing, 192 secrets, encrypting, 195-198 security management, GitOps workflows,\n\n191\n\nSTATUS field, 158, 161 syncPolicy, 162\n\nArgo CD Image Updater, 172\n\nConfigMap, 177 configuration options, 173 default commit message, 177 installing, 172 repository read access, 176 version constraint field, 176\n\nArgo Rollouts, 192\n\napplications, deploying, 208-215 Kubectl Plugin, 209\n\nargocd CLI tool, installing, 156 authentication Jib, 25 Tekton schemes, 114-116 automated policy, Argo CD, 162\n\nB base directory files, Kustomize, 44 build logs, checking, 39 build-kaniko.yaml file, 38 Buildah, 17\n\ncontainer images\n\nbuilding from scratch, 31 creating, 28-31\n\nDockerfile, 28\n\nbuildah containers command, 28 Buildah, OS support, 27 Buildpacks, 17\n\ncontainer images, creating, 32-35 Homebrew, 32\n\nC canary release process, 209-215 CD (continuous deployment)\n\nArgo CD, 155 Kubernetes, Tekton Tasks, 122-125\n\n218\n\n|\n\nIndex\n\nCentos, container images, 28 Chart.yaml file creating, 68 dependencies section, registering Charts,\n\n88-92\n\nCharts, 67\n\navailable, 85 default values, checking, 87 deploying, 84-87\n\nwith dependencies, 88-92 External Secrets, installing, 196 Helm repositories, installing from, 145 history command, 74 packaging/distributing, 82-83 public repositories, 84 publishing, 82 registering, 84 repositories, 82 sharing to other Charts, 74 uninstalling, 74 validating, 84\n\nCI (continuous integration), 99, 155\n\nDrone, 148\n\nCI/CD (continuous integration/continuous\n\ndelivery) GitHub Actions\n\ncompiling applications, 150-153 packaging applications, 150-153\n\nKubernetes, 3\n\nCLI tool, 15 Cloud Native Buildpacks (see Buildpacks) cloud native CI, 99 cluster decision resource generator, 203 cluster generator, 203 Cluster Resource Definitions (CRDs) (see CRDs (Cluster Resource Definitions))\n\nclusters\n\nArgo Rollouts, installing, 209 local, creating, 12-15\n\nClusterSecretStore resource, 197-198 ClusterTasks, 129 ConfigMap, 43\n\nArgo CD Image Updater, 177 generating, Kustomize, 60-66 rolling updates, Helm, 67 ConfigMapGenerator, 61-65 configuration properties\n\nhashes, 64 merging, 64",
      "content_length": 2267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "rolling updates, 67\n\ncontainer engines, Docker, 17 container images\n\nBuildah, creating, 28-31 Buildpacks, 32-35 Centos, 28 consuming, 21 creating, 17\n\ncommands, 20 pushing to registry, 118-120\n\nDocker\n\ncache, verifying, 21 creating, 18-23 HTTPD, creating, 28 Jib, creating, 23-27 Kubernetes, building with Shipwright, 35-41 layers, 20, 22\n\ninternet connections, 22\n\nlist of available, 21 naming, 20 OCI, building, 27-31 plugins, adding, 24 pushing to registry, 21 references, updating, 139-141 revision number, checking, 79 running, 22 structure, 18-19 updating\n\nautomatically with Argo CD, 171-178 Helm, 79-81 Kustomize, 50-52\n\nversion tag, updating, 50-52\n\ncontainer registry\n\ncredentials, adding, 149 GitHub, 153\n\ncontainer registry services, creating accounts,\n\n7-8\n\ncontainers, 17\n\naccessing, 23 publishing, 7\n\ncontinuous deployment (CD) (see CD (contin‐\n\nuous deployment))\n\ncontinuous integration (CI) (see CI (continuous\n\nintegration))\n\ncontinuous integration/continuous delivery\n\n(CI/CD) (see CI/CD (continuous integra‐ tion/continuous delivery))\n\nCRDs (Cluster Resource Definitions), event\n\nhandling, 135\n\nCRDs (Kubernetes Custom Resources), 100 credentials\n\ncontainer registries, adding, 149 managing in external resources, 195-198\n\nCRI-O, 28 cron expressions, 188\n\nD daemonless container images, 17 daemons, Docker, 17 default service account, Tekton, 114 dependencies section (Chart.yaml), registering\n\nCharts, 88-92\n\ndeployment\n\napplications\n\nArgo CD, 156-162 Argo Rollouts, 208-215 to multiple clusters, 200-205 automatic with webhooks, 198-200 Charts with dependencies, 88-92 Kubernetes, updating with Tekton using\n\nKustomize, 139-144\n\nrolling updates, triggering automatically,\n\n93-98 updating\n\ncontainer images, 50-51 deployment files, 161 with Tekton TaskRun, 145\n\ndeployment.yaml, Helm Charts, 69 development cycle, 5-6 DevOps, 5 DevSecOps, 191 Docker, 17\n\ncontainer images creating, 18-23 verifying, 21\n\ninstalling, 18\n\ndocker images command, 25 docker pull command, 21 docker run command, 22 Dockerfiles, 17 Buildah, 28 defining, 18-20\n\nDockerHub, accounts, 7-8 dockerless container images, 17, 23\n\n(see also Jib) creating, 23-27 kaniko, 37\n\nDrone\n\nIndex\n\n|\n\n219",
      "content_length": 2183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "components, 148 configuring, 149 installing, 148 Kubernetes pipelines, creating, 148-150 repositories, activating, 149\n\ndrone.yaml, creating, 149 dynamic storage support, Kubernetes, 130\n\nE environments, deploying to multiple, 57-60 event handling, CRDs, 135 EventListener, 136, 137 External Secrets, managing credentials, 195-198\n\nF forks, creating repositories, 11-12\n\nG generators (ApplicationSet resource), 201-203 Git\n\nmanaging Kubernetes Secrets, 192-195 repositories registration, 9-12, 179-182\n\ngit generator, 203 git-clone, 141 git-commit, 141 GitHub\n\naccounts, creating, 9-10 container registry, 153\n\nGitHub Actions applications\n\ncompiling, 150-153 packaging, 150-153\n\nworkflow, 151\n\nGitOps\n\nbenefits, 2 development cycle, 5-6 loops, 4 principles, 2 project structure, 4 security, 191 workflow, 5 Working Group, 2\n\nGradle, 23\n\nH hashes, configuration properties, 64 Helm, 67\n\n220\n\n|\n\nIndex\n\napplications, updating with Tekton Pipeline,\n\n144-146 Charts, 67, 68\n\nchecking default values, 87 creating directories, 68 deploying, 84-87 deploying with dependencies, 88-92 finding available, 85 history command, 74 installing External Secrets, 196 installing to Kubernetes clusters, 73 packaging/distributing, 82-83 public repositories, 84 publishing, 82 registering, 84 rendering, 72 sharing to other Charts, 74 uninstalling, 74 validating, 84\n\ncontainer images, updating, 79-81 default values, overriding, 73 elements, relationships of, 71 hooks, Argo CD support, 171 installed elements, listing, 74 manifests, deploying with Argo CD,\n\n168-171\n\nprojects, creating, 68-74 scaffolding projects, 75 template statements, reusable, 75-78 values, overriding, 81 Helm Chart repositories, 84 helm create <name> command, 75 helm rollback command, 80 helm template command, 72 helm-upgrade-from-repo Task, 144 _helpers.tpl, 75-78 history command, 80 Homebrew, Buildpacks, 32 HTTPD container image, creating, 28 httpd package, installing, 28\n\nI images, creating, 17 index.yaml, updating, 82 install command, 84, 85\n\nJ Java services, deploying, 88-92 Java, Jib, 24-25",
      "content_length": 2060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Jib, 17\n\nbenefits, 24 container images creating, 23-27 storing in cache, 26\n\nDockerfiles, 23 Java applications, 24-25\n\nJSON Patch, updating container image fields,\n\n52-55\n\nK kaniko, 36-39 kind, 16 kubectl\n\napply command, 46 Kustomize, 44 Tekton, installing, 101\n\nKubectl Plugin (Argo Rollouts), 209 Kubernetes, 1\n\nadvanced deployment techniques, 209 application deployment model, 4-5 applications\n\ndeploying to, 122-125 Tekton Pipelines, 125-134\n\nCI/CD (continuous integration/continuous\n\ndelivery), 3\n\nCLI tool, 15 clusters, creating locally, 12-15 ConfigMap, 43 ConfigMap, generating with Kustomize,\n\n60-66\n\ncontainer images, building with Shipwright,\n\n35-41\n\ncontainerized applications, 17 dynamic storage support, 130 ExternalSecrets, 195 fields\n\nadding with Kustomize, 54 updating with Kustomize, 52-56\n\nGitOps loops, 4 GitOps project structure, 4 Helm Charts\n\ninstalling, 73 listing installed elements, 74\n\ninstalling, 13 manifests\n\nautomatic updates, 139-144 setting deployment order in Argo CD,\n\n182-187\n\npipelines, creating with Drone, 148-150 resource files, creating, 44-45 resources, deploying with Kustomize, 44-49\n\nKubernetes Operators\n\ninstalling Tekton components, 102\n\nKubernetes Secrets, 37 creating, 115-115 Git repositories, registering, 181 managing, 192 ServiceAccount, attaching, 117 Tekton, 114\n\nkustomization.yaml, 44\n\nConfigMapGenerator, 62 container images, updating, 50 creating, 46 referencing external assets, 48-49 referencing from another kustomiza‐\n\ntion.yaml file, 47\n\nKustomize, 43\n\nbase directory files, 44 ConfigMap, generating, 60-66 container images, updating, 50-52 deployment, multiple namespaces, 57-60 Kubernetes adding fields, 54 Kubernetes manifests, automatic updates,\n\n139-144\n\nKubernetes resources, deploying, 44-49 Kubernetes, updating fields, 52-56 manifests, deploying with Argo CD,\n\n166-168\n\npreappend/append values to resources, 60 web pages, deploying, 44-47 kustomize build command, 51 kustomize command, building resources, 48-49\n\nL list generator, 203 local clusters, creating, 12-15 loops, GitOps, 4\n\nM manifests\n\nArgo CD IU annotations, 173 deploying, Argo CD, 178-182 Dockerfiles, 17 Helm, deploying with Argo CD, 168-171 Kustomize, deploying with Argo CD,\n\n166-168\n\nNode.js packages, 33\n\nIndex\n\n|\n\n221",
      "content_length": 2262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "synchronizing, 4 updating automatically, 139-144\n\nmatrix generator, 203 Maven, 23\n\ncontainer images, building, 24\n\nmerge generator, 203 Minikube\n\napplication deployment, Argo CD web‐\n\nhooks, 199\n\ncontainer/virtualization technologies, 12 installing, 13 IP, accessing, 160 platform-specific files, 13\n\nN namespace field, deploying applications, 57-60 namespaces\n\ndeploying to multiple, 57-60 tekton-pipelines, 102-103\n\nnewTag field, updating, 52 Node.js, Buildpacks container images, 33\n\nO OCI containers, 17 building, 27-31\n\nOpen Container Initiative, 17 OpenShift Pipelines, 129\n\nP pack builder inspect paketobuild‐\n\npacks/builder:base command, 33\n\npack builder suggest command, 33 package command, 82 passwords, Argo CD admin account, 156 Patch expressions, modifying Kubernetes\n\nresources, 55\n\npatch files, creating, 55 pipeline, creating for Kubernetes with Drone,\n\n148-150\n\nPipelineRun object, 128 Pipelines (Tekton), 100-101\n\napplications, deploying to Kubernetes,\n\n125-134\n\nautomating, 135-139 creating, 127 flow, 125 Helm-packaged applications, updating,\n\n144-146\n\n222\n\n|\n\nIndex\n\nmanifests, automatic updates, 139-144 Tasks, adding, 141 Tekton Tasks, 107\n\nplugins, building container images with Jib, 24 Podman, OS support, 27 PostgreSQL servers, deploying, 85-87 projects Helm\n\ncreating, 68-74 scaffolding, 75\n\nstructure, 4 .properties files, 65 Public Helm Chart repositories, 84 pull request generator, 203, 206 pull requests\n\ncreating, 207 deploying to clusters, 206-208\n\nQ Quay\n\nKubernetes Secrets, 37 logging into, 21 registration, 8\n\nR registries\n\ncontainer images, pushing to, 21 logging into, 21 repo add command, 84 repo index command, 83 repo list command, 84 repo update command, 85 repositories\n\nactivating in Drone, 149 directory layout, 82 forks, creating, 11-12 Git\n\nregistering, 9-12 registering with Argo CD, 179-182\n\nGitHub Actions, 151 Helm Charts, 82\n\ndeploying, 84-87 public, 84\n\nprivate, compiling/packaging applications\n\nwith Tekton, 114-116\n\nresource files (Kubernetes), creating, 44-45 resource hooks\n\nmanifest deployment, 183-184\n\ndeletion policies, 184",
      "content_length": 2088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "sync waves, 185\n\nmodifying manifest order, 182-187\n\nresources\n\nConfigMapGenerator, 61-65 deploying with Kustomize, 44-49 pruning by Argo CD, 164 Roles, ServiceAccounts, 123 rollback command, 80 Runner (Drone), 148\n\nS scm provider generator, 203 Sealed Secrets, 192 installing, 192 Secrets, creating, 193-194\n\nsearch command, 85 secrets\n\nDrone, adding to container registries, 149 encrypting with Argo CD, 195-198\n\nSecretStore resources, 197-198 security, GitOps workflows, 191 selfHeal property (Argo CD), 165 Server (Drone), 148 service.yaml (Helm Charts), 70 ServiceAccounts creating, 118 Roles, 123 secrets, attaching, 117 Tekton, 115-116 Tekton Tasks, creating, 123 Tekton Triggers, 136\n\nservices\n\nGitHub, 9-12 GitLab, 12\n\nsha256sum template function, 93, 96-98 Shipwright, building container images, 35-41 show command, 87 Skopeo, 28 SSH, registering Git repositories, 181 statements, reusable, 75-78 STATUS field (Argo CD), 158, 161 Strategic Merge Patch, 55 Sutter, Burr, \"Teaching Elephants to Dance (and\n\nFly!)\", 6\n\nsync waves, modifying manifest order, 182-187 syncPolicy, Argo CD, 162\n\nT TaskRun objects, 108\n\ncreating, 111-113 logs, 113 Tasks (Tekton) applications\n\ncontainerizing, 117-120 deploying to Kubernetes, 122-125\n\nbuild-app, 109-111\n\ncreating, 111-111\n\ncontainer image references, updating,\n\n139-141 creating, 107-108 displaying running, 134 Helm Charts, installing from Helm reposi‐\n\ntory, 145 parameters, 112 Pipelines\n\nadding to, 141 deploying applications to Kubernetes,\n\n125-134\n\nresults as input for succeeding Tasks, 143 ServiceAccounts, creating, 118, 123 Tekton Hub, 128 workspaces, persisting, 130\n\n\"Teaching Elephants to Dance (and Fly!)\", 6 Tekton, 36, 99-100 applications\n\ncompiling, 108-113 compiling from private repositories,\n\n114-116\n\ndeploying to Kubernetes, 122-125 packaging, 108-113 packaging from private repositories,\n\n114-116\n\nauthentication schemes, 114-116 configuration, verifying, 106 CRDs (Kubernetes Custom Resources), 100 default service account, 114 fields, 109-109 installing, 100-107 Kubernetes manifests, automatic updates,\n\n139-144\n\nKubernetes Secrets, 114 creating, 115-115\n\nmodules, 101 Pipelines, 100-101 creating, 127 deploying applications to Kubernetes,\n\n125-134\n\nflow, 125\n\nIndex\n\n|\n\n223",
      "content_length": 2252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "updating Helm-packaged applications,\n\n144-146 pods, 103-105 ServiceAccounts, 115-116 Tasks\n\nbuild-app, 109-111 containerizing applications, 117-120 creating, 107-108, 111-111 creating ServiceAccounts, 123 parameters, 112 Tekton Dashboard component\n\naccessing, 106 displaying running Tasks, 134 installing, 105-105\n\nTekton Hub, 128\n\nTasks, Helm support, 144 Tekton Pipelines component, 102 Tekton Triggers component\n\napplications\n\ncompiling automatically, 135-139 packaging automatically, 135-139\n\ninstalling, 103-105 ServiceAccounts, 136\n\ntemplate command, 81 template statements, reusable, 75-78 TLS client certificate, configuring, 180 TriggerBinding, 135 TriggerTemplate, 135\n\n224\n\n|\n\nIndex\n\nU update-digest, 141 upgrade command, 79-80\n\nV values.yaml (Helm Charts), 70 VirtualBox, installing, 12 virtualization systems, 12\n\nW web pages, deploying with Kustomize, 44-47 webhooks, 135\n\napplication deployment, 198-200 parameters, 138\n\nwelcome page, copying to container, 29 workflow, 5\n\ndevelopment cycle, 5\n\nWorking Group, 2 workspaces, persisting, 130\n\nY YAML, 43\n\nkustomization.yaml, 44 Patch expressions, 55",
      "content_length": 1112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "About the Authors\n\nNatale Vinto is a software engineer with more than 10 years of expertise in IT and ICT technologies and a consolidated background in Telecommunications and Linux operating systems. As a solution architect with a Java development background, he spent some years as an EMEA Specialist Solution Architect for OpenShift at Red Hat. He is coauthor of Modernizing Enterprise Java for O’Reilly. Today Natale is lead developer advocate at Red Hat, helping people within communities and customers have success with their Kubernetes and cloud native strategy. You can follow more frequent updates on his Twitter feed and connect with him on LinkedIn.\n\nAlex Soto Bueno is a director of developer experience at Red Hat. He is passionate about the Java world, software automation, and he believes in the open source software model. Alex is the coauthor of Testing Java Microservices (Manning), Quar‐ kus Cookbook (O’Reilly), and the forthcoming Kubernetes Secrets Management (Man‐ ning), and is a contributor to several open source projects. A Java Champion since 2017, he is also an international speaker and teacher at Salle URL University. You can follow more frequent updates on his Twitter feed and connect with him on LinkedIn.\n\nColophon\n\nThe animal on the cover of GitOps Cookbook is a yellow mongoose (Cynictis penicil‐ lata). These small mammals are found in sub-Saharan Africa, primarily in forests, woodlands, grasslands, and scrub. They are sometimes referred to as red meerkats. Yellow mongoose are smaller than most other species, weighing only 16–29 ounces. There are 12 subspecies that vary in color, body size (9–13 inches), tail (7–10 inches), and length of coat: the northern subspecies found in Botswana are typically smaller with grizzled grayish coats while the southern populations in South Africa and Namibia are larger and tawny yellow. All subspecies have slender bodies with lighter fur on the chin and underbelly, small ears, pointed noses, and bushy tails.\n\nYellow mongoose are carnivores that mainly feed on insects, birds, frogs, lizards, eggs and small rodents. They are social species and live in colonies of up to 20 individuals in extensive, permanent burrows with many entrances, chambers, and tunnels. Most of their day is spent foraging or sunbathing outside the burrow. In the wild, they breed from July to September with most females giving birth to two or three offspring in October and November. The young are born in an underground chamber and stay there until they are weaned (about 10 weeks). Yellow mongoose are considered fully grown at 10 months old.",
      "content_length": 2604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Yellow mongoose are classified as a species of least concern by the IUCN; their populations are stable and they don’t face any major threats. They do carry of strain of rabies in the wild and are seen as pests and hunted by farmers in parts of South Africa. Many of the animals on O’Reilly covers are endangered; all of them are important to the world.\n\nThe cover illustration is by Karen Montgomery, based on an antique line engraving from The Pictorial Museum of Animated Nature. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 663,
      "extraction_method": "Unstructured"
    }
  ]
}